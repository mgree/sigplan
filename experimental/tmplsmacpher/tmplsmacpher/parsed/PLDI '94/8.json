{"article_publication_date": "06-01-1994", "fulltext": "\n Improving Semi-static Branch Prediction by Code Replication Andreas Krall Institut fiir Computersprachen \nTechnische Universitat Wien Argentinierstrafie 8 A-104O Wien andi@mips. complang. tuwien. ac. at Abstract \n Speculative execution on superscalar processors de\u00admands substantially better branch prediction than \nwhat has been previously available. In this paper we present code replication techniques that improve \nthe accurracy of semi-static branch prediction to a level comparable to dynamic branch prediction schemes. \nOur technique uses profiling to collect information about the correlation be\u00adtween different branches \nand about the correlation be\u00adtween the subsequent outcomes of a single branch. Us\u00ading this information \nand code replication the outcome of branches is represented in the program state. Our ex\u00adperiments have \nshown that the misprediction rate can almost be halved while the code size is increased by one third. \n1 Introduction Branch prediction forecasts the direction a conditional branch will take. It reduces \nthe branch penalty in a pro\u00adcessor and is a basis for the application of compiler opti\u00admization techniques. \nIn this paper we are mainly inter\u00adested in the latter use, since we will apply branch predic\u00adtion to \ncompiler based speculative execution and other code motion techniques. Static branch prediction relies \nonly on information that is obtained by static analysis of the program. Semi-static branch prediction \nuses pro\u00adfiling [Wa191] to predict the branch direction. Dynamic branch prediction saves branch directions \nin hardware history registers and tables and uses this information to predict branches during run time. \nThe misprediction rate of semi-static branch prediction strategies is about Permission to co y without \nfee all or part of this material is granted provid Jthat the copies are not made or distributed for direct \ncommercial advantage, the ACM copyright notice and the title of the publication and its date appear, \nand notice is given that copying is by permission of the Association of Computing Machinery. To copy \notherwise, or to republish, requires a fee and/or specific permission. SIGPLAN 94-6/84 Orlando, Florida \nUSA 0 1994 ACM 0-89791 -662-xJ9410006..$3.5O half that of the best static branch prediction strategies \n[BL93]. The misprediction rate of the best dynamic branch prediction strategies is about half that of \nsemi\u00adstatic branch prediction strategies [YN93]. Compile time optimizations like code motion and speculative \nexecution rely on an accurate branch predic\u00adtion strategy. For many optimizations existing branch prediction \nstrategies are not sufficient. So we looked for a method to improve the accuracy of compile time branch \nprediction. Our approach replicates a piece of code, so that the branches in the replicated code pieces \nare more predictable than in the original code. This idea was inspired by the work of Pettis and Hanson \n[PH90], who use profiling for code positioning to improve cache behaviour, and by the work of Mueller \nand Whalley [MW92], who use code replication to avoid jumps. Chapter 2 presents existing branch prediction \nmeth\u00adods. Chapter 3 contains a description of our profiling tool and presents the results of profiling \nour benchmark suite. Chapter 4 describes the techniques for compact\u00ading the collected history information \nin order to make it usable for semi-static branch prediction. Chapter 5 explains the code replication \ntechniques and shows the effects on the code size. 2 Branch Prediction Strategies 2.1 Static Branch \nPrediction Smith [Smi81] explored some simple heuristics and com\u00adpared them with simple dynamic branch \nprediction st rat egies. He uses following static strategies: predict that all branches will be taken \n . predict that only certain branch operation codes will be taken  predict that all backward branches \nwill be taken  The misprediction rate of these simple branch pre\u00addiction strategies is about 30%, but \nsome benchmark programs have a misprediction rate of 65%. A more sophisticated implementation of static \nbranch prediction has been done by Ball and Larus [BL93]. Their branch prediction strategy is based on \na control flow analysis of the program to determine loops, Fur\u00adthermore, the code surrounding a branch \ndetermines the kind of the branch. They applied different heuristics in different lexicographic orders. \nThe most successful order was: Point pointer comparison (predict not taken) Call avoid branches to blocks \nwhich call a subrou\u00ad tine Opcode decide the branch direction on the branch in\u00ad struction opcode Return \navoid branches to blocks which return from a subroutine Store avoid branches to blocks which contain \na store instruction Loop predict that the loop branch will be taken Guard branch to a block which uses \nthe operands of the branch With this heuristics Ball and Larus reach an average misprediction rate of \n207 0, about twice the mispredic\u00adtion rate achieved by profile based branch prediction. 2.2 Semi-static \nBranch Prediction Semi-static branch prediction is based on profiling [Wa191]. McFarling and Hennessy \n[MH86] were the first to suggest the use of profiling for branch prediction and gave some data. Fisher \nand Freudenberger [FF92] made a comprehensive study of branch prediction based on profiling. Instead \nof using the misprediction rate as a measure, they gave the average number of executed in\u00adstructions \nper mispredicted branch for the programs of the SPEC benchmark suite. Furthermore they stud\u00adied the influence \nof different datasets on the accuracy of the prediction. Fisher and Freudenberger report be\u00adtween 80 \n70 and 100% of the prediction rate for the best prediction using another dataset instead of the reference \ndata set. The worst prediction varies between 50% and 100%. 2.3 Dynamic Branch Prediction Dynamic branch \nprediction depends on information available only at run time. Simple strategies have been studied by \nSmith [Smi81]. Among other strategies he proposed the following strategies: predict that a branch will \ntake the same direction as on its last execution  associate a counter with a branch and decide on the \ncounter value  The counter based strategy increments the counter using saturation arithmetic if the \nbranch was taken and decrements it otherwise. If the counter value is in the upper half of the value \nrange, the branch is predicted taken. If the value is in the lower half, the branch is predicted not \ntaken. A two bit counter gives the best result. The misprediction rate is comparable to the pro\u00adfile \nbased strategies. Today the best dynamic branch prediction strategies are based on two levels of history \ninformation. One level of information represents the outcome of the last branches. It is usually implemented \nas a shift register in hardware. The second level of history information contains the different patterns \nof history register values combined wit h a two bit counter, which gives the branch prediction for this \npattern. Yeh and Patt presented a strategy whith a history register for each branch and a pattern table \nfor each branch [YN92]. Pan, So and Rah\u00admeh presented a strategy with a single global history register \nand a pattern table for each branch [PSR92]. Since their strategy depends on the correlation of dif\u00adferent \nbranches, they called it branch correlation. Later Yeh and Patt studied all nine combinations of one \nglobal history register, a history register for a set of branches and a history register for each branch \nwith one global pattern table, a pattern table for a set of branches or a pattern table for each branch \n[YN93]. The best strat\u00adegy, a history register for each branch and a pattern table for a set of branches, \nachieved an average mispre\u00addiction rate of about 3% having an implementation cost of 128K bits.  3 Collecting \nBranch Correlation Information We are interested in a branch prediction strategy usable for predictions \nat compile time. So we tried to adapt the dynamic branch prediction strategies of [YN93] for semi-static \nbranch prediction. In contrast to a dynamic branch strategy we are not restricted by the size of the \nhistory tables. So we used a pattern table for each branch. Furthermore, we used only history register \nschemes which are meaningful for code replication. A global history register means that a branch depends \non other branches. We will call this strategy correlated branch strategy. A history register for each \nbranch (lo\u00adcal history register) means that this branch depends on previous executions of the same branch. \nWe will call this scheme loop branch strategy and branches which use this scheme loop branches. For each \npattern in the pattern table we predict the more frequent direction. Since no existing profiling or trace \ntool fulfilled our needs we developed our own profiling and analysis tools. To get a trace of the executed \nbranches we insert code in a program which writes trace information to a file. The trace information \ncontains the branch number and the branch direction. In compressed form a trace of 5 mil\u00adlion branches \noccupies about lMB. In contrast to the QPT profiling and tracing tool [Lar93], which inserts trace instructions \nin the object code of a program, our tool inserts trace code in the assembly language source of a program. \nThe advantage of our method is that address calculation and code relocation is done by the assembler. \nThe disadvantage of our method is that we cannot trace system library functions. Furthermore, the trace \ntool does a control flow analysis and saves the de\u00adscription of branches, a control flow graph and loop \nin\u00adformation in a file. An analysis tool processes the trace and generates tables that describe the branch \nprediction accuracy and the effects on code size. Programs with tracing enabled are about three times \nslower than with\u00adout. The analysis of the trace is done in a few seconds. A production version of the \nprofiling tool will include the first part of the analysis tool which transforms the trace data into \nthe pattern table. This enables profiling with an unlimited number of branches. With these tools we evaluated \na set of eight bench\u00admark programs. Since especially integer programs need better branch prediction, \nwe included only one floating point program. The programs are: abalone a board game employing alpha-beta \nsearch c-compiler the lcc compiler front end of Fraser &#38; Han\u00ad son compress a file compression utility \n(SPEC) ghostview an X postscript previewer predict our profiling and trace tool prolog the minivip Prolog \ninterpreter scheduler an instruction scheduler doduc hydrocode simulation (floating point) (SPEC) These \nbenchmark programs have been compiled with the C or Fortran compilers with optimization enabled. We traced \nthe whole program up to a maximum of 10 million branch instructions, For the purpose of com\u00adparison we \nevaluated dynamic and semi-static branch prediction strategies: last direction a branch takes the same \ndirection as the last time (dynamic) 2 bit counter decide on the value of a 2 bit counter (dynamic) two \nlevel 41K bit a lK entry 9 bit history register and a 16K entry pattern table with 2 bit counters (dynamic) \nprofile predict the most frequent direction (semi-static) 6 bit correlation predict using one global \n6 bit history register (semi-static) 6 bit loop use 6 bit history registers for every branch (semi-static) \n9 bit loop use 9 bit history registers for every branch (semi-static) loop correl ation the best of 6 \nbit correlation and 9 bit loop for each branch (semi-static) Furthermore, we collected information about \nthe static number of branches, the number of branches that were executed during the run of the program \nand the number of branches that could be improved by the loop\u00adcorrelation strategy compared to profile \nbranch predic\u00adtion. Table 1 gives the results.  4 The Branch Prediction State Machine Table 1 shows \nthat branch prediction strategies using history information significantly improves the predic\u00adtion accuracy. \nBut how can this information be used in a semi-static branch prediction scheme? As an ex\u00adample, we consider \na branch embedded in a loop that is alternating between taken and not taken. Figure 1 shows the flow \ngraph of an example loop. Basic block 1 contains the branch that can be im\u00adproved by code replication. \nThe loop is duplicated and the branch switches between the two copies. Each copy of the loop represents \na state that remembers the branch direction of the previous execution of this branch. State 0 has the \nmeaning that the last time the branch was not taken, State l has the meaning that the branch was taken. \nIn both copies of the loop the branch is now predicted correctly 100% of the time. Basic blocks 2b and \n3a are not replicated. Since there is no path to them they have been discarded. This is the scheme we \nimplement for 1 bit history in\u00adformation. Unfortunately, we cannot use this scheme for longer histories, \nsince this would increase the pro\u00adgram size too much. A replicated loop representing a 9 bit history \nwould result in 512 copies of the loop. On the other hand information in the history tables is very sparse. \nTable 2 gives the percentage of the pattern ta\u00adble fill rate. Only between 0.6 and 21 percent of the \n9 bit pattern table entrys of the executed branches are used. We therefore construct branch prediction \nstate machines for loop branches and correlated branches that 1 aba\u00ad c-com\u00ad com\u00ad ghost\u00ad pre\u00ad pro\u00ad sche\u00ad \ndo\u00ad lone piler press view diet log duler duc t=== 22.9 18.4 18.0 1.27 11.8 14.5 13.8 7.53 P 20.8 6.82 \n18.7 14.9 12.7 13.5 14.5 13.7 17.2 1.20 1,85 1.27 7.70 4.64 7.97 11.3 10.7 11.3 10.9 11.1 13.6 3.87 0.89 \n3.99 6 bit correlation 11.6 8.67 14.2 0.31 6.30 7.53 9.28 1.64 6 bit loop 9.67 9.33 13.4 0.41 5.22 7.18 \n7.76 1.55 9 bit loop 6.89 7.79 13.0 0.36 4.12 5.72 5.97 1.33 6.47 6.97 12.6 0.21 3.72 5.35 5.02 1.11 \n496 3645 170 1399 451 2324 490 665 \\ executed branches 311 2183 70 517 345 819 431 487 I imtmoved branches \n209 658 13 84 92 320 242 74 Table 1: misprediction rates of different branch prediction strategies in \npercent 1 la lb 23 2a3b 4 4a 4b 3 m original loop state O state 1 Figure 1: flow graph of an intra loop \nbranch and a 2 state machine aba\u00ad c-com\u00ad com\u00ad ghost\u00ad pre\u00ad pro\u00ad sche\u00ad do\u00ad lone piler press view diet log \nduler duc 1 bit history 92.6 84.2 67.9 68.1 82.9 85.0 86.5 84.1 2 bit history 84.6 66.1 46.1 42.9 64.6 \n68.6 75.6 60.9 3 bit history 74.4 49,5 31.4 25.0 48.2 53.8 65.8 40.4 4 bit history 62.9 36.1 21.8 14.1 \n35.8 41.3 56.8 25.2 5 bit history 52.1 25,8 16.0 7.85 26.7 31.3 48.4 15.1 6 bit history 42.5 18.2 12.3 \n4.27 20.0 23.2 39.6 8.79 7 bit history 34.3 12.7 10.1 2.30 14.8 16.6 30.8 5.05 8 bit history 27.1 8.70 \n8.77 1.22 10.6 11.5 22.7 2.85 9 bit historv 21.0 5.89 8.00 0.65 7.28 7.56 15.8 1.58 Table 2: fill rate \nof the history tables in percent 100 . 0 II IJ Figure 2: branch prediction state machine 5/1 1.I 1. \n  J&#38;nA nA L! L?l Figure 3: branch prediction state machine 4/2 use fewer states but have nearly \nthe same prediction ac\u00adcuracy, Furthermore, we divide loop branches in irztm loop branches that occur \ninside a loop, and exit loop branches which may leave the loop. 4.1 Intra Loop Branches Intra loop branches \ndo not leave the loop. For intra loop branches a state represents the last n branch directions of previous \niterations of the loop. It is necessary that each state can be reached from another state and via other \nstates from the initial state, i.e. the state used in the first iteration. An example for a valid intra \nloop state machine is the state machine 5/1 in figure 2. A state is a copy of the loop, e.g. the loop \nof figure 1. The digits in the state describe the directions of the last branches. O means that the branch \nwas not taken, l means that the branch was taken. The rightmost digit represent the direction of the \nlast iteration. State l and state 0 representing only one branch direction are used as catch-all states \nif more specialized states do not Figure 4: branch prediction state machine 3/3/3/1 match. An arc labeled \n0 describes the state transition if a branch was not taken. An arc labeled l describes the transition \nif the branch was not taken. Each state can be used as an initial state for the first iteration. It is \nalso possible to construct state machines where the smallest state represents the last two directions \nof the branch. In this case there are four catch-all states which match when specialized states fail. \nFigure 3 shows such a state machine. Another example is the state machine of figure 4 that has a loop \nof three states with size three (the states CC1lO , 101 and 011 ). This loop can be reached from the \ncatch all state l via the state 11 . Although a state machine as in figure 2 usually gives the best result \nfor a 6 state machine, we make an exhaustive search in the pattern table to find the best state machine. \nFor each 9 bit pattern we collected the number of taken and not taken branches. This information is used \nto com\u00adpute the number of taken and not taken branches for all shorter patterns. Adding now the counts \nfor the more frequent direction of all states of a branch prediction state machine and taking care that \npatterns are counted not more than once, we get the number of correct pre\u00addicted branches for the state \nmachine. The branch pre\u00addiction state machine with the highest number of correct predicted branches is \nselected. Table 3 shows the mis\u00adprediction rates of the benchmark programs, where only a reduced state \nmachine is used instead of the complete pattern table. A state machine with 2 states implements exactly \nthe 1 bit history scheme, so no data for a two bit state machine is presented. A state machine with n \nstates usually needs the history information up to a length n 1. So we grouped always a history with \nn bits with a n + 1 state machine to show the effect of accuracy loss. 101 replicated code small. The \ntable shows that the cor\u00ad relation information can be compacted with very small 1 loss. 0 Figure 5: \nloop exit branch machine  4.2 Loop Exit Branches Loop exit branches are those branches that go from \nin\u00adside the loop to the surrounding code. Therefore, they are more rest ricted than intra loop branches. \nLoop exit branch state machines have one initial state that rep\u00adresents the loop exit in the last execution \n(state 0 ). The other states represent the loop iterations (the states with an increasing number of 1 \n). Figure 5 shows a loop exit branch machine. It is not necessary that a loop remains in the state with \nthe longest history informa\u00adtion. If a loop has a high probability of an even or odd number of iterations, \nthe loop would change between the two states with the longest history information as shown in figure \n5. Such a behavior can be detected us\u00ading longer history information. The misprediction rates of loop \nexit state machines are shown in table 3, too. 4.3 Correlated Branches The state machines for correlated \nbranches are differ\u00adent from the state machines for loop branches. The states of a loop branch depend \non each other, the states of a correlated branch are independent. A state in a correlated branch state \nmachine represents a path from correlated branches to the branch to be predicted, The correlated branch \nstate machine is the set of those paths which give the lowest misprediction rate. One state cov\u00aders the \ncase where the control flow matches none of the paths. The code replication for correlated branches is \nsimilar to [MW92]. The difference is that our aim was to save information about the branch direction, \nwhereas the aim of Mueller and Whalley was to avoid uncondi\u00adtional jumps. Table 4 gives the misprediction \nrate of correlated branches. We used a maximum path length of n 1 for an n state machine to keep the \nsize of the  5 Branch Prediction and Pro\u00adgram Size To find intra loop, exit loop and other branches, \na con\u00adtrol flow analysis of the program is performed. The program is divided into basic blocks and a \ncontrol flow graph is constructed. Natural loop analysis [ASU86] is performed. With this information \nthe state machines for loop exit and intra loop branches are selected. For all branches all predecessors \nwith a path length less than the size of the state machine are collected, and the cor\u00adrelated branch \nstate machines are selected. The best available strategy for each branch is chosen. Branch pre\u00addiction \nis deteriorated because only half of the branches belong to a loop and because the state machines reduce \nthe accuracy of the branch prediction. Table 5 gives the misprediction rates of the benchmark programs \nfor the different sizes of the state machine ignoring the effects on program size. To study the effect \nof code replication on the code size, we added states to a program and measured how the code size increased \nand the misprediction rate was reduced. The states were added in such an order that the state that predicted \nthe largest number of branches and that increased the code size by the smallest amount was choosen first. \nThe first states reduce the mispredic\u00adtion rate substantially, later ones increase the code size considerably. \nThe appendix shows graphs of code size versus misprediction rate. The asymptotic behaviour is explained \nby the dependence between branches. If branches are in different loops, the number of states is only \nadded. If branches are in the same loop, the num\u00adber of states must be multiplied. Some programs reach \nthe best misprediction rate within a code size increase of a factor 1.5, other programs would increase \nthe code size more than thousand times. With the exception of abalone every program comes close to the \nbest achieva\u00adble misprediction rate by increasing the code size by less than 30%. An optimizer using \ncode replication for improving branch prediction will not improve the whole program, but only certain \nbranches. In general, an optimization technique like branch aligning or speculative execution is not \napplied to a branch whose prediction accuracy is low. If code replication improves the accuracy of the \nprediction for this branch, such an optimization can be applied. A cost function will calculate whether \nthe in\u00adcrease in code size (negative impact on instruction cache miss rate) is worth the gain in execution \ntime. 1 aba\u00ad c-com\u00ad com\u00ad ghost\u00ad pre\u00ad pro\u00ad sche\u00ad do- Ione piler press view diet log duler duc profile \n18.7 13.5 17.2 1.27 7.97 11.3 13.6 3.99 1 bit 16.9 12.4 16.0 0.63 7.36 10.0 10.4 2.27 2 bit 15.4 11.9 \n14.6 0.51 7.07 9.23 9.91 1.57 4 states exit I 15.4 12.0 15.7 0.57 7.08 9.49 10.2 2.02 4 bit 12.1 10.3 \n13.7 0.45 6.23 8.13 9.32 1.55 I 5 states loop I 13.0 10.7 14.0 0.46 6.41 8.51 9.53 1.55 I 5 states exit \nI 15.1 11.8 15.7 0.57 6.84 9.44 10.2 2.02 5 bit / 10.7 9.81 13.5 0.42 5.68 7.67 9.02 1.55 I 6 states \nloop I 12.3 10.5 13.8 0.44 6.26 8.21 9.42 1.55 I 6 states exit / 14,7 11.7 15.7 0.56 6.75 9.39 10.2 2.02 \n6 bit I 9.67 9.33 13,4 0.41 5.22 7.18 7.76 1.55 I 7 states loop I 11,3 10.3 13.6 0.42 5.96 7.98 8,49 \n1.55 ( 7 states exit 14.6 11.6 15.7 0.56 6.58 9.38 9.37 2.02 7 bit 8.76 8.78 13.2 0.40 4.74 6.74 7.21 \n1.55 8 states loop 10.7 10.1 13.6 0.42 5.76 7.84 8.32 1.55 8 states exit 14.4 11.6 15.6 0.55 6.48 9.37 \n9.30 2.02 8 bit 7.68 8,34 13.1 0.37 4.41 6.26 6.61 1.34 9 states loop 10.1 9.91 13.5 0.41 5.66 7.73 8.24 \n1.34 9 states exit 14.1 11.6 15.6 0.53 6.46 9.36 9.29 2.02 9 bit 6,89 7.79 13.0 0.36 4.12 5.72 5.97 1.34 \n10 states loop 9.64 9.71 13.4 0.39 5.51 7.61 8.17 1.34 10 states exit 13.8 11.6 15.6 0.53 6.46 9.35 9.28 \n2.02 Table 3: misprediction rates of loop and loop exit branches in percent aba-c-com-com-ghost-pre-pro-sche-do\u00ad \nlone piler press view diet log duler duc profile 18.7 13.5 17.2 1.27 7.97 11.3 13.6 3.99 1 bit 18.6 12.8 \n17.1 1.22 7.75 10.6 12.0 2.52 2 states 18.6 12.8 17.1 1.22 7.75 10.6 12.0 2.52 2 bit 16.2 11.7 17.1 1.05 \n7.37 9.89 11.1 1.95 3 states 16.2 11.7 17.1 1.05 7.37 9.90 11.1 1.95 3 bit 15.1 10.8 15.9 0.62 6.95 9.40 \n10.8 1.83 4 states 15.2 10.8 15.9 0.62 7.03 9.43 10.9 1.83 4 bit 13.9 9.58 $5.9 0.38 6.52 8.66 10.1 1.75 \n5 states 14.3 9.90 15.9 0.38 6.68 8.77 10.1 1.75 5 bit 12.7 9.12 15.6 0.34 6.39 8.08 9.61 1.70 6 states \n13.4 9.65 15.6 0.35 6.48 8.31 9.72 1.70 6 bit 11.6 8.67 14.2 0,32 6.30 7.53 9.28 1.64 7 states 12.6 9.42 \n14.2 0.34 6.40 8.02 9.50 1.64 Table 4: misprediction rates of correlated branches in percent aba\u00ad c-com\u00ad \ncom\u00ad lone piler press profile 18.7 13.5 17.2 2 states 16.8 12.5 16.1 3 states 14.6 11.1 14.9 4 states \n13.2 10.2 14.0 5 states 11.8 9.29 13.9 6 states 11.0 9.10 13.5 7 states 10.0 8.91 13.4 8 states 9.64 \n8.80 13.4 9 states 9.26 8.71 13.3 10 states 8.99 8.68 13.3 Table 5: best achievable  6 Further Work \nA problem of our code replication scheme is that the code size is multiplied if more than one branch \nin a loop should be improved. A possible solution treats all branches of that loop at the same time and \nconstructs a single state machine for all branches using a higher number of states. In that case the \nsearch for the optimal state machine must be replaced by a branch-and-bound search since the search time \ngrows exponentially with the number of states. Another work to be done is to measure the influence of \ndifferent data sets on the misprediction rate. We assume that code replicated programs are more sensitive \nto different data sets than the original program. So the results of Fisher and Freudenberger are not \nfully applicable [FF92]. Furthermore we will use the improved semi-static branch prediction strategy \nfor our global instruction scheduler and evaluate the effects on runtime and in\u00adstruction cache behaviour. \n 7 Conclusion We presented a code replication technique for im\u00adproving the accuracy of semi-static branch \nprediction. This technique combines different correlation strategies, Therefore, the prediction is sometimes \nmore accurate than dynamic prediction that uses only a single strat\u00adegy. Since our technique is semi-static, \nthe increased accuracy can be used by compile time optimization like code motion or speculative execution. \n ghost\u00ad pre\u00ad pro\u00ad sche\u00ad do\u00ad view diet log duler duc 1.27 7.97 11.3 13.6 3.99 0.82 7.58 9.86 11.8 2.33 \n0.59 7.26 9.05 10.8 1.62 0.44 6.80 8.59 10.6 1.57 0.34 6.24 7.86 9.87 1.56 0.31 6.03 7.50 9.47 1.51 0.30 \n5.76 7.29 8.47 1.48 0.30 5.60 7.24 8.38 1.48 0.30 5.56 7.18 8.34 1.28 0.29 5.45 7.12 8.32 1.28 misprediction \nrates in percent Acknowledgement We express our thanks to Manfred Brockhaus, An\u00adton Ertl, Ulrich Neumerkel, \nFranz Puntigam, and Jian Wang for their comments on earlier drafts of this pa\u00adper. We would also like \nto thank the reviewers for their helpful suggestions.  References [ASU86] Alfred V. Aho, Ravi Sethi, \nand Jeffrey D. Unm\u00adan. Compilers: Principles, Techniques, and Tools. Addison-Wesley, 1986. [BL93] Thomas \nBall and James R. Larus. Branch pre\u00addiction for free. In 1993 SIGPLAN Conference on Programming Language \nDesign and Imple\u00admentation. ACM, June 1993. [FF92] Joseph A. Fisher and Stefan M. Freuden\u00adberger. Predicting \nconditional branch direc\u00adt ions from previous runs of a program. In Fifth International Conference on \nArchitec\u00adtural Support for Programming Languages and operating Systems. ACM, October 1992. [Lar93] James \nR. Larus. Efficient program tracing. IEEE Computer, 26(5), May 1993. [MH86] Scott McFarling and John \nHennessy. Reducing the cost of branches. In 13th Annual Interna\u00adtional Symposium on Computer Architecture. \nACM, 1986. [MW92] Frank Mueller and David B. Whalley. Avoid\u00ading unconditional jumps by code replication. \nIn 1992 SIGPLAN Conference on Program\u00adming Language Design and Implementation. ACM, June 1992. [PH90] \nKarl Pettis and Robert C. Hansen. Profile guided code positioning. In 1990 SIGPLAN rnisprediction rate \nConference on Programming Language Design and Implementation. ACM, June 1990. [PSR92] Shien-Tai Pan, \nKimming So, and Joseph T. 15% Rahmeh. Improving the the accuracy of dy\u00adnamic branch prediction using \nbranch corre\u00ad10% \u00ad lation. In Fifth International Conference on Architectural Support for Programming \nLan\u00adguages and Operating Systems. ACM, October  5% 1992. code size 1 [Smi81] James E. Smith. A study \nof branch predic\u00adtion strategies. In 8th Annual International 1.25 1:5 1.75 Symposium on Computer Architecture. \nACM, 1981. Figure 7: c-compiler [Wa191] David E. Wall. Predicting program behavior using real or estimated \nprofiles, In 1991 5 IG-PLAN Conference on Programming Language Design and Implementation. ACM, June 1991. \n rnisprediction rate ~N92] Tse-Yu Yeh and Yale N.Patt. Alternative implementations of two-level adaptive \nbranch prediction. In 19th Annual International Symposium on Computer Architecture. ACM,   15%L 1992. \n10% [YN93] Tse-Yu Yeh and Yale N.Patt. A comparison of 1 dynamic branch predictors that use two levels \n 5% of branch history. In 20th Annual Interna\u00adcode size tional Symposium on Computer Architecture. 1 \nACM, 1993. 1 I II 1.25 1.5 1.75 Figure 8: compress A Misprediction Rate vs. Code Size misprediction \nrate misprediction rate 15%  15%t ----\u00ad 10% \u00ad10% 5%1 I code size ~ 1.25 1.75 5%Lize 1.25 1.5 1.75 \nFigure 6: abalone Figure 9: ghostview rnisprediction rate 15% 10% 1 kize1.25 1.5 1.75 Figure 10: predict \nrnisprediction rate I 15% 4 b======5% 1 code size 1.25 1:5 1.75 rnisprediction I I 10% i - rate code \nsize Figure 11: prolog Figure 13: doduc rnisprediction rate 15% / L5% 1 , 1.75 code size Figure 12: \nscheduler  \n\t\t\t", "proc_id": "178243", "abstract": "<p>Speculative execution on superscalar processors demands substantially better branch prediction than what has been previously available. In this paper we present code replication techniques that improve the accuracy of semi-static branch prediction to a level comparable to dynamic branch prediction schemes. Our technique uses profiling to collect information about the correlation between different branches and about the correlation between the subsequent outcomes of a single branch. Using this information and code replication the outcome of branches is represented in the program state. Our experiments have shown that the misprediction rate can almost be halved while the code size is increased by one third.</p>", "authors": [{"name": "Andreas Krall", "author_profile_id": "81100262538", "affiliation": "Technische Univ. Wien, Vienna, Austria", "person_id": "PP38030659", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/178243.178252", "year": "1994", "article_id": "178252", "conference": "PLDI", "title": "Improving semi-static branch prediction by code replication", "url": "http://dl.acm.org/citation.cfm?id=178252"}