{"article_publication_date": "06-01-1994", "fulltext": "\n Is Continuation-Passing Useful for Data Flow Analysis? Amr Sabry* Matthias Felleisen Department of \nComputer Science Rice University Houston, TX 77251 -1892t Abstract The gain of information occurs in \nnon-distributive analyses and is solely due to the duplication of the The widespread use of the continuation-passing \nstyle analysis of the continuation. The loss of inform\u00ad (CPS) transformation in compilers, optimizers, \nab\u00ad tion is due to the confusion of distinct procedure stract interpreters, and partial evaluators reflects \na returns. common belief that the transformation has a positive effect on the analysis of programs. Investigations \nby 2. The analyzer based on the continuation semantics Nielson [13] and Burn/Filho [5, 6] support, to \nsome de\u00ad produces\u00admore accurate results than both direct gree, this belief with theoretical results. \nHowever, they analyzers, but again only in non-distributive anal\u00ad do not pinpoint the source of increased \nabstract infor\u00ad yses due to the duplication of continuations along mation and do not explain the observation \nof many every execution path. However, when the analyzer people that continuation-passing confuses some \ncon\u00ad explicitly accounts for looping constructs, the re\u00ad ventional data flow analyses. sults of the semantic-CPS \nanalysis are no longer To study the impact of the CPS transformaticms on computable. program analysis, \nwe derive three canonical data flow analyzers for the core of an applicative higher-order programming \nlanguage. The first analyzer is based on a direct semantics of the language, the second on a In view \nof these results, we argue that, in practice, a direct data flow analysis that relies on some amount \nof duplication would be as satisfactory as a CPS analysis. continuation-semantics of the language, and \nthe last on the direct semantics of CPS terms. All analyzers 1 Compiling with CPS compute the control \nflow graph of the source program and hence our results apply to a large class of data flow Many compilers \nfor higher-order applicative languages analyses. A comparison of the information gathered by (Scheme, \nML, Common Lisp) map source programs to our analyzers establishes the following points: programs in continuation-passing \nstyle (CPS) [1, 10, 11, 17]. Compiler writers believe that the intermediate 1. The results of a direct \nanalysis of a source program representation based on CPS eases the production of are incomparable to \nthe results of an analysis of code and facilitates optimization. Numerous people the equivalent CPS program. \nIn other words, the also argue that the CPS transformation increases the translation of the sourc~ program \nto a CPS ver\u00ad.\u00ad precision of the data flow analysis that is necessary for sion may increase or decrease \nstatic information. advanced optimization [2, 3, 5, 6, 16]. 1 Even though CPS programs are widely accepted \nas *Supported in part by NSF grant CCR 89-17022 and by Texas ATP grant 91-003604014. Also partially supported \nby Arpa grant 8313, issued by ESD/AVS under Contract No. F196228\u00ad an advantageous intermediate representation, \nfew com\u00adpiler writers can pinpoint the advantages of the CPS 91-C-0168 under the direction of Robert \nHarper and Peter Lee. representation over other intermediate represent ations. t Temporary address: School \nof Computer Science, &#38;ruegie Mellon University, Pittsburgh, PA 15213. 1Researchers often express \nthis view in informal discussions as opposed to formal papers. We discussed and re-coniirmed this idea \nwith, among others, Charles Consel, and Olivier Danvy at LFP 92 [June 92], following the presentation \nof our paper on equational reasoning about programs in CPS [14]; in an email ex\u00ad change with Geoffrey \nBurn and Juarez FIlho [July 92]; in further discussions at POPL 93 with Daniel Weise; in email discussions \nwith Kelsey [July 93] and Shivers [May 93]; and in discussions and/or spicific perinission. with Burrs \nat FPCA 93 [June 93]. SIGPLAN 94-6/94 Orlando, Florida USA 0 1994 ACM 0-89791 -662-ti9410006..$3.5O 1 \n In an attempt to understand the principles of compiling with continuations and to determine its crucial \nprop\u00aderties, we recently found two important results: 1. The theory of equational manipulations of pro\u00adgrams \nin CPS based on the ~ rule of the lambda calculus has a simple counterpart for source pro\u00adgrams; in other \nwords , whatever optimizations are expressible via @-steps on CPS programs, can equally well be formulated \nfor source pro\u00adgrams [14]. 2. The code generation phase of a non-optimizing CPS compiler only requires \nthat the intermediate representation be normalized according to simple transformations. It is unnecessary \nto perform a full CPS translation [7].  Prompted by conversations with G. Burn [July 92] and with H. \nBoehm [August 92], and by the observation that the CPS transformation obscures some obvious properties \nof programs, we started to investigate the exact effect of the CPS program representation on the data \nflow analysis of programs. Our answer rejects the common belief that the CPS transformation is useful \nfor practical analyses. The organization of the rest of this paper is as fol\u00adlows. The next section introduces \nthe syntax and se\u00admantics of a typical higher-order language. Section 3 presents the CPS transformation. \nIn Section 4, we de\u00adrive various data flow analyzers from the standard se\u00admantics using the method of \nabstract interpretation. Section 5 presents all the formal theorems, which are discussed from a practical \nperspective in Section 6. For full proofs, we refer the reader to the technical report version of our \npaper [15]. 2 A: Syntax and Semantics Our source, language is a simple extension of the lan\u00adguage A of \nthe J-calculus. It corresponds to the core of typical higher-order languages like Scheme, Lisp, and ML. \nThe set of terms includes values, applications, let\u00adexpressions, and conditionals; the set of syntactic \nval\u00adues contains numerals (n E Z), variables (z E Var), primitive procedures, and user-defined procedures: \nM::= Vl(MiUf) l(let(z M) M)](ifOM MM) V ::= n I z I addl I subl I (kc.kf) Informally, the semantics \nof the language is as fol\u00adlows. All procedures are call-by-value, the evaluation of (let (z Ml) Af2) \ncomputes the value of &#38;ll and binds it to x in M2, and the conditional ifO branches to the second \nor third subexpression depending on whether the first subexpression evaluates to O or not. Though the \nlanguage is overly simple, it is rich enough for the primary purpose of the paper. The analysis of programs \nassumes that every inter\u00admediate result is named, and that all bound variables in a program are unique. \nThe restricted subset is the following language: M ::= V I (let (z V) M) [ (let (z (V V)) M) I (let (z \n(ifO V M M)) M) v ::= ~[zI addl Isubl I(kc.M) The assumptions simplify the semantics and the deriva\u00adtion \nof the data flow analyzers without restricting the set of valid programs: every term in A has a semanti\u00adcallyy \nequivalent normal form in the restricted subset. For example, the code fragment (~ (let (z 1) (g z))) \nbecomes: (let (z, 1) (let (XZ (g z,)) (let (Z3 (~ x,)) 33))). In general, the normalization process \nuses the reduc\u00adtions that we identified in previous work as the A\u00adreductions [7, 14].2 The semantics \nof the restricted subset of A is speci\u00adfied by the two predicates M and app defined in Fig\u00adure 1. It \nis straightforward to show that M is a partial function from terms, environments, and stores to an\u00adswers. \nThe environment is a finite table that maps the free variables to locations; the store is a finite table \nthat maps locations to values.3 An answer is a pair that consists of a run-time value and a store. The \nset of run-time values consists of numbers and closures. A closure is one of the procedure tags inc and \ndec, or a data-structure that contains the text of a user\u00addefined procedure and the environment at the \npoint of the creation of the closure. When applying a closure (cl x, M, p) to a value u, we extend the \nenclosed en\u00advironment at x with a new location and extend the store with the value u at the new location, \nThus, the bound variable of a procedure or a block is related to different locations, one for each invocation \nof the pro\u00adcedure. The function new takes a variable x, a store 2The normalization process does not affect \nthe results of the data flow analyzers. Intuitively, the first phase of A\u00adnormalization gives every subexpression \na name to which the data flow analyzer can associate information about the expres\u00adsion. Without A-normalization, \nthe analyzer would typically as\u00adsociate a label with every expression and attach the informa\u00ad tion about \neach expression at the corresponding label [S, 13, 16]. The two treatments are identical but the replacement \nof la\u00ad bels by variables simplifies the analyzers. The second phase of A-normalization re-orders the \nexpressions to reflect the or\u00ad der in which the interpreters will traverse them. For exam\u00ad ple, an expression \n(addl (let (z V) O)) would be rewritten as (let (z V) (addl O)). Again, the change is transparent to \nthe (abstract) interpreters since they evaluate both expressions in the same manner. 3 The form~ation \nof the semantics does not re@re a store, but the presence of the store simplifies the derivation of \ndata flow analyzers. Don mins: Auxiliary Function: #J: A(V) x Env x Sto * Val Ans = Val XSto #(n, p,s) \n= n Env = Var* Loc f#J(z, p, s) = S(p(z))Sto = Loce Val +(addl, p,s) = inc Val = Num +Clo #(subl, p,s) \n= dec Clo = (Var xAxEnv)+inc+ dec 4((AZ.W P>s) = (cl X,lf,p) M :(A x Env x Sto) ~ Ans u = #(v, p, $) \nu = @(V, p,s) (M, p[z := new(z)], s[new(z) := u]) M A (up, s) M (u, S) ((let (s V) M), p,s) M A UI = \n4( VI, P, S) ~z = d(V2, p, S) (UI, u2, S) app (w3, 53) (AI, P[Z := new(z)], ss[new(z) := us]) M A ((let \n(z (U U)) J4),p,s) M A w = ~(h, p,s) (M;, p,s) M (VI, SI) (M, p[z := new(z)], s~[new(z) := uI]) M A i= \n1if uo = O,i = 2otherwise. ((let (z (ifO ~ Jfl Af2)) M), p,s) M A app :(Vai x Val x Sto) ~ Ans (M, \np[z:= new(z)], s[new(z) := u]) M A (inc, n,s) app ((n+ 1),s) (dec, n,s) app ((n -1),s) ((cl z, kf, p), \nti, s) app A Figure 1: Direct (Store) Interpreter s, and returns a new location 1 from which it is pos-3.2 \nSemantic-CPS Transformation sible to recover z, i.e., new(z, s) = t ~ don(s) and The CPS interpreter \n(see Figure 2) maps expressions, x = new-1(1). (For brevity, we will often omit the environments, continuations, \nand stores to answers. Itsecond argument to new.) employs two auxiliary functions: appk, which is the \nCPS counterpart of app, and appr, which corresponds to the return operation of an abstract machine, The \n 3 Continuation-Passing Style latter operation binds the return value to a variable, A continuation-passing \nstyle transformation may be restores the environment, pops the control stack, and applied to the interpreter \nor to the source program. jumps to the next instruction. We distinguish the two approaches by referring \nto the The direct interpreter and the semantic-CPS inter\u00adfirst transformation as the sernantic-CPS transforma-preter \nproduce the same output. tion and to the second as the syntactic-CPS transfor- Lemma 3.1 Let A4 G A, \nfhen (M, p,s) M A if mation. We discuss both possibilities in this section. (M, p, nil,s) CA. 3.1 Continuations \n 3.3 Syntactic-CPS Transformation A continuation is the control state of an evaluator. For The transformation \nof a source program to a program example, during the evaluation of the procedure call in CPS uses two \nmutually recursive functions: ~ to (( Ja.Nl) 5) in (let (z (( Ja.Nl) 5)) NZ), the evaluator transform \nterms and V to transform values. The func\u00ad must remember the evaluation corded (let (z []) lVz) tion \nF takes an additional argument k, which is a vari\u00ad of the call as well as the environment p in which \nto able that represents the current continuation. evaluate N2. Typically, this information is packaged \nDefinition 3.2. (Syntactic-C.PS Transformation)in a frame and added to the continuation prior to the \nLet cps(A) be the language: procedure call. The evaluation of the body of the pro\u00adcedure N1 may itself \npush frames on the control stack. P ::= (k w) Thus the continuation K. can in general be represented \nI (let (x W) P)as a list of frames where each frame consists of an eval-I (w w (AZ. P)) uation context \nand an environment [7]: \\ (let (k Az.P) (ifO W P P)) K=(El, pi):: ..$ :: nil where Ei = (let (Xi [ ]) \nJ4i) w ::= ~ I z I addlk I sublk I (kck.P) Domains: Ans = Val x Sto Con = (A(E) x llnv) :: Con+ nil \nEnv = Var++ Loc Val = Num + Clo Sto = Loc-e+ Val Clo = (Var xAxEnw) +inc +dec C :(A xEnv xCon xSto) ~ \nAnn u = #(V, P,s) (M, (u, s)) appr A u = q$(V, p,s) (M, p[z := new(z)], R, s[taetu(z) := u]) C A (V, \np, /c,s) C A ((let (x V) JW), p,K, s) C A U1 = #( VI, p,s) uz = ~(VZ, p,s) (ul, uz, ((let (z []) J4), \np) :: K,S) appk A ((let (z (~ %)) A4), p, K,s) C A IIo = +(~, p,s) (Af,, p,((let (z []) &#38;f), p) :: \nK,s) C A j = I if UO= O, i = 2 otherwise. ((let (z (ifO ~ Afl AIz)) M), p,s,s) C A appk :(Val x Val \nx Con x Sto) ~ Ans (K, ((n -1), s)) appr A (ikf, p[x := new(z)], K, s[new(z) := u]) C A (inc, n, K,s) \nappk A (dec, n, ~,s) appk A ((cl z, M,p), u, K,s) appk A (K, ((n + 1), 9)) appr A aPPr : (Con x Arw) \n~ An$ (M, p[z := new(z)], ~, s[new(z) := u]) C A (((let (z []) ~), p) :: K, (u, s)) appr A (nil, A) appr \nA Figure 2: Semantic-CPS Interpreter where x c Vars, k c KVars and KVars n Vars = 0. The CPS transformation \nuses the functions 7 and V: ~ : A ~ cf)s(A) Fk[v] = (k V[v]) ~~[(let (z V) M)]= (let (z V[V]) Fk[Ml) \nFk[(let (z (V, Vz)) M)]= (VIV1] V[VZ] kc. Fk[M]) 3~[(let (z (ifO VO Ml Mz)) M)]= (let (k Az.F~[IU]) \n(ifO VIVO] Y wIM1] 7kI[J42])) ~ : A(V) ~ cps(fi)(w) V[n] =n V[z] =x V[addl] = addlk V[subl] = sublk \nV[(AZ.M)] = (Azk.7k[M]) B The evaluation for CPS programs is defined by Mc, a specialized version of \nthe direct interpreter M [7]. It handles procedures of two arguments and manip\u00adulates a larger set of \nrun-time values than the di\u00adrect interpreter that includes continuations of the form (co z, P, p) (see \nFigure 3). The larger set of run-time values reflects the salient aspect of the CPS transfor\u00admation: \nit reifies the continuation of the evaluator to an object that the program explicitly manipulates.4 4 \n~ prjncjple, we co~d use thedirect interpreter M to Wdu\u00ad ate CPS programs. However, this choice forces \ncontinuations to be represented as procedures, which is (unrealktic and) unnec\u00adessarily confusing for \ndata flow anal yzers. To establish the formal relationship between the be\u00adhavior of a direct term and \nthe behavior of its CPS\u00adtransform, we define the function 6 that relates direct run-time values to their \nCPS counterparts: 6(n) = n r$(inc) = inck 6(dec) = deck 6((C1 z,kf,p)) = (cl zk,y~[iw],p) We extend \n6 to work on stores by applying it to the value at each location and to answers by applying it to both \nthe value component and the store component. The following lemma describes the precise rela\u00adtionship \nbetween the semantic-CPS interpreter and the syntactic-CPS interpreter. The interpreters yield answers \nrelated by 6; the store resulting from the syntactic-CPS interpreter will contain additional en\u00adtries \nthat correspond to continuations. Lemma 3.3 Let M s A, then: (M, p, nil,s) C (ul, Sl) ifl (F~[M], p[k \n:= new(k)], ti(s)[new(k) := stop]) MC (6(u1), b(sl)[new(kl) := Kl, new(k2) := K2, . . .]). Together \nwith Lemma 3.1, this result also relates the syntactic-CPS interpreter to the direct one. Domains: Auxiliary \nFunction: Ans = Valx Sto gjC:CpS(A)(W) XEnVXStO -t VaJ Env = Var -e+ Loc @.(n, P,$) = ~ Sto = Loc-sH \nVal #c(z, p, s) = S(p(z)) Val = Num + Clo+ Con d~(addlk, p,s) = inck Clo = ( Var x KVar x cps(A) x Env) \ndc(sublh p, S) = deck + inek + deck 4c((Azk.P), p, s) = (cl Zk, P,p) Con = ( Var x cps(A) x Env) + stop \n M. : (cPs(A) x Env x Sto) ~ Ans K = s(p(k)) u = q$c(W, p,s) (~, (u, s)) apprc A u = #c(W, p,s) (P, p[z \n:= new(z)], s[new(z) := u]) Mc A ((k W), p, s) M= A ((let (z W ) P), p,s) M. A I( 1= I#C(wl,p, s) w = \nA(W 2, AS) (w, W, (CO Z, PtP),S) w. A ((~] l~z (~z.P)), As) M. A Uo = #c(wo, P, s) (p:, P[k := new(k)], \ns[new(k) := (co x, P, p)]) M. A z= 1if ILO = O,i = 2otherwise. ((let (k Jz.P) (ifO WO PI Pz)), p,s) \nM. A appc :(Vid x Val x Val x StO) ~ Ans (6, ((n + 1), s)) appr~ (~, ((n -1), s)) apprc A (inck, n, K,s) \nappc A (deck, n, ~,s) appc A (P, P[Z := new(z), k:= new(k)], s[new(z) := U, new(k) := K]) M. A ((cl zk, \nP, p), u, ~,s) appC A Upprc :(Val x Ans) ~ Ans (P, p[z := new(z)], s[new(x) := u]) M. A ((co z, P,p), \n(u, s)) apprC A (stop, A) apprc A Figure 3: Syntactic-CPS Interpreter  4 Constant Propagation by Ab\u00adst \nract Interpret at ion Using well-known ideas from the area of abstract in\u00adterpretation [4, 8, 13, 16], \nwe now derive a data flow analyzer from each of the three interpreters. The first step in the derivation \nis to associate one location with each variable that holds the potentially infinite set of values to \nwhich the variable is bound during the evalua\u00adtion of the program. Second, we approximate these sets \nof values so that each label is associated with a finite number of values. Finally, we modify the interpreters \nto detect and recover from all loops when computing over the universe of approximate values. 4.1 Abstracting \nProcedures The first step in the derivation of the data flow ana\u00adlyzers is to limit the number of locations \nthat can be created during the evaluation of a given program. One of the simple approximations, known \nas OCFA arlaly\u00adsis [16], is to associate one location for each variable and to collect all the values \nto which the variable is bound at that location. Formally, we approximate en\u00advironments p to ~ and stores \ns to F as follows: Since each variable is associated with exactly one location, we can choose that location \nto be the variable itself. Thus, if p = {Cl ~ new(zl), . . .}, ~={zlHtl ,,, .}. This approximation of \nthe environment does not provide any informa\u00adtion, that is, we can drop it completely. Thus, a closure \n(cl z, M, p) becomes an abstract clo\u00adsure (cle z, M), a continuation (El, pl) :: . . . :: nil becomes \nEl :: , ., :: nil, and a continua\u00adtion (co z, M, p) becomes an abstract continua\u00adtion (toe z, M). It \nfollows that, for each source program, the sets of abstract closures and abstract continuations are finite. \n For stores s = {new(zl) I+ Ul, . . .}, we first re\u00adcover the variable associated with each of the lo\u00adcations: \n{zl H Ul, . . .}. Then, to obtain the store  F, we merge all entries of the form z + u 1, z ~ U2,...for \nsome t into one entry x w {ul, Uz,...}. A collecting semantics like the above associates a set with \neach variable. Intuitively, the larger the set the less information is available at compile time about \nthe variable. To formalize this notion of precision , we note that the sets of collected values form \na com\u00adplete lattice ordered by set-inclusion; the least upper bound operation is set-union. Thus, the \nrelation is more precise than coincides with the lattice ordering. 4.2 Abstracting Integers Despite \nthe approximations of environments, stores, closures, and continuations, the collecting semantics still \nassociates an unbounded set of values with a vari\u00adable because the lattice of collected values contains \nin\u00adfinite chains of elements of decreasing precision, e.g,, 0 c {O} C {O, 1} ~ {O, 1,2} . . .. Since \nthese infinite chains may cause the anal ysis to diverge, we approxi\u00admate sets of numbers to abstract \nnumbers [9]: @=l, ~=n, and {nl, n2,...}=T. At this point, the universe of abstract values con\u00adsists \nof abstract numbers and abstract closures. It re\u00admains to impose an order ~ on the abstract values similar \nto the order ~ on collected values that coin\u00adcides with the relation is more precise than . For the direct \nand semantic-CPS interpreters, we organize the abstract values in a lattice that is the product of two \nlattices: the first is the traditional lattice N~ for con\u00adstant propagation [9], and the second is the \npower set of abstract closures (ordered by the subset relation) for control flow analysis [16]. The ordering \nrelation Q and the least upper bound operation u are defined component-wise. It is easy to check that, \nif S1 and S2 are sets of collected values, then S1 G S2 implies ~~~, For the syntactic-CPS collecting \ninterpreter, the sets of values include abstract continuations aa well. In that case, we use a lattice \nof abstract values that consists of the product of three lattices: the constant propaga\u00adtion lattice, \nthe power set of abstract closures, and the power set of abstract continuations. The ordering of abstract \nvalues induces an ordering on stores. If al and U2 are abstract stores, then U1 G U2 if for every variable \nz in the domain of al, al(z) g a2(z). The latter ordering induces a component-wise ordering on abstract \nanswers. We can now specify collecting interpreters that ma\u00adnipulate abstract values. The interpreters \nreplace addl and subl by addle and suble: addle(l) = L suble(l) = l_ addle(n) = (n + 1) suble(n) = (n \n 1) addle(T) = T suble(T) = T Figures 4, 5, and 6 contain the direct abstract collect\u00ad ing interpreters, \nthe semantic-CPS abstract collecting interpreter, and the synt attic-CPS abstract collecting interpreter \nrespectively. 4.3 Correctness The correctness criterion of an abstract collecting in\u00adterpreter is that \nits results approximate the actual ex\u00adecution of the program. For example, if the variable x gets bound \nto 5 along any actual execution path, the abstract collecting interpreter should associate an abstract \nvalue u ~ (5, 1) with the variable z. 1. If(itl, p,s)M (Ul, Sl), then (M, a) Me (U2, U2). 2. If (M, \np, K,s) C (UI, SI), then (M, i?, c7) Ce (U2, U2).  9. If(P, p,s) M. (Ul, Sl), then (P, o) M: (u2, m2). \n 4.4 Termination Interpreted naively, the specifications for the abstract collecting interpreters define \npartial functions that di\u00adverge on some inputs, and hence are not data flow al\u00adgorithms. However, this \nis not a problem, since it is possible to detect all loops in the derivations. More precisely, assume \nwe have the following fragment of a derivation tree: The evaluation of Ml in store al requires the value \nof Mi in store ui, which in turn requires the value of Mj in store uj, and so on. If the above fragment \nof the deriva\u00ad tion is indeed infinite, then one of the M s must be re\u00adpeated infinitely often as the \nabstract syntax tree of the program haa only a finite number of subtrees. Thus, without loss of generality, \nlet Ml = Mi = Mj = M. By inspection of the direct abstract collecting interpreter, 1. .. GuQuj. Quj . \n. . . As the lattice of abstract stores does not have any infinite ascending chains, one of the a s in \nthe sequence must be repeated: Ui = uj = u. Thus, all loops will result in two identical proof goals. \nHaving detected a loop, we return the least precise value paired with the current store. Thus, if the \nar\u00adguments (M, u) have already been considered, the di\u00adrect interpreter returns the answer ((T, CLT ), \na) where CLT is the set of all abstract closures in the pr~ gram. Similarly, the semantic-CPS interpreter \nreturns Domains: Auxiliary Functions: K= -iZJx X1 ~e:A(V)x% + \u00ad%=W*W Q@(n,u) = (n, 0) ~ = Num x P(m) \n#e(X, a) = a(z) ~ = (Varx A)+inc+dec q$e(addl, a) = (1, {inc}) ~e(subl, u) = (1, {dec}) #e((Az.M), C7) \n= (1, {(cle x, M)}) &#38;@:(Ax~)+~ u = de(~a) u = @(v,u) (~,a[z:=U(Z)u u]) Me A (v, IY) Me (u, a) ((let \n(z V) Jf), a) Me A U1 = @ (vi,tY) w, = I#e(W, IY) (u:I, u2, a) appe (u3, u3) (M, Im[z := us(z) u u,]) \nMe A ((let (z (K U)) M), u) Me A Uo = de(vo, ~) (~a,~) Me (ul, al) (kf, f-TI[~ := 171(Z) UUI]) Me A 8=lifu0 \n=(0,0), i=2if(0,0)~tt0.((let (z (ifO ~ Ml M2)) M), a) Me A (0,0) C tJO = ~e(h,a) (ikfI, IY) Me (uI, aI) \n(M2, u) Me (UZ,UZ) (M, (uI Uaz)[z := (a, uaz)(z)u(tt, L17Q)]) Me A ((let (z (ifiD % MI M2)) M), a) Me \nA appe:(TZX RI Xl%)--t G (clI, u,a) apple AI . . . (cl~, U,U) apple An ((n, {c~I,..., cL}), u,~) appe \nU,=l,n Ai app~e:(7%X~x3G)~ZR ?I = (addle(n), a) U = ($uble(n), O) (M, a[z := a(z) u v]) Me A (inc, (n, \nCL), a) appI e (u, a) (dec, (n, CL), a) apple (u, a) ((cle Z, AI), tt,~) apple A Figure 4: Direct Abstract \nCollecting Interpreter ((T, CJZT), a) to the continuation ~. If the syntactic-abstract direct values \nto abstract CPS values: CPS interpreter detects that the arguments P, a) have ( 6e((n, {cll,. ... Cli})) \n= (n, {w(dl),.. .,ve(di)},q already been considered, it returns ((T, CL , KT ), u) W((CF q, fwl)) = (Cle \nzlh, rk, [~l]) where KT is the set of all abstract continuations P (inc) = inck (toe Z, P) in the program. \nVe(dec) = deck In the remainder of the paper, we will use abstract collecting interpreter or data flow \nanalysis to refer The application of 6e to stores and answers is point\u00adto the terminating versions of \nthe interpreters that de-wise and component-wise respective y. tect loops as above. 5.1 Direct vs Syntactic-CPS \nThe first theorem establishes that the direct analysis of M may be more precise than the analysis of \n~k [M].  Formal Relationships Theorem 5.1 There exists M ~ A and u E Sto such that: After deriving \nthe data flow analyzers, we turn our at\u00ad tention to the relationship between them. For the con- (M, \ncr) Me (~1,~1), nection between the direct and syntactic-CPS analyses, we need an abstract version of \nthe function 6 that maps (f ~[M], @(a)[k := (1-,0, {stop})]) M: (u2, u2), -&#38;z = A(E) :: ~+nil ~ \n= Num x P(=) ?% = (Varx A)+inc+dec C@:(Ax~x%)-+Ans u = #e(V, C) (x, (u, a)) appre A u = @e(V, a) (M, \nw, CT[Z:= a(z) u u]) Ce A (v, H,a) ce A ((let (Z V) M), N,U) Ce A UI = @e(VI, U) U2 = ~e(vz,a) (ul, w,(let \n(z []) M) :: ~,a) appke A ((let (Z (Vl V2)) M), K, m) ce A UO = f#Je(VO, a) (M,, (let (x []) M) :: x, \na) Ce A i=lifuo=(O, O), i=2if(0,0)~uo. ((let (Z (ifO VO M, Af,)) M), H, u) Ce A (0,0) C uo = I+e(Vb, \na) (Jfl, (let (Z []) M):: N,a) ce -41 (Mz, (let (z []) M) :: IC,a) Ce A, ((let (z (ifO VO MI M2)) M), \nx, u) C@ AI u Az . appke :(~ x~ x Con x Sto) -+ G  (c/1, u, N, a) appkle Al . . . (cln, u,~, a) appkle \nAn ((n, {cJI,..., c~n}), U, ~,u) appke U,=l,n A, u = (addle(n), 9) (~, (u, a)) appr A u = (suble(n), \n0) (R, (u, a)) appre A (M, N, U[Z := a(z) u u]) Ce A (inc, (n, CL), K, a) appkl e A (dec, (n, CL), K, \na) appkl e A ((cle z, M), u, K, a) appkle A appre :(7Z XXX) + XZ (M, R,U[Z := a(x) u w]) Ce A ((let \n(z []) M) :: K, (u, a)) appre A (nil, A) appre A Figure 5: Semantic-CPS Abstract Collecting Interpreter \ne C$e(ul) ~ U2, and for each variable in the domain For the analysis of the CPS version, we have that: \nOfq, $e(ul(z!)) ~ C72(Z). Proof. Let 11 be (let (al (~ 1)) (let (az (~ 2)) a~)), Zk[kf] = (f 1 (kz~.(~ \n2 (h-q. (k az))))) and let: a = {al * (~,0,0), u = {al H (1,0), a2 ++ (J-)O,O), az = (1,0), f ~ (1, \n{(CF Zk,, (k, Z))}, o), $-(L, {(cle 2, Z)}), Zl+(l, o,o), z % (1,0)}. k + +(1,0, {stop})}. It is straightforward \nto calculate that the result of the direct abstract collecting interpreter is .41 = (UI, al ) The syntactic-CPS \nabstract collecting interpreter pro\u00ad where: duces the answer A2 = (U2, U2) where: U1 = (T,O) al = {al \nw (1,0), az -(T, O), ?&#38; = (T, CLT, KT) f-+ (.l, {(de x,x)}), Uz = {al w (T, tI, O), z H (T, O)}. \nw ++ (T, O,O), Domains: Auxiliary Function: X&#38;= TZi xl% @ : CPS(A)(W) x ~ + ~ %. Vare iZ2 $w:! \nj = $:,0) . ~ = Num x P(x) x P(Con) c, m. ( Var x KVar x cpLJ(A)) q$~(addlk, mj = (~, {inckl, 0) +inck+deck \n @S(sublk, u) = (1, {deck}, 0) ~ = ( Var x cps(A)) + stop d~((~~~.~), U) = (1, {(cle xk, P)}, a) K = \na(k) u = #~(W, a) (JC, (u, a)) appr~ A u = @(W, u) (P, a[z := u(z) u u]) M: A ((k W), a) M: A ((let (zW) \nP), u) M~ A w = I#S(W I,a) w = +$(W2,U) {w, W, (LO, (COe Z, p)), U) wpsA ((WI Wz (Az.P)), u) M: A IJO \n= 4g(w0,~) (Pi, u[k := u(k) U (1,0, (Coe X, P))]) MS A \\=lifuO=(O, O, O), i=2if(0,0,0)~u0.((let (k ~z.P) \n(ifO WO PI P2)), U) M: A (0, 0,0) c uO = 4:(W0, a) (PI, a[k := u(k) U (1,0, (COe z, P))]) M: AI (Pz, \na[k := u(k) U (L,o, (Coe z, P))]) Ms A2 ((let (k k.P) (ifO WO PI P2)), u) M: AI UAZ (clI, u, x, a) app?c \nAI . . . (cl , u, K, u) app?. An ((%{dl,..., C/n}, h ), u, tc, u) app? U,=l,n At u = (addie(n), 0, 0) \n(K, (u, IT)) appr-~ -4 u = (wble(n), 0,0) (K, (u, a)) appr~ A (inck, (n, CL, h ), K, a) apP~C A (deck, \n(n, CL, K), ~, U) app~c A (P,a[z:=a(z) u u, k := a(k) u K]) M: A ((cle zk, P), u, ~, u) app~C A (KI, \n(u, a)) awfc AI . . . (K~,(u, u)) appr~c An ((n, CL, {~,,.. ,, ~n}), (u, a)) appr$ U,=l,n A (P, a[z := \na(z) uu]) M: A ((co@ z, P), (u, a)) appr~c A (stop, A) appr~c A Figure 6: Syntactic-CPS Abstract Collecting \nInterpreter f+ (L, {(cle Zk,, (k, Z))}, o), The analysis of the source program is more precise since \nZI-+(T, O, O), it determines that the variable al is constant (=1), kl * (1,0, { (co e al, (~ 2 (Aaz. \n(k a2)))), while the analysis any information of the CPS about al. a program fails to produce (Coe a,, \n(k a,))}) k H (1,0, {stop})] Our second theorem states that the direct analysis of a program may also \ngive less information than the syntactic-CPS analysis. Together with the previous theorem, the result \nestablishes that the direct analysis of a source program is incomparable to the syntactic-CPS analysis. \nTheorem 5.2 There exists a term M E A such that: (M, u) MO (~1,~1), e (F~[Af], tie(a)[k := (1,0, {stop})]) \nM: (UZ, UZ), 60(u1) ~ U2, and for each van able x in the dO\u00admain of al, 6e(ul(x)) ~ m2(x). Proof, To \nillustrate the principle, we present two cases in which the analysis of the CPS program yields more information \nthan the direct analysis of the source pro\u00adgram. For the first case, take: M = (let (al (ifO z O 1)) \n(let (az (ifO al (+ al 3)(+ al 2))) az)) a = {al+ (1,0), az I+ (-L,O), Z* (T, O)} where (+ al 3) and \n(+ al 2) are the obvious abbrevi\u00adations. The direct analysis of the term, not knowing which branch to \ntake, merges the abstract values of O and 1 at the variable al and hence loses all information about \naZ, In contrast, the analysis of Yl[M] = (let (k ~a~.~~[(let (a, (ifO al (al +3) (al+ 2))) az)]) (ifO \nz (k O) (k l))) analyzes both (k O) and (k 1) in a store that maps k w (co@ al,.. .). The analysis of \neach execution path determines that the abstract value of az is (3, 0, 0), which improves on the direct \nanalysis. For the second case, take M: M = (let (al (f 3)) (let (CQ (ifO al 5(ifO(sublal)56))) az)) U \n= {a, H (1,0) a2 I-+ (40), f -(l, {(CF d,, o), (cle d,, l)}) do = (1,0), d~ + (1,0)] where we have not \nnamed the results of (subl al) and (ifO (subl al) 5 6) to avoid clutter. The direct analysis of A4 begins \nby applying both closures bound to f to the abstract value of 3. The analysis then combines the results \nof these two applications associating (O, 0) U (1, 0) = (T, 0) with al. As a consequence, the analysis \nloses all information about the value of az. In contrast, the analysis of the CPS version: (f 3 (Aa~.X~[(let \n(az (ifO al 5 (ifo (subl al) 5 6))) aa)])) duplicates the continuation (Aal.. ..) when evaluating the \napplication of each of the closures bound to f. The analysis determines that the value of az is (5,0,0) \nalong each execution path and hence improves on the analysis of the source program. s 5.2 Direct vs Semantic-CPS \nThe character of the relationship between the direct and the semantic-CPS analysis depends on a key prop\u00adert \ny of analyses. Definition 5.3. (Distributivify) An analysis is distributive if for all K, Ai, and n: \n(~, !_liaI,~ Ai) appre Aj iff (~, Al) apprO B1 . . . (~, An) apprO Etn and Aj = Ui=l,n Bn. s When Dtstributivity \ndoes not hold, e.g., for constant propagation [9], the semantic-CPS data flow analyzer may gain information \n~y duplicating the continuation along every execution path as in the right hand side of the condition. \nOtherwise, the results of the analyses are identical. Theorem 5.4 Let M 6 A, then (M, tc, m) Ce AI if \nand only if: Q (M, m) Me AZ, and (~, A2) appre A3, and Al ~A3, or if the Distributivity condition holds, \n(M, a) Me A2, and (~, Az) appre AI.  5.3 Syntactic-CPS vs Semantic-CPS The semantic-CPS analyzer may \nyield more precise re\u00adsults than the syntactic-CPS analyzer since the latter may confuse the continuations \ncollected at a given la\u00adbel. Theorem 5.5 Let M E A, then: (M, nil, a) C@ Al iff (F~[M], ($e(a)[k := (1,0, \n{stop})]) M: AZ where fie(A1) ~ AZ. 5 In the traditional framework, the lattice is usually inverted \nand the Distrib tdivity condition is stated using the greatest lower bound n. In our case, the condition \nshould be called Continuity but, to avoid confusion, we use the standard terminology. 10 6 Discussion \nof the Results  In summary, our theorems show that: 1, The syntactic-CPS analyzer may confuse some continuations, \ni.e., may analyze an infeasible path, and also duplicates the analysis of continuations along every execution \npath, i.e., may gather more information than the direct analyzer in non\u00addistributive analyses, 2. The \nsemantic-CPS analyzer does not suffer from the false return problem and increases the col\u00adlected information \nin non-distributive analyses by the duplication of the analysis of continuations. In the remainder of \nthis section, we discuss each of the properties of the CPS analyzers in detail. 6.1 False Returns In \npractice, many analyses do indeed confuse continu\u00adations when applied to CPS programs. For example, Shivers \ns OCFA analysis of CPS programs [16] merges distinct control paths unnecessarily. Shivers did not relate \nthe problem to CPS but his example [16: P.33] is essentially the example for Theorem 5,1. Given our result, \nwe can explain how the CPS trans\u00adformation confuses some data flow analyzers that assoc\u00adiate (approximate) \ninformation with program points. Because the CPS transformation reifies the continua\u00adtion to a value \nthat the program manipulates explic\u00aditly, the analysis of a CPS program is obligated to col\u00adlect, at \neach variable k, the set of continuations that k may refer to during the execution of the program. Thus, \nwhen considering a return, i.e., a call (k W), the analysis applies each of the continuations bound to \nk and merges the results. In contrast, the analysis of the source program and the semantic-CPS analysis \ndo not collect continuations, but only consider the current continuation at any program point.  6.2 \nDuplication The gain of information in semantic-CPS analyzers is folklore knowledge. Nielson [13] proved \nthat, for a small imperative language, the semantic-CPS analy\u00adsis computes the MOP (meet over all paths) \nsolution and the direct analysis computes the less precise MFP (maximum fixed point) solution; Filho \nand Burn [6] improved the abstract interpretations of typed call-by\u00adname languages using the CPS transformation. \nOur result shows that the gain in all cases is entirely due to the duplication of the analysis of the \ncontinuation along different execution paths. In the remainder of this section, we consider the impact \nof this duplication on the computability and cost of the analysis. Intuitively, the difference between \nthe direct seman\u00adtics and the CPS analyses is that the former merges all the values of an expression \nbefore analyzing the con\u00adtinuation and the latter apply the continuation to each of the values of an \nexpression and merge the results. Therefore, the duplication of the analysis of the contin\u00aduation depends \non the number of values an expression may have. Thus far, every expression in our language had only a \nfinite number of values: the analysis of a conditional expression may proceed along two paths, and the \nanal\u00adysis of a procedure call may proceed along some finite number of paths, one for each abstract closure \nthat the term in function position evaluates to. Consequently, at each conditional and at each call site, \nthe contin\u00aduation may be duplicated along each of the possible paths, at an overall eqmnential cost in \nthe analysis. In a realistic language, the duplication of the con\u00adtinuation causes the computation of \nthe result of the CPS analysis to become undecidable. To illustrate this point, we assume an extension \nof the language with an explicit looping construct and a sufficiently rich set of primitives. Let the \nconstruct loop be an infinite loop whose ex\u00adact collecting semantics returns the infinite set of val\u00adues \n{O, 1,2,.. .}.6 The extensions of the direct and semantic-CPS analyzers are: ~i = (i, O) Ai = (Ui,~) \n(IOOP, U) Me (1.l=,,m A) ~i = (i, 0) Ai = (~i, ~) (~, /li) appre Bi (IOOP, K,a) @ (Ui=O,m Bi) In the \ndirect interpreter, each Ui is an abstract number (i, 0) and the least upper bound of the set {~i [ i \n> O} is (T, 0). In the semantic-CPS case, the computation of U;ZO,W Bi is undecidable. The proof is an \nadaptation of Kam and Unman s proof [9] that it is undecidable to compute the MOP solution for a general \nprogram in an arbitrary monotone framework. 6.3 Conclusion In conclusion, a practical analysis based \non the CPS transformation should not perform any duplication when the analysis is distributive since \nthe duplica\u00adtion would not yield more precise answers. In non\u00addistributive cases, a CPS analysis should \nlimit the amount of duplication for both computability and ef\u00adficiency reasons. When the analysis of \na CPS pro\u00adgram does not perform any duplication, the net effect of transforming the program to CPS is \nto obscure the fact that there is only one control stack at any point eThe construct loop corresponds \nto the following program fragment z := O;while true z := x + 1 . during a computation. Hence a more \ntive is to combine heuristic in-lining direct-style analysis. practical algorithms alterna\u00adwith a [9] \nAcknowledgements [10] We thank Hans Boehm for discussions about the unde\u00adcidability of the semantic-CPS \nanalysis, and Geoffrey Burn, Bruce Duba, Juarez Filho, Cormac Flanagan, John Greiner, Bob Harper, Nevin \nHeintze, Peter Lee, and Frank Pfenning for comments on an earlier version of this paper and for discussions \nof the results. [11] References [12] [1] Appel, bridge A. Compiling with Continuations. University Press \n(1992). Cam\u00ad [13] [2] Bondorf, A. Improving binding times without ex\u00adplicit CPS-conversion. In Proceedings \nof the ACM Conference on Lisp and Functional Programming (1992) 1-10. [14] [3] [4] Consel, C, and Danvy, \nO. For a better support of st atic data flow. In Proceedings of the Confer\u00adence on Functional Programming \nLanguages and Computer Architecture (1991) 496-519. Cousot, P. and Cousot, R. Abstract interpreta\u00adtion: \nA unified lattice model for static analysis of programs by construction of approximation of fixpoints. \nIn Conference Record of the 4th ACM Symposium on Principles of Programming Lan\u00adguages (1977) 238-252. \n[15] [16] [5] Filho, J. Muylaert. Improving tions with CPS-translation, Manuscript. abstract interpreta\u00ad(1993). \nUnpublished [17] [6] Filho, J. Muylaert and Burn, G. Continuation passing transformation and abstract \ninterpreta\u00adtion. In Burn, G., Gay, S., and Ryan, M,, ed\u00aditors, Proceedings of the First Imperial College, \nDepartment of Computing, Workshop on Theory and Formal Methods (1993). [7] Flanagan, C., Sabry, A., Dubs, \nB. F., and Felleisen, M. The essence of compiling with con\u00adtinuations. In Proceedings of the ACM Sigplan \nConference on Programming Language Design and Implementation (1993) 237-247. [8] Hudak, P. and Young, \nJ. Collecting interpreta\u00adtions of expressions. ACM Transactions on Pro\u00adgramming Languages and Systems, \n13, 2 (April 1991) 269-290. Kam, J.B. and Unman, J. D. Monotone data flow analysis frameworks. Acts \nInformatica, 7 (1977) 305-317. Kelsey, R. and Hudak, P. Realistic compilation by program transformation. \nIn Conference Record of the 16th ACM Symposium on Principles of Pro gramming Languages (1989) 281-292. \nKranz, D., Kelsey, R., Rees, J., Hudak, P., Philbin, J., and Adams, N. Orbit: An optimiz\u00ading compiler \nfor Scheme. In Proceedings of the ACM Sigplan Symposium on Compiler Construc\u00adtion, Sigplan Notices, 21, \n7 (1986) 219-233. Marlowe, T.J. and Ryder, B.G. Properties of data flow frameworks: A unified model. \nActs lnformat\u00adica (1990). Nielson, F. A denot ational framework for data flow analysis. Acts Informatica, \n18 (1982) 265\u00ad 287. Sabry, A. and Felleisen, M. Reasoning about programs in continuation-passing style. \nLisp and Symbolic Computation, 6, 3/4 (1993) 289-360. Also in Proceedings of the ACM Conference on Lisp \nand Functional Programming, 1992, and Technical Report 92-180, Rice University. Sabry, A. and Felleisen, \nM. 1s Continuation-Passing Useful for Data Flow Analysis? Technical Report TR94-223, Rice University \n(1994). Shivers, O. Control-Flow Analysis of Higher- Order Languages or Taming Lambda. PhD thesis, Carnegie \nMellon University (1991). Steele, G. L. Rabbit: A Compiler for Scheme. MIT AI Memo 474, Massachusetts \nInstitute of Technol\u00adogy (1978).   \n\t\t\t", "proc_id": "178243", "abstract": "<p>The widespread use of the continuation-passing style (CPS) transformation in compilers, optimizers, abstract interpreters, and partial evaluators reflects a common belief that the transformation has a positive effect on the analysis of programs. Investigations by Nielson [13] and Burn/Filho [5,6] support, to some degree, this belief with theoretical results. However, they do not pinpoint the source of increased abstract information and do not explain the observation of many people that continuation-passing confuses some conventional data flow analyses.</p><p>To study the impact of the CPS transformation on program analysis, we derive three canonical data flow analyzers for the core of an applicative higher-order programming language. The first analyzer is based on a direct  semantics  of the language, the second on a continuation-semantics of the language, and the last on the direct semantics of CPS terms. All analyzers compute the control flow graph of the source program and hence our results apply to a large class of data flow analyses. A comparison of the information gathered by our analyzers establishes the following points:</p><list><item><p>1. The results of a direct analysis of a source program are <italic>incomparable</italic> to the results of an analysis of the equivalent CPS program. In other words, the translation of the source program to a CPS version may increase or decrease static information. The gain of information occurs in non-distributive analyses and is solely due to the <italic>duplication</italic> of the analysis of the continuation. The  loss of information is due to the confusion of distinct procedure returns.</p></item><item><p>2. The analyzer based on the continuation semantics produces more accurate results than both direct analyzers, but again only in non-distributive analyses due to the <italic>duplication</italic> of continuations along every execution path. However, when the analyzer explicitly accounts for looping constructs, the results of the semantic-CPS analysis are no longer computable.</p></item></list><p>In view of these results, we argue that, in practice, a direct data flow analysis that relies on some amount of duplication would be as satisfactory as a CPS analysis.</p>", "authors": [{"name": "Amr Sabry", "author_profile_id": "81100016804", "affiliation": "Department of Computer Science, Rice University, Houston, TX", "person_id": "P16266", "email_address": "", "orcid_id": ""}, {"name": "Matthias Felleisen", "author_profile_id": "81100323458", "affiliation": "Department of Computer Science, Rice University, Houston, TX", "person_id": "PP39037684", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/178243.178244", "year": "1994", "article_id": "178244", "conference": "PLDI", "title": "Is continuation-passing useful for data flow analysis?", "url": "http://dl.acm.org/citation.cfm?id=178244"}