{"article_publication_date": "06-01-1994", "fulltext": "\n A General Data Dependence Test for Dynamic, Pointer-Based Data Structures Joseph Hummer Laurie J. Hendrent \nAlexandru Nicolau* U. of California, Irvine McGill University U.of California, Irvine Abstract Optimizing \ncompilers require accurate dependence testing to enable numerous, performance-enhancing transformations. \nHowever, data dependence testing is a difficult problem, particularly in the presence of pointers. Though \nexisting approaches work well for pointers to named memory locations (i.e. other vari\u00ad ables), they are \noverly conservative in the case of point\u00ad ers to unnamed memory locations. The latter occurs in the context \nof dynamic, pointer-based data struc\u00ad tures, used in a variety of applications rangin from system software \nto computational geometry to 8 -body and circuit simulations. In this paper we present a new technique \nfor perform\u00ading more accurate data dependence testing in the pres\u00adence of dynamic, pointer-based data \nstructures. We will demonstrate its effectiveness by breaking false de\u00adpendence that existing approaches \ncannot, and pro\u00advide results which show that removing these depen\u00addence enables significant parallelization \nof a real ap\u00adplication. 1 Introduction and Motivation High-performance architectures rely upon powerful \noptimizing and parallelizing compilers to increase pro\u00adgram performance. One of the critical features \nof such compilers is accurate data dependence analysis [Ken90]. A good deal of work has been done in \nthe area of array analysis (see [PW86, ZC90, Ban93] for extensive references), with notable success. \nHowever, *Please direct correspondence to jhumrnel@ics.uci. edu. This work supported in part by an ARPA \nResearch Assistantship in Parallel Processing administered by the Institute for Advanced Computer Studies, \nUniversity of Maryland. t This work supported in part by FCAR, NSERC, and the McGill Faculty of Graduate \nStudies and Research. $T&#38; work supported in part by NSF grant CCR8704367 -d ONR grant NOO01486K0215. \nPermission to co y without fee all or part of this material is granted prowd J that the copies are not \nmade or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication \nand its date appear, and notice is given that copying is by permission of the Association of Computing \nMachinery. To copy otherwise, or to republish, requires a fee and/or specific permission. SiGPLAN 94-6/94 \nOrlando, Florida USA 0 1994 ACM 0-89791 -662-x/94/0006.. W.5O much less work has been done in the area \nof pointer analysis, and with varying degrees of success. This paper is concerned with developing an \naccu\u00adrate data dependence test in the presence of dynamic, pointer-based data structures. This is a problem \nwhich continues to grow in import ante, for two principal rea\u00adsons, Firstly, there is an increasing use \nof languages which support pointers, in particular C, C++, Ada, and FORTRAN 90. Secondly, pointers and \ndynamic data structures are important tools for achieving good performance. For example, oc-trees are \nimportant data structures in computational geometry [Sam90] and N\u00adbody simulations [App85, BH86, WS92], \nas are sparse matrices in circuit simulations [Kun86, SWG91] and many other applications. Existing techniques \nare either overly conservative in the presence of dynamic data structures, or work well for only a small \nset of structures (linked-lists and trees). In this paper we present a new, general dependence test which \ncan yield accurate results for a wide range of data structures, from simple tree-like structures to complex \ncyclic structures. Our test is based on theorem proving, using axioms which describe uniform proper\u00adties \nof the data structure. The test is general since its accuracy grows with the accuracy of the axioms, \nand it supports any data structure which possesses some form of regularity. The test can be used to disprove \nde\u00adpendence between two statements in a sequence, be\u00adtween loop iterations, and between statement blocks. \nAs a result, our dependence test can enable fine-grain, loop-level, and block-level transformations, \npotentially yielding significant improvements in performance. It is important to note that theorem proving \nin this con\u00adtext is decidable. The power and general applicability of theorem proving is well known, \nas is its general un\u00addecidability, Thus, a more subtle contribution of our work is the application of \ntheorem proving in a power\u00adful yet decidable manner. The format of this paper is as follows. In the next \nsection we discuss the problem in more detail, and present related work. In Section 3 we introduce our \napproach by way of example, followed by a more for\u00admal present ation of our dependence test in Section \n4. We then discuss the results of applying our test to a real application in Section 5, and present our \nconclu\u00adsions in Section 6. 2 Related Work Before discussing related work, it will be helpful to first \nclarify the problem we are trying to solve. = malloc( ...); i.nsert(head, q); q = &#38;i;P ... q = head;s: \n*p = 10; while ,,, < ... u: q->f = funo; T: i =20; = q->link;q > Figure 1: The two different pointer \nproblems. 2.1 Problem Clarification The problem of dependence testing in the presence ofpointers is better \nunderstood (and attacked) ifsepa\u00adrated into two, largely-distinct subproblems. The first concerns pointers \nto named memory locations (typi\u00adcally stack-based variables), the flavor of which is cap\u00adtured by the \nleft code fragment of Figure 1. In this ex\u00adample, there is an output dependence from statement S to statement \nT iflp points to .i at S. The second subproblem involves pointers to un\u00adnamed memory locations, which \narise from building data structures out of dynamically-allocated, heap\u00adbased memory. The right code fragment \nof Figure 1 depicts a common instance. In this case, there exists a loop-carried output dependence from \nthe statement U to itself 28 q from one iteration points to the same memory location as a q from a later \niteration. Note that we are unable to refer to these memory locations by name. We shall refer to the \nlatter subproblem as the pointer data structure dependence problem (PDSDP), and the former as the pointer \ntarget dependence prob\u00adlem (PTDP). This paper is concerned solely with PDSDPl . 2.2 Solution Components \nAny accurate solution to the pointer data structure dependence problem must consist of at least three \ncom\u00adponents, whose relationship is shown in Figure 2. In 1~ c one ~u~t ~so consider pointers to dynamic \nways ~d multi-dimensional arrays construct ed using pointers. Since the programmer s intent is a well-understood \ndata structure (the ar\u00adray) for which numerous dependence tests already exist, we view this as a special \ncase of PDSDP and feel it should be handled as such. particular, it is important to note that the dependence \ntester haa two inputs, each of which is distinct and can be obtained in various ways. Typically this \nin\u00adformation is obtained via automatic analysis of the program; in this case, two different analyses \nare nec\u00adessary, When appropriate, we shall refer to these aa data structure analysis and memory reference \nanalysis, respectively. It should be noted that these forms of analysis are often tightly intertwined. \nThis becomes obvious when a data structure is under modification structurally; maintaining accurate memory \nreferences requires an understanding of the data structure and how it is being changed. t Yes I No I \nMaybe Figure 2: Solution components to PDSDP.  2.3 Extending Solutions to PTDP There exist numerous \napproaches to the pointer tar\u00adget dependence problem [COU86, HPR89, LMSS91, LR92, CBC93, MLR+93, EGH94], \nall of which fol\u00adlow a similar analysis and dependence testing frame\u00adwork: the program is analyzed (perhaps \ninterprocedu\u00adrally), and at each program point the set of aliased variables is computed. Dependence testing \nis then performed by simply intersecting the appropriate sets. This scheme works well in PTDP since pointers \ngen\u00aderally refer to named memory locations i.e. variables. However, such store-based approaches [Deu92] \ndo not extend well to the domain of dynamic, pointer-based data structures, for the simple reason that \npointers may now refer to a seemingly infinite number of memory lo\u00adcations, while the dependence test \nis designed for a fixed number of memory locations. The typical solution is to retain the dependence \ntest from PTDP, and adopt a k-limited data structure anal\u00adysis [J M82]. Given a set of dynamically-allocated \nmem\u00adory locations, this haa the effect of assigning k unique names to the first k memory locations, and \na sin\u00adgle summary name to all remaining memory locations. This quickly becomes overly conservative-consider \na loop, for example. Even though each iteration may write to a different memory location (suppose the \nloop is updating a linked-list), at best the dependence test will prove that only the first k iterations \nare indepen\u00addent. It should be noted that the conservativeness of such k-limited approaches has been \naddressed to some degree by the work of Chase et. al. [CWZ90], and later improved upon by Plevyak et. \nal. [P CK94]. However, these techniques do not propose a more accurate depen\u00addence test, but rather a \nmore accurate data structure analysis which extends the k-limited scheme to provide better information \nin the case of linked-lists and trees.  2.4 PDSDP Only These approaches are based on a more powerful \nnam\u00ading scheme, designed specifically for the pointer data structure dependence problem. The general \nidea is to name memory locations by relation to one another, and then test for intersecting relations. \nGiven the poten\u00adtial complexity and number of such relations, the diffi\u00adculty is designing an accurate \ndependence test. These approaches are sometimes referred to as store less ap\u00ad proaches [Deu92]. Larus \net. al. [LH88] presented a technique for nam\u00ad ing memory locations using path expressions. Viewing a \ndata structure as a directed graph where edges are la\u00adbeled with the names of their respective pointer \nfields, each vertex is labeled with a regular expression denot\u00ading a path through the data structure \nstarting from a particular vertex v. Dependence testing is performed by intersecting path expressions, \nin particular the lan\u00adguages denoted by the corresponding regular expres\u00adsions. If the intersection is \nempty, a dependence does not exist. Path expressions are computed during data structure analysis and \nstored in alias graphs. Mem\u00adory reference analysis, however, collects access paths, which denote actual \ntraversals of the data structure by the program. Thus, the dependence tester is employed by mapping the \ngiven access paths to their appropriate path expressions, using the alias graphs. For trees, the dependence \ntest of Larus et, al. is a precise one. However, for DAGS and graphs the results are overly conservative. \nThe problem lies in the use of intersection, which forces an overly conservative map\u00adping from access \npath to path expression. For exam\u00adple, consider the leaf-linked binary tree shown in Figure 3. Since \nthe access paths root,LLNN and root.LRN lead to the same vertex, their corresponding path ex\u00adpressions \nmust produce a non-empty intersection (note that the intersection of LLNN and LRN would be empty, which \nis incorrect). Thus, to obtain correct results, these access paths are mapped to more conser\u00advative path \nexpressions (e.g. (L IR)+ N+). The result is that very similar access paths, such u root,LLN and root \n.LRN, are likewise mapped, resulting in a non\u00adempty intersection even though it can be proven that these \naccess paths will never lead to the same vertex (see Section 3.3). It should be noted that more accu\u00adrate \nmapping strategies may in fact exist; Larus et. al. did not address this (we explored the idea, but found \nour solutions to be inadequate), Hendren et. al. [HN90] discussed a similar technique for naming memory \nlocations, in this case using a sim\u00adpler form of paths. Paths are collected in a path ma\u00adtrix such that \nall paths between memory locations of interest are known. This approach is potentially less expensive \nthan that of Larus, yet also precise for trees. However, it fails to present a general dependence test, \nand does not handle cyclic data structures. The notion of access paths is also used by Deutsch [Deu92]. \nHe discusses a method of aliaa analysis based on a new, po\u00adtentially more powerful form of data structure \nanalysis. However, a general dependence test is not presented. Finally, Guarna [Gua88] took a different \napproach and used syntax trees as a naming scheme, These trees are then intersected to detect dependence. \nThough his technique was applicable to both PTDP and PDSDP, in the latter domain it is limited to tree-like \nstructures only. 2.5 Other Work There has been substantial work done on program analysis in regards \nto dynamic allocation. Such anal\u00adysis is useful in many related problems, e.g. reference counting and \nmemory lifetimes [Hud86, ISY88, RM88, Har89, Bak90, WH92] as well as memory placement [Har89, HA93]. \nIn particular, Harrison [Har89] per\u00adforms extensive analysis of dynamically allocated mem\u00adory; this work \nhas been extended in [HA93]. From this analysis it is possible to discover various properties of a data \nstructure (e.g. its treeness), and thus prove that certain dependence are not possible. However, a gen\u00aderal \ntechnique of dependence testing which exploits this information is not presented. Neirynck et. al. [NPD89] \ndeveloped an abstract in\u00adterpretation approach capable of coarse-grain depen\u00addence testing on higher-order \napplicative programs. The eflect system [LG88] is a language-based approach in which the effect of a \nstatement must be explic\u00aditly associated with a region of memory; this enables the compiler e.g. to perform \ncoarse-grain dependence testing. Klappholz et, al. [KKK90] discuss another language-based approach in \nwhich dependence testing relies upon programmer-supplied partition statements and software tagging. 3 \nOverview of Our Approach Our approach is most similar to that of Larus et. al. [L H88], since we also \nbase our naming scheme on regular expressions. Regular expressions allow more accurate naming than k-limited \nschemes, possess im\u00adportant theoretical properties such as decidability, and fit naturally into a standard \nanalysis framework (the effect of a statement sequence can be summarized us\u00ading concatenation, selection \nthrough alternation, and type LLBi.naryTree_t { integer d; LLBi.naryTree-t *L; LLBinaryTree-t *R; R LLBinaryTree-t \n*H; } A where AXIOUS are Al: Vp, p.L<>p.R, A2: V p <> q, p.(L[R) <> q.(LIR), A3: Vp <> q,p.N <> q.N, \nAA A4: Vp, p.(LIRllV)+ <>p.~;  0+0+0+0 Figure 3: A leaf-linked binary tree: type declaration, example \ndata structure. iteration via kleene star). Unlike Larus et. al., our de\u00adpendence testeris based ontheorem \nproving, providing a more powerful and accurate test. The critical inputs to our dependence tester are \ntwo\u00adfold: (1) axioms which define various aliasing proper\u00adties of the data structure, and (2) access \npaths for the two memory locations being referenced. The depen\u00addence tester then applies the axioms directly \nto the supplied access paths in an attempt to prove that no de\u00adpendence is possible. If a proof exists, \nthe dependence tester will find it and return NO. Otherwise, the tester halts and either returns yes \n(a dependence definitely exists) or Maybe (a dependence possibly exists). Theo\u00adrem proving is applied \nin a completely general manner, allowing more accurate results given more accurate in\u00adputs. In this first \npart of this section we define our notions of aliasing axioms and access paths. We then briefly overview \nthe framework necessary to support our de\u00adpendence tester. Finally, we present our test by way of example. \nA more formal discussion of our work follows in Section 4. 3.1 Aliasing Axioms and Access Paths Aliasing \naxioms define alissing properties which hold uniformly throughout a given data structure. An ax\u00adiom can \ntake one of three forms: 1. V p, p.REl <> p.RE2, 2. V p <> q, p.REl <> q,RE2, and 3. V p, p.REl = p,RE2. \n Viewing a data structure as a directed graph where edges are labeled with their corresponding pointer \nfield names, the variables p and q refer to any vertex in the graph. The regular expressions RE1 and \nRE2 denote sets of paths through the data structure. Thus, the access path p,REl denotes the set of vertices \nreached by starting at vertex p and traversing any path in REI. The semantics of an aliasing axiom are \nstraightfor\u00adward. Given an axiom V p, p.REl <> p,RE2, this states that V vertices p, the access path \np.REl never refers to the same vertex as the access path p.RE2. In other words, p,REl (1 p.RE2 == 0. \nAxioms of the form V p <> q, p.REl <> q.RE2 are defined similarly. Fi\u00adnally, axioms of the form V p, \np.REl = p,RE2 imply the opposite: V vertices p, the sets p.REl and p.RE2 are equivalent. This latter \nform is useful for describing cycles in a cyclic data structure. As an example, consider the leaf-linked \nbinary tree shown in Figure 3. Four axioms are supplied as part of the type declaration and define the \ncritical properties of this data structure. The first two axioms define the treeness of the substructure \nconsisting of L and R fields: Al states that L and R lead to different vertices from the same vertex, \nwhile A2 adds that these fields never lead back to the same vertex from different vertices. Note these \naxioms alone do not completely describe a tree however, since they allow the possibility of a cyclic \nedge from a leaf back to the root. A3 conveys that the sub\u00adstructure formed by the N fields is a linked-list, \nsince it states that different vertices will never lead to the same node via M forcing M-1abeled edges \ninto a linear ordering. Note that this axiom does not completely describe a linked-list, since it allows \na cyclic edge from the last node in the list to the first, Finally, A4 defines the acyclicness of the \ndata structure, in particular stat\u00ading that L and R form a true tree and that M forms an acyclic linked-list. \nFor completeness, note that since the axioms do not say otherwise, it is assumed that the L, R, and M \nfields form a DAG, which they do. Though exceedingly simple in nature, a set of alias\u00ading axioms is able \nto describe quite complicated data structures, such as sparse matrices (see Section 5) and tw~dimensional \nrange trees (a leaf-linked tree of leaf\u00adlinked trees, used in computational geometry [PS85]), See [HHN94] \nfor a more complete discussion.  3.2 Supporting Framework As pictured in Figure 2 (and discussed in \nSection 2.2), an accurate dependence test for PDSDP requires two types of information: information about \nthe data structure, and information about the referenced mem\u00adory locations. In the case of our dependence \ntest, the [LH88, HN90,Deu92] ~ 1 I \\/ 9 Theorem Prover Yes /No / Maybe Figure 4: Our solution components. \n former is supplied by way of the axioms, and the lat\u00adter through access paths. This information can \nbe col\u00adlected in various ways, as depicted in Figure 4. Axioms can be collected automatically (using \nmany of the tech\u00adniques discussed in Section 2), or supplied by the pro\u00adgrammer (and perhaps automatically \nverified). In the latter case, note that axioms can be specified indirectly using a higher level of abstraction, \ne.g. the ADDS data structure description language [HHN92] or graph types [KS93]. Access paths are straightforward \nto collect, since standard flow analysis techniques map nicely (and accurately) into regular expressions. \nVarious forms of path collection are discussed in [LH88, HN90, Deu92]. 3.3 An Example We consider an \nexample involving leaf-linked trees, a data structure used e.g. in N-body simulations [BH86]. In our \ncase we consider binary trees, an instance of which is shown in Figure 3. Also shown in the figure are \na set of axioms which we assume hold at the start of our example. Access paths will be collected and \npresented in the form of access path matrices (APM). There exists an APM at each program point, where \neach entry in an APM denotes a path (or set of paths) which may have been traversed up to (but not including) \nthat point in the program. Note that an APM does not summarize all possible paths between vertices in \na data structure, only paths explicitly traversed by the program. The key observation is that whenever \npossible, access paths should be collected in reference to fixed vertices in the data structure (Larus \net. al. [LH88] collected access paths in this manner). We will refer to these vertices as bandies. Handles \nare associated with pointer variables, and a new handle -hp is created each time its associated pointer \nvariable p is assigned to. The one exception is when p is assigned a value relative to itself, in which \ncase a new handle is not created; this is important e.g. when p is an induction variable for a loop (as \nwill be the case in Section 5). Existing handles are destroyed when they are no longer needed, i.e. they \nno longer anchor any access path. subr (LLBi.naryTree_t *root ) { LLBinaryTree_t *p, *q; root = root->L; \n= root->L; p = p->N; P s: p->d = 100; p = root; I: q = root->R = q->~; q T: return q->d; 3 Consider \nthe code fragment shown above. The ques\u00adtion is whether or not statement T is dependent on statement \nS. When the analysis reaches S, we have the following APM: root  I ApJ f  E!E1343 There are two handles \nand three access paths; for ex\u00adample, the vertex denoted by p can be reached via the handle-based access \npath hroot ,LLN. Continuing, we obtain the following APM at statement I: Notice that a new handle Ap2 \nwas added due to the assignment of root to p, and that the handle Jp is no longer of use and can be destroyed. \nEventually, we obtain the following APM at statement T: Is there a dependence from S to T? We scan the \nAPMs at statements S and T, looking for a handle com\u00admon to both p and q (though a common handle is not \nrequired by the dependence test, it generally leads to more accurate results). Finding one in _hroot, \nwe re\u00adplace p at S with the access path _hroot .LLN, and q at T with -hoot .LRN. Note that since none \nof the pointer fields in the data structure have been modified between S and T, we know that p s access \npath is still valid at T. Assuming that all four axioms of Figure 3 were valid at the start of subr, \nwe also know that these axioms are valid when T is reached. There is no dependence if it can be proven \nthat for all possible roots, and across all possible data struc\u00adtures under which these axioms hold, \nthe access paths can never lead to the same vertex. Thus, the theorem prover constructs the following \ntheorem of no depen\u00addence, and then attempts to prove this theorem: Theorem: T is not dependent on S \nif V ver\u00ad tices tioot, &#38;-oot .LLN <> Jmoot .LRN. The following paraphrsaed proof is derived automati\u00adcally: \nProof Applying A3, theorem is true if _hroot.LL <> -hroot.LR.  Since both paths start from the same \nvertex and begin with L, reduces to showing that _hroot .L <> Jmootl.R.  Applying Al, this holds. 0 \n  The dependence test thus returns No, and we can con\u00adclude that no dependence exists from S to T. 3.4 \nImpact of Structural Modifications When a data structure undergoes structural modifi\u00adcation, i.e. one \nor more of its pointer fields is updated, this can invalidate both access paths and axioms. For example, \ninserting a new vertex may lengthen an ac\u00adcess path, while the insertion process itself may tem\u00adporarily \nbreak any treeness axioms. It is the job of the analyzer to identify and understand such structural changes, \nand adjust its information accordingly. This is a difficult and common problem in all forms of pointer \nanalysis. On a more positive note however, we have found that many applications contain large portions \nof structurally read-only code, since the code which performs structural modifications is often localized \nfor maintainability. In our case, suppose we have a statement M which modifies a pointer field. If a \ndependence test is per\u00adformed across M, then the access path moved ovet M may need to be adjusted. Likewise, \nthe set of valicl axioms may also need to be updated. As access paths are collected, the set of axioms \nvalid after M must be recorded. If a dependence test is then given an access path whose construction \noccurs across M, the set of axioms to supply is thus the intersection of the axiom sets valid before \nand after M. 4 An Axiom-based Pointer Test First we present the algorithm behind our depen\u00addence test, \nthen a discussion of its time and space com\u00adplexity. 4.1 Algorithm APT, an Axiom-bssed Pointer Test, \nis a theorem proving system involving axioms specified using reg\u00adular expressions, Pointer values are \nexpressed as ac\u00adcess paths, which are also based on regular expressions. APT is most appropriate for \ntests involving data struc\u00adtures, since the axioms can describe properties of the data structure, while \nthe access paths can denote paths through the structure. We assume the existence of two statement execu\u00ad \n tions S and T, where S precedes T and each accesses a single field relative to some pointer (we assume \nthat expressions involving multiple fields have already been simplified into this format [HDE+93]): \n s: ... p->f ... ; T: ... q->g ... ; Furthermore, either S performs a write to p-z~, T per\u00adforms a write \nto q->g, or both, with no intervening write to the memory location denoted by p->f. There exists a data \ndependence from S to T, denoted S j T, iff p-> f and q->g denote overlapping memory regions. The input \nto the dependence test consists of a set d of applicable axioms, two expressions ES and ET (corresponding \nto p->.f and q-zg respectively), and two valid access paths APP and APq (corresponding to HP .PathP and \nHg .Pat hq respectively). The depen\u00addence test returns NO if it can be proven that no data dependence \nexists from S to T (with respect to the given expressions), or Yes if it can be proven that a data dependence \ndefinitely exists. Otherwise, the de\u00adpendence test returns Uaybe. deptest(d, Es, ET, APP, APq) begin \nif p and q are different types then return MO; if ~ and g do not overlap then return MO; assert (HP = \nHq); if PathP = Pathq and \\PathP\\ = 1 then return Yes; if proveDisj(A, Pathp, Pathq, ~, C) then return \nMO; return Maybe; end dept est;  The first step is to check for the possibility of a data de\u00adpendence. \nWe assume that pointers are not cast freely between data structure types, and that pointers to a vertex \nv point to the start of v in memory and not elsewhere. These are quite reasonable smumptions, since a \ncompiler will assume such when generating code (1) Figure 5: The two cases of proveDisj. to access fields \nrelative to a pointer. Though safe in ANSI C, these assumptions are not necessarily true in older K&#38;R \nC. In this csse special checks are needed to guarantee validity of the above assumptions. If these checks \nfail, then the first two tests in the dependence algorithm should be skipped. We also assume that ac\u00adcess \npaths share a common handle; the test for differ\u00adent handles is nearly identical, although its accuracy \ndepends on knowing the relationship between the two handles as well. There is a definite dependence if \nthe access paths are guaranteed to reach the same vertex; this is true if the paths PathP and Pathg are \nidentical and the cardinal\u00adity of these sets is one. In this csse the dependence test returns yes, Otherwise, \nwe attempt to prove the opposite, that a dependence is impossible. The core of the dependence test is \na general theorem proving system. The axioms are applied in all possible combinations in an attempt to \nprove that the access paths cannot lead to the same vertex. The idea is to apply the axioms to ever-increasing \nsuffixes of Pat hP and Pathq, in an attempt to prove that these suffixes cannot lead to the same vertex. \nSince the distinct\u00adness of the vertices visited along each access path is unknown, a proof must consider \ntwo cases: (1) suffixes may originate from the same vertex, and (2) suffixes may originate from distinct \nvertices. The situation is shown in Figure 5, where SP and S~ denote the suffixes and PP and Pg the resulting \nprefixes. Note the direct correlation between the two principle forms of axioms ( vp... and V p <> q \n. . . ) and the format of these two csses. The core of our dependence test, proveDisj, thus begins as \nfollows: proveDisj(d, P;, P;, S~, S:) begin if new suffixes do not exist then return False; let SP = \na longer suffix of path P; + S;; let PP = new prefix given choice of SP; let S g = a longer suffix of \npath P; + S;; let Pq = new prefix given choice of S g; A: let T1 = result of trying to prove: V vertices \nz, z.SP <> x.Sg; B: let T2 = result of trying to prove: V vertices x <> y, x.SP <> y.Sq; ifTI =True andT2=True \nthen return True; < remainder of proveDis j to follow; > The results T1 and T2 are obtained by direct \napplicw tion of a single axiom a from A. For example, consider the calculation of T1. For all axioms \na of the form V p, p.REl <> p. RE2, the dependence tester tries to show that either SP &#38; REI and \nSq ~ RE2, or vice versa. If either case is true, then the proof of step A succeeds and T1 becomes True. \nOtherwise the proof fails and T1 is set to False. The question of subset can be answered using some general \ntheory of regular ex\u00adpressions. Given two regular expressions R1 and R2, RI ~ R2 if All n complement \n(lvf2) = 0, where Ml and M2 are the DFAs constructed from R1 and R2, respectively. The algorithmic details \nare discussed in [HU79, DDQ78]. However, if only T1 or T2 is true, it may still be possible to prove \nno dependence. Consider Figure 5 once again: C: if T1 and can prove HP.PP = Hq.Pq then return True; D: \nif T2 and can prove HP .PP <> Hq .Pq then return True; E: < alternative, kleene star processing; > F: \nreturn proveDisj(A, F P, Pq, Sp, S9); end proveDisj;  Step C corresponds to case (1) and is true if \nthe access paths HP .Pp and Hq .Pq denote a definite dependence, Step D corresponds to case (2) and can \nbe answered recursively: proveD is j (A, PP, Pq, e, e), If these at\u00adtempts at a proof fail, the algorithm \ncontinues recur\u00adsively in search of new suffixes and a successful proof (steps E and F). Eventually, \nthe algorithm will either find a proof and succeed, or if no such proof exists, halt and fail. This completes \nthe core of the depen\u00addence testing algorithm. Two issues remain, however. Firstly is the algorithm behind \nsuffix generation. A regular expression consists of zero or more components, where each component is \neither c, a single field name2 ~, two alternative compo\u00adnents a/b, a kleene star component a*, or a parenthe\u00adsized \ncomponent (a). Suffixes are selected by starting with the original paths Pat hP and Pat hq and generat\u00ading \nthree sets of suffixes: last component of Path and (1$1) ~ = ~=t ~omponent of pat~ (1,0): Spq=last component \nof PathP &#38;d Sq =Z c, (0,1): SP = ~ and S, = last component of Pathq. This process is then repeated \nrecursively for case (1,1). For cases (1,0) and (0,1), only cases (1,0) and (0,1), respectively, are \nrepeated. This approach generates the exact set of all possible suffixes. Secondly is the handling of \nparenthesized, alterna\u00adtive, and kleene star components: if the current prefix ends in one of these components, \nhow is a new suillx generated? In the case of parenthesized components, the parentheses are simply stripped \nbefore suffix gen\u00aderation. For an alternative component a lb, it is first treated as a single component \n(in the hope of find\u00ading a proof more quickly) and processed as before by proveD is j. However, if the \nproof fails, we then split the alternatives and attempt to prove each csse sep\u00adarately. The difference \nis that both alternatives must result in a successful proof in order for the original proof to succeed. \nFinally, given a kleene star component, at first it is treated as a single component. If the proof fails, \nthen induction is employed. Suppose that only one of the prefixes ends in a kleene component, denoted \nby a*. An inductive proof requires three cases, each emphasizing a replacement for a*: 1. replace with \nc, 2. replace with a, 3. assume a a and replace with a* aa.  If the inductive step (3) cannot be \nproven directly from the inductive hypothesis, the theorem prover proceeds recursively in search of a \nproof (note that the replace\u00adment string cent ains a*). This will require the proof of additional base \ncases, and may even require further recursion. Eventually however, the process terminates since the original \nproof will either succeed (and thus halt) or fail on some base case due to the finite num\u00adber of axioms \n(and thus halt). If both prefixes end in kleene components, the resulting proof contains four major cases, \nsummarized as follows (we use kleene + to simplify the presentation, and we assume that PP ends in a* \nand Pg ends in b ): zFor ~impficity we ~~we that arrays of pointers do not exist~ and that field names \nare unique across type declarations. Lifting these restrictions is simply more detail. 1. replace with \n(c, e), 2, replace with (c, b+), 3. replace with (a+, .s), 4. replace with (a+, b+),  This last case \nis handled inductively via four sub-cases: 4.1 replace with (a, b), 4.2 replace with (a+, b), 4.3 replace \nwith (a, b+), 4.4 assume (a+, b+) and replace with (a+a, b+ b). Thus, a total of seven cases are required \nwhen both prefixes end in kleene components.  4.2 Complexity The algorithm will either find a proof \n(if one exists), or halt with failure (if one does not)3. However, this may require an exponential amount \nof time in the worst case. Suppose the original paths PathP and Pathq con\u00adtain n components. There exist \n0(n2) sets of suffixes which must be checked, and thus 0(n2) different proofs to be explored (this assumes \nthat the results of inter\u00admediate proofs are cached so that a proof attempt is never repeated, and that \nthe cache is maintained and searched efficiently). We shall refer to this set of proofs as P. Now we \nconsider the cost of exploring one proof in P, which corresponds to an execution of proveD is j. If \nthe paths do not contain alternative nor kleene star components, the cost of this proof is based on two \nfac\u00adtors. Firstly, step D may cause an intermediate proof. However, this intermediate proof is in P, \nso we pay this cost only once-either now or later. Hence we ignore it here. Secondly, steps A and B require \nsearching A for an applicable axiom to use in the proof, Each such search requires a subset operation, \nwhich is dominated by the time it takes to convert the two regular expres\u00adsions into equivalent DFAs. \nIn the worst case, this conversion process may produce DFAs of size 0(2n). With m axioms, the time cost \nof steps A and B be\u00adcomes 0(rn2n ). Therefore, in the worst case, the cost of exploring a single proof \nis exponential in terms of both time and space. If a proof in p involves a kleene component and ini\u00adtially \nfails (i.e. steps A-D fail), the cost of induction (step E) is a constant number of intermediate proofs \n(most of which are not in P). This constant depends on the number of base cases that are ultimately re\u00adquired, \nand is a function of the axioms. However, in the worst case, each intermediate proof may in turn in\u00advolve \nkleene star components, This has a multiplicative 3Note that the problem of static analysis in the presence \nof pointers has been shown to be undecidable [Lan92]. In relation to our work, this undecidabllity result \ncorresponds to the prob\u00adlem of performing accurate data structure and memory reference analysis (seeFigure \n2). effect, resulting in a proof with a worst-case exponen\u00adtial time complexity of O(c ). Worst-case \nspace com\u00adplexity is also exponential due to the DFA construction process. The same holds true if a proof \nin P involves an alter\u00adnative component. In this case however, an exponen\u00adtial number of axioms is required \nto force worst-case exponential time. In practice we have found that paths are relatively short (n is \non the order of ten) and relatively simple (few kleene and alternative components). This is a consequence \nof both the analysis (with its numerous handles) and the nature of typical applications (which feature \nwell-structured code and highly-regular data structures). Proof times thus become dominated by the RE \nto DFA conversion process, which generally runs in 0(n2 ) time and space complexity. Thus, we have found \nthat in practice, our dependence algorithm requires 0(n4) time and 0(n2) space. Of course, the proof \nprocess can be pruned heuristically and cutoff points set, allowing a tradeoff between accuracy and efficiency. \nThis may even be user controllable, e.g. via a compiler option. Results To demonstrate the effectiveness \nof our dependence test, a prototype was implemented and applied in a re\u00adalistic situation sparse matrices \nas used e.g. in circuit simulations [Kun86]. Sparse matrices are often implemented using orthog\u00adonal \niists [St a80], an example of which is shown in Figure 6 (along with suitable type declarations). The \nfundamental operations performed on sparse matrices are scaling, factoring, and solving; in terms of \nexecu\u00adtion time, scaling and solving are linear in the size of the data structure, whereas factoring \nis quadratic. We shall focus on factorization here. Gaussian elimination is used to factor the matrix, \nwhere the crucial consideration is minimizing jillins (el\u00adements added to the matrix as a direct result \nof the factorization process). Good pivot selection is one of the keys to reducing the number of fillins, \nand thus considerable effort is spent in selecting the best possi\u00adble pivot element at each factorization \nstep. Here is a high-level overview off act or: factor(ftl) begin for each successive row R k M let SM \n= submatrix[l?+ 1.. IV, R + 1..IV]; { compute fillin heuristic for each elem in SM; search SM for best \npivot p; adjust M to bring p into pivot position; add fillins to SM; perform elimination on each row \nof SM; } end factor; Notice that for each row of the matrix, four of the five steps operate on the entire \nsubmatrix. Thus, each such step executes in a row-by-row (or column-by-column) fashion, something akin \nto: let r := p-XmowE; Ll: while r != NULL { let e := r->ncolE; L2: while e ! = NULL { yet ;.. := e->ncolE; \n} let r := r->nrowE; } It turns out that in each step, the effect of statement S is either local to \nthe element e or the row r, revealing a good deal of parallelism in both the outer and inner loops. The \nproblem is exploiting this parallelism in the pres\u00adence of a complex, pointer-based data structure. We \nshall consider parallelizing the the outer loop Li; simi\u00adlar logic can be applied to the inner loop L2. \nConsider the first iteration of LI. The elements accessed are sum\u00admarized by the path expression Jr.ncolE(ncolE)*. \nThe elements accessed in subsequent iterations are thus summarized by Jm. (nrowE)+nco/E(nco/E)*. In fact, \nsince r is the induction variable for LI, these two paths expressions hold for any two iteration executions \ni and j (i < j). This leads to the following theorem of no dependence: Theorem T : s is not loop-carried \ndepen\u00ad dent on itself at the level of LI if V vertices -hr, Jur.ncolE+ <> Jm.nrowE+ncolE+. Since the \nsub-structure formed by the fields ncolE and nrowE is a DAG and not a tree, T cannot be proven by simply \nintersecting the given path expressions (see Section 2.4). Instead, the intersection of two, more conservative \npath expressions must be performed, re\u00adsulting in a non-empty intersection and thus an unsuc\u00adcessful \nproof. K-limited approaches will likewise fail to find a proof. However, the theorem prover we have outlined \nin Section 4 will be able to derive a proof automatically, assuming it is given enough information. At \nthe very least, the theorem prover must know that: (1) rows form linked-lists (not DAGs), (2) the end \nof a row or column does not lead to the start of a different row or column, and (3) the sub-structure \nis acyclic. This information can be conveyed by the following three ax\u00adioms, respectively: Al: V p <> \nq, p.ncolE <> q.ncolE, A2: V p, p.ncolE~ <> p,nrowE~, and A3: V p, p.(ncolE~nrowE)~ <> p.c. These axioms \ncan either be supplied by the program\u00admer (note that a single axiom along the lines of The\u00ad type Sparse14atrix.t \n{ Rovllclr-t *rous; ColIidr-t *COIS; };   F  n t -? type RowHdr_t { Elexaent-t *relems; nrowH nrowE \nRowHdr_t *nrowH; ]; type ColHdr.t o { Element-t *celems; ColHdr_t *ncolH; 3; type Element-t ,~-->:r v \n r/ 0 { real value; Element_t Element_t *nrowE; *ncolE; }; v o Figure 6: A sparse matrix: type declarations \nand example data structure. orem T will also suffice), or obtained using automatic data structure analysis \ntechniques (although current approaches appear too limitedto automatically). Regardless, these forourtheoremproyer \nto proveT, cal false dependence and revealing of parallelism to the compiler. omitted due to its length \n(there derive these axioms axioms are sufficient thus breaking acriti\u00adasignificant amount The proof has \nbeen are four initial cases since each access path ends in + , and many of these contain multiple sub-cases); \nthe complete set of axioms for a sparse matrix are given in Appendix A. The importance ofbreaking these \nfalse dependences is demonstrated by the speedup figures shown in Fig\u00adure 7. To collect these results, \nwe manually applied loop-level transformations and ran the resulting code on an8-PE Sequent multiprocessor. \nGiven the discus\u00adsion in Section 3.4, we collected two types of results. Firstly, we assumed a simplistic \nanalysis which only col\u00adlected access paths for structurally read-only portions of the code, These are \nthe partially parallel results. Secondly, we assumed a more sophisticated analysis ca\u00adpable of handling \nmodifications to the structure of the sparse matrices. These are the fully parallel results. Inthepartial \ncaee, the speedups aregoodbutnotlin\u00adear. The fully parallel case comes nmch closer to linear speedup, \nHowever, it remains sub-linear since one of the factorization steps ( adjust &#38;l to bring p into pivot \nposition ) is inherently sequential. Conclusion We have presented anew dependence test (APT) appropriate \nfor dynamic, pointer-based data struc\u00adtures. It is more accurate than existing approaches, and supports \nany data structure which possesses some form of regularity. Our test is a general one based on 1000X1OOO,N=1O, \n000 Factor only partial) Scale, Factor, Solve (partml) Factor only (full) Scale. Fact or. Solve (full) \n Figure 7: Sparse matrix 2 PEs 4 PES 7 PES 1.7 2.5 3.1 1.7 2.4 3.0 1.8 3.3 5.2 1.8 3.3 5.2 speedup results. \n theorem proving, allowing it to produce more accurate results given more accurate information. This \ninforma\u00adtion is supplied in the form of axioms, which describe uniform properties of the data structure, \nand access paths, which denote the memory locations in question. In essence, the dependence tester applies \nthe axioms to the access paths in an attempt to prove that these paths will never refer to the same memory \nlocation. Ac\u00adcess paths are easily collected using standard analysis techniques, and axioms can be obtained \nin a number of ways. Hence APT is not dependent on any particular form of data structure analysis. For \nthe expected case, our approach is practical. Its use can break false dependence between statements in \na sequence, iterations of a loop, or blocks of st atements. This in turn can enable the application of \nperformance\u00adenhancing transformations at the statement, loop, and block level. The effectiveness of our \ntest was demon\u00adstrated using a real example, namely sparse matrices. References [App85] Andrew W, AppeL \nAn efficient program for many-body simulation. SIAM J. Sci. Stat. Comput., 6(1):85-103, 1985. [Bak90] \nH. Baker. Unify and conquer (garbage, updat\u00ading, aliasing, ...) in functional languages. In Pro\u00ad [Ban93] \n[BH86] [CBC93] [COU86] [CWZ90] [DDQ78] [Deu92] [EGH94] [Gua88] [HA93] [Har89] [HDE+93] ceedings of the \n9o ACM Conference on LISP and Functional Programming, June 1990. U. Banerjee. Loop Trans~ormations for \nRestruc\u00adturing Compilers: The Foundations. Kluwer, 1993. Josh Barnes and Piet Hut. A hierarchical O(NlogN) \nforce-calculation algorithm. Nature, 324:446-449, 4 December 1986. The code can be obtained from Prof. \nBarnes at the University of Hawaii, or from jhummel@ics.uci. edu. J. Choi, M, Burke, and P, Carini, Efficient \nflow-sensitive interprocedural computation of pointer-induced aliases and side-effects. In Pro\u00adceedings \nof the ACM 20th Symposium on Prin\u00adciples of Programming Languages, pages 232\u00ad245, January 1993. D. Coutant. \nRetargetable high-level alias anal\u00adysis. In Proceedings o.f the ACM Symposium on Principles of Progmmming \nLanguages, pages 110-118, January 1986. D.R. Chase, M. Wegman, and F.K. Zadek. Analysis of pointers and \nstructures, In Pro\u00adceedings of the SIGPLA N 190 Conference on Progmmming Language Design and Implemen\u00adtation, \npages 296 310, 1990. P. Denning, J. Dennis, and J. Qualitz. Ma\u00adchines, Languages, and Computation. Prentice-Hall, \n1978. A. Deutsch. A storeless model of aliasing and its abstractions using finite representations of \nright-regular equivalence relations. In Proceed\u00adings of the IEEE 199.2 International Confer\u00adence on Computer \nLanguages, pages 2 13, April 1992. M. Emami, R. Ghiya, and L. Hendren. Context\u00adsensitive interprocedural \npoints-to anrdysis in the presence of function pointers. In Proceed\u00adings of the ACM SIGPLAN Conference \non Pro\u00adgmmming Language Design and Implementa\u00adtion, June 1994.  Vincent A. Guarna Jr. A technique for \nana\u00adlyzing pointer and structure references in par\u00adallel restructuring compilers. In Proceedings of the \nInternational Conference on Parallel Pro\u00ad cessing, volume 2, pages 212 220, 1988, W. Ludwell Harrison \nIII and Z. Ammarguellat, A program s eye view of Miprac. In U. Baner\u00adjee, D. Gelernter, A. Nicolau, and \nD. Padua, editors, Fifth International Workshop on Lan\u00adguages and Compilers for Parallel Computing, volume \n757 of Lecture Notes in Computer Sci\u00adence, pages 512 537. Springer-Verlag, 1993. W. Ludwell Harrison \nIII. The interprocedu\u00adraJ analysis and automatic parallelization of Scheme programs. Lisp and Symbolic \nCompu\u00adtation, 2(3/4):179-396, 1989, L. Hendren, C. Donawa, M. Emami, G. Gao, Justiani, and B. Sridharan. \nDesigning the Mc-CAT compiler based on a family of structured intermediate e representations. In U. Banerjee, \nD. Gelernter, A. Nicolau, and D. Padua, editors, Fifth International Workshop on Languages and Compilers \nfor Parallel Computing, volume 757 [HHN92] [HHN94] [HN90] [HPR89] [HU79] [Hud86] [ISY88] [JM82] [Ken90] \n[KKK90] [KS93] [Kun86] [Lan92] [LG88] of Lecture Notes in Computer Science, pages 406 420. Springer-Verlag, \n1993. L. Hendren, J. Hummel, and A. Nicolau. Ab\u00adstractions for recursive pointer data structures: Improving \nthe analysis and transformation of imperative programs, In Proceedings o,f the SIG-PLAN 92 Conference \non Programming Lan\u00adguage Design and Implementation, pages 249\u00ad260, June 1992. J. Hummel, L. Hendren, \nand A. Nicolau. A lan\u00adguage for conveying the aliasing properties of dynamic, pointer-based data structures. \nIn Pro\u00adceedings of the 8th International Parallel Pro\u00adcessing Symposium, April 1994, Laurie J, Hendren \nand Alexandru Nicolau. Par\u00adallelizing programs with recursive data struc\u00adtures. IEEE Tkans. on Parallel \nand Distributed Computing, 1(1):3547, January 1990. Susan Horwitz, Phd Pfeiffer, and Thomas Reps. Dependence \nanalysis for pointer variables. In Proceedings of the SIGPLAN 89 Conference on Programming Language Design \nand Implemen\u00adtation, pages 28-40, June 1989. J. Hopcroft and J. Unman. Introduction to Au\u00adtomata Theory, \nLanguages, and Computation. Addison-Wesley, 1979. P. Hudak. A semantic model of reference count\u00ading and \nits abstraction. In Proceedings of the 1986 ACM Conference on LISP and Functional Progmmming, 1986. K. \nInoue, H. Seki, and H. Yagi. Analysis of functional programs to detect run-time garbage cells. ACM TOPLAS, \n10(4):555 578, October 1988. N. D. Jones and S. Muchnick. A flexible ap preach to interprocedural data \nflow analysis and programs with recursive data structures. In 9th ACM Symposium on Principles of Programming \nLanguages, pages 66-74, 1982. K. Kennedy. Foreword of Supercompi/ers for Parallel and Vector Computers, \n1990. The text is written by Hans Zima with Barbara Chap man, available from the ACM Press. David Klappholz, \nApostolos D. Kallis, and Xi\u00adangyun Kang. Refined C: An update, In David Gelernter, Alexandru Nicolau, \nand David Padua, editors, Languages and Compilers for Pamllel Computing, pages 331-357. The MIT Press, \n1990. N. Klarlund and M. Schwartzbach. Graph types. In Proceedings of the ACM 20th Sympo\u00adsium on Principles \nof Programming Languages, pages 196 205, January 1993. K. Kundert. Sparse matrix techniques. In A. Ruehli, \neditor, Circuit Analysis, Simulation and Design, pages 281-324. Elsevier Science Publishers B.V. (North-Holland), \n186. W. Landi. Undecidability of static analysis. ACM Letters on Programming Languages and Systems, 1(4), \nDecember 1992. J. M. Lucaesen and D. K. Gifford. Polymor\u00ad phic effect systems. In Proceedings 15th ACM \nSymposium on Principles of Progmmming Lan\u00adguages, pages 47-57, 1988. [LH88] James R. Larus and Paul \nN, Hilfinger. De\u00adtecting conflicts between structure accesses. In Proceedings of the SIGPLAN 88 Conference \non Programming Language Design and Implemen\u00adtation, pages 21 34, June 1988. [LMSS91] J. Loeliger, R. \nMetzger, M. Seligman, and S. Stroud. Pointer target tracking -an empiri\u00adcal study. In Proceedings of \nSupercomputing 91, pages 14 23, November 1991. [LR92] W. Landi and B. Ryder. A safe approximation algorithm \nfor interprocedural pointer alissing, In Proceedings of the SIGPLAN 92 Conference on Programming Language \nDesign and Imple\u00admentation, pages 235-248, June 1992. [MLR+931 T. Marlowe, W. Landi, B. Ryder, J. Choi. \nM. Burke, and P. Carini. Pointe~-induced ali~ ing: A clarification. ACM SIGPLAN Notices, 28(9):67-70, \nSeptember 1993. [NPD89] A. Neirynck, P. Panangaden, and A. J. De\u00admers. Effect analysis in higher-order \nlanguages. International Journal of Pamllel Programming, 18(1):1-37, 1989. [PCK94] J, Plevyak, A. Chien, \nand V. Karamcheti. Anal\u00adysis of dynamic structures for efficient paral\u00adlel execution. In U. Banerjee, \nD. Gelernter, A. Nicolau, and D. Padua, editors, Sizth .lnter\u00adnational Workshop on Languages and Compil\u00aders \nfor Parallel Computing, volume 768 of Lec\u00adture Notes in Computer Science, pages 37-56. Springer-Verlag, \n1994. [PS85] F. Preparata and M. Shames. Computational Geometry: An Introduction. Springer-Verlag, 1985. \n[PW86] David A. Padua and Michael J. Wolfe. Ad\u00advanced compiler optimization for supercomput\u00aders. Communications \nof the ACM, 29(12), De\u00adcember 1986. [RM88] C. Ruggieri and T. P. Murtagh. Lifetime anal\u00adysis of dynamically \nallocated objects. In Pro\u00adceedings of the 15th ACM Symposium on .Prin\u00adciples of Progmmming Languages, \npages 285 293, 1988. [Sam90] Hanan Samet. Applications of Spatial Data Structures: Computer Graphics, \nImage Procew\u00ading, and GIS. Addison-Wesley, 1990. [Sta80] Thomas A. Standish. Data Structure Tech\u00adniques. \nAddison-Wesley, 1980. [SWG91] J. Singh, W. Weber, and A. Gupta. SPLASH: Stanford parallel applications \nfor shared\u00admemory. Technical Report CSL-TR-91-469, Stanford University, 1991. FTP to mo\u00adjave.st anford.edu. \n[WH92] E. Wang and P. Hilfinger. Analysis of recursive types in LISP-like languages. In Proceedings o.f \nthe 92 ACM Conference on LISP and Func\u00ad tional Programming, pages 216-225, June 1992. [WS92] M. Warren \nand J. Salmon. Astrophysical n\u00adbody simulations using hierarchical tree data structures. In Proceedings \nof Supercomputing 1992, pages 570 576, November 1992. [ZC90] Hans Zima and Barbara Chapman. Supercom\u00adpilers \nfor Pamllel and Vector Computers. ACM Press, 1990. Appendix A: Sparse Matrix Axioms The following twelve \naxioms can be used to accu\u00ad rately describe a sparse matrix. Due to space limita\u00ad tions we merely present \nthe axioms here; for a more detailed discussion see [HHN94]. Note that some ax\u00ad ioms are inferred since \npointer fields of different types should lead to different vertices, We develop the axioms bottom-up, \nstarting from the perspective of the matrix elements and finishing with the root of the sparse matrix. \nFirstly, we convey that the rows and columns form linked-lists, and that from any element of the matrix \nthe next row and column elements are distinct: V p<>q, p.nrowE <> q.nrowE, d p<>% p.ncolE <> q.ncolE, \nV p, p.nrowE <> p.ncolE. Next, we state directly that rows are disjoint, likewise for columns: V p, \np.ncolE <> p.nrowE~ncolE*, V p, p.nrowE* <> p.ncolEfnrowE . We also say that row and column headers \nform linked\u00ad lists: V p<>q, p.nrowH <> q.nrowH, V p<>q, p.ncolH <> q,ncolH. Once again we state the \ndisjointness of the rows (columns), this time from the perspective of the row (column) headers: V p<>q, \np.relern(ncolE)* <> q.relem(ncolE)*, V p<>q, p.celem(nrowE)* <> q.celem(nrowE)*. Since the root vertex \nalways refers to the first row (col\u00adumn) header, we view the root vertex as part of the row (column) \nheader linked-list: V p<>q, prows <> q.nrowH, V p<>q, p.cols <> q.ncolH. Finally, we state that a sparse \nmatrix is acyclic: V p, p.(rowslcolslrelemslcelemslnrowH[ncolHl nrowE[ncolE)+ <> p.c. As an aside, note \nthat since since a sparse matrix has only one root vertex, we can easily state the dis\u00adjointness of sparse \nmatrices: v p<>q, p.(rowslcols)(relems lcelernsl nrowH lncolHlnrowElncolE)* <> q.(rows[cols)(relems lcelems] \nnrowH\\nco/H lnrowE[nco/E)*.  \n\t\t\t", "proc_id": "178243", "abstract": "<p>Optimizing compilers require accurate dependence testing to enable numerous, performance-enhancing transformations. However, data dependence testing is a difficult problem, particularly in the presence of pointers. Though existing approaches work well for pointers to named memory locations (i.e. other variables), they are overly conservative in the case of pointers to unnamed memory locations. The latter occurs in the context of dynamic, pointer-based data structures, used in a variety of applications ranging from system software to computational geometry to N-body and circuit simulations.</p><p>In this paper we present a new technique for performing more accurate data dependence testing in the presence of dynamic, pointer-based data structures. We will demonstrate its effectiveness by breaking false dependences that existing approaches cannot, and provide results which show that removing these dependences enables significant parallelization of a real application.</p>", "authors": [{"name": "Joseph Hummel", "author_profile_id": "81100444459", "affiliation": "U. of California, Irvine", "person_id": "P149055", "email_address": "", "orcid_id": ""}, {"name": "Laurie J. Hendren", "author_profile_id": "81100646110", "affiliation": "McGill University", "person_id": "P169482", "email_address": "", "orcid_id": ""}, {"name": "Alexandru Nicolau", "author_profile_id": "81100139899", "affiliation": "U.of California, Irvine", "person_id": "PP39029364", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/178243.178262", "year": "1994", "article_id": "178262", "conference": "PLDI", "title": "A general data dependence test for dynamic, pointer-based data structures", "url": "http://dl.acm.org/citation.cfm?id=178262"}