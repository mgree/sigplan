{"article_publication_date": "06-01-1994", "fulltext": "\n Parallelizing Complex Scans and Reductions Allan L. Fisher Anwar M. Ghuloum School of Computer Science \nCarnegie Mellon University Pittsburgh PA 15213-3890 AbstractT We present a method for automatically \nextracting parallel prefix pro\u00adgrams from sequential loops, even in the presence of complicated con\u00additional \nstatements. Rather than searching for associative operators in the loop body directly, the method rests \non the observation that func\u00adtional composition itself is associative. Accordingly, we model the loop \nbody as a multivalued function of multiple parameters, and look for a closed-form representation of arbitrary \ncompositions of loop body instances. Careful analysis of conditionals allows this search to succeed in \ncases where existing automatic methods fail. The method has been implemented and used to generate code \nfor the iWarp paral\u00adlel computer. 1 Introduction A wide range of recurrences can be executed in parallel \nefficiently using various parallel prefix and reduction algorithms [4][11 ][14]. For example, the simple \nrecurrence in Figure 1a, where 8 is an associa\u00adtive operator, can be solved using a parallel computation \nof the form in Figure lb. Figure 1c illustrates the more realistic case when r~ > p, where p is the number \nof processors. A general form of such algo\u00adrithms can be described simply. The array or expression being \nreduced is distributed blockwise across the processing elements. Each process\u00ading element performs the \nreduction locally. Then the results for each processing element are combined in pairwise fashion in a \nbinary com\u00adbining tree. In the case of a reduction, the algorithm proceeds no fur\u00adther. The recurrence \nin Figure 1a can be computed by a parallel reduc\u00adtion. Were this recurrence to compute an array rather \nthan a scalar (i.e. a[i] = a [-i-l] @J B [i-l] ;), a parallel prejix sum operation, or a scan, would \nbe required to solve this so that all intermediate val\u00adues are computed. The simple algorithm presented \nabove only per\u00adforms half the necessary work. After the sweep up the combining tree, the partial results \nin the combining tree are propagated back down to f This research is sponsored by the Wright Laboratory, \nAeronautical Systems Center, Air Force Materiel Command, USAF, and the Advanced Research Projects Agency \n(ARPA) under grant numbers F33615-93-1-1330 and F3361 5-90-C-1465. The views and conclusions contained \nin this document are those of the authors and should not be interpreted as necessarily representing the \nofficiat policies or endorsements, either expressed or imptied, of Wright Laboratory or the U.S. Government. \nPermission to copy without fee all or part of this material is granted provided that the copies are not \nmade or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication \nand its date appear, and notice is given that copying is by permission of the Association of Computing \nMachinery. To copy otherwise, or to republish, rer+tires a fee ancf/or specific permission. SIGPLAN 94-6/94 \nOrlando, Florida USA 0 1994 ACM 0-89791 -662-)W410006..$3.5O 135 do i=1, n a = a @ B[i-1] 8 end do \nA (Bo 8 Bi) (B2 @ B3) (a) R )? (b) -/ Bo@...EIBp/I.I Bn,p&#38;. ~B2nIp-1 2n lpm,.aB3n Ip.1 3n lpB..aB4n \n1P-1 [ p.o p=l p.z p=3 (c) Figure 1 An associative recurrence and its parallel combining trees  the \nprocessing elements, and the local partial sums are updated. For n array elements and p processors, [8][12] \nshow that both reduction and parallel prefix operations can be performed on an EREW-PRAM in O(rs/p) time \nsteps if n = Q (plogp) and if the time complexity relationship T(n) = r n/Pi Cl -+ Czlogp, such that \nCl and Cz are both constants, holds. The first product term is the cost of the local computation phase \nof the reduction, and the second prod\u00aduct term is the cost of the combining phase of the reduction. For \nthe general recurrence, the time complexity is rhpnl T ~0~~ (i) is the cost of communicating the intermediate \nresults of the computation (conceptually, up and down the combining tree). If the operator @ were addition, \nT@ = 1 and TCOmm(i) = 1, since at most a single number is communicated at a time during the communication \nphase. The associativity of the addition operator allows the decomposition of multiply composed additions \nfor parallel execution. Some other examples of associative operators are integer multiplication and MAX. \nWhether this computation will adhere to a O(n/p ) time com\u00ad plexity bound depends on the complexity of \neach operator application and the cost of communicating intermediate results. In the cases of integer \nmultiplication and MAX, it is known that these operations sat\u00adisfy the criteria necessary to adhere to \nthis time bound. So our goal is to find efficient associative operators for which we can perform these \nparallel scans and reductions within the aforementioned time bounds. Blelloch has demonstrated that scans \nand reductions are powerful parallel primitives [3]. They can be used to express many useful com\u00adputations, \nin domains as diverse as matrix-vector multiplication, image processing, computational geometry, and \nsorting, as well as a multitude of simpler loop kernels. Many benchmark suites [10][15] include a significant \nnumber of recurrent loop bodies to test the abili\u00adties of compilers and computers. Automatic recognition \nand efficient solution generation for recur\u00adrences from serial code has mostly been limited to finding \na pattern that matches a known recurrences and then using one of a library of fast solutions to solve \nit. These methods are limited by several factors: They are limited by the set of recurrences programmed \ninto the compiler. They have limited ability to solve recurrences involving arbi\u00ad trarily nested conditional \noperators. . They are dependent on the syntactic quality of the source code. Pinter and Pinter s algorithm \n[16] does well at recognizing simple filtering (non-dependent) conditionals. However, they depend on \nthe syntactic quality of the source code rather than the semantic content. They pattern match to find \nthe recurrence operator. Syntactic quality of source code is generally a problem for the pattern matching \nmeth\u00adods employed in many commercial compilers. Several semantic tech\u00adniques [1][13][19] have been proposed \nas a step toward using algebraic properties to simplify complex loop body structures, thus addressing \nthe problem of source code quality. However, they are also limited by pattern matching for recurrence \noperators. Callahan [7] first proposed a similar model to ours for recurrent loops, although he does \nnot pursue aggressive symbolic analysis to automatically derive solu\u00adtions, instead suggesting a pattern \nmatch against a set of core recur\u00adrences. We propose and implement a technique which uses the following \nconcepts and methods to automatically extract parallelized code for recurrences: A model for executing \nrecurrent loop bodies which is associa\u00adtive in all abstractions.  Symbolic substitution of expressions. \n Linear relation feasibility testing to simplify complex condi\u00adtional structures.  Logic minimization \ntechniques to reduce conditional nesting structure.  A specialized unification algorithm which abstracts \nout sym\u00adbolically constant subexpressions.  We distinguish our technique from existing techniques with \nthe fol\u00adlowing: We are able to find solutions for a broad class of recurrences by actually extracting \nan efficient associative operator from the source code. We rely on the analytical abilities of the compiler \nrather than a database of recurrences. . We handle conditional operators embedded within recurrences \nin a general way. . Our model of recurrences is more general than prior approaches. The analysis here \ncan be extended to other forms of recurrence, such as pointer jumping and combining-sends. The rest of \nthis paper is structured as follows. Section 2 gives an overview of our technique. We discuss the models \nused by our method, and then we motivate the method itself by presenting an example. Section 3 gives \ntechnical details of how we model recurrent loops. Section 4 discusses an abstraction technique for extracting \neffi\u00adcient associative operators. Section 5 discusses some of the transfor\u00admations and analysis we employ \nto handle conditional expressions. Section 6 describes one example in detail. Section 7 discusses the \nscope and limitations of our technique. Section 8 discusses the imple\u00admentation and performance of our \ntechnique. Section 9 discusses future work. Section 10 concludes the paper. 2 Overview of our method \nIn our system, recurrent loop bodies are modelled as functions applied to recurrence variables. For example, \nthe loop in Figure 2a can be modelled by a series of functions g, = k. x + B, _ ~ applied to the recurrence \nvariables a [i], as in Figure 2b, We call this function a loop modelling ji.wrction. The same computation \nis achieved by pre\u00adcomputing the composed instances of the functions g, and then apply\u00ading the function \nto the first element in the array a, as in Figure 2c. In this model of the recurrence, we compute final \nvahres of the recur\u00adrence variable by al = G,(aO), where G, = g, * g,_ ~ c . . . gl and * is the function \ncomposition operator over g,. We refer to the func\u00adtions G, as composite ji.mctions. Note that no dependencies \nhinder parrrllelization in the application loop. The original loop is effectively executed when the composition \noperator is applied in a prefix oper\u00adation over the functions g, and then each resulting G, is applied \nto a.. The associativity of the composition operator allows for the applica\u00adtion of a parallel prefix \noperation, as in Figure 2d. The case for a com\u00adposition reduction is analogous, Recall that the time \ncomplexity bound of O(n/p) will hold if n = Q (~hgp) , and ~comm (i) and T, are constant. A naive com\u00adposition \nstrategy will double the complexity of the composed function at each step and yield a result that is \nas costly to apply as a serial exe\u00adcution of the loop. Furthermore, the cost of communication in the \ncombining steps of a parallel prefix operation is proportional to the size of the functions being communicated. \nIf the function size increases, the communication costs will similarly increase. Under these constraints, \nfinding an efficiently composable function entails finding a composition method with a constant time \ncomplexity and a static run-time representation for the modelling function. Requiring that the function \nrepresentation be closed under composition guaran\u00adtees both that the representation scheme will be of \na fixed size and that the composition method will be of constant complexity. For example, the composition \nof two instances of the modelling function for the loop in Figure 2a is g, *g, = Ix. (x+ Bj _ ~) + B, \n_ ~, The composite has doubled in complexity with respect to its two com\u00addoi=l to n ModeUing a[i] = \na[i-1] + B[i.-l] Function end do u (a) do i=l to n a[i] = g,(a[i-11) end do (b) G, =g, do i=2 to n \nG, = G,-l *g, end do do i.=1 to n a[i] = G,(a[O]) end do (c) A (G. Gl) (G2 G3)  /---- q (G. GI) .. \n A/ JJ. G3 Go % G2 , r== m EEZZIG *=0 J)=l p=z p=3 (d) Figure 2 A functional model for recurrent \nloops. ponent functions, requiring two additions where the original function had only one. However, observe \nthat the addition operator in the com\u00adposed result allows thereassociation gi. gj = )w. x+( Bt_l+Bj_ \nl). At each composition, thetwosymbolic constants can be abstracted into one symbolic constant. The resulting \ncomposite function g,og, = Ix. x+C has notincreased in complexity with respect to the original function. \nThe price paid for this efficiency in representa\u00adtion is that of a single addition operator at each dynamic \ncomposition to compute C = Bi_l+B, _I. Note that this transformation effec\u00adtively farms out the work \nto apply the final composite function to the composition method, which will be executed in parallel. \nIn the dynamic composition phase, it is not necessary to communi\u00adcate the entire function representation. \nOnly the abstracted constant C is necessary for the composition method we have derived. In the func\u00adtion \napplication phase, C is used to evaluate the function hx. x + C. Since each composition step takes a \nconstant amount of time and the representation is of a fixed size, the full composition parallel prefix \nor reduction operation will perform within the O(n/p ) time bounds. This example demonstrates that composing \ninstances of the loop modelling functions, simplifying the composite, and abstracting out symbolically \nconstant terms works well. This suggests a structure for Qc1CodeClosureTest Generation Futility/Failure \nFigure 3 Flow diagram for the technique, an iterative method by which we can generally analyze these \nfunc\u00adtions, in Figure 3. The starting point is the modelling function of the loop body. We start by symbolically \ncomposing two instances of the loop modelling function. In the simplification phase, the composite is \nanalyzed by reassociating operators and simplifying complicated con\u00additional nests. In the ternplatization \nphase, we abstract out symboli\u00adcally constant expressions to build the dynamic composition method. In \nthe closure testing phase, the resulting function is compared for iso\u00admorphism with the original functional \nmodel. If the isomorphism holds, the function is closed under the derived composition method. Consequently, \nit will satisfy our complexity constraints. If the isomor\u00adphism does not hold, the composite functions \nare used to repeat the process. If the analyzer is unable to perform further compositions, usually due \nto conditions arising because of the inclusion of condi\u00adtional operators, we exit with a failure condition. \nFutility termination is determined by setting a bound on the number of iterations. 3 Extracting modelling \nfunctions The code in Figure 4a for computing a linear recurrence (inspired by loop 19 of the Livermore \nLoops [10]) is used as an example in this section. The starting point for extracting functional models \nof loops is extracting strongly connected components of the expression level data dependence graph [23] \nfor the loop body, as in Figure 4b (only true dependence are shown). Strongly connected components are \nisolated and analyzed as sepa\u00adrate recurrences. In this example, there exists only one strongly con\u00adnected \ncomponent. The first pass at extracting functional models assigns a recurrence variable to each flow \ndependence in the graph. Destination nodes of both inter-and intra-loop flow dependence are replaced \nby the recurrence variable corresponding to the source of the dependence. The dependence distance determines \nwhich instance of the recurrence variable appears. In this code example, we have the coupled recurrences \na, = E, b,_, and b, = a, C, + D,. The next step eliminates non-loop carried instances of recurrence \nvariables in the expressions. The appropriate recurrence bodies are symbolically substituted for such \nrecurrence variable references within the expression bodies, as in a,= E1 bj. l and bl=(E, b, _l)Ci+ \nD,. The loop-carried recurrence variable instances are assigned bound do i=l, n a[i] = E[i] -b[i-1] \nb[i] = a[i]*C[i] + D[i] end do (a) $kb :x~~+ io ----------\u00ad ai Ct (b) do i=l, n a[i] = E[i] -b[i. -l] \nend do (c) Figure 4 A candtdate forjimction modelling. do i=l, n a[i] = c[i]*a[i. -ll + Dtil*b[i-11 b[i] \n= E[i]*b[i-1] + F[i]*a[i-1] end do Figure 5 A two-way mutual recurrence variable names (function parameters). \nThe functions are then con\u00adstructed by substituting recurrence variable references with the bound variables: \nf., = k(z Y). ~l-Yj fb, = ~(x>Y). (~i-y)cl+Dt. It is obvious in this case that this set of recurrences \nis not mutually recurrent. Only one of the functions need be analyzed and the original forms of the other \nfunctions can beusedto compute therest of the recurrence variables values. The absence of atrue mutual \nrecurrence between the functions or subset of functions implies that intra-itera\u00adtion flow dependence \nwere intrinsic parts of the strongly connected component. These strongly connected components would be \nbroken by the symbolic substitution of intra-iteration recurrence variable ref\u00aderences. Forexample, f~ \n= Lx. (-E, -x) C,+D, Canbeusedto &#38;n\u00aderate parallel prefix code to compute b[i]. The loop in Figure4c \nis generated tocompute a[i] andistrivially parallelizable. If the statements were truly mutually recurrent, \nthe functions are combined by forming a function which acts on atuple. For example, for the two variable \nlinear recurrence in Figure 5, the modelling func\u00adtion is f [a,,~,, = l(x, y). (cLx+D*y, E,x+Fiy). We \nhandle rrth order recurrences by transforming them into mutual recurrences, as in Figure 6. 4 Templatization \nIn the example presented in section 2, we found that, by abstracting out symbolically constant subexpressions, \nwe were able to trade off composition time complexity for composite function complexity. In the addition \nreduction example, using an addition at composition time a[i] = f(a[i-1], a[i-21, . . . . a[i-nl) -+ \na[i] = f(a[i-l]r tmpl, trnp2, . . . . tmp(n-1)) tmp(n-1) = tmp(n-2) ... tmp2 = tmpl tmpl = a[i-11 Figure \n6 Transforming nth order recurrences to n-wise mutual recurrences. doi=l, n a[i] = D[i.]*a[i-1] + B[i] \nend do Figure 7 A simplejrst order linear recurrence. fixed the complexity of the composite function. \nThe reason why this trade-off is so important is that we are transferring complexity from an inherently \nserial portion toaparallel portion of the computation. The serial portion is that ofevah.rating each \nfunction. The final composite function s computational complexity is a lower bound on the time complexity \nof its evaluation in the application loop. If some of the computational load is taken up by the composition \nmethod, which is applied in parallel, we have effectively converted this time complexity in the form \nof a serial work load into parallel work complexity. We achieve this by building thecomposition operator \nas we abstract out symbolically constant terms. We call this process of abstraction tem\u00adplatization. \nConsider therecurrence in Figure7, whose modelling function is h, = kx. D1x+B,. Thecomposition oftwoabstract \ninstances of h in this initial representation results in: h,. h, = kX. D1(D,X+B,)+B, = kX. DiDjX+D,B, \n+Bi = kx. (Clx+CJ, where C1 = DiD, and Cz = D, B,+ B,. No further composition is required as the function \nclass is closed under composition. From this, the composition function is directly inferred asthe two \noperations Cl = D,D, and Cz = D, B,+ B,. We call the functional composite with symbolic constants abstracted \nout a template finction. We call the symbolic constants which must be computed dynamically template variables. \nTemplate Function: Atemplate function is afunction with symbolically constant subexpressions abstracted \nout. Template Variables: Template variables are the variables used to replace symboli\u00ad cally constant \nsubexpressions which have been abstracted out. Here, by a simple distribution and an abstraction of coefficients \nin this polynomial, we have managed to reduce the complexity of the composite function. We have mitigated \nthe complexity of the compos\u00ad ite by computing portions of it in the composition operator, rather than \ndeferring evaluation until function application time. We can construct a method by which any expression \nwhich is polynomial in the bound variables of a function can be templatized by simply abstracting out \nsymbolically constant coefficient terms as template variables. The polynomial expression level templatizing \nalgorithm expects that its input is processed so that all multiplications are distributed through additive \nterms to create a sum of product terms. With this doi=l, n if (B[i] > max) then mex = B[i] endi f TNT \nBi Bj enddo x x (a) B,> B, Bl>x B,>x false false false x fatse false true B, fatse true false B, false \ntrue trne B, true false false x true false tme B, true true false B, true true true B, (b) h*&#38;:Bi>Bj \n  ==El%:: : ? (c) Figure 8 The rnoximum reduction problem. resulting polynomial, the templatizer \ngathers the coefficients of the bound variables of the function, and then abstracts these coefficients \nout as template variables, eliminating redundant or equivalent tem\u00adplate variables as necessary. Conditional \nstructure is more difficult to simplify through algebraic transformation than simple polynomial expressions. \nFor example, we apply a similar line of analysis to a recurrence with embedded condi\u00adtionals. For the \nloop body of Figure 8a, the loop modelling function is ~ = Lx. ( (Bl > x) ?B,: x) . Symbolically composing \ninstances of ~ results in: X*$ = ~. ((Bl> (( BJ>x) ?Bj:x))?Bl: (( B)>x)?Bj:x)) Switching function representations \nsimplify the analysis of condi\u00adtional nests, as they facilitate the application of standard logic minimi\u00adzation \nmethods. The switching function representation of the composite is shown in Figure 8b. The predicate \nB,> B, is symbolically constant. We call symboli\u00adcally constant predicates template predicates. Template \nPredicate: A template predicate is a relational expression in which all embedded terms are symbolically \nconstant. The template predicate is abstracted out and evaluated during each t Modelling functions are \nextended to include a conditional operator and rela\u00adtionrd operators. We use the conditional notation \n(predicate? truevah falseval) for conciseness. 1C= Bi<Bj:Bi:Bj I C<x I 7T xc Figure 9 Unlfiing subfunctions \nwith a template predicate. m I, 1 Bi >Bj Bi>x Bi B.>x Bi B>x (1 d J Bjx Bjx Figure 10 The composite maximum \n~nction in CNF-Exp form dynamic composition, yielding the two switching functions in Figure 8c. Note \nthat certain table entries here require logical contradictions between the predicates to be selected. \nThese entries are replaced with NP in the entries above. Applying the logic minimization method we discuss \nin section 5 will yield: Lx. (Bi > x) ?B,:x if (B, > Bj) ~ ~ = Ix. (BJ>x) ?Bj:x if (B, SBj) () While \nthese two functions seem to indicate that the original modelling function was closed under composition, \nsomehow they must be uni\u00adfied into one representation while retaining this desirable property. Note the \nsimilarity in the structure of the two simplified functions. They are nearly identical, except where \nsymbolic constants occur. We call this condition structural isomorphism. Structural Isomorphisrn Two \nexpressions are structurally isomorphic if, when abstract\u00ad ing out symbolically constant subexpressions, \nthere exists some ordering of the predicates in the conditional nest for which the CNF-Exp representing \nthem is isomorphic. Figure 9 graphically depicts these two functions. We can unify these two functions \nby creating an isomorphism mapping between them. The composite functions in this example are unified \nby creating the mapping (( B,, B,)). The template variables are selected from the mapping(s) based on \nthe template predicate: ~ ~ = lx. (C>x) ?C:x, where C = (Bi >Bj) ?Bi:Bj, as Figure9 illustrates. This \ntemplate function is structurally isomorphic to the original looping modelling function, confirming closure \nunder the newly constructed composition operator. At the expense of one comparison operator at each dynamic \ncom\u00adposition, we have guaranteed that the resulting composite function does not increase in complexity. \nThe effective run-time representation of intermediate composite functions is the template variable C, \nwhich we use to evaluate the template function in the application phase. Thus, the solution conforms \nto the O(rz/p ) time bounds. While analyzing this problem, we took advantage of the fact that predicates \nwith template predicates may be evaluated at composition time. Other than the conditional optimization \nwe discuss in section 5, the only way to reduce the complexity of composed conditional nests is to abstract \nout template predicates and add them to the composition operator. However, when template predicates are \nabstracted out, mul\u00adtiple versions of the function are generated for each combination of logical values \nof the template predicates. Unifying these subfunctions to reduce the complexity of such a composite \nimplies finding some structural correlation between them. An obvious way to find a correla\u00adtion is to \ncheck for structural isomorphism in the conditional nest structure. If the isomorphism exists, we find \npolynomial expression templatizations in the leaves of the conditional nest for each subfunc\u00adtion and \nthen pick among them by evaluating template predicates at composition time. It is by this unification \nmechanism that we recon\u00adcile the simple polynomial expression templatization algorithm with template \npredicate extraction in conditional nests. To facilitate the transformations and traversals necessary \nfor this algorithm, we convert conditional nests to a normal form, called Con\u00additional Normal Form Expression \n(CNF-Exp). Condition Normal Form Expression A CNF-Exp is defined to be conditional nest in which all \ncon\u00ad ditional expressions are distributed out of arithmetic operators, relational operators, and the \npredicates of conditional opera\u00ad tors. Figure 10 illustrates a CNF-Exp form for the first composite of \nthe maximum reduction example in Figure 9. We normalize functions into CNF-Exp form after logic minimization \nand template predicate abstraction. This allows for easy structure traversal when checking for isomorphism. \nIf an isomorphism mapping cannot be found to unify subfunctions, then the analysis terminates with failure. \nDetection of structural iso\u00admorphism and extraction of mappings is a potentially costly opera\u00adtion, given \nthat the number of possible condition orderings in a condi\u00adtional nest are exponential. However, in the \nminimization process we use some heuristics to order the conditions so that we can apply a sim\u00adple linear \nwalk over the expressions to construct the mapping and detect structural isomorphism. The ordering heuristics \nchecks only that the conditions of the same rank in the conditional nests of sub\u00adfunctions are themselves \nstructurally isomorphic to each other. The algorithm for templatization is described in high level applicative \nstyle in Figure 11. The algorithm assumes that its inputs are a set of func\u00adtions in CNF-Exp form and \npreprocessed for conditional ordering. There is a complication that the maximum reduction example does \nnot expose. The simplified composite function Ax. (x+ B,+ BZ<O)?O:x+B, +B, if B,<O f, ~ = kx. (x+ Bj<O)?Bi:x+Bj+Biif \nBi20 [) is from the example in section 6. A naive templatization from the mapping { (B, +B,, BJ) , (O, \nB,) , (B, +Bl, B, + B,) ) might produce three template variables for the template function function SimpleTemplatize(SoPs, \nTempVarSet) ;; collect polynomial coefficients foreach (ProdTenn in SOPS) Coeff = GetCoefficients(ProdTerm) \n;; check for redundancy if (not (newTempVar = find(Coeff, TempVarSet))) then newTempVar. GenTempVaro \nend if add(GenTempVaro, Coeff, TempVarSet) replace(Coeff, newTempVar, SOP) end foreach return TempVarSet \nend function Templatize(CNF-Exprs, CurrTemp) if (PolyExprs(CIW-Exprs)) then ;; polynomial leaf in all \nsubfunctions if (not (SimpleExprIsomorph(CNF-Exprs)) then ;; no isomorphism between subfunctions exit(Failure) \nend if return SimpleTemplatize(CNF-Exprs, CurrTemp) else if (CondExprs(CNF-Exprs)) then ;; conditional \nnode in all subfunctions return Templatize(map(RightExpr, CNF-Exprs), Templatize(map(LeftExpr, CNF-Exprs), \nTemplatize(map(PredExpr,CNF-Exprs ), CurrTemp))) else ;; no isomorphism between subfunctions exit(Failure) \nend if end Figure 11 The templatization algorithm for ordered, normalized subfunctions. kx. (X+ CI)?C2:X+C3, \nwhere Cl = (Bi<O)? (B, +B,):B,, C2 = (B, <O)? O: B,, and C3 = B,+ B,. However, the relationship Cl+ \nC2 = C3 suggests an alternative abstraction Lx. (x+ Cl) ? Cz: x + Cl + Cz, with the same values for Cl \nand Cz. This quantitative relationship might be important in sub\u00adsequent compositions of the composite \nfunction. We avoid this over\u00adabstraction by eliminating those template variables which can be expressed \nas simple linear combinations of other template variables. 5 Simplification of conditional operators \nSimplification of conditional operators is essential for dealing with recurrences containing conditional \ncontrol structures. The basic goal of the conditional analysis phase is to find and eliminate logical \ndis\u00ad crepancies and redundancy within an expression. The form of simplification shown in Figure 12a involves \nredun\u00addancy; if a conditional node has identical children, the conditional node can be eliminated. Another \ninvolves a discrepancy in which identical predicates are in an ancestor/descendant relationship within \n ?! tracing paths from the root to the leaves in the conditional nest, form\u00ad red ing table entries. Note \nthat we do not build an exhaustive switching function representation, though, for conceptual purposes, \nthey are rep\u00adresented as such in this paper. Building exhaustive switching func\u00ad b tions would be both \nexponential in time and space complexity. Simple conditional contradictions of the form in Figure 12a \nand b are elimi\u00ad predl predl .. .. FF \\ predl br1 br2 ab i-ii (b) a >b ... d 73 b>c d ... h CM c ab \n /!?3 (c) Figure 12 Simplijcations of conditional nests.  an expression, as in Figure 12b. The first \nexpression represents such a situation. As the conditional node with the identical predicate is a descendant \nof the false branch of the root condition node, the true branch of that node will never be evaluated, \nallowing the elimination of that branch. In other words, the conditional subnode becomes unnecessary \nas its evaluation is predetermined by its status as a descendant of a conditional node with an identical \npredicate, The sec\u00adond expression is the result of this simplification. Other logical discrepancies may \nrequire complex symbolic reason\u00ading to deduce relationships between dissimilar predicates; for exam\u00adple, \nthe predicates u <b, b <c, and c < a can never all evaluate as true, allowing the simplification in Figure \n12c. The module which uncovers logical discrepancies between the potential logical evalua\u00adtions of the \npredicates in the conditional nest can be structured in sev\u00aderal different ways. As the analyses involve \nlinear relational expressions, linear or integer programming formulations of the prob\u00adlem are options. \nHowever, the complexities of such algorithms (inte\u00adger linear programming is NP-complete) make other \napproaches worth considering. We resort to a pruning, heuristic search of the exhaustive set of subsets \nof the predicates in a conditional nest. In spirit, it is an optimized version of the Fourier-Motzkin \nmethod for deciding linear inequalities [9]. Other alternatives include fast meth\u00adods by Shostak for \nhandling special forms of linear inequalities [20]. Optimizations of conditionals are based on the switching \nfunction representation introduced earlier. The switching function is created by nated in the tabulation \nprocess. Logical inconsistencies are eliminated by striking lines from the switching function. Template \npredicate extraction is trivially achieved by generating multiple sub-tables of the original switching \nfunction for each possible evaluation of the template predicate. Conditional redundancies are eliminated \nby apply\u00ading multilevel logic minimization to the representative switching function. We base this step \non the standard logic minimization pack\u00adage, Espresso [6]. Consider the composite function (in CNF-Exp \nform) from the example in Figure 8: fi ~ = AX. ((( B,> X)? (Bi>Bj): (Bi>x))?Bj: (( Bj>x)?Bj:x)) This \nfinction was first converted to the switching function form, as demonstrated in section 2. Note that \nwhile the first occurrence of the predicate 1?,> x seems to indicate that there may be a circumstance \nunder which this predicate is true and the value x would be returned by the function, the second occurrence \npredicate guarantees that this will never happen. The switching function representation indicates this. \nThis is an example of the tabulation eliminating a simple condi\u00adtional contradiction. The subfimction \nswitching functions generated by removing the constant conditional B,> Bj are trivially derived from \nthe switching function. The conditional contradictions arising from conflicting predicates are easily \nstruck from these switching functions. Finally, the redundant conditional nodes were removed by the logic \nminimization procedure. 6 Example: maximum subsequence sum This code in Figure 13a computes the largest \nnon-negative contiguous subsequence sum of a series of real numbers. The second statement in the loop \nbody is simply an articulated maximum reduction. For the purposes of this demonstration, the variable \nso f ar is promoted to a vector so that the loop may be split between the two conditional state\u00adments. \nThe loop modelling function is fi = b. (x+ B, < O) ? O: x + B,. The first composition results in the \nfollowing function: ~ .fj = Ax. (((x+ Bj<o)?o:x+B,) +Bi<o)? o: ((x+ B,<o)?o:x+i3j) +Bi. The switching \nfunction representation of this function is shown in Figure 13b. Extracting the template predicate B,< \nO gives the switch\u00ading fimctions in Figure 13c. The three predicates (Bi <0, x + B, <0, x + Bj + Bi < \nO) contradict with the logical evaluations (true, true, false) and (false, false, true). These cases \nwere struck from the switch\u00ading functions. Logic minimization gives the following functions: kx. (x+ \nB,+ B,< O)? O:x+B, +B, if Bl<O fi $=  Ax, (x+ Bj<O)?B;:x+Bj+Biif Bi~O . The isomorphism mapping for \nthese two functions is { (B, + Bi, B,) , [) (O, Bi) , (Bj + Bi, Bj + Bi) }. There are two alternatives \nin templatiza\u00adtion. Over-abstracting would eliminate the quantitative relationship doi=l, n if (sofar \n+ B[i] < O) then sofar = O else sofar = sofar + B[i] end if if (Iuax < sofar) then max = sofar end if \nend do (a) B,<O I x+B,<O IX+BJ+B,<O I fafse false fafse x+B ,+B, fafse false tme o false true fafse \nB, fafse true tme B; true fafse false x+B,+B, true false true o true true fafse o true true true o (b) \nx+B,<O x+B,+B,<O false false x+B ,+Bi fafse true o true fafse NP true tme o x+ Bj<O x+ Bj+B, <O fafse \nfake x+B ,i-Bl fafse tme NP true false B, true true Bj (c) Figure 13 The maximum subsequence problem. \n between the first two pairs and the third, namely, that the third pair is the sum of the first two. \nThe resulting templatized composition is: (J*J) = (X+ C1<O)?C2:X+C1+C2, where Cl = (Bt< O) ?Bj+Bi:B, \nand Cz = (Bi<O) ?O:Bi. As this is not structurally isomorphic to the original function, we must recompose \nwith the new template function ~ = Lx. (X+ C,, <O)? C2,:X+C,, +C21. The templatized decomposition of \nthis function will reveal that this template function is closed under composition, yielding the following \nscheme for combining template variables: 7 Scope The framework presented here is general, however, the \nspecific tech\u00adniques are tune to a particular class of functions. The function class F is defined simply \nas the class of n-dimensional piecewise linear func\u00adtions with symbolic constants. We define piecewise \nlinear functions to be linear expressions embedded within relational expressions and con\u00additional operators. \nThis technique can analyze the class of polyregion-wise polynomial functions. Polyregion-wise polynomial \nfunctions are the complete class of functions defined over multiplication, addition (aud subtrac\u00adtion), \nand conditional and relational operators. However, the underly\u00ading abstraction mechanism and analytical \ntechniques are suited more toward piecewise linear functions. Note that we do not guarantee finding solutions \nif they exist. Some of the techniques presented here are necessarily heuristic in nature. Implementing \na full search through the space of functions is intracta\u00adble, both theoretically and practically, as \nsome of the subproblems presented here are exponential in nature, such as deciding linear equalities \nand logic minimization. From a compiler perspective, trad\u00ading off some thoroughness for speed is necessary. \nHowever, standard techniques for dealing with these problems efficiently have been uti\u00adlized in other \ndomains. In the case of logic minimization, work in VLSI design has been a particularly rich source of \nefficient algorithms [6]. The problem of deciding linear inequalities has occurred else\u00adwhere in compiler \nwork [17] [22], as well as a multitude of other research domains. 8 Implementation and performance The \ntechnique presented here has been implemented and used to gen\u00aderate code for the iWarp parallel computer. \nThe iWarp is a 8x8 grid of LIW computation cells with tightly integrated, high-speed communi\u00adcation paths \n[5]. We integrated the technique into the Fx compiler [21], which compiles both sequential Fortran and \na set of extensions similar to those offered in Fortran 90 and HPF. Because of the constraints we set \non our analysis, the overall time complexity of each generated scan and reduction is guaranteed to adhere \nto the O (n/p) bound. The important thing to consider is the overhead of these operations when executed \nas function scans and reductions, as opposed to explicitly parallel encodings of the opera\u00adtion. The \noverhead for each recurrence solution generated by our com\u00adpiler is accrued in both the space due to \nallocation of template variables and the time to initialize and apply the final composite func\u00adtion(s). \nWe will generally treat overhead as those computation or space requirements that might not otherwise \nbe used by an explicitly parallel encoding of the recurrence. The template for the code generated by \nthe compiler is shown in Figure 14. For a recurrent loop of length n, the space used by the k template \nvariables is simply kn. Compared to explicit encodings of the recurrences, the space overhead amounts \nto one of these template arrays. For example, if we do template variable computation in place , this \nonly saves space for one template variable. If the reduc\u00adtion or scan intrinsically requires multiple \nvalues be computed in the combining tree, the number of template variables will only reflect this. Step \n1: Initialize Template Variables Step 2: Compot3e Functions By Computing Template Variables and Template \nPredicates Step 3: Apply Function using Template Variables Figure 14 Steps for computing,~nctionally \nmodelled recurrences. Sum Reduction --64 Processors 5- Serial Exwmon -e-Automat! cally Parallelized -+--Intmwc \nFunction .n. \u00ad 4- A Figure 15 Timings oj automatically extracted, intrinsic, and serial sum reduction. \n A direct encoding of the scan will not necessarily improve the mem\u00adory requirements of the operation. \nFor example, consider the linear recurrence of Figure 7. We derive a method which uses two template variables \nin computing the composition. Note, however, that a first order linear recurrence generally requires \ntwo values to be propagated in an explicit parallel prefix or reduction implementation. We can reuse \nthe space for the value being computed, the recurrence variable, in the case of a scan operation. Thus, \nwe save at most one memory slot over the templatized version. So the space overhead is typically n for \neach reduction or scan of length n generated. We may amortize this overhead between scans and reductions \nby reuse of template variable space. The first component of the computational overhead for a reduction \noperation is that used to initialize template variables with array values prior to composition. This \nis comprised of k array assignments. Any computational complexity in computing the initial template values \nwhich are assigned is not overhead specific to this method, since an explicitly parallel encoding of \nthe reduction or scan would require the same computation to take place at some point. Since we only compute \ntemplate variables in the composition phase, we do not accrue any additional computational overhead over \nan explicitly parallel encod\u00ading. The function application step is the final overhead that can be attributed \nto the model. For a scan, this is an array wide application of the functions to the initial value of \nthe recurrence variable. A single application of the final function to the initial value suffices for \nreduc\u00adtions. So the time overhead for both types of function application is the same. Timings from an \nautomatically extracted addition reduction (referred to as Sum) on 64 processors are shown in Figure \n15, along Speedup -128k elements 20, ,+ ,.., 18 \u00ad,.\u00ad ,,, ,,., 16 \u00ad ,., ,,, Sum +--,.,, 14 -,.,, Max -+--MaxSub \n+ ,,, ,,, ,,. 12 \u00ad.,, ,,., 10-,/ ,., ,,, 8\u00ad ,,, ,., ,.\u00ad 6 -,,, ,.., ,,* 4 - ,/ / 2 - ,,.+ +, o! - I \no 10 20 30 40 50 60 70 Num Procs Figure16 Speedup for various automatically parallelized recurrences. \nH Vectorks o Noneorl?artki For@DM VsaOnzation Composition 311 311 3!3 3!4 315 316 317 318 319 3110311131123113321 \n322 33.3 3?1 332 Argonne hmp Number Figure 17 Comparative compiler performance on Argonne loops with \nthe performance of a serial implementation and the performance of the intrinsic. Despite the additional \noverhead of a function applica\u00adtion step and broadcast, the slopes of the automatically extracted code \nand the intrinsic addition reduction operation are fairly close. The rel\u00adative speedup efficiency of \nthe automatically generated code (com\u00adputed as automatic speeduplintrinsic speedup) for 64 processors \nconverges to over 75?Z0.Figure 16 displays speedup vs. number of pro\u00adcessors for the sum, maximum, and \nmaximum subsequence reduc\u00adtions. As expected, the speedup scales linearly with the number of processors. \nWe have tested our compiler on the Argonne [15] loop suite s reductions and scans, To test the robustness \nof the underlying detec\u00adtion technique, we have also compiled a set of the Argonne loops with a slight \nperturbation. The variation we chose was to add another vari\u00adable to carry values across loop iteration, \ncreating a breakable (cou\u00adpled) recurrence. We have drawn a comparative study between our compiler, the \nCray Research Fortran Compiler (CF77) and the Applied Parallel Research Forge/DM Fortran compiler [2] \nfor the Argonne loops and the varied loops in Figure 17. Our compiler com\u00adpares favorably to both compilers \nwith the basic set of Argonne loops. The one case in which we do not parallelize where CF77 does (Loop \n332) involves a loop exit statement. This is not a limitation of the underlying model and analysis of \nour technique. Rather, it is a limita\u00adtion of the compiler front end and code generator. Note that the \ndispar\u00adity in performance with the varied loop set is even more significant. This would confirm the \nreliance of the other compilers on the syntac\u00adtic quality of the source code. Our technique s performance \nis undis\u00adturbed by the variation. The speed of the compiler is an important consideration, especially \nin light of our reliance on approximations to exponential algorithms. The performance of the analyzer \nwas measured for examples pre\u00adsented this paper. The fastest time was 4.7 seconds, and the slowest 11.7 \nseconds, with an average time of 7.6 seconds. There is significant room for improvement in these numbers, \nas our significant portions of our implementation have not been optimized for speed. 9 Future work Many \nof the extensions discussed in this section involve compiler front end work. The analyzer module works \non some of the examples discussed in this section when they areconverted to functional form by hand and \nfed to the compiler, but are otherwise bound by the con\u00adstraints of the compiler front end. The technique, \nas presented here, works when applied to inner loops. Generalizing to regular multidimensional loops \nsimply requires a separate analysis for each dimension of the recursion. The more complicated case is \nthat for irregular nested loop. Segmented scans and reductions [3] are usually to be found in such loops. \nWe are cur\u00adrently developing techniques by which control flow constraints in such loops may be manipulated \nso that this technique can be applied and segmented scans may be extracted. The analysis as presented \nhere, can be directly applied to such loops, once transformed, Currently the compiler front end cannot \nhandle packing operations of the type in Figure 18a. This M a limitation of the compiler front end, We \nwill be implementing a strategy to deal with packing operations through the use of scalar promotion and \nloop fission, as in Figure 18b, The first loop can currently be parallelized using our technique, while \nthe second loop can be recognized as a gather and/or permute opera\u00adtion, An advantage of this approach \nis that it analyzes the computational portion of a recurrence without regard to the underlying data structure \ntraversal mechanism. One can view the problem of finding an associa\u00adtive operator for efficient parallel \ncombination and the problem of finding a communication program by which the recurrence can be implemented \nas independent problems. For example, general combin\u00ading-send operations are random permutes with combining \noperators to resolve write conflicts ([25] discusses these operations in the context of an explicitly \nparallel language construct). For example, consider the loop in Figure 18c. This loop computes a weighted \nhistogram using the array b as the histogram key and the array c as the weight vector. Because of the \nrandom writes in this loop, we have to assume that the entire array is modified at each iteration. Typically, \nthis would preclude automatic parallelization, however, this can be viewed as a slightly more complicated \nrecurrence operating on an entire array. A modelling function for this loop can be constructed and used \nto derive an associative combining operator to be used to perform the combin\u00ading step for write conflicts \nin parallel. Consider the case of a pointer jumping loop written in C in Figure 18d. The modelling function \nis ~P = Lx, (p-> data > x) ?p-> data: x. The only part of this abstract representation dependent on the \ntraversal doi=l, n if (ConditionEx,pr[i] ) then Dest [j++] = Source [i] j =j+l end if end do (a) doi=l, \nn if (Conditionj?xp r[i]) then j [i] = j[i-1] + 1 else j[i]= j[i-1] end if end do do i =1ton if (ConditionExpr[i]) \nthen Dest[j [i]] = Source[i] end if end do (b) do i =I,n a[b[i]] = a[b[i]] + e[i] end do (c) for (p = \nlist; P != NULL; p = p->next){ if (p->data > MAX) MAX = p->data; } (d) Figure18 Packing, histogram, \nand pointer jumping loops. pattern is the form of the symbolic constants, which will be abstracted out \nin the first pass of the analysis. The result of the abstract composi\u00adtion analysis may then be inserted \nin a program for executing a pointer jumping list-prefix algorithm [18][24]. Callahan s suggestions for \ntechniques to combine so-called bounded recurrences [7] might be useful for amortizing some of the communication \noverhead of scans and reductions, Since he uses a similar model, the mapping from our extracted operators \nand template functions to his methods for combining recurrences is direct and sim\u00adple. The analyzer can \nbe extended to handle additional operators, such as division and exponents. The back end currently generates \nthe scans andreductions directly from the composition methods derived by the analysis. However, it would \nbe beneficial to recognize certain special circumstances and generate alternate code. For example, we \ndo want to perform scans or reductions to compute loop induction variables. Rather, expressions in the \nloop indices should be substituted for index expressions involving the induction variables. Once we derive \nand simplify the functional representation of the induction variable com\u00adputation, performing such an \noperation would be simple. 10 Coidwion The observations that function composition is associative and \nthat recurrent loop bodies can be modelled as function applications lead us to a framework for extracting \nreduction and parallel prefix operations [6] Robert K. Brayton. Logic minimization algorithms for in \na general and flexible way. In addition to parallelizing recurrences VLSI synthesis. Kluwer Academic \nPublishers, Boston, which have defeated most, if not all, existing compilers, including the 1984. Maximum \nSubsequence problem and the MAXLOC and MINLOC [7] David Callahan. Recognizing and parallelizing bounded \nproblems without use of parasitic variables (i.e. Loop 24 of the lLiver\u00ad recurrences. In Proceedings \nof the Fourth Workshop on Languages and Compilers for Parallel Processing, Santa more loops [10]), for \nexisting automatic techniques. Moreover, our Clara, CA 1992. method also handles simple recurrences seamlessly, \nsubsuming the [8] Shyh-Ching Chen and David J. Kuck. Time and parallel capabilities of existing techniques, \nsuch as filtered operations and sim\u00ad processor bounds for linear recurrence systems. IEEE ple linear \nrecurrences. The technique we propose is not specific to any Transactions on Computers, C-24(7), July \n1975. particular parallel prefix implementation or system architecture for [9] R. J. Duffin. On Fourier \ns analysis of linear inequality performing prefix operations. systems. Mathematical Programming Study \nno. 1, North- Holland, 1974. This technique is not dependent on pattern matching a set of core [10] John \nT. Fee. An analysis of the computational and paral\u00ad recurrences. Rather, itusesa bootstrap approach to \nextract associative lel complexity of the livermore loops. Parallel Comput\u00ad operators, relying only on \nsome basic underlying analytical capabili\u00ad ing, 7:163-185, 1988. ties. As the underlying analytical engine \nevolves, the class of extract\u00ad [11] W. Daniel Hillis and Guy L. Steele Jr. Data parallel algo\u00ad able associative \noperators will evolve. rithms. Communications of the ACM, 29(12), December 1986. Despite some additional \noverhead in performing composition scans 12] Joseph Jaja. An Introduction to Parallel Algorithms. Add\u00ad \nand reductions, wehavefound performance close to that of the avail\u00ad ison Wesley, 1992. able intrinsic \noperations. We have also found the performance of our 13] Pierre Jouvelot and Babak Dehbonei. A Unified \nSemantic system to achieve reasonable relative speedup efficiencies and desir- Approach for the Vectorization \nand Parallelization of able scaling properties. Our compiler performs favorably on a bench- Generalized \nReductions. In ACM SIGARCH Interna\u00ad mark suite and is resilient to syntactic perturbations. tional Conference \non Supercomputing, Crete, 1989. We apply heuristic methods to reduce the complexity of our tech\u00ad 14] \nPeter M. Kogge and Harold S. Stone. A parallel algo\u00adrithm for the efficient solution of a general class \nof recur\u00ad nique without compromising the ability to recognize any paralleliz\u00ad rence equations. IEEE Transactions \non Computers, C\u00ad able recurrences wehaveseen to date. Inthelogic minimizer and the 22(8):786-793, August \n1973. predicate analysis, we rely on well-known and proven fast methods. 15] David Levine, David Callahan, \nand Jack Dongarra. A We are currently exploring the possibility that extracting arbitrarily Comparative \nStudy of Automatic Vectorizing Compilers. Mathematics and Computer Science Division, Argonne complex \nparallel prefix operations from this general model may be National Laboratory Technical Report MCS-P218-0391. \nuseful in parallelizing programs which have been traditionally diffi\u00ad 16] Shlomit S. Pinter and Ron Y. \nPinter. Program optimiza\u00ad cult to parallelize efficiently. Among the algorithms in which we see tion \nand parallelization using idioms. In Conference some promise are sorting algorithms, combining-sends, \nsparse matrix Record of the Eighteenth Annual ACM SIGACT-SIG\u00ad computations, andpointer jumping algorithms. \nWe believe that, since our analysis in independent of the underlying model abstraction method, our technique \nwill be more generally applicable than existing 17] PI.AN Symposium on Principles of Programming Lun \n-guages, pages 79-92, Orlando, FL, January 1991. William Pugh. The Omega test: a fast and practical inte\u00adger \nprogramming algorithm for dependence analysis. methods. University of Maryland at College Park, Computer \nSci\u00ad ence Technical Report CS-TR-2648, 1991. 11 References [18] M. Reid-Miller and G. E. Blelloch. List \nranking and list scan on the Cray C-90. Carnegie Mellon University, [1] Z. Ammarguellat and W.L. Harrison \n111. Automatic rec- School of Computer Science Technical Report CMU-CS\u00ad ognition of induction variables \nand recurrence relations 94-101. by abstract interpretation. In Proceedings of Sigplarr [19] X. Redon \nand P. Feautrier. Detection of recurrences in 1990, Yorktown Heights, NY, 1990. sequential programs with \nloops. In PARLE 93, Munich, [2] Applied Parallel Research, Inc. Forge Magic/DM User k Germany, pages \n132-145, June 1993. Guide. Version 1.0, 1993. [20] Robert Shostak. Deciding linear inequalities by comput\u00ad \n[3] Guy E. Blelloch. Scans as primitive parallel operations. IEEE Transactions on Computers, C-38(1 1): \n1526-1538, ing loop residues. Journal of the Association for Comput\u00ading Machinety, Vol. 28, No. 4, October \n1981, pp.769-779. November 1989. [21] Jaspal Subhlok, James M. Stichnoth, David R. O Hal\u00ad [4] Guy E. \nBlelloch, Siddhartha Chatterjee, and Marco Zagha. Solving linear recurrences with loop raking. In laron, \nand Thomas Gross. Exploiting task and data paral\u00adlelism on a multicomputer. In Proceedings of The Fourth \nProceedings Sixth International Parallel Processing Sym\u00adposium, March, 1992. ACM SIGPLAN Symposium on \nPrinciples &#38; Practice of Parallel Programming, May 1993, San Diego, CA. [5] S. Borkar, R. Cohn, G. \nCox, S. Gleason, T. Gross, H.T. Kung, M. Lam, B. Moore, C. Peterson, J. Pieper, L. Rankin, P.S. Tseng, \nJ. Sutton, J. Urbanski, and J. Webb. iWarp: An integrated solution to high-speed parallel com\u00ad [22] R. \nTriolet, F. Irigoin, and P. Feautrier. Direct paralleliza\u00adtion of call statements. In Proceedings of \nthe S{GPLAN 86 Symposium on Compiler Construction, pages 176\u00ad185, July 1986, Palo Alto, CA. puting. In \nSupercomputing 88, pages 330-339, Novem\u00ad [23] M. Wolfe. Optimizing Supercompilers for Supercomput\u00ad ber \n1988. ers. MIT Press, Cambridge, MA, 1989. [24] J. C. Wyllie. The complexity of parallel computations. \n Ph.D. thesis, Computer Science Department, Cornell University, Ithaca, NY, 1979. [25] P. Yang, J. Webb, \nJ. Stichnoth, D. O Hallaron, and T. Gross. Do &#38; Merge: Integrating parallel loops and reduc\u00adtions. \nIn The Sixth Annual Workshop on Z.asguages and Compilers for Parallel Computing, Portland, Oregon, August \n1993. \n\t\t\t", "proc_id": "178243", "abstract": "<p>We present a method for automatically extracting parallel prefix programs from sequential loops, even in the presence of complicated conditional statements. Rather than searching for associative operators in the loop body directly, the method rests on the observation that functional composition itself is associative. Accordingly, we model the loop body as a multivalued function of multiple parameters, and look for a closed-form representation of arbitrary compositions of loop body instances. Careful analysis of conditionals allows this search to succeed in cases where existing automatic methods fail. The method has been implemented and used to generate code for the iWarp parallel computer.</p>", "authors": [{"name": "Allan L. Fisher", "author_profile_id": "81502691939", "affiliation": "School of Computer Science, Carnegie Mellon University, Pittsburgh, PA", "person_id": "P15257", "email_address": "", "orcid_id": ""}, {"name": "Anwar M. Ghuloum", "author_profile_id": "81100409106", "affiliation": "School of Computer Science, Carnegie Mellon University, Pittsburgh, PA", "person_id": "P21162", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/178243.178255", "year": "1994", "article_id": "178255", "conference": "PLDI", "title": "Parallelizing complex scans and reductions", "url": "http://dl.acm.org/citation.cfm?id=178255"}