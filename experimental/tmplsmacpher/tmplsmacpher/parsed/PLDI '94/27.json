{"article_publication_date": "06-01-1994", "fulltext": "\n optimizing Dynamically-Dispatched Calls with Run-Time Type Feedback Urs Holzle Computer Systems Laboratory, \nStanford University, Stanford, CA urs@cs.stanford. edu David Ungar Sun Microsystems Laboratories, Mountain \nView, CA ungar@eng.sun.com Abstrach Object-oriented programs are difficult to optimize because they execute \nmany dynamically-dispatched calls. These calls cannot easily be eliminated because the compiler does \nnot know which callee will be invoked at runtime. We have developed a simple technique that feeds back \ntype information from the runtime system to the compiler. With this type feedback, the compiler can inline \nany dynamically-dispatched call. Our compiler drastically reduces the calI frequency of a suite of large \nSELF appli\u00adcations (by a factor of 3.6) and improves performance by a factor of 1.7. We believe that \ntype feedback could significantly reduce call frequencies and improve performance for most other object\u00adoriented \nlanguages (statically-typed or not) as well as for languages with type-dependent operations such as generic \narith\u00admetic. 1. Introduction Object-oriented programs are harder to optimize than programs written in \nlanguages like C or Fortran. There are two main reasons for this. First, object-oriented programming \nencourages code factoring and differential programming; as a result, procedures are smaller and procedure \ncalls more frequent. Second, it is hard to optimize calls because they use dynamic dispatch: the procedure \ninvoked by the call is not known until rtmtime because it depends on the dynamic type of the receiver. \nTherefore, a compiler usually cannot apply standard optimizations such as inline substitution or interprocedural \nanalysis to these calls. Consider the following example (written in pidgin C++): class Point { virtual \nfloat get_x( ) ; II get x coordinate virtual float get~ ( ) ; II ditto for y virtual float distance(Point \np) ; II compute distance between receiver and p } When the compiler encounters the expression p->get_xo, \nwhere p s declared type is Point, it cannot optimize the call because it does not know p s exact run,time \ntype. For example, there could be two subclasses of point, one for Cartesian points and one for polar \npoints: Permission to copy without fee all or part of this material is granted provided that the copies \nare not made or distributed for direet commercial advantaqe, the ACM copyright notice and the title of \nthe publication and is date appear, and notice is given that copying is by permission of the Association \nof Computing Machinery. To capy otherwise, or to republish, requires a fee and/or specific permission. \nSIGPLAN 94-6/$4 Ortando, Florida USA @ 1984 ACM 0-89791 -662-xJWO006..$3.5O class Carte sianPoint : Point \n{ float Xr Y; virtual float get_x ( ) { return x; } (other methods omitted) 1 class PolarPoint : Point \n{ float rho, theta; virtual float get_x( ) { return rho * cos (theta) ; ) (other methods omitted) Since \np could refer to either a cartes ianpoint or a Polar-Point instance at runtime, the compiler s type information \nis not precise enough to optimize the call: the compiler knows p s abstract type (i.e., the set of operations \nthat can be invoked and their signatures) but not its concrete type (i.e., the object s size, format, \nand the implementation of the operations). fire object-oriented languages exacerbate this problem because \nevery operation involves a dynamically-dispatched message send. For example, even very simple operations \nsuch as instance vari\u00adable accesses, integer addition, and array accesses conceptually involve message \nsends in SELF [U S87], the programming language used for this study. Consequently, a pure object-oriented \nlanguage like SELF offers an ideal test case for optimization techniques tack\u00adling the problem of frequent \ndynamically-dispatched calls. The rest of this paper describes our experience with a new optimi\u00adzation \ntechnique based on type feedback. With type feedback, our new compiler runs large SELF programs 1.7 times \nfaster than without, and 1.5 times faster than the previous SELF compiler which uses extensive compile-time \ntype analysis instead of type feedback. Although we have implemented type feedback only for the pure \ndynamically-typed object-oriented language SELF, the technique is language-independent and could be applied \nto stati\u00adcally-typed, non-pure languages as well. 2 Type Feedback The key idea of type feedback is to \nextract type information from executing programs and feed it back to the compiler (Figure 1). m I  \n =LEz!!!l Figure 1. Overview of Type Feedback Specifically, we use an instrumented version of a program \nto record the program s type projile, i.e., a list of receiver types (and, optionally, their frequencies) \nfor every single call site in the program. To obtain the type profile, the standard method dispatch mechanism \nis extended in some way to record the desired informa\u00adtion, e.g., by keeping a table of receiver types \nper call site. In the SELF system, no additional mechanism is needed to record receiver types since the \nsystem uses polymoq,rhic inline caches to speed up dynamic dispatch. As we have observed in [HCU91 ], \nthese caches record receiver types as a side-effect. Therefore, a program s type profile is readily available, \nand collecting the type feedback data does not incur any execution time overhead. However, the particular \nway in which type feedback information is collected is not important here; all that matters is that the \ninforma\u00adtion contains a list of receiver types (and, optionally, invocation counts) for each call site. \nThe program s type profile is then fed back into the compiler to generate optimized code. Using type \nfeedback, the compiler can optimize any dynamically-dispatched call (if desired) by predicting likely \nreceiver types and inlining the call for these types. In the above example, the expression x = p->get_x \n( ) could be compiled as if (p->class == Cartes ianPoint) { II inline CartesianPoint case x = p->x; } \nelse { // don t inline PolarPoint case because method is too big // this branch also covers all other \nreceiver types x = p->get_xo ; // dynamically-dispatched call } ForcartesianPo intreceivers, the above \ncode sequence will execute significantly faster since theoriginal virtual ftmction call is reduced to \nacomparison andasimple load instruction. Inlining not only eliminates the calling overhead but also enables \nthe compiler to optimize theinlined code using dataflow information particular to this call site. Some \noptimizations can enhance the benefits of inlining. Splitting [CU90] copies code following the i f statement \ninto the branches of the if, where itcanprofit from themore precise dataflow (or type) information that \nis specific to the branches of the if. However, splitting is limited to cases where the improved infcmna\u00adtion \ncan be used to optimize code immediately following (or very close to) the i f statement. If the code \nthat could benefit is further away, all code between it and the i f statement must be duplicated, and \nthe cost of the code increase may outweigh the benefits of the optimization. Another optimization, uncommon \nbranch elimination, is more aggressive and preserves the improved dataflow information throughout the \ncaller. Uncommon branch elimination was first suggested to us by John Maloney and was implemented in \nClham-hers SELF-91 compiler [Cha92] and(ina somewhat different and more aggressive form) in the SELF-93 \ncompiler described in the next section. The main idea is that the optimized code handles only the predicted \ncases. Of course, the code still has to test for the uncommon cases, but upon encountering such a case, \nit branches to a separate (less optimized) copy of the code which does not merge back into the optimized \nversion. Therefore, the optimized version s dataflow information is not polluted by the pessimistic alias \nand kill information caused by uncommon cases. For example, if the type feedback information indicates \nthat non-Cartesian points are almost never used, the expression x = p->get_xo could be compiled as if \n(p->~l~s~!= c~rtesi~npoint) { goto uncommon_case; // branch to separate version of the code that handles \nII non-Cartesion points and never branches back II to this code } // inline CartesianPoint get_xo x = \np->x; Now the code following this statement can be better optimized because the compiler knows p s class, \nand that get_x has no side\u00adeffects. Neither splitting nor uncommon branch elimination is necessary to \nimplement type feedback; we have presented them here merely as examples of optimizations that profit \nfrom opportunities created by type feedback. The SELF-93 compiler described below implements both optimization. \nPredicting future receiver types based on past receiver types is only an educated guess. Similar guesses \nare made by optimizing compilers that base decisions on execution profiles taken from previous runs [Wal19 \n1]. However, in our experience, type profiles are more stable than time profiles-if a receiver type dominates \na call site during one program execution, it also dominates during other executions. A recent study by \nGarret et al. [G+94] that measured the stability of type profiles in SELF, C++, and Cecil programs confirms \nour experience. 3. ~pe feedback in the SELF system This section describes the implementation of type \nfeedback in SELR although our implementation makes extensive use of possi\u00adbilities opened by dynamic \ncompilation, we wish to emphasize that dynamic compilation is not needed to implement type feedback. \nThe reader who is not interested in the particular details of the SELF implementation may safely skip \nthis section and continue with section 4. Section 5 discusses how type feedback could be implemented \nin a more conventional batch-style compilation environment. Since SELF is dynamically-typed, it has no \nexplicit notion of type. However, the implementation maintains internal type descriptors (called maps \n) that describe the exact format of each object (i.e., its storage layout, inheritance structure, etc.). \nIn the remainder of this paper, we will use type to refer to these internal implementa\u00adtion types. Translated \ninto C++ parlance, type stands for non\u00adabstract (concrete) class. 3.1 Dynamic recompilation The SELF-93 \nsystem uses dynamic recompilation not only to take advantage of type feedback but also to determine which \nparts of an application should be optimized at all. Figure 2 shows an overview of the compilation process \nof the system. When a source method is if executed is first invoked debugging[HCU92]  m-l ~~~d L= 0 \n:5: I Figure 2. Compilation in the SELF-93 system invoked for the first time, it is compiled quickly \nby a very simple, completely non-optimizing compiler. If the method is executed often, it is recompiled \nand optimized using type feedback. Some\u00adtimes, an optimized method is reoptimized to take advantage of \nadditional type information or to adapt it to changes in the program s type profile. Combining the optimizing \ncompiler with the fast non-optimizing compiler and dynamic recompilation allows SELF-93 to achieve high \nperformance while keeping compi\u00adlation pauses in the sub-second range [H0194]. In the following sections, \nwe will briefly discuss our implementa\u00adtion of dynamic recompilation; more details can be found in [H0194]. \n3.2 When to recompile Any dynamic recompilation system needs to decide when to recompile code. If the \nsystem recompiles too eagerly, compilation time is wasted; if it recompiles too lazily, performance will \nsuffer. SELF-93 uses invocation counts to drive recompilation. Each unop\u00adtimized method has its own counter \nthat is incremented in the method prologue. When the counter exceeds a certain limit, the recompiler \nis invoked to decide which method (if any) should be recompiled. If the method overflowing its counter \nisn t recom\u00adpiled, its counter is reset to zero. Counter values decay exponen\u00adtially with time (i.e., \nthe system monitors invocation rates, not pure invocation counts). Originally, we envisioned counters \nas a first step, to be used only until a better solution was found. However, in the course of our experiments \nwe came to realize that the trigger mechanism ( when ) is much less important forgood recompilation results \nthan the selection mechanism ( what ). 3.3 Whatto recompile To find a good candidate for recompilation, \nthe recompiler walks upthecall chain and inspects the callers of the method triggering the recompilation. \nA caller is recompilec[ if it performs many calls tounoptimized or small methods (the hope being that \nthese calls will be eliminated), or if it creates closure objects. (SELF imple\u00adments all control structures \nusing message passing and closures; when control structures are inlined, the closures can typically be \neliminated.) A simpler recompilation strategy would always recompile the method whose counter cwerflowed, \nsince it obvi\u00adouslywas invoked often. Butsuppose that themethod just returus a constant. Optimizing this \nmethod would not gain much; rather, the method should be inlined into its caller, and thus it is necessary \nto inspect the callers before deciding what to recompile. Ifarecompilee is found, it is (re)optimi:zed, \nandtheold version is discarded. During the compilation, the compiler marks the restart point (i.e., the \npoint where execution will be resumed) and tries to compute the contents of all live registers at that \npoint. If this is successful,t the reoptimized method replaces the corresponding unoptimized methods \non the stack, possibly replacing several unoptimized activation records with a single optimized activation \nrecord. (This process is the reverse of dlynamic deoptimization as described in [HCU92]; that paper also \ndescribes how the compiler represents the source-level state of optimized code.) The system tries to \noptimize an entire call chain from the top recompile down to the current execution point. (Usually, the \nrecompiled call chain is only one or two compiled methods deep.) Thus, if the newly optimized method \nisn t at the top of the stack, recompilation continues with the method s callee. If the old method cannot \nbe replaced on the stack, it is left to finish its current activation(s), but subsequent invocations \nwill always use the new, optimized version. Finally, the recompilation system also checks to see if recompila\u00adtion \nwas effective, i.e., if it actually improved the code. If the previous and new compiled methods have \nexactly the same non\u00adinlined calls, recompilation did not really gain anything, and thus t The ~OmPiler \nc~n~t always describe the register contents in Sorme-tevei terms since it does not track the effects \nof all optimizations in order to keep the compiler simple. However, it can always detect snch situations \nand signat them to the recompilation system, the new method is marked so it won t be considered for future \nrecompilation. 3.4 Inlining strategies Although type feedback enables the compiler to inline any call \nin the program, not all calls should be inlined. Deciding whether to inline a particular send is difficult \nfor several reasons. First, inlining one method may require other methods to be inlined as well (e.g., \nto reduce closure creation overhead). Second, even if the compiler could accurately estimate the local \nimpact of inlining a send, the overall performance impact may depend on the result of other inlining \ndecisions. For example, inlining a send maybe bene\u00adficial in one case but may hurt performance in another \ncase because other inlined sends increase register pressure so much that important variables cannot be \nregister-allocated. The current SELF compiler uses a set of simple rules to guide the inlining process. \nEssentially, methods are inlined if they are small, and if the estimated size of the caller (including \nall methods inlined so far) is not too big. The latter condition avoids excessive inlining that could \narise when many small methods are called. Determining the size of an inlining candidate is harder in \nSELF than in more traditional languages: since SELF is a pure object\u00adoriented language, it performs all \ncomputation via message sending, and thus virtually every source-code token represents a message send \nwhose cost (both in terms of space and time) is highly variable. To improve its estimates, the SELF compiler \nexam\u00adines previously-compiled optimized code where available. Besides being more accurate than source-level \nsize estimates, this approach also has the advantage of considering a bigger picture: typically, the \ncompiled method for a source method includes not only code for the method itself but also that of inlined \ncalls. By examining previously-compiled code, the compiler can obtain a better esti\u00admate of the ultimate \nspace cost of an inlining decision. 3.5 Structure of the SELF-93 compiler This section briefly describes \nthe optimizing SELF-93 compiler which combines simplicity with good compilation speed and good code quality. \nThe front end of the compiler performs a variety of optimizations that are necessary to achieve good \nperformance with pure object-oriented languages inlining (based on type feed\u00adback), customization [C \nUL89], and splitting [CU90] and gener\u00adates a graph of intermediate code nodes. The back end performs \nonly very few optimizations on the intermediate code before generating machine code. In particular, the \ncompiler does not perform full-fledged dataflow analysis or coloring register alloca\u00ad tion because we \nconsidered these techniques to be too expensive in terms of compilation speed. After computing the definitions \nand uses of each pseudo register, the compiler performs the following optitnizations: Closure analysis \ndetermines which closures can be eliminated because they are not needed as actual runtime objects.  \nCopy propagation propagates pseudo registers within basic blocks, and singly-assigned pseudo registers \nglobally. (These propagations can be performed without computing full dataflow information.)  Dead code \nelimination discards nodes whose results are no  longer needed. A simple usage-count based register \nallocator computes the register assignments, and the final machine code is generated in a single pass \nover the intermediate graph. The main differences between SELF-93 and the SELF-91 compiler described \nby Chambers [Cha92] are that we have substituted type feedback for iterative type analysis, and that \nour back end is less ambitious. As a result, SELF-93 is considerably simpler (11,000 vs. 26,000 lines \nof C++). However, compared to SELF-9 1, SELF-93 has several shortcomings: Inferior local code quality. \nThe compiler does not fill delay slots except within fixed code patterns. Also, code often contains branches \nthat branch to other (unconditional) branch instructions instead of directly branching to the final target. \nFinally, values may be repeatedly loaded from memory, even within the same basic block. This is especially \ninefficient if the loaded value is an uplevel-accessed variable since an entire sequence of loads (following \nthe lexical chain) is repeated in this case. Inferior register allocation. The register allocator is \nvery simple and can cause unnecessary register moves or spills. Redundant type tests. Since the compiler \ndoes not perform type analysis or full dataflow analysis, a value may be tested repeatedly for its type \neven though only the first test is necessary. It is hard to estimate the performance impact of these \nshor-tcom\u00adings. However, based on Chambers analysis of the SELF-91 compiler [Cha92] and an inspection \nof the compiled code of several programs, we believe that they slow down the large object\u00adoriented programs \nmeasured in this study by at least 10%. (For programs with small integer loops, the overhead can be much \nhigher.) Therefore, the performance of type feedback as reported in the next section is probably a conservative \nindication of what a fully optimizing SELF compiler with type feedback could achieve.  4. Results To \nevaluate the performance of the SELF-93 compiler and the contribution of type feedback, we measured the \nruntime perfor\u00admance of several large SELF programs (see Table A-1 in the appendix for a short description \nof the benchmarks). With the exception of the Richards benchmark, all programs are real appli\u00adcations \nthat were not written for benchmarking purposes. Table 1 lists the systems used in our study. System \nDescription SELF-93 The current SELF system using dynamic recompilation and type feedback; methods are \ncompiled by a fast non-optimizing compiler first, then recompiled with the optimizing compiler if necessag. \n SELF-93 Same as SELF-93, but without type feedback and nofeedback recompilation; all methods are afways \noptimized from the beginning. SELF-91 Chambers SELF compiler [Cha92] using iterative type analysis; \nall methods are afways optimized from the beginning. This compiler has been shown to achieve excellent \nperformance for smafler programs. Smalkalk-80 ParcPlace Smalkafk-80m release 4.0, generally regarded \nas the fastest commercial Smalltalk system (based on techniques described in [DS84]) UC++ GNU C and \nC++ compilers, version 2.4.5, using -02 optimization Lisp Sun CommonLisp 4.0TMusing full optimization \nTable 1: Systems used for benchmarking 4.1 Methodology To accurately measure execution times, the programs \nwere run under a SPARC simulator based on the Spa [1s-191] and Shade [CK93] tracing tools and the Dinero \ncache simulator [Hil187]. The simulator models the Cypress CY7C601 implementation of the SPARCTM architecture, \ni.e., the chip used in the SPARCstation-2TM workstation. The simulator also accurately models the memory \nsystem of a SPARCstation-2, with the exception of the cache organization. Instead of the unified direct-mapped \n64K cache of the SPARCsta\u00adtion-2, we simulate a machine with a 32K 2-way associative instruction cache \nand a 32K 2-way associative data cache using write-allocate with subblock placement. Write-allocate with \nsubblock placement caches allocate a cache line when a store instruction references a location not currently \nresiding in the cache. This organization is used in current workstations (e.g., the DECsta\u00ad tion 5000TM \nseries) and has been shown to be effective for programs with intensive heap allocation [KLS92], [Rei93], \n[DTM94]. We do not use the original SPARCstation-2 cache configuration because it suffers from large \nvariations in cache miss ratios caused by small differences in code and data positioning (we have observed \nvariations of up to 1570 of total execution time). With the changed cache configuration, these variations \nbecome much smaller (on the order of 2% of execution time) so that the perfor\u00admance of two systems can \nbe more accurately compared.t The execution times for the SELF programs reflect the performance of (re-)optimized \ncode, i.e., they do not include compile time. For the recompiling system, the programs were run until \nperformance stabilized, and the next run not involving compilations was used. (The impact of dynamic \nrecompilation on interactive performance is beyond the scope of this paper and will be the subject of \na sepa\u00adrate study.) SELF-9 1 and SELF-93 -nofeedback do not use recompi\u00adlation, so we used the second \nrun for our measurements. 4.2 Impact of type feedback on execution time To evaluate the performance impact \nof type feedback, we compared the three versions of the SELF system mentioned in Table 1. Figure 3 on \nthe next page shows the results (Table A-2 in the appendix contains detailed data). Comparing SELF-93 \nwith SEL&#38;93-nofeedback shows that type feedback significantly improves the quality of the generated \ncode, resulting in a speedup of 1.7 (geometric mean) even though SELF-93 -nofeedback always optimizes \nall-code whereas SELF-93 optimizes only parts of the code. (Sections 4.4 and 4.5 will analyze the reasons \nfor the increased performance of SELF-93 in more detail.) SELF-93 also outperforms SELF-9 1 by a considerable \nmargin, with a speedup of 1.5. Apparently, the better back end and iterative type analysis are not enough \nfor SELF-9 1 to compensate for the wealth of type infor\u00admation provided by type feedback. In fact, SELF-91 \nis only margin\u00adally faster than SELF-93 -nofeedback which does not use any type analysis. In other words, \nSELF-9 1 s type analysis appears to be largely ineffective for the programs we measured. 4.3 Impact \nof type feedback on call frequency Type feedback drastically reduces the number of calls executed by \nthe benchmark programs. Figure 4 shows the number of calls rela\u00adtive to unoptimized SELF, where each \nmessage send is imple\u00admented as a dynamically-dispatched call (with the exception of tToensureourof cache \norganization did not distort the results, thatchoice  we measured different cache organizations, including \n32K and 64K direct\u00admapped caches. While absolute execution times varied, the resulting performance ratios \n(e.g.. SELF-93 vs. SELF-93-nofeedback) were within 10% of the ratios presented here. faster E e  ~ ,.. \n.:..UL..3 Richards CecilComp a Mango Typeinf Self-91TTrf Geom. Self-93 I As. mean v . ! 1 1111I 1 0.0 \n0.5 1.0 1.5 2.0 2.5 3.0 3.5 Figure3. Performance impact oftype feedback (all speeds relative to SELF-93-nofeedback) \n accesses to instance variables in the receiver). Both SELF-91 and SELF-93 run many times faster than \nunoptimized programs. worse - Self-93  Cecilcomp+ Mango 1. 1 UI1 U13 1 Geom. IIIIIII o% 5% 1o% 15% \n20% 25% 30% 35% Figure 4. Impact of type feedback on number of calls (all numbers are relative to unoptimized \nSELF) Whereas 10-25% of the original calls remain in SELF-9 1 and SELF\u00ad93-no feedback, SELF-93 reduces \nthe call frequency to about 5% of the unoptimized system. Compared to the SELF systems without type feedback, \ncalls are reduced by a factor of 3.6. Since SELF-93\u00adnofcedback performs about the same number of calls \nas SELF-91, we can also assume that comparing SELF-93 to SELF-91 is fair, i.e., that the reduction in \ncall frequency and execution time is entirely due to type feedback and camot be attributed other differences \n(such as more aggressive irdining). As with performance, the sophisticated type analysis in SELF-91 fails \nto give it an advantage over SELF-93 -nofeedback when it comes to eliminating calls. 4.4 Type testing \noverhead Since type feedback transforms dynamically-dispatched calls into type tests followed by inlined \nmethods, it is interesting to look at the characteristics of these type tests. In SELF-93, type tests \nare used in two situations: for sends inlined by type feedback (inlined tests), and for the dispatch \nof non-inlined sends (dispatch tests). The latter are used because in dynamically-typed languages it \nis harder (but not impossible [Dri93]) to use indirect function calls for dynamically-dispatched calls. \nInstead, SELF uses Polymorphic Inline Caches [HCU91 ] which implement a dynamically \u00addispatched call \nas a typecase statement (to determine the receiver type) followed by a direct call. This implementation \nof dynamic dispatch can compete well with the indirect-call implementation typically used by C++ systems: \non the SPARCstation-2. SELF-93 uses an average of 12 cycles per dispatched call (including cache effects) \nfor the programs measured, whereas a C++ virtual call uses 10 cycles (excluding cache effects). The average \nnumber of type tests executed per send (i.e., the number of branches in the if statement testing for \nthe expected types) is very small. Figure 5 shows the distribution of the per\u00adbenchmark averages for \nSELF-93 -nofeedback (left boxes) and + inlined dispatch overall type tests type tests Figure 5. Number \nof type tests per dispatched send Box charts show the range of data (vertical lines) as well as tbe 25% \nand 75% percentiles (end of the boxes) and the me\u00addinn (horizontal lines). Where the median and mean \ndiffer significantly, we indicate the mean with a dot (o). SELF-93 (right boxes), Since we are interested \nin the work done per type test sequence, the data excludes sends requiring no type test, i.e. sends whose \nreceiver type was known with certainty. SELF-93 -nofeedback executes some inlined type tests because \nit uses static type prediction [DS84] to predict the receiver type of certain very frequent messages. \nStatic type prediction always predicts for a single type, except for sends to boolean receivers (true \nand false are two different types in SELF). Thus, the low average of 1.2 tests per send in SELF-93 -nofeedback \nis not surprising. What is surprising, however, is that type feedback reduces this average even more, \nto 1.08 tests per send. In other words, the vast majority of inlined type tests need only one comparison \nto find its target. Apparently, most sends optimized with type feedback have only one receiver type or \nare dominated by a single receiver type. For non-inlinecl sends, type feedback pushes up the median number \nof type tests per send from 1.35 to 1.7 tests per send. Type feedback does not actually increase the \ndegree of polymorphism of sends; however, since the compiler does not inline highly polymor\u00adphic sends \n(with 5 or more receiver types) but at the same time eliminates many of the other sends, the distribution \nof the remaining sends is skewed towards higher polymorphism, and thus the average number of type tests \nper send increases. Finally, the last category of Figure 5 shows that the overall number of type tests \nper send is reduced by type feedback. Does this mean that programs optimized by type feedback perform \nfewer type tests? Figure 6 shows that this is indeed the case: on average, SELF\u00ad I call  dispatch tests \noverhead1 i I I I typetest inlined tests overhead1 Tm %n2x?4 1 PrimMaker I i I Richards I 3 I 1 I Mango \nTypeinf uIl U13 Geom. mean 4 I 1 070 20970 40% 60% 80% 100% Figure 6. Number of type tests relative to \nSELF-93-nofeedback (upper bars: SEt.,F-93-nofeedback, lower bars: SELF-93) 93 programs execute 27% fewer \ntype tests. At first sight, such a reduction seems impossible: since dispatch is implemented as a type \ntest followed by a call, and type feedback just transforms this sequence into a type test followed by \ninlined code, it would seem that the total number of type tests should remain exactly the same since \ntype feedback merely turns dispatch tests into inlined tests. (Figure 6 confirms that many dispatch tests \nare indeed transformed into inlined tests.) ~pe feedback can reduce the number of type tests because \nthe compiler may statically know the types of the arguments of a send inlined via type feedback. For \nexample, suppose that a method m is called with a constant argument. If this send is not inlined, each \nsend in m to the argument will require a type test since the argu\u00adment s type is not known statically. \nHowever, after m has been inlined using type feedback, constant propagation can reach all uses of the \nconstant argument and eliminate the type tests. Thus, by inserting one type feedback test, the compiler \nhas eliminated other type tests and has reduced the overall number of type tests. In the benchmarks we \nmeasured, each type feedback test removed 0.8 other type tests on average, even though the compiler performs \nonly very rudimentary dataflow analysis. With a more sophisti\u00adcated analysis, this bonus might be even \nhigher. 4.5 Analysis of speedup Why does type feedback speed up programs? One reason for the increased \nspeed is the reduced call overhead, but how much of the speedup is obtained by just eliminating call \noverhead, and how much is due to other factors? Figure 7 shows that the sources of improved performance \ncan vary widely from benchmark to bench\u00admark. (The data assumes a savings of 10 cycles per eliminated \ncall since we could not measure the exact savings per call.) Depending on the benchmark, the reduced \ncall overhead represents between 6% and 63% of the total savings in execution time, with a median of \n13% and an arithmetic mean of zs~o (geometric mean: 18%). The reduced number of type tests contributes \nalmost as much to the speedup, with a median contribution of 17% and a mean of l!)~o, as does the reduced \nnumber of closure creations. Other effects (such as standard optimization that perform better with the \nincreased size of compiled methods) make the greatest contribution to the speedup (with a median of 45% \nand a mean of closures I other I III1 -20% o% 20% 40% 60% 80% fraction of total speedup achieved Figure \n7. Reasons for SELF-93 S improved performance 38%) but also show the largest variation. For one benchmark, \nthe contribution is actually negative, i.e., slows down execution. Some of the possible reasons for the \nslowdown are inferior register allo\u00adcation (because of increased register pressure), or higher instruc\u00adtion \ncache misses. (All of the above measurements include cache effects.) To summarize, the measurements in \nFigure 7 show that the perfor\u00admance improvement obtained by using type feedback is by no means dominated \nby the decreased call overhead. In most bench\u00admarks, factors other than call overhead dominate the savings \nin execution time. Inlining based on type feedback is an enabling optimization that allows other optimization \nto work better, thus creating indirect performance benefits in addition to the direct benefits obtained \nby eliminating calls. 4.6 Code growth Exponential code growth is a well-known potential problem of procedure \ninlining. However, the additional inlining performed by SELF-93 does not increase code size much over \nthe systems not using type feedback (Figure 8). On average, compiled code is only optimized code unoptimized \ncode SELF-93 nofeedback 1 SELF-93 I IIIIIII O% 20% 40% 60% 80% 100% 120% 140% Figure 8. Size of compiled \ncode relative to SELF-91 25% larger in SELF-93 than in SELF-91; comparing SELF-93\u00adnofeedback to SELF-91 \nshows that part of the code size increase may be caused by the inferior SELF-93 back end. For some programs, \nthe resulting code actually becomes smaller. This behavior suggests that previous SELF systems could \nnot irdine many attractive inlining candidates (i.e., very small methods), so that type feedback can \nreduce the call frequency by a factor of 3.6 with a code growth of only 15-25Y0. 4.7 Performance relative \nto other systems To provide some context about SELF S performance, we measured versions of the DeltaBlue \nand Richards benchmarks written in C++ and Smalltallc, as well as a Lisp version of Richards. (See Table \n1 for details about the C++ and Smalltalk systems, and Table A-5 in the Appendix for detailed performance \ndata; none of the other benchmarks are available in other languages.) Since it was not possible to run \nSmalltalk or Lisp with the simulator, we could only measure SPARCstation-2 CPU times. Simulated times \nof SELF programs usually are between 5 and 25% lower than measured execution times on a SPARCstation-2 \nsince the simulation models a better cache organization and does not include OS overhead. Therefore, \nfor comparison with SELF and C++, we reduced the measured Smalltalk and Lisp execution times by a conservative \n25%. Figure 10 shows the results. D Lisp Smalltalk-80 Self-93 faster 1virtuals) DeltaBlue Richards o% \n50~o 100% 150 70 200% 250% Figure 9. Execution speed (SELF-93= 100%) For DeltaBlue and Richards, SELF-93 \nruns 2.2 and 3.3 times faster than ParcPlace Smalltalk (generally regarded as the fastest commercially \navailable Smalltalk system) even though SELF S language model is purer and thus harder to implement efficiently \n[Cha92]. For Richards, SELF-93 runs 2.6 times faster than an equivalent CommonLisp program compiled with \nmaximum opti\u00admization and minimum safety (i.e., the Lisp code would not detect some runtime errors). \nIn conclusion, for these two programs SELF\u00ad93 runs two to three times faster than languages with roughly \ncomparable semantics. Comparing SELF and C++ is harder since the two languages have very different language \nmodels. SELF provides code reuse and safety by basing the language on extensible control structures, \npointer safety, bounds and overflow checking, generic and exten\u00adsible arithmetic, and pure message passing. \nOn the other hand, C++ omits these features (with the exception of virtual functions) in its quest for \nhigh performance. Consequently, the C++ programmer has a choice of programming style either she uses \nvirtual functions liberally to get more flexibility, reusability, and maintainability, or she minimizes \nvirtual function usage to get maximum performance. We have measured both extremes in order to compare \nSELF-93 S performance against C++. If the two C++ programs are hand-opti\u00admized to make minimal usage \nof virtual calls, C++ is 2.3 times faster than SELF-93. If all C++ functions are declared virtual, however, \nC++ is only 10% to 40% faster than SELF-93 despite SELF S clearly inferior back end. We have also measured \nthe size of compiled code relative to C++. This comparison should be taken cum gruno salis since our \nmeasurements are somewhat imprecise. Fkst, the SELF numbers include some code in the measurement loop \ncalling the actual benchmarks; since the two benchmarks are fairly small (10\u00ad40 Kbytes), this code may \ninflate the numbers for SELF. Second, all numbers include only the actual code generated by the compilers \nand exclude any library code needed by the programs (for both C++ programs the library code is an order \nof magnitude larger than the actual compiled code). Third, as we have mentioned above, SELF S execution \nsemantics are very different from C++ s, and additional code is sometimes needed to preserve them (e.g., \noverflow checks). Self-91 bigger Self-93 ichmds= 1 DeltaBlue IIIIII1 09 0 100% 200% 300% 400% 500% \n600% Figure 10. Code size relative to GNU C++ Figure 9 shows that for Richards and DeltaBlue, the additional \ninlining performed by SELF-93 actually decreases code size rela\u00adtive to SELF-91 (see Table A-6 in the \nappendix for absolute data). But compared to GNU C++ the code is larger, especially for DeltaBlue where \nseveral methods defined for constraints are customized to the three constraint types. In this particular \ncase, the compiler actually overcustomizes-not all of the customization is necessary to get good performance. \nThus, the code increase is not a result of type feedback but of overcustornization (type feedback actually \ndecreases DeltaBlue s code size).t Fortunately, our experi\u00adence with larger applications suggests that \nDeltaBlue is a patholog\u00adical case rather than the norm. 5. Applicability to other systems As demonstrated \nby the above measurements, type feedback works very well for SELF. How well would it work with more conven\u00adtional \nimplementation techniques (i.e., static compilation), and how does it apply to other languages? 5.1 Type \nfeedback and static compilation Type feedback is in no way dependent on the exotic implementa\u00adtion techniques \nused in SELF-93 (e.g., dynamic compilation or dynamic recompilation). If anything, these techniques make \nit harder to optimize programs: using dynamic compilation in an interactive system places high demands \non compile speed and space efficiency. For these reasons, the SELF-93 implementation of type feedback \nhas to cope with incomplete information (i.e., partial type profiles and inexact invocation counts) and \nmust refrain from performing some optimizations to achieve good compilation speed. Figure 11. ~pe feedback \nin a statically compiled system Thus, we believe that type feedback is probably easier to add to a conventional \nbatch-style compilation system. In such a system, optimization would proceed in three phases (Figure \n11). First, the executable is instrumented to record receiver types, for example with a gprof -like profiler \n[GKM83]. (The standard gpro f t With type feedback, h would be possible to customize less WgreSSivelY \n(thus reducing code size) since customization is no longer needed to enable inlining (i.e., with type \nfeedback the main benefit of customization is that it can reduce the number of type tests required). \nprofiler already collects almost all information needed by type feedback, except that its data is caller-specific \nrather than call-site specific, i.e., it does not separate two calls of foo if both come from the same \nfunction.) Then, the application is run with one or more test inputs that are representative of the expected \ninputs for production use. Finally, the collected type and profiling infmrna\u00adtion is fed back to the \ncompiler to produce the final optimized code. As mentioned above, static compilation has the advantage \nthat the compiler has complete information (i.e., a complete call graph and type profile) since optimization \nstarts after a complete program execution. In contrast, a dynamic recompilation system has to make decisions \nbased on incomplete information. For example, it cannot afford to keep a complete call graph, and the \nfirst recompi\u00adlation may be necessary while the program is still in the initializa\u00adtion phases so that \nthe type profile is not yet representative. On the other hand, a dynamic recompilation system has a significant \nadvantage because it can dynamically adapt to changes in the program s behavior. 5.2 Applicability to \nother langaages Obviously, type feedback could be used for other object-oriented languages (e.g., Smalltalk \nor C++), or for languages with generic operators that could be optimized with the type feedback informa\u00adtion \n(e.g., APL or Lisp). But how effective would it be? We cannot give a definitive answer since would require \nmeasurements of actual implementations, which are not available. Instead, we discuss the applicability \nof type feedback using Smalltalk and C++ as examples. Type feedback is directly applicable to Smalltalk, \nand we expect the resulting speedups to be similar to those achieved for SELF. Despite some language \ndifferences (e.g. prototype-vs. class-based inheritance), the two languages have very similar execution \nchar\u00adacteristics (e.g., a high frequency of message sends, intensive heap allocation, use of closures \nto implement user-defined control struc\u00adtures, etc.) and thus very similar sources of inefficiency. \nC++ s execution behavior (and language philosophy) is much further away from SELF, but we believe it \nwill nevertheless benefit from type feedback. First, measurements of large C++ programs [CGZ94] have \nshown that calls are almost five times more frequent in C++ programs than in C programs, and that the \naverage size of a C++ virtual function is only 30 instructions, six times smaller than the average C \nfunction. Second, the two C++ programs we measured in section 4.7 slowed down by factors of 1.7 and 2.2 \nwhen using virtual functions everywhere, demonstrating that current C++ compilers do not optimize such \ncalls well. Third, we expect that C++ programmers will make even more use of virtual functions in the \nfuture as they become more familiar with object\u00adoriented programming styles; for example, recent versions \nof the Interviews framework [LVC89] use virtual functions snore frequently than previous versions. To \ngive a concrete example, the DOC document editor measured in [CGZ94] performs a virtual call every 75 \ninstructions; given that a C++ virtual call uses about 5 instmctions and usually incurs two load stalls \nand a stall for the indirect function call, we estimate that this program spends roughly 10% of its time \ndispatching virtual functions. If type feedback could eliminate a large fraction of these calls, and \nif the indirect benefits of inlining in C++ are similar to those measured for SELF (i.e., total savings \nare 4-6 times higher than the call overhead alone, see Figure 7), substantial speedups appear possible. \nFor type feedback to work well, the dynamic number of receiver types per call site should be close to \none, i.e., one or two receiver types should dominate. A large fraction of call sites in C++ have this \nproperty [CG94][G+94], and it also holds in other object\u00adoriented programming languages (e.g., Smalltalk, \nSELF, Sather, and Eiffel); this is the reason that inline caching [DS84], [HCU91 ] works well in these \nlanguages as an implementation of dynamic dispatch. Therefore, we expect type feedback to work well for \nthese languages; the higher the frequency of dynamically\u00addispatched calls, the more beneficial type feedback \ncould be. 6. Related work Previous systems have used static type prediction to inline opera\u00adtions that \ndepend on the runtime type of their operands. For example, Lisp systems usually inline the integer case \nof generic arithmetic and handle all other type combinations with a call to a routine in the runtime \nsystem. The Deutsch-Schiffman Smalltalk compiler was the first object-oriented system to predict integer \nreceivers for common message names such as + [DS84]. However, none of these systems predicted types adaptively \nas does SELF-93. Other systems have used some form of runtime type information for optimization, although \nnot to the same extent as SELF-93 and not in combination with recompilation, For example, Mitchell s \nsystem [Mit70] specialized arithmetic operations to the rtmtime types of the operands (similar to SELF-89 \nS customization [CUL89]). Similarly, several APL compilers created specialized code for certain expressions \n(e.g. [Joh79], [Dyk77], [GW78]). Of these systems, the HP APL compiler [Dyk77] came closest to customization \nand type feedback. The system compiled code on a statement-by-statement basis. In addition to performing \nAPL\u00adspecific optimizations, compiled code was specialized according to the specific operand types (number \nof dimensions, size of each dimension, element type, etc.). This so-called hard code could execute much \nmore efficiently than more general versions since the cost of an APL operator varies wildly depending \non the actual argument types. If the code was invoked with incompatible types, a new version with less \nrestrictive assumptions was generated (so\u00adcalled soft code). Since the system never used type information \nto reoptimize code, the technique is more akin to customization than to type feedback. Customization \ncan be viewed as a restricted version of type feed\u00adback that attempts to minimize type tests by placing \nthe receiver type test at the beginning of the method. Unlike type feedback, customization benefits only \na restricted set of sends (namely those involving self ). As implemented in SELF, customization is also \nmore eager (i.e., all methods are always customized right away) and more static (all programs are treated \nthe same way). In contrast, type feedback in SELF-93 is more lazy and adaptive. The system described \nin this paper was inspired by the experi\u00admental proof-of-concept system described in [HCU91]. That system \nwas the first one to use type feedback (then called PlC\u00adbased inlining ) for optimization purposes. However, \nbeing an experimental system, its structure and performance was very different. It did not use dynamic \nrecompilation; methods had to be recompiled by hand, and the system lacked any mechanism determining \ngood recompilation candidates (i.e., it never looked at the callers). As a result, its speedup over a \nsystem without type feedback was modest (about 11%). Based on measurements of C++ programs, Calder and \nGrunwald [CG94] argue that type feed\u00adback would be beneficial for C++; their proposed if conversion appears \nto be identical to inline caching [DS84] and PIC-based inlining [HCU91 ], except that it is performed \nstatically. The Apple Object Pascal linker [App88] turned dynamically\u00addispatched calls into statically-bound \ncalls if a type had exactly one implementation (e.g., the system contained only a carte\u00ad sianpoint class \nand no PolarPoint class). The disadvantage of such a system is that it still leaves the procedure call \noverhead 333 even for very simple callees, does not optimize polymorphic calls, and precludes extensibility \nthrough dynamic linking. (Srivastava and Wall [SW92] perform more extensive link-time optimization but \ndo not optimize calls.) Some type inference systems (e.g., [APS93], [PR94]) can deter\u00admine the concrete \nreceiver types of message sends. Compared to type feedback, a type inference may provide more precise \ninfor\u00admation since it may be able to prove that only a single receiver type is possible at a given call \nsite. However, its information may also be less precise since it may include types that could occur in \ntheory but never happen in practice. (In other words, the informa\u00adtion lacks frequency data.) Like link-time \noptimizations, the main problem with type inference is that it requires knowledge of the entire program, \nthus precluding dynamic linking. Studies of inlining for more conventional languages like C or Fortran \nhave found that it often does not significantly increase execution speed but tends to significantly increase \ncode size (e.g., [DH88], [HwC89], [CHT91], [CM+92], [Hal191]). In contrast, inlining in SELF results \nin both significant speedups and only very moderate code growth. The main reason for this striking difference \nis that SELF methods are much smaller on average than C or Fortran procedures, so that inlining can actually \nreduce code size. (Because of dynamic dispatch, calls in object-oriented languages take up more instructions \nthan conventional procedure calls.) Furthermore, the additional inlining provided by type feedback enables \nsome optimizations to be more effective, reducing code size as well. Finally, inlining is more important \nfor object-oriented languages because calls are more frequent. While this is particu\u00adlarly true for pure \nobject-oriented kmguages, it is also true for hybrid languages like C++, as we have observed in section \n5.2. 7. Conclusions By using type information collected during previous execution of calls (type feedback), \nan optimizing compiler can replace dynami\u00adcally-dispatched calls with faster inline-substituted code \nsequences guarded by type tests for the common case(s). The process of collecting type information and \nthe inlining transformations based on that information are both straightforward and do not pose significant \nimplementation difficulties. We believe that type feed\u00adback is applicable to both statically-typed and \ndynamically-typed object-oriented languages (e.g., CLOS, C++, Smalltalk) and to languages with type-dependent \ngeneric operators (e.g., APL and Lisp). We have implemented a compilation system for SELF that dynami\u00adcally \nrecompiles often-used code and uses type feedback to generate better code. The system uses simple heuristics \nto decide which methods to recompile, how much to rely on type feedback, and how much to optimize. The \nresulting implementation is stable enough to be used by other researchers as part of their daily work. \nWith type feedback, a suite of large SELF applications runs 1.7 times faster than without type feedback, \nand performs 3.6 times fewer calls. On the two medium-sized programs also available in Smalltalk, our \nnew system outperforms a commercial Smalltalk implementation by factors of 2.2 and 3.3, respectively. \nWe believe that type feedback is an attractive optimization for situ\u00adations where the exact (implementation-level) \ntype of the argu\u00adments to a relatively costly operation is unknown at compile time, and where knowing \nthe types would allow the compiler to generate more efficient code. With the advent of object-oriented \nlanguages and their use of late-bound operations, such optimizations are likely to become more important \neven for statically-typed languages. Acknowledgments: We are very grateful to Bob Cmelik for making it \npossible to run SELF under Shade, to Mark D. Hill for Dinero, and to Gordon Irlam for Spanner. Many thanks \nalso to all the people who have commented on earlier versions of this paper: Lars Bak, Roger Hayes, Peter \nKessler, Brian Lewis, John Maloney, and Mario Wolczko, and the anonymous referees of PLDI 94 who provided \nvaluable suggestions for improvements. References [APS93] Ole Agesen, Jens Palsberg, and Michael 1. Schwartz\u00adbath. \nType Inference of SELF: Analysis of Objects with Dynamic and Multiple Inheritance. In ECOOP 93 Conference \nProceedings, p. 247-267. Kaiserslautem, Germany, July 1993. [App88] Apple Computer, Cupertino, 1988. \nInc. Object Pascal User s Manual. [CGZ94] Brad Calder, Dirk Grunwald, and Benjamin Zom. Quantijjing Behavioral \nD@erences Between C and C++ Programs. Technical Report CU-CS-698-94, University of Colorado, Boulder, \nJanuary 1994. [CG94] Brad Calder and Dirk Grrmwald. Reducing Indirect Function Call Overhead in C++ Programs. \nIn 21st Annual ACM Symposium on Pnkciples of Programming Languages, p. 397-408, January 1994. [Cha92] \nCraig Chambers, The Design and Implementation of the SELF Compiler, an Optimizing Compiler for Object-Oriented \nProgramming Languages. Ph.D. Thesis, Stan\u00adford University, April 1992 [Cha93] Craig Chambers. The Cecil \nLanguage -Specz~cation and Rationale. Technical Report CSE-TR-93-03-05, University of Washington, 1993. \n[CUL89] Craig Chambers, David Ungar, and Elgin Lee. An Effi\u00adcient Implementation of SELF, a Dynamically-Typed \nObject-Oriented Language Based on Prototypes. In 00PSL4 89 Conference Proceedings, p. 49-70, New Orleans, \nLA, October 1989. Published as SZGPLAN Notices 24(10), October 1989. [CU90] Craig Chambers and David \nUngar. Iterative Type Anal\u00adysis and Extended Message Splitting: Optimizing Dynamically-Typed Object-Oriented \nPrograms. In Proceedings of the SIGPLAN 90 Conference on Programming Language Design and Implementation, \np. 150-164, White Plains, NY, June 1990. Published as SIGPLAN Notices 25(6), June 1990. [CU93] Bay-Wei \nChang and David Ungar. Animatiom From cartoons to the user interface, User Inte~ace Software and Technology \nConference Proceedings, Atlanta, GA, November 1993. [CM+92] Pohua P. Chang, Scott A. Mahlke, William \nY. Chen, and Wen-Mei W. Hwu. Profile-guided automatic inline expansion for C programs. Sofhvare-Practice \nand Experience 22 (5): 349-369, May 1992. [CK93] Robert F, Cmelik and David Keppel. Shade: A Fast Instruction-Set \nSimulator for Execution Projiling. Tech\u00adnical Report SMLI TR-93\u00ad12, Sun Microsystems Labo\u00adratories, 1993, \nAlso published as Technical Report CSE\u00adTR-93-06-06, University of Washington, 1993. 334 [CHT91] K. \nD. Cooper, M. W. Hall, and L. Torczon. An ment with inline substitution. Sof~are Practice experi\u00adand \nExperience 21 (6): 581-601, June 1991, [DH88] Jack W. Davidson and Anne M. Holler. A study of a C function \ninliner. Software-Practice and Experience 18(8): 775-90, August 1988. [DS84] L. Peter Deutsch and Alan \nSchiffman. Efficient l[mple\u00admentation of the Smalltalk-80 System. Proceedings of the Ilth Symposium on \nthe Principles of Programming Lunguages, Salt Lake City, UT, 1984. [DTM94] Amer Diwan, David Tarditi, \nand Eliot Moss. Memory Subsystem Performance of Programs with Intensive Heap Allocation. In 21st Annual \nACM Symposium on Principles of Programming Languages, p. 1-14, January 1994. [Dri93] Karel Driesen. Selector \nTable Indexing and Sparse Arrays. 00PSLA 93 Conference Proceedings, p. 259\u00ad270, Washington, D. C., 1993. \nPublished as SIGPL4N Notices 28(10), September 1993. [Dyk77] Eric J. Van Dyke. A dynamic incremental \ncompiler for an interpretative language. HP Journal, p. 17-24, July 1977. [G+94] Charles D. Garret, Jeffrey \nDean, David Grove, and Craig Chambers. Measurement and Application of Dynamic Receiver Class Distributions. \nTechnical Report CSE-TR-94-03-05, University of Washington, February 1994. [GKM83] S. L. Graham, P. B. \nKessler, and M. K. McKusick. An Execution Profiler for Modular Programs. Sofware Practice and Experience \n13:671-685, 1983. [GW78] Leo J. Guibas and Douglas K. Wyatt. Compilation and Delayed Evaluation in APL. \nIn Ffth Annaal ACM Symposium on Principles of Programming Languages, p. 1-8, 1978. [Hal191] Mary Wolcott \nHall. ation. Technical Thesis), Computer sity, April 1991. Managing Inter-procedural Optimiz-Report COMP \nTR91 -157 (Ph.D. Science Department, Rice Univer\u00ad [Hil187] Mark D. Hill. Aspects of Cache Memory and \nInstruction Buffer Pe~ormance. Technical Report UCB/CSD 87/ 381, Computer Science Division, University \nof Cali\u00adfornia, Berkeley, November 1987. [HCU91] Urs Holzle, Craig Chambers, and David Ungar. Opti\u00admizing \nDynamically-Typed Object-Oriented Languages with Polymorphic Inline Caches, In ECOOP 91 Confer\u00adence Proceedings, \nGeneva, 1991. Published as Springer Verlag Lecture Notes in Computer Science 512, Springer Verlag, Berlin, \n1991. [HCU92] Urs Holzle, Craig Chambers, and David Ungar. Debug\u00adging Optimized Code With Dynamic Deoptimization. \nIn Proceedings of the SIGPLAN 92 Conference on Programming Language Design and Implementation, p. 21-38, \nSan Francisco, 1992. Published as SIG,PL4N Notices 27(6), June 1992. [H0194] Urs Holzle. Adaptive Optimization \nfor SELF: Recon\u00adciling High Pe~ormance with Exploratory Program\u00adming. Ph.D. Thesis, Stanford University, \nComputer Science Department, 1994. (In preparation.)  [HwC89] W. W. Hwu and P. P. Chang. Inline function \nexpansion for compiling C programs. In Proceedings of the SIGPL4N 89 Conference on Programming Language \nDesign and Implementation, p. 246-57, Portland, OR, June 1989. Published as SIGPLAN Notices 24(7), July \n1989. [Ir191] Gordon Irlam. SPA4PARC analyzer tool able via ftp from cs.adelaide.edu. au, 1991. set. \nAvail\u00ad [Joh79] Ronald L. Johnston. The Dynamic Incremental Compiler of APL\\3000. In Proceedings of the \nAPL 79 Conference. Published as APL Quote Quad 9(4), p. 82\u00ad87, 1979. [KLS92] Philip Koopman, Peter Lee, \nCache behavior of combinator Transactions on Programming 14 (2):265-297, April 1992. ar.d Daniel Siewiorek. \ngraph reduction. ACM Languages and Systems [LVC89] Mark Linton, John Vlissides, and Paul Composing User \nInterfaces with Interviews. Computer 22(2):8-22, February 1989. Calder. IEEE [Mit70] J. G. Mitchell, \nDesign and Construction of Flexible and Eficient Interactive Programming Systems. Ph.D. Thesis, Carnegie-Mellon \nUniversity, 1970. [PR94] Hemant D. Pande and Barbara G. Ryder, Determination for C++. Technical Report \n197a, Rutgers University, 1994. Static Type LCSR-TR\u00ad [Rei93] Mark Reinhold. Cache Pe~ormance of Garbage-Collected \nProgramming Languages. Technical Report MIT/LCS/TR-581 (Ph.D. Thesis), Massachusetts Insti\u00adtute of Technology, \nSeptember 1993. [SM+93] Michael Sannella, John Maloney, Bjom Freeman-Benson, and Alan Borning. Multi-way \nversus One-way Constraints in User Interfaces: Experience with the DeltaBlue Algorithm. Software-Practice \nand Experi\u00adence 23 (5): 529-566, May 1993. [SW92] Amitabh Srivastava and David Wall. A Practical System \nfor Intermodule Code Optimization at Link-Time. DEC WRL Research Report 92/6, December 1992. [US87] David \nUngar and Randall B. Smith. SELF The Power of Simplicity. In 00PSLA 87 Conference Proceedings, p. 227-241, \nOrlando, FL, October 1987. Published as SIGPLAN Notices 22(12), December 1987. Also published in Lisp \nand Symbolic Computation 4(3), Kluwer Academic Publishers, June 1991. [Wal191] David Wall. Predicting \nProgram Behavior Using Real or Estimated Profiles. In Proceedings of the SIGPL4N 91 Conference on Programming \nLanguage Design and Implementation, p. 59-70, Toronto, Canada, June 1991. Published as SIGPLAN Notices \n26(6), June 1991. 335  Appendix: Detailed Data II SELF-93 Benchmruk unoptnnized SELF-91 SELF-93 nofeedback \n1!1 1 Benchmnrk Sizea Description 500 two-wayconstraintsolver[SM+93]developedatthe Univemty of Washington \ni e &#38; e 3 PrimMnker 11(s3progrurugenerating glue stubsfor externalprimi\u00ad$ tivescallablefrom SELF \n~ ~ Richards 400 simple operating system simulator originally written . in BCPL by Martin Rkhards CecilCOmp \n11,500 Cecil-to-CcompdercompdingtheFibonaccifunc\u00adtion (thecompilersharesabout80%of Itscodewith theinterpreter,CecilInt) \nCeciUnt 9,000 interpreter for the Cecil Iauguage [Cha93] running a short Cecil test program Mnxrgo 7,000 \nautomaticallygeneratedIexerlprirserfor ANSI C, parsinga 700-lnreC file Ty@rf 8,600typeinferenceforSELF[APS93] \nUI1 15,2tN prototy euserinterfaceusinganimationtechniques [CU93]! U13 4,000 experimental3D userinterface \n Table A-1: Benchmark programs a Lines of code (excluding blank lines und comments). b ~lme for bO~ UI1 \n~d u13 excludes *e time spent In graphics primitives execuuon ume (ins) enchm k~ CecilCOmp 1,348 953 \n1,144 CecilInt 2,035 1,085 2,026 DeltaBlue 744 210 687 Mango 2,423 1,526 2,292 PrimMaker 2,520 1,227 \n2,279 Richards 922 591 693 Typeinf 1,448 769 1,388 IUI1 I 716I 686I 645I IU13 I 656I 528I 571I  Table \nA-2: Execution times The execution times of the above benchmarks were kept relatively short to allow \neasy simulation. To make sure that the small inputs do not distort the performance figures, we measured \nthree of the benchmarks with larger inputs. Table A-3 shows that the speedups achieved by type feedback \nare very similar to the speedups with smaller inputs. I, 1) enchm k- \\I CecilCOmp-2 97,21 71,51 1361 \n1,41 CecilIut-2 38.5 21.9 176 1.88 Mango-2 18,5 11,6 1,59 1.59 Table A-3: Performance of long-running \nbenchmarks a computedfrom thedatain Table A-2 Table A-4: Number of dynamically-dispatched calls PrirrrMaler \n3,934,308 819,277 76,273 602,217 Rich~ds 6,962,721 839,478 151,819 888,817 Typeinf 2,363,131 288,982 \n101,858 293,815 UI1 1,727,021 256,573 213,145 288,176 U13 1,274,863 274,262 101,884 301,344 execution \ntime (ins) System Richards DeltaBlue SELF-93 591 210 Smalltalk 2,580 600a C++ (all virtuals) 546 149 \nC++ (fin virrnals) 249 87 Lisp 2,010a NIA =ia=l Table A-5: Performance of other systems a elapsed time \n(see text) System H II SELF-93 11,3] 39.9 Table A-6: Size of compiled code 336  \n\t\t\t", "proc_id": "178243", "abstract": "", "authors": [{"name": "Urs H&#246;lzle", "author_profile_id": "81100400656", "affiliation": "Stanford Univ., Stanford, CA", "person_id": "P286890", "email_address": "", "orcid_id": ""}, {"name": "David Ungar", "author_profile_id": "81100365263", "affiliation": "Sun Microsystems Labs, Mountain View, CA", "person_id": "P64183", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/178243.178478", "year": "1994", "article_id": "178478", "conference": "PLDI", "title": "Optimizing dynamically-dispatched calls with run-time type feedback", "url": "http://dl.acm.org/citation.cfm?id=178478"}