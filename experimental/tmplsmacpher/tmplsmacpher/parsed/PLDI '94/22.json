{"article_publication_date": "06-01-1994", "fulltext": "\n Register Allocation over the Program Dependence Graph * Cindy Norris Lori. L. Pollock norris@udel.edu \npollock@hdel.edu Department of Computer and Information Sciences University of Delaware Newark, DE, 19716 \n(302)831-1953 Abstract This paper describes RAP, a Register Allocator that allocates regasters over the \nProgram Dependence Graph (PDG) representation of a program in a hierarchical manner. The PDG program \nrepresentation has been used successfully for scalar optimization, the detection and improvement of \nparallelism for vector machines, multtple processor machines, and machines that ex\u00adhibit instruction \nlevel parallelism, as well as debugging, the integration of different versions of a program, and translation \nof imperative programs for data jlow ma\u00adchines. By basing register allocation on the PDG, the register \nallocation phase may be more easily integrated and intertwined with other optimization analyses and transformations. \nIn addition, the advantages of a hi\u00aderarchical approach to global register allocation can be attained \nwithout constructing an additional structure used solely for register allocation. Our experimental results \nhave shown that on average, code allocated reg\u00adwters via RAP executed 2.7% faster than code allocated \nregisters via a standard global regtster allocator. 1 Introduction The Program Dependence Graph (PDG) \nrepresenta\u00adtion of a program has been used successfully as the ba\u00adsis for various scalar optimizations \n[16, 25, 23] as well as for detecting and improving parallelization for vector machines [28, 5], multiple \nprocessor machines [30, 3], and architectures that exhibit instruction level paral\u00adlelism [19, 7, 2]. \nVariations of the PDG have also been used for debugging and integrating different versions of *This work \nwas partially supported by NSF under grant CCR\u00ad9300212. Permission to co y without fee all or part of \nthis material is granted provid J that the copies are not made or distributed for direct commercial advantage, \nthe ACM copyright notice and the title of the publication and its date appear, and notice is given that \ncopying is by permission of the Association of Computing Machinery. To copy otherwise, or to republish, \nrequires a fee anctlor specific permission. SIGPWN 94-6/94 Orlando, Fiorfda USA @ 1994 ACM 0-89791 -662-W410006..$3.5O \na program via program slicing [29, 1, 18, 22, 21, 26], and to enable translation of imperative programs \nfor data flow machines and demand driven graph reduc\u00aders [4]. This paper describes a register allocator \nbased on the PDG and presents experimental evidence that shows how it usually outperforms a traditional \nglobal graph coloring register allocator. Our overall research interests have been directed to\u00adward the \nproblem of the sometimes conflicting goals of various tasks of optimizing and parallelizing compilers. \nIn particular, we have been investigating ways of pro\u00adviding cooperation between a register allocator \nand an instruction scheduler [24]. Our original motivation for building a register allocator based on \nthe PDG was to have a common program representation for both the register allocator and global instruction \nscheduler, as a first step towards integrating these two phases. The PDG provides a natural representation \nfor scheduling across basic block boundaries, and thus several global instruction scheduling methods \nhave been expressed as transformations over the PDG [6, 19, 2], for instance, region scheduling [19] \nand software pipelining [2]. How\u00adever, we could not find any indication in the literature of a register \nallocation technique that was based on the PDG. The widespread use of the PDG representation can be attributed \nto the fact that it represents both data and control dependence in a single program represen\u00adtation and \nit expresses only the essential partial or\u00addering of statements and predicates in a program that must \nbe followed in order to preserve the semantics of the original program. However, actual improvements \nin allocation over traditional global graph coloring reg\u00adister allocation schemes can be obtained by \nexploiting the hierarchical nature of the PDG. The hierarchical organization of the regions of a program \nthat have the same set of control dependence allows the register allo\u00adcator to process regions of the \nprogram separately, and more importantly, in a manner based on the program structure. When it is determined \nthat a variable needs to be spilled within a region, it may be possible to spill the variable only locally, \nwithout spilling it throughout the program. For example, a variable may be assigned to register RI in \none region, register R2 in another region, and spilled in another region. Like other ap\u00adproaches [12, \n20], a space savings can be obtained by performing register allocation for code segments sepa\u00adrately, \nwhich causes smaller interference graphs to be constructed than one interference graph for the whole \nprogram. The PDG depicts the hierarchical control re\u00adgions of a program making it easier to place spill \ncode in the less frequently executed regions of the program than standard global register allocation \ntechniques. This paper describes RAP, a Register Allocator that allocates registers over the PDG representation \nof a program in a hierarchical manner. The register alloca\u00adtion of each region is performed in a bottom \nup pass over the PDG, using an enhanced version of Chaitin s graph coloring algorithm for each region \n[14, 9]. On a top down pass, RAP attempts to move loads and stores out of loops. A third pass over the \nPDG re\u00admoves unnecessary loads and stores created by the hi\u00aderarchical register allocation process. Our \nexperimen\u00adtal results have demonstrated that on average, code allocated registers via RAP executed 2.7% \nfaster than code allocated registers via a standard global register allocator. The quality of the allocation \nby RAP is due primar\u00adily to the initial focus on smaller segments (regions) of the code, and then building \nupon these allocations to obtain allocations for larger segments (parent regions) of the code. Several \nother researchers have presented register allocation methods directed specifically toward improving the \noverall allocation by considering both lo\u00adcal register needs and global register usage in making register \nallocation and assignment decisions. Proebst\u00ading and Fischer [27] developed a probabilistic approach \nin which local allocation is followed by probabilistic global allocation performed iteratively from inner \nto outer loops. Although the register allocator performed very well on the Stanford Benchmarks, compile \ntime is significantly increased by this method, primarily due to the frequent recomputation of probabilities. \nA hierarchical approach to global register allocation was presented by Callahan and Koblenz [12] in which \nthey build a tree of tiles which covers the basic blocks of the control flow graph. In a bottom Up pass, \na graph coloring allocation of each tile is performed, sum\u00admarizing the allocation of subtiles within \nthe interfer\u00adence graph of the parent tile. A top down pass then binds virtual registers to physical \nregisters and inserts spill code. Although the Callahan/Koblenz allocation method and RAP both take a \nhierarchical approach, the two research efforts differ in several ways. Most importantly, RAP performs \nregister allocation over the PDG, rather than creating an additional tile structure that is used only \nfor register allocation. By using a common compiler representation, RAP can be more easily implemented \nin existing compilers and possibly intertwined with other compile time optimization anal\u00ad yses and transformations. \nThe underlying target architectures of the alloca\u00adtors are different, as RAP is designed to support a \nload/store architecture, while the Callahan/Koblenz allocation method presumes that operands can be ac\u00adcessed \nfrom memory. Thus, RAP is more suitable for RISC and pipelined architectures than the direct mem\u00adory \naccess of the Callahan/ Koblenz model. A register allocator for a load/store architecture needs to be \nable to allocate a register for every variable, even if it is only for a short time while the variable \nis being accessed. Spill code insertion consists of inserting loads immedi\u00adately before variable uses \nand stores immediately after variable definitions. In contrast, a register allocator for an architecture \nwhere variables can be accessed from memory can avoid allocating a register to a variable for a segment \nof code, but needs to insert loads and stores on the boundaries of code segments that differ in their \nallocation for that variable. The major differ\u00adences in the two register allocation algorithms are in \nthe approach to allocating within code segments, the time at which register allocation and assignment \nactu\u00adally take place, and spill cost calculations. No experi\u00admental results have been reported on the \nperformance of the Callahan/Koblenz allocator. We begin with a background section which describes the \nproblem of register allocation and gives a brief de\u00adscription of the Program Dependence Graph interme\u00ad \ndiate representation. Section 3 describes each of the phases of RAP. Our experimental study and results \nare presented in Section 4. 2 Background 2.1 Register Allocation The goal of register allocation is \nto map variables in an intermediate language to physical registers in order to minimize the number of \naccesses to mem\u00adory during program execution. Most recent research casts register allocation as a graph \ncoloring problem [11, 15,14,12,9,8, 13]. These techniques involve build\u00ading an interference graph in \nwhich nodes of the graph represent variables or virtual registers, and edges be\u00adtween two nodes indicate \nthat the corresponding vari\u00adables cannot be mapped to the same physical register because their values \nwill coexist during program exe\u00adcution. The register allocation is determined by color\u00ading the nodes \nof the graph with no more than it colors where k is the number of physical registers. Two nodes cannot \nbe assigned the same color if they are connected by an edge. I:i:=l 2: while (i< 10) { 3: j=i+l 4: ifQ=7) \n5: ... else 6: ... i=i+l7: }I 8: ... -> Data Dependence > Control Dependence ~~ Figure 1: Program Dependence \nGraph If a coloring cannot be found, a variable is spilled to memory. How the spilling is performed \nis depen\u00addent upon the target architecture and the actual global register allocation technique. A RISC \narchitecture re\u00adquires that operands be accessed from memory and thus a load is inserted before each \nuse of the spilled variable and a store after each definition. When the ar\u00adchitecture allows operands \nto be accessed out of mem\u00adory, spilling can consist of changing each reference to the variable to be \na reference to memory. 2.2 Program Dependence Graphs The Program Dependence Graph for a program is a \ndirected graph that represents the relevant control and data dependence between statements in the program. \nThe nodes of the graph are statements and predicate expressions that occur in the program. An edge repre\u00adsents \neither a control dependence or a data dependence among program components. A data dependence edge from \nz to y denotes the fact that executing y before z could alter the program s semantics because z and y \nmay reference the same memory location with at least one of them writing to that location. A control \ndepen\u00addence edge from p to n denotes the fact that predicate p immediately controls the execution of \nthe statement n. The source of a control dependence edge is either the entry node of the program or a \npredicate. A control dependence edge from a predicate node is labeled with either true or false, indicating \nthe value of the predi\u00adcate under which the statement at the sink of the edge will be executed. Special \nnodes called region nodes are inserted into the graph to summarize the set of control conditions for \na node and to group all of the nodes that are ex\u00adecuted under the same control conditions together as \nsuccessors of the same region node. For a given node n, each subset of its control dependence that is \ncom\u00admon with control dependence of another node, say m, is factored out, and a region node R is created \nto rep\u00adresent this control dependence subset. The common control dependence edges to n and m are replaced \nby edges R+n and R--+m. Each region node represents a set of control conditions, and after region nodes \nare inserted, each predicate node has at most one true out\u00ad going edge and one false outgoing edge. Thus, \nregion nocle insertio n creates a hierarchical organization of the PDG. Figure 1 shows a program segment \nand its PDG rep\u00adresentation. The data dependence due to the definition of z in instruction 1 and the \nuse of i in instruction 3 is represented by a directed edge from node 1 to node 3. The self dependence \ndue to the increment of scalar vari\u00adable z in instruction 7 is represented by the cyclic edge on node \n7. The WHILE loop condition and IF state\u00adment are represented by predicate nodes P 1 and P2, respectively. \nThe region node R1 represents the con\u00adtrol conditions on program entry, region node R2 rep\u00adresents the \nconditions on entering the loop or looping back to possibly execute another iteration of the loop, R3 \nrepresents the conditions under which the body of the loop is executed, R4 represents the THEN branch, \nand R5 represents the ELSE branch. 3 RAP The input to the RAP register allocator consists of the PDG \nwith attached low-level intermediate code state\u00adments which were generated assuming an infinite num\u00adber \nof available registers. Definitions and uses in the in\u00adtermediate code are references to virtual registers. \nThe goal of RAP is to map the unlimited number of virtual registers onto the limited number of physical \nregisters. RAP consists of three phases. The first phase of the al\u00adlocator computes the register allocation \nfor each region in a bottom up pass over the PDG. The second phase moves loads and stores out of loops \nwhere possible. The final phase is a local optimization that eliminates loads and stores in basic blocks \nwhen possible. The following subsections explain each of these phases. 3. I Bottom Up Allocation Instead \nof building the interference graph for the entire procedure at once as is done by most global register \nal\u00ad locators, RAP begins by building an interference graph for lea~regions. During a bottom up pass of \nthe PDG, it builds the interference graphs of each region, with  procedure rap(V, Gv ){ // Input: region \nnode V // Output: interference graph, Gv, for region node V R = set of m subregions of V spill = true \nwhile (spill) { add-region.conflicts(V, Gv ) add_subregion-conflicts(V, Gv ) talc-spill-costs(V, Gv \n) colorstack = simplify(Gv ) spilllist = color(Gv, colorstack) if (spill-list is empty){ combine spill \n= false fori= 1to m{ if (loop-uode(l?, ) == false) delete(GR, ) } !Ise insertspilI-code(V, spill-list) \n }} Figure 2: RAP on a region the interference graphs of subregions incorporate ed into the parent \nregion s graph. The interference graph for the entry region of the PDG has nodes to represent every virtual \nregister referenced in the PDG and the register assignment is done at this level. A region refers to \na region node in the PDG and all of its control dependence successors. The parent region refers to only \nthe topmost region node of the region. A subregion of the parent region refers to a subregion node and \nall of its control dependence successors. For example, in Figure 1, RI is the parent region of the region \nconsisting of RI, R2 , R3, R~, and R5. The subregion of this region consists of R,$?, R3, R4, and R5. \nAlso, R3 is the parent region of the region consist\u00ading of R3, R4, and R5. The subregions of this region \nare R4 and R5. A virtual register is local to a region if all references to that virtual register can \nbe found in intermediate code within the region; otherwise, the virtual register is global to that region. \nFigure 2 presents the RAP allocation procedure exe\u00adcuted on a region of the PDG. The add-region_ conflicts \nroutine adds nodes and edges to the interference graph for the region by looking at statements in the \nparent region only. The next routine, add-subregion-conflicts, incorporates the interference graphs of \nthe subregions of the region. The talc-spill-cost routine at t aches a spill cost to each node in the \nresulting interference graph. The simplify routine removes nodes from the interfer\u00ad ence graph and pushes \nthem onto a stack for coloring and color pops nodes from the stack and assigns each node a color different \nfrom the color assigned to each of its neighbors. The nodes that cannot be colored are returned by the \ncolor routine in a list of nodes to be spilled. If the list of nodes to be spilled is empty, then a new \ninterference graph is created for the region by combining those nodes which have been assigned the same \ncolor into a single node. The interference graphs of subregions, except those which represent a loop, \ncan be deleted. The interference graph of the region is saved for incorporation into the interference \ngraph of its parent region. If any nodes are marked for spilling, spill code is inserted and the interference \ngraph is rebuilt. The following subsecti ons describe each of these routines in more detail. 3.1.1 Building \nthe Interference Graph The interference graph for a region is built in two steps, add-region_ conj7icts \nand add-subregion-conflicts. The add-region-conflicts procedure builds the part of the interference graph \nfor the parent region interfer\u00adences, without concern for the subregions. This is sim\u00adilar to standard \nglobal register allocation techniques [14], except that RAP adds an interference between any two virtual \nregisters that are live on entrance to the parent region and referenced (either used or de\u00adfined) within \nthe region. These interferences need not be added in the standard technique for a given ba\u00adsic block \nbecause they will eventually be added as all basic blocks are examined. A virtual register that is not \nreferenced within the parent region will not be added to the interference graph by add-region-conflicts \neven though the virtual register may actually interfere with each node in the interference graph. Omitting \nthese unreferenced virtual registers insures that refer\u00adenced virtual registers are given priority when \ncoloring. An unreferenced virtual register, although possibly live within the parent region, will be \none of the first candi\u00addates for spilling. Figure 3 contains a section of code and its corre\u00adsponding \nPDG. The statements S1... S5 contain ref\u00aderences to virtual registers a, b, c, d, and e. Figure 3 (c) \ncontains the interference graph built for the parent of the region consisting of Rl, R2, and R3. The \ninter\u00adference graph consists of a node for each virtual regis\u00adt er referenced in the parent region. An \nedge between two nodes indicates that they may not be assigned the same register; for example, a and \nc cannot be assigned the same register because they are simultaneously live. Note that the interference \ngraph does not contain a node for virtual register d although d interferes with each node in the parent \ninterference graph. The add.subregion-con flicts procedure, shown in Fig\u00adure 4, incorporates the interferences \nof the subregions into the parent region s interference graph. The com\u00adbine step of each of the subregions \nallocation results in a final interference graph for each subregion that contains no more than k nodes \nwhere k is the number of physical registers. Thus, each node in the inter\u00adSl: a=b RI 0(3 @-----o S2: \nc=a+c if (P) (a) (b) oP S3: a=b+ S1 S2 d% else { \\ S4: e=10 R2R3 a S5: a=e S6: a=a+ bc } CPb S3s4 \nS5S6 ii?% 6 (c) (d) code pdg interference ~aphs Figure 3: Building the interference graph for a region \nprocedure add-subregion.conflicts(V, Gv ){ Ii Input: parent region node V, // interference graph, Gv, \nof V 11 Output: Gv modified to be interference // graph of region R = set of m subregions of V Vars \n= set of Virtual Registers referenced in either V or some R~ Livein = set of Virtual Registers live on \nentrance to V for each v~ 6 Vars where v~ 3 GV if ?Jk~ Livein add interference between Uk and all v 6 \nGv to Gv [~i=ltom for eachvcGR,{ add v s interferences c GR, to Gv for each vk C Vars where Uk 3 GR, \nif vk is live on entrance to R, add interference between vk and v to Gv Figure 4: Adding interferences \nto graph for region V ference graph may represent several virtual registers which RAP has determined \ncan be allocated to the same physical register in the subregion. Each node and its conflicts in an interference \ngraph of a subregion is added to the interference graph of the parent region, possibly by combining the \nsubregion node with one of the parent s nodes if the nodes cor\u00adrespond to the same virtual register. \nAll virtual regis\u00adters referenced in the parent region and any subregions of this region will be present \nin the new interference graph. If a virtual register is live in the parent region, not referenced in \nthe parent region, but referenced in one of the subregions, then a conflict must be added between the \nvirtual register and every virtual register referenced in the parent region. 1 Similarly, if a virtual \nregister is live, but not referenced in a subregion, but is referenced in the parent region or one of \nthe other sub\u00adregions, then a conflict must be added between that virtual register and all virtual registers \nreferenced in the subregion. After all of these conflicts are added, the interference graph is complete \nfor the parent re\u00adgion. Figures 3 (a) and 3 (b) show the interference graphs for the subregions R,2 and \nR3 of the region consisting of RI, R2? and R3. Figure 3 (b), which contains the inter\u00adference graph for \nR3, contains a single node for virtual registers a and e because the coloring routine colored these two \nvirtual registers the same color. Note that a and b in Figure 3 (a), which contains the interference \ngraph for R2, were not colored the same color because there are uses of both a and b outside of the subregion. \nAlthough a and b do not interfere in the subregion R2, they do interfere in the parent region and it \nwould be incorrect to assign them the same color and thus the same register. Figure 3 (d) contains the \ncomplete interference graph for the region consisting of RI, R2, and R3. The inter\u00adference graph of the \nregion is constructed by combining the interference graphs of the subregions and the inter\u00adference graph \nof the parent parent. For example, the node for virtual registers a and e in Figure 3 (b) is combined \nwith the node for virtual register a in 3 (a) and the node for virtual register a in the 3 (c). The edges \nin each interference graph are added to the in\u00ad 1The general PD G structure requires more analysis than \nthis to determine whether each of those interferences is necessary, For our particular implementation \nof the PDG (described in the experimental section), this is sufficient. terference graph for the whole \nregion. In addition to this straightforward creation of the region interference graph, interference edges \nmust be added correspond\u00ading to those virtual registers which are live in a region but not accessed init. \nFor example, an interference is added between d and each virtual register referenced in the parent region \nbecause d is live, but not accessed, in the parent region. 3.1.2 Calculating Spill Costs Figure 5givesan \noverview of the talc.spill-cosisalgo\u00adrithm. Initially, the algorithm determines which vir\u00adtual registers \nin the region have already been spilled in the region or are completely local to a subregion. Spilling \nthese virtual registers will not help to make the graph colorable (i.e., will not eliminate any inter\u00adferences) \nandthus thespillcost is set to be quite high to prevent them from being spilled in this region. For example, \nin Figure 3 (d), ignoring the combining of a with e, the virtual register ewhich islocal to subregion \nR3 interferes only with the virtual register dwhichis global tothe parent region. Spilling ewould create \ntwo new live ranges inthesubregion byinserting a store af\u00adter the definition of e and a load before the \nuse of e. Both of these live ranges would still interfere with d. Eliminating the interference between \nd and e can only be achieved by spilling d. If the virtual register has not been spilled and is not completely \nlocal to a subregion, the spill cost of a node is calculated by examining the code in the parent re\u00adgion. \nFor each virtual register, the spill cost is initial\u00adized to be the number of definitions and uses of \nthe virtual register in the parent region. This corresponds to placing a load before each use and a store \nafter each definition in the intermediate code of the parent region. In addition to spilling a virtual \nregister in the parent region, spill code may need to be inserted in a sub\u00adregion if the virtual register \nis live on entrance to or exit from a subregion, Although each node in a sub\u00adregion s interference graph \nwaa colorable at that level, incorporating the node into the interference graph of the parent region \nmay now cause it to be uncolorable. This is because the virtual register represented by the node may \nbe referenced in more than one subregion or in both the subregion and the parent region. Since these \nnodes represent the same virtual register, they are combined in the parent s interference graph possi\u00adbly \nincreasing the number of conflicts. Thus, the next loop of talc.spill-cosis determines whether a spill \nof a virtual register will require a load and/or a store to be inserted into a subregion and increments \nthe spill cost accordingly. In the last two loops of talc-spill-costs, the degree of each node is incremented \nappropriately, and the spill cost of each node is divided by its degree. The degree procedure talc-spill_costs(V, \nGv ){ // Input: parent region node V, // interference graph Gv // Output: spill cost of each node in \nGv NGV = set of nodes of Gv R = set of m subregions of V S = set of n statements in region V LiveinR, \n= set of virtual registers live on entrance to region R, and used in region R, LiveoutR, = Set Of VirtUd \nregkter. he on eXit frOm region R; and defined in region R; degree(node) = number of interferences of \nnode //avoid spilling those local to subregion or //already spilled for each node c NGV do{ if 3RI such \nthat V v c node, v is local to R, spill-cost (node) = 999999 else if some v 6 node is spilled in V spill-cost \n(node) = 999999 else spill-cost (node) = O } //initialize spill cost to #uses + #clefs in region fori=l \ntondo for each v referenced in S, { node = node E Gv representing v spill-cost (node) ++ 1 //kcrement \nfor load/store on boundaries fori=l tomdo for each node E NGV do{ if 3 v 6 node such that v C J%e;nR~ \nspill-costs(node) ++ if 3 v E node such that v 6 ,hJeoutR, spill-costs(node) ++ } //calculate degrees \nfor each node, c Gv for each node~ c Gv, ~# j if (3 no edge between nodet and nodej ) and (3vi 6 node, \nand 3VJ c nodej such that vi and VI are both global to v) { degree(node:)++ degree(node~)++ } for each \nnode c NGV do spilkost(node) = spill-cost(node) / degree(node) } Figure 5: Calculating spill costs of \neach node in the interference graph is equal to the number of neighbors of that node. When assigning \nthe node a color, the node cannot be assigned the same color as any of its neighbors. In addition, if \nthe virtual register represented by the node is global to a region, it cannot be assigned the same color \nas another global vir\u00adtual register. Although the two global virtual registers may not interfere in the \nparent region, they could pos\u00adsibly interfere in another region thus prohibiting them from being allocated \nto the same physical register. For this reason, the degrees of the two nodes are each in\u00adcremented by \none if both are global to this region, but do not interfere in the region.  3.1.3 Coloring the Interference \nGraph The interference graph is simplified by removing each node in the interference graph and pushing \nit onto a stack for coloring. The simplify phase repeatedly re\u00admoves a node with degree less than the \nnumber of phys\u00adical registers, k, or if none are available, a node with the least spill cost, and pushes \nit onto the stack. The nodes of the interference graph are colored by popping each node off of the stack \nand assigning it a color dif\u00adferent from the color assigned to each of its neighbors. If a node corresponds \nto a global virtual register, then this virtual register cannot be colored the same color as any other \nglobal virtual register, although it may be colored the same color as a local virtual register. Because \nof the order in which the nodes are pushed onto the stack, the nodes with the most expensive spill cost \nare colored first. If a node cannot be colored, it is added to a list of nodes to be spilled. If a node \ncannot be found with degree less thank and instead a node with least spill cost has to be removed, then \nChaitin s original technique [14] marks this node to be spilled and not pushed on the stack. Instead, \nthe technique used in RAP, which delays the identification of nodes to be spilled, is an enhancement \nto Chaitin s original coloring technique proposed by Briggs, Cooper, Kennedy and Torczon [9]. By deferring \nthe spilling un\u00adtil the nodes are popped from the stack, it may be possible to color a node which Chaitin \ns method would have spilled for two reasons. First, a neighbor of the node may have been spilled thus \npossibly leaving a color available for this node or second, two neighbors of the node may have been assigned \nthe same color. The set of nodes spilled by this method is a subset of the nodes spilled by Chaitin s \nmethod. 3.1.4 Inserting Spill Code The insert_spilLcode step removes each node from the list of nodes \nto be spilled and first adds spill code to the intermediate code of the parent region and then to the \nintermediate code of each subregion. For each spilled virtual register, a load is added before each of \nits uses and a store is added after each of its definitions in the intermediate code of the parent region. \nThe virtual register is then renamed in the parent region. Next, the spill code is inserted into each \nsubregion. If the virtual register is live on entrance to the subregion, a load is inserted before the \nfirst use in the subregion. In addition, a store is inserted after each definition of the virtual register \nwhich has a corresponding use outside of the subregion. The virtual register is then renamed, making \nit completely local to the subregion. This only inserts spill code within the parent region and its subregions, \nbut it may also be necessary to in\u00adsert some spill code outside of the region. For example, suppose a \nload is placed before the use of a spilled vir\u00adtual register and the definition corresponding to that \nuse is outside the region. A store must be placed af\u00adter this definition; otherwise, the load will not \nretrieve the correct value. Similarly, if a store is placed af\u00adter a definition which has a corresponding \nuse outside of the region, then a load must be placed before this use. The insertion of spill code outside \nof the region is done recursively so that each stored definition has a load placed prior to the corresponding \nuse and each loaded use has a store placed after the corresponding definition. This technique does not \nplace loads and stores after every use and definition of the virtual reg\u00adister since (1) the virtual \nregister could correspond to more than one live range and (2) a load is only placed before the first \nuse in a subregion, and a store is only placed after definitions which have uses outside of the subregion. \nThis can cause spill code to be inserted into a region in which the virtual register was not actually \nspilled by the allocation; we attempt to move some of this spill code to less frequently executed segments \n(dr eliminate it entirely) in a later phase. 3.1.5 Combining After the interference graph for the parent \nregion has been colored, the same color nodes of the interference graph are combined and this interference \ngraph is saved for incorporation into the interference graph of its par\u00adent region. Although there may \nbe many virtual reg\u00adisters referenced in the region, the final interference graph contains at most k \nnodes, where k is the number of physical registers. The interference graphs of each subregion may now \nbe deleted unless the subregion rep\u00adresents the top region node of a loop. The interference graph of \na loop region is saved for spill code movement. 3.2 Spill Code Movement After the allocation phase, RAP \nattempts to move loads and stores outside of loops which were possibly inserted there because the virtual \nregister was spilled (1) ldm r2, 20 (4) stm 20, r2 ... no redef of r2. .. ... no redef of r2. .. ldm \nr2, 20 stm 20, r2 (2) ldm r2, 20 (5) stm 20, r2 . . . no redef of r2, . . . . .no redef of r2. . . ldm \nr3, 20 mv r3, r2 . . . no redef of r3. . . (3) ldm r2, 20 stm 20, r3 . . .no redef of r2. . . stm 20, \nr2 Figure 6: Elimination of unnecessary loads and stores in another region. This separate phase to move \nthe spill code out of loops is simpler than preventing the insertion of the spill code inside the loop \nduring the initial allocation phase. The loads and stores of a vir\u00adtual register may be removed from \na loop if the virtual register was not combined with another virtual regis\u00adter in the region. This information \nis available in the interference graph for the loop region. The spill code movement phase proceeds in \na top down traversal of the PDG so that moving loads and stores outside of the entire loop nest is attempted \nbe\u00adfore moving the loads and stores out of inner loops of that nest. Special spill nodes are created \nin the PDG to hold the moved spill code. A load of a virtual reg\u00adister must be inserted in the spill \nnode immediately prior to the loop if the first reference in the loop is a use. Similarly, a store must \nbe inserted in the spill node immediately following the exit of the loop if the loop contains a definition \nof the virtual register with a future use outside the loop. 3.3 Optimization In the PDG used by RAP, \nthere is a region node corre\u00adsponding to each source code statement. This perlmits the interference graph \nat each of these levels to be quite small, but can also cause a basic block to be divided into several \nsubregions. A virtual register which is ref\u00aderenced in these subregions and spilled in the parent region \nmay cause the virtual register to be renamed in each of these subregions. If the renamed virtual reg\u00adisters \nare assigned to the same physical register, then unnecessary spill code is created in the basic block. \nThe final phase of RAP performs a local optimization which eliminates these unnecessary loads and stores. \nThe last phase of RAP examines the spill code in\u00adserted into each basic block and removes spill code \nof the forms noted in Figure 6. In this figure, ldm is a load direct and stm is a store direct instruction. \nThe register references refer to the assigned physical regis\u00adters. In (1), the second Zdm can be eliminated \nbecause register r2 still contains the value loaded from memory. In all of the examples except (2), the \nsecond memory reference can be eliminated. In (2), the Zdm can be re\u00ad placed by a copy statement which \ncopies the contents of r2 into r3. 4 Experimental Study The performance of RAP has been compared to \nthe performance of a conventional global register allocator which we call GRA. GRA is basically an implemen\u00adtation \nof Chaitin s global register allocator with two exceptions: (1) The enhancement suggested by Briggs et. \nal. [9] has been incorporated. (2) No coalescing or rematerialization is done [14, 11]. These modifications \nwere made to GRA in order to present a fair compar\u00adison with the current version of our prototype imple\u00admentation \nof RAP. We have incorporated the Briggs enhancement into RAP, but RAP currently does not include coalescing \nin its traditional form or rematerial\u00adization. The front end for RAP is the pdgcc compiler, devel\u00adoped \nat the University of Pittsburgh [17], which accepts C source code M input and outputs the correspond\u00ading \nPDG, RAP inputs the PDG representation of the C program and first generates and attaches low-level intermediate \ncode to the corresponding region nodes. The intermediate code representation is iloc developed at Rice \nUniversity for the development of optimizing compilers [1 O]. RAP performs register allocation over the \nPDG representation and generates code with a cor\u00adrect register assignment. Alternatively, RAP can sim\u00adply \noutput the unallocated iloc code which is then used as input to GRA for experimental comparison. An iloc \ninterpreter is used to count the number of cycles re\u00adquired to execute the code. For this study, we assume \nthat each instruction takes one cycle to execute. Performance measurements of RAP and GRA have been \ntaken for 13 of the Livermore Loops, the cLinpack routines, implement ations of heapsort, hanoi, sieve \nand some of the Stanford routines. Table 1 presents the results of these experiments for register set \nsizes 3, 5, 7 and 9. The tot columns show the percent\u00adage decrease in the total executed cycles, calculated \nas (cycies(GRA) cycZes(RAP))/cycies( GRA) where cycles is the number of cycles required to execute the \ncode as determined by the iloc interpreter. The ld and st columns indicate what portion of the percentage \nis due to a change in the number of loads executed and stores executed, respectively. The remaining portion \ncan be attributed to an increase or decrease in the number of copy statements executed. The -0.0 entries \nin the table indicate a very small negative percentage. The +0.0 entries indicate a very small positive \npercent\u00adage. An entry is blank in the table if the allocated code does not contain spill code. n Benchmark \nII Number of Registers n H II tot I ld I St II tot I ld I St II tot I ld I St II tot I ld I St H 1 H \nIi II 100~6 100D7 100;13 100;14 ClinDack II II II It 11.2 3.8 5.5 2.9 I I I [ 6.3 0.0 1.2 1.8 I I I I \n1.0 +0.0 -0.1 -1.4 II II II II 5.4 2.6 3.6 -2.5 I I I 2.9 1.5 -0.0 -0.0 -.. i 0.0 \\ ).0 I -1.5 I -2.1 \nI -0.0 -1.6 0.6 -1.5 -ii 0.0 0.5 -1.4 8.8 1.6 -3.3 3.8 -1.3 3.8 0.0 !1 --\u00ad( II 5.3 II 5.4 I -H--z5-1 \n+0.0 II 4.5 I 0.2 I I I I -0.0 -0.0 -0.7 -0.6 -0.0 8.8 5.2 5.3 3.9 IH3.8 5.1 .s 2 II ---I 5.3 II 5.4 \nI -tt-%Fi II 2.3 I 0.0 1.2 -2.9 I -0.7 I I -Rim I -0.0 1 [ H II .62 II -.. 77 1 . . 77 . 1 11 ..\u00adnn 1 \nI 5.0 II 1.7 I -4.5 I 3.6 II -3.8 I -8.3 I 0.8 II -3.2 I -8.4 I 0.9 H hsort Hanoi main In ov Nsieve sei \nve Stanford H initmatrix innerproduct intmm permute swap initialize u perm fit place trial remove Duzzle \nqueens try doit u Average II -0.3 -11.3 8.5. . 32.6 -4.7 -5.5 -19.2 14.1 9.1 8.0 7.5 -20.9 -9.4 12.9 \n-13.8 5.3 12.3 -14.4 3.5 1.7 I I 1 I 2.7 -6.7 5.7. 14.6 -7.1 -8.2 -11.0 9.1 9.1 1.0 1.2 -12.6 -10.8 3.9 \n-13.6 -5.8 -8.7 0.1 I 1 \\ I I -3.8 -3.5 2.8--\u00ad4.9 -2.4 -0.1 -8.3 3.2 0.0 0.0 0.0 -8.3 -3.1 8.1 -4.6 -0.0 \n-5.2 0.0 II II 11 II 4.3 -10.4 1.2___ 14.4 -3.0 3.2 -3.1 2.6 11.1 7.7 6.5 -9.1 3.9 13.5 4.5 -1.8 12.3 \n-6.1 1.2 2.7 ] I 1 I I 0.5 -5.7 0.6 -8.3 0.1 -6.4 -13.4 -1.2 -1.9 -0.3 -6.3 -5.2 -22.0 ] I 1 I I 1 I \n1 0.4 -5.7 0.8 -0.1 0.0 -0.0 -0.4 -0.1 -0.2 -0.1 -0.2 .0.6 -0.1 II 4.5 II 8.3 II 12 11 ---II 3.4 II 6.1 \n0.0 -3.3 96 II 7.7 II --\u00ad11.1 6.5 -0.6 -8.1 13.8 -0.6 1.3 12.3 -25.2 3.8 2.6 I 1 I I I -0.8 -0.4 +0.0 \n:3.6 -6.8 -5.2 -11.8 -2.5 -5.4 -0.1 \u00ad-23.1 0.3 I I ! I I I 1.6 -2.4 0.0 -0.1 -0.0 -0.3 -1.3 -0.5 -0.3 \n-0.2 \u00ad-4.4 0.0 II 1! II II II 0.8 8.3 1?..\u00ad-5.2 6.3 0.0 3.6 96 7.7 . 11.1 6.5 5.1 -2.9 17.9 4.3 7.1 \n123 : -14.2 4.0 3.7 I 1 I I I -4.0 -3.9 -3.8 -7.0 -0.3 -0.0 \u00ad-14.8 0.2 -1.8 I 1 -2.6 I H -0.1 -1.4 -0.5 \n-I-n a ..\u00ad3 -1.8 1 0.0 ! Tablel: Percentage decrease in cycles executed For example, when 3 registers \nare available, the Liv\u00ad ermoreloop6 code allocated registers via RAP executes 11.2% faster than the \nsame code allocated registers via GRA. 6.3% of the 11.2% is due to a reduction in the number of loads \nexecuted and 1.0% of the 11 .2?Z0is due to the reduction in the number of stores executed. The remaining \n3.9% is due to the reduction in the number of copy statements executed. Although neither GRA nor RAP \ndoes coalescing, a copy statement in the unallo\u00adcated iloc code can be eliminated when both operands \nof the copy are allocated the same register. The last row in Table 1 indicates the average per\u00adcentage \ndecrease in the total number of cycles executed for each of the register set sizes. For register set \nsizes of 3, 5, 7, and 9 the average percent decrease is 1.7, 2.7, 2.6 and 3.7, respectively. The average \npercent decrease over all the experiments is 2.7. When the register set consists of 9 registers, the \npercentage decrease in the to\u00adtal number of cycles executed can be mostly attributed to a reduction in \nthe number of copy statements since little spilling occurs in these routines with a register set of that \nsize. For a register set size of 3, 25 of the 37 routines demonstrated a positive percentage decrease \nin the total number of cycles executed. For a regis\u00adter set size of 9, 30 of the 37 routines demonstrated \na positive percentage decrease. We found that although coalescing was not imple\u00admented, RAP was much \nbetter at eliminating copy statements than GRA. This was simply because each region in our PDG likely \nconsists of only a few iloc statements, This is due to the pdgcc compiler which creates a region node \nfor each C statement. The inter\u00adference graph for such a region would consist of only a few nodes. The \njirstjitcoloring technique that is used colors a node with the first available color that it can find \nthus maximizing register reuse. The combination of a small interference graph and the first fit coloring \ntechnique would often cause the two nodes correspond\u00ading to the operands of the copy to be assigned the \nsame color and thus the same register, effectively eliminating the copy statement. Implementing an explicit \ncoalesc\u00ad ing step in GRA and RAP is likely to increase the per\u00ad formance of each, but particularly, it \nshould improve the performance of GRA. Although good for coalescing, the small regions were not always \ngood for adding spill code. If a virtual reg\u00adister is spilled within a region, then a store is added \nafter each definition and a load is added before each use. In addition, a load is added before the first \nuse of the definition in each subregion containing a use of that virtual register. Having a large number \nof regions can cause an excess amount of spill code to be inserted. For example in Figure 7, our pdg \nimplementation would put statements S2 and S3 into the separate regions R2 and R3 as indicated, if S, \n2 and S3 were generated from two different source code statements, If virtual regis-S1: a= RI S2: =a \nS1 R3 CAb S3: =a S2 S3 ?)0 Figure 7: Effect of small regions on spilling ter a is spilled when coloring \nthe interference graph for the region with parent RI, then RAP inserts a load prior to the first use \nof a in subregions containing a use of the definition of a in R1. In this case, a load would be inserted \nprior to S2 and prior to S9. If the two statements were in the same subregion, then only a load before \nS2 would need to be inserted. For this rea\u00adson, we believe that a PDG implementation in which the region \ncontains more intermediate code statements than is possible in our PDG implementation would al\u00adlow RAP \nto decrease the amount of spill code inserted. In Figure 7, if RAP chooses to spill a while coloring \nthe interference graph for region RI, itwill insert as much spill code as GRA would when choosing to \nspill that variable. However, if R1 is the parent region node for a loop region, RAP may move the spill \ncode for a out of the region. A single load for a may be placed prior to the entrance of R1 and the two \nloads within the region can be eliminated. Thus, although RAP may add more spill code to subregions than \nis necessary, spill code cleanup may be able to eliminate some of that code. Some of the differences \nin the results of the allocators can be attributed simply to the difference in spill cost calculations. \nWhen RAP calculates the cost of spilling a node of the interference graph built for a particular region, \nit may be examining only a subset of the total uses and definitions of the variable. In contrast, GRA \ncalculates the spill cost by counting each use and defi\u00adnition of a variable in the whole procedure. \nOne of the inherent problems in this method of spill cost calcula\u00adtion is that spilling a node with the \ncheapest cost does not necessarily cause the interference graph to be col\u00adorable in the next iteration. \nThis can be a problem in GRA and in RAP. Upon analyzing our results closely, we found that occasionally \nthe spill cost caused RAP to make a smart decision resulting in less spilling than GRA and occasionally, \nvice-versa. Conclusions References In this work, we set out with the goal of developing a global register \nallocator based on the PDG represen\u00adtation of a program without sacrificing the quality of theresulting \nregister allocations. This paper described how we achieved this goal, and in fact, usually obtain improved \nallocations over traditional global graph col\u00adoring register allocators, by exploiting the region par\u00adtitioning \nof the PDG. Our experiments suggested some ways in which we could improve the performance of RAP. First, \nwe found that at least some of the improvement of RAP over GRA can be attributed to the ability of RAP \nto elim\u00adinate copy statements without actually performing an explicit coalescing step. However, we also \nfound rou\u00adtines in which GRA allocated code contained fewer copy statements than RAP allocated code. \nWe ex\u00adpect that the performance of RAP will be improved by implementing coalescing, and we are interested \nin comparing the results when coalescing is performed by both RAP and GRA. In addition, it is likely \nthat the performance of RAP could be improved by increasing the number of iloc statements within a region. \nWe are currently examin\u00ading the possibility of obtaining some flexibility on the size of the region for \nwhich an interference graph is constructed. Finally, as Figure 7 suggests, better place\u00adment of spill \ncode is desired. RAP currently attempts to move spill code out of loop regions, but moving spill code \nout of any subregion is also likely to reduce the amount of spill code executed. Unlike the hierarchi\u00adcal \ntile structure of Callahan and Koblenz [12], a re\u00adgion of a PDG may contain multiple exits making spill \ncode placement more difficult. The tile structure al\u00adlows placement of spill code in the single entrance \nand the single exit of the tile tree. Instead of attempting to identify the possibly multiple exits of \na region, RAP in\u00adserts spill code within the region and attempts to move the spill code out of the region \nin a later phase. Our current research also involves investigating pos\u00adsible collaborations between RAP, \na region scheduler and a local instruction scheduler. We believe that RAP will aid in the phase integration \ndesired for generating efficient final code because the register allocator now shares the same program \nrepresentation with many other tasks of an optimizing or parallelizing compiler. Acknowledgements We \nwould like to thank the University of Pittsburgh compiler group for providing us with the pdgcc com\u00adpiler. \nAlso, our thanks to the program committee for their helpful suggestions for the final paper. [1] H. Agrawal \nand J. R. Horgan. Dynamic program slicing. In Proceedings of the SIGPLAN 90 Con\u00adference on Programming \nLanguage Design and Im\u00adplementation, pages 246 256, White Plains, NY, June 1990. [2] V. H. Allan, J. \nJanardhan, R. M. Lee, and M. Srinivas. Enhanced region scheduling on a program dependence graph. In Proceedings \nof the twenty-fiflh International Symposium on Microar\u00adchitecture, pages 72 80, Portland, OR, 1992. [3] \nF. E. Allen, M. Burke, P. Charles, R. Cytron, and J. Ferrante. An overview of the PTRAN analysis system \nfor multiprocessing. Journal of Parallel and Distributed Computing, 5:617 640, 1988. [4] R. A. Ballance, \nA. B. Maccabe, and K. J. Ot\u00adtenstein. The program dependence web: a repre\u00adsentation supporting control-, \ndata-, and demand\u00addriven interpretation of imperative languages. In Proceedings of the SIGPLAN 90 Conference \non Programming Language Design and Implementa\u00adtion, pages 257 271, White Plains, NY, June 1990. [5] W. \nBaxter and H. R. Bauer, III. The program dependence graph and vectorization. In Proceedings of the Sixteenth \nAnnual ACM SIGACT/SIGPLAN Symposium on Principles of Programming Languages, Austin, TX, 1989. [6] D. \nBernstein and M. Rodeh. Global instruction scheduling for superscalar machines. In Proceed\u00adings of the \nACM SIGPLAN 91 Conference on Programming Language De.wgn and Implementa\u00adtion, Toronto, June 1991. [7] \nDavid Bernstein and Michael Rodeh. Global in\u00adstruction scheduling for superscalar machines. In Proceedings \nof the SIGPLAN 91 Conference on Programming Language Design and Implementa\u00adtion, Toronto, CANADA, June \n1991. [8] Preston Briggs. Register allocation vta graph col\u00adoring. PhD thesis, Rice University, April \n1992. [9] Preston Briggs, Keith D. Cooper, Ken Kennedy, and Linda Torczon. Coloring heuristics for register \nallocation. In Proceedings of the ACM SIGPLAN 89 Conference on Programming Language Design and Implementation, \nJuly 1989. [10] Preston Briggs, Keith D. Cooper, and Linda Tor\u00adczon. R programming environment newsletter \n#44. Department of Computer Science, Rice Uni\u00ad versity, September 1987. [11] [12] [13] [14] [15] [16] \n[17] [18] [19] [20] [21] [22] Preston Briggs, Keith D. Cooper, and Linda Torc\u00ad zon. Rematerialization. \nIn Proceedings of the S IG- PLAN 92 Conference on Programming Language Design and Implementation, June \n1992. David Callahan and Brian Koblenz. Register al\u00adlocation via hierarchical graph coloring, In Pro\u00ad \nceedings of the SIGPLAN 91 Conference on Pro\u00adgramming Language Design and Implementation, pages 192-203, \nToronto, CANADA, June 1991. G. J. Chaitin. Register allocation and spilling via graph coloring. In SIGPLAN \nSymposium on Com\u00adpiler Construction, Boston, June 1982. Gregory Chaitin, Marc Auslander, Ashok K. Chandra, \nJohn Cocke, Martin E. Hopkins, and Pe\u00adter W. Markstein. Register allocation via coloring. Computer Languages, \n6:47-57, January 1981. Frederick Chow and John Hennessy. cation by priority-based coloring. In the SIGPLAN \n84 Symposium on struction, June 1984. Jeanne Ferrante, Karl Warren. The program use in optimization. \ngramming Languages 1987. J. Ottenstein, dependence Register allo-Proceedings of Compiler Con\u00ad and Joe \nD. graph and its ACM Transactions on Pro\u00adand Systems, 9(3):319-349, Claude-Nicolas Fiechter. PDG C Compiler. \nUni\u00adversity of Pittsburgh, 1992. J. H. Griffin and K. J. Ottenstein. PROBE: a dependence-based program \nbrowser. Techni\u00adcal Report LA-UR-89-1823, Los Alamos National Laboratory, Los Alamos, NM, November 1989. \nRajiv Gupta and Mary Lou Sofia. Region schedul\u00ading: An approach for detecting and redistributing parallelism. \nIEEE Transactions on Software En\u00adgineering, 16(4):421-431, 1990. Rajiv Gupta, Mary Lou Soffa, and Tim \nSteele. Register allocation via clique separators. In Pro\u00adceedings of the SIGPLAN 89 Conference on Pro\u00adgramming \nLanguage Design and Implementation, Portland, Oregon, June 1989. S. Horwitz. Identifying the semantic \nand textual differences between two versions of a program. In Proceedings of the SIGPLAN 90 Conference \non Programming Language Design and Implementa\u00adtion, pages 234 245, White Plains, NY, June 1990. S. Horwitz, \nJ. Prins, and T. Reps. Integrating non\u00adinterfering versions of programs. In Proceedings of the Fifteenth \nAnnual ACM SIGACT\\SIGPLAN Symposium on Principles of Programming Lan\u00adguages, pages 133-145, San Diego, \nCA, 1988. [23] D. J. Kuck, R. H. Kuhn, B. Leasure, D, A. Padua, and M. Wolfe. Dependence graphs and \ncompiler optimizations. In Proceedings of the Eighth Annual ACM Symposium on Principles of Programming \nLanguages, pages 207-218, 1981. [24] Cindy Norris and Lori L. Pollock. A scheduler\u00adsensitive global register \nallocator. In Supercom\u00adputing 93 Proceedings, Portland, OR, November [25] [26] 1993. K. J. Ottenstein. \nbased on a cyclic cal Report 81-1, Michigan Tech. K. J. Ottenstein gram dependence ment environment, \nPLAIV\\SIGSOFT ware Development Pittsburgh, PA, [27] Todd A. Proebsting abilistic register An intermediate \nprogram form data-dependence graph. Techni- Department of Computer Science, University, 1981. and L. \nM. Ottenstein. The pro\u00adgraph in a software develop-In Proceedings of A CM SIG-Symposium on Practical \nSoft-Environments, pages 177 184, April 1984. and Charles N. Fischer. Prob\u00adallocation. In Proceedings \nof the SIGPLAN 92 Conference on Programming Lan\u00adguage Design and Implementation, pages 300-310, San Francisco, \nCA, June 1992. [28] J, Warren. A hierarchical basis for reordering transformations. In Proceedings of \nthe Eleventh Annual ACM Symposium on Principles of Pro\u00adgramming Languages, pages 272-282, 1984. [29] \nM. Weiser. Program slicing. IEEE Transactions on Sofiware Engineering, SE-10(4):352-357, 1984. [30] M. \nJ. Wolfe. Research Monographs in Parallel and Distributed Computing. The MIT Press, 1989.  \n\t\t\t", "proc_id": "178243", "abstract": "<p>This paper describes RAP, a Register Allocator that allocates registers over the Program Dependence Graph (PDG) representation of a program in a hierarchical manner. The PDG program representation has been used successfully for scalar optimizations, the detection and improvement of parallelism for vector machines, multiple processor machines, and machines that exhibit instruction level parallelism, as well as debugging, the integration of different versions of a program, and translation of imperative programs for data flow machines. By basing register allocation on the PDG, the register allocation phase may be more easily integrated and intertwined with other optimization analyses and transformations. In addition, the advantages of a hierarchical approach to global register allocation can be attained without constructing an additional structure used solely for register allocation. Our experimental results have shown that on average, code allocated registers via RAP executed 2.7% faster than code allocated registers via a standard global register allocator.</p>", "authors": [{"name": "Cindy Norris", "author_profile_id": "81407593242", "affiliation": "Department of Computer and Information Sciences, University of Delaware, Newark, DE", "person_id": "PP77025858", "email_address": "", "orcid_id": ""}, {"name": "Lori L. Pollock", "author_profile_id": "81100368013", "affiliation": "Department of Computer and Information Sciences, University of Delaware, Newark, DE", "person_id": "P173221", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/178243.178427", "year": "1994", "article_id": "178427", "conference": "PLDI", "title": "Register allocation over the program dependence graph", "url": "http://dl.acm.org/citation.cfm?id=178427"}