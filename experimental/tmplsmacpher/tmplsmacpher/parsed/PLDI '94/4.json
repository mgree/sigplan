{"article_publication_date": "06-01-1994", "fulltext": "\n Link-Time Optimization of Address Calculation on a 64-bit Architecture Amitabh Srivastava and David \nW. Wall Digital Equipment Western Research Laboratory 250 University Ave., Palo Alto, CA 94301 {amitabh, \nwal.l}@decwrl .pa. dec. com Abstract Compilers for new machines with 64-bit addresses must gen\u00ad erate \ncode that works when the memory used by the program is large. procedures and global variables are accessed \nin\u00ad directly via global address tables, and calling conventions include code to establish the addressability \nof the appropri\u00ad ate tables. In the common case of a program that does not require a lot of memory, all \nof this can be simplified consid\u00ad erably, with a corresponding reduction in program size and execution \ntime. We have used our link-time code modification system OM to perform program transformations related \nto globat address use on the Alpha AXP. Though simple, many of these are whole-program optimization that \ncan be done only when we can see the entire program at once, so link-time is an ideal occasion to perform \nthem. This paper describes the optimization performed and shows their effects on program size and performance. \nRel\u00ad atively modest transformations, possible without moving code, improve the performance of SPEC benchmarks \nby an average of 1.5Y0.More ambitious transformations, requiring an understanding of program structure \nthat is thorough but not difficult at link-time, can do even better, reducing pro\u00ad gram size by 10% or \nmore, and improving performance by an average of 3.87i0. Even a program compiled monolithically with \ninterpro\u00ad cedurat optimization can benefit nearly as much from this teehnique, if it contains statically-linked \npre-compiled li\u00ad brary code. When the benchmark sources were compiled in this way, we were still able \nto improve their performance by 1.35% with the modest transformations and 3.4% with the ambitious transformations. \nPermission to co y without fee all or part of this material is granted provid J that the copies are not \nmade or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication \nand its date appear, and notice is given that copying is by permission of the Association of Computing \nMachinery. To copy otherwise, or to republish, requires a tee and/or specific permission. SIGPLAN 94-6/94 \nOrlando, Florida USA @ 1994 ACM 0-89791 -862-xJ9410006..$3.5O 1 Introduction Obtaining addresses of \nprocedures or global data on a new 64-bit RISC machine such as the Alpha AXP [4] presents an elementary \nbut unavoidable problem. Traditional 32\u00adbit RISCS have evaded the worst aspects of the problem by using \nvarious architectural tricks, such as special-format instructions like the SPARC Catl [2], which packs \na 2-bit opeode and a 30-bit word address into a 32-bit instruction, or special-purpose instructions like \nthe MIPS Load-Upper-Immediate (LUI) [3], which facilitates the construction of a 32-bit address from \ntwo 16-bit immediate fields, These techniques break down when we move to machines with 64\u00adbit addressesand \n32-bit instructions. The solution used by the Alpha AXP compilers running Alpha/OSF is a typical one \nfor the coming generation of 64\u00adbit machines. Global addresses are colleeted together into one or more \nglobal address tables (GA Ik). Each separately\u00adcompiled module has its own GAT, but at link-time the \nGATs are collected together into one GAT section. The generated code accesses the GAT via a dedicated \nregister called the globaf pointer (GP) together with a 16-bit immediate dis\u00adplacement. Accessing a program \nobject then involves two steps. First, the addressof the object is loaded from the GAT section. We refer \nto this load as an address load, to distinguish it from the traditional Load-Address operation, which \nadds a literal displacement toabaseregister anddoesnotaccessmemory at all. The address obtained by this \naddress load is then used to load or store the variable, or jump to the procedure, that it refers to. \nIn general, each routine must have an associated value for the global pointer, which may differ from \nthe value for some other routine. There are two reasons for this. First, for large programs, the global \naddress table may be so large that it cannot be accessed via a single unchanging global pointer. Second, \na module from a dynamically-linked shared library must have its own global addresstable, which may be \nmapped to an address far from the table for the rest of the program,l A compiler looking at a single \nmodule does not know whether the complete program is large or small, nor whether the module is destined \nto be part of a shared library. It must generate code that will work regardless, even if this general \ncode is not the fastest code in some cases. Improving this code is straightforward, but in general requires \nanalysis and code generation that crosses module boundaries; a thorough job requires this to include \neven library routines, which usu\u00adatly were compiled long before a particular application. Link-time code \noptimization can be very helpful in dealing with this problem. At link-time, the optimizer can analyze \nthe entire program, see how big the address table needs to be, and simplify address calculation when \npossible. How thoroughly the linker can do this depends primarily on how hard it is willing to work. \nSome improvement is possible even for a traditional linker that limits each change to the replacement \nof one instruction by another. Even more improvement is obtained by giving the linker the ability to \nmove instructions wound, because it can therefore exploit optimization opportunities better than the \ntraditional linker, and also find opportunities the traditional linker could not. This paper describes \nour tool for link-time address opti\u00admization, and shows how much performance improvement is achieved. \nWe start by detailing our assumptions about code generation on 64-bit machines, and on the Alpha AXP \nin particular. We then discuss how the resulting code can be improved, and describe our system for doing \nso. Finally we present the static and dynamic measurements of how much we were able to improve the code. \nGenerating code for 64-bit machines Our model for compiling on a 64-bit processor follows from the considerations \ndescribed above. We assume that a refer\u00adence to a procedure or variable is generally done by loading \nthe address from a global address table (GAT) and follow\u00ading the resulting pointer to get to the object \nin question. A dedicated register called the globai pointer (GP) is used to maintain the addressability \nof the globat address table. The different GATs need not be adjacent in memory (though they may be); \nin particular, modules linked dyttamicatly from 1It might seem that procedures in the same compilation \nunit would always use the same GP, but the semantics of shared libraries means that a call that the compiler \nthinks is intra-unit might turn out to be inter-unit at dynamic\u00adlink-time. In particular, a module with \nexported routines P1 and P2 can be dynamically linked witfr a program even though a different implementation \nof P1 is already present in the executable; If E! calls PI, it gets tie version already present, not \nthe one in the same module [ 1]. Under static linking, of shared libraries may have their code, GAT, \nand data in mem\u00adory far from the statically linked part of the program. At link-time, each separately-compiled \nmodule has one or more GATs used by the procedures in that module. When the linker combines these modules, \nit treats these GATs as lit\u00aderal pools, removing duplicate addresses and merging the individual GATs \ninto a single large GAT if possible. All values in a GAT must be accessible via its GP value and a machine-dependent \ndisplacement, so merging into one largeGAT will not always bepossible~ As aresult, procedttre\u00adcalling \nconventions must suppose that the caller and catlee will need different values of GP, and must therefore \ninclude code to setup a new value for GP when a procedure is called, and code to restore the GP value \nfor the caller upon return. Either of these pieces of code may be conventionally done by either the caller \nor the catlee. On the AXP under Alpha/OSF, each procedure is respon\u00adsible for setting up its own proper \nGP value, both when the procedure is entered and when control returns to the proce\u00addure after it makes \na cdl. It does this by adding a 32-bit displacement to a register containing a nearby code address. Calling \nconventions ensure that such a register exists, and memory layout conventions ensure that a procedure \ns GAT is close enough. Adding a 32-bit displacement to a register value can be done in two instructions: \nthe Load-Address-High (LDAH), which adds the upper 16 bits of the displace\u00adment, and an ordinary Load-Address \n(LDA) to add the lower 16 bits. Figure 1 shows a typicat AXP catling sequence, The rou\u00adtine on the left \nsets its GP on entry and resets it after return from the contained call. On entry it computes the GP \nfrom the value of the PV register, which by convention contains the procedure entry address; after the \nreturn it uses the return address register RA. The 32-bit displacement is different be\u00adtween these two \noperations by exactly the difference between the two code addresses on which the computation is based. \nm-,.-11-4 -_---l.._- .. -:.. . . . . . fin , l__ .- -. ------ J 1 llG UUICU ~lUGCUU1 C S~LS11S~W1l Ur \n111 LIlfS MIIIC Way, ittttt need not save the caller s GP. The address of the destina\u00ad tion procedure \nis obtained by loading it from the GAT as we described previously. The AXP has a limited-range call instruction \nwhose des\u00ad tination is specified by a 21-bit word displacement from the program counter. If the destination \nis near enough in the same compilation unit, the compiler can use this BSR instruction instead of the \nmore general JSR. The BSR does not require us to load the destination address into PV, but the compiled \ncode normally does so anyway, because the called procedure needs the PV in order to set up its value \nfor GP. course, this situation would result in a link error because the symbol PI is With the calling \nconventions properly setting the GP, ac\u00ad multiply-defied. Note that it is possible to optrmize a calf \nto art unexported cessing global variables is easy. Obtaining the address of a routine in the same module \nat compile-time. variable isdonebyanaddressloadfrom theGAT,inthesame ldah gp := pV + 8192<<16 ldah gp \n:= pV + 216<<16 lda gp := gp + 28576 lda gp := gp + 12036 ... lda sp := sp -32 load pv := 144(gp) /rjsr \nrar (PV) ldah gp := ra + 8192<<16 Ida gp := gp -\\k:isp+ 28524 = p + 32 Figure l: Callingconventions \nontheAlphaAXP load rl := 188 (gp) load rl := 188(gp) load rl := 188(gp) load r2 := O(rl) store O(rl) \n:= r2 (a) get address of V (b) fetch value of V (c) assign value to V Figure 2: Global variable accesson \nthe Alpha AXP waythatweobtain theaddressofthe destinationproc&#38;ure in a call, A fetch from or assignment \nto the variable con\u00adsists of the address load followed by a load or store through the address just loaded. \nFigure 2 shows these three code fragments. 3 Improving the compiled code The code generation conventions \ndescribed above are con\u00adservative the compiler does not know how big the eventual program will be, so \nit generates code that will work regard\u00adless. In a typical reasonably-size program, however, the resulting \ncode may be slower than necessary. If we can see the whole program at once, there are several ways to \nimprove this code. First, the GAT section is often small enough that nearby user variables can be referenced \ndirectly via the GP, rather than indirectly through addresses in the GAT. (This is even more effective \nif the compiler segregates the small data into its own data section, even if the compiled code accessessmall \nand large data identically.) Second, most programs are small enough that a few distinct GATs suffictz \nmost often one is enough. We can omit GP\u00adresetting after calls between routines that use the same GAT. \nThird, we can replace the general call instruction by the BSR instruction, if the destination procedure \nis near enough. If thetwoproceduresalsosharethesameGAT,wecanchange the destination of the BSR to skip \nthe GP-setting instructions in the destination procedure prolog. This in turn means that we have no more \nuse for the destination address in the PV, and can omit that address load. Fourth, removing address loads \nfor the above reasons means that we can remove many addresses from the GAT altogether. This GAT-reduction \nacts to increase data locality, but also means that the GAT gets smaller, perhaps enabling afresh round \nof the other improvements. Finally, optimization cause widespread changes to the program code, including \nthe removal of many multi-cycle load instructions. The code we started with had been pipeline scheduled \nat compile-time, based in part on the presence of these loads. It is reasonable to suppose that we can \nimprove the code still further by performing pipeline scheduling as a final step. Most of these modifications \nare examples of whole\u00adprogram optimizations. They can be done only if we can examine the entire program \nat once, or at least (if shared libraries are used) the entire statically-linked part of it. Oth\u00aderwise, \nfor example, we do not know how big the GAT and data sections will eventually be, and therefore cannot \nchange GAT-based variable references into GP-based ref\u00aderences. Even a compiler that can compile and \noptimize many modules together will be unable to perform these op\u00adtimization, if the program contains \nmodules in other source languages or library modules compiled at an earlier time. We therefore advocate \ndoing these optimization at link\u00adtime. This requires the linker to have a rather deeper under\u00adstanding \nof the program control flow than has hitherto been typical for linkers, but this analysis is not difficult, \nespecially since the loader format normally provides us with a few hints. References to the GAT section \nmust be marked for reloca\u00adtion, because otherwise normal linking will not work. For similar reasons, \nthe AXP compilers include links between an instruction that loads an address and the subsequent in\u00adstructions \nthat use it. Finally, the loader format identifies procedure boundaries and specifies the correet value \nof GP for each procedure.  4 Our study We have built an optimizing linker for the Alpha AXP using our \nobject-code modification system OM. A more complete description of how OM works appears in our previous \npa\u00adper [6]. The OM linker translates the object code of the entire program into symbolic form, recovering \nthe original struc\u00adture of loops, conditionals, case-statements, and procedures. It then analyzes this \nsymbolic form and transforms it by in\u00adstrumenting or optimizing it, and generatesexecutable object code \nfrom the result. It can be thorough but still conservative in understanding the input object code because \nit can use the loader symbol table and the relocation tables to clarify the code. The key idea behind \nOM is the translation into symbolic form and back. This means that we can perform interesting transformations \nlike code optimization on object code with\u00adout having to keep track of the effects of thesetransformations \non address constants and branch displacements. (The sym\u00adbolic form is also rather more amenable to dataftow \nand other analysis, though these were not needed in this study.) Because OM lets us work with a symbolic \nform, we can easily delete and reorder instructions. Both of these capabil\u00adities are important. Deletion \nlets us get rid of the instructions we don t need. Reordering lets us create opportunities that would \nnot exist otherwise. For example, the instructions to set the GP on entry to a procedure are often moved \nlater in the code by the compile-time pipeline scheduling, prevent\u00ading us from changing the BSR instruction \nto skip them; if we can restore them to their logical place at the beginning of the procedure, we can \navoid executing them on most or atl of the calls. On the other hand, translation to a symbolic form is \nrather more work than a traditional linker normally does. If we are unwilling to delete or reorder the \ncode, there are still gains to be had. Instead of deleting unneeded instructions, we can replace them \nby no-ops. This has two benefits. First, changing normal instructions to no-ops removes data depen\u00ad dencies. \nA no-op might cost nothing even if we don t delete it, beeause it might fit into some other instruction \ns latency delay or in a multiple-issue slot unavailable to the original instruction. Second, replacing \naddress loads with no-ops re\u00ad moves the danger of a cache miss or page fault. The GAT might be used consistently \nenough to stay resident, but re\u00ad moving a reference altogether renders the question moot. For this study, \nwe have therefore used OM to implement two different levels of address optimization: OM-full and OM-simple. \nOM-simple consists of the address-calculation optimiza\u00ad tion that a traditional linker can do using only \nlocal analysis and no code motion. Unneeded instructions are removed by changing them to no-ops, and \nthe order of the remaining in\u00adstructions is never changed. If a variable is sufficiently close to the \nGAT, we change references to it into GP-relative ref\u00aderences; this includes even references within the \nreach only via a 32-bit displacement, because the LDAH instruction lets us make a direct GP-relative \nreference in the same number of instructions as an indirect reference via the GAT. When possible, we \nchange GP-swapping instructions to no-ops and call instructions to BSRS. We sort the common symbols by \nsize and place them with the small data sections near the GAT, and use a simple heuristic to pick a good \nvalue for the GP, Ohl-full consists of the full set of address-calculation op\u00adtimization discussed previously. \nThese require OM to un\u00adderstand the control structure of the program, finding all the transfers of controls \nand all the places where code addresses are used either explicitly or implicitly. OM-full can option\u00adally \ninclude rescheduling the code after all the other optimiza\u00adtion are finished. Rescheduling includes quadword-aligning \ninstructions that are the targets of backward branches, which is intended to improve the behavior of \nthe AXP S dual-issue and cache. We applied OM-simple and OM-full to code produced by the standard Alpha/OSF \noptimizing compilers. 2 The only changes OM made to the code were the address-calculation optimization \ndescribed above. OM S analysis and optimiza\u00adtion encompassed all the modules in a program, including \nuser modules and non-shared versions of all library modules, because we wanted to maximize the performance \nimprove\u00adment and because OM does not yet support shared libraries. The programs we used are the SPEC92 \nsuite with the excep\u00adtion of gee, which we could obtain only in 32-bit mode. 5 Results We measured \nOM S effectiveness on two different versions of each benchmark, which we call compile-each and compile\u00ad \nall. Compile-each was obtained by compiling the benchmark s source files separately with only intraprocedural \nglobal optimization.3 We used OM to link the resulting object files and the standard libraries, performing \neither OM-simple or OM-full in the process. We then took static and dynamic measurements of the resulting \ncode, and compared them to measurements of code produced by linking the compile-each version with no \nlink-time optimization. The compile-all version was obtained by linking all of the benchmark s source \nfiles together into a single object file with 2DEC FOIIIm for Alpha/OSF, and the DEC O.SF/l T1.3 -1 @eV. \n V3.3 19.3) C compiler. 3Compiler optron -02.  Mean of 19 pgms n. d.V alvinn compres doduc ear eqntott \nespress fpppp flB\u00adhydro2d Ii M mdljdp2 d mdljsp2 g nasa7 #B ora .5 uBuuuq\u00adspice su2cor swm256 tomcatv \nwave5 J ~ ? ;jj g .>; 0 auuug jJ $: $ :.:, Figure 3: Static fraction of address loads removed, the maximum \nlevel of compile-time optimization: This op\u00adtimization level does both intraprocedural and interprocedu\u00adral \noptimization, including some of the address-calculation optimization OM does. As before, OM then linked \nthe compile-all version with the pre-compiled standard libraries, performing either OM-simple or OM-full. \nWe took static and dynamic measurements of the resulting code, and com\u00adpared them to measurements of \ncode produced by linking the compile- all version with no link-time optimization. Finally, we measured \nthe processing time required by OM and by the standard linker for the supported levels of opti\u00admization. \n 5.1 Static measurements We measured three static properties of the test suite after optimization by \nOM-simple and OM-full, compared to the original program. We lirst measured how many address loads appeared. \nWe then measured how often the various parts of the procedure-calling conventions were required. Finally \nwe simply measured the total number of instructions. We first counted how many address loads were eliminated \nby OM-simple and OM-full. An address load can be elim\u00adinated in the two ways we discussed earlier. It \ncan be con\u00adverted changed to a load-address operation in the form of an LDA or LDAH. Or it can be nullified \nchanged to a no-op (in OM-simple) or deleted altogether (in OM-full). Figure 3 shows the static fraction \nof address loads that were eliminated for each program. The two leftmost bars give the changes for thecompile-each \nversions of the programs, the two rightmost tcOmpjler ~PtiOn.04 for Fortrarr and -03 -~ for c. whether \nconverted (dark) or nullified (light) for the compile-all versions. The dark part of each bar shows how \nmany address loads were converted, and the light part shows how many were nullified. The bar lengths \nin the large key to the left show the unweighed arithmetic means of the values in the 19 test programs. \nOM-simple finds essentially all of the available opportu\u00adnities for converting address loads to load-addresses. \n(This should happen for any program whose static data area is less than 232 bytes.) On average about \nthe same number of ad\u00address loads are nullified into no-ops. About half of all address loads are either \nconverted or nullified. OM-full manages to eliminate nearly all of the address loads. The number of address \nloads converted to load\u00adaddresses decreases slightly: GAT-reduction lets us reach more of the data via \nthe GP, so OM-full can nullify distant references that OM-simple can only convert. An important source \nof improvement is the bookkeeping code involved in procedure calls. An unoptimized call site has four \ninstructions: one to load the PV with the destination address, one for the JSR, and two to reset the \nGP after return\u00ading. In the right circumstances, OM can change the JSR to a BSR and nullify the other \nthree instructions. Figure 4 shows how well it could do this. The compiler itself can perform these transformations \nfor calls to an unexported procedure in the same compilation unit, but the no OM results show that there \nare few such calls. Even when the benchmark source files are compiled together with interprocedttral \noptimization, statically around 85% of the calls still require all the bookkeeping code. In contrast, \neven OM-simple can change essentially all JSRS in the test programs to BSRS; this requires no analysis \n Conlp I comp each all Mean of 19 pgms    1.01I i comp comp each I all Figure 4: Static fraction of \ncalls requiring alvinn compres doduc ear eqntott mmmm w espress fpppp hydro2d mmmmm mdljsp2 nasa7 \nora mmmmm su2c0r swm256 tomcatv wave5 mmmm alvinn compres doduc ear eqntott mmmmm espress fpppp hydro2d \nIi mdljdp2 mmmmm mdljsp2 nasa7 ora mmmmm su2cor swm256 tomcatv wave5 mmmm PV-loads (top) and GP-reset \ncode (bottom) 54 Mean of or a! * hydro2d Zo E q :.,,::::,,,. .,,. . ~ p: ,.....-:::: ,::,:::,::,,,:::::, \n.:~,;,::::fi,; ,;,;,: ,,,,,,,,,,,.:,:,::;,:,.:. u$! ,,,,,,,,. . :Y,.,,.. ,,,, y.. : :.::.:::: :,,,:,::..:,.. \n,:,:,.,.::,., ,;:;:,??::.:. : ~,,,,::::,,:.:::::,,,,:.:.: y:,:,,:,,:spice :,: :fi:~ ,y,:; + d s= IUuagg \ncomp comp each all Figure 5: Static fraction at all except to look up destinations in the GAT and see \nif they are close enough. If the GP-setting instructions of the called procedure appeared first, it would \nalso be easy to change the BSR to skip them. Unfortunately, compile-time scheduling often moved them, \npreventing OM-simple from doing this. This in turn prevents OM-simple from nullifying the PV\u00adload at \nthe call site, because the PV is needed so that the GP-setting code in the cak will do the right thing. \nOM-full removes all but a few PV-loads; those remaining are in calls through procedure variables, preventing \nus from examining the destination procedure. OM-simple does a better job at nullifying the two GP\u00adresetting \ninstructions. It can usually change them to no-ops even if they were moved by compile-time scheduling. \nOM\u00adfull does a little better, using its understanding of the control structure to find a few cases OM-simple \nmisses. Surprisingly, the results without using OM show that there is little difference in these measures \nbetween code that was in\u00adterprocedurally optimized by the compiler (compile-all) and code that was not \n(compile-each). Compile-time interproce\u00addural optimization can improve calls from one user routine to \nanother, but cannot help calls to previously compiled library routines.5 The combination of improvements \nto variable-access code and to procedure-calling overhead results in a substantial reduction in the static \nnumber of useful instructions required. 5~i~ lack of differ~n~e is partly irrelevant. he effect of interprocedur~ \noptirnkatimr is the inlining of user routines; if a mukiply-inlirted user routine contains a library \ncall then that call will be replicated. This should not harm performance because the library routine \nwill be called the same dyrramic number of times, but it will tend to increase the static number of CSUSthat \ncannot be improved. Ii mdljdp2 mdljsp2 nasa7 ora Baggga su2cor swm256 tomcatv wave5 of instructions \nnullified Figure 5 shows that OM nullifies a substantial fraction of the code. Each graph shows first \nhow well OM-simple and OM\u00adfull did on the compile-each version of the program, and then how well they \ndid on the compile-all version. Again, the key on the left side shows the unweighed arithmetic means \nover the set of programs. Even OM-simple nullifies (but does not delete) around 6% of the instructions. \nOM-full deletes an astonishing eleven percent of the instructions on average, and often more. OM S ability \nto improve the code is not dependent on whether the code was originally compiled with interproce\u00addural \noptimization: the average improvement of compile-all code was nearly equal to that of compile-each code. \nCompile\u00adtime interprocedural optimization does not have access to the whole program and can therefore \ndo nothing to improve ac\u00adcess to variables. Moreover, while it can improve calis from one user routine \nto another, it cannot be as helpful on calls to previously compiled library routines. These calls are \nin fact quite common; in the spice benchmark, for example, stati\u00adcally half the calls are from one library \nroutine to another! OM-full reduced the size of the GAT by an entire order of magnitude, reducing it \nto between 370 and 15% of its original size. In was slightly more effective on compile-each versions \nthan on compile-all versions, because compile-all does a little GAT-reduction of its own before OM gets \na chance. 5.2 Dynamic measurements We ran the different versions of the benchmarks on a DEC\u00adstation 3000 \nModel 400 AXP, a dual-issue machine, and mea\u00adsured the user and system time according to the system clock \nas measured by the systime facility. To provide standards to alvinn compres doduc ear Mean of.19 pgms \n20 p = g o y .c ~ 10 .= = =a ~z:JY * o ,,. ,, , , ,, . ;.:. ti ,, ,. ,,, .,.,.,.,-. .. ., ..:.:.:. \n... : ,,:,.,> ., ,.,,,.+,: : : ,,,., .$. ... .,:..,.,.,., ,. : :. , , ,,.::: , , comp comp each all \nFigure 6: Improvement in performance relative compare to, we first linked the compile-each and compile-all \nversion of each benchmark with no link-time optimization at all, and timed the result. We then linked \nusing OM-simple and OM-full, and compared the resulting execution time to the corresponding standard \ntime. Figure6 shows the improvement of code optimized by OM at link-time over code not optimized at link-time. \nAs before, each graph shows first how well OM-simple and OM-full did at improving the compile-each version \nof the program, and then how well they did at improving the compile-all version. The key on the left \nside shows the unweighed arithmetic means over the set of programs. The dynamic improvements were smaller \nthan the static improvements, for a couple of reasons: cache misses and page faults mean that many cycles \nare spent doing things other than user instructions, and the dual issue of this model of the AXP means \nthat some instructions come free so that deleting them does not help. Nevertheless the results are satisfying \ngiven how easy they are to attain. OM-simple improved the performance of thecompile-each programs by \n1.5% on average. Seven benchmarks improved by more than 1%; the median was 0.6%. OM-full did consid\u00aderably \nbetter, improving the code by 3.8% on the average and by more than 5% in six cases; the median was 2.8%. \nUnfor\u00adtunately, OM-full made the code slightly slower in two cases. Da Dq :x X eqntott espress fpppp \nhydro2d x ;::y,: au D mdljdp2 mdt isD2 nasa7 Ii ,, ,,.: . :j.: .,.,:t :.:. ~ ;,:,:,::::::,::::: EIl \n:::: i:\\ ora Sc: spice qSU2C0 ::: ,:,::::: u..,,:,::t? . :.:::::.,../., Da swm256 tomcatv wave5 to \nprogram without link-time optimization This was probably because of cache or paging effects, since loop \nalignment and scheduling didn t eliminate the problem. OM S payoff was nearly as high even on the compile-all \nversions. OM-simple improved code by an average of 1.35Y0, and OM-full by 3.470; this is 90?Z0of the \nimprovement seen on the compile-each versions. Again, OM-full improved six programs by more than 5%, \nand the median was again 2.8%. Even if interprocedural optimization speeds up the program, it still leaves \nplenty for a link-time address-calculation opti\u00admizer to find. We expected that rescheduling the code \nafter finishing the address-calculation optimization would greatly improve the performance. Although \nthe code OM starts with was sched\u00aduled at compile-time, the scheduling was done in the presence of a \nlarge number of address loads that OM later removed. Load operations have a significant latency even \nwhen they hit in the cache, and removing that latency allows the in\u00ad structions that fill it to be used \nfor other kinds of latency. To take advantage of this, OM quadword-aligned instruc\u00ad tions that were the \ntargets of backward branches and then scheduled each basic block using a version of the standard AXP/OSF \nscheduler6 very similar to the scheduler used by the assembler and thus indirectly by the OSF C compiler. \nTo our surprise, however, scheduling made only a small 6~mk~ to Mike Rickabaugh for providing his. a% \ni--Compress doduc ear espresso eqntott hydro2d ti mdljdp2 mdljsp2 nasa7 ora Sc spice su2cor swm256 tomcatv \nwave5 standard link 0.07 0.06 0.28 0.12 0.19 0.08 0.25 0.26 0.15 0.27 0.28 0.27 0.24 0.21 0.35 0.27 0.24 \n0.20 0.28 interproc build . 2.12 6.12 41.49 10.12 67.84 12.27 16.77 19.66 54.67 9.95 9.48 11.38 2.21 \n42.16 84.38 25,81 4.61 2.95 57.86 OM tirtker no opt simple full wlsched 0.28 0.29 0.33 1.72 0.20 0.20 \n0.22 1<22 1.34 1.43 1.64 18.58 0.39 0.41 0.47 2.59 0.88 0.92 1.02 5.46 0.28 0.29 0.33 1.77 1.19 1.25 \n1.43 68.12 1.23 1.29 1.48 8.71 0.55 0.59 0.64 3.09 1.23 1.29 1.48 8.44 1.23 1.29 1.47 8.39 1.22 1.28 \n1.45 9.03 1.03 1.09 1.26 7.05 0.94 1.00 1.11 5.83 1.89 2.00 2.31 17.90 1.32 1.40 1.61 10.68 1.08 1.14 \n1.29 7.45 0.94 0.99 1.13 6.48 1.47 1.57 1.81 12.95 Figure 7: Build times in seconds for Id from objects, \ncompile from source8 with maximum optimization, and OM from objects difference, raising the average improvement \ngiven by OM\u00adfull from 3.8% to 4.2% for compile-each code, and from 3.4% to 3.6% for interprocedttrally \noptimized compile-all code. We are still unsure why this is so. It may be that the initial schedule is \nstill good enough even after our changes. Also, the schedulers used by either compiler make some use \nof memory alias information that is unavailable to us unless we do an expensive data flow analysis, so \nit may also be that our scheduler is just inferior to the compile-time schedulers, In two cases, scheduling \nnoticeably degradedperfortnance. In the ear benchmark, the loop-target alignment is at fault when we \nscheduled it without alignment the performance was improved. In nasa7 even the unscheduled code was degrade@ \nwe suspect this is because deleting unnecessary code caused two parts of the program to fight over the \ncache. 5.3 Processing time Figure 7 shows the time required to build each benchmark in a variety of ways. \nThe tirst column shows the time to do a traditionat link using the standard linker on separately compiled \nobject files and standard libraries. The second column shows the time to build the benchmark from its \nsource files, by compiling them with interprocedural optimization and linking them with the standard \nlibraries. The next four columns show the time to link the benchmark using OM on separately compiled \nobject files and standard libraries, with no OM optimization, OM-simple, OM-full, and OM-full with scheduling. \n OM S technique of working on a symbolic intermediate form is intended to support a variety of link-time \nwhole\u00adprogram optimization. Using OM does introduce some overhead, as we can see by comparing the processing \ntime for OM without optimization to that of a standard link. Nev\u00adertheless, OM S processing time is not \nextravagant. Even OM-full can handle any of these programs in a couple of see\u00adends. A rebuild using interprocedural \ncompiler optimization, in contrast, often takes many tens of seconds. Link-time 8cheduling proved to \nbe rather expensive, par\u00adticularly for applications like fpppp and doduc with very large basic blocks. \nThis is not tremendously surprising; scheduling is usually a superlinear process, since it essentially \nrequires sorting the instruction in a basic block according to a depen\u00addency relation. The minor payoff \nwe saw from scheduling suggests that rescheduling is often not worth the co8t. A full compile from sources \nwith interprocedural opti\u00admization is nearly always more expensive than OM. This is unsurprising (and \nnot entirely fair), since this kind of com\u00adpilation does much more analysis and optimization than OM \nneeds for the address-calculation optimization. Nonethe\u00adless, it suggests that address-calculation optimization \nare quite cheap and could profitably be done without bringing a full interprocedural optimizer to bear. \n6 Discussion The whole-program optimization discussed here require us to examine the entire statically-linked \npart of the program at once.7 It is not coincidental, therefore, that we describe these as link-time \noptimization: only when we are sure that there is no more code to be included can we know whether these \noptimization can be performed. Ilte program we ex\u00adamine must contain everything that will be included \nin the final executable, including any statically-linked library code. Without this assurance, for example, \nwe do not know how big the GAT and data sections will eventually be, and therefore cannot change GAT-based \nvariable references into GP-based references. Given that we need to examine the entire statically-linked \npart of the program, it seems quite natural to do it in the linker. There are alternatives, but each \nhas considerable drawbacks. The following sections discuss them in turn. Monolithic compilation Monolithic \ncompilation is an old technique with a venerable history. We give the compiler the ability to compile \nseveral modules at once, producing a single big object file. To per\u00adform whole-program optimizations, \nthe monolithic compile would have to include the sources files for all the modules being linked, including \nlibrary modules. In other words, we would have to do all our compilation just prior to linking, thus \nmaking compile-time and link-time essentially synony\u00admous. Monolithic compilation may be a good approach \nfor some kinds of very aggressive interproccdural optimization, because high level structure like declarations, \ntypes, prag\u00admas, and other information about the programmer s intent are readily available. Nevertheless, \nfor whole-program opti\u00admization it has three problems. First, monolithic compilation can be very expensive. \nRe\u00ad compiling all of the source modules can take one or two orders of magnitude more time than even \na very ambitious optimizing link. Monolithic compilation might be able to improve the code more than \nan optimizing linker can, but the address-calculation optimizations we discuss here are very cheap to \ndo at link-time, and a user might quite reasonably want to have them but not be willing to pay the price \nof a full-blown global optimization. Second, monolithic compilation cart be awkward. If arty libraries \nme statically-linked, the compilation must include the sources of the library modules. These sources \nmay not even be available to a user, and even if they are it is a gen\u00ad uine burden to have to pick through \nthem finding exactly those library modules needed. In practice libraries are rarely TfiOU@oM dOe~~Ot~~rr~m,ly \nSUppOrt CdtS to sharedlibraries, tiere is no fundamental problem widt doing so, since the code and data \nfor shared routines is dynamically finked in to a different parr of rhe address spacethan rhe srarically-linked \npart. Supporting such calls requires maintaining the rightsymbolicinformation, butitdoesnotaffectouranatysisofthestatically-Iinked \npart. Barring run-time optimization, however, calls to dynamically linked library routines cannot be \noptimized as statically linked calls can. included in monolithic compiles.s As a result, monolithic compilation \ncan be used for interprocedural optimization, but not for whole-program optimization. Third, in some \ncases it really is easier for a processor out\u00adside the compiler to do a complete job. Some compilers \ninclude machine-independent phases that are insulated from the machine-dependent parts. Others compile \ninto an abstract assembly language, relying on the assembler for low-level instruction selection and \npipeline scheduling. At the other extreme, many programs have modules in more than one lan\u00adguage. Even \nif a monolithic compilation system can accept a collection of modules in different languages and can \ncom\u00adpile it all the way down to the final machine language, the possibility remains that an unexpected \nlanguage or compiler will emerge that isn t included in the scheme. A third-pmty C++ compiler, for example, \nhas no way of benefiting from optimization that an in-house monolithic compiler performs. Intermediate-language \nlinking A second alternative to link-time optimization is intermediate-language linking. At compile-time, \nindividual modules are compiled only as far as art intermediate lan\u00adguage. Object files are then really \nIL-files, libraries are full of IL-modules rather than true object modules, and the final code generation \nis always done at link-time. In its pure form, this approach is usually expensive as well. Postponing \nall code generation until link-time shortens com\u00adpile time but greatly increases link time. This lessens \nthe advantages of separate compilation and makes a fast build nearly impossible even when the user has \nno need for in\u00adtertnodule optimization. As a result, this approach is rarely followed in its pure form. \nTwo different compromises are common. In one, parallel versions of each library are main\u00adtained, one \nin IL-form and one in object-form, requiring twice the disk space and encouraging inconsistencies and \nerror. In the other, IL-linking is done by recompiling atl the user mod\u00adules into IL-form and IL-1inking \nthe results, which reduces this to the case of monolithic compilation. Optimistic compilation A third \naltemativeto link-time optimization is what we might call optimistic compilation. To compile a module, \nwe make optimistic assumptions about the results of hypothetical link\u00adtime optimization, and compile \nthe code to match these Swe foflowed thi5 practice when we measured Oh 1 S improvement of compde-time \ninterprocedurafly optimized code all of the benchmark sourceswerecompiledasaottit,butnotthelibrary sources.Infact,wehave \nno sottrces for the tibrary routines, so we could not have included them in any case. This situation \nis typical of most users. results. Then all the linker must do is confirm that the as\u00adsumptions were \ntrue. If they were not, the linker reports the fact (and refuses to link) perhaps presenting advice about \nthe switches that will tell the compiler to make less optimistic assumptions. The -G option of MIPS-based \ncompilers is an example of this: modules are compiled such that vtiables with sizes below a certain default \nthreshold are included in the quickly-accessed small sections, but if there are too many such variables \nthe program will not link, and recompilation with a lower threshold is required. Optimistic compilation \nhas several disadvantages. First, selecting a set of optimistic assumptions is difficul~ the best set \nof assumptions differs from program to program. Picking a default set that are usually satisfied means \nthat final exe@a\u00adbles seldom run as fast as they could. Programmers whose programs violate the default \nassumptions are burdened with the need to understand messy details about how the compil\u00aders and linker \nwork. In essence it places responsibility on the programmer to make tradeoffs that an optimizing linker \nis in a better position to make, and requires the development of an interface to let those tradeoffs \nbe made. The program\u00admer s task is complicated further by the likely need to specify different assumptions \nfor different modules. What s left? We know of no other alternative approaches in wide use. Link-time \noptimization has several advantages over these at\u00adtematives. First, whole-program optimization can be \ndone in one place, essentially independent of the compiler and source language. Second, even optimization \nthat do not require the whole program to be present, such as conversion of JSRS to BSRS, can be done \nmore safely by the linke~ the compiler must be told whether the compiled code will be statically\u00adlinked \nor included in a shared library, but the linker atready knows. Third, statically-linked library code \ncan be included in whole-program optimization in the most natural way, since the linker handles it in \nexactly the same way that it handles user code. Fourth, when the compiler has important infor\u00admation \nto offer, it can pass it to the linker through relocation or symbol tables; getting information from \nthe linker back to the compiler is rather messier. Finally, this approach opens the door to other use \nlink-time transformations, such as the movement of very smatl pieces of loop-invariant code across procedure \nboundaries [6], or flexible program instrumenta\u00adtion tools [5]. Every program build includes a link step. \nOne naturatly wants a linker to be fast under normal circumstances: we probably do not want the linker \nroutinely to perform ex\u00adpensive optimization that require flow analysis or other compiler-style algorithms. \nThe optimization discussed here are not of this kind: they are cheap, fast optimization that should be \navailable to the user routinely, even during the build-debug-rebuild cycle of a program under development. \n7 Conclusions The conservative address-calculation code that a compiler must generate on 64-bit architectures \ncan be measurably im\u00adproved by link-time optimization. Even a quite modest ap\u00adproach can make a difference \nthis approach never moves instructions and therefore fits well in a standard linker. Big\u00adger gains are \npossible at low cost by being willing to move code. In particular, programs can be made 10 percent smaller \nand several percent faster by bringing to bear relatively sim\u00adple anatysis of the program s control structure. \nMoreover, this machinery in turn enables more sophisticated link-time optimization [6] that can improve \nthe code still further over what is possible through compile-time optimization atone. 8 Acknowledgements \nWe are very grateful to all of the Digital compiler people who have helped us with or commented upon \nthis paper. We should mention in particular Mike Rickabaugh and Ken Lesniak for their explanations and \nhelp in understanding and tracking the product linker. Mike and Ken began to explore these issues independently \nof us, and had implemented most of the OM-simple optimization by the time we heard about it. Kent Glossop \ngave us valuable information about the GEM compilers, and Mark Himelstein was invaluable in explaining \nthe AXP exception-handling protocols. Russell Kao and Jeff Mogul asked penetrating questions about dynamic \nlinking and the semantics of shared libraries. All of these people, as well as the anonymous PLDI reviewers, \nmade very useful comments on earlier drafts, which we hope we have used to greatly improve the resulting \npaper. Our thanks to you all. References [1] Digital Equipment Corporation. DEC OSF/1 Program\u00admer s Guide, \nsection 3.2.3: Name Resolution. Digital Equipment Corporation, 1993. [2] Robert B. Garner, et at. The \nScalable Processor Archi\u00adtecture (SPARC). Digest of Papers: Compcon 88, pp. 278-283, March 1988. [3] \nGerry Kane. MIPS R2000 Rise Architecture. Prentice Hall, 1987. [4] Richard L. Sites, ed. Alpha Architecture \nReference Man\u00adual. Digital Press, 1992. [5] Amitabh Srivastava and Alan Eustace. ATOM -A Sys\u00adtem for \nBuilding Customized program Analysis Tools. Proceedings of the SIGPLAN 94 Conference on Pro\u00adgramming \nLanguage Design and Implementation, to appear. Also available as WRL Research Report 94/2, March 1994. \n[6] Amitabh Srivastava and David W. Wall. A practical sys\u00adtem for intermodule code optimization at link-time. \nJour\u00adnal of Programming Languages 1(l), pp. 1-18, March 1993. Also available as WRL Research Report 92/6, \nDe\u00adcember 1992. \n\t\t\t", "proc_id": "178243", "abstract": "<p>Compilers for new machines with 64-bit addresses must generate code that works when the memory used by the program is large. Procedures and global variables are accessed indirectly via <italic>global address tables</italic>, and calling conventions include code to establish the addressability of the appropriate tables. In the common case of a program that does <italic>not</italic> require a lot of memory, all of this can be simplified considerably, with a corresponding reduction in program size and execution time.</p><p>We have used our link-time code modification system OM to perform program transformations related to global address use on the Alpha AXP. Though simple, many of these are<italic>whole-program</italic> optimizations that can be done only when we can see the entire  program at once, so link-time is an ideal occasion to perform them.</p><p>This paper describes the optimizations performed and shows their effects on program size and performance. Relatively modest transformations, possible without moving code, improve the performance of SPEC benchmarks by an average of 1.5%. More ambitious transformations, requiring an understanding of program structure that is thorough but not difficult at link-time, can do even better, reducing program size by 10% or more, and improving performance by an average of 3.8%.</p><p>Even a program compiled monolithically with interprocedural optimization can benefit nearly as much from this technique, if it contains statically-linked pre-compiled library code. When the benchmark sources were compiled in this way, we  were still able to improve their performance by 1.35% with the modest transformations and 3.4% with the ambitious transformations.</p>", "authors": [{"name": "Amitabh Srivastava", "author_profile_id": "81100062504", "affiliation": "Digital Equipment Western Research Laboratory, 250 University Ave., Palo Alto, CA", "person_id": "PP31025371", "email_address": "", "orcid_id": ""}, {"name": "David W. Wall", "author_profile_id": "81100439103", "affiliation": "Digital Equipment Western Research Laboratory, 250 University Ave., Palo Alto, CA", "person_id": "PP39042865", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/178243.178248", "year": "1994", "article_id": "178248", "conference": "PLDI", "title": "Link-time optimization of address calculation on a 64-bit architecture", "url": "http://dl.acm.org/citation.cfm?id=178248"}