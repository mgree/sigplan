{"article_publication_date": "06-01-1994", "fulltext": "\n Partial Dead Code Elimination .Jens Knoop Oliver Ruthing Bernhard Steffen Universitat Passau * CAU Kiel \nt Universitat Passau * lmoo@fmi. uni-passau.de or@informatik. uni-kiel.d400.de steffen@fmi. uni-passau.de \nAbstract A new aggressive algorithm for the elimination of par\u00ad tially dead code is presented, i.e., \nof code which is only dead on some program paths. Besides being more pow\u00ad erful than the usual approaches \nto dead code elimina\u00ad tion, this algorithm is optimal in the following sense: partially dead code remaining \nin the resulting program cannot be eliminated without changing the branching structure or the semantics \nof the programt or without impairing some program executions. Our approach is based on techniques for \npartial re\u00ad dundancy elimination. Besides some new technical problems there is a significant difference \nhere: partial dead code elimination introduces second order effects, which we overcome by means of exhaustive \nmotion and elimination steps. The optimality and the uniqueness of the program obtained is proved by \nmeans of a new technique which is universally applicable and partic\u00ad ularly useful in the case of mutually \ninterdependent program optimizationso Topics: data flow analysis, program optimization, dead code elimination, \npartial redundancy elimination, code motion, assignment motion, bit-vector data flow analyses.  Motivation \nDead code elimination is a technique for improving the efficiency of a program by avoiding the execution \nof un\u00ad Fakultiit fiir Mathematik und Informatik, Universitiit Paa\u00ad sau, Innstra5e 33, D-94032 Resau, \nGermany. tInstitut fiir Informatik und Praktische Mathematik, Christian-Albrechts-Universitat, Preui3eretraf3e \n1 9, D-24105 Kiel, Germany. Permission to copy without fee all or part of this material is granted provided \nthat the copies are not made or distributed for direct commercial advanta$e, the ACM copyright notice \nand the title of the publication and Its date appear, and notice is given that copying is by permission \nof the Association of Computing Machinery. To copy otherwise, or to republish, requires a fee and/or \nspecific permission. SIGPLAN 94-6194 Orlando, Rorida USA @ 1994 ACM O-89791 -662-XJ9410006..$3.5O necessary \nstatements at run-time. Usually, an assign\u00adment is considered unnecessary, if it is totally dead, i.e., \nif the content of its left hand side variable is not used in the remainder of the program. Thus partially \ndead assignments as the one in node 1 in Figure 1, which is dead on the left but alive on the right branch, \nare not considered. 1 y:= a+b * 2 3 Y:= 4 5 + (j b out(xb) Figure 1: A Simple Motivating Example However, \nby moving the assignment g:= a + b from node 1 to the entry of node 3 and node 4 this assign\u00adment becomes \ndead at node 3 and can be removed as shown in Figure 2. We present an aggressive algorithm for partial \ndead code elimination, which optimally captures this effect: partially dead code remaining in the resulting \nprogram cannot be eliminated without changing the branching structure or the semantics of the program, \nor without impairing some program executions. The point of our algorithm is to move partially dead statements \nas far as possible in the direction of the control flow while maintaining the program semantics. This \nprocess places the statements in an as specific con\u00adtext as possible, and therefore maximizes the potential \nof dead code, which is subsequently eliminated. This approach is essentially dual to partial redun\u00ad4 \ny:= a+b3 [Y:= 51 I Figure 2: Partially Dead Assignment Removed dancy elimination [9, 11, 12, 22, 23, \n26], where com\u00adputations are moved against the control flow as far as possible, in order to make their \neffects as universal as possible. Thus similar techniques can be applied, How\u00adever, moving assignments \nturns out to be more intri\u00adcate, because both moving and eliminating assignments can mutually influence \neach other as illustrated in Fig\u00adure 3:  P-- 4 c:=y-e 5 x:= .+1 6 I J&#38;i 7 ~ out(c) 8 out(x) 91 \nFigure 3: Illustrating Second Order Effects The most significant inefficiency in this example pro\u00adgram \nis obviously the loop invariant code fragment in node 2, which cannot be removed from the loop by standard \ntechniques for loop invariant code motion, since the first instruction defines an operand of the sec\u00adond \nassignment.1 Our algorithm performs the optimization displayed in Figure 4 in two steps: Removing the \nsecond assign\u00adment from the loop suspends the blockade of the first 1Note that even interleaving code \nmotion and copy propa\u00adgation as suggested in [10] only succeeds in removing the right hand side computations \nfrom the loop, but the assignment to x would remain in it, 41 I L --r - a 7 c:= y-e $= q A) out(c) \nx 9 % Figure 4: The Result assignment, which then can be removed from the loop as well. The systematic \ntreatment of such second order effects is an important part of our algorithm. 11 x:=a+b 21 41 >,. .. \n... ......... ..... -. In ..-., X:= (j 15 v 7- Figure 5: Illustrating the Treatment of Loops In addition \nto covering second order effects, our al\u00adgorithm captures arbitrary control flow structures and elegantly \nsolves the usual problem of distinguishing be\u00adtween profitable code motion across loop structures and \nfatal code motion into loops. In fact, it guarantees that each execution of the resulting program is \nat least aa fast as the similar execution of the original program, w the set of statements which must \nbe executed can only be reduced. This is illustrated in the example of Figure 5, which contains two loop \nconstructs, one of branching structure of the program under considera\u00ad which is even irreducible. tion. \nHowever, the algorithm sketched and discussed in their paper is not capable of moving statements out \n1 of loops or even across loops. Moreover! it only con\u00ad siders transformations that place one occurrence \nof a possibly complex partially dead statement at a single 2 later point where it is live. This restriction \nforbids some attractive optimization. For instance, in Figure 3 the assignment c := y e could not be \nremoved from node 4 4, and, as a consequence, all second order movements ,---\u00ad ,, are missed as well, \nThis example could possibly be .. . . . . . . .. ,.\u00ad dealt with by an extension vaguely mentioned in \ntheir of their paper. algorithm However$ which is even this d ~ x,= extension would still fail to capture \nmovements that t 51 require the simultaneous treatment of several occur\u00ad rences of a specific pattern. \nFor instance, in Figure 7 7 y:=y+x 8 the and partially node 2 dead assignments of can only be eliminated \na:= a + 1 at node by a simultaneous 1 treatment of both occurrences. 91 d 1 a:= a+l b) 1 9 P 10~ L 2 \n6out(a) a := a+l 2 4a:= a+] Figure 6: The Result Figure 6 shows that our algorithm moves the assign\u00adment \nof node 1 across the first irreducible loop con\u00adstruct, removes it as dead code on the branch leading \nthrough node 6, and inserts it into a new node S4,5 on the edge connecting node 4 and node 5. It is worth \nnoting that the assignment in node S4,5 is still par\u00adtially dead. However, the elimination of this partially \ndead assignment would require to move %:= a+ b into the second loop, which would dramatically impair \nsome program executions. Related Work The idea of assignment sinking and its use in dead code elimination \nin a global optimizer is already sketched in [25], However, this algorithm is restricted to a few special \ncontrol flow patterns, and does not address the general problem at all. Moreover, in [9] Dhamdhere proposed \nan extension of partial redundancy elimina\u00adtion to assignment movement, where, in contrast to our approach, \nassignments are hoisted rather than sunk, which does not allow any elimination of partially dead code. \nRecently, Feigen et al. pointed to the importance of partial dead code elimination [13]. Their algorithm \nis characterized by considering more comp~ex state\u00adments as movement candidates whenever the elemen\u00adtary \nstatements are blocked. Thus, in contrast to the usual code motion algorithms, it may modify the out(a) \ny:= a+b ~, ! .-. . -. .-. -.... , ~, \\ . ... ,. .../ 33 a:= a+l 4 Out(x+y) y:= a+b e Outfx+y) i 4 c13 \n52+out(a+b) 5 a:= a+l out(a+b) 61ZI 6 % i t Figure 7: Illustrating m-to-n Sinkings 13riggs and Cooper \ns algorithm [4] published in this proceedings employs instruction sinking for the reas\u00adsociation of expressions. \nAs a by-product some par\u00adtially dead assignments can be removed. However, in contrast to our algorithm \ntheir strategy of instruction sinking can significantly impair certain program execu\u00adtions, since instructions \ncan be moved into loops in a way which cannot be repaired by a subsequent partial redundancy elimination. \nFor example in Figure 6 their algorithm would sink the instruction of node S4,5 into the loop to node \n7. Note that subsequent partial re\u00addundancy elimination fails to hoist it back because of safety reasons. \nIn [7, 8] Dhamdhere presents an application of code hoisting and sinking techniques to register assignment \nwhich, however, does not provide a contribution to the general problem of partial dead code elimination. \n Finally, instruction scheduling techniques me usu-a flow graphy i.e. y by edges leading from a node \nwith ally restricted to basic blocks or for-loops and focus on more than one successor to a node with \nmore than one specific goals of code generation, for instance to yield predecessor (cf. [6, 10, 22, 23]). \nshort evaluation sequences with respect to some ma\u00ad chine model [1, 28], or to prepare the code for efficient \nexecution on a parallel or pipelined machine [3, 15]. Structure of the Paper The paper develops along \nthe following lines. After the preliminary Section 2, Section 3 presents the cen\u00adtral notions of our \napproach and establishes the es\u00adsential feat ures of partial dead code elimination. Sub\u00adsequently, Section \n4 gives a detailed discussion about second order effects, and Section 5 develops our algo\u00adrit hrn. Finally, \na complexity estimation is presented in Section 6 and conclusions are drawn in Section 7 . 2 Preliminaries \nWe consider variables a c V, terms tE T, and di\u00ad rected flow graphs G = (N, E,s, e) with node set N and \nedge set E. Nodes n ~ N represent basic blocks of statements, edges (m, n) ~ E thenondeterministic branching \nstructure of G, ands and e the unique start node and end node of G, which are both assumed to represent \nthe empty statement skip and not to possess any predecessors and successors, respectively. Statements \nare classified into the following three groups: the assignment statements of the form v:= t, the empty \nstatement skip, and the relevant statements forcing all their operands to be alive. For the ease of presentation, \nthe relevant statements are given by ex\u00adplicit output operations of the form out(t) here.2 We will further \nuse the notion lhs to refer to the left hand side variable of an assignment statement J, Moreover, $ucc(n)=df \n{ m ~(n, m) 6 E } and P-red(n)=df { m \\ (m, n) c E } denote the set of all suc\u00adcessors and predecessors \nof a node n, respectively. A path p in G is a sequence of nodes (nl, ,. ,, nk ), where v I < i < k. n;+l \nE succ(ni), and P[rn, n] denotes the set of all finite paths from m to n. Every node n ~ N is assumed \nto lie on a path from s to e. Finally? an as\u00adsignment pattern a is a string of the form x := -t. The \nset of all assignment patterns (occurring in a program) is denoted by AP. 2.1 Cr~tical Edges Like partial \nredundancy elimination also partial dead code elimination can be blocked by critical edges in 2 In practice, \n~~~ditions in if-statements and ~signments to global variables (i.e., variablee whose declaration is \noutside the scope of the flow graph under consideration) must be considered relevant MI well. It is straightforward \nto extend our approach accordhgly. @Ab) I 1 x:=a+b 1 ..... sl,~[{}~ a+b ~ . . . . . .. 2 3x:= 2 3x:= \nb% Figure 8: Critical Edges In Figure 8(a) the assignment z:= a + b at node 1 is partially dead with \nrespect to the assignment at node 3. However, this partially dead assignment can\u00adnot safely be eliminated \nby moving it to its successors? because this may introduce a new assignment on a path entering node 2 \non the left branch. On the other hand, it can safely be eliminated after inserting a synthetic node S1,2 \nin the critical edge (1, 2), as illustrated in Figure 8(b). In the following, we therefore restrict our \nattention to programs where every critical edge has been split by inserting a synthetic node,  3 Partial \nDead Code Elimina\u00adtion Conceptually, partial dead code elimination stands for any sequence of o assignment \nsinkings and o dead code eliminations  as formally defined below. Definition 3.1 (Assignment Sinking) \nLet Q ~ x:= t be an assignment pattern. An assign\u00adment sinking for a is a program transformation that \n* eliminates some occurrences of a, e inserts instances of a at the entry or the exit of some basic blocks \nbeing reachable from a basic block with an eliminated occurrence of a. In order to be admissible, the \nsinking of assignments must be semantics preserving. Obviously, the sinking of an assignment pattern \na = x:= t is blocked by an instruction that e modifies an operand of t or e uses the variable z or E \nmodifies the variable ~. Thus, we define: Definition 3.2 (Admissible Assign. Sinking) /in assignment \nsinking for a is admissible, i,, it sat\u00adisfies the following two conditions: 1. The t-emoved assignments \nare substituted, i.e., on every program path leading from n to e, where an occurrence of Q has been eliminated \nat n, an in\u00adstance of a has been inserted at a node m on the path such that a is not blocked by ang instruction \nbetween n and m. 2. The inserted assignments are justified, i.e., on ev\u00adey program path leading from \ns to n, where an instance of @ has been inserted at n, an occur\u00adrence of cs has been eliminated at a \nnode m on the path such that Q is not blocked by any instruc\u00adtion between m and n.  Definition 3.3 (Assignment \nElimination) An assignment elimination for a is a program trans\u00adformation that eliminates some original \noccurrences of cs in the argument program. Like the sinking of assignments also their elimination must \nbe admissible, which leads to the notion of dead assignments. An occurrence of an assignment pattern \na~ z:= t in a basic block n is dead, if its left-hand side variable z is dead, i.e., on every path from \nn to e every right-hand side occurrence of z following the considered instance of a is preceded by a \nmodification of Z. This simple definition, however, is too strong in order to characterize all assignments \nthat are of no use for any relevant computation. The following re\u00adcursive definition yields such a characterization. \nAn occurrence of an assignment pattern a a z := t in a basic block n is faint (cp. [16, 18]), if its \nleft-hand side variable w is faint, i.e. on every path from n to e every right-hand side occurrence of \nx following the instance of a is either preceded by a modification of a or is in an assignment whose \nleft hand side variable is faint as well. The following example taken from [18] shows a faint assignment \nwhich is out of the scope of dead code elimination. Thus faint code elimination is more powerful than \ndead code elimination. On the other hand, in contrast to faint code elimination dead code elimination \ncan be based on an efficient bit-vector data flow analysis. We therefore consider both techniques in \nthe sequel. Definition 3.4 (Dead (Faint) Code Elim.) A dead (faint) code elimination for an assignment \npat\u00adtern cs is an assignment elimination for a, where some dead (faint) occurrences of a are eliminated. \n 1 2 3 X:=X+1 @ Figure 9: A Faint but not a Dead Assignment It is worth noting that any admissible assignment \nsink\u00ading preserves the program semantics. This is not true for assignment eliminations. In fact, even \ndead (faint) code eliminations may change the semantics of a pro\u00adgram by reducing the potential of run-time \nerrors.3 However, these are the only possible changes of the semantics induced by dead (faint) code elimination. \nIn particular, the evaluation of every program instruction which remains in the program is guaranteed \nto behave exactly as before. Definition 3.5 (Part. Dead (Faint) Code Elim.) Partial dead (faint ) code \nelimination PDE (PFE) is an arbitrary sequence of admissible assignment sink\u00adings and dead (faint) code \neliminations. r h the fOllOWing We Wi]] Write ~ l_pDE @ (G bpFE @) if the flow graph G results from applying \nan admis\u00adsible assignment sinking or a dead (faint) code elimi\u00adnation to G. For a given flow graph G \nwe denote the universe of programs resulting from partial dead (faint) code elimination T G {PDE, PFE} \nby G.=4{G I GI-; G J For the rest of this section let T c {PDE, PFE}. A key notion of this paper then \nis: Definition 3.6 (Optimality of PDE (PFE)) 1. Let G , G 6 ~~. Then G is better4 than G , in signs \nG L G!, if and only if ~p ~ p[S, e] VCI E w@. CY#(fW) < CI#(f@I) where ff#(p@ ) and ~#(p@ ) denote the \nnumber of occurrences of the assignment pattern a on p in G: and G , respectively.5 3Think e.g. of an \noverflow or a dk ision by zero caused by the evaluation of the right hand side term of an eliminated \naeeignment. 4Note that t~le relation is reflexive. In fact, at least as good would be the more precise \nbut uglier notion. 5Remember that the branching etructure is preserved. Hence, starting from a path in \nG, we can eaaily identify corresponding paths in G and G . ,2 G 6 g, isoptima] ij and only if G* is \nbetter than any other program in G.. The relation better is a pre-order cm g7, Le., it is re\u00adflexive \nand transitive (but not antisymmetric). Hence, there may be several programs being optimal in the sense \nof Definition 3.6. On the other hand, it is not obvious that g. has an optimal element at all. VVe will \ntherefore present a constructive criterion guaranteeing the existence of an optimal element. This criterion, \nwhich is based cm a slight generalization of Tarski s Fixpoint 13eorem, is tailored to deal with. mutually \ninterdependent (program) transformations. In this set\u00adting, we consider the partial order E7 on ~. defined \nby &#38; =~f (~ (l F,)*, and a finite family of functions YG ,G EGVE F,. G &#38; (7 -f(q c=f(G ) Given \nsuch a family of functions F=, we can apply the generalized version of Tarski s Fixed Point Theorem presented \nin [14], in order to obtain: Theorem .3.% (Existence of Optitial Rograms) ~, has an optimal element (uwt \n~) which can. be com\u00adputed @ any sequence o~~unction applications that con\u00adtains ail elements of ~. sufficiently \nojten. The optimal program is not unique. However, one can show that there exists a canonical representative \nwhich is unique up to some reordering in basic Mocks.  4 Second Order Effects in Par\u00adtial Dead code \nEh nination h ~hk section we will discuss the interdependencies between the various sinking and elimination \nsteps of assignments. For comparison let us first consider the situation in partial redundancy elimination. \nAlso par\u00adtial redundancy elimination is conceptually composed of two kinds of elementary program transformations. \nFirst, hoisting computations, and second, eliminating tot al redundancies. However, partial redundancy \nelim\u00adination can be done independently for every program term t. Thus a single application of each step \nis suf\u00adficient to yield an optimal result. Unfortunately, this does not hold for partial dead code elimination. \nIn fact, there are various kinds of second order effects that need to be considered. We are now going \nto systemat\u00adically discuss these effects, which are fully captured by our algorithm presented in Section \n!5. 4,1 Sinking-Elimination Effects This is the effect of primary interest: an assignment is sunk. until \nit can be eliminated by dead (faint) code elimination. Reconsider the motivating examples in Section \n1 for illustration. 4.2 SM&#38;g-SM&#38;g Efkts The sinking of am assignment may open the way for other \nassignments to sink, if it is a use-or redefinition site for these assignments or if it modifies an operand \nof its right-hand side term. The last case is illustrated in Figure 10(a). Without previously sinking \nthe as\u00adsignment of node 2 the assignment of node 1 can sink at most to the entry of node 2. Here it is \nblocked, since any further sinking would corrupt the value of its right-hand side expression. However, \nanticipating the sinking of the assignment at node 2 to node .5, the as\u00adsignment at node 1 can be sunk \nto node 3 and node 4, where dead code elimination finally removes the oc\u00adcurrence at node 3 as displayed \nin Figure 10(b). 2 a:= 2 ~ y:= 4 ~ y,= ~ y:=a+b 5 x:= a+c 5 a:= x:= a~c * Olft(x$y)  (j IEl* Out(x+y) \n 6 da t i Figure 10: Sinl&#38;g-Sinking Effect 4.3 Elimination-Sinking Effects For similar reasons as \nabove, the elimination of dead as\u00adsignments may enable the sinking of other assignments, see Figure 1l(a) \nfor illustration. Here, none of the as\u00adsignments at node 1 and node 2 can be sunk without violating the \nadmissibility. However, the assignment a:= . . . at node 1 can be removed by dead code elim\u00adination, \nsince its value is not used anymore. Now this removal enables the assignment ~ := a + b to be sunk to \nnode 4 and 5 in order to eliminate further partially dead assignments leading to the program displayed \nin Figure Ii(b). b) i Figure 11: Elimination-Sinking-Effect  4.4 Elimination-Elimination Effects This \neffect is illustrated in Figure 12(a). Here, the as\u00adsignment at node 4 is dead and can be eliminated, \nsince on every path leading to the end node the left-hand side variable v is redefined before it is used. \nSubsequently, the assignment to a at node 1, which was not dead before due to its usage at -node 4, becomes \ndead and can be removed as shown in Figure 12(b). It is worth noting that this example shows a second \norder effect for partial dead code elimination but a first order effect for partial faint code elimination: \nboth as\u00ad signments at node 1 and node 4 are faint, and hence could be eliminated simultaneously by faint \ncode elim\u00ad ination. 2 3 4 y:= a+b fj y,= c+d + Figure 12: Elimination-Elimination Effect  5 The Algorithm \nIn this section we present our algorithm for the opti\u00admal elimination of partially dead (faint) assignments. \nWe first give an overview of the algorithm, and subse\u00adquently describe the relevant steps in more detail. \n5.1 Overview The algorithm pde (pfe) consists of two main proce\u00addures that are repeated until the program \nstabilizes: A procedure dce ( jbe) for the elimination of dead (faint) assignments, which is controlled \nby a dead (faint) variable analysis. A procedure ask for assignment sinking, which is controlled by a \ndelayability analysis working on bit-vectors of sinkhg candidates. We are now going to describe these \nprocedures in detail. A complexity estimation is given in Section 6.  5.2 Eliminating Dead (Faint) Assign\u00adments \nThe elimination of dead (faint) assignments is based on the determination of dead (faint) variables, \nDead variables can be computed by means of a backwards directed bit-vector data flow analysis [2, 17, \n24], A standard formulation can be found in Table 1, where N-DEAD,(z) (or X-DEAD,(%)) mean that variable \nz is dead at the entry (or exit) of statement t. Addi\u00adtionally, this table shows the equation system \nfor the faint variable analysis, where analogously to the dead variable analysis N-FAINT,(z) (or X-FAINT,(Z)) \nmean that variable x is faint at the entry (or exit) of statement L. Though the faint problem does not \nhave a bit-vector form, it can easily be solved by means of an iterative worldist algorithm operating \nslotwise on bit\u00advectors (cp. [10]). The only subtlety here is that a slot (L, z) for an assignment statement \nL maybe influenced not only by the mslot of some successor node i, but also by the slot (L, VW,). This \nmust be taken care of by additionally updating the worklist with all slots (L, z), where z is a right-hand \nside variable of L, whenever the slot (L, VW,) has been processed successfully. R is worth noting that \nthis does not cause any problems for the correctness and complexity of the method (cf. Section 6.1). \nAfter having computed the greatest solution for one of the equation systems specified in Table 1, the \ncorre\u00adsponding program transformation is very simple: The Elimination Step: Process every basic Mock \nby successively eliminating all assignments whose left-hand side variables are dead (faint) immediately \nafter them. Standard methods to dead code elimination are usually based on definition-use graphs [2, \n21], which connect the definition sites of a variable with their correspond\u00ading use sites, Thus, dead \nassignments can be identi\u00adfied indirectly by means of a simple marking algorithm working on the definition-use \ngraph. If this algorithm uses optimistic assumptions every faint assignment is  detected in time proportional \nto the size of the graph. order: Unfortunately, definition-use graphs are usually quite The Insertion \nStep;large, i.e. of order O(i2 v) in the worst case, where i Process every basic block by successively \ninserting in\u00addenotes the number of instructions and v the number stances of every assignment pattern \na at the entry (orof variables occurring in the flow graph /30]. The al\u00ad exit) of n if N-INSERT.(a) (or \nX-INSERT.(a)) gorithm of [5] improves on this result by working on is satisfied. a sparse definition-use \ngraph based on the SSA form. This results in a worst case time complexity of O(i v), which coincides \nwith the complexity of our simple iter\u00ad 5.4 Termination of the ~lobal Algo\u00adative algorithm (cf. Section \n6.1). rit hm The algorithm terminates as soon as both steps, the 5.3 Sinking of Assignments dead (faint \n) code elimination and the assignment The program transformation of this stage is based on a sinking \nleave the program invariant. In the case delayability analysis ([22, 23]), which was designed to of dead \n(faint) code elimination this simply meam determine how far a hoisted computation can be sunk that no \nfurther assignments are eliminated, and in from its earliest initialization point in order to mini-the \ncase of assignment sinking this holds, if ev\u00admize the lifetimes of temporaries introduced by par-ery \nbasic block n satisfies N-INSERT. = false and tial redundancy elimination, while maintaining compu-X-INSERT. \n= LOCDELAYEDm. tational optimality. Table 2 presents the delayability analysis adapted to our situation \nin a bit-vector format,  5.5 Results where each bit corresponds to an assignment pattern occurring \nin the program. Here N-DELAYED. and Denoting the final programs that result from applying X-DELAYEDn \nintuitively mean that some sinking our algorithm for partial dead (faint ) code elimination candidates \nof Q can be moved to the entry or the exit to G by Gp~. and GPfe, respectively, we have: of basic block \nn respectively, and sinking candidates are occurrences of an assignment $:= t inside a basic Theorem \n5.1 (Correctness) block that are not Mocked, i.e., neither followed by a J. GPA G GPDEmodification of \nan operand of t nor by a modification or a usage of $. See Figure 13 for illustration. Note ~. GPf. \nC GPFE that among the various occurrences of an assignment pattern in a basic Mock at most the last one \nis a can- Moreover, it is easy to prove that .FpDE=,jf {dce, ask} didate for global sinking, because \nevery occurrence is and f_PFE =df {fee, ask} satisfy the dominance and Mocked at least by the subsequent \noccurrence. rn . .\u00ad monotonicit y property of Section 3, Hence, we can apply Theorem 3.7, which establishes \nthe following op\u00adtimality results: :=:= y a+b y a+b Theorem 5.2 (OptimaHty Theorem) a:=c x := 39 x := \n3*y 1. GPde is optimal in ~pDE y := a+b ?$$ , 2. GPfe is optimal in L7PFE a:=c a:=d :!w~i Sinking Candidate \n6 Complexity Figure 13: Sinking Candidates of y:= a + b Parameterized in the complexities of its components, \n The greatest solution of the equation system dis-e Cdce, cf., and c ..h : the complexities of the played \nin Table 2 characterizes the program points, data flow analyses of the corresponding compo\u00adwhere instances \nof the assignment pattern a must nent transformations, be inserted, by means of the insertion predicates \nOv; the maximal factor by which the number of N-INSERT and X-INSERT. The subsequent pro\u00adinstructions \nmay increase during the application gram transformation is again very simple, because it of the algorithm, \nand can easily be shown that all assignment patterns that must be inserted at a particular program point \nare in-6Due to edge ~Plitting th~re ,g,reno insertions at the exit of dependent and can therefore be \nplaced in an arbitrary branching nodes. Local Predicates e USED (Z): ~ is a right-hand side variable \nof the instruction ~.a e REIN-USED (z): z is a right-hand side variable of the relevant instruction I,. \nE ASS-USED (a): z is a right-hand side variable of the assignment stat ernent ~. e MODL(Z): z is the \nleft-hand side variable of the instruction ~. The Dead Variable Analysis: (In bit-vector forrnulation~) \nN-DEAD, =4 7USEDL * (X-DEAD, + MOD,) X-DEAD, =df N-DEADi n Xswcc($) The Faint Variable Analysis: (Slotwise \nsimultaneously for all variables z) N-J?AINT,(z) =df lRELV-USED,(Z) * (X-FAINT,(Z) + MOD,(Z)) * (X-FAINT, \n(lhs,) -I-TASS-USED,(Z)) X-FAINT,(Z) =M N-FAINT2($) n K8UCC(L) Table 1: Dead &#38; Faint Variable Analysis \nIn particular, all variables occurring in relevant statements are considered right-hand side variables. \nb~otb ~+5e5 me employed at the instruction level. This, however, is important only for the fht variable \nmalysis. In fact, the dead variable analysis can straightforwardly be modified to work on basic blocks. \nLocal Predicates: o LOCDELAYE13n(cs): There is a sinkhg candidate of a in n. e LOC13LOCKEDn(cs): The \nsinking of a is blocked by some instruction of n. Delayabilit y Analysis: fake if n=s N-DELAYED. =W ~ \nX-DELAYED~ othewise { mQ.red(n) X-DELAYED. =df LOCDEL.AYED. + N-DELAYED. * lLOCBLOCKEDn Insertion Points: \nN-INSERT. =df N-DELAYED. * LOCBLOCKEDn X-INSERT. =df X-DELAYED. * TN-DELAYEDm x m~succ(n) Table 2: Delayability \nAnalysis and Insertion Points Or: the maximal number of applications of the faint code elimination is \nproportional to both the num\u00adcomponent t ransformat ions? ber of instructions and the number of program \nvari\u00ad pale: O(r (c~ce + ca.~ + w oi)) pfe: O(r (cf.. + ca,k + w o i)) where i denotes the number of instructions \noccur\u00ad ring in the original program. The factor w ci is caused by the actual intermediate transformations \nand up\u00ad dates of the local predicates. The following three subsections provide a detailed es\u00ad timation \nof the parameters mentioned above in terms of the number of basic blocks b, the rmmber of in\u00ad st ructions \ni, the number of variables v and the num\u00ad ber of assignment patterns a of the original program. Subsequently, \nthe overall complexity is sketched more roughly in terms of a uniform parameter n reflecting the program \nsize of the argument program. 6.1 Complexity of the Component Transformations  6.1.1 Delayability Analysis \n The delayability analysis realizing the essential part of the assignment sinking procedure is a forward \ndirected bit-vector data flow analysis. For well-structured flow graphs the efficient bit-vector techniques \n[19, 20, 29] become applicable, yielding an almost linear complex\u00adit y in terms of fast bit-vector operations. \nFor arbitrary control flow structures, however, the slotwise approach of [10] is the best we can do yielding \nO(b . a) as the worst case time complexity for the assignment sinking procedure. 6.1.2 Dead (Faint) \nVariable Analysis Like the delayability also the dead code analysis is a bit-vector problem. Thus replacing \nthe parameter a by v the same estimations apply for the worst case time complexity. Unfortunately, the \n,fuint variable analysis is not a bit-vector problem, i.e. the solution cannot be computed for each variable \nindependently. Thus there are no special algorithms for structured programs, and the slotwise approach \nof [10] must always be applied. Note that the structure of faint code analysis requires a computation \nat the instruction level. We will now prove that under the usual assumptions that E the size of program \nterms is bound by a constant, and E the number of edges is of order b ables. Investigating the equation \nsystem of the faint vari\u00adable analysis of Table 1 reveals that during the iterative computation of the \ngreatest solution of the equation system each slot for a variable ~ can change its value at most once \nfrom true to false. These changes are the only reason for updating the worklist: E if the slot is part \nof an entry bit-vector at an in\u00adstruction L then for all predecessors L1 of ~ the z-slots of the exit \nbit-vectors are added to the cur\u00adrent worklist, and e if the slot is part of an exit bit-vector of an \nin\u00adstruction ~ then al~ ~-slots of the same bit-vector are added to the current worklist where y is a \nright-hand side variable of ~. Thus one can easily establish that every edge of the control flow graph \nwill be considered at most v times during the analysis in order to reach the correspond\u00ading successor. \nApplying our assumptions therefore yields that the number of worklist entries written by the faint code \nanalysis algorithm is at most propor\u00adtional to the program size and the number of program variables. \nA similar argument suffices to prove that the global cost of the faint code analysis, including the ef\u00adfort \nfor slot processing, is still of the same order, which completes the proof. As the size of the program \nmay increase by a factor w during the execution of our algorithm (see Section 6.2), the intermediate \nprogram size during the execution of our overall algorithm can only be estimated by O(w. i), yielding \na worst case complexity of O(w oi ov) for the required faint ana~ysis steps.  6.2 Estimating the Code \nSize Using induction on the length of a shortest (acyclic) path p reaching a node n it can easily be \nshown that the number of instructions that can be inserted at n is bound by the number of instructions \non p. Thus the number of instructions at a basic block will never exceed i showing that w is of order \nO(b) in the worst case. In practice, however, we expect that w is bound by a constant.  6.3 Estimating \nthe Number of Itera\u00adtions Applying the shortest path argument of Section 6.2 again, the number of assignments \nthat are inserted at a node n during the application of the overall algorithm is bound by i. Thus the \nnumber of dce (fee) and ask applications can be estimated by i. b, yielding that r is at most quadratic \nin-the program size. However? we conjecture that r only depends linearly on i.  6.4 Summary Combining \nthe results of the previous subsections, we have: E O(r), O(cdce), o(c~~k), and O(w i) can be esti\u00admated \nby O(n2), and E O(cfce) can be estimated by O(ns). This guarantees that partial dead code elimination \nis of order O(n4) and partial faint code elimination is of order 0(n5) in the worst case. These estimations \nare very pessimistic. Already us\u00ading our conjecture would reduce the complexities to 0(n3) and O(n4), \nrespectively, Moreover, using the reasonable assumption that the factor w that indicates the degree of \nstatic code replication is of order saves another factor of n for the partial faint elimination, and \nit saves (almost) a factor of n for tial dead code elimination whenever fast bit-vecor niques are applicable. \nThus we expect a quadratic haviour (O(n2)) for partial dead code elimination at most a cubic behaviour \n(O(n3 ) ) for partial faint elimination. Summarizing, the overall time bound for our 0(1) code par\u00adtech\u00ad \nbe\u00adand code algor\u00ad ithms is only slightly worse than the one for the sig\u00adnificantly weaker technique \nof dead code elimination based on definition-use graphs, and it is comparable with the complexity of \nother aggressive code motion techniques. E.g., the algorithm for global value num\u00adbering of [27], which \nrequires reducible flow graphs and guarantees optimality only for acyclic program struc\u00adtures, is of \nthird order,  Conclusions We have presented a new aggressive algorithm for the optimal elimination \nof partially dead (faint) code, which captures all second order effects that are due to the mutual dependence \nbetween assignment sink\u00ading and dead (faint) code elimination. This algorithm is comparably expensive \naa other aggressive optimiza\u00adtion methods. Its complexity ranges from 0(n2) for the dead version and \nrealistic structured programs to O(n5) for the faint version and the completely unre\u00adstricted worst case. \nThus as other aggressive methods, our algorithm should typically be employed for the op\u00adtimization of \ntime-critical sections of code of moderate size. In general, modifications of our algorithm should be \napplied that limit the number of assignment sinking and dead (faint) code elimination steps. We are cur\u00adrently \ninvestigating heuristics guiding this limitation, which range from simply cutting the global iteration \nprocess after some given amount of time or a fixed num\u00adber of iterations to localizing the optimization \nprocess to hot areas , 8 Acknowledgements We are very grateful to the anonymous referees, and Gerald \nLuttgen for their valuable comments,  References [1] A. V, Aho, S. C, Johnson, generation for expressions \npressions. Journai of the 1977, [2] A. V. Aho, R. rs: Principles, Wesley, 1985. [3] D. Bernstein tion \nscheduling Sethi, and and J. D. Unman. Code with common subex-ACM, 24(1):146 -160, J. D. Unman. Compile- \nTechniques and Tools. Addison\u00ad and M. Rodeh, Global instruc\u00adfor superscalar machines, In Proc. ACM SIGPLAIV \nConference on Program\u00ad ming Language Design and Implementation 91, volume 26,6 of ACM SIGPLAN Noticesy \npages 241 255, Toronto, Ontario, June 1991. [4] P, Briggs and K. D. Cooper. Effective partial re\u00addundancy \nelimination, In Proc. ACM SIGPLAN Conference on Programming Language Design and Implementation 9d, 1994, \n[5] R. Cytron, J. Ferrante, B. Rosen, M. Wegman, and F. K. Zadeck. Efficiently computing static sin\u00adgle \nassignment form and the control dependency graph. ACM Transactions on Programming Lan\u00adguages and S@ems, \n13(4):451 49o, 1991. [6] D. M. Dhamdhere. A fast algorithm for code movement optimization. ACM SIGPLAN \nNotices, 23(10):172 -180, 1988. [7] D. M, Dhamdhere. Register assignment using code placement techniques. \nJournal of Computer Lan\u00adguages, 13(2):75 -93, 1988, [8] D. M. Dhamdhere. A usually linear algorithm for \nregister assignment using edge placement of load and store instructions. Journal of Computer Lan\u00adguages, \n15(2):83 94, 1990. [9] D. M. Dhamdhere. Practical adaptation of the global optimization algorithm of \nMorel and Ren\u00advoise. ACM Transactions on Programming Lan\u00adguages and S~stems, 13(2):291 294, 1991. Tech\u00adnical \nCorrespondence. [10] D. M. Dhamdhere, B. K. Rosen, and l?. K. Zadeck. How to analyze large programs \nefficiently and in\u00adformatively. In Proc. ACM SIGPLAN Conference on Programming Language Design and Implemen\u00adtation \n92, volume 27,7 of ACM SIGPLAN Notices, pages 212-223, San Francisco, CA, June 1992. [11] K.-H. Drechsler \nand M. P. Stadel. A solution to a problem with Morel and Renvoise s Global optimization by suppression \nof partial redundan\u00adcies s ACM Transactions on Programming Lan\u00adguages and Systems, 10(4):635 640, 1988. \nTech\u00adnical Correspondence. [12] K.-IL Drechsler and M. P. Stadel. A variation of Knoop, Riithing and \nSteffen s lazy code motion. ACM SIGPLAN Notices, 28(5):29 -38, 1993. [13] L, Feigen, D. Klappholz, R. \nCasazza, and X. Xue. The revival transformation. In Conf. Record of the 21nd ACM Symposium on the Principles \nof Pro\u00adgramming Languages, Portland, Oregon, 1994. [14] A. Geser, J. Knoop, G. Luttgen, O. Ruthing, and \nB. Steffen. Chaotic fixed point iterations, MIl?-Bericht 9403, F akultat fur Mathematik und Infor\u00admatik, \nUniversit at Passau, Germany, 1994. [15] P. B. Gibbons and S. S. Muchnik. Efficient in\u00adstruction scheduling \nfor a pipline architecture. In Proc. ACM SIGPLAN Sjmposium on Compiler Construction 86, volume 21, 7 \nof A CM SIGPLAN Notices, pages 11-16, June 1986. [16] R. Giegerich, U. Moncke, and R. Wilhelm. In\u00advariance \nof approximative semantics with respect to program transformations. In Proc. of the third Conference \nof the European Co-operation in Infor\u00admatics, Informatik-Fachberichte 50, pages 1 10. Springer, 1981. \n[17] M. S. Hecht. Flow Anal@s of Computer Pro\u00adgrams, Elsevier, North-Holland, 1977. [18] S. Horwitz, \nA. Demers, and T. Teitelbaum. An efficient general iterative algorithm for data flow analysis. Acts Informatica, \n24:679 694, 1987. [19] J. B. Kam and J, D. Unman. Global data flow analysis and iterative algorithms. \nJournal of the ACM, 23(1)! 158 -171, 1976. [20] K. Kennedy. Node listings applied to data flow analysis. \nIn Conf. Record of the Pd ACM Sgm\u00adposium on the Principles of Programming Lan\u00adguages, pages 10-21, Palo \nAlto, CA, 1975. [21] K. Kennedy. A survey of data flow analysis tech\u00adniques. In S. S. Muchnick and N. \nD. Jones, editors, Program Flow Analysis: Theory and Applications, chapter 1, pages 5 54. Prentice Hall, \nEnglewood Cliffs, NJ, 1981. [22] J. Knoop, O. Riithing, and B. Steffen. Opti\u00admal code mot ion: Theory \nand practice. To ap\u00adpear in ACM Transactions on Programming Lan\u00adguages and Systems. Available as MIP-Bericht \n9310, Fakultat fur Mathematik und Informatik, Universitat Passau, Germany, 1993.35 pages. [23] J. Knoop, \n0. Ruthing, and B, Steffen. Lazy code motion. In Proc. ACM SIGPLAN Conference on Programming Language \nDesign and lmplementa\u00adtion 92, volume 27,7 of ACM SIGPLAN Notices, pages 224-234, San Francisco, CA, \nJune 1992. [24] L. T. Kou. On live-dead analysis for global data flow problems. Journal of the ACM, 24(3):473 \n\u00ad483, July 1977, [25] R. J. Mintz, G. A. Fisher, and M. Sharir. The design of a global optimizer. In \nProc. ACM SIG-PLAN Symposium on Compiler Construction 79, volume 1~, 8 of ACM SIGPLAN Notices, pages \n226 23$ Denver, Col., 1979. [26] E, Morel and C. Renvoise. Global optimization by suppression of partial \nredundancies. Communica\u00adtions of the ACM, 22(2):96 -103, 1979. [27] B. K. Rosen, M. N. Wegman, and F. \nK. Zadeck. Global value numbers and redundant computa\u00adtions. In Conf. Record of the l~h ACM Sgm\u00adposium \non the Principles of Programming Lan\u00adguages, pages 12-27, San Diego, CA, 1988. [28] R. Sethi and J. D. \nUnman. The generation of opt imal code for arithmetic expressions. Journal of the ACM, 17(4):715 728, \n1970, [29] R. E. Tarjan. Applications of path compression on balanced trees. Journal of the ACM, 26(4):690 \n-715, 1979. [30] M. N. Wegman and F. K. Zadeck. Constant prop\u00adagation with conditional branches. ACM \nTrans\u00adactions on Programming Languages and Systems, 13(2), April 1991.   \n\t\t\t", "proc_id": "178243", "abstract": "<p>A new aggressive algorithm for the elimination of partially dead code is presented, i.e., of code which is only dead on some program paths. Besides being more powerful than the usual approaches to dead code elimination, this algorithm is <italic>optimal</italic> in the following sense: partially dead code remaining in the resulting program cannot be eliminated without changing the branching structure or the semantics of the program, or without impairing some program executions.</p><p>Our approach is based on techniques for partial redundancy elimination. Besides some new technical problems there is a significant difference here: partial dead code elimination introduces second order effects, which we overcome by means of exhaustive motion and elimination steps. The optimality and  the uniqueness of the program obtained is proved by means of a new technique which is universally applicable and particularly useful in the case of mutually interdependent program optimizations.</p>", "authors": [{"name": "Jens Knoop", "author_profile_id": "81100197024", "affiliation": "Universit&#228;t Passau", "person_id": "PP40024734", "email_address": "", "orcid_id": ""}, {"name": "Oliver R&#252;thing", "author_profile_id": "81100258003", "affiliation": "CAU Kiel", "person_id": "PP31094852", "email_address": "", "orcid_id": ""}, {"name": "Bernhard Steffen", "author_profile_id": "81100210663", "affiliation": "Universit&#228;t Passau", "person_id": "PP40024876", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/178243.178256", "year": "1994", "article_id": "178256", "conference": "PLDI", "title": "Partial dead code elimination", "url": "http://dl.acm.org/citation.cfm?id=178256"}