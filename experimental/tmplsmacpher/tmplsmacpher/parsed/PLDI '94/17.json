{"article_publication_date": "06-01-1994", "fulltext": "\n Cache Performance of Garbage-Collected Programs Mark B. Reinhold NEC Research Institute Four Independence \nway Princeton, New Jersey 08540 mbr@research. nj. net. com Abstract. As processor speeds continue to \nimprove relative to main-memory access times, cache performance is becom\u00ading an increasingly important \ncomponent of program perfor\u00admance. Prior work on the cache performance of garbage\u00adcollected programs \neither argues or assumes that conventional garbage-collection methods will yield poor performance, and \nhas therefore concentrated on new collection algorithms de\u00adsigned specifically to improve cache-level \nreference locality. This paper argues to the contrary: Many programs writ\u00adten in garbage-collected languages \nare naturally well-suited to the direct-mapped caches typically found in modern computer systems. Garbage-collected \nprograms written in a mostly\u00adfunctional style should perform well when simple linear stor\u00ad age allocation \nand an infrequently-run generational compact\u00ading collector are employed; sophisticated collectors intended \nto improve cache performance are unlikely to be necessary. As locality becomes ever more important to \nprogram perfor\u00ad mance, programs of this kind may turn out to have a signif\u00ad icant performance advantage \nover programs written in tradi\u00ad tional languages. 1. Introduction One of the most prominent trends in \ncomputer technology involves the relative speeds of processors and main-memory chips: Processors are \ngetting faster, by a factor of 1.5 to 2 per year, while the speed of main-memory chips is improving only \nslowly [13, 14]. This widening gap has mot ivat ed hardware designers to seek improved performance by \ninserting one or more high-speed cache memories between the processor and the main memory. A cache miss \non current high-performance machines costs tens of processor cycles; if the present trend continues, \na miss on such machines will soon cost hundreds of cycles. Thus cache performance is becoming an increasingly \nimportant component of program performance. Garbage collectors have long been used to improve the per\u00adformance \nof programs by improving their virtual-memory per\u00adformance; this is done by designing the collector to \nmove heap\u00adallocated data objects so that most working data is kept in Permission to copy without fee \nall or part of this material is granted provided that the copies are not made or dktrfbuted for direct \ncommercial acfvanta~e, the ACM copyright notice and the title of the publication and Its date appear, \nand notice is given that copying is by permission of the Association of Computing Machinery, To copy \notherwise, or to republish, requires a fee and/or specific permission. SIGPLAN 94-6/94 Orlando, Florida \nUSA 0 1994 ACM 0-89791 -662-x19410006..$3.5O physical memory [7, 12, 25, 35]. Recently, several researchers \nhave suggested that a collector might also be used to improve cache performance, by designing it to move \nobjects so that most working data is kept in the cache [39, 42]. Because caches are so much smaller than \nmain memory, a collector of this kind must be invoked more frequently than ordinary col\u00adlectors if it \nis to be effective. While the cost of running such an aggressive collector may be significant, the hope \nis that it will be smaller than the improvement that is achieved by reducing the program s cache misses. \nThe proponents of aggressive collection either argue or as\u00adsume that programs written in garbage-collected \nlanguages will have poor cache performance if little or no garbage col\u00adlection is done. In contrast, \nthe primary claim of this pa\u00adper is that many such programs are naturally well-suited to the direct-mapped \ncaches typically found in high-performance computer systems. This conclusion, and its corollaries, rest \nupon a study of five nontrivial, long-running Scheme pro\u00adgrams, compiled and run in a high-quality Scheme \nsystem. After a survey of prior work in $2, a brief description of the test programs in $3, and a delineation \nof the cache design space in ~4, the results of two cache-performance experiments are presented. A control \nexperiment, described in $5, shows that the test programs have good cache performance without any garbage \ncollection at all. A second experiment, discussed in 36, shows that the programs should perform well \nwith a simple, infrequent ly-run generational compacting collector. The goal of the remainder of the \npaper is to generalize these results to other Scheme programs and to programs in other garbage-collected \nlanguages. In ~7, an analysis of the test programs memory usage patterns reveals that the mostly\u00adfunctional \nprogramming style typically used in Scheme pro\u00adgrams, in combination with simple linear storage allocation, \ncauses most data objects to be dispersed in time and space so that references to them cause little cache \ninterference. The analysis is generalized in 58, where it ia argued that the be\u00adhavioral properties leading \nto good cache performance shouId hold for other Scheme programs, and are likely to hold for programs \nin other garbage-collected languages. Therefore the conclusion that a simple, infrequently-run generational \ncom\u00adpacting collector should lead to good program performance is widely applicable, although it may not \napply to programs with tight constraints on memory usage or response time. The paper closes by conjecturing \nthat garbage-collected languages may have a significant performance advantage over more tra\u00additional \nlanguages on fast computer systems. 2. Prior work Generational collectors can improve the performance \nof vir\u00adtual memories by repeatedly reusing a modest amount of memory on a relatively small time scale. \nWilson, Lam, and Moher were the first to suggest that this idea could be ap\u00adplied on an even smaller \nscale to improve the performance of caches [38, 39]. While intuitively appealing, it is not obvi\u00adous \nthat this approach will work once the fundamental dif\u00adferences between virtual memories and caches are \nconsidered. In particular, the cost of a page fault is many orders of mag\u00adnitude larger than that of \na cache miss, memory pages are several orders of magnitude larger than cache blocks, and virtual memories \ntypically employ some approximation to a least-recently-used replacement policy, while practical caches \nare direct-mapped or perhaps set-associative, with a small set size. Nonetheless, thus was born the notion \nof what is here called aggressive garbage collection. An aggressive garbage collec\u00adtor is essentially \na generational collector [2, 22, 36] with a new-object area, or first generation, that is sufficiently \nsmall to fit mostly or entirely in the cache. From their meamre\u00adments of four Scheme programs, Wilson \net al. conclude that an aggressive collector can yield lower data-cache miss ratios than a Cheney-style \ncompacting semispace collector [6]. Along similar lines, Zorn compared two different genera\u00adtional collectors. \nOne, a noncompacting mark-and-sweep col\u00adlector, moves objects only when they are advanced from one generation \nto the next; the other is a more traditional copy\u00ading collector [41, 42]. Zorn measured the cache performance \nof four large Lisp programs running with these collectors in various configurations, ranging from frequently-run \nto very aggressive. He shows that the data-cache miss ratios of the programs are improved by the collectors, \nand conjectures that aggressive collection will be an effective means for improving program performance. \nThe arguments made in favor of aggressive garbage collec\u00ad tion share three flaws. First, the supporting \nmeasurements use data-cache miss ratios as the primary metric of program performance, but miss ratios \nare not necessarily correlated with program running times; specifically, they do not account for the \ntemporsJ costs of cache activity and garbage collec\u00ad tion. Second, the measured program runs make, at \nmost, only a few tens of millions of references, but with large caches it is especially important to \nuse long program runs in order to obtain believable results from trace-driven cache simula\u00ad tions [4]. \nFinally, and most importantly, the proponents of aggressive collection either argue or assume that programs \nwritten in garbage-collected languages will have poor cache performance if little or no garbage collection \nis done. They do not, however, provide measurements to support this con\u00ad clusion; in particular, they \ndo not measure the performance of their test programs when run with a simple, infrequently-run garbage \ncollector or with no collector at all. Without such control experiments, it is impossible to argue that \na sophisti\u00ad cated collection strategy is desirable, much less necessary. Diwan, Tarditi, and Moss recently \npublished results sup\u00adporting the proposition that garbage-collected languages can have good cache performance \n[8, 9]. Using a temporal metric, rather than miss ratios, they studied the cache performance of eight \nfairly substantial ML programs running in Standard ML of New Jersey [3, 24]. While they used a more accu\u00adrate \ncache and memory-system simulator than that employed here, they only considered configurations similar \nto those of currently-available machines, and they did not measure cache and collector costs separately. \n3. Test programs The test programs studied in this paper are written in Scheme, a lexically-scoped dialect \nof Lisp that supports first-class pro\u00adcedures [31]. The primary reason for choosing Scheme was the availability \nof both a high-quality implementation, namely the Yale T system, and a set of realistic test programs. \nThe T system contains ORBIT, one of the best Scheme compilers currently available [19, 20, 29, 30]. Having \nbeen in production use for several years, T has been used by many people to write nontrivial programs. \nThe five test programs and their input data are: ORBIT, the native compiler of the T system, compiling \nitsel~ IMPS, an interactive theorem prover [10], running its internal consis\u00adtency checks and proving \na simple combinatorial identity; LP, a reduction engine for a typed A-calculus [1, 32], typecheck\u00ading \na complex, non-normsJzing J-term and then applying one million &#38;reduction steps to it; NBODY, an \nimplementation of Zhao s linear-time three-dimensional N-body simulation al\u00adgorithm [34, 40], computing \nthe accelerations of 256 point\u00admasses distributed uniformly in a cube and starting at rest; and GAMBIT, \nanother Scheme compiler [11], quite different from ORBIT, compiling the machine-independent portion of \nitself. These programs represent several different kinds of appli\u00adcations and programming styles. They \nvary in size, but each allocates many megabytes of data and executes billions of in\u00adstructions: Lines \nAllot Insns Refs ORBIT 15,332 94.4MB 3.68E9 1.03E9 IMPS 42,119 41.lMB 4.13E9 1.09E9 LP 2,981 58.6MB 2.21E9 \n.64E9 NBODY 857 126.lMB 2.43E9 .63E9 GAMBIT 15,004 106.9MB 7.35E9 2.00E9 The first column shows the \nsize of each program, measured in lines of Scheme source text. The remaining columns show the number \nof bytes allocated, the number of instructions ex\u00adecuted, and the number of data loads and stores made \nby each program when run, without garbage collection, on its input data. These program runs are significantly \nlonger than those used in most previous studies of the cache performance of garbage-collected languages \n[39, 42]. The programs were compiled and run in version 3.1 of the T system running on a MIPS R3000-baaed \ncomputer [17]. Mea\u00ad surements of memory and cache behavior were made by run\u00ad ning each program, together \nwith the T system itself, under an instruction-level emulator for the MIPS architecture; neither 5. Cache \nperformance wit bout garbage collection the programs nor T were modified in any significant way. The \ntest programs, with their respective inputs, are all non\u00adinteractive. The performance of interactive \ngarbage-collected programs depends not only upon program and collector be\u00adhavior, but upon the cost, \nin instruction cycles and cache misses, of kernel context switches and user interactions. A study of \nthe cache performance of such programs is left to future work. 4. Cache design parameters The portion \nof the cache design space considered in this paper is constrained in seversJ ways. Only direct-mapped \ncaches are considered. Because they are the simplest to implement, direct-mapped caches have faster access \ntimes than other types of caches [15, 27]; they are the most common type of cache in current high-performance \ncomputers. The caches are sssumed to be virtually, rather than physically, indexed. A wide range of cache \nsizes is considered, from 32KB to 4MB. This range includes current typical sizes for single-level off-chip \ncaches (32 64KB) and for second-or third-level caches in multi-level systems ( l 4MB). The cache-block \nsize ranges, in powers of two, from 16 to 256 bytes. Main memory will be discussed in terms of memory \nblocks, which are aasumed to be the same size as cache blocks. The fetch size, i. e,, the unit of transfer \nbetween the cache and main memory, is also assumed to be equal to the block size. Only one level of caching \nis considered; no attempt is made to measure the performance of memory systems with multi\u00adlevel caches. \nThe results reported here are expected to extend to the two-and even three-level caches that are becoming \ncommon. An informative analysis of multi-level cache per\u00adformance, however, requires a more sophisticated \nmemory\u00adsystem simulator than that employed here, so a thorough in\u00advestigation is left to future work. \nThe caches are aasumed to have a write-miss policy of write-validate; this is equivalent to write-allocate \nwith sub\u00adblock placement, with a sub-block size of one word. In con\u00adtraat to the more common fetch-on-write \npolicy, write-validate avoids fetching the contents of memory blocks in which every word is written before \nbeing read. Jouppi has demonstrated that write-vahdate can yield significant performance improve\u00adments \nfor C and Fortran programs [16]; Koopman et al. first noted the benefits of this policy for garbage-collected \npro\u00adgrams [18]. The impact of fetch-on-write upon the perfor\u00ad mance of the test programs will be discussed \nbriefly in ~5. The temporal cost of writing data to main memory, which depends upon the write hit policy, \nis not analyzed in detail. Properties of practical memory systems and of the test pro\u00adgrams themselves \nimply that these costs should be small [33]; preliminary measurements support this conclusion. Finally, \nonly data-cache performance is considered. While instruction caches are expected to perform reasonably \nwell for Scheme programs, an investigation of instruction-cache per\u00adformance is beyond the scope of this \nwork. In order to determine the extent to which the cache perfor\u00admance of the test programs can be improved, \nthe control ex\u00adperiment meaaures their cache performance when they are run without any garbage collection \nat all. This is done sim\u00adply by disabling the collector; during each program run, data objects are allocated \nlinearly in a single contiguous area. If these measurements were to show that the programs have poor \ncache performance without collection, then some method of improving cache performance would be called \nfor. In fact, the control experiment shows that the programs have good cache performance. The results \nwill be described in terms of cache overheads, which express the temporal cost of cache activity relative \nto the programs idealized running times. The time required to service a miss by fetching the target memory \nblock into the cache, i.e., the miss penalty, depends upon details of the main-memory system and upon \nthe block size. In particular, the miss penalty varies directly with the block size, since more time \nis required to transfer larger blocks in a given memory system. For concreteness, the miss penal\u00adties \nused here are baaed upon the main-memory system stud\u00adied by Przybylskl [27, $3.3.2]. This memory has \nan address setup time of 30ns, an access time of 180ns, and a transfer time of 30ns for each 16 bytes \ntransferred. Thus a transfer of n bytes requires 30 + 180 + 30 x [n/161 nanoseconds. Two hypotheticti \nprocessors are considered. The slow pro\u00adcessor, representing currently-available workstation-claas ma\u00adchines, \nhaa a cycle time of 30ns (i. e., a 33 megahertz clock); the faat processor, representing high-performance \nmachines available in the near future, has a cycle time of 2ns (a 500 megahertz clock). With these cycle \ntimes, the miss penalties for the various block sizes, measured in processor cycles, are: Block size \n16 32 64 128 256 (bytes) Slow penalty 8 9 11 15 23 (cycles) Fast penalty 120 135 165 225 345 In light \nof expected improvements in memory-chip technol\u00adogy, especially improvements in bandwidth, the faat -processor \npenalties are conservative [28]. The hit time, i.e., the time required to access a block that is already \nin the cache, is as\u00adsumed to be one cycle for both processors. Thus, if a reference hits in the cache, \nthe processor does not stall. The cache overhead of a program, Ocache, is the amount of time spent waiting \nfor misses to be serviced expressed as a fraction of the program s idealized running time, in which no \nmisses occur and one instruction completes in every cycle. That is, M ~Tog x P Ocache= ~ , prog where \nMprog is the total number of misses during the program run, P is the miss penalty, in processor cycles, \nand Iprog is the total number of instructions executed by the program. The more familiar metric of cycles \nper instruction [4] is one plus the overhead of each cache in the memory hierarchy. With these assumptions \nand definitions in hand, the aver\u00ad age cache overhead for the test programs, run without garbage collection, \ncan be calculated: 256B k -k ... *  --\u00ad 128B 64B - \u00ad 32B -----\u00ad 16B Q. G \\a 0.001~ 32K 64K 128K 256~ \n512K lM 2M 4M Cache size (bytes) There are two sets of curves in this graph, one for each of the hypothetical \nprocessors. The height of a data point shows the average cache overhead (OCache), across all programs, \nfor the given block size, cache size, and processor speed. The test programs individual cache overheads \nare all close to the average. For the slow processor, even a small 32KB cache has an overhead of less \nthan five percent when the block size is 16 bytes. For the fast processor, a lMB cache is required in \norder to achieve a similar overhead, but fast machines are expected to have caches at least that large. \nCaches in such machines are likely to employ larger block sizes, but, with a sufficiently large cache, \nit is still possible to achieve an overhead of less than five percent. For both processors, larger caches \nand smaller block sizes always yield superior performance. The cache overheads shown above assume a write-miss \npol\u00adicy of write-validate. For the Scheme test programs, write\u00advalidate always outperforms the more common \nfetch-on-write policy, but sometimes by only a small margin~ For a given program, the number of fetches \navoided by write-validate de\u00adpends inversely upon the block size and is independent of the cache size. \nFor the slow processor, a fetch-on-write pol\u00adicy never increases the average cache overhead by much more \nthan one percent. For the fast processor, the increased over\u00adhead due to fetch-on-write is more significant, \nranging from less than four percent, for 256-byte blocks, to nearly 20%, for 16-byte blocks. With the \ncache sizes considered here, it is im\u00adpossible to attain an overall cache overhead of less than five \npercent for the fast processor without a write-validate policy; an overhead of less than ten percent, \nhowever, is achievable wit h a 2MB cache and 128-byte blocks. The above overheads do not include the \ncost of writing data from the processor to memory, in the case of a write-through cache, or from the \ncache back to memory, in the case of a *In contrsst, the ML programs studied by Diwan et d. appear to \nrequire write-validate for good cache performance [9]. This is most likely due to their extremely high \nallocation rates, which are, in part, a result of the fact that SML/NJ allocates procedure-call frames \nin the heap rather than on a stack. write-back cache. Preliminary measurements have shown that the write \noverheads of write-back caches are low. For the slow processor, write overheads are almost always less \nthan one percent; for the fast processor, write overheads for caches of lMB or more are almost always \nless than three percent. The write overheads of write-through caches have not yet been measured, and \nmay be somewhat higher. The control experiment has revealed, then, that the test programs have good cache \nperformance when run without any garbage collection at all. With appropriate write-miss and write-hit \npolicies, it is possible to achieve a cache overhead of less than five percent. Overheads are higher \nwith less favor\u00ad able policies; for the fast processor, they approach thirteen percent assuming realistic \ncache configurations. With so lit\u00ad tle room for improvement, it is not clear that improving cache performance \nshould be a priority. No method for improving cache performance that imposes a significant overhead of \nits own could be effective. 6. Program performance with a simple collector In practice, it is not possible \nto run programs with an un\u00ad bounded amount of physical memory, so some garbage collec\u00ad tion must be done \nin order to ensure good virtual-memory per\u00ad formance. Because a single page fault may cost hundreds of \nthousands, if not millions, of processor cycles, spending some computational effort in a collector in \norder to avoid paging is worthwhile. The results of the control experiment suggest that a good collector \nwill be one that collects rarely, in order to approximate the non-collection case and thereby take ad\u00ad \nvantage of the programs naturally good cache performance, yet frequently enough to minimize page faults* \nThis hypothesis is tested in the second experiment, which measures the cost of running the test programs \nin a modest amount of memory with a simple, efficient, and infrequently\u00ad run Cheney-style compacting \nsemispace collector [6]. Gener\u00ad ational compacting collectors typically provide better perfor\u00ad mance \nthan Cheney s algorithm, but the T system does not include such a collector. All but one of the programs \nperform well wit h the Cheney collector, and should therefore perform well with a simple and infrequently-run \ngenerational compact\u00ad ing collector; the remaining program requires such a collector for good performance. \nGarbage-coUection overhead. During a program run, a garbage collector imposes both direct and indirect \ncosts. Directly, the collector itself executes Igc instructions and causes Mgc cache misses. The magnitude \nof Igc depends upon the amount of work done by the collector; that of M9C depends upon the collect or \ns own memory reference patterns. Indirectly, there are two ways in which the collector affects the number \nof misses that occur while the program is running. Each time the collector is invoked, its memory references \nre\u00ad move some, or possibly all, of the program s state from the cache; when the program resumes, more \ncache misses occur as that state is restored. The collector can also move data *Cf. White s proposal \nfor improving virtual-memory performance [37]. objects in memory, which may improve (or degrade) the \nob-The overheads for lMPS and LP, discussed below, are not jects reference locality, thereby decreasing \n(or increasing) the program s miss count. These effects are together reflected in AMPT09, which is the \nchange in the program s miss count rel\u00adative to Mpmg, its miss count when run in the same cache without \ngarbage collection. If the collector improves the pro\u00adgram s cache performance by more than enough to \nmake up for the cost of restoring the program s cache state after each collection, then AMPrOg will be \nnegative. The collector can also cause the program to execute AIProg more instructions. This occurs in \nthe T system because haah\u00adtable keys are computed from object addresses. Because the collector can move \nobjects, each table is automatically re\u00adhashed, upon its next reference, after a collection. For the \ntest programs, the cost of rehaahing is usually small. When run with a given collector, the garbage-collection \noverhead of a program is the sum of these temporal costs, ex\u00adpressed as a fraction of the program s idealized \nrunning time. That is, ~ = (~gc + AMpmJx p + Igc + AIprog gc I prog where P is again the miss penalty, \nin processor cycles, and lPmg is the total number of instructions executed by the pro\u00adgram. Because AMpro9 \ncan be negative, it is possible for OgC to be zero or negative, which will be the case if the collec\u00adtor \nimproves the program s cache performance by more than enough to pay for its own running cost. The running \ntime of a program, taking both czwhe and collection costs into account, is (O,.aChe+ Ogc + 1) x ~pr-~g. \nBe\u00adcause Ogc can be negative, it is possible for the sum of the overheads to be zero, although this would \nrequire an impos\u00adsibly perfect collector that executes no instructions, causes no cache misses, eliminates \nall of the program s cache misses, and does not cause the program to execute any extra instruc\u00adtions. \nThe goal of methods such as aggressive collection is to achieve an overhead that is sufficiently negative \nto counter Ocache significantly. Program performance, When run with the Cheney collector, configured \nto use 16MB semispaces, the collection overheads (Ogc) of three of the five test programs are fairly \nlow: 0.077-~ -\\ la ., ,. ~----@-L-: -&#38; ----o-----o .:-c W--.--w v_\u00ad $v -e --e-- ~~-+~ . +.. e _+ \n-o ~ al + +Fast ho 0.00- $., o slow ; -.,j ORBIT u . ------NBODY . -0.027\u00ad + -...+ GAMBIT 32K 64K \n128K 256K 512K lM 2M 4M Cache size (bytes) shown here.* This graph shows data for 64-byte blocks; overheads \nfor other block sizes are similar. There is one set of curves for each hypothetical processor; in each \nset, there is one curve for each of three of the test programs. The height of a data point shows the \nmeasured collection overhead for a program when run with the Cheney collector in a cache of the indicated \nsize. With the slow processor, all overheads are less than four percent; with the faat processor, overheads \nare usually higher, reaching a maximum of 7.7%, but are still acceptable. For each program, the variations \nin collection overheads are due to the number of cache misses caused by the collec\u00ad tor itself and to \nthe collector s effect upon the program s miss count. Even this simple collector might affect a program \ns cache performance by improving or degrading its reference lo\u00ad cality as it moves data objects in memory, \nso another source of variation is the extent to which this type of effect occurs. For a given cache size \nand processor speed, the cache over\u00ad heads differ because of these factors and because the amount of \nwork done by the collector is program-dependent, being a function of the number of non-garbage objects \nat the time of each collection. For one program, garbage-collection overhead is negative in two cases, \nindicating a significant improvement. These nega\u00ad tive overheads are not, however, due to a general improvement \nof the program s reference localky by the collector. When run without collection, the program in question, \nNBODY, has a few memory blocks that thrash in sufficiently small caches. That is, the memory blocks collide, \nbecause they share the same cache block, and they are referenced in such a way that they frequently displace \neach other. The Cheney collector happens to move the objects involved, thereby eliminating the thrash\u00ad \ning behavior and significantly reducing the number of misses. This improvement is not as noticeable in \na 32KB cache be\u00ad cause there are so many more misses in a cache of that size to begin with; it does not \noccur in caches larger than 128KB because these memory blocks map to different cache blocks in larger \ncaches. To eliminate cache thrashing does not re\u00ad quire a specialized garbage collector, but can be achieved \nby straightforward static met hods that move frequently-accessed objects so that they do not collide \n[33]. The above graph only shows garbage-collection overhead data for ORBIT, NBODY, and GAMBIT. IMPS \nsuffers from a more extreme case of the thrashing behavior just described, so its overheads are highly \nvariable. When thraahing does not occur, the overheads for IMPS are comparable to those above. Overheads \nfor LP are not shown because they are uniformly 40% or higher. LP creates a large data structure that \ngrows monotonically in size until the end of its run. Thus, unlike the other programs, the amount of \nwork done by the Ch\u00ad eney collector in successive collections increases, since it must copy this structure \neach time. A simple generational collector would avoid this problem, for it would copy the live objects \nin younger generations more frequently than those in older *When the collector runs, the cache simulator \nemploys a fetch-on-write policy; thus this graph slightly over-reports collection overheads. generations. \nAlthough such a collector would impose costs be\u00adyond those of the Cheney collector, including the overheads \nof managing several generations and of detecting and updating pointers from old objects to new objects, \nthe work avoided by not repeatedly copying long-lived structures should more than counter those costs. \nLike the Cheney collector, a generational collector should be run infrequently in order to take advantage \nof the programs naturally good cache performance. The collection overhead of an aggressive garbage collector \nis likely to be significantly higher than that of an infrequently\u00adrun generational collector. With a \nsmaller first generation, an aggressive collector will be invoked more frequently than a non-aggressive \ngenerational collector. An aggressive collector will incur all the costs of an ordinary generational \ncollector; moreover, because more frequent collections leave less time for new objects to become garbage \nbefore being copied to the next generation, an aggressive collector will spend relatively more time copying \nobjects from the new-object area. It seems likely that this added copying cost will be significantly \nlarger than the meager improvement in cache performance that is possible. Thus, even if an aggressive \ncollector could reduce cache overhead to zero, it would be unlikely to pay for its cost over that of \nan infrequently-run generational collector. 7. Analysis of memory behavior The control experiment showed \nthat the five Scheme test programs have good cache performance when run without a garbage collector or \nany other mechanism that might im\u00adprove reference locality; the second experiment showed that the programs \nshould perform well with an infrequently-run generational compacting collector. The analysis presented \nin this section will establish that, when run without garbage collection, the test programs have good \ncache performance because their memory behaviors are naturally well-suited to direct-mapped caches. In \nthe next section, the analysis will be generalized to other Scheme programs and to programs in other \ngarbage-collected languages, thereby generalizing the experimental conclusions. Some foundations must \nbe laid before proceeding with the analysis. A plot of the cache misses that occur during part of a program \nrun will be examined in order to develop a visual idea of the connection between memory behavior and \ncache activity, and a coarse-grained unit of time will be defined. The analysis will then show that \nlinear allocation spreads dynamically-allocated data objects both spatially, throughout the cache, and \ntemporally, within each cache block. Most dy\u00adnamic objects are so short-lived that they are dead (i. \ne., will not be referenced again) by the time their cache blocks are re-used for newer dynamic objects; \nthus, assuming no other interference, dynamic objects will be allocated, live, and die entirely in the \ncache. Nearly all other objects are not very ac\u00adtive and are not referenced many times, so references \nto them cannot be a significant source of interference. The programs do contain some long-lived and frequently-referenced \nobjects; these objects are very rare, however, and turn out to improve cache performance more often than \nthey degrade it. Sweeping the cache. Practical memory systems are designed to move fixed-size blocks \nbetween main memory and the cache. Thus the behaviors of blocks, rather than those of data ob\u00adjects, \nare the focus of the analysis. Block behaviors, of course, reflect the behaviors of the objects that \nthey contain. Most Scheme objects are just a few words long, so a block typically contains at least \na handful of objects. When the garbage collector is disabled, the memory blocks for dynamically-allocated \ndata objects (i. e., dynamic blocks) are contained in a single contiguous area. The allocation pointeT \ncontains the address of the next available dynamic word and is incremented by each allocation action; \nwith the collector disabled, it starts at the base of the dynamic area and grows upward, wit bout bound, \nuntil the end of the run. Each time an object is allocated, its component words are initialized, typically \nin ascending address order. Each time an initialization store reaches a new dynamic memory block, an \nallocation miss occurs, flushlng the cache block, if necessary, and assigning it to the new memory block. \nA direct-mapped cache maps a memory block to a cache block by taking the memory block s index modulo \nthe cache size (in blocks); thus the allocation pointer continually sweeps the cache from one end to \nthe other, leaving a trail of newly-allocated objects.* Because the advancement of the allocation pointer \nentails cache misses, each sweep through the cache appears as a bro\u00adken diagonal line when cache misses \nare plotted as a function of time. Shown on the next page is a partial cache-miss plot for a short run \nof ORBIT in a 64KB, direct-mapped cache with 64-byte blocks. The horizontal z axis is calibrated in data \nref\u00aderences, which are the fundamental time unit of the analysis; there are 1024 references for each \ndot width. On the vertical y axis, there is one dot width for each of the 1024 blocks in the cache. A \ndot is shown at (z, y) if at leaat one miss occurred in cache block y during the x h 1024-reference interval. \nThe slope of an allocation-miss line at a given point in time reflects the program s allocation rate, \nrelative to its ref\u00aderence rate, at that time. The partial run shown in the plot is typical in that the \nallocation rate usually changes slowly, but sometimes changes quickly. For example, the nearly ver\u00adtical \nsegment in the lower part of the second full line indicates that the allocation rate is very rapid for \na short time; this is probably due to the allocation of one or a few large objects. The allocation pointer \nmust traverse the entire cache be\u00adfore it revisits a cache block, so some time will elapse before a cache \nblock is flushed and reassigned to another memory block due to allocation activity. In any given cache \nblock, each interval between two successive allocation misses is an allocation cycle. The length of an \nallocation cycle depends in\u00adversely upon the prevailing allocation rate between its defining allocation \nmisses. When the allocation rate is faster, cycles are shorter; when the rate is slower, cycles are longer. \nFor the test programs, allocation cycles in 64KB caches range from several hundred thousand to two million \nreferences in length. *With a fetch-on-write policy, each allocation miss causes the contents of the \nnew block to be fetched from memory. Each word in a dynamic block is initialized before it is read, so \nthe data retrieved by this fetch will never be used. A write-validate policy avoids these useless fetches, \n ., .. ,, ., !. /. .,. ,.. , . .. . . -\u00ad , ,. ./ ...- .. . }2 ,,,=,,=, 6 ,;..-: ,, Time (x102{re;) \nOne-cycle dynamic blocks. The lifetime of a memory block is the closed time interval bounded by its first \nand final refer\u00adences. A natural consequence of a mostly-functional program\u00adming style is that most dynamic \nblocks have extremely short lifetimes:  ~ l.o - J. n .\u00ad , 3 -d\u00ad % Q o.5\u00ad + 64KB .+9 ORBIT v $  IMPS \na) - . LP .g\u00ad------NBODY ~ GAMBIT ,,, ,,, ,,, ,,, 1 32K lM 32M lG Lifetime (references) This graph \nshows the cumulative frequency distributions of dynamic-block lifetimes in each test program, assuming \na block size of 64 bytes. A point on a curve shows, in its y value, the fraction of dynamic blocks with \nlifetimes no greater than its z value. In two of the programs, about half of all dynamic blocks have \nlifetimes no greater than 64K references; in the other three programs, this is true of even more blocks. \nMost, and sometimes nearly all, dynamic blocks have such short lifetimes that they are one-cycle blocks. \nA one-cycle block is a block whose lifetime is completely contained in its initial allocation cycle. \nBecause every one-cycle block is dead by the time the allocation pointer revisits its cache block, one\u00adcycle \nblocks cannot interfere with each other. Absent other ,-. ... -_ ,, -.= ..= -:: ~.---..= ... . interference, \none-cycle blocks will be allocated, live, and die entirely in the cache. Even when the test programs \nare run in a relatively small 64KB cache, most dynamic blocks are one-cycle blocks. In the above graph, \nthe fraction of dynamic blocks that are one-cycle blocks in a 64KB cache is shown by the marker on each \ncurve. In all of the programs, at least half, and often more than eighty percent, of all dynamic blocks \nare one-cycle blocks. lnterjerence. If all memory blocks were one-cycle dynamic blocks, then the only \ncache misses would be allocation misses. While most, and sometimes nearly all, memory blocks in the test \nprograms fall into this category, a significant number do not. Some dynamic blocks are multi-cycle dynamic \nblocks; i.e., they survive their initial allocation cycles. There are also static blocks, which exist \nwhen a program starts running. Static blocks contain the program itself, the procedure-call stack, and \ndata structures and code for the compiler, library, and runtime system. The remainder of the analysis \nargues that, whatever their category, most memory blocks are refer\u00adenced in such a way that they are \nnot significant sources of cache interference. Interference from dynamic blocks, Most dynamic blocks \nhave extremely short lifetimes, and so they have little opportunity to cause other blocks to be removed \nfrom the cache; thus they cannot be significant sources of interference. Multi-cycle dynamic blocks have \nlonger lifetimes, but they are unlikely to be significant sources of interference. A mem\u00adory block is \nactive in an allocation cycle if it is referenced at least once in that cycle. Most multi-cycle blocks \nare not very active: When each test program is run in a 64KB cache, at least 90% of its multi-cycle blocks \nare active in no more than four distinct allocation cycles. Multi-cycle blocks are likely to be distributed \nthroughout the cache, since it is unlikely that the allocation times of multi-cycle blocks will be synchronized \nwith the sweep of the allocation pointer through the cache. The amount of interference created by all \ndynamic blocks is limited by the fact that most such blocks are referenced just a few times, regardless \nof their lifetimes. If a block is not referenced many times, then it cannot be a significant source of \ninterference. In the test programs, for example, with 64\u00adbyte blocks, most dynamic blocks are referenced \nbetween 32 and 63 times. A 64-byte block contains 16 words, so, on av\u00aderage, each word is referenced \nbetween two and four times, This figure is another consequence of the mostly-functional style typically \nused in Scheme programs. Because program\u00admers are encouraged to create and use data structures freely, \nit is quite common, e.g., for a list to be created by one proce\u00ad dure, psix+ed to another, and then traversed \njust once or twice before being discarded. Interference from static blocks. Static blocks are arranged \nin an essentially random fashion, so they are uniformly spread throughout the cache; one static block \nis about as likely as any other to collide with some other block. Nearly all static blocks behave in \na manner similar to that of multi-cycle dynamic blocks: The amount of interference they can create is \nlimited because they are only active in a few allocation cycles and they are not referenced many times. \nA few static blocks are referenced far more frequently than all other blocks. Say that a block is busy \nif it accounts for at least one thousandth of a program s references. In the test programs, nearly all \nfrequently-referenced blocks meet this threshold. In each program there are between 59 and 155 busy static \nblocks (i. e., less than .02% of all active blocks), yet together they account, on average, for 75% of \nall references. In the test programs, busy static blocks arise in three ways. Most commonly, they contain \nclosures for frequently-called procedures. Busy blocks also contain the procedure-call stack; in each \nprogram, nearly all stack references are concentrated in a small, contiguous group of extremely busy \nblocks. Finally, the very busiest blocks contain a small vector internal to the T runtime system that \naccounts, on average, for 6.7% of all references. From behavior to performance. Consider how the activity \nthat takes place in a single cache block determines that block s in\u00addependent, local performance. A cache \nblock will see refer\u00adences to one new dynamic memory block at the start of each allocation cycle; this \nblock is likely to be a short-lived, cme\u00adcycle block. A cache block may also see references to a few \nmulti-cycle dynamic blocks and a few non-busy static blocks; these blocks are likely to be active in \njust a few allocation cycles. All of these memory blocks, short-lived or otherwise, are non-busy, and \nso will be referenced relatively few times. A cache block might also see references to one or more busy \nblocks. In the worst case, two or more busy blocks map to the cache block. Because busy blocks are so \nfrequently referenced, they may thrash, making the cache block s local performance quite bad. Thrashing \nmemory blocks are visible as horizontal stripes in cache miss plots. In the best case, ex\u00adactly one busy \nblock maps to the cache block. Every reference to another memory block might entail two misses, namely \none to reference the other block and another to restore the busy block. But the sheer number of references \nto the busy block will generate enough hits to far outweigh these misses, so the cache block s local \nperformance will be very good. Since most cache blocks will not have any busy blocks mapped to them, \ntheir local performance will fall between these best and worst cases. There are few busy blocks relative \nto the number of cache blocks, even in a small cache, so they are unlikely to collide and thrash. Moreover, \nit is often the case that many busy blocks are in the stack area, where they do not collide. Therefore \nthe best case is expected to be more common than the worst case. The effect of a cache block s local \nperformance on the over\u00adall global performance of the cache depends upon the total number of references \nthat it sees. Most cache blocks, with no busy blocks and relatively few references, will have a small \neffect on the cache s global performance. The worst-and best\u00adcase cache blocks will play a much more \nsignificant role; the positive effect of the best cases should more than outweigh the negative effect \nof the worst cases. The relationship between local and global performance and the balancing of worst-and \nbest-case cache blocks is illus\u00adtrated in the following graph of cache activity in ORBIT: 0.246-,-1 c1 \n,4 ~ .. . ..,1 P ., .., 0.100 z . . .. .:1 P :1 .. . k . .\u00ad . .... -. .-.* :.,... , . ..\u00ad_ti --. -- \n I o 1023 Cache blocks, in ascending reference-count order In this graph, the 1024 cache blocks of a \n64KB cache with 64\u00adbyte blocks are arranged on the x axis in ascending reference\u00adcount order; the least-referenced \ncache block is on the left, while the most-referenced cache block is on the right. The dotted and dashed \ncurves, associated with the right\u00adhand scale, show the cumulative distributions among cache blocks of \nmisses and references, respectively. A point on the dotted curve indicates, in its y value, the fraction \nof all misses (excluding allocation misses) that occur in the z h least-referenced cache block or to \ncache blocks referenced no more than that block; similarly, the dashed curve accumulates cache-block \nreference counts. These curves grow quickly only toward the right-hand side of the graph; thus, unsurprisingly, \nmost misses occur in the most-referenced cache blocks. The dots are associated with the left-hand miss-ratio \nscale, which is logarithmic. There is one dot for each cache block; its height records the local miss \nratio of that block. Dots in the upper quarter of the graph represent bad local perfor\u00admance, while those \nin the lower quarter represent good local performance. Some of the less-referenced cache blocks per\u00adform \nbadly, but the local miss ratios of most of these blocks fall into the central half of the graph. The \nhundred or so most-referenced cache blocks have local miss ratios ranging from very bad (the worst case) \nto very good (the best case). Finally, the solid cumulative miss-ratio curve shows the significance \nof each cache block s local performance to the global performance of the cache. A point on this curve \nreflects, in its g value, what the miss ratio of the cache would be if only cache blocks at and prior \nto its z value were being considered. The change in the y value at some point, relative to that of the \npreceding point, shows the effect of the local performance of the cache block at that point upon the \ncache s global performance. The height of the endpoint of the curve is the global miss ratio of the cache. \nBecause most cache blocks do not account for many refer\u00adences, the cumulative miss ratio does not change \nsignificantly until it reaches the more-referenced cache blocks. At that point, however, it becomes more \nvolatile, with worst-and best-case cache blocks pulling it up and down, respectively. The best-case cache \nblocks prevail in the end, pulling the cu\u00admulative miss ratio down and more than making up for the worst \ncaaes. Because of the logarithmic miss-ratio scale, the final drop in the curve appears small, but in \nfact it falls from 0.027 to 0.017, a factor of about 1.6. A similar pattern holds, with variations, for \nthe other test programs when run in a 64KB cache. For example, NBODY exhibits a more pronounced concentration \nof references and misses in the last few cache blocks. In some programs, such as LP, there are no worst-case \ncache blocks, so the best-case blocks just improve the cache s global performance over that of the less-referenced \ncache blocks. If a program thrsses, there will be jumps in the cumulative miss-rat io curve; IMPS, for \nexample, thrashes in a 64KB cache: 0.972 ; !3 0.100 .2 0.010 2 0.001 Sometimes, misses are spread throughout \nthe cache; this is the case for GAMBIT: 0.318 0.100 0.001 o 1023 Cache blocks, in ascending reference-count \norder Most of the less-referenced blocks in GAMBIT perform badly, with typical local miss ratios roughly \nan order of magnitude higher than those seen in the other programs. This may be due to the fact that \nGAMBIT has many long-lived dynamic blocks; if these blocks are referenced around the same time, they \ncould cause many cache misses. In the end, however, the best-case cache blocks again pull the global \nmiss ratio down to a more satisfactory level. The examples presented so far have been limited to 64KB \ncaches. As was seen in 35, the cache performance of the test programs improves dramatically as the cache \nsize increases. With more cache blocks, busy memory blocks are less likely to collide; thus the worst \ncase becomes less common and the best case becomes more common. Also, allocation cycles dou\u00adble (approximately) \nin length each time the cache size dou\u00adbles; thus even more dynamic blocks will tend to be one-cycle \nblocks, improving the local performance of the less-referenced cache blocks. These trends are illustrated \nin the cache-activity graph of ORBIT running in a 128KB cache: 0.259 ,. .. 0.100: o 1023 Cache blocks, \nin ascending reference-count order The single jump implies that only one cache block is thrash\u00ading; \nit turns out that this cache block sees references to a busy stack block and a busy non-stack static \nblock in almost per\u00adfect alternation. As noted in 36, to avoid thrashing does not require a special-purpose \ngarbage collector, but merely some care in choosing the locations of busy objects. 0 20i7 Cache blocks, \nin ascending reference-count order In the larger cache, more of the most-referenced cache blocks have \ngood local performance. The performance of the less\u00adreferenced cache blocks is also improved, as they \nare more tightly clustered about the cumulative miss-ratio curve, which is lower than that of the smaller \ncache. The analysis has only examined 64-byte blocks. Most data objects in Scheme programs are smaller \nthan even 16-byte blocks. If smaller blocks were used, the objects responsi\u00adble for causing blocks to \nbe long-lived or busy would tiect a smaller fraction of all memory blocks. Thus, performing the behavioral \nmeasurements at a smaller block size should re\u00adveal stronger extremes in various properties. For example, \na larger fraction of dynamic blocks should be one-cycle blocks, and there should be relatively fewer \nbusy blocks. Similarly, performing the measurements at a larger block size should re\u00adveal weaker extremes. \nThese expectations are consistent with the observation, made in 35, that smaller block sizes yield su\u00adperior \nperformance, at least within the range of block sizes being considered. 8. Generalizing the analysis \nAccording to the foregoing analysis, a program will have good cache performance if it satisfies the following \nthree properties: (1) The program has few busy memory blocks; (2) The program has many short-lived, \none-cycle dynamic blocks; (3) Nearly all other blocks are only active in a few allocation cycles, and \nare referenced just a few times.  None of these properties are specific to the test programs, therefore \nthey should hold for other Scheme programs written in a similar, mostly-functional style. Moreover, none \nof these properties are specific to Scheme, hence: Conjecture 1. Properties (l)-(3) above hold for pro\u00ad \ngrams written in a mostly-functional style in garbage\u00ad collected languages other than Scheme. Reasoning \nabout other programming styles leads to: Conjecture 2. Properties (l) (3) above hold across a range of \nprogramming styles in garbage-collected lan\u00ad guages. This conjecture is based upon the observation that, \nacross programming styles, allocation rates vary inversely with ob\u00adject lifetimes. Recall that the number \nof one-cycle dynamic blocks de\u00ad pends upon the cache size, the allocation rate, and the life\u00ad times of \ndynamic blocks; the lifetimes of dynamic blocks, in turn, depend upon the lifetimes of the objects they \ncontain. In a language that encourages a more functional style than Scheme, e.g., ML [26], the allocation \nrate is higher, but object lifetimes are shorter. Therefore ML programs may also have a large number \nof one-cycle dynamic blocks. There is no reason to believe that ML programs will have substantially more \nbusy blocks, and the highly-functional style suggests that there will be even fewer multi-cycle non-busy \nblocks. Thus it is plausible that properties (l) (3) will hold for ML programs. Toward the other end \nof the function&#38;imperative spec\u00ad trum, in a language that encourages a more imperative style than \nScheme, e.g., CLU or Modula-3 [5, 23], object lifetimes are longer, but allocation rates are lower. Therefore \nprograms in these languages may also have a large number of one-cycle dynamic blocks. In this case, however, \nit is less clear that properties (1) and (3) will hold. A more imperative style may engender more multi-cycle, \nnon-busy blocks that are active in many cycles. Thus the sharp distinction between busy and non-busy \nblocks that was observed in $7 might not be seen in CLU or Modula-3 programs. Nonetheless, properties \n(l)-(3) may hold in languages that encourage a slightly more imper\u00adative style than Scheme. These considerations \nlead, finally, to: Conjecture 3. Allocation can be faster than mutation. That is, on machines where cache \nperformance can have a significant impact on program performance, the performance of programs written \nin a mostly-functional style in a linearly\u00adallocating, garbage-collected language may be superior to \nthat of programs written in an imperative style in a language with\u00adout garbage collection. The intuitive \nargument for this conjecture is as follows. Al\u00adlocation activity is like a wave that continually sweeps \nthrough the cache; the allocation pointer defines the crest of the wave. A program written in a mostly-functional \nstyle rides the al\u00adlocation wave, just as a surfer rides an ocean wave. The pro\u00adgram loads data from \nold dynamic blocks in front of the wave s crest; there is a good chance that these blocks are still in \nthe cache, since the vast majority of dynamic blocks are one-cycle blocks and there is little interference \nfrom most other blocks. The program then computes on this data and stores the re\u00adsult, usually in new \ndynamic blocks just behind the crest; it is highly likely that these blocks are still in the cache, since \nthey were just allocated. The work involved in copying data from old dynamic blocks to new ones, together \nwith the accompanying cost of even\u00adtually running a garbage collector, may seem wasteful. An imperative \nstyle, however, will have other costs. In place of garbage-collection costs will be the costs of allocation \nand deallocation operations, which could be significant in terms of both instructions executed and cache \nmisses incurred. More importantly, a program written in an imperative style will not benefit from the \nnaturally good cache performance that is implied by properties ( l) (3) above. In an imperative program, \nwhether two data objects interfere in the cache, or even thrash, is usually a matter of chance. There \nare static\u00ad analysis methods for improving the cache performance of spe\u00ad cific types of imperative programs; \ne.g., an optimization called ZJocking can improve the cache performance of matrix compu\u00ad tations [21]. \nIt seems unlikely, however, that methods will be found for improving the cache performance of a wide \nclass of imperative programs, especially a class that includes programs that make use of many small and \nshort-lived data objects. Proponents of garbage-collected programming languages have long argued their \ncase from standpoints of correctness and programmer productivity. It is possible that such lan\u00ad guages \nmay also have a significant performance advantage on machines where cache performance is important to \nprogram performance. 9. Conclusion Inexorable trends in computer technology are making cache performance \nan increasingly important part of program per\u00adformance. Prior work on the cache performance of garbage\u00adcollected \nlanguages either argues or assumes that programs written in these languages will have poor cache performance \nif little or no garbage collection is done. This paper has ar\u00adgued to the contrary: Many such programs \nare naturally well\u00ad-suited to the direct-mapped caches typically found in high\u00adperformance computer systems. \nThis conclusion is supported by measurements of the cache performance of five nontrivial Scheme programs, \nby a quflltative analysis of how the pro\u00adgrams memory behaviors determine their cache performance, and \nby considerations of how programming style determines memory behavior. From this conclusion it follows \nthat the experimental re\u00adsults obtained for the five Scheme test programs should ap\u00adply to a wide range \nof programs in a variety of garbage\u00adcollected languages. The best memory-allocation strategy will be \nlinear allocation, which distributes dynamic objects both spatially, throughout the cache, and temporally, \nwithin each cache block. The best garbage-collection strategy will be one of infrequent generational \ncompacting collection, which can approximate the idealized case of no collection and thereby takes advantage \nof a program s naturally good cache perfor\u00admance. This advice is not universally applicable, for infrequent \ncol\u00adlection is not appropriate for all programs. If a program must run in a small physical memory, then \nthe space efficiency of frequent generational collection or of mark-and-sweep meth\u00adods may be preferred. \nFor an interactive program, guarantee\u00ading a bound on collector pause times may be more important than \noptimizing the program s running time. When infre\u00adquent collection is acceptable, however, complex and \ncostly means for improving cache performance, such as aggressive collection, are unlikely to be necessary. \nThe measurements and analysis presented here also provide guidance for the future, when processors will \nbe faster, rela\u00adtive to main memories, than they are now. Mostly-functional, garbage-collected programs \nwill benefit from some means for avoiding useless memory fetches, such as a write-miss pol\u00adicy of write-vahdate. \nBecause such programs naturally make good use of direct-mapped caches, they may prove to have a significant \nperformance advantage over programs written in traditional languages. Acknowledgements This work owes \nmuch to the enthusiasm, encouragement, and advice of John Guttag. Helpful comments on drafts of this \nmaterial were provided by Mark Day, Bert Halstead, Suresh Jagannathan, Butler Lampson, Scott Nettles, \nJim O Toole, Tim Shepard, Guy Steele, and several anonymous reviewers. The authors of various software \nsystems provided both code and assistance: Josh Guttman (IMPS), Mark Hill (the TYCHO cache simulator, \nwhich was used to validate the simulator em\u00adployed here), David Kranz (ORBIT), and Tom Simon (NBODY). \nMost of this work was performed at the Laboratory for Computer Science of the Massachusetts Institute \nof Technol\u00adogy, and was supported by the Advanced Research Projects Agency of the Department of Defense, \nmonitored by the Of\u00adfice of Naval Research under contracts NOO014-89-J-1988 and NOO014-92-J-1795. References \n[1] Luigia Aiello and Gianfranco Prini. An efficient interpreter for the lambda-calculus. Journal of \nComputer and System Sc~ences, 23(3):383-424, December 1981. [2] Andrew W. Appel. Simple generational \ngarbage collec\u00adtion and fast allocation. Software-Practice and Experience, 19(2):171 183, February 1989. \n[3] Andrew W. Appel. Compiling with Continuations. Cam\u00adbridge University Press, 1992. [4] Anita Borg, \nR. E. Kessler, Georgia Lazana, and David W. Wall. Long Address Traces from RISC Machines: Genera\u00adtion \nand Analysis. Research report 89/14, Digital Equipment Corporation Western Research Laboratory, Palo \nAlto, Cali\u00adfornia, September 1989. [5] Luca Cardelli, James Donahue, Lucille Glaasman, Mick Jor\u00addan, \nBill Kalsow, and Greg Nelson. Modula-3 Report (re\u00advised). Research Report 52, Digital Equipment Corporation \nSystems Research Center, Palo Alto, California, November 1989. [6] C. J. Cheney. A nonrecursive list \ncompacting algorithm. Communications of the ACM, 13(11):677 678, November 1970. [7] Robert Courts. Improving \nlocality of reference in a garbage\u00adcollecting memory management eystem. Communications of the ACM, 31(9):1128 \n1138, September 1988. [8] Amer Diwan, David Tarditi, and Eliot Moss. Memory Sub\u00adsystem Performance of \nPrograms with Intensive Heap Alloca\u00adtion. Technical Report 93-227, School of Computer Science, Carnegie \nMellon University, Pittsburgh, Pennsylvania, De\u00adcember 1993. [9] Amer Diwan, David Tarditi, and Eliot \nMoss. Memory sub\u00adsystem performance of programs with copying garbage col\u00adlection. In .% gmposium on Principles \nof Programming Lan\u00adguages, pages 1 13, ACM, January 1994. [10] William M. Farmer, Joshua D. Guttman, \nand F. Javler Thayer. IMPS: An interactive mathematical proof system. In M. E. Stickel, editor, Tenth \nInternational Conference on Automated Deductton, pages 653 654, Volume 449 of Lecture Notes in Artificial \nIntelligence, Springer-Verlag, 1990. [11] Mark Feeley and James S. Miller. A parallel virtual machine \nfor efficient Scheme compilation. In Conference on Lisp and Functional Programming, pages 119-130, ACM, \n1990. [12] Robert R. Fenichel and Jerome C. Yochelson. A LISP garbage-collector for virtual-memory computer \nsystems. Communications of the ACM, 12(11):611 612, November 1969. [13] John L. Hennessey and David A. \nPatterson. Computer Archi\u00adtecture: A Quantitative Approach. Morgan Kaufmann, Palo Alto, California, 1990. \n[14] John L. Hennessy and Norman P. Jouppi. Computer technol\u00ad [31] Jonathan A. Rees and William Clinger. \nRevised3 report on ogy and architecture: An evolving interaction. IEEE Comp\u00ad the algorithmic language \nScheme. ACM SIGPLAN Notices, uter, 24(9):18 29, September 1991. 21(12), December 1986. [15] Mark D. Hill. \nAspects of Cache Memory and Instruction [32] Mark B. Reinhold. Z ypechecking Is Undecidable When Type \nJ Ih@er Performance. Ph.D. thesis, Computer Science Divi\u00ad 1s a Type. Technical Report 458, MIT Laboratory \nfor Com\u00ad sion, University of California at Berkeley, November 1987. puter Science, Cambridge, Msm.achusetts, \nDecember 1989. Available as UCB/CSD Technical Report 87/381. [33] Mark B. Reinhold. Cache Performance \nof Garbage-Collected [16] Norman P. Jouppi. Cache write policies and performance. Programming Languages. \nTechnical Report 581, MIT Lab- In International Sgmposium on Computer Architecture, oratory for Computer \nScience, Cambridge, Massachusetts, pages 191 201, IEEE, May 1993. September 1993. [17] Gerry Kane. MIPS \nRISC Architecture. Prentice-Hall, Engle\u00ad [34] Thomas D. Simon. Optimization of an O(N) Algorithm for \nwood Cliffs, NJ, 1988. N-bod~ Simulations. Bachelor s thesis, Department of Elec\u00ad [18] Philip Cache J. \nKoopman, behavior of Jr., Peter combinator Lee, and Daniel graph reduction. P. Siewiorek. ACM Trans\u00ad \ntrical stitute Engineering and Computer of Technology, December Science, 1991. Massachusetts In\u00ad actions \non Programming Languages and Systems, 14(2) :265 [35] Patrick G. Sobalvarro. A Lifetime-based Garbage \nCollector 297, April 1992. for LISP Systems on General-Purpose Computers. Bachelor s [19] David A. Kranz. \nOrbit: An Optimizing Ph.D. thesis, Yale University, New February 1988. Compiler Haven, for Scheme. Connecticut, \nthesis, Department of Science, Massachusetts 1988. Electrical Institute Engineering and of Technology, \nComputer September [20] David A. Kranz, Richard Kelsey, Jonathan A. Rees, Paul Hudak, James Philbin, \nand Norman I. Adams. Orbit: An optimizing compiler for Scheme. In Symposium on Compiler Construction, \npages 219 233, ACM, June 1986. [36] David Ungar. Generation scavenging: A non-disruptive high performance \nfitorage reclamation algorithm. In S~m\u00adposium on Pmctical Software Development Environments, pages 157 \n167, ACM, April 1984. [21] Monica S. Lam, Edward E. Rothberg, and Michael E. Wolf. The cache performance \nand optimizations of blocked algo\u00adrithms. In Conference on Architectural Support for Program\u00ad [37] Jon \nL. White. Address/memory management LISP environment or, GC considered harmful. ference, pages 119-127, \nACM, July 1980. for In a gigantic Lisp Con\u00ad ming Languages and Operating Systems, pages 63 74, ACM, [38] \nPaul R. Wilson, Michael S. Lam, and Thomas G. Moher. April 1991. Caching Considerations for Generational \nGarbage Collec\u00ad [22] Henry Lieberman and Carl Hewitt. A real-time garbage lector based on the lifetimes \nof objects. Communications the ACM, 26(6):419 429, June 1983. col\u00adof tion: A Case for Large and Set-Associative \nReport UIC-EECS-90-5, Software Systems versity of Illinois at Chicago, Chicago, IL, Caches. Technical \nLaboratory, Uni-December 1990. [23] Barbara Liskov, Russell Atkinson, Toby J. Craig Schaffert, Robert \nScheifler, and Reference Manual. Volume 114 of Lecture Science, Springer-Verlag, Berlin, 1981. Bloom, \nEliot Moss, Alan Snyder. CL U Notes in Computer [39] Paul R. Wilson, Michael S. Lam, and Thomas G. Moher. \nCaching considerations for generational garbage collection. In Conference on Lisp and Functional Programming, \npages 32\u00ad42, ACM, 1992. An earlier version appeared as [38]. [24] Robin Milner, Mads tion of Standard \nML. 1990. Tofte, MIT and Robert Harper. The Defini-Press, Cambridge, Massachusetts, [40] Feng Zhao. An \nO(N) Algorithm for Three-dimenwonal N\u00adbody simulations. Master s thesis, Department of Electrical Engineering \nand Computer Science, Massachusetts Institute of Technology, October 1987. [25] David A. Moon. Garbage \ncollection Conference on Lisp and Functional 246, ACM, 1984. in a large Lisp Programming, system. In \npages 235\u00ad [41] Benjamin G. Zorn. Comparative Performance Evaluation of Garbage Collection Algorithms. \nPh.D. thesis, Computer Sci\u00adence Division, University of California at Berkeley, December [26] Laurence \nC. Paulson. ML for the Working Programmer. 1989. Available as UCB/CSD Technical Report 89/544. Cambridge \nUniversity Press, 1992. [42] Benjamin G. Zorn. The Effect of Garbage Collection on [27] Steven A. Przybylski. \nCache and Memory Hierarchy Design: Cache Performance. Technical Report CU-CS-528-91, De- A Performance-Directed \nApproach. Morgan Kaufmann, Palo partment of Computer Science, University of Colorado at Alto, California, \n1990. Boulder, May 1991. [28] Steven A. Przybylski. DRAMs for new memory subsystems. Microprocessor Report, \n15 February, 8 March, and 29 March 1993. [29] Jonathan A. Rees. The T Manual. Computer Science De\u00ad partment, \nYale University, New Haven, Connecticut, fourth edition, January 1984. [30] Jonathan A. Rees and Norman \nI. Adams. T: A dialect of Lisp or, Lambda The ultimate software tool. In Con~erence on Lisp and Functional \nPvogmmmmg, pages 114 122, ACM, 1982.  \n\t\t\t", "proc_id": "178243", "abstract": "<p>As processor speeds continue to improve relative to main-memory access times, cache performance is becoming an increasingly important component of program performance. Prior work on the cache performance of garbage-collected programs either argues or assumes that conventional garbage-collection methods will yield poor performance, and has therefore concentrated on new collection algorithms designed specifically to improve cache-level reference locality.</p><p>This paper argues to the contrary: Many programs written in garbage-collected languages are naturally well-suited to the direct-mapped caches typically found in modern computer systems. Garbage-collected programs written in a mostly-functional style should perform well when simple linear storage allocation and an infrequently-run generational compacting collector are employed; sophisticated collectors intended to improve cache performance are unlikely to be necessary. As locality becomes ever more important to program performance, programs of this kind may turn out to have a significant performance advantage over programs written in traditional languages.</p>", "authors": [{"name": "Mark B. Reinhold", "author_profile_id": "81100625218", "affiliation": "NEC Research Institute, Four Independence way, Princeton, New Jersey", "person_id": "P190937", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/178243.178261", "year": "1994", "article_id": "178261", "conference": "PLDI", "title": "Cache performance of garbage-collected programs", "url": "http://dl.acm.org/citation.cfm?id=178261"}