{"article_publication_date": "06-01-1994", "fulltext": "\n Precise Compile-Time Performance Prediction for Superscalar-Based Computers Ko-Yang Wang* IBM T. J. \nWatson Research Center, P.O. Box 704, Yorktown Heights, NY 10598, USA Abstract Optimizing compileTs \n(particularly parallel compileTs) are constrained by theiT ability to pTedict peTfoTmance consequences \nof the transformations they apply. Many facioTs, such as unknowns in coniTolstwctur es, dy\u00adnamic behavioT \nofprogTams, and complexity of the un\u00addedying haTdwar e, make it very dificult for compiiem to estimate \niheperfor mance of the transformations ac\u00adcuTatelg and efficiently. In this paper, we pTesent a performance \nprediction framework that combines sev\u00aderal innovative approaches to solve this pToblem. First, the framework \nemploys a detailed, architectur e-spec ific, but portable, cost model that can be used to estimate the \ncost of straight line code efficiently. Second, aggregated costs of loops and conditional statements \naTe computed and r epresenied symbolically. This avoids unnecessary, premature guesses and pTeserves \nthe pTecision of the prediction. Third, symbolic comparison a!lows compil\u00adem to choose the best transformation \ndynamically and systematically. Some methodologies for applying the framework to optimizing parallel \ncompilers to support automatic, performance-guided program TestrwctuTing aTe discussed. Introduction \nCompiler optimization is ad hoc. Optimizing parallel compilers improve the performance of user programs \nby applying a sequence of restructuring transformations to uncover parallelism provided by the underlying \nar\u00adchitecture. Many restructuring techniques may signifi\u00adcantly change the program performance. Performance \n*For correspondence: kyrf@riat son. ibm. com Permission to co y without fee all or part of this material \nis granted providJ that the copisa are not made or distributed for diract commercial advanta~e, the ACM \ncopyright notice and the title of the publication and Radate appear, and notice is given that copying \nis by permission of the Association of (kmputhg Machinery. To copy otherwise, or to republish, requires \na fee and/or spedic permission. SIGPLAN 94-6/94 Orfando, Florida USA @ 1994 ACM 0-89791-662-x19410008..$3.5O \ntrade-offs among applicable restructuring transforma\u00adtions have to be evaluated carefully. Unfortunately, \nmost optimizing parallel compilers do not have the per\u00adformance estimation capability to match their \nsophisti\u00adcated program restructuring capability. As a result, it is not uncommon for an optimizing compiler \nto apply a sequence of complicated program restructuring trans\u00adformations and yield only modest performance \ngains or even performance losses. 1.1 Difficulties of Static Performance Prediction The problem of predicting \nprogram performance at compile time is inherently difficult. First, some critical information needed \nby the compiler may not be avail\u00adable at compile time. Often, such information depends on the input data \nof the program. Second, detailed knowledge of architecture features need to be incorpo\u00adrated into the \ncost model to estimate the performance of programs on the target machine. This often leads to a non-portable \nsystem that only works for a particular type of machines. Multiprocessor machines add an\u00adother dimension \nof complexity to the performance pre\u00addiction. Third, the optimization unit of the compiler has to take \ninto consideration the low-level optimiza\u00adtion done by the compiler back-end. This is particu\u00adlarly true \nfor superscalar architectures since they rely heavily on low-level compiler optimization to achieve their \npotential performance. Fourth, due to the large number of decision points in a compiler, estimations \nof program performance have to be done repeatedly in the decision making process. Therefore, the compiler \nis severely constrained as to how much computing re\u00adsources it can spend on performance estimation. Fi\u00adnally, \neffects of compounding estimations may magnify errors significantly when estimations for multiple pro\u00ad \ngram pieces are combined. Prior researchers [1, 3, 5, 6, 7, 8, 9, 10, 11, 13, 15, 16, 17, 18, 19] attempted \nto solve the above problems by using heuristics, profiling, run-time tests, querying users, analytical \nmodels, or combinations of the above. So far, the results have been mixed. The fundamental problems \nof efficiency, accuracy and portability of per\u00ad formance estimation are still the weak points of state\u00ad \nof-the-art optimizing compilers.  1.2 Low Level Parallelism in Super\u00adscalar Architectures A recent \ntrend in parallel computer architect~re is to use superscalar processors to exploit the low level parallelism \n(such as the IBM SPIS that use RS 6000 chips or Cray T3Ds that use Alpha Chips), Super\u00adscalar architectures \nhave multiple instruction execution units that can operate concurrently. Other features such as instruction \npipelines, operation overlapping, cache prefetching, and powerful instructions (such as multiply-and-adds) \nare also incorporated into the ar\u00adchitecture. This can significantly decrease the program execution time \nbut also greatly increases the complex\u00adity for the compiler to estimate the performance of the program. \nIf not applied carefully, a conventional cost estimation model may be off by a factor of ten or more! \nEstimating the execution time of assembly code on superscalar architectures is already complicated. Es\u00adtimating \nthe performance of high level language pro\u00adgrams, such as Fortran and its variants, is even more challenging. \nThe back-end code generator may perform a sequence of low-level optimization that are aimed at increasing \nlow-level parallelism or locality. If the cost estimate fails to take these factors into consid\u00aderation, \nthe resulting estimate may be seriously dis\u00adtorted. We do not know any compiler that currently utilizes \ncompile-time performance predictions of super\u00adscalar architectures in program restructuring. 1.3 Requirements \nA good performance prediction framework for paral\u00adlel and sequential compilers has to meet the following \nrequirements: Precision: The prediction has to be accurate for the compiler to make correct decisions. \nEfficiency: The performance prediction needs to be very efficient to make repeated calls practical dur\u00ading \nthe program optimization process. Robustness: The framework should be able to han\u00addle programs with unknowns \nin control structures, unknown branching probabilities, etc. Portability: Given the complexity and cost \nof par\u00adallel compilers and shortened hardware product cycle time, port ability across multiple hardware \nplatforms is ever more important for parallel com\u00adpilers. 1.4 Problem Statement The major focus of this \npaper is a framework that can estimate performance of user programs efficiently and accurately across \ndifferent architecture platforms. Algorithms and methodologies for estimating perfor\u00ad mance of programs \non superscalar-based architectures efficiently will be presented. 1.5 Our Approach We designed a performance \nprediction framework that attempts to fulfill the requirements discussed in Sec\u00adtion refrequire by integrating \nseveral approaches into a unified framework. The key ideas behind our de\u00adsign are: 1. Estimate the cost \nof straight line code (code without branches and iterations) as accurately and efficiently as possible. \n2. When aggregating perfor\u00admance data, estimates of values of unknowns in control structures are delayed \nas long as possible. This is ac\u00adcomplished by representing the performance data as a symbolic expression \nof polynomials whose variables are unknown values in program constructs. 3. Recognize situations where \nthe estimates of values of unknowns can be eliminated. 4. Methodologies for comparing performance symbolically \nare introduced. 1.6 Organization of the Paper The remaining sections of the paper are organized as follows. \nIn Section 2, we introduce a performance pre\u00addiction framework and present its components, data structures \nand algorithms for estimating program per\u00adformance efficiently and accurately, In Section 3, pro\u00adgram \noptimization using performance prediction is dis\u00adcussed. We discuss methodologies for comparing effects \nof different transformations using the symbolic perfor\u00admance expressions. Situations when performance \npre\u00addiction can be avoided or simplified are also discussed. In Section 4, we summarize our work and \npresent future research directions for optimizing parallel compilers.  2 A Precise Performance Pre\u00addiction \nFramework In this section, we present a performance prediction framework that can be used in optimizing \ncompilers. The structure of the framework is shown in Figure 1. Expressions from a sequence of statements \nare sent to the instmtction translation module which translates op\u00aderations of the high level language \ninto low level in\u00adstructions. The generated instruction stream is then fed into the instmction, memory, \nand communication cost models The instruction cost model estimates the cost of executing the instructions \nby taking the low w-z covemble cost tmwverable w $t l..................~ II Lkw??Lu ...............+ \n\u00ad i-add : ,, Performance prediction module htsfrttction b#S$ytl h ,  =l-i- - ---@ 1 Transformer Figure \n1: Framework of the Performance Prediction System level parallelism of the architecture and data depen\u00ad \ndencies of he program into account. The memory i cost model estimates the cost related to cache accesses \nand page faults. For distributed memory machines, message passing instructions are sent along with the \nsequential cost estimation to the communication cost module to get cost of moving data among processors. \nThe cost estimations of the program fragments are then combined through the performance aggregation model. \nUnknowns in control statements and array sub\u00adscripts are treated as variables in the performance ex\u00adpressions. \nThe performance of a compound statement is computed symbolically to avoid error magnification when estimations \nare combined, Symbolic comparison of the performance expressions is supported. It can be applied to guide \nprogram restructuring or choose run-time tests based on sensitivity analysis (see Sec\u00adtion 3.4). 2.1 \nA Cost Model for Modeling Paral\u00adlelism in Superscalar Architectures To account for the low level parallelism \npresent in mod\u00adern CPUS, a cost model for straight line code based on detailed architecture features \nis defined. The cost model that we designed can be applied to both tradi\u00adtional processors and modern \nsuperscalar architectures. For the latter case, it captures the overlapping effects and honors data dependencies. \nCost of operations is assigned based on operation units that we called atomic operations. Atomic oper\u00adations \nare specific low level instructions supported by coverable cost ,., I ;% . uncoverable cost covemble \ncost  uncmwable cost  Wwh 11 I ,: II ~ovemble cost I afwddih ,:Time Figure 2: Example of instruction \ncosts. the wrocessor architecture. Each atomic operation has . a cost associated with it. Unlike previous \ncost models, the cost of operations is divided into two components: noncoverable cost: The time that \na functional unit actually dedicates to the operation.  coverable cost: The time when the next operation \nthat does not depend on the result of the current operation can be started. Instructions that use the \nresult of the operation have to wait until the current operation is completed.  For example, with the \nIBM Power architecture, each floating-point add operation has one cycle of noncover\u00adable cost and one \ncycle of coverable cost on the floating point unit. If the compiler can schedule another oper\u00adation on \nthe same functional unit, the cost of the op\u00aderation can be thought of as one cycle, but if no other \nexecutable operations can be found to fill the coverable cycle, then the operation will cost two cycles. \nAn operation can have costs on multiple functional units. For example a floating point store operation \nwill occupy one floating point unit for two cycles with one cycle being coverable and will occupy one \ninteger unit for one cycle on the IBM Power architecture. A conceptual view of our cost model of superscalar \narchitecture is a two dimensional unit with multiple functional bins in one dimension and time slots \nin an\u00adother dimension (see Figure 3). Operations are repre\u00adsented as a two dimension objects which have \ncosts on each functional unit being chained together (Figure 2). Noncoverable costs of an operation are \nrepresented as solid objects that cannot coexist with other objects in dol=l, lSO c 1)= c(l) + m(l) . \nb(l) end i0 brancm I [ Stemc(1) EExE3n r=r+ql)  ki!iiid load c(l) mm3 , r U r = a(l)+ b(l) load b(l) \nEiEm3 Ioaa a(l) EsEEl .... ..................... .................!... ........................., qiin \nL ...............................................................................I FXU FPU BrmchU CR \nLO.SWU Figure 3: An example of dropping operations into time slots of the bins, the same time slot, and \ncoverable costs are represented as transparent objects which can share the time slots with a noncoverable \nobject of another operation. Fur\u00adthermore, the top of an operation object can be viewed as a filter which \ncan blocks operations that depend on the result of the operation. Operations that do not use the result \nof another operation can pass through its filter if there are open time slots below it that the operations \ncan fit in. All costs of an operation have to fit in all functional units at the same time for it to \noccupy the time slots. For architectures with mul\u00adtiple operation pipes, more bins can be added. .The \nbins are flushed before being used for another block of statements. Estimating the cost of executing \na sequence of op\u00aderations can be viewed as finding a way to drop all operation objects into the virtual \narchitecture bin with the goal of minimizing the unfilled slots (see the exam\u00adple in Figure 3). 1 The \ntotal cost of the operations is the time difference between the highest time slot and the lowest time \nslot occupied by the operations. Under our cost model, we assume that operations can be reordered based \non mathematical rules and depen\u00addence relations, and the compiler is intelligent enough to order instructions \nso that full overlapping is possible. Since the cost model is supposed to model the combi\u00adnation of the \ncompiler and the architecture, it should try to imitate the compiler, not to outperform it. By taking \nmultiple functional units and dependency 1 I hope this reminds you the computer game Tetris. into account, \nour approach is much more precise than operation-count based cost models. On the other hand, the key \nfactor in deciding whether this approach is use\u00adful or not lies in the efficiency of the implementation. \nA!gorithm For Estimating Operation Cost The problem of scheduling a list of operations with constraints \nis an NP complete problem. Translating the problem from finding the schedule and cost of the operations \ninto a block matching problem of mini\u00admizing the time-span in completing the operations led us to efficient \nalgorithms of finding approximated solu\u00adtions. Below we describe an efficient way of estimating the cost \nof operations. Our approximate solution for the scheduling problem is to place the cost object of each \noperation into the lowest time slots that all cost com\u00adponents of the operation can fit simultaneously. \n2 We derived a linear time algorithm to solve this modified problem. Before we describe the algorithm, \nwe will first discuss some observations that can greatly simplify the solution. First, only the overall \ncost of the operations in the basic block is of interest; there is no need to keep track of the actual \norder or schedule of the operations. Sec\u00adond, since the mission of the algorithm is to find the lowest \ntime slots for an operation in t+e functional units, the data structure that represents the time slots \nneeds to allow the cost model to search through or up\u00addate the time slots quickly. Third, only a certain \nnum\u00adber of slots (called ~ocus span) under the highest occu\u00adpied time slot need to be considered. This \ncan greatly reduce the complexity of the problem when reordering of operations is considered. It also \nminimizes the space requirement for storing information about time slots. Furthermore, the ~oczu span \nis an adjustable parame\u00adter, thus allowing more flexible allocation of computing resources based on accuracy \nand efficiency considera\u00adtions, Based on the above observations, the time slots of instruction execution \nunits are decomposed into lists of alternating filled and empty blocks that are repre\u00adsented by a two-dimensional \narray. The first and last slots of a block are used to record the size of the block. If the block is \nempty, we record the negative value of the block size (see Figure 4). The array representation has the \nadvantages of double linked lists since reaching the adjacent blocks is only one operation. It also allows \ncorresponding time slots in other bins be found quickly. By looking at blocks instead of individual array \nele\u00adments, simultaneously searching for empty spaces in multiple bins can be done much more efficiently \nwith our data structure than regular array or list represen\u00adtations. 2This cost model can be used as \na base for a full feature scheduling algorithm by allowing reordering of operations that are already \nscheduled. Case 1 Upspam >0, Lwc.pms >0 Sass 2 Up6paW >0, Lmvspmw -0 empty slots psrtially tilled slots \n Figure 4: Data structure used for representing a list of time slots for a functional unit. 2.2 The Instruction \nTranslation Mod\u00adule The instruction translation module has two responsi\u00ad bilities, First, it converts \nexpressions in a high level language (such as HPF) into machine level instructions. Second, it imitates \nthe compiler back-end to perform common optimizations so that the cost estimate will match the cost of \ncode that will eventually be gener\u00adated. It is possible that one can use an existing module in the compiler \n the back-end code generator -to gen\u00aderate low level code for performance estimation pur\u00adposes. In fact, \nthe IBM x{f and Z?C compilers provide such a facility for estimating the cost of assembly in\u00adstructions \n[3]. However, since the compiler needs the cost estimate during the program restructuring process (before \nback-end is invoked); it is impractical at this stage to do code generation for every intermediate step. \nTherefore, an efficient substitute is needed. The in\u00adstruction translation module utilizes information \nmade available by the program analysis module and a de\u00adtailed knowledge about language and machines to \ngen\u00aderate a stream of operations based on the input pro\u00adgram. 2.2.1 Adapting to Languages and Target \nMa\u00adchines The instruction translation module uses four table~ to perform the instruction translation: \nthe high-level op\u00ad eration tabie which represents operations in the high level languages; the basic operation \ntable, which is a table of predefine operations that are type-specific, Figure .5: Adding an atomic operation \nto time slots of a functional unit. Iptr+elois[!plq ........ .. . . . .. . . :. .::,,. :..,,:,,, Iow+.xxt \n.:.:.: Ib n+ .... .... ........ . . ,pw II( !  is capable of representing operations in different lan\u00adguages; \nthe atomic opeTation table which contains pre\u00addefine atomic operations that represent low level ma\u00adchine \noperations; and the atomic operation cost table, which stores the costs of each atomic operation. The \nactual operation translation is done in a two level translation process (as shown in Figure 6). In the \nfirst level, the operation specialization mapping trans\u00adlates language specific expressions into language \ninde\u00adpendent basic operations such as integer-add operation, floating-point multiply-add operation, etc. \nThe second level translation, called at omit opera\u00adtion mapping, translates the basic operations into \na list of atomic operations. The cost model then uses the atomic opeTation cost table to access costs \nof each operation (different atomic operations may have the same costs), OpeTation specialization mapping \nis language depen\u00addent but architecture independent, while atomic opeT\u00adation mapping is architecture \ndependent but language independent. This simple arrangement has many ad\u00advantages. First, different opeTation \nspecialization map\u00adpings can be defined for different high level languages; and different atomic operation \nmappings and atomic opepation cost tables can be defined for different archi\u00adtectures. This allows the \ncost model be used for multi\u00ad ple languages and architectures. Since the cost model operates at the machine \nlevel, the atomic operation cost table can be easily set up based on manufactur\u00ader s specifications. \nWhen low level cost information is not available, a training-set like approach can be used that the cost \nmodel needs to imitate these optimiza\u00ad tion to get accurate estimates. The following is a list High leVel \noperetion Saslc operation Atomtc operation Atomic operation of low-level compiler optimizations that \nour cost model table table table cost table can estimate their effects:3 0P ADD SADD IADD FADD DADD \na e  M operanon atomic speaahzation operabon mappmg mapping Figure 6: Tables used for translating \nhigh level opera\u00adtions into atomic operations. with some loss of precision. Adding a new architecture \nto the cost model is a matter of defining the atomic operation mapping and the atomic operation cost \ntable. Some architectures have operations that take vari\u00adable time to execute. For example, on IBM RS \n6000, the integer multiply takes three cycles when the multi\u00adplier has a value between -128 and 127, \nbut takes five cycles for general values. For this case, multiple basic operations are used to represent \nthe integer-multiply operation , and the operation specialization mapping can map different cases to \ndifferent basic operations. Architecture specific operations such as the multiply\u00adand-add operations \nare recognized by the compiler and represented in the high level operation table. They are mapped to \nlow level atomic operations if the architec\u00ad t ure supports them. The effect oft he limited number of \nregisters on performance is simulated by using a heuris\u00adtic that forces a store after certain number \nof loads. 2.2.2 Specialization for Compilers There many some optimization that are routinely ap\u00ad plied \nby compilers. These transformations include op\u00ad eration overlapping (see Section 2.1)} code motion to \n move loop invariant or inductive expressions out of loops, common sub-expression optimization, dead \ncode elimination, branch predict ion, speculative scheduling, etc. Most of these optimizations are done \nby the com\u00adpiler back end [4] (or are more convenient to be left to the back-end to do). However, the \nperformance estima\u00adtion is used by program restructurer which is several phases before the code generation \nphase. This implies e Speculative scheduling and code motion. Specula\u00adtive scheduling and code motion \nare handled nat\u00adurally by the base model. e Branch optimization. IBM xlf and XIC compilers are capable \nof branch optimizations, such as code replication, gluing, branch swapping, etc. to mini\u00admize the cost \nof branches [3]. The cost model han\u00addles the branch optimization by matching shapes of the cost blocks \nto decide whether the branching cost needs to be included. e Loop unrolling. If the compiler does not \nunroll the loop in the transformation phase, it might unroll the loop in the code generation phase. For \na loop with a small basic block, unrolling the loop a few times is usually enough to enlarge the innermost \nsimple basic block so that sufficient overlapping is possible. Our model provides two ways for esti\u00admating \ncost saving of unrolling a loop: examining the shape of the cost block or dropping the inner\u00admost basic \nblock into the functional bins multiple times. o Sum-reduction operations, etc. The cost model can use \npattern matching techniques to recog\u00adnize some commonly used operations such as sum\u00adreductions for which \nall but one store instruction can be eliminated by using registers. The same technique can be applied \nto other operations such as inner products, array-constant multiply, or ar\u00adray multiplications, etc. \nCommon sub-expressions, loop inyariant or in\u00adductive expressions. Evaluating common sub\u00adexpression only \nonce and moving loop invariant or loop inductive expressions outside loops are done in the operation \nspecialization phase. Two func\u00adtional bins are used to count the one-time and it\u00aderative costs separately. \n Porting the cost model to a new compiler is more in\u00advolved because the cost model needs to be tuned \nbased on the low level optimization capability of the compiler. To ease this process, flags representing \nthe optimiza\u00adtion capabilities of the back-end are defined and used for tuning the cost model. 3 the \ncost model does not need to do most of the analysis needed for these tasks since program analyzer can \nprovide these information. model for superscalar architectures using cost blocks. 2.4.1 Symbolic Cost \nAggregation Figure 7: Results of straight line code examples. 2.2.3 Preliminary Results Figure 7 shows \nsome preliminary data of our system. Our result is obtained by applying the ptran2 compiler on some small \nF90 programs. It is compared against the estimation provided by IBM xlf compiler. 4 Since the cycle counts \nprovided by xlf does not include func\u00adtion call and memory costs, they are excluded from our results \nfor comparison purpose. For the table in Figure 7, F 1-F7 are innermost ba\u00adsic blocks taken from Purdue \nbenchmarks in the HPF Benchmark suite. Matmul is the innermost basic block of a matrix-multiply loop \nwhich is blocked and unrolled 4 times in both dimensions (a total of 16 FMA opera\u00adtions in the basic \nblock). Jacobi is the innermost basic block of Jacobi loops. And RB is the innermost basic block of the \nred-black loops. This comparison is prelim\u00adinary and only covers the straight-line cost estimation. A \nmore thorough performance comparison needs to be done to access the overall performance of the predict\u00adion. \nWe plan to compare the estimated performance of the HPF benchmark programs to the actual run-time. This \nwill be done when the communication cost and symbolic manipulation modules are completed. 2.3 The Memory \nAccess Cost The memory access cost (cache misses, TLB misses and page faults) is computed independent \nfrom the straight line code estimation because the former is a more global matter. Many methodologies \nfor estimating cache cost were proposed [8, 14, 10]. We adopt an algorithm that was int reduced in [8]. \nThe total number of cache line accesses is counted and the cost of filling these cache lines is used \nto approximate the memory cost. 2.4 Cost Aggregation of Compound Statements We will first discuss a cost \naggregation model for gen\u00aderal architectures and then present a cost aggregation ~The IBM xlf compiler \nprints out a listing of assembly code with a cycle count for each assembly instruction when the flags \n-qdebug=cycles -qlisting are specified. We identified the basic blocks and added up the cycle counts \nby hand to get the reference data. At the simple basic block level, the counters for each basic operation \ncontain integer values. At the com\u00ad pound statement level, we use symbolic expressions that we call performance \nexpressions to represent the estimated performance. The cost of executing a sequential loop is the cost \nof running all iterations plus the cost of computing upper and lower bounds: C(do Zk = lbk, ubk, step \n{B}) = C(lbk) + C(dk) + C(s@) + XkeIfer C(B1 ~) For conditional statements, the cost of the condi\u00adtional \nstatement is the sum of the follows: the cost of the conditional expression, the cost of the true branch \n(C(Bt)) times the branching probability of the true branch (pt (cond)), the cost of the false branch \n(C(l?f )) times the branching probability of the false branch (pf (cond)), and the estimated branching \ncost cbr (which may be zero, see Section 2.2): C(zf (cod) Bt else Bf eno?if) = C(cond) + p~(cond) * C(Bt) \n+ P$(cond) * C(Bj) + cb, The major difference between our cost aggregation model and previous work is \nthat we compute and rep\u00adresent performance expressions symbolically when con\u00adtrol structures contain \nunknowns. This preserve the precision of the estimates and has a profound affect on the way the estimates \nare used in optimization. 2.4.2 Cost Blocks and Their Uses in Cost Ag\u00adgregation The first and last occupied \ntime slots in functional units define the actual cost of a basic block and the area they enclosed is \ncalled the cost block (as shown in Figure 8) of the basic block. Using our data struc\u00adture, the shape \nof the cost block is defined by the first and last rows and the height of the block. The shape of the \ncost block reveals many useful information that can be used to combine costs of adjacent basic blocks \nor aggregate costs of compound statements. For exam\u00adple, overlapping between basic blocks or iterations \nof a loop can be estimated by matching the top and bottom of the geometry shape of the cost block (see \nexample in Figure 9). By checking the ratio of the occupied and empty slots in the critical functional \nbin(s), the com\u00adpiler can decide whether statement reordering and loop unrolling are beneficial. The \nshapes of the cost blocks can be used to decide the order of statement blocks or the rough estimation \nof the loop unrolling factor. Uuuu Es ! ,.. ..,,. , ,. ..=> . ... ... .... .. ............ FXU FPU \nBranchU CR-LogicU Figure 8: A cost block. cost of basic blockl v cost of combining basw cost of basic \nblock 2 block 1 and 2 Figure 9: An example of overlapping between basic blocks. The cost of branch operations \ncan be estimated by checking the number of load instructions before opera\u00adtions in other units started \n(this can be approximated as the difference between the bottom of FXU and other units). 3 Performance-Guided \nProgram Optimization The compiler is interested in both comparing the effects of different transformations \nand in choosing the best parameters in a selected transformation. Basing these decisions on performance \nestimation allows the com\u00adpiler to make better decisions. In traditional compil\u00aders, when there are unknowns \nin the control structures, the compilers guess the values of the unknowns (or the reaching probabilities). \nAlthough this makes the per\u00adformance comparison simple (comparing two numbers), the results are highly \nunreliable. Our idea is to delay the guesses by incorporating these unknowns into the performance expressions. \nThere are many situations where it is possible to determine whether the expres\u00adsion is positive or negative \nbased on bounds on the variables. In these cases, the compiler may not have to guess values of the unknowns. \n3.1 Symbolic Comparison of Perfor\u00admance Expressions The solution of the problem of symbolic comparison \nof performance expressions is the subject of another paper, but we discuss a few examples here. For example, \nif the difference in the performance ex\u00adpressions is a polynomial of one variable, 5 then it is simple \nto find the roots of the equations for polyno\u00admials of up to degree of 4. And since polynomials are continuous, \nit is usually straight forward to determine the range(s) where the expression is positive. Assum\u00ading \nperformance changes of transformations f and g are C(j) and C(g), and let P = C(f) C(g), and P+ and \nP be the positive and negative functions of P then f is better than g on regions where P-! = 0. The function \nP can be used to find regions where f perfor\u00admance better than g. Either the value of the function, size \nof the area where P+ and P are noruzero, or in\u00adtegral values of P+ and P can be used to compare the transformations \nf and g. Figure 10 shows regions of a general cubical function for which the value of the function is \nnegative. For cases where the bounds on the related variables are not enough to decide whether the value \nof the ex\u00adpression is positive, the compiler can compute the con\u00addition when the value is positive (this \ncan be used in 5since loop transformations modify only one structure at a time, this is likely to happen. \ny=f(x) 4 lb / 32 y=ax+bx+cx+d,a>O Ib<=x<=ub Figure 10: A cubical polynomial and its values generating \nrun-time tests), or guess the values of the unknowns that are involved. It is also possible for the compiler \nto change expres\u00adsions to simpler expressions by dropping some terms. For example, if the range of z \nis [3, 100], then the equation 4X4 + 2X3 4X + l/x3 can be changed into 4x4 +2x3 4x. 3.2 Automatic Program \nTransforma\u00adtion Based on Performance Estinna\u00adtion One importance feature of the proposed framework is \nthat it supports automatic program optimization. Based on the symbolic performance comparison, the compiler \ncan utilize graph search algorithms, such as the A* algorithm, to choose program transformation sequence \nsystematically. 3.3 Minimizing Cost in Estimating Per\u00adformance In addition to the use of efficient algorithms \nin com\u00adputing the cost estimations, the cost of performance prediction can be reduced by updating the \nprediction incrementally and avoiding unnecessary computations whenever possible. 3.3.1 Incremental Update \nof Predictions The performance prediction framework needs to sup\u00adport incremental update so that cost \nof maintaining up-to-date performance during the program optimiza\u00adtion process is as small as possible. \nTo avoid unnecessary recomputing, each transforma\u00adtion defines an aflected region of performance based \non the structure it changes. The affected region is defined for each category of the estimation. For \nexample, when a loop is blocked, the execution time for the straight line code inside the loop is not \nchanged, but the ex\u00adpression that computes the number of iterations should be changed. The cache access \ncost for the loop is also changed and we can either substitute the bounds of the new inner loop into \nthe expression (if they are repre\u00adsented symbolically another advantage for symbolic processing) or \nrecompute the cache cost. When choosing among two transformations, only the changes that the transformations \nhave on the perfor\u00admance expressions need to be computed. This usually allows cheaper evaluation before \nthe transformations are actually carried out. 3.3.2 Avoid Unnecessary Performance Com\u00adputations There \nare many cases when detailed performance data is not needed, or the performance computation or rep\u00adresentation \ncan be simplified to improve efficiency. The following heuristics can be used to minimize the cost of \nstatic performance prediction. If the two branches of a conditional statement have performance estimations \nthat are very close, the reaching probability of the two branches can be ignored and the performance \nof the conditional statement can be simplified. Some simple conditional expressions whose reach\u00ading probabilities \ncan be guessed should be recog\u00adnized whenever possible. For example, when a variable in the conditional \nexpression is a loop in\u00addex, we may assume equal probability for each it\u00aderation of the loop (i.e. for \na loop with loop bounds in [lb, ub] and step step, the probability that the index has a particular value \nis step/(ub lb). If the cost of one branch of a conditional statement is small and the iteration set \nthat falls into that branch is also small compared to the other branch, then the cost of that branch \nmay be ignored. For example, consider the loop statement that has a nest ed conditional statement (where \nk is unknown): doi=l, n,l if (i le. k) then Bt else lilf endif enddo  the cost of the conditional statement \ninside loop is: C(L) = k*c(B,) + (n k) *c(Bf) and if C(Bt) N C(Bj ) then the cost expression can be \nfurther simplified to be: C(L) = n * C(l?$)  3.4 Profiling and Run-Time Tests Profiling [15] can be \nused to eliminate some variables that result from unknown values in the control struc\u00ad tures (such as \nthe branching probabilities of conditional statements). This is useful when the program behavior is relatively \nindependent of the input data. Multiple branches of instructions guided by well\u00adchosen run-time tests \ncan be effective for programs whose performances depend on input data. However, deciding when and how \nto generate effective run-time tests is an open problem. Usually only a few run-time tests can be afforded \nto maximize the benefit of multi\u00adple branches of control. Excessive run-time tests may lead to negative \neffects on performance. Our use of per\u00adformance expressions provides a foundation for solving this problem. \nAfter the performance expression is found for a pro\u00adgram fragment, sensitivity analysis can be applied \nto find the top few variables that produce the most pertur\u00adbations to the performance. (Sensitivity analysis \nvaries the values of the variables for small amounts and mea\u00adsures the resulting perturbations to the \nvalues of the function). Run-time tests can be formulated based on the most sensitive variables. Furthermore, \nthe condi\u00adtions on the performance expressions can be used to formulate the run-times tests. 3.5 Procedure \nand Library Routine In\u00adterface. Table look-up of the performance expression can be used to find the cost \nof external function calls or li\u00adbrary routines. If the source code of a library is not available, approaches \nsuch as the training-sets can be used to get run-time measurements to build the perfor\u00admance table. However, \nthis usually results in estimates that are only good for certain function arguments. If source code is \navailable, the performance expressions of the external library routines can be computed and stored in \nan external libraTy cost table. The perfor\u00admance expressions are parameterized with the formal parameters. \nActual parameters are substituted at the call site to get more specific performance expressions.  4 \nRelated Work The load/store modeling method used in [9, 5] char\u00adacterizes the performance of shared memory \narchitec\u00adtures by a set of templates of vector load, store, and nop instructions. This works at the \nassembly level and only reflects the cost of memory hierarchy. D. At\u00adapattu and D. Gannon [AtGa89] built \nan interactive tool that used a similar analytical machine model to predict performance for Alliant FX/8. \nV. Sarkar [15] computes at compile time a set of performance parameters and estimates execution time \nbased on profiling data for single assignment languages. The symbolic performance analysis we introduced \nhere allows the compiler to rely less on profiling and, at the mean time, preserves the accuracy of the \nestimation. V. Balasundaram et al. [2] presented a performance estimator for evaluating the relative \nefficiency of data partitioning schemes by computing cost of message passing statically. They assumed \nconstant loop bounds and guessed for values of unknowns in programs. T. Fahringer and H. Zima [7] discussed \na static per\u00adformance prediction tool which uses a combination of a parameter-based performance tool \nand a profiler. Their system attempts to correlate statically computed parameters and the actual measurements, \nwhile our model actually derives a precise mathematical expres\u00adsion based on a set of implicit parameters \nto represent the run-time cost of the program. A. Gemund [17] de\u00adfines a modeling language to model the \nserialization effects of parallel computer systems. Our code-model for message-passing is based on [19] \nwhich is a parameterized, static, performance predic\u00adtion tool that supports different types of architectures. \nThis tools characterized program performance into a set of cost categories that includes instructions, \ncache, message passing, synchronization, and hot spot con\u00adtentions, etc. To summarize, the work reported \nin this paper dis\u00adtinct from previous work in the following areas. 1.The framework we presented in this \npaper is com\u00adprehensive. Different categories of program costs are unified into a single, comparable \nperformance expression. 2. Our cost model for superscalar architecture is the first cost model that can \nestimate performance of high level programs accurately. The two-level translation approach makes it portable \nacross dif\u00adferent architecture platforms. 3. Low level compiler optimizations that are usually done \nat compiler back-end are imitated in the op\u00aderation translation process. This allows the cost model to \naccurately model the program perfor\u00admance at the source language level. 4. Through the symbolic manipulation \nof perfor\u00admance data, the compiler delays or avoids having to guess values of unknowns in control structures. \nThis not only preserves the accuracy of the perfor\u00admance prediction, but also enables the compiler to \n use symbolic comparison to make optimization de\u00adcisions that are not otherwise possible.  Conclusion \n In this paper, we have described a framework for per\u00adformance prediction for superscalar-based parallel \ncom\u00adputers. The framework is implemented and is an inte\u00adgrated module of the PTRAN II compiler which \nis a prototype HPF compiler [12]. Preliminary results show that the predictions are fairly accurate for \nstraight-line code on the RS6000 based machines. Work is underway on communication cost for distributed \nmemory com\u00ad puters. The framework combines efficient but detailed cost scalar architectures, the compiler-time \nguesses to four innovative ideas: an model for modeling super\u00adidea of delaying or avoiding preserve precision \nby sym\u00ad bolic manipulation, the two level operation transla\u00adtion mapping that makes performance prediction \nat the source level possible and maintains portability, and the use of symbolic comparison to select \ntransformations. This research can be applied to other optimizing com\u00adpilers and may enhance their optimization \ncapability significantly, Many techniques, such as evaluating the effects of transformations symbolically, \nefficient compu\u00adtation and manipulation of performance data, and sys\u00adtematic graphic search algorithms \nfor guiding program transformation based on performance prediction need to be further developed to make \nautomatic program optimization feasible. Nevertheless, the work reported in this paper paves a foundat \nion parallel compilers that employ to aggressively optimize program ACKNOWLEDGMENTS I would like to acknowledge \nfor a new generation of performance prediction automatically. the valuable comments and insight of Fran \nAllen, Mike Burke, W.M, Ching, Jeanne Ferrante, Manish Gupta, Roy Ju, K.G. Kumar, Sam Midkiff, Emily \nPlachy, Vivek Sarkar, Edith Schon\u00adberg, and Peter Sweeney. References [1]J. Andrews and C. D. Polychronopoulos. \nAn An\u00adalytical AppToach to PeTfomnance/Cost Modeling of Parallel ComputeTs. PhD thesis, University of \nIllinois at Urbana-Champaign, Ctr. Supercomput\u00ad ing Res. &#38; Dev., April 1110. [2] V. Balasundaram, \nG. U. Kremer. A static guide data partitioning Jte ThiTd ACM Sigpian 1991, CSRD Report No. Fox, K. Kennedy, \nand performance estimator to decisions. In Proceeding of Symposium on PTincipies and pTactice of paTallel \npTogTamming (PPOPP), April 1991. [3] D. Bernstein, D. Cohen, Y. Lavon, and V. Rainish. Performance evaluation \nof instruction scheduling on the ibm rise system/6000. In Proceedings of MICRO-25, pages 226-235, 1992. \n[4] D. Bernstein and M. Rodeh. Global instruction scheduling for superscalar machines. In Proceed\u00adings \nof the ACM SIGPLAN 91 Conference on PTo \u00adgTamming Language Design and Implementation, pages 241 255, \nToronto, Ontario, Canada, June 1991. [5] F. Bodin, D. Windheiser, W. Jalby, D. Atapattu, M. Lee, and \nD Gannon. Performance evalua\u00adtion and prediction for parallel algorithms on the bbn gp1000. In Proceedings \nof the 1990 Intern\u00adational Conference on Supemornputing, pages 401 413, August 1990.  [6] T. Fahringer, \nR. Blasko, and H. P. Zima. Au\u00adtomatic performance prediction to support paral\u00adlelization of fortran programs \nfor massively paral\u00adlel systems. In PTOC. 6th ACM International Con\u00adference on Supe?computing, pages \n347-356, Wash\u00adington D. C., July 1992. [7] T. Fahringer and H. Zima. A static parame\u00adter based performance \nprediction tool for paral\u00adlel programs. In P? oceedings of ihe 7th lntema\u00adtional Conference on Supe?computing, \npages 207 219, Tokyo, Japan, July 1993. [8] J. Ferrante, V. Sarkar, and W. Thrash. On es\u00adtimating and \nenhancing cache effect iveness. In P? oceedings of the .jth Inte? nationai Wo? kshop on Languages and \nCompileTs foT Paraiiel Computing, pages 328-343, Santa Clara, California, USA, Au\u00adgust 1991. [9] K. Gallivan, \nW. Jalby, A. Malony, and H. Wi\u00adjshoff. Performance prediction of loop constructs on multiprocessor hierarchical-memory \nsystems. In P? oceedings of the ACM Inte? nationai Confer\u00ad ence on SupeTcomputing, 1989. [10] D. Gannon, \nW. Jalby, and K. Gallivan. Strate\u00adgies for cache and local memory management by global program transformation. \nIn Proceedings of the 1987 Inte?naiiona~ Conference on Supercom\u00adputing, pages 229-254, 1987. [11] M. \nGupta and P. Banerjee. mation of communication ers. In Proc. 6th International Symposium, Beverly Hills, \nCompile-time esti\u00adcosts on multicomput-Parallel Processing California, March 1992. [12] M. Gupta, S. \nMidkiff, E. Schonberg, P. Sweeney, K.Y. Wang, and M. Burke. Ptran ii -a compiler for high performance \nfortran. In P? oceedings of dth workshop on CompileTs for PaTallel Computers, Dec 1993. [13] S. Hiranandani, \nK. Kennedy, and C. Tseng. Eval\u00aduation of compiler optimization for Fortran D on MIMD distributed-memory \nmachines. In Proceed\u00adings of the 6th ACM Inie? naiionai conference on SupeTcomputing, pages 1-14, July \n1992. [14] M. Lam, E. Rothberg, and M. Wolf. The cache performance and optimizations of blocked algo\u00adrithms. \nIn Proceedings of the dth International Conference on Architectural Support foT Prog? am\u00adming Languages \nand Operation Systems, Santa Clara, CA, April 1991. [15] V. Sarkar. Partitioning and Scheduling PaTallel \nProgTams for Multip? ocessors. pitman, London, 1989. [16] B. Stramm and F. Berman. Predicting the per\u00adformance \nof large programs on scalable multicom\u00adputers. In P? oceedings of the Scalable High Perfor\u00admance Computing \nConference, Williamsburgl VA, April 1992. [17] A. J. C. van Gemund. Performance prediction of parallel \nprocessing systems: the pamela methodol\u00ad ogy. In Proceedings of the 7th International Con\u00adference on \nSupercomputing, pages 318-327, Tokyo, Japan, July 1993. [18] K. Wang and D. Gannon. Applying ai techniques \nto program optimization for parallel computers. In Parallel Processing for Supercomputers and AT\u00adtijcial \nIntelligence, pages 441 486. McGraw-Hill, New York, New York, 1989. [19] K. Wang and E. Houstis. A performance \npredic\u00adtion model for parallel compilers. Technical Re\u00adport CSD-TR-1041, Department of Computer Sci\u00adences, \nPurdue University, November 1990.  \n\t\t\t", "proc_id": "178243", "abstract": "<p>Optimizing compilers (particularly parallel compilers) are constrained by their ability to predict performance consequences of the transformations they apply. Many factors, such as unknowns in control structures, dynamic behavior of programs, and complexity of the underlying hardware, make it very difficult for compilers to estimate the performance of the transformations accurately and efficiently. In this paper, we present a performance prediction framework that combines several innovative approaches to solve this problem. First, the framework employs a detailed, architecture-specific, but portable, cost model that can be used to estimate the cost of straight line code efficiently. Second, aggregated costs of loops and conditional statements are computed and represented symbolically. This avoids unnecessary, premature guesses and preserves the precision of the prediction. Third, symbolic comparison allows compilers to choose the best transformation dynamically and systematically. Some methodologies for applying the framework to optimizing parallel compilers to support automatic, performance-guided program restructuring are discussed.</p>", "authors": [{"name": "Ko-Yang Wang", "author_profile_id": "81100212919", "affiliation": "IBM T. J. Watson Research Center,P.O. Box 704, Yorktown Heights, NY, USA", "person_id": "P162538", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/178243.178250", "year": "1994", "article_id": "178250", "conference": "PLDI", "title": "Precise compile-time performance prediction for superscalar-based computers", "url": "http://dl.acm.org/citation.cfm?id=178250"}