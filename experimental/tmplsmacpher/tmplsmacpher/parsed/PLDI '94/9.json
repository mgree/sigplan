{"article_publication_date": "06-01-1994", "fulltext": "\n GIVE-N-TAKE A Balanced Code Placement Framework Reinhardvon Hanxleden Ken Kennedy Center for Research \non Parallel Computation Department of Computer Science, Rice University P.O. Box 1892, Houston, TX 77251 \nE-mail: {reinhardlken}@rice.edu Abstract GIVE-N-TAKE is a code placement framework which uses a general \nproducer-consumer concept. An advan\u00adtage of GIVE-N-TAKE over existing partial redundancy elimination \ntechniques is its concept of production re\u00adgions, instead of single locations, which can be bene\u00adficial \nfor general latency hiding. GIVE-N-TAKE guar\u00adantees balanced production, that is, each production will \nbe started and stopped once. The framework can also take advantage of production coming for free, as \ninduced by side effects, without disturbing balance. GIVE-N-TAKE can place production either before or \nafter consumption, and it also provides the option to hoist code out of potentially zero-trip loop (nest) \ncon\u00adstructs. GIVE-N-TAKE uses a fast elimination method based on Tarjan intervals, with a complexity \nlinear in the program size in most cases. We have implemented GIVE-N-TAKE as part of a Fortran D compiler \nprototype, where it solves various communication generation problems associated with compiling data-parallel \nlanguages onto distributed\u00admemory architectures. Introduction Partial Redundancy Elimination (PRE) \nis a classical optimization framework for moving and placing code in a program. Example applications \ninclude common subexpression elimination, loop invariant code motion, and strength reduction. The original \ndataflow frame\u00adwork for performing PRE was developed by Morel and *This work is supported by an IBM fellowship, \nand by the Na\u00adtional Aeronautics and Space Administration/the National Sci\u00adence Foundation under grant \n#AS C-9349459. Permission to copy without fee all or part of this material is granted provided that the \ncopies are not made or distributed for direct commercial advantage, the ACM copyright notice and the \ntitle of the publication and its date appear, and notice is giVen that copyin is by permission of the \nAssociation of Computing Machinery. % o copy otherwise, or to republish, requires a fee ancf/or specific \npermission. SIGPLAN 94-6/94 Orlando, florfda USA @ 1994 ACM 0-89791 -662-x1940006..$3.5O Renvoise [MR79] \nand has since then experienced vari\u00adous refinements [JD82, DS88, Dha88a, Dha91, DRZ92, KRS92]. However, \nthe PRE frameworks developed to date still have certain limitations, which become appar\u00ad ent when trying \nto apply them to more complex code placement tasks. Atomicity: PRE implicitly assumes that the code \nfragments it moves, generates, or modifies are atomic in that they need only a single location in the \nprogram to be executed. For example, when placing the computation of a common subexpres\u00adsion, PRE will \nspecify only one location in the program, and code will be generated at that lo\u00adcation to perform the \nentire computation. Later optimizations may then reschedule the individual instructions, for example \nto hide memory access delays, but PRE itself does not provide any such mechanism. Ignoring side effects: \nTaking again the example of common subexpression elimination, classical PRE assumes that each common \nsubexpression has to be computed somewhere; i.e., nothing comes for free. However, there are problems \nwhere side ef\u00adfects of other operations can eliminate the need for actual code placement. For example, \nwhen plac\u00ading register loads and stores, certain loads may become redundant with previous definitions. \nThis is generally treated as a special case} for example by developing different, but interdependent \nsets of equations for loads and stores [Dha88b]. Pessimistic loop handling: One difficulty with flow \nanalysis has traditionally been the treatment of loop constructs that allow zero-trip instances, like \na Fortran DO loop. Hoisting code out of such loops is generally considered unsafe, as it may introduce \nstatements on paths where they have not existed before. However, unless the computation to be moved may \nchange the meaning of the program (for example by introducing a division by zero), we often would like \nto hoist computation out of such loops even if the number of iterations is not known at compile time. \nSeveral techniques exist to handle zero-trip loops, like for example adding an extra guard and a pre\u00ad \nheader node to each loop [Sor89], explicitly intro\u00ad ducing zero-trip paths [DK83], or collapsing inner\u00ad \nmost loops [HKK+ 92]. These strategies, however, result in some loss of information due to explicit control \nflow graph manipulations, and they do not fully apply to nested loops. This paper presents a data flow \nframework, called GIVE-N-TAKE, that aims to overcome these limitations in a general context. It is applicable \nto a broad class of code generation/placement problems, including the classical domains of PRE techniques \nas well as memory hierarchy related problems, like prefetching and com\u00admunication generation. GIVE-N-TAKE \nis subject to a set of correctness and optimality criteria (see Sec\u00adtion 3.2); for example, each consumption \nmust be pre\u00adceded by a production, and any generated code should be executed as infrequently as possible. \nHowever, the solutions computed by GIVE-N-TAKE vary depending on which kind of problem it is applied \nto. In a BE-FORE problem, items have to be produced before they are needed (e. g., for fetching an operand), \nwhereas in an AFTER problem, they have to be produced after\u00adwards (e. g., for storing a result). Intuitively, \none can think of an AFTER problem as a BEFORE problem with reversed flow of control. Orthogonally we \ncan classify a problem as EAGER when it asks for production as early as possible (e. g., sending a message), \nor as LAZY when it wants produc\u00adtion aa late as possible (e. g,, receiving a message); this definition \nassumes a BEFORE problem. For an AFTER problem, early and late have to be interchanged. (Classical PRE, \nfor example, can be classified as a LAZY, BEFORE problem.) This means that the same framework can be \nused for different flavors of problems; there are no separate sets of equations for loads and stores \n[Dha88b], or for READS and WRITES [GV91]. The rest of this paper is organized as follows. Sec\u00adtion 2 \nintroduces the communication generation prob\u00adlem, which will be used as an illustrating example ap\u00adplication \nof GIVE-N-TAKE. Section 3 provides further intuition for the GIVE-N-TAKE framework and some background \non the type of flow graph and neighbor relations used by the GIVE-N-TAKE equations. Sec\u00adtion 4 states \nthe actual equations and argues informally for their correctness and efficiency. Section 5 gives an efficient \nalgorithm for solving the GIVE-N-TAKE equa\u00adtions, Section 6 concludes with a brief summary. A dis\u00adcussion \nof possible extensions, such as the combination of GIVE-N-TAKE with dependence analysis, and for\u00admal \ncorrectness proofs of GIVE-N-TAKE can be found elsewhere [HK93]. doi=l, N y(i) =... enddo if test then \ndoj=l, N z(j) =.,. enddo dok=l, N . ..= ~(a(~)) enddo else dol=l, N . . . = z(a(l)) enddo endif Figure \n1: An instance of the communication placement problem, where the array x is assumed to be distributed \nor shared. Each reference to z in the k and 1 loops necessitates a global READ operation, whereby a pro\u00adcessor \nreferencing some element of z receives it from its owner. Possible communication placements are shown \nin Fi,gure 2. 2 A Code Placement Example: Communication Generation An example of code placement is the \ngeneration of com\u00admunication statements when compiling data parallel languages, like Fortran D [HKT92a] \nor HIGH PERFOR-MANCE FORTRAN [KLS+94]. For example, a processor of a distributed memory machine may reference \nowned data, which by default reside on the processor, as well as non-owned data, which reside on other \nprocessors. Local references to non-owned data induce a need for communication, in this case a READ of \nthe referenced data from other processors. Figure 1 shows an example node code containing references \nto distributed data. Since generating an individual message for each da\u00adtum to be exchanged would be \nprohibitively expen\u00adsive on most architectures, optimizations like mes\u00adsage vectorization, latency hiding, \nand avoiding redun\u00addant communication are crucial for achieving accept\u00adable performance [H KT92b]. The \nprofitability of such optimizations depends heavily on the actual machine characteristics; however, even \nfor machines with low latencies or shared-memory architectures, the perfor\u00admance can benefit from maximizing \nreuse and mini\u00admizing the total number of shared data accesses. Figure 2 compares two possible communication \nplacements for the example from Figure 1. Note that the GIVE-N-TAKE solution shown on the right would \ngenerally be considered unsafe, since for N < 1 the loops would not be executed. ln the communication \ngeneration problem, however, we generally rather ac\u00ad~ept the risk of slight overcommunication than not \ndoa=l, N if test then if test then READ~.n~{z(a(l: N))} y(2) =... doi=], N doi=l, N doi=lj N enddo Z(a(i)) \n= . . . Z(a(i)) = . . . y(i) =... if test then enddo enddo enddo doj=l, N if test then z(j) enddo dok=l, \n=... N doj=l, z(j) enddo N =... READRecu{x(u(l: N))} E5EEm dok=l, N . . . = Z(a(k)) . . . = Z(a(k)) enddo \nenddo else else dol=l, N READRecU{X(u(l: N))} dol=l, N EZ3m!l . . . = X( Cl(l)) . ..= enddo X(49) enddo \nendif endif Figure 2: Possible communication placements for the code in Figure 1. A naive code generation, \nshown on the left, results in a total of N messages to be ex\u00ad changed, without any latency hiding. The \nsolution provided by GIVE-N-TAKE, shown on the right, needs just one message and uses the i-loop for \nlatency hid\u00ad ing (z(a(k)) and z(a(i)) can be recognized as identical based on the subscript value numbers). \nhoist communication. Furthermore, it is often the case that non-execution of a loop also means that no \ncom\u00admunication needs to be performed (in the example, N <1 implies z(a(l :N)) = 0). Note that the examples \nshown in this paper do not include data declarations, initializations, distribution statements, etc. \nThe communication statements are in a high level format that does not include any schedule parameters, \nmessage tags, and so on, Communication schedule generation, which is a non-trivial problem in itself \n[H KK+92], and the conversion from global to lo\u00adcal name space are also excluded. These and other im\u00adplementation \ndetails on the usage of GIVE-N-TAKE for communication generation, like the value number based data flow \nuniverse, are described elsewhere [Han93]. If we do not use a strict owner computes rule [CK88], then \nnon-owned data may not only be locally refer\u00adenced, but also locally defined. We assume that these data \nhave to be written back to their owners before they can be used by other processors, as shown in Fig\u00adure \n3. (An alternative would be the direct exchange between a non-owner that writes data and another non\u00adowner \nthat reads them [GS93]. This could also be ac\u00adcommodated by GXVE-N-TAKE, but especially in the presence \nof indirect references it would result in more doj=l, N . ..= z(.i +5) enddo endif dok=l, N m doj=l, \nN ... = Z(k + 5) . ..= x(j + 5)enddo enddo E endif dok=l, N ... = z(k + 5) enddo Figure 3: Example \nof a code with local definitions of potentially non-owned data (left), and a corresponding placement \nof global WRITES (right). complicated code generation.) Dependence analysis can guide such optimizations, \nfor example by guaranteeing the safety of hoisting com\u00admunication out of a loop nest. However, dependence \nanalysis alone is not powerful enough to take advan\u00adtage of all optimization opportunities, since it \nonly compares pairs of occurrences (i. e,, references or def\u00adinitions) and does not take into account \nhow control flow links them together. Therefore, combinations of dependence analysis and PRE have been \nused, for ex\u00adample for determining reaching definitions [GS90] or performing scalar replacement [CK92]. \nDuesterwald et al. incorporate iteration distance vectors (assuming regular array references) into an \narray reference data flo-w framework, which is then applied to memory op\u00adtimization and controlled loop \nunrollir,g [DGS93]. Several researchers have already addressed the com\u00admunication generation problem, \nalthough often re\u00adstricted to relatively simple array reference patterns. Amarasinghe and Lam optimize \ncommunication gen\u00aderation using Last Write Trees [A L93]. They assume affine loop bounds and array indices, \nthey do not al\u00adlow loops within conditionals (such as in Figure 1). Gupta and Schonberg use Available \nSection Descrip\u00adtors, computed by interval based data flow analysis, to determine the availability of \ndata on a virtual processor grid [GS93]. They apply (regular) mapping functions to map this information \nto individual processors and list redundant communication elimination and commu\u00adnication generation as \npossible applications. Granston and Veidenbaum combine dependence analysis and PRE to detect redundant \nglobal memory accesses in parallelized and vectorized codes [GV91]. Their tech\u00adnique tries to eliminate \nthese operations where possi\u00adble, also across loop nests and in the presence of con\u00additionals, and they \neliminate reads of non-owned vari\u00adables if these variables have already been read or writ\u00adten locally. \nHowever, they assume atomicity, and they also assume that the program is already annotated with read/write \noperations; they do not try to hoist memory accesses to less frequently executed regions. While these \nworks address many important aspects of communication generation that are outside of the scope of GIVE-N-TAKE \nitself, such as name space map\u00adpings or regular section analysis, they do not seem to be general and \npowerful enough with respect to com\u00admunication p~acement. In the following, it is this aspect that we \nwill focus on.  3 The Give-N-Take Framework The basic idea behind the GIVE-N-TAKE framework is to view \nthe given code generation problem as a producer-consumer process. In addition to being pro\u00adduced and \nconsumed, data may also be destroyed be\u00adfore consumption. Furthermore, whatever has been produced can \nbe consumed arbitrarily often, until it gets destroyed. Data flow frameworks are commonly characterized \nby a pair (L, F), where L is a meet semilattice and F is a class of functions (see Marlowe and Ryder \n[MR90] for a discussion of these and other general aspects of data flow frameworks). Roughly speaking, \nL characterizes the solution space (or universe) of the framework, such as the set of common subexpressions \nor available con\u00adstants, and their interrelationships. F contains func\u00adtions that operate on L and compute \nthe desired infor\u00admation about the program. Together with a flow graph (consisting of nodes, edges, and \na root) and a map\u00adping from graph nodes or edges to F, this framework constitutes a data flow problem, \nwhich can be solved to analyze and optimize a certain aspect of a specific program. However, since we \nview GIVE-N-TAKE as a fairly general code placement mechanism, this paper will focus mostly on F, the \nclass of functions that we use to propagate information about consumption and production through a given \nprogram. 3.1 Communication Placement with Give-N-Take The problem of generating READS can be interpreted \nas a BEFORE problem as follows: Each reference to non-owned data consumes these data. Each READ operation, \nwhere a processor p sends data that it owns to another processor q that re\u00adceives and references these \ndata, produces the data sent. e Each non-local definition (i. e., a definition on an\u00ad .. other processor) \nof non-owned data destroys these data. To split each READ into a READSen~ (the send issued at the owner) \nand a READ~e,. (the corresponding receive at the referencing processor), we need both the EAGER and the \nLAZY solution of the framework. We want to send as early as possible and receive as late as possible; \nsince this is a BEFORE problem, the READ send s will be given by the EAGER solution, and the READReCU \nS will be the LAZY solution. For placing global WRITES, the non-owned defini\u00adtions can be viewed as consumers, \njust as non-owned references, and we have to insert producers which in this case communicate data back \nto their owners (in\u00adstead of from their owners). Since we want to write data after they have been defined, \nthis is an AFTER problem. Note that in this scenario, the previous prob\u00adlem of analyzing communication \nfor non-owned refer\u00adences can be modified to take advantage of non-owned definitions if they are later \nlocally referenced; Z.~., non\u00adowned definitions can also be viewed as statements that produce non-owned \nreferences as a side effect ( for free ), potentially saving unnecessary communication to and from the \nowner. Again, we can split each WRITE into a WRITEs~nd (given by the LAZY solution, since WRITE is an \nAFTER problem) and a WRITER,CU (the EAGER solution). 3.2 Correctness and Optimality Given a program with \nsome pattern of consumption and destruction, our framework has to determine a set of producers that meet \ncertain correctness require\u00adments and optimality criteria. The requirements that GIVE-N-TAKE has to meet \nto be correct are the fol\u00adlowing (with their specific implications when applied to communication generation): \n(Cl) Balance: lf we compute both the EAGER and the LAZY solution for a given problem, then these solutions \nhave to match each other; see Figure 4, (For each executed READ 5end, exactly one match\u00ad ing READ Recu \nwill be executed, and vice versa; similarly for WRITESend s and WRITERecU s. ) (C2) ,$ a~ety: Everything \nproduced will be consumed; see Figure 5, (No unnecessary READS or WRITES. In our specific case, this \nis more an optimization than a correctness issue. ) A special case are zero-trip loop constructs, like \na Fortran DO loop. GIVE-N-TAKE tries to hoist 110 items out of such loops, unless explicitly told oth\u00aderwise \non a general [HK93] or case-by-case (Sec\u00ad tion 4,1) basis. (C3) S@icz ency: For each consumer at node \nn in the program, there must be a producer on each incoming path reaching n, without any destroyer in \nbetween; see Figure 6. (All references to non\u00adowned data must be locally satisfiable due to pre\u00adceding \nREADS or local definitions, without inter\u00advening non-local definitions, and all definitions of non-owned \ndata must be brought back to their owners by WRITES before being referenced non\u00adlocally or communicated \nby a READ.) The optimization criteria, subject to the correctness constraints stated above, are: (01) \nNothing produced already (and not destroyed yet) will be produced again; see Figure 7. (Noth\u00ading will \nbe recommunicated, unless it has been non-locally redefined.) (02) There are as few producers as possible; \nsee Fig\u00adure 8. (Communicate as little as possible. ) (03) Things are produced as early as possible for \nEAGER, BEFORE and LAZY, AFTER problems; see Figure 9. (Send as early as possible.)  (03 ) Things are \nproduced as late as possible for LAZY, BEFORE and EAGER, AFTER problems; see Figure 10. (Receive as late \npossible.) Note that while the correctness criteria are treated as strict requirements that GIVE-N-TAKE \nmust ful\u00adfill [H K93], the optimality criteria are viewed more as general guidelines (and are phrased \ncorrespondingly vague).  3.3 The Interval Flow Graph A general data flow analysis algorithm that considers \nloop nesting hierarchies is interval analysis. It can be used for forward problems (like available expressions) \n[A1170, COC70] and backward problems (like live vari\u00adables) [Ken71], and it has also been used for code \nmo\u00adtion [DP93] and incremental analysis [Bur90]. We are using a variant of interval analysis that is \nbased on Tar\u00adjan intervals [Tar74]. Like Allen-Cocke intervals, a Tar\u00adjan interval T(h) is a set of control \nflow nodes that cor\u00adresponds to a loop in the program text, entered through a unique header node h, where \nh @ T(h). However, Tarjan intervals include only nodes that are part of this loop (i. e., together with \ntheir headers they form nested, &#38;&#38; Figure 4: Left: unbalanced production, where one EAGER(X) \nproduction is followed by an arbitrary num\u00adber of LAZY(X) productions. Right: possible solution obeying \ncorrectness criterion Cl. Eaa(;) Consume(X) Consume(X) Destroy(X) Destroy(X) eeFigure 5: Left: unsafe \nproduction. Right: possible solution obeying C2. Destroy(X) Destroy(X) E&#38;@) Eager(X) Laz X @@ Consume(X) \nConsume(X) Q Q  strongly connected regions), whereas Allen-Cocke in- Figure 6: Left: insufficient production. \nRight: possible tervals include in addition all nodes whose predeces\u00ad solution obeying C3. sors are all \nin T(h); Z.e., they might include an acyclic structure dangling off the loop. In that sense, Tarjan \nConsume(x) a Eager(X) Laz m Figure 7: Left: redundant solution obevirw 01. Eager(X) Eager(Y) Lazy(X) \n0 + Figure 8: Left: too many solution obeying 02. Desrroy@) Ea?azer(X) Consume(X) 8 Figure 9: Left: \ntoo late solution obeying 03. + E&#38;r(x) , 4 I Figure 10: Left: too early solution obe~in~ 03 , Eaa(x) \n&#38;i Consume&#38;) 1 production. Right: possible Eager(X,Y) Lazy(X,Y) Consume(X,Y) 8 producers. Right: \npossible Destroy(x) b Eager(X) a Lazy(X) Consume&#38;) 8 production. Right: possible + Eager(X) a, \nLazy(X) production. Right: possible intervals reflect the loop structure more closely than Allen-Cocke \nintervals [RP86]. Note that a node nested in multiple loops is a member of the Tarjan interval of the \nheader of each enclosing loop. Unlike in classical interval analysis, we do not explic\u00aditly construct \na sequence of graphs in which intervals are recursively collapsed into single nodes. Instead, we operate \non one interval flow graph G = (N, E), with nodes N and edges E. ROOT E N is the unique root of G, which \nis viewed as a header node for the entire pro\u00adgram. For n c N, LEVEL(?Z) is the loop nesting level of \nn, counted from the outside in; LEVEL(ROOT) = O. We define T (n) = @for all non-header nodes n, and T+(n) \n= T(n) U {n} for all nodes n. We also de\u00adfine CHILDREN to be the set of all nodes in T(n) which are one \nlevel deeper than n; GHILDREN(Tt) = {c I c c T(n), LEVEL(C) = LEVEL(?1) + 1}. For each rn C CHILDREN(n), \nwe define ~(rn) to be the immedi\u00ad ately enclosing interval, T(n). One of the main differences between \nG and a stan\u00addard control flow graph is the way in which edges e = (m, n) c E are constructed and classified. \nIn ad\u00addition to edges that correspond to actual control-flow edges, E may also contain SYNTHETIC edges, \nwhich connect the header h of an interval T(h) to all sinks (ex\u00adcluding T+ (h)) of edges originating \nwithin T(h). Each non-SYNTHETIC edge (m, n) is classified as having one of the following types. ENTRY: \nAn edge from an interval header to a node within the interval; n E T(m). CYCLE: An edge from a node in \nan interval to the header of the interval; m c T(n). JUMP: An edge from a node in an interval to a node \noutside of the interval that is not the header node; ~h : m E Z (h), n @ T+ (h). This corresponds to \na jump out of a loop. FORWARD: An edge that is none of the above; dh : m < T (h) ~ n E !? (h). we also \ndefine HEADER(n) = m if n is the sink of an ENTRY edge originating in m (otherwise, HEADER(n) = 0). Note \nthat CYCLE and JUMP edges correspond to Tar\u00adjan s cycle and cross edges, respectively [Tar74]. How\u00adever, \nwe divide his forward edges into FORWARD and ENTRY edges depending on whether they enter an in\u00ad terval \nor not (while others divide them into forward and tree edges depending on whether they are part of an \nembedded tree or not). Note also that for each JUMP edge (m, n), G contains LEVEL(ni) LEVEL(n) SYNTHETIC \nedges. GIVE-N-TAKE requires G to have the following prop\u00aderties: 112 iioa=l, N y(a(i)) = ... if test(i) \ngoto 77 enddo doj=l, N ... enddo 77 dok=l, N . . . = z(k + 10) +y(b(k)) enddo Figure 11: Example code. \nWe wish to use the j-loop for latency hiding in case the branch out of the i-loop is not taken. G is \nreducible; i.e., each loop has a unique header node. This can be achieved, for example, by node splitting \n[CM69].  For each non-empty interval T(I1), there exists a unique n c T(h) such that (n, h) G E; i. \ne., there is only one CYCLE edge out of T(/J), We will refer to node n as LASTCHILD(h).  There are no \ncriticai edqes, which connect a node with multiple outgoing ;dg es to a node with mult\u00adiple incoming \nedges. This can be achieved, for example, by inserting synthetzc nodes [KRS92]. Code generated for synthetic \nnodes would reside in newly created basic blocks, like for example a new else branch or a landing pad \nfor a jump out of a loop.  Intuitively, a critical edge might indicate a location in the program where \nwe cannot place production with\u00adout affecting paths that are not supposed to be af\u00adfected by the production. \nThe code shown in Figure 3 is a case of placing production at a synthetic node (the added else branch). \nNote that for the EAGER production on the else branch (the READs,nd{$(6 : N +5)} ), a naively placed \nmatching LAZY production (a READRecu{$(6 : N + 5)} ) might be located right before the k-loop, since \nLAZY productions are generally delayed as far as possible. This, however, would vio\u00adlate balance, since \non the then branch the correspond\u00ading EAGER production has already been matched by a LAZY production. \nTherefore, the LAZY production is moved up into the else branch. Each of the requirements above can lead \nto a growth of G and can therefore slow GIVE-N-TAKE down. (For example, inserting synthetic nodes makes \n(9(N) = (9(E).) However, i t has been noted by several re\u00adsearchers that for typical programs, both the \naver\u00adage out-degree of flow graph nodes and the maximal loop nesting depth can be assumed to be bounded \nby small constant independent of the size of the pro\u00adgram [M R90]. Therefore, the increase of G should \nbe Level O Level 1 Level 2 If entry ! O Root ----------.I :1 :1 :1 :{ :1 :11 ~W-cycle Figure 12: Flow \ngraph for the code from Figure 11. The dashed nodes are synthetic nodes inserted to break critical edges. \nThe dashed edge (2, 10) is a SYNTHETIC edge caused by JUMP edge (4, 10) (since 4 ~ T(2)), All non-FORVVARD, \nnon-Synthetic edges are labeled as either ENTRY, CYCLE, or JUMP edges. fairly small for well structured \nprograms. Figure 12 shows the interval flow graph for the code in Figure 11. The i-loop, for example, \ncorresponds to the interval 2 (2) formed by nodes 3, 4, 5, with header 2 (again, the header itself is \nnot part of the interval). Note that FORWARD edges are the only non-SYNTHETIC edges that do not cross \nnesting level boundaries. 3.4 Traversal Orders and Neighbor Re\u00adlations The order in which the nodes of \nthe interval flow graph are visited depends on the given problem type (BEFORE/AFTER, EAGER/LAZY) and \non the pass of the GIVE-N-TAKE framework that is currently being solved (see Section 5). E induces two \npartial orderings on N: Vertically: Given a FORWARD/JUMP edge (m, 71), a FORWARD order visits m before \nn, and a BAL:K -WARD order visits rn after n. Horizontally: Given m, n c N such that m E T(n), an UPWARD \norder visits m before n, whereas a DOWNWARD order visits m after n, Since these partial orderings are \northogonal, they can be combined into PREORDER (FORWARD and DOWN-WARD), POSTORDER (FORWARD and LJPWARD), \nand the corresponding reverse orderings. For example, the nodes in Figure 12 are numbered in PREORDER. \nNote that in a BEFORE problem, the flow of information is not necessarily in FORWARD order; this will \nbecome ap\u00adparent in the discussion of the algorithm in Section 5. A data flow variable for some n E N \nmight be defined in terms of variables of other nodes that are in some relation to n with respect to \nG. Therefore, we not only have to walk G in a certain order, but we also have to access for each n c \nN a subset of N {n} that has a certain relationship with n. In general, we are inter\u00adested in information \nresiding at predecessors or succes\u00adsors. However, we are also considering through which type of edge \nthey are connected to n. The edge type carries information about how the neighboring nodes are related \nto each other (for example, whether mov\u00ading production from one node to the other constitutes a hoist \nout of a loop or not). The type also indicates whether this information has already been computed under \nthe current node visiting order or not. Let TYPE be a set of edge types, where the letters C, E, F, J, \nand S indicate CYCLE, ENTRY, FORWARD, JUMP, and SYNTHETIC edges, respectively. GIVE-N-TAKE uses the following \nneighbor relations: PREDsTypE(n): The source nodes of edges reaching n of a type in TYPE. SuCcsT E(n): \nThe sink nodes of edges originating from n of a type in TYPE. The conventional predecessors and successors \nare then PREDSCE (n) and SuccsCEF (n), respectively, which we will abbreviate as PREDS(n) and SUCCS(n), \nrespectively. We will refer to the transitive clo\u00adsures of PREDSFJ(n) and Succs FJ(n) as the ances\u00ad tors \nand descendants of n, respectively. Note that {LASTCHILD(n)} = PREDSC(n), and {HEADER(n)} = PREDSE(n). \nNote also the following implications of the lack of critical edges: * Let e = (n, s) be a JUMP edge. \nThen there exists an h c N with n ~ T(h), s @T+(h). Since T+ (h) is by definition strongly connected, \nn must have successors within T+ (h). Since s is also a suc\u00adcessor of n, n must have multiple outgoing \nedges. However, G does not have critical edges, there\u00adfore s has only one predecessor, which is n; i. \ne., F REDSCEF(S) = 0. In other words, the sink of a JUMP edge, like node 10 in Figure 12, never has any \npredecessors besides the source of the JUMP edge. Let e = (n, h) be a CYCLE edge. It follows that h \nis an interval header, which by definition has rrlulti\u00adple predecessors. Since h is a successor of n, \nn may not have any other successors (otherwise e would be critical). However, it is h @ SUCCSEFJ(n), \nIt follows SUCCSEFJ(7t) = 0 for each source n of an CYCLE edge. Even though the equations and their correctness \nand effectiveness are the same for both BEFORE and AFTER problems, we will for simplicity assume in the \nfollowing that we are solving a BEFORE problem unless noted otherwise.  4 Give-N-Take Equations Given \na set of initial variables for each node n E N, which describe consumption, destruction, and side ef\u00adfects \nat the corresponding location in the program] GIVE-N-TAKE computes the production as a set of re\u00adsult \nvariables for each node. Intermediate stages are the propagation and blocking of consumption, and the \nplacing of production. In the following, let 71E N, let L denote the empty set, and let T be the whole \ndata flow universe. If an equation asks for certain neighbors (like PREDSFJ (n)) and there are no such \nneighbors (such as for a loop en\u00adtry node), then an empty set results. Subscripts tn, out denote variables \nfor the entry and the exit of a node, re\u00adspectively (reverse for AFTER problems). Subscript 10C indicates \ninformation collected only from nodes within the same interval (nodes in J(n)), and init identifies variables \nthat are supplied as input to GIVE-N-TAKE. Figure 13 cent ains the equations for the data flow variables, \nwhich will be introduced in the following sec\u00adtions. We will provide example values from the READ instance \nfor the graph in Figure 12, where $~, ya, ~b correspond to references z(k+ 10), y(a(i)), and y(b(k)), \nrespectively (values at ROOT are excluded for simplici\u00adty). 4.1 Initial Variables The following variables \nget initialized depending on the problem to solve, where 1 is the default value. STEALtn,t(n): All elements \nwhose production would be voided at n, This can also be used to prevent hoisting productions out of zero-trip \nloops, if so desired. In our communication problem, this includes an array portion p if either the contents \nof this portion get modified at n, or if p itself gets changed, for example if p is an indirect array \nreference and n modifies the indirection array [H K93]. STEAL(n) = STEAL,.i,(n) USTEALIoC(LASTCHILD( \nn)) GIVE(n) = GIVEtnt,(n) UGIVE,..(LASTCHILD( n)) BLOCK(n) = STEAL(n) UGIVE(n)U u BLOCK/c(s) sesuccsE(n) \nTAKENoU,(n) = n TAKEN?.(s) .EsuccsF s(n) TAKE(n) = TAKEin2,(n) U ( U TAKEN,.(s) s6succsE(n) -STEAL(71)) \nU((TAKENoUt(n) o u TAKEIOC(S)) BLOCK(n)) scsuccsE(n) TAKEN,.(n) = TAKE(n) U (TAKENoU,(n) BLOCK(n)) \nBLOCK,oc(n) = (BLOCK(n) U u BLOCK{..(S)) sEsuccsF(n) TAKE(n) TAKEioc(n) = TAKE(n) U ( u TAKEIOC(S) \nBLOCK(n)) .EsuccsE (n) (1) (2) (3) (4)  (!5) (6) (7) (8)  GIVEioJn) = (GIVE(n) U TAKE(n) U n \nGIVEI..(P)) -STEAL(n) (9) p~PREDSFJ(fL) STEALIoc(n) = STEAL(n) U U (STEAL,oc(p) -GIVE,..(P)) U U STEAL,oc(p) \n(lo) PEPREDSFJ(?a) pEPREDSs(tz) GIVEN,n(n) = GIVEN(HEADER(n)) u GIVENOJP) u (TAKENt.(n) n GIVENoU,(q)) \n(11) nu PCPREDSFJ(~) gEpREDSFJ(n) TAKENin(n) for an EAGER Problem, GIVEN(n) = GIVENt.(n) U (12) TAKE(n) \nfor a LAZY Problem. { GIVENoUt(n) = (GIVE(n) U GIVEN(n)) STEAL(n) (13) RESz. (n) = GIVEN(n) GIVENt. \n(n) (14) RESoUt(n) = U GIVEN,n(s) -GIVENou,(n) (15) SCSUCCSFJ(?Z) Figure 13: GIVE-N-TAKE equations. For \nFigure 12, we have for example yb ~ Ya c GWnit({3}). STEAL,nit({3}). (Read as: For the READ prob\u00ad lem, \nthe variable STEALzwzt at node 3 contains the TAKE%.it(n): The set of consumers at n. array portion referenced \nby y(b(k)). ) For communication generation, this is the set of GIVEz~,t(n): All elements that come \nfor free, i.e., non-owned array references. which are already produced at n. ~~,yb ~ TAKE,n,,({ 13}), \nIf we do not use the owner computes rule in com\u00ad munication generation, then this includes local 4.2 \nPropagating Consumption definitions of non-owned data, since a later refer\u00ad ence to these data does not \nneed to communicate The following variables, together with the variables de\u00ad them in any more. fined \nin Section 4.3, analyze consumption. STEAL(n): All elements whose production would be voided by n itself \n(given by STEAL, n,t(n)), or by some m < T(n) without being resupplied by a descendant of m within T(n) \n(given by STEALIoc(LASTCHILD (n))). yb E STEAL({2,3}) GIVE(n): All elements that are already produced \nat n, or at some node in T(n) without being stolen later within T(n). BLOCK(n): Elements whose production \nis blocked by n, i.e., whose production cannot be hoisted across n because it is stolen or already produced \nat n or a node in T(n). y., yb = BLOCK({2,3}). Takeout: Things guaranteed to be consumed (be\u00adfore being \nstolen) on all paths originating in n, ex\u00adcluding n itself. Here we have to consider not only FORWARD \nand JUMP edges, but also SYNTHETIC edges. (Otherwise we might violate safety by pro\u00adducing something \nwhose only consumer may be skipped due to a jump out of a loop). xk, yb C TAKE Nout({2,6,7,9 . . . 11}); \nalso, ~k E TAKEN OUt({l}). TAKE(n): The set of consumers at n. This includes items that are guaranteed \nto be consumed by nodes in T(n) (the TAKEN ~nterm) and not stolen at n, and items that may be consumed \nby T (n) (the TAKEIOC term) and are guaranteed to be con\u00adsumed on exit from n without being blocked by \nn.   $~,~b ~ TAKE({12,13}) TAKEN,n(n): Similar to TAKEN OUt,except that the ef\u00adfects of n itself are \nincluded. %k,~b C TAKEN m({6,7,9. .13}); also, x~ e TAKEN,n({l,2}). BLOCKIOc(n): Items blocked by n or \nby descendants of n within J(n) without being consumed. y~,yb E BLOCK/m({l . . .3}). TAKEjOC(n): Items \ntaken by n, by descendants of n within J(n), or by nodes within T(n). Here (unlike for BLOC K /OC) we \nhave to explicitly include suc\u00adcessors on ENTRY edges, since they are not guar\u00adanteed to be reflected \nin TAKE (which has to be conservatively small), whereas they will always be considered by BLOCK(n) (which \nis conservatively large). ~k,yb E TAKEi.c({6, 7,9. ..l3}); also, z~ C TAKEIOC({ 1,2}). 4.3 Blocking \nConsumption The following variables are used by the interval headers to determine whether items are stolen \nor taken within the interval. GIVE~..(n): Items produced by 71or by ancestors of 71 within the same interval. \nHere items are treated as produced also if they are consumed, since con\u00adsumption is guaranteed to be \nsatisfied by a pro\u00adduction. y= E GIVE/cx({2 . ..7.11}); l}); Xk, ?jb 6 GlvEb.({12. .14}).  STEAL(OC(n): \nItems stolen by 71, or stolen by a prede\u00adcessor p of n without being resupplied by p. Fur\u00adthermore, if \nthere exists a p s PREDSs(n) (i. e., 7~ is the sink of a JUMP edge, and p is the header of an interval \nenclosing the source of the JUMP edge but not n itself), then we also have to in\u00adcIude items stolen by \np; however, since the inter\u00adval headed by p is not guaranteed to be completed before n is reached (since \ntaking the JUMP edge corresponds to a jump from within the interval), we cannot exclude items resupplied \nby p (which would be given by GIVErOc(p)). Yb ~ STEAL1O.({2 . ..7.9...12, 14}).  4.4 Placing Production \nAfter analyzing what is consumed (and not already produced) at each node, the production needed to sat\u00adisfy \nall consumers is computed by the following vari\u00adables. (As described in Section 5, the following vari\u00adables \nmay differ for the EAGER and for the LAZY so\u00adlution; this will be indicated in the examples by super\u00adscripts. \n) GlVEN,n(7~): Things that are guaranteed to be avail\u00adable at the entry of n (or, for an AFTER problem, \nthe exit of n.) If n is a first child, then it has everything available that is available at its header \n(and PREDSFJ = 0). Otherwise, things are guar\u00adanteed to be produced if they are produced along all incoming \npaths, or if they are produced at least along some incoming paths and guaranteed to be consumed. In the \nlatter case, the result variable RESOUt will ensure that things will be produced also along the paths \nthat originally did not have them available (see Equation 15). x~ c GIVEN::9 ({2 . 14}); Y. E GIVE N~;ger({4 \n. ..14}). yb E GIVE N~;ger({7...9,11...14}). ~k,yb E GlVEN&#38;({13, 14}); y~ e GIVEN 7({4. .14}). \n GIVEN(n): Items guaranteed to be available at n itself, READSend{Z(ll : N + 10)} either because they \ncome from predecessors of n, doi=l, N or because they are consumed by n itself, or, for ?J(a(i)) = . \n. . an EAGER problem, by a descendant of n. if test(i) then z, E GIVENeager({l . . . 14}); y~ E GlVENeager({4 \n. . . 14}); y, c GlVENeager({6 . . . 14}). w ~~,yb E GIVEN a~y({12 . . .14}); goto 77 Ya e GlvEN@({4 \n. . . 14}). endif GIVEN OUt(n): Things that are available on exit from n. This includes whatever comes \nfrom at n itself, but it excludes things stolen by n. a~ c GIVEN fife ({l . . . 14}); y. e GIVEN ;.: \n ({2 . . . 14}); Yb e GIVEN % ({6 . . . 14}). %k,yb~ GlvEN5;~({12 ...14}); y. E GIVEN !::({2 . . . 14}). \n 4.5 Result Variables The result of GIVE-N-TAKE analysis is expressed by the following variables. RES,. \n(n): The production generated at the entry of n. This includes everything that is guaranteed to be available \nat n itself but is not yet available at the entry of n. The READ,Send S stem from Zk E RES~~ger({1}) \nand yb 6 RES~~9er({6, 10}); the READ Re.V S are $~, yb E R@Z ({12}). RESOUt(n): The production at the \nexit of n. This in\u00adcludes items whose availability has been guaran\u00adteed to some successors of n and that \nare not al\u00adready available on exit from n. In Figure 12, there is no production needed on exit. Note \nthat .z E RESO.t(n) implies by Equation 15 that ~ @ GIVE NOUt(n , but that for some s c SUCCSFJ(n) 1 \nandp c PREDSF (s) {n}, z c GIVEN OUt(p)must hold. In other words, n must have a successor s which in \nturn has a predecessor p # n that produces an x which is consumed by s and not produced by n. Furthermore, \nthe lack of critical edges implies that s must be the only successor of n, and therefore it does not \nmatter whether we use union or intersection in Equation 15. Figure 14 shows the code from Figure 11 annotated \nwith communication generation as computed by GIVE-N-TAKE. 5 Solving the Equations This section presents \nan algorithm, GiueNTake, for solving a code placement problem using the GIVE-N\u00adenddo  EEi2BM! do~=l, \nN ... enddo 7 7 I READR.CV{Z(ll : N + 10), Y(b(I : N))} dok=l, N . . . = Z(k+ 10) +y(b(k)) enddo Figure \n14: The code from Figure 11 annotated with communication statements. TAKE framework. Section 4 already \nlisted the equa\u00adtions that lead from the initial data flow variables to the result variables. What is \nleft towards an actual algorithm is a recipe for evaluating these equations. 5.1 The Constraints The \nobjective of the algorithm is to assign the flow variables at each node a value that is consistent with \nall equations; i. e., we have to reach a jixed point. Note that the number of evaluation iterations to \nreach a fixed point may be constant, as is usually the case in interval analysis. In general, the evaluation \norder is also impor\u00adtant for the convergence rate and, in some cases, termi\u00adnation behavior of the algorithm. \nFor GIVE-N-TAKE, there actually exists an order where the right hand side of each equation to be evaluated \nis already fully known due to previous computation. Therefore, GiveNTake has to evaluate each equation \nonly once for each node, which implies guaranteed termination and low compu\u00adtational complexity (it also \nimplies fastness [GW76]). However, since the direction of the flow of informa\u00adtion varies across the \nequations, we still need multiple passes over the control flow graph, solving a different set of equations \nduring each pass. An objective for GiveNTake is to minimize the num\u00adber of passes, therefore we partition \nthe equations into different sets that can be evaluated concurrently, Z.e., within the same pass. It \nturns out that each of the Sections 4.2, 4.3, 4.4, and 4.5 defines one set of equa\u00adtions that can be \nevaluated concurrently. We will re\u00adfer to these sets as S1 (Equations 1.. . 8), S2 (Equa\u00adtions 9, 10), \nS3 (Equations 11 . . . 13), and S4 (Equa\u00adtions 14, 15), respectively. Since all equations except Equation \n12 in S3 are the same for EAGER and LAZY problems and S1 and S2 are computed before S3, the variables \ndefined in S1 and S2 are the same for both kinds of problems. Therefore, we need to differentiate between \nEAGER and LAZY only for variables defined in S3 and S4. We distinguish these variables by super\u00adscripts \neager and lazy. To determine an order for solving the GIVE-N-TAKE equations that yields a fixed point \nafter evaluating each equation only once, we have to make sure that an equation is evaluated after the \nright hand side is fully known. Inspection of the equations yields the following constraints: S1 should \nbe evaluated in BACKWARD order (for example, because Equation 8 defines TAKEIOc(n) in terms of TAKEIOc(s), \nwith s c SUCCSEF(n)).  S1 should also be evaluated in UPWARD order (e.g., Equation 3)).  S1 (n) ( the \nequations from S1 for node n ) should be computed before S2(n), but after SZ(CHILDREN(n)).  S2 should \nbe evaluated in FORWARD order.  e S3 must be computed in FORWARD, DOWNWARD fashion (z. e., PREORDER), \nafter S1. o S4 has to be evaluated after S1 and S3, in any order. Intuitively, these constraints express \nthat information about consumption is flowing up and back, whereas the availability of production gets \npropagated forward and down. The production to be inserted at a node, however, again depends on the successors \nof the node. 5.2 The Algorithm The resulting algorithm is shown in Figure 15. A for\u00admal proof that it \ndoes indeed obey all ordering con\u00adstraints, as well as a proof that GIVE-N-TAKE meets the correctness \nconstraints (Cl), (C2), and (C3), can be found elsewhere [HK93]. As already noted, each equation is evaluated \nonly once for each node. Furthermore, each equation de\u00adpends only on a subset of neighbors. Therefore, \nthe to\u00adtal complexity of GIVE-N-TAKE is O(E) steps (where the cost of each step depends on the current \nlattice and its representation, for example bit vectors of a certain length). As already noted in Section \n13.3, E can be as\u00adsumed to be of a size in the order of the program size in most cases; under this assumption, \nGIVE-N-TAKE as well as other interval-based elimination methods have linear time complexity. Procedure \nG~veNTake Input: G = (N, E); VT, 6 N: TAKE, ~,Jn), STEAL, nit(n), GIVEtnJn) Output: Vn c N: RESe ge (n) \nand/or RESrazy(n) forall n G N, in REVERSEPREORDER forall c E CHILDREN(n), in FORWARDorder Compute Equations \n9, 10 endforall Compute Equations 1. ..8 endforalI forall n E N, in PREORDER Compute Equations 11...13 \nfor EAGER/LAZY endforall forall nc N Compute Equations 14,15 for EAGER/LAZY endforall end  Figure 15: \nAlgorithm GzveNTake computing an EAGER/LAZY code placement. RES without subscripts stands for both RESin \nand RESOUt. 5.3 BEFORE VS. AFTER Problems We mentioned earlier that an AFTER problem can es\u00adsentially \nbe treated as a BEFORE problem with reversed flow of control. However, this also means that the re\u00adversed \nflow graph has to fulfill the same requirements from Section 3.3 as the original graph, which is not \ntriv\u00adially the case. For example, ENTRY edges may become CYCLE edges (and vice versa), but each loop \nmay have only one CYCLE edge; this can be satisfied by adding nodes similar to the SYNTHETIC nodes. More \nseverely is the requirement for G to be reducible, which will be violated if the original graph had any \nJUMP edges, since these will become jumps tnto loops. In fact, this would prevent us from determining \na unique set of in\u00adtervals for the reverse G. For example, consider the flow graph in Figure 16, which \nmay be the result of solving an AFTER problem for a program containing a jump out of a loop, A consumption \nplaced at node 4 might be hoisted into its header (node 3), which would be unsafe (due to the path 1-2-5-3). \nIn our implementation, we handle this case by using the same interval structure as for the original graph, \nand preventing hoisting production out of loops that contain JUMP edges. This can be done by either ac\u00adcordingly \ninitializing STEAL~~~t for each header of a loop containing a JUMP edge, or by ignoring for these headers \nthe contributions to TAKE coming from the loop body (see Equation 5). Figure 16: Flow graph containing \na jump mto sloop. Note the synthetic (dashed) edge between nodes 2 5.4 A Note on Synthetic Nodes Having \ncomputed the result variables with GIVE-N-TAKE, one still has to perform the actual program optimizations \nby modifying the analyzed code. This step might be complicated by having production placed at a synthetic \nnode, which would require new basic blocks (see Figure 3). However, it may often be possi\u00adble to shift \nproduction to a neighboring non-synthetic node. This can either be done at code generation time, or by \npost-processing the results of GIVE-N-TAKE, in a way that is similar to a mechanism employed in edge-placement \n[Dha88a] for avoiding code prolifera\u00adtion. Our implementation took the latter route, by running a backward \npass on G which checks whether these movements can be done without conflicts. 6 Summary This paper has \noutlined a general code generation framework, based on Tarjan intervals, that handles sev\u00aderal different \nclasses of problems. Unlike previous ap\u00adproaches, it does not assume atomicity. Instead, GIVE-N-TAKE \nprovides both EAGER and LAZY solutions, and it guarantees their balance across arbitrary control flow. \nFurthermore, GIVE-N-TAKE can be applied to both BEFORE and AFTER problems, and it can take ad\u00advantage \nof side effects to further eliminate unnecessary production without affecting balance. Other nice prop\u00aderties \nof GIVE-N-TAKE include the option to hoist code out of zero-trip loop constructs even for nested loops, \nand the natural handling of irregular loop bounds and access pat terns. Note, however, that like with \ncode placement strate\u00adgies in general, there may be conflicting goals in how far to separate production \nand consumption. Often the computations compete for resources, like registers or message buffers, which \ncould cause some (optimiza\u00adtion to have a negative effect in practice. While GIVE-N-TAKE does not address \nthis issue directly, cer\u00adtain extensions (such as a heuristic for inserting addi\u00ad tional STEALzn,t s \nwhich blocks production) could help to solve this conflict. Other possible extensions are the combination \nwith dependence analysis (for exam\u00adple by refining the initial assignments to TAKE8.,t and STEAL$.,t), \nor a more thorough treatment of jumps out of loops for AFTER problems. While our current ap\u00adproach (Section \n5.3) prevents unsafe code generation, it may miss some otherwise legal optimization. Related to that \nis the issue of analyzing irreducible graphs in general. We have implemented GIVE-N-TAKE in C++ as part \nof a Fortran D compiler, where it is used to gen\u00aderate messages for distributed memory machines. We generate \nREADS, WRITES, and WRITES combined with different reduction operations (such as summation), all of which \ncan be placed either atomically (for example, for a library call), or divided into sends and receives. \nThe non-atomicity and balance attributes enables mes\u00adsage latency hiding and other optimization to be \nper\u00adformed across arbitrary control flow. GIVE-N-TAKE s flexibility allowed us to apply the same algorithm \nto very different tasks that traditionally were solved with separate frameworks, This simplified the \nimplementa\u00adtion in the Fortran D compiler significantly. We expect GIVE-N-TAKE to have potential use \nin other areas as well, like general memory hierarchy issues (cache prefetching, register allocation, \nparallel 1/0) and classic partial redundancy elimination appli\u00adcations (common sub expression elimination, \nloop in\u00advariant code motion, etc.). Acknowledgements We thank Paul Havlak, Chuck Koelbel, and Barbara \nRyder for many fruitful data-flow discussions; Paul has also developed much of the symbolic analysis \nunderly\u00ading our implementation. Lani Granston, Uli Kremer, Nat McIntosh, and Jerry Roth proofread the \npaper and were very helpful, especially regarding the illustrat\u00ading examples. We also thank the PLDI \nreferees, who pointed out several errors and unclarities and aided with the overall presentation, References \n[AL@ S. P. Amara.singlle and M. S. Lam. COnmnuli\u00adcation optimization and code generation for dis\u00adtributed \nmemory maclunes. ACM SIGPLA N No\u00adtices, 28(6):126 138, June 1993. Proceedings of the ACM SIGPLAN 93 C \nonfeTence on Programming Language Design and Implementation. [Al170] F. E. Allen. Control flow analysis. \n.4 CM SIGFLA N Notices, 5(7):1 19, 1970. [Bur90] M. Burke. An interval-based approach to exhaustive and \nincremental interprocedural data-flow analysis. [CK88] [CK92] [CM69] [COC70] [DGS93] [Dha88a] [Dha88b] \n[Dha91] [DK83] [DP93] [DRZ92] [DS88] [GS90] [GS93] [GV91] ACM Transactions on Programming Languages and \nSystems, 12(3):341-395, July 1990. D. Callahan aud K. Keuuedy. Compiliug programs for distributed-memory \nmultiprocessors. .lotimal o.f Supercomputing, 2:151 169, October 1988. S. Carr and K. Kennedy. Scalar \nreplacement iu the presence of conditional control flow. Technical Re\u00adport TR92283, Rice University, \nCRPC, November 1992. To appear in Software -Practice &#38; Experi\u00ad ence. J. Cocke and R. Miller. Some \naualysis teclmiques for optimizing computer programs. In Proceedings of the .2nd Annual Hawaii International \nConference on System Sciences, pages 143 146, 1969. J. Cocke. Global common sub expression elimination. \nACM SIGPLAN Notices, 5(7):20 24, 1970. E. Duesterwald, R. Gupta, and M. L. Soffa. A prac\u00adtical data flow \nframework for array refereuce aua]\u00adysis and its use in optimization. ACM SIGPLAN Notice$, 28(6):68-77, \nJune 1993. Proceedings of the ACM SIGPLAN 93 Conference on Programming Language Desagn and Implementation. \nD .M. Dharudhere. A fast algorithm for code movement optimization. ACM SIGPLAN Notices, 23(10):172 180, \n1988. D .M. Dharndhere. Register assignment using code placement techniques. Computer Languages, 13(2):75 \n93, 1988. D .M. Dhamdhere. Practical adaptation of the global optimization algorithm of Morel and Renvoise. \nACM Transactions on Programming Languages and Sys\u00adtems, 13(2):291 294, April 1991. D .M. Dhamdhere and \nJ.S. Keith. Characterization of program loops in code optimization. ComputeT Languages, 8:69 76, 1983. \n D .M. Dhamdhere and H. Patil. An e!.iminatiou al\u00adgorithm for bidirectional data flow problems usiug \nedge placement. ACM Transactions on PTogTam\u00adming Languages and Systems, 15(2):312 336, April 1993.  \nD.M. Dharndhere, B.K. Rosen, and F.K. Zadeck. How to analyze large programs efficiently aud infor\u00admatively. \nIn p?_oceedings of the ACM SIGPLAN 92 Conference on ProgTam Language Design and Imple\u00admentation, pages \n212 223, San Francisco, CA, Jnue 1992. K. Drechsler and M. Stadel. A solutiou to a problem with Morel \nand Reuvoise s Global optimization by suppression of partial rednudancies . ACM Trans\u00adactions on PTogvamming \nLanguages and Systems, 10(4):635 640, October 1988. T. Gross and P. Steeukiste. Structured dataflow \naualysis for arrays and its use in an optimiz\u00ading compiler. Soflware-P7actice and Expemence, 20(2):133 \n155, February 1990. M. Gupta and E. Schonberg. A framework for ex\u00adploiting data availability to optimize \ncommunication. In Proceedings of the Smth WoTkshop on Languages and Compt/em for Para/~el Computing, \nPortland, OR, August 1993. E. Granston and A. Veidenbaum. Detecting redun\u00addaut accesses to array data. \nIn Proceedings of Super\u00adcomputing 91, Albuquerque, NM, November 1991.  [GW76] S. Graham and M. Wegman. \nA fast and usually lin\u00adear algorithm for global data flow anal ysis. Jo urn a 1 of the A CM, 23(1) :172 \n202, January 1976. [Han93] R. v. Hanxleden. Handing irregular problems with Fortran D A preliminary \nreport. In Proceedings of the FouTth WoTkshop on Compil\u00ade~s joT Parallel Computers, Delft, The Nether\u00adlands, \nDecember 1993. D Newsletter #9, avail\u00adable via anouymow ftp from softlib. rice. edu as pub/CRPC-TRs/report \ns/CRPC-TR93339-S. [HK93] R. v. Hanxleden and K. Keuuedy. A code place\u00adment framework aud its application \nto commu\u00adnication geueratiou. Te.huical Report CRPC\u00adTR93337-S, Ceuter for Research on Parallel Coul\u00adputation, \nRice University, October 1993. Avail\u00adable via anonymous ftp from soft lib. rice edu as pub/CRPC-TRs/report \ns/CRPC-TR93337-S. [HKK+ 92] R. v. Hanxleden, K. Keunedy, C. Koelbel, R. Das, and J. %dtz. Compiler analysis \nfor irregular problems in Fortran D. Ln U. Banerjee et al., editor, Lecture Notes in Compute7 Sctence, \nvolume 757, pages 97 111. Springer, Berlin, August 1992. From the Pro\u00adceedings of the Fifth Workshop \non Languages and CompileTs fo~ PaTa(/el Conzputtng, New Haven, CT. Available via anonymous ftp from soft \nlib rice. edu as pub/CRPC-TRs/report s/ CRPC-TR92287-S. [HKT92a] S. Hiranandani, K. Kennedy, aud C. \nTseug. Com\u00adpiler support for machine-independent parallel pro\u00adgramming in Fortrau D. In J. Saltz and \nP. Mehrotra, editors, Languages, CompileTs, and Run-Time Envi\u00adronments for DistTibuied Memory Machznes. \nNorth-Holland, Amsterdam, The Netherlauds, 1992. [HKT92b] S. Hiranandani, K. Kennedy, and C. Tseng. Eval\u00aduation \nof compiler optimization for Fortran D on MIMD distributed-memory machiues. In Proceed\u00adings of the 1992 \nACM International Conference on Supercomputing, Washington, DC, July 1992. [JD8 2] S.M. Joshi and D.M. \nDhamcfhere. A composite hoistiug-streugth reduction trausformatiou for global program optimization, parts \nI &#38; 11. International Journal o.f ComputeT Mathematics, 11:21-41, 111\u00ad126, 1982. [Ken71] K. Kennedy. \nA global flow analysis algorithm. Int eT\u00adnattonal Jou rna[ of CompuieT Mathematics, 3:5 15, 1971. [KLS+94] \nC. Koelbel, D. Lovemau, R. Schreiber, G. Steele, Jr., and M. Zosel. The High PeTjormance Foriran Hand\u00adbook. \nThe MIT Press, Cambridge, MA, 1994. [KRS92] J. Knoop, O. Riithiug, and B. Steffen. Lazy code motion. \nIu Proceedings of the ACM SIGPLAN 92 Conference on PTogram Language Design and Im \u00adp~ementation, San \nFrancisco, CA, June 1992. [MR79] E. Morel and C. Renvoise. Global optimization by suppression of partial \nredundancies. Communica\u00adtions of the A CM, 22(2):96 103, February 1979, [MR90] T. Marlowe and B. Ryder. \nProperties of data flow frameworks. Acts InfoTmattca, 28:121-163, 1990. [RP86] B. G. Ryder aud M. C. \nPaull. Elimination algorithms for data flow analysis. ACM Computing Suweys, 18:77 316, 1986. [Sar89] \nA. Sorkin. Some comments on A so]utiou to a prob\u00adlem with Morel and Renvoise s Global optimiza\u00adtion by \nsuppression of partial redundancies . ACM Transactions on Programming Languages and Sys\u00adtems, 11(4):666 \n668, October 1989. [Tar74] R. E. Tarjan. Testing flow graph reducibility. Journal of Computer and System \nScaences, 9:355 365, 1974. \n\t\t\t", "proc_id": "178243", "abstract": "<p>GIVE-N-TAKE is a code placement framework which uses a general producer-consumer concept. An advantage of GIVE-N-TAKE over existing partial redundancy elimination techniques is its concept of production <italic>regions</italic>, instead of single locations, which can be beneficial for general latency hiding. GIVE-N-TAKE guaranteed <italic>balanced</italic> production, that is, each production will be started and stopped once. The framework can also take advantage of production coming &#8220;for free,&#8221; as induced by side effects, without disturbing balance. GIVE-N-TAKE can place production either before or after consumption, and it also provides the option to hoist code out of potentially zero-trip loop (nest) constructs. GIVE-N-TAKE uses a fast elimination method based on  Tarjan intervals, with a complexity linear in the program size in most cases.</p><p>We have implemented GIVE-N-TAKE as part of a Fortran D compiler prototype, where it solves various communication generation problems associated with compiling data-parallel languages onto distributed-memory architectures.</p>", "authors": [{"name": "Reinhard von Hanxleden", "author_profile_id": "81100013092", "affiliation": "Rice Univ., Houston, TX", "person_id": "PP14017722", "email_address": "", "orcid_id": ""}, {"name": "Ken Kennedy", "author_profile_id": "81100453545", "affiliation": "Rice Univ., Houston, TX", "person_id": "PP40027435", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/178243.178253", "year": "1994", "article_id": "178253", "conference": "PLDI", "title": "GIVE-N-TAKE&#8212;a balanced code placement framework", "url": "http://dl.acm.org/citation.cfm?id=178253"}