{"article_publication_date": "01-26-1981", "fulltext": "\n Permission to make digital or hard copies of part or all of this work or personal or classroom use is \ngranted without fee provided that copies are not made or distributed for profit or commercial advantage \nand that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, \nto post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. &#38;#169; \n1981 ACM 0-89791-029-X $5.00 statement or implicitly raised by some action in the statement, e.g. an \narithmetic overflow. When a statement that raises an exception is executed, the normal execution flowissuspended \nandahand/er is invoked. Since exception name are statically scoped, it is clear when a handier and a \nraise statement refer to the same or different exceptions. The handler that is invoked for a particular \nexception depends on where the exception was raised and what method is used to locate the handler. A \nhandler catches an exception when it is selected in response to the raising of a particular exception. \nThe statements that are associated with the catching handler are executed. After completing the handler, \nexecution may continue at a number of points depending on the capabilities of the mechanism, the default \nfor continuing after a handler, and the statement used to exit the handler. 2.2. Specific features A \nsmall number of characteristics will adequately define most exception handling mechanisms. The following \nsubset of characteristics are important if program optimization is to be considered. Hm?cfler Association. \nException handlers can be assoc\u00adiated with an exception name in a static or dynamic fashion. Static association \nof handlers means that handlers are statically attached to exception names and program com\u00adponents. Therefore, \ngiven the state of the call chain and a static point in the currently active procedure, it is possible \nto find out which handler will respond to a given exception. Dynamic association means that tfne program \ncan dynam\u00adically alter which handler will be invoked if an exception arises at some part of the program. \n Handler Location. Once an exception is raised, a handler may be located either by static or dynamic \nmeans. Static handler location means that the handler is located based on the static scoping of the statement \nthat raised the exception. Dynamic handler location is normally done by going up the current call chain \nto find potential handlers. Most exception handling mechanisms employ a combination of static and dynamic \nhandler association. A single-level mechanism is a dynamic mechanism with the restriction that the immediate \ncaller must provide a handler for an exception; that handler may decide to continue propagating the exception \nby raising it again. For simplicity, we will allow only one handler per procedure for each visible exception. \nThe handler will be visible for the entire procedure and located dynamically. Multiple handlers with \nstatic association are easily accom\u00admodated. Termination or Resumption. Once a handler has been located \nand its body executed, control may continue according to one of several control paths. Resumption causes \nexecution to continue at the statement following the raise statement, or within the statement that raised \nthe exception (in the case of certain implicitly raised, predefine exceptions). Termination means that \none or more levels of the currently active call chain are aborted, usually up to the procedure containing \nthe handler, and execution continues in or at the end of that procedure. Some exception handling mechanisms \nprovide both resumption and termination models, while other mechanisms provide only termination. In some \ncases different types of exceptions are introduced to indicate that either resumption or termination \nare disallowed. For example, Mesa [6] defines predefine exceptions, such as overflow, to be unresumable \nerror type exceptions. There are a variety of perturbations in the definition of where execution continues \nin the termination medel; usually, execution continues at the statement following the block associated \nwith {he handler that caught the exception. Since we have assumed that handlers are in effect for entire \nprocedures, termination is defined to mean that control continues by returning from the procedure containing \nthe handler that caught the exception. This is the definition used by Ada. We do not consider handlers \nthat raise exceptions. Precfefined and User Exceptions. User exceptions are defined, specified and raised \nonly by the programmer with some type of raise statement. Predefine exceptions can be raised implicitly \nby the program at any point. Arithmetic. overflow is a typical predefine exception; it can be raised \nirv~licitly within a statement as opposed to using a separate raise statement. All exception mechanisms \nprovide some implicitly raised predefi ned exceptions. The exception handling mechanisms present in various \nlanguages are summarized in the following table. Handler Handler Handler Language Association Invocation \nContinuation PL/1 dynamic static both dynamic Mesa static static both dynamic CLU static single-level \ntermination dynamic Ada static multilevel termination dynamic There are three major issues addressed \nin the remainder of this papec 1. Which mechanisms prevent the use of standard optimization techniques \neither because these mecha\u00adnisms make it impossible to use the standard techni\u00adques or because the techniques \nbecome too expensive to employ? 2. Are there restricted exception handling mechanisms that are useful \nand still permit efficient analysis for optimization? 3. With restricted mechanisms what algorithms \ncan be used and how effective and efficient are they?  3. Optimization difficulties Exception handling \nmechanisms cause problems for local and global program optimization algorithms for a variety of reasons, \nrooted in one or more of the above characteristics. Local optimization is affected by a small class of \nfeatures, which also affect global optimizations in the same manner; these difficulties are discussed \nunder the local optimization features. A separate set of problems arising for global optimization are \ndiscussed. The general problems caused by exception handling mechanisms are Handler execution, dynamic \nhandler location, and handler termination affect the execution of any proce\u00addure that raises an exception. \n A procedure that does not raise an exception itself may be affected if it calls a procecture that raises \nan exception. In genera:, if any procedure can raise an exception, all the procedures that may be on \nthe call  chain at that time can be affected. e Redefined exceptions may be implicitly raised at almost \nany point in the program, causing an alteration in the control flow. If the termination model is used, \nthe example in Figure 3\u00ad1 illustrates both cases @ and . . Case . occurs when PI is aborted by the raise \nstatement. Case * occurs for procedure P2, which is active when the exception is raised in procedure \nPI. .Since the handler for the exception Error is in P3, the remainder of procedures PI and P2 are aborted \nwhen the exception is raised. The presence of the raise statement in PI can be used to correctly analyze \nPI. However, from examining only P2 there is indication that the procedure may be aborted during its \ncall to P1. program Examplel; var V,X: ~ exception Erro~ {declares an exception} procedure Pl; begin \n...... if c then raise ErroC ..... {raises (or signals) the exception Error} end; procedure P&#38; begin \nv:= 3 PI; v:= 4 {aborted when the raise occurs in PI} end; procecfure P3; handler ErroC {Invoked if exception \nError is propagated into P3} begin {handler for Error} X:=5; end; begin {P3} P2; end; begi n{Main} x:= \n1; P3; {Execution continues here after the handler in P3 is executed} X:=x+l; end. Figure 3-1: Example \nof the effects of exception handling Certain exception handling features pose general sorts of problems, \nthat are difficult to overcome and independent of the optimization method used. Most of these features \nare currently considered to lead to poor programming practice or obscure program constructions. The major \nexample of such a practice is dynamic handler association. Dynamic handler association makes it essentially \nimpossible to determine which handler will be invoked when an exception is raised at some point in the \nprogram. An optimizer must consider all handlers as potential catchers. If many of these handlers update \nglobal variables or do not use resumption, the situation becomes extremely complex. No attempt S made \nto construct algorithms to work with dynamic handler association, instead such facilities are probably \nbetter omitted from modern programming languages. 3.1. Local optimization Since a raise statement introduces \na basic block boundary and local optimization is defined to exclude any optimization across a basic block \nboundary, user defined exceptions do not affect traditional local optimization. However, predefine exceptions \ncause significant problems, because it is not possible to determine exactly where they will arise at \ncompile-time. Suppose the variable A is stored into two locations in the same basic block, then the first \nstore may be eliminated. If some operation between the two stores implicitly raises a predefine exception \nat runtime, then the value of A is incorrect because the store had not been done. 3.2. Global optimization \nFor the purposes of this paper, global optimization is defined to be any optimization or code generation \npractice (e.g. delaying stores) that is done across a basic block boundary, including an explicit or \nimplicit raise statement. Most global optimization will be affected by predefine exceptions in a manner \nsimilar to the effects caused for local optimization. Global optimization are also affected by user defined \nexceptions. Global optimization is faced with two classes of problems: indirect and direct effects. Indirecf \neffects result from the process of locating and completing handlers (but not their execution effects \non data objects); this can involve the termination of one or more procedures. Direct effects are those \nthat arise from the execution of some handler in response to an exception. Direct effects can be analyzed \nby examining the result of executing any of the possible handlers at the point where the exception is \nraised. Figure 3-1 demonstrates both the direct and indirect effects. An indirect effect occurs when \nP2 is aborted. If we assume that the statement v : = 3 is eliminated in procedure P2, then the indirect \neffect invalidates that optimization. When the exception occurs, P2 is aborted and a store to the variable \nv is never done resuiting in an error, if the variable is live. A direct effect occurs when the handler \ninside of P3 is executed as a result of the raise in PI. If the optimizer uses interprocedural information \ngathered from PI, P2, and P3 to replace the statement x:=x+1 by x:=2, the result of this optimization \nwill be incorrect when the exception Error is raised. 4. Exception handling facilities that will allow \nOptimization The complexity of optimizing programs with exception handling facilities arises primarily \nfrom the existence of predefine, implicitly raised exceptions and the need to compute indirect and direct \neffects for any call or raise statement. The existence of implicitly raised, predefine exceptions causes \nthe most serious problems. Because these can occur almost anywhere in a program, it is impossible to \noptimize such a program. A significant simplification of the analysis is possible by limiting the contexts \nin which predefine exceptions can occur. To limit the contexts each procedure (or possibly even each \nstatement) should explicitly indicate which predefine ex\u00adceptions it can raise. Raising any unspecified \npredefine exception must bean error, and when such an error occurs the state of the variables need not \nexactly reflect the state in the original source program at the point the exception occurred. This rule \nallows complete unhindered optimization in a procedure that does not specify that it raises predefine \nexception% if such an exception does occur, it is treated as an error and the optimizer is relieved C! \nthe responsibility for the potential inconsistencies that may result. The optimization of procedures \nand statements that permit predefine excep\u00adtions may still be expensive. The use of dynamic handler location \nmakes the compu\u00adtation of indirect and direct effects nontrivial. If handlers were statically located, \nit would be possible to determine which handler would be invoked for every raise statement at compile-time. \nThen, the indirect effects would be easy to determine and the direct effects could be treated as if the \nraise statement were a procedure call to the handler. Unfortunately, dynamic handler location is a prime \nfeature of all exception handling mechanisms and must be accommo. dated. Indirect effects can cause the \ntermination of an active procedure at any procedure/function call, or at any raise statement. Whether \nthe termination or resumption model is used has a variety of effects on optimization algorithms. If the \nexception hsndling mechanism requires resumption, then no indirect effects exist. Unfortunately, no mechanism \nrequires resumption; termination is always an alternative. Although resumption may be better from an \noptimization viewpoint, it conflicts with a major goal of exception handling clean, exceptional termination. \nAlternatively, if the mechanism requires termination, then the indirect effects must be analyzed, Mechanisms \nthat require termination, rather than allow resumption, are simplier to analyze and more precise. Supporting \nconditional resumption requires more complex data flow analysis equations and tends to yield slightly \ndegraded information. This is certainly a minor price to pay, if the resumption model has merit from \nthe programming viewpoint. This topic is highly debated [4,6,5] Throughout the remainder of this paper \nwe consider only the termination semantics to simplify the analysis. Handling resumption semantics follows \nthe same pattern, and allowing both models requires only minor extensions. The optimi\u00adzation results \nare considerably less accurate if a single handler or multiple candidate handlers for a single exception \ncan either resume or terminate the call chain. Goodenough [4] suggests that exception handling mech\u00adanisms \nbe constrained in two important ways. Fkst, only single-level handler location should be supported. Second, \neach procedure should explicitly declare what exceptions it can raise. The CLU exception handling mechanism \nhas these properties [5]. There are several major simplifications for optimization when this restricted \nmechanism is used. The exceptions that can abort a procedure are explicitly declared; this essentially \neliminates the requirement to compute indirect effects.  The computation of direct effects is easier \nand more accurate. The potential set of handlers for a given raise statement is confined to handlers \nwithin the procedure and/or in its callers.  Although these restrictions are ~romising from the optimi\u00adzation \nviewpoint and have also been proposed as superior for methodological reasons, we will not employ them \nin this paper, since most facilities are less restrictive.  5. Optimization in the presence of excep\u00adtions \nTo effectively optimize programs with the restricted exception handling facilities that were discussed \nin the previous section, it is necessary to examine exactly what sort of situations can occur in a program. \nThe information needed to effectively perform optimizations involves knowing when a call can be aborted \nor raise an exception, the resulting data flow information, and the effects of executing exception handlers \nat various points. The general method employed to allow optimization of programs containing exception \nhandling is three park 1. The program flow graph and basic blocks are trans\u00adformed to indicate the effects \nof explicit and impticit raise statements. Local optimization may occur after this step. 2. The indirect \neffects for each procedure are computed and the flow graph is transformed to account for these. Intraprocedural \nglobal optimization, where calls and raise statements invalidate all data flow information about globals, \ncan be used after this step. 3. The direct effects are computed and the information at calls and raises \nis augmented to reflect this infor\u00admation. Complete, interprocedural program optimi\u00adzation techniques \ncan then be used.  5.1. Local optimization This model of local optimization does not involve optimization \nacross calls or explicit raise statements, but since many statements in a procedure can raise predefine \nexceptions, certain optimizations will be incorrect. The following scheme will insure that only correct \noptimization are preformed. For a given procedure determine the set of allowable predefine exceptions \nin that procedure.  For any operator in the intermediate form represen\u00adtation that could raise an exception \n(either user defined or predefine), isolate the operator in its own  basic block (and hence its own \nnode in the flow graph). For example, consider the following program fragment and the optimized quad-type \nintermediate representation. Assume that the predefine exception arithmetic overflow is permitted. Program \nFragment Optimized Form A := B+C, D:= E+F D:= E+~ A:=C A:=Q Since the operation B + C can raise an overflow \nexcep tion, which should be handled, the optimization is incorrect because the statement should not be \neliminated. Furthermore, if E + F causes an exception, the code is incorrect because the original store \nto A has been elimin\u00adated. If arithmetic overflow is allowed, then the two program statements containing \nthe addition operators should be isolated in separate basic blocks. This will insure that. the first \nstatement is not removed by standard local optimi\u00adzation. This transformation will ensure that no optimization \nmay count on the completion of sn operator containing a potential, implicit raise statement. Because \nof this all local optimization will be safe. This relies on the assumptions that an unspecified predefine \nexception is treated as an error and that at the error point certain variables may not accurately reflect \nthe original source program. Because of the nature of programming errors and optimization, most optimized \nprograms will have this drawback for all runtime errors, independent of the use of exception handling. \nA large number of the intermediate form operators can cause some type of predefine exception, e.g. most \narithmetic operators. If the set of allowed exceptions is not specified, the procedure will be broken \ninto extremely small basic blocks, which will yield very little optimization potential. For global optimization \nthe number of flow graph nodes will make data flow analysis substantially more costly. Most often the \nprogrammer does not want to employ predefine exceptions in a procedure and considers these situations \nas errors. Optimization should not suffer when the predefine exceptions are not employed. 5.2. Global \noptimization Global optimization requires the same information as is needed for local optimization plus \nthe direct and indirect effects. The information on which operators inay initiate exceptions is obtained \nin the same manner as for local optimization. Using the information on which intermediate form operators \nmay raise and propagate exceptions, the optimizer can avoid optimizafions that rely on the completed \nexecution of the operator. This is implemented by transforming the flow graph to guarantee that the analysis \nof the new flow graph will provide correct information if the flow graph component is aborted. Consider \na flow graph with an operator that may abort in node n (either a raise or a call that is aborted). After \nthe transformation for the local optimization, if node rr contains an operator that may raise an exception, \nit is the only operator in n. The same process is is done for any procedure call that may aborted, using \nthe indirect effect information calculated in the next section. Then a new flow graph that will have \nthe correct properties is obtained by the following transformation: o If the operator in node n is an \nexplicit raise statement then any out-arc to a successor node of n is eliminated. (Recall that we are \nonly considering the termination model.) The node n adds an out-arc that is a flow graph exit this arc \nrepresents the possibility of aborting the operator, if the raise occurs, Figure 5-1 shows the original \nand transformed flow graph for procedure P2 in Figure 3-1. Original Transformed Flow graph Flow graph \n99 A transformed flow graph represents the potential effect of aborting node n. The disadvantage of \nthis transformation is an increase in the size of the flow graph. This is probably not a major problem, \nunless a large number of operators can raise predefine exceptions. In this case, it may be more efficient \nnot !0 attempt analysis within the offending procedure; the interprocedural indirect and direct effects \nof this procedure can still be computed by worst case assumptions. Interprocedural optimization also \nrequires that direct effects be examined. The direct effects occur from executing handlers in response \nto exceptions that are raised; the interprocedural effects of these handlers must be analyzed. This is \ndone by augmenting the flow graph at the call statement with the results of the interprocedural analysis. \nIf resumption is supported, then the raise statement must also be augmented with summary information \nfor handlers that may catch the exception and resume. The information added to the node reflects both \nthe usual sort of interpro\u00adcedural information plus the information that is obtained by examining the \neffects of executing handlers in response to exceptions. 6. Finding the data flow effects of excep. tion \nhandling Two sets of equations are necessary: equations that specify the indirect effect information \nfor the flow graph transformation, and equations that specify the data flow effects of potential handler \ninvocations. These equations do not consider the issues of aliased names and name conflicts for scoped \nvariables. These problems can be handled using the techniques of [2, 3]. This will result in slightly \nmore complex equations. 6.1. Indirect effects Analyzing the indirect effects by transforming the flow \ngraph, requires that calls that can be aborted must be discovered. A call aborts if it does not return \nto the statement following its call in the calling procedure. .A set of data flow equations that define \nthe potential set of aborting calls is speci; ied. Inputs. . P a program consisting of procedures. e \nRas~] the set of exceptions that may be raised by procedure p, both user and predefine. e Han~] the \nset of exceptions with handlers in P. Call~] the set of procedures that may be called in p. outputs. \nAborf~] the set of exceptions that cause a call top to abort. Data flow equations. For each procedure \np ~ P, Abort~] . (Ras~] U Bf,o]) -Han~] where B~] = {b] b C Abort[q] and q G Call[p]} Claim. Exception \ne may cause a call top to abort * e E Abort@]. .hstification. Exception e causes a call to p to abort \n@ 204 either e is raised in p or e is propagated into p and not handled there. B~] is the set of exceptions \npropagated into p. Therefore, Ras~] U B~] is the set of possible exceptions raised in p. Set subtraction \nof Han[p] yields the set of exceptions thatcan occur inpandnot rehandled, i.e. those that cause an abort \nof a call to p. Example. Ras[Pl] = {Error}, Abort[Pl] = {Error] B[P2] = {Error}, Abort[P2] = {Error} \nB[P3] = {Error}, Abort[P3] = 0 Solving the data flow equations. The equations can be iteratively solved \nstatilng with the solution: Abort~] . RasIp] Han~]. 6.2. Direct effects After transforming the flow \ngraph to account for the indirect effects, the direct effects can be computed. If,0 is a procedure, we \nwould like to compute CHANGE~,q], the set of globals and parameters that might be changed in p when it \nis called from. q, including the effects of exception handling. It is important to distinguish the caller \nand well as the callee, since the direct effect of a particular call may depend on a handler that is \nexecuted in the caller. For example, if the program in Figure 3-1 contains another procedure P4 with \na different handler for Error, and P4 calls P2, then the direct effect of calling P2 would depend on \nthe handler in P4. Hence, Change(P2,P4) need not equal Change(P2,P3). Inputs. e All inputs from the indirect \neffect algorithm. Defb] the set of formal parameters and global variables with local definitions in \np. This information is obtained ~ restructuring of the individual proce\u00addure flow graphs using the information \nabout indirect effects. e Hdef[e,p] the set of global variables with definitions in the handler for \nexception e within procedure p (if such a handler exists). We assume that if a handler for e occurs in \np, that it is possible for e to be raised or propagated into p. *G set of global variables. E the set \nof all exceptions. 4 F[p] set of formal variable parameters of p. Outfxlts. e Change@,q] the set of \nvariables that may be changed in calling p from q. (Other information similar to Change, e.g. interprocedural \nRef could be obtained in the same manner.) Data ffow equations. The equation for Change~,q] is Change~,q] \n= Def@] U Klp] U A@] U I@] U Cb,q] where K~] = {k I k E (Change[q,p] fl G) and q 6 Call~] } A@] = {m \nI m E (F~] U G) and m is the i actual parameter passed in a call, within p, to procedure r, and the \njth formal parameter of r 6 Change[r,p]} ff~] ={vIvEHdef[e@], fof alle6E} Clp,q] = {v I v E Hdef[e,q] \nand e E Abort~]} Claim. The variable v may be changed as a result of a call top from procedure q = v \nC Change[p,q]. Justification. The variable v maybe changed by a call to p within q = it is changed ,by \nsome action in p or by some procedure called by p. K~] and Mb] compute the interprocedural effects within \np that arise fr. m global variables and reference parameters, respectively. The equation is straightforward, \nbut it does not account for aliasing or name scopes, which can be accommodated using [3] or [2]. This \nset accounts for all potential changes of a variable, except those occurring in a handler inside of p \nor cf. H~] defines the effect of executing any of the handlers in p. C~,q] is exactly the effect of executing \na handler for an exception raised in (or propagated into) p and caught in q. Thus, Change[p,q] exactly \ndefines the desired set. Example. Def[Pl] = KIPI] = MIPI] = HIP1] = 0, Change[Pl,P2] = @ Def[P2] = {v}, \nK[P2] = fvf[P2] = H[P2] = CIP1 ,P2] = fZf, Change[P2,P3] = {v} Since Abort[P2] = {Error}, C[P2,P3] = \n{x}, Change[P3,Main] = {x,v}. Some exception handling mechanisms require that the search for a handler \nalways begin in the. caller of the procedure raising the exception. This is easily accommo\u00addated in both \nthe equations for Abort and Change. Resumption requires extra effort to handle but does not significantly \ncomplicate the computation. Solving the data ffow equations. These equations can also be solved using \nan iterative technique. 6.3. Implementation The application of these techniques requires a number of \npasses in the most obvious implementation. 1. Build the initial flow graph/basic block representation \nusing the information about explicit and implicit raises. At the same time, gather the information needed \nas inputs to compute Abort. 2. Compute the indirect effects, i.e. Abort. 3. Transform the flow graphs \nto represent the indirect effects. 4. Compute the direct effect information. 5. Append that information \nas appropriate and start the actual optimizations.  The major objection to this strategy is that it \nappears to require two passes over the entire program to accomplish correct data flow analysis and flow \ngraph transformation, before optimization can begin. Because Abort computes the possibility of an abort \nand not the fact that a procedure call must abort, the information is not as accurate as possible. 205 \n 7. Conclusions Optimization in the presence of reasonable exception handling facilities has been shown \nto be feasible. The exception handling facilities must be designed with certain limitations on their \nuse, especially in the area of implicitly raised exceptions. Then after transforming the control flow \ngraph, and solving a set of interprocedural data flow analysis problems, conventional program optimization \ntechniques are applicable. The major problem, which is currently unsolved, is to find the minimum number \nof, zrsses necessary to accumulate this information. Other current areas of investigation include the \npractical implementation of these algorithms and empirical studies of the use of exception Bibliography \n1. Aho, A.V. and Unman J. D.. Principles of Compiler Design. Addison Wesley, Menlo Park, 1977. 2. Banning, \nJ.P. An Efficient Way to Find the Side Effects of Procedure Calls and the Aliases of Variables. Proc. \nof the Sixth ACM Symposium on Principles of Programming Languages, ACM SIGPLAN/SIGACT, 1979, pp. 724-736. \n 3. Barth, J.M. A Practical Interprocedural Data Flow Analysis Algorithm. Comm. ACM 21,9 (Sept 1978), \n724\u00ad  736. 4. Goodenough, J.B. Exception Iandling: Issues and a proposed notation. Comm. ACM 18, 12 \n(Dee 1975), 683\u00ad 696. 5. Liskhov, B.H. and Synder A. Exception Handling in CLU. IEEE Trans. on Software \nEngineering SE-5,6 (Nov 1979), 547-557. 6. Mitchell, J. G., May bury, W. and Sweet, R. Mesa Lan\u00adguage \nManual Version 5. Xerox Palo Alto Research Center, Palo Alto, Ca., 1979. 7. OS PL/1 Checkout and Optimizing \nCompilers: Language Reference ManuaL IBM Corporation, 1979. 8. Spillman, T.C. Exposing Side-Effects \nin a PL/1 Optimizing Compiler. Proceedings of the IFIP Congress 1971, Amsterdam, 1971, pp. 376-381. \n  \n\t\t\t", "proc_id": "567532", "abstract": "Optimization of programs that may contain exception handling facilities requires new techniques. Program optimizations that do not account for exception handling facilities may incorrectly transform a procedure that can raise an exception, producing different results from the unoptimized version. Restricted exception handling mechanisms that allow optimization are discussed. Data flow equations for determining the effects of exception handling are presented.", "authors": [{"name": "John Hennessy", "author_profile_id": "81100207767", "affiliation": "Stanford University", "person_id": "P144245", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/567532.567554", "year": "1981", "article_id": "567554", "conference": "POPL", "title": "Program optimization and exception handling", "url": "http://dl.acm.org/citation.cfm?id=567554"}