{"article_publication_date": "01-26-1981", "fulltext": "\n Permission to make digital or hard copies of part or all of this work or personal or classroom use is \ngranted without fee provided that copies are not made or distributed for profit or commercial advantage \nand that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, \nto post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. &#38;#169; \n1981 ACM 0-89791-029-X $5.00 Our use of the words exhaustive and demand follows [BJ78a, BJ78b]. Exhaustive \nalgorithms for two problems (dead variables and available expressions) are stated in [BJ78a]. The dead \nvariables problem can be reduced to reaching (essentially what is more often known as clef-use chaining \n) in the usual way. An exhaustive algorithm for reaching is stated in [BJ78b] and then modified to support \nanalysis of dead variables on demand. Admittedly inefficient, the basic procedure k improved in Section \n2.2 of [BJ78b]. The improvements are tied to details of the special problem being solved. Even with the \nimprovements, it takes just as much work to satisfy a demand that is repeated (after a small change in \na large program) as it did to satisfy the original demand. 2. UPDATING This section considers only the \nupdating aspects of incremental algorithms. Suppose an algorithm ALG has input @and output Q, where @isalarge \ncomplex object, such as a global flow problem or an undirected graph with weighted edges (traveling salesperson \nproblem) or a program annotated with invariants to be verified. Although @ cannot be conveniently described \nby listing a few numbers, it is still natural to speak of small changes in .9 and their effects on the \noutput Q. Applying ALG to a value for $ yields the appropriate value for S2 at a cost C(Y). Ordinarily, \nALG can be considered fast if C(Y) is a linear or almost linear function of any natural measure of size \nfor .9. In some applications, however, ,9 assumes a sequence 9 o, .... ~K of closely related values. \nTo obtain the correspondkrg sequence .4?., .... QK of outputs by applying ALG repeatedly will cost C($ \no)+ + c(9fJ. (2.1) If K is roughly similar to the maximum size input encountered, then the actual cost \nof using a linear algorithm will be quadratic. For example, a language-oriented editor may attempt to \nflag syntax errors as soon as they are made. Even the fastest parsing algorithm will be slow in use if \nthe entire program is reparsed whenever a small change is made by an interactive user. This observation \nhas led to work like [GM79, GM80, Weg80] on parsers that can update parse trees when their input strings \nchange. There has been remarkably little research on updating for the tasks other than parsing that a \nsophisticated program development tool might be asked to perform. (For example, the author knows of only \none incremental verification system [M079].) Perhaps ALG can be modified so as to permit updating Q \nin light of a change in &#38;P without a complete reanalysis. The modified algorithm ALG has input .9 \nat an initial call on ALG , but a subsequent call has input Ai? that specifies only how .9 has changed \nsince the most recent call. In addition to the public output s2, ALG has a private output @ for its own \nfuture use. The information in @ includes 9 and whatever intermediate results are wanted. A subsequent \ncall on ALG with input A~ will simulate the action of ALG with new input t? + A9, where the previous \n@ value is recovered from t%?. (The + sign has no technical meaning and merely emphasizes the description \nof one value for ~ as the result of changing another value. ) If @ includes enough additional information \nand ALG was originally designed with some concern for updating, it may be possible for ALG to simulate \nALG without redoing all the work. For a sequence .9., .... YK described by giving the initial input .9 \n0 and then a succession of changes Al@, .... AK.9 (with # k = Yk_l + Ak9 for 1 s k s K), the total cost \nof the initial call with input @o and the K subsequent calls with inputs Ak,r? has the form C init(@o) \n+ C ,ub&#38;20, AI.9 ) + ... + C ,.@?K-1, AK@> (2.2) w~ere @j is the value of @ after #j has been processed. \nIf the cost C ,ub~(~, A&#38;) depends more on the size of A.9 than on that of .9?, then (2.2) is fikely \nto be an improvement over (2. 1) when a long sequence of small changes is contemplated. Of course we \ncannot make any sweeping claims before particular algorithms have been chosen and the three cost functions \nC, C init, C ,*, have been analyzed. For small K it may be better to apply ALG repeatedly than to construct \nelaborate records of previous computations. There is a simple general method for deriving ALG from ALG \nin many cases. A number of assumptions on ALG are required. First, the input .9 cannot be modified by \nALG. Second, Y and changes in # are given explicitly in terms of separate components. There is a set \nNin of component names (e.g. pairs of integers if .9 is a 2-dimensional array) such that @ is determined \nby its components 9% as i ranges over Nin. A change in 91 has no effect on ~j for i # j. Though Nin may \nnot be the same for all inputs, we do assume that Nin is constant over any sequence .9., ~1, .... ~K \nof inputs related by small changes. For us, small changes are changes in a few components that leave \nthe structure alone. (In the example of a 2-dimensional array, the number of rows and columns is constant. \n) Third, Q and any large intermediate variables (whose recomputations we wish to avoid) are also determined \ncomponentwise and constant in structure, say with component set NOUt for s2. To minimize notation we \nwill assume that any large intermediate variables have been added to Q and need 118 not be considered \nseparately. (This addition has independent motivation in our main example. ) Fourth, ALG manipulates \ncomponents of Q in a fixed order. However complex NOut and the logic for visiting components may be, \nthey depend on structure alone. The fourth assumption needs some elaboration. Some components of Q may \nbe changed more than once. Uses and changes may appear in any order. We allow for complexity but need \nto assume predictability. As soon as the structure N,n of 9 has been obtained, we can obtain more than \njust the structure NOut of 42. From N,n we can determine a worklist W, whose length will be denoted I \nW 1, such that the component Ws for each integer s with 1 s s s I W I is a component name in NOut. There \nare I W I major steps in ALG s work with input Y, and the s-th step changes .@ for n = Ws. Moreover, \nthe components of Y and Q used by the s-th step can also be determined. For 1 < s s I W I there are sets \n(Nin I S) G Nin and (Nout Is) s Nou, such that .% k changed to the result of a certain computation (which \nmay depend on s) with components @i for i in (Nin I s) and .Qj for j in (NOU, I s). No other components \nmay affect the value assigned to $%. For n = Ws, the s-th step may thus be described as STEP : Qn ~ COSTLY(S, \n9[Nin I s], QIFJOut I 5]), where Y[Nin Is] is the collection of all 91 for i in (N,n I s). Likewise for \nf2[NOUt I s]. For the moment, COSTLY is a black box whose operation is presumed to account for most of \nthe costs incurred by ALG. (Some readers may now wish to preview Section 4 before continuing with the \ngeneral discussion in this section and the next one.) Whether or not ALG explicitly computes W and the \n(Nin I s), (Nout I S) sets, the effect of ALG has been assumed to be equivalent to that of Algorithm \n2.A. For conversion to incremental use there are several strategies that complement (not compete with) \neach other, but only one is available before more is assumed about W or COSTLY. Whenever ALG performs \nits version STEp ,ubs of STEp, it does not blindly do COSTLY. Instead, it consults a record of what happened \nat this same step s in the previous call on ALG . It compares relevant component values and, if possible, \navoids doing COSTLY by recalling a previously obtained COSTLY value. To expedite such comparisons ALG \nuses a bit NEWIN(i) for each i in Nin and a blt NEWOUT(j) for each j in Nouv To save previous COSTLY \nvalues, ALG uses @ for s = 1, .... I W I . Each % s variable holds a possible Qn value for n = Ws. The \nstructure 6 and a copy #old of the input 9 are in the private output s72,as are W and the various (Nin \nI s), (Nout Is) sets. A subsequent call on ALG performs as shown in Algorithm 2.B. 4LG : [ get Nin; / \nDetermine the structure of 9. / (Compute the worklist W and the relevant component sets (Nin I 5), (Nout \n15) forallswithl <s< IWI.); get (YI for all i in Nin); / Get the rest of Y. / Q -CHEAP(Y); / This is \nany initialization so cheap that / / we do not mind repeating the work. / / It need not initialize all \ncomponents of Q. / forsfromlto IWI do [n+Ws; STEP : Qn +-COSTLY(S, 9 [N,n I s], L?[NOU, I s]) 1; put \n@ 1 Algorithm 2.A. This kind of algorithm will be converted for incremental use. ALG ,Ub, : [ recall%; \n/ From%? we get the structural information. / / We also get the old input #old and / / the old COSTLY \nvalue%% for each s. */ get A*, NOTE_INPUT : (Set 9 and the NEWIN bits.); /* Details in Alg. 2.C. / INIT_OUTPUT \n: (Initialize Q and the NEWOUT bits.); / Details in Alg. 2.C. / forsfromlto IWI do [n-Ws; STEP sub, : \n(Adjust Q%, NEWOUT(S), @.) / Details in Alg. 2.D. / 1; .90,d-=-$ ;save 9?; put Q 1 Algorithm 2.B. Paradigm \nfor updating. In a very high-level style that postpones decisions about how to represent input changes \nand that treats CHEAP as a black box, the details for NOTE_INPUT (resp. INIT_OUTPUT) use the separate \nstructures .9 and ~Ol,j (resp. .4?and 4?temP, where ~temp k a temPorarY cOPY of @). In the main example \nit will be easy to optimize away much of this duplication. NOTE_INPUT : [ &#38; -@old+ A9; for alliin \nNin do if .% = .901di then NEWIN(i) -false else NEWIN(i) -true  1 //// INIT_OUTPUT : [ @ + CHEAP(Y); \ni2temP +-CHEAP(9 0,,); for all j in Nout do if @j = QtemPj / An uninitialized component is / /* treated \nas having a special value that / / no initialized component could have. / then NEWOUT(j) +-false else \nNEWOUT(j) +-true 1 Algorithm 2.C. Details for NOTE_INPUT and INIT_OUTPUT.  s f@,ub$ : [ if [Some i in \n(Nin Is) has NEWIN(i) or some j in (Nout Is) has NEWOUT(j)l then NEED_COSTLY : [ Qn +-COSTLY(S, 3[Nin \nI s], .QINout I s]); ifQn =% s then NEWOUT(n) -false else [ NEWOUT(n) -true; %?s-% ] 1 else [ NEWOUT(n) \ne false; .Qn + % ] 1 Algorithm 2.D. Whenever possible, STEP is simulated without doing COSTLY. An initial \ncall on ALG is very much like a call on ALG, but it assigns to ~s as well as Qn at STEP init. With save \n.?? added to put 4?, an initial call on ALG saves the information needed by subsequent calls to simulate \nALG. Logically, ALG is correct under the four assumptions we have made, Whether it improves upon repeated \nuse of ALG is a more complicated question. It is presumed that most of the cost C(3) of processing ~ \nwith ALG is incurred in doing COSTLY(S, ...) for s = 1, .... I W 1. Suppose first that assignments fike \ni?hr + @ are much less costly than doing COSTLY(S, ...). If the number of s values that need to do COSTLY \n(S, ...) in STEp ,ub, is much less than I WI, we can expect a saving. (The cost of storing % between \ncalls on ALG is being ignored. At least when I W I is roughly the same as I NOut 1, this seems reasonable. \n) With small changes in the input there are few true bits NEWIN(O and therefore few initial true bits \nNEWOUT(j), at least for likely choices of CHEAP. When the structure of W and the relevant component sets \n(Nin Is), (Nout I S) k favorable, true bits in NEWOUT do not propagate far in NOut as s advances. In \nparticular, we hope ALG does not have many long sequences (sl, S2, S3, ...) with Sk in (Nou, I Sk+l). \nVarious choices of ALG for the same problem can differ greatly hr this respect. Even when doing COSTLY \nonce is not very costly, we have an opportunity for savings when the worklist structure is especially \nsimple. Details must wait until after we have stndied demand-driven analysis in the next section. Another \nstrategy for improving ALG looks at COSTLY instead of treating it as a black box. Suppose the arguments \n~[Nin I s] and .@[NOut[s] to COSTLY(S, ...) have changed, so that we cannot just reuse the old value \n%. Perhaps the old value can updated. Perhaps we can apply a function NOT_SO_COSTLY(s, ... ) to $3s that \nwill yield the current CO STLY(S, ...) value for the changed arguments without actually redoing COSTLY. \nIn particular, if COSTLY(S, ...) is given by an expression built from operators like set union, there \nis a wealth of possibilities for choosing NOT_SO_COSTLY. See [Pa79]. A more speculative possibility: \napply our main strategy recursively, treating COSTLY(S, ...) as the whole ALG. 3. DEMAND ANALYSIS A worklist \nindex r is a prerequisite for a worklist index s iff r < s and the component name n = Wr satisfies n \nE (Nout IS)and no r with r < r < shas Wr = n. Intuitively, the s-th step can be done as soon as all its \nprerequisites have been done. The demand closure of a set S of worklist indices is the smallest set containing \nS, all prerequisites of members of S, all prerequisites of prerequisites, and so on. Given component \nnames D s Nout for which output is demanded, let each n in D have u(n) = max { s I Ws = n }. (We assume \nthe set is nonempty.) Let u*(D) be the demand closure of o(D) = { o(n) I n E D ]. Note that the ability \nto find u (D) for any D !Z NOut is implied by knowledge of W and the relevant component sets. In our \nmain example the U*(D) computations will be straightforward. For demand-driven use, the original exhaustive \nALG could simply be replaced by Algorithm 3.A. DEMALG : [ get Nin; / Determine the structure of 9. */ \n(Compute the worklist W and the relevant component sets (Nin I s), (Nout I s) forallswithl <s< IWI.); \nget (@i for all i in Nin); / Get the rest of ,9. / get D; S -o*(D); 4? -CHEAP(9); forsfromlto IWI do \nif s S then [n+-Ws; STEP : Qn +-COSTLY(S, ~[Nin ! s], .QINOUt I s]) 1; put (Qj for all j in D) 1 Algorithm \n3.A. Paradigm fordemand-driven processing. For ALG the demand-driven form is more complicated because \nthe demand set may change between calls. A change in D may add new worklist indices to o*(D). If s is \na new member of u*(D) then STEP ~ub, should perform COSTLY(S, ...) even if all the relevant NEWIN(i) \nand NEWOUT(j) bits happen to be false. The additional housekeeping is readily performed with a bit NEWSTEP(s) \nfor each s in the range from 1 to IW1. These bits are added to t%!. The only difference between ALG init \nand DEMALG init is that NEWSTEP(s) is set to false for each sin S and is set to true otherwise. Now ALG \n~ub~ can be rewritten as DEMALG ~ub~ in Algorithm 3.B. For demand analysis there are no changes in NOTE_INPUT \nor INIT_OUTPUT, but the simulation of STEP tests the bit NEWSTEP(s) as well as the relevant NEWIN(i) \nand NEWOUT(j) bits. The details are in Algorithm 3 .C. In some of the ALG choices to be considered in \nSection 4, there is an underlying tree structure 2. The exact relation between 2 and the worklist structure \nis somewhat lengthy to describe, but it is a good approximation to say that W specifies a bottom-up traversal \nof X followed by a top-down traversal. Each output component n = Ws for s in the bottom-up traversal \nhas (Nin I s) and (Nout \\ s) among the off spring of s considered in 2, and each such n is Ws for DEMALG \n,Ub, : [ recall .%?; / In addition to what ~ contains in Alg. 2.B, / / it contains a bit NEWSTEP(s) for \neach s. / get A.9 ; get D; S -o*(D); NOTE_INPUT : (Set q and the NEWIN bits.); / Details in Alg. 2.C, \nas before. */ INIT OUTPUT : (Initialize 42 and the NEWOUT bits.); / Details in Alg. 2.C, as before. / \nforsfromlto [WI do ifs6S then [ n+Ws; DEMSTEP ,ub, : (Adjust Qn, NEWOUT(s), W.); / Details in Alg. 3.C. \n*/ NEWSTEP(s) -false 1 else NEWSTEP(s) -true; $ Old ~ 9 ; save .97; put (Qj for all j in D) 1 Algorithm \n3. B. Paradigm for converting ALG to fully Incremental form, with updating and processing on demand. \n DEMSTEP ,ub~ : [ if [NEWSTEP(S) or some i in (Nin Is) has NEWIN(i) or some j in (Nout Is) has NEWOUT(j)] \nthen NEED_ COSTLY : [ % ~ COSTLY(S, ~[Nin I s], .QINOut I s]); if 9?n= % s then NEWOUT(n) -false else \n[ NEWOUT(n) -true; i% A @ ] 1 else [ NEWOUT(n) -false; % -%?s] 1 Algorithm 3.C. Details for simulating \nSTEP in DEMALG ~Ub~. exactly one s in the entire range from 1 to I W I . Now suppose X. is a large subtree \nof 2 such that no input component associated with X ~ has changed since the previous call on ALG . The \nportion of the bottom-up traversal dealing with 20 can be skipped over, in favor of simply assigning \n% sO to 4?no and false to NEW OUT(no), where no = Wso and so corresponds to visiting the root of 2 ~. \nThis may leave garbage output associated with proper descendants of the root of 2., but we know exactly \nwhere the garbage may be and we know that h cannot affect the rest of the bottom-up traversal. In the \ntree-structured ALG choices considered in Section 4, the top-down traversal is such that garbage within \nZ. cannot affect anything outside. All the errors due to skipping over Z. are confined within 2.. These \nerrors are invisible if no output component associated with X. is demanded. In short, even when assignments \nlike Qn ~ %s are comparable in expense to doing COSTLY, we can still save by performing an incremental \nanalysis that skips over any large subtree that is free of input changes and output demands. Subtrees \nlike this will be frequently encountered in subsequent calls on ALG when a large program with a fairly \nclear structure is being modified. The cost comparisons in Section 4 assume that either .% - %s has been \nimplemented in a way with neglible cost (compared with doing COSTLY) or maximal subtrees free of input \nchanges and output demands are recognized. 4. SUMMARY OF AN APPLICATION In global flow analysis the \nstructural information in @ consists mainly of a (finite directed) graph G and a designated set of entry \nnodes in G. Each node represents a portion of the text of a program, with entry nodes for program entry \npoints, The arcs together with the entry nodes form the set Nin of input component names, Local information \nis associated with each arc and entry information is associated with each entry node. The set Nout of \noutput component names consists mainly of the set of all nodes in G. We seek global information at each \nnode that is valid whenever control reaches the node. Analysis methods differ in what they assume about \nG and about the operations available for manipulating global flow information. For the moment, we emphasize \nwhat is common to all methods. There are important applications with sequences of inputs ~ that have \nconstant control flow, especially in interprocedural analysis. We analyze a procedure under varying assumptions \nabout its callers (the entry information) and about the procedures it calls (the local information associated \nwith flow of control across a call), keeping the procedure itself fixed throughout. Our paradigm is rigorously \napplicable in situations like this. More generally, a program change may extend to the control fluw, \nbut control flow changes are often so small and simple that even a prudent and reliability-conscious \ncompiler writer may be willing to tinker with DEMALG in an effort to accommodate them, despite the current \nlack of a clear general paradigm for letting Nin vary in Section 2. As ALG varies among the known flow \nanalysis algorithms, the performance of DEMALG varies widely. Unable to characterize the variation in \na precise theorem, we will consider a typical example and prolpose a rule of thumb. (For ease of reference, \nthe rule is stated in Section 5.) Real examples like those in [Ca77] would take too much space, so our \nexample is artificial. We mitigate the artificiality with statement labels whose intuitive connotations \nare helpful. For r = 1, 2, 3, . let HUNGRY, be a bit variable and let EATr be a program statement. The \ncompound statement DINE, : [ while HUNGRYI do EAT,; . while HUNGRY, do EAT, 1 (4.1) works its way through \nan r-course dinner, eating as much as necessary of each course to satisfy hunger for that kind of food. \nThe specific form of DINE, in (4.1) is less important than the fact that it contributes at least O(r) \nnodes and arcs to the control flow, no matter which of the various text-to-graph translations is used. \nLet COOK and WASH be other statements, possibly very complex, and consider the compound statement [ COOK; \nDINE,; WASH ] (4.2) that cooks the dinner, dines, and then washes the dishes and utensils. Some of the \ninformation gleamed from analysis of COOK is of interest to WASH but not to DINE,. In particular, consider \nthe question of whei.her the broiler pan is dirty. This information is generated by COOK and used by \nWASH, while DINE, neither uses nor affects the state of the broiler pan. To verify that DINE, is transparent \nto the broiler pan will cost at least O(r), bwt once this is known it will remain true as long as DINE, \ndoes not change, no matter what happens to COOK. Now suppose COOK changes in a way that changes the information \nit generates. Intuitively, we can send the news from COOK tc~ WASH at a cost independent of r, with DINEr \ntreated m a unit. The trick is to get a global flow analysis algorithm (which is unaware of the broad \npurposes connoted by our statement labels) to simulate the behavior of sensible people. Can we avoid \nthe cost of telling everything in DINEr about the broiler pan whenever COOK changes, yet keep WASH fully \ninformed? 122 Yes. High-level analysis [R080] needs to determine the effect of DINEr on the broiler pan \nanyway. With this choice of ALG, it is natural to add what DINEr does (to information about the broiler \npan) to the output of exhaustive analyais. In effect, we extend local information from single arcs to \ncertain classes of paths, in thk case the paths through DINEr Useful for diagnostics [R077] and needed \nanyway to determine what WASH knows when ALG is from [R080], extended local information deserves to be \nadded to the output. With this understanding about Nout, we derive DEMALG from the general paradigm and \nfind that it acts just as sensible people do. News about the broiler pan is sent from COOK to WASH (having \nbeen demanded there) without any r-dependent cost. Some other flow analysis algorithms, even when they \nhave better worst-case time bounds from the exhaustive viewpoint, are inefficient at updating. The cost \nof computing what DINEr does is avoided in the initial call, but the cost of propagating irrelevant news \nthroughout DINE1 is incurred in every subsequent call. Consider the algorithms in [AC76, GW76]. Like \nhigh-level analysis, these are elimination algorithms: they snmmarize the effects of certain claases \nof paths in extended local information. The classes chosen, however, are unfike those of [R080]. Even \nwhen the extended local information is added to the output of ALG from [AC76, G w76], it still takes \nat least O(r) time for DEMALG derived from [AC76, GW76] to send news from COOK to WASH. All elimination \nalgorithms have tree-structured worklists, but the trees in low-level algorithms like [AC76, GW76] forget \nall aspects of program structure other than loops and nesting of loops. In (4.2) there is no nesting \nof loops and therefore relatively little to be gained from the low-level tree. Unlike elimination algorithms, \niterative algorithms summarize no classes of paths and have no additional outputs. These algorithms begin \nwith an admittedly implausible initial guess about the output, then correct the guess repeatedly until \nit stabilizes. Iterative algorithms differ in how they organize the scan for nodes where corrections \nmay be needed. The attribute grammar [BJ78a], rPostorder [HU75], and node listing [Ke75] approaches fit \nour paradigm, with W a sequence of nodes (repetitions allowed). With these choices of ALG the behavior \nof DEMALG on (4.2) is much the same as with [AC76, GW76], but now larger coefficients are hidden in O(r) \nbecause the loops in DINE, are not recognized as units. In examples with nested loops, [AC76, GW76] pull \nahead of iterative algorithms but seem to remain behind [R080] as long as we avoid the examples where \ndifferences between high-level and low-level graphs prevent brief comparisons from being fair to both \nways of modelling control flow. 5. CONCLUSIONS Incremental data flow analysis is important for program \ndevelopment tools that alert the programmer to data flow anomalies (which often are symptoms of errors) \nand for compilers like the one envisioned in [Ca77]. This compiler detects and exploits new opportunities \ncreated by its own actions. Each item in the compiler s tool kit is very simple and cannot do much, but \nthe whole is more than the sum of its parts when one tool makes opportunities for another and data flow \ninformation is updated to detect the opportunities. Such a synergistic compiler can and should keep the \nfrequency of updating low, thanks largely to wise choices of the data flow questions to be posed. The \nonly known way to forego updating altogether, however, is to forego some optimization as well. For example, \nconstant propagation to a test of variable A may cause fewer definitions of another variable B to reach \na certain use of B. The definitions and the use are still accessible, but some of the definitions no \nlonger have a clear path to the use after the test has been optimized away. The clef-use chains require \nnontrivial updating in cases like this. Program development tools like the Cornell Program Synthesizer \n[TR80] and various language-oriented editors [Weg80] have a great deal in common with synergistic compilers \nwhen one considers the technical problems that arise in extending them to provide more support for developing \nlarger programs. Early in program development it is important to alert the programmer to data flow anomalies, \nsuch as a while loop governed by a condition that is invariant across the loop body because none of the \nvariables in the condition can be changed. Anomalies like this arise frequently when a very high-level \nintention is being squeezed into the control and data structures available in any one real language; \nit is important to warn the programmer while the intention is still fresh in hk or her mind. Only an \nincremental analysis is likely to be fast enough for use during program input. Later in program development \nit is important to have a smooth, computer-assisted path from a very modular, easy-to-understand-and-modify \nprogram to one that runs fast enough to do serious work. The original version can be retained to facilitate \nlater modifications, after which the path will need to be traversed again. Conventional optimizing compilers \ncertainly help, but they have already reached a level of complexity at which it is a serious burden for \nthe programmer to specify all the options before the compiler begins and to peruse a mass of cryptic \nmessages available only after the compiler finishes. Proponents of advanced optimization techniques like \nrecursion removal [AS78] recommend that the techniques be applied by the programmer (perhaps with clerical \nassistance from the computer) rather than by a conventional noninteractive compiler with no access to \nthe programmer s insight. Of course the programmer should be able to struggle with efficiency in the \nsame friendly environment [as in TR80, Weg80] wherein a struggle with correctness has just ended, especially \nsince the transition from correctness worries to efficiency worries is not sharply defined in practice. \nThe syntax-directed control flow representation in high-level analysis [R080] meshes nicely with the \nrepresentation chosen in [TR80], and Section 4 has shown that high-level analysis is especially amenable \nto incremental use. The technical problems in building a program development tool (for people concerned \nwith run-time efficiency) have much in common with the technical problems in building a synergistic compiler, \nThk paper has addressed some of these problems. A rule of thumb was promised early in Section 4. For \nthe sake of clarity and reliability, it is good to arrive at incremental algorithms indirectly. Begin \nwith an exhaustive algorithm ALG and argue for (perhaps even prove) its correctness. Convert ALG for \nincremental use by some systematic changes (not necessarily the ones we have studied) that preserve correctness, \nand tidy up the result. Devising a way to do XYZ is hard enough without the added burden of trying to \ndo XYZ incrementally from the start. On the other hand, we have seen that DEMALG may be an order of magnitude \nfaster for one choice of ALG than for another that solves the same problem at least as quickly in the \nexhaustive mode. The designer of ALG should have a general awareness of the incremental conversion to \nbe used, and should not be unduly concerned about computational complexity bounds of the traditional \nkind. Compute what is natural and interesting, not just what is necessary in order to obtain the minimum \nacceptable output. Find a natural tree structure in the input (anything complicated that is nonetheless \nintelligible probably has at least one) and exploit it. Summarize large subtrees with the same kind of \ninformation associated with leaves. Thk may sound platitudinous, but it is the result of working through \nthe behavior of major flow analysis algorithms when used incrementally. Followed (perhaps unconsciously) \nin the design of elimination algorithms, the advice does work. Better verbalizations can be expected, \nas theoreticians realize that exhaustive analysis is not the only kind needing careful mathematical treatment, \nThis paper has tried to hasten that realization. REFERENCES AC76. Allen, F. E., and Cocke, J. A program \ndata flow analysis procedure. Corrrrn. ACM 19 (1976), 137-147. AS78. Auslander, M. A., and Strong, H.R. \nSystematic recursion removal. Cmrrm.ACM21 (1978), 127-134. BJ78a. Babich, W. A., and Jazayeri, M. The \nmethod of attributes for data flow analysis (Part I. Exhaustive analysis). Acta Informatica 10 (1978), \n245-264. BJ78b. Babich, W. A., and Jazayeri, M. The method of attributes for data flow analysis (Part \nII. Demand analysis). Acta Znformatica 10 (1978),265-272. Ca77. Carter, J.L. A case study of a new code \ngenerating technique for compilers. Comm, ACM 20 (1977), 914-920. GM79. Ghezzi, C., and Mandrioli, D. \nIncremental parsing. ACM Trans. on Programming Languages and Systems 1 (1979), 58-70. GM80. Ghezzi, C., \nand Mandrioli, D. Augmenting parsers to support incrementality. .l. ACM27 (1980), 564-579. GW76. Graham, \nS.L., and Wegman, M. A fast and usually linear algorithm for global flow analysis. J. ACM 23 (1976), \n172-202. HU75. Hecht, M. S., and Unman, J.D. A simple algorithm for global data flow analysis problems. \nSIAM J. Computing 4 (1975), 519-532. Ke75. Kennedy, K.W. Node listings appfied to data flow analysis. \nProc. 2nd ACM Symp. on Principles of Programming Languages (January 1975), 10-21. M079. Moriconi, M.S. \nAdesigner/verifier s assistant. IEEE Trans. on Software Engineering5 (1979),387-401, Pa79. Paige, R. \nExpression continuity and the formal differentiation of algorithms. Ph. D. Thesis, New York University, \nSeptember 1979. R077. Rosen, B.K. Applications of high-level control flow. Proc. 4th ACM Symp. on Principles \nof Programming Languages (January 1977), 38-47. R080. Rosen, B.K. Monoids for rapid data flow analysis. \nSIAMJ. Computing9 (1980), 159-196. TR80. Teitelbaum, T., and Reps, T. The Cornell Program Synthesizer: \nasyntax-directed programming environment. TR 80-421, Computer Sci. Dept., Cornell University, Ithaca \nNY, May 1980. Weg80. Wegman, M.N. Parsing for a structural editor. Proc. 21st IEEE Symp. on Foundations \nof Computer Science (October, 1980). \n\t\t\t", "proc_id": "567532", "abstract": "In analysis of programs and many other kinds of computation, algorithms are commonly written to have an input <i>P</i> and an output <i>Q,</i> where both <i>P</i> and <i>Q</i> are large and complicated objects. For example, <i>P</i> might be a routing problem and <i>Q</i> might be a solution to <i>P.</i> Although documented and discussed in this <i>exhaustive</i> style, algorithms are sometimes intended for use in contexts with two departures from the one-time analysis of an entire new input. First, the current value of <i>P</i> is the result of a small change to a previous value of <i>P.</i> We want to <i>update</i> the results of the previous analysis without redoing all of the work. Second, accurate information is only wanted in some designated portion of the large output <i>Q.</i> Possibly inaccurate information may appear elsewhere in <i>Q.</i> We want the analysis to be <i>demand-driven:</i> accurate where accuracy is demanded, but not burdened by the cost of providing much more than is demanded. This paper studies demand-driven algorithms capable of updating without extensive reanalysls. Such algorithms are called <i>incremental,</i> in contrast with the one-time analysis of an entire new input contemplated by exhaustive algorithms. In some cases, it is shown that an exhaustive algorithm can be easily recast in an efficient incremental style. Other algorithms for the same problem may be much less amenable to incremental use. It is shown that traditional exhaustive computational complexity bounds are poor predictors of incremental performance. The main examples are taken from global data flow analysis.", "authors": [{"name": "Barry K. Rosen", "author_profile_id": "81100316668", "affiliation": "IBM Thomas J. Watson Research Center, Yorktown Heights, NY", "person_id": "P28116", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/567532.567545", "year": "1981", "article_id": "567545", "conference": "POPL", "title": "Linear cost is sometimes quadratic", "url": "http://dl.acm.org/citation.cfm?id=567545"}