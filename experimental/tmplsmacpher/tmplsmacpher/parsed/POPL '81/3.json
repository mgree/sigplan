{"article_publication_date": "01-26-1981", "fulltext": "\n Permission to make digital or hard copies of part or all of this work or personal or classroom use is \ngranted without fee provided that copies are not made or distributed for profit or commercial advantage \nand that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, \nto post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. &#38;#169; \n1981 ACM 0-89791-029-X $5.00 top-down algorithm. (These derivations are detailed in [Scherlis80],) Part \nof the Earley s algorithm derivation is described below, 2. Internrd Specialization Let us illustrate \nthe idea of internal specialization with a simple example, Consider the following program for reversing \nlists: reu(z) + if z= Athen A else reu(tl(z)) o cons(hd(z), A). Here, hd, tl, and cons are the usual \nfunctions for select\u00ading and constructing lists; A denotes the empty list, and the infix operator o is \nanother name for the function append, which concatenates two lists. Suppose that this program is used \nin a context where we require only the first element of the reverse of a given list z. We describe this \nsituation by using the notation: Compute: hd(reu(z)) (where z #A) Given: rev(z) + if z = A then A else \nrev(tl(z)) o cons(hd(z), A). This program computes the desired value by construct\u00ading a reversed version \nof the initial list and then taking the hd of the result. Using the transformation rules we describe \nbelow, we can specialize the auxiliary function rev into a new function hdreu. We obtain a new pro\u00adgram, \nCompute: h.drev(z) (where z #A) Given: hdrev(z) + if nufl(tl(z)) then M(z) eise hdreu(t/(z)), which produces \nresults identical to those of the initial specification, except without doing any list construc\u00adtion \nat all. In a similar way, if we notice that in the definition of rev, rev(z) + if z= Athen A eise rev(tl(z)) \no cons(hd(z), A), the recursive calls are always of the form rev(u) o v (for some lists u and u), then \nby deriving a version of rev specialized to the case in which its output will always be appended to a \nlist, we will obtain a more efficient implementation of rev itself. That is, we transform Compute: rev(u) \no v Given: reu(z) t if z = A then A else reu(tl(z)) o com(hd(z), A) into Compute: reu2(u, u) Given: rev2(u, \nu) + if u = A then u else reu2(ti(u), cow(hd(u), v)). From this we can obtain the program, rev(z) + if \nz= Athen A else reu2(t/(z), cow(hd(z), A)) reu2(u, u)+ if u= Athen u else rev2(t/(u), corw(hd(u), v)), \nor more simply rev(z) + rev2(z, A) rev2(u, u)+ if u= Athen v else rev2(t/(u), com(hd(u), v)). If cons \nis the only list constructing function, then the append function must make at least as many calls to \ncons as there are elements in its first argument. This implies that the initial rev program must use \na num\u00ad ber of cons s proportional to the square of the length of the input list. The final program, on \nthe other hand, is linear in the number of cons s it uses, and, because of its iterative structure, can \nbe implemented very efficiently. 3. Expression Procedures The specialization technique we develop is \nbased on a new approach to the transformation of applicative programs: We expand an ordinary language \nof recur\u00adsion equations to include a generalized procedure con\u00adstruct the expression procedure. This \ngreatly en\u00adhances our ability to manipulate programs in the initial language. Indeed, the expression \nprocedure provides a way of expressing information not just about the properties of individual program \nelements, but also about the way they relate to each other, This ability to represent program-specific \nfacts allows us to manipu\u00adlate these facts in the same way as any other part of a program. Consequently, \nthe set of transformation rules we require (not including simplification rules for primitive symbols \nsuch as cons) is very small. An expression procedure represents a method for evaluating a specific class \nof expressions. An example of an expression procedure is rev(u) ou+ifu=Athenu else rev(ti(u)) o com(hd(u), \nv). This definition specifies a method for evaluating ex\u00ad pressions which are of the form rev(u) ov where \nu and v 42 are lists. Now, to evaluate an instance of the expression at run-time, an interpreter could, \nrather than evaluate the constituents of the expression in the usual way, use instead the expression \nprocedure, and directly return a value. Note that since the else clause is an instance of the left-hand \nside, this expression procedure is recur\u00aduive, That is, the expression procedure can be used to evaluate \nits own else clause. Expression procedures are different from other pro\u00adcedures in several ways, First, \ntheir left-hand sides are complex expressions which (as we will see below) al\u00adready have meanings associated \nwith them, This leads us to restrict our attention to expression procedures which are corasitderzt wh?se \nleft and right hand sides are equivalent u priori. Thus, given the usual definition of gcd, the expression \nprocedure gcd(z, gcd(y, z)) t y is considered inconsistent, since the two sides of the definition are \nnot always equivalent. In addition, an expression procedure must repre\u00adsent a progressive method for \ncomputing the expression it defines. The expression procedure, gcd(z, y) + gcd(y, z), is not progressive \nsince it introduces looping evalua\u00adtions of terms whose values were previously effectively computable. \nConsistency and progressiveness are properties of an expression procedure with respect to some given \nprogram. Consider, for example, the program, Either of the expression procedures, a+ b+b+a b-l-at a+b, \nis progressive relative to the given program, After one has been added to the program, however, the other \nceases to be progressive, since its addition would intro\u00adduce looping evaluations where there were none \npre\u00adviously. In general, it may be quite dificult to establish consistency and progressiveness for arbitrary \nexpres\u00adsion procedures. In our application, however, we will introduce expression procedures in a systematic \nway, using transformation rules that guarantee new expres\u00adsion procedures to be both consistent and progressive. \nWe can intuitively view an expression procedure as both an assertion and a program. It is a program in \nthat it specifies a method for evaluating certain ex\u00adpressions, yet it is an assertion in that we use \nit to describe relationships between the names in a program: If all the names appearing in the left hand \nside of a consistent and progressive expression procedure are defined elsewhere in a program using conventional \npro\u00adcedure definitions, then the expression procedure can be eliminated without changing the meaning \nof that program. 4. A Simple Programming Language In our examples, we will be using a simple ap\u00adplicative \nlanguage of functional recursive programs ex\u00adtended to admit expression procedures. We outline here some \nof its characteristics. Syntax. A term is either a constant, a variable, or an expression of the form \nh(tl, t2,,.., tJ, where h is a function symbol and the t~ are terms. A term is ground if it contains \nno variables. In any program there must always be certain func\u00adtion names, like cons and if-then-else, \nwhich are as\u00adsumed to be defined elsewhere, We call these predefine function names primitive symbols. \nThe other names, which are given explicit definitions by a programmer, are called derivat he symboh. \nA definition is an expression of the form $4-t, where s is a nonatomic term and t is any term. The term \ns is called the name part of the definition. If it is of the form /(zl,..., zJ, where / is a derivative \nfunction symbol and the zi are all distinct variables, then the definition is a basic definition; otherwise \nit is called a cornplez definitiori or expression procedure. The term t, the body of the definition, \ncan contain no variables which do not also occur in s. We often refer to the name part of a complex definition \nas a cornplez name or phrase. An example of a basic definition is gcd(z, y) t if z # O then gcd(rem(y,,z), \nz) else y. , Some examples of complex definitions (expression pro\u00adcedures) consistent with this basic \ndefinition are gcd(z, 2) + if even(z) then 2 else 1, gcd(z, Z) 4-Z, gcd(2z, 2y) * 2 gcd(z, y). A program \nis an expression of the form tl, . . ..jwa. dl;...;k where the ~j are derivative symbols and the dj are \ndefinitions, The fi are called the principal symbols of the program. The other function symbols are called \nlocal symbols, Local and principal symbols are distinguished to allow the introduction and elimination \nof local auxil\u00adiary definitions while maintaining equivalence of pro\u00adgrams. Principal symbol specifications \nwill be omitted when obvious from context, A basic program is a program all of whose defini\u00adtions are \nbasic. The basic ~egrnent of a program is that basic program consisting of the principal symbols and \nthe basic definitions of P. We require that every derivative name that occurs in the program or is a \nprincipal name be defined by ex\u00adactly one basic definition. This implies that all names occurring in \nexpression procedure names and bodies are either primitive or have basic definitions; thus, the consistency \nproperty of an expression procedure can be tested with respect to the basic segment of the program in \nwhich it occurs. An example of a program is rev : rev(z) + reu2(z, A) ; rev(w) o u +-rev2(u, u) ; rev2(u, \nu) i--if u = A then u else rev(tl(u)) o corw(hcl(u), v) The first and third definitions are basic; the \nsecond is an expression procedure. The derivative symbols are rev and reu2; rev is principal, and reu2 \nis local. Semantics. We give meaning to programs by means of an evaluator model. In such a model, we \nassume that all primitive names are interpreted, in the sense that their meanings are built into the \nsimplification mechanism of the evaluator. The derivative names, those names that are defined by the \nprogrammer, are given meaning by the evaluation process. In the case of basic programs, we give meaning \nto (ground) terms in the conventional manner, replacing a derivative name by the appropriately instantiated \nbody of its definition, simplifying, and then repeating the same process for the resulting expression. \nEvaluation terminates when only primitive names remain, In gen\u00aderal, the termination properties of an \nevaluator are determined by the particular algorithm used for select\u00ading the derivative names to expand. \nThis basic evaluator model extends naturally to programs with expression procedures, as is detailed in \n[Scherlis80]. The resulting full evaluator has the property that there is, in general, more than one \nway to evaluate a given term, We can, however, define consistency and progressiveness in such a way that \n(a) the four transformations all preserve consistency and progressiveness and (b) if all expression procedures \nare consistent and progressive, then all evaluation paths for a term will have identical termination \nproperties and results. 5. The Transformation Rules Besides simplifications, we use four fundamental \nprogram transformation rules. Two of these introduce new definitions: Abstraction introduces new basic \ndefi\u00adnitions and composition introduces new expression pro\u00adcedures. The third rule, application, expands \nboth types of definitions, replacing definition names by defi\u00adnition bodies. The fourth rule, definition \nelimination, is used to eliminate unnecessary definitions, In the descriptions of these rules, we use \nthe nota\u00adtion, Ul}. ... un t 21, .O. ,zn to indicate the result of the parallel substitution of the terms \nui for the variables zi in the term L Thus, f(z, g(y), z) ; ; = j(y, g(z), z). 1 Often, rather than writing \na specification for a list of variabIes or terms like ul,. ... Un, we may instead write simply U. Thus, \nwe could have written tl~ for the expression above. We will also make use of the notions of strict sub\u00adterm \nand proper definition instance: For a given pro\u00adgram, a term s is a strict subterrn of a term t if s \nis evaluated as a part of the evaluation of t.For example, if g and h are derivative, then the strict \nsubterms of if 2 = 3 then f(z) else g(h(z)) are 2, 3, 2 = 3, z, h.(z), g(h(z)), and the entire term. \nNote that ~(z) is not a strict subterm since it is not evaluated, The pair (s1$ tl~) is a proper de/initiora \ninstance of a definition 8 + t if for each i, either Zi is a strict subterm of t or evaluation of ui \nalways terminates. A proper definition instance of j(z, y) + h(z, k(y)) is (j(9(4, u), h(g(u), k(v))). \nSimilarly, (j(4), 3) is a proper definition instance of j(z) +3, but {~(z), 3) is not, We will state \nthe transformation rules for a call\u00adby-uake evaluator. The call-by-name case is similar. Composition. \nThe cornpo.sitionrule introduces new expression procedures (that are consistent and progres\u00adsive) by \nembedding an existing definition in a given expression, This rule is a generalization of the partial \napplication operation [Burstal171]. Let s t u be a definition in a given program P, and (s , u ) be a \nproper definition instance. Composition produces a new program PI which is the program P with an additional \ncomplex definition S *tu t z z where the variable z occurs exactly once in, and is a strict subterm of, \na term t. Notes. Restricting the pair (s , u ) to be a proper defini\u00adtion instance prevents use of composition \nto eliminate looping evaluations. For example, if evaluation of k(u) always loops, then evaluation of \nthe term ~(k[2)) must also loop. Composing ~(z) t 3 into j(k(u)) t 3 must be illegal since the new definition \nwould permit evaluation of f(k(2)) to result in 3. In a similar way, we can show that the restriction \non z prevents introduction of loops, Ezample: Suppose we are given the list reversing pro\u00adgram rev(z) \n-if z= Athen A else rev(tl(z)) o cons(hd(z), A). We can derive a more efficient version of this program \nby composing the definition of rev to form an expres\u00adsion procedure for the expression rev(u) o U. rev(u) \nou+(ifu=A then A else reu(tt(u)) o cons(hd(u), A)) o v To simplify this new definition, we bring the \no v inside the conditional and simplify the result using the three properties of append, (Uov)ow=uo(vow) \ncoras(u, A) o v = cons(u, u) AOV=V, to obtain rev(u) ovtifu=Athenv else rev(t/(u)) o cons(hd(u), v), \nwhich is recursive, as we noted above. Observe now that none of the recursive calls to rev need ever \nbe evaluated, since this expression procedure can be used instead. Further, observe that with this expression \nprocedure it is possible to evaluate reu(z) using a num\u00ad ber of cons calls equal to the length of z, \nThus, the unmodified definition of rev, together with this expres\u00ad sion procedure as a subroutine, give \nan optimal non\u00ad destructive list reversal function. Abstraction. The abstraction rule corresponds to \nthe familiar operation of forming a new subroutine for an existing section of code in a program. Suppose \nm definitions in a given program P can be written in the form Tim t p .,., t lz $1 4-tl s~ + tm z z \nThen abstraction produces a new program with these definitions replaced by the new definitions, where \nJ is a derivative name which does not occur anywhere in P and each zi occurs as a strict subterm of t. \nNotes: The restriction on the zi prevents abstraction of a definition like k(z) + if g(z) then hi(z) \nelse hz(z) into the pair of definitions k(z) + j(z, h2(z)) ~(u, v) + if g(u) then hi(u) else v. If \nevaluation of h2(z) does not terminate for some values of x, then this abstraction may introduce looping \neval\u00ad uations where there were none previously. Ezarrqde: The last function computes the last element \nof alist z: last(z) + if z= Athen A elscif tl(z) = A then hd(z) else tast(tl(z)) Observe that ti(z) \nwill be computed twice in all but one iteration of this program. Abstraction can be used to eliminate \nthis extra computation. last(z) t if z = A then A else lastb(tl(z), z) hzstb(u, v) + if u = A then hd(v) \nelse last(u) A zamgde (continuing the rev derivation): It may seem that the gain in efficiency we realized \nin the rev ex\u00adample comes only at the expense of implementing a facility for evaluating expression procedures. \nFortu\u00adnately, this is not the case; we can use the abstraction and application rules to eliminate our \ndependence on the expression procedure for rev(u) o u while maintaini\u00adng the current level of efficiency. \nWe start by abstract\u00ading the entire body of the expression procedure into a new definition, which we \nwill call rev2, It happens that the procedures for rev and rev(u) o v have matching right-hand sides; \ntherefore, we can abstract the body of rev as well. rev(z) + rev2(z, A) rev(u) o v + reu2(u, U) rev2(u, \nv)+ if u= Athen u else reu(ti(u)) o cons(hd(u), v). Application. Application or unfolding is used to \nreplace an instance of a basic or complex name by the body of its definition, Let s + u be a definition \nin a given program P, and (s , u ) be a proper definition instance. Application produces a new program \nP which is the program P with a definition t t f]; replaced by the new defini\u00ad tion t+-t?lthe variable \nz occurs in the term ~ ,where f,  Notes: The pair (s , u ) must be a proper definition instance to prevent \napplication from eliminating loops. For example, if evaluation of k(u) loops then applying the definition \n~(z) t 3 in the body of h(u) t j(k(u)) would cause evaluation of h to terminate where it did not previously. \nExample (continuing the rev derivation): The expres\u00adsion procedure in the rev example is now implemented \nas a pair of mutually recursive procedures, rev(u)ov and rev2. The application rule enables us to expand \nthe expression procedure call in the body of reu2, replacing it by its body, which is a call to rev2. \nWe obtain rev(z) + rev2(z, A) rev(u) o u + rev2(u, u) reu2(u, v)+ if u= Athen v else reu2(t/(u), corw(hd(u), \nu)). Definition Elimination. (a) Any expression proce\u00addure can be dropped from a program, (b) Any set \nof basic definitions which defines a set of local derivative names that do not appear in the body parts \nof any other definitions, can be dropped from a program. ZVotes. The second part of this rule allows \nelimina\u00adtion not only of individual definitions, but of mutually recursive sets of definitions as well. \nEzample (concluding the rev derivation): In the new rev program, we notice that there are no longer any \ninstances of the expression rev(u) o v in the program, so the expression procedure can never be called, \nBy the definition elimination rule we can drop it to obtain the basic program, rev(z) t rev2(z, A) reu2(u, \nu) t if u = A then v else rev2(tl(u), corw(hd(u), u)). We have thus eliminated the expression procedure \nwhile maintaining the e5ciency gained through its introduc\u00adtion, Note that had we not abstracted the \nbody of rev earlier, we would have obtained the equivalent program rev(z) + if z= Athen A else rev2(t/(z), \ncorw(hd(z), A)) reu2(u, u)+ if u= Athen u else reu2(tl(u), corw(hd(u), u)). The use of expression procedures \nin this deriva\u00adtion allowed us to express facts about the relation be\u00adtween the list reversing and list \nappending functions in a natural way. Observe, however, that we used the expression procedure only in \nthe intermediate steps of the derivation; thus we could derive the advantages of its use without having \nto develop an implementation for it. Theorem: Let P be a basic program, Any basic pro\u00adgram P obtained \nfrom P by a sequence of applica\u00adtions of the four transformation rules is strongly equivalent to P, That \nis, P terminates if and only if P terminates, and when they both tgrminate they yield identical results. \nThe proof of his theorem involves showing that each of the transformations preserves both the equiv\u00adalence \nof basic segments of programs and the consis\u00adtency and progressiveness properties of expression pro\u00adcedures, \nA detailed presentation of the proof may be found in [Scherlis80]. 6. The Specialization Technique An \nExample. To illustrate the specialization tech\u00adnique, we show how to specialize the rev function to the \ncase in which only the first element of the reversed list is required. Compute: hd(reu(z)) (where z #A) \nGiven: rev(z) + if z = A then A else rev(tl(z)) o cons(hd(z), A) We assume throughout that the initial \nlist is nonempty. (Such restrictions can be made explicit if the language is extended to include conditional \nde~nitiotas. We dis\u00adcuss this possibility below.) It seems natural that we should be able to develop \na specialized version of rev in which computation not necessary to the determination of the Ml of the \nresult is eliminated. We do this by forming an expression procedure for hd(reu(z)) using the composition \nrule. hd(reu(z)) + hd(if z = A then A else reu(tl(z)) o corw(hd(z), A)) (where z #A) Our goal in simplifying \nthis expression procedure} be\u00adsides eliminating unnecessary computation, is to elim\u00adinate the recursive \ncall or, failing this, to bring it into the specialized form. If the simplification is successful, then \nthe efficiency improvement would be realized at all levels of recursion, We simplify the body in two \nsteps, First, the as\u00adsumption that z # A enables us to replace the entire conditional by its else clause. \nSecond, we derive ex\u00adpression procedures for (rev(z) = A) and hd(s o t) from the definitions of rev and \nappend, and apply them in the body. We obtain hd(reu(z)) t if tf(z) = A then M(z) else hd(reu(tl(z))) \n(where z # A), which (because of the ifcondition) is recursive. It is thus possible to compute hd(reu(z)) \nwithout ever calling rev, It remains now to transform this into a basic pro\u00adgram, eliminating the expression \nprocedure while retain\u00ading the efficiency gain. As in the rev derivation, this is done in two steps, \nfirst by abstracting the body of the expression procedure, hd(reu(z)) 4-hdreu(z) (where z #A) hdreu(z) \n+ if tf(z) = A then M(z) else hd(rev(tf(z))) and then applying the new expression procedure, both in \nthe body and in the original computation specifica\u00adtion. We obtain the basic program, Compute: hdrev(z) \n(where z #A) G-iven: hdreu(zj t if ti(z) = A theri hd(z) else Mrev(tl(z)], Specialization. Thus, the \nspecialization process has three steps. Suppose we are given a definition of a function $ that has recursive \ncalls labeled jl, jz, . . . . and suppose that j is called in a specialized context, h(f). Compute: h(j) \nf+... ff2.. f2 ... Given: We perform the following steps: (1) (Cornpo$e) Compose the definition of f \ninto an expression procedure for h(f). (2) (Simplify) Simplify this expression pro\u00adcedure until each \nrecursive call in the body either has the specialized form, or has been eliminated. If this fails, then \nezit. (3) (Rename) Abstract and apply, in effect renaming h(j) to hf.  Compute: hf Given: hft... hflhf \n2 In practice, we usually specialize by degrees, in order to obtain the best possible improvement to \na program. For example, if the compute specification above had been,  Compute: h~(h~(f)), then we would \nhave attempted specialization first on h2(f). If this specialization had succeeded, then we would have \nattempted to specialize the resulting func\u00adtion h2 f to hl (h2j). Now if this second specialization fails, \nthen we would at least realize the benefits of the first specialization. Note that if the first specialization \nhad failed, then it is still possible for a full specializa\u00adtion of j to hl (h2(j)) to succeed, since \nthe additional assumptions may permit more simplification of the ex\u00adpression procedure body. There are \ntwo sources of heuristic difficulty in the specialization process. The first difficulty is in the selection \nof the initial algorithm: A wrong choice here may mean that the sequence of specialization steps is longer \nand more complicated than necessary, or even that an adequately efficient specialized algorithm is not \npractically obtainable. The second difficulty is in the specialization process itself Specialization \nis a sequen\u00adtial process, in which an algorithm is transformed into a more specific one in a series of \nsteps, each step reflecting some observation about the behavior of the algorithm at that point. It sometimes \nhappens that a particular step of specialization relies on an observation involving some deep property \nof the algorithm. Such eureka steps (in the terminology of Burstall and Darlington, [Burstal177]) may \nrequire some inspiration to find. Observe that the initial rev example consisted of a single step of \nspecialization. The heuristic di5culty in this derivation, as in similar rev derivations else\u00adwhere, \nwas the choice of the phrase rev(u) o u for con\u00adsideration. It is thh choice that lets us take advantage \nof the associativity property of append to obtain the simplification, The reader may verify that the \nsame improvement would have been obtained with the more highly specialized expression rev(ti(u)) o coras(hd(ti), \nu). 7. QurJi5ers Thus far, we have been able to create specialized versions of a definition by makkg \nexpression proce\u00addures for new expressions that contain the given name. The specialization condition \nis thus expressed as a con\u00adstraint on the syntactic context of instances of the given name. It is often \nthe case that we need to impose a spe\u00adcialization condition that cannot be expressed in this syntactic \nway. In such cases, we specify the condition by means of special condition tags, called gualijier~. We \nwrite a qualified definition as {u} s-t, where s and t are terms and the qualifier u is a primitive \nterm that evaluates to a truth value. If a definition has a qualifier, then the definition can be expanded \nby an evaluator or by the application transformation only in contexts in which the qualifier simplifies \nto true. Thus, we can think of qualifiers as specifying preconditions for definitions, (This is similar \nto Wegbreit s notion of corztezt tags [Wegbreit76],) Qualified definitions are formed using an extension \nof the composition rule: If $ t t is a definition, u is a primitive term denoting a truth value, and \nthe free variables of u are a subset of the free variables of s, then by composition we can add the new \ndefinition {u} $+ t. The presence of the qualifier implies that the body of the new definition can be \nsimplified under the assump. tion that the qualifier is true. For example, we can specialize the definition \nof the hzst function last(x) + if z= Athen h elseif ti(z) = A then h.d(z) else kz.st(ti(z)). to the \ncase in which the argument is a nonempty list by adding the qualifier {z#A} . The resulting definition \nsimplifies to {z#A} last(z) + if tl(z) = A then M(z) else {tl(z)#A} hJd(t/(z)), which is recursive, \nWe have used the qualifier notation in the body of this definition to remind us that the condition holds \nfor the recursive call. 8. Two examples We outline here two examples of specialization. Square root, \nConsider the following program for computing the integer square root of a nonnegative integer, s(z) + \nq(o, z) q(i, z)-ifi2~z <(i+ 1)2 theni else q(i + 1, z). This program finds the square root of z by an \nobvious brute force algorithm, requiring two additions and two multiplications at each iteration. We \ndevelop a more efficient version by specializing q in several steps. The first of these steps is given \nhere. We make the following observation concerning the if test in the definition of q: It appears that \nthe calls to q are arranged such that i2 < z is always true before q(i, z) is called. If this is indeed \ntrue, then half of the if test is unnecessary, We can test this hypothesis for specialization by composing \nq with a qualifier: {~a<z}q(i, z) + if iz < z < (i+ 1)2 then else q(i + I, z), and using the qualifier \nto simplify the body {~ <z} q(i, z) + if z < (i+ 1)2 then i else q(i + 1, z), For our specialization \nto succeed, we must show that all calls to q are in the specialized form. Since the negation of the if \ntest is true in the else clause, {i <.} q(i, z) t if z < (i+ 1)2 then i else {(i+ l) < Z} q(i + 1, z), \n we see that the expression procedure is recursive. In addition, since z is nonnegative, it is clear \nthat the qualifier condition is true for the initial call of q from s. We have s(z) + {0 s2} q(o, z) \n{F<z} q(i, z) + if z < (z + 1)2 then i else {(i+l) SZ} q(i + 1, z), so our specialization is successful. \nFurther use of the specialization technique enables us to obtain the equivalent iterative program, s(z) \n+ r(O, 1,3, z) r(i, rn, n,z)+ifz<rn theni elser(i + 1,m+ n,n+ 2,2), which does only three additions \nat each step. (A similar derivation appears in [Katz78].) Earley s algorithm. To illustrate a nontrivial \nap\u00adplication of our techniques, we outline here two steps in the derivation of Earley s context-free \nparsing algo\u00adrithm that appears in [Scherlis80]. In thk derivation, we start with a simple program for \nthe cferivet rela\u00adtion ~ in context-free languages. Our goal is to test [s]+[s21,..., aM]; that is, to \ntest whether the start symbol derives a final string for a particular grammar. In the same way that we \nspecialized the defnition of rev(z) to hd(rev(z)) above, we specialize here the cferiues relation to \nobtain an improved parsing algo\u00adrithm. It is clear, however, that if we try to specialize a ~/3 directly \nto our goal, we will not be able to get the recursive calls in the body into the specialized form. Thus, \nwe must specialize only partially. We do this in two steps, In the first step, we restrict the string/3 \nto be a substring [a~+l, ..., a~] of the final string, and show that this implies that the arguments \nof the recursive calls are also in this form. In the second step, we restrict the range of possibilities \nfor a, adding a top\u00addown element to this bottom-up algorithm. The ob\u00adservation we make is that a is always \nthe prefix of the right-hand side of a production. That is, we are only interested in testing a % [ai+l, \n..., ~j] when there are A and T such that A -+ a oq, The specialization steps are successful, so we re\u00adname, \nreplacing the complex name A+ao7 A ~~[ui+l,...,aj] by a new function of five parameters, Q(A, a, 7, \ni, j). This gives a recursive version of Earley s aigorithm. We can interpret the five parameters of \nQ in terms of Ear)ey s state set notation ([Earley70]): [A+ a.7, i] C Zj s Q(A, a, 7, i,j) The actual \nimplementation of Earley s algorithm now involves only the straightforward construction of a tab\u00adleau \nrepresenting a dynamic programming array for this recursive algorithm Q. Acknowledgements. I am grateful \nto Zohar Manna and Richard Wal\u00addinger for their good advice on the presentation of this work, I also \nwish to thank Richard Korf, Nico Haber\u00admann, Friedrich von Henke, and Pierre Wolper for their helpful \ncomments on drafts of this paper. Bibliography [Burstal171] R.M. Burstall, J.S. Collins, and R.J. Popples\u00adtone, \nProgramming in POP-2. Edinburgh Univer\u00adsity Press, 19710 [Burstal177] R,M, Burstall and J. Darlington, \nA Transfor\u00admation System for Developing Recursive Programs. Jourrsa/ of the ACM, Vol. 24, No. 1, January \n1977. [Darlington78a] J. Darlington, A Synthesis of Several Sorting Algorithms, Acts Inforrnaticq Vol. \n11, Page 1, 1978. [Earley70] J. Earley, An Efficient Context-Free Parsing Algorithm. Communications oj \nthe ACM, Vol. 13, No. 2, February 1970, [Earley74] J. Earley, High Level Operations in Automatic Programming. \nSymposium on Very High Level Languagec, SIGPLAN Noticesl Vol. 9, No. 4, 1974. [Green78] C.C. Green and \nD.R. Barstow, On program Synthesis Knowledge. Artificial Intelligence, Vol. 10, Page 241, 1978. [Katz78] \nS. Kats, Program Optimisation Using Invariant. IEEE Transaction on Sojtvare Engineering, Vol. SE-4, No. \n5, September, 1978, [Kott78] L. Kott, About R. Burstall and J. Darlington s Transformation System: A \nTheoretical Study. Program fiansjormatio~ B. Robinet, cd., Dunod, 1978. [Manna 79] Z. Manna and R. Waldinger, \nSynthesis: Dreams + Programs. IEEE Transactions on SojtcoareEngi\u00adneering, Vol. SE-5, No. 4, July 1979. \n [Paige77] R. Paige and J.T. Schwarts, Reduction in Strength of High Level Operations. Fourth Sympo\u00adsium \non Principles o/Programming Languaget, January 1977. [Scherlis80] W, L, Scherlis, ~;~;~~~s Pgj;;o;e\\#d \nProgram Derivation. . . , . puter Science Report STAN-CS-80-818, August 1980. [Wegbreit73] B. Wegbreit, \nProcedure Closure in EL1. Center for Research in Computing Technology, Harvard University, May 1973. \n[Wegbreit76] B. Wegbreit, Goal-Directed Program Trans\u00adformation. Third Symposium on Principles oj Pro. \ngramming Language#, January 1976. \n\t\t\t", "proc_id": "567532", "abstract": "We investigate the specialization of programs by means of program transformation techniques. There are two goals of this investigation: the construction of program synthesis tools, and a better understanding of the development of algorithms.By extending an ordinary language of recursion equations to include a generalized procedure construct (the <i>expression procedure</i>), our ability to manipulate programs in that language is greatly enhanced. The expression procedure provides a means of expressing information not just about the properties of individual program elements, but also about the way they relate to each other.A set of four operations for transforming programs in this extended language is presented. These operations, unlike the transformation rules of Burstall and Darlington and of Manna and Waldinger, preserve the strong equivalence of programs.This set of operations forms the basis of a general-purpose specialization technique for recursive programs. The practical value of this technique has been demonstrated in the systematic development of many programs, including several context-free parsing algorithms. We outline here the development of Earley's algorithm by specialization.This paper is an informal exposition of part of the results of the author's Ph.D. thesis [Scherlis80].", "authors": [{"name": "William L. Scherlis", "author_profile_id": "81100605830", "affiliation": "Stanford University and Carnegie-Mellon University, Pittsburgh, PA", "person_id": "P299938", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/567532.567536", "year": "1981", "article_id": "567536", "conference": "POPL", "title": "Program improvement by internal specialization", "url": "http://dl.acm.org/citation.cfm?id=567536"}