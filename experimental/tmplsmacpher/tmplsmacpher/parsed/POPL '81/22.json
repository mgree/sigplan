{"article_publication_date": "01-26-1981", "fulltext": "\n Permission to make digital or hard copies of part or all of this work or personal or classroom use is \ngranted without fee provided that copies are not made or distributed for profit or commercial advantage \nand that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, \nto post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. &#38;#169; \n1981 ACM 0-89791-029-X $5.00 algorithm is expressed as a program in that lang\u00aduage, when the program \nis compiled, and when it is executed. Most languages have an expliticly stated dependence between consecutive \nstatements in the control flow graph (e.g., PASCAL, ALGOL, FORTBAN, SNOBOL, etc.). A few languages [AcDe79], \n[ArGP78] are defined so that some types of dependence are disallowed and others are greatly reduced, \nhowever, making the programmer responsible for reducing de pendences imposes a difficult task on the \nprogram mer. High-speed computer systems with multiple functional units or pipelines commonly employ \na lookahead control unit that breaks dependence at execution time. Studies have shown that the aver\u00adage \nspeedup due to lookahead hardware is about a factor of 2 [Kuck78]. We believe that compilers are most \nwell suited to solving the problem of breaking program dependence. In fact, much work has been done on \nthis prob lem in the past. The renaming of variables and code motion are traditional optimization techniques \nwhich result in an improved dependence graph [AhU173], [Grie71]. Transformations explicitly aimed at \nlookahead control units are also well known [A1C072]. In this paper, we assume that a dependence graph \nexists and that it is sharp (i.e., arcs are included only when necessary). We will show in Section 3 \nthat a number of graph transformations exist to remove arcs from a sharp dependence graph. In Section \n4, we discuss node transformations that abstract the graph in useful ways. This results in a directed \nacyclic graph that is amenable to rather straightforward code generation. 2. Source Language and Dependence \nGraphs 2.1 Source Language The transformations described in this paper will operate on programs written \nin a language con\u00adsisting of three types of statements: assignment, &#38;oh loops, and UJ/ L(&#38; loops. \nThe last two are well\u00adknown compound statements. In this paper, ~Oh loops are restricted in such a way \nthat the initial value and the increment of the index variable is always one, and the loop limit is always \na constant or a variable. These are not serious restrictions, since any more general ~Oh loop can be \nautomati\u00adcally translated into this form [BCKT79]. Only two types of variables are allowed, scalars and \narrays of arbitrary dimension. In this paper, variable element will stand for either a scalar variable \nor an array variable element. Assignment statements are of the form <variable> = <expression>. The expression \ncan be any valid Boolean or arith\u00admetic expression, or something of the form id <Boolean-expression> \nJ%Zn <expressionl>. &#38;e <expression>. The value of this last construct will be <expres\u00ad sion > if \n<Boolean expression> is true, and 1 <expression > otherwise. 2 It is certainly theoretically possible \nto automatically translate FORTRAN or ALGOL-like programs into equivalent go -to-less programs. This \nhas been discussed in [BoJa66], [ AsMa75] . However, the programs resulting from the transformations \ndescribed in these papers could be too complex for practical purposes. A more practical approach to the \nautomatic improve\u00adment of program structures has been taken in [Bake78] . The resulting programs in this \nlast case, however, could include go to statements. The assumption we make in this paper is that if the \nsource program is written in a FORTRAN-or ALGOL-like language, some sort of preprocessor will attempt \nto translate it into the above lan guage but without going to the extreme of pro\u00adducing unduly complex \nprograms. The translator may then act only on segments of the program pro\u00adduced by the preprocessor, \nleaving the rest of the program as it is. This is similar to the approach taken in the PARAFRASE system. \nOur experience indicates that in most programs a large percentage of the code can be translated into \nwell structured, go to-less code.  2.2 Dependence Graphs The main tool we will use in the translation \nprocess is the dependence graph. This is a di graph whose nodes represent program components, and whose \narcs are one of five different types. A dependence graph can be built at different levels of abstraction; \nwe will discuss this later in the section. For now, we will assume that a graph node represents one of \nthe three types of program components: assignment statements, 40Z100P headers, and Wh&#38;2 loop headers. \nWe will assign labels to all the program components. These labels will be of the form Ai for assignment \nstatements, Fi for ~o)t loop headers, and (Ui for w/ti&#38; loop headers. An example of a program and \nits depend ence graph is given in Fig. 2 (a) . A graph arc represents one of the five pos\u00adsible relations \nbetween the program components. We now proceed to define these five relations, starting with the loop \ndependence relation. Definition A program component C (either an assignment statement or a loop header) \nis said to be loop dependent on a loop header L(either a ~Oh loop header or a dd?fl loop header) if C \nis embed\u00added in the loop statement whose header is L, or if C=L. In the text we will denote this dependence \nas L 6L C, and in the dependence graph it will be represented by arcs of the form shown in Fig. l(a). \n In Fig. 2, the reader can find examples of the loop dependence relation. We now proceed to dis cuss \nthe next three types of relations. Definition Assume a component C and n different loop headers Ll, . \n. . . Ln, such that (1) Li &#38;LC  i=l, . ..jn (2) Li (SL Li+l isl, . . ..n l (3) C is not loop dependent \non any other loop header.1  An instance C(kl, . . . . kn) of a component C is de\u00ad fined as the component \nC when for 1 ~ i sn the loop whose header is Li is executing its ki-th iteration. Notice that when Li \nis a ~o,h loop header, ki will be the value of its index variable. This is because we have restricted \nthe initial value and the increment in ~ok loops to one. When all L Ln are fjofi loop headers, (kl, . \n. . . kn) 1  is called an index set.  Component instances have two sets of variable elements associated \nwith them: a set of inputs and a set of outpute. The set of inputs are those vari\u00adable elements fetched \nbyjthe component instance, and the set of outputs are those variable elements modified by it. When the \ncomponent is a W~L loop header, the set of outputs is empty, and the set of inputs is given by the Boolean \nexpression in the header. When the component is a 40X loop header, say F, the instance F(..., 1) (i.e., \nF at the first iteration of the loop of which it is a header) has the index variable as output, and the \nloop limit as input if it is a variable. The instances F(..., k), k > 1, also have the index variable \nas input. For an assignment statement the set of inputs is deter\u00admined by the expression, and the set \nof outputs by the variable on the left-hand side. Notice that the set of outputs of a program component \nhas always only one element. Definition Consider two, not necessarily dis\u00adtinct components Cr and Cs \nand one instance of each, Cr(z) and c~~), such that cr(~) is executed before Cs(~) in the proper serial \nexecution of the program. We say that (a) Cs (~) is output dependent on Cr (~), denoted Cr(~) 6\u00b0 Cs(~) \niff they have the same output variable element. (b) Cs(~) is antidependent on Cr(~), denoted Cr(~) 6A \nCs(~), iff the output variable ele\u00ad ment of Cs(~) is an input variable element of Cr(z) . (c) Cs(~) is \nflow dependent on Cr(;), denoted Cr(~) 6 Cs(~) iff the output variable element of Cr(;) is an input variable \nelement of Cs(~), and there is no other instance Ct(~) executed after Cr(~) but before Cs(~) such that \nCr(~) 6\u00b0 Ct(~). (Intuitively, the val~e computed by Cr(i) is actually used by Cs(j).) A program component \nCr is said to be output, anti, lNotice that if C ia a loop header, then C = L n or flow dependent on \ncomponent C iff there exiet s 7I and ~ such that Cr(~) is, respectively, output, anti, or flow dependent \non Cs(~). The arcs repre\u00ad senting the previous three relations are given in Figs. l(b)-(d). The presence \nof array variables poscsparticu lar problems in the computation of the previous three relations. For \nexample, assume two assign ment statements A and A2 such that an array vari\u00ad 1 able V appears on the \nleft-hand side of Al and on the right-hand side of A2. To determine whether Al 6 A2, we need to determine \nwhether for some instances Al(y) and A2(~), with Al(y) executed before A2(~), the element of V modified \nby A1(~) is the same as the element of V fetched by A2(~). If the subscript of V is a constant in both \nstate\u00adments, this is a trivial task. The other extreme is when it is not possible to make such a determi\u00adnation \nat compile time because the subscript of V is a function of the program input. In this case, we have \nto be conservative and assume that the flow dependence relation holds in order to guar\u00adantee the correctness \nof our transformations. An intermediate case is when the subscript of V in Al(z) is a (possibly multidimensional) \nfunction ~(~), and A2(~) a function ~(~). U. Banerjee [Bane76], [Bane79]has developed eff~cient algo\u00adrithms \nto determ~ne whether f(~) = g(~) for some ~ and ~ when both f and ~ are polynomials (which is often the \ncase). We do not know of any efficient ~lgori_thm to make such a determination when any of f or g is \na more general nonlinear function. In such cases, we are again conservative and assume that the flow \ndependence relation holds. Another problem is caused by the fact that some instructions may be executed \nconditionally; in this case, as before, we have to be conservative and assume the dependence when in \ndoubt. The fifth relation between components is that of input dependence. Definition A component Cl ie \nsaid to be ~ dependent on another component C2, denoted C2 61 Cl, iff the same variable name appears \nas input to both I Cl and C2. Notice that the 6 relation is sym metric. Input dependence is represented \nby arcs like the one shown in Fig. l(e). For some of the transformations described later, we will need \nmore information than that pro\u00advided by the dependence graph as described. This additional information \nwill be conveyed by the internal flow graph which describes the internal etructure of each component. \nThe internal flow graph of a Wh&#38;2 header will be the syntax tree of the Boolean expression in the \nheader. For an assignment statement, the internal flow graph will be a tree with the left-hand side variable \nas root and the syntax tree of expression as the only sub tree connected to it. For the arcs in the internal \n 209 flow graph, we will use the same type of arc used to represent flow dependence (Fig. l(b)) since \nthe concepts represented in both cases are the same. In uJ/L&#38;? headers and assignment statements, \nthis arc points towards the root. Example The W&#38;&#38; header uJh.i&#38;? (A(I) ~ C) v (B < 1) has \nthe following internal flow graph, The assignment statement ci-~~B~h~fiA+l (?IA(?c has the following \ninternal flow graph The internal flow graph of a dote header of the form ~CJk <index-variable> = 1 ZO \n<limiL> is as follows 1 <limit> A <index variable>  22!s where the I operator performs all functions \nof the ~Oh header, like assigning one to the index vari\u00adable the first time it is executed, and adding \none to the index variable and comparing the result with the limit on subsequent executions. The nodes \nin the internal flow graph repre senting variables or constants are called atoms. Because of this, a \ndependence graph including the internal flow graph is said to be represented at the atomic level. In \nsuch a graph, the data depen dence arcs (anti, output and flow) will emanate and arrive at the atoms \nthat cause these dependence. In Fig. 3, we show part of the dependence graph at the atomic level for \nthe program in Fig. 2(a). Later in the paper we are going to treat some compound statements as a single \nunit. For this purpose, we will name a compound statement with the label of the statement header. This \nmeans that such a label will have two functions; however, in the text the specific meaning of the label \nwill al\u00adways be clear from the context. In the graphic representation, a node representing a whole compound \nstatement will be represented by two concentric circles; such nodes will be called compound nodes. The \nconcepts of instance, dependence, sets of in\u00ad pute, and seta of outputs can be very easily ex\u00ad tended \nto deal with compound statements. In this paper, however, we will rely on the intuition of the reader \nand will not define such concepts. A final comment. Since dependence graphs may become quite complex \nwhen all arcs are drawn, we will represent only those arcs of interest in the examples discussed in the \nrest of the paper. 3. Arc Transformations In this section, we present some source pro\u00adgram transformations \nthat will modify the depen\u00addence graph by either removing arcs or breaking cycles. Theee transformations \nare renaming, ex\u00adpansion, node splitting, and forward substitution. 3.1 Renaming Sometimes scalar or \nstructured variables are used for different purposee at different points in a program. This is done sometimes \nto increase the readability of the program and often to decrease memory requirements. This approach is \nadequate for sequential programming. However, the use of the same memory location for different purposes \ncould impose unnecessary aequentiality constraints on parallel programs. The renaming transformation \nwill assign different names to different usea of the same variable, and as a consequence some out\u00adput \ndependence arcs and some antidependence arcs will be removed from the dependence graph of the program. \nExample 3.1 The program shown in Fig. Z(a) uses the variable A in three statements inside the ~o&#38; \nloop; this introduces a large number of arcs in the dependence graph (Fig. 2(b)). The variable A can \n(1) be replaced by two variables, A and A(2), as shown in Fig. 2(b). This eliminates several Out\u00adput \ndependence and antidependence arcs.  We now present an algorithm for renaming scalar variables. A powerful \nalgorithm for re\u00adnaming structured variables is an open problem. Renaming Algorithm for Scalar Variables \nAssuminx -, a program, P, and a scalar variable. sayA, in P. [1] Build G, the dependence graph of P at \nthe atomic level. [2] Consider G , the subgraph of G consisting of the intercomponent flow dependence \narea only (i.e., we drop all other arcs in eluding the flow dependence arcs in the internal flow graph). \nFind the connected components of G where A appears. Assume there are k such components C 1 C2 ... Ck \n. (1) [3] Introduce k different variable names A , A(2) A(k) ,..., , none of them used in P. For ~ ~ \ni ~.k, replace the occurrences of A in CibyA(l) . 210 Example 3.2 Part of the dependence graph G for \nthe program in Fig. 2 (a) is shown in Fig. 4. Since there are two connected components involving A, we \n(1) introduce two new variables, A and A(2) to obtain the program in Fig. 2(b). The concept of scalar \nrenaming has been known for a number of years [AhU173]. 3.2 Expansion Expansion is a transformation \nthat is not as well known as renaming (though it is implemented in the compilers for both the Burroughs \nBSP and the CRAY-1), but is of prime importance in compiling for parallel machines. The object of expansion \nis to take a variable that was used inside a ~OZ loop and to change it into a higher dimensional array \n(or other suitable data aggregate). Like renaming, this process reduces the number of arcs in the de\u00adpendence \ngraph. In this case, this is achieved by giving each iteration of the {OE loop its own set of locations. \nExample 3.3 The dependence graph in Fig. 2(b) includes many output dependence and antidependence (1) \n arcs because of the scalar va~iables A , A(2), and Y. If these variables are expanded into arrays by \nthe algorithm described below, we obtain the program in Fig. 2(c) whose dependence graph is much simpler. \nw We now describe an algorithm for the expansion of scalar variables. To this end, we will need three \ndefinitions. Definition A component or compound statement C is said to be directly @l loop dependent \non a @fi loop header F, denoted F ;L C, iff (1) F-dL C, and (2) there is no other {OZ loop header F \nsuch  that F6L F tiL C. Definition A sequence of ~ofi loop headers Fo, F1 , .,,, Finis said to form \na-iff Fi ;L Fi+l  O<i<m-1.  In other words, a sequence of ~oh loop headers forms a chain when their \nrespective loops are nested in the order indicated in the sequence, and there is no other ~ofi loop in \nthe nesting. Definition Given a component or compound state\u00ad . ment C, a ~OX loop header F such that \nF 6 L C, and a scalar variable V, we say that C forwards V to the next iteration of F iff (1) V is an \noutput variable of C; (2) there is no statement ~, with F 6L D, which appears after C in the text of \nthe 100p such that V is an output variable of D; (3) there is no wh,dh? loop header OJsuch that F dL \nW6L C; and (4) in the execution of the body of F, V could be fetched before it is modified.  m Intuitively, \nC forwards V to the next itera tion of a {ofi loop F if the only value given to V by C at any iteration \nof F is still the value of V when the next iteration of F starts, and that value could be used in that \niteration. Scalar Expansion Algorithm Consider an output variable, V, of a ~ok loop FO with dependence \ngraph G. In the algorithm, we will assume that V(I1, 12, ..,, Im) and V(I1, 12, . . . . I ,0, . . . . \nO),m~O, represent the same memorymlocation (if m = O we have V and V(O, . . . . o).) [1] For all components \nC inG, execute step [2]. Then go to step [5] . [21 Let Fo, Fl, . . . . Fm form a chain of {oh loop headers, \nsuch that Fm;L C. . . . . I be the index variables et 10 11 m of these headers. If V is an input variable \nof C, execute step [3]. If V is an output variable of C, execute step [4]. [3] Let n~mbe the largest \nnumber such that there exist a component ~ with Ln 6L o, and V is an output variable of ~. Replace all \noccurrences of V on the right\u00ad hand side of C by (1) V(IO-l, . . . . In-l-l, In) if there is a component \nV forwarding V to the next iteration of Ln such that O 6 C. (2) V(IO-l , .,., In_l-l, In-l) otherwise. \n [4] Replace the occurrence of V on the left-hand side of Cby (1) V(IO-l, . . . . Ire-l-l, Im) if C \nforwards V to the next iteration of Lm. (2) V(IO-l, . . . . Im_l-l, Ire-l) otherwise.  [5] For all \nloops ({ok or titik? loops) Lm inside Lo execute step [6]. Then go to step [7]. [6] Let Fo, Fl, . . . \n. Fm_l ~rma chain of {Oh loop headers with Fm_l 6L L If V is an m output variable of Lm, insert the assignment \nstatement V(IO-l, . . . . Im_2-1, a) = V(IO l, . . . . I m_l-l, 6) immediately after the end statement \nof L . m O if Lm is a uJh.&#38; loop Here .8 . max(uLm, O) if Lm is a ~oh loop with [ upper limit ul. \nm and I If Lm forwards V to the next m-1 iteration of Fm_l a I -1 otherwise m-1 [ [71 Immediately after \nthe Lnd {Oh of ~o, insert the statement V = V(ULO) where ugO is the upper limit of F m o As was the \ncase for renaming, a good expansion algorithm for array variables is an open problem. 3.3 Node Splitting \nThe node splitting transformation attempts to break cycles in the dependence graph by reposition\u00ading \nantidependence arcs. This is achieved through the introduction of new assignment statements. Example \n3.4 The dependence graph in Fig. 2(c) includes a cycle which can be broken if the arc representing .A4 \n6A A5 is repositioned. To do this, we split A4 into two assignment statements AL and A; as shown in Fig. \n2(d). Node Splitting Algorithm Consider a dependence graph G at the component level with the loop dependence \narcs removed, and a cycle C in G. [1] If the cycle C disappears when G is repre\u00adsented at the atomic \nlevel, then C includes an antidependence arc, say A, and the algo\u00adrithm can proceed to step [2]. If C \ndoes not disappear, stop. [2] If the arc A emanates from an atom a in component C, then introduce a new \nassignment statement of the form T + a, and replace all occurrences of u in C by T, where T is a variable \nnot appearing anywhere in the origi\u00adnal program. [3] Apply the expansion transformation to T. w Example \n3.5 Part of the dependence graph at the atomic level for the program in Fig. 2(c) is shown in Fig. 5. \nNotice that the cycle in the graph of Fig. 2(c) disappears in Fig. 5. To remove the cycle, we introduce \nthe statement T = X(1+1) and replace A4 by A 2)(1) = T +X(1-1). After T is ex\u00ad panded, we will obtain \nthe program in Fig. 2(d). The expansion and node splitting transformations, as discussed above, change \nthe source program by introducing arrays and assignment statements. The goals of these two transformations \ncan also be achieved by architectural means. Consider, for ex\u00adample, the DO ALL instruction of the Burroughs \nFM? multiprocessor [LuBa80]. Variables in the body of DO ALL are defined as local when they belong to \nneither the set of inputs nor the set of outputs of the DO ALL. A copy of all the local variables is \ncreated in the local memory of each processor be\u00adfore a DO ALL starts execution. This has the same effect \nas expansion. Also, all variables in the set of inputs of the DO ALL are fetched before the DO ALL starta \nexecution. This produces the same effect as node splitting.  3.4 Forward Substitution The forward substitution \ntransformation elimi\u00adnates flow dependence arcs from component level dependence graphs by substituting \nthe right hand side expression of an assignment statement, , into the right-hand sides of other assignment \nstatements. The main use of this transformation is that it could be applied before tree-height reduction \n[Kuck78], enhancing the result of this last trans\u00adformation. Example 3.6 Consider the program segment \nin Fig. 6(a). Assume that only the variable F is used outside the segment. After applying forward substitution, \nwe obtain the program segment in Fig. 7(a). The atomic dependence graph, if we assume that expressions \nare evaluated from left to right, ia shown in Fig. 7(b). However, if we do not assume any evaluation \norder and apply tree\u00adheight reduction, we obtain the dependence graph in Fig. 7(c), which is much better \nthan the graph in Fig. 6(b) from the parallel processing point of view. m Assume a set of consecutive \nassignment state ments A A An in a program P. To forward 1 2  substitute a given Ai whose left-hand \nside is a scalar variable, S, we proceed as follows. Scalar Forward Substitution Algorithm [1] Apply \nrenaming to all scalars on the right\u00adhand side of Ai. [2] For allAj, j > i, such that (1) Ai 6Aj (2) \nthere is noAk, i <k<j such that  Ai 6A Ak (since scalar variables have been renamed, always caused \nreplace the hand side of Sin A.. 3 this antidependence by an array variable), expression on the right-Ai \nby all occurrences is of [3] Apply dead code e limination [Grie71]. 4. Dependence Graph Abstraction \nGraph abstraction is a process by which a set of nodes and their internal arcs are merged into a single \ncompound node. Any arcs incident to (or from) the set are made incident to (or from) the compound node. \nGraph abstraction has been used in many areas of computer science. In particular, it has been used to \norganize optimization in several ways. For example, an interval is a graph abstraction [Cock70] used \nin data flow analysis. Graph abstraction has also been used to control the scope of optimization as in \nthe SIMPL optimizer of [ZeBa74], which optimizes structured blocks from the inside out. We use graph \nabstraction in yet a different way. Graph abstraction can be used to isolate sets of statements that \ncan be translated into high quality machine code only when taken as an ensemble. Two examples of this \ntype of graph abstraction will be presented. 4.1 LooP Distribution LooP distribution abstracts dependence \ngraphs by finding and merging each strongly connected com\u00adponent in the body of a loop along with the \nloop header node into a compound node. (A strongly connected component (SCC) is a maximal set of nodes \nsuch that there is a path,between any pair of nodes in the set.) Similarly, each loop body node not in \nany SCC, an independent node (IN), is merged with the loop header node into another compound node. Fig. \n8 shows how the node merging in loop distribu\u00adtion is performed. The following algorithm describes loop \ndistri\u00adbution. The most time-consuming step in the algo rithm is step 1, finding the SCCS. However, it \ntakes only o(n log n) time on a loop containing n statements if a depth-first algorithm such as Tarjan \ns algorithm is used [AhHU74]. This compares favorably with the fast data flow analysis algo rithms such \nas [GrWe76]. Loop Distribution Algorithm Consider a {oh loop F. whose body consists of the statements \n(simple or compound) S1, . . . . Sn. To distribute Fo, we proceed as follows. [1] Compute the dependence \ngraph G for ~. and 1   n [2] Delete F. and create a Jo/c loop header node Foj for each SCC, and each \nIN in the depen\u00ad dence graph. Make each statement in an SCC or IN loop dependent on the loop header as \nsociated with the SCC or IN, and flow depen\u00addent if the statement refers to the loop in\u00addex (i.e., a \n{ok loop is created for each SCC and each IN). [3] Build a new dependence graph by creating a compound \nnode for each ~ok loop. The loop distribution algorithm can implement several optimization, depending \non how the depen\u00addence graph is computed in step 1. We will give two specific examples. 4.1.1 Loop Distribution \nfor Vector Processors The first optimization uses loop distribution to generate vector operations from \nmultistatement loops . This is achieved by constructing a depen dence graph consisting of flow, anti, \nand output dependence arcs for the multistatement loop and performing the 100P distribution algorithm. \nThe dependence graph output from loop distribution in this case is called a p artial order graph, and \neach node in this graph is called a m-block. (The term m-block stems from the fact that loop distribution \npartitions the nodes in the graph into equivalence classes.) Two types of n-blocks are derived. m-blocks \nwhose bodies are INs represent vector operations> the goal of the optimization. m-blocks, whose bodies \nare SCCS, are called recurrences. (As a rule of thumb, there is approximately one recur\u00adrence per loop \nin scientific source programs.) Al\u00adthough recurrences, nonvector operations, are not the most efficient \noperations on vector and array processors, we have found that relatively few re\u00adcurrences are intractable. \nMost recurrences are SCCS connected by only flow dependence, primarily because loop distribution is applied \nafter several optimization which remove anti and output depen\u00addence arcs (Section 3). These recurrences \nare most often linear recurrences, such as the row sum of a matrix, which can be speeded up [Kuck78] \nbut are still slower than vector operations on vector processors. ([BcKT79] is a recent description of \nresults in this area.) Loop distribution applied to a linear recurrence in effect abstracts the SCC to \na single node representing a call to a linear recurrence solver. Other types of SCC that occur frequently \nare: Boolean recurrences which can be substantially speeded up [BaGK80], and simple non linear recurrences \n[Park77]. The partial order graph constructed in step 3 of the Loop Distribution Algorithm is a directed \nacyclic graph. It can be used to schedule the vector operations and recurrences on a parallel processor. \nThe longest chain in the partial order graph defines the minimum execution time for the original source \nprogram loop. The maximum width or anti-chain in the graph defines an upper bound on the number of processors \nthat can be used in parallel computation. Example 4.1 Loop distribution for vector processors produces \nthe program in Fig. 2(e) when applied to the program in Fig. 2(d). All state\u00ad ments become vector operations \nexcept for state\u00adments A4 and A5 which constitute a linear recur\u00adrence. In the transformed,program, the \nstatements are topologically sorted by the partial order graph.  4.1.2 Loop Distribution for Memory \nManagement A second application of loop distribution is in memory management; we call this name clustering \n[AbKL79]. Example 4.2 [AbKL79] Consider the program in Fig. 9(a). If each array referenced in this pro\u00adgram \nis on a distinct page, or distinct sets of pages, then the F1 loop requires 9 data pages to execute \nefficiently (with a minimum of page faults). After this type of loop distribution is applied, the transformed \nprogram (Fig. 9(b)) requires only 5 data pages to execute efficiently. Loop distri\u00adbution has improved \nthe program s data locality. The input to loop distribution for memory management is a dependence graph \nconstructed for flow, anti, output, and input dependence. Loop  distribution in this case does not use \nSCC but name clusters defined next. Therefore, in the algorithm above SCC should be replaced by name \ncluster. Definition The set of variables referenced in a set of statements is called a name set (NS) \nand is a function of some statement set (SS). We can also compute the set of statements referencing any \nvariable in a name set. Let SSO be any statement in a given loop. Call its name set NS and find the \nstatement set of NS m o o call it SS Loop distribution iterates this se\u00ad 1 quence until a stable statement \nset is found; this set is called a name cluster.  One might assume that in an average loop, the logical \nflow of the loop would connect all its statements into one name cluster; however, we have found that \nby using loop distribution the data page requirements of programs can be reduced by a factor of 6 [AbKL79]. \nLoop distribution for memory management can be compared with global register assignment (GRA) algorithms. \nSophisticated GRA algorithms such as [Beat74] generate roughly the SS of each variable referenced in \nthe loop. ( ROughly here implies that if at some point in the loop the variable is dead, then a new SS \nis started.) A register is allocated for each SS. Computing SSs requires a connected component computation. \nGRA algorithms in this class can reallocate a register several times within a loop, but clustering does \nnot reallocate page frames because of the relatively higher cost of page swapping.  4.2 Loop Fusion \nAs a graph abstraction, loop fusion is used selectively to merge two compound ~ofi loop nodes. Thus, \nit is nearly the inverse of loop distribution. But where loop distribution is applied globally to a loop, \nloop fusion is applied selectively. LooP Fusion Algorithm Consider two {ok loops FO, and F1 with the \nfollowing characteristics: (1) Both FO and F1 have the same loop limit. (2) F. and F1 are consecutive \nin the source  program with Fo aPPearing before F1 (If they are not consecutive, then try to make them \nconsecutive by moving the statements separating FO and F1 before FO or after F1 whenever possible,) \n[1] Let .s s be the statements in tEe 0.1  O.n body of Fo, and S1 ~, .... S1 m the state\u00ad ments in \nthe body of F1. Temporarily create a ~Oh loop containing SO ~, . . . . SO ~, .S1 ~, . . . . s ~ ~ (renaming \nthe index variable occur\u00ad rences if necessary), and compute its depen\u00addence graph G. [2] If G contains \nany arc fromsl i to so j for some 1<i <n, 1<j <m, then fusion is not poss~ble~ Othe wis=, replace the \nloops F. and F1 by a single loop containing the body of both loops In the past, loop fusion has been \napplied glo\u00ad bally to reduce the overhead of loop control [A1C072], [Love77]. We will show that by applying \nloop fusion selectively with different criteria, different optimization can be realized. Loop Fusion \nfor Virtual Memory Management Above we saw that loop distribution applied to the proper dependence graph \ncan reduce the data memory requirements of a source program. LooP fusion can be subsequently used to \nreduce unneces sary swapping. The criteria used to select pairs of loops for fusion in this case is that \nthe NS (Name Set is defined above) of one loop is con\u00adtained in the NS of the other. The following ex\u00adample \nillustrates loop fusion for virtual memory management. Example 4.3 If the program in Fig. 9(b) is in\u00adput \nto the loop fusion algorithm, the program will be transformed as shown in Fig. 10. In this case, the \nNS of each of the loops before loop fusion is: Name Set (NS) !K?.Q.P {A,B,c,G,H} 1 {D,E,F,x} F2 F3 {D,E,F} \n The NS for F3 is contained in the NS for F2, all conditions for fusion are satisfied, so ~ and 2 F3 \nare fused. Page swapping has been reduced because once a page of D, E, and F are loaded, all operations \nusing these pages are performed. H Loop Fusion for Vector Register Processors Processors having vector \nregisters such as the CRAY-1 present novel requirements for code genera tors. We will present a sequence \nof transformations which performs very well in this environment. @his sequence is contained in a vectorizer \nfor pipe\u00adlined machines described in [KKLW80].) LooP distribution for vector processors first isolates \nthe recurrences from the vector operations. Loop fusion is applied to increase the NS of each loop until \nit is as large as the number of vector registers available. o Loop blocking [AbKL79] transforms single \n~Ofi loops into doubly nested loops. The inner loop is set to the size of the vector regis\u00adters. The \nouter loop increments in steps of the register size through the original loop range. Loop interchanging \n[Kuck80] attempts to avoid 214 recurrences on the inner loop and to reduce memory register traffic by \ninterchanging loops when possible. . Register assignment assigns vector registers globally.. It also \ngenerates loads and stores at the entry and exit of each register alloca\u00adtion block. We present this \noptimization sequence at this junc\u00adture because of the role played by loop fusion in particular. The \ninitial loop distribution generates vector operations that are much larger than the register size. Loop \nblocking remedies that. How\u00adever, without loop fusion the outer loop overhead would have to be paid for \neach vector operation. At the same time, fusing all of the outer loops to\u00adgether may over-allocate the \navailable vec~or regis\u00adters. Therefore, the loop fusion criterion used in this case is whether the NS \nof the fused loop will be larger than the number of available vector regis\u00adters. Once fused loops with \nNS approximately as large as the available registers are created, regis\u00adter assignment need not be as \ncomplex. (We might label it a global few-to-few assignment strategy, using Day s terminology [DayW70]. \nThe only other published method to optimize vector register as\u00adsignment is found in [DUKU78], which describes \na global assignment based on usage counts with no re\u00adallocation of vector registers. 5. Conclusion The \ntechniques of this paper have been imple\u00admented and used in compiling to improve the per\u00adformance of \nordinary programs for several architec\u00adtures. The transformations of Section 3 remove de\u00adpendence arcs \nand hence increase independence be\u00adtween nodes, while those of Section 4 abstract from a graph parts \nwhich are convenient for code genera\u00adtion. By removing cycles, they yield a DAG that also is convenient \nfor scheduling. It is important to realize that the resulting graph usually consists of many nodes that \ncan be compiled directly into the machine languages of cur\u00adrent high-performance machines. Array operations \narise from independent nodes after loop distribu\u00adtion. The recurrences arising from strongly con\u00adnected \ncomponent cycles are often linear. Many ms\u00adchines have reduction instructions and some (e.g., BSP) have \na more general linear recurrence solving instructions or functions. The remaining parts of a dependence \ngraph must be executed using scalar in structions. Application of these ideaa to machines that can execute \nmanv scalar operations at once is discussed in [PaKL80j. References [AbKL79] W. Abu-Sufah, D. Kuck, and \nD, Lawrie, Automatic Program Transformations for Virtual Memory Computers, Proc. of the 1979 Nat l. Computer \nConf., pp. 969 974, June 1979. [AcDe79] W. B. Ackerman and J. B, Dennis, Val--A Value Oriented Algorithmic \nLanguage: Preliminary Reference Manual, Lab. for Computer Science (TR-218), MIT, Cam\u00adbridge, MA, June \n1979. AhHU74j A. V. Aho, J. E. Hopcroft, and J. D. Unman, The Design and Analysia of Computer Algorithms, \nAddison-Wesley, MA, 1974. AhU173] A. V. Aho and J. D. Unman, The Theory of Parsin g, Translation, and \nCompiling, vol. 2: Compiling, Prentice-Hall, Englewood Cliffs, NJ, 1973. [AlCo72] F. E. Allen and J. \nCocke, A Catalogue of Optimizing Transformations, in Design and Optimization of Compilers (R. Rustin, \nEd.), Prentice-Hall, Inc., NJ, pp. 1-30, 1972. ArGP78] Arvind, K. P. Gostelow, and W. Plouffe, An Asynchronous \nProgramming Language and Computing Machine, University of California at Irvine, CA, Dept. of Information \nand Computer Science Rpt. l14a, Dec. 1978. AsMa75] E. Ascroft and Z. Manna, Translating Program Schemes \nto While-Schemas, SIAM J. on Computing, Vol. 4, No. 2, pp. 125-146, June 1975.  [BaGK80] U. Banerjee, \nD. Gajski, and D. J. Kuck, Array Machine Control Units for Loops Containing IFs, Proc. of the 1980 Int \n1. Conf. on Parallel Processing, Harbor Springs, MI, pp. 28-36, Aug. 1980. [Bake77] B. S. Baker, An Algorithm \nfor Structur\u00ading Flow Graphs, J. of the ACM, Vol. 24, No. 1, pp. 98-120, Jan. 1977. [Bane76] U. Banerjee, \nData Dependence in Ordinary Programs, M.S. thesis, Univ. of Ill. at Urbana-Champaign, Dept. of Comput. \nSci. Rpt. No. 76-837, Nov. 1976. [Bane79] U. Banerjee, Speedup of Ordinary Pro grams, Ph.D. thesis, Univ. \nof Ill. at Urb. Champ. , Dept. of Comput. Sci. Rpt. No. 79-989, Oct. 1979. [BCKT79] U. Banerjee, S. C. \nChen, D. J. Kuck, and R. A. Towle, Time and Parallel Proces\u00adsor Bounds for Fortran-Like Loops, IEEE Trans. \non Computers, Vol. c-28, No. 9, pp. 660-670, Sept. 1979.  [Beat74] J. C. Beatty, Register Assignment \nAlgo\u00adrithm for Generation of Highly Opti\u00admized Object Code, IBM J. of Res. and ~. , Vol. 18, No. 1, \npp. 20-39, Jan. 1974. [BoJa66] C, Bohm and G. Jacopini, Flow Diagrams, Turing Machines and Languages \nwith Only Two Formation Rules, Comm. of the ACM, Vol. 9, No. 5, pp. 366-371, May 1966. [Cock70] J. Cocke, \nGlobal Subexpression Elimina\u00adtion, SIGPLAN Notices, Vol. 5, No. 7, PP. 20-24, 1970. 215 [DayW70] W. \nH. E. Day, Compiler Assignment of Fig. 1. Five types of dependence graph arcs Data Items to Registers, \n IBM syst~. J_. , Vol. 9, No. 4, pp. 281 317, 19700 (a) loop dependence .  -3 [DuKu78] D. D. Dunlop \nAllocation and in J. the C. Knight, Register SL/1 Compiler, ~. (b) flow dependence > of the 1978 LASL \nWorkshop on Vector &#38; (c) output dependence A > parallel AlamOs, processors, LA-7491-C, NM, pp. 205-211, \nSept. Los 1978. (d) (e) antidependence input dependence A ,, T > > [Grie71] D. Gries, Compiler Construction \nfor Digi tal Computers, Wiley &#38; Sons, NY, 1971. [GrWe76] S. L. Graham and M. Wegman, A Fast and \nUsually Linear Algorithm Flow Analysis, J. of the No. 1, pp. 172-202, 1976. for Global ACM, Vol. 23, \nFig, 2. Successive to a program application of four transforms [KKLW80] D. J. Kuck, R. H. Kuhn, B. Leasure, \nand F1 : 60LI=l,toN ,, M. Wolfe, Analysis and Transformation Al: A=A+l 3 2 of to Programs appear in \nfor Parallel Proc. of the Computation, Fourth Int 1. A2 : Y=A+2 A . .\u00ad F-h / Computer Software Oct. \n1980. &#38; Applications Conf., A3 , Z(I) = Y +V(I) / \\ [Kuck78] D. J. Kuck, The Structure of Computers \n 4: A5 : A = X(I) X(1+1) = W(I) + X(1-1) + 1 @ A A . and computations, Sons, Inc. , NY, Vol. 1978. I, \nJohn Wiley &#38; A6 : W(I+l) = X(I) + 1 Original program cnd JOJL 2 (a) [Kuck80] D. J. Kuck, Class Notes \nfor C.S. 433, Univ. of Comput. Ill. Sci. , at Urb.-Champ. 1979. , Dept. of F1 : Al: (joJLI=l,toN A(l) \n= A(2) + ~ [KuMC72] D. J. Kuck, On the Y. Muraoka, and S. Number of Operations C. Chen, Simulta A2 : \nY=A(1)+2 neously Executable in FORTRAN-Like A3 : Z(I) = Y+V(I) Programs and Their Resulting IEEE Trans. \non Computers, Vol. Speed Up, C-21, A4 : A(2) = X(1+1) + X(1-1) No. 12, pp. 1293-1310, Dec. 1972. A5 : \nX(I) = W(I) + 1 [Love77] D. B. Loveman, Program Improvement by 6: W(I+l) = X(I) + 1 After renaming \nSource-to-Source Transformation, ~. atd ~Oh 2(b) of the ACM, Vol. 20, No. 1, pp. 121\u00ad 145, Jan. 1977. \nF1 : ~okI=ltoN [LuBa80] S. F. Lundetrom and G. H. Barnes, trollable MIMD Architecture, A Proc. Con\u00adof \nAl: A(l) (l_Q = A 2) (1-1)+1 the 1980 Int 1. Conf. on Parallel A2 : Y(I-1) = A(l) (I-1)+2 processing, \npp. 19 27, Aug. 1980. A3 : Z(I) = Y(I-1) + V(I) [PaKL80] D. A. Padua, High-Speed pilation D. J. Kuck, \nand D. H. Lawrie, Multiprocessors and Com-Techniques, Special Issue on 4: A5 : A(2) X(I) (I) = = X(1+1)+X(1-1) \nW(I) + 1 Parallel Computers, Processing, Vol. c 29, IEEE No. Trans. 9, pp. on 763\u00ad 6: W(I+l) = X(I) + \n1 After expansion 776, Sept. 1980. [Park77] D. S. Parker, Jr., Nonlinear Recurrences and Parallel Computation, \n in High speed F1 : fjoxI=l-toN Computer and pp. 317 320, Algorithm Academic Organization, Press, Inc., \n1977. Al: A(l) (I-l) = A 2)(1-1)+1 2: Y(I-1) = A(l) (I-l)t2 [ZeBa74] M. V. Zelkowitz and tion of Structured \nW. G. Bail, Programs, Optimiza-Software A3 : Z(I) = Y(I-1) + V(I) %Ctice and Experience, Vol. 4, No. \n1, A;: T(I-1) = X(1+1) pp. 51-57, 1974. A;: A(2) (I) = T(I-l)+X(I-1) A5 : X(I) = W(I) + 1 ~ 6: W(I+l) \n= X(I) + 1 After ting node split end fjOk 2(d) 216 Fig. 5. Partial dependence graph at the atomicF~1: \n@JLI=l ZON a level for the program in Fig. 2(c)1.1 o A; : T(I-1) = X(1+1) F A5 : X(I) =W(I) + 1 1.2 \n0 A6 : W(I+l) =X(I) + 1 end {OZ /=~3: ~oh I=lfON F 1.3 A; : A(2) (I) = T(I-l)+X(I-1) @ end ljofi F14: \nfjoh 1=1 XON ) Al : A(l) (1-1) = A(2) (1-1)+1 Fig. 6. Original program for Example 3.6 1.4 end ~ox 0 \nF~5: ~Ofi 1=1 ZON Al: A=B+C A2 : Y(I-1) = A 1) (1~1)+2 B c A2 : D=E+A end 40L 1.5 Al + F16: ~OXI=l ZON \no A3 : F=G+D A3 : Z(I) = Y(I-1) + V(I) A . 8 (a) ad 60L F 1.6 Q do After distribution EA 2 (e) A2 \n+ Fig. 3. Partial dependence graph at the atomic D level for the program in Fig. 2(a) m GD A3 + F \n6i5 (b) Fig. 4. Partial dependence graph at the atomic level for the program in Fig. 2(a) for Example \n3.6 and Fig. 7. Transformed program twO possible atomic dependence graphs A;: F=B+C+E+G B c + (a) E \n+ G + Y (b) F1 : dOXI=l,tON Al: A(I) = B(I) + C(I) Scc IN before abstraction (a) replicated loop header \n(b) resulting abstracted graph (c) Fig. 8. LOOP distribution as a graph abstraction A2 : D(I) = E(I)+F(I)+X(I) \nF1 _ Al A3 : G(I) = B(I) + H(I) QB wd fjOk \\ A F2 : @L~=~&#38;oN \\3 Ah: \\ E(J) = D(J) * F(J) A2 cnd \n~ofi F2 A4o\u00ad-~ Original program (a) F~1: fjcJtI=lZON F1 A Al , A(I) =B(I) +C(I) 1 Q \\ \\ 43 : G(I) \n= B(I) + H(I) . e.vtd fjO)L A3 8 F12: If OX 1=1 XON A2 , D(I)= E(I)+F(I)+X(I) F   1.2 AZ cnd dot \no F2 : ~O&#38;J==l~ON E(J) = D(J) * F(J) 4 : oF2 _A4 End ~OJL x Transformed program (b) Fig. 9. \nLOOP distribution for memory management ~1: fjO&#38;I=l~CIN F1 AlAl: A(I) = B(I) + C(I) Q \\ A3: \\ \nG(I) = B(I) + H(I) \\ wd fjOk A3 F2 : ~OfiI=lxON 8 A2 : D(I) = E(I) + F(I) + X(I) A4 : E(I) = D(I) * \nF(I) end ~O&#38; Q-Q \\  \\ 6 A4 Fig. 10. LOOP fusion for virtual memory management 218  \n\t\t\t", "proc_id": "567532", "abstract": "Dependence graphs can be used as a vehicle for formulating and implementing compiler optimizations. This paper defines such graphs and discusses two kinds of transformations. The first are simple rewriting transformations that remove dependence arcs. The second are abstraction transformations that deal more globally with a dependence graph. These transformations have been implemented and applied to several different types of high-speed architectures.", "authors": [{"name": "D. J. Kuck", "author_profile_id": "81406594662", "affiliation": "University of Illinois at Urbana-Champaign, Urbana, Illinois", "person_id": "PP77042379", "email_address": "", "orcid_id": ""}, {"name": "R. H. Kuhn", "author_profile_id": "81100443732", "affiliation": "University of Illinois at Urbana-Champaign, Urbana, Illinois", "person_id": "PP40027319", "email_address": "", "orcid_id": ""}, {"name": "D. A. Padua", "author_profile_id": "81452612804", "affiliation": "University of Illinois at Urbana-Champaign, Urbana, Illinois", "person_id": "PP40028927", "email_address": "", "orcid_id": ""}, {"name": "B. Leasure", "author_profile_id": "81100137872", "affiliation": "University of Illinois at Urbana-Champaign, Urbana, Illinois", "person_id": "PP14058921", "email_address": "", "orcid_id": ""}, {"name": "M. Wolfe", "author_profile_id": "81100031703", "affiliation": "University of Illinois at Urbana-Champaign, Urbana, Illinois", "person_id": "PP132031061", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/567532.567555", "year": "1981", "article_id": "567555", "conference": "POPL", "title": "Dependence graphs and compiler optimizations", "url": "http://dl.acm.org/citation.cfm?id=567555"}