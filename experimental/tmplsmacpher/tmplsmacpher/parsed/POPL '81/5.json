{"article_publication_date": "01-26-1981", "fulltext": "\n Permission to make digital or hard copies of part or all of this work or personal or classroom use is \ngranted without fee provided that copies are not made or distributed for profit or commercial advantage \nand that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, \nto post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. &#38;#169; \n1981 ACM 0-89791-029-X $5.00 Consider the computation of an APL expression. When arrays are large, each \nexpression s execution represents a large quantity of paged computation and, moreover, the rules for \nformulating a computation are available from the APL semantics in terms of the com\u00adplex primitives. Thus, \nwith the other paging parameters given, such as the number of available frames, we can seek techniques \nthat will generate an efficiently paged organization for an expression. An APL program s computation \ngenerally consists of relatively few expres\u00adsion executions, each of which performs a large portion of \nthe program s computation. So an efficient technique for paged expression execution provides an efficient \ncomputation for the entire program +. This approach is clearly not limited to APL, but could apply to \nany functional programming language, such as SETL [23], where expression operators can have large data \ncomputations and the semantics of these computations are defined at the operator level. Such an approach \nwould be extremely useful in a personal computing sys\u00adtem where all memory resources are assigned to \none user and which sometimes are single language pro\u00adcessors. This would expand the computation power \nof small processors where often memory size is the limit\u00ading resource. In summary, our problem is to \ndevelop techniques that transform an APL expression into an efficiently paged implementation, where the \nimplementation includes the necessary paging functions. This paper presents an overview of our methods \nfor performing this task. The paper begins by considering APL computation at the scalar level, examining \nprevious studies, and by presenting a model of APL operator and function [161 computation which relates \nan execution to its data requirements. Paged computation begins with an exam\u00adple APL operator execution \nthat serves as a reference for the remaining discussion. A data structure, called a uniform multi-level \narray (UMLA), is then introduced; this structure describes array storage as it is logically used in our \npaged computations, and it proves to sim\u00adplify the task of selecting an implementation and deter\u00admining \nwhat paging must be done. Then the data access needs for an APL computation are examined, first at the \nt It is possible, of course, for a user to write APL programs that consist only of scalar level operations \nand that are sub\u00adject to the difficulties noted earlier. However, such a program does not follow the \nintended use of the language. scalar level and then at the paging level. A scheme for defining data \nselection during computation is defined. This scheme is similar to models developed for APL optimization \nat the scalar level [1,15,21]; how\u00adever, as seen in the subsequent section, it also shows needs at the \npaging level. Using the properties of APL computation and our structures for representing computation \nat the paging level, we present an over\u00adview of the overall method. 2.0. How to Execute an APL Operator. \nOur examination of paging APL begins by looking at the characteristics of the scalar computation in APL. \nAPL scalar computation has been studied to develop methods for optimizing and compiling APL [1,15,18,21,24]. \nThese studies have utilized the proper\u00adties of APL computation, particularly the use of data in APL operators \nand functions, and, since paging require\u00adments are ultimately determined by scalar data needs, we have \nused the ideas in these studies in seeking our paging techniques. For reference in our discussion we \nuse a simple example expression. Consider the APL expression C+ A+~B (1) where A is a 4X 12 array, B \nis a 12x 4 array and the result C will be a 4X 12 array. This expression s compu\u00adtation simply adds the \nmatrix A to the transpose of the matrix B, element by element, producing the matrix C. A naive APL interpreter \nwould execute expression (1) as two array operations, one transposing B into a temporary array and the \nsecond adding this temporary array to A to produce C. An optimizing interpreter or compiler [1,15,18,21,26] \ntries to eliminate construc\u00adtion of temporary arrays between operators and scalar computations that do \nnot contribute to the output. Such a processor performs this expression as a single scalar product operator \n[161 with the transpose performed implicitly by pipelining (or streaming [15]) it into the data access \nof B. This optimized approach to execution is particularly appealing in our situation since the paging \ncosts for the temporary array would be elim\u00adinated. We treat operators (and functionstt) like other tt \nTO simplify the exposition we slightly misuse the term operators by using it to mean both APL operators \nand APL functions. researchers in APL processing, where expressions such as C ~ A + QB simply represent \nan instance of the scalar product operator. That is, to say, an operator is viewed as the natural control \nunit of APL where the operator s operands can be specified by an expression of selection operations, \n such as transpose, that transform the data access. Under this view the task of evaluating an optimized \nAPL expression is essen\u00adtially: (1) Partition the expression into subexpressions where the computation \ncan be pipelined. (2) Within each subexpression, transform the expres\u00adsion into an expression of operators \n(moving data selection down the tree ). (3) Execute each subexpression in order, pipelining the data \nuntil the value of the subexpression is computed.  Traditionally, certain operations like grade-up \nwill break the streaming [15] and they define the subex\u00adpression points for step (1). Implementation \nof step (2) varies. It can be done implicitly by a delayed evaluation scheme or explicitly performed \nwith transformations on data access, such as in Perlis Ladder Method [211 or the scheme of Guibas and \nWyatt [15]. Our problem must also consider in the number of available frames for step (1), along with \nhow they are to be utilized. In this paper our concern is how to execute the. pipe\u00adline subexpressions, \nby concentrating on operator computations, but we do not consider how best to tiivide up the expression \nfor the available resources. There is a basic computation scheme that we use to characterize the semantics \nof an APL operator: (a) An APL operator is a sequence of scalar functions that are repeated over a range \nof index values. (b) There is a driver array that specifies the ranges of index values, where each scalar \nfunction execu\u00adtion corresponds to a driver index. The driver is used to indicate the ordering of the \nscalar functions -the semantics generally provide for many legal orderings for step (a). (c) For each \noperand array, there is a data selection function which, when applied to the driver array in\u00addex, gives \nthe operand s constituent in the scalar function (a).  For our example expression (1) we would have: \nScalar function: Cij + Au + Bji for 1< f< 4 and l<j<12. dliVe.r Way: 4X 12 array; one legal order corresponds \nto row-major order of the driver (this is ravel order). For scalar product any ordering is legal. data \nselection functions: array A: fA (<i,j>) = <i,j> array B: f~(,<i,j>) = <j, i> array C: fc(<i,j>) = <i,j> \nwhere &#38;. denotes the mapping from driver to arraY x indices, indicating the element selected. The \nbasic concept behind viewing the driver as an array is that each axis represents a traditional loop index \nand the sequence of driver elements (i.e. ,the computation sequence) in some way corresponds to the @xis \nstructure. For scalar product, any driver sequence represents a legal computation. However, with some \noperators (e.g. scan), there are constraints on the order pnd even on the range of index values. Other \nresearchers with APL have had similar viewpoints of APL computation. The resulting computa\u00adtion in Abrams \nAPL machine [1] behaved in this way. The closest designs are Perlis Ladders [18,211 and the coordinate \nladder of Guibas and Wyatt [15], except that in these models the driver structure was given as a set \nof nested loops rather than an array. In these studies, ravel order (i.e., row-major order on the driver) \nwas the only computation sequencing considered. 3.0. An Example of Paged APL Operator Execution. An \nappealing approach to paged computation in APL is to extend the concept of optimized scalar computation \nso that each operator efficiently computes a page at a time, rather than a scalar, and this page is pipelined \nto the other operators in the expression. There are many additional considerations, but we can see the \nbasic idea with an example, TO begin we must choose some storage organiza\u00adtion, i.e. pagination, for \neach of the arrays. Then for a paged computation, scalar computation sequence must be selected and the \npoints at which to pull and push pages must be defined, along with what frames in which to place them. \nGiven the storage organization, the potential effectiveness of a page management strategy depends highly \non our choice of scalar computation sequence. In our computation scheme for APL, the data selection functions \n(which are defined by the semantics) along with our selection of driver sequence completely 1234 5678 \n9 10 11 12 1314151617181920 212223 24 25 26 27 28 293031 32 33 3435 36 37383940 41 424344 45464748 (a) \nArray A. 1234 5678 9 10 11 12 13 14 15 16 17 18 1920 21 22 23 24 25 262728 293031 32 33343536 37 38 \n3940 41 4243 44 45 4647 48 (c) Array C. Fig.1: Paged arrays for C~A define what (and when) pages are \nneeded for the com\u00adputation. Consequently, the properties of a driver and the data selection functions \nat the paging level could suggest a reasonable strategy for page management. Returning to our example \nof C-A +?PB, we can see how driver could be set up to control the computation. Assume that arrays A and \nB are paged as in Fig. 1. We are at liberty to select the pagination of C and let us select it to be \nas shown in Fig. 1, identical to that of A. Note that the small value of 8 is used only to simplify examples; \nour only assumption on the page size is that it isapower of 2. The array elements in Fig. 1 are given \nby their row-major order number (l-origin) and the lines indi\u00adcate the page boundaries. The paginations \nin Fig. 1 are forms of the BOX representation [7,8] which represents an array as uniformly shaped blocks \n(subarrays) that are the pages. In a BOX pagination the shape of the page blocks is sized to equal the \npage size, so all side lengths of a page block are a power of 2. With BOX, there are generally many valid \nrepresentations in which an array can be paginated. The shape of the page block is generally used to \ndescribe a specific representation, e.g., the arrays in Fig. 1 are 2X 4-BOXes and a page is identified \nby the index giving its position at the block level array (element 5 of A is in page < 1,2>). We commonly \nuse balanced BOX representations, where a BOX is balanced when the ratio of every pair of side lengths \ndoes not exceed 2. Assuming the storage organizations of Fig. 1, we must choose some legal computation \nfor our expression (1) where paging can be efficiently managed. Previous 1234 5678 9 10 11 12 13 14 15 \n16 17 18 1920 21 222324 25 2627 28 293031 32 33 343536 37383940 41 4243 44 H 45 46 47 48 H (b) Array \nB. +QB (page size = 8), studies in improving paging performance [4,10,12,17,19,20,22,24] suggest that \nthe desired sequence will maximize the use of each page once in core having a high locality [3,4,9,10,17,20,24] \nof page use, We will try to find a sequence with this pro\u00adperty by examining how the page structure of \neach operand corresponds with the driver. On the 4X 12 array driver, the selected data pages from arrays \nA and C are 2X 4 blocks. For array B of the transpose (Q) theeffect selection is to transpose the array \nof pages and to trans\u00adpose each page-array; consequently, B s pages correspond to 4X 2 blocks on the \ndriver. Figure 2 shows the driver s viewpoint of the first page of each operand. 15 1234 1234 26 13141516 \n13141516 37 48 C page. A page. @B page. Fig. 2: Driver viewpoint of first operand page. Knowing the data \nselection functions for our exam\u00adple, we observe that a 4X 4 block in the driver matches up the page \nshapes of the operands, making the data needed to compute the scalar functions represented by a 4X 4 \ndriver block be completely contained in a pair of pages from each operand. Assuming that 6 frames are \navailable, we can perform the computation efficiently by sequencing the driver in 4x 4 matched up blocks. \nFigure 3 shows the contents of the page frames for the first 4X 4 driver block. C frames A frames B \nframes Ew%zlm Fig. 3: A 4x4 matched up block for C + A + Q B. The example computation, complete with \npaging, is straightforward. Two frames are assigned to each operand. Then for each 4X 4 driver block \nwe fetch the input pages for the block, execute the acaiar function steps for that driver block, and \nthen push the completed pair of output pages (reusing the same set of frames). The performance of the \nresulting computation is optimal since each input page is pulled oniy once and each out\u00adput pushed only \nonce. The significant property of our example computa\u00adtion is that the driver and its correspondence \nto the operand s pages is used to design the entire procedure. The driver s blocks define the points \nwhere paging must occur and the data access functions determine what pages would be needed. This is the \nbasic scheme we wish to apply every operator. That is, find a driver whose blocks represents sections \nof each operand with high locality, e.g., matched up blocks, which can be constructed to function within \nthe available number of frames. Then, after selecting the driver, we compute the operation by sequencing \nthe scalar function steps in driver blocks, pulling the input pages and pushing (or pipelining ) the \noutput pages appropriately, at the start and end of each block. Clearly, to apply such a scheme for paging \nAPL operators in general many issues must be considered. Most significantly, we must be able to select \nsome block structure for the driver with each operator, that will satisfy the requirements of our scheme \nand, by using our scheme, page the computation efficiently. From the simple example above, it is certainly \nnot clear that such a structure can be found for all the potentially complex data access functions in \nAPL. Other complica\u00adtions, such as operating in the number of available frames must also be considered. \nAs we shall see, the primary problem rests in formulation of block com\u00adputations for the APL operators. \n4.0. Representing Logical Storage Use. In our sample computation, the paging functions were easily specified \nby viewing the arrays as represented in blocks. Paging was performed at each driver block and the pages \ntransferred were those con\u00adtaining the operand data in the block corresponding to the driver. We define \na generalization of the concept block representations, called a uniform multi-level array, which gives \nus a data structure to describe such array representations. Using this structure we can for\u00admulate simple \ntransformations for finding the pages that will be needed for a block of a paged APL computa\u00adtion in \ngeneral. Informally, a uniform multi-level array (or UMLA) represents an array s storage as a multi-level \narray where at each level arrays have the same shape; how\u00adever, our notation is unusual. Consider, ~for \nexample, the pagination of array A shown in Fig. 1; the 2-level array that represents A s elements in \npages is a 2X 3 array of 2X 4 arrays. Since the arrays at each level are uniform in shape, the shape \nof this multi-level representation can be specified with a matrix where each row gives the shape of the \narrays at the corresponding level. For the pagination of A in Fig. 1, this shape matrix is ~ ~. II The \nposition of an array s element under an UMLA representation is defined by the index at every level in \nthe representation. Like the UMLA shape , this index can be specified with a matrix whose rows con\u00adtain \nthe index values corresponding with the levels. For example, A s element <3,6> (rmo 30) is in page <2,2> \nat offset <1,2> in the page, so the index matrix is ~ ~ . [1 The matrix notation is convenient. To define \nan array as partitioned into a certain block shape, construct the top row of a shape matrix by dividing \neach array axis length by the respective block side lengths and then the second row is simply equal to \nblock shape+. Conse\u00adquently, in the shape matrix, the product of the terms on each column equals the \ncorresponding axis length in the array. The relation between this index matrix and an element s usual \nindex is a form of mixed radix number system -identical, in each column, to that used for a row-major \nordering with tIf necessary, the array length is < padded to make the d&#38; i\u00ad sion possible. the radix \ngiven by the shape matrix. With the above example we have: < [(2 1)x2+(1-1)1+1, [(2 1)x4+(2 1)1+1> = \n<3,6>. To define a uniform multi-level array (UMLA) in general let A be an APL of shape nlx  x n,, \nlet D=<dI, ..., d,> be a vector of integers and let I= <I,,... ,1, > be the array index of an element \nin A. A dx r matrix of non-negative integers S is a uni\u00ad form multi-level array ( UMLA) with starting \npoint D for A when for every axis j, 1< j < r: nj = IISij (2) i= 1 The S-index of the element Z of A \nis a dx r matrix J of non-negative integers where for every axis j, l~j~r, we havet: Ij= [f (Jjj l)x \n~ Skj +dj. (3) i= 1 k=i+l I For the UMLA S we refer to d as the depth and r as the rank. For both an \nindex and shape matrix, the rows are also called levels and the columns axes. The starting point D is \nusually the array origin (<1, . . . . 1>) and it is assumed to be this value unless otherwise specified. \nA depth 2 UMLA describes an array of blocks (or array of arrays ) representation with index matrices \nof the elements in a block having the same top row (whose value is the block index ). A depth 1 UMLA \nis simply conventional indices and an UMLA of depth 3 would represent an array of arrays of arrays. The \ngeneral multi-level array form that is implied by our UMLA representation has been called a uniform array \nby Ghandour and Mezei [14]. Floyd [13] used a structure similar to an UMLA in discussing matrix operations. \nFor our scalar product example the use of data by the operands can be given with an UMLA. For arrays \nA and C the UMLA 1~ ~1 represents data use and for B we have ~3 ~ . 1 For each-operand the second-level \narrays [1 of these respective UMLAS define the data that is accessed in the blocks for the computation. \nIn addition, the driver for the computation in the example can be defined with an UMLA, in this case \nequal to that for A and C. Using principal array ordertt [7,14] on the t Note X and II over empty ranges \nhave their usual interpre\u00adtation of O and 1, respectively. t t Principal order is a multi-level row-major \norder where driver UMLA produces the sequence used in the example. With our computations, an UMLA will \nrepresent the way an array is referenced, in blocks of that UMLA as in the scalar product example. Although \nwe use UMLAS with different depths, the storage usage for any one operand array can always be reduced \nto a 2-level viewpoint. Our BOX paginations can also be represented by UMLAS and, consequently, we can \ndetermine what to page by finding what pages are referenced in a computation block. The fact that the \nUMLA index computation (3) is a mixed radix encod\u00ading simplifies this task. Let S with starting point \nD be the UMLA representing the data use of an operand array A used in a computation. Let XIX  . XX, \nbe the shape of the blocks in S (the second row) where paging is needed. Say that A is paged with a WIX \n. . . x w,-BOX. It will be the case that, for each axis j, Xj is a power of 2 and generally Xj > Wj. \nAssuming this, the indices of the pages needed for computation block 1= <11, . ...1,> are given by the \nlimits aj < Kj < ,Bj for each axis j, where: aj zjx~+ w +1 Wj J II (dj_l) pj=(zj+l)x++ + (l ej) WjJ \nII where Ej is 1 if dj = 1 and O otherwise. These rules are found directly by transforming the UMLA index \ndefinition (3). Observe the number of pages needed is the product of the ratios of the compu\u00adtation block \nside lengths to the side lengths in the pagi\u00adnation, plus a factor if D is not origin. Returning to our \nscalar product example, let Z=< ZI,Z2> be the block (top-level) index in the driver then the pages for \nZ are ttt each level is sequenced in row-major order. for A and C: <111-lJ+2+n J2> for B: <[12-lj+2+rI \n,z~> where 1G n <2. The above calculation assumes that computation block is not smaller that the pagination \nside on every axis, i.e. Xj > Wj. The only exception to this rule occurs with diagonal transpose. With \ndiagonal transpose only the elements along the diagonal of the pagination block are ever referenced, \nand if the diagonalized sides do not have the same pagination side lengths, the shortest will be used. \nIn this case the limits defining the pages are given by: Zjxxj + (dj l) ~j= +1 Wj II ~, = (Zj+l)xxj _ \n1 + (dj_l) +1 J I j I for every axis j. For example with the array A in Fig. 1, the compu\u00adtation of \n1 1~ A only references 2 positions in each page. In this case the computation block length could be 2 \nwhile the pagination on the first axis is 4. UMLAS of depth greater than 3 are used to describe more \ncomplex orderings in computation. For example, our example scalar product computation could be performed \nin 4 frames if we subsequence the 4x 4 block computation by 2X 4 blocks and page A and C with each 2X \n4 block along with paging B with each 4X 4 block. The desired driver for this computation is the UMLA \n\\ ~ which represents a 1X 3 array whose entries [1 are 2X 1 arrays of 2X 4 arrays. Another view of this \nUMLA is that it defines 2 block representations, a 2x4 block and a 4X 4 block, and for our application \nthis view is useful. For an UMLA in general, we can view each level as defining a block representation \nin the array. In usual terms, the shape of the level k blocks defined by an UMLA is the columnwise product \nof the bottom d k rows in the UMLA shape matrix. To transform a UMLA S s index into the equivalent two-level \nblock index at level k, simply encode the rows 1 to k of the S-index using the rule (3) with rows 1 to \nk Of s, We commonly view UMLAS as representing blocks at each level. The paging of an operand array willl \nbe per\u00adformed with respect to some level in the driver UMLA, which we identify as the paging level for \nthe operand. Consequently, our calculation of pages above will apply in general. 5.0. Data Access Function \nSemantics. The previous section provided transformations for calculating the pages needed for a block \nof data in an ar\u00adray. To discover how to construct sequencing that will access the operand arrays in \nblocks, we must examine the characteristics of the data selection functions for APL. We first consider \ndata access at the scalar level, defining a data selection scheme that can describe the data selection \nfunctions in APL. Later, in the next sec\u00adtion, we will see how this scheme can be extended to describe \ndata access characteristics at the paging level. Our model is similar to Perlis ladder parameters [21] \nand the universal selector of Guibas and Wyatt [15] but it is more general since it allows for extending \narrays (which can be done with the take operation). Let A be an array of shape nlx 00. xnr and let s \n>1 be an integer. r = <p, y, A,{,8, r> is a rank s data selector scheme for source array A, with parameters: \np= <p~, . . . ,j)~> vector of non-negative integers, the target array (driver) shape Ofr. y= <yl, . \n. ., Y,> vector of non-negative integers, the real length, giving the number of positions on each source \naxis where data from pages is selected by r (not ex\u00adtended). A= <A l,.. ., A,> vector of integers, the \nselector origin, giving the index in A corresponding to target index origin. ~=<lg~,...,ir> vector of \nnon-negative integers, the real base, giving the index in A that corresponds to the target index whose \nentries have the smallest values where data is fetched from A s pages (with no extension 4 equals A). \n8= <8,, ...,8,> vector of non-zero entries, the axis step, giving the A s axis Po\u00adsition increment found \nwhen the corresponding target axis position is incremented by 1. m sx r matrix, the axis direction matrix, \nwhere w ~j = 1 if target axis k corresponds to source axis J and w kj = O otherwise. Every column in \nrr contains at 69 most a single entry of 1. where )7 will have the property that for every target ar\u00adray \nindex Z, if J is the source index corresponding to Z, then J = (z l)e7rxij+A (4) [I where e represents \nmatrix multiplication, and x and + are pairwise matrix operations. The expression (4) represents the \ntransformation from the target (driver) index Z to the source index ./. This transformation can also \nbe written for each source axis j as: (1~-l)x8j+Aj if for some k,(l~k<~) %~j= 1 Jj = Aj otherwise (n \nkj = O for every k) { This transformation computes the index of the source element accessed for driver \nindex 1; however, if this in\u00addex is outside the limits specified by -y and L, the value accessed is the \nextended value for that array. A set of parameter values in this scheme will give the data selec\u00adtion \nfunction for an operand. The main differences between this model and the universal selector of Guibas \nand Wyatt [15] are the description of axis correspondence with the axis direc\u00adtion matrix and the parameters \nfor extented selec\u00adtion. To see some of these differences, we examine parameters for the data selection \noperations in APL. Let A be an array of shape n,x .. xn, and let B=< bI, . . . , b, >. For dyadic transpose \n(C* B@A ), where C is rank s, the data selector parameters for A are: for each target axis k (1< k< s): \np~ = min(n, I b, = k, I<l<r) for each source axis j (1 <j < r): ~j= 1 Yj = nj Aj=l aj=l and for each \ntarget axis k and source axis j: 1 if bj=k kj @ otherwise- { For our scalar product example, array B \nhas param\u00adeters: = <4,12> A= <1,1> =~ y = <12,4> 8 = <1,1> P T  = II:: Thus the index transformation \nwould be: ((<z~,z*> l)op jx<l,l> + <1,1> = <z~,l~> as expected. Note that the axis direction matrix T \nbehaves like an adjacency matrix for a graph: the rows represent the driver axis nodes and the columns \nrepresent the source axis nodes and an edge (~kj= 1) implies that the driver axis value determines that \nof the source, i.e., the axes correspond. Also note for trans\u00adpose, if an driver axis represents a diagonal \ntranspose in the source then its row in w has a 1 in each column whose respective source axis is diagonalized. \nReverse ($ [k]A ) behaves like previous models by reversing the starting position on the reversed axis \nand changing the axis step. for each target axis j (1< j< r): Pj= nj for each source axis j (i< j S r): \n nj if j=k Yj=nj Aj= ~ otherwise { ifj= k <j=Aj /jj= ;1 otherwise ( r is the rx r indentity matrix. \n Note that both the selector origin and real base must be reversed. The take operation (B1 A ) behaves \nlike previous models, except in the extended take case, which can extend the length of an axis. For take, \nour parameters are: for each target axis j (l<j< r): Pj= Ibjl for each source axis j (l<j< r): yj = rein{ \nIbj 1, nj) nj+bj+l if bj<O Aj= ~ otherwise I max(nj + bj + 1, 1] if bj<O Cj= 1 otherwise ai=l (w is \nthe r~ r indentity matrix. As an example, let A be a 4X 12 array and consider 5 81 A. The data selector \nparameters are: p = <5,8> A=-<1,5>=4 y = <4,8> 8 == <1,1> Ir is the 2x 2 identity matrix. For the above \nparameters, the index transformation for a target element <11,12> is: H 6.0. Selecting Data in Blocks. \n(<11,12> 1). ~ ~ X<l,l> + <195> = <~l,~z+q> Using our data selection scheme to represent data This offsets \nthe second axis appropriately, and when II> 5 the extended value is fetched. For scalar subscription \n(A [; ; D; . ; ]) param\u00adeters, the axis direction matrix has a column of zeros for the constant axis \nand the selector origin is set to that value. For these APL selection operations the parameters have \nbeen specified as set values rather than transformations. There are rules for composing two conforming \ndata selectors into a single selector [71 and they are basically the result of substituting in the in\u00ad \ndex transformation (4) of one data seletor into the oth\u00ad er and performing the matrix operations to simplify. \nOur data selection scheme is also useful in examin\u00ading the data selection made by operators such as outer \nproduct and inner product. We can specify that there is no correspondence between a driver axis and an \naxis in the operand with a row of zeros in the axis direction matrix. Modifing a data selector by adding \nrows of zeros to the axis direction matrix has an effect similar to Gui\u00adbas and Wyatt s conforming reshape \nbut is much more general. An example will suffice to exhibit how selection parameters are constructed \nwith complex operators. Let A be a 4X12 array and B be a 12x4 array. The output of C -A +.x B is a 4X \n4 array. The driver of the operator is a 4X 12x 4 array, and we must note that with inner product and \nreduction, our legal driver orderings reverse the sequencing along the axis of reduction, so that the \nsecond driver axis would be sequenced in re\u00adverse order. In each of the array s data selectors, the target \narray is the driver shape, the real length is the ar\u00adray shape, the selector origin and the real base \nare sim\u00adply origin and all axis step entries are 1. The key param\u00adeters are the axis direction matrices \nwhich are: The scalar function step for a driver index < Z,J,K > is: A [l,J1 if J==12 C[Z K] -(A [I,J] \no B[J,K]) v CIZ,KI otherwise { and these indices are determined directly by applying the respective index \ntransformation to the driver index. The importance of this model for our problem is that using it we \ncan see data needs at both the scalar level and the paging level. selection functions in an operator \nwe can now establish a block correspondence between a given driver and the data selected in the operand. \nIntuitively, the blocks in a driver and operand correspond when the difference in block structure between \ntwo points in the driver is the same as the difference in the the block structure of the points selected, \nEquivalently, we could say that the relation between a driver index and the source element selected is \nindependent with respect to the level structure of the representations, so a change at an axis and level \nin the driver will influnce the operand only at that level in those axes that correspond to the driver \none. Thus, for a given driver UMILA and the operand s data selector we wish to con\u00adstruct a UMLA for \nthe operand so that driver and source blocks correspond. We first define such a source UMLA and then \nsee that it behaves in the correct manner. For some operator, let A represent an operand ar\u00adray having \na shape of nlx . .. Xnr. let r = <p, y,k, ~,ti, m> be the A s data selector, let T be a driver UMLA with \na depth of d. For A we define lo\u00adcalized source of T with r to be a depth d UMLA S with starting point \nD = < dl, . . . , d~ > where: (a) For every source axis j, 1< j< r, and level i below the top, 2< i< \nd, and for driver axis k that corresponds with j (n ~j = 1): Sij = TkjX I/)j I (b) The starting point \nD, for each source axis j: (Aj l)modxj + 1 if 8j>l and nkj=l dj = (Aj l)modxj Xj+2 if 8j< 1 and ~kj=l \n1 otherwise I d where Xj = II S... i=2 v In the above definition XIX .  xx, is the block shape at the \ntop level of S which is the largest block partitioning represented by S. Condition (a) guarantees that \nat each level i> 1 the length of the source block on an axis equals that of the corresponding axis in \nthe driver block if 18j I>1 this would be the length spanned by the driver block. In our scalar product \nexample, with each driver block we can make 4 steps along an axis within a block so there must be 4 steps \non the axis of in the corresponding block from an operand s. Note that this definition implies that if \na driver axis represents a diagonal transpose then all source axes diagonalized to a driver axes have \nthe same block lengths at each level as the driver (up to multipli\u00adcation by 13j). The starting point \nD in (b) is designed to appropriately place the point corresponding to selec\u00adtor origin A within each \nblock level (where i > 1) and it is choosen to normalize to origin if possible. Appropriate placement \nof selector origin within a block depends on which direction an axis is read. If an axis j is read in \na forward direction (bj > O) then the selector origin point should start at 1 (the smallest index position \non the axis in the -block); when reading in reverse this value should be the largest index position entry \n(SU) since the data selected runs from SO to 1 while the driver runs from 1 to Tjk. By computing the \nS-index ~ of the selector orgin A, using the index used to compute one output page. Using a localized \nsource UMLA will always form a block correspondence between the driver and operand array. The following \nrelationship between local\u00adized source and driver indices exhibits this for the gen\u00aderal case by demonstrating \nthe independence of data selection with respect to levels : For some operation let T be a depth d rank \ns driver, let A be an operand, r be A s rank, and r = <p, y, A,~,13, v> A s data selector for the opera\u00adtion. \nIf S is the localized source UMLA for A with starting point D = < dl, . . ., d, > then for any driver \nelement 1, its T-index, the source element selected has a S-index J such that for every level i (1< i< \nd) and every source axis j (1< j < r): (Iik -l)xtij + iij if 7rkj= 1 Jij = ~,, (5) otherwise v I definition \n(3), we observe that it has this desired behavior: [ (kj l)+xj] + 1 if i=l iij = 1 if fij>landl<i<d Sij \niftij<-l and l<i<d I where Xj is defined above and j is a source axis. D is different from origin whenever \nblocks starting at origin would not line up with those selected. Con\u00adsider, for example, the computation \nB -1 2 J ,4 where A is a 4X 12 array and both A and B are paged in 2X 4-BOXS. The driver UMLA for this \ncomputation to con\u00adstuct B a page at a time is like the scalar product exam\u00adple [~ ~]. The localized \nsource UMLA for A has the same shape as the driver UMLA however, it has a start\u00ading point of D = <2,3>. \nFigure 4 diagrams how this UMLA partitions the data in A. 123456789101112 1314 15161718192021 222324 \n2526 2728293031323334 35 36 37 38 394041 42 43 4445 46 41 48 Fig. 4: UMLA [~ ~] with starting point <2,3> \non A. The blocks in the diagram show how the data from A is moved to the pages in B, with each block \ngiving the data for a page in B. Although these localized source blocks define how data is selected in \nA they clearly do not line up with the page block boundaries in A; However, our equations in Section \n4 will still compute the correct pages, showing 4 pages of A are where ~ is the S-index of A. Equivalently, \nin terms of matrix operations, this relation is: J = (( Z l)en x~+~ (6) 1 The basic idea of why this \nrelation must hold is seen by examining the index transformation on a source axis. Clearly if a source \naxis j is constant this relation holds since Tkj = O for all driver axis k. If j is not con\u00adstant then \nthe index positions on the axes are selected with an arithmetic progression by the corresponding driver \naxis k index position. With respect to an axis k and level i, an UMLA simply defines a partitioning the \nindex positions on the array axis k into equally spaced intervals. Since the data selection is an arithmetic \npro\u00adgression intervals ef length Tik in the driver array axis k correspond intervals of length Sti on \nA s axis j. Picking a starting point that aligns these intervals for d Xj = II Si. will align these intervals \nfor every level i> 1, i=2 J and this is the case with dj. Translating this observation into terms of \nUMLAS gives us our resultt. This relation exhibits our block correspondence by showing each axis and \nlevel behave independently. Consequently positions within a driver block must be within the corresponding \nsource block as well and positions that mark the boundaries of a driver block must do so in the source. \nReturning to our scalar product example, the driver UMLA was T = [~ ~] and the localized source of array \nt For a different but a more detailed proof see [7]. B is the UMLA S = [~ ~]. For the driver T-index \n1 and J the S-index of the element selected from B then by the above we can rewrite the index transformation \nas: Thus we can directly determine the paging needs for B from its data selection parameters and the \ndriver UMLA block level used for B s paging, The index transforma\u00adtion for both operand arrays A and \nC shows that 1 is selected for each. A final remark is that the above relation (5) gives a correspondence \nfor each level that is a arithmetic pro\u00adgression. Consequently, we can construct a stepper function [15] \nat each level to increment the position address in steps rather than actually performing the index computation \n(3) to determine the address of the data selected. 7.0. Constructing, a Paged APL Procedure. With the \nresults from the previous sections we know the rules for constructing a block computation for APL along \nwith equations for computing the paging needs once the shape of blocks are decided. Conse\u00adquently, the \nprocess of generating a procedure for an APL operator reduces to the following questions: (1) How do \nwe choose a pagination for the output? (2) How do we select the driver UMLA (along with which levels \ndo the operand get pages)? (3) What do we do if the number of available frames is insufficient for the \ngiven input organizations?  With the algorithms to decide these questions, an APL compiler or interpreter \ncan generate a procedure for an APL operator with paging, and by using the expression manipulations discussed \nin Section 2, these operator computations can be pipelined to execute an expres\u00ad sion efficiently. Our \napproach to these problems follows the same ideas used for the scalar product example of Section 2, and \nin this section we overview our tech\u00ad niques. Our approach to choosing an output pagination (BOX) shape, \nquestion (l), is to examine, for each out\u00adput axis, the lengths of the corresponding+ input BOX * Here \ncorresponding means that the axes corres~ond with sides and to select a output BOX to match up with the \ninputs. Our algorithm is quite simple. inputs: The input operands paginations and the operator s data \nselectors for the input and output operands. output: w= <Wl, . . . , Wr> the output BOX shape. 1. for \nk :=1 to r do: Initialize Wk to the largest input BOX side length among those input axes that correspond \nwith the output axis k; 2. if W is larger than a BOX then reduce the size of W by dividing sides by \n2 until it is a BOX; 3. if W is smaller than a BOX then increase the size of W by multiplying sides \nby 2 until it is a BOX;  Briefly, we construct the matched up block shape from the inputs, step 1, and \nthen adjust the side lengths of shape W into a BOX shape for the output. Recall that all side lengths \nin a BOX are powers of 2, so the largest among a set of BOX sides will be the least common multiple and \na power of 2. Thus if the size of the block W after step 1 is larger than a page, successive divisions \nby 2 on W s sides will eventually reduce the size of W to the page size; similarly if the size of W is \nsmaller than a page. In our scalar product example, the matched up input shape was 4X 4 and so a 2X4-BOX \nwas selected. We should note that if the input BOXes are balanced we can always find a balanced BOX for \nthe output a more refined procedure is given in [7]. The task of selecting the driver UMLA, question \n(2), in reality involves two questions: (2a) What is the driver UMLA shape? (2b) What sequence of driver \nelements is implied by the UMLA ? For (2b) there are generally many orderings where the operands will \nbe accessed in the blocks of a given driver UMLA. However, we simply fix the sequence implied by an UMLA, \ndepending on the operator. The sequences are divided into two main classes. For operators that do not \ninvolve a reduction of the inputs, e.g., scalar product and outer product, principal order [14] is used \nto generate the sequence from the driver UMLA. With operators using a reduction++ ~, e.g., reduction \nand inner product, a sequencing called reduction-block order is used. the same driver axis. ttt Exceptscan \nwhich has its own special order [7]. Driver UMLAS for these operators have at least 3 levels and there \nis a special inputs paging level where the output is paged above it and inputs are paged at that level \nor below (commonly the second level). Reduction-block order is similar to principal order except with \nthe driver axis of reduction: 1. The axis of reduction is stepped in reverse and all other axes are stepped \nas in principal order. 2. For steps changing driver indices above the inputs paging level and below, \nall steps on the axis of reduction are made before any other axis is stepped. 3. At the inputs paging \nlevel and below, the sequence steps change driver indices like princi\u00adpal order (except that the axis \nof reduction is steppedin reverse).  The effect of this sequence is to reduce a row of blocks to form \na block of output. These order\u00adings have the advantage of producing the output in a principal order and, \nconsequently, operators in an expression can be easily pipelined to omit tem\u00adporary output array construction \nbetween operator computations. Selecting the driver UMLA dependson whether the operator performs a reduction \nor not; however, the basic idea in all cases is to formulate a matched up block and then add levels to \nsubsequence by blocks to save space. Our algorithm marks operands to indicate that its paging level has \nbeen assigned: inputs: The operator s driver array shape, the operands paginations and their respective \ndata selectors for the operator. output: A depth d rank r UMLA driver S and the levels where each operand \nis paged. 1. Initially d is 2, paging levels are set to 1, and operands are not marked. 2. Set <S21, \n. . . ,S2, > to the matched up block shape for all operands (input and out\u00adputs) and compute the first \nrow of S; 3. if the operator performs a reduction then do; 4. Increase d to 3 and set < S31, . . . \n,S3,> to  the matched up block shape using only the input operands and, subdivide the second level by, \nfor each axis . . redacirm i.. S2j by S2j+S3j; 6. Change the paging level for the inputs to 2 (the output \npaging level is 1) and mark the output operand; 7. end;  8. while all the operands are not marked \ndo  9. Compute for each non-marked operand a space factort equal to the product of the ratio of Sdj \nwith the operand s respective pagination side length for each driver axis j which corresponds to the \noperand; 10. if the space factors of the non-marked operands are equal then mark the remaining operands \n(their paging level is d l); 11. else do;  11. Mark the operand (s) having the smallest space factor; \n 12. Increment d by 1; set <Sdl, . . . ,Sd, >  to the matched up shape of the remaining non-marked operands \nsub\u00ad dividing level d 1, and set the pag\u00ad ing level of the non-marked operands to d 1; 13. end; 14. \nend;  Steps 1 to 7 initialize the driver UMLA to the matched UP block shape making the appropriate construction \nfor operators performing reduction. The remaining steps simply add levels for subsequencing (clearly \nthe depth is at most 3 for monadic operators and 4 for dyadic operators). Note that whenever an operand \narray is assigned a level, the block at that level has been matched up with its pagination and consequently \nthe criteria required for the computa\u00adtion of pages in Section 4 will be satisfied. For our scalar product \nexample, the driver UMLA of [~ ~] is computed in the first 2 steps, and since all the space factors compute \nto 2, the algo\u00adrithms stops. This is the driver UMLA used in the. example. Note this procedure can be \nimproved by considering more special cases [7] to get subse\u00adquencings such as the 4 frame computation \nof our scalar product example. Our final question, (3), is what to do if the number of page frames is \ninadequate. This problem can generally be managed by storage reorganization [7, 8, 12]. Using reorganization \nwe can reduce the size of blocks when we match up and, conse\u00ad quently, reduce the space requirements. \nIf, for example, our scalar product computation had to be performed in 3 frames, then after reorganizing \nB into a 4X 2-BOX, available storage would be sufficient. With storage reorganized, we simply t This \nequals the number of pages needed for the operand if the localized source starts at origin. recompute \nthe output pagination and driver UMLA by the above algorithms. Using the above procedures we can formulate \na scheme for computing an APL operator with paging following the ideas suggested in section 3: 1. Select \nan output pagination and driver UMLA (reorganize if necessary). 2. Step through each driver block, changing \ndriver index above the bottom level, using the appropriate order for the operater. 3. For all input \noperands, if an input operand s index is changed at its paging level then pull the pages for that block \nof input. 4. Perform all scalar functions for the driver block, stepping the driver index through the \nbottom level. 5. If the output operand s index is changed at its paging level then push the now com\u00adpleted \noutput pages in the block.  To fill in this scheme we need the number of available frames, the paginations \nof the inputs and the parameters in our model for the semantics of the operator to be computed. The result \nis a pro\u00adcedure to execute the operator with paging. We have analyzed the performance of the procedures \ngenerated for APL and found their performance to be quite efficient, particularly compared to APL with \nconventional paging [71. 8.0, Summary, Our approach to paging APL has exhibited that allowing some paging \nto be managed as a Language Processing task could be done efficiently and lead to improved system performance. \nOur analysis of the APL semantics allowed us to observe the charac\u00adteristics of storage use in compation \nso we could essentially interface the functions of paging within them. Two major factors in permitting \nour method to be developed were the UMLA structure, to characterize storage use, and the a model of data \naccess for APL. Together they enabled us to easily analyze the potential effects of a computation, so \nthat an efficient alternative could be selected, and formulate the necessary transformations to compute \nthe paging needs simply. Consequently, the task of generating a paged APL computation could be done easily \nand efficiently. We are in the process of imple\u00ad menting a version of APL which incorporates our approach \nto paging. References [11 Abrams, P. S., An APL Machine, Ph. D. Thesis, Stanford Report SLAC-144, February \n1970. [21 Association for Computing Machinery, Proceedings of a symposium on storage allocation, CACM \n4, 10 (Oct. 1961) [31 Aho, A. V., Denning, P. J., and Unman, J. D., Principles of Optimal Page Replacement. \nJACM 18:1 (January 1971), pp. 80-92. [41 Barrese, A. L., and Shapiro, S. D., Structuring Pro\u00adgrams for \nEfficient Operation in Virtual Memory Systems, IEEE Transitions on Software Engineer\u00ading, Vol. SE-5:6 \n(November 1979), pp. 643-652. [51 Belady, L. A., A Study of Replacement Algorithms for a Virtual Storage \nComputer, IBM Systems Jour\u00adnal 5:2 (1966), pp. 78-101. [61 Bobrow, D. G., and Murphy, D. L,, Structure \nof a LISP System Using Two-Level Storage, CACM 10:5 (March 1967), pp. 155-159. [71 Condry, M. W., Paging \nas a Programming Language Task, Ph.D. Thesis, Yale University, August 1980. [81 Condry, M. W., On Arbitrary \nArray Pagination Strategies: Storage and Reorganization, Proc. Johns Hopkins Conference on Information \nand System Sciences (March 1978), pp. 385-390. [91 Denning, P. J., Virtual Memory, Computing Sur\u00adveys \n2:$ (September 1970), pp. 158-189. \u00ad [10] Elshoff, J. L., Some Programming Techniques for Processing \nMttlti-Dimensional Matrices in a Paging Environment, Proc. of AFIPS National Computer Conf., Voi 43 (1974), \npp. 185-193. [111 Falkoff, A. D., Iverson, K. E,, The Design of APL, IBM Journal Research and Development \n16:4 (July, 1973). [12] Fischer, P. C., and Probert, R. L., Storage Reor\u00adganization Techniques for Matrix \nComputations, CACM 22:7 (Ju1y, 1979), pp. 405-414. [13] Floyd, R. W., Permuting Information in Idealized \nTwo-Level Storage, in Complexi~ of Computer Com\u00adputations, R. E. Miller and J. W. Thatcher, Eds., Plenum \nPress, 1972, pp. 105-109. [141 Ghandour, Z., and Mezei, J., General Arrays, Operators and Functions, \nIBM Journal of Research and Development Vol. 17:4 (July 1973), 335-353, [15] Gttibas, L. J., and Wyatt, \nD. K., Compilation and Delayed Evaluation in APL, Proceedings of the Fifth Symposium on Principles of \nProgramming La~guages (1978), pp. 1-8. [161 Iverson, K. E., Operators, ACM Transactions on Programming \nLanguages and Systems Vol. 1:2 (October, 1979), pp. 161-176. [17] McKeller, A. C., and Coffman, E. G., \nThe Organi\u00adzation of Matrices and Matrix Operations in a Paged Multiprogramming Environment, C-ACM 12:3 \n(March, 1969), pp. 153-165. 75 [18] Miller, T. C., Tentative Compilation: A Design for an APL Compiler, \nAPL 79: ACM STAPL/SIGPLAN Congress (May 1979), pp. 88\u00ad 95. [19] Moler, C. B., Matrix Computations with \nFOR- TRAN and Paging, CACM 15:4 (April, 1972), pp. 268-270. [20] Orbits, D. A., and Calahan, D. A., Data \nFlow Con\u00adsideration in Implementing a Full Matrix Solver with Backing Store on the CRAY-1, Systems Engr. \nLab. Report No. 98, Univ. Michigan, March 1977. [21] Perlis, A. J., Steps Toward an APL Compiler --Updated. \nYale Research Report #24, March 1975. [22] Rogers, L. D., Optimal Paging Strategies and Stabil\u00adity Considerations \nfor Solving Large Linear Sys\u00adtems, Ph. D. Thesis, University of Waterloo, 1973. [23] Schwartz, J. T., \nOn Programming: An Interim Report on the SETL Project; Installment 1: Gen\u00aderalities, Installment 2: The \nSETL Language and Examples of its Use, Courant Inst. Of Math. Sci., New York Univ., (1973). [24] Trivedi, \nK. S., On the Paging Performance of Array Algorithms, IEEE Transactions on Computers, Vol. C-26:1O (October, \n1977), pp. 938-947. [251 Trivedi, K. S., Prepaging and Applications to Array Algorithms, IEEE Transitions \non Computers, Vol. C-25:9 (September 1976), pp. 915-921. [26] van Dyke, E. J., A Dynamic Incremental \nCompiler for an Interpretive Language, Hewlett-Packard Journal (July 1977), pp. 17-23.  \n\t\t\t", "proc_id": "567532", "abstract": "This paper examines \"language processing\" approach to paging where the of the programming language compiler or interpreter is responsible for generating the necessary control code for the page management of a program. We explore this idea for <i>APL</i> and describe an approach to incorporating in a program the necessary paging functions. The semantics of <i>APL</i> computation are examined to observe how paging operations can be incorporated into the computation. We discuss a model of data access in <i>APL</i> that exhibits storage use for both scalar and page references. A data structure that encodes the logical use of data from an array is introduced. We find that ordering computations efficiently and computing paging needs can be determined by simple transformations on this structure. This analysis leads us to an efficient method for paging an <i>APL</i> computation. Our approach builds on previous studies for efficiently executing <i>APL.</i>", "authors": [{"name": "Michael W. Condry", "author_profile_id": "81100081117", "affiliation": "Princeton University", "person_id": "PP39026281", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/567532.567538", "year": "1981", "article_id": "567538", "conference": "POPL", "title": "Paging as a language processing task", "url": "http://dl.acm.org/citation.cfm?id=567538"}