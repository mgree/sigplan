{"article_publication_date": "10-21-2007", "fulltext": "\n Using HPM-Sampling to Drive Dynamic Compilation Dries Buytaert Andy Georges Michael Hind* Matthew Arnold* \nLieven Eeckhout Koen De Bosschere Department of Electronics and Information Systems, Ghent University, \nBelgium *IBM T.J. Watson Research Center, New York, NY, USA {dbuytaer,ageorges}@elis.ugent.be, {hindm,marnold}@us.ibm.com, \n{leeckhou, kdb}@elis.ugent.be Abstract All high-performance production JVMs employ an adaptive strategy \nfor program execution. Methods are .rst executed unoptimized and then an online pro.ling mechanism is \nused to .nd a subset of methods that should be optimized during the same execution. This paper empirically \nevaluates the de\u00adsign space of several pro.lers for initiating dynamic com\u00adpilation and shows that existing \nonline pro.ling schemes suffer from several limitations. They provide an insuf.cient number of samples, \nare untimely, and have limited accu\u00adracy at determining the frequently executed methods. We de\u00adscribe \nand comprehensively evaluate HPM-sampling, a sim\u00adple but effective pro.ling scheme for .nding optimization \ncandidates using hardware performance monitors (HPMs) that addresses the aforementioned limitations. \nWe show that HPM-sampling is more accurate; has low overhead; and im\u00adproves performance by 5.7% on average \nand up to 18.3% when compared to the default system in Jikes RVM, without changing the compiler. Categories \nand Subject Descriptors D.3.4 [Programming languages]: Processors Compilers; Optimization; Run\u00adtime environments \nGeneral Terms Measurement, Performance Keywords Hardware Performance Monitors, Java, Just-in\u00adtime compilation, \nPro.ling 1. Introduction Many of today s commercial applications are written in dy\u00adnamic, type-safe, \nobject-oriented languages, such as Java, because of the increased productivity and robustness these languages \nprovide. The dynamic semantics of such a lan- Permission to make digital or hard copies of all or part \nof this work for personal or classroom use is granted without fee provided that copies are not made or \ndistributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. OOPSLA 07, October 21 25, 2007, Montr\u00b4ebec, Canada. eal, Qu\u00b4 \nguage require a dynamic execution environment called a virtual machine (VM). To achieve high performance, \npro\u00adduction Java virtual machines contain at least two modes of execution: 1) unoptimized execution, \nusing interpreta\u00adtion [21, 28, 18] or a simple dynamic compiler [16, 6, 10, 8] that produces code quickly, \nand 2) optimized execution us\u00ading an optimizing dynamic compiler. Methods are .rst ex\u00adecuted using the \nunoptimized execution strategy. An online pro.ling mechanism is used to .nd a subset of methods to optimize \nduring the same execution. Many systems enhance this scheme to provide multiple levels of optimized execu\u00adtion \n[6, 18, 28], with increasing compilation cost and bene\u00ad.ts at each level. A crucial component to this \nstrategy is the ability to .nd the important methods for optimization in a low-overhead and accurate \nmanner. Two approaches that are commonly used to .nd optimiza\u00adtion candidates are method invocation counters \n[10, 18, 21, 28] and timer-based sampling [6, 8, 18, 28, 30]. The coun\u00adters approach counts the number \nof method invocations and, optionally, loop iterations. Timer-based sampling records the currently executing \nmethod at regular intervals using an op\u00aderating system timer. Although invocations counters can be used \nfor pro.ling unoptimized code, their overhead makes them a poor choice for use in optimized code. As \na result, VMs that use multi\u00adple levels of optimization rely exclusively on sampling for identifying \noptimized methods that need to be promoted to higher levels. Having an accurate sampler is critical to \nen\u00adsure that methods do not get stuck at their .rst level of opti\u00admization, or in unoptimized code if \na sample-only approach is employed [6, 8]. Most VMs rely on an operating system timer interrupt to perform \nsampling, but this approach has a number of draw\u00adbacks. First, the minimum timer interrupt varies depending \non the version of the OS, and in many cases can result in too few samples being taken. Second, the sample-taking \nmecha\u00adnism is untimely and inaccurate because there is a delay be\u00adtween the timer going off and the sample \nbeing taken. Third, the minimum sample rate does not change when moving to newer, faster hardware; thus, \nthe effective sample rate (rela\u00ad c Copyright &#38;#169; 2007 ACM 978-1-59593-786-5/07/0010. . . $5.00 \ntive to the program execution) continues to decrease as hard\u00adware performance improves. This work advocates \na different approach, using the hard\u00adware performance monitors (HPMs) on modern processors to assist \nin .nding optimization candidates. This HPM\u00adsampling approach measures the time spent in methods more \naccurately than any existing sample-based approach, yet re\u00admains low-overhead and can be used effectively \nfor both optimized and unoptimized code. In addition, it allows for more frequent sampling rates compared \nto timer-based sam\u00adpling, and is more robust across hardware implementations and operating systems. This \npaper makes the following contributions: We describe and empirically evaluate the design space of several \nexisting sample-based pro.lers for driving dy\u00adnamic compilation;  We describe the design and implementation \nof an HPM\u00adsampling approach for driving dynamic compilation; and  We empirically evaluate the proposed \nHPM approach in Jikes RVM, demonstrating that it has higher accuracy than existing techniques, and improves \nperformance by 5.7% on average and up to 18.3%.  To the best of our knowledge, no production VM uses \nHPM-sampling to identify optimization candidates to drive dynamic compilation. This work illustrates \nthat this tech\u00adnique results in signi.cant performance improvement and thus has the potential to improve \nexisting VMs with minimal effort and without any changes to the dynamic compiler. The rest of this paper \nis organized as follows. Section 2 provides further background information for this work. Sec\u00adtion 3 \ndetails the HPM-sampling approach we propose. After detailing our experimental setup in Section 4, Sec\u00adtion \n5 presents a detailed evaluation of the HPM-sampling approach compared to existing techniques; the evaluation \nin\u00adcludes overall performance, overhead, accuracy, and robust\u00adness. Section 6 compares our contribution \nto related work and Section 7 concludes and discusses future directions. 2. Background This section describes \nbackground for this work. Speci.\u00adcally, it describes the design space of sampling techniques for .nding \noptimization candidates and discusses the short\u00adcomings of these techniques; gives relevant details of \nJikes RVM; and summarizes hardware performance monitor fa\u00adcilities, focusing on the particular features \nwe employ in this work. 2.1 Sampling Design Space Two important factors in implementing any method sam\u00adpling \napproach are 1) the trigger mechanism and 2) the sam\u00adpling mechanism. Figure 1 summarizes this 2-dimensional \ndesign space for sampling-based pro.lers. The horizontal axis shows the trigger mechanism choices and \nthe vertical Figure 1. The design space for sampling pro.lers: sampling versus trigger mechanism.  axis \nshows the sampling mechanism choices. The bullets in Figure 1 represent viable design points in the design \nspace of sampling-based pro.lers we will discuss the nonviable design points later. Section 5 compares \nthe performance of these viable design points and shows that HPM-immediate is the best performing sampling-based \npro.ling approach. 2.1.1 Trigger Mechanisms The trigger mechanism is the technique used to periodically \ninitiate the sample-taking process. All production JVMs that we are aware of that use sampling to .nd \noptimization can\u00addidates use an operating system feature, such as nanosleep or setitimer, as the trigger \nmechanism for .nding methods to optimize [6, 8, 30, 28, 18]. The nanosleep approach uses a native thread \nrunning concurrently with the VM. When the timer interrupt goes off, this native thread is scheduled \nto execute and then set a bit in the VM. When the VM next ex\u00adecutes and sees the bit is set, a sample \nis taken. The setitimer approach does not use a native thread and thus has one less level of indirection. \nInstead, the interrupt handler for the VM sets the bit in the VM when the timer interrupt goes off. The \nthird option, HPM, will be described in Section 3. Timer-based sampling (using nanosleep or setitimer) \nis a low overhead pro.ling technique that can be used on all forms of code, both unoptimized and optimized \ncode. It al\u00adlows for the reoptimization of optimized code to higher lev\u00adels, as well as for performing \nonline pro.le-directed opti\u00admizations, when the pro.le changes, such as adaptive inlin\u00ading [6]. However, \ntimer-based sampling does have the following limitations: Not enough data points: The timer granularity \nis depen\u00addent on the operating system settings, e.g., a mainstream operating system, such as Linux 2.6, \nprovides a common granularity of 4 ms, which means that at most 250 sam\u00adples/second can be taken. Other \noperating systems may not offer such a .ne granularity. For example, in earlier versions of Linux the \ngranularity was only 10 ms, result\u00ading in at most 100 samples per second. Furthermore, the granularity \nis not necessarily something that can be eas\u00adily changed because it will likely require rebuilding the \nkernel, which may not be feasible in all environments and will not be possible when source code is not \navailable. Lack of robustness among hardware implementations: Timer-based sampling is not robust among \nmachines with different clock speeds and different microarchitec\u00adtures because a faster machine will \nexecute more instruc\u00adtions between timer-interrupts. Given a .xed operating system granularity, a timer-based \npro.ler will collect fewer data points as microprocessors attain higher per\u00adformance. Section 5 demonstrates \nthis point empirically. Not timely: There is a delay from when the operating sys\u00adtem discovers that a \nthread needs to be noti.ed about a timer event and when it schedules the thread that re\u00adquested the noti.cation \nfor initiating the sample taking. As we will see in Section 3, the HPM approach addresses all of these \nshortcomings.  2.1.2 Sampling Mechanisms Once the sample-taking process has been initiated by some trigger \nmechanism, a sample can be taken. Two options exist for taking the sample (the vertical axis in Figure \n1): immediate and polling. An immediate approach inspects the executing thread(s) to determine which \nmethod(s) they are executing. The polling approach sets a bit in the VM that the executing threads check \nat well-de.ned points in the code, called polling points. When a polling point is executed with the bit \nset, a sample is taken. Polling schemes are attractive for pro.ling because many VMs already insert polling \npoints into compiled code (some\u00adtimes called yieldpoints) to allow stopping the executing thread(s) when \nsystem services need to be performed. The sampling pro.ler can piggyback on these existing polling points \nto collect pro.le data with essentially no additional overhead. Polling points are a popular mechanism \nfor stopping ap\u00adplication threads because the VM often needs to enforce cer\u00adtain invariants when threads \nare stopped. For example, when garbage collection occurs, the VM needs to provide the col\u00adlector with \nthe set of registers, globals, and stack locations that contain pointers. Stopping threads via polling \nsigni.\u00adcantly reduces the number of program points at which these invariants must be maintained. However, \nusing polling as a sampling mechanism does have some shortcomings: Not timely: Although the timer expires \nat regular intervals, there is some delay until the next polling point is reached. This is in addition \nto the delay imposed by timer-based pro.ling as described in the previous section. In particu\u00adlar, the \ntrigger mechanism sets a bit in the VM to notify the VM that a sample needs to be taken. When the VM \ngets scheduled for execution, the sample is not taken im\u00admediately. The VM has to wait until the next \npolling point is reached before a sample can be taken. Limited accuracy: Untimely sampling at polling \npoints may also impact accuracy. For example, consider a polling point that occurs after a time-consuming \noper\u00adation that is not part of Java, such as an I/O operation or a call to native code. It is likely \nthat the timer will expire during the time-consuming operation so unless the native code clears the sampling \n.ag before returning (or the VM somehow ensures that it was never set), the next polling point executed \nin Java code will have an ar\u00adti.cially high probability of being sampled. Essentially, time spent in \nnative code (which generally does not con\u00adtain polling points) may be incorrectly credited to the caller \nJava method. An additional source of inaccuracy is that some VMs, such as Jikes RVM, do not insert polling \npoints in all methods and thus some methods cannot be sam\u00adpled. For example, native methods (not compiled \nby the VM s compilers), small methods that are always inlined (polling point elided for ef.ciency reasons), \nand low\u00adlevel VM methods do not have polling points. Overhead: Polling requires the executing code to \nperform a bit-checking instruction followed by a conditional branch to sampling code. This bit-checking \ncode must always be executed, regardless of whether the bit has been set. Re\u00adducing the number of bit-checking \ninstructions can re\u00adduce this overhead (as Jikes RVM does for trivial meth\u00adods), but it will also reduce \nthe accuracy as mentioned above. To avoid these limitations we advocate an immediate approach as described \nin Section 3.  2.2 Jikes RVM Jikes RVM is an open source Java VM that is written in Java and has been \nwidely used for research on virtual ma\u00adchines [2, 3]. Methods running on Jikes RVM are initially compiled \nby a baseline compiler, which produces unopti\u00admized code quickly. An optimizing compiler is used to re\u00adcompile \nimportant methods with one of its three optimiza\u00adtion levels: 0, 1, and 2 [6, 7]. Jikes RVM uses timer-based \nsampling with polling for .nding methods to be considered for optimization. Speci.\u00adcally, a timer-interrupt \nis used to set a bit in the virtual ma\u00adchine. On our platform, Linux/IA32, the default Jikes RVM system \ndoes this every 20 ms, which was the smallest level of granularity on Linux when that code was written \nseveral years ago. However, current Linux 2.6 kernels allow for a .ner granularity of 4 ms by default.1 \nTherefore, we also compare our work to an improved default system, where the timer-interrupt is smaller \nthan 20 ms. Section 5 discusses the performance improvements obtained by reducing this inter\u00ad 1 The granularity \nprovided by the OS is a tradeoff between system respon\u00adsiveness and the overhead introduced by the OS \nscheduler. Hence, the timer granularity cannot be too small. rupt value. This illustrates an important \nshortcoming of an OS timer-based approach: as new versions of the OS are used, the sampling code in the \nVM may need to be adjusted. Jikes RVM provides two implementation choices for timer-based sampling: 1) \nnanosleep-polling, and 2) setitimer\u00adpolling. The .rst strategy, which is the default, spawns a auxiliary, \nnative thread at VM startup. This thread uses the nanosleep system call, and sets a bit in the VM when \nawo\u00adken before looping to the next nanosleep call. The polling mechanism checks this bit. The second \nstrategy, setitimer, initiates the timer interrupt handler at VM startup time to set a bit in the VM \nwhen the timer goes off. Setitimer does not require an auxiliary thread. In both cases, the timer resolu\u00adtion \nis limited by the operating system timer resolution. When methods get compiled, polling instructions \ncalled yield-points are inserted into the method entries, method exits, and loop back edges. These instructions \ncheck to see if the VM bit is set, and if so, control goes to the Jikes RVM thread scheduler to schedule \nanother thread. If the VM bit is not set, execution continues in the method at the instruction after \nthe yield-point. Before switching to another thread, the system performs some simple pro.ling by recording \nthe top method on the stack (for loop or exit yield-points) or the second method on top of the stack \n(entry yield-points) into a buffer. When N method samples have been recorded, an organizer thread is \nactivated to summarize the buffer and pass this summary to the controller thread, which decides whether \nany recompilation should occur. The default value for N is 3. The controller thread uses a cost/bene.t \nanalysis to de\u00adtermine if a sampled method should be recompiled [6, 7]. It computes the expected future \nexecution time for the method at each candidate optimization level. This time includes the cost for performing \nthe compilation and the time for exe\u00adcuting the method in the future at that level. The compila\u00adtion \ncost is estimated using the expected compilation rate as a function of the size of the method for each \noptimiza\u00adtion level. The expected future execution time for a sampled method is assumed to be the amount \nof time the method has executed so far, scaled by the expected speedup of the can\u00addidate optimization \nlevel.2 For example, the model assumes that a method that has executed for N seconds will execute for \nN more seconds divided by the speedup of the level com\u00adpared to the current level. The system uses the \npro.le data to determine the amount of time that has been spent in a method. After considering all optimization \nlevels greater than the current level for a method, the model compares the sum of the compilation cost \nand expected future execution time with the expected future execution time at the current level, i.e., \nthe future time for the method at a level versus performing no 2 This speedup and the compilation rate \nare constants in the VM. They are currently obtained of.ine by measuring their values in the SPECjvm98 \nbenchmark suite. recompilation. The action associated with the minimum fu\u00adture execution time is chosen. \nRecompilations are performed by a separate compilation thread. 2.3 Hardware Performance Monitors Modern \nprocessors are usually equipped with a set of perfor\u00admance event counter registers also known as hardware \nper\u00adformance monitors (HPMs). These registers can be used by the microprocessor to count events that \noccur while a pro\u00adgram is executing. The HPM hardware can be con.gured to count elapsed cycles, retired \ninstructions, cache misses, etc. Besides simple counting, the hardware performance counter architecture \ncan also be con.gured to generate an interrupt when a counter over.ows. This interrupt can be converted \nto a signal that is delivered immediately to the process us\u00ading the HPMs. This technique is known as \nevent-based sam\u00adpling. The HPM-sampling approach proposed in this paper uses event-based sampling using \nthe elapsed cycle count as its interrupt triggering event. 3. HPM-Immediate Sampling This section describes \nour new sampling technique. It .rst discusses the merits of immediate sampling and then de\u00adscribes HPM-based \nsample triggering. When combined, this leads to HPM-immediate sampling, which we advocate in this paper. \n3.1 Bene.ts of Immediate Sampling An advantage of using immediate sampling is that it avoids an often \nundocumented source of overhead that is present in polling-based pro.lers: the restrictions imposed on \nyield\u00adpoint placement. VMs often place yield-points on method prologues and loop backedges to ensure \nthat threads can be stopped within a .nite amount of time to perform system services. How\u00adever, this \nplacement can be optimized further without affect\u00ading correctness. For example, methods with no loops \nand no calls do not require a yield-point because only a .nite amount of execution can occur before the \nmethod returns. However, when using a polling-based pro.ler, removing yield-points impacts pro.le accuracy. \nIn fact, yield-points need to be placed on method epilogues as well as prologues to make a polling-based \nsampler accurate; without the epi\u00adlogue yield-points, samples that are triggered during the ex\u00adecution \nof a callee may be incorrectly attributed to the caller after the callee returns. For this reason, Jikes \nRVM places epilogue yield-points in all methods, except for the most triv\u00adial methods that are always \ninlined. There are no restrictions on yield-point placement when using an immediate sampling mechanism. \nAll epilogue yield-points can be removed, and the prologue and backedge yield-point placement can be \noptimized appropriately as long as it maintains correctness for the runtime system. The experimental \nresults in Section 5 include a breakdown to input : CPU context output : void begin registers . GetRegisters \n(CPU context); processor . GetJikesProcessorAddress (); if isJavaFrame (processor, registers) then stackFrame \n. GetFrame (processor); methodID . GetMethodID (stackFrame); sampleCount . sampleCount + 1; samplesArray \n[sampleCount ] . methodID; end HPMInterruptResumeResetCounter () end Algorithm 1: HPM signal handler \nas implemented in JikesRVM, where the method ID resides on the stack. show how much performance is gained \nby removing epi\u00adlogue yield-points.  3.2 HPM-based Sampling HPM-based sampling relies on the hardware \nperformance monitors sending the executing process a signal when a counter over.ows. At VM startup, we \ncon.gure a HPM register to count cycles, and we de.ne the over.ow threshold (the sampling interval or \nthe reciprocal of the sample rate). We also de.ne which signal the HPM driver should send to the VM process \nwhen the counter over.ows.3 Instead of setting a bit that can later be checked by the VM, as is done \nwith polling, the VM acquires a sample immediately upon receiving the appropriate signal from the HPM \ndriver. Several approaches can be used to determine the executing method. For example, the program counter \ncan be used to determine the method in the VM s compiled code index. Alternatively, if the method ID \nis stored on the stack, as is done in Jikes RVM, the method ID can be read directly from the top of the \nstack. In our implementation, we take the latter approach, as illustrated in Algorithm 1: the state of \nthe running threads is checked, the method residing on the top of the stack is sampled, and the method \nID is copied in the sample buffer. Because the executing method can be in any state, the sampler needs \nto check whether the stack top contains a valid Java frame; if the stack frame is not a valid Java frame, \nthe sample is dropped. On average, less than 0.5% of all samples gathered in our benchmark suite using \nan immediate technique are invalid.  3.3 How HPM-immediate Fits in the Design Space of Sampling-Based \nPro.ling Having explained both immediate sampling and HPM\u00adsampling, we can now better understand how \nthe HPM\u00adimmediate sampling-based pro.ling approach relates to the 3 Our implementation uses SIGUSR1; \nany of the 32 POSIX real-time sig\u00adnals can be used. other sampling-based approaches in the design space. \nHPM\u00adimmediate sampling shows the following advantages over timer-based sampling: (i) sample points can \nbe collected at a .ner time granularity, (ii) performance is more robust across platforms with different \nclock frequencies and thread scheduling quanta, and (iii) it is more timely, i.e., a sample is taken \nimmediately, not at the next thread scheduling point. Compared to polling-based sampling, HPM-immediate \nsam\u00adpling (i) is more accurate and (ii) incurs less overhead. Referring back to Figure 1, there are two \ndesign points in sampling-based pro.ling that we do not explore because they are not desirable: nanosleep-immediate \nand HPM\u00adpolling. The nanosleep-immediate approach does not offer any advantage over setitimer-immediate: \nto get a sample, nanosleep incurs an even larger delay compared to setitimer, as explained previously. \nThe HPM-polling approach is not desirable because it would combine a timely trigger mech\u00adanism (HPM-sampling) \nwith a non-timely sampling mecha\u00adnism (polling). 3.4 HPM Platform Dependencies HPMs are present on virtually \nall modern microprocessors and can be used by applications if they are accessible from a user privilege \nlevel. Not all microprocessors offer the same HPM events, or expose them in the same way. For example, \non IA-32 platforms, low overhead drivers pro\u00advide access to the HPM infrastructure, but programming the \ncounters differs for each manufacturer and/or processor model. One way in which a VM implementor can \nresolve these differences is by encapsulating the HPM subsystem in a platform-dependent dynamic library. \nAs such, HPM\u00adsampling is portable across all common platforms. Standardizing the HPM interfaces is desirable \nbecause it can enable better synergy between the hardware and the vir\u00adtual machine. In many ways it is \na chicken-and-egg problem; without concrete examples of HPMs being used to improve performance, there \nis little motivation for software and hard\u00adware vendors to standardize their implementations. How\u00adever, \nwe hope this and other recent work [1, 23, 27] will show the potential bene.t of using HPMs to improve \nvirtual machine performance. Furthermore, collecting HPM data can have different costs on different processors \nor different microarchitectures. As our technique does not require the software to read the HPM counters, \nbut instead relies on the hardware itself to track the counters and to send the executing process a signal \nonly when a counter over.ows, the performance of reading the counters does not affect the portability \nof our technique. 4. Experimental Setup Before evaluating the HPM-sampling approach, we .rst de\u00adtail \nour experimental setup: the virtual machine, the bench\u00admarks, and the hardware platforms. 4.1 Virtual \nmachine As mentioned before, we use Jikes RVM, in particular the CVS version of Jikes RVM from April \n10th, 2006. To ensure a fair comparison between HPM-immediate sampling with the other sampling-based \npro.lers described in Section 2, we replace only the sampling-based pro.ler in Jikes RVM. The cost/bene.t \nmodel for determining when to optimize to a speci.c optimization level and the compilers itself remain \nunchanged across all sampling-based pro.lers. In addition, we ensure that Jikes RVM s thread scheduling \nquantum re\u00admains unchanged at 20ms across different sampling-based pro.lers with different sampling rates. \nThe experiments are run on a Linux 2.6 kernel. We use the perfctr tool version 2.6.19 [22] for accessing \nthe HPMs.  4.2 Benchmarks Table 1 gives our benchmark suite. We use the SPECjvm98 benchmark suite [26] \n(.rst seven rows), the DaCapo bench\u00admark suite [9] (next six rows), and the pseudojbb bench\u00admark [25] \n(last row). SPECjvm98 is a client-side Java bench\u00admark suite. We run all SPECjvm98 benchmarks with the \nlargest input set (-s100). The DaCapo benchmark is a re\u00adcently introduced open-source benchmark suite; \nwe use re\u00adlease version 2006-10. We use only the benchmarks that execute properly on our baseline system, \nthe April 10th, 2006 CVS version of Jikes RVM. SPECjbb2000 emulates the middle-tier of a three-tier system; \nwe use pseudojbb, which runs for a .xed amount of work, i.e., for a .xed num\u00adber of transactions, in \ncontrast to SPECjbb2000, which runs for a .xed amount of time. We consider 35K transactions as the input \nto pseudojbb. The second column in Table 1 shows the number of application threads. The third column \ngives the number of application methods executed at least once; this does not include VM methods or library \nmethods used by the VM. The fourth column gives the running time on our main hardware platform, the Athlon \nXP 3000+, using the default Jikes RVM con.guration. We consider two ways of evaluating performance, namely \none-run performance and steady-state performance, and use the statistical rigorous performance evaluation \nmethodol\u00adogy as described by Georges et al. [15]. For one-run per\u00adformance, we run each application 11 \ntimes each in a new VM invocation, exclude the measurement of the .rst run, and then report average performance \nacross the remaining 10 runs. We use a Student t-test with a 95% con.dence in\u00adterval to verify that performance \ndifferences are statistically meaningful. For SPECjbb2000 we use a single warehouse for measuring one-run \nperformance. For steady-state performance, we use a similar methodol\u00adogy, but instead of measuring performance \nof a single run, we measure performance for 50 consecutive iterations of the same benchmark in 11 VM \ninvocations, of which the .rst invocation is discarded. Running a benchmark multiple Table 1. Benchmark \ncharacteristics for the default Jikes RVM con.guration on the Athlon XP 3000+ hardware plat\u00adform. Benchmark \nApplication threads No. methods executed Running time (s) compress 1 189 6.1 jess 1 590 2.9 db 1 184 \n12.1 javac 1 913 6.5 mpegaudio 1 359 5.8 mtrt 3 314 3.9 jack 1 423 3.3 antlr 1 1419 5.6 bloat 1 1891 \n14.2 fop 1 2472 6.1 hsqldb 13 1277 7.3 jython 1 3093 21.2 pmd 1 2117 14.8 pseudojbb 1 812 6.6  Processor \nFrequency L2 RAM Bus 1500+ 3000+ 1.33GHz 2.1GHz 256KB 512KB 1GB 2GB 133MHz 166MHz Table 2. Hardware \nplatforms times can be done easily for the SPECjvm98 and the DaCapo benchmarks using the running harness. \n 4.3 Hardware platforms We consider two hardware platforms, both are AMD Athlon XP microprocessor-based \ncomputer systems. The important difference between both platforms is that they run at dif\u00adferent clock \nfrequencies and have different memory hierar\u00adchies, see Table 2. The reason for using two hardware plat\u00adforms \nwith different clock frequencies is to demonstrate that the performance of HPM-sampling is more robust \nacross hardware platforms with different clock frequencies than other sampling-based pro.lers. 5. Evaluation \nThis section evaluates the HPM-immediate sampling pro\u00ad.ler and compares it against existing sampling \npro.lers. This comparison includes performance along two dimen\u00adsions: one-run and steady-state, as well \nas measurements of overhead, accuracy, and stability. 5.1 Performance Evaluation We .rst evaluate the \nperformance of the various sampling\u00adbased pro.lers we consider both one-run and steady-state performance \nin the following two subsections. 5.1.1 One-run Performance Impact of sampling rate. Before presenting \nper-benchmark performance results, we .rst quantify the impact of the sam\u00adple rate on average performance. \nFigure 2 shows the percent\u00adage average performance improvement on the Athlon XP 3000+ machine across \nall benchmarks as a function of the sampling interval compared to the default Jikes RVM, which uses a \nsampling interval of 20ms. The horizontal axis varies the sampling interval from 0.1ms to 40ms for the \nnanosleep\u00adand setitimer-sampling approaches. For the HPM-sampling approach, the sampling interval varies \nfrom 100K cycles up to 90M cycles. On the Athlon XP 3000+, this is equivalent to a sampling interval \nvarying from 0.047ms to 42.85ms. Curves are shown for all four sampling-based pro.lers; for the immediate-sampling \nmethods, we also show a version including and excluding epilogue yield-points to help quan\u00adtify the reason \nfor performance improvement. Because an immediate approach does not require any polling points, the preferred \ncon.guration for the immediate sampling ap\u00adproach is with no yield-points. We make several observations \nfrom Figure 2. First, com\u00adparing the setitimer-immediate versus the setitimer-polling curves clearly \nshows that an immediate sampling approach outperforms a polling-based sampling mechanism on our benchmark \nsuite. The setitimer-immediate curve achieves a higher speedup than setitimer-polling over the entire \nsample rate range. Second, HPM-based sampling outperforms OS\u00adtriggered sampling the HPM-immediate curve \nachieves higher speedups than setitimer-immediate. Third, removing epilogue yield-points yields a slight \nperformance improve\u00adment for both the HPM-based and OS-triggered immediate sampling approaches. So, in \nsummary the overall perfor\u00admance improvement for the HPM-sampling approach that we advocate in this paper \ncomes from three sources: (i) HPM-based triggering instead of OS-triggered sampling, (ii) immediate sampling \ninstead of polling-based sampling, and (iii) the removal of epilogue yield-points. Each sampling-based \npro.ler has a best sample rate for our benchmark suite. Values below this rate result in too much overhead. \nValues above the rate result in a less accu\u00adrate pro.le. We use an interval of 9M cycles (approximately \n4.3ms) for the HPM-immediate approach in the remain\u00adder of this paper.4 Other sampling-based pro.lers \nachieve their best performance at different sample rates in all other results presented in this paper, \nwe use the best sam\u00adple rate per sampling-based pro.ler. For example, for the setitimer-immediate approach \nwith no yield-points the best sampling interval on our benchmark suite is 4ms. The default 4 As Figure \n2 shows, we explored a wide range of values between 2M and 9M for the HPM-immediate approach. An ANOVA \nand a Tukey HSD post hoc [15, 20] test with a con.dence level of 95% reveal that in only 1.5% of the \ncases (7 out of 468 comparisons), the execution times differ signi.cantly. This means one can use any \nof the given rates in [2M; 9M] without suffering a signi.cant performance penalty. Therefore, we chose \n9M as the best value for HPM-immediate. Jikes RVM with nanosleep-polling has a sampling interval of \n20ms. Per-benchmark results. Figure 3 shows the per-benchmark percentage performance improvements of \nall sampling pro\u00ad.ler approaches (using each pro.ler s best sample rate) rel\u00adative to the default Jikes \nRVM s nanosleep-polling sampling approach, which uses a sampling interval of 20ms. The graph on the left \nin Figure 3 is for the best sample rates on the Athlon XP 1500+ machine. The graph on the right is for \nthe best sample rates on the Athlon XP 3000+. The results in Figure 3 clearly show that HPM-immediate \nsampling signi.cantly outperforms the other sampling pro\u00ad.ler approaches. In particular, HPM-immediate \nresults in an average 5.7% performance speedup compared to the default Jikes RVM nanosleep-polling approach \non the Athlon XP 3000+ machine and 3.9% on the Athlon XP 1500+ machine. HPM-immediate sampling results \nin a greater than 5% per\u00adformance speedup for many benchmarks (on the Athlon XP 3000+): antlr (5.9%), \nmpegaudio (6.5%), jack (6.6%), javac (6.6%), hsqldb (7.8%), jess (9.6%) and mtrt (18.3%). As mentioned \nbefore, this overall performance improve\u00adment comes from three sources. First, immediate sampling yields \nan average 3.0% speedup over polling-based sam\u00adpling. Second, HPM-sampling yields an additional average \n2.1% speedup over OS-triggered sampling. Third, elimi\u00adnating the epilogue yield-points contributes an \nadditional 0.6% speedup on average. For some benchmarks, removing the epilogue yield-points results in \nsigni.cant performance speedups, for example jack (4.1%) and bloat (3.4%) on the Athlon XP 3000+ machine. \nStatistical signi.cance. Furthermore, these performance improvements are statistically signi.cant. We \nuse a one\u00adsided Student t-test with a 95% con.dence level following the methodology proposed by Georges \net al. [15] to verify that HPM-immediate-no-yieldpoints does result in a signif\u00adicant performance increase \nover the non-HPM techniques. For each comparison, we require one test where the null hypothesis is that \nboth compared techniques result in the same execution time on average, the alternative hypothesis is \nthat HPM-immediate-no-yieldpoints has a smaller execution time. The null hypothesis is rejected for 10 \nout of 14 bench\u00admarks when we compare to nanosleep-polling; it is rejected for 8 out of 14 benchmarks \nwhen we compare to setitimer\u00adimmediate-no-yieldpoints; and it is rejected for even 5 out of 14 benchmarks \nwhen we compare to HPM-immediate. This means that HPM-immediate-no-yieldpoints outperforms the best execution \ntimes compared to the other techniques with 95% certainty. Robust performance across machines. The two \ngraphs in Figure 3 also show that the HPM-immediate sampling pro\u00ad.ler achieves higher speedups on the \nAthlon XP 3000+ ma\u00adchine than on the 1500+ machine. This observation supports our claim that HPM-sampling \nis more robust across hard\u00adFigure 2. Percentage average performance speedup on the Athlon XP 3000+ machine \nfor the various sampling pro.lers as a function of sample rate, relative to default Jikes RVM, which \nuses a sampling interval of 20ms. Figure 3. These .gures show the percentage performance improvement \nrelative to the default Jikes RVM con.guration (nanosleep-polling with a 20ms sampling interval). Each \ncon.guration uses the single best sampling interval as determined from the data in Figure 2 for all benchmarks. \nThe left and right graphs give improvement on the Athlon XP 1500+ with a 20M sample interval and the \nAthlon XP 3000+ with a 9M sample interval, respectively. On the former, the nanosleep-polling bars show \nno improvement, because the default con.guration rate performed best for that particular technique; as \na result, the left graph has one bar less. ware platforms, with potentially different clock frequencies. \nThe reason is that OS-triggered pro.lers collect relatively fewer samples as clock frequency increases \n(assuming that the OS time quantum remains unchanged). As such, the pro\u00ad.le collected by an OS-triggered \npro.ler becomes relatively less accurate compared to HPM-sampling when clock fre\u00adquency increases.  \n  5.1.2 Steady-state Performance We now evaluate the steady-state performance of HPM\u00adsampling for long-running \nJava applications. This is done by iterating each benchmark 50 times in a single VM invo\u00adcation. This \nprocess is repeated 11 times (11 VM invocations of 50 iterations each); the .rst VM invocation is a warmup \nrun and is discarded. Figure 4 shows the average execution time per benchmark iteration across all VM \ninvocations and all benchmarks. Two observations can be made: (i) it takes several benchmark iter\u00adations \nbefore we reach steady-state behavior, i.e., the execu\u00adtion time per benchmark iteration slowly decreases \nwith the number of iterations, and (ii) while HPM-immediate is ini\u00adtially faster, the other sampling \nmechanisms perform equally well for steady-state behavior. This suggests that HPM\u00adimmediate is faster \nat identifying and compiling hot meth\u00adods, but that if the hot methods of an application are stable and \nthe application runs long enough, the other mechanisms also succeed at identifying these hot methods. \nOnce all im\u00adportant methods have been fully optimized, no mechanism has a signi.cant competitive advantage \nover the other, and they exhibit similar behavior. In summary, HPM-sampling yields faster one-run times \nand does not hurt steady-state performance. Nevertheless, in case a long-running applica\u00adtion would experience \na phase change at some point during the execution, we believe HPM-immediate will be more re\u00adsponsive \nto this phase change by triggering adaptive recom\u00adpilations more quickly. 5.2 Analysis To get better \ninsight into why HPM-sampling results in im\u00adproved performance, we now present a detailed analysis of \nthe number of method recompilations, the optimization level these methods reach, the overhead of HPM-sampling, \nthe accuracy of a sampling pro.le, and the stability of HPM\u00adsampling across multiple runs. 5.2.1 Recompilation \nActivity Table 3 shows a detailed analysis of the number of methods sampled, the number of methods presented \nto the analytical cost/bene.t model, and the number of method recompila\u00adtions for the default system \nand the four sampling-based pro\u00ad.lers. The difference between default and nanosleep-polling is that the \ndefault system uses 20ms as the sleep interval, whereas nanosleep-polling uses the best sleep interval \nfor our benchmark suite, which is 4ms. Table 3 shows that HPM-sampling collects more samples (2066 versus \n344 to Figure 5. The average number of method recompilations by optimization level across all benchmarks \non the Athlon XP 3000+.  1249, on average) and also presents more samples to the an\u00adalytical cost/bene.t \nmodel (594 versus 153 to 408, on aver\u00adage) than OS-triggered sampling. This also results in more method \nrecompilations (134 versus 63 to 109, on average). Figure 5 provides more details by showing the number \nof method recompilations by optimization level. These are average numbers across all benchmarks on the \nAthlon XP 3000+ as reported by Jikes RVM s logging system. This .g\u00adure shows that HPM-immediate sampling \nresults in more re\u00adcompilations, and that more methods are compiled to higher levels of optimization, \ni.e., compared to nanosleep polling, HPM-immediate compiles 27% more methods to optimiza\u00adtion level 0 \nand 46% more methods to optimization level 1. Figure 3 showed that HPM-immediate sampling resulted in \nan 18.3% performance speedup for mtrt. To better under\u00adstand this speedup we performed additional runs \nof the base\u00ad Benchmark Default Nanosleep-polling Setitimer-polling Setitimer-immediate HPM-immediate \nS P C S P C S P C S P C S P C compress 258 162 16 1010 173 15 1190 195 17 977 288 19 1536 437 23 jess \n126 82 35 425 191 45 499 216 47 416 199 49 714 299 55 db 530 56 9 1950 87 9 2340 92 9 1959 159 9 3349 \n189 10 javac 251 147 80 893 447 139 1048 510 159 1004 426 150 1647 620 195 mpegaudio 250 199 58 900 453 \n75 1065 526 80 906 634 81 1489 908 98 mtrt 128 98 33 460 204 54 529 230 58 538 281 63 847 382 82 jack \n140 56 27 510 162 41 585 192 47 451 110 22 758 185 32 antlr 249 120 60 855 339 100 1040 402 112 750 269 \n105 1263 403 138 bloat 601 150 60 2145 404 111 2595 490 117 2255 329 111 3733 486 144 fop 273 60 37 995 \n199 63 1175 230 70 576 185 74 957 275 103 hsqldb 247 180 91 869 478 133 995 525 138 1117 500 152 1816 \n747 181 jython 928 426 203 3294 1079 342 3813 1220 357 3169 1145 350 5178 1667 426 pmd 565 244 93 1995 \n623 154 2355 721 165 2355 704 158 3938 989 206 pseudojbb 271 161 85 950 451 129 1114 521 146 1014 489 \n152 1697 729 190 average 344 153 63 1232 378 101 1453 433 109 1249 408 107 2066 594 134 Table 3. Detailed \nsample pro.le analysis for one-run performance on the Athlon XP 3000+: S stands for the number of method \nsamples taken, P stands for the number of methods that have been proposed to the analytical cost/bene.t \nmodel and C stands for the number of method recompilations. line and HPM-immediate con.gurations with \nJikes RVM s logging level set to report the time of all recompilation events.5 We ran each con.guration \n11 times and the sig\u00adni.cant speedup of HPM, relative to the baseline con.gu\u00adration, occurred on all \nruns, therefore, we conclude that the compilation decisions leading to the speedup were present in all \n11 HPM-immediate runs. There were 34 methods that were compiled by the HPM-immediate con.guration in \nall runs, and of these 34 methods, only 13 were compiled on all baseline runs. Taking the average time \nat which these 13 methods were compiled, HPM compiles these methods 28% sooner, with a maximum of 47% \nsooner and a minimum of 3% sooner. Although this does not concretely explain why HPM-immediate is improving \nperformance, it does show that HPM-immediate is more responsive in compiling the important methods, which \nmost likely explains the speedup.  5.2.2 Overhead Collecting samples and recompiling methods obviously \nin\u00adcurs overhead. To amortize this cost, dynamic compilation systems need to balance code quality with \nthe time spent sampling methods and compiling them. To evaluate the over\u00adhead this imposes on the system, \nwe investigate its two com\u00adponents: (i) the overhead of collecting the samples and (ii) the overhead \nof consuming these samples by the adaptive optimization system. As explained in Section 2.2, the latter \nis composed of three parts implemented as separate threads in Jikes RVM: (a) the organizer, (b) the controller, \nand (c) the compiler. To identify the overhead of only collecting samples, we modi.ed Jikes RVM to discard \nsamples after they have been 5 This logging added a small amount of overhead to each con.guration, but \nthe speedup remained about the same.  Figure 6. The average overhead through HPM-immediate\u00adno-yieldpoints \nsampling for collecting samples at various sample rates. captured in both an HPM-immediate con.guration \nand the baseline nanosleep-polling con.guration. In both con.gu\u00adrations no samples are analyzed and no \nmethods are re\u00adcompiled by the optimizing compiler. By comparing the execution times from the HPM-immediate \ncon.guration with those of the default Jikes RVM con.guration that uses nanosleep-polling with the 20ms \nsample interval, we can study the overhead of collecting HPM samples. Figure 6 shows this overhead for \na range of HPM sample rates aver\u00adaged across all benchmarks; at the sampling interval of 9M cycles, the \noverhead added by HPM-immediate sampling is limited to 0.2%. To identify the overhead of processing the \nsamples we look at the time spent in the organizer, controller, and com\u00adpiler. Because each of these \nsubsystems runs in their own Figure 7. The average overhead of consuming the samples across all benchmark. \nThe default systems uses 20ms as the sample interval, where as the other systems use their best sample \nintervals for our benchmark suite. thread, we use Jikes RVM s logging system to capture each thread \ns execution time. Figure 7 shows the fraction of the time spent in the organizer, controller, and the \ncom\u00adpiler for all sampling pro.ler approaches averaged across all benchmarks. Based on Figure 7, we conclude \nthat al\u00adthough HPM-immediate collects many more samples, the overhead of the adaptive optimization system \nincreases by only roughly 1% (mainly due to the compiler). This illus\u00adtrates that Jikes RVM s cost/bene.t \nmodel successfully bal\u00adances code quality with time spent collecting samples and recompiling methods \n even when presented with signif\u00adicantly more samples as discussed in the previous section. We believe \nthis property is crucial for the applicability of HPM-immediate sampling.  5.2.3 Accuracy To assess \nthe accuracy of the pro.le collected through HPM\u00adimmediate sampling, we would like to compare the various \nsampling approaches with a perfect pro.le. A perfect pro.le satis.es the property that the number of \nsamples for each method executed is perfectly correlated with the time spent in that method. Such a pro.le \ncannot be obtained, short of doing complete system simulation. Instead, we obtain a detailed pro.le using \na frequent HPM sample rate (a sample is taken every 10K cycles) and compare the pro.les collected through \nthe various sampling pro.ler approaches with this detailed pro.le. We determine accuracy as follows. \nWe run each bench\u00admark 30 times in a new VM instance for each sampling ap\u00adproach (including the detailed \npro.le collection) and capture how often each method is sampled in a pro.le. A pro.le is a vector where \neach entry represents a method along with its sample count. We subsequently compute an average pro\u00ad.le \nacross these 30 benchmark runs. We then use the metrics described below to determine the accuracy for \na sampling Figure 8. This graph quanti.es sampling accuracy using the unweighted accuracy metric: the \naverage overlap with the detailed pro.le is shown on the vertical axis for the top N percent of hot methods \non the horizontal axis. approach by comparing its pro.le with the detailed pro.le. We use both an unweighted \nand a weighted metric. Unweighted metric. The unweighted metric gives the percentage of methods that \nappear in both vectors, re\u00adgardless of their sample counts. For example, the vectors x = ((a, 5), (b, \n0), (c, 2)) and y = ((a, 30), (b, 4), (c, 0)) have corresponding presence vectors px = (1, 0, 1) and \npy = (1, 1, 0), respectively. The common presence vector then is pcommon = (1, 0, 0). Therefore, pcommon \nhas a score of .33, which is the sum of its elements divided by the num\u00adber of elements. This metric \nattempts to measure how many methods in the detailed pro.le also appear in the pro.les for the sam\u00adpling \napproach of interest. Because the metric ignores how often a method is sampled in the detailed pro.le, \nit seems appropriate to consider only the top N percent of most fre\u00adquently executed methods in the detailed \npro.le. Figure 8 shows the average unweighted metric for the various sampling-based pro.lers for the \ntop N percent of hot methods. The horizontal axis shows the value of N. The vertical axis shows the unweighted \nmetric score, so higher is better for this metric. For example, the accuracy of the pro.lers .nding the \ntop 20% of methods found in the detailed pro.le is 57.4%, 56.3%, 70.2%, and 74.5% for nanosleep-polling, \nsetitimer-polling, setitimer-immediate\u00adno-yieldpoints, and HPM-immediate-no-yieldpoints, respec\u00adtively. \nImmediate sampling techniques are clearly superior to polling-based techniques on our benchmark suite. \nWeighted metric. The weighted metric considers the sam\u00adple counts associated with each method and computes \nan overlap score. To determine the weighted overlap score, we .rst normalize each vector with respect \nto the total num\u00adber of samples taken in that vector. This yields two vectors with relative sample counts. \nTaking the element-wise min\u00adFigure 9. This .gure shows the accuracy using the weighted metric of various \nsampling-based pro.lers compared to the detailed pro.le. imum of these vectors gives the weighted presence \nvector. The score then is the sum of the elements in the presence vector. For example, for x and y as \nde.ned above, the score is 0.71. Indeed, the only element that has samples in both 5 30 vectors, a, scores \n=0.71 in x and =0.88 in 5+0+2 30+4+0 y. Figure 9 demonstrates the accuracy using the weighted metric \nfor all benchmarks. This graph shows that polling\u00adbased sampling achieves the lowest accuracy on average \n(59.3%). The immediate techniques score much better, at\u00adtaining 74.2% on average for setitimer. HPM improves \nthis even further to a 78.0% accuracy on average compared to the detailed pro.le.  5.2.4 Stability It \nis desirable for a sampling mechanism to detect hot meth\u00adods in a consistent manner, i.e., when running \na program twice, the same methods should be sampled (in the same rel\u00adative amount) and optimized so that \ndifferent program runs result in similar execution times. We call this sampling sta\u00adbility. To evaluate \nstability, we perform 30 runs holding the sampling mechanism constant. We use the weighted metric described \nin the previous section to pairwise compare all the vectors for the different benchmark runs with the \nsame sam\u00adpling mechanism. We report the stability score as the mean of these values. For example, given \nthree vectors x = ((a, 5), (b, 1), (c, 4)), y = ((a, 6), (b, 0), (c, 3)) and z = ((a, 5), (b, 2), (c, \n3)) that correspond to three benchmark runs of a particular con.guration, the stability is computed as \nfollows. First, we normalize the vectors and take the element-wise min\u00adimum of all the different combinations \nof vectors. Com\u00ad 11 paring x and y yields ((a, ), (b, 0), (c, )), comparing x 23 11 3 and z yields ((a, \n), (b, ), (c, )) and comparing y and 2 10 10 13 z results in ((a, ), (b, 0), (c, )). Next, we compute \nthe 2 10 sum of the elements in each of the vectors and compute the .nal stability score as the mean \nof these values. That is, 0.83+0.9+0.8 3 =0.84. Figure 10 compares the stability of the following .ve \ncon\u00ad.gurations: (i) nanosleep-polling, (ii) setitimer-polling, (iii) setitimer-immediate, (iv) HPM-immediate, \nand (v) the HPM con.guration with a sample rate of 10K cycles, i.e., the de\u00adtailed pro.le. The detailed \npro.le is very stable (on average the stability is 95.1%). On average, nanosleep-polling and setitimer-polling \nhave a stability score of 75.9% and 76.6%, respectively. The average stability for setitimer-immediate \nis 78.2%, and HPM-immediate has the best stability score (83.3%) of the practical sampling approaches. \n6. Related Work This section describes related work. We focus mostly on pro\u00ad.ling techniques for .nding \nimportant methods in language\u00adlevel virtual machines. We brie.y mention other areas re\u00adlated to online \npro.ling in dynamic optimization systems. 6.1 Existing Virtual Machines As mentioned earlier, Jikes RVM \n[6] uses a compile-only approach to program execution with multiple recompilation levels. All recompilation \nis driven by a polling-based sam\u00adpler that is triggered by an operating system timer. BEA s JRockit [8, \n24] VM also uses a compile-only ap\u00adproach. A sampler thread is used to determine methods to optimize. \nThis thread suspends the application threads and takes a sample at regular intervals. Although full details \nare not publicly available, the sampler seems to be triggered by an operating system mechanism. It is \nnot clear if the sam\u00adples are taken immediately or if a polling approach is used. Friberg s MS thesis \n[14] extends JRockit to explore the use of HPM events on an Itanium 2 to better perform speci.c compiler \noptimizations in a JIT. The thesis reports that us\u00ading HPM events improves performance by an average \nof 4.7%. This work also reports that using HPM events to drive only recompilation (as we advocate in \nthis work) does not improve performance, but does compile fewer methods. In a workshop presentation, \nEastman et al. [13] report similar work to Friberg s work. The slides claim to extend JRockit to use \nan HPM-immediate approach using Itanium 2 events to drive optimization. Unlike Friberg they do not report \nhow the new approach compares to existing approach for .nding op\u00adtimization candidates, but instead focus \non how HPM events improve performance when used by compiler optimizations. Whaley [30] describes a sampling \npro.ler that is imple\u00admented as a separate thread. The thread awakes periodically and samples the application \nthreads for the purpose of build\u00ading a dynamic call graph. He mentions that this sampling thread could \nbe triggered by operating system or processor mechanisms, but used an operating system sleep function \nin his implementation. No performance results are reported comparing the various trigger approaches. \nThe IBM DK for Java [28] interprets methods initially and uses method entry and back edge counters to \n.nd methods to optimize. Multi\u00adple optimization levels are used. A variant of the sampling technique \ndescribed by Whaley (without building the dy\u00adnamic call graph) is used to detect compiled methods that \nrequire further optimization. IBM s J9 VM [18] is similar to the IBM DK for Java in that it is also interpreter-based \nwith multiple optimization levels. It uses a sampling thread to periodically take samples. The implementation \nof the sampler is similar to Jikes RVM in that it uses a polling-based approach that is triggered by \nan operating system timer. Intel s ORP VM [10] employs a compile-only strategy with one level of recompilation. \nBoth the initial and the optimizing compiler use per-method counters to track im\u00adportant methods. A method \nis recompiled when the counter passes a threshold or when a separate thread .nds a method with a counter \nvalue that suggests compiling it in the back\u00adground. Optimized methods are not recompiled. No sam\u00adpling \nis used. In a technical report, Tam et al. [29] extend Intel s ORP to use HPMs to trigger dynamic code \nrecompilation. They in\u00adstrument method prologues and epilogues to read the HPM cycle counter. The cycle \ncounter value is read upon invo\u00adcation of a method and upon returning from the method, and used to compute \nthe time spent in each method. These deltas are accumulated on a per method basis, and when a method \ns accumulated time exceeds a given threshold (and that method has been executed at least two times), \nthe method is recompiled at the next optimization level. They report large overheads, and conclude that \nthis technique can\u00adnot be used in practice. Our approach is different in that it does not use instrumentation \nand relies on sampling to .nd candidate methods. Sun s HotSpot VM [21] interprets methods initially and \nuses method entry and back edge counters to .nd methods to optimize. No published information is available \non what, if any, mechanism exists for recompiling optimized methods and if sampling is used. However, \nHotSpot was greatly in.u\u00adenced by the Self-93 system [16], which used a compile-only approach triggered \nby decayed invocation counters. Opti\u00admized methods were not further optimized at higher levels. In summary, \nno production VMs use HPMs as a trig\u00adger mechanism for .nding optimization candidates. Two de\u00adscriptions \nof extending JRockit to use HPMs do exist in the form of MS thesis and a slides-only workshop. In contrast \nto our work, neither showed any improvement using this ap\u00adproach and no comprehensive evaluation was \nperformed. 6.2 Other Related Work Lu et al. [17] describe the ADORE dynamic binary opti\u00admization system, \nwhich uses event-sampling of HPMs to construct a path pro.le that is used to select traces for op\u00adtimization. \nAt a high level this is a similar approach to what we advocate, HPM pro.ling for .nding optimization \ncandi\u00addates, but details of the optimization system (a binary opti\u00admizer versus a virtual machine) are \nquite different. Adl-Tabatabai et al. [1] used HPMs as a pro.ling mecha\u00adnism to guide the placement of \nprefetch instructions in the ORP virtual machine. We believe prefetching instructions are inserted by \nforcing methods to be recompiled. Although this work may compile a method more than once based on HPM \ninformation, it does not rely on HPMs as a general pro.ling mechanism to .nd optimization candidates. \nSu and Lipasti [27] describe a hybrid hardware-software system that relies on hardware support for detecting \nexceptions in spec\u00adi.ed regions of code. The virtual machine then performs speculative optimizations \non these guarded region. Schnei\u00adder et al. [23] show how hardware performance monitors can be used to \ndrive locality-improving optimizations in a virtual machine. Although these works are positive exam\u00adples \nof how hardware can be used to improve performance in a virtual machine environment, neither address \nthe speci.c problem we address: .nding candidates for recompilation. Ammons et al. [4] show how HPMs \ncan be incorporated into an of.ine path pro.ler. Andersen et al. [5] describe the DCPI pro.ler, which \nuses HPMs to trigger a sampling approach to understand program performance. None of these works use HPMs \nto .nd optimization candidates. Zhang et al. [31] describe the Morph Monitor component of an online binary \noptimizer that uses an operating sys\u00adtem kernel extension to implement an immediate pro.ling approach. \nDuesterwald et al. [12] describe a low-overhead software pro.ling scheme for .nding hot paths in a binary \noptimization systems. Although both works are tackling the problem of .nding optimization candidates, \nneither use HPMs. Merten et al. [19] propose hardware extensions to monitor branch behavior for runtime \noptimization at the basic block and trace stream level. Conte et al. [11] describe a hardware extension \nfor dedicated pro.ling. Our work differs in that we use existing hardware mechanisms for .nding hot methods. \n7. Conclusions and Future Work To our knowledge, this is the .rst work to empirically eval\u00aduate the design \nspace of several sampling-based pro.lers for dynamic compilation. We describe a technique, HPM\u00adsampling, \nthat leverages hardware performance monitors (HPMs) to identify methods for optimization. In addition, \nwe show that an immediate sampling approach is signif\u00adicantly more accurate in identifying hot methods \nthan a polling-based approach. Furthermore, an immediate sam\u00adpling approach allows for eliminating epilogue \nyield-points. We show that, when put together, HPM-immediate sam\u00adpling with epilogue yield-point elimination \noutperforms all other sampling techniques. Compared to the default Jikes RVM con.guration, we report \na performance improvement of 5.7% on average and up to 18.3% without modifying the compiler. Moreover, \nwe show that HPM-based sampling consistently improves the accuracy, robustness, and stability of the \ncollected sample data. We believe there are a number of potentially interesting directions for future \nresearch. First, this paper used the cy\u00adcle counter event to sample methods. Other HPM events may also \nbe useful for identifying methods for optimization, such as cache miss count events and branch misprediction \ncount events. Second, it may be interesting to apply the improved accuracy of HPM-based sampling to other \nparts of the adap\u00adtive optimization system, such as method inlining. This work has demonstrated that \nthere is potential for better synergy between the hardware and virtual machines. Both try to exploit \na program s execution behavior, but little synergy has been demonstrated to date. This is partially due \nto the narrow communication channel between hardware and software, as well as the lack of cross-subdiscipline \ninnova\u00adtion in these areas. We hope this work encourages others to explore this fruitful area of greater \nhardware-virtual machine synergy. Acknowledgments We thank Steve Fink, David Grove, and Martin Hirzel \nfor useful discussions about this work and Laureen Treacy for proofreading earlier drafts of this work. \nWe also thank the anonymous reviewers for their valuable feedback. Dries Buytaert is supported by a fellowship \nfrom the Institute for the Promotion of Innovation by Science and Technology in Flanders (IWT). Andy \nGeorges is a Research Assistant at Ghent University. Lieven Eeckhout is a Postdoc\u00adtoral Fellow of the \nFund for Scienti.c Research Flanders (Belgium) (F.W.O. Vlaanderen). Ghent University is a member of the \nHiPEAC Network of Excellence. References [1] A.-R. Adl-Tabatabai, R. L. Hudson, M. J. Serrano, and S. \nSubramoney. Prefetch injection based on hardware monitoring and object metadata. In Proceedings of the \nACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI), 267 276, June 2004. \n [2] B. Alpern, C. R. Attanasio, J. J. Barton, M. G. Burke, P. Cheng, J.-D. Choi, A. Cocchi, S. J. Fink, \nD. Grove, M. Hind, S. F. Hummel, D. Lieber, V. Litvinov, M. F. Mergen, T. Ngo, J. R. Russell, V. Sarkar, \nM. J. Serrano, J. C. Shepherd, S. E. Smith, V. C. Sreedhar, H. Srinivasan, and J. Whaley. The Jalape \nno virtual machine. IBM Systems Journal, 39(1):211 238, Feb. 2000. [3] B. Alpern, S. Augart, S. Blackburn, \nM. Butrico, A. Cocchi, P. Cheng, J. Dolby, S. Fink, D. Grove, M. Hind, K. McKinley, M. Mergen, J. Moss, \nT. Ngo, V. Sarkar, and M. Trapp. The Jikes Research Virtual Machine project: Building and open-source \nresearch community. IBM Systems Journal, 44(2):399 417, 2005. [4] G. Ammons, T. Ball, and J. R. Larus. \nExploiting hardware performance counters with .ow and context sensitive pro.ling. In Proceedings of the \nACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI), 85 96, May 1997. [5] \nJ. M. Anderson, L. M. Berc, J. Dean, S. Ghemawat, M. R. Henzinger, S. tak A. Leung, R. L. Sites, M. T. \nVandevoorde, C. A. Waldspurger, and W. E. Weihl. Continuous pro.ling: Where have all the cycles gone? \nACM Transactions on Computer Systems, 15(4):357 390, Nov. 1997. [6] M. Arnold, S. Fink, D. Grove, M. \nHind, and P. F. Sweeney. Adaptive optimization in the Jalape no JVM. In Proceedings of the ACM SIGPLAN \nConference on Object-Oriented Pro\u00adgramming, Systems, Languages and Applications (OOPSLA), 47 65, Oct. \n2000. [7] M. Arnold, S. Fink, D. Grove, M. Hind, and P. F. Sweeney. Architecture and policy for adaptive \noptimization in virtual machines. Technical Report 23429, IBM Research, Nov. 2004. [8] BEA. BEA JRockit: \nJava for the enterprise Technical white paper. http://www.bea.com, Jan. 2006. [9] S. M. Blackburn, R. \nGarner, C. Hoffmann, A. M. Khang, K. S. McKinley, R. Bentzur, A. Diwan, D. Feinberg, D. Frampton, S. \nZ. Guyer, M. Hirzel, A. Hosking, M. Jump, H. Lee, J. Eliot, B. Moss, A. Phansalkar, D. Stefanovic,\u00b4 T. \nVanDrunen, D. von Dincklage, and B. Wiedermann. The DaCapo benchmarks: Java benchmarking development \nand analysis. In Proceedings of the ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages \nand Applications (OOPSLA), 169 190, Oct. 2006. [10] M. Cierniak, M. Eng, N. Glew, B. Lewis, and J. Stichnoth. \nThe open runtime platform: A .exible high-performance managed runtime environment. Intel Technology Journal, \n7(1):5 18, 2003. [11] T. M. Conte, K. N. Menezes, and M. A. Hirsch. Accurate and practical pro.le-driven \ncompilation using the pro.le buffer. In Proceedings of the Annual ACM/IEEE International Symposium on \nMicroarchitecture (MICRO), 36 45, Dec. 1996. [12] E. Duesterwald and V. Bala. Software pro.ling for hot \npath prediction: Less is more. In Proceedings of the International Conference on Architectural Support \nfor Programming Languages and Operating Systems (ASPLOS), 202 211, Nov. 2000. [13] G. Eastman, S. Aundhe, \nR. Knight, and R. Kasten. Dynamic pro.le-guided optimization in the BEA JRockit JVM, In 3rd Workshop \non Managed Runtime Environments (MRE) held in conjunction with the IEEE/ACM International Symposium on \nCode Generation and Optimization (CGO), Mar. 2005. [14] S. Friberg. Dynamic pro.le guided optimization \nin a VEE on IA-64. Master s thesis, KTH Royal Institute of Technology, 2004. IMIT/LECS-2004-69. [15] \nA. Georges, D. Buytaert, and L. Eeckhout. Statistically rig\u00adorous Java performance evaluation. In Proceedings \nof the ACM SIGPLAN Conference on Object-Oriented Program\u00adming, Systems, Languages, and Applications (OOPSLA), \nOct. 2007. [16] U. H\u00a8olzle and D. Ungar. Reconciling responsiveness with performance in pure object-oriented \nlanguages. ACM Transactions on Programming Languages and Systems (TOPLAS), 18(4):355 400, July 1996. \n[17] J. Lu, H. Chen, P.-C. Yew, and W.-C. Hsu. Design and implementation of a lightweighted dynamic optimization \nsystem. Journal of Instruction-Level Parallelism, 6, 2004. [18] D. Maier, P. Ramarao, M. Stoodley, and \nV. Sundaresan. Ex\u00adperiences with multithreading and dynamic class loading in a Java just-in-time compiler. \nIn Proceedings of the Inter\u00adnational Symposium on Code Generation and Optimization (CGO), 87 97, Mar. \n2006. [19] M. C. Merten, A. R. Trick, R. D. Barnes, E. M. Nystrom, C. N. George, J. C. Gyllenhaal, and \nW. mei W. Hwu. An architectural framework for runtime optimization. IEEE Transactions on Computers, 50(6):567 \n589, 2001. [20] J. Neter, M. H. Kutner, W. Wasserman, and C. J. Nachtsheim. Applied Linear Statistical \nModels. WCB/McGraw-Hill, 1996. [21] M. Paleczny, C. Vick, and C. Click. The Java Hotspot server compiler. \nIn Proceedings of the Java Virtual Machine Research and Technology Symposium (JVM), pages 1 12, Apr. \n2001. [22] perfctr. perfctr version 2.6.19. http://user.it.uu.se/ mikpe/linux/perfctr. [23] F. T. Schneider, \nM. Payer, and T. R. Gross. Online optimizations driven by hardware performance monitoring. In Proceedings \nof the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI), 373 382, June \n2007. [24] K. Shiv, R. Iyer, C. Newburn, J. Dahlstedt, M. Lagergren, and O. Lindholm. Impact of JIT JVM \noptimizations on Java application performance. In Proceedings of the 7th Annual Workshop on Interaction \nbetween Compilers and Computer Architecture (INTERACT) held in conjunction with the International Symposium \non High-Performance Computer Architecture (HPCA), 5 13, Mar. 2003. [25] Standard Performance Evaluation \nCorporation. SPECjbb2000 Java Business Benchmark. http://www.spec.org/jbb2000. [26] Standard Performance \nEvaluation Corporation. SPECjvm98 Benchmarks. http://www.spec.org/jvm98. [27] L. Su and M. H. Lipasti. \nSpeculative optimization using hardware-monitored guarded regions for Java virtual ma\u00adchines. In Proceedings \nof the 3rd International Conference on Virtual Execution Environments (VEE), 22 32, June 2007. [28] T. \nSuganuma, T. Yasue, M. Kawahito, H. Komatsu, and T. Nakatani. Design and evaluation of dynamic optimizations \nfor a Java just-in-time compiler. ACM Transactions on Programming Languages and Systems (TOPLAS), 27(4):732 \n785, July 2005. [29] D. Tam and J. Wu. Using hardware counters to improve dynamic compilation. Technical \nReport ECE1724, Elec\u00adtrical and Computer Engineering Department University of Toronto, Dec. 2003. [30] \nJ. Whaley. A portable sampling-based pro.ler for Java virtual machines. In Proceedings of the ACM 2000 \nConference on Java Grande, 78 87, June 2000. [31] X. Zhang, Z. Wang, N. Gloy, J. B. Chen, and M. D. Smith. \nSystem support for automatic pro.ling and optimization. In Proceedings of the Sixteenth ACM Symposium \non Operating Systems Principles (SOSP), 15 26, Oct. 1997.   \n\t\t\t", "proc_id": "1297027", "abstract": "<p>All high-performance production JVMs employ an adaptive strategy for program execution. Methods are first executed unoptimized and then an online profiling mechanism is used to find a subset of methods that should be optimized during the same execution. This paper empirically evaluates the design space of several profilers for initiating dynamic compilation and shows that existing online profiling schemes suffer from several limitations. They provide an insufficient number of samples, are untimely, and have limited accuracy at determining the frequently executed methods. We describe and comprehensively evaluate HPM-sampling, a simple but effective profiling scheme for finding optimization candidates using hardware performance monitors (HPMs) that addresses the aforementioned limitations. We show that HPM-sampling is more accurate; has low overhead; and improves performance by 5.7% on average and up to 18.3% when compared to the default system in Jikes RVM, without changing the compiler.</p>", "authors": [{"name": "Dries Buytaert", "author_profile_id": "81100468396", "affiliation": "Ghent University, Ghent, Belgium", "person_id": "P698113", "email_address": "", "orcid_id": ""}, {"name": "Andy Georges", "author_profile_id": "81100487568", "affiliation": "Ghent University, Ghent, Belgium", "person_id": "P643379", "email_address": "", "orcid_id": ""}, {"name": "Michael Hind", "author_profile_id": "81339504521", "affiliation": "IBM T.J. Watson Research, Hawthorne, NY", "person_id": "PP39096373", "email_address": "", "orcid_id": ""}, {"name": "Matthew Arnold", "author_profile_id": "81100021720", "affiliation": "IBM T.J. Watson Research, Hawthorne, NY", "person_id": "PP39023576", "email_address": "", "orcid_id": ""}, {"name": "Lieven Eeckhout", "author_profile_id": "81330490198", "affiliation": "Ghent University, Ghent, Belgium", "person_id": "PP40033523", "email_address": "", "orcid_id": ""}, {"name": "Koen De Bosschere", "author_profile_id": "81100123309", "affiliation": "Ghent University, Ghent, Belgium", "person_id": "P162550", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1297027.1297068", "year": "2007", "article_id": "1297068", "conference": "OOPSLA", "title": "Using hpm-sampling to drive dynamic compilation", "url": "http://dl.acm.org/citation.cfm?id=1297068"}