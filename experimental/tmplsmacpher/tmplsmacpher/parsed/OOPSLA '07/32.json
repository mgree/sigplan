{"article_publication_date": "10-21-2007", "fulltext": "\n Making Trace Monitors Feasible Pavel Avgustinov Julian Tibble Oege de Moor Programming Tools Group, \nUniversity of Oxford, UK {pavel.avgustinov, julian.tibble, oege.de.moor}@comlab.ox.ac.uk Abstract A \ntrace monitor observes an execution trace at runtime; when it recognises a speci.ed sequence of events, \nthe mon\u00aditor runs extra code. In the aspect-oriented programming community, the idea originated as a \ngeneralisation of the advice-trigger mechanism: instead of matching on single events (joinpoints), one \nmatches on a sequence of events. The runtime veri.cation community has been investigating similar mechanisms \nfor a number of years, specifying the event patterns in terms of temporal logic, and applying the monitors \nto hardware and software. In recent years trace monitors have been adapted for use with mainstream object-oriented \nlanguages. In this setting, a crucial feature is to allow the programmer to quantify over groups of related \nobjects when expressing the sequence of events to match. While many language proposals exist for allowing \nsuch features, until now no implementation had scalable performance: execution on all but very simple \nexamples was infeasible. This paper recti.es that situation, by identifying two opti\u00admisations for generating \nfeasible trace monitors from declar\u00adative speci.cations of the relevant event pattern. We restrict ourselves \nto optimisations that do not have a signi.cant im\u00adpact on compile-time: they only analyse the event pattern, \nand not the monitored code itself. The .rst optimisation is an important improvement over an earlier \nproposal in [2] to avoid space leaks. The second optimisation is a form of indexing for partial matches. \nSuch indexing needs to be very carefully designed to avoid intro\u00adducing new space leaks, and the resulting \ndata structure is highly non-trivial. Categories and Subject Descriptors D.3.4 [Programming Languages]: \nProcessors Compilers General Terms Experimentation, Languages, Performance Permission to make digital \nor hard copies of all or part of this work for personal or classroom use is granted without fee provided \nthat copies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. OOPSLA 07, October 21 25, 2007, Montr\u00b4eal, \nQu\u00b4ebec, Canada. Copyright c .2007 ACM 978-1-59593-786-5/07/0010. . . $5.00 Keywords Program monitors, \nruntime veri.cation, pro\u00adgram analysis 1. INTRODUCTION Trace monitors observe the current execution trace, \nand ex\u00adecute some extra code when the trace matches a given pat\u00adtern. Many runtime veri.cation concerns \ncan be expressed as trace monitors very naturally, simply by picking out vio\u00adlating traces. A trace monitor \nis usually speci.ed declaratively in two parts: .rstly, a pattern describing which traces should match, \nand secondly, an action that should be executed when a pro\u00adgram trace matches. The actual implementation \nof the trace monitor is automatically generated from its speci.cation, typically in the form of instrumentation \nof a base program. There is a very large amount of previous research on this topic, e.g. [2, 7, 8, 11, \n13 16, 19, 21, 22, 25, 26, 28, 30]. These studies range from applications in medical image generation \nthrough business rules to theoretical investigations of the un\u00adderlying calculus. The way the patterns \nare speci.ed varies, and temporal logic, regular expressions and context-free lan\u00adguages have all been \nconsidered. One theme shines through all of these previous works: trace monitors are an attractive, useful \nnotion, worthy of integration into a mainstream programming language. This has not happened, however, \nbecause it turns out to be very dif.cult to generate ef.cient code when the trace monitor is phrased \nas a declarative speci.cation. The challenge is particularly severe when the speci.ca\u00adtions trace the \nbehaviour of a group of several objects si\u00admultaneously. For example, when checking that a lock is al\u00adways \nacquired and released in the same method invocation, we need to track the locked resource, as well as \nthe method invocation in question. Similarly, when checking that a col\u00adlection is not modi.ed while iteration \nis in progress, we need to track the state of the collection as well as its iterators. For this reason, \nnumerous of the above proposals (in partic\u00adular [2, 11, 14, 21, 25]) have a facility for binding multiple \nvariables during the matching process. Many more systems do not allow free variables in the trace patterns; \nin our ex\u00adperience, this feature is indispensable for many real-world examples, and therefore the focus \nof this work is optimising systems with free variables. In our experiments, however, none of these systems \n(in\u00adcluding our own in [2]) managed to generate feasible trace monitors. The generated monitors are adequate \nas a proof of concept, but they cannot be used in practice on substantial systems. For some applications, \nit may be possible to use the sub\u00adstantial body of related work on static type-state veri.ca\u00adtion, e.g. \n[18]. However, that invariably entails interprocedu\u00adral analysis of the observed code. This is costly, \nand the as\u00adsumption that all the code is available before load-time can\u00adnot be satis.ed in practice. \nFurthermore, before such costly techniques are explored, we must .rst determine how far one can get with \ncheaper techniques. Contribution This paper shows, for the .rst time, how to generate feasible trace \nmonitors from declarative speci.ca\u00adtions that use free variables. Furthermore, our techniques rely only \non analysis of the trace speci.cation, not on costly whole-program analyses of the monitored code. Any \nmon\u00aditoring system, regardless of the chosen speci.cation for\u00admalism, must implement the techniques presented \nhere to achieve feasibility. Speci.cally, the detailed contributions are as follows: Benchmarks: We \npresent the .rst benchmark set of substantial, realistic applications of runtime monitoring.  Each of \nthese monitors has been coded by hand (in the programming language AspectJ), and also in the speci.\u00adcation \nformalism of [2].  This set of benchmarks (which is publicly available at [1]) thus provides the .rst \nsolid basis for experimental evaluation of trace monitoring features.  Leak detection and prevention: \n We demonstrate that space leaks are a show-stopping bottleneck when na\u00a8ively generating trace monitors. \n In [2] an analysis was brie.y sketched for eliminating that problem. We identify a crucial .aw in that \nanalysis, and show how it can be remedied via the novel notion of persistent weak references. We then \ngo on to show how to extend the analysis to detect and prevent additional leaks.  Unlike [2], we then \nproceed to carefully evaluate the effects of the optimisation.  Indexing of partial solution sets: \nWe exhibit another show-stopping performance problem, namely the need to update the set of partial solutions \nwhenever a relevant event occurs.  We propose an automatic technique for choosing an ap\u00adpropriate index \nstructure on the set of partial solutions, which is purely based on the monitor speci.cation.  Furthermore, \nwe present an indexing data structure that does not introduce new space leaks, and thus combines well \nwith the above leak prevention technique. Again this involves very careful and subtle use of weak references: \nna\u00a8ive indexing would worsen space leakage.  Finally, we provide a set of algorithms to update such \nindexing data structures (avoiding costly unnecessary in\u00adtermediate data-structures) and present an argument \nfor why these algorithms are correct.  2. TRACE MONITORING We .rst outline the basic strategy for generating \nexecutable code from trace monitors, via a speci.c example. Variations of the same code generation strategy \ncan be found in any trace monitoring system, but our primary focus is on trace\u00admatches [2] the system \nthat this work evaluates and opti\u00admises. As previously mentioned, a trace monitor is a combina\u00adtion of \na pattern and an action to run when the program trace matches that pattern. In a tracematch, the pattern \nis de.ned in three parts: a set of variables, an alphabet of symbols (de\u00ad.ned in terms of the variables), \nand a regular expression over the symbols. A common runtime veri.cation property is that of safe\u00adenumeration: \nAfter an enumeration is created, the data-source upon which it is based may not be modi.ed while the \nenu\u00admeration is in use that is, until the last call to its nextElement() method. To check this property \nwith a tracematch, two variables are required. One of them will range over collections that might be \niterated; in the Java 1.2 API these are instances of the class Vector, and so we will use the identi.er \nv. The second variable, e, ranges over enumerations. Symbols are de.ned using AspectJ pointcuts a language \nfor intercepting run\u00adtime events [23] but for clarity we give an informal de.\u00adnition of the three required \nsymbols. CREATE an enumeration e is created from a Vector v UPDATE the Vector v is modi.ed NEXT the nextElement() \nmethod is called on e Finally, the regexp which speci.es a violation of the safe\u00adenumeration property \nis CREATE NEXT* UPDATE+ NEXT . What does it mean for such a regular expression (which includes free variables) \nto match the program trace? Con\u00adsider the execution history shown in Table 1. The left side of the table \nshows a sequence of symbol-matching events from a program trace. For each event, the corresponding symbol \nand the objects bound to v and e are shown. On the right side of the table, we show two substitutions \nof objects for tracematch variables. Each substitution de.nes a projection of the original sequence to \na string over the sym\u00adbol alphabet, found by including the symbols from lines on the left of the table \nthat have compatible variable bindings. The tracematch action is triggered if there is some sub\u00adstitution \nfor which the regular expression matches a suf.x of the projected string. For example, in the table above, \nthe substitution v = v1 . e = e1 results in the projected string CREATE NEXT UPDATE NEXT, which matches \nthe regular Symbol v e v = v2 v = v1 e = e3 e = e1 CREATE v1 e1 CREATE CREATE v1 e2 NEXT e2 NEXT e1 NEXT \nUPDATE v1 UPDATE CREATE v2 e3 CREATE NEXT e3 NEXT NEXT e1 NEXT UPDATE v2 UPDATE (*) Table 1. An example \nexecution history Figure 1. An automaton with states labelled by constraints expression. The action \nis triggered at the event which com\u00adpleted the match (i.e. the line marked (*), not the line after, even \nthough the projected string still matches). When a tracematch is compiled, the regular expression is \ntranslated into a .nite state automaton. Such automata are well understood, and can be easily constructed \nfrom regular expressions. Variants also exist for other formalisms such as context-free grammars and \nlinear temporal logic. In par\u00adticular, automata are much better suited to online matching, which is why \nthey are used here instead of keeping a regular expression representation. As we saw above, matches with \ndifferent variable bind\u00adings should be independent of each other and may be in\u00adterleaved. This means \nthat, conceptually at least, a separate .nite automaton is required for every possible binding an unbounded \nnumber. However, it is possible to simulate this collection of automata by using a single automaton and \nlabelling its states with constraints. An example of such a la\u00adbelled automaton for the safe-enumeration \nproperty is shown in Figure 1. A constraint C, labelling a state i, is interpreted as follows: there \nis an automaton in state i for each vari\u00adable binding that satis.es C . Constraints are boolean ex\u00adpressions, \nconsisting of (x = v), (x .v), ., and . (for = tracematch variables x and objects v), and are stored \nin dis\u00adjunctive normal form. The reader may be wondering, as is quite common in our experience, why \nwe chose to label the states of the automaton with constraints. Would it not be simpler to store each \nvariable binding together with a state counter and, when an event occurs, .nd the appropriate binding \nand just update the counter? There are two problems with this technique. Firstly, it is not applicable \nin cases where the implementation may store partial variable bindings (where only some of the monitor \nvariables are bound to objects). Secondly, the .ne\u00adgrained leak-elimination strategies described in Section \n4 are not possible when bindings are stored in this way. For the interested reader, the actual source \nfor the safe\u00adenumeration monitor is shown below, but the focus of this paper is code generation rather \nthan language design. 1 tracematch(Vector v, Enumeration e) { 2 sym create enum after returning(e) : \n3 call (Enumeration+.new (..)) &#38;&#38; args(v); 4 sym call next before : 5 call (Object Enumeration.nextElement()) \n&#38;&#38; target(e); 6 sym update source after : 7 vector update () &#38;&#38; target(v); 8 9 create \nenum call next * update source + call next 10 11 { 12 throw new ConcurrentModi.cationException (); 13 \n} 14 } Lines 2 7 declare the individual event patterns, Line 9 con\u00adtains the regular pattern that is \nmatched against the current trace, and Lines 11 13 constitute the extra code that is exe\u00adcuted upon a \nsuccessful match. A detailed discussion of the pertinent language design decisions can be found in [2]. \n 3. BENCHMARKS We now present our benchmark collection. In selecting benchmarks, our main inspiration \nwas the literature on run\u00adtime veri.cation, and also on trace-based aspect-oriented programming. We scrupulously \nincluded base programs where the overheads are likely to be substantial. An alter\u00adnative, taken in [9], \nis to apply a single tracematch to many unrelated base programs. This allows research into cheap strategies \nfor eliminating inapplicable monitors (indeed [9] demonstrates such a strategy). However, it is not appropriate \nin this context because it would wrongly give the impression that overheads are low, simply because the \nbenchmarks may not heavily engage in the monitored events. Also, we pro\u00advide a gold standard for each \nbenchmark, consisting of a hand-coded best possible solution. To our knowledge, this is the .rst collection \nof trace mon\u00aditoring benchmarks especially constructed to cover a wide variety of properties, while highlighting \npotential overheads. It is publicly available for others to use in comparative ex\u00adperiments. Indeed, \nthe fact that the designers of JavaMOP   recently adapted some of our examples for their own sys\u00ad tem \n[17] con.rms that these benchmarks are not tailored just for tracematches, and lends support to our claim \nthat they set a useful standard for all runtime veri.cation systems. Below we .rst informally describe \nthe benchmarks, each consisting of a (monitor, base program) pair. Then we de.ne the gold standard to \ncompare against, namely highly hand\u00ad optimised AspectJ implementations. Finally we take our ini\u00ad tial \nmeasurements, using the na\u00a8ive implementation of trace\u00ad matches to determine where the main performance \nproblems are. Monitors and base programs SAFEENUM This is the example mentioned in the previous section: \nwe would like to throw an exception when a collection is modi.ed while an enumeration over that collection \nis in progress. We apply it to JHotDraw [20], a Java graphics package that allows the user to animate \na drawing s components in a visually appealing manner. The animation loop contains a call to Thread.sleep \nto slow down movement; we removed that to truly observe the overheads incurred by trace monitoring, and \nalso min\u00adimised the window to factor out the cost of display oper\u00adations. The effect of these measures \nis that enumerating the .gure elements at each frame dominates the cost of the animation loop, and one \nwould thus expect overheads to be very large indeed. JHotdraw s use of enumerations is actually unsafe \nbecause one can edit the drawing while animation is in progress, and when that is done, our mon\u00aditor \ncatches the violation. NULLTRACK This is a debugging concern. We aim to pro\u00advide a trace monitor that \ncan help track down the cause for a NullPointerException . We do this by intercepting all .eld writes \nwhere a .eld f is set to null, followed by no intervening assignment of a non-null value until we see \na read of f followed by a null pointer exception the trace monitor then reports all .elds that were \nset to null, and where that happened. However, to improve precision of the reported possible causes, \nwe also demand that the .eld get that is assumed to have triggered the exception and the exception itself \noccur on the same line. Note that this is intrinsically a very expensive monitor, because it involves \na large number of instrumentation points: all .eld reads and writes, and all method calls. We applied \nit to CertRevSim, a discrete event simulator used to evaluate the performance of various certi.cate revocation \nschemes. HASHCODE Another common runtime veri.cation con\u00adcern, this trace monitor checks the property \nthat when\u00adever an object is stored in a data structure indexed by its hash code (e.g. a HashSet), and \nsubsequently a mem\u00adbership test is executed, the object s hash code hasn t changed in the meantime (if \nit has, we could get false negatives). This is expensive to check at runtime for a different reason: \nif the base program makes heavy use of hash-based data structures, the trace monitor will have to keep \ntrack of very many partial matches, one for each object stored. We applied this to two different base \nprograms: Firstly, AProVE [3], a termination prover for term-rewriting sys\u00adtems which we chose for its \nheavy use of hash sets, and also because it is precisely the type of complex, large ap\u00adplication where \nthis tracematch can be helpful. We also applied HASHCODE to Weka [31], a machine learning li\u00adbrary that \nmakes extensive use of HashMaps, but which is substantially smaller than AProVE. OBSERVER The observer \npattern is a popular example in the aspect-oriented programming community. Whenever a subject changes \nits state, all registered observers must be noti.ed; with AOP (and with trace monitoring) it is possible \nto achieve this without the subject explicitly making such noti.cations. The base program here is AJHotDraw \n[27], an aspect\u00adoriented rewrite of JHotDraw, which uses the observer pattern for its display updates. \nAgain, this benchmark has been chosen to fairly measure worst-case over\u00adheads: in AJHotDraw, each subject \nhas precisely one observer; there is thus considerable cost involved in us\u00ading a data structure that \ncaters for multiple observers, without knowledge of the one-to-one correspondence. Again, we removed \nall delays from the animation loop and minimised the window to factor out display opera\u00adtion overheads. \nDBPOOLING This is an example proposed by Laddad in his AspectJ text book [24]. The idea is that in a \npartic\u00adular database application, creating and establishing con\u00adnections is the most expensive operation, \nso, whenever possible, we want to pool existing connections and reuse them, rather than creating new \nones. The base program in this case is arti.cial a slight modi.cation of the example in Laddad s book, \nsimply connecting to the database multiple times and perform\u00ading database operations. The AspectJ version \nwe used is Laddad s. Note that for this example, we expect the trace monitor to improve performance rather \nthan hinder it, since it would prevent unnecessary connections from being established. LUINMETH Here \nwe are concerned with checking a stylis\u00adtic rule. Whenever a lock is acquired, it should be re\u00adleased \nbefore the enclosing method returns. Checking this rule with a trace monitor was proposed as a moti\u00advating \nexample by the authors of PQL [25]. It is particu\u00adlarly interesting because it requires matching method \nen\u00adtry/exit pairs at runtime, which is in general a context\u00adfree language problem. However, making judicious \nuse of variable bindings, it turns out that it is possible to express this using weaker formalisms; we \nencoded it in tracematches, which only allow regular expressions as patterns. The base program we chose \nhere is Jigsaw [29], the W3C s leading edge web server platform. It makes fre\u00adquent use of locking and \nunlocking, so there are plenty of points of interest. In fact, Jigsaw violates the style rule being checked, \nand our monitor catches the violations. REWEAVE The abc compiler makes use of an optimisa\u00adtion phase \ntermed reweaving, during which the effects of weaving aspect code are undone and weaving is repeated \nusing more precise analysis results obtained on the pre\u00adviously woven code [5]. Of course the correctness \nof this relies on properly undoing the effects of the .rst weaving step, so that the reweaving process \ncan start form a clean state. In terms of trace monitoring, if a .eld is written to during the execution \nof the weave() method, is not written to during unweaving, and is read during reweaving, then it is very \nlikely that it was not reset properly, and an error should be reported. We applied this to the abc compiler \nitself, and used the instrumented version to compile a small program to ob\u00adtain our numbers. Gold standard \nAspectJ implementations We implemented the trace monitoring concern for each of these benchmarks as a \ntracematch, and also created a hand-coded, hand\u00adoptimised set of plain AspectJ aspects that manually \nachieve the goal. We did this in order to have a gold standard for each benchmark, the best possible \nimplementation an opti\u00admising compiler might aim for. In all cases, the AspectJ code is much longer and \nmore complex than the tracematch speci.cation. For example, it often makes nifty use of weak references, \nand sometimes it makes additional assumptions not available to the compiler (for instance that all collections \nbeing enumerated have been created in user code and not in libraries). In short, the As\u00adpectJ version \nis what an expert programmer would do for the problem in hand. Discussing the entire set of hand-coded \nsolutions is be\u00adyond the scope of this paper, but we may highlight just one example in detail. The general \nidea is that we want to follow the standard method of ensuring fail-fast iterators (keeping modi.cation \ncounters), but without modifying the standard libraries. For that purpose, we subclass Vector to MyVector, \nwhich keeps a version number as a .eld, which gets incre\u00admented upon each update operation. That increment \nis im\u00adplemented with a piece of advice; with another piece of ad\u00advice, all constructor calls on Vector \nare replaced by construc\u00adtor calls on MyVector. It is for the replacement of those con\u00adstructor calls \nthat we need to be sure all instances of Vector are created within JHotDraw and not in some library code. \nAn automatic test for verifying this property would require costly interprocedural analysis of the monitored \ncode.  Each implementation of Enumeration also has such a ver\u00adsion number, copied from the underlying \nvector upon cre\u00adation; this is introduced by the aspect as an intertype declara\u00adtion on the Enumeration \ninterface. Each time an enumeration step is taken, a piece of advice compares the version num\u00adber of \nthe enumeration with that of the underlying vector: when they are not equal, an exception is thrown. \nAll pieces of advice need to be declared as synchronized. Such an implementation of the concern relies \non a deep understanding of the property, and it seems unlikely that automatic code synthesis would ever \nbe able to obtain it from a declarative speci.cation like a tracematch pattern. Measurements The initial \nperformance measurements are given in Table 2; all benchmarks were run on a Java HotSpot 1.5.0 11 VM \nrunning on Linux. For this set of measure\u00adments, we disabled most tracematch optimisations, simply allowing \nthe implementation to perform its code generation. In particular, no attention is paid to potential space \nleaks or to organising partial match sets; this is the approach that seems to be prevalent in the .eld. \nLet us examine some of the trends exhibited by these numbers in detail. We can see that the tracematch \nperformance is very close to that of AspectJ in the case of DBPOOLING; this can be explained by the fact \nthat the benchmark does expen\u00adsive database processing that dominates the monitoring over\u00adheads. More \nsurprisingly, performance is very good in the case of LUINMETH, which is applied to a signi.cantly larger \nbase program. Here, the explanation is in the trace\u00admatch pattern. Recall that it is intended to .nd \noccurrences of a lock being acquired and not released by the time the ac\u00adquiring method returns. When \na method that acquired a lock does return, therefore, it is clear that none of the associated partial \nmatches will need further updates, and they are inval\u00adidated. There is no build-up of live partial matches \nover the course of the benchmark. Performance looks less promising for some of the other benchmarks. \nREWEAVE is more than 50% slower, which is actually a lot worse than it sounds, because the instru\u00admentation \nonly affects a small part of the benchmark exe\u00adcution (the reweaving cycle); HASHCODE/WEKA is more than \ntwo times slower, NULLTRACK is unsurprisingly hugely affected by the large amounts of instrumentation, \ntak\u00ading more than 1000 times longer than equivalent AspectJ in\u00adstrumentation, and SAFEENUM and HASHCODE/APROVE \nare completely infeasible for our generated trace monitors. Even though we let each of them run for several \nhours, they did not reach anywhere near the end of the respective com\u00adputations. The huge slowdown was \nespecially visible with SAFEENUM, applied to a JHotDraw animation: the amount of animation steps per \nsecond dropped very rapidly until it was taking more than 10 seconds per frame, and that time was still \nincreasing. These numbers alone should be suf.cient to motivate the need for further optimisations. ID \nMONITOR BASE KSLOC NONE ASPECTJ NOOPT 1 SAFEENUM JHOTDRAW 9.5 3.1S 3.3S >90M 2 NULLTRACK CERTREVSIM 1.4 \n0.15S 0.6S 748S 3 HASHCODE APROVE 438.7 345S 478S >90M 4 HASHCODE WEKA 9.9 2.6S 2.7S 5.5S 5 OBSERVER \nAJHOTDRAW 21.1 4.2S 4.4S 9.2S 6 DBPOOLING ARTIFICIAL <0.1 30.5S 2.9S 3.3S 7 LUINMETH JIGSAW 100.9 14.1S \n18.0S 21.6S 8 REWEAVE ABC 51.2 7.8S 8.0S 12.0S Table 2. Benchmark runtimes: No instrumentation, hand-optimised \nAspectJ and na\u00a8ive tracematches 4. MEMORY LEAK ELIMINATION In [2], Allan et al. brie.y sketch an analysis \nand code gen\u00aderation strategy to avoid introducing memory leaks into the system. They give one example \nto show its effectiveness (a version of SAFEENUM), but there is no proper experimen\u00adtal validation. Crucially, \nthere is a subtle but important .aw in their proposal, which we shall correct below by introduc\u00ading the \nnovel notion of persistent weak references. We then expand that previous work by generalising the analysis \nto a larger class of memory leaks, allowing early invalidation of partial matches even in situations \nthat previously would have leaked. It is interesting to note that the effectiveness of this optimisation \non its own critically depends on the heap size of the Java virtual machine. The overall aim is to enable \ngarbage collection of partial matches that are guaranteed not to reach a .nal state in the automaton. \nRoughly, that can be achieved when completing the match would require an extra event on an object that \nis already garbage-collected itself; but because that object has expired, the extra event cannot occur. \nWe .rst describe the analysis required to detect that situation. Crucial to our strategy is the concept \nof a weak reference. Normally, a reference to a runtime object prevents that ob\u00adject from being reclaimed \nby the garbage collector. Weak references allow the programmer to refer to an object with\u00adout preventing \nits destruction. In Java, this takes the form of the special class java.lang.ref.WeakRef, whose constructor \ntakes the referent object as an argument. WeakRef provides a method called get () calling it will return \nthe referent, if it still exists, and null otherwise. Categorising references The analysis works on the \n.nite state automaton generated from the regular expression of the tracematch. For each non-initial non-.nal \nstate in that automaton, the free variables of the tracematch are divided into three categories: collectableWeakRefs \nVariables that are bound on every path from the current state to a .nal state. weakRefs Variables that \nare not used in the tracematch body and are not in the above set. strongRefs Variables that are not in \nthe above two sets. As an example, consider the SAFEENUM tracematch pre\u00adsented above. It only has two \nnon-initial non-.nal states (cf. Figure 1). From the .rst of these, we need to take both an UPDATE and \na NEXT transition to reach the .nal state; UP-DATE binds the v variable and NEXT binds e, so both of \nthese are collectableWeakRefs. On the second state, we only need to see a NEXT to get to the .nal state, \nso the only col\u00adlectableWeakRefs variable is e. Since v isn t used in the tracematch body, it is a weakRef \n if it was, it would be classi.ed as a strongRef on that state. Exploiting the categorisation For variables \nin the .rst cat\u00adegory, it is suf.cient to keep weak references (i.e. references that do not prevent garbage \ncollection) to the bound values, since we are guaranteed to bind them again before reaching a .nal state, \nand could keep a strong reference then (if nec\u00adessary). Moreover, if one of these weak references expires, \nthen we can discard the entire partial match, since it cannot possibly reach a .nal state any path to \na .nal state would have to bind the expired runtime value, which is impossible. This observation might \nwell improve the memory behaviour of trace monitors, since it could reduce the number of live partial \nmatches. Allan et al. claim that for weakRef variables, we also only need to keep weak references. The \nreason is that even if the runtime object expires, it would not actually be used, and so keeping a strong \nreference would unnecessarily prevent its garbage collection. A reference to it is only kept for matching \npurposes. Note, however, that discarding partial matches when such a variable expires is not justi.ed, \nsince by de.nition we can reach a .nal state without necessarily binding it again. It turns out that \nthis is an oversimpli.ed view that can lead to not all matches being successfully completed; we will \nexplain this in a moment. Finally, variables that are not necessarily re-bound on every path to a .nal \nstate and are used in the tracematch body must be kept alive; hence we need to keep strong references \nsuch variables form the strongRef category. Of course, there are certain tracematches which inher\u00adently \ndo introduce space leaks, and this categorisation of free variables allows the compiler to issue a warning \nto that effect: if there exists a non-initial non-.nal state for which collectableWeakRefs is empty, \nthen partial matches in that state could conceivably accumulate to an unbounded number without ever being \ndiscarded, and a warning should be emit\u00adted. Such warnings are very helpful in practice, because it is \neasy to forget about performance when writing declarative monitor speci.cations. Persistent weak references \nTo see why the original pro\u00adposed treatment of weakRefs is not sound, consider the fol\u00adlowing simple \nexample: Suppose we have a tracematch with two symbols, A and B, and that A binds a tracematch vari\u00adable \nx. The pattern is AB, and the tracematch body doesn t use x, so that it is a weakRef. Imagine at some \npoint during program execution, we have the following constraint on x: (x = v1 ) .(x = v2 ) and that \nthen both v1 and v2 expire. Weak references to ex\u00adpired objects return null, and so now the constraint \nbecomes (x =?) .(x =?) that is, we cannot tell the two disjuncts apart any more. Thus, when we see another \nB event, the tracematch body would be run once instead of twice. This small example shows that we need \nto treat weak references that do not invalidate their entire partial match specially: We need to be able \nto tell them apart even after they expire. We propose the concept of persistent weak ref\u00aderences, as \nexplained below, to address this issue. The de.ning characteristic of a persistent weak reference should \nbe that after its referent expires, calling get () returns not null, but some object that uniquely identi.es \nthe origi\u00adnal referent. Moreover, all persistent weak references to the same object should return the \nsame value after it has been garbage-collected. It is not immediately obvious how one could achieve such \nbehaviour, but our work on indexing (cf. Section 5) suggests an approach that works: Make use of collectable \nkey identity maps. We proceed by de.ning PersistentWeakRef, a subclass of the standard WeakRef class \nwhich has no publicly visi\u00adble constructors. Rather, it provides a static public method getRefFor(Object \no) that can be used to create new refer\u00adences. This method maintains a static identity map m from runtime \nobjects to associated instances of PersistentWeakRef; when called, it .rst checks if m already contains \nits parame\u00adter, and if so simply returns the associated value. Otherwise, it constructs a new PersistentWeakRef, \nrecords the correspon\u00addence in m and returns it. Effectively this ensures that only one persistent weak \nreference object is ever constructed for each runtime value. The map m has special handling for its keys: \nThey are stored as weak references (so as not to prevent their garbage collection), and moreover when \nthey expire, the associated key/value pairs are discarded. In this way, the memory used by the the PersistentWeakRef \nclass is proportional to the number of live referents, that is, it doesn t introduce any memory leaks \nitself. Finally, we need to de.ne the behaviour of the get () method on our new class. This proceeds \nas follows: First of all, dispatch to the superclass. If the result is non-null, the object is still \nalive and we can simply return it. If the result is null, we can return this that is, the PersistentWeakRef \ninstance. This satis.es our two requirements above, as we can still tell apart weak references to expired \nobjects, and references to the same runtime object will return the same value when it expires. Collectable \ncombinations of variables It is possible that a state may have no collectableWeaks but have one or more \ncombinations of variables for which, if all the objects bound to these variables (by a single partial \nmatch) expire, the whole partial match may be discarded. For example, the following pattern detects erroneous \nuses of the Reader and InputStream library classes. The pattern is taken from a benchmark by Bodden et \nal, in their paper on using whole-program static analysis to optimise trace\u00admatches [9]. create (readR|readI \n) * (closeR|closeI )+ (readR|readI ) The pattern uses two variables, r and i, for the Reader and InputStream, \nrespectively. The create symbol is the only sym\u00adbol which binds both variables. The variable r is also \nbound by readR and closeR, whilst i is bound by readI and closeI. The combination of alternation and \nthe inherent symme\u00adtry of the pattern means that neither of the variables are ever classi.ed as collectableWeakRefs. \nAlthough the ob\u00adjects bound to r and i may be garbage collected, the data structure for each partial \nmatch will never be discarded un\u00adless the match is completed. Evidently this is a preventable source \nof memory leaks: if, for a single partial match, both the object bound to r and the object bound to i \nexpire, then it is impossible to complete the match, and thus it could be discarded. We have extended \nthe leak-elimination analysis to com\u00adpute such combinations of variables. The extension adapts the technique, \nused at runtime, of labelling states with boolean expressions. We will refer to these expressions as \ncollecting constraints. They consist of ., ., and assertions x (for tracematches variables x). For example, \na state la\u00adbelled x . (y . z) means a partial match on this state can safely be discarded if the object \nbound to x has expired or the objects bound to y and z have both expired. Collecting constraints are \ncalculated using the following a formula, where i -. j means there is a transition from state i to state \nj labelled with the symbol a, and bound a is the set of tracematch variables bound by a. . . . . collectable \ni = collectable j . x i a -.j x.bounda Since there may be cycles in a tracematch automaton, the equation \nis solved for each state simultaneously by iteration to .nd the least-.xpoint. Note that this analysis \ndetects a strictly larger class of memory leaks than the previous analysis. Variables cate\u00adgorised as \ncollectableWeakRefs by the previous analysis are exactly those which appear as singleton assertions (i.e. \nlike x in x . (y . z)) when the collecting constraint is ex\u00adpressed in disjunctive normal form. Measurements \nLet us now examine the effects of these op\u00adtimisations on our set of benchmarks. Note that the following \nnumbers are taken with standard heap settings (i.e. a maxi\u00admum heap of 64M). We can see that we have \nmade substan\u00adtial performance improvements in some cases: NULLTRACK runs more than ten times faster now, \nHASHCODE/WEKA runs 30% faster, REWEAVE has improved somewhat, and most pleasingly SAFEENUM has become \nfeasible. Some of the other benchmarks show little change, however. ID ASPECTJ LEAKELIM NOOPT 1 3.3S \n97S >90M 2 0.6S 61.5S 748S 3 478S >90M >90M 4 2.7S 3.9S 5.5S 5 4.4S 9.4S 9.2S 6 2.9S 3.3S 3.3S 7 18.0S \n21.4S 21.6S 8 8.0S 11.4S 12.0S Table 3. Run times in seconds after leak elimination. It is worth examining \nSAFEENUM a bit more closely, as it is the clearest winner of leak elimination. As stated earlier, the \nbase program is JHotDraw, a Java .gure editor. For the benchmark, its animation routines were modi.ed \nby remov\u00ading all delays, so that the .gure elements are moved around the screen as fast as possible. \nAnimation is implemented by enumerating the .gure elements at each step, and moving each of them slightly; \nin total, 100000 animation steps are performed. There is a crucial difference between this trace monitor \nand the one in our LUINMETH benchmark, which proved to have very low overheads even without leak elimination: \nSAFEENUM checks that the next () method is never called on an enumeration after the underlying collection \nhas been updated; in particular, there is no single event after which we can be sure that a speci.c partial \nmatch will never lead to a successful match, so the number of potential matches grows unboundedly with \nbase program execution. In LUINMETH, we could discard partial matches when returning from the associated \nmethod. Thus, SAFEENUM is plagued by terrible memory perfor\u00admance, since it has to keep a steadily growing \nnumber of objects in memory; also, all of these must be updated after every relevant event, and this \nexplains the huge slowdown we described in the previous section. Consider now the effects of the space \nleak elimination on this. The enumeration object will be classi.ed as one of the collectableWeakRefs; \nthus, it can still be garbage-collected, and when it is, all associated partial matches are discarded. \nNow, each enumeration expires when the associated anima\u00adtion step is completed the following animation \nstep cre\u00adates a new one. So whenever we complete an animation step, we drop all associated matching state, \nwhich leads to the benchmark s becoming feasible. Moreover, rather than hav\u00ading unboundedly increasing \nmemory behaviour, it exhibits practically constant memory usage very slightly above that of the uninstrumented \nprogram, as described by Allan et al. at least if we force periodic garbage collector runs (cf. Figure \n8). The situation with NULLTRACK is similar; here, too, we have a pattern with no de.nite end event which \nwould allow us to discard partial matches, and therefore be\u00ading able to reduce the amount of work as \nobjects expire is absolutely crucial. So, it seems that space leak elimination is indispensable for many \napplications. We have observed signi.cant speed\u00adups after enabling the optimisation, particularly for \nopen\u00adended trace monitors (i.e. those for which it is not pos\u00adsible to rule out a match s completion \nbefore program ter\u00admination). Most notably, many liveness or safety properties, which are popular in \nthe runtime veri.cation community and assert that some good condition always holds or some bad sequence \nof events never occurs, are of this type. Without observing object garbage collection and invalidating \npartial matches based on that, such trace monitors would have to keep track of ever-growing sets of potential \nmatches. However, there is something highly unsatisfactory about the technique presented so far. The \nfact is that the impact of this optimisation critically depends on the performance of the garbage collector. \nIn modern JVMs, garbage collections tend to happen when the available heap space is running low; concretely, \nthis means that if we run the program with a smaller heap, we ll see more GC runs, and conversely there \nwill be fewer runs on a larger heap. This leads to the somewhat paradoxical situation that a particular \nprogram can run more slowly if it is given more heap space. Figure 2 illustrates precisely this effect, \ntaking SAFEENUM as an example. With very small heaps, the garbage collector runs all the time, and this \nslows the benchmark down. The optimum heap size in this case seems to be around 10M this strikes a good \nbalance between eliminating invalidated partial matches and not taking up too much computation time. \nBeyond that, there is a roughly linear increase in ex\u00adecution time with heap size, as the garbage collector \nruns less and less frequently, and hence the effect of the optimi\u00adsation is diminished more and more. \nIndeed, with a heap size of 1.7GB, SAFEENUM remains infeasible it fails to terminate even after several \nhours. It is worth mentioning at this point that all of our bench\u00admarks that didn t show great improvements \nafter this opti\u00admisation perform small enough computations that even with lots of live matching state \nin memory, the standard heap Figure 2. Runtime for SAFEENUM against heap size. size is big enough. This \nis the reason why leak elimina\u00adtion seemed to have a small effect the garbage collec\u00adtor isn t triggered \noften enough to make a signi.cant dif\u00adference. Running the benchmarks with a smaller heap size does show \ngreater improvements, but of course ideally we want to have an implementation that doesn t behave worse \nwith more memory. Still, in our experience, the analysis of Allan et al. suc\u00adceeds in eliminating many \nspace leaks (when forcing reg\u00adular garbage collector runs), and correctly emits warnings when there could \nbe a leak. Moreover, the early cleanup of invalidated matching state leads to substantial performance \ngains in many situations. Our extension to the analysis al\u00adlows us to handle even cases where the original \nstrategy would have failed, particularly patterns making heavy use of alternations. However, tracematch \nperformance is still rather worse than that of equivalent AspectJ instrumentation, even in the best case. \nFor SAFEENUM, the AspectJ version imple\u00adments the usual technique for implementing safe iterators by \nputting logical time stamps on collections and iterators it seems unlikely that this idea can be automatically \nsynthe\u00adsised from the speci.cation. The HASHCODE/APROVE (3) benchmark remains in\u00adfeasible. Close examination \nof its behaviour reveals that space leaks have been eliminated, but all time is spent iter\u00adating over \na large set of live partial matches. AProVE makes heavy use of hash sets, and each object stored in a \nhash set is potentially the source of a match completion. We will ad\u00address this in the next section. \n 5. INDEXING Recall that the basic implementation of trace monitors (as explained in Section 2) consists \nof a .nite state machine where the states have been labelled with constraints; and that constraints are \nboolean combinations of variable bindings to objects. The performance numbers shown so far were taken \nwith a simple implementation that represents such constraints na\u00a8ively as sets of disjuncts. As we will \nsee, large numbers of stored disjuncts will likely be irrelevant to any single update, but with a simple \nset representation every single one must be iterated over for each update. The overheads this causes \nmake trace-monitoring infeasible for large classes of programs and monitors. This section details a data-structure \nfor partitioning disjuncts, and algorithms for updating such structures, that avoid processing irrelevant \ndisjuncts. The methods shown preserve matching behaviour and extend the techniques from Section 4 so \nthat no new memory leaks are introduced. To see what it means for a disjunct to be irrelevant we must \nsummarise parts of the previous work on the semantics of tracematch matching [2]. A tracematch symbol \nis mod\u00adelled as a function from events to constraints. symbol = event . constraint For example, if a \nsymbol a does not match the event e then a(e)= false, but if a does match the event e just in the case \nthat the tracematch variable x is bound to the object o, then a(e)=(x = o). The set of all symbols declared \nby a tracematch is written a A. We write j -. i to mean there is a transition in the trace\u00admatch automaton \nfrom state j to state i that is labelled with the symbol a. For each state i, labeli denotes the constraint \nlabelling it. When an event e occurs, the new label at each state i, written label' , is i .. def . label' \n= . (labelj .a(e)) . i a j-.i . labeli .\u00aca(e) (1) a.A The .rst line of this equation says that if there \nis a partial match in state j, and the variable bindings for that partial match are compatible with a(e), \nthen that partial match can transition to state i. These are called positive updates. The second line \nstates that some transition must be taken for each partial match, unless no symbol can be matched to \ne that would result in compatible bindings. These are called negative updates. To illustrate, consider \nthe safe-enumeration monitor from Section 2, together with the variable bindings shown in Fig\u00adure 3. \nSuppose that a NEXT event occured with the variable binding (e = e2 ). The calculations that should be \nperformed to obtain the new constraints, in accordance with Equation 1, are shown in Figure 4. Indeed, \nwhen using a simple-set im\u00adplementation, these calculations must be performed. How\u00adever over half of \nthem are redundant: note that three out of the .ve new disjuncts, when simpli.ed, are either false or \nFigure 5. The new constraints after a NEXT event has oc\u00adcured.  unchanged that is, a disjunct that \nis identical to one previ\u00adously labelling the same state. These are the irrelevant dis\u00adjuncts. The simpli.ed \nversion of the constraints is shown in Figure 5. In general, suppose that the constraint labelling some \nstate j has a disjunct d, which contains the equality (x = o1 ). If an event e occurs, and there is an \na transition from j to i, we can see from the positive updates in Equation 1 that d. a(e) will be calculated \nas part of the new constraint labelling state i. Suppose, however, that the constraint generated by matching \nthe event to the symbol a contains (x = o2 ) where o1 .o2 . It is guaranteed that d . = a(e) = false, \nbecause it contains two contradictory constraints on x. The disjunct d is therefore irrelevant to a at \nthe event e. A similar situation is found when calculating negative updates. Suppose that state i is \nlabelled with a disjunct d = (x = o1 ) . d ' , and the same event e occurs such that a(e)=(x = o2 ) . \nc (for some predicate c). Equation 1 shows that computing the negative updates for i will involve Figure \n6. An example automaton state calculating d .\u00aca(e): d .\u00aca(e) = d .\u00ac((x = o2 ) .c) = d .((x .= o2 ) .\u00acc) \n= (d .(x =.o2 )) .(d .\u00acc) = ((x = o1 ) .d ' .(x .= o2 )) .(d .\u00acc) = ((x = o1 ) .d ' ) .(d .\u00acc) = d .(d \n.\u00acc) = d In this case, d is also irrelevant for negative updates not because it is falsi.ed, but because \nd is unchanged after the update and continues to label state i. The goal of indexing is to partition \nthe disjuncts stored at each state so that as many irrelevant disjuncts are ignored as possible for each \nupdate. 5.1 Choosing a Partition The tracematch implementation automatically chooses, for each state, \na set of variables with which to partition the disjuncts stored at that state. For illustration, consider \nthe state i, shown in Figure 6. Only the variables x and y are guaranteed to be bound and the state has \nthree outgoing transitions labelled a, b, and c. The symbol a binds x and y, b binds x and z, and c binds \njust z. What variables should be used to partition disjuncts labelling i? Firstly, a variable can only \nbe used to partition disjuncts at a state if it is guaranteed to be bound at that state and is also bound \nby an outgoing transition. If this is not the case, then the de.nition of irrelevance shown above does \nnot apply. There are therefore some transitions which cannot bene.t from indexing; the c-transition on \nstate i is such a transition because it only binds z, and z is not guaranteed to be bound at state i. \nThe strategy for choosing partition variables is to bene.t as many outgoing transitions as possible, \nwhilst ignoring those those that cannot bene.t. More precisely, the set of partition variables is found \nby intersecting the variables that are guaranteed to be bound with the variables bound by each symbol \nthat can bene.t from indexing. In the case of state i, the set is found by intersecting the set of variables \nthat are guaranteed bound, {x, y}, with {x, y}and {x, z}(for a and b, respectively c is not con\u00adsidered \nbecause it cannot bene.t from partitioning). There\u00adfore, the disjuncts at this state would be indexed \nby their binding for x. Figure 7. The safe-enumeration constraints, as stored using indexing. It is \npossible that this method results in no partition at all because there are two or more mutually exclusive \nsets that could be partitioned on. The safe-enumeration monitor we have been considering is an example \nof this: the UPDATE event only binds the vector v, whilst the NEXT event only binds the enumeration e. \nIndeed, in general, there may be some examples where it is most performant to not partition the disjuncts \nat all. However, it is likely that the programmer will be able to judge which symbols are going to match \nthe most often. For this reason, symbols may be marked as frequent in a tracematch. If no partition can \nbe chosen by the method described above then the process is repeated for just the frequent symbols. In \nthe case of safe-enumeration, we marked the NEXT symbol as frequent, which meant that the variable e \nwas chosen to index on.  5.2 A Data-Structure for Indexing Partitions are represented by the tracematch \nimplementation as trees, by using multiple levels of maps. Each level in the tree corresponds to a variable, \nand the map-keys at that level are objects bound to that variable. For safe-enumeration, the same constraints \nthat appeared in Figure 3 are stored using indexing as shown in Figure 7. Writing the implementation \nof these maps requires careful effort in order not to break the optimisations of Section 4, because each \nmap s keys are objects in the monitored program and the map must keep references to them. The maps are \nspecialised hash-tables that may keep weak references to the keys and can have extra code that is trig\u00adgered \nwhen a weak reference expires. The behaviour upon a reference expiring differs, depending on the classi.cation \nof the variable being indexed, as described above. If the key variable is a collectableWeakRef, then \nev\u00adery disjunct on the sub-tree indexed by that expired weak\u00adreference also must have a collectable weak-reference \nto that garbage-collected object it is therefore safe to drop the entire branch when the binding expires. \nNote that this is a particularly fast way of discarding invalidated constraints: rather than having to \niterate over and check each one in turn, all constraints on this state with the same expired binding \nare dropped at once. There is a potential problem, however, in that this could introduce a race condition. \nIf an event occurs that does not bene.t from the current index variable (as described above), we have \nto iterate over all key/value pairs at the current level in the indexing map. If the garbage collector \ninvalidates one of the keys during this iteration, it will be removed from the map, and we have violated \nthe Java API requirement of fail\u00adfast iteration. The solution is a custom map implementation that knowingly \ndeviates from the usual iterator contract. We allow safe modi.cations to the map during an iteration, \nand take care to ensure that dropping a key/value pair due to key expiry is safe in this sense. In fact, \nthere is one more pitfall of allowing the map implementation to discard branches at unde.ned times: One \ncannotrelyonaniterator s hasNext() methodtogivetheright result, since all remaining key/value pairs could \nconceivably bedroppedbeforethecallto next() evenif hasNext() returned true. Thus, another contract modi.cation \nis necessary: we allow next () to return null if there is no further element to iterate. If the index \nvariable is a strongRef, then no further care needs to be taken; we can use a simple identity hash map. \nNote that this is unlike the standard HashMap implemen\u00adtation, which considers keys to be equal subject \nto their equals () method we really need object identity, due to the tracematch semantics. Finally, \nlet us consider the situation where the indexing variable is a weakRef, i.e. where we cannot invalidate \ncon\u00adstraints due to the variable being garbage-collected, but still need to keep a weak reference. Recall \nthat this case proved especially tricky in Section 4. It may seem that indexing does not make sense for \nsuch variables. It is, however, still the case that constraints may bene.t from indexing, at least while \nan object is still alive and there are events that bind it. Once it expires, we will never have to explicitly \nlook it up in the map, but we need to keep the associated constraints accessible to iteration. One approach \nmight be to group together all key/value pairs with an expired key; as stated above, as long as we can \niterate over them we can perform updates correctly. However that gives rise to rather unpleasant race \nconditions. When do we perform this grouping operation? Since a garbage collection can occur at any time, \nsuppose one happened during an iteration of the key/value pairs. By merging the invalidated set into \nanother, we could end up either not iterating it or iterating it twice. The approach we propose, therefore, \nis to reuse our work from earlier and use a specialised indexing map that stores its keys in a PersistentWeakRef \n(cf. Section 4). Recall that only one such weak reference is constructed for each runtime value, and \nthat once that value expires, calling get () returns the weak reference itself. The result is that the \nindexing map is still fully iterable after the key expires, and key lookups are possible while the key \nis alive, which is what we aimed to achieve. It is easy to see that this does not result in additional \nspace leaks, since after a key expires the memory overhead for having seen a runtime value is constant \nand very small, and will be fully eliminated once associated partial matches complete or fail. 5.3 Updating \nIndexed Disjuncts We follow the code-generation policy previously described for tracematches [2]. A method \nis generated for each trace\u00admatch symbol. This method is triggered when an event oc\u00adcurs which matches \nthat symbol, for some variable bindings. The method uses these bindings to calculate changes to the constraints \nlabelling the automaton. These changes are tem\u00adporarily queued. Once a method has been run for each sym\u00adbol \nthat matches the event, the queued changes are used to update the main constraints. Without indexing, \nthe pseudo-code for the method which updates the constraints for a symbol a is: def update a(event bindings): \nfor (j,i) in a-transitions: label[i].pos = label[i].pos or (label[j].original and event bindings) neg \n= neg and not(event bindings) Such a method is run for each symbol that corresponds to an event. After \neach applicable update method is run, the results are combined: def combine(): for i in states: label[i].original \n= label[i].pos or (label[i].original and neg) label[i].pos = false neg = true Note how the two halves \nof these methods correspond to the two lines of Equation 1, respectively. The only difference is that \nhere the results are imperatively built symbol-by-symbol using temporary variables. The variables are \nas follows: label[i].original a set of disjuncts storing the constraint for state i  label[i].pos \n a temporary set of disjuncts in which the positive updates for state i are accumulated  neg a temporary \nset of disjuncts in which the negative updates for all states are accumulated  To re.ne this approach \nto use indexed constraints, the .rst step is to replace the sets used for the original sets with the \nmulti-level maps discussed above. The pos sets are replaced with linked-lists called qeueue . Once the \nrelevant disjuncts for an update are located in the multi-level map, the results of processing them are \nstored using the queue lists. We use lists rather than sets, because the queues must be reset every iteration \nand this adds large amounts of overhead with sets.   The pseudo code for the update methods that use \nindex\u00ading is as follows, although note that what is shown here is just pseudo code for clarity; in the \nactual implementation, specialised code is generated for each operation shown here and the loops are \nstatically unrolled. def update a(event bindings): for (j,i) in a-transitions: for disjunct in label[j].original.lookup(event \nbindings): label[i].queue.append(disjunct and event bindings) neg.andNot(event bindings) def combine(): \nfor i in states: label[i].original.and(all event bindings) label[i].original.insertAll(label[i].queue) \nlabel[i].queue = [] neg.reset() Consider again the safe-enumeration example. In contrast to the .ve \ncalculations shown in Figure 4, computing the same NEXT update, for (e = e2 ), now only two disjunct \ncalculations one positive and one negative update to (v = v1 .e = e2 ) and four relatively inexpensive \nmap lookups. 5.4 Evaluation Let us now examine the effect on the benchmark times, as displayed in Table \n4. ID ASPECTJ FULLOPT LEAKELIM NOOPT 1 2 3 4 5 6 7 8 3.3S 0.6S 478S 2.7S 4.4S 2.9S 18.0S 8.0S 15.3S 1.6S \n627S 3.1S 9.8S 3.3S 21.2S 9.5S 97S 61.5S >90M 3.9S 9.4S 3.3S 21.4S 11.4S >90M 748S >90M 5.5S 9.2S 3.3S \n21.6S 12.0S Table 4. Complete benchmark results. Not surprisingly, in some cases indexing deteriorates \nper\u00adformance,inparticularfor OBSERVER (5).Aswementioned earlier, in this application each subject has \nexactly one ob\u00adserver. It follows that the indexing structure only adds to the overhead of accessing \nthat one element. This type of slow\u00addown could be eliminated by introducing indexing in a dy\u00adnamic fashion, \nonly building the index when the number of disjuncts in a set exceeds a given threshold. Overall, however, \nthe effect of indexing is hugely bene\u00ad.cial. In the case of NULLTRACK (2), it reduces the execu\u00adtion \ntime from 61.5s to 1.6s, which is particularly impressive considering the huge number of instrumentation \npoints. Fur\u00adthermore, APROVE (3) now becomes feasible to execute, and we can observe a huge speed-up \nin SAFEENUM (1). HASHCODE/WEKA (4) and REWEAVE (8) also bene.t. Perhaps the most pleasing side-effect \nof indexing, how\u00adever, is the elimination of the crucial dependency on garbage collector performance \nthat we observed in Section 4: It is now the case that running a benchmark with more memory will not \nautomatically mean worse performance, since we only iterate relevant disjuncts. Of course it is theoretically \npossible that we update disjuncts that are relevant but would have expired after a garbage-collector \nrun, but we weren t able to measure this in practice. We conclude that the combination of leak elimination \nand indexing is a conditio sine qua non for the generation of ef.cient trace monitors. Neither of the \ntwo techniques would work as well in isolation: Without indexing, leak elimination depends on the JVM \ns memory measurement, and without leak elimination, indexing would run out of memory on any substantial \nbenchmarks.  6. RELATED WORK In the introduction, we already indicated that while there is a substantial \nbody of work on trace monitoring, there are not a lot of systems available. As the focus of this paper \nis ef\u00ad.cient implementation, we only review such systems here. Our original intention was to provide \na detailed comparative study of the most mature trace monitoring systems; it turned out, however, that \nmany of the systems were not available to the general public, and even with those that were, we fre\u00adquently \nran into basic problems that prevented our experi\u00admental evaluation. Table 5 gives an overview, comparing \nthe salient features. The .rst .ve systems in the table are all publicly available and allow trace monitors \nto be applied to Java programs. They are, therefore, broadly comparable even though in AspectJ event \nsequences must be matched by hand-coding the monitor. The .ve systems on the bottom of the table are \neither not publicly available, or (in the case of Arachne) apply to another programming language, namely \nC. The table attempts a comparison with respect to a number of criteria. Firstly, the purpose of the \nsystem: Many are geared solely towards runtime veri.cation, whereas others (mostly with a background \nin aspect-oriented programming) are actually intended to augment the monitored program by running extra \ncode when a matching trace is found, or maybe by replacing an event with new code. Next, we examine the \nissue of integration with a pro\u00adgramming language. Several of the systems are deeply in\u00adtegrated with \nAspectJ, but some others (for instance PQL) are stand-alone tools. The advantage of programming lan\u00adguage \nintegration is enhanced checking of the speci.cations at compile-time. There is considerable variety \nin the way patterns are spec\u00adi.ed. Not all systems allow variables to be bound by the matching process: \nwithout such binding, it is dif.cult to write patterns that monitor the behaviour of a speci.c set of \nobjects. The exact-match column in Table 5 refers to the matching process. There are two different styles \nof seman\u00adtics: One can either demand that every single event be ac\u00adcounted for by the pattern, or one \ncan allow arbitrary events to occur in between matched statements, as does, for exam\u00adple, PQL. We refer \nto the former as an exact-match seman\u00adtics , and to the latter as a skipping semantics . The precise \nimplications of this design choice are very interesting, but beyond the scope of this document; we refer \nthe interested reader to [6] for an in-depth discussion. Finally, a number of systems allow context-free \npatterns as opposed to merely .nite state machines. While we have not considered the implementation of \nsuch rich patterns in this paper, it is clear that the same techniques apply there to avoid space leaks, \nand to index partial solution sets. The next section of Table 5 examines the characteristics of the implementation. \nOnly very few systems have based their implementation on a semantics. For tracematches, a proof of the \ncorrespondence between its declarative and op\u00aderational semantics is presented in [2]. Tracematches pio\u00adneered \nthe use of leak prevention and indexing as described in this paper, though these techniques have been \npicked up by JavaMOP to some extent see the relevant discussion below. Other systems are not concerned \nwith space leaks, and pay the associated performance penalty. The authors of HAWK kindly agreed to run \nour SAFEENUM benchmark for us (HAWK is not available for download), but memory leaks proved prohibitive. \nOur experience with PQL is described below. Tracematches are also the only system that automatically \nspecialises the generated code to the pattern again, of course, without using interprocedural analysis. \nFurther dras\u00adtic improvements in ef.ciency are possible in some applica\u00adtions when interprocedural analysis \nis employed. The most sophisticated system of this kind is PQL, employing a BDD\u00adbased static analysis \nto rule out instrumentation points at compile-time. Unfortunately, we were not able to get the static \nanalysis to work. A similar optimisation has been tried in the context of tracematches [9]: The .ndings \nwere not very encouraging, showing that often the static analysis made only a very small difference in \nruntime. The .nal column indicates whether a system can be freely downloaded. Where this was the case \nand the system could process Java, we tried to express our benchmarks. The per\u00adformance of J-LO on SAFEENUM \nwas such that we gave up on attempting further experiments (with its author s bless\u00ading). Our experience \nwith AspectJ has been presented in this paper as the hand-optimised gold standard against which other \nsystems should be measured. The time spent coming up with implementations in each case was substantial. \nThe .ndings with the remaining two systems were more interest\u00ading. PQL The Program Query Language (PQL \n[25]) was pro\u00adposed as a stand-alone tool to .nd bugs in Java programs by writing queries over execution \ntraces. A PQL query can SYSTEM PURPOSE PATTERNS IMPLEMENTATION fault. nding functionality integration \nvariables exact-match context-free semantics leak busting indexing specialisation static match availability \ntracematches [2] \u00b1 + + + + - + + + + + [9] + PQL [25] + - - + - + - - - - + + J-LO [26] + - + + - - + \n- - - - + JavaMOP [11] + + - \u00b1 - - - \u00b1 \u00b1 - - + AspectJ [4] - + + - - - - - - - - + tracecuts [30] \u00b1 + \n+ - + + - - - - - - PTQL [21] + - - + - + - - - - + - HAWK [14] + - - + - + - - - - - - Alpha [8] \u00b1 + \n- + + + - - - - - + Arachne [19] \u00b1 + + - + - - - - - - + Table 5. Systems for trace monitoring. be named, \ncan make use of free variables, and picks out events by writing fragments of concrete Java syntax. It \nem\u00adploys a skipping semantics, that is, it allows any event to occur between matched statements. Due \nto the fact that the named queries can be (mutually) recursive, PQL can express context-free properties \nof the trace quite naturally. PQL does not include any optimisations to avoid space leaks, and indeed \nwhen we encoded SAFEENUM, we ob\u00adserved a steep linear growth of memory usage over time: it was impossible \nto complete the benchmark without pro\u00adviding the JVM with more memory (cf. Figure 8 for a com\u00adparison \nwith MOP and tracematches). Still, PQL completed the benchmark in 580 seconds, signi.cantly faster than \nthe na\u00a8ive tracematch but also rather slower than tracematches with leak elimination (97s) or full optimisations \n(15.3s). Unfortunately, we ran into signi.cant problems when try\u00ading our other benchmarks. Several of \nthem (both HASH-CODE benchmarks, for example) cannot be expressed due to limitations of the PQL language; \nthe problem with HASH-CODE is that PQL cannot bind primitive types like int (we con.rmed this with the \nauthors of PQL). Also, it is impos\u00adsible to intercept and bind assignments to .elds, and so we couldn \nt express NULLTRACK or REWEAVE. DBPOOLING and LUINMETH are both expressible, but do not work with the \nPQL 0.1 or 0.2 implementations for technical reasons. JavaMOP JavaMOP is an implementation of the paradigm \nof monitor-oriented programming [11]. It provides a frame\u00adwork for so-called logic plugins to generate \na trace monitor from their own domain-speci.c trace pattern language; such a plugin for regular expressions \nis prede.ned, making it nat\u00adural to compare our work on tracematches with JavaMOP and to investigate \nto what extent our .ndings carry over. However, several different plugins (e.g. for LTL) are also available, \nand many of the design decisions in the system are in.uenced by the need to keep the core plugin-independent. \nIndeed, one of the main design goals of JavaMOP is to stay as general as possible with respect to the \nMOP framework. It supports three kinds of monitors (inline, outline and of.ine), and can trigger extra \ncode both when patterns are validated and violated. In what follows, when we say JavaMOP , we shall \nmean JavaMOP with the regular expressions plugin, inline moni\u00adtors and validation handlers , as this \ncon.guration is closest in spirit to tracematches. The system generates AspectJ source code, which then \nneeds to be compiled with an AspectJ compiler to produce the instrumented program. Our discussion of \nJavaMOP will be structured as follows. First, we discuss some subtle dif\u00adferences with tracematches at \nthe level of language design, as they have some impact on the implementation. Next, we zoom in on the \nimplementation of JavaMOP, comparing it to tracematches both in terms of space usage and time ef.\u00adciency. \nTo be precise, we shall compare against the so-called centralised indexing implementation of JavaMOP \n the alternative, decentralised indexing is brie.y reviewed in our Future Work section. Finally, we draw \nsome conclu\u00adsions from this detailed comparison between tracematches and JavaMOP. Language design. Broadly \nspeaking, JavaMOP is very sim\u00adilar to tracematches. However, there are four signi.cant dif\u00adferences: \nFirst, all variables must be bound by the .rst symbol in a pattern. This condition is not satis.ed by \ntwo of our benchmarks. In NULLTRACK, the pattern is setnull anyget npe , and the anyget symbol binds \ninformation about the .eld read, reporting it when a match occurs. The condition that all variables be \nbound by the .rst symbol also fails in the LUINMETH benchmark. Second, at most one state in the automaton \ncorresponding to the pattern can be associated with each set of variable bindings. In tracematches, no \nsuch restriction exists. This can be a problem, since typically the automata generated for trace properties \nare non-deterministic, and so only keeping one of all the different states such an NFA might be in will \nmiss some matches. (One could, of course, determinise the pattern at the cost of an exponential blow\u00adup, \nbut JavaMOP does not do this.)  Third, symbols (or events , as they are called in Java-MOP) are considered \natomic, and so there is no provi\u00adsion for overlap if multiple different symbols match the same runtime \nevent, the monitor state is updated once for each such symbol. This is a conscious design deci\u00adsion, \nbut one which signi.cantly simpli.es the required monitor update code. In tracematches, the symbols \nin the pattern are simply logical properties, and so it is natural that they can over\u00adlap; the implementation \ntakes special care to preserve the semantics in this case. Updating matching state several times for \na single event and potentially triggering the tracematch multiple times with the same bindings after \njust one event would clearly run contrary to our seman\u00adtics [2].  Fourth, in JavaMOP, variable bindings \nfrom the pattern cannot be directly used in the code that is triggered when the pattern matches. This \nis in part a consequence of the logic-plugin-independent design of the system. If we wished to use the \nbindings, they must be explicitly main\u00adtained by hand within the plugin. By contrast, trace\u00admatches do \nnot impose such a restriction, freeing the user from the need to maintain bindings manually.  The designers \nof JavaMOP have thus made a number of pragmatic decisions, balancing expressiveness against ease of implementation \nand their overarching goal of preserv\u00ading plugin-independence. As described in [2], the design of tracematches \ntakes a formal semantics as its starting point, and then implements exactly that semantics, without making \nany concessions to facilitate more ef.cient implementation. It is interesting, therefore, to examine \nthe price one pays for such generality, and the bene.ts that derive from JavaMOPs design choices. Implementation. \nSome of the optimisations and benchmarks proposed in this paper were .rst disseminated in a technical \nreport, and pleasingly the developers of JavaMOP incorpo\u00adrated parts of our work into their system. At \nthe time of writ\u00ading, several of our benchmark trace monitors had been ex\u00adpressed in their formalism \nand made available on their web page [17]; also, a weak form of leak elimination and index\u00ading is supported \nby JavaMOP in a plugin-independent way. The main difference between our approach and that of JavaMOP \nis in the representation of partial matches: While tracematches conceptually store an automaton labelled \nwith Figure 8. Memory usage for SAFEENUM (top to bottom line): JavaMOP using bindings, PQL, JavaMOP not \nusing bindings, tracematches using bindings.  constraints, JavaMOP uses a dedicated monitor instance \nfor each distinct set of variable bindings, and updates a state counter on that. As mentioned above, \nthis entails several problems, particularly the fact that it is impossible to use different space leak \nelimination strategies on different au\u00adtomaton states. Indeed, the approach taken by JavaMOP is almost \ndisarming in its simplicity: Store each monitor in\u00adstance in a set of indexing trees, one such tree for \neach tuple of variables bound by some monitor event, in such a way that keys are allowed to expire, and \nwhen they do associ\u00adated sub-trees are discarded. In this way, monitor instances are kept alive for just \nas long as some event enabling them might occur (note, however, that this is not as effective as our \ncollectable-binding-sets, or even just collectableWeakRefs, because if there is some event that binds \nno monitor variable, all monitor instances would be stored in a set without ever being removed). Unfortunately, \nthis scheme breaks down as soon as the user needs to inspect the acquired variable bindings and use them \nupon a successful match. According to the JavaMOP developers, the proper procedure there is to instrument \nthe events with additional code that stores the binding in .elds of the monitor instance, and then use \nthose .elds. Unfor\u00adtunately, this implies keeping strong references to bindings, which will prevent them \nfrom ever being cleared. Moreover, it is not possible for the user to avoid space leaks by manually using \nweak references when storing the bindings in .elds, since that wouldn t guarantee their avail\u00adability \nat the time of match completion. The only way around this seems to be an automaton-state speci.c handling \nof bindings, as implemented in tracematches. However, such an advanced strategy seems infeasible without \nin-depth knowl\u00adedge of the underlying formalism, and so this cannot be implemented in JavaMOP without \nsacri.cing some of the plugin-independence. Figure 8 illustrates the effectiveness of space leak elimi\u00adnation \non the SAFEENUM benchmark for tracematches and MOP. We can see that the sophisticated leak elimination \nstrategy described in Section 4 is successful, and the memory usage for tracematches is essentially constant, \neven though all bindings are used in the body (the line is almost su\u00adperimposed on the X axis, at an \naverage memory usage of 1.1MB). The second line from the bottom corresponds to JavaMOP with a validation \nhandler that doesn t use the bind\u00adings. Memory usage is reasonable, but there is still a clear upwards \ntrend. The next line corresponds to PQL, and the top line corresponds to storing bindings in .elds of \nthe mon\u00aditor instance and using them in the validation handler, as advised by the developers of JavaMOP. \nEssentially, this re\u00adsults in no heap object that was bound ever being released for garbage collection, \nand memory usage explodes. It is also interesting to compare the performance of the two systems, since \ntypically JavaMOP s approach of stor\u00ading an automaton instance for each set of bindings seems more natural \nthan the alternative of annotating states with constraints. As we observed above, it forces the restriction \nthat any free variable of the monitor must be bound in the .rst observed event, and we wanted to determine \nif the price that tracematches pay for their generality is prohibitive. Therefore, we tried to express \nall of our benchmarks in JavaMOP, guided by the examples on its website [17]. As we have already observed, \nNULLTRACK and LUINMETH violate JavaMOP s assumption that all variables be bound upfront, so these cannot \nbe expressed directly. Still, it is sometimes possible to manually tweak such cases into the form expected \nby JavaMOP, by rephrasing events or the entire pattern, or perhaps by using so-called event actions \nthese are blocks of Java code that run when an event matches, and while they somewhat undermine the declarative \nnature of the speci.cation, they certainly increase expressiveness. We were able to express NULL-TRACK \nusing such event actions the location and line number to be reported is stored on .elds of the monitor \nin\u00adstance and destructively updated with each new event match. Note that this technique would not work \nin cases which re\u00adquire different monitor instances for different values of the binding that is being \nhandled in this way, but NULLTRACK happens to .t the bill. It is important to realise that this ac\u00adtually \nsigni.cantly simpli.es the pattern from JavaMOP s point of view, since it reduces the number of variable \nbind\u00adings it needs to keep track of and index on. No such assump\u00adtion is available to the tracematch. \nThe above technique doesn t work for LUINMETH, as different monitor instances for each binding are needed. \nOne might think it possible to emulate it manually in event actions, but there is currently no way in \nJavaMOP to inspect the call stack, and so this benchmark is not expressible at this time (we have con.rmed \nthis with the designers of JavaMOP). Finally, DBPooling requires around symbols (i.e. sym\u00adbols that \nintercept and replace the last matching event), which MOP does not support. In summary, six of our eight \nbenchmarks are expressible in JavaMOP. The overall results are shown in Table 6. The MOP num\u00adber for \nSAFEENUM shown is for the monitor that doesn t use the bindings in the validation handler (and hence \nal\u00admost succeeds in eliminating memory leaks), as that was signi.cantly faster. As we can see, JavaMOP \noutperforms tracematches on HASHCODE/APROVE, OBSERVER and REWEAVE, typically by a margin of 20% or less, \nwhile tracematches hold the upper hand on the SAFEENUM (due to the superior leak elimination) and HASHCODE/WEKA. \nThe NULLTRACK numbers in the table are not directly com\u00adparable, as the tracematch does more work: JavaMOP \navoids the need to index on the source location and line number by use of event actions, as described \nabove. Conclusions. In conclusion, JavaMOP highlights some inter\u00adesting restrictions to the language \ndesign that enable simpler code generation than that for tracematches. For some bench\u00admarks, the savings \ncan be as high as 20%, but the price paid is that many patterns are not expressible. It is natural, there\u00adfore, \nto wonder whether those same restrictions could be im\u00adplemented as optimisations for tracematches. The \nprimary reason for the ef.ciency savings is Java-MOP s built-in requirement that all monitor variables \nbe bound by the .rst observed event, which allows for more straightforward update code (in the terms \nused in Section 5, the negative updates never modify partial matches, they ei\u00adther leave them unchanged \nor discard them). We intend to implement a similar optimisation in the tracematches sys\u00adtem on an on-demand \nbasis because we generate speci.c update code for each automaton state, it would be easy to use the \nabove optimisation at any state that guarantees all vari\u00adables bound, which, in the special case that \nthe .rst event binds all variables, would give us the same result as intro\u00adducing the restriction from \nJavaMOP. Another opportunity is to exploit non-overlapping sym\u00adbols. As said, in JavaMOP events are always \nunique, and no two symbols can match the same event simultaneously some arbitrary order is imposed. \nIn tracematches, we could implement an analysis that proves disjointness of symbols. Using such disjointness \ninformation, it is possible to make destructive updates to the constraints that label states, and that \nin turn would lead to the generation of simpler (and hence more ef.cient) code. Static analyses In the \nintroduction we already alluded to techniques that have been devised for static type-state veri.\u00adcation \n(e.g. [18]). These analyses can be employed to show that certain program points can never contribute \nto the suc\u00adcessful match of a particular trace pattern, thus avoiding the need for instrumentation indeed, \na staged analysis follow\u00ading this approach and extending it to the context of trace\u00admatches was recently \nproposed in [9, 10]. ID MONITOR BASE KSLOC TM MOP 1 2 3 4 5 8 SAFEENUM NULLTRACK HASHCODE HASHCODE OBSERVER \nREWEAVE JHOTDRAW CERTREVSIM APROVE WEKA AJHOTDRAW ABC 9.5 1.4 438.7 9.9 21.1 51.2 15.3S 1.6S 627S 3.1S \n9.8S 9.5S 19.3S 0.8S 590S 3.6S 10.3S 8.3S Table 6. Comparison of benchmark runtimes between tracematches \nand JavaMOP The analyses of [9, 10] are particularly effective when tracematches are blindly applied \nto a large number of pro\u00adgrams, because then tracematches fail to apply for easy-to\u00adcheck reasons (say, \nno locks are acquired or released at all, so LUINMETH fails to apply overall). Indeed the numbers reported \nin [9,10] are very encouraging for that type of trace\u00admatch usage. The situation is somewhat different, \nhowever, when benchmarks actually exercise all events in a tracematch, as is the case in our benchmark \nsuite. For instance, one cannot hope to completely eliminate the instrumentation costs in examples like \nSAFEENUM, since it catches actual violations of the property in question. Moreover, such analyses suffer \nfrom the usual problems for whole-program and callgraph analysis: It is very hard to make sure the result \nis sound in the presence of multi-threading, dynamic class loading and re.ection, which are quite common \nin real-world Java pro\u00adgrams. Indeed, technical problems connected with these is\u00adsues prevent us from \nreporting numbers for our benchmarks with the analysis from [10] applied. However, it is important to \nrealise when such static anal\u00adyses will help. Typically, they are concerned with remov\u00ading provably unnecessary \ninstrumentation. When writing trace monitors for runtime veri.cation concerns, a success\u00adful match indicates \nthat the program misbehaved, and so in an ideal case one might hope that a static analysis could re\u00admove \nall instrumentation, proving the program correct with respect to the given property. However, when viewing \ntrace monitors as an extension of pointcuts in an aspect-oriented setting, they typically con\u00adtribute \nan essential part of the system s functionality our OBSERVER example is just such a case. Such monitors \nby de.nition can not be optimised away, because they will match. Particularly in this context, it is \ntherefore important to do the best possible code generation for the given pattern, as there is no hope \nthat a suf.ciently sophisticated analy\u00adsis might remove all overheads. The techniques presented in this \npaper are indispensable for that.  7. FUTURE WORK It is natural to ask whether further improvements \nare pos\u00adsible, beyond the optimisations presented here. We are cur\u00adrently investigating several possibilities: \n Reducing redundancy One obvious way to carry our in\u00addexing scheme further would be to note that it is \nredundant to store binding information both labelling the edges of an indexing tree, and on the partial \nmatches at the leaves. We could then further specialise the partial match representation by discarding \nthe redundant information. In the extreme case where every variable appears as an index, we would only \nhave to store a simple counter recording the current automa\u00adton state. Some care has to be taken, however \n this is only well\u00adde.ned if any symbol that can occur at the start of a matched trace binds all tracematch \nvariables. In fact, this occurs fre\u00adquently, and so such an optimisation seems promising: we already \ndiscussed this under JavaMOP in the Related Work section. Indeed, since tracematches already generate \nspe\u00adcialised code for each automaton state, one could take this further by using this technique for all \nvariables for which it makes sense, for each state. Disjoint pattern symbols Also, in our earlier discussion \nof JavaMOP, we noted that JavaMOP s simple code generation is partly due to its decision not to consider \noverlapping symbols in trace speci.cations. Because tracematches are embedded in the AspectJ lan\u00adguage, \nand there pointcuts can overlap, it is not possible to follow that design here. However, it is easy to \nenvisage an analysis that proves disjointness of particular symbols. Us\u00ading the result of such an analysis, \nwe can simplify the code that updates constraint labels on automaton states for the dis\u00adjoint symbols, \nmaking most operations destructive. Bound variable correlation In many examples, there ex\u00adists a many-to-one \nrelationship between the objects bound in a trace monitor. For example, every enumeration corre\u00adsponds \nto one collection, every observer has one subject, and so on. We can thus improve the implementation \nof index\u00ading by moving the .rst level of the indexing tree into .elds on objects. For example, in the \nOBSERVER tracematch, we might store all observers as a .eld on the subject. Note that this has the added \nbene.t that when the subject is garbage collected, so are its observers. Implementing this automatically \nrequires some annota\u00adtions on the speci.cation, as well as a fairly complex anal\u00adysis of the base program, \nhowever, going well beyond the cheap techniques we have introduced here. In particular, such analysis \nis necessary when it is not deemed acceptable to modify library code, to check that certain objects are \ncon\u00adstructed only in the compiled code, and not elsewhere. In the presence of such an analysis, we could \ngenerate code that is much closer to our hand-coded AspectJ gold standard for benchmarks like SAFEENUM, \nas described above. A related optimisation was introduced by the designers of MOP in [11] for the special \ncase of single-variable speci.ca\u00adtions only. In that context, the above would take the form of simply \nstoring the monitor instance on a .eld of the bound object. We .rst sketched an extended technique incorporat\u00ading \nhandling of multiple variables in [2], and a limited ver\u00adsion is implemented in JavaMOP [12, 17]. That \nimplemen\u00adtation, called decentralised indexing , avoids the need for an expensive analysis by assuming \nit is permissible to trans\u00adform the code of the Java standard libraries, and in many applications that \nassumption is not satis.ed. Dynamic indexing We noted that for some benchmarks, indexing aversely affects \nperformance because the indexed sets are too small; in particular, this is the case for OB-SERVER applied \nto AJHOTDRAW. The obvious solution is to set a threshold: sets below the threshold are not indexed, and \nthose above it are. A lot remains to be done to optimise trace monitor perfor\u00admance. It is clear, however, \nthat we have identi.ed the two essential techniques that make trace monitoring feasible in the .rst place. \n 8. CONCLUSIONS This paper demonstrates, for the .rst time, how feasible trace monitors can be generated \nfrom speci.cations. It thus complements the substantial body of work that argued the desirability of \ntrace monitors as a language feature. This result was obtained through two techniques: the elimination \nof space leaks, and a sophisticated data struc\u00adture for organising sets of partial matches. Neither of \nthese techniques requires interprocedural analysis, and they can thus be employed without excessive compile-time \ncosts. Our techniques approach the speed of hand-coded, hand\u00adoptimised monitors to within a factor of \n3 at worst. The leak elimination analysis was suggested in [2], but the strategy proposed there contained \na crucial .aw that made it unsound. We have shown how to rectify this by in\u00adtroducing the novel notion \nof persistent weak references, and how to extend the results to effectively optimise a wider class of \nspeci.cations. Furthermore, we presented a thorough ex\u00adperimental evaluation of the effectiveness of \nthe new solu\u00adtion. The fact that the original .aw went undetected for so long is cause for some concern. \nAt present there are no for\u00admal veri.cation techniques available for data structures that make use of \nweak references. We are currently investigating the development of such veri.cation techniques, using \nthe data structure presented here as a motivating example. All results of this paper, ranging from our \nbenchmark suite to the optimisations, are applicable to most other trace monitoring systems, and we have \nthus opened the way for many comparative experiments in future. These are already starting to happen, \nas witnessed by the recent adoption of some of the techniques presented here by the JavaMOP system. \n ACKNOWLEDGEMENTS Neil Ongkingco did a huge amount of work on getting the benchmarks to run for an early \nversion of this paper, in particular JigSaw and AJHotDraw. He also suggested many improvements to the \npaper itself we re very grateful for his insights and his help. We would also like to thank the other \nmembers of the abc team for their continued help and support. We had inspiring discussions with the authors \nof Java-MOP, Feng Chen and Grigore Ros\u00b8u, which greatly clari.ed the relation between JavaMOP and tracematches. \n References [1] The abc team. Benchmarks for Trace Monitoring. Scripts and sources to compile and run \nthe benchmarks: http: //aspectbench.org/benchmarks. [2] Chris Allan, Pavel Avgustinov, Aske Simon Christensen, \nLaurie Hendren, Sascha Kuzins, Ond.rej Lhot\u00b4ak, Oege de Moor, Damien Sereni, Ganesh Sittampalam, and \nJulian Tibble. Adding Trace Matching with Free Variables to AspectJ. In Object-Oriented Programming, \nSystems, Languages and Applications, pages 345 364. ACM Press, 2005. [3] AProVE. Automated Program Veri.cation \nEnvironment. http://aprove.informatik.rwth-aachen.de/, 2006. [4] AspectJ Eclipse Home. The AspectJ home \npage. http: //eclipse.org/aspectj/, 2003. [5] Pavel Avgustinov, Aske Simon Christensen, Laurie Hendren, \nSascha Kuzins, Jennifer Lhot\u00b4ak, Ond.rej Lhot\u00b4ak, Oege de Moor, Damien Sereni, Ganesh Sittampalam, and \nJulian Tibble. Optimising AspectJ. In Programming Language Design and Implementation (PLDI), pages 117 \n128. ACM Press, 2005. [6] Pavel Avgustinov, Oege de Moor, and Julian Tibble. On the semantics of trace \nmonitoring patterns. In Runtime Veri.cation, 2007. [7] Howard Barringer, Allen Goldberg, Klaus Havelund, \nand Koushik Sen. Rule-based runtime veri.cation. In Fifth International Conference on Veri.cation, Model \nChecking and Abstract Interpretation (VMCAI 04), volume 2937, pages 44 57. Lecture Notes in Computer \nScience, 2003. [8] Christoph Bockisch, Mira Mezini, and Klaus Ostermann. Quantifying over dynamic properties \nof program execution. In 2nd Dynamic Aspects Workshop (DAW05), Technical Report 05.01, pages 71 75. Research \nInstitute for Advanced Computer Science, 2005. [9] Eric Bodden, Laurie Hendren, and Ond.rej Lhot\u00b4ak. \nA staged static program analysis to improve the performance of runtime monitoring. In Proceedings of \nthe European Conference on Object-Oriented Programming, Lecture Notes in Computer Science, page to appear. \nSpringer, 2007. [10] Eric Bodden, Patrick Lam, and Laurie Hendren. Flow\u00adsensitive static optimizations \nfor runtime monitors. Technical Report abc-2007-3, abc project, 2007. http://abc. comlab.ox.ac.uk/techreports#abc-2007-3. \n[11] Feng Chen and Grigore Ros\u00b8u. Towards monitoring-oriented programming: A paradigm combining speci.cation \nand implementation. In Workshop on Runtime Veri.cation (RV 03), volume 89(2) of ENTCS, pages 108 127, \n2003. [12] Feng Chen and Grigore Ros\u00b8u. Mop: An ef.cient and generic runtime veri.cation framework. In \nDavid Bacon, editor, Proceedings of OOPSLA 2007, 2007. [13] Mar\u00b4ia Augustina Cibr\u00b4an and Bart Verheecke. \nDynamic business rules for web service composition. In 2nd Dynamic Aspects Workshop (DAW05), pages 13 \n18, 2005. [14] Marcelo d Amorim and Klaus Havelund. Event-based runtime veri.cation of java programs. \nIn WODA 05: Proceedings of the third international workshop on Dynamic analysis, pages 1 7. ACM Press, \n2005. [15] R\u00b4emi Douence, Thomas Fritz, Nicolas Loriant, Jean-Marc Menaud, Marc S\u00b4egura, and Mario S\u00a8udholt. \nAn expressive aspect language for system applications with arachne. In Aspect-Oriented Software Development, \npages 27 38. ACM Press, 2005. [16] R\u00b4emi Douence, Olivier Motelet, and Mario S\u00a8udholt. A formal de.nition \nof crosscuts. In Akinori Yonezawa and Satoshi Matsuoka, editors, Re.ection 2001, volume 2192 of Lecture \nNotes in Computer Science, pages 170 186. Springer, 2001. [17] Grigore Rosu et al. JavaMOP homepage. \nhttp://fsl.cs. uiuc.edu/index.php/JavaMOP, 2007. [18] Stephen Fink, Eran Yahav, Nurit Dor, G. Ramalingam, \nand Emmanuel Geay. Effective typestate veri.cation in the presence of aliasing. In ISSTA 06: Proceedings \nof the 2006 international symposium on Software testing and analysis, pages 133 144, New York, NY, USA, \n2006. ACM Press. [19] Thomas Fritz, Marc S\u00b4egura, Mario S\u00a8udholt, Egon Wuchner, and Jean-Marc Menaud. \nAn application of dynamic AOP to medical image generation. In 2nd Dynamic Aspects Workshop (DAW05), Technical \nReport 05.01, pages 5 12. Research Institute for Advanced Computer Science, 2005. [20] Erich Gamma. \nJHotDraw. Available from http:// sourceforge.net/projects/jhotdraw, 2004. [21] Simon Goldsmith, Robert \nO Callahan, and Alex Aiken. Relational queries over program traces. In Proceedings of the 20th Annual \nACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages and Applications, pages 385 \n402. ACM Press, 2005. [22] Peter Hui and James Riely. Temporal aspects as security automata. In Foundations \nof Aspect-Oriented Languages (FOAL 2006), Workshop at AOSD 2006, Technical Report #06-01, pages 19 28. \nIowa State University, 2006. [23] Gregor Kiczales, Erik Hilsdale, Jim Hugunin, Mik Kersten, Jeffrey Palm, \nand William G. Griswold. An overview of AspectJ. In J. Lindskov Knudsen, editor, European Conference \non Object-oriented Programming, volume 2072 of Lecture Notes in Computer Science, pages 327 353. Springer, \n2001. [24] Ramnivas Laddad. AspectJ in Action. Manning, 2003. [25] Michael Martin, Benjamin Livshits, \nand Monica S. Lam. Finding application errors using PQL: a program query language. In Proceedings of \nthe 20th Annual ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages and Applications, \npages 365 383. ACM Press, 2005. [26] Volker Stolz and Eric Bodden. Temporal Assertions using AspectJ. \nIn Electronic Notes in Theoretical Computer Science, volume 144, pages 109 124, 2006. [27] Arie van Deursen, \nLeon Moonen, and Marius Marin. AJHotDraw. http://sourceforge.net/projects/ ajhotdraw/, 2006. [28] Wim \nVanderperren, Davy Suv\u00b4e, Mar\u00b4ia Augustina Cibr\u00b4an, and Bruno De Fraine. Stateful aspects in JAsCo. In \nSoftware Composition: 4th International Workshop, volume 3628 of Lecture Notes in Computer Science. Springer, \n2005. [29] w3c. Jigsaw. http://www.w3.org/Jigsaw/, 2006. [30] Robert Walker and Kevin Viggers. Implementing \nprotocols via declarative event patterns. In ACM Sigsoft International Symposium on Foundations of Software \nEngineering (FSE\u00ad12), pages 159 169. ACM Press, 2004. [31] Ian H. Witten and Eibe Frank. Data Mining: \nPractical Machine Learning Tools and Techniques with Java imple\u00admentations. Morgan Kaufmann Publishers, \n2000. \n\t\t\t", "proc_id": "1297027", "abstract": "<p>A <i>trace monitor</i> observes an execution trace at runtime; when it recognises a specified sequence of events, the monitor runs extra code. In the aspect-oriented programming community, the idea originatedas a generalisation of the advice-trigger mechanism: instead of matchingon single events (joinpoints), one matches on a sequence of events. The runtime verification community has been investigating similar mechanisms for a number of years, specifying the event patterns in terms of temporal logic, and applying the monitors to hardware and software.</p> <p>In recent years trace monitors have been adapted for use with mainstream object-oriented languages. In this setting, a crucial feature is to allow the programmer to quantify over groups of related objects when expressing the sequence of events to match. While many language proposals exist for allowing such features, until now no implementation had scalable performance: execution on all but very simple examples was infeasible.</p> <p>This paper rectifies that situation, by identifying two optimisations for <i>generating feasible</i> trace monitors from declarative specifications of the relevant event pattern. We restrict ourselves to optimisations that do not have a significant impact on compile-time: they only analyse the event pattern, and not the monitored code itself.</p> <p>The first optimisation is an important improvement over an earlier proposal in [2] to avoid space leaks. The second optimisation is a form of indexing for partial matches. Such indexing needs to be very carefully designed to avoid introducing new space leaks, and the resulting data structure is highly non-trivial.</p>", "authors": [{"name": "Pavel Avgustinov", "author_profile_id": "81100580373", "affiliation": "University of Oxford, Oxford, United Kingdom", "person_id": "PP14200390", "email_address": "", "orcid_id": ""}, {"name": "Julian Tibble", "author_profile_id": "81100521744", "affiliation": "University of Oxford, Oxford, United Kingdom", "person_id": "PP39046745", "email_address": "", "orcid_id": ""}, {"name": "Oege de Moor", "author_profile_id": "81100198102", "affiliation": "University of Oxford, Oxford, United Kingdom", "person_id": "PP14078760", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1297027.1297070", "year": "2007", "article_id": "1297070", "conference": "OOPSLA", "title": "Making trace monitors feasible", "url": "http://dl.acm.org/citation.cfm?id=1297070"}