{"article_publication_date": "10-21-2007", "fulltext": "\n The Transactional Memory / Garbage Collection Analogy Dan Grossman University of Washington djg@cs.washington.edu \nAbstract This essay presents remarkable similarities between transac\u00adtional memory and garbage collection. \nThe connections are fascinating in their own right, and they let us better under\u00adstand one technology \nby thinking about the corresponding issues for the other. Categories and Subject Descriptors D.1.3 [Programming \ntechniques]: Concurrent Programming Parallel program\u00adming; D.3.3 [Programming languages]: Language Con\u00adstructs \nand Features Concurrent programming structures; D.3.4 [Programming languages]: Processors Memory man\u00adagement \n(garbage collection) General Terms Languages Keywords Transactional Memory, Garbage Collection 1. Introduction \nTransactional memory is currently one of the hottest topics in computer-science research, having attracted \nthe focus of researchers in programming languages, computer architec\u00adture, and parallel programming, \nas well as the attention of development groups at major software and hardware compa\u00adnies. The fundamental \nsource of the excitement is the belief that by replacing locks and condition variables with trans\u00adactions \nwe can make it easier to write correct and ef.cient shared-memory parallel programs. Having made the \nsemantics and implementation of trans\u00adactional memory a large piece of my research agenda [44, 46, 32, \n19], I believe it is crucial to ask why we believe transactional memory is such a step forward. If the \nreasons are shallow or marginal, then transactional memory should probably just be a current fad, as \nsome critics think it is. If we cannot identify crisp and precise reasons why trans\u00adactions are an improvement \nover locks, then we are being neither good scientists nor good engineers. Permission to make digital \nor hard copies of all or part of this work for personal or classroom use is granted without fee provided \nthat copies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. OOPSLA 07, October 21 25, 2007, Montr\u00b4eal, \nQu\u00b4ebec, Canada. Copyright c . 2007 ACM 978-1-59593-786-5/07/0010. . . $5.00 The purpose of this article \nis not to rehash excellent but previously published examples where software transactions provide an enormous \nbene.t (though for background they are brie.y discussed), nor is it to add some more examples to the \nlitany. Rather, it is to present a more general perspective that I have developed over the last two years. \nThis perspec\u00adtive is summarized in a one-sentence analogy: Transactional memory (TM) is to shared-memory \nconcurrency as garbage collection (GC) is to memory management. Having shared this sentence with many \npeople, I have come to realize that this sort of pithy analogy has advantages and disadvantages. On the \none hand, it sparks discussion and is easy to remember. When fully understood, such an analogy can inspire \nnew research ideas and let one adapt terminology from one side of the analogy for use in the other. On \nthe other hand, it is easy to misinterpret an analogy, apply it too broadly, or dismiss it as just a \nslogan. The key is to understand that an analogy like this one is not a complete argument; it is an introductory \nremark for a more complete discussion pointing out the remark s deeper meaning and limitations. This \narticle is designed to provide a cogent starting point for that discussion. The primary goal is to use \nour understanding of garbage collection to better understand transactional memory (and possibly vice-versa). \nThe presentation of the TM/GC analogy that follows will demonstrate that the analogy is much deeper than, \nhere are two technologies that make programming easier. However, it will not conclude that TM will make \nconcurrent program\u00adming as easy as sequential programming with GC. Rather, it will lead us to the balanced \nand obvious-once-you-say-it conclusion that transactions make it easy to de.ne critical sections (which \nis a huge help in writing and maintaining shared-memory programs) but provide no help in identifying \nwhere a critical section should begin or end (which remains an enormous challenge). I begin by providing \na cursory review of memory man\u00adagement, garbage collection, concurrency, and transactional memory (Section \n2). This non-analogical discussion simply introduces relevant de.nitions for the two sides and may leave \nyou wondering how they could possibly have much to do with each other. I then present the core of the \nanalogy, un\u00adcovering many uncanny similarities even at a detailed level (Section 3). This discussion \ncan then be balanced with the primary place the analogy does not hold, which is exactly the essence of \nwhat makes concurrent programming inherently more dif.cult no matter what synchronization mechanisms \nare provided (Section 4). Having completed the crux of the argument, I then pro\u00advide some additional \ncontext. First is a brief detour for an analogous type-theoretic treatment of manual memory man\u00adagement \nand locking, a prior focus of my research that pro\u00advides some backstory for how the TM/GC analogy came \nto be (Section 5). Second are some conjectures one can make by pushing the analogy (perhaps) too far \n(Section 6). Finally, the conclusion describes the intended effects of publishing this article (Section \n7).  2. Background A full introduction to garbage collection and transactional memory is clearly beyond \nour scope (excellent overviews exist for GC [48, 33] and TM [35]), so this section will just introduce \nenough de.nitions to understand most of the claims that follow and provide some motivation for TM. Some \nreaders may be able to skip much of this section. For the sake of speci.city, I will assume programs \nare written in a modern object-oriented language (e.g., Java) and interthread communication is via mutable \nshared-memory. Much of the discussion applies to other paradigms (e.g., mostly-functional programming) \nbut less to communica\u00adtion via message-passing. (I will not wade into the merits of shared memory versus \nmessage passing. Assuming that shared memory is one model we will continue to use for the foreseeable \nfuture, it is worth improving.) 2.1 Garbage Collection When a program creates an object, space for it \nis allocated in the heap. In a language with manual memory management, this space is explicitly reclaimed \n(freed) by the programmer. Accessing an object after reclaiming its space is a dangling\u00adpointer dereference \nand behavior is typically unde.ned. Not reclaiming space in a timely manner is a space leak and hurts \nperformance. Garbage collection automates memory reclamation. The key idea is determining reachability: \nan object is reclaimed only after there is no sequence of references (i.e., a path) from a root (a global \nvariable or (live) local variable) to the object. Reachability can be determined via tracing (an algorithm \nthat starts with the roots and .nds all reachable objects) or automatic reference-counting (an algorithm \nthat maintains the number of references to each object, reclaim\u00ading an object when its count reaches \nzero). These algorithms are duals [5]. In practice, ef.cient garbage collectors use var\u00adious high-level \ntechniques (e.g., generational collection) and low-level tricks (e.g., pointer-reversal). GC eliminates \ndangling-pointer dereferences. Space leaks are possible exactly when reachability is an imprecise ap\u00adproximation \nof object lifetime. To avoid imprecision, pro\u00adgrammers need to avoid having dead objects (i.e., objects \nthey are done with) remain reachable from roots. One help\u00adful language feature is weak pointers references \nthat do not count for reachability . If the garbage collector reclaims the target of a weak pointer, \nit updates the pointer to indicate it cannot be used (e.g., by setting it to null). Conservative collection \ntreats integers as possible point\u00aders, so an object allocated at address a is not reclaimed (nor therefore, \nis anything reachable from it) if a live piece of memory holds the integer a. Accurate (i.e., nonconserva\u00adtive) \ncollection uses tighter coupling between the compiler, which generates code for creating and accessing \nobjects, and the run-time system, which includes the garbage-collector proper. The compiler can provide \nother static information to the collector, such as whether local variables are live at cer\u00adtain program \npoints. Real-time collection ensures the garbage collector never pauses the program for longer than a \n.xed threshold. The key complication is ensuring the collector can keep up (i.e., reclaim garbage as \nfast as it can be created) while meeting its deadlines, else space can become exhausted even though there \nis memory available for reclamation. 2.2 Transactional Memory The assumed concurrency model allows programmers \nto create additional threads to execute code in parallel with all the other threads. Preemptive scheduling \nmeans a thread can be stopped at any point so other threads can use one of the available processors. \nThreads must communicate to coor\u00addinate the computation they are completing together. With shared memory, \none thread can write to a .eld of an object and another thread can then read the value written. Shared \nmemory and preemption (or true parallelism) are a dif.\u00adcult combination (e.g., a thread might be preempted \nbetween executing data=x; and executing data_changed=true;), so languages provide synchronization mechanisms \nby which programmers can prevent some thread interleavings. For example, mutual-exclusion locks have \nacquire and release operations. If thread A invokes the acquire operation on a lock that thread B has \nacquired but not yet released, then thread A is blocked (does not run) until A releases the lock and \nB holds the lock. Incorrect locking protocols can lead to races (undesirable interleavings) or deadlocks \n(a cycle of threads that can never proceed because they are all waiting for a blocked thread to release \na lock). Transactional memory provides a synchronization mech\u00adanism that is easier-to-use but harder-to-implement \nthan locks. At its simplest, it is just a new statement form atomic{s} that executes the statement s \nas though there is no interleaved computation from other threads. In principle, s can include arbitrary \ncode, but in practice systems typically limit some operations, such as I/O, foreign-function calls, or \ncreating new threads. An explicit abort statement lets pro\u00adgrammers indicate the body of the atomic block \nshould be retried again later.1 For example, a dequeue method for a synchronized queue might be: // block \nuntil an object is available. // getNextObject fails if the queue is empty. Object dequeue() { atomic \n{ if(isEmpty()) abort; return getNextObject(); } } TM implementations try to execute the atomic-block \nbody s concurrently with other computation, implicitly aborting and retrying if a con.ict is detected. \nThis is im\u00adportant for performance (not stopping all other threads for each atomic block) and fairness: \nif s runs too long, other threads must be allowed to continue and the thread executing s should retry \nthe transaction. In a different shared-memory state, s may complete quickly. Con.icts are usually de.ned \nas memory con.icts: s and another thread access the same memory and at least one access is a write. The \nessence of a TM implementation is two-fold: detecting con.icts and ensuring all of a transaction s updates \nto shared memory appear to happen at once . The distinction between weak-and strong-atomicity [8] refers \nto a system s behavior when a memory access not within the dynamic scope of an atomic block con.icts \nwith a concurrent access (by another thread) within such a scope. Weak-atomicity systems can violate \na transaction s isolation in this case, and can produce much stranger program behav\u00adior than is generally \nappreciated [46]. Prohibiting memory con.icts between parallel transac\u00adtions is sometimes unnecessarily \nconservative. For example, if two transactions both use a unique-ID generator, they may both increment \na counter but there is no logical con.ict. Open nesting is a language construct supporting such non\u00adcon.ict \naccess. The statement open{s} executes s within a transaction, but (1) accesses in s are not considered \nfor con\u00ad.ict detection and (2) accesses in s are not undone if the transaction aborts. Obstruction-freedom \nis, roughly speaking, the property that any transaction can continue (i.e., advance its program counter) \neven if all other transactions are suspended. Some TM implementations have this property and some do \nnot; its importance is fairly controversial [14]. Transactions are a classic concept in databases and \ndis\u00adtributed systems. Transactional support in hardware [30], 1 This abort is an abort-and-retry; some \nsystems also have an abort-and\u00adcontinue. programming languages [21], and libraries [45] had early advocates, \nwith recent interest beginning with Harris and Fraser s work for Java [24]. Approaches to implement\u00ading \nTM in compilers [46, 27, 2, 25, 26, 44, 36, 32], li\u00adbraries [29, 37, 28], hardware [23, 9, 38, 42, 4, \n40, 41], and software/hardware hybrids [12, 34] have been pub\u00adlished, and transactions are part of several \nnext-generation languages [3, 10, 11].  2.3 Motivations for Transactional Memory In general, TM advocates \nbelieve it is better than locking be\u00adcause it has software-engineering bene.ts avoiding locks dif.culties \nand performance bene.ts due to optimistic concurrency, transactions proceed in parallel unless there \nare dynamic memory con.icts. Several idioms where TM is su\u00adperior have been given:2 It is easier to \nevolve software to include new synchro\u00adnized operations. For example, consider the simple bank\u00adaccount \nclass in Figure 1. If version 1 of the software did not anticipate the need for a transfer method, the \nself-locking approach makes sense. Given this, modi\u00adfying the software to support transfer without po\u00adtential \nraces (see transfer_wrong1) or deadlock (see transfer_wrong2) requires wide-scale changes involv\u00ading \nsubtle lock-order protocols. This issue arises in Java s StringBuffer append method, which is presumably \nwhy this method is not guaranteed to be atomic [15].  It is easier to mix .ne-grained and coarse-grained \nop\u00aderations. For example, most hashtable operations access only a small part of the table, but supporting \nparallel insert and lookup operations while still having a cor\u00adrectly synchronized resize table operation \nis dif.cult with locks and trivial with TM.  It is easier to write code that is ef.cient when memory\u00adcon.icts \nare rare while remaining correct in case they occur. For example, allowing parallel access to both ends \nof a double-ended queue is dif.cult with locks because there can be contention, but only when the queue \nhas fewer than two elements [39]. A solution using TM is trivial.  With the addition of the orelse combinator \n[25], in which atomic { s1 } orelse { s2 } tries s2 atom\u00adically if s1 aborts (retrying the whole thing \nif s2 also aborts), we can combine alternative atomic actions, such as trying to dequeue from one of \ntwo synchronized queues, blocking only if both are empty. This addition affords shared-memory concurrency \nsome of the advan\u00adtages of Concurrent ML s choice operator for message passing [43].  2 None of these \nexamples are new. To the best of my knowledge, they were originally presented in various forms by Flanagan, \nHarris, Herlihy, and Peyton Jones, respectively. class Account { class Account { float balance; float \nbalance; synchronized void deposit(float amt) { void deposit(float amt) { balance += amt; atomic { balance \n+= amt; } } } synchronized void withdraw(float amt) { void withdraw(float amt) { if(balance < amt) atomic \n{ throw new OutOfMoneyError(); if(balance < amt) balance -= amt; throw new OutOfMoneyError(); } balance \n-= amt; void transfer_wrong1(Acct other, float amt) { } other.withdraw(amt); } // race condition: wrong \nsum of balances void transfer(Acct other, this.deposit(amt); float amt) { } atomic { synchronized void \ntransfer_wrong2(Acct other, float amt) { other.withdraw(amt); // can deadlock with parallel reverse-transfer \nthis.deposit(amt); other.withdraw(amt); } this.deposit(amt); } } } Figure 1. Example showing the dif.culty \nof extending software written with locks. If the transfer method must not allow other threads to see \nan incorrect sum-of-balances-in-all-accounts, then the .rst attempt on the left is wrong and the second \nattempt can deadlock. In the TM code on the right, adding transfer is straightforward. In addition, advocates \nsometimes claim TM does not suf\u00adfer from races or deadlocks or that, it is obviously easier. The reasons \nfor these claims and the limitations of them will become apparent in the subsequent two sections.  \n3. The Core Analogy Without further ado, I now present the similarities between transactional memory \nand garbage collection, from the prob\u00adlems they solve, to the way they solve them, to how poor pro\u00adgramming \npractice can nullify their advantages. The points in this section are all technical in nature; any analogies \nbe\u00adtween the social processes behind the technologies of GC and TM are relegated to Section 6. To appreciate \nfully the analogy I recommend reading each section twice. First read the descriptions of GC and TM all \nat once to make sure they are accurate and relevant. Then read the descriptions by interleaving sentences \n(or even phrases) to appreciate that the structure is identical with the difference being primarily the \nsubstitution of a few nouns. The Problems Memory management is dif.cult because one must use memory reclamation \nto balance correctness, i.e., avoiding dangling-pointer dereferences, and performance, i.e., avoid\u00ading \nthe loss of space or even the ability to continue due to space exhaustion. In programs that manually \nmanage mem\u00adory, the programmer uses subtle whole-program protocols to avoid errors. One of the simpler \napproaches manually asso\u00adciates each data object with a reference count and requires a nonzero count \nwhen accessing the data (freeing the mem\u00adory when the count is zero). To avoid lost space, it is suf\u00ad.cient \nto disallow cycles in the object graph, but in prac\u00adtice this requirement is too burdensome. Sharing \nreference counts among objects (cf. region-based memory manage\u00adment [16]) reduces the number of reference \ncounts but may increase space consumption. Unfortunately, memory-management protocols are non\u00admodular: \nCallers and callees must know what data the other may access to avoid deallocating needed data or making \nun\u00adneeded data unreachable. A small change for example, a new function that needs data previously deemed \nno longer necessary may require wide-scale changes or introduce bugs. In essence, memory management \ninvolves nonlocal properties: Correctness requires knowing what data subse\u00adquent computation will access. \nOne must reason about how data is used across time to determine when to deallocate an object. If a program \nchange affects when an object is used for the last time, the program s memory management may become wrong \nor inef.cient. Concurrent programming is dif.cult because one must use synchronization to balance correctness, \ni.e., avoiding race conditions, and performance, i.e., avoiding the loss of parallelism or even the ability \nto continue due to deadlock. In programs that manually manage mutual-exclusion locks, the programmer \nuses subtle whole-program protocols to avoid errors. One of the simpler approaches associates each data \nobject with a lock and holds the lock when accessing the data. To avoid deadlock, it is suf.cient to \nenforce a partial\u00adorder on the order a thread acquires locks, but in practice this requirement is too \nburdensome. Sharing locks among objects reduces the number of locks but may reduce paral\u00adlelism. Unfortunately, \nconcurrency protocols are non-modular: Callers and callees must know what data the other may ac\u00adcess \nto avoid releasing locks still needed or acquiring locks that could make threads deadlocked. A small \nchange for example, a new function that must update two thread-shared objects atomically with respect \nto other threads may require wide-scale changes or introduce bugs. In essence, concurrent programming \ninvolves nonlocal properties: Cor\u00adrectness requires knowing what data concurrently executing computation \nwill access. One must reason about how data is used across threads to determine when to acquire a lock. \nIf a program change affects when an object is used concurrently, the program s synchronization protocol \nmay become wrong or inef.cient. The Solutions GC takes the subtle whole-program protocols suf.cient \nto avoid dangling-pointer dereferences and space leaks and moves them into the language implementation. \nAs such, they can be implemented once and for all by experts focused only on their correct and ef.cient \nimplementation. Program\u00admers specify only what data points to what, relying on the implementation to \nbe correct (no dangling pointers) and ef\u00ad.cient (reclaiming unreachable memory in a timely man\u00adner). \nNote the garbage collector does maintain subtle whole\u00adprogram invariants, often with the support of the \ncompiler and/or hardware. As examples, header words may identify which .elds hold pointers and a generational \ncollector may assume there are no unknown pointers from mature ob\u00adjects to young objects. The whole-program \nprotocols necessary for GC are most easily implemented in some combination of the compiler (particularly \nfor read and/or write barriers) and the run\u00adtime system (including hardware) because we can localize \nthe implementation of the protocols. Put another way, the dif.culty of implementation does not increase \nwith the size of the source program. In theory, garbage collection can improve performance by increasing \nspatial locality (due to object-relocation), but in practice we pay a moderate performance cost for software\u00adengineering \nbene.ts. TM takes the subtle whole-program protocols suf.cient to avoid races and deadlock and moves \nthem into the lan\u00adguage implementation. As such, they can be implemented once and for all by experts \nfocused only on their cor\u00adrect and ef.cient implementation. Programmers specify only what must be performed \natomically (as viewed from other threads), relying on the implementation to be correct (no atomicity \nviolations) and ef.cient (reasonably paral\u00adlel, particularly when transactions do not contend for data). \nNote the transactional-memory implementation does main\u00adtain subtle whole-program invariants, often with \nthe sup\u00adport of the compiler and/or hardware. As examples, header words may hold version numbers and \nsystems optimizing for thread-local data may assume there are no pointers from thread-shared objects \nto thread-local objects. The whole-program protocols necessary for TM are most easily implemented in \nsome combination of the compiler (particularly for read and/or write barriers) and the run\u00adtime system \n(including hardware) because we can localize the implementation of the protocols. Put another way, the \ndif.culty of implementation does not increase with the size of the source program. In theory, transactional \nmemory can improve perfor\u00admance by increasing parallelism (due to optimistic concur\u00adrency), but in practice \nwe may pay a moderate performance cost for software-engineering bene.ts.  Incomplete Solutions GC is \nprobably not a natural match for all parts of all ap\u00adplications. Examples may include applications where \ntrad\u00ading space for time is a bad performance decision or where heap-allocated data lifetime follows an \nidiom not closely ap\u00adproximated by reachability. Language features such as weak pointers allow reachable \nmemory to be reclaimed, but us\u00ading such features correctly is best left to experts or easily recognized \nsituations such as a software cache. Recogniz\u00ading that GC may not always be appropriate, languages can \ncomplement it with support for other idioms, such as re\u00adgions [6, 22, 20]. In the extreme, programmers \ncan code manual memory management on top of garbage collection, destroying the advantages of garbage \ncollection. Figure 2 shows one ba\u00adsic approach in Java.3 More ef.cient implementations (e.g., using a \nfree list) are straightforward extensions. A program\u00admer can then treat mallocT as the way to get fresh \nT objects, but an object passed to freeT may be returned by mallocT, reintroducing the dif.culties of \ndangling pointers.4 In prac\u00adtice, we can expect less extreme idioms that still introduce application-level \nbuffers for frequently used objects. TM is probably not a natural match for all parts of all applications. \nExamples may include applications where op\u00adtimistically attempting parallel transactions is a bad perfor\u00ad \n3 The code supports only objects of a single type, but separate per-type allocators or fancier tricks \nsuch as re.ection can avoid this. 4 The fact that the program cannot seg fault is little consolation; \nin fact, if my C program has dangling-pointer dereferences I hope it does seg fault rather than silently \nuse aliases to just-allocated memory.  class AllocT { class Lock { T[] buffer = new T[100]; boolean \nheld = false; boolean[] available = new boolean[100]; void acquire() { AllocT() { while(true) for(int \ni=0; i<1000; ++i) buffer[i] = new T(); atomic { for(int i=0; i<1000; ++i) available[i] = true; if(!held) \n{ } held=true; T mallocT() { return; for(int i=0; i < 1000; ++i) { } if(!available[i]) continue; } available[i] \n= false; } return buffer[i]; void release() { } atomic { held = false; } throw new OutOfMemoryError(); \n//could resize buffer } } } void freeT(T t) { for(int i=0; i < 1000; ++i) if(buffer[i]==t) available[i] \n= true; } } Figure 2. Left: Java code building manual memory management on top of garbage collection. \nRight: Java code with atomic statements building locks on top of transactional memory. mance decision \nor where correct synchronization follows an idiom not closely approximated by data con.icts. Language \nfeatures such as open-nested transactions allow transactions with data con.icts to succeed in parallel, \nbut using such fea\u00adtures correctly is best left to experts or easily recognized situ\u00adations such as unique-identi.er \ngeneration. Recognizing that TM may not always be appropriate, most prototypes con\u00adtinue to support other \nidioms, such as locks and condition variables. In the extreme, programmers can code locks on top of transactional \nmemory, destroying the advantages of trans\u00adactional memory. Figure 2 shows one basic approach in Java. \nMore powerful libraries (e.g., supporting reentrancy) are straightforward extensions. A programmer can \nthen treat the Lock methods as synchronization primitives and reintro\u00adduce the dif.culties of locking. \nIn practice, we can expect less extreme idioms that still reintroduce application-level races and deadlocks. \n Two Basic Approaches Despite a wide variety of garbage-collection algorithms in terms of details, there \nare two fundamental approaches. First, tracing collection uses tracing code to .nd all live data and \ngarbage-collect the rest. The code running the ap\u00adplication simply treats all data as live, delaying \nthe check for unreachable data. Second, automatic reference-counting checks the number of references \nto an object while the appli\u00adcation runs, allowing unreachable data to be found immedi\u00adately. However, \na cycle of garbage can lead to data being kept when it should not be, so other techniques (e.g., trial \ndele\u00adtion ) can complement the core reference-counting mech\u00adanism. In practice, compilers employ deferred \nreference counting to avoid the overhead of constantly manipulating reference counts while delaying the \ncheck for garbage. Despite a wide variety of transactional-memory algo\u00adrithms in terms of details, there \nare two fundamental ap\u00adproaches. First, update-on-commit TM uses private copies of accessed data and \nrelies on a validate/commit protocol to detect con.icts and re.ect changes back to shared mem\u00adory. The \ncode running the application simply assumes there will be no con.icts, delaying the check for inconsistent \ndata. Second, update-in-place TM checks the consistency of data while the application runs, allowing \ninconsistent data to be detected immediately and the transaction aborted. However, a cycle of transactions \ncan cause themselves to all abort when it is unnecessary,5 so other techniques (e.g., priority\u00adbased \nschemes) can complement the core update-in-place mechanism. In practice, compilers employ optimistic \nreads with update-in-place to avoid the overhead of checking con\u00adsistency on every .eld read while delaying \nthe check for transaction-consistency. I/O GC is dif.cult to reconcile with external effects such as \ninput and output of pointers because it is not always pos\u00adsible to establish what pointer values may \nbe encoded by 5 This point is subtle: Suppose transaction A seeks to write to object o which has already \nbeen written to by transaction B. An update-in-place system might abort A,but if B is going to abort \nlater (perhaps because it attempts to write something already written to by A), we could let A proceed. \noutput. Because such hidden pointers do not matter unless the external world uses them to generate input \nthat rema\u00adterializes the pointer, the essence of the problem is input\u00adafter-output. In a distributed \nsetting, GC becomes more dif.\u00adcult because it requires consensus on when data is no longer needed. In \npractice, serializing objects is often suf.cient. TM is dif.cult to reconcile with external effects such \nas input and output because it is not always meaning-preserving to delay output until a transaction commits. \nBecause such output does not matter unless the external world uses it to generate input needed for the \ntransaction, the essence of the problem is input-after-output. In a distributed setting, TM becomes more \ndif.cult because it requires consensus on when a transaction is completed. It is not yet clear what suf.ces \nin practice. False Sharing For reasons of performance and simplicity, garbage col\u00adlectors typically \nreclaim only entire objects, rather than re\u00adclaiming parts of objects that contain dead .elds. That is, \nmemory management is done with object-level granular\u00adity. As a result, extra space can be consumed, but \nspace\u00adconscious programmers aware of object-level granularity can restructure data to circumvent this \napproximation be\u00adcause object size is under programmer control. However, with conservative garbage collection, \nprogram\u00admers can no longer fully control how much memory is reach\u00adable. Because the memory address at \nwhich an object is al\u00adlocated is uncontrollable, a collision with a live integer value could lead to \nspace consumption. For reasons of performance and simplicity, some im\u00adplementations of transactional-memory \ndetect memory con\u00ad.icts between entire objects, rather than permitting parallel access to distinct parts \nof objects. That is, con.ict manage\u00adment is done with object-level granularity. As a result, ex\u00adtra contention \ncan occur, but parallelism-conscious program\u00admers aware of object-level granularity can restructure data \nto circumvent this approximation because object size is under programmer control. However, with granularity \ncoarser than objects (e.g., at cache lines), programmers can no longer fully control how many false con.icts \noccur. Because the memory address at which an object is allocated is uncontrollable, adjacent placement \nof independent objects could lead to lost paral\u00adlelism. Progress Guarantees Most garbage collectors \ndo not make real-time guaran\u00adtees. Providing such worst-case guarantees can incur sub\u00adstantial extra \ncost in the expected case, so real-time collec\u00adtion is typically eschewed unless an application needs \nit. The key complication is continuing to make progress with collec\u00adtion while the program could be performing \narbitrary opera\u00adtions on the reachable objects the collector is analyzing. Some implementations of transactional \nmemory do not make obstruction-freedomguarantees. Providing such worst\u00adcase guarantees can incur substantial \nextra cost in the ex\u00adpected case, so obstruction-freedom should perhaps be es\u00adchewed unless an application \nneeds it. The key complication is continuing to make progress with any transaction while another thread \ncould be suspended after having accessed any of the objects the transaction is accessing. Static-Analysis \nImprovements Compile-time information can improve the performance of GC. The most common approach is \nliveness analysis for determining that the contents of a local variable is not used after a certain program \npoint. This information allows the collector to treat fewer local variables as roots. Other anal\u00adyses \ncan also prove useful. For example, in a generational setting, static analysis can remove write barriers \nfor objects that are de.nitely in the nursery [49]. Compile-time information can improve the performance \nof TM. The most common approach is escape analysis for determining that the contents of a local variable \nis not reach\u00adable from multiple threads before a certain program point. This information allows the transactional-memory \nimple\u00admentation to treat fewer memory accesses as potential mem\u00adory con.icts. Other analyses can also \nprove useful. For ex\u00adample, in a strong-atomicity setting, static analysis can re\u00admove write barriers \nfor objects that are de.nitely never ac\u00adcessed in a transaction [46]. Conclusion Figure 3 summarizes \nsome of the connections discussed. In general, the point is that GC and TM aim to automate what is otherwise \nan error-prone and detailed task of bal\u00adancing correctness and performance. As general solutions, GC \nand TM rely on approximations that seem to work well for many but not all applications, and they cannot \nperform well with code speci.cally aimed at circumventing their ad\u00advantages. Ef.cient and elegant approaches \nto implementing and improving GC/TM can involve any combination of code generation, static analysis, \nrun-time systems, and hardware.  4. The Essence of Concurrency The previous section described many \nways that transactional memory is like garbage collection. The most exciting as\u00adpect is that it makes \ndeveloping, maintaining, and evolv\u00ading concurrent programs easier, by moving the low-level synchronization \nprotocols into the programming-language implementation. In so doing, we let the application develop\u00adFigure \n3. Summary of some differences in nouns between the two sides of the analogy. memory management concurrency \ndangling pointers races space exhaustion deadlock regions locks garbage collection transactional memory \nreachability memory con.icts nursery data thread-local data weak pointers open nesting I/O of pointers \nI/O in transactions tracing update-on-commit automatic reference counting update-in-place deferred reference \ncounting optimistic reads conservative collection false memory-con.icts real-time collection obstruction-freedom \nliveness analysis escape analysis ers focus on the higher-level concern of determining where critical \nsections should begin and end, i.e., determining which shared-memory states should be accessible to mul\u00adtiple \nthreads. However, the claim that TM makes concurrent program\u00adming easier must not be misconstrued as \na claim that it makes concurrent programming easy. Delimiting critical sections is a fundamentally dif.cult \napplication-speci.c challenge with by de.nition no analogue in sequen\u00adtial programming. Critical sections \nthat are too small lead to application-level races because other threads may see in\u00adconsistent state \n(e.g., the sum of two bank-account balances may be too large). Critical sections that are too large may \nimpede application-level progress because they deny access to an intermediate state another thread needs. \nAn example adapted from Blundell et al. [8] in Figure 4 illustrates this point. In the example, Thread \n2 s critical section can complete successfully only after Thread 1 s .rst critical section, and Thread \n1 s second critical section can complete successfully only after Thread 2 s critical section. Therefore, \nprogram behavior is altered (from terminating to nonterminating) if Thread 1 s critical sections are \ncombined by placing them in a larger atomic block. This example, however, does not contradict the claim \nthat TM enables composable critical sections, such as the account-transfer example in Section 2. Unlike \nwith programs built out of locks, we can create larger critical sections out of smaller ones without \nintroducing deadlock or changing the locking protocol for code already written. Whether doing so is appropriate \nfor the application falls under the essential dif.culty of shared-memory concurrent programming, i.e., \ndelimiting critical sections. I have heard advocates of TM claim that atomic blocks are more declarative \nthan locks, but I have not seen a Initially, x = y = 0 Thread 1 Thread 2 //atomic { atomic { atomic { \nif(x==0) x=1; abort; } y=1; atomic { } if(y==0) abort; } //} Figure 4. An example showing that enlarging \ncritical sec\u00adtions can break application correctness. Uncommenting the atomic statement in Thread 1 leads \nto application-level dead\u00adlock. precise justi.cation of this claim. Here it is: The essence of shared-memory \nconcurrent programming is deciding where critical sections should begin and end. With atomic blocks, \nprogrammers do precisely that rather than encode critical sections via other synchronization mechanisms. \nThat is, they declare where interleaved computation from other threads is and is not allowed. Returning \nto our analogy, there is a connection to be made. Concurrency adds expressiveness (the ability to have \nmultiple threads of control) and performance potential (via parallelism) just as memory reclamation adds \nexpressive\u00adness (the ability to compute with a conceptually unbounded amount of memory rather than statically \nallocating all ob\u00adjects) and performance potential (via a memory footprint matching dynamic behavior). \nOn the memory management side, the essential dif.culty introduced is to avoid using too much memory, \nsomething much easier to control, albeit con\u00adservatively, when you allocate all your memory at the begin\u00adning \nof program execution. There is, however, a large qualitative difference in the two essential dif.culties.6 \nAvoiding space leaks in a garbage\u00adcollected program just seems much easier than avoiding incorrect interleavings \nin a transactional-memory program. That is why TM, despite being a great leap forward, will not make \nconcurrent programming easy. 5. A Brief Digression for Types It turns out GC and TM are not the only \nsolutions to mem\u00adory management and concurrency that enjoy remarkable similarities. The type systems \nunderlying statically checked languages for region-based memory management [47] and lock-based data-race \nprevention [1] are essentially identi\u00adcally structured type-and-effect systems. In adapting work on both \nto the Cyclone programming language, I was able to exploit this similarity to provide a simpler and more \nregular 6 There is also a theoretical difference: Unbounded memory is necessary for Turing-completeness; \nconcurrency is not. type system [17, 20, 18]. I will not repeat that development here, but by very brie.y \nsketching the similarity, my intent is to: Provide further evidence that memory management and concurrency \nare problems with very similar structure, not just, two different problems in software development. \n Suggest (though I cannot prove it, even to myself) that my background in type systems provided the intellectual \ngrounding that allowed me to stumble across the GC/TM analogy.  In region-based memory management, we \ncan have these primitives:7 new_region() create a new region free_region(rgn) deallocate rgn and all \nits objects use_region(rgn){s} allow access to objects in rgn in statement s new (rgn) C() put a new \nobject in region rgn A type-and-effect system can ensure a region s objects are created or accessed \nonly in the dynamic scope of an appropriate use_region and a region is freed only outside such a statement. \nAs such, the only dynamic checks are for has this region already been deallocated and occur on en\u00adtry \nto use_region or on free_region. The key to type soundness (no dangling-pointer dereferences) is using \nfresh type variables to ensure every region has a type distinct from every other region. The key to expressiveness \nis parametric polymorphism so that methods can be parameterized over the regions in which the data they \naccess resides. A compu\u00adtation s effect is the set of regions that may need to be live while the computation \nis performed. In lock-based data-race prevention, we can have these primitives:8 new_lock() create a \nnew lock synchronized(lk){s} allow access to objects guarded by lk in statement s new (lk) C() create \nnew object guarded by lk A type-and-effect system can ensure the objects guarded by a lock are accessed \nonly in the dynamic scope of an appropriate synchronized. As such, the only dynamic checks are for is \nthis lock available and occur on entry to synchronized. The key to type soundness (no data races) is \nusing fresh type variables to ensure every lock has a type distinct from every other lock. The key to \nexpressiveness is 7 This description is similar to the primitives in many systems, but not exactly like \nany of them. 8 See previous note. parametric polymorphism so that methods can be parame\u00adterized over \nthe locks guarding the data they access. A com\u00adputation s effect is the set of locks that may need to \nbe held while the computation is performed. 6. Unsubstantiated Conjectures So far, I have been careful \nto focus on the essence of the technologies being compared, emphasizing that GC and TM share a remarkably \nsimilar structure without jumping to the incorrect conclusion that TM will make concurrent program\u00adming \nas easy as sequential programming. While you might quibble with some speci.c aspect of the analogy as \npre\u00adsented, the sheer number of correspondences suggests there is something fundamental between approaches \nto memory management and concurrency. Leaving the realm of thoughtful technological inquiry to stretch \nthe analogy further proves too irresistible. After all, GC and TM have both been developed in the real \nworld, with all the engineering and social pressures that entails. Provided we do not take the conclusions \ntoo seriously, we can try to draw parallels at this metalevel as well. I will make three claims about \nthe development and success of GC and then extrapolate to what it would mean for TM to follow a similar \narc. Claim #1: GC did not need hardware support to succeed. Hardware support for GC has certainly been \nconsidered and built, and at least some believed GC would never be fast enough to be accepted without \nit. But special-purpose hardware has trouble supporting algorithmic advances in a timely fashion; GC \nhardware just never kept up. Business priorities may have been another, possibly primary, reason GC hardware \nnever predominated. Nonetheless, architec\u00adtural features (from the instruction set to the cache write\u00adback \npolicy) still can have signi.cant effects on the perfor\u00admance of languages with GC [13, 7, 31]. TM can \nalso bene.t from hardware support, particularly the ability to detect memory con.icts with technology \nat the level of cache-coherence protocols. However, if the history of GC is a guide, hardware does not \nneed any speci.c notion of a transaction for TM to succeed. Moreover, other archi\u00adtectural features may \nhave a bigger effect on performance than we currently realize. Of course, hardware support for transactions, \nparticularly schemes for mixing software and hardware transactions [12, 34], are currently a more active \narea in the architecture community than hardware GC ever was. So, turning the analogy around, perhaps \nthe time is ripe for a resurgence of research into GC hardware. Claim #2: GC took decades longer to reach \nmainstream adoption than its initial developers expected. The lag time between the basic research on \nGC and its widespread popularity is a standard example of computer\u00adscience research taking a nearly unbearable \namount of time to pay off, but still being well worth it in the end. While it is possible the excitement \naround TM will help expedite the process of widespread adoption and in general tech\u00adnology adoption \nis accelerating I think we should be pre\u00adpared for the TM lag time to be longer than anyone expects. \nThis in no way reduces the importance of TM research. Claim #3: Mandatory GC is usually suf.cient despite \nits approximations. As already described, GC essentially relies on the ap\u00adproximation that reachable \nobjects may be live, and this ap\u00adproximation can make an arbitrary amount of memory live arbitrarily \nlonger. For programmers to avoid suffering from this, unsafe languages can provide a back-door for explicit \nmemory deallocation and safe languages can provide fea\u00adtures like weak pointers. In practice, these features \nare some\u00adtimes necessary, but plenty of practical systems have been built that rely exclusively on reachability \nfor determining liveness. Moreover, the exact de.nition of what is reach\u00adable which in theory is necessary \nfor reasoning about program performance is typically left unspeci.ed and compiler optimizations are \nallowed to subtly change reacha\u00adbility information. I have argued the TM analogue of the reachability \nap\u00adproximation is the memory-con.ict approximation as\u00adsuming that two transactions accessing the same \nmemory (where at least one access is a write) cannot proceed in paral\u00adlel. The back-door for letting \nprogrammers avoid this ap\u00adproximation is open-nesting. The question then is whether open-nesting is so \nimportant that it must be addressed as a primary obstacle to developing transactional-memory imple\u00admentations. \nThe limitations of not having open-nesting and the situations where it is the best solution may be few, \njust as many programmers in garbage-collected languages never bother with weak pointers. Moreover, the \nexact de.nition of what is a memory con.ict as well as related issues of how con.icts are arbitrated \n(e.g., notions of fairness) may not prove important for most programs. 7. Conclusion A good analogy \ncan provoke thought, provide perspective, guide research, and promote an idea. An analogy need not be \nvalid science (i.e., a proof) nor a complete and total correspondence. Rather, it can serve to describe \nconcisely (if imperfectly) one idea in terms of another better-known idea. Humans often learn and understand \nvia analogies, so I believe a thought-provoking analogy can in and of itself serve as a contribution \ntoward our common research goals. My intent has been to present how one can view trans\u00adactional memory \nfrom the perspective of garbage collec\u00adtion, which though perhaps surprising at .rst springs from fundamental \nsimilarities between memory management and synchronization. In so doing, I have made a case for trans\u00adactional \nmemory that I personally .nd quite compelling, which is why I continue to do research on the topic. To \nrestate it succinctly, by moving mutual-exclusion protocols into the language implementation (any combination \nof com\u00adpiler, run-time system, and hardware), we make it easier to write and maintain shared-memory concurrent \nprograms in a more modular fashion. This argument is not the only one that has been put forth in favor \nof transactional memory; it is simply my personal opinion that it is the most important. Equally important, \nthis argument does not oversell trans\u00adactional memory: We still need better tools and method\u00adologies \nto help programmers determine where transactions should begin and end.9 Delimiting transactions is the \nessen\u00adtial dif.culty of concurrent programming, and making trans\u00adactions a language primitive does not \nchange this. For me, the most important conclusion arising from the analogy is that GC and TM rely on \nsimple and usually-good\u00adenough approximations (namely, reachability and memory\u00adcon.icts) that are subject \nto false-sharing problems. This fact can inform how we teach programmers to use TM (and GC) effectively \nand can guide research into reducing the approx\u00adimations. I can credit it with inspiring the not-accessed-in\u00adtransaction \nanalysis I developed recently [46] and I am hope\u00adful it and other points in the analogy can inspire more \nideas. Indeed, the primary intended effect of this presentation is to incite such thoughts in others, \nwhether readers agree or more interestingly disagree with the analogy. In particular: If you believe \nthe GC/TM analogy is useful, can you use it to advance our understanding of TM or GC? For exam\u00adple, is \nthere a TM analogue of generational collection? This question is crucial if one ascribes to the interpre\u00adtation \nof history in which GC was less practical prior to generational collection. More abstractly, is there \na uni.ed theory of TM as beautiful is Bacon et al s uni.ed the\u00adory of GC [5] in which tracing and automatic \nreference\u00adcounting are algorithmic duals?  If you believe the GC/TM analogy is .awed or deem\u00adphasizes \nsome crucial aspect of TM, can you identify why? I have essentially ignored issues of fairness and contention \nmanagement, which some may feel are essen\u00adtial aspects of TM. Does considering these issues funda\u00admentally \nchange what we should conclude?  Are my conjectures about TM s future based on GC s past that hardware \nsupport is unnecessary, that TM will take longer to reach the mainstream than we expect, and that open-nesting \nis not always necessary likely? Are they preventable? Should we try to prevent them? If you are in a \nposition to direct TM s future, can you seek guidance from the history of GC?  9 This conclusion is \nnot to discount classic and current work in this area, but such a discussion is beyond the present scope. \n  Acknowledgments Conversations with Emery Berger, Jim Larus, Jan Vitek, members of the WASP group at \nthe University of Washing\u00adton, and many others led to some of the ideas presented here. The author s \nresearch on transactional memory has been generously supported by Intel Corporation, Microsoft Cor\u00adporation, \nand the University of Washington Royalty Re\u00adsearch Fund. References [1] M. Abadi, C. Flanagan, and S. \nN. Freund. Types for safe locking: Static race detection for Java. ACM Transactions on Programming Languages \nand Systems, 28(2), 2006. [2] A.-R. Adl-Tabatabai, B. Lewis, V. Menon, B. R. Murphy, B. Saha, and T. \nShpeisman. Compiler and runtime support for ef.cient software transactional memory. In ACM Conference \non Programming Language Design and Implementation, 2006. [3] E. Allen, D. Chase, J. Hallet, V. Luchangco, \nJ.-W. Maessen, S. Ryu, G. L. Steele Jr., and S. Tobin-Hochstadt. The Fortress language speci.cation, \nversion 1.0\u00df, Mar. 2007. http://research.sun.com/projects/plrg/ Publications/fortress1.0beta.pdf. [4] \nC. S. Ananian, K. Asanovic, B. C. Kuszmaul, C. E. Leiserson, and S. Lie. Unbounded transactional memory. \nIn 11th International Symposium on High-Performance Computer Architecture, 2005. [5] D. F. Bacon, P. \nCheng, and V. T. Rajan. A uni.ed theory of garbage collection. In ACM Conference on Object-Oriented Programming, \nSystems, Languages, and Applications, 2004. [6] G. Bellella, editor. The Real-Time Speci.cation for Java. \nAddison-Wesley, 2000. [7] S. M. Blackburn, P. Cheng, and K. S. McKinley. Myths and realities: The performance \nimpact of garbage collection. In SIGMETRICS -Proceedings of the International Conference on Measurements \nand Modeling of Computer Systems, 2004. [8] C. Blundell, E. C. Lewis, and M. Martin. Subtleties of transactional \nmemory atomicity semantics. Computer Architecture Letters, 5(2), 2006. [9] B. D. Carlstrom, J. Chung, \nA. McDonald, H. Cha., C. Kozyrakis, and K. Olukotun. The Atomos transactional programming language. In \nACM Conference on Program\u00adming Language Design and Implementation, 2006. [10] P. Charles, C. Grothoff, \nV. Saraswat, C. Donawa, A. Kielstra, K. Ebcioglu, C. von Praun, and V. Sarkar. X10: An object\u00adoriented \napproach to non-uniform cluster computing. In ACM Conference on Object-Oriented Programming, Systems, \nLanguages, and Applications, 2005. [11] Cray Inc. Chapel speci.cation 0.4. http://chapel.cs.washington.edu/speci.cation.pdf. \n[12] P. Damron, A. Fedorova, Y. Lev, V. Luchangco, M. Moir, and D. Nussbaum. Hybrid transactional memory. \nIn International Conference on Architectural Support for Programming Languages and Operating Systems, \n2006. [13] A. Diwan, D. Tarditi, and J. E. B. Moss. Memory system performance of programs with intensive \nheap allocation. ACM Transactions on Computer Systems, 13(3), 1995. [14] R. Ennals. Software transactional \nmemory should not be lock free. Technical Report IRC-TR-06-052, In\u00adtel Research Cambridge, 2006. http://berkeley.intel\u00adresearch.net/rennals/pubs/052RobEnnals.pdf. \n[15] C. Flanagan and S. Qadeer. A type and effect system for atomicity. In ACM Conference on Programming \nLanguage Design and Implementation, 2003. [16] D. Gay and A. Aiken. Language support for regions. In \nACM Conference on Programming Language Design and Implementation, 2001. [17] D. Grossman. Safe Programming \nat the C Level of Abstraction. PhD thesis, Cornell University, 2003. [18] D. Grossman. Type-safe multithreading \nin Cyclone. In ACM Workshop on Types in Language Design and Implementation, 2003. [19] D. Grossman, J. \nManson, and W. Pugh. What do high-level memory models mean for transactions? In ACM SIGPLAN Workshop \non Memory Systems Performance &#38; Correctness, 2006. [20] D. Grossman, G. Morrisett, T. Jim, M. Hicks, \nY. Wang, and J. Cheney. Region-based memory management in Cyclone. In ACM Conference on Programming Language \nDesign and Implementation, 2002. [21] N. Haines, D. Kindred, J. G. Morrisett, S. M. Nettles, and J. M. \nWing. Composing .rst-class transactions. ACM Transactions on Programming Languages and Systems, 16(6), \n1994. [22] N. Hallenberg, M. Elsman, and M. Tofte. Combining region inference and garbage collection. \nIn ACM Conference on Programming Language Design and Implementation, 2002. [23] L. Hammond, B. D. Carlstrom, \nV. Wong, B. Hertzberg, M. Chen, C. Kozyrakis, and K. Olukotun. Programming with transactional coherence \nand consistency (TCC). In International Conference on Architectural Support for Programming Languages \nand Operating Systems, 2004. [24] T. Harris and K. Fraser. Language support for lightweight transactions. \nIn ACM Conference on Object-Oriented Programming, Systems, Languages, and Applications, 2003. [25] T. \nHarris, S. Marlow, S. P. Jones, and M. Herlihy. Composable memory transactions. In ACM Symposium on Principles \nand Practice of Parallel Programming, 2005. [26] T. Harris, S. Marlow, and S. Peyton Jones. Haskell on \na shared-memory multiprocessor. In Proceedings of the 2005 ACM SIGPLAN Workshop on Haskell, 2005. [27] \nT. Harris, M. Plesko, A. Shinnar, and D. Tarditi. Optimizing memory transactions. In ACM Conference on \nProgramming Language Design and Implementation, 2006. [28] M. Herlihy, V. Luchangco, and M. Moir. A .exible \nframework for implementing software transactional memory. In ACM Conference on Object-Oriented Programming, \nSystems, Languages, and Applications, 2006. [29] M. Herlihy, V. Luchangco, M. Moir, and I. William N. \nScherer. Software transactional memory for dynamic\u00adsized data structures. In ACM Symposium on Principles \nof Distributed Computing, 2003. [30] M. Herlihy and J. E. B. Moss. Transactional memory: architectural \nsupport for lock-free data structures. In International Symposium on Computer Architecture, 1993. [31] \nM. Hertz and E. D. Berger. Quantifying the performance of garbage collection vs. explicit memory management. \nIn ACM Conference on Object-Oriented Programming, Systems, Languages, and Applications, 2005. [32] B. \nHindman and D. Grossman. Atomicity via source-to\u00adsource translation. In ACM SIGPLAN Workshop on Memory \nSystems Performance &#38; Correctness, 2006. [33] R. E. Jones. Garbage Collection: Algorithms for Automatic \nDynamic Memory Management. Wiley, 1996. [34] S. Kumar, M. Chu, C. J. Hughes, P. Kundu, and A. Nguyen. \nHybrid transactional memory. In ACM Symposium on Principles and Practice of Parallel Programming, 2006. \n[35] J. R. Larus and R. Rajwar. Transactional Memory. Morgan &#38; Claypool Publishers, 2006. [36] J. \nManson, J. Baker, A. Cunei, S. Jagannathan, M. Prochazka, B. Xin, and J. Vitek. Preemptible atomic regions \nfor real-time Java. In 26th IEEE Real-Time Systems Symposium, 2005. [37] V. J. Marathe, W. N. Scherer, \nand M. L. Scott. Adaptive software transactional memory. In International Symposium on Distributed Computing, \n2005. [38] A. McDonald, J. Chung, B. D. Carlstrom, C. Cao Minh, H. Cha., C. Kozyrakis, and K. Olukotun. \nArchitectural se\u00admantics for practical transactional memory. In International Symposium on Computer Architecture, \n2006. [39] M. M. Michael and M. L. Scott. Simple, fast, and practical non-blocking and blocking concurrent \nqueue algorithms. In ACM Symposium on Principles of Distributed Computing, 1996. [40] K. E. Moore, J. \nBobba, M. J. Moravan, M. D. Hill, and D. A. Wood. LogTM: Log-based transactional memory. In 12th International \nSymposium on High-Performance Computer Architecture, 2006. [41] M. J. Moravan, J. Bobba, K. E. Moore, \nL. Yen, M. D. Hill, B. Liblit, M. M. Swift, and D. A. Wood. Supporting nested transactional memory in \nLogTM. In 12th International Conference on Architectural Support for Programming Languages and Operating \nSystems, 2006. [42] R. Rajwar, M. Herlihy, and K. Lai. Virtualizing transactional memory. In 32nd International \nSymposium on Computer Architecture, 2005. [43] J. H. Reppy. Concurrent Programming in ML. Cambridge University \nPress, 1999. [44] M. F. Ringenburg and D. Grossman. AtomCaml: First\u00adclass atomicity via rollback. In \n10th ACM International Conference on Functional Programming, 2005. [45] N. Shavit and D. Touitou. Software \ntransactional memory. Distributed Computing, Special Issue(10), 1997. [46] T. Shpeisman, V. Menon, A.-R. \nAdl-Tabatabai, S. Balensiefer, D. Grossman, R. Hudson, K. Moore, and B. Saha. Enforcing isolation and \nordering in STM. In ACM Conference on Programming Language Design and Implementation, 2007. [47] M. Tofte \nand J.-P. Talpin. Region-based memory manage\u00adment. Information and Computation, 132(2), 1997. [48] P. \nR. Wilson. Uniprocessor garbage collection techniques. Technical report, University of Texas, 1994. [49] \nK. Zee and M. Rinard. Write barrier removal by static anal\u00adysis. In ACM Conference on Object-Oriented \nProgramming, Systems, Languages, and Applications, 2002.  \n\t\t\t", "proc_id": "1297027", "abstract": "<p>This essay presents remarkable similarities between transactional memory and garbage collection. The connections are fascinating in their own right, and they let us better understand one technology by thinking about the corresponding issues for the other.</p>", "authors": [{"name": "Dan Grossman", "author_profile_id": "81405594870", "affiliation": "University of Washington, Seattle, WA", "person_id": "PP39095351", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1297027.1297080", "year": "2007", "article_id": "1297080", "conference": "OOPSLA", "title": "The transactional memory / garbage collection analogy", "url": "http://dl.acm.org/citation.cfm?id=1297080"}