{"article_publication_date": "10-21-2007", "fulltext": "\n MicroPhase: An Approach to Proactively Invoking Garbage Collection for Improved Performance Feng Xian, \nWitawas Srisa-an, and Hong Jiang Department of Computer Science &#38; Engineering University of Nebraska-Lincoln \nLincoln, NE 68588-0115 {fxian,witty,jiang}@cse.unl.edu Abstract To date, the most commonly used criterion \nfor invoking garbage collection (GC) is based on heap usage; that is, garbage collection is invoked when \nthe heap or an area in\u00adside the heap is full. This approach can suffer from two per\u00adformance shortcomings: \nuntimely garbage collection invo\u00adcations and large volumes of surviving objects. In this work, we explore \na new GC triggering approach called MicroPhase that exploits two observations: (i) allocation requests \noccur in phases and (ii) phase boundaries coincide with times when most objects also die. Thus, proactively \ninvoking garbage collection at these phase boundaries can yield high ef.\u00adciency. We extended the HotSpot \nvirtual machine from Sun Microsystems to support MicroPhase and conducted exper\u00adiments using 20 benchmarks. \nThe experimental results indi\u00adcate that our technique can reduce the GC times in 19 ap\u00adplications. The \ndifferences in GC overhead range from an increase of 1% to a decrease of 26% when the heap is set to \ntwice the maximum live-size. As a result, MicroPhase can improve the overall performance of 13 benchmarks. \nThe per\u00adformance differences range from a degradation of 2.5% to an improvement of 14%. Categories and \nSubject Descriptors D.3.4 [Programming Language]: Processors Memory management (garbage collection) General \nTerms Experimentation, Languages, Performance 1. Introduction Garbage collection (GC) is a process that \nautomatically reclaims dynamically allocated memory. The bene.ts of Permission to make digital or hard \ncopies of all or part of this work for personal or classroom use is granted without fee provided that \ncopies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. OOPSLA 07, October 21 25, 2007, Montr\u00b4eal, \nQu\u00b4ebec, Canada. Copyright c . 2007 ACM 978-1-59593-786-5/07/0010. . . $5.00 garbage collection include \nelimination of memory leaks and dangling pointers. It also promotes cleaner code as memory management \nconcerns are not interleaved with execution logics. Categorically, there are two approaches to garbage \ncollection: reference counting and tracing. The latter is the main focus of this paper. Tracing garbage \ncollectors (e.g. copying generational, mark-sweep, mark-compact) often require two phases: (i) identi.cation \nof reachable objects, and (ii) reclamation of dead objects en masse. Many implementations of these col\u00adlectors \nare stop-the-world, which means that all execution threads (except for the garbage collection thread) \nmust be stopped during garbage collection. To date, the most com\u00admonly used GC invocation criterion is \nspace-based (we refer to a collector using the space-based criteria as the space\u00adbased approach); that \nis, when the volume of objects in a heap reaches a certain prede.ned threshold, garbage col\u00adlection is \ninvoked. It has been well documented that this invocation criterion leads to the following shortcomings: \n1. Untimely invocation Garbage collection is usually in\u00advoked when applications need to create a large \nnumber of objects. In such a scenario, garbage collection often prevents an application from allocating \nobjects freely at its most critical times [18]. Such untimely invocations can greatly affect the usability \nof mission critical appli\u00adcations. 2. Large volume of surviving objects A consequence of untimely GC \ninvocations is that a large number of ob\u00adjects created just prior to a GC invocation will not have died \n(we refer to them as surviving new objects or SNOs); thus, spending any effort collecting SNOs is wasteful. \nIn copying generational collectors, the promotion of SNOs reduces the minor collection ef.ciency and \nresults in a higher frequency of full collection invocations. Two ap\u00adproaches have been introduced to \novercome this short\u00adcoming by concentrating the collection effort on older objects and avoiding collecting \nnewly created objects [3, 30]. However, these approaches do not address the  fundamental issue of untimely \ninvocations, which is the root cause of this shortcoming. This work: We introduce MicroPhase, a generalized \nGC triggering mechanism that considers ef.ciency instead of heap usage as an invocation criterion. MicroPhase \nleverages two key observations: 1. Allocation requests and reductions of live objects often occur in \nphases [7, 10, 24]. 2. A portion of execution with light or no allocation activi\u00adties (which appears \nas an allocation pause) mostly coin\u00adcides with the time when most objects die [21]. We illus\u00adtrate this \nobservation in Section 2.  We modi.ed the generational garbage collector in the HotSpot virtual machine \nto exploit these two observations by monitoring the object allocation behavior and invoking the corresponding \ngarbage collector (minor or full) when an allocation pause is detected. In doing so, MicroPhase avoids \ninvoking GC during intense allocation phases, resulting in a major decrease in the volume of SNOs. We \nthen compared the performance of MicroPhase against that of the space-based approach. Our experiment \nis con\u00adducted using twenty benchmarks in a setting where the heap size is set to be twice the maximum \nvolume of live objects or live-size. This setting allows most applications to have rea\u00adsonable GC overheads. \nHowever, three benchmarks indicate that such a setting makes the heap extremely tight during their executions \n(i.e. the GC overhead is more than 50% of execution time). The experimental results show that MicroPhase \nis more effective than the space-based approach, and thereby, can reduce the garbage collection times \nof 19 benchmarks. The differences in GC times range from an increase of 1% to a decrease of 26%. A study \nof minimum mutator utilization (MMU) also shows that MicroPhase is less disruptive to mu\u00adtator execution \nin 13 benchmarks when the window sizes are small. As a result, MicroPhase can improve the overall per\u00adformance \nof 13 benchmarks. The overall performance differ\u00adences range from a degradation of 2.5% to an improvement \nof 14% under a tight heap environment. Outline: The remainder of this paper is organized as fol\u00adlows. \nSection 2 describes the current issues with space-based triggering and suggests a possible solution based \non two val\u00adidated hypotheses. Section 3 provides an overview of Mi\u00adcroPhase and the implementation details. \nSection 4 and 5 outline the experimental methodology, evaluate the overhead and accuracy of MicroPhase, \nand report the experimental results comparing MicroPhase with the HotSpot collector. Section 6 discusses \nthe implications of our work and future challenges. Section 7 summarizes existing efforts in the .eld \nrelated to this work. Section 8 concludes this paper.  2. Motivation The motivation of our work is to \nexplore an alternative ap\u00adproach to invoking garbage collection that does not solely rely on space usage, \nbut instead relies on garbage collection ef.ciency. We conjecture that such an alternative exists due \nto the following reported observations and hypotheses: 1. A study by Dieckmann and H\u00a8olzle clearly shows \nthat in many applications, the volume of live objects or live-size increases and decreases in phases \n[10]. Similar behavior is displayed in the work by Shaham et al. [24]. In ad\u00addition, work by Wilson and \nMoher [33, 34] shows that GC can be opportunistically invoked during and right af\u00adter long compute-bound \nphases, when most objects die. These .ndings imply that it is possible to ef.ciently col\u00adlect objects \nif each GC invocation occurs at a location with a small volume of live objects. 2. Stefanovi\u00b4c et al. \nobserve that GC is often invoked after a new phase has begun; thus, many objects created in the new phase \ndo not have time to die [30]. This observation is con.rmed by Chen et al. when they report that the space-based \napproach often invokes GC after the phase boundary especially if the new phase starts by allocating many \nnew objects [8].  These two observations lead us to the following hypotheses: 2.1 Hypothesis 1: Allocation \nPhases Do Exist An increase in the volume of live objects (as shown by [10]) indicates a period of intense \nallocation requests. A sudden decrease in the volume of live objects is likely an indication of periods \nwith much less allocation intensity. Such behavior corresponds to the notion of phase boundaries as reported \nby [8, 33]. At this time, we wish to introduce two important terms that will be used throughout the paper: \nexecution pause and allocation pause. De.nition: An execution pause is de.ned as a time period in which \na mutator thread is suspended. Thus, there are no object allocation activities. De.nition: An allocation \npause is de.ned as a suf.ciently long execution period with no allocation requests made by a mutator \nthread. For example, if a mutator thread is suspended due to an I/O request, the detected pause is an \nexecution pause, which has no effect on heap usage and liveness of objects because the mutator thread \nis not active. Therefore, the execution pause is unlikely to be an ef.cient GC invocation point. On the \nother hand, an allocation pause occurs when a thread makes two consecutive allocation requests (with \nno execu\u00adtion interruption in between), and the interval between these two requests is suf.ciently long. \nWithin this pause, previ\u00adously created objects are manipulated and should be dead by Table 1. Allocation \npauses occur in every application. However, the length and frequency of pauses can vary signi.cantly \namong applications. Benchmark Description Pause length (bytecodes) Number of pauses average min max \ncompress A utility to compress/uncompress large .les. 206513 20 8035180 20 db Performs DB functions on \nmemory resident database. 700 19 2192020 441 jack A Java parser generator with lexical analyzers. 154 \n20 2666 11796 javac JDK 1.0.2 Java compiler. 288 16 24310 12573 jess A Java expert shell system based \non NASA s CLIPS expert shell system. 204 19 7616 13101 mpegaudio A mpeg-3 audio stream decoder. 236 10 \n2666 32 mtrt A dual-threaded program that ray traces an image .le. 300 10 1068 13985 raytrace Works on \na scene depicting a dinasaur. 263 23 1056 302 antlr Parses several gramma .les and generates a parser \nand lexical analyzer for each. 1075 13 234122 141 bloat Performs a number of optimizations and analysis \non Java bytecode .les. 346 10 35765 8012 chart Plots a number of complex line graphs and renders them \nas PDF. 288 12 34326 7441 eclipse Executes some of non-GUI jdt performance tests for the Eclipse IDE. \n322 12 234122 34012 fop Formats an XSL-FO .le and generates a PDF .le. 652 10 12249 2470 hsqldb Executes \na number of transactions against a model of a banking application. 440 10 20077 565 jython Interprets \nthe pybench. 1145 10 719632 3934 luindex Uses lucene to index a set of documents. 898 13 108033 739 lusearch \nUses lucene to do a text search of keywords over a corpus of data. 521 11 4800 916 pmd Analyzes a set \nof Java classes for a range of source code problems. 5 10 240 2738 xalan Transforms XML documents into \nHTML. 12 4 90 405 jbb2000 A Java program emulating a 3-tier system with emphasis on the middle tier. \n616 0 12248 10012  jbb2000 antlr bloat chart eclipse fop hsqldb jython luindex lusearch pmd xalan DaCapo \nBenchmarks Figure 1. More characterization of allocation pauses in each benchmark the end of the pause. \nOne major challenge is to segregate the allocation pauses from the execution pauses. A detailed description \nof our approach to overcome this challenge is provided in Section 3. We then empirically characterized \nallocation pauses in all benchmarks. Figure 1 illustrates the boxplot of pause length characteristic \nof each benchmark. Each boxplot can be inter\u00adpreted as follows: the box contains the middle 50% of the \ndata from the 75th percentile of the data set (represented by the upper edge of the box) to the 25th \npercentile (represented by the lower edge); the line in the box represents the median value of the data \nset; the short dash lines at both ends of the vertical line indicate the 5th percentile and 95th percentile \nof the data set. Table 1 also reports the mean, maximum, min\u00adimum, and number of occurrences of allocation \npauses. The result shows that the intervals of the allocation pauses can vary from several bytecodes \nto a few million bytecodes. In addition, the frequency of these pauses signi.cantly varies among applications. \nNevertheless, the table and the .gure clearly show that allocation pauses do exist in all applica\u00adtions. \n 2.2 Hypothesis 2: Strong Relation between Allocation Pauses and Live-Size Reductions Our hypothesis \nis based on (i) an intuition that fewer allo\u00adcation requests are likely the main reason for the decreasing \nvolume of live objects, and (ii) the weak generational hy\u00adpothesis, which states that most objects die \nyoung [17, 32]. Thus, the majority of objects created during an intense allo\u00adcation phase should die \nsoon after the phase is over. We utilized the Merlin Algorithm [15] to obtain the live\u00adsize throughout \nthe execution of every benchmark applica\u00adtion. We then conducted experiments to obtain the heap us\u00adage \nof every application over time. We eliminated disrup\u00adtions due to GC by setting the heap size for each \napplica\u00adtion to be large enough to allow the application to execute without having to invoke GC. At the \nsame time, limiting the maximum heap size to be smaller than the main memory ca\u00adpacity eliminates paging. \nWe then superimposed the live-size and the heap usage information over the execution time. Figure 2 illustrates \nthe result of jack, a benchmark program in SPECjvm98. The .g\u00adure on the left indicates that there are \nsixteen major live\u00adsize reduction events (appearing as saw tooth). The .gure on the right shows a magni.ed \nversion of the .rst live\u00adsize reduction event. Note that the allocated-size increases throughout the \nexecution. However, there are also short al\u00adlocation pauses, which appear as short horizontal lines in \nthe allocated-size plot. An interesting observation from the .gure to the right is that the major live-size \nreduction occurs during the allo\u00adcation pause. In fact, each of the allocation pauses in jack coincides \nwith a live-size reduction event. Furthermore, our study indicates that over 90% of allocation pauses \ncoincide with live-size reduction events in 18 out of 20 benchmarks. The magnitude of each live-size \nreduction event also varies from a few bytes to several megabytes in some applications (as shown in Table \n2).   Benchmark Relation between Magnitude of a live-size allocation pauses and reduction in a pause \n(KB) live-size reductions (%) average min max    compress 91 31 7 db 94 11 0.176 jack 90 15 0.7 javac \n91 4702 16 jess 96 18 0.3 mpegaudio mtrt 81 92 31 8 7 0.048 51 15 raytrace 86 1 0.048 74 antlr 96 40 \n0.072 905 bloat 95 19 0.032 1865 chart 96 26 0.07 2012 eclipse 98 102 0.1 34021 fop 96 21 0.072 7391 \nhsqldb 95 65 0.072 4913 jython 97 16 0.072 396 luindex 96 24 0.072 548 lusearch 93 17 0.032 72 pmd 98 \n40 0.072 267 xalan 95 6 0.024 347 jbb2000 96 6 0.04 Table 2. Analysis results of correlation between \nallocation pauses and live-sizes. However, we also found that many allocation pauses do not coincide \nwith live-size reduction events. Upon further investigations, we discovered that there are many execution \nevents that cause allocation pauses. For example, the thin\u00adlock mechanism [2] used by HotSpot can cause \nan appli\u00adcation to spend some of its execution time in busy-waiting loops. Each time a thread enters \nsuch a loop, it appears as an allocation pause, but it is not likely to correspond to a live-size reduction. \nOther execution events that appear as al\u00adlocation pauses without live-size reductions include iteration \nof large arrays and initialization of large objects. Summary: We have shown that our hypotheses are valid \nin all benchmarks. In the next section, we leverage these insights to construct mechanisms to detect \nthese allocation pauses in order to invoke garbage collection in a timely manner.  3. Introducing MicroPhase \nWe implemented MicroPhase in HotSpot, the .agship Java Virtual Machine (JVM) from Sun Microsystems. HotSpot \nutilizes generational collector to manage the heap that is di\u00advided into three partitions: nursery, mature \nspace and per\u00admanent space. The nursery is further subdivided into eden, to and from spaces. The generational \ncollector in HotSpot works as follows. All objects are created in the nursery. When the nursery is full, \nminor collection is invoked to pro\u00admote surviving objects to the mature space. When the mature allocation \npause Figure 2. An illustration of a relation between pauses and live-size reductions in jack. The graph \non the right is the magni.ed version of the .rst live-size reduction event in the graph on the left. \nspace is full, full collection is invoked to perform whole heap collection using mark-compact algorithm \n[17, 31]. Typi\u00adcally, the nursery is con.gured to be much smaller than the mature space. The permanent \nspace is used to hold objects that live for the duration of the program; thereby, this area is not garbage \ncollected. We modi.ed HotSpot to include two additional features, allocation pause detection (incorporated \ninto the memory allocator) and dynamic triggering mechanism (incorporated into the garbage collector), \nwhich determines if minor or full collection should be invoked when an allocation pause is detected. \n 3.1 Allocation Pause Detection The basic notion of the proposed allocation pause detection is quite \nsimple: measure the interval between two consecu\u00adtive allocation requests by each thread, and if the \ninterval is suf.ciently long, consider it an allocation pause. While the notion is straightforward, the \nactual implementation and its application face two major challenges: 1. Detect allocation pauses accurately \nand inexpensively. Our technique must be able to segregate allocation pauses from execution pauses. In \naddition, each application may have a distinct allocation pause behavior. MicroPhase must be designed \nto work effectively in the presence of such variations. 2. Decide when allocation pauses should be used \nas a GC triggering criterion. Frequently occurring allocation pauses can cause GC to be invoked excessively. \nIn or\u00adder to allow .exible application of the technique, Mi\u00adcroPhase must support multiple modes of operation \nsuch as pure pause-based, pure space-based, and a combina\u00adtion of the two.  Ti-2 Ti-1 Ti Time line allocation \nINTERVAL allocation INTERVAL Figure 3. Detecting allocation pauses Allocation Pause Detection Algorithm. \nWe modi.ed the memory allocator in HotSpot to measure the number of CPU cycles needed to satisfy a given \nnumber allocation requests (INTERVAL). To distinguish between an allocation pause and an execution pause, \nwe created a cooperative pro.ling environment where the virtual machine, the operating sys\u00adtem, and the \nunderlying processors work together to allow detailed pro.ling of thread execution [14]. Our technique \nutilizes a per-thread timer, an extension to the Linux kernel, to record the execution cycles that can \nbe obtained through performance-monitoring instructions such as RDTSC in X86 based processors [25]. Each \ntime a thread is scheduled, the value of the cycle counter register (rdtsc register in X86 processors) \nis recorded as cyclestart in the thread structure (task struct in Linux). When the thread is suspended, \nthe new rdtsc value is read and stored in cycleend. The difference between cyclestart and cycleend is \naccumulated in cycletotal.The value of cycletotal is ac\u00adcessible via a system call [22]. The pro.ling \noverhead is considered as part of the runtime overhead of MicroPhase. (A study of the runtime overhead \nis presented in Section 4.1.) With this mechanism, the time that a thread spent on blocking is .ltered \nout. For example, when INTERVAL is set to 2 (the minimum value), each measurement of an allocation pause \nis taken be\u00adtween two allocation requests. When an allocation request is satis.ed, an associated time \nstamp (T ) based on per-thread\u00adtimer is recorded for the purpose of calculating the alloca\u00adtion pause. \nFigure 3 illustrates our allocation pause detec\u00ad Ti-Ti-1 tion mechanism. Basically, if >d, where d is \na Ti-1-Ti-2 prede.ned threshold indicating an unusually large ratio, we can assume that the period between \nTi and Ti-1 contains an allocation pause. We used this technique instead of al\u00adways calculating a pause \nbetween two allocation requests to reduce the overhead of pause calculation and detection. Used heap \n(1) heapUsage Heap capacity (2) if heapUsage > hThreshold then (9) endif  (3) if a pause is detected \nthen (4) Trigger GC (5) endif /* Heap has exhuasted but pause is not detected */ (6) if space_available \n< requested_size then (7) Trigger GC (8) endif Figure 4. A generalized algorithm to incorporate allocation \npauses as an invocation criterion If INTERVAL is too short (2 for example), the detection overhead will \nbe high. On the other hand, if INTERVAL is too long, the system may miss allocation pauses. Similarly, \nif d is set too large, smaller allocation pauses may not be detected. On the other hand, if d is set \ntoo small, execution events such as iteration of large arrays or short busy-wait loops can also be detected \nas the allocation pauses. Our goal is to identify INTERVAL and d that yield good detection ac\u00adcuracy \nand low detection overhead across all benchmarks. In Section 4, we report the results of our experiment \nto iden\u00adtify these two generalized values applicable to all bench\u00admarks. When should allocation pauses \nbe used as a criterion? Once these allocation pauses are detected, should they be used as the sole criterion \nor a supplemental criterion for trig\u00adgering GC? One option is to use these allocation pauses as the criterion \nin an uncontrolled or pure pause-based mode, meaning that GC is invoked whenever an allocation pause \nis detected. In this option, one possible shortcoming is ex\u00adcessive GC invocations if allocation pauses \nare detected fre\u00adquently. The second option is to use these allocation pauses as a criterion in a controlled \nmode; i.e. invoking GC at each detected allocation pause only after the heap usage has already exceeded \na prede.ned heap usage threshold or hT hreshold. Prior to this point, the pause detection mecha\u00adnism \nis not enabled. The second option is more advantageous because it strikes a balance between higher ef.ciency \nin reclaiming ob\u00adjects and lower frequency of invoking GC. In both cases, the space-based technique is \nused as a back-up triggering mech\u00adanism in a scenario where allocation pauses are not detected prior \nto heap exhaustion. Figure 4 outlines the generalized algorithm of MicroPhase. An empirical study of \nthese two modes is presented in the next section. 3.2 Triggering Mechanism Our earlier study (see Section \n2) has already shown that the volumes of live-size reduction can vary signi.cantly within an application. \nTherefore, when MicroPhase is used in a gen\u00aderational collector, one re.nement that must be made to the \ngeneralized algorithm (Figure 4, line 4) is predicting which area of the heap (nursery or mature) contains \na larger volume of dead objects. This prediction allows MicroPhase to invoke the corresponding collector \n(i.e. minor or full) when an al\u00adlocation pause is detected. As the .rst step toward building such a predictor, \nwe conducted experiments to characterize the live-size behaviors. Study of live-size behavior. For each \nlive-size reduction, we measured the volume of dead objects as well as the ratio between dead objects \nthat were short-lived and dead objects that were long-lived. We de.ned short-lived objects as objects \nwith lifetimes of less than 20% of the maximum live-size [4]. Figure 5 shows our analyses of jack and \njavac, the two benchmarks with distinctive live-size characteristics. Each dot in the graph represents \na live-size reduction. The x-axis represents the reduction volume, and the y-axis indicates the percentage \nof dead objects that are short-lived. The analysis of jack (Figure 5a) indicates that over 98% of the \nlive-size reductions are due to short-lived objects. This suggests that when a pause is detected, most \nof the dead objects should be in the nursery. On the contrary, the analysis of javac (Figure 5b) indicates \nthat both short-lived and long\u00adlived objects are responsible for the live-size reductions; therefore, \nit becomes less obvious whether minor or full GC should be invoked when a pause is detected. Figure 5 \nalso indicates that most of the largest live-size reductions are due to long-lived objects. For example, \njavac has four major live-size reductions. Over 99% of the dead objects in these four major reductions \nhave very long lifes\u00adpans. Similarly, 98% of the dead objects responsible for the seventeen major live-size \nreductions in jack are long-lived. In other words, these objects die in the mature space. The re\u00adsult \nof our analysis indicates that in most cases, minor collec\u00adtion should be invoked when a pause is detected. \nHowever,  Short-lived objects (%) 80 Each of the live-size reductions in this box is mainlydue to short-lived \nobjects (80% to 100% of objects are short-lived). 60 40 Example 20 There are 17 major live-size reductions \nin this box. Each reduction is mainly due to long-lived objects (fewer than 2% of objects are short-lived). \n0 1 10 100The magnitude of live-size reduction (KB) The magnitude of live-size redution (KB) (a) jack \n(b) javac Figure 5. Investigation of the types of objects responsible for live-size reductions. Each \ndot represents a livesize reduction event. The x-axis indicates the volume of the reduction in KB, and \nthe y-axis indicates the percentage of objects that are short\u00adlived in a reduction. For example, each \nreduction event occurring in the box labeled Example has the reduction volume of about 40KB, and 40% \nof objects that die in the reduction event are short-lived. higher ef.ciency can be obtained if we can \npredict instances when it is better to invoke full collection. When to invoke full collection? Once the \nnursery occu\u00adpancy reaches hT hreshold, our triggering algorithm in\u00advokes minor collection each time \nan allocation pause is en\u00adcountered. As part of each minor collection invocation, its collection ef.ciency \nis also calculated (the volume of col\u00adlected objects/the minor space usage prior to the collection). \nIf the ef.ciency is higher than a prede.ned minor collection threshold or thminor, nothing else is done. \nIf the ef.ciency is below thminor, full collection is invoked. Notice that in the subsequent experiments, \nwe set the value of thminor to be 80%. We selected this value because in all benchmarks, the minor collection \nef.ciencies seldom fall below 80%. While experimenting with this approach, we made two observations: \n1. A large portion of allocation pauses not associated with live-size reductions are originated from \na set of method call sites. 2. A large portion of allocation pauses with major live-size reductions \nare originated from another set of method call sites.  We exploited these two observations to further \noptimize our triggering mechanism by incorporating bad-site table and major-reduction table to memorize \nmethod call sites (a method call site is represented by the thread id, method id, and method call depth) \nthat usually create allocation pauses with poor live-size reductions and allocation pauses with large \nlive-size reductions, respectively. Figure 6 describes the optimized algorithm. Similar to the algorithm \ndescribed earlier, the optimized algorithm invokes minor collection upon detecting a pause and calculates \nthe collection ef.ciency. If the ef.ciency is less than thminor, full collection is invoked and, once \nagain, its ef.ciency is calculated. If this ef.ciency is below the full collection threshold or thfull, \nthe method-call site is recorded in the bad-site table (line 15 in Figure 6). If the ef.\u00adciency is higher \nthan thfull, the method-call site is recorded in the major-reduction table (line 13 in Figure 6). Later, \nif a site in the bad-site table initiates a pause, no garbage col\u00adlection is invoked (lines 2 and 3 in \nFigure 6). On the other hand, if a site in the major-reduction table initiates a pause, full collection \nis invoked directly (lines 4 and 5 in Figure 6). In the subsequent experiments, we set the value of thfull \nto be 60%. We selected this value because our preliminary study indicated that the ef.ciencies of mature \ncollection rarely fall below 60% in all benchmarks. We also set each of the hash tables (i.e. the bad-site \ntable and major-reduction table) to 20KB, which is suf.cient for our benchmarks. 4. Evaluation of MicroPhase \nIn this section, we describe the experimental environment used to evaluate the performance and accuracy \nof Mi\u00adcroPhase. Our benchmark collection comprised of SPECjvm98 benchmark suite [27], DaCapo benchmark \nsuite (the ver\u00adsion as of 10/2006) [5] with the default con.guration and SPECjbb2000 [26] with 8 warehouses. \nThese programs were selected for their ability to test the performance of garbage collectors by providing \ncomplex heap behaviors and wide ranging memory demands. Table 3 highlights the key char\u00adacteristics of \nour benchmarks. (1) S the current method call site (2) if S in Bad-Site Table then (3) Do not trigger \nGC at this pause (4) else if S in Major-Reduction Table then (5) Trigger full collection (6) else \n (7) Trigger minor collection (8) Eff1 Compute the efficiency of the minor collection (9)  if ( Eff1 \n< th minor) then (10) Trigger a full collection (11) Eff2 Compute the efficiency of the full collection \n (12) if ( Eff2 > ) then  th full (13) Insert S into Major-Reduction Table (14) else (15) Insert S \ninto Bad-Site Table (16) endif (17) endif (18) endif  Figure 6. Optimized GC triggering algorithm \nin Mi\u00adcroPhase We ran all of the benchmarks in a machine with two AMD Opteron processors and 16GB of \nphysical memory. In all experiments, we used the MicroPhase enabled Hotspot (shipped as part of JKD version \n1.4), and the young gen\u00aderation area was set to the default value used by Sun for the 64-bit Opteron, \nwhich is 1/3 of the entire heap. The heap size was set to be twice of the maximum live-size to allow \nreasonable invocation frequency while still yielding good garbage collection performance in most applications. \nIn case of multithreaded benchmarks (mtrt, eclipse, hsqldb, lusearch, xalan,and SPECjbb2000), we enabled \nthe Thread-Local Allocation Buffer (TLAB) feature in HotSpot to reduce the synchronization overhead during \nobject allocations. We executed our benchmarks in a standalone mode with all non\u00adessential daemons and \nservices shut down to minimize the number of other threads competing for the processor time. 4.1 Overhead \nand Accuracy of MicroPhase To evaluate the accuracy of MicroPhase, the heap was set to a very large size \nso that all applications can run without having to invoke GC. The only exception is javac,which forces \nGC four times during its execution. Because computational overhead of the pause detec\u00adtion algorithm \nis tightly related to the allocation pressure (i.e. more allocation requests mean higher overhead), we \nselected three benchmarks with three distinctive memory pressures: javac (light pressure), bloat (moderate \npressure) and SPECjbb2000 (intense pressure). Figure 7 represents the overhead of MicroPhase (measured \nin percentage of the overall execution time) of each of the three Java benchmarks over a wide range of \nsampling intervals. The graph shows an expected trend of decreasing computation overhead with each larger \nsampling interval (INTERVAL). Our experiment shows that the overhead ranges from 3% to 5% of the overall \nFigure 7. Effect of sampling interval on allocation pause detection overhead  Sampling Interval (INTERVAL) \n Sampling Interval (INTERVAL) Figure 8. Miss rate and misprediction rate of pause detec\u00adtion algorithm \nin javac execution time if pause detection is attempted at each allo\u00adcation request. The overhead can \nbe further reduced to less than 1% if INTERVAL is 16 allocations or larger. We then studied the accuracy \nof MicroPhase by mea\u00adsuring the miss rate (number of allocation pauses not de\u00adtected/total number of \nallocation pauses) and the mispredic\u00ad Benchmark Maximum live-size Allocated objects N umber GC execu \ntion overhead (%)  compress 6.5 110.46 0.01 1 3.7 db 8.5 78.04 3.21 1 2.7 jack 0.75 181.19 5.90 1 10 \njavac 8.09 212.99 6.31 1 30.1 jess 1.11 297.10 7.94 1 56.1 mpegaudio 0.7 1.16 0.02 1 0.5 mtrt 7.08 139.99 \n6.64 2 77 raytrace 4.5 159.92 6.37 1 5.1 antlr 1.1 278.02 5.24 1 5.4   bloat 3.51 1092.90 27.69 1 10.6 \nchart 12.5 787.38 27.19 1 8.4 eclipse 12.7 888.57 47.14 11 13.7 fop 7 54.27 1.22 1 4.6 hsqldb 67.87 134.35 \n4.43 85 57.1 jython 2.75 1216.95 19.73 1 18.2 luindex 1.4 398.79 11.34 1 11.2 lusearch 12.1 2102.24 16.40 \n30 21.1 pmd 15.99 810.53 33.88 1 23.9 xalan 20.11 1018.95 14.65 7 24.7     jbb2000 (8 whs.) 231.08 \n2900.11 91.21 8 19 Table 3. Basic characteristic of SPECjvm98, SPECjbb2000, and DaCapo benchmark suites. \nFor SPECjbb2000, the workload is set to 8 warehouses. For DaCapo, the default workload is used. tion \nrate (number of detected allocation pauses with no live\u00adsize reductions/number of detected allocation \npauses). For the sake of brevity, we only depict the result from javac. We chose javac because our previous \nstudy indicated that (i) the lengths of the allocation pauses are wide ranging, (ii) there are over \n12000 instances of allocation pauses (see Table 1), and (iii) there are a large number of allocation \npauses with no live-size reductions. Figure 8 shows the miss rate (missRate) and misprediction rate (misspredictRate) \nover wide ranging values of INTERVAL and d. Based on Fig\u00adure 8, we made two observations:  1. Larger \nd results in a smaller misprediction rate but a larger miss rate. The contrary occurs when d is small. \nBecause higher misprediction rates can cause more un\u00adtimely GC invocations, the misprediction rate is \nconsid\u00adered more critical than the miss rate. 2. Larger INTERVAL results in smaller misprediction rate \nbut larger miss rate. The contrary occurs when INTER-VAL is small.  From the observations, the ideal \ncon.guration of INTER-VAL and d should yield small miss rate and accurate predic\u00adtion across all applications. \nWe used the following scoring formula to rank each con.guration of C(INTERVAL, d): Score(C)= w\u00d7mispredictRate(C)+(1-w)\u00d7missRate(C) \nThe parameter w is the weight of the misprediction rate, which is considered more critical than the miss \nrate. We conducted an experiment to identify a good value for w and found that 0.9 works well in all \napplications. The result shows that when INTERVAL is equal to 16 and d is equal to 50, we can achieve \nthe misprediction rates of about 3% and the miss rates of less than 10% in most applications. 4.2 Uncontrolled \nversus Controlled Modes In MicroPhase, when hT hreshold is set to zero, the col\u00adlector operates in the \nuncontrolled mode (i.e. MicroPhase triggers GC each time a pause is detected). On the other hand, when \nhT hreshold is set to 100%, the traditional space-based criterion is used to trigger GC. Any other val\u00adues \nof hT hreshold between these two extremes set the collector to operate in the controlled mode (i.e. pauses \nare used to trigger GC after the heap usage has surpassed hT hreshold). We conducted an experiment to \nidentify the optimal hT hreshold for every application. Our experi\u00adment consisted of running every benchmark \nwith a set of hT hreshold values ranging from 0 to 100 in multiples of 10. Every con.guration of each \nbenchmark was executed three times, and the value of hT hreshold that yielded the shortest GC time was \nconsidered the optimal value for that benchmark. The result of our experiment is reported in Table 4. \nOur result indicates that the controlled mode is more ef\u00adfective than both the uncontrolled mode and \nthe space-based mode as there are no applications that yield the shortest GC time when hT hreshold is \nequal to 0% or when hT hreshold is equal to 100%. In subsequent experiments, we will com\u00adpare the GC \nperformances of MicroPhase with the optimal hT hreshold and .xed hT hreshold con.gurations (we re\u00adTable \n4. Identifying the optimal heap usage threshold (hT hreshold) for each application Benchmark Optimal \nhT hreshold (%) GC time reduction (%) compress 60 2.73 db 70 3.83 jack 80 1.96 javac 80 9.94 jess 70 \n26.47 mpegaudio 70 1.84 mtrt 80 16.99 raytrace 60 15.36 antlr 60 5.12 bloat 80 1.95 chart 60 5.12 eclipse \n70 11.64 fop 70 0.11 hsqldb 90 10.94 jython 80 3.17 luindex 70 21.05 lusearch 80 2.07 pmd 60 8.29 xalan \n70 -0.59 jbb2000 (8 whs.) 70 11.79 fer to the latter con.guration as .xed-value) against that of the \ndefault space-based collector in HotSpot. 5. Performance Comparisons So far, we have identi.ed the optimal \nvalue of hT hreshold for each application. However, obtaining this value in real\u00adworld settings may require \nconsiderable amount of tuning effort, and thus, may not always be practical. Therefore, we introduced \na .xed-value con.guration as another sys\u00adtem under investigation. The .xed-value system uses the same \nhT hreshold across all applications. By examining the optimal values of hThreshold, we notice that the \nmajority of them are 70% and 80%. In the .xed-value con.gura\u00adtion, hT hreshold is set to 70%, which represents \nthe av\u00aderage of all the optimal values. Notice that there are eight applications db, jess, mpegaudio, \neclipse, fop, luindex, xalan, and SPECjbb2000 that also have 70% as the op\u00adtimal hThreshold. We then \nconducted experiments to study the perfor\u00admances of MicroPhase with the optimal con.guration, Mi\u00adcroPhase \nwith the .xed-value con.guration, and the space\u00adbased triggering approach. For every technique, we ran \neach benchmark three times, and the one with the short\u00adest garbage collection time was used. We then \ncompared the results with respect to three performance metrics: over\u00adall garbage collection time, minimum \nmutator utilization (MMU),and overall performance. Note that for the overall performance, we measured \nexecution time in all benchmarks except SPECjbb2000 in which we measured the throughput. Before we discuss \nthe results of our experiments, we wish to report the basic GC behaviors when these three triggering \ntechniques are applied. 5.1 Overall Garbage Collection Time Table 5 quanti.es the reduction in the number \nof GC in\u00advocations and the times spent in minor and full collection when both con.gurations of MicroPhase \nare used instead of the space-based technique. Moreover, the table also shows small differences in GC \nperformances between the .xed\u00advalue con.guration and the optimal con.guration. Figure 9 highlights the \ndifferences in GC performances between the two con.gurations of MicroPhase. Notice that the .xed\u00advalue \ncon.guration shows no more than 5% increase in the overall garbage collection time when compared to the \nopti\u00admal con.guration. In jess, MicroPhase invokes eight times more minor col\u00adlection; however it also \ninvokes 239 times less full collec\u00adtion. Such a magnitude of reduction is achieved because Mi\u00adcroPhase \npromotes fewer objects to the mature generation. As a result, our technique can reduce the total time \nthat jess spent in garbage collection by 26%. Similar results are seen in javac, mtrt, chart, luindex,and \npmd where MicroPhase invokes many times more minor collection, but reduces the number of full collection \ninvocations and the time spent in performing full collection (see Table 5 and Figure 10). In compress, \njack, mpegaudio, SPECjbb2000, bloat, hsqldb, jython,and lusearch, MicroPhase can reduce the minor col\u00adlection \ntimes, full collection times, and overall GC times. In db and raytrace, full collection is rarely invoked. \nThus, the reductions in minor collection times are the factor that re\u00adduces the overall GC time. Xalan \nis the only application that does not bene.t from using MicroPhase. This is because xalan has the smallest \noccurrences of allocation pauses. Moreover, each pause only accounts for a relatively small live-size \nreduction. Bloat also makes a very interesting case study. All three techniques report exactly the same \nnumbers of minor collection (1390) and full collection (1) invocations in bloat.We also .nd that MicroPhase \n(both con.gurations) yields slightly shorter times in minor, full, and overall collection. Shorter minor \ncollection time is achieved because MicroPhase promotes fewer objects. Shorter mature collection time \nis obtained because MicroPhase invokes the only full collection at a time that yields higher ef.ciency \nthan that of the space-based approach. Hsqldb is another interesting benchmark. Because it uses memory \nresident databases, a large number of objects are kept alive for most of the execution. Thus the maximum \nlive-size is nearly 50% of the total allocated objects. The optimal performance is also achieved when \nhT hreshold is set to 90%, meaning that MicroPhase is not considered until the heap is nearly full. Still, \nthe utilization of MicroPhase results in two more minor collection invocations, but with Figure 9. Normalized \nGC time reductions of MicroPhase with .xed hT hreshold (70%) over MicroPhase with optimal hT hreshold \n  Benchmark Space-based scheme MicroPhase (optimized) MicroPhase (hThreshold =70%) Minor GCs Full GCs \nMinor GCs Full GCs Minor GCs Full GCs Calls Seconds) Calls Seconds Calls Seconds Calls Seconds Calls \nSeconds Calls Seconds compress 44 0.02 21 0.17 40 0.02 21 0.17 44 0.02 21 0.17 db* 117 0.15 4 0.16 126 \n0.14 4 0.16 126 0.14 4 0.16 jack 345 0.24 20 0.27 347 0.23 19 0.27 345 0.24 20 0.27 javac 323 1.07 33 \n2.55 333 1.17 30 2.09 391 1.16 28 2.12 jess* 566 0.03 551 9.83 574 0.04 212 7.21 574 0.04 212 7.21 mpegaudio* \n1 0.004 2 0.02 1 0.003 2 0.02 1 0.003 2 0.02 mtrt 134 0.15 251 15.8 132 0.16 261 13.08 141 0.16 242 13.16 \nraytrace 304 0.20 2 0.020 304 0.17 2 0.021 307 0.17 2 0.022 antlr 558 0.67 8 0.21 570 0.63 8 0.2 558 \n0.67 8 0.2 bloat 1390 2 1 0.05 1390 1.97 1 0.04 1391 1.98 1 0.05 chart 445 2.09 8 1.1 428 2.11 6 0.75 \n448 2.13 6 0.76 eclipse* 2760 10.71 3 0.58 2773 8.88 7 1.09 2773 8.88 7 1.09 fop* 55 0.43 0 0 55 0.43 \n0 0 55 0.43 0 0 hsqldb 12 1.02 5 2.91 14 1 5 2.5 14 1 5 2.51 jython 1858 8.7 2 0.13 1860 8.42 2 0.13 \n1887 8.65 2 0.13 luindex* 765 0.67 260 5.22 842 0.74 179 3.91 842 0.74 179 3.91 lusearch 4012 4.61 7 \n0.22 4026 4.54 7 0.19 4059 4.58 7 0.2 pmd 377 2.54 9 1.32 382 2.56 9 0.98 384 2.59 9 0.99 xalan* 601 \n2.76 33 2.3 651 2.82 33 2.27 651 2.82 33 2.27   jbb2000*(8 whs) 4798 140.77 284 96.03 4771 128.4 280 \n80.47 4771 128.4 280 80.47 Table 5. Comparing GC behaviors when MicroPhase (optimal and .xed-ratio con.gurations) \nand space-based triggering are used (* indicates application that have the same optimal hT hreshold as \nthe .xed-ratio hT hreshold) slightly less time spent in each invocation when compared to that of the \nspace-based approach. This is another indication that MicroPhase improves the ef.ciency of minor collection. \nAs a result, a small GC time reduction is achieved with MicroPhase using the optimal hT hreshold value. \nIn the .xed-value approach, the earlier application of MicroPhase (at 70% instead of 90%) also triggers \nthe same numbers of minor collection and full collection invocations as the optimal con.guration. However, \nslightly more time is spent in full collection than that of the optimal con.guration. Raytrace and eclipse \nare the only two benchmarks that spend more time performing full collection when MicroPhase is used. \nWith the space-based approach, these two applica\u00adtions invoke full collection only a few times: two and \nthree invocations for raytrace and eclipse, respectively. With Mi\u00adcroPhase, raytrace still invokes the \nfull collection twice, but at different points in execution than the invocation points in the space-based \ntechnique. Unlike hsqldb, these two points are not as ef.cient as the invocation points in the space-based \ntechnique. This difference contributes to slightly longer time 40 30 20 (%) 10  0 -10  -20 Figure \n10. Percentage of GC time reductions when .xed-value con.guration of MicroPhase is used. spent in full \ncollection. For eclipse, there are several pauses that yield less than 80% minor collection ef.ciencies. \nWhen this occurs, MicroPhase invokes the full collection. As a re\u00adsult, MicroPhase invokes the full collection \nfour more times than the space-based approach. MicroPhase also performs well in SPECjbb2000, an ap\u00adplication \nthat allocates nearly 3 GB of space. Even though the differences in the number of minor and full collection \ninvocations between MicroPhase and the space-based ap\u00adproach are very small, MicroPhase can reduce the \naverage times spent in each of the minor and full collection invoca\u00adtions. As a result, MicroPhase can \nachieve a 12% reduction in the overall garbage collection time. 5.2 Garbage Collection Pause We used \nMMU (minimum mutator utilization) [9] to mea\u00adsure the pause time and mutator utilization. Mutator utiliza\u00adtion \nis the fraction of the time that an application (or mu\u00adtator) executes within a given window. For example, \ngiven an execution window of 10 ms, within that time the collec\u00adtor runs for 4 ms, and the mutator runs \nfor 6 ms. Thus, the mutator utilization is 60%. The minimum mutator utilization (MMU) is the minimum \nutilization across all execution win\u00addows of the same size. For example, an MMU of 40% at 10 ms means \nthat the application will at least execute 4 ms out of every 10 ms. Figure 11 and Figure 12 depict the \nMMU of every benchmark. The x-intercept indicates the maximum pause time, and the asymptotic y-value \nindicates the mutator utilization. In luindex, the utilization of MicroPhase is better than the space-based \nscheme throughout all window sizes. As shown in Figure 12(q), the space-based scheme has the largest \nx\u00adintercept value of around 0.03s, and its mutator utilization is about 88%. The x-intercept of MicroPhase \nis 33% smaller (at about 0.02s) due to fewer object promotions, resulting in the mutator utilization \n(asymptotic y-value) of 92%. The main reason for higher MMU is due to the MicroPhase abil\u00adity to reduce \nthe minor, full, and overall GC times. Mi\u00adcroPhase also reduces minimum pause times and slightly improves \nthe overall MMU in jess, mtrt,and hsqldb. For SPECjbb2000, the mutator utilization of MicroPhase is better \nthan space-based scheme when the window size is smaller than 16 seconds (see Figure 11(b)). After that, \nthe utilizations of the two schemes are nearly the same. In db, jack, javac, bloat, chart, eclipse, jython, \nlusearch,and pmd, we also observed that MicroPhase could improve the MMUs when the window sizes are small. \nFor xalan, the MMU of MicroPhase is nearly the same as the MMU of the space-based scheme. This is because \nboth triggering mechanisms yield nearly the same number of minor and full collection invocations. Moreover, \nthe times spent in minor collection are also nearly the same. As a result, very little differences are \nobserved. The other four benchmarks that yield similar MMU results are compress, mpegaudio, raytrace,and \nfop. For antlr, both con.gurations of MicroPhase have worse utilization than the space-based technique. \nFurther investi\u00adgation reveals that antlr has very bursty allocation requests, which may be due to the \nvolume of objects needed to store the contents of each .le and grammars. Within each burst, there are \nalso small allocation pauses that cause MicroPhase to invoke minor collection in burst. As a result, \nthe mutator utilization when the windows are small is not as high as the space-based approach. space-basedspace-based \nspace-basedMicroPhase (optimal) MicroPhase (optimal)MicroPhase (fixed-ratio) MicroPhase (optimal) MicroPhase \n(fixed-ratio) (a) compress (b) db* (c) jack space-basedMicroPhase (optimal)space-based space-basedMicroPhase \n(fixed-ratio) MicroPhase (optimal) MicroPhase (optimal) (d) javac (e) jess* (f) mpegaudio* space-basedspace-basedspace-based \nMicroPhase (optimal)MicroPhase (optimal) MicroPhase (fixed-ratio) MicroPhase (fixed-ratio) MicroPhase \n(optimal) (g) mtrt (h) raytrace (i) SPECjbb2000* Figure 11. Minimum mutator utilizations (MMUs) of SPECjvm98 \nbenchmarks and SPECjbb2000 In conclusion, we found that MicroPhase improves mu\u00adtator utilization in .fteen \nout of the twenty benchmarks. It also has no positive or negative effects on the other four benchmarks \nand degrades the MMU of only one benchmark (antlr). 5.3 Overall Performance Figure 13 depicts the reduction \nin the overall execution time of each of the SPECjvm98 and DaCapo benchmarks. The graph shows reductions \nin the execution times of 12 bench\u00admarks, ranging from 1% to 14%. For SPECjbb2000, we compared the throughput \nof MicroPhase with that of the space-based approach instead of comparing the overall ex\u00adecution time. \nAs explained earlier, MicroPhase yields higher utilization when the window size is smaller than 16 seconds \nand very similar utilization to the space-based approach af\u00adterward. As a result, MicroPhase improves \nthe throughput by 5%. For bloat, chart, fop, jack and jython, MicroPhase can only achieve slight reductions \nin GC times, and thus, these reductions are offset by the runtime overhead to enable Mi\u00adcroPhase (pause \ndetection and minor or full collection deci\u00adsion), which collectively account for about 2% overhead to \nthe mutator. As a result, the overall execution times of these applications increase by 0.5% to 2%. For \nlusearch, MicroPhase cannot signi.cantly reduce the amount of times spent in minor collection and full \ncollection. Thus, the overall execution time is half a percent longer. We suspect that the major reason \nfor MicroPhase inef.ciency in lusearch is multithreading. (More discussion about apply\u00ad space-basedMicroPhase \n(optimal)MicroPhase (fixed-ratio) (j) antlr space-based MicroPhase (optimal) (m) eclipse* space-basedMicroPhase \n(optimal)MicroPhase (fixed-ratio) (k) bloat space-based MicroPhase (optimal) (n) fop* space-basedMicroPhase \n(optimal)MicroPhase (fixed-ratio) (l) chart space-basedMicroPhase (optimal)MicroPhase (fixed-ratio) (o) \nhsqldb space-basedspace-based MicroPhase (optimal)space-based MicroPhase (optimal) MicroPhase (fixed-ratio) \nMicroPhase (fixed-ratio)MicroPhase (optimal) (p) jython (q) luindex* (r) lusearch space-basedMicroPhase \n(optimal)space-basedMicroPhase (fixed-ratio) MicroPhase (optimal) (s) pmd (t) xalan* Figure 12. Minimum \nmutator utilizations (MMUs) of DaCapo benchmarks ing MicroPhase in multithreaded environments is discussed \nin Section 6.) Because our technique detects pauses occur\u00adring within each thread, it is likely that \na pause detected in one thread does not represent allocation phase bound\u00adaries in other threads. While \nsuch an issue also exists in SPECjbb2000 and hsqldb, it does not manifest itself be\u00adcause: 1. hsqldb \nallocates a large number of long-lived objects. Moreover, GC also dominates the cost of execution. Our \ntechnique was able to predict proper locations to invoke the full collection, and therefore, reasonable \nreductions in the overall GC time and execution time are achieved. However, MicroPhase was less successful \nin predicting when to ef.ciently invoke the minor collection, a more dif.cult task due to different allocation \nphases in each thread. 2. SPECjbb2000 spawns eleven threads but only eight are actively used. Moreover, \nthese eight threads are launched sequentially as the tasks become more demanding. As a result, MicroPhase \ncan work ef.ciently early on when the number of active threads is still small.  We also observe that \nMicroPhase can also impose pos\u00aditive effects on the mutator performance. For example, its adoption, while \nincurring some runtime overheads, can also reduce the occurrences of write-barriers and intergenera\u00adtional \npointers because there are fewer surviving new objects (SNOs). 5.4 Heap Size Sensitivity To investigate \nhow different heap sizes affect MicroPhase, we conducted an experiment utilizing .ve different heap con.gurations \nin all applications: twice (2X), three times (3X), four times (4X), .ve times (5X), and six times (6X) \nthe maximum live-size. Figure 14 depicts our results. The x-axis is the normalized heap size (relative \nto the maximum live-size), and y-axis is the overall GC time reduction of Mi\u00adcroPhase compared to the \nspace-based approach operating at the same heap size. In most benchmarks, our result indicates that MicroPhase \nhas the highest reduction of GC time when the heap con.g\u00aduration is 2X due to more frequent GC invocations, \nwhich create more opportunities for savings. In addition, the likeli\u00adhood that these GCs are triggered \nin an untimely manner in the space-based approach (with 2X con.guration) is much higher than in the larger \ncon.gurations; as the heap becomes larger, the need to invoke GC lessens. We also observe that in xalan, \npmd, jython, luindex, eclipse, bloat, chart,and SPECjbb2000, MicroPhase also performs very well when \nthe heap is larger than 2X. The dominating cost of minor collection is the main reason for MicroPhase \nto perform well. In these applications, minor collection is invoked very frequently while the full collec\u00adtion \nis rarely called (see Table 5). Enlarging the heap would signi.cantly reduce the number of full collection \ninvocations or eliminate the need to invoke it altogether; however, the minor collection is still periodically \ninvoked. Because Mi\u00adcroPhase often yields higher minor collection ef.ciencies, it can reduce collection \ntime of each minor GC invocation. In addition, as the time spent in GC becomes smaller with a larger \nheap size, a slight reduction in the GC time can result in a large percentage of improvement. 6. Future \nStudies In this paper, we have shown that MicroPhase can provide an effective alternative to the traditional \nspace-based triggering mechanism. However, these experimental results should not be viewed as the best \nperformance capability of MicroPhase; instead, they represent a glimpse of its potential bene.ts. Because \nthis is the .rst attempt to use allocation phases to trigger GC, there is still plenty of room for improvements \nand further studies.  For example, it is unclear how ef.cient the proposed technique is on large multithreaded \napplications running on uniprocessor systems. When a pause is detected in an exe\u00adcution thread, it does \nnot mean that other mutator threads are also experiencing allocation pauses. Thus, triggering GC at this \npoint may be locally optimal based on the alloca\u00adtion behavior of that thread but not globally optimal \nwith respect to the remaining threads. One possible solution is to record allocation status of every \nthreads and invoke GC when the majority is experiencing allocation pauses. We also observed from our \npast studies [36, 35] that the amount of work done by each thread in large server applications is not \nuniform. For example, we noticed that 10% of threads in SPECjAppServer2004 contribute to about 80% to \n90% of all allocated objects. It may be possible to apply MicroPhase to these threads and then use space-based \ntechnique with the remaining threads. We are currently investigating this ap\u00adproach. Another existing \ntechniques designed for multithreaded programs in multi-processor environments are thread local heaps \nand thread speci.c heaps [28, 12]. The main idea is to create one subheap for every thread. Any objects \nshared by multiple threads are allocated in a shared heap area. Applying MicroPhase to these techniques \nwill make pause detection more straightforward and allow GC to be invoked independently in each subheap. \nRecent efforts by Guyer and McKinley [13] and Zee and Rinard [37] have shown the power of .ow-sensitive \npointer analysis to improve garbage collection performance. Flow\u00adsensitive pointer analysis can also \nbe used to optimize Mi\u00adcroPhase to identify areas with intense and light allocation requests and target \nthe areas with light requests as possible GC invocation points. 7. Related Work In a working paper [33], \nWilson proposes opportunistic garbage collection, a non-disruptive generational technique that hides \nGC pauses in compute-bound phases of program execution. The rationale behind this approach is that there \nare two major execution phases in a program: compute\u00adbound and user-interactive. Thus, Wilson proposes \ntwo heuristics to hide pauses due to minor and major collection. First, user-oriented heuristics schedule \nscavenges (i) at the end of non-interactive segments of program execution and (ii) at times when the \nsystem is idle. Second, computation\u00adoriented heuristic schedules scavenges at a minimum stack height, \nwhere the volume of live objects is small. We think that the computation-oriented heuristic may be an \nindirect approach to identify allocation pauses [33]. Later on, Wilson and Moher [34] present the design \nof op\u00adportunistic garbage collection. To support the user-oriented heuristics, they attach a small routine \nto each user-interactive routine to measure the time since the last user interaction. Another routine \nis used to detect user-interaction pauses. When a pause is detected, the system decides whether to scavenge \nand how many generations to scavenge. They also suggest that each user-interaction pause can still create \na large enough volume of objects to cause minor collection invocations. In such a scenario, the authors \nclaim that the goal of hiding these pauses is still accomplished as these in\u00advocations occur during an \ninteraction pause. Comparatively, each of our allocation pauses is much more .ne-grained than each of \ntheir user-interaction pauses. When an allocation pause occurs, there are no objects allo\u00adcations at \nall while objects are still created during a user\u00adinteraction pause. Thus, one major distinction is that \nour technique attempts to trigger GC at allocation pauses while their approach tries to trigger GC at \neach user-interaction pause, in which many allocation pauses can still occur. Another major difference \nis that our primary objective is to improve the garbage collection performance, while their primary objective \nis to hide GC pauses. As by-products of these two primary objectives, our approach can yield shorter \npauses in some applications, while their approach of\u00adten yields better GC performance. In terms of implementa\u00adtion, \nMicroPhase relies on low-overhead runtime monitoring mechanisms and not program annotations to guide \nthe GC triggering decision process. Work by Qian et al. [21] suggests that allocation phases exist in \n.ve Java applications, javac, jack, jess, db,and mtrt from SPECjvm98. They de.ne activity zones where \na zone represents an execution area with a certain allocation characteristic. They use allocation rate \nand allocation vari\u00adation as two criteria in identifying activity zones. They use calculation windows \nto de.ne the number of sample points needed to calculate the allocation rates. Through simulation, they \nevaluate the accuracy of their zone detection algorithm and project the improvement in GC ef.ciency and \nheap us\u00adage when their approach is used. Their results show that in applications with clearly distinct \nphases (jack and jess), sig\u00adni.cant performance gains can be achieved (17% and 30%, respectively). However, \ntheir zone detection algorithm fails to detect many if any zones in the remaining three applica\u00adtions, \nresulting in modest performance improvement. Comparing to our work, the main motivation is the same; \nthat is there are allocation phases in most Java applications and triggering GC during the execution \nzones with no allo\u00adcation activities can improve GC performance. However, we leverage allocation pauses \ninstead of changes in allocation rates as a triggering criterion. The results show that our al\u00adgorithm \ncan detect execution areas with no allocation activ\u00adity more accurately than their technique (e.g. our \ntechnique can detect many pauses in mtrt, while their technique de\u00adtects none). Another major difference \nis that our results are based on actual implementation inside a generational col\u00adlector, while their \nresults are based on simulation of mark\u00adsweep collector. Work by Buytaert et al. [7] uses pro.le-directed \napproach called garbage collection hints or GCH to improve the per\u00adformance of the Appel collector in \nJikes RVM [16]. Their technique uses off-line pro.ling to identify favorable col\u00adlection points (FCPs). \nThe collector then dynamically se\u00adlects whether minor or full collection should be invoked when a favorable \ncollection point is reached. Experimental results (using six SPECjvm98 benchmark) show that GCH can achieve \nup to 10% reduction in execution time and 29% reduction in garbage collection time. To identify FCPs, \nan off-line analysis calculates the live/time function, which represents the volume of live ob\u00adjects \n(in bytes) over total allocated bytes. The FCPs for each application are then selected from the pro.led \nresult; each FCP is represented by a method. They discovered that some benchmarks have as many as three \nFCPs while others have only one FCP. A cost model is then used to decide if no GC, full GC, or minor \nGC should be invoked at each FCP. There are three cost components in the model: minor collection cost, \nfull collection cost, and remembered set processing cost. One major difference between their technique \nand ours is that we do not use pro.ling to detect good collection points, instead, we associate these \npoints with allocation pauses. We have yet to study the relationship between our pauses and their FCPs \nbut we hypothesize that they are the same. Because we do not use pro.ling, we cannot estimate the amount \nof live-size at each detected point. Instead, we use past performances at these method invocation sites \nto predict whether to invoke minor or full collection. Brecht et al. [6] suggests that the amount of \navailable memory should be used to regulate heap resizing and in\u00advoke garbage collection, instead of \nusing heap usage as the main GC invocation criteria. Their technique leverages the heap usage information \nto select a GC policy from a pool of three policies: if the heap is lightly utilized, en\u00adlarge it aggressively; \nif the heap is heavily utilized, invoke GC aggressively; and if the last collection is not effective, \ndo not collect. We also use heap usage information to reg\u00adulate when MicroPhase should be utilized (i.e. \nwhen heap usage surpasses hT hreshold ). Once MicroPhase is fully engaged, we leverage allocation phase \ninformation to invoke GC. Their technique does not consider phase information. Efforts by Bacon et al. \nand Gestegard et al. [1, 23] explore the use of time-based garbage collection to improve real\u00adtime performance \nby using time to control GC triggering. In effect, these garbage collectors become periodic instead of \nsporadic [23]. Chen et al. [8] proposes a technique that proactively invokes GC to improve cache and \npage localities. One consequence of using the space-based criterion is a large volume of surviving new \nobjects or SNOs, Work by Stefanovi\u00b4c et al. proposes Older-First algorithm [30, 29], which concentrates \nits collection effort on older objects. Similarly, work by Blackburn et al. proposes the Beltway framework \n[3], in which the heap is partitioned into several belts, and each belt consist of several segments. \nBeltway uses Older-First algorithm to collect each belt. While Older-First and Beltway can effectively \ndeal with SNOs, they do not address the issue of untimely GC invocation. Phase is often de.ned as a period \nof work (e.g. compu\u00adtation, allocation). In phase aware computing, phase infor\u00admation is used to assist \nwith resource reclamation or pro\u00adgram optimization. Work by Nagpurkar et al. proposes an online framework \nfor phase detection [20, 19]. The phase detection algorithm executes concurrently with the program and \ndetects phase boundaries by computing and evaluating the similarity between two windows of pro.le elements \n(e.g. execution events). Our technique can be classi.ed as an in\u00adstance of their proposed framework. \nWork by Ding et al. de.nes a phase as a unit of recurring behavior, and boundaries of a phase can be \nuniquely marked in its program [11]. Their technique called gated memory control leverages phase information \nto monitor and control memory usage. One interesting aspect of their work is the preventive memory management, \nwhich considers invoking GC at each of the outer most phase instances (loops or func\u00adtions). Within a \nphase, the technique lets the heap grow unimpeded. Currently, the relationship between our alloca\u00adtion \npauses and their program phases is still unclear. We sus\u00adpect that pauses resulting in large live-size \nreductions may be the same as their outer phases. However, their scheme may not detect smaller live-size \nreductions in the inner phases. It is also unclear how their technique can be integrated with generational \ncollectors (i.e. would their technique invoke mi\u00adnor or full upon entering outer phases). 8. Conclusion \nWe introduce MicroPhase, a GC triggering technique that considers GC ef.ciency, instead of heap usage, \nas the main invocation criterion. Thus, MicroPhase can alleviate two existing shortcomings associated \nwith the space-based ap\u00adproach: untimely GC invocations and large volumes of sur\u00adviving newly created \nobjects. The foundation of MicroPhase is based on two observations commonly found in Java pro\u00adgrams: \n(i) allocation requests occur in phases, and (ii) the phase boundaries coincide with times that most \nobjects also die. As part of the implementation of MicroPhase, we ex\u00adtended HotSpot to support two additional \nmechanisms, pause detection and dynamic GC triggering. The pause de\u00adtection mechanism is sampling-based \nand can accurately distinguish between actual allocation pauses from other run\u00adtime delays. The dynamic \ntriggering mechanism leverages allocation site information to select whether minor collec\u00adtion or full \ncollection should be invoked when a pause is detected. We evaluated the performance of MicroPhase us\u00ading \na collection of 20 Java benchmarks. The experimental results indicate that our technique can reduce the \nGC times in 19 applications, ranging from an increase of 1% to a de\u00adcrease of 26% when the heap is set \nto be twice the maximum live-size. As a result, the overall performance improvement ranges from a degradation \nof 2.5% to an improvement of 14% in 13 applications. 9. Acknowledgments This work was sponsored in part \nby the National Sci\u00adence Foundation through awards CNS-0411043 and CNS\u00ad0720757, and by the Army Research \nOf.ce through DURIP award W911NF-04-1-0104. We also thank the anonymous reviewers for providing insightful \ncomments for the .nal version of this paper. References [1] David F. Bacon, Perry Cheng, and V. T. Rajan. \nControlling fragmentation and space consumption in the Metronome, a real-time garbage collector for Java. \nIn Proceedings of the 2003 ACM SIGPLAN/SIGBED Conference on Languages, Compilers, and Tools for Embedded \nSystems (LCTES), pages 81 92, San Diego, California, USA, June 2003. [2] David F. Bacon, Ravi Konuru, \nChet Murthy, and Mauricio Serrano. Thin locks: Featherweight synchronization for Java. In Proceedings \nof the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI), pages 258 268, \nMontreal, Quebec, Canada, June 1998. [3] S.M.Blackburn, R.E.Jones, K. S. McKinley, and J. E . B. Moss. \nBeltway: Getting around garbage collection gridlock. In Proceedings of the ACM SIGPLAN Programming Languages \nDesign and Implementation (PLDI), pages 153 164, Berlin, Germany, 2002. [4] S. M. Blackburn, S. Singhai, \nM. Hertz, K. S. McKinely, and J. E. B. Moss. Pretenuring for Java. In Proceedings of the ACM SIGPLAN \nConference on Object Oriented Program\u00adming Systems Languages and Applications (OOPSLA), pages 342 352, \nTampa Bay, FL, October 2001. [5] Stephen M. Blackburn, Robin Garner, Chris Hoffmann, As\u00adjad M. Khang, \nKathryn S. McKinley, Rotem Bentzur, Amer Diwan, Daniel Feinberg, Daniel Frampton, Samuel Z. Guyer, Martin \nHirzel, Antony Hosking, Maria Jump, Han Lee, J. Eliot, B. Moss, Aashish Phansalkar, Darko Stefanovi\u00b4c, \nThomas VanDrunen, Daniel von Dincklage, and Ben Wieder\u00admann. The DaCapo benchmarks: Java benchmarking \ndevel\u00adopment and analysis. In Proceedings of the ACM SIGPLAN Conference on Object-Oriented Programming \nSystems, Lan\u00adguages, and Applications (OOPSLA), pages 169 190, Port\u00adland, Oregon, USA, 2006. [6] T. Brecht, \nE. Arjomandi, C. Li, and H. Pham. Controlling garbage collection and heap growth to reduce the execution \ntime of Java applications. In Proceedings of the Object-Oriented Programming Systems Languages and Applications \n(OOPSLA), pages 353 366, Tampa Bay, FL, USA, 2001. [7] Dries Buytaert, Kris Venstermans, Lieven Eeckhout, \nand Koen De Bosschere. Garbage collection hints. In Proceedings of the First International Conference \non High Performance Embedded Architectures and Compilers (HiPEAC 2005), pages 233 248, Barcelona, Spain, \n11 2005. Springer Verlag. [8] WenKe Chen, Sanjay Bhansali, Trishul Chilimbi, Xiaofeng Gao, and Weihaw \nChuang. Pro.le-guided proactive garbage collection for locality optimization. In Proceedings of the ACM \nSIGPLAN Conference on Programming Language Design and Implementation (OOPSLA), pages 332 340, Ottawa, \nOntario, Canada, 2006. [9] Perry Cheng and Guy E. Blelloch. A parallel, real-time garbage collector. \nIn Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Imple\u00admentation (PLDI), \npages 125 136, Snowbird, Utah, USA, 2001. [10] S. Dieckmann and U. H\u00a8olzle. A study of the allocation \nbe\u00adhavior of the SPECjvm98 Java benchmarks. In Proceedings of the European Conference on Object-Oriented \nProgram\u00adming (ECOOP), pages 92 115, Lisbon, Portugal, June 1999. Springer Verlag. [11] Chen Ding, Chengliang \nZhang, Xipeng Shen, and Mitsunori Ogihara. Gated memory control for memory monitoring, leak detection \nand garbage collection. In Proceedings of the Workshop on Memory System Performance (MSP), pages 62 67, \nChicago, Illinois, 2005. [12] Tamar Domani, Gal Goldshtein, Elliot K. Kolodner, Ethan Lewis, Erez Petrank, \nand Dafna Sheinwald. Thread-local heaps for Java. SIGPLAN Notices, 38(2 supplement):76 87, 2003. [13] \nSamuel Z. Guyer, Kathryn S. McKinley, and Daniel Framp\u00adton. Free-me: a static analysis for automatic \nindividual object reclamation. In Proceedings of the ACM SIGPLAN Confer\u00adence on Programming Language \nDesign and Implementation (PLDI), pages 364 375, Ottawa, Ontario, Canada, 2006. [14] Matthias Hauswirth, \nPeter F. Sweeney, Amer Diwan, and Michael Hind. Vertical pro.ling: Understanding the behavior of object-oriented \napplications. In Proceedings of the ACM SIGPLAN Conference on Object-Oriented Programming Systems, Languages, \nand Applications (OOPSLA), pages 251 269, Vancouver, British Columbia, Canada, October 2004. [15] Matthew \nHertz, Stephen M Blackburn, J Eliot B Moss, Kathryn S. McKinley, and Darko Stefanovi\u00b4c. Error\u00adfree garbage \ncollection traces: How to cheat and not get caught. In Proceedings of the 2002 ACM International Conference \non Measurement and Modeling of Computer Systems (SIGMETRICS), pages 140 151, Marina Del Rey, California, \n2002.  [16]IBM.JikesResearchVirtualMachine.http://jikesrvm.sourceforge.net. [17] R. Jones and R. Lins. \nGarbage Collection: Algorithms for automatic Dynamic Memory Management. John Wiley and Sons, 1998. [18] \nPeter Mikhalenko. Real-time Java: An introduction. On-Line Article, Last visited: July 2007. http://www.onjava.com/pub/a/onjava/2006/05/10/real-time\u00adjava-introduction.html. \n[19] Priya Nagpurkar and Chandra Krintz. Visualization and analysis of phased behavior in Java programs. \nIn Proceedings of the ACM International Conference on the Principles and Practice of Programming in Java \n(PPPJ), Las Vegas, Nevada, USA, 2004. [20] Priya Nagpurkar, Chandra Krintz, Michael Hind, Peter F. Sweeney, \nand V. T. Rajan. Online phase detection algorithms. In Proceedings of the International Symposium on \nCode Generation and Optimization (CGO), pages 111 123, Manhattan, NY, USA, 2006. [21] Yang Qian, Wei \nHuang, Witawas Srisa-an, and J. Morris Chang. Allocation Pattern and GC Triggering. Tech\u00adnical Report \nTR-UNL-CSE-2003-0017, University of Nebraska Lincoln, Lincoln, Nebraska, U.S.A., October 2003. http://lakota.unl.edu//facdb/TechReportArchive/ \nTR-UNL-CSE-2003-0017.pdf. [22] Yang Qian, W. Srisa-an, T. Skotiniotis, and J. M. Chang. A cycle-accurate \nper-thread timer for Linux operating system. In Proceedings of IEEE International Symposium on Performance \nAnalysis of Systems and Software (ISPASS), pages 38 44, Tucson, Arizona, USA, November 2001. [23] Sven \nGestegard Robertz and Roger Henriksson. Time\u00adtriggered garbage collection: robust and adaptive real-time \nGC scheduling for embedded systems. In LCTES 03: Proceedings of the 2003 ACM SIGPLAN conference on Language, \ncompiler, and tool for embedded systems, pages 93 102, San Diego, California, USA, 2003. [24] Ran Shaham, \nElliot K. Kolodner, and Mooly Sagiv. On effec\u00adtiveness of GC in Java. In Proceedings of the International \nSymposium on Memory Management (ISMM), pages 12 17, Minneapolis, Minnesota, United States, 2000. [25] \nJon Shemitz. Using RDTSC for benchmarking code on Pentium computers, Last visited: July 2007. http://www.midnightbeach.com/jon/pubs/rdtsc.htm. \n[26] Standard Performance Evaluation Corporation. SPECjbb2000. White Paper, Last visited: July 2007. \nhttp://www.spec.org/osg/jbb2000/docs/whitepaper.html. [27] Standard Performance Evaluation Corporation. \nSPECjvm98 benchmarks, Last visited: July 2007. http://www.spec.org/osg/jvm98. [28] Bjarne Steensgaard. \nThread-speci.c heaps for multi-threaded programs. In Proceedings of the International Symposium on Memory \nManagement, pages 18 24, Minneapolis, Min\u00adnesota, United States, 2000. [29] Darko Stefanovi\u00b4c, Matthew \nHertz, Stephen M. Blackburn, Kathryn S. McKinley, and J. Eliot B. Moss. Older-.rst garbage collection \nin practice: Evaluation in a Java virtual machine. SIGPLAN Notices, 38(2 supplement):25 36, 2003. [30] \nDarkoStefanovi\u00b4c,KathrynS.McKinley,andJ.EliotB.Moss. Age-based garbage collection. In Proceedings of \nthe ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications (OOPSLA), \npages 370 381, Denver, Colorado, United States, November 1999. [31] Sun Microsystems. Java technology \nis everywhere, sur\u00adpasses 1.5 billion devices worldwide. Press Release, Febru\u00adary 2004. http://www.sun.com/smi/Press/sun.ash/2004\u00ad02/sun.ash.20040219.1.html. \n[32] David Ungar. Generation scavenging: A non-disruptive high performance storage reclamation algorithm. \nIn Proceedings of the First ACM SIGSOFT/SIGPLAN Software Engineering Symposium on Practical Software \nDevelopment Environ\u00adments, pages 157 167, 1984. [33] P. R. Wilson. Opportunistic garbage collection. \nACM SIGPLAN Notices, 23(12):98 102, 1988. [34] Paul R. Wilson and Thomas G. Moher. Design of the opportunistic \ngarbage collector. ACM SIGPLAN Notices, 24:23 35, 1989. [35] Feng Xian, Witawas Srisa-an, ChengHuan Jia, \nand Hong Jiang. AS-GC: An ef.cient generational garbage collector for Java application servers. In Proceedings \nof the European Conference on Object-Oriented Programming (ECOOP), pages 126 150, Berlin, Germany, July \n2007. [36] Feng Xian, Witawas Srisa-an, and Hong Jiang. Investigat\u00ading the throughput degradation behavior \nof Java application servers: A view from inside the virtual machine. In Pro\u00adceedings of the 4th International \nConference on Principles and Practices of Programming in Java (PPPJ), pages 40 49, Mannheim, Germany, \n2006. [37] Karen Zee and Martin Rinard. Write barrier removal by static analysis. In Proceedings of the \nConference on Object-Oriented Programming Systems, Languages, and Applica\u00adtions (OOPSLA), pages 191 210, \nSeattle, Washington, USA, 2002.   \n\t\t\t", "proc_id": "1297027", "abstract": "<p>To date, the most commonly used criterion for invoking garbage collection (GC) is based on heap usage; that is, garbage collection is invoked when the heap or an area inside the heap is full. This approach can suffer from two performance shortcomings: untimely garbage collection invocations and large volumes of surviving objects. In this work, we explore a new GC triggering approach called <i>MicroPhase</i> that exploits two observations: (i) allocation requests occur in phases and (ii) phase boundaries coincide with times when most objects also die. Thus, proactively invoking garbage collection at these phase boundaries can yield high efficiency. We extended the HotSpot virtual machine from Sun Microsystems to support MicroPhase and conducted experiments using 20 benchmarks. The experimental results indicate that our technique can reduce the GC times in 19 applications. The differences in GC overhead range from an increase of 1% to a decrease of 26% when the heap is set to twice the maximum live-size. As a result, MicroPhase can improve the overall performance of 13 benchmarks. The performance differences range from a degradation of 2.5% to an improvement of 14%.</p>", "authors": [{"name": "Feng Xian", "author_profile_id": "81100606421", "affiliation": "University of Nebraska-Lincoln, Lincoln, NE", "person_id": "PP39115861", "email_address": "", "orcid_id": ""}, {"name": "Witawas Srisa-an", "author_profile_id": "81100018125", "affiliation": "University of Nebraska-Lincoln, Lincoln, NE", "person_id": "PP39109429", "email_address": "", "orcid_id": ""}, {"name": "Hong Jiang", "author_profile_id": "81361606829", "affiliation": "University of Nebraska-Lincoln, Lincoln, NE", "person_id": "PP40040092", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1297027.1297034", "year": "2007", "article_id": "1297034", "conference": "OOPSLA", "title": "Microphase: an approach to proactively invoking garbage collection for improved performance", "url": "http://dl.acm.org/citation.cfm?id=1297034"}