{"article_publication_date": "10-21-2007", "fulltext": "\n Scalable Omniscient Debugging \u00b4 Guillaume Pothier * EricTanter e Piquer Jos\u00b4 DCC University of Chile \nAvenida Blanco Encalada 2120 Santiago, Chile {gpothier,etanter,jpiquer}@dcc.uchile.cl Abstract Omniscient \ndebuggers make it possible to navigate back\u00adwards in time within a program execution trace, drastically \nimproving the task of debugging complex applications. Still, they are mostly ignored in practice due \nto the challenges raised by the potentially huge size of the execution traces. This paper shows that \nomniscient debugging can be real\u00adistically realized through the use of different techniques addressing \nef.ciency, scalability and usability. We present TOD,a portableTrace-Oriented Debugger forJava, which \ncombinesanef.cient instrumentationforevent generation,a specialized distributed database for scalable \nstorage and ef\u00ad.cient querying, support for partial traces in order to reduce the trace volume to relevant \nevents, and innovative inter\u00adface components for interactive trace navigation and analy\u00adsis in the development \nenvironment. Provided a reasonable infrastructure,the performanceofTOD allowsa responsive debuggingexperiencein \nthefaceof large programs. Categories and Subject Descriptors D.2.5[Testing and Debugging]: Debugging \naids; D.2.6[Programming Envi\u00adronments]: Integrated environments; D.3.4 [Processors]: Debuggers; H.2.3[Languages]: \nQuery languages; H.2.4 [Systems]: Distributed databases; H.2.4[Systems]: Query processing GeneralTerms \nAlgorithms, Design, Performance Keywords Omniscient debugging, scalability, execution traces, specialized \ndistributed database, partial traces, in\u00adterface components * Guillaume Pothieris .nancedbya PhD grant \nfrom NIC Chile \u00b4 E.Tanteris partially .nancedbythe Millenium Nucleus CenterforWeb Research, Grant P04-067-F, \nMideplan, Chile, and by FONDECYT Project 11060493. Permission to make digital or hard copies of all or \npart of this work for personal or classroom use is granted without fee provided that copies are not made \nor distributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page.To copyotherwise, to republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. OOPSLA 07, October 21 25, 2007, Montr \u00b4ebec, Canada. eal, Qu\u00b4Copyright \nc &#38;#169; 2007ACM 978-1-59593-786-5/07/0010...$5.00 1. Introduction Debugging softwareisa majortaskof \nthe softwaredevelop\u00adment process, both in terms of time and cost. Unfortunately debuggers in most development \nenvironments only provide very minimal assistance and debugging remains a tedious and time-consuming \ntask. There are two traditional approaches to debugging: log\u00adbased debugging and breakpoint-based debugging. \nThe .rst approach consists in inserting logging statements within the source code, in order to produce \nan ad-hoc trace during pro\u00adgram execution. This technique exposes the actual history of executionbut \n(a) it requires cumbersome and widespread modi.cations to the source code, and (b) it does not scale \nbecause manual analysis of huge traces is hard. The second approach consists in running the program under \na dedicated debugger which allows the programmer to pause the exe\u00adcution at determined points, inspect \nmemory contents, and then continue execution step-by-step. Although not subject to the two issues of \nlog-based debugging, breakpoint-based debugging is limited: when execution is paused, the infor\u00admation \nabout the previous state and activity of the program is limited to introspection of the current call \nstack. Omniscient debuggers, also known as back-in-time or post-mortem debuggers, overcome all these \nissues [11, 15, 17]. An omniscient debugger records the events that occur during the execution of the \ndebugged program, and then lets the user conveniently navigate through the obtained execu\u00adtion trace. \nThis approach combines the advantages of log\u00adbased debugging past activity is never lost and those of \nbreakpoint-based debugging easy navigation, step-by-step execution, complete stack inspection. An omniscient \ndebug\u00adger can simulate step-by-step execution forward and back\u00adward, and makes it possible to immediately \nanswer ques\u00adtions that would otherwise require a signi.cant effort, like At what point was variable x \nassigned value y ? or What was the state of object o when it was passed as an argument to the method \nfoo ? . Whiletheadvantagesof omniscientdebuggingover tradi\u00adtional approaches are incredibly clear, it \nhas had a very lim\u00adited impact in production environments, and is still mostly seen as an unrealistic \napproach. It is true that omniscient debugging raises important issues. First, except when us\u00ading specialized \nhardware probing ports [10], the emission of events causes a signi.cant overhead to the debugged ap\u00adplication. \nSecond, as emphasized in [9, 20, 28], for CPU\u00adintensive applications, the execution trace can rapidly \nbe\u00adcome huge (hundreds of million events), implying that (a) trace data must be stored very quickly and \nrequires scalable storage; and (b) the user interface of the debugger must be responsive enough this \nrequires fast query execution on huge traces , and must assist the user in overcoming the cognitiveburdenof \ndealing witha large amountof informa\u00adtion in order to rapidly locate the points of interest. The contribution \nof this paper is to show that omni\u00adscient debugging can be realistically realized through the use of \ndifferent techniques enhancing ef.ciency, scalability, and usability. This claimisvalidatedbyTOD1,a portable \nTrace-Oriented Debugger for Javaintegrated into the Eclipse IDE [5].TOD features: Ef.cient event generation \nbased on a compact trace model, a custom binary encoding of events, and a fast, portable low-level weaver. \n Specialized distributed database engine for scalable andfast storingand queryingofevents, whichleverages \nthe highly-constrained nature of execution traces. On a dedicated 10-node clusterTOD handlesa sustained \ninput rate of approx. 470kEv/s (thousands events per second), and hundreds of queries per second.  Support \nfor partial traces by offering static and dy\u00adnamic mechanisms for selective trace generation, and ad\u00adequate \nreporting of incomplete information.  Responsive GUI thanks to ef.cient query processing; TOD was used \nto debug an application as complex as Eclipse while preserving interactivity.  Specialized GUI components \nproviding high-level views on huge event traces for more effective navigation, such as thread murals. \n Section2 details the features and challenges of omniscient debugging. Section 3 overviews the architecture \nof TOD, the event model, and the GUI components. Section 4 de\u00adscribes the ef.cient indexing scheme of \nTOD for storing and querying events, and Section 5 shows how it is paral\u00adlelized. Benchmarks areprovidedin \nSection6.Weexplain the advantage of partial traces and how theyare dealt with in Section 7. Related work \nis discussed in details in Section 8. Section9concludes, and identi.es opportunities for further enhancements \nin the .eld. 1http://reflex.dcc.uchile.cl/TOD for download and small illustra\u00adtive videos. 2. Challenges \nof Omniscient Debugging Wenowpresent the main features of an omniscient debugger compared to traditional \ndebuggers, and outline the scalabil\u00adity challenges of omniscient debugging. 2.1 Features of an omniscient \ndebugger An omniscient debugger (OD) provides four major features: stepping, state reconstitution, control \n.ow reconstitution, and root cause .nding. The latter is unique to omniscient debuggers, while others \nare typical debugger features. In breakpoint-based debuggers, Stepping consists in exe\u00adcutingthetarget \nprogramone instructionatatime.Thereare two variants of stepping: step over executes behavior2 call statements \nwithout halting inside the called behavior, while step into halts at the beginning of the called behavior. \nState reconstitution consists in letting the programmer inspect ob\u00adject state when the target program \nis halted. Control .ow re\u00adconstitution permitsto obtainaviewonthe currentcall stack of the program, with \nbound variables and objects. ODs ex\u00adtend these three features with complete freedom with respect to time: \nstepping can be done both forward and backward in time, programmers can inspect the state of objects \nas they were at anygiven point in time, and can freely browse the control .ow tree. Finally, one of the \nmost useful features of ODs is their ability to .nd when and in which context a particular .eld or variable \nwas given a certain value. Indeed, bugs often manifest long after their root cause occurs. For instance, \ntryingto dereferencea null reference obtained fromagiven .eld causes a crash, which is the symptom of \nthebug. The information the programmer needs is when was the .eld set to null.With breakpoint-baseddebuggers,evenifexecution \nis halted just before thefaulty dereference, the root cause of thebug canbe already lost, e.g.because \nthe code that caused it is not in the call stack anymore. 2.2 Scalability challenges Underlying the \nfeatures presented above lies the necessity to generate and record execution traces. The potentially \nhuge size of these traces poses several scalability challenges, which are the main reason for the lack \nof production-quality ODs. Events must be recorded quickly, preferably in real time, so that (a) debugging \ncan begin immediately after the target program terminates or crashes, and (b) runtime overhead is minimized \nto preserve overall performance of the debugged program, and interactivity when needed (e.g. debugging \nEclipse).  The debugger should cause minimal interference to the target program in order to not affect \nits behavior. In par\u00adticular,the address space and memory management of the target process should not \nbe altered.  2We give methods and constructors the collective name ofbehaviors. Figure 1. High-level \narchitectureofTOD The event storage capacity of an omniscient debugger must be aligned with the expected \nnumber of events in a useful trace: with GHz CPUs, hundreds of millions events can be generated in only \na few minutes of exe\u00adcution.  Queries on the execution trace must execute at a speed compatible with \nuser interaction, e.g. in tenths of seconds for operations like stepping.  Informationmustbe presentedinawaythat \naddressesthe cognitiveburdenofnavigating throughhugeevent traces, enabling rapidbug identi.cation.  \nThis work addresses the above issues via optimized event representations and aggressive indexing, a simple \nquery model, a distributed database backend, support for partial traces, and specialized presentation \nand interaction compo\u00adnents. 3. OverviewofTOD TODisaTrace-Oriented Debugger forJava that addresses the \nscalability issues identi.ed above. The objective is to ad\u00address these issues in order to obtain an omniscient \ndebugger thatis practically applicable. This sectiongives anoverview ofTOD via its architecture, theevent \nmodel, and the GUI components. 3.1 Architecture TOD is designed around two central ideas: to decouple \nthe core of the debugger from the target program execution, and to be portable. It is made up of three \ncomponents (Fig. 1): the target JavaVirtual Machine (JVM) in which the debugged program runs and emits \nevents, the debugger core that im\u00adplements the main functionalities of TOD, and the debug\u00adger frontend \nthrough which the user interactively queries and navigates in the execution trace. The rationale for \nstoring events in a database rather than in memory as done in other omniscient debuggers [11, 15, 17] \nis precisely to address some of the challenges discussed in Sect. 2.2: storing events in the address \nspace of the target application is not scalable past a few hundred megabytes of trace data, and interferes \nwith memory management, in par\u00adticular with thegarbage collector. The increased capture cost incurred \nby the use of a database is compensated by a better scalability and non intrusiveness. As a side effect, \nthe abil\u00adity to serialize execution traces allows for post-mortem de\u00adbugging, which opens interesting \nperspectives for software companies willing to offer software with high-quality sup\u00adport: overlooking \nthe storage cost, a navigatable execution trace is a far more relevant input for a bug report than an \nad-hoc text description. During execution, the target application emits events that are sent to the \ndebugger core, where they are recorded and indexed in an event database. The way events are emitted is \ndiscussed later. The event database leverages the pecu\u00adliarities of the event stream and the restricted \nset of possi\u00adble queries to provide both high recording throughput and good query performance (see Sect. \n4 and 5). The debug\u00adger core contains another database, the structure database, which contains static \ninformation about the target applica\u00adtion. In particular itkeeps track of the 32-bit integer identi\u00ad.ers \nthat are assigned to structural elements of the target pro\u00adgram(i.e. classes, methods, and .elds). Queries \nperformed bythe user rely on both theevent and the structure databases. 3.2 Representation and emission \nof events We now introduce the representation of events and event traces, as well as how events are emitted \nby a debugged application inTOD. Event and trace model. An event is a structure character\u00adized by a number \nof attributes chosen among the set A = {a0, ..., ak}.We note e.aj the value of attribute aj of event \ne.For each j . [0..k], let Dj be the domain of aj , i.e. the setofall distinctvalues that canbetakenby \naj for anyevent in the trace. An event trace T = (e1, ..., en) is an ordered sequence of n heterogeneous \nevents. The a0 attribute corresponds to the timestamp of the event;itis characterizedby thefact that \n(a) all events have a value for a0, (b) there exists a complete order on D0 and (c) the events in T are \nordered by their value of a0.Table1 shows which concreteevents are captured and what are their attributes. \nEmission of events. The debugger core ofTOD captures events emitted by the target application (Fig. 1). \nThere are threewaysin whichevents canbe emitted: specialized hard\u00adware trace ports [10], virtual machine \nor interpreter instru\u00admentation [16], and application code instrumentation [11, 15]. TOD uses the last \none: although not as fast as hard\u00adware probes and signi.cantly more space-consuming than VM-level instrumentation \nin terms of code size, application instrumentation is also much more portable and easier to im\u00adplement. \nInTOD, the JVM that hosts the target application is set up to use a JVMTI3 native agent. The agent intercepts \nclass load events and replaces the original class de.nitions by in\u00adstrumented versions. Instrumentation \nitselfis performed by the weaver in the debugger core: the agent sends the original bytecode to the core, \nthe weaver instruments the class and 3JVMTI:JavaVirtual MachineTool Interface, partof theJava5platform. \nkind v ts v tid v depthv pev v loc v .d v bid vid idx val v ret tgtv exc args Field write (FW) v v v \nv v v v v Local var. write (VW) v v v v v v v v v Array write (AW) v v v v v v v v Exception (Ex) v v \nv v v v v v v Behavior call (BC) v v v v v v v v v Behavior enter (Bn) v v v v v v v v v Behavior exit \n(Bx) Row headers areevent kinds and column headers are the possible attributes(ts:timestamp, tid:thread \nid, depth:call stack depth, pev:pointer to parentevent, loc:source code location, .d:.eld id, bid:behavior \nid, vid:local variable id, idx:array index, val:value, ret:return value, tgt:target, exc:exception, args: \narguments). Table 1. Events and their attributes. stores structural information in the structure database, \nand the modi.ed class is sent back to the target JVM where it is eventually loaded (Fig. 1). The agent \ncaches instrumented classes on the hard disk to reduce the number of inter\u00adprocess round trips. This \nis particularly useful for frequently\u00adused classes such as those in the JDK. InstrumentationisdoneusingtheASM \nbytecodemanipu\u00adlationlibrary[3]:event emissioncodeisaddedbeforeand/or after speci.c bytecode patterns \nin the original code, such as a .eld writeoramethod call.Whenthe instrumentedcodeis executed, events \nare constructed along with their attributes, serialized in a custom binary format, and sent through a \nsocket to the event database. Non-ambiguous event timing. Althoughevent timestamps are obtained through \nthe nanosecond-precision time service of Java, its potential lack of accuracy makes it is possible for \nseveral events of the same thread to share the same timestamp value. As this is incompatible with the \nevent indexing scheme used by TOD (Sect. 4), we shift original timestamp values a few bits to the left \nand use the free bits todifferentiateeventsofthesamethreadthatsharethe same timestamp. When comparing \nthe timestamps of events of different threads, we use the original timestamps to preserve inter-thread \nevent ordering. Scoped trace capture. The instrumentation scheme de\u00adscribed above is selective, that \nis, it is possible to supply user-de.ned .lters that limit the number of emitted events. This feature \nis described in Section 7. Object identi.cation. The JVMTI agent of TOD assigns a unique identi.er to \neach object in the target application; whenever an event needs to refer to an object it uses this identi.er. \nAdditionally objects whose state cannot be recon\u00adstituted, like String and Exception, are sent in a serial\u00adized \nform the .rst time they are referenced. As an excep\u00adtion to this mechanism objects that represent primitive \nval\u00adues(e.g. Integer, Float, etc.) are passedbyvalue.  3.3 Low-level queries: cursors and counts All \nthe features presented in Section 2.1 (stepping, state reconstitution, control .ow reconstitution and \nroot cause  The current position of the cursor is depicted by the bold line between events4 and 5. Events \nthat match the cursor s predicate are grayed. Suc\u00adcessive calls to next() return events 5, 6, 11 and \n14; calling posNext(11) positions the cursor between events 10 and 11; calling posP rev(11) posi\u00adtions \nit between events 11 and 12. Figure2. Navigation amongevents matchinga cursor pred\u00adicate. operation semantics \nnext()/ Returns the next/previous matching event prev() and moves the cursor forward/backward. posNext(t)/ \nMoves so that the next call to next()/prev() posP rev(t) returns the .rst/last event whose timestamp \nis greater/lesser than or equal to t. posNext(ev)/ Moves so that the next call to next()/prev() posP \nrev(ev) returns the given event. Table 2. Cursor operations. .nding) can be expressed in terms of two \nlow-level queries: cursors and counts, which we introduce below. Both are based on .ltering events in \nthe trace according to some conditions on their attributes. Conditions can be anyboolean combination \nof simple predicates of the form attribute = value, where value is a constant. For instance (kind = FW \n. kind = BC) . target = obj145. If Q is such a condition and e is an event, we de.ne the predicate function \nQ(e) whose value is true iff e veri.es condition Q. Cursors. We de.ne cursor(Q) as an iterator over events \nthat match condition Q (Fig. 2). Cursors have a current position that is situated between two consecutive \nevents (or at the beginning or end of the trace). A cursor supports a numberof navigation operations, \nas showninTable2. Counts. Given a time interval [t1,t2] divided in s slices of length dt =(t2 - t1)/s \neach, and a condition Q on event attributes, a count query returns an array of s integers such that s[i]= \n|{e . T : within(e, t1 + i \u00b7 dt) . Q(e)}| where within(e, t) . e.ts = t . e.ts < t + dt. Each slot of \nthe array contains the number of events matching Q that occur during the corresponding time slice. 3.4 \nHigh-level queries We now explain how cursors and counts are algorithmically combined to implement the \nhigh-level features described in Sect. 2.1. Section 4 discusses the aggressive database optimization \nenabledby using only .ltering-based queries. Stepping. We de.ne stepper as an object that has a cur\u00adrent \nevent ev and supports forward and backward step into and step over operations.For instance, forward step \ninto is de.ned as follows: c . cursor(thread = ev.thread) c.posP rev(ev);ev . c.next() Forward stepover \nchanges the cursor condition to:thread = ev.thread . depth = ev.depth. Backward stepping is sym\u00admetric \nto forward stepping. State reconstitution. Thevalue v ofa.eld f ofaparticular object o at time t can \nbe retrieved as follows: c . cursor(kind = FW . fid = f . target = o) c.posP rev(t);v . c.prev().val \nThe state of an object can be retrieved by performing the same operation for each .eld. Stack frames \nare reconstituted in a similar way, using variable write events instead of .eld write events. Control \n.ow reconstitution. Events that occurred in the top-level control .ow of a given method call event e \nare retrieved as follows: c . cursor(thread = e.thread . depth = e.depth + 1) c.posP rev(e.ts);cflow \n= () repeat ev = c.next();cflow . cflow .(ev) until ev.kind = BEx Root cause .nder. Determining how \na .eld has been as\u00adsignedan undesiredvalueisas directasthe state reconstruc\u00adtion query above: instead \nof obtaining the value of the .eld write event that assigned the value to the .eld, the event it\u00adselfismade \ncurrent,givingaccesstothecontextatthattime. Backward-in-time exploration of the cause can go on like \nthis, up to the root cause.  3.5 User interface components The frontend of TOD can be used standalone \nor as a plu\u00adgin for the Eclipse Java IDE (Fig. 3). The user navigates be\u00adtween different views using \nwidely-understood web browser metaphors (hyperlinks, back button). The available views are: object inspector, \ncontrol .ow, and murals. The object inspector view shows reconstitutions of objects, and allows root \ncause .nding for .eldvalues throughaconvenient why? link next to each .eld. The control .ow view showsa \nrecon\u00adstitution of the control .ow and allows stepping operations as well as root cause .nding for local \nvariable values. Murals. High-level overviews are useful for spotting ab\u00adnormal behavior patterns. However \nrepresenting a huge number of events in a limited number of pixels is dif.cult. Jerding and Stasko introduced \nthe information mural [12] as a reduced representation of an entire information space that .ts entirely \nwithin a display window . TOD features event murals, which are graphs that show the evolution of event \ndensity, or number of events per unit of time, in a given period: Thread murals show the event density \nof each thread for the whole execution of the target application (Fig. 4).  Object activity murals show \nthe density of calls to meth\u00adods of a particular object.  Method murals show the density of calls to \na particular method on anyobject.  In all cases densities are obtained through counts (Sect. 3.3), \nwhere the length of the time slices corresponds to the space occupied by a single pixel bar in the mural. \nThe user can zoom and pan the murals; when the zoom level permits to distinguish individual events the \nuser can select an event and see its context in a stepper view. Thread murals have a variety of applications, \ne.g. to understand the interplay between threads, or spotting dead-and livelocks. 4. High-Speed Database \nBackend Wenow describeand analyzethe database backendofTOD, which allows foref.cient queryexecution while \nbeingfast enough to allow a high recording throughput. Section 5 shows how our solution is amenable to \nparallelization, and Section6reports on actual performance measurements. The need to develop a specialized \ndatabase backend for TODwasmotivatedbythepoor performanceof widely-used database management systems for \nour purposes: for instance Postgres and Oracle only support storing events at a rate of 50 and 500 events \nper second respectively, while we rather aim at rates in the order of hundreds of thousands events per \nsecond [22]. Our high-throughput specialized database backend leverages the following speci.cities of \nthe event stream of an execution trace: (a) the event stream is read\u00adonly, (b) events arrive orderedbytimestamp4 \nand (c) queries are limited to .ltering. Sect. 4.1 describes the indexing scheme used by the database. \nSections 4.2, 4.3 and 4.4 analyze the cost of exe\u00adcuting the queries describedin Sect. 3.3. Finally Sect. \n4.5 an\u00adalyzes the recording throughput that can be achieved by the 4Events of different threads might \narrive out of order because of the way serializedeventsarebuffered;as reorderingthemischeap(onlythelastfew \nevents must be considered) we assume events have been reordered before theyreach the backend. Button(A) \nlaunchesthe program with trace recording.The usernavigatesinthe control.ow(B)using steppingbuttons(C),orby \nclickingonanevent.The line corresponding to the current event is highlighted in the source window (D). \nThe state of the stack frames and current object is shown in window (E). The usercanjumptothe instructionthatsetavariableor.eldtoits \ncurrentvalueby clickingthe why? link next to it. Figure 3. Stepping withTODin Eclipse. The graphs shows \nthe density of events of each thread along a time axis. Figure 4. Thread murals. system and presents \nan important trade-offbetween memory requirements and ef.ciency. 4.1 Aggressive indexingofevents In \nmost database management systems the indexing scheme consists in maintaining one index on attribute value \nfor se\u00adlected attributes. Such an index permits to quickly retrieve the records that have a speci.c value \nfor the indexed at\u00adtribute.TOD adoptsa more aggressive indexing schemein which there is a separate index \non timestamp for each dis\u00adtinct value of each attribute. This enables a highly-ef.cient processing of \nthe cursors and counts queries de.ned in Sect. 3.3, and at the same time permits to sustain a high recording \nthroughput. Using the notation de.ned in Section 3.2 we de.ne the in\u00addex set of trace T on attribute \naj as, in a .rst approximation, a function ISj : Dj .. N* for j . [1..k] so that ISj maps anypossible \nvalue v of aj to an index, which is a sequence ofevent pointers.Apointer i appears in index ISj(v) if \nand only if ei.aj = v, where ei is the ith event of T . Those in\u00addexes can be used directly to retrieve \nall events that match a simple query of the form aj = v;compound conditions are discussed in Sect. 4.2. \n However,TOD queries consistnotonlyin .nding match\u00adingevents,but alsoin .nding matchingevents that occurred \nat, after or before a particular point in time. Therefore indexes contain timestamps in addition to event \npointers. Hence in a second approximation, an index ISj (v) isa se\u00adquence of (ts, i) entries, ordered \nby their value of ts. In such an index an event near a particular timestamp can be retrieved using a \nbinary search. It is nevertheless much more ef.cient to use a B+Tree structure (Fig. 5). Re.ning the \nabove de.nition, the index ISj (v) becomesa hierarchicalindexcomprisingseverallev\u00adels. The index sequence \ndescribed above constitutes level 0. The (ts, i) entries of that level-0 index are stored on the hard \ndisk in small pages, where each page contains a number of entries pertaining to the same index. When \nsuch a page is Figure 5. B+Tree index. full, an entry of the form (ts, pid) is created in the level-1 \nindex: ts is taken from the .rst (ts, i) entry of the recently\u00ad.lled page, and pid is a pointer to that \npage. Level-1 en\u00adtries arein turn accumulatedin pages; whenalevel-1 page is.lled,alevel-2entryis created,andsoon.Thetoplevelal\u00adways \ncontains a single page, called the root page. The num\u00adberoflevels abovelevel0of anindexis called the \nheight of the index. In such a structure the number of page accesses necessary to retrieve an event near \na given timestamp is at most the height of the index. Storage requirements Experiments show that the \naverage size of an event is IeI = 38 bytes. The size of a level-0 entry is I(ts, i)I = 16 bytes (two \n64-bits integers). The size of upper-level entries is I(ts, pid)I = 12 bytes(pid is only 32 bits). The \nexperimentally-determined optimal page size is P = 4096 bytes, therefore level-0 index pages contain \n256 entries, upper-level index pages contain 341 entries and event pages contain 108 events in average. \nThe height h of indexes is logarithmic with respect to the number of entries andin practiceneverexceeds5(anindexofheight5allows \nfor 3415 4 \u00b7 1012 entries). The amount of index data generated for each event is actually greater than \nthe event itself. In our experiments we found that in average an event plus the associated index data \noccupies 190 bytes of storage, although the event itself occupies only 38 bytes.  4.2 Cost of event \nretrieval We now present the algorithms that permit to retrieve events matching an arbitrary predicate \nin linear time with respect to the size of the involved indexes. The algorithms are for timestamp-order \nretrieval; reverse-timestamp retrieval has the same cost. Single-term conditions. Forasimple conditionoftheform \naj = C where C is a constant, we can retrieve matching events ordered by timestamp simply by obtaining \nthe (ts, i) entries from Ij(C).Ifthe actualeventis needed(i.e. for cur\u00adsors), it is directly retrieved \nfrom the trace as ei;otherwise (i.e. for counts) the event does not need to be accessed. In any case, \nall entries can be retrieved in linear time, as the index is simply scanned once. Conjunctiveconditions. \nForaboolean conjunctionofsim\u00adple conditions of the form aj1 = C1 . ... . ajm = Cm, we Algorithm1 MERGE-JOIN \nmerge-join(S, j1,...,jm,C1,...,Cm): result .\u00d8 forl =1 to m do index[l] . Ijl (Cl), pos[l] . 1 while \nthere are more elements do match . true, refI .-1 minL .-1, minT S . +8 forl =1 to m do (curT S, curI) \n. index[l][pos[l]] if refI = -1 then refI . curI else if curI then = refI match . false if curTS < minTS \nthen minT S . curT S, minL . l if match then result . result .{srefI }pos[minL] . pos[minL]+1 use a \nvariant of the sort merge join algorithm [1], widely\u00adused in database management systems, to identify \nmatching events without accessing them (Algorithm 1): we obtain the Ijl (Cl) forevery simple condition, \nand for each we maintain a pointer to a current (tsl,il) entry. Then we loop: at every step we check \nif all of the il are equal, in which case we addanyofthe current entriestothe result:thefactthatthey \nall point to the same event means that the event matches all conditions. Then we advance the pointer \nof the index whose current entry has the minimum value of ts. As each index is scanned only once and \nthere is no nested loop, merge join runs in linear time with respect to the sum of the sizes of the considered \nindexes. Generic boolean conditions. The above can be general\u00adized to any compound boolean condition, \nby performing a merge join for each conjunction and a regular merge (the merging step of merge sort)for \neach disjunction. The cost thus remains linear with respect to the sum of the sizes of the considered \nindexes. Because both merge join and regular merge are stream operators(i.e. theyproduce an output tuple \nas soon as theyhave received enough input tuples, without needing past or future input tuples), it is \npossible to pipeline them so that no intermediate results have to be stored. 4.3 Cost of cursors Cursors \nsupport retrieving matching events in forward or backward timestamp order, and absolute positioning by \ntimestamp. Given a compound .ltering condition, one in\u00addexis used for each simple condition component.Apointer \nto a current entry is associated to each index and the merg\u00ading algorithms described above are applied, \nincrementing or decrementing the pointer of each index as dictated by the desired retrieval order. The \ncost of retrieving succes\u00adsive matching events is extremely variable depending on the number of components \nof the condition and the density of matching events. Algorithm2 FIND-POSITION .nd-position(I = Ij (v), \nt): page . root(I) level . height(I) while level > 0 do (ts, pid)= binarySearch(page, t) page . getP \nage(I, pid) level . level - 1 (ts, i)= binarySearch(page, t) return i For absolute positioning, we reposition \nthe pointer of each index so that the next timestamp of the entry is im\u00admediately before or after the \nspeci.ed timestamp. This is achieved by performing a binary search of the given times\u00adtamp at each level \nof the index, starting by the root (Algo\u00adrithm 2). The number of page accesses needed by this oper\u00adation \nis at most equal to the height of the index, and can be lessif some pages are foundin the pagebuffer. \n 4.4 Cost of counts The counts queries retrieve the number of matching events in every time slice of \nlength dt of a given interval. There are two ways these counts can be obtained. Merge counts. The simplest \nway is to use the merging al\u00adgorithms described previously: whenever a (ts, i) index en\u00adtry corresponding \nto a matching event is found, the count of the time slice containing ts is incremented, without needing \nto fetch the actual event. This method works for arbitrary compound conditions but can be very costly \nif counts are required over a large interval. Fast counts. In some cases we can leverage our hierarchi\u00adcalindex \nstructureto obtain countsatamuchlower cost.Al\u00adthough this optimization applies only to simple conditions, \nit is useful e.g. to compute thread murals of the whole execu\u00adtion trace.Its scope canbeextendedif indexesof \ncompound conditions are materialized(i.e. a new index is generated that references events that match \nthe compound condition), a topic we do not address here. The number of time slices requested in a counting \nquery usually does not depend on the sizeof the intervalbut rather on the number of pixels of the debugger \nfrontend window (Sect. 3.4). Therefore when counts are requestedovera large interval, each time slice \nis also large. Because a higher\u00adlevel index entry is created when a lower-level page is full (Sect. 4.1), \nwe can know the number n of level-0 entries that are between two level-l entries for l> 0: l-1 I(ts, \ni)II(ts, pid)I n = \u00b7 PP Given twoconsecutivelevel-l entries (ts1, pid1) and (ts2, pid2) of index Ij(C) \nwe know that n events matching aj = C oc\u00adcurred between ts1 and ts2. This information can then be used \nto provide average counts at a reduced cost. The in\u00addex levels to use are determined by the ratio between \nthe requested time slice length dt and the interval ts2 - ts1 between successive entries in each level. \nNote that various levels can be used during the execution of the same request, taking into accountvariationsinthe \ndistributionof matching events: if the time between successive entries in level l is larger than dt we \ndrill down into level l - 1, and conversely we roll up to level l +1 if the time interval is too short. \n 4.5 Cost of indexing The above sections show that the indexing scheme of the database allows for ef.cient \nquery execution. It remains to show that indexes can be created ef.ciently so as to allow a high recording \nthroughput. This section shows that this can be achievedby carefully tuning memory requirements. For \neach event that enters the database there are at most k = |A|- 1 indexes to update (as there is no separate \nindex on a0). Experiments indicate that on average k = 10. Given that events arrive in order with respect \nto a0, it is not necessary to use the costly B+Tree insert method for updating an index. Instead, the \nmuch cheaper bulk load method is used, which consists in appending an entry at the end of the current \nlevel-0 page, and at the end of higher\u00adlevel pages whenever a lower-level page is .lled. The I/O and \nmemory costs of this operation are as follows: Ifthe currentpageofeachleveloftheindexcanbekept in memory, \nan I/O cost is incurred only when a page is .lled.Theaverage numberofpage accessesper incoming event \nis: IeI + k \u00b7 (I(ts, i)I + A) c 0.05 P where A is the contributionoflevel1and above: h-1 i I(ts, i)II(ts, \npid)I A = \u00b7I(ts, pid)I\u00b7 PP i=0 If only the current level-0 page of the index can bekept in memory, when \na page is .lled it must be written, the current level-1 page read, updated and written back to disk. \nThe contribution of higher levels become: h-1 i I(ts, i)II(ts, pid)I A = \u00b7 2P \u00b7 PP i=0 The average number \nof page access per incoming event is then 0.13. If no index page can be kept in memory, every update \nimplies the three operations above, giving 2 \u00b7 k = 20 accesses per event. In order to achieve a high \nrecording throughput it is there\u00adfore crucial to minimize the number of page accesses per in\u00adcomingevent:atleastonepageperindex \nshouldbekeptin memory so as to avoid the last situation, which is 150 times more costly than the second \nsituation above. The numberof indexesvaries from7forthe kind index set to more than8 millions for the \nobject indexsetinanexecution traceof720 millionevents. Figure 6. Number of indexes for each index set \nin an Eclipse execution trace. Memory requirements The memory requirements of the system depend on the \nnumber of indexes to maintain, which in turn depends on the size of the domain of each attribute. Figure6showsthe \ndomainsizeofeach attributeas observed with a large execution trace of an Eclipse session (720 mil- Index \nset splitting Number of indexes Entries per index Query cost No d N v d B/d B/N v d B/d Yes . . . . . \n. N v d . . . B/N v d N\u00b7B Nv d  Effect of index set splitting on the number of indexes, number of entries \nper index, and query cost. N is the number of split components and B is the total number of entries in \nthe index set. Table 3. Index set splitting. value component. As shown in Sect. 4.2, the ef.ciency of \nqueries is proportional to the size of the involved indexes. Table3 summarizes the slowdown incurredby \nsplitting an index set containing d indexes and totalling B entries. Each index in the set contains on \naverage B/d entries, thus the cost of a query on one of those indexes is proportional to B/d. If the \nindex set is split the number of indexes per index v set becomes d. = N d and there are on average \nB/d. en- N tries per index. The cost of the query becomes proportional \u00b7 d1- 1 to N \u00b7 B/d., yielding \na slowdown of N \u00b7 d/d. = N . For instance, withd = 107 and N =2, the slowdownwould lions events). The \ndomain of object ids largely dominates all other domains, reaching almost 10 millions distinct val\u00ad ues. \nMaintaining the corresponding indexes would require be approx. 6, 300. Although this might seem prohibitive, \nit P \u00b7 107 = 40GBofbuffer space, whichis nota reasonable is important to note that the index sets that \nare subject to .gure.Asolutiontothisproblemistosplittheindexsetsof splitting have domains orders of magnitude \nlarger than other large attributes, as explained below. index sets (Fig. 6), thus each individual index \nis small com- Index set splitting As maintaining millions of indexes is not practically feasible, we \ndevised a strategy that permits to trade memory requirements for recordingthroughput and querying ef.ciency: \nattributes with large domains are split into components that are indexed separately. Let aj be an attribute \nand d = |Dj| the number of distinct values it can take (hence d is also the number of indexes in the \ncorresponding index set). Assuming that all the distinct values are the .rst d positive integers which \nis always the case in practice , any value v of Dj can be represented in binary by n = log2(d) bits. \nSuch a value can be split into N components of n/N bits each, and instead of having a single index set \non attribute aj there are now N index sets, one for each component. The number of indexes to maintain \nv pared to the indexes of non-split index sets. As in practice most queries are compound and involve \nboth split and non\u00adsplit indexsets, the contribution of split indexsets to the total cost of the query \nis reasonable. To illustrate this, let us consider the state reconstitution query of Sect. 3.4, which \nis based on a conjunction of con\u00additions on the .eld id and object id attributes5. In the Eclipse trace \npreviouslymentioned there are about 10, 000 distinct .eld id values and 10, 000, 000 distinct object \nid values (Fig.6).Witha trace containing B events, assuming a uni\u00adform distribution of .eld id and object \nid values, and as\u00adsuming that every event has a value for both attributes, each index on .eld id would \ncontain B/104 entries; each index on object id would contain vB/107 entries without index set \u00b72n/N becomes \nN = N \u00b7 N d instead of d,yieldingadramatic splitting, and approx. B/ 107 = B/3, 160 with index set \nreduction of memory requirements, even for N as low as 2.For instance with d = 107, corresponding to \nthe size of the object id domain, the memory requirements using index set splitting with N =2 would be \nreduced from 40GB to 25MB. Index set splitting therefore implies huge reduction of memory requirements. \nLet us now assess the impact of this technique on ef.ciency.For recording, the number of index updatesis \nmultipliedat mostby N. Given that not all events havevaluesforsplit attributes,the actualslowdownislower. \nFor querying, booleanexpressionsinvolving split indexes are replaced by a conjunction of N conditions, \none for each splitting and N =2. Therefore the actual slowdown of index set splitting for the compound \nquery is: cost with splitting 1/104 +2/3160 = 7 cost without splitting 1/104 +1/107 Despite this slowdown, \nstate reconstitution queries ex\u00adecute fast enough to be used interactively in the debugger frontend, \nas will be shown in Sect. 6.2. 5The kind = FW part of the query is omitted in practice because only Field \nWrite events have a value for the .eld id attribute. Figure 7. Architecture of the distributed database \nbackend. 5. ScalingUp witha Debugging Cluster The ef.cient indexing and retrieval techniques used in \nthe event database of TOD can bene.t from parallelization. This sectionshowshowTOD supportsadistributed \ndatabase backend, allowing its ef.ciencyto increase linearly in terms of the number of nodes, within \ncertain limits. 5.1 Distributed Architecture The architectureof the distributed backendofTOD consists \nof three layers (Fig. 7): A dispatcher that receives the events from the target program and distributes \nthem to a number of database nodes. The dispatcher maintains a local sending queue for each connected \ndatabase node. A receiving thread reads each incomingeventand forwardsittothe smallest queue, so as to \nachieve proper load balancing between database nodes.  A number of database nodes, each of which receives \na subset of all generated events. Theyare individually able to index events and process queries in the \nsame way as the non-distributed backend described in Section 4. No change to the indexing structure is \nnecessary.  Aqueryaggregator that receives queries from the debug\u00adger frontend, passes them to each \ndatabase node and ag\u00adgregates the results before returning them to the frontend.  Note that neither \nthe structure database nor the weaver men\u00adtioned in 3.1 need to be parallelized as their processing and \nstorage requirements are modest.  5.2 Scalability Parallelization Both event recording and query process\u00ading \nare embarrassingly parallel problems, that is to say, their parallelization is straightforward because \nno special coordi\u00adnation is required between the parallel tasks. In particular, queries do not need to \nperform any kind of joins between events. All database nodes can perform the same query in\u00addependently \nand then send their results to the aggregator, which is able to merge them ef.ciently. Furthermore, \ncursors and counts have very light process\u00ading and bandwidth requirements onthe aggregator, enabling \nexcellent scalability properties: Parallel cursors. When the aggregator receives a cursor query with \n.ltering condition Q it requests a similar cursor to each database node and returns an aggregat\u00ading cursor \nto the client. In the same way regular cursors merge entries from various indexes, the aggregating cur\u00adsor \nmerges events from each of its base cursors using the regular merge algorithm from merge sort. Parallel \ncounts. The aggregator obtains partial count results from each node and simply returns a new counts array \nwhere the value of each slot is the sum of the values of the corresponding slot in each partial result \narray. Scalability limits The throughput of this architecture is theoretically linear in terms of the \nnumber of database nodes. Howeverthe scalabilityisin practice limitedby oneof these factors: the dispatcher \n(resp. aggregator) can act as a bottle\u00adneck for recording throughput (resp. query processing), or the \nnetwork link bandwidth can be saturated. In our current implementation, the actual bottleneck is the \ndispatcher, as reported in details in the following benchmarks. 6. Benchmarks This section reports on \na .rst set of benchmarks evaluating different aspectsofTOD.In particular,we .rst measurethe overhead \nimposed on a running application debugged with TOD, and then report on the ef.ciency and scalability \nof the distributed database backend, both in terms of recording throughput and query evaluation. 6.1 \nTrace capture overhead Capturingtheexecution traceofadebugged program causes a signi.cant runtimeoverhead.We \nmeasureditintwo dif\u00adferent scenarios: A fully-instrumented, CPU-intensive toy program de\u00adsigned to represent \na worst-case situation, in which the debugged applications emits events at a rate as high as the CPU \ncan handle;  An interactive Eclipse session re.ecting a real-world sit\u00aduation, in which the JDK classes \nare not instrumented (partial traces are further discussed in Sect. 7), and in which the interaction \nbetween the user and the debugged application entailsthattheevent emissionrateisless sus\u00adtained in time \nthan in the worst case above.  In these experiments only the event emission overhead causedbyTODis \nmeasured, not its database performance. Therefore events are simply written to disk, without anyin\u00addexing. \nBoth benchmarks were conducted ona PentiumM 2GHz notebook with 1GB of RAM running Linux kernel 2.6.17 \nand the Sun 1.5.0 08 JVM. Setup RAM time emit. rec. rate ovh. None 16 1.53 - - - 1 ODB1 500 179 110m \n5m 614 116 ODB2 64 188 110m 530k 585 122 TOD 16 173 90m 90m 520 113 The RAM column is the JVM heap size \nin MB. The time column is the execution time in seconds. The emit. and rec. columns indicate the number \nof events emitted, and recorded (available to the debugger). The rate column is the recording throughput, \nin kEv/s. The ovh. column is the overhead compared to the standalone execution. Table 4. Overhead of \nevent emission. Worst-case scenario WeuseaCPU-intensiveprogram that creates 100 Object instances and \nthen iterates 10 million times in a loop taking one of these objects at random and passing it to a method \nthat performs a simple arithmetic operation on its hash code. The program does not call any non-instrumented \nmethod. Therefore, every execution step emit events, so the event emission rate is bounded only by the \nCPU speed. We compare the execution time of this program running (a) standalone, (b) withTOD and (c) \nwith the ODB [15] om\u00adniscient debugger forJava. Results are presentedinTable4. With ODB, events are storedin \nthe JVM heap of the target program; oldevents are discarded when the heapis full.We therefore conducted \ntwo ODB tests, varying the JVM heap size: with 500MB of heap we were able to record5million events out \nof the 110 million emitted during program execu\u00adtion, while with 64MB we could record only 500,000events. \nOntheotherhandwithTODwe wereabletorecordallemit\u00adted events6 without interfering with the JVM heap. In \nspite of the heavier processingin the caseofTOD whereevents are serialized and written to disk rather \nthan simplykept in RAM the overhead imposed on the application execution timeis similarinTOD and ODB: \naround 115 times the cost of standalone execution. The execution trace generated by TOD weighs in at \n3.6GB. Eclipse session This experiment consists in performing a sequence of actions in the Eclipse IDE, \nwith and without trace capture. Note that only the classes of the Eclipse IDE are instrumented, not those \nof the JDK (Sect. 7). The actions performed are: creation of a new project, creation of a few classes, \nedition of their source code using auto-completion and other productivity features, execution of a rename \nrefactoring, and step-by-step execution of the created program under the Eclipse integrated debugger. \nThe following quantitative observations can be made: The Eclipse session is 10 times slower with trace \ncapture enabled: it takes 244s (4 min.) without trace capture and 2324s (38 min.) with trace capture. \n6The different numbers of emitted events between TOD and ODB are apparently due to differences in the \ntrace model. The recorded execution trace comprises around 720 mil\u00adlion events and weighs in at 33GB. \nThe average event emission rate is 313kEv/s, 40% less than the worst-case scenario presented above. \nOn the qualitative side, this experiment shows that: The start-up time of Eclipse is greatly augmented \nwhen trace capture is enabled, due to the loading of instru\u00admented classes (which are roughly 3 times \nbigger than non-instrumented classes).  The Java source editor remains interactive for typing, although \nthere is a noticeable slowdown.  Some operations, such as invoking auto-completion, gen\u00aderating constructors \nor getters, or stepping with the de\u00adbugger,are signi.cantly slower with trace capture,but at a tolerable \nlevel.  Even though using TOD implies a perceptible slowdown of the debugged program, we believe that \nthe bene.ts of omniscient debugging in quickly pinpointing hard-to-.nd bugsfar outweigh this inconvenience. \n 6.2 Database performance To evaluate the performance of the distributed database of TOD we conducted \nmeasurements of recording throughput and query performance against the captured Eclipse trace of Sect. \n6.1. Recall that the database performance is crucial to the debuggingexperience: (a) trace recording \nshould ideally be in real time so that it is possible to start a debugging session as soon as the debugged \nprogram terminates (or reaches some determined state), and (b) the database must process queries intimes \ncompatible with human interaction so that the navigation interface is responsive. Cluster setup. TheTOD \ndistributed trace databasewas de\u00adployed for these experiments on a dedicated cluster consist\u00adingof10 \ndatabase nodes (3GHz Intel Pentium4with Hyper-Threading disabled, 1GB of RAM, Sun JDK 1.5.0 08) and one \ndispatcher node (2.13GHz Intel Core2, 2GB of RAM, Sun JDK 1.6.0). The nodes are connected through a Giga\u00adbit \nEthernet switch,but only the dispatcher node hasa Gi\u00adgabit link; the database nodes have a 100Mbit/s \nlink. Each database node has a partition with 38GB of free space on a 80GB 7200RPM SATAhard drive. Recording. \nThe .rst experiment consists in determining the maximum throughput achievable by the dispatcher, with \nevent storage and indexing disabled in the database nodes. AsshowninFig.8,inthesetupwithonlyone databasenode, \nthe recording throughput is limited to 250kEv/s, which cor\u00adrespondstothe limitationimposedbythe 100Mbit/snetwork \nlink.With morethanonenode,the dispatcherisabletohan\u00addle up to 470kEv/s, regardless of the number of nodes. \nThis represents around 20MB/s of outgoing network traf.c on the dispatcher, whichislowerthanwhatis achievablewithaGi\u00adgabit \nlink: surprisingly the bottleneck of the dispatcher is the Figure 8. Dispatcher throughput. CPU. Pro.ling \nshows that most of the time is spent copying buffers in methods of thejava.io framework.We assume that \nthis issuewould disappearin an optimizedC versionof the dispatcher. Fig. 9 shows the evolution of recording \nthroughput as eventsareaddedtothe database.Inaverage,asingle database node is able to handle around 54kEv/s, \nand with 10 nodes we reach the dispatcher limit with 470kEv/s. The through\u00adput of 54kEv/s of a single \nnode translates to around 10MB/s of disk writes. This is lower than the maximum throughput of the disks \nthat were used (around 40MB/s), and again the bottleneck is the CPU: most of the time is spent in methods \nof DataInputStream marshalling and unmarshalling prim\u00aditivevalues. Note that with less than4nodesitis \nimpossible to record the whole trace due to disk space constraints; there\u00adfore the following benchmarks \nconsider scalability starting at4nodes. Stepping queries. Figure 10 shows the ef.ciency of the step into \nand step over queries (Sect. 3.4). These results are obtained by starting a stepper at a random timestamp \non each of the 350 threads of the recorded Eclipse session and performing 100 step operations. It is \nclear that step into queriesarefasterthanstepover queries,duetothefactthat the former translate to a \ncursor query on thread id while the latter additionally use the call stack depth. The ef.ciency of both \nstep queries surprisingly decreases as more database nodes are used; we are currently investigating this \nissue more thoroughly. In any case, step queries arefast enough to be used interactively, since they \nexecute in less than a hundred milliseconds in the worst case. Object reconstitution queries. The ef.ciencyof \nobject re\u00adconstitution queries is measured as the time taken to recon\u00adstitute the state of random objects \nof the Eclipse execution trace at different points in time. Figure 11 shows that these queries scale \nwell withthe numberof nodes.Onaverage,in\u00addividual .eld values are retrieved in 120ms to 350ms. The time \nto reconstitute a full object is directly proportional to 1 7444 233 31.9x 1.54 2 4062 222 18.3x 1.11 \n8 1206 120 10.1x 0.18 10 1114 117 9.5x 0.21  The merge and fast columns indicate the average query execution \ntime using two counting methods. The speedup column indicates how much faster is the fast method. The \ndist. column is the distortion of the fast method compared to the exact merge method. ni Table 5. Comparisonof \nmerge andfast count queries. ni the number of its .elds, thus the time to reconstitute an ob\u00adjectof7 \n.elds(anaverage number)is comprised between 0.8sand2.4s.Notethattheobject inspectorwindowofTOD updates \nasynchronously, so that the user is not blocked until the state of the current object is fully reconstituted. \nCount queries. We measured the execution speed of count queries and compared the two counting methods \ndescribed in Sect. 4.4: merge counts and fast counts. We requested the event counts for each of the 350 \nthreads of the Eclipse execution trace, on the entire time span of the trace and divided in n =1, 000 \nsubintervals. A comparison of the two count techniques is shown in Table5.Fast counts perform10to30 timefaster \nthan merge counts while providing a very precise approximation, with a distortion7 below 2%. Figure 12 \nshows that merge counts scale very well but fast counts less so, because as each node records lessevents,thefast \ncount algorithm must more frequently resort to lower-level indexes. 6.3 Summary Figure 13 summarizes \nour experimental results regarding trace capture and recording: the rate of event emission varies from \n313kEv/s for a partially-instrumented interac\u00adtive Eclipse session to 520kEv/s for a fully-instrumented \nCPU-intensive program; the recording throughput boasts a perfect scalabilityupto8nodes,and reaches 470kEv/swith \n10 database nodes, where it is limited by the dispatcher bot\u00adtleneck. It is therefore possible to record \nexecution traces almost in real time. Count queries display good scalability, while step queries scale \npoorly. Still, the database is able to execute queries at a speed compatible with interactive navigation. \nAs a bottom line, although the results presented in this section could with no doubt be further enhanced \nthrough various optimizations, they already represent a consequent improvement over other existing implementations \nof omni\u00adscient debuggers.TODis practically usable today,even on large traces produced by complex applications. \nPP 7Calculated as: ( =1 merge[i] =1 |merge[i] - fast[i]|)/  Figure 9. Recording throughput.    7. \nWorking withPartialTraces AlthoughTODis designedto supporthugeexecution traces, it is not always practical \nto record each and every event: the runtime overhead of event capture is important (Sect. 6.1), and so \nis the storage requirement. The idea of partial traces istoleveragethefact that duringthedevelopmentofa \npiece of software, some components are trusted, i.e. mature and well-tested, and it may not be necessary \nto generate and store events for the inner activities of these components. This section shows how scoped \ntrace capture canfacilitate debugging and how TOD makes it possible to work with partial traces. 7.1 \nMotivating example: debugging theTOD Eclipse plugin Let us consider as an example the debugging of the \nTOD Eclipse plugin itself. This example is fairly representative of componentdevelopmentforexisting, \ntrusted, frameworks or plugin architectures. Here, we might be interested in two typesofbugs:thosethatare \ninternaltothepluginandthose that relate to the interaction between the plugin and the plat\u00adform. In the \n.rst case, we do not need to capture events that occur within the Eclipse platform because it is considered \ntrusted.Inthe second case,wehaveto recordeventsthatoc\u00adcur within the platform, but not necessarily all \nof them: it might be enough to record events of the Java tooling (JDT), or only of some part of it, for \ninstance the UI. Figure 14 shows the impact of different trace scoping strategies on both the number \nof emitted events and the runtimeoverheadof trace capture, duringdifferent phasesof theexecutionoftheTOD \nplugin.Inthis smallexperimentwe see thatby appropriately scoping the trace capture, there are up to .ve \norders of magnitude of difference in the number of emitted events (Fig. 14a), and that thegains in runtime \noverhead can be up to 20 times (Fig. 14b). 7.2 Dealing with missing information Working with partial \ntraces greatly enhances the applicabil\u00adity ofTOD,butit implies that some information is lacking to reconstruct \nthe whole history of the debugged program. It is therefore important that TOD systematically reports \non missing information so that the user can soundly reason about the presented information. Missing information \nmani\u00adfests in control .ow and state reconstitution.  (b) runtime overhead The measures are taken after \nthe following execution phases are passed: the IDE starts up; theTOD launch con.guration dialog is opened; \nthe target program is run; the control .ow view is opened; events are navigated step by step; and the \nIDE exits. Figure 14. Emitted events and runtime overhead using scoped capture. Control .ow reconstitution. \nWhen non-instrumented code is called from instrumented code, and in turn calls instru\u00admented code, some \ncontrol .ow information is lost. Such a case is illustrated in Fig. 15. The code in Fig. 15a calls a \nnon-instrumented JDK method(Collections.sort) from an instrumented one (the main method). The sort method \nin turn calls the instrumented Comp.compare method,but indirectly (through the sort and mergeSort methods \nof Arrays). In Fig. 15b the small dots indi\u00adcate that control .ow information is missing. In the ab\u00adsence \nof such an indication the user might think that Comp.compare was called directly and was the only method \ncalled by sort, which is not the case. State reconstitution. If a class has a non-private .eld that is \nwritten to by non-instrumented code, the value of this public class Comp implements Comparator<String> \n{public int compare(String s1, String s2) {return s1.compareToIgnoreCase(s2); } public static void main(String[] \nargs) { List l = new ArrayList(); l.add(\"A\"); l.add(\"B\"); Collections.sort(l, new Comp()); }} (a) Code \nexcerpt  (b) Control .ow view Figure15. Materialization of incomplete control .owinfor\u00admation. .eld \nat a given point in time cannot be determined accu\u00adrately.TOD represents these .eldsina distinctive color \nin the corresponding views. Again, without such a warn\u00ading the user might not be able to reason accurately \nabout the program.  7.3 Specifying partial traces Partial traces are supported by means of mechanisms \nsimi\u00adlar to those of partial behavioral re.ection [27]: both spatial and temporal selectionofevent generation.For \nspatial scop\u00ading,TODsupports class selectors, which are predicates on classes that should generateevents(e.g. \nclasses of a certain set of packages).8For temporal scoping,TOD supports dy\u00adnamic activation of event \ngeneration, either globally or per thread, through a simple API. This is particularly useful in situations \nwhereabug occurs aftera long running time, or under speci.c dynamic conditions (which may for instance \nbe related to control or data .ow properties). Implementation In our current prototype event emission \ncode is woven with the original application code at load time. As a consequence, the spatial scope of \nevent emission is .xed for the whole debugging session.9Temporal scoping is achieved by a .ag check in \nevent emission code, so there is still very light runtime overhead when event emission is disabled at \nruntime, compared to non-instrumented code. 8It would be possible to re.ne spatial scoping using operation \nselec\u00adtors [27], enabling expression-level selection to further reduce the size of theexecution trace.This \nfeaturehasnotyetbeenintegratedinTOD. 9Although recent JVMs allow classes to be modi.ed at runtime, we \ndo not yet use this feature. 8. Related work TheworkonTOD relatestodifferent areas. Omniscientde\u00adbuggingof \ncourse,butexecution trace recordingis also used in a broader range of program understanding approaches, \nin particular query-based debugging and pro.ling. Tech\u00adniques improving the ef.ciency of program understanding \nhave been proposed in several areas like pro.ling and de\u00adbuggingof distributed application.We also discusshow \nour work on the database of TOD relates to general database techniques. Omniscient debugging. Three proposals \nof omniscient de\u00adbuggers are relatedtoTOD. ZStep95isareversible stepper for Lisp. In addition to the \nstandard features of omniscient debuggers, ZStep 95 provides animated views of data struc\u00adtures of the \ndebugged program. It provides excellent solu\u00adtions to the cognitive issues of debuggingbut does not ad\u00address \nperformance and scalability. More recently, Bil Lewis proposed an omniscient debugger for Java, ODB [15], \nand Hofer et al. implemented a similar system for Smalltalk, called Unstuck [11]. Thework onTODwas actually \ninspired by the omniscient debugger of Bil Lewis. It has the ability tonotonlynavigatetheexecutionhistorybut \nalsoto restore the state of the program as it was at anygiven point in time, sothatitsexecutioncanbe \nresumedatthatpoint.Eventsare stored in the target program s address space, which has seri\u00adous limitationsin \ntermsof scalabilityand potential in.uence of the debugger on the behavior of the debugged application. \nFor scalability, ODB makes it possible to set a .xed limit on the numberofevents that canbekeptin theexecution \ntrace; olderevents are discarded.TOD provides much better scala\u00adbility,as demonstrated in this paper. \nFurthermore, both ODB and Unstuck lack the high-level overviews that are provided byTODin murals. CodeGuide \n[21] is a commercial development envi\u00adronment for Java that features a back-in-time debugger. Breakpoint-based \ndebugging can be combined with trace\u00adbased, bi-directional stepping. The trace is however limited to \nthe last few thousands events, and the important feature of root cause .nding is not available. High-level \noverviews are also not provided. Query-based debugging. Query-based debugging consists in identifying \nevents that match a query expressed in a high\u00adlevel language. Queries can be formulated a priori (be\u00adfore \nrunning the program) or a posteriori (after the pro\u00adgram has been executed). In Hy+ [4] a-posteriori \nqueries are expressed in a graphical language and deal with dis\u00adtributed computations. PQL [19] provides \na very high-level and powerful a-priori query model. Whyline [13] guides the programmer by proposing \na set of possible a-posteriori queries.TheTQuel languageallows programmerstoexpress a-priori queries \ndeclaratively, providing explicit support for temporal queries [25]. LeDoux and Parker [14] formulate \na-posteriori Prolog queries on the execution of concurrent Ada programs. Opium [7] uses Prolog queries \nto debug Pro\u00adlog programs and seamlessly supports breakpoint-based de\u00adbugging and trace-based debugging. \nCoca [6] uses a-priori Prolog queriestodebugCprograms.Inan upsidedownap\u00adproach the Mercury Declarative \nDebugger [18] asks the user questions about the correctness of computations performed by the program \nso as to quickly locate incorrect ones. The limited class of queries supported by TOD is suf\u00ad.cient \nfor the features of omniscient debugging, but scal\u00adability and ef.ciency come at the expense of a much \nless expressive query model than those provided in the above ap\u00adproaches. Although inTOD basic queries \ncan be combined algorithmically, queries that relate several events cannot be executed ef.ciently. Trace \nreduction. One of the priorities of pro.ling is to re\u00adduce the performance impact of the tool on the \ntarget ap\u00adplication. One technique consists in reducing the trace via clustering [20, 28], which also \nreduces the overhead of cap\u00adture, although at the price of a loss in precision. Debug\u00adging distributed \napplications bene.ts from high-level trace recording, as only message sends between nodes need be recorded \n[20, 4]: useful views of the computations can be provided with much less information than that used in \nom\u00adniscient debugging. Opium [7] lets the user specify pre\u00ad.ltering predicates, that by .ltering out \nuninteresting events permit to reduce the number of context switches between the debugged process and \nthe debugger. Mercury [18]bydefault only records events up to a certain call depth; if more detail is \nneeded relevant goals are automatically re-executed. With TOD we took the opposite approach, providing \na scalable event database that copes with huge execution traces.HoweverTOD also providesa mean for reducing \nthe size of the execution traces by letting the user select which parts of the program emit events. Replay-based \ndebugging. Back-in-time debugging can be achievedby replaying the debugged program until some de\u00adtermined \npoint before the current execution point. Igor [8] and Bdb [2] make use of periodic state saving, or \ncheck\u00adpoints, to reduce the time needed to reach a particular past execution point: execution is resumed \nat the last checkpoint preceding the desired execution point. The main advantage of replay-based debugging \ncompared to trace capture is the lower runtime overhead (around 2x for Bdb and 4x for Igor, versusa maximumof \n115x forTOD).However, backward debugging moves can be slow, especially when going far away from the current \nexecution point. Furthermore, if the execution point is moved backwards, moving it again for\u00adwards means \nre-executing the program, which is not practi\u00adcal for long-running programs.WithTOD the entire history \nof the program is available and freely navigatable. Acrucial issue of replay-based debugging is that \nof deter\u00administic replay: system calls that rely on external resources such as network connections might \nreturn different results at different times. Bdb [2] and Jockey [23] address this by recording the results \nof non-deterministic system calls and reinjecting them into the program when it is replayed. How\u00adever \nthis is a brittle solution as manysystem calls must be handled in different ways. Database techniques. \nFinally, the importance of physical data layout in the ef.ciencyof several relational data index\u00ading \ntechniques has been shown in [1]. Seshadri et al. [24] present query plan optimization techniques for \nsequential databases, a superset of execution trace databases like ours. Stonebraker et al. [26] makea \nstrong pointinfavorof spe\u00adcialized database management systems for speci.c applica\u00adtions. Ourwork onTOD \napplies classical indexing and pag\u00ading techniques in a domain-speci.c manner, leveraging the very speci.cities \nof execution traces. 9. Conclusion Assuming the great potential of omniscient debuggers in al\u00adleviating \none of the most tedious and costly part of software development, this work shows that it is realistic \nto provide omniscient debuggers in modern development environments if appropriate measures are taken \nto address the associated ef.ciency, scalability, and usability issues. Wehave presentedTOD,aTrace-OrientedDebuggerfor \nJava, which contributes to the scalability of omniscient de\u00adbugging at three levels:(a) at the trace \ngeneration level, by relying on an ef.cient ad-hoc weaver providing selective emissionofevents encodedina \nconcise binary format; (b) at the storage and query level, by proposing a specialized dis\u00adtributed database \nwith an optimized indexing scheme; and (c) atthe user interfacelevel,byproviding specialized inter\u00adface \ncomponents, in particular murals, which ease the inter\u00adactiveanalysisofhugeevent traces,and visual feedbacksup\u00adportingthe \nuseof partial traces.The scalabilityofTODhas been shown by giving both a complexity analysis of the in\u00addexing \nand querying algorithms, andby reporting on bench\u00admarks of the actual prototype. There are several promising \ndirections for experimenting with other techniques enhancing the applicability of omni\u00adscient debuggers. \nFull-scale experiments of using TOD in large real-world development projects would be invaluable for \nempirically assessing the bene.ts of omniscient debug\u00adging. At the database level, indexes for frequently-used \ncom\u00adpound conditions could be materialized so as to improve queryef.ciency, and theoverhead causedbyevent \nemission could be strongly reduced by not reifying redundant infor\u00admation.For dealing with partial traces,itwouldbe \ninterest\u00ading to leverage the hot swap feature of modern JVMs for addingor removing instrumentationat \nruntime.Itwouldalso be worthy to re.ne spatial scoping to the expression level, and to explore the use \nof static analysis to reduce even more the set of generated events. Finally, speci.c behavior simu\u00adlations \ncould be provided for trusted and widely-used classes (e.g. ArrayList), so that their state could be \nreconstituted without needing to fully instrument their internals. Acknowledgments WethankBilLewisforhis \ninspiringworkonthe Omniscient Debugger, JohanFabry and Jacques Noy\u00b4e for their invalu\u00adable feedback on \nthis manuscript, and Sergio Aguilera and Hernan Cuevas for their support with the cluster used for the \nbenchmarks.We also thank the anonymous OOPSLA re\u00adviewers for their insightful comments. This work is \npartially .nanced by the EU NoE CoreGRID. References [1] M. Blasgen and K. Eswaran. Storage and access \nin relational databases. IBM SystemsJournal, 16(4):363, 1977. [2] Bob Boothe. Ef.cient algorithms for \nbidirectional debug\u00adging. In PLDI 00:Proceedingsof theACM SIGPLAN 2000 conference on Programming language \ndesign and implemen\u00adtation, pages 299 310, NewYork, NY, USA, 2000.ACM Press. \u00b4a code manipulation tool \nto implement adaptable systems. In [3] Eric Bruneton, Romain Lenglet, and Thierry Coupaye. ASM: Proceedingsof \nthe ASF(ACM SIGOPSFrance)Journ\u00b4 ees Composants 2002: Adaptable and extensible component systems, November \n2002. [4] Mariano P. Consens, Masum Z. Hasan, and Alberto O. Mendelzon.Visualizing and querying distributedevent \ntraces with Hy+. In Proceedings of the International Conference on Application of Databases, volume 819, \npages 123 141. LNCS, 1994. [5] Jim des Rivi`eres and JohnWiegand. Eclipse:A platform for integrating \ndevelopment tools. IBM Systems Journal, 43(2):371 383, 2004. [6] Mireille Ducasse.\u00b4Coca: an automated \ndebugger for c. In ICSE 99: Proceedings of the 21st international conference on Software engineering, \npages 504 513, Los Alamitos, CA, USA, 1999. IEEE Computer Society Press. [7] Mireille Ducass\u00b4e. Opium:Anextendable \ntrace analyzerfor prolog. Journal of Logic Programming, 39(1-3):177 223, 1999. [8] Stuart I. Feldman \nand Channing B. Brown. Igor: a system for program debugging via reversible execution. In PADD 88: Proceedings \nof the 1988ACM SIGPLAN and SIGOPS workshop onParallel and distributed debugging, pages 112 123,NewYork,NY, \nUSA, 1988.ACM Press. [9] Abdelwahab Hamou-Lhadj and Timothy C. Lethbridge. Compression techniques to \nsimplify the analysis of large execution traces. In IWPC 02: Proceedings of the 10th InternationalWorkshop \non Program Comprehension, page 159,Washington, DC, USA, 2002. IEEE Computer Society. [10] Charles R. \nHill. A real-time microprocessor debugging technique. In SIGSOFT 83: Proceedings of the symposium on \nHigh-level debugging, pages 145 148, NewYork, NY, USA, 1983.ACM Press. [11] Christoph Hofer, Marcus Denker, \nand St\u00b4ephane Ducasse. Implementing a backward-in-time debugger. In Proceedings of NODe 06, volume P-88, \npages 17 32. Lecture Notes in Informatics, 2006. [12] DeanF. Jerding and JohnT. Stasko. The information \nmural: Atechnique for displaying and navigating large information spaces. IEEETrans.Vis. Comput. Graph., \n4(3):257 271, 1998. [13] Andrew J.Ko and Brad A. Myers. Designing the whyline: a debugging interface \nfor asking questions about program behavior. In Elizabeth Dykstra-Erickson and Manfred Tscheligi, editors, \nCHI, pages 151 158.ACM, 2004. [14] Carol H. LeDoux and D. Stott Parker, Jr. Saving traces for ada debugging. \nIn Ada in Use, Proceedings of the Ada International Conference, pages 97 108, September 1985. Published \nasACM Ada Letters,volume5, number2. [15] Bil Lewis. Debugging backwards in time. In M. Ronsse and K. \nDe Bosschere, editors, Proceedings of theFifth In\u00adternationalWorkshop onAutomated Debugging (AADEBUG \n2003), Ghent, Belgium, 2003. [16] Henry Lieberman. Reversible object-oriented interpreters. In JeanB\u00b4ezivin, \nJean-Marie Hullot, Pierre Cointe, and Henry Lieberman, editors, ECOOP, volume 276 of Lecture Notes in \nComputer Science, pages 11 19. Springer, 1987. [17] Henry Liebermanand ChristoperFry. ZStep95:Areversible, \nanimated source code stepper. In John Stasko, John Domingue, Marc H. Brown, and Blaine A. Price, editors, \nSoftware Visualization Programming as a Multimedia Experience, pages 277 292, Cambridge, MA-London, \n1998. The MIT Press. [18] Ian MacLarty, Zoltan Somogyi, and Mark Brown. Divide\u00adand-query and subterm \ndependencytracking in the mercury declarative debugger. In AADEBUG 05: Proceedings of the sixth international \nsymposium onAutomated analysis-driven debugging, pages 59 68, NewYork, NY, USA, 2005.ACM Press. [19] \nMichael Martin, Benjamin Livshits, and Monica S. Lam. Finding application errors and security .aws using \nPQL: a program query language. ACM SIGPLAN Notices, 40(10):365 383, October 2005. [20] O. Y. Nickolayev, \nP. C. Roth, and D. A. Reed. Real\u00adtime statistical clustering for event trace reduction. The International \nJournal of Supercomputer Applications and High Performance Computing, 11(2):144 159, Summer 1997. [21] \nOmniCore. Codeguide back-in-time debugger. [22] Guillaume Pothier. Benchmarks of COTS database manage\u00adment \nsystems. Technical Report TR/DCC-2006-16, Univer\u00adsity of Chile, October 2006. [23] Yasushi Saito. Jockey: \na user-space library for record-replay debugging. In Proceedings of the sixth international sympo\u00adsium \nonAutomated analysis-driven debugging (AADEBUG 2005), pages 69 76,NewYork,NY, USA, 2005.ACM Press. [24] \nPraveen Seshadri, Miron Livny, and Raghu Ramakrishnan. Sequence query processing. SIGMOD Record(ACM Special \nInterest Group on Management of Data), 23(2):430 441, June 1994. [25] Richard Snodgrass. Monitoring in \na software development environment: A relational approach. SIGPLAN Not., 19(5):124 131, 1984. [26] Michael \nStonebraker, Chuck Bear, UgurC\u00b8 etintemel, Mitch Cherniack,Tingjian Ge, Nabil Hachem, Stavros Harizopou\u00adlos, \nJohn Lifter, Jennie Rogers, and StanleyB. Zdonik. One size .ts all? part 2: Benchmarking studies. In \nConference on Innovative Data Systems Research (CIDR 2007), pages 173 184. www.crdrdb.org, 2007. \u00b4 Partial \nbehavioral re.ection: Spatial and temporal selection of rei.cation. In Ron Crocker and Guy L. Steele, \nJr., editors, Proceedingsof the 18thACM SIGPLAN Conference on Object-Oriented Programming Systems, Languages \nand Applications (OOPSLA 2003), pages 27 46, Anaheim, CA, USA, October 2003.ACM Press.ACM SIGPLAN Notices, \n38(11). [27] EricTanter,JacquesNoy\u00b4e, Denis Caromel,and Pierre Cointe. [28] Andy Zaidman and Serge Demeyer. \nManaging trace data volume through a heuristical clustering process based on event execution frequency. \nIn CSMR 04: Proceedings of the Eighth EuromicroWorking Conference on Software Maintenance and Reengineering \n(CSMR 04), page 329, Washington, DC, USA, 2004. IEEE Computer Society.    \n\t\t\t", "proc_id": "1297027", "abstract": "<p>Omniscient debuggers make it possible to navigate backwards in time within a program execution trace, drastically improving the task of debugging complex applications. Still, they are mostly ignored in practice due to the challenges raised by the potentially huge size of the execution traces. This paper shows that omniscient debugging can be realistically realized through the use of different techniques addressing efficiency, scalability and usability. We present TOD, a portable Trace-Oriented Debugger for Java, which combines an efficient instrumentation for event generation, a specialized distributed database for scalable storage and efficient querying, support for partial traces in order to reduce the trace volume to relevant events, and innovative interface components for interactive trace navigation and analysis in the development environment. Provided a reasonable infrastructure, the performance of TOD allows a responsive debugging experience in the face of large programs.</p>", "authors": [{"name": "Guillaume Pothier", "author_profile_id": "81339523016", "affiliation": "University of Chile, Santiago, Chile", "person_id": "PP39109178", "email_address": "", "orcid_id": ""}, {"name": "&#201;ric Tanter", "author_profile_id": "81100346970", "affiliation": "University of Chile, Santiago, Chile", "person_id": "PP39038952", "email_address": "", "orcid_id": ""}, {"name": "Jos&#233; Piquer", "author_profile_id": "81100290956", "affiliation": "University of Chile, Santiago, Chile", "person_id": "P905210", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1297027.1297067", "year": "2007", "article_id": "1297067", "conference": "OOPSLA", "title": "Scalable omniscient debugging", "url": "http://dl.acm.org/citation.cfm?id=1297067"}