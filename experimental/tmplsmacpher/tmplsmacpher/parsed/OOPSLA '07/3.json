{"article_publication_date": "10-21-2007", "fulltext": "\n Statistically RigorousJavaPerformanceEvaluation Andy Georges Dries Buytaert Lieven Eeckhout Department \nof Electronics and Information Systems, Ghent University, Belgium {ageorges,dbuytaer,leeckhou}@elis.ugent.be \nAbstract Java performance is far from being trivial to benchmark because it is affected by various factors \nsuch as the Java application, its input, the virtual machine, the garbage collector, the heap size, etc. \nIn addition, non-determinismat run-time causes theexecution time ofaJavaprogramtodifferfromrunto run.Therearea \nnumberof sourcesof non-determinismsuchasJust-In-Time(JIT) compilation and optimization in the virtual \nmachine (VM) driven by timer\u00adbased method sampling,thread scheduling,garbagecollection, and various system \neffects. There exist a wide variety of Java performance evaluation methodologies used by researchers \nand benchmarkers. These methodologies differ from each other in a number of ways. Some report average \nperformance over a number of runs of the same experiment; others report the best or second best performance \nob\u00adserved; yet others report the worst. Some iterate the benchmark multiple times within a single VM \ninvocation; others consider mul\u00adtiple VM invocations and iterate a single benchmark execution; yet othersconsider \nmultiple VM invocations and iterate the benchmark multiple times. This paper shows that prevalent methodologies \ncan be mis\u00adleading, and can even lead to incorrect conclusions. The reason is that the data analysis \nis not statistically rigorous. In this pa\u00adper, wepresenta surveyofexistingJava performanceevaluation \nmethodologies and discuss the importance of statistically rigorous data analysis for dealing with non-determinism.We \nadvocate ap\u00adproaches to quantify startup as well as steady-state performance, and,in addition,weprovidetheJavaStats \nsoftwareto automatically obtain performance numbers in a rigorous manner. Although this paper focuses \nonJava performanceevaluation, manyof the issues addressed in this paper also apply to other programming \nlanguages and systems thatbuild ona managed runtime system. Categories and Subject Descriptors D.2.8[Software \nEn\u00adgineering]: Metrics Performance measures; D.3.4[Pro\u00adgramming Languages]:Processors Run-time environments \nPermission to make digital or hard copies of all or part of this work for personal or classroom use is \ngranted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page.To copyotherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. OOPSLA \n07, October 21 25, 2007, Montr \u00b4ebec, Canada. eal, Qu\u00b4Copyright c &#38;#169; 2007ACM 978-1-59593-786-5/07/0010...$5.00 \nGeneral Terms Experimentation, Measurement, Perfor\u00admance Keywords Java, benchmarking, data analysis, \nmethodol\u00adogy, statistics 1. Introduction Benchmarking is at the heart of experimental computer sci\u00adence \nresearch and development. Market analysts compare commercial products based on published performance \nnum\u00adbers. Developers benchmark products under development to assess their performance. And researchers \nuse benchmark\u00ading to evaluate the impact on performance of their novel re\u00adsearch ideas. As such, it is \nabsolutely crucial to have a rig\u00adorous benchmarking methodology.Anon-rigorous method\u00adology may skew the \noverall picture, and may even lead to incorrect conclusions. And this may drive research and de\u00advelopment \nin a non-productive direction, or may lead to a non-optimal product brought to market. Managed runtime \nsystems are particularly challenging to benchmark because there are numerous factors affect\u00ading overall \nperformance, which is of lesser concern when it comes to benchmarking compiled programming languages \nsuch as C. Benchmarkers are well aware of the dif.culty in quantifying managed runtime system performance \nwhich is illustratedbyanumberof research papers publishedoverthe past few years showing the complex interactions \nbetween low-leveleventsandoverall performance[5,11,12,17,24]. More speci.cally,recentworkonJavaperformance \nmethod\u00adologies [7, 10] stressed the importance of a well chosen and well motivated experimental design: \nit was pointed out that the results presentedinaJava performance study are subject to the benchmarks, \nthe inputs, the VM, the heap size, and the hardware platform that are chosen in the experimental setup. \nNot appropriately considering and motivating one of thesekeyaspects, or not appropriately describing \nthe context within which the results were obtained and how theyshould be interpreted may give a skewed \nview, and may even be misleading or at worst be incorrect. The orthogonal axis to experimental design \nin a perfor\u00admance evaluation methodology, is data analysis, or how to analyze and report the results. \nMore speci.cally, a per\u00adformance evaluation methodology needs to adequately deal with the non-determinism \nin the experimental setup. In a Java system, or managed runtime system in general, there area numberof \nsourcesof non-determinism thataffectover\u00adall performance.One potentialsourceof non-determinismis Just-In-Time \n(JIT) compilation.Avirtual machine (VM) that uses timer-based sampling to drive the VM compilation and \noptimization subsystem may lead to non-determinism and execution time variability: different executions \nof the same program may result in different samples being taken and, by consequence, different methods \nbeing compiled and op\u00adtimized to different levels of optimization. Another source of non-determinism \ncomes from thread scheduling in time\u00adshared and multiprocessor systems. Running multithreaded workloads, \nas is the case for most Java programs, requires thread scheduling in the operating system and/or virtual \nma\u00adchine. Different executions of the same program may in\u00adtroduce different thread schedules, and may \nresult in dif\u00adferent interactions between threads, affecting overall per\u00adformance. The non-determinism \nintroduced by JIT compi\u00adlation and thread scheduling may affect the points in time where garbage collections \noccur. Garbage collection in its turn may affect program locality, and thus memory system performance \nas well asoverall system performance.Yet an\u00adother source of non-determinism is various system effects, \nsuch as system interrupts this is not speci.c to managed runtime systems though as it is a general concern \nwhen run\u00adning experiments on real hardware. From an extensive literature survey, we found that there \nare a plethora of prevalent approaches, both in experimen\u00adtal design and data analysis for benchmarking \nJava perfor\u00admance. Prevalent data analysis approaches for dealing with non-determinism are not statistically \nrigorous though. Some report the average performance number across multiple runs of the same experiments; \nothers report the best performance number, others report the second best performance number and yet others \nreport the worst. In this paper, we argue that not appropriately specifying theexperimental design and \nnot using a statistically rigorous data analysis can be mislead\u00ading and can even lead to incorrect conclusions. \nThis paper advocates using statistics theory as a rigorous data analysis approach for dealing with the \nnon-determinism in managed runtime systems. The pitfall in using a prevalent method is illustrated in \nFigure 1 which compares the execution time for running JikesRVMwith.vegarbage collectors(CopyMS,GenCopy, \nGenMS, MarkSweep and SemiSpace) for the SPECjvm98 db benchmark with a 120MB heap size the experi\u00admental \nsetup will be detailed later. This graph compares the prevalent best method which reports the best perfor\u00admance \nnumber (or smallest execution time) among 30 mea\u00adsurements against a statistically rigorous method which \nre\u00adports 95% con.dence intervals; the best method does not control non-determinism, and corresponds to \nthe SPEC re\u00adporting rules [23]. Based on the best method, one would Figure 1. An example illustrating \nthe pitfall of prevalent Java performance data analysis methods: the best method is shown on the left \nand the statistically rigorous method is shown on the right. This is for db and a 120MB heap size. conclude \nthat the performance for the CopyMS and Gen-Copycollectors is about the same. The statistically rigorous \nmethod though shows that GenCopy signi.cantly outper\u00adforms CopyMS. Similarly, based on the best method, \none would conclude that SemiSpace clearly outperforms Gen-Copy. The reality though is that the con.dence \nintervals for both garbage collectors overlap and, as a result, the per\u00adformance difference seen between \nboth garbage collectors is likely due to the random performance variations in the system under measurement.Infact,we \nobservealargeper\u00adformance variation for SemiSpace, and at least one really good run along with a large \nnumber of less impressive runs. The best method reports the really good run whereas a sta\u00adtistically \nrigorous approach reliably reports thattheaverage scores for GenCopy and SemiSpace are very close to \neach other. This paper makes the following contributions: We demonstrate that there is a major pitfall \nassoci\u00adated with today s prevalent Java performance evaluation methodologies, especially in terms of \ndata analysis. The pitfall is that they may yield misleading and even in\u00adcorrect conclusions. The reason \nis that the data analysis employed by these methodologies is not statistically rig\u00adorous.  We advocate \nadding statistical rigor to performanceeval\u00aduation studies of managed runtime systems, and in partic\u00adular \nJava systems. The motivation for statistically rigor\u00adous data analysis is that statistics, and in particular \ncon\u00ad.dence intervals, enable one to determine whether dif\u00adferences observed in measurements are due to \nrandom .uctuations in the measurements or due to actual differ\u00adencesinthe alternatives comparedagainst \neach other.We discuss how to compute con.dence intervals and discuss techniques to compare multiple alternatives. \n We survey existing performance evaluation methodolo\u00adgies for start-up and steady-state performance, \nand ad\u00advocate the following methods.For start-up performance, we advise to: (i) take multiple measurements \nwhere each  measurement comprises one VM invocation and a sin\u00adgle benchmark iteration, and (ii) compute \ncon.dence in\u00adtervals across these measurements.For steady-state per\u00adformance, we advise to: (i) take \nmultiple measurements where each measurement comprises one VM invocation and multiple benchmark iterations, \n(ii) in each of these measurements, collect performance numbers for differ\u00adent iterations once performance \nreaches steady-state, i.e., after the start-up phase, and (iii) compute con.dence in\u00adtervals across these \nmeasurements (multiple benchmark iterations across multiple VM invocations). We provide publicly available \nsoftware, called JavaStats, to enable a benchmarker to easily collect the information required to do \na statistically rigorous Java performance analysis. In particular, JavaStats monitors the variability \nobserved in the measurements to determine the number of measurements that needto be taken to reach a \ndesired con.dence interval for a given con.dence level. Java-Stats readilyworks for both the SPECjvm98 \nand DaCapo benchmark suites, and is available at http://www.elis.ugent.be/JavaStats. This paperis organized \nas follows.We .rst presenta sur\u00advey in Section 2 on Java performance evaluation method\u00adologies in use \ntoday. Subsequently, in Section 3, we discuss general statistics theory and how it applies to Java perfor\u00admance \nanalysis. Section 4 then translates these theoretical concepts to practical methodologies for quantifying \nstartup and steady-state performance. After detailing our experi\u00admental setup in Section 5, we then assess \nin Section6 the prevalent evaluation methodologies compared to the statis\u00adtically rigorous methodologies \nadvocated in this paper.We show that in manypractical situations, prevalent methodolo\u00adgies can be misleading, \nor even yield incorrect conclusions. Finally, we summarize and conclude in Section 7. 2. PrevalentJavaPerformanceEvaluation \nMethodologies There is a wide range of Java performance evaluation methodologies in use today. In order \nto illustrate this, we have performed a survey among the Java performance pa\u00adpers published in the last \nfew years (from 2000 onwards) in premier conferences such as Object-Oriented Programming, Systems, Languages \nand Applications (OOPSLA), Pro\u00adgramming Language Design and Implementation (PLDI), Virtual Execution \nEnvironments (VEE), Memory Man\u00adagement (ISMM) and Code Generation and Optimization (CGO). In total, we \nexamined the methodology used in 50 papers. Surprisingly enough, about one third of the papers (16 out \nof the 50 papers) does not specify the methodology used in the paper. This not only makes it dif.cult \nfor other researchers to reproduce the results presented in the paper, it also makes understanding and \ninterpreting the results hard. During our survey, we found that not specifying or only partially specifying \nthe performance evaluation methodol\u00adogy is particularly the case for not too recent papers, more speci.cally \nfor papers between 2000 and 2003. More recent papers on the other hand, typically have a more detailed \ndescription of their methodology. This is in sync with the growing awareness of the importance of a rigorous \nperfor\u00admance evaluation methodology. For example, Eeckhout et al. [10] show that Java performance is \ndependent on the in\u00adput set given to the Java application as well as on the virtual machine that runs \nthe Java application. Blackburn et al. [7] con.rm these .ndings and show that a Java performance evaluation \nmethodology,next to considering multiple JVMs, should also consider multiple heap sizes as well as multiple \nhardware platforms. Choosing a particular heap size and/or a particular hardware platform may draw afairly \ndifferent picture and may even lead to opposite conclusions. In spite of these recent advances towards \na rigorous Java performance benchmarking methodology, there is no con\u00adsensus among researchers on what \nmethodology to use. In fact, almost all research groups come with theirown method-ology.Wenow discuss \nsome general featuresof thesepreva\u00adlent methodologies and subsequently illustrate this using a number \nof example methodologies. 2.1 General methodology features In the following discussion, we make a distinction \nbetween experimental design and data analysis. Experimental design refers to setting up the experiments \nto be run and requires good understanding of the system being measured. Data analysis refers to analyzing \nthe data obtained from the ex\u00adperiments. As will become clear from this paper, both ex\u00adperimental design \nand data analysis are equally important in the overall performance evaluation methodology. 2.1.1 Data \nanalysis Average or median versus best versus worst run. Some methodologies report the average or median \nexecution time acrossa numberof runs typically more than3 runs are considered; some go up to 50 runs. \nOthers report the best or second best performance number, and yet others report the worst performance \nnumber. The SPEC run rules for example state that SPECjvm98 benchmarkers must run their Java application \nat least twice, and report both the best and worst of all runs. The intuition behind the worst performance \nnumber is to report a perfor\u00admance number that represents program execution intermin\u00adgled with class \nloading and JIT compilation. The intuition behind the best performance number is to report a perfor\u00admance \nnumber where overall performance is mostly domi\u00adnatedby programexecution, i.e., class loading and JIT \ncom\u00adpilation are less of a contributor to overall performance. The most popular approaches are average \nand best 8 and 10 papers out of the 50 papers in our survey, re\u00adspectively; median, second best and \nworst are less frequent, namely4,4and3papers, respectively. Con.dence intervals versus a single performance \nnumber. In only a small minority of the research papers (4 out of 50), con.dence intervals are reported \nto characterize the variability across multiple runs. The others papers though report a single performance \nnumber. 2.1.2 Experimental design OneVMinvocationversus multipleVMinvocations. The SPECjvm98 benchmark \nsuite as well as the DaCapo bench\u00admark suite come with a benchmark harness. The harness al\u00adlows for running \na Java benchmark multiple times within a single VM invocation. Throughout the paper, we will re\u00adfer to \nmultiple benchmark runs within a single VM invoca\u00adtion as benchmark iterations. In this scenario, the \n.rst iter\u00adation of the benchmark will perform class loading and most of the JIT (re)compilation; subsequent \niterations will expe\u00adrience less (re)compilations. Researchers mostly interested in steady-state performance \ntypically run their experiments in this scenario and report a performance number based on the subsequent \niterations, not the .rst iteration. Researchers interested in startup performance will typically initiate \nmul\u00adtiple VM invocations running the benchmark only once. Including compilationversus excluding compilation. \nSome researchers report performance numbers that include JIT compilation overhead, while others report \nperformance numbers excluding JIT compilation overhead. In a man\u00adaged runtime system, JIT (re)compilation \nis performed at run-time, and by consequence, becomes part of the overall execution. Some researchers \nwant to exclude JIT compila\u00adtion overhead from their performance numbers in order to isolate Java application \nperformance and to make the mea\u00adsurements (more) deterministic, i.e., have less variability in the performance \nnumbers across multiple runs. Anumber of approaches have been proposed to exclude compilation overhead. \nOne approach is to compile all meth\u00adodsexecuted duringa .rstexecutionof theJava application, i.e., all \nmethods executed are compiled to a predetermined optimization level, in some cases the highest optimization \nlevel. The second run, which is the timing run, does not do anycompilation. Another approach, which is \nbecoming increasingly popular, is called replay compilation [14, 20], which is used in 7 out of the 50 \npapers in our survey. In replay compilation, a number of runs are performed while logging the methods \nthat are compiled and at which opti\u00admization level these methods are optimized. Based on this logging \ninformation,acompilationplanis determined.Some researchers select the methods that are optimized in the \nma\u00adjority of the runs, and set the optimization level for the se\u00adlected methods at the highest optimization \nlevels observed in the majority of the runs; others pick the compilation plan that yields the best performance. \nOnce the compilation plan is established, two benchmark runs are done in a single VM invocation: the \n.rst run does the compilation according to the compilation plan, the second run then is the timing run \nwith adaptive (re)compilation turned off. Forced GCs before measurement. Some researchers per\u00adforma full-heapgarbage \ncollection (GC) before doinga per\u00adformance measurement. This reduces the non-determinism observed across \nmultiple runs due to garbage collections kicking in at different times across different runs. Other considerations. \nOther considerations concerning the experimental design include one hardware platform ver\u00adsus multiple \nhardware platforms; one heap size versus mul\u00adtiple heap sizes; a single VM implementation versus multi\u00adple \nVM implementations; and back-to-back measurements ( aaabbb ) versus interleaved measurements ( ababab \n).  2.2 Example methodologies To demonstrate the diversity in prevalent Java performance evaluation \nmethodologies, bothin termsofexperimentalde\u00adsign and data analysis, we refer toTable1 which summa\u00adrizes \nthe main features of a number of example methodolo\u00adgies. We want to emphasize up front that our goal \nis not to pick on these researchers; we just want to illustrate the wide diversity in Java performance \nevaluation methodolo\u00adgies around today. Infact, this wide diversity illustrates the growing need forarigorous \nperformanceevaluation method\u00adology; manyresearchers struggle coming up with a method\u00adologyand,asa result,different \nresearch groupsendupusing different methodologies. The example methodologies sum\u00admarizedinTable1 are \namong the most rigorous methodolo\u00adgies observed during our survey: these researchers clearly describe \nand/or motivate their methodology whereas many others do not. For the sake of illustration, we now discuss \nthree well de\u00adscribed and well motivated Java performance methodologies in more detail. Example 1. McGacheyand \nHosking [18] (methodologyB inTable1) iterate each benchmark11times withina single VM invocation. The \n.rst iteration compiles all methods at the highest optimization level. The subsequent 10 iterations do \nnot include anycompilation activity and are considered the timing iterations. Only the timing iterations \nare reported; the .rst compilation iteration is discarded. And a full-heap garbage collection is performed \nbefore each timing iteration. The performance number reportedinthe paperistheaverage performance over \nthese 10 timing iterations along with a 90% con.dence interval. Example 2: Startup versus steady-state. \nArnold et al. [1, 2] (methodologiesFandGinTable1) makea clear distinc\u00adtion between startup and steady-state \nperformance. They evaluate the startup regime by timing the .rst run of a benchmark execution with a \nmedium input set (s10 for SPECjvm98). They report the minimum execution time methodology A B C D E F \nG H I J K L M reference [4] [18] [21] [22] [25] [1] [2] [20] [7, 14] [23] [8] [3] [9] Data analysis \nperformance number from a single run average performance number from multiple runs median performance \nnumber from multiple runs best performance number from multiple runs second best performance number from \nmultiple runs worst performance number from multiple runs v v v v v v v v v v v v v con.dence interval \nfrom multiple runs v Experimental design one VM invocation, one benchmark iteration one VM invocation, \nmultiple benchmark iterations multiple VM invocations, one benchmark iteration multiple VM invocations, \nmultiple benchmark iterations v v v v v v v v v v including JIT compilation excluding JIT compilation \nall methods are compiled before measurement replay compilation v v v v v v v v v v v v v full-heap garbage \ncollection before measurement v v a single hardware platform multiple hardware platforms v v v v v v \nv v v v v v v a single heap size multiple heap sizes v v v v v v v v v v v v v a single VM implementation \nmultiple VM implementations v v v v v v v v v v v v v back-to-back measurements interleaved measurements \nv Table 1. CharacterizingprevalentJavaperformanceevaluation methodologies(inthe columns)in termsofanumberof \nfeatures (inthe rows):the v symbol means that the given methodology uses the given feature; the absence \nof the v symbol means the methodology does not use the given feature, or, at least, the feature is not \ndocumented. across .ve benchmark executions, each benchmark execu\u00adtion triggeringa newVMinvocation.For \nmeasuring steady\u00adstate performance, in [1], Arnold et al. report the minimum execution time across .ve \nbenchmark executions with a large input set(s100 for SPECjvm98) within a single VM invocation. Arnold \net al. [2] use a different methodology for measuring steady-state performance. They do 10 ex\u00adperiments \nwhere each benchmark runs for approximately 4 minutes; this results in 10 times N runs. They then take \nthe median execution time across these 10 experiments, re\u00adsulting in N median execution times and then \nreport the minimum medianexecution time. All the performance num\u00adbers reportedinclude JIT compilation \nand optimization, as well asgarbage collection activity. Example 3: Replay compilation. Our thirdexample \nmethod\u00adology uses replay compilationtodrivethe performanceeval\u00aduation.Theideaofreplay compilation discussedin[5,7,14, \n20] (methodologiesHandIinTable1)istobuilda com\u00adpilation plan by running each benchmark n times with the \nadaptive runtime system enabled, logging the names of the methods that are optimized and their optimization \nlevels. Theythen select the best compilation plan. The benchmark\u00ading experiment then proceeds as follows: \n(i) the .rst bench\u00admark run performs compilation using the compilation plan, (ii)a full-heapgarbage collectionis \nperformed, and (iii) the benchmark is run a second time with adaptive optimization turned off. This is \ndone m times,andthebestrunis reported. Reporting the .rst benchmark run is called the mix method; reporting \nthe second run is called the stable method. 3. Statistically RigorousPerformance Evaluation We advocate \nstatistically rigorous data analysis as an impor\u00adtantpartofaJavaperformanceevaluation methodology.This \nsection describes fundamental statistics theory as described in many statistics textbooks, see for example \n[15, 16, 19], and discusses how statistics theory applies to Java perfor\u00admance data analysis. The next \nsection then discusses how to add statistical rigor in practice. 3.1 Errors in experimental measurements \nAs a .rst step, it is useful to classify errors as we observe them in experimental measurements in two \nmain groups: systematic errors and random errors. Systematic errors are typically due to someexperimental \nmistake or incorrect pro\u00adcedure which introducesabias into the measurements. These errors obviously affect \nthe accuracyof the results. It is up to the experimenter to control and eliminate systematic errors. \nIf not, the overall conclusions, even with a statistically rig\u00adorous data analysis, may be misleading. \nRandom errors, on the other hand, are unpredictable and non-deterministic. They are unbiased in that \na random er\u00adror may decrease or increase a measurement. There may be manysources of random errors in \nthe system. In practice, an important concern is the presence of perturbing events that are unrelated \nto what the experimenter is aiming at measur\u00ading, such as external system events, that cause outliers \nto appear in the measurements. Outliers need to be examined closely, and if the outliers are a result \nof a perturbing event, theyshouldbe discarded.Taking the best measurement also alleviates theissue with \noutliers, however, we advocate dis\u00adcarding outliers and applying statistically rigorous data anal\u00adysis \nto the remaining measurements. While it is impossible to predict random errors, it is possibletodevelopa \nstatistical modelto describetheoverall effect of random errors on the experimental results, which we \ndo next. 3.2 Con.dence intervalsfor the mean In each experiment, a number of samples is taken from an \nunderlying population. A con.dence interval for the mean derived from these samples then quanti.es the \nrange of val\u00aduesthathaveagiven probabilityof includingthe actualpop\u00adulation mean. While the way in which \na con.dence interval is computed is essentially similar for all experiments, a dis\u00adtinction needs to \nbe made depending on the number of sam\u00adples gathered from the underlying population [16]: (i) the number \nof samples n is large (typically, n = 30), and (ii) the number of samples n is small (typically, n< 30).We \nnow discuss both cases. 3.2.1 When the number of measurements is large (n = 30) Building a con.dence \ninterval requires that we have a num\u00adber of measurements xi, 1 = i = n, from a population with mean \u00b5 \nand variance s2. The mean of these measurements x\u00afis computed as n i=1 xi x\u00af=. n We will approximate \nthe actual true value\u00b5 by the mean of our measurements x\u00afand we will compute a range of val\u00adues [c1,c2] \naround x\u00afthat de.nes the con.dence interval at a given probability (called the con.dence level). The \ncon\u00ad.dence interval [c1,c2] is de.ned such that the probability of \u00b5 being between c1 and c2 equals 1 \n- a;a is called the signi.cance level and (1 - a) is called the con.dence level. Computing the con.dence \nintervalbuilds on the central limit theory. The central limit theory states that, for large values of \nn (typically n = 30), x\u00afis approximately Gaussian v distributed with mean \u00b5 and standard deviation s/ \nn, pro\u00advided that the samples xi, 1 = i = n,are (i) independent and (ii) come from the same population \nwith mean \u00b5 and .nite standard deviation s. Because the signi.cance level a is chosen a priori, we need \nto determine c1 and c2 such that Pr[c1 = \u00b5 = c2]= 1 - a holds. Typically, c1 and c2 are chosen to form \na symmetric interval around x\u00af, i.e., Pr[\u00b5<c1]= Pr[\u00b5> c2]= a/2. Applying the central-limit theorem, we \n.nd that s c1 =\u00afx - z1-a/2 v n s c2 = x\u00af+ z1-a/2 v , n with x\u00afthe sample mean, n the number of measurements \nand s the sample standard deviation computed as follows: n (xi - x\u00af)2 i=1 s = . n - 1 The value z1-a/2 \nis de.ned such that a random variable Z that is Gaussian distributed with mean \u00b5 =0 and variance s2 =1, \nobeys the following property: Pr[Z = z1-a/2]=1 - a/2. The value z1-a/2 is typically obtained from a precomputed \ntable. 3.2.2 When the number of measurements is small (n< 30) Abasic assumption made in the above derivation \nis that the 2 sample variance sprovides a good estimate of the actual variances2. This assumption enabled \nus to approximate z = v (\u00afx - \u00b5)/(s/ n) as a standard normally distributed random variable, and by consequence \nto compute the con.dence interval for x\u00af.Thisis generallythe caseforexperimentswith a large number of \nsamples, e.g., n = 30. However,forarelatively small numberof samples, which is typically assumed to mean \nn< 30, the sample variance s2 can be signi.cantly different from the actual variance s2 of the underlying \npopulation [16]. In this case, it can be shown that the distribution of the transformed value t = v (\u00afx \n- \u00b5)/(s/ n) follows the Student s t-distribution with n - 1 degrees of freedom. By consequence, the con.dence \ninterval can then be computed as: s c1 =\u00afx - t1-a/2;n-1 v n s c2 = x\u00af+ t1-a/2;n-1 v , n with the value \nt1-a/2;n-1 de.ned such that a random vari\u00adable T that follows the Student s t distribution with n - 1 \ndegrees of freedom, obeys: Pr[T = t1-a/2;n-1]=1 - a/2. The value t1-a/2;n-1 is typically obtained from \na precom\u00adputed table. It is interesting to note that as the number of measurements n increases, the Student \nt-distribution ap\u00adproaches the Gaussian distribution. 3.2.3 Discussion Interpretation. In order to interpret \nexperimental results with con.dence intervals, we need to have a good under\u00adstanding of what a con.dence \ninterval actually means. A 90% con.dence interval, i.e., a con.dence interval with a 90% con.dence level, \nmeans that there is a 90% probability that the actual distribution mean of the underlying popula\u00adtion, \n\u00b5, is within the con.dence interval. Increasing the con\u00ad.dence level to 95% means that we are increasing \nthe prob\u00adability that the actual mean is within the con.dence interval. Since we do not change our measurements, \nthe only way to increase the probability of the mean being within this new con.dence interval is to increase \nits size. By consequence, a 95% con.dence interval will be larger than a 90% con.\u00addence interval; likewise, \na 99% con.dence interval will be larger than a 95% con.dence interval. Note on normality. It is also \nimportant to emphasize that computing con.dence intervals does not require that the un\u00adderlying data \nis Gaussian or normally distributed. The cen\u00adtral limit theory,which is at the foundation of the con.dence \ninterval computation, states that x\u00afis normally distributed ir\u00adrespective of the underlying distribution \nof the population from which the measurements are taken. In other words, even if the population is not \nnormally distributed, the av\u00aderage measurement mean x\u00afis approximately Gaussian dis\u00adtributed if the measurements \nare taken independently from each other.  3.3 Comparing two alternatives Sofar, we were only concerned \nabout computing the con\u00ad.dence interval for the mean of a single system. In terms of a Java performance \nevaluation setup, this is a single Java benchmark with a given input running on a single virtual machine \nwithagiven heap size running onagiven hardware platform.However,in manypractical situations,aresearcher \nor benchmarkerwantsto comparethe performanceoftwoor more systems. In this section, we focus on comparing \ntwo alternatives; the next section then discusses comparing more thantwo alternatives.Apractical use \ncase scenario couldbe to compare the performance of twovirtual machines running the same benchmark withagiven \nheap size onagiven hard\u00adware platform. Another example use case is comparing the performance of two garbage \ncollectors for a given bench\u00admark, heap size and virtual machine on a given hardware platform. The simplest \napproach to comparing twoalternatives is to determine whether the con.dence intervals for the two sets \nof measurementsoverlap.Iftheydooverlap, thenwe cannot conclude that the differences seen in the mean \nvalues are not due to random .uctuations in the measurements. In other words,thedifference seeninthe \nmeanvaluesis possiblydue to randomeffects.Ifthecon.dence intervalsdonotoverlap, however, we conclude \nthat there is no evidence to suggest that there is not a statistically signi.cant difference. Note the \ncareful wording here. There is still a probability a that the differences observed in our measurements \nare simply due to random effects in our measurements. In other words, we cannot assure with a 100% certainty \nthat there is an actual difference between the compared alternatives. In some cases, taking such weak \nconclusions may not be very satisfying people tend to like strong and af.rmative conclusions but it \nis the best we can do given the statistical nature of the measurements. Consider now two alternatives \nwith n1 measurements of the .rst alternative and n2 measurements of the second alternative. We then .rst \ndetermine the sample means x\u00af1 and x\u00af2 and the sample standard deviations s1 and s2. We subsequently \ncompute the difference of the means as x\u00af= x\u00af1 - x\u00af2. The standard deviation sx of this difference of \nthe mean values is then computed as: 22 ss sx = 1 + 2 . n1 n2 The con.dence interval for the difference \nof the means is thengivenby c1 =\u00afx - z1-a/2sx c2 =\u00afx + z1-a/2sx. If this con.dence interval includes \nzero, we can conclude, at the con.dence level chosen, that there is no statistically signi.cant difference \nbetween the two alternatives. The above only holds in case the number of measure\u00adments is large on both \nsystems, i.e., n1 = 30 and n2 = 30. In case the number of measurements on at least one of the two systems \nis smaller than 30, then we can no longer assume that the difference of the means is normally distributed.We \nthen need to resort to the Student s t distribution by replac\u00ading the value z1-a/2 in the above formula \nwith t1-a/2;ndf ; the degrees of freedom ndf isthentobe approximatedbythe integer number nearest to \n22 2 ss 1 + 2 n1 n2 ndf 22 =(s1/n1)2 + (s2/n2)2 . n1-1 n2-1  3.4 Comparing more than two alternatives: \nANOVA The approach discussed in the previous section to compar\u00ading twoalternatives is simple and intuitively \nappealing, how\u00adever,itis limitedto comparingtwo alternatives.Amore gen\u00aderal and more robust technique \nis called Analysis of Vari\u00adance (ANOVA). ANOVAseparates the total variation in a set of measurementsintoa \ncomponent due to random .uctua\u00adtionsinthe measurementsanda componentduetothe actual differences among \nthe alternatives. In other words, ANOVA separates the total variation observed in (i) the variation ob\u00adserved \nwithin each alternative, which is assumed to be a result of random effects in the measurements, and (ii) \nthe variation between the alternatives. If the variation between the alternatives is larger than the \nvariation within each alter\u00adnative, then it can be concluded that there is a statistically signi.cant \ndifference between the alternatives. ANOVA as\u00adsumes that the variance in measurement error is the same \nfor all of the alternatives. Also, ANOVA assumes that the errors in the measurements for the different \nalternatives are independent and Gaussian distributed. However, ANOVAis fairly robust towards non-normality, \nespecially in case there is a balanced number of measurements for each of the alter\u00adnatives. To present \nthe general idea behind ANOVA it is conve-Table 2. Organizing the n measurements for k alternatives \nAlternatives Measurements 1 2 1 y11 y21 2 y12 y22 . . . . . . . . . j y1j y2j . . . . . . . . . k y1k \ny2k Overall mean . . . . . . . . . . . . . . . . . . . . . i yi1 yi2 . . . yij . . . yik . . . . . . \n. . . . . . . . . . . . . . . n Column means yn1 \u00afy.1 yn2 \u00afy.2 . . . . . . ynj \u00afy.j . . . . . . ynk \u00afy.k \n\u00afy.. nient to organize the measurements as shown in Table 2: there are n \u00b7 k measurements n measurements \nfor all k in an ANOVAanalysis. alternatives. The column means are de.ned as: n i=1 yij y\u00af.j = , n and \nthe overall mean is de.ned as: kn j=1 i=1 yij y\u00af.. = . n \u00b7 k It is then useful to compute the variation \ndue to the effects of the alternatives, sum-of-squares due to the alternatives (SSA), as the sum of the \nsquares of the differences between the mean of the measurements for each alternative and the overall \nmean, or: k SSA = n (\u00afy.j - y\u00af..)2 . j=1 The variation due to random effects within an alternative is \ncomputed as the sum of the squares of the differences (or errors) between the individual measurements \nand their respective alternative mean, or: kn SSE =(yij - y\u00af.j)2 . j=1 i=1 Finally, the sum-of-squares \ntotal, SST, or the sum of squares of the differences between the individual measurements and the overall \nmean is de.ned as: kn SST =(yij - y\u00af..)2 . j=1 i=1 It can be shown that SST = SSA + SSE. Or, in other \nwords, the total variation observed can be split up into a within alternative (SSE) component and a between \nalternatives (SSA) component. The intuitive understanding of an ANOVAanalysis now is to quantify whether \nthevariation across alternatives, SSA, is larger in some statistical sense than the variation within \neach alternative, SSE, which is due to random measurement errors.Asimplewayofdoingthisisto comparethe \nfractions SSA/SST versus SSE/SST . A statistically more rigor\u00adous approach is to apply a statistical \ntest, called the F-test, which is used to test whether two variances are signi.cantly different.We will \nnot go into further detail here about how to apply the F-test, however,we refer the interested reader \nto a reference textbook on statistics, such as [16]. After completing an ANOVAtest, we may conclude that \nthere is a statistically signi.cant difference between the al\u00adternatives, however, the ANOVA test does \nnot tell us be\u00adtween which alternatives there is a statistically signi.cant difference. There exists \na number of techniques to .nd out between which alternatives thereisor thereisnota statisti\u00adcally signi.cant \ndifference. One approach, which we will be using in this paper, is called theTukeyHSD (Honestly Sig\u00adni.cantly \nDifferent) test. The advantage of theTukeyHSD test over simpler approaches, such as pairwise t-tests \nfor comparing means, is that it limits the probability of making an incorrect conclusion in case there \nis no statistically signif\u00adicant difference between all the means and in case most of the meansareequalbutoneortwoaredifferent.Fora \nmore detailed discussion, we refer to the specialized literature. In summary, an ANOVAanalysis allows \nfor varying one input variable within theexperiment.Forexample,in case a benchmarker wants to compare \nthe performance of four virtual machines for a given benchmark, a given heap size and a given hardware \nplatform, the virtual machine then is the input variable and the four virtual machines are the four alternatives. \nAnother example where an ANOVA analysis can be used is when a benchmarker wants to compare the performanceofvariousgarbage \ncollectorsforagiven virtual machine, a given benchmark and a given system setup.  3.5 Multi-factor and \nmultivariate experiments Multi-factor ANOVA. The ANOVAanalysis discussed in theprevious sectionisaso \ncalled one-factorANOVA, mean\u00ading that only a single input variable can be varied during the setup. A \nmulti-factor ANOVA allows for studying the effect of multiple input variables and all of their interac\u00adtions, \nalong with an indication of the magnitude of the mea\u00adsurement error.Forexample, anexperiment where both \nthe garbage collector and the heap size arevaried, could provide deep insight into the effect on overall \nperformance of both thegarbage collector and the heap size individually as well as the interactionof \nboth thegarbage collector and the heap size. Multivariate ANOVA. The ANOVAanalyses discussed so far only \nconsider, what is called, a single dependent vari\u00adable. In a Java context, this means that an ANOVAanalysis \nonly allows for making conclusions about a single bench\u00admark. However, a benchmarker typically considers \na num\u00adber of benchmarks and is interested in the performance for allthe benchmarks itis importantfora \nperformanceeval\u00aduation study to consider a large enough set of representative benchmarks.Amultivariate \nANOVA(MANOVA)allows for considering multiple dependentvariables, or multiple bench\u00admarks, withinonesingleexperiment.Thekeypointofper\u00adforming \na MANOVAinstead of multiple ANOVAanalyses on the individual dependent variables, is that a MANOVA analysis \ntakes into account the correlation across the depen\u00addent variables whereas multiple ANOVAs do not.  \n3.6 Discussion In the previous sections, we explored a wide range of statis\u00adtical techniques and we discussed \nhow to apply these tech\u00adniques within a Java performance evaluation context. How\u00adever, using the more \ncomplex analyses, such as multi-factor ANOVA and MANOVA, raises two concerns. First, their output is \noften non-intuitive and in many cases hard to un\u00adderstand without deep background knowledge in statistics. \nSecond, as mentioned before, doing all the measurements re\u00adquired as input to the analyses can be very \ntime-consuming, up to the point where it becomesintractable.For these rea\u00adsons, we limit ourselves to \na Java performance evaluation methodology that is practical yet statistically rigorous. The methodology \nthat we present computes con.dence intervals which allows for doing comparisons between alternatives \non a per-benchmark basis, as discussed in sections 3.3 and 3.4. Of course,a benchmarkerwhoisknowledgeablein \nstatistics may perform more complex analyses. 4. Apractical statistically rigorous methodology Having \ndiscussed the general theory of statistics and how it relates to Java performance evaluation, we suggest \nmore practical and statistically rigorous methodologies for quan\u00adtifying startup and steady-state Java \nperformance by com\u00adbining a number of existing approaches. The evaluation sec\u00adtion in this paper then \ncompares the accuracy of prevalent data analysis methodologies against these statistically rigor\u00adous \nmethodologies. Notation. We refer to xij as the measurement of the j-th benchmark iteration of the i-th \nVM invocation. 4.1 Startup performance The goal of measuring start-up performance is to measure how quickly \na Java virtual machine can execute a relatively short-running Java program. There are twokey differences \nbetween startup and steady-state performance. First, startup performance includes class loading whereas \nsteady-state performance does not, and, second, startup performance is affectedby JIT compilation, substantially \nmore than steady\u00adstate performance. For measuring startup performance, we advocate a two\u00adstep methodology: \n1. Measure the execution time of multiple VM invocations, each VM invocation running a single benchmark \nitera\u00adtion. This results in p measurements xij with 1 = i = p and j =1. 2. Compute the con.dence interval \nfor a given con.dence level as described in Section 3.2. If there are more than 30 measurements, use \nthe standard normal z-statistic; otherwise use the Student t-statistic.  Recall that the central limittheory \nassumes that the mea\u00adsurements are independent. This may not be true in prac\u00adtice, because the .rst VM \ninvocation in a series of measure\u00adments may change system state that persists past this .rst VM invocation, \nsuch as dynamically loaded libraries persist\u00ading in physical memory or data persisting in the disk cache. \nTo reach independence, we discard the .rst VM invocation for each benchmark from our measurements and \nonly retain the subsequent measurements, as done by several other re\u00adsearchers; this assumes that the \nlibraries are loaded when doing the measurements.  4.2 Steady-state performance Steady-state performance \nconcerns long-running applica\u00adtions for which start-up is of less interest, i.e., the appli\u00adcation s \ntotal running time largely exceeds start-up time. Since most of the JIT compilation is performed during \nstart\u00adup, steady-state performance suffers less from variability due to JIT compilation. However, the \nother sources of non\u00addeterminism, such as thread scheduling and system effects, still remain under steady-state, \nand thus need to be consid\u00adered. There are two issues with quantifying steady-state per\u00adformance. The \n.rst issue is to determine when steady-state performance is reached. Long-running applicationstypically \nrun on large or streaming input data sets. Benchmarkers typ\u00adically approximate long-running benchmarksby \nrunningex\u00adisting benchmarks with short inputs multiple times within a single VM invocation, i.e., the \nbenchmark is iterated multi\u00adple times. The question then is how many benchmark iter\u00adations do we need \nto consider before we reach steady-state performance within a single VM invocation? This is a dif\u00ad.cult \nquestion to answer in general; the answer will differ from application to application, and in some cases \nit may take a very long time before steady-state is reached. The second issue with steady-state performance \nis that different VM invocations running multiple benchmark iter\u00adations may result in different steady-state \nperformances [2]. Different methods may be optimized at different levels of optimization across different \nVM invocations, changing steady-state performance. To address these two issues, we advocate a four-step \nmethodology for quantifying steady-state performance: 1. Consider p VM invocations, each VM invocation \nrunning at most q benchmark iterations. Suppose that we want to retain k measurements per invocation. \n2.For each VM invocation i, determine the iteration si where steady-state performance is reached, i.e., \nonce the coef.cient of variation (CoV)1 of the k iterations(si - k to si)falls below a preset threshold, \nsay 0.01 or 0.02. 3.For each VM invocation, compute the mean its con.dence interval. Or, if the desired \nlevel of precision is not reached after a preset number of runs, e.g., 30 runs, the obtained con.dence \ninterval is simply reported along with the sample mean. For steady-state performance, JavaStats collects \nexecu\u00adtion times across multiple VM invocations and across mul\u00adtiple benchmark iterations within a single \nVM invocation. JavaStats consists of a script running multiple VM invoca\u00adtions as well asabenchmark harness \ntriggering multiple iter\u00adations withina singleVMinvocation.The outputfor steady\u00adstate performance is \nsimilar to what is reported above for startup performance. SPECjvm98 as well as the DaCapo benchmark \nsuite al\u00adready come with a harness to set the desired number of benchmark iterations within a single \nVM invocation. The current version of the DaCapo harness also determines how manyiterations are needed \nto achieve a desired level of co\u00adef.cient of variation (CoV). As soon as the observed CoV drops below \na given threshold (the convergence target) for a xi of the k benchmark iterationsunder steady-state: \ngivenwindowof iterations,theexecutiontimeforthenextit\u00aderation is reported. JavaStats extends the existing \nharnesses si xi \u00af\u00af (i) by enabling measurements across multiple VM invoca\u00ad = xij. tions instead of a \nsingle VM invocation, and (ii) by comput\u00ad j=si-k ing and reporting con.dence intervals. These Java performance \nanalysis methodologies do not \u00af\u00af 4. Compute the con.dence interval for a given con.dence xi, and xi measure\u00ad \ncontrol non-determinism. However, a statistically rigorous data analysis approach can also be applied \ntogether with an level across the computed means from the different VM invocations. The overall mean \nequals \u00af=x p experimental design that controls the non-determinism, such as replay compilation. Con.dence \nintervals can be used to i=1 the con.dence interval is computed over the ments. quantify the remaining \nrandom .uctuations in the system under measurement. \u00af xi across ations within a single VM invocation \ni, and subsequently A.nal note that we would like to make is that collecting compute the con.dence interval \nacross the p VM invoca-the measurements for a statistically rigorous data analysis We thus .rst compute \nthe meanmultiple iter\u00ad xi means, see steps3and4from above. The reason for doing so is to reach independence \nacross the mea-large number of VM invocations and multiple benchmark \u00af surements from which we compute \nthe con.dence interval: iterations per VM invocation (in case of steady-state per\u00adthe various iterations \nwithin a single VM invocation are not formance). Under time pressure, statistically rigorous data canbe \ntime-consuming, especiallyiftheexperiment needsa tions using the \u00af xi VM invocations are independent. \n 4.3 In practice Tofacilitatethe applicationof these start-upand steady-state performance evaluation \nmethodologies, we provide publicly available software called JavaStats 2 that readily works with the \nSPECjvm98 and DaCapo benchmark suites.For startup performance, a script (i) triggers multiple VM invocations \nrunning a single benchmark iteration, (ii) monitors the exe\u00adcution time of each invocation, and (iii) \ncomputes the con.\u00addence interval foragiven con.dencelevel.If the con.dence interval achieves a desired \nlevel of precision, i.e., the con.\u00addence interval is within 1% or 2% of the sample mean, the script stops \ntheexperiment, and reportsthe sample mean and 1CoV is de.ned as the standard deviation s dividedby the \nmean x\u00af. 2Available athttp://www.elis.UGent.be/JavaStats/. independent, however, the mean valuesacross \nmultiple analysis can still be applied considering a limited number of measurements, however, the con.dence \nintervals will be looser. 5. Experimental Setup The next section will evaluate prevalent data analysis \nap\u00adproachesagainst statistically rigorous data analysis.For do\u00adingso,we consideranexperimentin whichwe \ncomparevar\u00adiousgarbage collection(GC) strategies similartowhatis being done in the GC research literature. \nThis section dis\u00adcusses the experimental setup: the virtual machine con.gu\u00adrations, the benchmarks and \nthe hardware platforms. 5.1 Virtual machine and GC strategies WeusetheJikes ResearchVirtual Machine(RVM)[1]which \nis an open source Java virtual machine written in Java. Jikes RVM employs baseline compilation to compile \na method upon its .rst execution; hot methods are sampled by an OS\u00adtriggered sampling mechanism and subsequently \nscheduled for further optimization. There are three optimization levels inJikesRVM:0,1and2.Weusethe February12,2007SVN \nversionof JikesRVMin allof ourexperiments. We consider .ve garbage collection strategies in to\u00adtal, all \nimplemented in the Memory Management Toolkit (MMTk) [6], the garbage collection toolkit provided with \ntheJikesRVM.The.vegarbage collection strategies are:(i) CopyMS, (ii) GenCopy, (iii) GenMS, (iv) MarkSweep, \nand (v) SemiSpace; the generational collectors use a variable\u00adsize nursery. GC poses a complex space-time \ntrade-off, and it is unclear which GC strategy is the winner without doing a detailedexperimentation.We \ndid not include the GenRC, MarkCompact and RefCount collectors from MMTk, be\u00adcause we were unable to \nsuccessfully run Jikes with the GenRC and MarkCompact collector for some of the bench\u00admarks; and RefCount \ndid yield performance numbers that are statistically signi.cantly worse than anyother GC strat\u00adegy across \nall benchmarks. 5.2 Benchmarks Table 3 shows the benchmarks used in this study. We use the SPECjvm98 \nbenchmarks [23] (.rst seven rows), as well as seven DaCapo benchmarks [7] (next seven rows). SPECjvm98isa \nclient-sideJava benchmark suite consisting of seven benchmarks.We run all SPECjvm98 benchmarks with the \nlargest input set(-s100). The DaCapo benchmark is a recently introduced open-source benchmark suite; \nwe use releaseversion 2006-10-MR2.We use the seven bench\u00admarks that execute properly on the February \n12, 2007 SVN version of Jikes RVM. We use the default (medium size) input set for the DaCapo benchmarks. \nIn all of our experiments, we consider a per-benchmark heap size range, following [5].Wevary the heap \nsize from a minimumheapsizeupto6timesthis minimumheapsize, in steps of 0.25 times the minimum heap size. \nThe per\u00adbenchmark minimum heap sizes are showninTable3.  5.3 Hardware platforms Following the advice \nby Blackburn et al. [7], we consider multiple hardware platforms in our performance evaluation methodology: \na 2.1GHz AMD Athlon XP, a 2.8GHz Intel Pentium 4, and a 1.42GHz Mac PowerPC G4 machine. The AMD Athlon \nand Intel Pentium4have 2GB of main mem\u00adory; the Mac PowerPC G4 has 1GB of main memory. These machines \nrun the Linux operating system, version 2.6.18. In allof ourexperimentswe consideran otherwise idleand \nun\u00adloaded machine. 6. Evaluation We now evaluate the proposed statistically rigorous Java performance \ndata analysis methodology in three steps.We .rst measure Java program run-time variability. In a second \nTable 3. SPECjvm98 (top seven) and DaCapo (bottom seven) benchmarks considered in this paper. The rightmost \ncolumn indicates the minimum heap size, as a multiple of 8MB, for which all GC strategies run to completion. \nbenchmark description min heap size (MB) compress .le compression 24 jess puzzle solving 32 db database \n32 javac Java compiler 32 mpegaudio MPEG decompression 16 mtrt raytracing 32 jack parsing 24 antlr parsing \n32 bloat Java bytecode optimization 56 fop PDF generation from XSL-FO 56 hsqldb database 176 jython Python \ninterpreter 72 luindex document indexing 32 pmd Java class analysis 64  Figure 2. Run-time variability \nnormalized to the mean exe\u00adcution time for start-up performance. These experiments as\u00adsume 30 VM invocations \non the AMD Athlon platform with the GenMS collector and a per-benchmark heap size that is twice as large \nas the minimal heap size reported inTable 3. The dot represents the median. step, we compare prevalent \nmethods from Section2against the statistically rigorous method from Section 3. And as a .nal step, we \ndemonstrate the use of the software provided to perform a statistically rigorous performance evaluation. \n6.1 Run-time variability The basic motivation for this work is that running a Java program introduces \nrun-time variability caused by non\u00addeterminism. Figure2demonstrates this run-timevariability for start-up \nperformance, and Figure3 shows the same for Figure 3. Run-time variability normalized to the mean ex\u00adecution \ntime for steady-state performance. These experi\u00adments assume 10 VM invocations and 30 benchmark iter\u00adations \nper VM invocation on the AMD Athlon platform with the GenMS collector and a per-benchmark heap size that \nis twice as large as the minimal heap size reported inTable 3. The dot represents the median. steady-state \nperformance3. Thisexperiment assumes30VM invocations (and a single benchmark iteration) for start-up \nperformance and 10 VM invocations and 30 benchmark it\u00aderations per VM invocation for steady-state performance. \nThese graphs show violin plots [13]4; all values are nor\u00admalized to a mean of one. A violin plot is very \nsimilar to a box plot. The middle point shows the median; the thick vertical line represents the .rst \nand third quartile (50% of all the data points are between the .rst and third quartile); the thinvertical \nline represents the upper andlower adjacent values (representing an extreme data point or 1.5 times the \ninterquartile range from the median, whichever comes .rst); and the top and bottom values show the maximum \nand min\u00adimum values. The important difference with a box plot is that the shape of the violin plot represents \nthe density: the wider the violin plot, the higher the density. In other words, a violin plot visualizes \na distribution s probability density function whereas a box plot does not. There are a couple of interesting \nobservations to be made from these graphs. First, run-time variability can be fairly signi.cant, for \nboth startup and steady-state performance. For most of the benchmarks, the coef.cient of variation (CoV), \nde.ned as the standard deviation divided by the mean, is around 2% and is higher for several benchmarks. \nSecond, the maximum performance difference between the maximum and minimum performance number varies \nacross the benchmarks, and is generally around 8% for startup and 20% for steady-state. Third, most of \nthe violin plots in Fig\u00ad 3The antlr benchmarkdoesnot appearinFigure3becausewe were unable to run more \nthan a few iterations within a single VM invocation. 4The graphs shown were made using R a freely available \nstatistical framework usingthe vioplot packageavailablefromtheCRAN(at http: //www.r-project.org). ures \n2 and 3 show that the measurement data is approxi\u00admately Gaussian distributed with thebulk weight around \nthe mean. Statistical analyses, such as theKolmogorov-Smirnov test,donotrejectthehypothesisthatinmostofour \nmeasure\u00adments the data is approximately Gaussian distributed in fact, this is the case for more than \n90% of the experiments done in this paper involving multiple hardware platforms, benchmarks, GC strategies \nand heap sizes. However, in a minority of the measurements we observe non-Gaussian distributed data; \nsome of these have skewed distributions or bimodal distributions. Note however that a non-Gaussian distribution \ndoes not affect the generality of the proposed statistically rigorous data analysis technique. As discussed \nin section 3.2.3, the central limit theory does not assume the measurement data to be Gaussian distributed; \nalso, ANOVA is robust towards non-normality.  6.2 Evaluating prevalent methodologies We now compare \nthe prevalent data analysis methodologies against the statistically rigorous data analysis approach ad\u00advocatedin \nthis paper.For doing so, we setup anexperiment in which we (pairwise) compare the overall performance \nof various garbage collectors over a range of heap sizes. We consider the various GC strategies as outlined \nin section 5, a range of heap sizes from the minimum heap size up to six times this minimum heap size \nin 0.25 minimum heap size increments there are 21 heap sizes in total. Computing con.dence intervals \nfor the statistically rigorous methodol\u00adogy is done, following section 3, by applying an ANOVA andaTukeyHSD \ntestto compute simultaneous95% con.\u00addence intervals for all the GC strategies per benchmark and per heap \nsize. To evaluate the accuracy of the prevalent performance evaluation methodologies we consider all \npossible pairwise GC strategy comparisons for all heap sizes considered.For each heap size, we then determine \nwhether prevalent data analysis leads to the same conclusion as statistically rigorous data analysis.In \notherwords, there are C2 = 10 pairwise GC 5 comparisons per heap size and per benchmark. Or, 210 GC comparisons \nin total across all heap sizes per benchmark. We now classify all of these comparisons in six cate\u00adgories, \nseeTable4, and then report the relative frequencyof each of these six categories. These results help \nus better un\u00adderstand the frequencyof misleading and incorrect conclu\u00adsions using prevalent performance \nmethodologies.We make a distinction between overlapping con.dence intervals and non-overlapping con.dence \nintervals, according to the sta\u00adtistically rigorous methodology. Overlapping con.dence intervals. Overlapping \ncon.dence intervals indicate that the performance differences observed may be due to random .uctuations. \nAs a result, any con\u00adclusion taken by a methodology that concludes that one al\u00adternative performs better \nthan another is questionable. The only valid conclusion with overlapping con.dence intervals  Table \n4. Classifying conclusions by a prevalent methodology in comparison to a statistically rigorous methodology. \nis that there is no statistically signi.cant difference between the alternatives. Performance analysistypically \ndoes not state that one al\u00adternative is better than another when the performance dif\u00adferenceisvery small \nthough.To mimicthis practice,wein\u00adtroduce a threshold . to classify decisions: a performance difference \nsmaller than . is considered a small performance difference anda performance difference larger than . \nis con\u00adsideredalarge performance difference.Wevary the . thresh\u00adold from 1% up to 3%. Now, in case the \nperformance difference by the prevalent methodology is considered large, we conclude the prevalent methodology \nto be misleading . In other words, the preva\u00adlent methodology says thereisa signi.cant performance dif\u00adference \nwhereas the statistics conclude that this performance difference may be due to random .uctuations. If \nthe perfor\u00admance difference is small based on the prevalent methodol\u00adogy, we consider the prevalent methodology \nto be indica\u00adtive . Non-overlapping con.dence intervals. Non-overlapping con.dence intervals suggest \nthat we can conclude that there are statistically signi.cant performance differences among the alternatives. \nThere are two possibilities for non\u00adoverlapping con.dence intervals.Ifthe rankingbythe statis\u00adtically \nrigorous methodology is the same as the ranking by the prevalent methodology, then the prevalent methodology \nis considered correct. If the methodologies have opposite rankings, then the prevalent methodology is \nconsidered to be incorrect. To incorporate a performance analyst s subjective judge\u00adment, modeled through \nthe . threshold from above, we make one more distinction based on whether the performance dif\u00adference \nis considered small or large. In particular, if the prevalent methodology states there is a small difference, \nthe conclusionis classi.edtobe misleading.Infact, thereisa statistically signi.cant performance difference, \nhowever, the performance difference is small. Wehavefour classi.cation categories for non-overlapping \ncon.dence intervals, seeTable 4. If the performance differ\u00adencebytheprevalent methodologyislarger than \n., and the ranking by the prevalent methodology equals the ranking by the statistically rigorous methodology, \nthen the prevalent methodology is considered to be correct ; if the prevalent methodology has the opposite \nranking as the statistically rigorous methodology, the prevalent methodology is consid\u00adered incorrect \n. In case of a small performance difference according to the prevalent methodology, and the same rank\u00ading \nas the statistically rigorous methodology, the prevalent methodology is considered to be misleading but \ncorrect ; in case of an opposite ranking, the prevalent methodology is considered misleading and incorrect \n. 6.2.1 Start-up performance We .rst focus on start-up performance. For now, we limit ourselves to prevalent \nmethodologies that do not use replay compilation. We treat steady-state performance and replay compilation \nin subsequent sections. Figure 4 shows the percentage GC comparisons by the prevalent data analysis approaches \nleading to indicative, misleading and incorrect conclusions for . = 1% and . = 2% thresholds. The various \ngraphs show different hard\u00adware platforms and different . thresholds. The various bars in these graphs \nshowvarious prevalent methodologies. There are bars for reporting the best, the second best, the worst, \nthe mean and the median performance number; for 3, 5, 10 and 30 VM invocations and a single benchmark \niteration for example, the best of 3 means taking the best perfor\u00admance number out of 3 VM invocations. \nThe statistically rigorous methodology that we compare against considers 30 VM invocations and a single \nbenchmark iteration per VM invocation, and considers 95% con.dence intervals. There areanumberof interesting \nobservationstobe made from these graphs. First of all, prevalent methods can be misleading in a substantial \nfraction of comparisons between alternatives, i.e., the total fraction misleading comparisons ranges \nup to 16%.In otherwords,inupto16%ofthe comparisons, the prevalent methodology makes too strong a statement \nsaying that one alternative is better than another.  Forafair numberof comparisons, the prevalent method\u00adology \ncan even lead to incorrect conclusions, i.e., the prevalent methodology says one alternative is better \n(by more than . percent) than another, whereas the statisti\u00adcally rigorous methodology takes the opposite \nconclusion based on non-overlapping con.dence intervals.For some prevalent methodologies, the fraction \nof incorrect com\u00adparisons can be more than 3%.  We also observe that some prevalent methodologies per\u00adform \nbetter than others. In particular, mean and median are consistently better than best, second best and \nworst. The accuracyof the mean and median methods seems to improve with the number of measurements, whereas \nthe best, second best and worst methods do not.  (a) AMD Athlon . = 1% (b) AMD Athlon . = 2% (c) \nIntelPentium4 . = 1% (d) IntelPentium4 . = 2% (e)PowerPCG4 . = 1% (f)PowerPCG4 . = 2% Figure 4. \nPercentage GC comparisons by prevalent data analysis approaches leading to incorrect, misleading or indicative \nconclusions. Results are shown for the AMD Athlon machine with . = 1% (a) and . = 2% (b), for the Intel \nPentium 4 machine with . = 1% (c) and . = 2% (d), and for the PowerPC G4 with . = 1% (e) and . = 2% (f). \n Increasing the . threshold reduces the number of in\u00adcorrect conclusions by the prevalent methodologies \nand at the same time also reduces the number of mislead\u00ading and correct conclusions. By consequence, \nthe num\u00adber of misleading-but-correct, misleading-and-incorrect and indicative conclusions increases, \nor, in other words, the conclusiveness of a prevalent methodology reduces with an increasing . threshold. \nFigure5shows the clas\u00adsi.cation as a function of the . threshold for the javac benchmark, which we found \nto be a representative ex\u00adample benchmark. The important conclusion here is that increasing the . threshold \nfor a prevalent methodology does not replace a statistically rigorous methodology.  One .nal interesting \nobservation that is consistent with the observations made by Blackburn et al. [7], is that the results \npresentedin Figure4 vary across different hard\u00adware platforms. In addition, the results also vary across \nbenchmarks, see Figure 6 which shows per-benchmark   Figure 7. The (in)accuracy of comparing the GenMS \nGC strategy against four other GC strategies using prevalent methodologies, for . =1%on the AMD Athlon \nmachine. results for the best prevalent method; we obtained sim\u00adilar results for the other methods. Some \nbenchmarks are more sensitive to the data analysis method than oth-ers.For example, jess and hsqldb are \nalmost insensitive, whereas other benchmarks have a large fraction mislead\u00ading and incorrect conclusions; \ndb and javac for example show more than 3% incorrect conclusions. AVM developer use case. Theevaluation \nsofar quanti.ed comparing all GC strategies against all other GC strategies, a special use case. Typically, \na researcher or developer is merely interestedin comparinganewfeatureagainst already existing approaches.Tomimic \nthis use case, we compareone GC strategy, GenMS, against all other four GC strategies. The results are \nshownin Figure7and arevery muchin line with the results presented in Figure 4: prevalent data analysis \nmethods are misleading in many cases, and in some cases even incorrect. An application developer use \ncase. Our next case study takes a look from the perspective of an application devel\u00adoper by looking at \nthe performance of a single benchmark. Figure8shows two graphs for db for the best of 30 and the con.dence \ninterval based performance evaluation methods. The different curves represent differentgarbage collectors. \nThese graphs clearly show that different conclusions may be (a) mean of 30 measurements with a 95% con.dence \ninterval  Figure 8. Startup execution time (in seconds) for db as a functionof heap size for.vegarbage \ncollectors; meanof30 measurements with 95% con.dence intervals (top) and best of 30 measurements (bottom). \ntaken depending on theevaluation method used.Forexam\u00adple, for heap sizes between 80MB and 120MB, one \nwould conclude using the best method that CopyMS clearly out\u00adperforms MarkSweep and performs almost equally \nwell as GenCopy. However, the con.dence intervals show that the performance difference between CopyMS \nand MarkSweep could be due to random .uctuations, and in addition, the sta\u00adtistically rigorous method \nclearly shows that GenCopysub\u00adstantially outperforms CopyMS. Figure9 shows similar graphs for antlr. \nFigure 10 sug\u00adgests that for large heap sizes (6 to 20 times the minimum) most of the performance differences \nobserved between the CopyMS, GenCopy and SemiSpacegarbage collectors are due to non-determinism. These \nexperiments clearly illus\u00adtrate that both experimental design and data analysis are important factors \nin a Java performance analysis method\u00adology. Experimental design may reveal performance differ\u00adences \namong design alternatives,but without statistical data analysis, we do not knowif these differences are \nmeaningful.  6.2.2 Steady-state performance Figure 11 shows normalized execution time (averaged over \na number of benchmarks) as a function of the number iter\u00adations for a single VM invocation. This graph \nshows that it takesa numberof iterations before steady-state performance is reached: the .rst 3 iterations \nobviously seem to be part (a) mean of 30 measurements with a 95% con.dence interval (a) mean of 30 measurements \nwith a 95% con.dence interval (b) best of 30 measurements Figure 9. Startup execution time (in seconds) \nfor antlr as a functionof heap size for.vegarbage collectors; meanof30 measurements with 95% con.dence \nintervals (top) and best of 30 measurements (bottom). of startup performance, and it takes more than \n10 iterations before we actually reach steady-state performance. For quantifying steady-state performance, \nfollowing Sec\u00adtion 4.2, we retain k = 10 iterations per VM invocation for which the CoV is smaller than \n0.02. Figure 12 com\u00adpares three prevalent steady-state performance methodolo\u00adgies against the statistically \nrigorous approach: (i) best of median (take the median per iteration across all VM invoca\u00adtions, and \nthen select the best median iteration), (ii) best per\u00adformance number, and (iii) second best performance \nnum\u00adber across all iterations and across all VM invocations.For these prevalent methods we consider1,3and5VMinvoca\u00adtions \nand 3, 5, 10 and 30 iterations per VM invocation. The general conclusion concerning the accuracyof the \nprevalent methods is similar to those for startup performance. Preva\u00adlent methods are misleading in more \n20% of the cases for a . = 1% threshold, more than 10% for a . = 2% threshold, and more than5%fora . \n= 3% threshold. Also, the number of incorrect conclusions is not negligible (a few percent for small \n. thresholds).  6.2.3 Replay compilation Replay compilation is an increasingly popular experimental \ndesign setup that removes the non-determinism from com\u00adpilation in the VM. It is particularly convenient \nfor speci.c topics of research. One suchexample is GC research: replay compilation enables the experimenter \nto focus on GC per\u00ad  Figure 11. Normalized execution time as a function of the number of iterations \non the AMD Athlon machine. formance while controlling non-determinism by the VM s adaptive compilation \nand optimization subsystem. The goal of this section is twofold. First, we focus on experimental design \nand quantify how replay compilation compares against non-controlled compilation, assuming sta\u00adtistically \nrigorous data analysis. Second, we compare preva\u00adlent data analysis techniques against statistically \nrigorous data analysis under replay compilation. In our replay compilation approach, we analyze7bench\u00admark \nruns in separate VM invocations and take the optimal (yieldingthe shortestexecutiontime) compilation \nplan.We alsoevaluated the majority plan and obtained similar results. (a) . = 1% (b) . = 2% tions \nand y iterations per VM invocation; for SPECjvm98 on the AMD Athlon machine. The compilation plan is \nderived for start-up performance us\u00ading the GenMS con.guration with a 512MB heap size. The timing run \nconsists of two benchmark iterations: the .rst one, called mix, includes compilation activity, and the \nsec\u00adond one, called stable, does not include compilation activity. A full GC is performed between these \ntwo iterations. The timing runsare repeated multipletimes(3,5,10and30times in our setup). Experimental \ndesign. Figure 13 compares mix replay ver\u00adsus startup performance as well as stable replay versus steady-state \nperformance, assuming non-controlled compi\u00adlation. We assume statistically rigorous data analysis for \nboth the replay compilation and non-controlled compila\u00adtion experimental setups. We classify all GC comparisons \nin three categories: agree , disagree and inconclusive , see Table 5, and display the disagree and inconclusive \ncategories in Figure 13.We observe replay compilation and non-controlled compilation agree in 56% to \n72% of all cases, and are inconclusive in 17% (DaCapo mix versus startup) to 37% (SPECjvm98 stable versus \nsteady-state) of all cases. In up to 12% of all cases, see SPECjvm98 mix versus startup (a) mix vs. startup \n(b) stable vs. steady-state  overlapping intervals non-controlled compilation non-overlapping intervals, \nA>B non-overlapping intervals, B>A replay compilation overlapping intervals agree inconclusive inconclusive \nnon-overlapping intervals A>B B>A inconclusive agree disagree disagree agree Table 5. Classifying conclusions \nby replay compilation versus non-controlled compilation. (a) jess  Figure 15. Comparing prevalent data \nanalysis versus statis\u00adtically rigorous data analysis under stable replay compila\u00adtion, assuming . = \n1% on the AMD Athlon platform.  and DaCapo stable versus steady-state, both experimental designs disagree. \nThese two experimental designs offer dif\u00adferent garbage collection loads and thus expose different space-time \ntrade-offs that the collectors make. Data analysis. We now assume replay compilation as the experimental \ndesign setup, and compare prevalent data anal\u00adysis versus statistically rigorous data analysis. Figures \n14 and 15 show the results for mix replay versus startup perfor\u00admance, and stable replay versus steady-state \nperformance, respectively. These results show that prevalent data analysis can be misleading under replay \ncompilation for startup per\u00adformance: the fraction misleading conclusions is around 5%, see Figure 14.For \nsteady-state performance, the number of misleading conclusions is less than 4%, see Figure 15.  6.3 \nStatistically rigorous performance evaluation in practice As discussed in Section 3, the width of the \ncon.dence inter\u00adval is a function of the number of measurements n. In gen\u00aderal, the width of the con.dence \ninterval decreases with an increasing number of measurements as shown in Figure 16. The width of the \n95% con.dence interval is shown as a per\u00adcentageofthe mean(ontheverticalaxis)andasafunctionof the number \nof measurements taken (on the horizontal axis). We show three example benchmarks:jess, db and mtrt for \na 80MB heap size on the AMD Athlon machine. The various curves represent differentgarbage collectors \nfor start-up per\u00adformance. The interesting observation here is that the width of the con.dence interval \nlargely depends on both the bench\u00admark and thegarbage collector.Forexample, the widthof the con.dence \ninterval for the GenCopycollector for jess is fairlylarge, more than3%,evenfor30 measurements.For the \nMarkSweep and GenMS collectors for db on the other hand, the con.dence interval is much smaller, around \n1% even after less than 10 measurements. These observations motivated us to come up with an au\u00adtomated \nway of determining how many measurements are needed to achievea desired con.dence interval width.For \nexample, for db and the MarkSweep and GenMS collectors, a handful of measurements will suf.ce to achieve \na very small con.dence interval, whereas for jess and the GenCopy collector many more measurements are \nneeded. JavaStats consists of a script (to initiate multiple VM invocations) and a harness (to initiate \nmultiple benchmark iterations within a single VM invocation) that computes the width of the con\u00ad.dence \ninterval while the measurements are being taken. It takes as input the desired con.dence interval width \n(for ex\u00adample 2% or 3%) for a given con.dence level and a maxi\u00admum number of VM invocations and benchmark \niterations. JavaStats stops the measurements and reports the con.dence interval as soon as the desired \ncon.dence interval width is achieved or the maximum number of VM invocations and benchmark iterations \nis reached. Figure 17 reports the number of VM invocations required for start-up performanceto achievea2% \ncon.dence interval width with a maximum number of VM invocations, p = 30 for jess, db and mtrt on the \nAMD Athlon as a function of heap size for the.vegarbage collectors. The interesting ob\u00adservation here \nis that the number of measurements taken varies from benchmark to benchmark, from collector to col\u00adlector \nand from heap size to heap size. This once again shows why an automated way of collecting measurements \nis desir\u00adable. Having to take fewer measurements for a desired level of con.dence speedsuptheexperiments \ncomparedto taking a .xed number of measurements. 7. Summary Non-determinism due to JIT compilation, thread \nschedul\u00ading, garbage collection and various system effects, makes quantifying Java performance far from \nbeing straightfor\u00adward. Prevalent data analysis approaches deal with non\u00addeterminism in a wide variety \nof ways. This paper showed that prevalent data analysis approaches can be misleading and can even lead \nto incorrect conclusions. This paper introduced statistically rigorous Java per\u00adformance methodologies \nfor quantifying Java startup and steady-state performance. In addition, it presented JavaStats, publicly \navailable software to automatically perform rigor\u00adous performance evaluation. For startup performance, \nwe (a) jess  Figure 17. Figure shows how many measurements are re\u00adquired before reachinga2% con.dence \nintervalontheAMD Athlon machine. run multiple VM invocations executing a single benchmark iteration and \nsubsequently compute con.dence intervals.For steady-state performance, we run multiple VM invocations, \neachexecuting multiple benchmarkiterations.Wethen com\u00adputea con.dence interval based on the benchmark \niterations across the various VM invocations once performance vari\u00adability drops below a given threshold. \nWe believe this paper is a step towards statistical rigor invarious performanceevaluation studies.Java \nperformance analysis papers, and papers presenting experimental results in general, very often report \nperformance improvements be\u00adtween two or more alternatives. Most likely, if the perfor\u00admance differences \nbetween the alternatives are large, a sta\u00adtistically rigorous method will not alter the overall picture \nnor affect the general conclusions obtained using prevalent methods. However, for relatively small performance \ndiffer\u00adences (that are within the margin of experimental error), not using statistical rigor may lead \nto incorrect conclusions. Acknowledgments We would like to thank Steve Blackburn, Michael Hind, Matthew \nArnold, Kathryn McKinley, and the anonymous reviewers for their valuable comments their detailed suggestions \ngreatly helped us improving this paper. Andy Georges is supported by Ghent University. Dries Buytaert \nis supported by the Institute for the Promotion of Innova\u00adtionby Science andTechnologyin Flanders (IWT). \nLieven Eeckhout is a Postdoctoral Fellow of the Fund for Scienti.c Research Flanders (Belgium) (FWO Vlaanderen). \nReferences [1] M. Arnold, S. Fink, D. Grove, M. Hind, andP.F. Sweeney. Adaptive optimization in the Jalape \nIn OOPSLA, no JVM. pages 47 65, Oct. 2000. [2] M. Arnold, M. Hind, and B. G. Ryder. Online feedback\u00addirected \noptimization of Java. In OOPSLA, pages 111 129, Nov. 2002. [3] K. Barabash,Y. Ossia, and E. Petrank. \nMostly concurrent garbage collectionrevisited. In OOPSLA, pages 255 268, Nov. 2003. [4] O. Ben-Yitzhak, \nI. Goft, E. K. Kolodner, K. Kuiper, and V. Leikehman. An algorithm for parallel incremental compaction. \nIn ISMM, pages 207 212, Feb. 2003. [5] S. Blackburn, P. Cheng, and K. McKinley. Myths and reality: The \nperformance impact ofgarbage collection. In SIGMETRICS, pages 25 36, June 2004. [6]S. Blackburn,P. Cheng,andK. \nMcKinley. Oilandwater? High performancegarbage collectioninJavawith JMTk.In ICSE, pages 137 146, May \n2004. [7] S. M. Blackburn, R. Garner,C. Hoffmann, A. M. Khang, K. S. McKinley, R. Bentzur, A. Diwan, \nD. Feinberg, D. Frampton, S. Z. Guyer, M. Hirzel, A. Hosking, M. Jump, H. Lee, J. E. B. Moss,A. Phansalkar,D.Stefanovic,T.VanDrunen,D.von \nDincklage, and B.Wiedermann. The DaCapo benchmarks: Java benchmarking development and analysis. In OOPSLA, \npages 169 190, Oct. 2006. [8] S. M. Blackburn and K. S. McKinley. In or out?: Putting write barriers \nin their place. In ISMM, pages 281 290, June 2002. [9] S. M. Blackburn and K. S. McKinley. Ulterior reference \ncounting:Fastgarbage collection withouta longwait. In OOPSLA, pages 344 358, Oct. 2003. [10] L. Eeckhout, \nA. Georges, and K. De Bosschere. How Java programs interact with virtual machines at the microarchitec\u00adtural \nlevel. In OOPSLA, pages 169 186, Oct. 2003. [11] D. Gu, C.Verbrugge, and E. M. Gagnon. Relativefactors \nin performance analysis of Java virtual machines. In VEE, pages 111 121, June 2006. [12] M. Hauswirth, \nP. F. Sweeney, A. Diwan, and M. Hind. Vertical pro.ling: Understanding the behavior of object\u00adpriented \napplications. In OOPSLA, pages 251 269, Oct. 2004. [13] J.L. Hintze, and R.D. Nelson. Violin Plots: A \nBox Plot-Density Trace Synergism In The American Statistician, Volume 52(2), pages 181 184, May 1998. \n[14] X. Huang, S. M. Blackburn, K. S. McKinley, J. E. B. Moss, Z.Wang, andP. Cheng. Thegarbage collection \nadvantage: Improving program locality. In OOPSLA, pages 69 80, Oct. 2004. [15] R.A. Johnson and D.W. \nWichern Applied Multivariate Statistical Analysis Prentice Hall, 2002 [16] D. J. Lilja. Measuring ComputerPerformance:A \nPracti\u00adtioner s Guide. Cambridge University Press, 2000. [17] J. Maebe, D. Buytaert, L. Eeckhout, and \nK. De Bosschere. Javana: A system for building customized Java program analysis tools. In OOPSLA, pages \n153 168, Oct. 2006. [18] P. McGacheyand A. L. Hosking. Reducing generational copy reserveoverhead withfallback \ncompaction. In ISMM, pages 17 28, June 2006. [19] J. Neter,M.H.Kutner,W.Wasserman, andC.J. Nachtsheim \nApplied Linear Statistical Models WCB/McGraw-Hill, 1996. [20] N. Sachindran and J. E. B. Moss. Mark-copy:Fast \ncopying GC with less space overhead. In OOPSLA, pages 326 343, Oct. 2003. [21] K. Sagonas and J.Wilhelmsson. \nMark and split. In ISMM, pages 29 39, June 2006. [22] D. Siegwart and M. Hirzel. Improving locality with \nparallel hierarchical copying GC. In ISMM, pages 52 63, June 2006. [23] Standard Performance Evaluation \nCorporation. SPECjvm98 Benchmarks. http://www.spec.org/jvm98. [24]P.F. Sweeney,M. Hauswirth,B. Cahoon,P. \nCheng,A.Diwan, D. Grove, and M. Hind. Using hardware performance monitors to understand the behavior \nof Java applications. In VM, pages 57 72, May 2004. [25] C. Zhang, K. Kelsey, X. Shen, C. Ding, M. Hertz, \nand M. Ogihara. Program-level adaptive memory management. In ISMM, pages 174 183, June 2006.   \n\t\t\t", "proc_id": "1297027", "abstract": "<p>Java performance is far from being trivial to benchmark because it is affected by various factors such as the Java application, its input, the virtual machine, the garbage collector, the heap size, etc. In addition, non-determinism at run-time causes the execution time of a Java program to differ from run to run. There are a number of sources of non-determinism such as Just-In-Time (JIT) compilation and optimization in the virtual machine (VM) driven by timer-based method sampling, thread scheduling, garbage collection, and various.</p> <p>There exist a wide variety of Java performance evaluation methodologies usedby researchers and benchmarkers. These methodologies differ from each other in a number of ways. Some report average performance over a number of runs of the same experiment; others report the best or second best performance observed; yet others report the worst. Some iterate the benchmark multiple times within a single VM invocation; others consider multiple VM invocations and iterate a single benchmark execution; yet others consider multiple VM invocations and iterate the benchmark multiple times.</p> <p>This paper shows that prevalent methodologies can be misleading, and can even lead to incorrect conclusions. The reason is that the data analysis is not statistically rigorous. In this paper, we present a survey of existing Java performance evaluation methodologies and discuss the importance of statistically rigorous data analysis for dealing with non-determinism. We advocate approaches to quantify startup as well as steady-state performance, and, in addition, we provide the JavaStats software to automatically obtain performance numbers in a rigorous manner. Although this paper focuses on Java performance evaluation, many of the issues addressed in this paper also apply to other programming languages and systems that build on a managed runtime system.</p>", "authors": [{"name": "Andy Georges", "author_profile_id": "81100487568", "affiliation": "Ghent University, Ghent, Belgium", "person_id": "P643379", "email_address": "", "orcid_id": ""}, {"name": "Dries Buytaert", "author_profile_id": "81100468396", "affiliation": "Ghent University, Ghent, Belgium", "person_id": "P698113", "email_address": "", "orcid_id": ""}, {"name": "Lieven Eeckhout", "author_profile_id": "81330490198", "affiliation": "Ghent University, Ghent, Belgium", "person_id": "PP40033523", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1297027.1297033", "year": "2007", "article_id": "1297033", "conference": "OOPSLA", "title": "Statistically rigorous java performance evaluation", "url": "http://dl.acm.org/citation.cfm?id=1297033"}