{"article_publication_date": "10-21-2007", "fulltext": "\n Using Early Phase Termination To Eliminate Load Imbalances At Barrier Synchronization Points Martin \nRinard Department of Electrical Engineering and Computer Science Computer Science and Arti.cial Intelligence \nLaboratory Massachusetts Institute of Technology Cambridge, MA 021139 ...... ............ Abstract We \npresent a new technique, early phase termination,for eliminating idle processors in parallel computations \nthat use barrier synchronization. This technique simply terminates each parallel phase as soon as there \nare too few remaining tasks to keep all of the processors busy. Although this technique completely eliminates \nthe idling that would otherwise occur at barrier synchronization points, it may also change the computation \nand therefore the result that the computation produces. We address this issue by pro\u00adviding probabilistic \ndistortion models that characterize how the use of early phase termination distorts the result that the \ncomputation produces. Our experimental results show that for our set of benchmark applications, 1) early \nphase termi\u00adnation can improve the performance of the parallel compu\u00adtation, 2) the distortion is small \n(or can be made to be small with the use of an appropriate compensation technique) and 3) the distortion \nmodels provide accurate and tight distor\u00adtion bounds. These bounds can enable users to evaluate the effect \nof early phase termination and con.dently accept re\u00adsults from parallel computations that use this technique \nif they .nd the distortion bounds to be acceptable. Finally, we identify a general computational pattern \nthat works well with early phase termination and explain why computations that exhibit this pattern can \ntolerate the early termination of parallel tasks without producing unacceptable results. Categories and \nSubject Descriptors D.1.2 [Programming Techniques]: Concurrent Programming; D.2.5 [Software Engineering]: \nTesting and Debugging; Permission to make digital or hard copies of all or part of this work for personal \nor classroom use is granted without fee provided that copies are not made or distributed for pro.t or \ncommercial advantage and that copies bear this notice and the full citation on the .rst page. To copy \notherwise, to republish, to post on servers or to redistribute to lists, requires prior speci.c permission \nand/or a fee. OOPSLA 07, October 21 25, 2007, Montr\u00b4eal, Qu\u00b4ebec, Canada. Copyright c .2007 ACM 978-1-59593-786-5/07/0010. \n. . $5.00 D.3.3 [Programming Languages]: Language Constructs and Features General Terms Performance, \nReliability, Design, Lan\u00adguages Keywords Barrier Synchronization, Parallel Computing, Probabilistic Distortion \nModels, Early Phase Termination  1. Introduction Many parallel programs exhibit a parallelism pattern \ncon\u00adsisting of alternating parallel and serial phases. Each parallel phase consists of a set of tasks \nthat can execute in parallel. When all of the tasks .nish, a serial phase consisting of a single task \ncontinues the computation until the start of the next parallel phase. Each serial phase typically accesses \ndata from all of the tasks in the preceding parallel phase. These tasks must therefore .nish before the \nsubsequent serial phase can begin. The synchronization that accomplishes this tem\u00adporal separation is \ncalled barrier synchronization because it imposes a temporal barrier that separates the two phases. A \nwell-known issue with barrier synchronization is bar\u00adrier idling, which occurs at the end of the parallel \nphase when there are few tasks left to execute. If there is a mis\u00admatch in processor speeds or task sizes, \nprocessors may be left idle as they wait for the remaining tasks to .nish. This idling can limit the \nperformance of the parallel computation. We propose a simple solution, early phase termination, to barrier \nidling: when there are too few tasks to keep all of the processors busy at the end of the parallel phase, \nsimply terminate the remaining tasks and proceed immediately to the subsequent serial phase. This solution \ncompletely elim\u00adinates any barrier idling. It may, however, change the com\u00adputation and therefore the \nresult that the computation pro\u00adduces. There are two potential questions: 1) how much does the result \nchange, and 2) how predictable is the change? If the change is suf.ciently small and predictable, users \nmay be willing to accept the perturbed result in exchange for the in\u00adcreased performance associated with \nthe elimination of bar\u00adrier synchronization. 1.1 Distortion Models To enable the user to evaluate the \npotential impact of early phase termination on the results of their computations, we build a statistical \ndistortion model that characterizes the ef\u00adfect of terminating tasks [20]. We obtain and use this model \nas follows: . Task Decomposition: The developer speci.es the task decomposition by identifying task blocks \nin the program. Each task block consists of a block of code whose ex\u00adecution corresponds to a task in \nthe computation. Note that a given task block may execute many times during the course of a computation \nand may therefore generate many tasks into the computation. . Phase Identi.cation: The developer identi.es \nthe paral\u00adlel and serial phases in the computation. . Baseline: We obtain several sample inputs for which \nthe program is known to generate correct output. We run the program on these inputs, executing every \ntask to completion, and record the outputs that it generates. . Criticality Testing: We con.gure the \nexecution platform to randomly skip executions of selected tasks at target skip rates. We then select \neach task block in the program in turn, skip executions of that task block at a targeted rate, and observe \nthe resulting output distortion. If the task skips produce unacceptable distortion or cause the computation \nto fail, we mark the task block as critical, otherwise we mark the task block as skippable. . Distortion \nModel: Given a set of skippable task blocks, we run repeated trials in which we randomly select a tar\u00adget \ntask skip rate for each skippable task block, execute the computation, then record both the observed \ntask skip rates and the resulting output distortion. We then use re\u00adgression [8] to obtain a probabilistic \nmodel that estimates the distortion as a function of the task skip rates. . Production Runs: We use early \nphase termination in the production runs to eliminate barrier idling. We use the re\u00adsulting observed \nproportion of early terminated tasks and the distortion model to obtain an estimated distortion and con.dence \ninterval around the distortion. The estimated distortion and con.dence interval allow the user to deter\u00admine \nwhether or not to accept the result. We have applied this approach to several benchmark ap\u00adplications \nselected from the Jade benchmark suite [17]. Our results show that the models are quite accurate (they \nhave good statistical properties and usually explain almost all of the observed variation in the distortion \ndata) and that the es\u00adtimated distortion and con.dence intervals are small enough to provide useful distortion \nbounds in practice. And it is pos\u00adsible to use models developed on one set of inputs to accu\u00adrately predict \ndistortion properties for other inputs. Finally, our results indicate that the elimination of barrier \nidling can reduce the overall execution time and increase the perfor\u00admance of our benchmark applications. \n 1.2 Computation Pattern Based on our experience with our benchmark applications, we have identi.ed a \ngeneral computation pattern that inter\u00adacts well with our approach. Speci.cally, the parallel phases \nin our applications generate many contributions to a .nal result, then combine the contributions to obtain \nthe result. If each parallel task either generates or combines contribu\u00adtions, the net effect of terminating \ntasks early is simply to discard some of the contributions. Our results indicate that the distortion \nassociated with discarding these contributions is often quite small (or that it is possible to accurately \ncom\u00adpensate for the discarded contributions). Identifying this pattern provides a conceptual framework \nthat can help users better understand the effect of early phase termination on their computations. One \nof our benchmark applications, for example, performs a Monte Carlo simula\u00adtion in which each set of trials \ncomprises a contribution. The net effect of early phase termination is simply to drop some of the trials. \nAnother application traces a set of rays through a medium to compute the density of different parts of \nthe medium. The net effect of early phase termination is simply to drop some of the traced rays. We anticipate \nthat this kind of understanding, which translates the effect of early phase termination back into concepts \nfrom the underlying application domain, can help users determine if they are comfortable using the technique. \nIn the two examples above, it is clear that, in practice, both computations have enough redundancy (either \nfrom other tri\u00adals or other traced rays) to easily tolerate the loss of some of the contributions without \nsubstantially impairing the utility of the .nal result. Indeed, users may .nd that understanding the \neffects of early phase termination at the level of the ap\u00adplication domain may provide a more compelling \ncase for its use than the (in comparison relatively opaque) probabilistic distortion models. 1.3 A Broader \nPerspective To place early phase termination in a broader perspective, consider that almost all scienti.c \ncomputations are inher\u00adently inaccurate in that they are designed to produce an ap\u00adproximation to an \nideal result rather than the ideal result it\u00adself. Many computational chemistry programs, for example, \nuse classical or semi-empirical methods instead of ab initio quantum mechanics (which is computationally \nintractable for larger molecules) [10]. Many scienti.c computations dis\u00adcretize conceptually continuous \n.elds to enable the represen\u00adtation and computation of approximate solutions on digital computers, with \nthe granularity of the discretization deter\u00admined, in part, by factors such as the amount of available \nmemory and computational power [15]. As these examples illustrate, accuracy versus performance tradeoffs \nstrongly in\u00ad.uence the form and basic approach of almost all scienti.c computations. The key question \nfor such computations, then, is not cor\u00adrectness or incorrectness, but accuracy and pragmatic feasi\u00adbility \n does the computation produce a result that is close enough to the ideal result within an acceptable \ntime frame, and, if so, can the user determine that the result is acceptably close the ideal result? \nViewed from this perspective, early phase termination is simply yet another technique for ob\u00adtaining \nacceptable performance at the potential cost of some accuracy. And the probabilistic distortion models \nenable the user to evaluate the effect of the technique and determine whether or not the .nal result \nis acceptably accurate. It is worth considering how these issues might play out in different application \ndomains. Many information retrieval computations, for example, map a subcomputation over a set of discrete \nitems, then combine the results, with the over\u00adall computation able to tolerate the loss of some of the \nsub\u00adcomputations [7]. Because of the ability of the human sen\u00adsory system to tolerate noise and other \nprocessing artifacts, computer graphics and other sensory processing computa\u00adtions can often drop subcomputations \nwithout unacceptably degrading the .nal output [14, 19]. As these examples il\u00adlustrate, early phase termination \nmay be feasible in practice for a wide range of computations, including computations for which probabilistic \ndistortion models may not be readily available. 1.4 Contributions This paper makes the following contributions: \n. Elimination of Barrier Idling: It introduces the concept of eliminating barrier idling by terminating \ntasks at the end of parallel phases. . Distortion Models: It introduces the use of probabilis\u00adtic distortion \nmodels for characterizing the effect of early phase termination on the result that the program gener\u00adates. \nThese models provide an estimate of the distortion and probabilistic accuracy bounds around this distortion \nestimate. .Explanation: It identi.es a computation pattern that works well with early phase termination \nand explains why computations that exhibit this pattern can terminate tasks before they complete and \nstill produce acceptable results. . Experimental Results: It provides experimental results for our set \nof benchmark applications. These results indi\u00adcate that the distortion and accuracy bounds are small \nenough for practical use and that eliminating barrier idling can improve the overall parallel performance. \nThe remainder of the paper is structured as follows. In Section 2 we provide an example that illustrates \nour pro\u00adgramming model. Section 3 presents the methodology we use to obtain the distortion models. Section \n4 presents our experience applying our technique to a set of scienti.c com\u00adputations. We present related \nwork in Section 5 and conclude in Section 6.  2. Programming Model Figure 1 presents a simple procedure, \nthe ...procedure, that we use to illustrate the programming model. This pro\u00adcedure is written in the \nJade programming language [17]; it computes and returns the sum of .numbers. The Jade .....construct \non line .indicates that the loop from lines .through .comprise a parallel phase. All of the tasks in \nthis phase must complete before the program continues past line ... The task block on lines .and .uses \nthe Jade ........ construct to specify that the computation of each number is a task. Each task computes \na number ....and adds the number into the corresponding partial sum ......The ........access speci.cation \non line .uses the access speci.cation operations .....and .....to specify that the code in the body of \nthe task may, when it executes, read and/or write the shared object which .references. The code starting \nat line ..following the parallel phase is the subsequent sequential phase. The loop on lines .. through \n..adds up the partial sums into the .nal sum .... The ...procedure returns ...at line ... ................................... \n........... .............. ........ ................... .. ........ .. ...................... .. .................. \n... .... ......... .................... ........... ................... ........... .... ............. \n.... Figure 1. Example Jade Program When it executes this program, the Jade implementation creates a \nnew task for every execution of the task block on lines .and .. It dynamically analyzes the data dependences \nbetween tasks to exploit any available concurrency; Jade implementations exist for a variety of parallel \ncomputing platforms [17]. To support the development of the distortion models, we modi.ed the Jade implementation \nto accept a target task skip rate for each task block. When the program runs, the Jade implementation \nrandomly skips the corresponding tasks at the speci.ed rates. To construct the distortion model, our \ntechnique .rst runs the program on several inputs for which it is known to produce correct output. It \nrecords the output (in this case the value of ...that ...returns), then runs a sequence of trials that \nrandomly skip executions of each task block at a randomly selected skip rate. For each trial it records \nthe output of the computation and the actual task skip rate for each task block. To quantify the impact \nof the task skips on the output, the technique computes the distortion associated with each trial. In \nour example the distortion is simply ..... ....,where .is the correct output and . is the observed output \nfrom the trial with task skips. Dividing the difference ... by . makes it possible to meaningfully compare \ndistortions from executions with different correct outputs. The result of the sampling phase is a set \nof observations ...,where .is the actual task skip rate for the task block in the program and .is the \nobserved distortion. Our technique takes this set of observations and uses regression [8] to ob\u00adtain \na linear model ....................of the dis\u00adtortion. Here .and .are the regression coef.cients and \n.. and ..provide the con.dence bounds for these coef.cients. For the program in our example, the regression \nproduces the following distortion model. . ............................. In this model ..., which correctly \nestimates that there should be no distortion if there are no skipped tasks. The coef.cient ..., which \nindicates that every increase in the task skip rate produces the same increase in the distortion. So, \nfor example, an increase of 10% in the task skip rate would produce an increase in the distortion of \n0.10.1 In addition to the model, the regression algorithm pro\u00advides a variety of statistics that evaluate \nhow well the model .ts the data. In our example ..is 1, which means that the model perfectly explains \nthe variation in the data. Finally, given a task skip rate ., the regression can provide a con.\u00addence \nbound .around the estimated distortion .....In our example the maximum 95% con.dence bound over all of \nthe 2064 sampled task skip rate points is 0.0025, which provides a tight con.dence interval around the \nestimated dis\u00adtortion. We anticipate the following usage scenario. The user has obtained the model and \nnow runs the program in parallel with early phase termination enabled. The program comes back with an \noutput . . It also informs the user that it termi\u00adnated several tasks early at the end of parallel phases. \nThe 1 It turns out that for this example it is possible to apply the bias compen\u00adsation technique discussed \nin Section 3.8 to obtain an estimator with an expected distortion of 0.0 for task failure rates within \nthe sampled range of 0.0 to 0.75. This technique enables the program to acceptably tolerate much higher \ntask skip rates. user plugs the early task terminate rate into the model to get an estimated distortion \n....and con.dence bound .. The user then evaluates the distortion and con.dence bound to determine if, \nwith high enough likelihood, the distortion is acceptable. 3. Distortion Models We obtain and evaluate \nthe distortion model for each pro\u00adgram as follows. 3.1 Standard Executions Our approach applies to programs \nthat produce an output of the form ........., where each output component ..is a number. We obtain several \nrepresentative test inputs for which the program is known to execute without failures, run the program \non these inputs, and record the correct output .........for each input. 3.2 Distortion De.nition Given \na correct output .........and an observed output . ....... ., the following quantity ., which we call \nthe dis\u00adtortion, measures the accuracy of the observed output: .. . . . ..... ..  .... .... . ... The \ncloser the distortion is to zero, the less distorted is the observed output. Note that because each difference \n.... .is scaled by the corresponding correct output component ..and because the sum is divided by the \nnumber of output components .,it is possible to meaningfully compare distortions .obtained from executions \non different inputs even if the inputs cause the program to produce outputs with different numbers of \ncomponents .and different correct component values ... Note that the distortion equation weights each \noutput value ..equally. It is possible to use a set of weights .. to generalize the distortion equation \nfor programs whose outputs are not all equally important. Each weight ..would capture the importance \nof the corresponding output ..: .. . . . ..... .. .. .... .... . ... . . where the ..satisfy ........ \n3.3 Criticality Testing It turns out that some programs have task blocks that must al\u00adways execute completely \nfor the program to produce accept\u00adably accurate output. We experimentally detect these criti\u00adcal task \nblocks as follows. We .rst con.gure the underlying execution engine (in our case the Jade runtime system) \nto randomly skip executions of a selected task block at a spec\u00adi.ed rate (our criticality testing executions \nskip 10% of the executions of the selected task block). We then select each task block in turn and run \nthe program at least ten times with the execution engine randomly skipping that task block at the speci.ed \nrate. If any of these runs does not produce any output at all (typically because the program failed) \nor if the mean distortion from all of the runs is larger than the spec\u00adi.ed acceptable distortion for \ncriticality testing (we use an acceptable distortion of 0.10), we identify the task block as a critical \ntask block that must execute completely. Otherwise, we consider the task block to be a skippable task \nblock. The purpose of the remaining steps is to characterize how skip\u00adping tasks from skippable task \nblocks affects the distortion of the result that the program produces. 3.4 Distortion Sampling Runs \nWe next run a set of trials in which we randomly select a target skip rate for each skippable task block, \nrun the pro\u00adgram on each of the representative inputs with the execution engine randomly skipping executions \nof each task block at its target skip rate, then record the distortion and actual skip rate for each \nskippable task block for that run. If we have .skippable task blocks, the result is a set of observations \n.. ............,where ..is the distortion and .. .........are the actual skip rates for the .skippable \ntask blocks in the . th trial. Our motivation for selecting target skip rates randomly (instead of using \nsome more systematic approach) was sim\u00adply to distribute the trials more or less evenly across the skip \nrate space with no biases. That said, we anticipate that al\u00admost any approach that distributes the trials \nreasonably well across the skip rate space should produce a reasonably accu\u00adrate model. 3.5 Distortion \nModel Given the results from the distortion sampling runs, we use multiple linear regression [8] to compute \na linear least\u00adsquares distortion model of the form . .. ............................ ... where the .are \nthe least-squares coef.cients for the regres\u00adsion and the ..provide the con.dence intervals for these \nco\u00adef.cients (we use 95% con.dence intervals in this paper). Given skip rates ..for all of the skippable \ntask blocks, this model produces a distortion estimate .............of the expected accuracy of the result \nproduced by a computation with task block skip rates .......... The regression also produces an F value \nthat assesses how well the model predicts the data and an ..value that indi\u00adcates how much of the variation \nin the data the model ac\u00adcounts for. Moreover, given a speci.c point .........in the skip rate space, \nthe regression can produce a con.dence interval around the distortion estimate .............at that point. \nIt is possible to obtain con.dence intervals for what\u00adever alpha level one desires; in this paper we \nuse an alpha level of 0.05, which produces a 95% con.dence interval. We use the SAS system to compute \nthe regression [8]. 3.6 Number of Distortion Sampling Runs In general, there is a trade-off between \nthe accuracy of the distortion model and the number of trial runs. More trial runs produce more samples, \nwhich in turn produce a more accurate model. But performing more trial runs takes more time. So one potential \nquestion is how many trial runs does it take to obtain an acceptably accurate model? In principle, one \ncould use the con.dence intervals to an\u00adswer this question as the number of samples increases, the sizes \nof the con.dence intervals should decrease. One could therefore periodically recalculate the distortion \nmodel and resulting con.dence intervals during the sampling phase to stop sampling when the con.dence \nintervals either converge or become acceptably small. In practice, we found it unnecessary to control \nthe sam\u00adpling phase this closely. Speci.cally, we simply let the script that performed the sampling runs \nexecute for about a day for each program before collecting the data and computing the distortion model. \nFor our set of benchmark programs, we found that this approach delivered acceptably accurate dis\u00adtortion \nmodels. 3.7 Using the Model One goal of the distortion model is to allow a user running the program \nto obtain an estimate of how any early task ter\u00adminations in that execution affected the accuracy of \nthe re\u00adsult that the program produced. In particular, the user would take the observed early task termination \nrates ........., apply the distortion model to obtain an estimated distortion .............along with \nits associated con.dence interval to evaluate whether the terminations were likely to have un\u00adacceptably \ndistorted the result. In this scenario, several is\u00adsues are likely to be of interest: . Distortion: How \nquickly does the distortion grow as a function of the task block skip rates? We evaluate this issue by \nexamining the model coef.cients .. The smaller the model coef.cients, the less the results are affected \nby early terminations of the corresponding tasks. .Bounds: How small are the con.dence intervals? We \nevaluate this issue by computing the minimum and max\u00adimum sizes of the upper con.dence intervals at all \nof the ........points observed during the distortion sampling runs. For a user to accept a distorted \nresult, both the dis\u00adtortion estimate and the upper con.dence interval must be small enough to make the \nlikelihood that an unaccept\u00adably large actual distortion has occurred remote enough for the user to accept. \nThe upper con.dence interval pro\u00advides the appropriate bound for this purpose the dis\u00adtortion is inherently \nbounded below by zero and becomes more acceptable the closer it gets to this bound. . Predictive Power: \nWe use our test inputs to obtain the regression model. We anticipate that users will apply the model \nto executions of the program running on other in\u00adputs. The issue is whether a model derived from execu\u00adtions \non one set of inputs can accurately predict the dis\u00adtortion for other inputs. Our experimental results \nshow that, for our set of benchmark applications, our models can accurately predict distortions for unseen \ninputs [20]. . Sampling Time: It is possible for the time required to perform the trial runs to exceed \nthe time gained by the application of early phase termination during production runs. We anticipate usage \nscenarios in which the over\u00adhead of the trial runs is pro.tably amortized over the pro\u00adduction runs, \neither because the production runs take sub\u00adstantially longer to execute than the trial runs or because \nthe program will be used for many more production runs than trial runs. It may also be feasible to perform \nthe trial runs during a lead time between development and deploy\u00adment. 3.8 Bias De.nition and Use The \ndistortion measures the absolute error induced by skip\u00adping a set of tasks. It is also sometimes useful \nto consider whether there is any systematic direction to the error. The following quantity .measures \nthe bias of the outputs: . . ..... . .. ... ... Note that this is the same formula as the distortion \nwith the exception that it preserves the sign of the summands. Errors with different signs may therefore \ncancel each other out in the computation of the bias instead of accumulating as for the distortion. If \nthere is a systematic bias, it may be possible to compen\u00adsate for the bias to obtain a more accurate \nresult. Consider, for example, the special case of a program with a single out\u00adput component .. If we \nknow that the bias at a certain point is ., we can simply divide the observed output . by ..... to obtain \nan estimate of the correct output whose expected distortion is 0. The reasoning in this example generalizes \nto handle pro\u00adgrams with multiple output components ......... the key is to generalize our methodology \nto obtain a separate distortion and bias model for each different output compo\u00adnent ... It is then possible \nto correct each output component individually to eliminate the bias for that component. If the output \ncomponents do, in fact, exhibit a systematic bias in the face of skipped task blocks, the primary obstacle \nto ap\u00adplying this technique is the number of output components. For programs with large numbers of output \ncomponents it may be dif.cult to perform the number of trials required to obtain a useful model of the \ndistortion and bias for each in\u00addividual output component.  4. Experimental Results We apply early \nphase termination to three scienti.c compu\u00adtations: . String: String uses seismic travel-time inversion \nto con\u00adstruct a discrete velocity model of the geological medium between two oil wells[12]. Each element \nof the velocity model records how fast sound waves travel through the corresponding part of the medium. \nThe seismic data are collected by .ring non-destructive seismic sources in one well and recording the \nseismic waves digitally as they ar\u00adrive at the other well. The travel times of the waves can be measured \nfrom the resulting seismic traces. The appli\u00adcation uses the travel-time data to iteratively compute \nthe velocity model. . Water: Water evaluates forces and potentials in a system of water molecules in \nthe liquid state. Water is derived from the Perfect Club benchmark MDG [4] and performs the same computation. \n. Search: Search is an application from the Stanford Elec\u00adtrical Engineering department [5, 6]. It simulates \nthe in\u00adteraction of several electron beams at different energy levels with a variety of solids. It uses \na Monte-Carlo technique to simulate the elastic scattering of electrons from the electron beam into the \nsolid. The result of this simulation is used to measure how closely an empirical equation for electron \nscattering matches a full quantum\u00admechanical expansion of the wave equation stored in ta\u00adbles. In addition \nto these computations, the Jade benchmark suite contains Panel Cholesky, which performs a Cholesky factorization \nof a sparse matrix; Volume Rendering, which generates a sequence of images of a set of volume data; and \nOcean, whose core computation uses an iterative method to solve a set of spatial partial differential \nequations [17]. Panel Cholesky is not a candidate for early phase termination be\u00adcause it has an irregular \nconcurrency pattern with no easily identi.able serial and parallel phases. Volume Rendering, on the other \nhand, is a perfect candidate for early phase termina\u00adtion it uses ray casting to produce two-dimensional \npro\u00adjections of a three-dimensional data set [16]. Applying early phase termination to this computation \nwould result in the computation casting somewhat fewer rays, which we expect would not substantially \naffect the quality of the .nal image. Unfortunately, we were unable to run Volume Rendering on our current \ncomputational platform because of incompatibil\u00adities associated with the input and output data .le formats. \nWhile it is possible to apply early phase termination to Ocean, it does not improve the performance. \nAlthough early phase termination eliminates the overhead associated with barrier idling at the end of \neach parallel phase, it also causes the phase to drop some relaxation computations, which in turn causes \nthe computation to take more iterations to con\u00adverge. The net effect is a decrease in performance [20]. \n4.1 Methodology For each of our benchmark applications, we perform the fol\u00adlowing steps to evaluate the \neffect of early phase termination on the parallel performance and the distortion. 4.1.1 Inputs and Outputs \nFor each application we choose a set of inputs (the goal is to obtain a set of inputs that re.ect how \nthe program will be used in practice). In general, an application may produce multiple outputs, only \nsome of which are important for the user. We therefore select a set of outputs to use in the calculation \nof the distortion metric (the goal is to select important outputs that motivated the development of the \napplication in the .rst place). 4.1.2 Distortion Model We next apply the procedure outlined in Section \n3 to obtain a distortion model for each of our programs. In our experi\u00adments the target skip rates for \nour distortion sampling runs range from 0 (skip no execution of the task block) to 0.75 (skip three out \nof every four executions). For each program we run the script that performed the distortion sampling \nruns for about a day. The number of sampling runs for each test input varied between approximately 500 \nand 5000 depend\u00ading on the program. 4.1.3 Performance We calculate the parallel performance of each \napplication as follows. We .rst instrument the application and the Jade implementation to emit a log \nof events in the computation. These events include the beginning and end of each parallel and serial \nphase and each task. We next run the application sequentially on each input. We use the resulting event \nlogs to compute the running time of the parallel computation running on .processors as follows. We .rst \nscan the log to .nd the parallel phases and compute the execution time of each task in each phase. We \nalso compute the time required to perform each serial phase. We next use an event-driven simulation to \ncompute the amount of time required to execute the computation in paral\u00adlel on .processors. Speci.cally, \nwe use a standard dynamic load-balancing algorithm to schedule tasks onto processors during each parallel \nphase, the algorithm maintains a list of waiting tasks. When a processor .nishes a task, the al\u00adgorithm \nselects the next task in the list and assigns it to the newly ready processor for execution. This algorithm \nhas the property that no processor will become idle as long as there are tasks that are waiting to execute. \nWe work with two ver\u00adsions of this scheduling algorithm: . Natural: In most programs there is a natural \ntask execu\u00adtion order. This order corresponds to the order in which a sequential execution of the program \nwould execute the corresponding computations. This order is usually deter\u00admined by the order in which \nthe sequential computation traverses the data structures that the parallel tasks access. In the Natural \nscheduler, tasks appear the list of waiting tasks in the natural execution order. Note that with the \nNatural scheduler, early phase termina\u00adtion will tend to terminate sets of tasks that 1) occur next to \neach other in the natural execution order, and 2) occur near the end of the order. Note that this could \nbe a signif\u00adicant source of bias, since such sets of tasks may tend to operate on 1) related pieces of \ndata, and 2) extreme parts of the computational domain (which may differ signi.\u00adcantly from the computational \ndomain as a whole). Recall that the distortion sampling runs select the tasks to skip randomly. The assumption \nbehind the resulting probabilistic distortion model is that there is no system\u00adatic bias in the set of \ntasks that early phase selection ter\u00adminates. One potential problem with the Natural sched\u00aduler is the \npossibility that the bias it exhibits in the termi\u00adnated tasks could invalidate the distortion model. \n. Random: In the Random scheduler, tasks appear in the list of waiting tasks in a random order. The goal \nis to ar\u00adrange to have the early phase termination algorithm ter\u00adminate a randomly selected set of tasks \nat the end of the parallel phase. Note that the Random scheduler may not completely achieve this goal. \nIn particular, longer tasks are more likely to be executing at the end of the paral\u00adlel phase and are \ntherefore more likely to be terminated by the early phase termination algorithm. For our set of applications \nthis phenomenon does not cause the actual distortion to diverge signi.cantly from the predicted dis\u00adtortion. \nWhen applying early phase termination to a new application it may nevertheless be prudent to recognize \nthat this phenomenon could affect the ability of the dis\u00adtortion model to accurately predict the actual \ndistortion. Without early phase termination, we compute the end time of each parallel phase as the time \nwhen the last proces\u00adsor .nishes its last task at the end of the parallel phase. With early phase termination \nthere are two possibilities. If the par\u00adallel phase contains tasks from critical task blocks, we com\u00adpute \nthe end time of the parallel phase as described above for the version without early phase termination. \nIf the paral\u00adlel phase does not contain tasks from critical task blocks, we compute the end time as the \ntime when the .rst processor to become idle at the end of the parallel phase completes its last task. \nThe time required to perform each parallel phase is then simply the end time of the phase minus the start \ntime of the phase (the time when the previous serial phase .nished). Given a time for each parallel phase, \nwe compute the overall parallel execution time as the sum of the times required to complete all of the \nparallel and serial phases. We then divide the total sequential execution time (the sum of the times \nre\u00adquired to complete all of the tasks and serial phases) by the overall parallel execution time to obtain \nthe speedup. We cal\u00adculate three different parallel speedup measures: . Optimized Speedup: The speedup \nwith early phase ter\u00admination. . Standard Speedup: The speedup without early phase termination (all tasks \nin parallel phases execute to com\u00adpletion). . Balanced Speedup: The hypothetical speedup of a com\u00adputation \nthat executes all tasks to completion with per\u00adfect load balancing in parallel phases. The overall paral\u00adlel \nexecution time is the sum of the parallel task execution times divided by the number of processors .plus \nthe time spent in serial phases. The speedup is the total sequential execution time divided by this quantity. \nTo appreciate the value of the balanced speedup measure, understand that there are two distinct ways \nin which early phase termination can improve the performance. First, it can improve the overall load \nbalance by eliminating idle time at the end of parallel phases. Second, it can cause the compu\u00adtation \nto perform less work by eliminating computation in terminated tasks. The balanced speedup measure separates \nthese two effects by presenting the speedup that would result if the load balance was perfect. Any remaining \nperformance increase is caused by a reduction in the amount of work that the computation performs. This \nsimulation assumes that it takes no time to manage the parallel computation to create, schedule, and \nsyn\u00adchronize the tasks. While these activities clearly must take some time, in practice we expect the \nassociated overhead to be amortized away to negligible levels given reasonable task sizes (which our \nbenchmarks have). A potentially more problematic assumption is that the parallel machine pro\u00advides a \nlow-overhead mechanism that one processor (specif\u00adically, the .rst processor to go idle at the end of \nthe parallel phase) can use to terminate tasks running on other proces\u00adsors. While most parallel machines \nprovide such a mecha\u00adnism, it is typically available only via a relatively high over\u00adhead interaction \nwith the operating system. Exposing such mechanisms more directly could reduce the overhead of im\u00adplementing \nearly phase termination. One factor that should simplify the development of an effective task termination \nmechanism is that the computation should be relatively in\u00adsensitive to the latency of the mechanism. \nSpeci.cally, it should be feasible to overlap the execution of the remaining tasks from one parallel \nphase with the execution of the next serial phase while the mechanism terminates the remaining parallel \ntasks. 4.1.4 Distortion We calculate the distortion for each application as follows. We instrument the \nevent-driven simulation to print, at the end of each parallel phase, the identi.ers of all of the tasks \nthat are still executing when the .rst processor to become idle at the end of the phase completes its \nlast task (these are the tasks that early phase termination would terminate). We then rerun the computation, \nbut con.gure the Jade run-time system to use these identi.ers to skip all of the correspond\u00ading tasks. \nWe then use the results from the original sequential execution (which executed all tasks to completion) \nto com\u00adpute the resulting distortion caused by skipping terminated tasks. This distortion calculation \nassumes that terminated tasks have no effect whatsoever on the overall computation. On message-passing \nmachines one could easily achieve this ef\u00adfect by simply discarding the updates from terminated tasks \n(these updates would all be resident in the memory of the machine executing the task) [21]. On shared-memory \nma\u00adchines, one could use transactional memory [13, 1, 2] to achieve this effect simply make each task \nexecute as a transaction. We anticipate, however, that for many applications it should be viable to simply \nlet each task s effects become im\u00admediately visible. This approach would include the effects of more \nof the computation into the .nal result and should therefore improve the accuracy of this result. One \nissue is the potential for data structure corruption if the system happens to terminate a task in the \nmiddle of an update. It is possible to address this issue by enforcing a minimal acceptable atom\u00adicity \ngranularity for each data structure update, with either the developer or a program analysis algorithm \nidentifying an appropriate granularity [18]. For our set of benchmark programs, this issue does not arise \nat all it is possible to terminate any parallel task at any point without corrupting the data structures \nor unacceptably distorting the .nal result (the only distortion would be the anticipated distortion that \ncomes from discarding the remaining computation in the task). Note that including more of the computation \ninto the .nal result can affect the accuracy of any applied bias compen\u00adsation. Because this technique \nattempts to compensate for any missing computation, including the effects of additional computation into \nthe starting point can result in overcompen\u00adsation. Transactional task execution eliminates this issue. \nIt may also be possible to base the bias compensation not on the number of tasks that execute to completion, \nbut on an estimate of the percentage of the total computation whose effects were included into the starting \npoint for the compen\u00adsation calculation. This percentage would include, of course, those parts of each \nterminated task that executed before the system terminated the task at the end of the parallel phase. \n  4.2 String String repeatedly traces a set of rays from one well to the other. The velocity model between \nthe two wells determines both the path and the simulated travel time of each ray. The computation records \nthe difference between the simulated and measured travel times. It backprojects the difference lin\u00adearly \nalong the path of the ray. At the end of the phase the computation averages the backprojected differences \nto con\u00adstruct an improved velocity model. The process continues for a speci.ed number of iterations. \nWe ran String on four input .les: A.pro, B.pro, C.pro, and D.pro. A.pro and B.pro have the same seismic \ndata and starting velocity models but different ray tracing parameters (these parameters control the \ndensity and orientation of the set of traced rays). Similarly, C.pro and D.pro also have the same seismic \ndata and starting velocity models (which are different from the seismic data and starting velocity mod\u00adels \nfor A.pro and B.pro) but different ray tracing parame\u00adters. The output is the new velocity model for \nthe geology between the oil wells. This output is a large matrix of num\u00adbers; the size of this matrix \nvaries depending on the size of the starting velocity model. 4.2.1 Task Blocks and Criticality Testing \nString has .ve task blocks. Executions of the .rst task block shoot rays through the current velocity \nmodel, storing their intermediate results into blocks of storage allocated for that purpose. Executions \nof the second task block combine these results into a single block of storage that executions of the \nthird task block use to compute a new velocity model. The fourth task block creates data structures used \nin the remain\u00ading computation; the .fth task block deallocates these data structures at the end of the \ncomputation. Our criticality testing runs revealed that the .rst two task blocks are skippable, while \nthe third and fourth task blocks are critical. We attribute the criticality of the third task block to \nthe fact that skipping its executions leaves some of the values of the old velocity model in place, which \ncauses sig\u00adni.cant distortion. The fourth task block is critical because skipping its tasks leaves the \nremaining computation without a place to store some of its results. Skipping tasks from the .fth task \nblock has no effect at all on the result, but leaves the data structures allocated at the end of the \ncomputation. 4.2.2 Performance Results Table 1 presents the performance results for String running with \nthe Random and Natural schedulers. The table has several columns: . Application: The name of the application \n String, Wa\u00adter, or Search. . Processors: The number of processors executing the computation. . Input: \nThe input for the computation. . Optimized Speedup, Random Scheduler: The speedup for the computation \nusing the Random scheduler and early phase termination. . Standard Speedup, Random Scheduler: The speedup \nfor the computation using the Random scheduler when every parallel task executes to completion. . Optimized \nSpeedup, Natural Scheduler: The speedup for the computation using the Natural scheduler and early phase \ntermination. . Standard Speedup, Natural Scheduler: The speedup for the computation using the Natural \nscheduler when every parallel task executes to completion. . Balanced Speedup: The speedup for the hypothetical \nperfectly load balanced computation in which every task would execute to completion with no idle time \nin parallel phases. The performance results show that String is inherently well suited for parallel execution \n the vast majority of the execution time is spent in parallel phases and the load is reasonably well \nbalanced. Under these circumstances early phase termination is useful primarily for larger numbers of \nprocessors. We note that the speedups are close to identical for both schedulers. The Balanced Speedup \nnumbers indicate that the performance increase for early phase termination is roughly evenly split between \nan improvement in the load balance and reduction in the amount of work that the computation per\u00adforms. \nSeveral runs have superlinear speedup; this is possi\u00adble because early phase termination can cause the \nparallel version to perform less work than the sequential version. 4.2.3 Distortion Model Figure 2 presents \nthe distortion model for String. We present each regression coef.cient in the form ....,where is the \ncoef.cient itself and ....provides the 95% con.dence bounds for that coef.cient. The F value for this \nmodel is in the thousands, the corresponding p value is less than 0.0001, and the ..value is around 44%, \nwhich re.ects the fact that the representative inputs used to obtain this model have somewhat different \ndistortion properties. The maximum up\u00adper 95% con.dence bound for any sampled task block skip point .....is \n0.0229. An appropriate probabilistic bound on the distortion at, for example, a 50% task skip rate for \nexecutions of task blocks 1 and 2 is therefore approximately 0.063. The coef.cients .and .are very small, \nwhich indi\u00adcates that the distortion is hardly affected at all by skipping tasks. It is therefore possible \nto skip a large proportion of the tasks without incurring substantial distortion.  4.2.4 Distortion \nResults Table 2 presents the distortion results for String running with the Random and Natural schedulers. \nIn addition to the Application, Processors, and Input columns, which have the same meaning as in the \nperformance tables, the table has the following columns: .Actual Distortion, Random Scheduler: The distortion \nfor the computation using the Random scheduler and early phase termination. Optimized Standard Optimized \nStandard Speedup, Speedup, Speedup, Speedup, Random Random Natural Natural Balanced Application Processors \nInput Scheduler Scheduler Scheduler Scheduler Speedup String 60 A.pro 60.34 53.25 60.33 53.16 56.66 \nString 80 A.pro 82.30 69.60 82.33 69.51 74.14 String 100 A.pro 100.90 82.42 100.76 82.21 90.99 String \n120 A.pro 129.67 100.63 129.60 100.42 107.24 String 60 B.pro 61.37 54.02 61.36 54.07 57.52 String 80 \nB.pro 84.30 70.99 84.27 71.10 75.63 String 100 B.pro 103.78 84.31 103.70 84.48 93.24 String 120 B.pro \n134.64 103.58 134.62 103.86 110.39 String 60 C.pro 53.94 48.21 53.94 48.27 51.00 String 80 C.pro 70.86 \n61.27 70.85 61.37 64.70 String 100 C.pro 84.11 70.92 84.09 71.05 77.14 String 120 C.pro 103.24 83.96 \n103.21 84.24 88.49 String 60 D.pro 56.53 50.24 56.53 50.29 53.27 String 80 D.pro 75.37 64.54 75.41 64.70 \n68.43 String 100 D.pro 90.67 75.42 90.64 75.58 82.51 String 120 D.pro 113.00 90.23 113.25 90.70 95.63 \n Table 1. Performance Results for String . ........................................................ \nFigure 2. Distortion Model for String Actual Distortion, Actual Distortion, Predicted Application Processors \nInput Random Scheduler Natural Scheduler Distortion String 60 A.pro 0.0058 0.0291 0.0091[.0.0237] String \n80 A.pro 0.0074 0.0396 0.0120[.0.0237] String 100 A.pro 0.0133 0.0421 0.0149[.0.0237] String 120 A.pro \n0.0134 0.0437 0.0179[.0.0237] String 60 B.pro 0.0086 0.0175 0.0091[.0.0237] String 80 B.pro 0.0094 0.0259 \n0.0120[.0.0237] String 100 B.pro 0.0101 0.0293 0.0149[.0.0237] String 120 B.pro 0.0103 0.0331 0.0179[.0.0237] \nString 60 C.pro 0.0014 0.0026 0.0091[.0.0237] String 80 C.pro 0.0017 0.0030 0.0120[.0.0237] String 100 \nC.pro 0.0021 0.0027 0.0149[.0.0237] String 120 C.pro 0.0020 0.0028 0.0179[.0.0237] String 60 D.pro 0.0008 \n0.0019 0.0091[.0.0237] String 80 D.pro 0.0015 0.0026 0.0120[.0.0237] String 100 D.pro 0.0018 0.0036 0.0149[.0.0237] \nString 120 D.pro 0.0015 0.0045 0.0179[.0.0237] Table 2. Distortion Results for String . Actual Distortion, \nNatural Scheduler: The distortion for the computation using the Natural scheduler and early phase termination. \n. Predicted Distortion: The predicted distortion of the computation from the distortion model for the \nappli\u00adcation. We present the predicted distortion in the form .....,where .is the predicted distortion \nitself and .... provides the con.dence bounds for that distortion. With both schedulers, the actual distortions \nfor inputs A.pro and B.pro are always larger than the corresponding distortions for inputs C.pro and \nD.pro. We attribute this difference to differences in the input .les (A.pro and B.pro share an input \n.le; C.pro and D.pro share a different input .le). These data underscore the need to use representative \ninputs for the distortion sampling runs if one is to obtain an accurate distortion model. With the Random \nscheduler, the actual distortions for inputs A.pro and B.pro track the predicted distortions. The actual \ndistortions for inputs C.pro and B.pro are always smaller than the predicted distortions. The accuracy \nbounds are reasonably tight and the distortion is always small. It is possible to use early phase termination \nfor this application for all numbers of processors in our experiments. We note that with the Natural \nscheduler, the actual dis\u00adtortions for inputs A.pro and B.pro appear to be somewhat larger than the model \npredicts. This phenomenon may be due to a correlation between the terminated tasks in the runs that use \nthe Natural scheduler. Speci.cally, the Natural scheduler tends to leave a set of tasks that trace adjacent \nrays execut\u00ading together at the end of the corresponding parallel phases. This correlation may tend to \nundermine the redundancy that keeps the overall distortion small in the distortion sampling runs and \nthe runs that use the Random scheduler.  4.3 Water We ran Water on four different inputs; the inputs \nvary in the number of molecules they cause Water to simulate. Specif\u00adically, the inputs produce simulations \nof 343, 512, 729, and 1000 molecules. Water calculates several values of potential interest, including \nthe total energy, kinetic energy, and po\u00adtential energy. We choose to measure the distortion of the total \nenergy (in part because it includes contributions from all of the other partial energy calculations); \nit would be pos\u00adsible to extend this measure to include the different partial energy values explicitly. \n4.3.1 Task Blocks and Criticality Testing Water has four task blocks. Executions of the .rst task block \ncompute the intermolecular forces between pairs of molecules, storing their intermediate results into \nblocks of storage allocated for that purpose. Executions of the second task block sum up all of the intermediate \nresults to pro\u00adduce the .nal intermolecular forces. Similarly, executions of the third task block compute \nthe intermolecular contri\u00adbutions to the potential energy, storing intermediate results into blocks of \nstorage allocated for that purpose. Executions of the fourth task block sum up the intermediate results \nto produce the .nal intermolecular potential energy. Our criticality testing experiments revealed no \ncritical task blocks all task blocks produce a mean distortion of less than 0.1 when skipped at the \ntarget 10% rate during our criticality testing runs. 4.3.2 Performance Results Table 3 presents the \nperformance results for Water running with the Random and Natural schedulers. As for String, the speedups \nare comparable for both schedulers. Note that the speedup increases as the input size increases, indicating \nthat the computation spends relatively more time in parallel phases as the number of molecules increases. \nThe Balanced Speedup numbers indicate that the perfor\u00admance increase for early phase termination is split \nmore or less evenly between an improvement in the load balance and reduction in the amount of work that \nthe computation per\u00adforms. Unlike String, the reduction in work does not cause any of the runs to exhibit \nsuperlinear speedup. 4.3.3 Distortion Model Figure 3 presents the distortion model for Water. The F \nvalue for this model is in the tens of thousands, the corresponding p value is less than 0.0001, and \nthe ..value is above 98%, which indicates that the model explains over 98% of the variation in the data. \nThe maximum upper 95% con.dence bound for any sampled task block skip point .........is 0.586. Given \na task block skip point ........., the quantity ....................provides an appropriate upper bound \nfor the actual distortion. The model coef.cients are relatively large, which indi\u00adcates that the distortion \nincreases relatively quickly when the computation skips tasks. For example, the coef.cient for .. is \n0.56. So, for example, a 10% skip rate for task block 3 results in a 0.056 increase in the estimated \ndistortion. If the skip rate is 50% for all task blocks, the estimated distortion is approximately 0.6 \n in other words, the observed out\u00adput with skipped tasks is estimated to be more than a factor of two \ndifferent from the correct output! The models would therefore appear to indicate that the vast majority \nof the tasks in Water must execute to completion for the computation to produce an acceptable output. \nNote, however, that Sec\u00adtion 4.3.5 describes a way to compensate for the bias in the output to obtain \na new output with an estimated distortion of zero. With this bias compensation, Water can tolerate much \nhigher task block skip rates.  4.3.4 Distortion Results Table 4 presents the distortion results for \nWater running with the Random and Natural schedulers. With the Random scheduler, the actual distortions \nfor all inputs closely track the predicted distortions and the accuracy bounds are reason\u00ad Optimized \nStandard Optimized Standard Speedup, Speedup, Speedup, Speedup, Random Random Natural Natural Balanced \nApplication Processors Input Scheduler Scheduler Scheduler Scheduler Speedup Water 60 343 40.09 33.40 \n41.94 33.61 37.37 Water 80 343 47.99 38.54 49.53 38.08 44.17 Water 100 343 54.45 42.52 56.16 41.58 49.60 \nWater 120 343 60.06 45.71 63.68 45.25 54.01 Water 60 512 44.12 38.42 45.57 38.90 41.84 Water 80 512 53.84 \n45.50 56.17 46.29 50.59 Water 100 512 62.19 51.14 64.53 51.98 57.86 Water 120 512 69.37 55.76 72.54 56.60 \n63.98 Water 60 729 46.33 41.89 47.20 42.08 44.64 Water 80 729 57.35 50.57 58.25 50.53 54.76 Water 100 \n729 66.93 57.83 68.73 58.42 63.39 Water 120 729 75.36 63.87 77.62 64.49 70.83 Water 60 1000 49.63 45.73 \n50.36 45.17 48.14 Water 80 1000 62.51 55.73 63.56 55.11 60.15 Water 100 1000 73.95 64.15 74.93 63.02 \n70.74 Water 120 1000 84.39 71.25 86.54 70.48 80.16 Table 3. Performance Results for Water . ............................................................................................ \nFigure 3. Distortion Model for Water Actual Distortion, Actual Distortion, Predicted Application Processors \nInput Random Scheduler Natural Scheduler Distortion Water 60 343 0.2471 0.0629 0.2281[.0.0581] Water \n80 343 0.3088 0.0927 0.3017[.0.0581] Water 100 343 0.3441 0.1203 0.3753[.0.0581] Water 120 343 0.4068 \n0.1631 0.4488[.0.0581] Water 60 512 0.1491 0.0334 0.1564[.0.0581] Water 80 512 0.1997 0.0616 0.2057[.0.0581] \nWater 100 512 0.2512 0.0816 0.2550[.0.0581] Water 120 512 0.2859 0.0876 0.3043[.0.0581] Water 60 729 \n0.0918 0.0235 0.1131[.0.0581] Water 80 729 0.1283 0.0155 0.1478[.0.0581] Water 100 729 0.1549 0.0357 \n0.1824[.0.0581] Water 120 729 0.1913 0.0453 0.2170[.0.0581] Water 60 1000 0.0741 0.0223 0.0855[.0.0581] \nWater 80 1000 0.1011 0.0247 0.1107[.0.0581] Water 100 1000 0.1268 0.0155 0.1359[.0.0581] Water 120 1000 \n0.1557 0.0355 0.1612[.0.0581] Table 4. Distortion Results for Water Actual Distortion Actual Distortion \nAfter Bias Compensation, After Bias Compensation, Application Processors Input Random Scheduler Natural \nScheduler Water 60 343 0.0256 0.2127 Water 80 343 0.0113 0.2978 Water 100 343 0.0486 0.4064 Water 120 \n343 0.0751 0.5167 Water 60 512 0.0077 0.1447 Water 80 512 0.0065 0.1802 Water 100 512 0.0040 0.2314 Water \n120 512 0.0254 0.3101 Water 60 729 0.0230 0.1000 Water 80 729 0.0217 0.1539 Water 100 729 0.0325 0.1781 \nWater 120 729 0.0318 0.2180 Water 60 1000 0.0114 0.0680 Water 80 1000 0.0097 0.0955 Water 100 1000 0.0095 \n0.1382 Water 120 1000 0.0054 0.1487 Table 5. Actual Distortion for Water After Bias Compensation ably \ntight. Because the distortions are large for large num\u00adbers of processors, this application requires \nthe use of bias compensation (see Section 4.3.5) to make early phase termi\u00adnation practical as the number \nof processors grows. With the Natural scheduler, however, the actual distor\u00adtions differ substantially \nfrom the predicted distortions. In fact, they are substantially lower than the predicted distor\u00adtions. \nWhile this property makes the results more accurate than expected, it also causes the bias compensation \nmech\u00adanism to overcompensate, resulting in less accurate results after bias compensation (see Section \n4.3.5). We attribute this deviation from the predicted distortion to the fact that the Natural scheduler \ntends to cause the run-time system to terminate a set of tasks that compute values for molecules in extreme \npoints in the computational domain. These molecules apparently make smaller contributions to the total \nenergy than molecules that are closer to the center of the domain. The net result is that the total energy \nchanges less than the distortion model (which is based on eliminating computations from randomly selected \nmolecules) predicts.  4.3.5 Bias Compensation It turns out that, for every execution of Water, the bias \nis the same as the distortion (which implies that the bias estimator .............equals the distortion \nestimator ............... Because Water has a single output, it is possible to compen\u00adsate for the bias \nby simply dividing the observed output by .................to obtain an output estimator with an expected \ndistortion of zero and a con.dence interval of the same size as the con.dence interval of the distortion. \nTable 5 presents the distortion results for Water with the Random scheduler and Natural schedulers after \nbias com\u00adpensation. In addition to the Application, Processors, and Input columns, which have the same \nmeaning as in the per\u00adformance and distortion tables, the table has the following columns: . Actual Distortion \nAfter Bias Compensation, Random Scheduler: The distortion for the computation after bias compensation \nusing the Random scheduler and early phase termination. . Actual Distortion After Bias Compensation, \nNatu\u00adral Scheduler: The distortion for the computation after bias compensation using the Natural scheduler \nand early phase termination. The numbers show that, with the Random scheduler, bias compensation eliminates \nmuch of the distortion, enabling the use of early phase termination for this application for large numbers \nof processors. With the Natural scheduler, however, the deviation from the predicted distortion makes \nbias compensation impractical.  4.4 Search Search simulates the interaction of solids and electron beams. \nIt uses a Monte-Carlo approach to trace the paths that a set of electrons take through the solid. It \ncounts the number of electrons that emerge back out of the solid and (implicitly) the number that remain \ntrapped inside. We ran Search on four different inputs; the inputs vary in the backscattering parameters, \nin particular in the num\u00ad Optimized Standard Optimized Standard Speedup, Speedup, Speedup, Speedup, \nRandom Random Natural Natural Balanced Application Processors Input Scheduler Scheduler Scheduler Scheduler \nSpeedup Search 60 1 60.21 52.36 54.19 47.64 56.49 Search 80 1 81.09 67.43 71.56 60.22 73.85 Search 100 \n1 100.06 79.84 85.28 70.52 90.55 Search 120 1 125.09 94.69 104.37 81.91 106.62 Search 60 2 58.38 51.25 \n58.55 51.01 54.78 Search 80 2 76.99 65.06 78.87 65.69 70.95 Search 100 2 95.18 77.52 96.18 78.38 86.21 \nSearch 120 2 116.69 90.66 122.16 92.93 100.65 Search 60 3 59.36 51.89 59.71 51.66 55.81 Search 80 3 79.06 \n66.80 81.01 66.88 72.69 Search 100 3 98.70 79.24 99.84 79.49 88.81 Search 120 3 121.22 92.79 127.09 95.48 \n104.21 Search 60 4 62.29 53.59 62.65 53.65 58.22 Search 80 4 84.30 69.60 86.04 70.01 76.85 Search 100 \n4 105.62 83.26 108.05 84.61 95.12 Search 120 4 132.24 98.39 139.53 101.46 113.03 Table 6. Performance \nResults for Search . .................................... Figure 4. Distortion Model for Search Actual \nDistortion, Actual Distortion, Predicted Application Processors Input Random Scheduler Natural Scheduler \nDistortion Search 60 1 0.0271 0.0253 0.0194[.0.013] Search 80 1 0.0314 0.0340 0.0243[.0.013] Search \n100 1 0.0349 0.0353 0.0292[.0.013] Search 120 1 0.0389 0.0393 0.0341[.0.013] Search 60 2 0.0259 0.0260 \n0.0194[.0.013] Search 80 2 0.0267 0.0328 0.0243[.0.013] Search 100 2 0.0352 0.0348 0.0292[.0.013] Search \n120 2 0.0399 0.0419 0.0341[.0.013] Search 60 3 0.0248 0.0293 0.0194[.0.013] Search 80 3 0.0263 0.0299 \n0.0243[.0.013] Search 100 3 0.0350 0.0373 0.0292[.0.013] Search 120 3 0.0395 0.0451 0.0341[.0.013] Search \n60 4 0.0165 0.0178 0.0194[.0.013] Search 80 4 0.0192 0.0210 0.0243[.0.013] Search 100 4 0.0217 0.0267 \n0.0292[.0.013] Search 120 4 0.0263 0.0252 0.0341[.0.013] Table 7. Distortion Results for Search ber \nof electron paths that they simulate. The application cal\u00adculates and outputs a backscattering coef.cient \nfor 51 dif\u00adferent solid/energy level pairs; this coef.cient indicates the percentage of the electrons \nthat escape back out of the solid. We take the resulting sequence of 51 backscattering coef.\u00adcients as \nthe output of the application. 4.4.1 Task Blocks and Criticality Testing Search has one task block which \nuses a Monte-Carlo simula\u00adtion to trace the paths of the electrons through the solid. Our criticality \ntesting experiments revealed that this task block is skippable since it produced a mean distortion of \nless than 0.1 when skipped at the target 10% rate during our criticality testing runs. 4.4.2 Performance \nResults Table 6 presents the performance results for Search running with the Random and Natural schedulers, \nrespectively. As for String and Water, the speedups are comparable for both schedulers. The results show \nthat Search, like many Monte-Carlo simulations, can bene.t substantially from parallel execution. The \nBalanced Speedup numbers indicate that the perfor\u00admance increase for early phase termination is split \nmore or less evenly between an improvement in the load balance and reduction in the amount of work that \nthe computation per\u00adforms. Note that the reduction in work associated with early task termination causes \nsome of the runs to exhibit superlin\u00adear speedup. 4.4.3 Distortion Model Figure 4 presents the distortion \nmodel for Search. The F value for this model is in the tens of thousands, the corre\u00adsponding p value \nis less than 0.0001, and the ..value is above 96%, which indicates that the model explains over 96% of \nthe variation in the data. The maximum upper 95% con.dence bound for any sampled task block skip point \n..is 0.0136. An appropriate bound on the distortion at, for exam\u00adple, a 25% task skip rate is therefore \napproximately 0.039. The coef.cient .is relatively small, which indicates that the distortion increases \nrelatively slowly as the computation skips more tasks. In particular, a 10% skip rate for task block \n1 results in approximately ....increase in the estimated distortion. The application is therefore relatively \nresilient to skipping tasks. 4.4.4 Distortion Results Table 7 presents the distortion results for Search \nrunning with the Random and Natural schedulers. Unlike String and Water, the choice of scheduler has \nno impact on the distor\u00adtion. This makes sense since the tasks differ only in the ran\u00addom numbers that \nthey use to drive their part of the Monte-Carlo simulation. One would therefore expect there to be no \ncorrelation between the position of the task in the natural ex\u00adecution order and the effect of the task \non the computation. The experimental results are consistent with this expectation. The actual distortions \nclosely track the predicted distor\u00adtions, the accuracy bounds are reasonably tight, and the dis\u00adtortion \nis always small. In particular, it is always smaller than the distortion of approximately 0.08 that results \nfrom simply changing the random number seed for the Monte-Carlo sim\u00adulation. It is possible to use early \nphase termination for this application for all numbers of processors in our experiments.  4.5 Discussion \nIn retrospect, it is possible to reconstruct the reasons why early task terminations cause the applications \nin our study to behave the way they do. The skippable tasks in Water, Search, and String all either compute \na set of contributions that are combined to obtain a .nal result, or combine these contributions to produce \nthe .nal result. Although the appli\u00adcations themselves perform complex, detailed computations, it is \npossible to come up with a relatively simple high-level characterization of the behavior of each application \nthat ex\u00adplains the observed results. At a high level, the tasks in Search essentially sample a population \nof electron paths. The net effect of skipping a task is to discard the samples that the task would have \nper\u00adformed. The net effect of performing fewer samples is that the resulting estimate of the property \nof interest in the pop\u00adulation may be somewhat less accurate. The coef.cient . in Search s distortion \nmodel indicates that skipping half the tasks (in effect, sampling half of the points in the sample space) \ncan cause a distortion of around 0.06. To place this loss of accuracy in perspective, consider that simply \nchang\u00ading the random number seed that drives the Monte Carlo simulation in Search can cause a distortion \nof 0.08 in the standard computation. At a high level, the skippable tasks in String either sample a population \nof rays projected through the velocity model of the geology between two oil wells or combine the results \nof this sampling process. Skipping tasks therefore has the effect of discarding projected rays. Because \nthe combination operator averages the contributions, there is no bias and early task terminations have \nlittle effect on the accuracy of the .nal computation. At a high level, Water essentially computes sums \nof pos\u00aditive numbers. The net effect of terminating tasks early is to remove some of the positive numbers \nfrom the sums. On average, the resulting relative reduction in the values of the sums will be roughly \nproportional to the percentage of num\u00adbers removed from the sums. The coef.cients in the distor\u00adtion \nmodel capture the relative contribution of each partial sum to the .nal output total energy of the system \nof water molecules. Because all of the summed numbers are positive, it is possible to model the bias \nas a linear function of the task skip rates and apply that bias to correct the output. One can view the \nresulting computation as selecting a subset of the numbers to sum, computing the sum of that subset, \nthen using the size of the subset to extrapolate the partial sum to obtain an estimate of the sum of \nall of the numbers. 4.5.1 Redundancy One of the reasons that our computations work well with early phase \ntermination is that they have some inherent re\u00addundancy. In String, the redundancy comes from the multi\u00adple \nrays that it traces through the velocity model if the computation loses some rays, the remaining rays \nusually cover enough of the velocity model to deliver an acceptable result. If Search loses some traced \nelectrons, the remaining electrons usually cover enough of the possible interactions with the solid for \nthe computation to deliver an acceptable result. Water has a different kind of redundancy different \nmolecules have correlated interactions. This correlation makes it possible to sample some of the interactions, \nthen extrapolate to obtain an accurate estimate of the .nal result. The distortion results from the Natural \nscheduler highlight the importance of choosing an appropriate unbiased set of sampled interactions. In \nall of these applications, redundancy comes with in\u00adcreased computation String traces more rays, Water \ncom\u00adputes more interactions, and Search traces more electrons than they need to generate acceptable results. \nNote that early phase termination, by dropping some of these subcomputa\u00adtions, consumes some of this \nredundancy. One may very well wonder if this is the best way to exploit this redundancy. One could instead, \nfor example, simply change the program or its input to reduce the amount of computation up front. An \nadvantage of early phase termination is that it dynam\u00adically tailors its consumption of redundancy to \nthe speci.c characteristics of the executing parallel computation. It can therefore maximize the delivered \nperformance bene.t in addition to reducing the amount of computation, it also elim\u00adinates the idle time \nassociated with load imbalances in the underlying computation. Our results show that, for our set of \nbenchmarks, the phenomena have comparable effects on the performance. And early phase termination has \nthe addi\u00adtional bene.t that it can eliminate the possibility of incurring very substantial performance \ndecreases if anomalies such as slow processors or unequal assignments of computation to tasks cause drastic \nload imbalances. 4.5.2 User Acceptance We anticipate that two different aspects of our approach will \naffect user acceptance. First, the presence of the probabilis\u00adtic distortion model will enable users \nto quantitatively eval\u00aduate the impact of early phase termination on the results that their parallel \ncomputations produce. Most scienti.c compu\u00adtations are designed to produce only an approximate solution \nto a complex set of equations; users typically understand and accept the imprecision inherent in the \nunderlying computa\u00adtional model and method. We anticipate that the distortion model will allow users \nto quantitatively evaluate the magni\u00adtude of any additional imprecision, and that in many cases the result \nwill remain acceptably accurate. Second, we anticipate that many users will develop a qualitative understanding \nof the effect of early phase termi\u00adnation on their computation, and that this understanding will make \nit easier for them to accept the result that the computa\u00adtion produces. Both String and Search, for example, \nhave an inherent precision versus execution time tradeoff. For String, tracing more rays through the \ngeological medium between the oil wells takes more time but delivers a more precise result. For Search, \nsimulating more electron paths requires more time but, once again, delivers a more precise result. Both \nof these applications accept con.guration .les that al\u00adlow users to control either the number of traced \nrays (String) or simulated electron paths (Search). Users routinely manip\u00adulate their con.guration .les \nto manage the execution time, understanding that reducing the number of traced rays or simulated electron \npaths may change the overall precision of the result that the computations produce. Note that early phase \ntermination has basically the same overall effect on the computation as using the con.guration .le to \nchange the number of traced rays or simulated electron paths. For String, terminating tasks early reduces \nthe number of traced rays. For Search, terminating tasks early reduces the number of simulated electron \npaths. And the bene.t (a reduction in the execution time) is the same. Moreover, early phase termination \nprovides the additional bene.t of terminating precisely those tasks that deliver the greatest improvement \nin the execution time while minimizing the amount of discarded computation. Based on our experience with \nthe developers and users of String and Search [17], we believe that the probabilistic dis\u00adtortion models \nwould, by themselves, provide enough infor\u00admation for users to accept the use of early phase termination \nfor their production runs. Moreover, once the users under\u00adstood the overall effect of early phase termination \non their computations, we believe they would actually welcome its use for the production runs. For Water, \nthe use of early phase termination in combi\u00adnation with bias compensation has the effect of converting \na full computation of all interactions into a randomized com\u00adputation that samples a (usually large) \nsubset of the interac\u00adtions, then uses extrapolation to compute an approximation of the .nal result. \nGiven the resulting small distortions and tight accuracy bounds for this application, we anticipate that \nthe probabilistic accuracy models would enable users to ac\u00adcept the results. The results also suggest \nthat random sam\u00adpling plus extrapolation may be a worthwhile general tech\u00adnique for reducing computation \ntimes in other computations. 4.5.3 Implications for Other Programs In general, we anticipate that many \ncomputations will turn out to have the same general pattern as String, Water, and Search. Many computer \ngraphics computations have this high-level pattern [14], as do information retrieval computa\u00adtions [7]. \nAnd of course other scienti.c computations share this pattern [3]. We anticipate that our technique can \nbe ap\u00adplied to eliminate barrier idling in these kinds of computa\u00adtions as well as the scienti.c computations \ndiscussed in this paper. We applied early phase termination to existing parallel programs that already \nhad, at the cost of some development effort, been engineered to balance the load more or less evenly \nacross modest numbers of processors. The integra\u00adtion of early phase termination into the initial development \nof other parallel programs may make it possible to obtain an acceptable load balance with less engineering \neffort. For both existing and new applications, another bene.t is the protection early phase termination \ncan provide against ex\u00adtreme imbalances caused either by an anomalously uneven distribution of work across \ntasks or by computing environ\u00adment effects such as heterogeneous processors with different computing \nspeeds.  5. Related Work Asynchronous iteration [9] relaxes standard ordering con\u00adstraints in iterative \nsolvers to allow parallel processors to proceed without synchronization, typically as they operate on \ndifferent regions of the problem that potentially share bor\u00adder elements. The lack of synchronization \nintroduces nonde\u00adterminism into the computation as different processors asyn\u00adchronously read and write \nthe same (typically border) el\u00adements. Our technique, in contrast, completely eliminates some computations \nrather than running computations asyn\u00adchronously at variable execution rates. Barrier idling has been \nlong recognized as an issue in parallel computing. Fuzzy barriers [11] address this problem by identifying \nadditional instructions that an otherwise idle processor can execute while it is waiting at a barrier. \nTo work well, fuzzy barriers require the availability of enough additional instructions to hide the load \nimbalance otherwise responsible for the barrier idling. Early phase termination differs in that it is \nappropriate for arbitrarily unbalanced computations and may change the result that the program produces. \nSeveral existing systems obtain additional robustness in the face of errors by discarding problematic \ntasks. MapRe\u00adduce discards records that cause the record processing task to fail multiple times [7]. \nWe know of a graphics rendering algorithm that discards computations associated with prob\u00adlematic triangles \ninstead of including complex special-case code that attempts to render the triangle into the scene [14]. \nThese systems differ from ours in that 1) the motivation is primarily robustness, not performance improvements, \nand 2) they provide no indication of the effect of discarding com\u00adputations on the resulting outputs. \nWe initially developed our distortion models to provide accuracy bounds for programs that tolerate hardware \nor soft\u00adware failures by discarding tasks [20]. In addition to the dis\u00adtortion models, we also developed \ntiming models. Together, the distortion and timing models can help users success\u00adfully apply strategies \nthat purposefully discard tasks to re\u00adduce the amount of work that the computation performs (and hence \nthe amount of time it takes to perform the computa\u00adtion) while keeping the resulting distortion within \nacceptable bounds. The technique presented in this paper also accepts bounded distortion in return for \nbetter performance. But the primary purpose of the technique presented in this paper is to eliminate \nbarrier idling, not to reduce the total amount of work that the computation performs (although it may \nhave this effect in practice). 6. Conclusion Barrier idling can impair the scalability and performance \nof parallel computations. We have presented a technique, early phase termination, that can completely \neliminate bar\u00adrier idling at the cost of some distortion in the result that the application produces. \nIf this distortion is small enough and predictable enough, users may be willing to accept the distortion \nin return for the performance increase. Our results with several benchmark applications show that early \nphase termination can improve the scalability of the computation. Moreover, when the computation inter\u00adacts \nwell with early phase termination, the resulting distor\u00adtion is small and predictable our distortion \nmodels accu\u00adrately predict the distortion and provide reasonable accuracy bounds. These models therefore \nenable users to 1) determine if early phase termination is appropriate for their computa\u00adtion and satis.es \ntheir accuracy needs, and 2) if so, to con.\u00addently determine when a computation that uses early phase \ntermination has produced an acceptable result. Finally, we have identi.ed a computation pattern that \nexplains why our applications work well with early phase termination. Given that other classes of applications \nshare this pattern, it may very well prove to be fruitful to explore the use of this technique (as well \nas other techniques that leverage the ability to tolerate subcomputation failures) more widely. 7. Acknowledgements \nI would like to thank Gilbert Rinard for drawing my attention to the necessary presence of redundancy \nin computations that can sustain failures and still produce accurate outputs. 8. Support This research \nwas supported in part by the Singapore-MIT Alliance, DARPA Cooperative Agreement FA 8750-04-2\u00ad0254, NSF \nGrant CCR-0086154, NSF Grant CCR-0341620, NSF Grant CCF-0209075, and NSF Grant CCR-0325283. References \n[1] C. Ananian and M. Rinard. Ef.cient object-based software transactions. In Proceedings of the Workshop \non Synchro\u00adnization and Concurrency in Object-Oriented Languages, San Diego, CA, Oct. 2005. [2] C. S. \nAnanian. Architectural and Compiler Support for Strongly Atomic Transactional Memory. PhD thesis, Dept. \nof Electrical Engineering and Computer Science, Massachusetts Institute of Technology, May 2007. [3] \nJ. Barnes and P. Hut. A hierarchical O(NlogN) force calculation algorithm. Nature, 324(4):446 449, Dec. \n1986. [4] W. Blume and R. Eigenmann. Performance analysis of parallelizing compilers on the Perfect Benchmarks \nprograms. IEEE Transactions on Parallel and Distributed Systems, 3(6):643 656, Nov. 1992. [5] R.Browning, \nT. Li, B.Chui,J.Ye, R. Pease, Z. Czyzewski, and D. Joy. Empirical forms for the electron/atom elastic \nscattering cross sections from 0.1-30keV. J. Appl. Phys., 76(4):2016 2022, Aug. 1994. [6] R.Browning, \nT. Li, B.Chui,J.Ye, R. Pease, Z. Czyzewski, and D. Joy. Low-energy electron/atom elastic scattering cross \nsections for 0.1-30keV. Scanning, 17(4):250 253, July/August 1995. [7] J. Dean and S. Ghemawat. Mapreduce: \nSimpli.ed data processing on large clusters. In Proceedings of the 6th Sym\u00adposium on Operating Systems \nDesign and Implementation, San Francisco, CA, Dec. 2004. [8] R. Freund and R. Littell. SAS System for \nRegression.SAS Publishing, 2000. [9] A. Frommer and D. Szyld. On asynchronous iterations. [10] J. Goodman. \nChemical Applications of Molecular Modeling. Royal Society of Chemistry, 2007. [11] R. Gupta. The fuzzy \nbarrier: A mechanism for high speed synchronization of processors. In Proceedings of the 3rd International \nConference on Architectural Support for Programming Languages and Operating Systems, Boston, MA, Apr. \n1989. [12] J. Harris, S. Lazaratos, and R. Michelena. Tomographic string inversion. In Proceedings of \nthe 60th Annual International Meeting, Society of Exploration and Geophysics, Extended Abstracts, pages \n82 85, 1990. [13] M. Herlihy and J. Moss. Transactional memory: architectural support for lock-free data \nstructures. In Proceedings of the 20th International Symposium on Computer Architecture, San Diego, CA, \nMay 1993. [14] T. Kay and J. Kajiya. Ray tracing complex scenes. Computer Graphics (Proceedings of SIGGRAPH \n86), 20(4):269 78, Aug. 1986. [15] C. Moler. Numerical Computing with Matlab. Society for Industrial \nand Applied Mathematics, 2004. [16] J. Nieh and M. Levoy. Volume rendering on scalable shared\u00admemory \nMIMD architectures. Technical Report CSL-TR-92\u00ad537, Computer Systems Laboratory, Stanford Univ., Stanford, \nCalif., Aug. 1992. [17] M. Rinard. The Design, Implementation and Evaluation of Jade, a Portable, Implicitly \nParallel Programming Language. PhD thesis, Dept. of Computer Science, Stanford Univ., Stanford, Calif., \n1994. [18] M. Rinard. Effective .ne-grain synchronization for automat\u00adically parallelized programs using \noptimistic synchronization primitives. ACM Transactions on Computer Systems, 19(4), Nov. 1999. [19] M. \nRinard. Exploring the acceptability envelope. In 2005 ACM SIGPLAN Conference on Object-Oriented Program\u00adming \nSystems, Languages, and Applications Companion (OOPSLA 05 Companion) Onwards! Session, Oct. 2005. [20] \nM. Rinard. Probabilistic accuracy bounds for fault-tolerant computations that discard tasks. In Proceedings \nof the 2006 ACM International Conference on Supercomputing,Cairns, Australia, June 2006. [21] D. Scales \nand M. S. Lam. Transparent fault tolerance for parallel applications on networks of workstations. In \nProceedings of the 1996 Usenix Technical Conference,Jan. 1996.  \n\t\t\t", "proc_id": "1297027", "abstract": "<p>We present a new technique, <i>early phase termination</i>, for eliminating idle processors in parallel computations that use barrier synchronization. This technique simply terminates each parallel phaseas soon as there are too few remaining tasks to keep all of the processors busy.</p> <p>Although this technique completely eliminates the idling that would other wise occur at barrier synchronization points, it may also change the computation and therefore the result that the computation produces. We address this issue by providing <i>probabilistic distortion models</i> that characterize how the use of early phase termination distorts the result that the computation produces. Our experimental results show that for our set of benchmark applications, 1) early phase termination can improve the performance of the parallel computation, 2) the distortion is small (or can be made to be small with the use of an appropriate compensation technique) and 3) the distortion models provide accurate and tight distortion bounds. These bounds can enable users to evaluate the effect of early phase termination and confidently accept results from parallel computations that use this technique if they find the distortion bounds to be acceptable.</p> <p>Finally, we identify a general computational pattern that works well with early phase termination and explain why computations that exhibit this pattern can tolerate the early termination of parallel tasks without producing unacceptable results.</p>", "authors": [{"name": "Martin C. Rinard", "author_profile_id": "81100087275", "affiliation": "Massachusetts Institute of Technology, Cambridge, MA", "person_id": "P192533", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1297027.1297055", "year": "2007", "article_id": "1297055", "conference": "OOPSLA", "title": "Using early phase termination to eliminate load imbalances at barrier synchronization points", "url": "http://dl.acm.org/citation.cfm?id=1297055"}