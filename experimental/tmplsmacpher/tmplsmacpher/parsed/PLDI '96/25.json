{"article_publication_date": "05-01-1996", "fulltext": "\n Data Flow Frequency Analysis G. Rarnalingam IBM T.J. Watson Research Center P.O. Box 704, Yorktown Heights, \nNY, 10598, USA rama(?wat son. ibm. com Abstract Conventional datatlow analysis computes information about \nwhat facts may or will not hold during the execution of a program. Sometimes it is useful, for program \noptimization, to know how often or with what probability a fact holds true during program execution. \nIn this paper, we provide a pre\u00adcise formulation of this problem for a large class of datatlow problems \n the class of finite bi-distributive subset prob\u00adlems. We show how it can be reduced to a generalization \nof the standard dataflow analysis problem, one that requires a sum-over-all-paths quantity instead of \nthe usual meet-over\u00adall-paths quantity. We show that Kildall s result expressing the meet-over-all-paths \nvalue as a maximal-fixed-point car\u00adries over to the generalized setting. We then outline ways to adapt \nthe standard dataflow analysis algorithms to solve this generalized problem, both in the intraprocedural \nand the interprocedural case. 1 Introduction Conventional dataflow analysis computes information about \nwhat facts may or will not hold during the execution of a program. An example is type analysis , which \nanswers questions such as whether a particular expression may evalu\u00ad ate to a value of a particular type \nat run time, Sometimes it is useful, for program optimization, to know how often or with what probability \na fact holds true during program execution. Thus, for example, useful optimization can be performed if \nit is known that an expression evaluates to a value of a particular type with high probability or high \nfrequency ([6]). The probability that a certain branch in the program will be taken at run time can be \nestimated either statically or us\u00ading profiling information [14, 23, 3]. In this paper, we show how the \nfrequency or probability of datallow facts can be computed for a large class of dataflow problems, a \nclass we call finite bi-distributive subset (FBS) problems, once every edge in the control-flow graph \nis annotated with a probability. Permission to make digitalhuwd copy of part or all of this work for \npetsonal or classroom use is ranted without fee provided that oopisa are not made or distributed for \npro i t or commercial advantage, the copyright notice, the title of the publication and its date appear, \nand nob is given that @pying is by permission of ACM, Inc. To copy ofherwiso, to republish, to post on \nservers, or to redistribute to lists, requires prior specific permission andlor a fee. PLDI 96 W96 PA, \nUSA 01996 ACM O-69791 -795-219640005... $505O Finite bi-distributivity requires that the set of dataflow \nfacts be finite and that the dataflow functions distribute over both set union and set intersection. \nThis class of datatlow analysis problems is more general than the class of separable prob\u00adlems (also \nknown as gen/lcill or bit-vector problems) but less general than the class of finite distributive subset \nproblems introduced by Reps et al [17]. The problem of computing the expected frequency of exe\u00adcution \nof the various statements in a program, given a control\u00adflow graph whose edges are Iabelled with a probability, \nhas been studied previously [16, 23]. We are not aware of any previous work in generalizing these techniques \nto general dataftow analysis problems. We provide a theoretical foun\u00addation for datajowfrequency analysis \nand outline algorithms for the problem. The paper is organized as follows. In Section 2 we present an \noverview of the lattice theoretic framework for datafiow analysis and the class of finite distributive \nsubset problems. In Section 3 we provide a precise formulation of the problem studied in this paper. \nWe show how it can be reduced to a generalization of the standard datatlow analysis problem, one that \nrequires a sum-over-all-paths quantity instead of the usual meet-over-all-paths quantity, In Section \n4 we show that Kildall s result expressing the meet-over-all-paths value as a maximal-fixed-point carries \nover to the generalized set\u00adting. In Section 5 we show how this result can be utilized to compute the \nrequired solution using the standard elimina\u00adtion techniques for dataflow analysis. We also reinterpret \nthe problem in terms of the exploded control-flow graph intro\u00adduced by Reps et al. and show that the \nproblem reduces to a fairly well-known Markov chain problem on the exploded graph (with one minor difference). \nIn Section 6 we show how to extend the results to handle procedures. Section 7 presents our conclusion, \n 2 Preliminary Material We first review the lattice-theoretic framework for dataflow amdysis developed \nby Kildall [10]. A monotonic datatlow analysis problem is a tuple (L, fl, F, c, G, r, M), where: L is \na semilattice with meet operation 11  F ~ L + L is a set of monotonic functions from L to 4!/  G = \n(V,E) is a control-flow graph with entry vertex r  c c L is the dataflow fact associated with the entry \nvertex.  M : E + F is a map from G s edges to dataflow functions  (For convenience, we assume that \nthe entry vertex has no predecessors. We will also ignore procedures until Section 6.) The function M \ncan be extended to map every path in the graph to a function from L to L: if p is a path [el, ea ,..., \ne&#38;] then M(p) is defined to be M(ek) oM(ek_l).. sM(el). (M(n) is the identity function.) Given a \npath p from the entry vertex r to some vertex u, we define state(p) to be M(p)(c). The meet-over-all-paths \n(MOP) solution S(tJ) to the dataflow analysis problem is defined as follows: S(U) clef ilpa+zt)w(+ state(p) \nHere ~aths(~, u) denotes the set of all paths p from r to u. Kildall showed that the MOP solution is \nthe same as the maximal fixed point (Ml?) of the following collection of equations: 23W = m M(w + U)(ZV) \nfor every u GV -{r} v *U 2J. =C provided that every M(e) is distributive. 1 The starting point for our \nwork is a restricted version of the monotonic dataflow analysis problem: a finite distributive dataflow \nanalysis problem is a dataflow analysis problem where the semilattice L is the powerset 2D of a finite \nset D, F ~ 2D +d 2D, the set of distributive functions from 2D to 2D, and the meet operation is set union. \n(This is essentially the intraprocedural version of the framework introduced by Reps et al. [17]. Analysis \nproblems over 2= whose meet operation is set intersection can be transformed into a set union problem \nby considering its dual, as noted in [17].) Essentially, D is a (finite) set of facts that may or may \nnot hold true at various program points and the problem is to determine which facts hold true at each \nprogram point. Occasionally, it will be convenient to think of 2D not as the powerset of D, but as the \nset of functions from D to {0,1} ( eg., as a bitvector ), flus, if the meet-over-all-paths solution at \na given program point is U, then m(d) is 1 if fact d may hold at that program point, and a(d) is O if \nd will 1 A function f : L A L is said to be distributive if f (cl n crz) = f (ul ) il $(uz). Strictly \nspeaking, Kildatl s result requires that every dataflow function f distribute over arbitrary meets: f \n(lloe~ a) = ll~~s f(a). ItI particular, when S is empty, the meet over the empty set is T, the top element \nof the semi-lattice, and the distributivity condition requiresthat j(T) = T. The usual gerdkill functions \ndo not satisfy this. Luckily, the requirement that f(T) = T is relevant only when there are vertices \nin the graph that are unreachable from entry vertex. Hence, ttds is not a particularly significant problem, \nnot hold at that program point. We will treat u c 2D as a subset of D in some places and as a function \nfrom D to {0,1 } in some places. (In other words, we will use a set and its characteristic function interchangeably.) \nThe context should make the usage clear. As an example, consider the reaching definitions problem. D \nwill, in this case, be the set of definitions (statements that assign some value to some variable) in \nthe program. If c is the solution for aparticularprogram point, then u(d) will be 1 if definition d may \nreach that program point, and O otherwise.  3 The Expected Frequency of Dataflow Facts We are interested \nin extending the dataflow analysis to de\u00adtermine not just whether some fact may or may not hold at a \nprogram point, but also the probability that the fact may hold true at a given point (or the number of \ntimes the fact is expected to hold true at the given point). Assume that every edge e in the control-flow \ngraph is associated with a probability Web(e). We interpret prob(u ~ v) as the prob\u00adability that execution \nwill follow the edge u -+ v once it reaches u 2. Obviously, the sum of the probabilities over all outgoing \nedges must be 1 for every vertex except the exit vertex, which has no outgoing edge. (Note that we do \nnot consider how these probabilities are obtained. These may be estimated using various heuristics or \nmay be based on profile data. See Section 7.) Consider a path q from the entry vertex to some vertex \nu. The product of prob(e) over all edges e in path q, which we will denote mob(q), is the probability \nthat program execution will follow the path q. Recall that state(q) denotes (an abstraction of) the state \nat u after execution along this path. Let us now combine these information to identify the the number \nof times the various facts are expected to hold true at a given point over a single execution of the \nprogram. Definition 1 E(u) =d.j prob(q) x .qEPath#(?.,w) ~(U, d) =~ej prob(q) E gGPLiths(t.,tt) state(q) \n(d)=l E(u) denotes the number of times a vertex u is expected to execute during program execution, while \nl?(u, d) denotes the number of times fact d is expected to hold true at pro\u00adgram point u. Recall that \nstate(q)(d) is 1 if d holds true in state(q) and O otherwise, (Thus, E(u, d)/E(u) gives us the probability \nthat d holds true at u.) 2An essential assumption made hero is that the probability of the program execution \nfollowing a particular branch is independent of the execution history, which obviously does not hold \ntrue in reatity. Thk assumption is, however, necessaryto get a handle on the problem. Note: For any \npath q and vertex u, let occurs(u, q) de\u00adnote the number of occurrences of u in q. Similarly, let hokis(d, \nu, q) denote the number of occurrences of u in q where c1holds true (during abstract execution with respect \nto the datrdlow analysis framework under consideration). Let eait denote the exit vertex of the control-flow \ngraph. One might argue that the expected values should be defined as follows: E(u) = prob(q).occurs(u, \nq) E qCPatibs(r, ecit) E(u, d) = prob(q).holds(d, u, q) z ggPath8(r, ezit) It can be shown that the above \ndefinition yields the same values as Definition 1 for every u for which there exists a pathfiom u to \nezit whose probability is non-zero, The above definition, however, does not take non-terminating program \nexecution into account, and it is somewhat complicated to extend the definition to handle such cases. \nDefinition 1, however, handles such cases also correctly, which is why we chose that definition. Now, \nwe are interested in computing the quantities l?(u) and E(u, d) for every u and d. Our first step will \nbe to reformulate the problem using a framework that is like a dataflow analysis framework, with one \nsignificant difference. Let ?3?+denote the set of non-negative reals, extended with oo. Let DA denote \nthe set D extended by a new element A not in D. Let 3?~A denote maps from DA to $?+. The set 3?~A is \nthe domain of values we are interested in. (As explained above, we are interested in computing 1111+ \n1 values at every vertex u, namely l?(u) and l?(u, d) for every d c D. Elements of $?: essentially combine \nthese IDI + 1 values into a vector.) Our domain of values R? is not a semilattice. Instead of the usual \nmeet operation, we will use a non-idempotent sum operation. We define the summation operator+ $?~h x \n8:A + !R:A as follows: This definition is extended in a straightforward fashion to infinite sums: for \nany (possibly infinite) index set 1. The infinite sum &#38;I(U~(d)) iS well-defin~ since we me dealing \nonly with non-negative numbers. If ~iGI (ai (d)) does not converge to a finite value, then its value \nis co. We define the scalar multiplication of maps from DA to %+ by a non-negative real number a (. : \n$?+ x $?~A + ~~h ) as follows: (-a)(d) =dej =I(a(d)) The only tricky part here is defining O.oo. We define \nO.co to be O. This turns out to be the right definition for us, since it guarantees that multiplication \nby O is a continuous function: O.(~i~I ~i) = ~iE1(o.~i). For ~Y non-~ro ~, wx is 00. We inject values \nfrom 2D into !3?$ as follows: given u E 2D, we define? 6 2DA c !3?~Aby: 3(A) =def 1 ~(d) =def a(d) ford \nc D For typographical reasons, we will also denote the injection function by i: thus, i(u) ndef ~. (Note \nthat ~ can also be thought of as a subset of of DA, namely a U {A}.) Now consider the following sum over \nall paths quantity: Definition 2 ~(~) =def prob(g).staz(q) E gePatiM(r,u) The above definition essentially \ncombines the IDI + 1 scalar quantities defined for every vertex u by Definition 1 into one vector quantity, \nas the following lemma shows. Lemma 1 F(u)(A) = E(u) F(u)(d) = E(u, d) for d E D Proof See Appendix. \n0 In standard dataflow analysis, we deal with the domain 2D the values to be computed at every vertex \nbelong to 2D and the tmnsfer functions associated with the edges of the graph belong to 2D ~ 2D. Here, \nwe are trying to switch to the domain of R~A. We have seen how we can inject a value a from our original \ndataflow lattice into the value ~ into the new pseudo-lattice of interest. Now, let us see how to inject \na transfer function T e 2D -d 2D into a corresponding transfer function 7 E ?R~A -?R~A. We will do this \nin stages. Consider a distributive function ~ c 2D -2D. The function ~is completely determinedly ~(d) \nand{ ~(d) I d 6 D }. Define r : DA -+ 2D c 3?fA by: The above definition may be understood easiest by \ninter\u00adpreting T({d}) and T(4) as subsets of DA and as the set difference operator. The function T* is \nessentially the representation relation of T defined by Reps et al. (See [17].) The representation relation \nof T, denoted by R~, is a binary relation on DA. We can express RT in terms of T* as follows: Conversely, \nT* can be expressed in terms of R. as follows: T (z) = {!/ I (%V) ~ R r} As Reps et al. show, the representation \nrelation can be pic\u00adtorially depicted as a bipartite graph on 2( ID I + 1) vertices. For example, let \nD be the set {a, b, c}. Then, the represen\u00adtation relation of the function AX.(X (1 {c}) U {b} can be \nrepresented by the following graph 3:   ~.-xt Aa~c While Reps et al. abstract this information as a \nbinary relation from DA to DA, it is more convenient for our purpose to abstract it as a function from \nDA to 2DA. For convenience, we have been treating elements of 2DA as being subsets of DA in the current \ndiscussion. We will now switch to treating them as functions from DA to {O, 1}, and hence, also as functions \nfrom DA to R+. That is, we can also think of r as being a function from DA to W~A. The next thing to \ndo is to show how to inject a trarts\u00ad fer function r c 2= -+d 2= into a corresponding transfer function \nF E !R:A ~ %? . We do this as follows4: deDA We will soon see why this definition makes sense, at least \nfor functions that satisfy a simple property. We say that a function T c 2D --t 2D is bi-distributive \nif it distributes over both set union and set intersection. that is ifi 7-(xu Y) = T(x) u T(Y) T(x n \nl ) = T(X) n T(Y) In terms of the representation relation discussed above, a distributive function is \nbi-distributive iff every vertex in the representation relation has at most one incoming edge. The usual \nbit vector problems, such as reaching definitions, sIn fie ~On~Xtof Stidmd datallow anatysis, thereprwematim \nrelation serves, among other things, to rep(ace the domain 2= by the domain 2DA. This has the side benefit \nthat the transfer functions can be extended to be truly distributive that is, to satisfy j(I#) = I#J.See \nfootnote 1. With 2D asthe domain, we cannot distinguish between the following two situations: (i) execution \nreaches a program point, but none of the datatlow facts hold true (ii) execution does not reach the program \npoint at at]. Both of these are abstractly represented by the empty set. Once we switch to the powerset \nof DA, however, we can make this distinction. The empty set denotes that execution does not reach the \nprogram point, while {A} denotes that execution reaches the pro~ram point b.t none of the dataflow ftwta \nhold true. 4Let us get ahead of ourselves, for the moment in order to understand thk definition. Consider \nart edge u + v with transfer function T. Assume that the state at u is u g $?: : for every d, u(d) indicates \nhow often d is expected to hold trne at u. Assume T* (d) tells us which facts will hold true at u if \nd holds true at u. To make things simpler for us, let us also assume that for every fact d whether d \nholds true at v depends only on (some) one fact at u. Can we say how often each fact d is expected to \nhold true at V? This is the quantity, ;(u), that we are trying to define. The definition follows automaticrdly \nfrom this reasoning. Also seeLemma 4 and its proof for some more intuition behind the definition. available \nexpressions, and live variables, have bi-distributive transfer functions. Copy constant propagation (eg., \nsee [20]) is an example that is not a bit vector problem but is a bi-distributive problem. On the other \nhand, the possibly\u00aduninitialized variables problem is not bi-distributive: after a statement z := y + \nz , z is possibly-uninitialized if y or z is possibly-uninitialized before the statemen~ thus, the representation \nrelation for the associated transfer function will contain two incoming edges for the vertex corresponding \nto the fact z is possibly-uninitialized . Lemma 2 r~ = ?(~), provided T is bi-distributive Proof See \nAppendix. 0 The above lemma essentially says that the following dia\u00adgram commutes (recall that i(a) = \n~): f-l Remember that our intention is to move the analysis from domain 2D to domain !R~ . The commutativity \nof the above diagram indicates that we are on the right track. We are not done yet. Now, consider the \ntransfer function M(e) associated with an edge e. We define a new transfer function ~(e) : ?R~A -+ 8?~A \nby: X7(e) =~ef prob(e).M2) We cart extend the transfer function to paths as usual. Let us now recap what \nwe have at this stage: we have a domain iR~A of values, we have a summary function + defined on $?? and \nwe have associated every edge e of the control-flow graph with a transfer function ~(e) E $l~h ~ $?? \n. We have essentially a dataflow analysis framework, albeit one in which the summary operator is not \nidempotent. The following lemma establishes that performing abstract execution in this new framework \nis equivalent to performing abstract execution in the original framework and weighting the result by \nthe probability of execution of the path. Thus, we have folded in the probabilities into the framework. \nLemma 3 Assume that M(e) is bi-distributive for every edge e. Then, for any path q in the graph and any \nn c 2D ~(q)(6) = prob(q).M@(a) Proof See Appendix. 0 We will now establish that the sum-over-all-paths \nquantity computed using this framework yields the values we set out to compute in the first place. Definition \n3 G(u) =d,f ~ ~(!?)(~) ~e~LZth6(r,U) Theorem 1 If M(e) is bi-distributive for every edge e then G(u) \n= F(u). Proof Follows directly from Lemma 3 and Definition 2. l This concludes our first task: we have \nshown that the values we want to compute (defined in Definition 1) are given by the sum-over-all-paths \nsolution to a non-idempotent datafiow analysis framework. The Expected Frequencies as a Least Fixed Point \n Our next task is to show how the desired sum-over-all-paths quantity can be computed as the fixed point \nof a collection of equations. Why doesn t this follow directly from Kildall s resuIt? Primarily because, \nour summary operator + is not a meet or join operator (it is not idempotent) and our set of facts is \nnot a lattice. What we wiil see in this section is that Kildall s theorem generalizes and holds true \ndespite this. A function r c !l?~ ~ $?: is said to be a linear transfor\u00admation if it dMributes over addition \nand scalar multiplication: T(a.z + p.y) = a,T(z) + p,r(y) It is said to be continuous if it distributes \nover infinite sums as well: T(Z aa.zi) = ~ a$.r(z~) iEI iEI Lemma 4 (a) ~ T : 2D ~ 2D is distributive, \nthen ? : R? -+ %? is a continuous linear transformation. (b) If T: %fA + 8?f)A is a continuous linear \ntransformation, then so is~.rfor O < @ <1. Proof See Appendix. Those familiar with linear algebra may \nnote that (R? , +,.) is somewhat like a vector space, except that the underlying Set DA k a Semiring \nand not a field. DA serves as a basis for !R~A. The definition of 7 itself is the standard way of extending \na function defined on the basis to a linear transformation on the whole space. (The restriction to distributive \n~ comes about because 7 is defined only for distributive ~.) 0 Definition 4 Let us denote the following \ncollection of equa\u00adtions by Q. ~V+U Z(U + U)(xo) for every v 6 V -{r} Xu = ; ifv=r { Theorem 2 If ~(e) \nis a continuous linear transformation for every edge e, then the sum-over-all-paths values G form a~edpoint \nof the set of equations Q, (That is, the assignment < Xu := G(u) > satisfies the collection of equations \nQ.) Proof See Appendix. 0 We now establish a result similar to Kam and Unman s result[8, 9] relating \nMOP and Ml+? In the following theorem, the ordering on !R~A is the ordering induced by the usual arithmetic \nordering on $?+. That is, given al 6 R: and aa C R? , we say that al < c72iff Vd c ~A.~1(~) < u2(cl). \nTheorem 3 If the transformation function ~(e) is a linear transformation for every edge e, then G is \nless than or equal to any fixed point of Q. Proof See Appendix. 0 Theorem 4 If ~(e) is a continuous linear \ntransformation for every edge e, then G is the Ieastjixed point of the set of equations Q. Proof Immediate \nfrom Theorems 2 and 3. 0 It follows from the above theorem and Lemma 4 that the least fixed point of \nQ will yield the expected frequencies for finite bi-distributive subset problems. 5 Computing the Least \nFixed Point We now turn to the question of how to compute the desired information. We begin by looking \nmore closely at the set of equations Q. Observe that the domain of the variables in the set of equations \nQ is ?R~A that is, each variable ZU corresponds to a ID I+ l-dimensional vector. B y replacing the vector \nvariable au by the set of scalar variables {!l(ti,d) Id E DA} (where y(.,d) corresponds tO au (d)), we \ncan break down each equation in Q into ID! + 1 equations. The structure of the resulting set of equations \nis better understood by considering the exploded control-flow graph introduced by Reps et al. [17]. The \nexploded graph is obtained by replacing every vertex u in the control-flow graph by IDI + 1 vertices \n{(u, d) I d c ~A} and by re\u00adplacing every edge u ~ v by the set of edges {(u, dl) ~ (% da) I (all, ~2) \nC &#38;r(ti+~)}. WCdl that R, denotes the representation relation of the function T. Thus, the edges \nin the control-flow graph are replaced by the representation relation of the transfer function associated \nwith the edge. The original datailow analysis problem reduces to that of reachability in the exploded \ngraph (in the single procedure case). Every vertex (w, d) in the exploded graph corresponds to our new \nscalar variable ~(u,d). Let us label every edge (u, dl) + (v, d2) in the exploded graph with the same \nprob\u00adability as that of the corresponding edge in the original graph: prob((u, dl) + (II, d2)) =dej prob(u \n+ v), Now Q re\u00adduces to a set of equations of the form t(d) ift =(r, d) = ~$+t prob(s -+ ~).~, otherwise \n{ (Here, the summation is taken over all edges s -+ tin the exploded graph, Also, recall that r is the \nentry vertex of the original graph.) Thus, we have a set of linear equations with a particularly simple \nform. In fact, they reveal that just as dataflow analysis reduces to reachability in the exploded graph, \nthe problem we are interested in reduces to a simple algebraic path prob\u00adlem (see [7, 5]) over the exploded \ngraph namely the alge\u00adbraicpath problem over the closed semiring (32+, +,.,*, O, 1) whose closure operator \n* is defined by: l/(1 cz) ifa <1 a {= ~ (#) =&#38;f 00 ifa ~1 { $=0 (This algebraic path problem is known \nas the Markov chain problem5.) Consequently, any of the standard algorithms for solving algebraic path \nproblems can be used here. These algorithms have a worst-case complexity of O(n3), where n is the number \nof vertices in the exploded graph (which is O(lvl.pl). The algorithms for the algebraic path problem \nare usually inefficient for datatlow analysis because they fail to utilize the sparsity of the graph \nand because they end up computing unnecessary information (in the form of the solution to the all pairs \nproblem). For our purposes, it would be more effi\u00adcient to adapt the various elimination-based dataflow \nanalysis algorithms to solve this problem, (See [18] for a survey of elimination algorithms,) These algorithms \nare often linear or almost-linear in the size of the graph, (The Allen-Cocke interval analysis algorithm \n[1], for example, is linear for re\u00adducible flow graphs whose loop nesting depth is bounded by a constant. \nTarjan s algorithm [22] is almost-linear for all reducible flow graphs. These algorithms for work for \narbi\u00adtrary graphs as well, but their complexity can be higher for irreducible graphs.) We can apply the \nelimination algorithms to either the origi\u00adnal graph or the exploded graph. The exploded graph is larger \nthe number of edges in the exploded graph is 0(1111.1~1), where II?! is the number of edges in the original \ngraph; this is because every vertex in the representation relation of a bi\u00ad distributive function has \nat most one incoming edge but the closure and composition of the corresponding dataflow func\u00adtions can \nbe computed in constant time. The original graph is smaller, but the closure and composition of the corresponding \ndataflow functions can be more expensive 0( 1.D13)in the worst case. It appears to be difficult to determine \nwhich of 6One minor difference is that in Markov chains it is assumed that the sum of the probabilities \nover aU edges going out of a vertex is 1. While thk is true for the originaf control-flow gmph, it need \nnot be true in the exploded graph, these two alternatives is better based on the worst-case com\u00ad plexity \nmeasure. In general, it appears to be better to apply the elimination algorithm to the exploded graph \nas it exploits the sparsity of the graph in performing the closure and com\u00ad position of dataflow functions. \nHowever, it does have the drawback that the exploded graph may be irreducible even when the original \ngraph is reducible. We now address an issue that may come to the mind of some readers. A set of linear \nequations over real variables usually has (a) a unique solution, or (b) multiple solutions, or (c) no \nsolution. How do these possibilities relate to our problem, which involves a set of linear equations \nover the domain 3?+? The addition of oo (and the restriction to positive reals) eliminates possibility \n(c) a solution always exists. When multiple solutions exist, the existence of a least fixed point is \nalso guaranteed (because of the restriction to positive reals). These possibilities are seen best when \nwe consider how a variable is eliminated from a recursive equation, Consider an equation like vu = a~.y~ \n+Z where z is an expression not containing Vu, If au < 1, then this equation reduces to the non-recursive \ny~ = (1/(1 au)).z If aU ~ 1 and z is non-zero, then the equation has no finite solution. The unique \nsolution, within our domain, is VU = cm, If aa = 1 artd z = O,then there are infinitely many solutions \nfor VU, but yv = O is the least fixed point (which is what we want to compute). If ati > 1 and z = O, \nthen the solutions arey~=coand~a = O, with the latter being the least fixed point. All of the above possibilities \nare captured precisely by the following transformed non-recursive equation: Y. = Q;.z where the closure \noperator * is defined as previously. (Recall that CO.O= O.) Note that the closure operator *is a constant \ntime operation. Use of the closure operator and elimination techniques imply that we do not have to worry \nabout the convergence of the algorithm.  6 Handling Interprocedural Analysis Procedures add a new twist \nto the meet-over-all-paths for\u00admulation of dataflow analysis problems. A program with multiple procedures \nis represented by a supergraph that con\u00adtains a control-flow graph GP for every procedure p. Each control-flow \ngraph GP has its own entry vertex, denoted by rP, and its own exit vertex, denoted by ezitP. The entry \nver\u00adtex of a unique procedure, denoted main, is considered to be the entry vertex of the whole program. \nA procedure call-site s is represented by two vertices, the call vertex cali~ and the return-site vertex \nreturn, and two edges caii, ~ rP (the call edge) and ezitP + retum~ (the return edge), where p is the \nprocedure called. A precise interprocedural datallow analysis algorithm computes the meet-over-all-valid-paths \nsolution, as op\u00adposed to the meet-over-all-paths solution [21, 11]. If one considers the corresponding \ncall edge cail$ ~ Tp and re\u00adturn edge eziiP ~ return, to be matching open and close parentheses, then \na path is said to be a same level valid path if the sequence of call and return edges in the path forms \na well-balanced sequence of parentheses. A path is said to be a valid path if it can be expressed as \nthe concatenation of zero or more same level valid paths and call edges. We will denote by VP(U, v) and \nSLVP(U, v) the set of all valid paths and same level valid paths, respectively, from u to o. We generalize \nDefinition 3, in the interprocedural case, to take the sum over all valid paths. (All call and return \nedges are labelled with a probability of 1.) We now show how we can compute this quantity by adapting \nexisting in\u00adterprocedural and intraprocedural analysis techniques, The algorithm consists of the following \nphases. Phase 1: Compute summary information for each proce\u00addure. We define the summary of any set of \npaths P to be the quatity &#38;P(PTOb(P).m(P)), which we denote by E(P). The quantity Z7(P) is itself \na linear transformation on Y?~A, which can be represented by a ( IDI + 1) x (ID I + 1) matrix of real \nvalues or by a labelled bipartite graph from ID I + 1 vertices to I.DI + 1 vertices, just like a labelled \nrepresentation relation. The first phase of the algorithm is to compute summary information for each \nprocedure p that summarizes the set of all same level valid paths from Tp to ezitP i.e., the quantity \n~(SLVP(Tp, ezdp)). In the absence of recursion, this information can be computed bottom-up over the call \ngraph using the intraprocedural algorithm for each procedure. In the presence of recursion, however, \nthis information has to be computed iteratively, as below. Introduce a symbolic variable XP to denote \nthe summary information of each procedure p. (As explained earlier, XP represents ( IDI + 1) x ( ID I \n+ 1) scalar variables.) Replace all interprocedural edges by summary edges: for every call\u00ad site s to \na procedure q, introduce an edge between ca118 and Tetumz, whose transfer function is Xq. Now, analyze \neach procedure separately. The analysis of each control-flow graph, however, can\u00adnot be done as before \n(in the intraprocedural case), since the edges of the representation relation of the summary edges are \nlabellcd with symbolic variables instead of real values. In\u00adstead, we perform the analysis using an adaptation \nof Tarjan s generalized path algorithm [22]. This algorithm performs the elimination symbolically and \ncomputes a (symbolic) regular expression (i.e., an expression containing the operators +, ., and ) for \neach vertex u in the procedure p. This quantity, in our case, essentially expresses 17(SLVP(rp, u)) as \na sym\u00adbolic expression over the set of variables {Xq 1p calls q}. Consider, in particular, the value \ncomputed for the exit vertex of all procedures. This expresses the summary func\u00adtion of each procedure \nas an expression over the summary function of the procedures it calls. We solve this collection of equations \nusing iteration (until we numerically converge to a fixed point). Unlike in the intraprocedural case, \nwe may now have non-linear equations, such as z = Sz/2 + x/4 + 1/4, which is why we need to use iterative \ntechniques, instead of elimination, to solve the equations. There is, however, one problem using iterative \ntechniques here the iteration may not converge if there is infinite recursion in the call graph (i.e., \nif the expected execution frequency of some procedure is cm in the fixed point). One possible way of \ntackling this problem is to first compute just the expected execution frequency of each procedure, which \ncan be done using just elimination techniques [23,24]. If none of these values are 00 we may proceed \nwith the iterative techniques to compute the frequency of dataflow facts. If any of these values is 00, \nwe can heuristically alter the probability labelling, as in [23], so that these values are finite. Once \nthe summary values have been computed, every ~(SZYP(Tp, u)) (where u is a vertex in procedure p) can \nnow be computed by evaluating the corresponding symbolic expression. Phase 2: Compute the solution for \nthe entry vertex of all procedures: Construct the call graph of the program, repre\u00adsenting each call \nsite by an edge from the calling procedure to the called procedure. Consider a call-sites in a procedure \np. Associate the edge corresponding to this call-site with the transfer function ~(SLVP(rP, call,)) \nthis quantity is readily available from the results of the analysis of the first phase. Apply the intra-procedural \nalgorithm to the call-graph this gives us the meet-over-all-valid-paths solution for the entry vertex \nof each procedure. Phase 3; Compute the rest of the solution: The meet-over\u00adall-valid-paths solution \nfor a vertex u in a procedure p can be obtained by applying ~(SLVP(Tp, u)) to the solution for Tp. 7 \nConclusion This work was originally motivated by procedure cloning an optimization that generates specializations \nof procedures for specific subtypes of the types of their formal parameters [6]. More efficient code \ncan be generated for the special\u00adized versions than for the unspecialized version because the types of \nthe arguments are more prccisel y known. However, it is necessary to be selective about which specializations \nare generated to avoid code explosion. This requires knowl\u00adedge about which procedures are executed often \nand about how frequently arguments of different types are passed to the various procedures. Such information \nis typically gath\u00adered at run-time using tracing tools that record the relevant information. This paper \nexplores the possibility of computing such in\u00adformation statically once every branch out of a conditional \nnode is labelled with a value that indicates the probability that that branch is taken. The probability \nlabelling itself may be computed using simple heuristics, profiling tools, or static analysis. [23, 24,14, \n3]. Even when the labelling is computed using profiling tools, it is certainly more convenient to have \nto gather only this simple data at run-time, rather than hav\u00ading to gather, at run-time, specialized \ninformation that varies from one anatysis problem to another. Profile data, unlike the problem-specific \ndataiiow information, can be gathered using existing off-the-shelf execution profilers. This paper is \na first step towards addressing this problem. It presents a theoretical foundation, as well as algorithms, \nfor computing the frequency of datatlow facts. More work, however, remains to be done. Consider the \ntype analysis problem that arises in the above-mentioned cloning optimization. There do exist simple, \nbut effective, type analysis algorithms (for C+~ see [2]) that fatl into the FBS framework. Other, more \npowerful, type analysis algo\u00adrithms [12, 13, 4], however, are not even distributive. The results in this \npaper do not apply to such algorithms. It re\u00admains to be seen what can be done about such algorithms. \n(At the least, there is hope that for finite monotonic datatlow analysis problems we can formulate the \nappropriate equations whose least fixed point is an upper bound on the frequency of dataflow facts, extending \nthe corresponding result for mo\u00adnotonic datallow analysis.) A second issue concerns the fact that the \nproblem requires solving equations over real-valued variables, unlike tradi\u00adtional dataflow analysis \nproblems which deal with discrete lattices. Consequently, we need to deal with issues such as precision \nand error magnification . (For instance, how sensitive is the solution to the initial probability assignment? \nCan a small change in the probability assignment cause a big change in the final solution?) Hence, in \ndesigning al\u00adgorithms to solve this problem, it seems appropriate to go beyond traditional dataflow elimination \nalgorithms and look at elimination and iteration schemes used in the area of nu\u00admerical analysis to deal \nwith such issues. A related question that needs to be addressed concerns the convergence and complexity \nof the iterative methods used in the interproce\u00addural case. (The intraprocedural problem can be solved \nusing elimination techniques, where convergence is not a problem.) A third issue that deserves consideration \nis that we assume that the probability of the program execution following a particular branch is independent \nof the execution history. It remains to be seen how much this approximation can affect the solution, \nThe problem of computing the expected frequency of ex\u00adecution of the various statements in a program, \ngiven a control-flow graph whose edges are labelled with a proba\u00adbility, has been studied previously \n[16, 23, 24]. A couple of recent papers have utilized estimates of the probabili\u00adty of specific facts \nfor program optimization. Proebsting and Fischer [15] outline a register allocation atgorithm that utilizes \nestimates of the probability of a register-allocated variable remaining in the register at a subsequent \nuse of the variable. Patterson [14] computes the probability of various variables having various values \nand utilizes this information for branch prediction, which has applications to several op\u00adtimization \nsuch as instruction scheduling. These papers, however, present no results relating the quantities computed \nto any formally defined quantity (such as a sum-over-all-path quantity). Nor do they explore the problem \nin the generalized setting that we do. The exploded control-flow graph introduced by Reps et al, [17] \nprovides a natural and easy way to understand our problem reduction and algorithm. There are, in fact, \nseveral structural similarities between our work and an extension of the Reps et al. work outlined by \nSagiv et al. [20, 19]. Sagiv et al. study a class of datatlow problems they call the lnter\u00adprocedural \nDistributive Environment (IDE) problems. The similarities arise from the fact that while Sagiv et al. \nshow that the IDE problems reduce to a simpler datatlow prob\u00adlem (i.e., one involving a simpler lattice) \nover the exploded graph, we show that the dataflow frequency problem (for FBS problems) reduces to a \nsimple Markov chain problem over the exploded graph. The primary difference between the tinat reduced \nform of the problems in the two cases is that the summary operator is idempotent in one case but not \nthe other.  Acknowledgements The author thanks the anonymous referees for their helpful comments. \n References [1] F.E. Allen and J. Cocke. A program data flow analy\u00adsis procedure. Commun. of the ACM, \n19(3): 137 147, March 1976. [2] D. F. Bacon, M. Wegman, and K. Zadeck. Rapid type analysis for C++. Technical \nReport RC number pend\u00ading, IBM T.J. Watson Research Center, 1996. [3] T. Ball and J.R. Lams. Optimally \nprofiling and tracing programs. In Conference Record of the Nineteenth ACM Symposium on Principles of \nProgramming Languages, pages 59 70, 1992. [4] Paul R. Carini, Michael Hind, and Harini Srinivasan. Flow-sensitive \ntype analysis for C++. Technicat Report RC 20267, IBM T.J. Watson Research Center, 1995. [5] T.H. Cormen, \nC.E. Leiserson, and R.L. Rivest. Intro\u00adduction to Algorithms. MIT Press, Cambridge, MA, 1990. [6] J. \nDean, C. Chambers, and D. Grove. Selective special\u00adization for object-oriented languages. In Proceedings \nof the ACMSIGPLAN 95 Conference on Programming Language Design and Implementation, pages 93-102, 1995. \n [7] M. Gondran and M, Minoux, Graphs and Algorithms. John Wiley and Sons, New York, 1984. [8] J.B. Kam \nand J.D. Unman. Global data flow analysis and iterative algorithms. Y. ACM, 23:158-171,1976. [9] J.B. \nKarn and J.D. Unman. Monotone data flow analysis frameworks. Acts Informatica, 7:305-317,1977. [10] G. \nKildall. A unified approach to global program op\u00adtimization. In Conference Record of the First ACM Symposium \non Principles of Programming Languages, pages 194-206, New York, NY, 1973. ACM. [11] J. Knoop and B. \nSteffen. The interprocedural coinci\u00addence theorem. In U. Kastens and P. Pfahler, editors, Proceedings \nof the Fourth International Conference on Compiler Construction, Lecture Notes in Computer Sci\u00adence Vol. \n641, pages 125 140. Springer Verlag, 1992. [12] Hemant D. Pande and Barbara G. Ryder. Static type determination \nfor C++. In Proceedings of the Sixth Usenix C+ + Technical Conference, pages 85 97, April 1994. [13] \nHemant D. Pande and Barbara G. Ryder. Static type determination and aliasing for C++. Technical Report \nLCSR-TR-250, Department of Computer Science, Rut\u00adgers University, July 1995. [14] J.R.C. Patterson. Accurate \nstatic branch prediction by value range propagation. In Proceedings of the ACM SIGPLAN 95 Conference \non Programming Language Design and Implementation, pages 67 78, 1995. [15] T. Proebsting and C. Fischer. \nProbabilistic register al\u00adlocation. In Proceedings of the ACM SIGPLAN 92 Conference on Programming Language \nDesign andlm\u00adplementation, pages 300-310,1992. [16] C.V. Rarnamoorthy. Discrete markov analysis of com\u00adputer \nprograms. In Proceedings of the ACM 20th Na\u00adtional Conference, pages 386-391, 1965. [17] T. Reps, S. \nHorwitz, and M. Sagiv. Precise interpro\u00adcedural dataflow analysis via graph reachability. In Conference \nRecord of the Twenty-Second ACM Sympo\u00adsium on Principles of Programming Languages, pages 49-61,1995. \n[18] B. Ryder and M. Paull, Elimination algorithms for data flow analysis. ACM Computing Surveys, 18(3), \nSeptember 1986. [19] M. Sagiv, T. Reps, and S. Horwitz, Precise interpro\u00adcedural dataflow analysis with \napplications to constant propagation, To appear in Theoretical Computer Sci\u00adence. (Also Technicat Report \nCS-TR-95-1284, Com\u00adputer Sciences Department, University of Wlsconsin-Madison, 1995). [20] M. Sagiv, \nT. Reps, and S. Horwitz. Precise interpro\u00adcedural dataflow analysis with applications to constant propagation. \nIn Proceedings of FASE 95: Colloquium on Formal Approaches in SofMare Engineering, pages 651-665,1995. \n[21] M. Sharir and A. Pnueli. Two approaches to interpro\u00adcedural data flow analysis. In S.S. Muchnick \nand N.D. Jones, editors, Program Flow Analysis: Theory andAp\u00adplications, pages 189 233. Prentice-Hall, \nEnglewood Cliffs, NJ, 1981. [22] R.E. Tarjan. Fast algorithms for solving path problems. J. ACM, 28:594-614,1981. \n  [23] T.A. Wagner, V. Maverick, S.L. Graham, and M.A. Harrison. Accurate static estimators for program \nop\u00adtimization. In Proceedings of the ACM SIGPLAN 94 Conference on Programming Language Design andlm\u00adplementation, \npages 85 96, 1994. [24] Y. Wu and J.R. Larus. Static branch frequency and program profile analysis. In \nProceedings of the 27th International Symposium on Microarchitecture, pages 1 1 1, 1994. Appendix Proof \nof Lemma 1 F(u)(A) = ( ~ p7 Ob(~). da=(q) ) (A) gEPatib8(r,u) = ~ ( WO~(q). (sta=(q) (A))) ga%ths(r,u) \n= prob(q) x gGPczth$(r,u) (using equation 1) = E(u) Similarly, for any d c D, F(u)(d) = ( ~ prob(q).sta=(g)) \n(d) gEPaths(r,u) = ~ ( mob(q). (s~~~(d(~)) ) gEPatha(r,u) = ~ ( prob(q). (s~a~e(q)(~)) ) gePatibs(r,u) \n(using equation 1) = prob(q) z gGPLath#(?.,t4) atate(g)(d)=l (since state(q)(d) E {O, 1}) = E(u, d) 1 \nLemma 5 Let T E 2D -+ 2D be bi-distributive and x 6 D. Then (a) ~x c r(a) then there exists an unique \nd E S such that z c r (d) (b) ifx @~(a) thenfir all d c @a @~ (d). Proof (a) We have z ~ ~(u). First \nconsider the case where w E T(4). Here, we have z E T*(A) and a @ ~ (d) for any d c D. (This follows \ndirectly from the definition of T*.) Now assume that z @ T(g$). Since r is distributive,~(a) = ~(ud~,y \n{d}) = Udeo r{d}. Hence, there must be somed c c such that z E ~{d}, This d has to be unique: if z c \nr{dl} and z E T{da}, where dl # da then we have x E T({dl}) fl T({da}) = r({dl} fl {da}) (since T is \nbi-distributive) = r(~). This contradicts our assumption that z Q T(q$), Hence, it follows that there \nis an unique d ~ ~ such that a c ~ (d). (b) This follows directly from the monotonicity of r: if z @ \n~(a) then for all d c a z @ ~({d}) and z @ r(q$), The result follows. l Proof of Lemma 2 We need to show \nthat @)(z) = 7(3)(z), for all z E DA. First consider any z E r(a) or, more precisely, any z 6 D such \nthat T(c)(z) = 1. Lemma 5 implies that there exists an unique d E @ such that w c T*(d) or, more precisely, \nthere exists an unique d E DA such that ~(d) = 1 and T*(d)(z) = 1. Consequently, ~(d). ~ (d)(z) = O for \nall other d c DA. Thus, we have ?(3)(2!) = (d~Add).~ (d) ) (z) = ~ ( a(d). ~*(d)(~)) dEDh = 1 (since \nthere is a unique d such that F(d). ~ (d)(z) = 1) = ~F) (~) Now consider any z @ r(a) or, more precisely, \nany z c D such that r(m)(z) = O. The previous lemma implies that there for all d E 5 z @ T*(d) or, more \nprecisely, for all d E DA if ~(d) = 1 then T*(d)(z) = O. In other words, ~(d).~* (d)(z) = O for all d \nc DA. Consequently, we have Now consider x = A. 0 Proof of Lemma 3 This follows by induction on the path \nlength. The lemma follows trivially for the empty path (whose transfer function is the identity function \nin both frameworks). Assume that the result holds for a specific path q. We will show that it holds true \nfor the path q.e (consisting of the path q followed by the edge e). Z(q.e) (~) = Z(e) ( m(q)(~) ) = ~(e) \n( prob(q).w ) (using the induction hypothesis) &#38; = (prob(e).M~)) (prob(q)odZ(q)(a)) (using the definition \nof ~(e)) .\u00ad= prob(e).gwob(q) .(~(e)(~(q)(a))) (since MT) is a linear transformation) -= prob(q.e).(~(e) \n(~(q)(a))) (using Lemma 2) ~ = prob(q.e).(M(q. e)(u)) 0 Proof of Lemma 4 (a) Assume that T : 2D a 2D \nis distributive, Note that T = Uv+u UPCT, {P.(o + u)}. we have = ~ai.( ~ ( ,(4.7*(4 ) ieI deDA (b) \nAssume that T : R? ~ 3?~A is a continuous, linear transformation. v-u (using the inductive hypothesis) \niEI ia ~ s~ (since (SU I u is a vertex) is a fixed point) The theorem follows since the sum over an \ninfinite set X iCI is simply the limit of the sums overall finite subsets of X. 0 0 Proof of Theorem \n2 The result trivially holds true for the entry vertex r, since the empty path is the only path from \nr to r. (Recall that we have assumed that the entry vertex has no predecessors.) Consider any vertex \nu other than the entry vertex. Proof of Theorem 3 Let (su I u is a vertex) be a fixed point of the set \nof equations Q. Let T be any finite set of paths from the entry vertex r to a vertex u. We now show that \n~g~T ~(q)(~) < Sti. We do this by induction over the sum of the lengths of the paths in T. There are \ntwo base cases: (i) T is empty: in this case, ~qe~ ~(q)(~) is O (i.e., kLO) and the result holds (since \nthe fixed point is an element of Wf ). (ii) T is a singleton consisting of the empty path: In this case, \nu must be the entry vertex T. Hc% ~q@ fi(q) (6) is ~, and the result holds true. Now for the induction \nstep. For every predecessor v of w let Tv denote the set of paths in T that end with edge v + u. \n\t\t\t", "proc_id": "231379", "abstract": "Conventional dataflow analysis computes information about what facts may or will not hold during the execution of a program. Sometimes it is useful, for program optimization, to know <i>how often</i> or with <i>what probability</i> a fact holds true during program execution. In this paper, we provide a precise formulation of this problem for a large class of dataflow problems --- the class of finite bi-distributive subset problems. We show how it can be reduced to a generalization of the standard dataflow analysis problem, one that requires a sum-over-all-paths quantity instead of the usual meet-overall-paths quantity. We show that Kildall's result expressing the meet-over-all-paths value as a maximal-fixed-point carries over to the generalized setting. We then outline ways to adapt the standard dataflow analysis algorithms to solve this generalized problem, both in the intraprocedural and the interprocedural case.", "authors": [{"name": "G. Ramalingam", "author_profile_id": "81100519054", "affiliation": "IBM T.J. Watson Research Center, P.O. Box 704, Yorktown Heights, NY", "person_id": "PP31045870", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/231379.231433", "year": "1996", "article_id": "231433", "conference": "PLDI", "title": "Data flow frequency analysis", "url": "http://dl.acm.org/citation.cfm?id=231433"}