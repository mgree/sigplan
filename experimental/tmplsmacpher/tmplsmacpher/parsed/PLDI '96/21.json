{"article_publication_date": "05-01-1996", "fulltext": "\n Relocating Machine Instructions by Currying Norman Ramsey Department of Computer Sciences, Purdue University \n1398 Computer Science Building, West Lafayette, IN 47907 nr(lcs. purdue. edu http://www.cs .purdue.edu/homes/nr \nAbstract Relocation adjusts machine instructions to account for changes m the locations of the instructions \nthemselves or of external symbols to which they refer. Standard linkers implement a finite set of relocation \ntransformations, suit\u00ad able for a single architecture. These transformations are enumerated, named, and \nengravedin a machine-dependent object-file format, and linkers must recognize them by name. These names \nand their associated transformations are an unnecessary source of machine-dependence. The New Jersey \nMachine-Code Toolkit is an application generator. It helps programmers create applications that manipulate \nmachine code, including linkers. Guided by a short instruction-set specification, the toolkit generates \nthe bit-manipulating code. Instructions are described by con\u00ad structors, which denote functions mapping \nlists of operands to instructions binary representations. Any operand can be designated as relocatable, \nmeaning that theoperand s value need not be known at the time the instruction is en\u00ad coded. Forinstructions \nwith relocatable operands, the tool\u00ad kit computes relocating transformations. Tool writers can use the \ntoolkit to create machine-independent software that relocates machine instructions. mid, a retargetable \nlinker built with the toolkit, needs only 20 lines of C code for relocation, and that code is machine-independent. \nThe toolkit discovers relocating transformations by cur\u00ad rying encoding functions. An attempt toencodeaninstruc\u00ad \ntion with a relocatable operand resultsin the creation of a closure. The closure can be applied when \nthe values of the relocatable operands become known. Currying provides a general, machine-independent \nmethod of relocation. Currying rewrites aA-term into twonested Xterms. The standard implementation has \nthe first J allocate a closure and store therein its operands and a pointer to the second A. Using this \nstrategy in the toolkit means that, when it builds an application, the toolkit generates code for many \ndifferent inner A-terms one for each instruction that uses a  Permissionto rnaksdigifabtmrdcopyof partorall \nof thisworkfor personal or classroom use is granted without fee provided that copies are not made or \ndistributed for profit or oommeroial advantage, the copyright notice, the title of the publication and \nits date appear, and noti~ is given that copying is by permission of ACM, Inc. To copy otherwise, to \nrepublish, to poston servers,or to redistributeto lists,requirespriorspecificpermission andlor a fee. \nPLDI 96 5/96 PA, USA 01996 ACM 0-69791-795-2/96/0005...$3.50 relocatable address. Hoisting some of the \ncomputation out of the second A into the first makes many of the second As ident ical a handful are enough \nfor a whole instruction set, This optimization reduces the size of machine-dependent as\u00adsembly and linking \ncode by 15 20~0 for the MIPS, SPARC, and PowerPC, and by about 3070 for the Pentium. It also makes the \nsecond As equivalent to relocating transforma\u00adtions named in standard object-file formats. 1 Introduction \nCompiling whole programs is slow; compiling units sep\u00adarately and linking the compiled units into a program \nspeeds up the edit-compile-go cycle. For separate com\u00adpilation, a compiler must be able to emit instructions \nand data without knowing the exact locations either of the instructions and data the compiler itself \nemits, or of the instructions and data emitted by other compila\u00adtions. Using assembly language makes \nthis task easy, because in assembly language all locations are repre\u00adsented symbolically. Symbolic, assembly-like \nunits can be linked to form programs (Fraser and Hanson 1982; Jones 1983), but the linker or loader must \ntranslate all units from symbolic form into the binary representa\u00adtion required by the target hardware. \nIt is believed to be more efficient to translate each unit separately into a binary form called relocatable \nobject code. Object code must contain more than just instruc\u00adtions and data. To support delayed binding \nof loca\u00adtions, it must also represent The symbols defined in the object file and the locations to which \nthey are bound.  The symbols imported from other units, i.e., external symbols.  The transformations \nthat must be applied to the instructions and data to account for its eventual placement at an absolute \naddress and also for the placements of the external symbols on which it depends.  Applying these transformations \nis called relocation. Current object-code formats force tool writers to handle relocation in a machine-dependent \nway. Given an architecture, a human being examines the instruc\u00adtion set and determines which operands \ncan be relocat\u00adable addresses and what relocating transformations are needed. Each transformation is \nnamed, and linkers and other tools must recognize transformations by name. The names are informal and \nmachine-dependent, so retargetable tools that manipulate object code must recognize each set of names \non each machine. This paper makes several contributions. It presents a machine-independent, automatic \nmethod of discov\u00adering relocating transformations. It presents an opti\u00admization that makes the cost of \nthe automatic method comparable to the cost of hand-implemented methods and makes the discovered transformations \nequivalent to the transformations used in standard object-file for\u00admats. Finally, the paper gives a machine-independent \nrepresentation of the transformations. This new technique for relocating machine instruc\u00adtions is an \nenabling technology for building machine\u00adindependent tools for static, incremental, and dynamic linking. \nIt will also simplify the construction of re\u00adtargetable tools that transform object code. Object\u00adcode \ntransformation, which is growing in importance, is used for profiling and tracing (Ball and Larus 1992), \ntesting (Hastings and Joyce 1992), enforcing protec\u00adtion (Wahbe et al. 1993), optimization (Srivastava \nand Wall 1993), and binary translation (Sites et al. 1993). There are even frameworks for creating applications \nthat transform object code (Johnson 1990; Larus and Schnarr 1995; Srivastava and Eustace 1994). The techniques \npresented here build on the New Jersey Machine-Code Toolkit (Ramsey and Ferndn\u00addez 1995), which reads \na compact machine description and generates functions that encode instructions. The machine description \nrelates two representations of in\u00adstructions: as ymbolic representation akin to assembly language and \nthe binary representation used by the hardware. The symbolic representation of an instruc\u00adtion includes \nits name, a list of operands, and a sug\u00adgested assembly-language syntax. The author of the machine description \nindicates which operands are re\u00adlocatable addresses. Currying the encoding function with respect to those \noperands results in a relocating transformation. Currying rewrites the encoding function into two nested \nA-terms. In the standard implementation, the outer A allocates a closure and stores therein its operands \nand a pointer to the inner A, which uses the contents of the closure to encode (relocate) the instruc\u00adtion. \nThe inner k are the relocating transformations discovered by the toolkit, and the closures take the place \nof relocation entries in traditional object files. Using the standard implementation of currying, the \ntoolkit generates code for many different inner A\u00adterms one for each instruction that uses a relocat\u00adable \naddress. Hoisting some of the computation out of the inner ~ into the outer makes many of the inner As \nidentical a handful are enough for a whole instruc\u00adtion set. This optimization is closely related to \nfully lazy lambda-lifting (Peyton Jones 1987). It reduces the size of machine-dependent assembly and \nlinking code by 15 20% for the MIPS, SPARC, and PowerPC, and by about 3070 for the Pentium. It also makes \nthe relocating transformations discovered by the toolkit equivalent to those that are now implemented \nby hand. To support machine-independent use of these transfor\u00admations, the toolkit associates each one \nwith a string that can be interpreted to have the effect of applying the transformation. These strings \ncan be used in an object file as meaningful, formal, machine-independent names. 2 Describing instruction \nencodings The New Jersey Machine-Code Toolkit describes the binary representation of an instruction \nas a sequence of tokens. On a RISC machine, each instruction is a single 32-bit token. On a machine like \nthe Pentium, formats vary; for example, the instruction add 612 [DXI , 33 has an 8-bit opcode token, \nfollowed by another 8-bit token that has both opcode and address-mode bits, followed by the 32-bit displacement \n612 and the 8-bit immediate operand 33. Each token in an instruction is partitioned into fields; a field \nis a contiguous range of bits within a token. Fields contain opcodes, operands, modes, or other in\u00adformation. \nOpcodes and operands can be distributed among multiple fields. On RISC machines, different instruction \nformats are represented by different parti\u00adtions of the instruction token. Patterns constrain the values \nof fields; they may con\u00adstrain fields in a single token or in a sequence of tokens. They can be used \nto describe binary representations of opcodes, of whole instructions, and of groups of in\u00adstructions. \nConstructors connect the symbolic and binary rep\u00adresentations of instructions. At a symbolic level, an \ninstruction is an opcode (the constructor) applied to a list of operands. The result of the application \nis a se\u00adquence of tokens, which is described by a pattern. For each constructor, the toolkit derives \nan encoding func\u00adtion that emits the constructor s binary representation. We get relocating transformations \nby currying the en\u00adcoding functions. The encoding functions generated from a machine description form \npart of an application\u00adprogram interface (API) to an assembler for that ma\u00adchine. The toolkit includes \na library of other functions that complete the API. Tokens and fields The same rule applies to the uses \nof rs and rt on the right-hand side. A machine description includes the names, sizes, and positions of \nthe fields used to form tokens. The in\u00adformation can be found in architecture manuals. For example, the \nMIPS manual (Kane 1988, p A-3) gives this informal field specification: 31 2625 2120 1615 0 rs rt immediate \n CJP 31 2625 0 OD t arjzet 31 26 25 2120 1615 1110 65 0 Op rs rt rd shemt funct This informal specification \ncan be formalized in a ma\u00adchine description as follows: fields of instruction (32) Op 26:31 rs 21:25 \nrt 16:20 rd 11:15 shamt 6:10 funct 0:5 target 0:25 inuned 0:15 offset 0:15 base 21:25 cond 16:20 breakcode \n6:25 ft 16:20 fs 11:15 fd 6:10 format 21:24 This declaration defines not only the fields used inthe formats \npictured above but also offset, cond, and other synonyms that appear in the MIPS manual. Patterns Patterns \nconstrain both the division of streams into tokens andthevalues of the fields inthose tokens. They are \ncomposed from constraints on fields. A constraint fixes therange ofvalues afield may have. The typical \nrange has a single element, e.g., op = 1. Patterns may be composed by conjunction (k), concatenation \n(;), or disjunction (1). Conjoining patterns constrains fields within a single token; concatenating them \nconstrains a sequence of tokens. In this paper we use patterns that constrain all the bits in a sequence \nof tokens; such patterns are equivalent to binary representations. Constructors A constructor connects \nthe symbolic and binary rep\u00ad resentations of an instruction by mapping a list of operands to a pattern. \nThe left-hand side of a con\u00ad structor specification resembles the assembly-language syntax of the instruction \nspecified. The right-hand side contains a pattern that describes the binary representa\u00ad tion of the instruction. \nThat pattern may contain free identifiers, which refer to the constructor s operands. For example, the \nfollowing constructor describes the MIPS add instruction: constructors add rd, rs, rt is op = O &#38; \nfunct =32&#38; rd&#38;rskrt where, on the right-hand side, rd is an abbreviation for the pattern constraining \nthe field rd to be equal to the first operand, since the first operand is named rd. Some instructions \nhave operands that cannot be used directly as field values. The most common are PC\u00adrelative branches, \nin which the operand is the target address, but the corresponding field cent ains the dif\u00adference between \nthe target address and the program counter. Constructor specifications may include equa\u00adtions that express \nrelationships between operands and fields. For example, the specifications for the MIPS bne and blt zal \ninstructions are: constructors bltzal rs, addr {addr=L+4 * offset! } is op=lkcond= 16 &#38; rs &#38; \noffset; L: epsilon bne rs, rt, addr {addr=L+4 * offset! } is op = 5 &#38; rs &#38; rt &#38; offset; L: \nepsilon epsilon is the pattern specifying the empty sequence of tokens. Here it serves only as an anchor \nfor the label L, which is bound to the location of the instruction fol\u00ad lowing the branch. The exclamation \npoint in offset! is a sign-extension operator. The equation in braces specifies the relationship between \nthe target address addr and the offset used in the instruction s binary represent ation: A branch target \naddress is computed from the sum of the address of the instruction in the delay slot and the 16-bit off \nset, [sign-extended and multi\u00ad plied by 4] (Kane 1988, p A-23). The toolkit solves this equation to compute \noffset as a function of addr and the program counter. The equa\u00adtion has a solution only when the target \naddress and the program counter differ by a multiple of 4 and when the computed offset fits in 16 bits, \nand the generated encoding function checks these conditions.  3 Instruction encoding and relocation \nThe toolkit uses the field locations and constraints to figure out the bit manipulations needed to encode \nan instruction. If addr and the program counter are known at the time a bltzal, for example, is encoded, \nwe can emit the binary representation directly by using the following function:l ~(rs, addr).emit(l <<26 \nI 16<<161 ((addr -PC -4)>> 2) &#38; (216 -1) \\ rs << 21), where I use C notation for bit manipulation. \n1 <<26 encodes the op = 1 constraint, 16<<16 encodes the 1To simplify the presentation, I have omitted \nsuch details as the check that the target address and program counter differ by a multiple of 4. cond \n= 16 constraint, and rs<<21 puts the operand rs into the instruction. The remaining disjunct expresses \nthe computation needed to compute the vaiue of the off set field given a target address addr and a program \ncounter PC. The computation includes arithmetic, a shift, and a narrowing to 16 bits. PC represents the \naddress at which the instruction is to be located. If PC and addr are unknown, we can t emit the in\u00adstruction; \nwe must create relocation information in\u00adstead. A typical compiler or assembler emits the instruction \nwith the displacement bits set to zero, along with a relocation entry that tells the linker how to adjust \nthe displacement bits when the rele\u00advant locations become known. The relocation entry names the instruction, \nthe address on which it de\u00adpends, and the transformation needed to adjust the displacement bits. We can \ndiscover relocating transformations from the toolkit s description of an instruction. The descrip\u00adtion \ntells us how an instruction s operands determine its final, binary representation after relocation. When \ncalled upon to emit an instruction referring to an un\u00adknown location, an assembler must delay encoding, \nemit a partial instruction, and record a relocating transformation that can be used to compute the final \ninstruction once the location is known. This proce\u00addure amounts to currying the encoding function. We \nmust know which operands are relocatable addresses, since these are the operands the values of which \nmay not be known when an object file is created. The MIPS specification contains the directive relocatable \naddr, which specifies that all operands named addr are relo\u00adcatable addresses. Currying the bltzal encoding \nfunction yields Ars.Aaddr.emit(l <<26 I 16<< 16[ ((addr -PC -4)>> 2) &#38; (2 -1) I m << 21). When applied \nto a particular rs, this encoding func\u00adtion returns a closure containing rs and the inner X term. To \ngenerate C or Modula-3 code, it helps to con\u00advert to an explicit closure-passing style (Appel 1992, Chapter \n10). Converted functions, i.e., function val\u00adues, are represented by closures. A closure is a record \ncontaining a J-term, which represents the function s algorithmic content, and the values of the function \ns free variables. In the A-term, the function s free variables are replaced by references to the closure, \nand the closure becomes an explicit argument to the term. After the transformation, the X-term has no \nfree variables. In relocation, the inner A-terms describe relocating transformations. When an encoding \nfunction is cur\u00adried, different applications of the outer function create different closures. These closures \nshare a A-term, but they differ in the other contents of the closure record the values of the free variables. \nFor example, every bltzal closure has the form ~bltz.al = (A(7?, addr).emit(l <<26 [ 16<<161 ((addr -PC \n-4)>> 2) &#38; (2 -1)1 7?[1] << 21), rs), but different closures may have different values of rs. Closure \nconversion also changes the way functions are invoked. In the original form, we could invoke a relocation \nclosure 7? = Aaddr. . . . by simple function application 7? addr. After closure conversion, we must fetch \nthe A-term out of the closure and pass the closure as an extra argument. If the closure-converted version \nis %?bltzal, we invoke it by ~t.lt.al [0] (%.1t..l, addr). The implementation of closure conversion is \nstraight\u00adforward. We add a closure argument to each function. We discover the free variables in the body \nof the func\u00adtion and put each in the closure, and we replace each occurrence of a free variable in the \nbody with code to get its value from the closure. 4 Optimizing relocation closures In the scheme outlined \nabove, each relocatable instruc\u00adtion needs its own closure function. Compiling these functions takes \ntime, and they take up space in an ap\u00adplication or an object file. We can reduce the number of closure \nfunctions by moving computation from the inner A to the outer A I call this movement hoisting, by analogy \nwith the CPS transformation that moves variable definitions from one scope to another. It sim\u00adplifies \nthe inner As, creating opportunities for them to be shared. Hoisting is very closely related to fully \nlazy lambda-lifting (Peyton Jones 1987, Chapter 15), and the analysis required to implement it is reminiscent \nof the binding-time analyses used in partial evaluation (Jones, Sestoft, and S@ndergaard 1989). Unlike \nthese other techniques, hoisting is not intended to make pro\u00adgrams run faster. Hoisting might result \nin marginally faster linking, but its purpose is to reduce the number of different A-terms needed to \nimplement relocation. Hoisting is implemented by a variation on closure conversion. Operands of outer \n% are available before those of inner As. If we think in terms of binding times, A-bound arguments are \nalways late, i.e., not available to compute with until the function is applied. Free vari\u00adables are always \nearly, i.e., available to compute with when the closure is created. In ordinary closure conver\u00adsion, \nfree variables are replaced with references to the closure. To perform the hoisting transformation, we \nwant to replace not only free variables but also terms that depend only on free variables. Such terms \nare called free expressions in Peyton Jones (1987), and a free expression that is not a proper subexpression \nof an\u00adother free expression is said to be maximal. Fully lazy lambda-lifting rewrites A-terms to make \nthe maximal free expressions additional operands; hoisting moves them into the closure. For example, \nto convert the function Ac.a + b + c, we hoist a + b, creating a closure of the form (~(l?, c).72-[1] \n+ c, a -t b). We can implement closure-conversion with hoisting by rewriting a function s abstract syntax \ntree in a bottom-up walk: Leaf nodes are free expressions unless they are variables bound by the innermost \nenclosing A-abstraction. Internal nodes are free expressions if and only if all their children are free \nexpressions. To simplify the computation, replace each free internal node e= f(el, ez, . . . en) with \na fresh variable v, which is free by definition. To remember what v stands for, create the substitution \na = v F+ ~(el, ez, . . . en). Compose these substitutions during the tree walk. When we reach a A-abstraction, \nall free expressions have been replaced with variables, and since no vari\u00adable can be a proper subexpression \nof another, the free variables represent the maximal free expressions of the original A-term. We could \nrecover the original body of the A-term by applying the substitution cr to it, but in\u00adstead we closure-convert \nthe rewritten form, then apply the substitution to the closure. Thus, in the example given above, 1.We \nbegin with Ac.a + b + c. 2. We rewrite it to Ac.v + c, with substitution o=vt-+a+b. .3, By ordinary closure \nconversion, we get 7?= (A(73, C).n[l] + c, v). 4. We apply n to the closure, producing R = (A(R, c).R[l] \n+ c, a + b). We can save a minor computation by applying o only to the variables in the closure; applying \nit to the A-term has no effect since after closure conversion the A-term has no free variables. To get \nbetter results with closures for machine in\u00adstructions, we rewrite expressions involving associa\u00adtive \nand commutative operators to bring free expres\u00ad sions together. This rewriting step can reduce the number \nof maximal free expressions, resulting in sim\u00adpler As and smaller closures. The relevant operators in\u00adclude \ninteger addition, assuming that it does not over\u00adflow, and bitwise or. Briggs and Cooper (1994) use an \nequivalent technique to improve the effectiveness of partial-redundancy elimination in a traditional \nopti\u00ad mizing compiler; the rank they assign to each variable corresponds to the number of As between \na free occur\u00ad rence of a variable and its binding instance. Rearranging associative and commutative operators \ngives the following relocation closure for blt zal:2 7?,,1.=.1 = (A(%3, addr).emit(7i?[l]l ((addr -PC \n+7?[2]) >> 2) &#38; (2 -l)), 1<<26116<< 16]rs <<21, 4). The new A-term can be shared with other relative\u00adbranch \ninstructions, since all information about the opcode and about the register argument rs has been hoisted \nout of the A-term and into the closure. Hoisting moves integer literals, like 4 in this ex\u00adample, into \nclosures. Such literals take up space, and we can improve the closures by using a heuristic: if a value \nto be stored in the closure is an integer literal, push it back into the A-term instead of storing it \nin the closure. We don t push other constant expressions into the A-term. The heuristic works because \ninteger liter\u00adals tend to arise from address computations, which are typically the same across instructions, \nbut other con\u00adstant expressions often come from opcodes, which are different for every instruction. To \npreserve the distinc\u00adtion, we delay constant folding until after hoisting. Applying the heuristic to \nthe bltzal instruction yields a smaller closure: R,ltz.l = (A(7?, addr).ernit( R[l]l ((addr -PC -4)>> \n2) &#38; (2 -l)), 1<<26116<< 16[rs <<21). The literal 4 has moved back into the A-term.  5 Relocation \nclosures in C Creating efficient C code to perform relocation by cur\u00adrying requires some refinements. \nThere is no need to put global variables in any closure, because globals are accessible to all functions. \nTherefore, there is no need to convert top-level functions to closure-passing style, because all their \nfree variables are globals. This is just as well, since C programmers expect functions in an API to be \nimplemented in standard C style, not in closure-passing style! The encoding functions and relocation \nclosures gen\u00aderated by the toolkit treat relocatable addresses as val\u00adues of an abstract data type.3 \nAn address may be 2The astute reader may wonder why the literal 216 1 used in masking is not moved to \nthe cloeure. The toolkit s intermediate form restricts masking operations to conetants of the form 2k \n 1, and the constant k is attached directly to the &#38; operator, so 216 1 is not a free expression \nin the sense defined above. For similar reasons, the 2 in (. . .) >>2 is not moved to the closure. 3The \ntoolkit IS library of machine-independent assembly and linking code represents a relocatable address \nas a label plus a constant offset. This representation is adequate for almost all Unix applications (Szymanski \n197 S), but application writers could substitute another representation. (type of closure)= typedef \nstruct Ol_l_closure { ClosureHeader h; /* contains lambda-term, etc . . . */ ClosureLocation 10C; struct \n{ RAddr al; unsigned u1; } v; } *01-1-Closure;  (relocating transformation)= static void _clofun.l(Ol_l_Closure \n-c, Emitter emit.at) { emit-at(_c->loc, -c-X.U1 I location(-c->v.al) -pc-location(_c->loc) -4 >> 2 k \nOxffff, 4) ; } (closure creation)= { Ol_l-Closure _c = (01-1-Closure) malloc(sizeof *_c); static struct \nclosure_header _h = { _clofun_l, . . . ]; -c >h = k_h; (initialize _c->loc with current PC) _c->v.al \n= addr; -c->v.u1 =1<<26 I16<< 16 Irs <<21; (save closure _c for future use) Figure 1: Representing closures \nin C knownorunknown, andaknownaddress maybeforced to reveal its location by applying force to it. The \nlo\u00adcation is represented as an integer. The address itself is supplied when the instruction is encoded; \nwhat may not yet be available is the actual location denotedby the address. We have to keep track of \nthe address, so wecanforceit toalocationat relocation time, and the easiest way is to store it in the \nclosure. Ordinary encoding functions, which create no relo\u00adcation information, emit code at a current \nlo cation, which is part of the global state of the toolkit s en\u00adcoding library. Relocation closures \nshould not emit in\u00adstructions at the current location, but at the location of the original encoding attempt. \nThis location, too, is stored in the closure, and instead of emit , which emits a token at the current \nlocation, we use emit _at , which emits a token at a location given explicitly. The program counter, \nPC, gets special treatment. Itis another name for the location of the original en\u00adcoding attempt, and \nwe have to save this location so we know where to put the relocated instruction. Ifwe handled PC as we \nhandle other variables, we would store it in the closure, but since it is already in a spe\u00adcial part \nof the closure, we rewrite references to PC to refer to that location. Applying these refinements to \nthe MIPS bltzal in\u00adstruction produces an encoding function that can be represented as follows: A(rs, \naddr).(A(73).emit_at(7? [l], 13[3] I ((force %?[2] -force 7?[1] -4)>> 2) &#38; (216 l)), Pc, addr, 1<<26116<< \n16\\rs <<21 ) The real encoding function is still more complicated, since it emits the instruction directly \nwhen addr and PC are known, and it also checks the multiple-of-4 and fits-in-16-bits conditions. The \nclosure-converted form is easily represented in C, as shown in Figure 1. As with the other examples, \nFigure 1 omits all checking code, as well as such details as converting pointer types and recording the \nsize of the closure. The C code binds emit_at as late as possible; the late binding enables different \nimplementations in different applications. The final argument to emit_at is the size of the token being \nemitted; that size has been omitted from the other examples in this paper. The closure shown in Figure \n1 has the same in\u00adformation as a relocation entry used in standard object-code formats like COFF (Gircys \n1988) and ELF (Prentice Hall 1993a). For example, a COFF reloca\u00adtion entry contains an r.vaddr that corresponds \nto the 10C field; both store the location of the instruc\u00adtion to be relocated. It contains an r.qmndx \nfield that corresponds to the v. al field; both store the relocat\u00adable address on which the relocation \ndepends. Finally it contains an r.type field that corresponds to the h field; both identify the relocating \ntransformation. ELF relocation entries are similar, except ELF combines r-s ymndx and r.t ype into a \nsingle word. Relocation entries in standard formats have nothing corresponding to the v. UI field of \nthe closure shown in Figure 1; in\u00adstead, they store that information in the space to be oc\u00ad cupied by \nthe instruction after relocation. The toolkit could use this space-saving trick, which would reduce the \nlargest closure numbers in Table 1, but for the time being it seems more interesting to make reloca\u00adtion \nclosures idempotent. Idempotent closures should be useful in tools that relocate instructions repeatedly, \nlike incremental linkers. A final refinement is needed to write relocation clo\u00adsures to disk. In memory, \nthe relocating transformation is represented as a function pointer, which is neither machine-independent \nnor meaningful when written to disk. Instead, we describe relocating transformations using a subset of \nPostScript (Ramsey 1992), extended with special operators to get addresses and values out of closures. \nThe machine-independent representation of the transformation in the bltzal closure is -4 1 cla force \nadd C1-1OC force sub -2 bitshift 16 narrows 1 CIV orb C1 1OC force 4 emit-at The first line takes the \nrelocatable address from the closure, subtracts 4, and subtracts the location of the instruction being \nrelocated, computing addr PC 4. The second line shifts right 2 bits, narrows to 16 bits, and combines \nthe result with the rest of the instruc\u00adtion, as stored in the closure. The third line stores the instruction, \nwhich is 4 bytes wide, at the proper location. The toolkit generates a table that associates the function \npointers used in closures with machine\u00adindependent strings like the one shown above. The length of the \nstrings is not a problem. Only one copy of each transformation need appear in an object file, and it \ncan go in a string table. It is true, however, that even a small subset of PostScript is overkill for \nsuch simple computations. Applications might be better served by a customized bytecode language and an \ninterpreter for that language. Bytecodes would also yield a space savings; with a suitable choice of \nbytecodes, it would be easy to represent the bltzal transformation in 13 bytes.  6 Experimental results \nI have implemented currying and hoisting in the New Jersey Machine-Code Toolkit (Ramsey and Fernfin\u00addez \n1995). mld (Fernimdez 1995), a retargetable, op\u00adtimizing linker, uses encoding functions and relocating \ntransformations generated by the toolkit. mld needs only 20 lines of C code for relocation, and it uses \nthe same code on all platforms; the code keeps a list of relo\u00adcation closures and applies them when the \naddresses on which they depend become known. Other applications that might use the generated encoding \nand relocating code include assemblers, linkers, whole-program opti\u00admizers, and object-code transformers. \nOne can imagine several measures of the performance of a relocation method: the space required to store \nthe relocation code, the time required to execute it, the space required to store object modules, and \nthe time required to execute the relocated binary code. Ideally, one could use these measures to compare \ncurrying with standard methods of relocation, but mld is the only linker that uses relocation by currying, \nand mid s as\u00adsumptions make meaningful comparisons difficult. For example, mld uses no object modules, \nso it is impos\u00adsible to measure their sizes. This section focuses on the size of the relocation code \nand the time required to execute it. There is no need to compare the speed of bi\u00adnary codes as relocated \nby currying or by hand-written code, since both methods result in identical executable binaries. This \nsection also compares SPARC relocating trans\u00adformations discovered by the toolkit with transforma\u00adtions \ndefined by the ELF object-code standard. Code size and hoisting I used the toolkit to generate encoding \nand relocating code for the MIPS, SPARC, and Pentium, as speci\u00adfied in Ramsey and Fernandez (1994), and \nalso for the PowerPC 604, as specified by Doug Currie of Flavors Technology. Table 1 shows the amount \nof space con\u00adsumed in an application by generated encoding func\u00adtions and relocating transformations. \nThe column la\u00adbels across the top name the specifications of the target machines for which object code \ncan be generated or re\u00adlocated. The results in Table 1 depend only on the specifications and on the toolkit \nitsel~ they are inde\u00adpendent of the program being relocated. The upper part of Table 1 describes properties \nof the instruction-set specifications and of the code gen\u00aderated to implement them. Each instruction \naccounts for an encoding function, as does each addressing mode. A relocatable instruction has an operand \nthat is or contains a relocatable address. The table shows how hoisting reduces the number of closure \nfunctions. On the Pentium, the number of closure functions, with\u00ad61.OK 51.3K 207.3K 160.9K 142.9K 117.6K \n3063.4K 2098.6K   EnBaME Instruction counts give the sizes of the instruction sets. Sizes in K are \ncode sizes needed to encode and relocate all instructions. Table 1: Space savings from hoisting optimization \nout hoisting, is greater than the number of relocat-of hoisting on the Pentium will have to wait until \nthis able instructions, because the toolkit expands address-code bloat is eliminated. Practical applications, \nlike ing modes inline and generates a different closure for mid, use a subset of the full Pentium specification. \neach combination of instruction and addressing mode. Hoisting reduces the size of relocation code to \nat Many instructions on the Pentium use one of 8 pos-most a few percent of the size of encoding code. \nThe sible addressing modes, of which 5 involve relocatable sizes of relocation functions in SPARC object \nfiles are addresses. The last line in the top half of Table 1 shows MIPS SPARC PPC 604 Pentiumthe number \nof extra words (in addition to the location) stored in the largest closure. 1.6K 0.9K 1.7K 9.6K The \ntoolkit supports cross-architecture assembly and To compare one of these sizes with hand-written code, \nI linking. The lower part of Table 1 shows how much examined the SPARC relocation code used in the GNU \nspace the encoding and relocating functions take up and Solaris linkers. These linkers implement all \n23 of for all available combinations of host and target ma\u00adthe transformations in the ELF standard, but \nthe tool\u00adchine.4 Each row label identifies a different host ma\u00adkit implements only 5. The GNU code is \ntable-driven; chine, on which the relocation code runs. The data in the Solaris code is not. the table \nare the sizes as compiled with gee, for code The sizes of the three different implementations of re\u00adgenerated \nwith and without hoisting. The savings from location are comparable. The toolkit uses 1004 bytes of hoisting \nare shown in bold, as ratios. The reduction in code to implement 5 transformations. The GNU linker object-code \nsize ranges from 15 20% on the RISC spec\u00aduses 1420 bytes of code to interpret table entries, and ifications \nto about 30% on the Pentium specification. the table requires 106 bytes per transformation, for a The \nsavings are higher on the Pentium because propor\u00adtotal of 1950 bytes for 5 transformations. The Solaris \ntionally more instructions take operands that include linker uses 5192 bytes of code to implement 23 \ntrans\u00adrelocatable addresses. formations. The encoding functions generated by the toolkit take lots of \nspace because the toolkit trades space for time, generating specialized code for every combination of \nSpeed of relocation instruction and addressing mode. The toolklt does not I estimated differences in \nrelocation speed by trans\u00adencode an effective address until it knows in what in\u00adplanting GNU relocation \ncode into mid. I identified struction the address is used. This choice is a poor one the SPARC ELF target \nas the easiest to add to mid. I for the Pentium; the specialized code grows to stag\u00ad removed significant \nportions of the GNU code to try to gering size because of the inline expansion of address\u00admake the link-time \nassumptions like mid s assumptions. ing modes. A meaningful measurement of the value For example, I eliminated \nsupport for multiple symbol 41do not have accessto a PowerPC to act w a host machine. tables and for \ngeneration of relocatable object code, and I removed many sanity checks and assertions. The GNU code \nrelocates into and out of specialized sec\u00adtions, but mld works directly in memory, so where possible \nI modified the GNU code to use memory ad\u00addresses directly instead of sections and offsets. To use the \nmodified GNU code in mid, I translated the toolkit s relocation closures into ELF-style relo\u00adcation entries. \nI used mld to link four of the SPEC benchmarks (eqntott, li, gee, and espresso), doing relocation both \nways in each run. The two methods yield identical instructions. Using the gett imeof day system call, \nI measured elapsed time on a SPARC LX with 48 MB of physical memory. Relocation by curry\u00ading is about \n20% faster on eqntott, li, and gee; the GNU code is about 20% faster on espresso. Depend\u00ading on the benchmark, \na run requires 2,000 to 40,000 transformations. I do not know why espresso is differ\u00adent. The measured \ncode is artificial, and I am willing to conclude only that the costs are comparable. One source of artificiality \nis that mld does not store sections in contiguous memory locations, because it uses a lifetime-based \nmemory allocator (Hanson 1990) and it does not know section sizes in advance. As a consequence, every \ntime an instruction is relocated, the relocation code must search for the contiguous block containing \nthat instruction. If the searching is done in advance and the search time not counted, relocation time \ndrops by 2070 or more. This change simulates the operation of a linker of object modules, in which sections \nare always contiguous because they are made so by the assembler. Relationship to standards The relocating \ntransformations discovered by the tool\u00adkit are closely related to those used in standard object formats. \nFor example, the toolkit discovers two trans\u00adformations for the SPARC, and they are equivalent to the \ntransformations named R. SPARC-WDISP22 and R_SPARC_WDISP30 in the ELF format for the SPARC (Prentice \nHall 1993b), provided we represent the relo\u00adcatable address as the sum of the label S and the off\u00adset \nA. (In ELF terminology, these values are called the symbol and the addend.) It discovers only two transformations \nbecause the toolkit specification for the SPARC designates fewer operands as relocatable than the standard \nSPARC assembly language, We can make the toolkit discover more transformations simply by making more \noperands relocatable; for the SPARC, it requires a 7-line change to a 200-line specifi\u00adcation. The toolkit \nthen discovers 253 relocating trans\u00adformations, which are reduced to 6 by hoisting. The new transformations \nare R. SPARC. 13, R_ SPARC_22, R_ SPARC_H122, and a combination of R_ SPARC_H122 and R_ SPARC_LO 10. \nIf we add constructors to store relocatable addresses in 8-bit, 16-bit, and 32-bit to\u00adkens, the toolkit \ndiscovers R_ SPARC_8, R_ SPARC_ 16, and R_ SPARC_32. In the presence of these extra re\u00adlocatable operands \nand the extra relocating trans\u00adformations, the savings from hoisting increases from 20% to 30%. There \nare transformations the toolkit does not dis\u00adcover. Some are specialized versions of the ones that are \ndiscovered. For example, several ELF transforma\u00adtions are specialized to refer to locations relative \nto the start of a global offset table or a procedure linkage table. Some relocation entries in standard \nobject files cannot be discovered by the toolkit because they repre\u00adsent more than just transformations. \nFor example, the R_ SPARC_GLOB_DAT relocation entry names the same transformation as R-SPARC_32, but \nit also instructs the linker to create an entry in the global offset table. Size of object code Because \nmld creates no object modules, I cannot di\u00adrectly measure the effects of relocation by currying on the \nsizes of object modules, but I can make predictions based on the experiment of embedding GNU relocation \ncode in mid. Each relocation closure can be translated into a relocation entry using the standard ELF \nrepre\u00adsent ation. The only auxiliary data structure needed is a mapping of small integers to A-terms; \ninstead of being fixed by a machine-dependent object-code stan\u00addard, this mapping must be stored in an \nobject file. Assuming a reasonable byte-coded representation of A-terms, the mapping could probably be \nrepresented in 100 bytes per object file. It could be made even smaller by using Proebsting s superoperator \ntech\u00adnique (Proebsting 1995).  7 Discussion Hoisting works well on the RISC machines because most instructions \noccupy a single token. It works less well on the Pentium, because the toolkit creates a sin\u00adgle closure \nfor an encoding of an instruction even when that encoding is a sequence of tokens. The relocat\u00ading transformation \nfor that closure must relocate every token in the sequence. To minimize the number of re\u00adlocating transformations, \nit would be better to create a closure for each token that depends on a relocat\u00adable address, since relocating \ntransformations for sin\u00adgle tokens can be reused more freely than those for sequences. Moreover, by splitting \nup sequences, we could emit some tokens immediately, not having to wait until relocation time. The logical \nextreme of this approach would be to create separate closures for in\u00addividual fields, not just tokens. \nThis technique would probably not reduce the number of transformations fur\u00adther, but it would make it \neasy to create idempotent transformations without having to store partial tokens in closures. The abstract \nview of relocatable addresses has a cost, If we exposed the label i-offset represen\u00adtation at code-generation \ntime, we could realize ex\u00adtra savings. Offsets are always available at encoding time, and they could \nbe hoisted out of closure func\u00adtions. Storage requirements would be reduced because a label occupies \nno more than half the space of a (la\u00adbel, offset) pair. (The ELF object-code standard en\u00adables such space \noptimization by providing for reloca\u00adtion entries both with and without offsets.) Exposing the representation \nof relocatable addresses would also make it possible to treat certain labels, like those of the ELF global \noffset table and procedure linkage table, as special cases. Such treatment would make it possible to \nshrink machine-independent object code by moving these special labels back into the As. Currying and \nhoisting make it possible to write effi\u00adcient, machine-independent tools that relocate machine instructions. \nThe New Jersey Machine-Code Toolkit can derive C implementations of relocating transfor\u00admations from \na set of machine descriptions, and a tool writer can incorporate those implementations to pro\u00advide efficient \nrelocation on a number of platforms. If the tool includes an interpreter for a bytecode rep\u00adresentation \nof relocating transformations, it can relo\u00adcate instructions for any machine even a machine that doesn \nt exist when it is released. Acknowledgements Mary Fernandez helped create the toolkit on which this \nwork is based, and she put together an mld I could use for performance measurements. Peter Sestoft provided \nhelpful pointers to the literature on partial evaluation and functional programming, Mary Fern6ndez, \nVince Russo, Zhong Shao, Michal Young, and the anonymous reviewers criticized the manuscript in helpful \nways, References Appel, Andrew W. 1992. Compiling with Continuations. Cambridge: Cambridge University \nPress. Ball, Thomas and James R. Larus. 1992 (January). Opti\u00ad mally profiling and tracing programs. In \nConference Record of the 19th Annual ACM Symposium on Prin\u00ad ciples of Programming Languages, pages 59 \n70, Albu\u00ad querque, NM. Briggs, Preston and Keith D. Cooper. 1994 (June). Ef\u00ad fective partial redundancy \nelimination. Proceedings of the ACM SIGPLAN 94 Conference on Programmmg Language Design and Implementation, \nin SIGPLAN Notices, 29(6):159-170. FernAndez, Mary F. 1995 (June). Simple and effective link\u00adtime optimization \nof Modula-3 programs. Proceedings of the ACM SIGPLAN 95 Conference on Program\u00adming Language Design and \nImplementation, in SIG-PLAN Notices, 30(6):103-115. Fraaer, Christopher W. and David R. Hanson. 1982 \n(April). A machine-independent linker. Software-Practice &#38; Elcperience, 12(4):351-366. Gircys, Gintaras \nR. 1988 (November). Understanding and Using COFF. Nutshell Handbooks. Sebastopol, CA: O Reilly &#38; \nAssociates. Hanson, David R. 1990 (January). Fast allocation and deallocation of memory based on object \nlifetimes. Sofiware-Practice @ Experience, 20(1):5-12. Hastings, Reed and Bob Joyce. 1992 (January). \nPurify: Fast detection of memory leaks and access errors. In Proceedings of the Winter USENIX Conference, \npages 125-136. Johnson, Stephen C. 1990. Postloading for fun and profit. In Proceedings of the Winter \nUSENLY Conference, pages 325-330. Jones, Neil D., Peter Sestoft, and Harald SOndergaard. 1989. Mix: A \nself-applicable partial evaluator for ex\u00adperiments in compiler generation. Lisp and Symbolic Computation, \n2(1):9-50. Jones, Douglas W. 1983 (August). Assembly language as object code. Soflware-Pract~ce U Experience, \n13(8). Kane, Gerry. 1988. MIPS RISC Architecture. Englewood Cliffs, NJ: Prentice Hall. Larus, James R. \nand Eric Schnarr. 1995 (June). EEL: machine-independent executable editing. In Proceed\u00adings of the ACM \nSIGPLAN 95 Conference on Pro\u00adgramming Language Design and Implementation. Peyton Jones, Simon L. 1987. \nThe Implementation of Func\u00adtional Programming Languages. International Series in Computer Science. Englewood \nCliffs, NJ: Prentice Hall. Prentice Hall. 1993a. System V Application Binary Inter\u00adface. Third edition. \nEnglewood Cliffs, NJ. Unix Press. Prentice Hall. 1993b. System V Application Bvnary In\u00adterface, SPARC \nArchitecture Processor Supplement. Third edition. Englewood Cliffs, NJ. Unix Press. Proebsting, Todd \nA. 1995 (January). Optimizing an ANSI C interpreter with superoperators. In Conference Record of the \n22nd Annual ACM Symposium on Pri\u00adnciples of Programming Languages, pages 322 332, San Francisco, California. \nRamsey, Norman and Mary F. Fernindez. 1994 (October). New Jersey Machine-Code Toolkit architecture speci\u00adfications. \nTechnical Report TR-470-94, Department of Computer Science, Princeton University. 1995 (January). The \nNew Jersey Machine-Code Toolkit. In Proceedings of the 1995 USENIX Technical Conference, pages 289 302, \nNew Orleans, LA. Ramsey, Norman. 1992 (December). A Retargetable Debug\u00adger. PhD thesis, Princeton University, \nDepartment of Computer Science. Also Technical Report CS-TR-403\u00ad 92. Sites, Richard L., Anton Chernoff, \nMatthew B, Kirk, Mau. rice P. Marks, and Scott G. Robinson. 1993 (Febru\u00adary). Binary translation. Communications \nof the ACM, 36(2):69-81. Srivastava, Amitabh and Alan Eustace. 1994 (June). Atom: A system for building \ncustomized program analysis tools, In Proceedings of the ACM SIGPLAN 94 Con\u00adference on Programming Language \nDesign and Imple\u00admentatiotz, pages 196 205. Srivastava, Amitabh and David W. Wall. 1993 (March). A practical \nsystem for intermodule code optimization. Journal of Programming Languages, 1: 1 18. Also avail\u00adable \nas WRL Research Report 92/6, December 1992. Szymanski, Thomas G. 1978 (April). Assembling code for machines \nwith span-dependent instructions. Commu\u00ad nications of the ACM, 21(4):300 308. Wahbe, Robert, Steven \nLucco, Thomas E. Anderson, and Susan L. Graham. 1993 (December). Efficient software\u00adbased fault isolation. \nIn Proc. Fourteenth ACM Sympo\u00adsium on Operating System Principles, pages 203 216.  \n\t\t\t", "proc_id": "231379", "abstract": "Relocation adjusts machine instructions to account for changes in the locations of the instructions themselves or of external symbols to which they refer. Standard linkers implement a finite set of relocation transformations, suitable for a single architecture. These transformations are enumerated, named, and engraved in a machine-dependent object-file format, and linkers must recognize them by name. These names and their associated transformations are an unnecessary source of machine-dependence.The New Jersey Machine-Code Toolkit is an application generator. It helps programmers create applications that manipulate machine code, including linkers. Guided by a short instruction-set specification, the toolkit generates the bit-manipulating code. Instructions are described by <i>constructors</i>, which denote functions mapping lists of operands to instructions' binary representations. Any operand can be designated as \"relocatable,\" meaning that the operand's value need not be known at the time the instruction is encoded. For instructions with relocatable operands, the toolkit computes relocating transformations. Tool writers can use the toolkit to create machine-independent software that relocates machine instructions. mld, a retargetable linker built with the toolkit, needs only 20 lines of C code for relocation, and that code is machine-independent.The toolkit discovers relocating transformations by currying encoding functions. An attempt to encode an instruction with a relocatable operand results in the creation of a closure. The closure can be applied when the values of the relocatable operands become known. Currying provides a general, machine-independent method of relocation.Currying rewrites a &amp;lambda;-term into two nested &amp;lambda;-terms. The standard implementation has the first &amp;lambda; allocate a closure and store therein its operands and a pointer to the second &amp;lambda;. Using this strategy in the toolkit means that, when it builds an application, the toolkit generates code for many different inner &amp;lambda;-terms---one for each instruction that uses a relocatable address. Hoisting some of the computation out of the second &amp;lambda; into the first makes many of the second &amp;lambda;s identical---a handful are enough for a whole instruction set. This optimization reduces the size of machine-dependent assembly and linking code by 15--20% for the MIPS, SPARC, and PowerPC, and by about 30% for the Pentium. It also makes the second &amp;lambda;s equivalent to relocating transformations named in standard object-file formats.", "authors": [{"name": "Norman Ramsey", "author_profile_id": "81100300481", "affiliation": "Department of Computer Sciences, Purdue University, 1398 Computer Science Building, West Lafayette, IN", "person_id": "PP14110628", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/231379.231429", "year": "1996", "article_id": "231429", "conference": "PLDI", "title": "Relocating machine instructions by currying", "url": "http://dl.acm.org/citation.cfm?id=231429"}