{"article_publication_date": "05-01-1996", "fulltext": "\n Fast, Effective Dynamic Compilation Joel Auslander, Matthai Philipose, Craig Chambers, Susan J. Eggers, \nand Brian N. Bershad Department of Computer Science and Engineering University of Washington {auskmd,matthai,chanrbers,eggers,bershad) \nt?cs.Washington.edu Abstract Dynamic compilation enables optimization based on the values of invariant \ndata computed at run-time. Using the vafues of theserun\u00adtime constsmts, a dynamic compiler can eliminate \ntheir memory loads, perform constant propagation and folding, remove branches they determine, and fully \nunroll loops they bound. However, the performance benefits of the more efficient, dynamically-compiled \ncode are offset by the run-time cost of the dynamic compile. Our approach to dynamic compilation strives \nfor both fast dynamic compilation and high-quality dynamically-compiled code: the programmer annotates \nregions of the programs that should be compiled dynamically; a static, optimizing compiler automatically \nproduces pre-optimized machine-code templates, using a pair of dataflow analyses that identify which \nvariables will be constant at run-time; and a simple, dynamic compiler copies the templates, patching \nin the computed values of the run-time constants, to produce optimized, executable code. Our work targets \ngeneral\u00adpurpose, imperative programming languages, initially C. Initial experiments applying dynamic \ncompilation to C programs have produced speedups ranging from 1.2 to 1.8. One man s variable is another \nman s constant. adapted from Alan J. Perlis per90] Introduction Traditional compilation performs optimizations \nthat either are independent of the actual values of program variables or depend on tbe values of compile-time \nconstants. It is unable to optimize around variables whose values are invariant during program execution, \nbut are unknown until then. Consequently, these variables must be reinterpreted on each run-time use \nand cannot trigger value-based optimization. Our work applies dynamic compilation to enlarge the scope \nof optimization to include those that depend on the values of variables that are computed at run-time, \nand once computed, remain fixed for some significant period of time. By compiling performance-critical \nparts of the program at run-time, more efficient code can be produced. For example, run-time constants \ncan become instruction immediate rather than memory loads, constant propagation and folding can be applied \nto them, conditioned branches based on them can be eliminated, and loops they control cart be fully unrolled. \nData structures can be considered run-time constants whenever they are accessed through run-time constant \npointers. Applications with such run-time constants include interpreters (where the data structure that \nrepresents the program being interpreted is the run\u00adtime constant), simulators (where the circuit or \narchitecture description is a run-time constant), graphics renderers (where the scene or viewing parameters \nare constant), numerical codes (where scalars, vectors, matrices, or the patterns of sparsity can be \nrun-time constant), and extensible operating system kernels (where the current set of extensions to the \nkernel is run-time constant @SP+95,CEA+96]). Dynamic compilation can realize overall performance improvements, \nhowever, only if the execution time savings from the dynamically-generated code is greater than the time \nto compile at run-time. Different trade-offs between dynamic compilation time and dynamically-compiled \ncode quality are possible, and previous systems have explored several alternatives [CN96,EHK96,LL96]. \nOur approach strives to achieve the best of both options: fast dynamic compilation and high-quality dynamically-generated \ncode. We do this by planning out most of the actions of dynamic compilation in advance (during static \ncompilation), based on the static knowledge of which variables and data structures will be invariant \nat run-time, but without knowing their exact run-time vafues. Our dynamic compilation system targets \ngeneral-purpose, imperative programming languages, initially C. Because of the difficulty in this general \nsetting of automatically identifying which data structures will be invariant over which portions of the \nprogram, and where this invariance will be profitable to exploit through dynamic compilation, our current \nimplementation relies on simple, programmer-inserted annotations. The annotations indicate which parts \nof the program should be compiled dynamically (called dynamic regions) and which source variables will \nbe constant during execution of the dynamic regions. Through a kind of constant propagation and constant \nfolding, our system automatically identifies other derived run-time constants in the region, Our dynamic \ncompilation system is composed of both a static and a dynamic compiler. To achieve fast dynamic compile \ntimes, the static compiler produces pre-compiled machine-code femplates, whose instructions contsin holes \nthat will be filled in with run-time constant values. The static compiler also generates set-up code \nto calculate the values of derived run-time constants, and directives that instruct the dynamic compiler \nhow to produce executable code from the templates and the set-up code s computed constants. ~ ~ Permissionb \nmake digitalhr d COPYof part or all of this work for personal or dassmom use is ranted without fee provided \nthat @pies are not made or distribubsdtir pm i t or eommeroial advantage. the mpyright notica, the title \nof (he publication and its date appear, and notice is given that copying is by permission of ACM, Inc. \nTo copy otherwka, to republish, to postonservers,or to redistributeto lists,requirespriorspecificpermission \nanctlora fee. PLDI 96 5/96 PA, USA O 1996 ACM 0-69791-79549640005...$5959 Given this advance preparation, \nthe dynamic compiler (called the stitcher) simply follows the directives to copy the machine-code templates \nand fill in the holes with the appropriate constants. The run-time overhead of dynamic compilation (the \nstitcher, set-up code, and directives) is executed at most once per dynamic region; the dynamically-compiled \ntemplates become part of the application and usually are executed marry times. 149 To generate high. \nqu~ity dynamically-compiled code, the static compiler applies standard global optimization to the machine-code \ntemplates, optimizing them in the context of their enclosing procedure. It also plans out the effect \nof run-time constant-based optimization, so that the final, optimized templates contain only the calculations \nthat remain after these optimization have been performed. Our dynamic compilation work is novel in several \nrespects. First, it is capable of handling the full functionality of C, without restricting its normal \nprogramming style. Second, automatic run-time constant derivation is accomplished via two interconnected \ndataflow analyses, one that identifies run-time constants and another that determines their reachability \nconditions downstream of run-time constant branches. When executed in parallel, they provide an analysis \nthat is general enough to handle unstructured control flow. Finally, by integrating our analyses into \nan optimizing compiler, dynamically-compiled code can be heavily optimized with its surrounding code, \nwith few limitations on the kinds of optimization that can be applied. This paper presents our dynamic \ncompilation framework and the algorithms that optimize dynamic regions with respect to their nm\u00adtime \nconstants. The next section outlines the programmer annotations that drive the analysis. Sections 3 and \n4 describe the static and dynamic compilers, respectively. Section 5 reports on empirical studies of \nthe effectiveness of dynamically compiling several programs. Section 6 provides a detailed comparison \nto related work, and section 7 concludes with a summary of our main contributions and directions for \nfuture work. Programmer Annotations Our current implementation relies on programmer annotations to specify \nwhich regions of code to dynamically compile, which variables to treat as run-time constants, and which \nloops to completely unroll. To illustrate the annotations (as well as the rest of the dynamic compilation \nframework), we will use the following cache lookup routine of a hypothetical cache simttl~tor (the bold \nkeywords are the programmer-supplied annotations): cacheResult cacheLookup (void addr, Cache *cache) \n{ dynaraicRegion ( cache ) { I* cache is run-lime constanr */ unsigned blockSize = cache->blockSize; \nunsigned numLines = cache ->numLines; unsigned tag = (unsigned) addr / (blockSize * numLines) ; unsigned \nline = ( (ursigned) addr / blockSize) % ntiines; setStructure **setArray = cache->lines [line] ->sets; \nint assoc = cache->associativity; int set; unrolled for (set = O; set < assoc; set++) { if (setArray[set]dynsuaic->tag \n== tag) return CacheHit; } return 2acheMiss; ) /*end ofdynamicRegion*/ } dyra-cStegivxa delineates the \nsection of code that will be dynamically compiled (in the example, the body of the cacheLookup function). \nThe arguments to dy%mraicRegion indicate which source variables are constant at the entry of the dynamic \nregion and remain unchanged throughout this and all future executions. The static compiler automatically \ncomputes all run-timeconstants thatarederived fromthisinitial set,asdescnbed in section 3.1. There is \nno restriction on the kind of data that we can * Our current implementation uses a collection of lower-level \nannotations that provide the same information but do not require modifications to the C parser. treat \nas a run-time constant; in particular, the contents of arrays and pointer-based data structures are assumed \nto be run-time constants whenever accessed through run-time constant pointers. For partially-constant \ndata structures, we use an additional annotation on memory dereference operators to indicate that the \nresult of the dereference isavtiable, even ifitsmgument isconstmt, e.g., x := dynaraic* p, x :=pdynaraic \n->f, and x := a dyna.rai c [ i ]. Qn the above example, the tags stored in the cache arenot constant. \n) unrolled directs the dynamic compiler to completely unroll a loop. The loop termination condition must \nbe governed by a run\u00adtime constant. Complete unrolling is a critical dynamic optimization, because it \nallows loop induction variables and data derived from them to be run-time constants (the value of an \ninduction variable in each unrolled copy of the loop is a distinct, fixed vrdue). Since not all loops \ngoverned by run-time constants are profitable or practical to unroll, we only unroll annotated loops. \nWe can automatically check whether an annotated loop is legal to unroll, using the analyses described \nin section 3.1. For some applications, it is important to produce several compiled versions for a single \ndynamic region, each optimized for a different set of run-time constants. For example, if the cache simulator \nwere simulating multiple cache confi~~rations simultaneously, each configuration would have its own cache \nvalues and need cache lookup code specialized to each of them. Accordingly, we allow a dynamic region \nto be keyed by a list of run-time constants. Separate code is generated dynamically for each distinct \ncombination of values of the key variables; the generated code is cached and reused for later invocations \nof the region with the same key values. The dynamic region of the multi-cache simulator example would \nbe annotated as follows: dynasa.icRegi.on key(cache) (/*no other con.rtants */) { . . . } Given these \nannotations, our system manages all other aspects of dynamic compilation automatically. Programmers are \ninsulated from the dynamically-compiled code, and the interface to procedures containing dynamic regions \nis unchanged. This is in contrast to some other systems that require more programmer involvement in the \nmanagement of dynamically-compiled code [EP92,EHK96,CN96]. We chose this set of annotations as a balance \nbetween a completely manurd approach and a completely automatic approach. Given a few select annotations, \nour compiler can automatically identify the derived run-time constants and perform several important \noptimization, saving the programmer much of the tedious and error-prone effort of a fully-manual approach. \nThe drawback is that errors intheannotations canlead to incorrect optimizations being performed dynamically. \nUnfortunately, automatic, safe, and effective dynamic compilation is quite challenging: it requires whole-program \nside-effect and pointer analysis to reason about invariance of variables and (parts of) data structures, \nanalysis of loops to judge termination conditions, profile information to choose both dynamic regions \nthat are the most profitable and loops that make sense to fully unroll, and so on. Data structures that \nare invariant for only part of a program s execution or routines that should bereplicated for different \ninvariant clients are even harder to handle automatically. Ourlong-temn goal is to try to automate most \nof thedynarrdc compilation process, but fornow our simple annotations are both practical to use and facilitate \nearly experimentation with different choices for run-time constants and dynamic regions. Annotations \nmay also be useful as a human\u00adreadable intermediate representation for more automatic implementations. \nt For this ex~pie, it turns out that this annotation is unnecessary, since the dereferenced pointer is \nnot run-time constant. 150 3 The Static Compiler The static compiler compiles procedures that do not \ncontain dynamic regions normally, For procedures with dynamic regions, it performs the following four \nsteps: * It identifies which variables and expressions within the dynamic region will be constant at \nrun-time, based on the set of variables annotated at the start of the region. This step plans the constant \npropagation, constant folding, dead code elimination, and loop unrolling that will be performed by the \nstitcher at run\u00adtime. * It splits each dynamic region subgraph into set-up and template code subgraphs, \nreplacing the region s original subgraph with the corresponding pair of subgraphs. * It optimizes the \ncontrol flow graph for each procedure, applying all standard optimization with few restrictions. * It \ngenerates machine code and stitcher directives. We have chosen to embed our support for dynamic compilation \ninto a standard, optimizing compiler framework for two reasons. First, we wished to generate high-quality \ndynamically-compiled code; we therefore integrated our specialized analyses into an infrastructure that \nalready performed sophisticated optimization. Second, we wished to support a variety of general-purpose \nprogramming languages, including C, without restricting their normal programming style. Accordingly, \nour analyses and transformations operate at the lower but more general level of control flow graphs connecting \nthree-address code [ASU86], rather than the higher, language-specific level of abstract syntax trees \n(as does some other work this [CN96,KR96,LL96])~ Our analyses go to sr%e length to sup&#38;~ partially \nunstructured control flow graphs well, since these graphs occur frequently in C programs. We consider \nthe increased generality of our analyses to be an important contribution of our work. The rest of this \nsection discusses the four steps executed by the static compiler when compiling code that contains dynamic \nregions. 3.1 Computing Derived Run-Time Constants As the first step in compiling a dynamic region, the \nstatic compiler computes the set of variables and expressions that are constantt at each point in the \ndynamic region, given the set of constants annotated by the programmer. This analysis is similar to binding \ntime analysis in off-line partial evaluators [SZ88,JGS93] (except that our analysis is at the level of \ncontrol flow graphs rather than abstract syntax trees) and to traditional constant propagation and folding \n(except that our analysis must cope with knowing only that a variable will be constant, not what the \nconstant value is). We have developed a pair of intercomected analyses, one that computes the set of \nrun-time constant variables at each program point, and another that refines that solution by computing \nreachability information downstream of run-time constant branches. We first describe the run-time constants \nanalysis alone, and then augment it with the reachability analysis. Appendix A contains a more precise \nspecification of our algorithms, * By unstmcture&#38; we mean graphs that are not composed solely of \nnested singIe-entry/single-exit regions corresponding to syntactic nesting, but rather have some control \nflow transfers that do not respect the syntactic nesting structure. By this definition, commotdy-occurring \nunstructured corNtucts in C include switch statements with fall-through case s, break and continue statements, \nand goto statements (for instance, implementing multi-level loop exits and hand-eliminated tail recursion). \nt For brevi~ we use. the term cOnStMt tIJ refer to inn-time Constmts. which include compile-time constants \nas a special case. The run-time constants analysis is a forward dataflow analysis that computes the \nset of variables that are run-time constants for each program point. (To simplify the exposition, we \nassume that the dynamic region is in static single assignment (SSA) form [AWZ88,CFR+89].) At the start \nof a region, the set of constants is the set of variables specified by the programmer. Analysis proceeds \nby propagating this initial set of constants through the dynamic region s control flow graph, updating \nthe set after each instruction as follows: * x : = y: x is a constant iff y is a constant. X:=yopz: x \nis a constant iffy and z are constants and op is an idempotent, side-effect-free, non-trapping operato~ \nfor example, / is excluded, since it might trap. Unary operations are handled similarly. X:= f(yl, . \n. . , Yn ): x is a constant iff the Yi are constants and f is an idempotent, side-effect-free, non-trapping \nfunction, such as ntax or COS. mal 10C is excluded, since it is not idempotent. .X:= *P: x is a constant \niff p is a constant, * x : = dynamic* p: x k not a constant. * *p : =x: Stores have no effect on the \nset of constants. A load through a constant pointer whose target has been modified to hold a non-constant \nvalue during execution of the dynamic region should use dynamic *. After a control flow merge, if a \nvariable has the same run-time\u00adconstant reaching definition along all predecessors, it is considered \na constant after the merge. However, if a variable has different reaching definitions along different \npredecessors, the value of the variable may not be a run-time constant after the merge, even if all reaching \ndefinitions before the merge are constant. For example, in the following control flow graph, if testis \nnot a constant, after the merge x could be 1 on some executions and 2 on others, and hence it cannot \nbe treated as a run-time constant. (h the figure, the sets labeling each arc represent the computed constant \nsets at those program points.) t~ := test /* assume Iesr is not constant *I if (test) { (} t~? } %l~e \n1/ (} (}.X2 =2; * ] xl and x2 are constants / = @(xl, xZ); 2X3 is not constant V v  On the other hand, \nif test is a constant, for any execution of the dynamic region in a given program run, either testis \nalways true and x is always 1 after the merge, or test is always false and x is always 2 after the merge. \nIn the first case, the 1$function after the merge is not an idempotent operator (and so its result is \nnot constant 151 irrespective of whether its arguments are constant), while in the second case it is. \nt~ := test I* testis a constant V (tl} if (test) ( x* =1; * .-. z ~1~ } <lse { (tl} {tl}X2 = 2; xl :=1 \nX2 := 2 ;* xl and X2 are constants *1 X3 = +(X1,X2); Xzltl) {xl, t~ I* x, is constant *I const: (xl, \nx~, tl} X3 := $l(x~,x~) (X3, tl} Identifying constant merges whose comesponding branches have constant \npredicates cart be done for structured, nested control flow graphs by identifying diamond-shaped, single-entry/single-exit \nsubgraphs.* However, to handle general C programs well, we need to identify constant merges even in unstructured \ncontrol flow graphs. Accordingly, we supplement our run-time constants analysis with a reachability analysis \nthat computes the conditions (in terms of branch outcomes for run-time constant branches) under which \neach program point can be reached during execution. Then, if the reachability conditions for each merge \npredecessor are mutually exclusive, the merge is labeled as a constant merge and can use the better idempotent-$ \nrul~ otherwise, the merge must use the more conservative non-idempotent-$ rule. Our reachability analysis \nis a forward dataflow analysis that is performed in parallel with the run-time constants analysis.+ The \nreachability analysis computes conjunctions and disjunctions of branch conditions at each program point, \nwhere each branch condition has the form B+.,$ for a constant branch B (either 2-way or n-way) that goes \nto successor arc S. We use sets of sets to represent disjunctions of conjunctions, in conjunctive normal \nform (CNF). For example, the set {{A+ T), {A+~ B-+1}}, computed for program point p, can only be executed \nif A s constant predicate is true or if A s constant predicate is false and B s constant switch value \ntakes case 1. At the start of the dynamic region, the reachability condition is true (represented by \nthe set {{}}), since the entry is always reachable. Straight-line code and branches whose predicates \nare not run-time constants have no effect on the reachability analysis. A branch B whose predicate is \na run-time constant updates the reachability set along successor S by atrd ing in the condition B+,S \n(in the CNF set representation, B+~ k added to each of the element sets). At merges, the incoming conditions \nare or d together. (At the representation level, the incoming sets are combkted with set union, followed \nby simplifications which reduce sets of the form {{A+~ CS},{A+F, cs},~.s} to {{ CS},DS}.) The following \nexample illustrates the results of reachability analysis on an unstructured control flow graph for two \ndifferent situations (the labels on the arcs * Alpem et al. extended $ functions to include an argument \nrepresenting the corresponding branch predicate, for structured if and 1 oop constructs [AWZ88]. This \nwould allow $ to be treated as idempotent for atl merges: if all the reaching definitions and the branch \npredicate were constant, then the result would be constant. Unfortunately, this technique dces not extend \neasily to unstructured control flow graphs. t The ~achabiliry malysis uses the resuks of run-time Constantsm~ysis \nto identify run-time constant branches, and the run-time constants analysis uses the results of reachability \nanatysis to choose between $ merge rules [CC95]. in this figure are reachability conditions in the CNF \nset representation, not sets of run-time constants): if (a) ( ...h l... ; } else { switch (b) ( case \n1: ...N.... Pfall through *I case 2: ...0.... break; case 3: ...p... ; goto L; ) :.Q...; ?.R... ; L: \nIf a and b are constant branches: -L-({N {(a+7)) ...M... I {{a+F,b+l}} ...N... I 1 {{a+F,b+l},(a+F,b+2}) \nCon.m ... ... [R Cortst: If only a is a constant branch: {{})L  {{a+n} ...M... I1 {{a+F)} ...N... 11 \n1 Const: R ... ... [ var: In the upper graph, where both a and b are cortstrmt, the reachability analysis \ndetermines that all three merges are constant merges, since the reachability conditions of the predecessors \nof each of the merges are mutually exclusive. As a result, any variables identified as run-time constant \nalong all predecessor branches of a merge will be considered constant after the merge. In the lower graph, \nonly one of the merges is constant. Conjunctions of branch conditions support sequences of branches, \nwhile disjunctions of branch conditions are crucial for coping with unstructured graphs.t An analysis \nbased solely on abstract syntax trees would have a difficult time identifying as many run-time constants \non unstructured programs. A loop head is a merge node. Since the reachability conditions of the loop \nentry arc and the loop back edge arc will not normally be mutually-exclusive, our analysis as described \nso far will treat the loop head as a non-constant merge. This is safe, and it is appropriate for loops \nthat are not unrolled. For unrolled loops, however, only one predecessor arc will ever enter an unrolled \ncopy of the loop head merge at run-time: either the loop entry predecessor (for the first iteration) \nor the loop back edge predecessor (for subsequent iterations). Accordingly, we mark all loop heads for \nunrolled loops as constant merges. As a consequence, loop induction variables can be identified as constants \nwithin the loop body. The following example illustrates how labeling an unrolled loop head as constant \nenables the main loop induction variable p to be marked constant (arcs are labeled by constant sets and/or \nreachability conditions, depending on what information changes at that arc): + {lSt} {{}} / 1st is constant \nV unrolled for(p = lst; Pi := 1st ~ != ~~; (PI, lst}p = p->next) { const: F p is constant / . . . (Plr \nP3,1st} ) P2 := 0( P1, P3) t := (P2 != NULL) {(t+q} ... [1 P3 := p2 ->next I J {p2,p3,t,kt} T The following \ncode shows (using underlining) the expressions identified as run-time constants in the dynamic region \nof the cache lookup example from section 2: dynam.icRegion(cache) ( ze . -->blo~ ; s . ca~; unsigned \ntag = .* (unsigned) addr / J&#38;&#38;.&#38;Slze ; unsigned line = ((unsigned) addr / ~) % ~; setStructure \n**setArray = --> m[linel->sets; APt set ; ~ =. < ++ ( if (setArray[~]dynamic->tag .= tag) return CacheHit; \n) return CacheMiss; } 3.2 Extracting Set-Up Codeand Templates After identifying run-time constant calculations, \nthe static compiler divides each dynamic region into twoseparate subgraphs: set-up code and template \ncode. Set-up code includes alI the calculations that define run-time constants. Templates contain all \nthe remaining code within the region, with holes embedded in some instructions for run-time constant \noperands. Additionally, templates contain marker pseudo-instructions identifying the entry, exit, snd \nback Theextratlexibility ofbeingabletorepresent disjunctionsdoes, however, lead to a worst-case sirs \nof a reachability condition for a program point that is exponential in the number of constant branches \nin the dynamic region. In practice, the size of reachability conditions has been smatl. edge arcs of \nunrolled loops to help generate stitcher directives (described in section 3.4). The control flow connections \nof the two subgraphs are the same as in the original dynamic region. Once constructed, these two subgraphs \nreplace the original subgraph of the dynamic region, roughly as follows: dynamic region entrance Jirsttime? \nset-up code &#38; template code + dynamic region exit Theset-upcodeis executedonlythe firsttimethedynamic \nregion is entered, and it calculates all the run-time constants needed in the region. The set-up code \nstores all the constants referenced by template code in a table data structure, which it passes to the \ndynamic compiler for use in instantiating templates into executable code (asdescribed infection 4). Formost \ncode, run-time constant table space can be pre-allocated, enabling the set-up code to store computed \nconstants quickly into the table. However, for fully\u00adunrolled loops, an unbounded amount of space maybe \nneeded. We solve this problem by allocating a fresh record for each iteration of an unrolled loop. Within \neach iteration, we can statically allocate the run-time constants computed within that iteration. The \nset-up and template code for the cache Iookup routine (expressed in C rather than as a flow graph, for \nreadability) is shown in Figure 1. The set-up code calculates all constants and stores those needed by \nthe template code into the table t. The last element of t acts as the head of a linked-list of table \nrecords for the run-time constants within the unrolled loop. The template code contains hole markers \nfor the necessary run-time constants, plus markers that delimit the unrolled loop s iterations. 3.3 Optimization \nA major goal of our approach is to allow MI optimization to be performed on procedures that contain dynamic \nregions. In particular, we wish optirnizations such as global common subexpression elimination and global \nregister allocation to be performed across dynamic region boundaries. Optimizations can be performed \nboth before and atler the body of the dynamic region is divided into set-up and template code. We pIace \nno restriction on optimizations performed before this division. Optimization performed afterwards must \nbe modified slightly to deal with the special semantics of hole operands in templates. For the most part, \nthe compiler s analyses can treat each hole marker asacompile-time constant oftmknownvahse. However, \nin a few circumstances hole markers must be treated differently: * Instructions in a template subgraph \nthat contain hole markers cannot be moved (e.g., by code motion or instruction scheduling) outside the \ntemplate subgraph. * Holemmker vduesshould not retreated aslegal values outside the dynamic region. In \nparticular, copy propagation should not propagate references to holes outside the dynamic region. * 1401es \nthat correspond to induction variables defined in run\u00adtime unrolled loops cannot be treated as loop-invariant \nwith respect to the unrolled loop; each iteration of the unrolled loop will get its own distinct version \nof the value. In our current implementation, we conservatively satisfy these requirements by placing \nbarriers to code motion and other 153 Original dynamic region, after run-time constants identification: \ndynemicRegion(cache) { lze . r?che ->blocw; . cache ->numL~; mluned unsigned tag = (unsigned) addr / \nunsigned line = ((unsigned) addr / blockSize )%m&#38;in.eS; setStructure * setArray = ~ [line]->sets; \n ~; ,.. . ; unxolled for (set =. set < AS.sot; set ++) { if (setArray[W]dynarnic->tag == tag) return \nCacheHit; ) return CacheMiss; J Set-upcode: t = allocateTable (5); I allocate space forconstanttable \nl t[O] = tO = cache->blockSize; t[l] . tl = cache->ntiines; t[2] =to * tl; t[3] = cache->lines; assoc \n= cache->associativity: P notusedintemplates*/ loopTable = &#38;t[4]; Pheadofunrolledloop s listoftables \nV for (set = O; ; set++) { lt = *loopTable = allocateTable(3); It[O] = ltO = (set < assoc); if (!ltO) \nbreak; lt[l] = set; loopTable = &#38;lt[2]; Pnex!poiruerforloop s linkedlist *I ) Template code (where \nhole4,x references the Xth entty of the appropriate iteration of the loop headed by t [4 ] ): LO: enter \nregion marker; L1 : unsigned tag = (Unsigned)addr / hole2; L2 : unsigned tl = (Unsigned)addr 1 holeo; \nL3 : unsigned line = tl % holel; L4 : set_structure **setArray = hole3[linel->sets; L5 : unrolled loop \nentry marker; L6 : constant branch marker (hole4:o) L7 : t2 = setArraylhole4.11 ->tag; La: if (t2 == \ntag) { L9 : unrolled loop em t marker; return CacheHit; } L1O unrolled loop back edge marker; Lll unrolled \nloots exit marker; return CacheMiss; L12: exitregionmarker; Stitcher directives, ianorina labels: START(LO) \n-\u00adHOLE(L1,2,2) HOLE(L2 2,0) HOLE(L3,2,1) HOLE(L4 1,3) ENTER_LooP(L5 ,4) CONST_BRANCH (L6,4:O) HoLE(L7,2,4:1) \nBRANCH(L8) ExIT_LooP(L9 ) RESTART_LOOP (L1O,4:2 EXIT_LOOP(Lll) END(L12) Shape of constants table computed \nby set-up code: t + blockSize nurnLines %S *nL lines loopTableO-~ set. < assoc set. loopTablel .. Figure \n1: Set-Up Code, Templates, and Directives optirnizations atthestart and end of the template code and \nat the head of unrolled loops.  3.4 Code Generation The final step of static compilation is to generate \nmachine code from the optimized control flow graph. For dynamic regions, code generation also produces \nstitcher directives. The stitcher directives forrnasimple instrttction set, described in Table 1. Directives \nare generated as a side-effect of generating code for instructions containing holes (toinform the stitcher \nto patch in the hole s nm\u00adtime value), for the markers that identify the entry, exit, and back\u00adedge branches \nof unrolled loops (to inform the stitcher how to break up the pieces of the loop for loop unrolling), \nand for any pc-relative instructions, such as branches (which need to be adjusted when the stitcher copies \nthe templates). Section 4 describes the actions performed by the stitcher for these directives. Table \nl: Stitcher Directives Directive When Generated START(inst) beginning of template code END(insr) end \nof template code I HOLE(inst, operand #, const s table index) I hole marker operand in instr I lCONST_BRANCH(inst, \ntest3t&#38;le index) lholemmkerm brmcht=t I ENTER_LOOP(insr, ~able header index) unrolled loop en@y marker \nEXIT_LOOP(inst) unrolledloopexit marker lRESTART_LOOP(ins/, nexttableindex) Iunrolled backedgemsrker \nI lBRANCH(insr) Iuc-relativeinstruction I LABEL(ins?) target of pc-rdative instr. The stitcher directives \nfor the cache lookup example appear in Figure 1. 4 The Stitcher Given the preparation by the static compiler, \nthe stitcher has only to follow the directives to instantiate the machine-code templates. Most of these \ntasks are straightforward, such as copying blocks of machine code between directive labels and updating \noffsets in pc\u00adrelative branch and call instructions. For run-time constant branches, the stitcher uses \nthe corresponding value in the run-time constants table (computed by the set-up code) to select the appropriate \nsuccessor path, implicitly performing dead code elimination of the other path(s). For unrolled loops, \nthe stitcher traverses links in the run-time constants table to access the appropriate subtable for each \niteration; the directives at loop entries, exits, and back edges instruct the stitcher when to switch \nbetween subtables. The stitcher also patches the vahres of run-time constants into the machine-code template \nholes. For an integer operand, the static compiler has selected an instruction that admits the hole as \nan immediate operand, and the stitcher first tries to fit the run-time constant integer value into it. \nIf the value is toolsrge, the stitcher either generates code to load the value from a table of large \nrun-time constants or constructs it from smaller constants. For floating-point and pointer constant operands, \nwhich typically will not fit into the immediate field, the static compiler inserts the load instruction \nas part of producing template code, so that the load can be better scheduled during regular static optimizations. \nAs initialIy constructed, the nested structure of the run-time constants table requires afairarnount \nofrun-time bookkeeping to track which loop iteration is being handled. In addition, accesses to the table \nare likely to be sparse, since only the large or non-integer 154 Table 2: Speedup and Breakeven Point \nResults Asymptotic SpeedupBenchmark Run-time Constant (static/Configurations dynamic region times) Reverse-polishstack-baseddesk \nixy-3y~-XL+(x+5) * 1.7 calculator y-x) +x+y-l ( 1690/997) Scalar-matrix multiply (adapted IOOX800matrix, \nmulti-1.6 from lJiHK96]) JIiedbyall scalars1..100 (16000/10000) Sparse matrix-vector multiply !OOx21Xlmatrix, \n10 ele-1.8 nent.show, 570 density (76200/43200) )6x96 matrix, 5 elements/ 1.5 OW,5% density (13200/8840) \n Event dispatcher in an extensible f predicate types; 10 dif-1.4 OS [BSti95,CEA+96] erent event guards \n(4606/3228) QuickSort record sorter (extended I keys, each of a different 1.2 from [KEH93]) YPe (1 140/960) \n2 keys, each of a differ-1.2 :nt type (1310/1060) run-time constants are accessed indirectly. To avoid \nthese problems, the stitcher constructs a second linearized, compressed array to hold the large and non-integer \nconstants. Since loads from the linearized table are fast, requiring only a dedicated base pointer register \nand a fixed index for each reference, the stitcher fills holes for large and non-integer constants with \nreferences to the linearized table, The structured table is deallocated after stitching completes. The \nstitcher performs simple peephole optimization that exploit the actual values of the constant operands. \nFor example, integer multiplications by constants are rewritten as shifts, adds, and subtracts, and unsigned \ndivisions and modulus s by powers of two become shifts and bit-wise sod s, respectively. The final code \ngenerated by the stitcher for the cache lookup example, invoked for a cache configuration of 512 lines, \n32-byte blocks, and 4-way set associativity, is the following (where cacheLines is an address loaded \nfrom the linearized run-time constants table): uns&#38;ned tag . (unsigned) addr >> 14; unsigned line \n. ( (Unsigned) addr >> 5) &#38; 511; setStructure **setArray . cacheLines[line] ->sets; if (setArray[O] \n->tag == tag) goto Ll; if (setArray[l] ->tag == tag) goto Ll; if (setArray[2]->tag == tag) goto Ll; if \n(setArray[3]->tag .= tag) goto Ll; return CacheMiss; Ll: return CacheHit; In our design, the static compiler \nis separated from the stitcher through a simple interface language comprised of directives and the run-time \nconstants table. Analtemative wotrldbe to fold together the set-up code with the stitcher, with the set-up \ncode directly invoking stitcher primitives at appropriate places or even generating instantiated machine \ncode directly without copying templates, as is done in some other systems [CN96,LL96]. This approach \nwould eliminate the need for directives and for the intermediate constants table computed by the current \nset-up code, and consequently would most likely produce significantly quicker dynamic compiles. Our current \napproach is a convenient intermediate point, since it is simple, flexible, and reasonably fast, and it \nside-steps difficult problems of planning out final stitcher activities for arbitrary control flow graphs \nprior to optimization. 155 Dynaxnic Cycles/Instruction Compilation Stitched Breakeven Point Overhead: \n(number of set-up &#38; stitcher instructions (1000s cycles) stitched) . 916 interpretations with 452 \n734 different x, y values 183 (865) 31392 individual muki-260 4032 placations 34.3 (73) 2645 matrix 83,700 \n7390 multiplications 3,580 (11810) 1858 matix 7,070 2478 multiplications 1,030 (3269) 722 event dispatches \n638 597 357 (1667) 3050 records 444 8446 105 (65) 4760 records 790 6869 400 (173) 5 Experimental Assessment \nWe embedded our static analysis in the Multiflow optimizing compiler [LFK+93,FRN84] and dynamically compiled \nkernels from several application domains (Table 2). All programs, both the static and dynamic versions, \nwere executed onaDECAlpha21064, Each program wasexectrted with a variety ofrun-time constant configurations; \nwe report results for two configurations for those programs whose speedups were sensitive to multiple \nfactors. For example, execution times for sparse matrix multiplication depend on both the size of the \nmatrix and the number of non-sparse elements per row. Speedups on scalar-matrix multiply, on the other \nhand, are relatively independent of the size of the matrix. Our preliminary results show good asymptotic \nspeedups over statically-compiIed code, but, as yet, high *dynamic compilation overhead, leading to high \nbreakeven points. As mentioned in the previous section, the overhead of dynamic compilation is due to \nour separation of set-up code from the stitcher, leading to extra intermediate data structures and stitcher \ndirective interpreter costs. Merging these components into a single pass should drastically reduce our \ndynamic compilation costs without affecting our asymptotic speedups. In some applications, most of the \ntemplate code corresponds to array loads and stores, which limits speedup. If all references to an array \nare through run-time constant offsets, then some array eIements can be allocated to registers by the \nstitcher. We have begun experimenting with a variation of Wall s register actions used in his link-time \nregister allocator ~a186]: the static compiler produces directives that indicate how to remove or modify \ninstructions if a particular array element is stored in a registe~ the stitcher then executes these directives \nto eliminate loads, stores, and address arithmetic, after choosing registers for some number of array \nelements. We have obtained a speedup of 4.1 (as opposed to the current 1.7) on the calculator program \nusing this technique. * Asymptotic speedups were determined by comparing hardware cycle counter values \nfor statically and dynamically compiled versions of each program s dynamic region. The breakeven point \nis the lowest number of executions of the dynarnicatly-compi[ed code (including the overhead of executing \nset-up and stitcher code) at which the dynamic version is profitable. TabIe 3: Optimization Benchmark \nConstant Static Branch Folding Elimination Calculator 4 4 ! t Scatar-matrix multiply Sparse matrix-vector \nmuttiply J Event dispatcher d d Record sorter d 4 Several optirrrizations, all applied dynamically, \nwere responsible for the asymptotic speedups (Table 3). Although constant folding, load elimination, \nand complete loop unrolling were used most often, each optimization was important for some application. \n6 Related Work 6.1 Partial-Evaluation-Based Dynamic Compilers Most closely related to our work are other \ndynamic compilation systems that incorporate ideas from partial evahration [SZ88,JGS93]. Partial evaluators \nenable a phased compilation strategy, where a program is compiled, given partial knowledge of its input, \nto produce a new faster program that takes the remaining input. Analogously, our static compiler compiles \na dynamic region, given the partial knowledge that some of the variables at entry to the region will \nbe invariant; the output of the static compiler is a subprogram whose compilation is completed by the \nstitcher. Off\u00adline partial evaluators incorporate a binding time analysis that determines which variables \ndepend only on the known inputs, much like our run-time constants identification analysis. Sophisticated \npartial evaluators handle partially-known data structures, as does our compiler. On the other hand, partial \nevaluators are usually source-to-source transformers for purely functional languages, whose analyses \nare expressed as abstract interpretations over the abstract syntax of the program; our system is an intermediate \nrepresentation-to-optimized machine-code translator for general\u00adpurpose imperative languages, whose analyses \noperate over 10w\u00adlevel control flow graphs. Partial evaluators operate interprocedurally (but over relatively \nsmall programs), often handle higher-order functions, and can produce multiple, specialized versions \nof procedures to maximize the flow of known inforination. Our compiler currently is only intraprocedural, \nbut it can produce multiple compiled versions of a single dynamic region. Both Leone and Lee [LL96] and \nConsel and Noel [CN96] use a partial evaluation-based framework to build dynamic compilers. Leone and \nLee s system, called Fabius, applies dynamic compilation to a first-order, purely-functional subset of \nML. The programmer uses explicit currying to indicate where dynamic compilation is to be applied. As \neach argument to a turned function is supplied, a new function that takes the remaining ar=~ments is \ndynamically compiled, specialized to the run-time value of the first argument. An intraprocedural binding \ntime analysis on the original function body identifies the calculations that depend only on the early \nargument values. The dynamic compilation step is fast, because the statically-generated code for a function \ncontains the calculations that are based only on the first argument, interspersed with emit pseudo-instructions \nthat generate the remaining code. However, the dynamically-generated code is not optimized across instructions. \n(Leone and Lee suggest extending their run-time code generator to perform register assignment at dynamic \ncompile-time; however, this will slow dynamic compilation.) In contrast, our compiler targets a more \ngeneral programming model and strives for both fast dynamic compilation and fast dynamic execution. Finally, \nFabius is safe, n that the compileroptirnizations do not affect program correctness; however, Fabius \nachieves safety by AppIied Dynamically Load Dead Code Complete Loop Strength Elimination Elimination \nUnrolling Reduction id d 44 dJ dJ 4 d4 d disallowing side-effects, The correctness of our transformations \ndepends on the correctness of the programmer annotations. Consel and Noel s system, developed concurrently \nwith ours, follows a very similar structure. It too is a compiler for C programs that produces machine-code \ntemplates with holes that are instantiated at rim-time. Their system pre-plans run-time constant propagation \nand folding, dead branch elimination, and loop unrolling, like ours. Some key differences in our approaches \nare the following: * Their system follows more closely the traditional partial evaluation approach. Programmers \n&#38;notate arguments of the top-level procedure to be dynamically compiled, global variables, and components \nof data structures as run-time constant. Their binding time analysis then interprocedurally identifies \nderived run-time constants. Our annotations currently apply only intraprocedurally, but our annotations \noffer more flexibility in treating a variable or data structure as constant in one context but variable \nin another. 4 They do not describe their binding time analysis, other than to show that its output annotates \nsyntax trees, and their remaining transformations are expressed as tree transformations. They do not \nanalyze reachability conditions for constant branches. This suggests that they would have difficulty \ncoping with the unstructured C programs that we handle. . To produce machine-code templates, they generate \nC code containing special marker code sequences, compile it with a regular C compiler, and then post-process \nthe assembly code to rediscover the markers and identi~ templates and holes. The post-processing tool \nis specific to a particular target machine and compiler, and relies on the compiler s optirnizations \nnot interfering with the marker structure. Our approach directly modifies an optimizing compiler to avoid \nsuch limitations, at some cost in implementation effort. . They do not perform peephole optirnizations \nat dynamic compile time, nor do they maintain a table of large constants for faster run-time access. \n. In their system, the programmer is responsible for managing the code pointers that are returned from \ninvoking a dynamically-compiled function. Our system takes care of this automatically, including managing \na keyed collection of code pointers for different invocation contexts. . To handle C pointers and support \npartially-constant data structures, they include an automatic pointer/alias analysis (which currently \nis not sound in the presence of C casts and pointer arithmetic), while we rely on programmer annotations. \nAlthough more susceptible to programmer errors, annotations can identify constants that are beyond the \nability of current alias analyses. In addition, they do not do alias analysis of callers of the dynamically-compiled \nfunction, so they cannot automatically identify which formal parameters and global variables really are \nconstant. They rely on the programmer to use the generated code pointer appropriately, analogous to our \nreliance on the comectness bf the programmer assertions. 156 Guenter, Knoblock, and Ruf have developed \na specialized compiler that applies partial evaluation-like techniques to a graphics rendering application \n[GKR95,KR96]. While not producing machine code at run-time, their system does analyze the rendering procedures \nto produce multiple, specialized versions for different combinations of constant arewments, and dynamically \ncomputes and caches the results of constant calculations in a data structure much like our run-time constants \ntable. They observed speedups of up to 100 for their particular application. 6.2 Other General-Purpose \nDynamic Compilers Keppel, Eggers, and Henry [KEH93,Kep96] developed a library for manually constructing \nexpression trees and then compiling them into callable machine code from within a program, in a portable \nfashion, They also developed a template-based approach. Their experiments demonstrated that these techniques \noutperformed the best statically-compiled, hand-tuned code in several applications. In a similar vein, \nEngler and Proebsting developed DCG [EF94], a library for constructing and manipulating expression trees \nthat exploits the IBURG portable code generator library [Pro92]. The code generator infrastructure performed \nno optimizations other than instruction selection. Engler, Hsieh, and Kaashoek developed C &#38;HK96], \nan extension of the C language that makes constructing and manipulating expression trees look syntactically \nlike fra=ments of C code, greatly easing the programming burden. DCG is used as the back-end infrastructure, \nMore recently, PoIetto, Engler, and Kaashoek have retargeted C to use a template-based back-end [PEK96]. \nCompared to our approach, these manual approaches offer more flexibility of optimization (since the programmer \nis responsible for performing all global optimizations by hand), but at the cost of longer dynamic compilation \ntimes (with the exception of template\u00adbased C) and more tedious and error-prone programming work. 6.3 \nOther Dynamic Compilation Systems A number of previous systems have exploited dynamic compilation for \nrun-time performance or flexibility gains, for example, in graphics displaying [PLR85], operating system \noperations [PAAB+95,PM188], object-oriented [DS84,CU89,HU94]. Ho~e~er, these systems did no~y%~~ dynamic \ncompilation available to the programmer in more general scenarios. Conclusions We have designed and \nbuilt a dynamic compilation framework for general-purpose imperative languages like C whose twin goals \nare high-quaIity dynamically-compiled code and low run-time compilation overhead. Several factors contribute \nto the quality of the dynamically-compiled code optimizing dynamic regions within the context of their \nenclosing procedure, planning out optitnizations that depend on run-time constants (including the capability \nto analyze unstructured control flow), segregating the set-up code that applies these optimizations at \nrun-time from the repeatedly-executed templates, and embeddhg this entire analysis within an optimizing \nstatic compiler. Dynamic compilation overhead is reduced by presenting the dynamic compiler with almost \ncompletely constructed machine code. Initial speedups over a set of statically-compiled C programs range \nfrom 1.2 to 1.8. We plan to extend our framework in several dimensions: to provide run-time constants \nand reachability analyses on the interprocedural level, to more fully automate the selection of run-time \nconstants and dynamic regions, to merge set-up code with stitching for faster dynamic compilation, to \nprovide dynamic compilation support for other input languages, and to extend our benchmark suite to other \napplication areas and larger programs. Acknowledgments We would like to thank John O Donnell (Equator \nTechnologies, Inc.) and Tryggve Fossum (Digital Equipment Corp.) for the source for the Alpha AXP version \nof the Multiflow compile~ Ben Cutler (Equator Technologies, Inc.), Michael Adler, and Geoff Lowney (Digital \nEquipment Corp.) for technical advice in altering it; Marc Friedman for work on the design and implementation \nof an early version of our framework; Charles Consel, Wilson Hsieh, Jack Lo, and the PLDI referees for \ntheir helpful comments on the submitted version of the pape~ Jeffrey Dean and David Grove for discussions \non the run-time constant identification and reachability analyses; and David Keppel, Markus Mock, Scott \nvan Woudenberg, and in particular Brian Grant for their help with the applications. The research was \nsupported by ARPA contract NOOO14-94-1-1136, NSF PYI Award MIP-9058439, NSF NYI Award CCR94-57767, and \nONR grant NOO014-92-J-1395. References [ASU86] A.V. Aho, R. Sethi, and J.D. Unman. Compilers: Principles, \nTechniques, and Tools. Addison-Wesley, 1986. [AWZ88] B. Alpem, M.N. Wegman, and F.K. Zadeck. Detecting \nequality of variables in programs. In Symposium on Principles of Pro\u00adgramming Languages, January 1988. \n~SP+95] B.N. Bershad, S. Savage, P. Pardyak, E.G. Sirer, M. Fiuczynski, D. Becker, S. Eggers, and C. \nChambers. Extensibility, safety and performance in the SPIN operating system. [n Symposium on Op\u00aderating \nSystems Principles, November 1995. [CC95] C. Click and K.D. Cooper. Combining anafyses, combining opti\u00adrnizations. \nACM Transactions on Programming Lunguages and Systems, 17(2), March 1995. [CEA+96] C. Chambers, S.J.Eggers, \nJ. Auslander, M. Philipose, M. Mock, and P. Pardyak. Automatic dynamic compilation support for event \ndispatching in extensible systems. fn Workshop on Compil\u00ader Support for Systems So$ware, February 1996. \n[CFR+89] R. Cyrron, J. Fermrrte, B.K. Rosen, M.N. Wegman, and F.K. Zadeck. An efficient method of computing \nstatic single assign\u00adment form. In Symposium on Principles of Progrming rhn\u00adguages, January 1989. [CN96] \nC. Consel and F. Noi41.A generaJapproach for run-time special\u00adization and its application to C. In Symposium \non Principles of Programming Lvrguages, Januasy 1996. [CU89] C. Chambers asrdD. Ungar. Customization: \nOptimizing compiler technology for SeIf, a dynamically-typed object-oriented pro\u00adgramming language. In \nConference on Programming Lrznguage Design and Implementation, JuIy 1989. ~S84] L. Peter Deutsch and \nAlIan M. Schiffman. Efficient implementa\u00adtion of the Smafltatk-80 system. In Symposium on Pn nciples \nof Programming Languages, January 1984. ~HK96] D.R. Engler, W.C. Hsieh, and M.F. Kaashoek. C: A language \nfor high-level, efficient, and machine-independent dynamic code generation. in Symposium on Principles \nof Programming .!xzn\u00adguages, January 1996. @W94] D.R. Engler and T.A. Proebsting. DCG: An efficient, \nretargetable dynamic code generation system. In International Conference on Architectural Support for \nProgramming Languages and Operat\u00ading Systems, October 1994. ~N84] J.A. Fisher, J.C. Rrrttenberg, and \nA. Nlcolau. Prmdlel processing: A smart compiler and a dumb machine. In Symposium on Com\u00adpiler Construction, \n1984. [GKR95] B. Guenter, T.B. KnobIock, and E. Ruf. Specinfizing shaders. In S[GGRAPH 95, 1995. IJIU94] \nU. Holzle and D. Ungrrr. Optimizing dynanricafly-dispatched calls with run-time type feedback. In Conference \non Program\u00adming Lrnguage Design and lmplementarion, June 1994. [JGS93] N, Jww., C, Gornard, and P, S.srofs. \n~amai Evahartrm and Au\u00adtomatic Progrrsm Generation. Prentice Hall, 1993. [KEH93] D. Keppel, S.J. Eggers, \nand R.R. Henry. Evrduating mntime\u00adcompiled, vafrre-specific optirnizarions. Technicrd Report 93-11\u00ad02, \nUniversity of Washington, Department of Computer Science &#38; Engineering, 1993. 157 [Kep96] D. Keppel. \nRuntime code generation. Technical report, Universi\u00adty of Washington, Department of Computer Science&#38;Engineer\u00ading, \n1996. [KR96] T.B. Knoblock and E. Ruf. Data specialization. In Conference on Programming Language Design \nand Implementation, May 1996. ~FK+93] P.G. Lowney, S.M. Freudenberger, T.J. Karzes, W.D. Lichten\u00adstein, \nR.P. Nix, J.S. O Donnell, ~d J.C. Ruttenberg. The Multi\u00adflow trace scheduling compifer. Journal of Supercomputing, \n7, 1993. &#38;L96] M. Leone and P. Lee. Optimizing ML with run-time code gener\u00adation. In Conference on \nProgramming Language Design and Im\u00adplementation, May 1996. [PEK96] M. PoIetto, D.R. Engler, and M.F. \nKaashoek. tee: a template\u00adbased compiler for C. In WorLrhop on Compiler Support for Syx\u00adterrrs Software, \nFebruary 1996. [Per90] A.J. Perfis. Epigrams on programming. In Communications of rhe ACM, 1990. [PLR85] \nR. PAe, B.N. Locanthi, and J.F. Reiser. Hardware/softwam trade-offs for bitmap graphics on the Blit. \nSojlware -Practice and Experience, 15(2), 1985. [PM188] C. Pu, H. Massalin, and J. Ioarmidis. The Synthesis \nkernel. Com\u00adputing Systems, (l), winter 1988. [Pro92] T.A. Proebsting. Simple and efficient BURS table \ngeneration. fn Conference on Programming Language Design and Implementa\u00adtion, July 1992. [SZ88] P. Sestoft \nand A.V. Zatmdin. Annotated Bibliography on Partial Evaluation and Mixed Computation. North. Holhmd, \n1988. wrd86] D.W. Wafl. Global register allocation at link time. In Symposium on Compiler Construction, \nJune 1986. 158 Appendix A Specifications of Analyses We use a kind of abstract interpretation-style, \nlattice-theoretic specification for our dataflow analyses.  A.1 Run-Time Constant Identification Analysis \nThe domains and operations for instructions, variables, and constant variables: Ins t = set of straighr-line \ninstructions in dynamic region BranchIns t = set of branch instructions (both if and switch branches) \nin region POint = set of program points &#38; ow arcs between instructions) in dynamic region successors: \nInst + BranchInst + 2p0int successors(i) = set ofpoints afier i Var = se! of variables in dynamic region \nConstants = 2var; an element of this domain is the set of variables known to be constant at aprogrampoint; \ng is the lattice < ordering operator for this domain C denotes the solution to the run-time constant \nanalysis: C: Point+ Constants ClP) = greatestjlredpoint solution (largest set of run-time constants) \nto following htaflow equations at point p The interpretation of our analysis is Vpe point, if v e ~p), \nthen v is defined to the same value each time it is executed at ran-time, assuming the programmer annotations \nare correct. llte initial set of constants at the start of the dynamic region: dPO) = setof variables \nlabeled as constants by programmer annotation at start of region ~w is the flow function for straight-line \ninstructions, computing t? e set of constants at the point after an instruction from the set of constants \nat the point before the instruction: Inst + Constants -) Constants [~ , = k] =5 = csu (X}, where k is \na compile-time constant [x :=yopz]cs= csu (x }, if (y, zIscs and OPisidempotewandside-efeet-free. CS-( \nx }, orherwi$e Ux := f(Yl, . . .,yn)l Cs = csu(x}, if(yl, . . . ,yn)Gc.Sa~ f is idempotent and sade-effect-free, \nCS-(x }, otherwise [x := y] Cs= csu{x), if {y}Gcs, cs-( x }, otherwise [x := dynamic y] CS = CS-(X} \nK x := yn Cs= Cs [x := $(X1, . . ..xn)n Cs= cs-{x~, . . . , Xn}u(x}, if (xl, . . . ,xn}sc.s. CS-{X, X1, \n. . . , ~}, otherwise %,.. !ch k the flow function for branch nodes. Given a set of .. ----\u00adconstants \nbefore the branch, it computes a mapping from successor points to the constant sets at those points, \ni.e., branches have no effect on the set of computed constants: Cb,@~h: BranchInst + Constants * (Point \n+ Constants) = {(s, cs) Is c successors(b)} Cbramh b Cs The lattice meet function computes the set of \nconstants after a merge (this depends on the solution ~to the reachability analysis at the merge): Constants \nx Constants 4 Constants ~onsml.s: I Icon=tmt, (c% f,c52) : CS1 u CS2, zfexcluswe(cnl, cnz), where cnl \n= ~p,) for merge predecessor pi, Csl n CS2, otherwise (The meet function and the $ functions should \nbe coordinated. The $ functions should be part of the merge itself rather than located after the merge, \nso that none of the values defined only along a subset of the predecessor branches survive to the point \nafter the merge.)  A.2 Reachability Conditions Analysis Thefollowing additional domain supports the \nreachability analysis. We define the values of this domain using a grammar rule (which we assume is always \nsimplified to conjunctive normal form): condition : z= B+S, where Be BranchIns t and SE Successors I \nCondition A Condition I Condition v Condition I true Two conditions can be mutually exclusive: exclusive \n(cnl, cn2) = (cnl=~cn2) A (cn~B~S:cnl), where a(B~Sl) = Vsesucce~Om(B), s#sl . This rule implies that \nB+SI and B+S2 are mutually exclusive iff s~# s~. Reverse logical implication is the lattice S ordering \noperator for this domain, i.e., if cnl =$ cn2, then cn2 S cnl. B+sl and B+S2 are mutually exclusive if \nS1 # S2. ~denotes the solution to the reachability analysis equations: ~ point+ condition ti) = greateslfiedpoint \nsolution (most constrained set of branch outcomes) to following dataflow equations at point p The interpretation \nof our analysis is Vpe Point, @p) = ~.tua~(P), where fUUi(P) represents the ac~~ branch outcomes at inn-time: \n~tiI(P) = ABe BranchInst (vsetaken-successors(B) (B+$)), where taken-successors(B) are those successors \nof B that are taken at run-time. The initial set of reachability conditions at start of dynamic region: \n~ PO = Tc..az~ion = tme ~OW is the flow function for straight-line instructions, which have no effect \non the reachability conditions: ~w: Inst + Condition + Condition ~wicn=cn ~rmch is the flow function \nfor branch nodes. Branches with run-, time constant predicates restrict the conditions along successor \nbranches, while non-constant branches have no effect on the reachability analysis: ~~ch: BranchInst + \ncondition -) (Point + Condition) %rmch bcn = { (s, cn A &#38;s ) Is e successors(b)}, ifb = [q? ] and \nq e C(before(b)) { (s, cn) Is e successors(b)}, otherwise The meet function, computing reachability conditions \nafter a merge, is simple disjunction: ~each: Condition Condition+ Condition ~each (cnl, cn, ) = crq V \ncn2 159  \n\t\t\t", "proc_id": "231379", "abstract": "Dynamic compilation enables optimization based on the values of invariant data computed at run-time. Using the values of these run-time constants, a dynamic compiler can eliminate their memory loads, perform constant propagation and folding, remove branches they determine, and fully unroll loops they bound. However, the performance benefits of the more efficient, dynamically-compiled code are offset by the run-time cost of the dynamic compile. Our approach to dynamic compilation strives for both fast dynamic compilation <i>and</i> high-quality dynamically-compiled code: the programmer annotates regions of the programs that should be compiled dynamically; a static, optimizing compiler automatically produces pre-optimized machine-code templates, using a pair of dataflow analyses that identify which variables will be constant at run-time; and a simple, dynamic compiler copies the templates, patching in the computed values of the run-time constants, to produce optimized, executable code. Our work targets general- purpose, imperative programming languages, initially C. Initial experiments applying dynamic compilation to C programs have produced speedups ranging from 1.2 to 1.8.", "authors": [{"name": "Joel Auslander", "author_profile_id": "81418594366", "affiliation": "Department of Computer Science and Engineering, University of Washington", "person_id": "P142159", "email_address": "", "orcid_id": ""}, {"name": "Matthai Philipose", "author_profile_id": "81100041707", "affiliation": "Department of Computer Science and Engineering, University of Washington", "person_id": "PP14026343", "email_address": "", "orcid_id": ""}, {"name": "Craig Chambers", "author_profile_id": "81100528252", "affiliation": "Department of Computer Science and Engineering, University of Washington", "person_id": "PP39047060", "email_address": "", "orcid_id": ""}, {"name": "Susan J. Eggers", "author_profile_id": "81100262930", "affiliation": "Department of Computer Science and Engineering, University of Washington", "person_id": "PP15027685", "email_address": "", "orcid_id": ""}, {"name": "Brian N. Bershad", "author_profile_id": "81100572122", "affiliation": "Department of Computer Science and Engineering, University of Washington", "person_id": "P32458", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/231379.231409", "year": "1996", "article_id": "231409", "conference": "PLDI", "title": "Fast, effective dynamic compilation", "url": "http://dl.acm.org/citation.cfm?id=231409"}