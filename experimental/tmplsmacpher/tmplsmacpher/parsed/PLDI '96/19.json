{"article_publication_date": "05-01-1996", "fulltext": "\n Realistic Compilation By Partial Evaluation Michael Sperber PeterThiemann Wilhelm-Schickard-Institut \nfifr Informatik Universit22t Tiibingen Sand 13, D-72076 Ttibingen, Germany {sperber ,thi.emar.m}~inf \normatik.uni-tuebingen .de Abstract IWO key steps in the compilation of strict functional languages are \nthe conversion of higher-order functions to data structures (clo- WVS) and the transformation to tail-recursive \nstyle. We show how to perform both steps at once by applying first-order offline partial evaluation to \na suitable interpreter. The resulting code is easy to transliterate to low-level C or native code. We \nhave implemented the compilation to C; it yields a performance comparable to that of other modern Scheme-to-C \ncompilers. In addition, we have inte\u00adgrated various optimizat.ions such as constant propagation, higher\u00adorder \nremoval, and arity raising simply by modifying the under\u00adlying interpreter. Purely first-order methods \nsuffice to achieve the transformations. Our approach is an instance of semantics-directed compiler generation. \n Keywords semantics-directed compiler generation, partial evalu\u00adation, compilation of higher-order functional \nlanguages Partial evaluation is an automatic program transformation that performs aggressive constant \npropagation [28, 18]. when applied to an interpreter with respect to a constant ( static ) input pro\u00ad \ngram for the interpreter, partial evaluation performs compilation into the target language of the partial \nevaluator. Naive interpreters subjected to oflline partial evaluation usually produce straightfor\u00adward \ncompiled code. Moreover, if the input language of the inter\u00ad preter, and the input and output languages \nof the partial evaluator are identical-that is, if it is a self-interpreter-the compilation is essentially \nthe identity function. However, if the interpreter uses only a subset of the subject lan\u00adguage, so do \nthe compiled programs. In addition, after changing the interpreter to propagate more information statically, \nthe produced compiler performs optimization. The generation of optimizing pro\u00adgram transformers by partial \nevaluation is called the interprz%ive approach [ 24]. It has been applied to a wide range of problems: \nto the generation of optimizing specidzers, supercompilers, and deforesters [24, 25]-albeit in the context \nof first-order languages. We show that the interpretive approach can achieve optimizing compilation \nof a strict, higher-order functional language. Our com\u00adpilation system consists of two parts: an optimizing \ntransformer, which translates higher-order recursion equations into first-order tail-recursive Scheme \nprograms, generated automatically from a suitable interpreter by partial evaluation, and a simple, hand-written \n Permissiontoreeksdigitslkrd copyofpartorailorthisworkforpersonal orclsasreernuseis rantedwithoutfeeprovidedthatcopiesarenotmade \ni or disbibuted for pro t or commercial sdvanta e, the copyright notice, the title of the publication \nand its date appear, ani notice is given that copying is by permission of ACM, Inc. To copy othenvise, \nto republish, to poston servers,orto redistributeto lists,requirespriorspecificpermission sndhr a fee. \nPLDI 96596 PA, USA CJ 1936 ACM 0-89791-795-2/96/0005...$3.50 206 tmnslator from first-order tail-recursive \nScheme into low-level C. It is also possible to generate the back-end translator by the same method from \nan interpreter, using a partial evaluator for C [2]. Hence, partial evaluation offers the development \nof a Scheme com\u00adpiler for the price of writing two interpreters. The automatic con\u00adversion to tail form \nis also the first solution to Jones s 1987 chal\u00adlenge 11.5 [26]. The optimizing compiler performs aggressive \nconstant propa\u00adgation and higher-order removti, it is a specialize in its own right. For its generation, \nwe exploit two principles: the speciczlizer pr\u00ad ojections for the generation of the transformer, and \nthe language preservation property of offline partial evaluators for the transla\u00adtion of higher-order \nprograms into first-order tail-recursive code. We have generated the transfomner from an interpreter \nusing the partial evaluator Unmix. Unmix, a descendant of the Moscow specialize [36], dating back to \n1990, treats only a first-order sub\u00ad set of Scheme, and does not handle partially static data structures. \nSince our transformer performs a much more powerful specializa\u00ad tion on higher-order Scheme, and does \nhandle partially static data structures, we have achieved a bootstrapping effect. Our work also refutes \nthe 1991 claim of Consel and Danvy [17] that realistic compiler generation by partial evaluation is only \npos\u00ad sible through recent advances in partial evaluation technology. We show that neither higher-order \nspecialization nor partially static data structures are vital to achieve realistic compilation. A sim\u00ad \nple first-order partial evaluator suffices to do the job, even for a higher-order subject language. Overview \nWe start with a small example for specialization and translation into tail-recursive code in Sec. 1. \nSection 2 is a brief in\u00adtroduction to partial evaluation. In Sec. 3 we explain the two basic principles \nneeded to generate stand-alone compilers by partial eval\u00aduation: the specialize projections and the languuge \npreserv~ion property. Section 4 shows how to turn a simple-minded recursive\u00addescent interpreter into \na two-level interpret from which the par\u00adtial evaluator produces the optimizing compiler. In Sec. 5, \nwe de\u00adscribe OUTapproach to generating C code from the compiler output by hand, followed by a recipe \non how to achieve the same effect by using partial evaluation again. Section 6 presents experimental \nresults. and Sec. 7 discusses related work. 1 Examples We illustrate the transformations that our compiler \nperforms by ap\u00adplying them to a version of append written in continuation-passing style (CPS): (clef \nine (append x y) (cps-appemd x y (lambda (x) x))) (define (cps-append x y c) (if (null? x) (c y) (cps-append \n(cdr X) y (l@da (xy) (C (cons (c= x) xy)))))) The compiler converts the program to first-order tail-recursive \nScheme. It residualizes the lambda appearing in the program, and represents the resulting functions by \nclosures. Closures consist of a closure label identifying its originating expression, and the values \nof their free variables. They are constructed by make-clo sure and accessed by closure-label and closure-f \nreeval The closure label 10 denotes the identity, 24 the inner continuation. Whenever the program applies \na closure, it dispatches on the closure-label component. This happens for both applications of the continuation \nc, once in sl-evsd.-$3 for (c y) and once in sl-eval-$9 for the other application. Note that the identifier \nnames in the residual program have been replaced by generic names from the interpreter. Namely, the counterparts \nto the original identifiers now have the form cv-vals-cm. (clef in. (append x y) (sl-eval-$3 (make-closure \n10) y x)) (define (sl-.val-$3 cv-vd.-$l cv-vals-$2 cv-vale-$3) (if (null? cv-vals-$3) (if (equal? 10 \n(closure-label CV-VelS-$l) ) cv-vsls-$2 (do-closure-cv-bindings-$2 cv-vals-$2 cv-vals-$1) ) (sl-eval-$3 \n(meke-clOs.re 24 cv-val.-$l cv-val.-$3) cv-vals-$2 (c&#38; cv-vale-$3) ) ) ) (defina (do-cloeure-cv-bindings-$2 \nfirst-val closure) (sl-eval-$9 (closum-freeval closure 1) first -val (closure-freeval closure O))) (define \n(sl-eval-$9 cv-v.l.-$l CV-ValS-$2 CV-VS.IS-$3) (if (equal? 10 (closure-label cv-val.-$3) ) (cons (car \ncv-vals-$1) cv-vals-$2) (do-closum.-cv-bindings-$2 (cons (car cv-vals-$1) cv-vals-$2) CV-VSIS-$3) ) \n) When given a known first argument (foo bax), the compikt performs specialization (clef in. (append-$1 \ny) (cone foo (cons bar y))) The next step in the compilation is the translation to C. We have omitted \nactual output. Section 5 describes our C back end. Partial Evaluation issues Partial evaluation is a \nspecialization technique: If parts of the input of a subject program are known at compile time, a partial \nevaluator generates a nmidurd program specialized with respect to the static input The residual program \nonly takes the remaining, dynamic p.rt. of tie inputesparameters, nnd produces the same results \u00ad the \nsubject program applied to the full input, Partiai evaluation can remove interpretive overhead and produce \nsignificant speed\u00ad UPS[28]. An ofline partial evaluator consists of a binding-time analysis and a reducer. \nThe binding-time analysis, applied to the subject program and the binding times of its arguments, annotates \neach expression in the program with a binding time, static or dynamic. The reducer processes the annotated \nprogram and the static part of the iupu~ reducing static expressions and rebuilding dynamic ones, driven \nby the annotations. Whereas simple-minded binding\u00adtime analyses only handle the binding times completely \nstatic and completely dynamic; more sophisticated variants also treat partially static daza [33, 32,9, \n16]. In contns~ online partial evaluators [48, 38] are one-pass pro\u00adgrams which decide online whether \nto reduce or rebuild an ex\u00adpression. They are generally more powerful than their offline coun\u00adterparts \nbecause they exploit information about actual values-rath\u00ader than only their binding times-to decide \nwhether to reduce or rebuild. For our experiments, we use Unmix, a simple offline partial evaluator for \na first-order, purely functional subset of Scheme. Un\u00admix employs classic Mix technology [29], and does \nnot handle par\u00adtially static data. However, its post-processor performs arity rais\u00ading [37] which is \ncrucial to the generation of efficient residual pro\u00adgrams in the absence of partially static data. 3 \nPrerequisites for Compiler Generation The interpreters described here exploit two basic principles: The \nspecialize projections specify how to generate specializes from interpreters, and the language-pwserv~ \non property of offline par\u00adtial evaluation is the basis for higher-order removal and conversion to tail \nform. The Specialize Projections Partial evaluation of interpreters can perform compilation. The specification \nof au S-interpreter int written in L is ~int~~ PS inp = [Ps]. inp where ~-] L is the execution of an \nL-program, PS is an S-program, and inp is its input. An L+ L-partial evaluator pe can compile PS into \nan equivalent L-program PL such that [PL]Linp = [Ps]s inp as described by the first Futamura projection \n[21]: PL = be]. int d Ps. The sd superscript to int means that the partial evrduator is to treat the \nfirst argument of int as static, the second as dynamic. Exploiting repeated self-application, the second \nand third Futa\u00ad mnra projections describe the generation of compilers and compiler generators [28]. \nA generalization of the Futamuraprojections shows how to gen\u00ad erate specializes, or constant-propagating \noptimizers from a two\u00ad level interpreter [24, 2S] 2int which accepts the input to the in\u00ad terpreted program \nin two parts: one static and one dynamic. The interpreter tries to perform each operation with the static \npart of the input tirsc only if that fails, the dynamic part is consulted. Residual programs result from \nthe first specicdizerpmjection [23, 25]: RL = ~e] 2int d Ps inpa where inps is the static part of the \ninput and RL is tie sp~i~\u00ad ized program. Analogous to the Futamura projections, stand-alone specializes \nand speciaiizer generators result result from the second and third specialize projection. We will introduce \nan ordinary one-level interpreter and then show how to extend it to two levels. 207 int compiler Futarnura \nProjections I El+m I 2int specialize SpecislizerProjections I Figure 1: Generation of compilers and \nspecializes The Language Preservation Property Mix-style offlinepar\u00adtial evaluators have the language \npwservalion properly which is obvious from inspecting their specialization phase [28]. For any sublanguage \nL ~ L which includes all L \u00adcomputable values as literals, and for any binding-time\u00adannotated L-program \nP every dynamic expression of which belongs to L , be] P z c L holds for arbitrary static z. Specialization \nof an interpreter can translate higher-order to first-order programs: Suppose pe is a partial evaluator \nfor a sub\u00adset C of Scheme. The first-order language F in which tie inter\u00adpreter is written has F ~ C. \nFinally, the interpreter itself executes programs in the higher-order Scheme subset H. See Fig. 1 for \nan illustration. Because pe preserves the F-ness of the subject pro\u00adgram, the residual programs PF = \n[Pf)] htsd f H = [cOrnpiler]PH the compiled program-and RF = be] 2int $d PH inp = [specializer]P~inp \n the specialized program-are F-programs. 4 Deriving the Interpreter We start from a straightiorward, \nenvironment-based interpreter and transform it step by step By subjecting the interpreter to closure \nconversion, the gen\u00aderated transformer performs closure conversion as well. Converting the interpreter \nto tail form leads to a transformer into tail form. Next adding constant propagation in static data turns \nthe transformer into an optimizer. Finally, we introduce a generalization strategy h ensure ter\u00admination. \nA Straightforward Interpreter Figure 2 defines the syntax of the purely functional Scheme sub\u00adset treated \nby our interpreters. For the sake of simplicity, we have restricted it to lambda abstractions of one \nargument. Figure 3 shows a standard interpreter for the Scheme subset. The meta-language is a call-by-value \nlambda calculus enriched with constants, sums, and products. T -i El I Ez denotes the Mc-Carthy conditional. \nThe notation ValUe* + Value k a shorthand for the sum of () + Value, Value + Value, Value x Value + Value \netc. We have omitted the injections and case analysis for elements of Value. We assume that each expression \nis uniquely la\u00adbeled by an t E Label. Where necessary,we indicate the label by a superscript. # serves \nfor both function and label lookup. V G Variable P c ProcName O e Operators K G Constants E G Expression \nD G Definition II C Program E ::= VI Ifl(if EEE)l(OE*)l(PE*)[ (let ((V E)) E) I (lambda (V) E) I (E E) \nD ::= (define (P V*) E) II ~:= D+ Figure 2: Syntax t G Label Value = BaseValue -+ (Value+ Value) p G \nEnv = Variable+ Value @ G ProcEnv = (ProcName + Label) + Expression K[.] : Constants+ Value q.] : Operators \n+ Value + Value &#38;[-] : Expression + Env + Value &#38;[v]p = p[v] &#38;[K]p = K[K &#38;[(if EI E, \nIk)]p = &#38;[E1 p + &#38;[E2]p [ &#38;[E@ 1 &#38;[(o El . . . En)]p = Cqp](qpl]p, . . . . q%lk) &#38;[(P \nEl . . . EJ]p = &#38;[@(P)][~ t+ &#38;[Einp] &#38;[(let ((V E,)) E,)]p = &#38;[Ez]p[V ~ &#38;[E,lJp] \n&#38;[(lambda (V) E)]p = Ay.qJqp[v 1+ y] &#38;~(EI Ez)]p = (&#38; UE11P)(&#38;UE21P) Figure 3: A standard \ncall-by-value interpreter 4.2 Removing A As the first step towmds true compilation, we apply Reynolds \ns de\u00adfunctionalization [35], and change the representation of functions in the interpreter to C1OSUWNconsisting \nof the label of the origi\u00adnating lambda expression, and the values of its free variables (see Fig. 4). \n(freevars(l) computes the list of the free variables of the expression at t in an axbitrary but tixed \norder.) Consequently, we . now have a first-order interpreter for a higher-order language: Closure = \nLabel x Value* Value = BaseValue + Closure &#38;[(lambdal (V) E)]p = let VI . . . Vn =jiievars(t) in \n(1,pVI ...pVn) &#38;r(EI E2)]/2 = let (e,v,...vn) = &#38;[El]p (lambd;(V) E) = +(1) . . . V. =fteevars(?) \nin &#38;[E][V + &#38;[E2]p, Vl++vl,.. .vnivnlnl I !4 Figure 4 Changes to interpreter after closure conversion \nThe interpreter shown in Fig. 4 does not specialize effectively yet. On closure application, the label \nt is dynamic. Hence, the lambda expression in the program text would normally be dy\u00adnamic as well which \nwould lead to unwanted interpretive overhead in the specialized code. Instead, the actual interpreter \nemploys a binding-time improvement to make the expression argument static again-called The Tlick [28]: \nOn closure application, the inter\u00adpreter loops over all lambda expressions that could have generated \n 208 the closure to be applied, comparing each one with 4 successively. When the interpreter finds the \nlambda belonging tot, it continues interpretation with the now static expression. The interpreter em\u00adploys \na simple equational flow analysis [11] to restrict the set of lambdas which it needs to test. The residual \ncode then performs a sequential dispatch on closure application, 4.3 Converting to Tail Form In the \nnext step we convert the interpreter to tail-recursive s~le, Again, by changing the interpreter, the \ngenerated compiler per\u00adforms the ccmesponding transformation. In a higher-order setting, we would transform \nthe interpreter into CPS [35, 20, 4]. CPS makes control flow explicit by encoding the current evaluation \ncon\u00adtext as a function. As we only have first-order methods at our dis\u00adposal, we encode evaluation contexts \nin the same way as functions: by closures. Thus, we encode the current evaluation context di\u00adrectly as \na function, avoiding sn explicit CPS transformation. ::= SE I(ifSEE E) I(P SE*) [(SE 1?) :E ::= V IK \nI(OSE*) I[lambda (V) E) Figure 5: Desugared syntax In our interpreter, a desugaring phase reduces the \nnumber of dif\u00adferent evaluation contexts to one-the application of a closure. In non-tail psitions we \nonly allow simple expressions which evaluate directly to values-constants, variables, applications of \nprimitives, and lambda abstractions. Figore 5 shows the simplified syntax. In the specification, SE is \nfor simple expressions. The desugar\u00ading phase simply moves the non-tail expressions into parameters to \nlambda abstractions. Thus, the expression (f(g z)) becomes ((lambda(~)(~ ~))(g z)). In addition, the \ndesugarer replaces lets by equivalent applications of lambda abstractions. The tail-recursive interpreter \nis shown in Fig. 6. S evaluates simple expressions. E* evaluates serious expressions. &#38;* has an additional \nargumen~ a context stack which keeps track of pend\u00ading context closures. When E* reaches a simple expression \nSE, it evaluates SE via S, and passes the result to C which processes the stack of pending contexts. \nC applies the closure on top. If the context stack is empty, C s argument is the final result of the \ninter\u00adpretation. S need not be tail-recursive: All calls to S are statically unfold\u00adable, and consequently \nnever perform function calls. Hence, par\u00adtially evaluating the interpreter in Fig. 6 yields tail-recursive \nresid\u00adual programs. 4.4 Propagating Constants Now we turn the transformer into an optimizer to first-order \ntail\u00adrecursive code. We split the environment p into a static and a dy\u00adnamic pm converting the interpreter \ninto a two-level interpreter and making it amenable to the specialize projections. To support partially \nstatic data structures, we change p to associate names with completely static value descriptions instead \nof dynamic values. A value description may represent an arbitrary partially static data ob\u00adjecti desc \n::= quote(K) I cons (desc, desc) I c1os(Y, desc ) I cv(i) A value description can be a completely static \natomic value (quote), a pair of value descriptions (cons), a partially static clo\u00adsure (c1os), or a configuration \nvariable [24, 47, 45] whose value is stored in a separate environment a. @sh(u) yields an unused mnfigumtion \nvariable. Figure 7 shows the two-level interpreter with the following func\u00adtions: S* [.] evaluates a \nsimple expression to a value description. The constructors cons and lambda evaluate to the comespond\u00ading \ndescriptions. For selector and primitive applications, the interpreter first examines if it can reduce \nthem statically-for example, when car is applied to a cons description. If that is not possible, the \ninterpreter generates a new configuration variable and maps it to the dynamic result of the expression. \nTherefore, S* [-j returns a new configuration variable envi\u00adronment along with the value description. \nAgain, all non-tail calls in the definition are statically unfoldable. Note that a simple expression \nis static if all its free variables refer to static value descriptions (those that do not contain cv \ncomponents). For static simple expressions, S* [.] always produces a static value. D[-] evaluates an \narbitrary value description to a value. &#38; [-.] is the main evaluation function. It is analogous to \nthe &#38;*~-] function in the simple tail-recursive interpreter in Fig. 6. The main difference is in \nthe handling of if: The interpreter tries to determine the conditional statically first. Only if that \nfails, it introduces a residual conditional. Our implementation can actually infer a static if more often \nthan the interpreter shown in Fig. 6, for example on null? tests on cons descriptions with dynamic components. \nCo handles context application, analogous to C in Fig. 6. C also needs to distinguish between static \nand dynamic con\u00adtexts. For static contexts, it is trivial to prepare a suitable environment and continue \nevaluation. For dynamic contexts, the interpreted needs to introduce new configuration variables for \ntheir (dynamic) free variables. The interpreter presented here is not yet suitable for successful offline \npartial evaluation. Some standard binding-time improve\u00adments [28] are necessary to ensure that p and \n~ as well as the ex\u00adpression to be evaluated stay static. For instance, the interpreter also performs \nThe Trick on the application of a dynamic context just as the interpreter shown in Fig. 4, 4.5 Addressing \nNon-Termination Woes The two-level interpreter exhibited in the last section is first-order, tail-recursive, \nand performs constant propagation. However, partial evaluation with respect to a static input program \ndoes not terminate for non-tail-recursive input programs: Mix-style partial evaluators such as Unmix \ndo not detect and properly handle static data struc\u00adtures that grow without bounds under dynamic control. \nOur inter\u00adpreter propagates such data in three places: 1. The stack of evaluation contexts may contain \na context that leads to its own repeated evaluation. 2. A closure may contain a closure generated from \nthe same lambda expression as part of the value of one of its free vari\u00adables. 3. Applications of cons \nmay nest.  Exactly these conditions lead to self-embedding data structures which potentially grow infinitely. \nThe critical data structures must be generalized (coerced to dynamic values) which removes their static \nvalue from the view of the partial evaluator. For closures and data structures, generalization is straightforward: \nThe interpreter replaces the offending value descriptions by fresh cv descriptions, and adds the generalized \nvalues to u. To handle dynamic evaluation contexts, we must split the context stack into a static part \nand a 209 Y C Context = Closure* S[.] : SimpleExpression + Env + Value : Expression + Env -+ Context+ \nValue : [-1 : Value+ Context+ Value S[v]p = pv S[K]p = K[K] S[(0 SEI . . . sEn)]p = OIO](SISEI]p, . \n. . . S[SEn]p) S[(lambdal (V) E)]p = let VI . . . Vn =fmvars(t) in (-?,pVI . . . pVn) s [sqp~ = C(S[SE]p)7 \n&#38; [(if SE E, Ez)]p7 = S[SE]p + &#38;*[EI]p7 I &#38;* [E2]p7 &#38;*[(P SEI . . . SEn)]p7 = &#38;*[@(P)][vI \nI+ SISEI]P, . . . . V. * S[SJ%RP17 &#38;*[(SE E)]/q = &#38;*[E]p(SISEljp : 7) CV((4, V1 . . .%) : 7 \n) = let (lambda (V) E) = @(t) VI . . . Vn = jreevam(l) in&#38;*[E][V-v, Vl +vl, . . . , v. +) %]7 Cv[] \n=v Figure 6: Tail-recursive interpreter dynamic part and use the dynamic stack for critical contexts \nthat may caus; non-termination. We have implemented an online strategy and an offline analysis for generalization: \nOnline Generalization Self-embedding data can only grow with\u00adout bounds inside of the branches of dynamic \nconditionals and throngh bodies of dynamic lambdas [10]. Under the on\u00adline strategy [46], our interpreter \ndelays generalization until it encounters a dynamic conditional. In that case, the inter\u00adpreter scans \np and 7 for critical data structures and closures, and generalizes them as described above. Evaluation \ncontin\u00adues using dynamic evaluation mntexts. Oflline Generalization Analysis An alternative approach \nuses a flow analysis [40, 8] to determine statically which lambdas and which cons expressions may lead \nto criticrd data in the interpreter. The corresponding descriptions are generrdized on creation. As for \ncritical evaluation contexts, they are mere\u00adly closures already caught by the analysis. The online strategy \nis less conservative since it generalizes only under dynamic conditionals. It necessarily generalizes \nless and propagates more static information. However, the online approach delays the generalization too \nlong: The interpreter can only detect self-embedding when it has already occurred. Consequently, the \nrespective code is already part of the residual program. Thus, the underlying data structures and loops \nare unrolled at least once be\u00adfore generalization happens, leading to redundant code. This is a well-known \nproblem in online partial evaluation [38]. 5 Compilation to C We describe two ways to achieve compilation \nto the C language. The first one describes a very simple translation implemented by hand. It has been \nimplemented and used to obtain the experimental data presented below. The second one presents a more \nspecula\u00adtive approach which again employs partial evaluation to obtain a C program from our Scheme subset. \nIt has not been camied out in practice. 5.1 By Hand The output language So of the partial evaluation \nprocess is a tail\u00adrecursive first-order subset of Scheme which has a simple translit\u00aderation to C. The \ntranslation of an So program to C yields a single function program. Procedure headers are translated \ninto labels, hence (tail-recursive) function calls turn ont to be gotos. Parameters are passed in a fixed \nnumber of variables local to progr~ but global to all procedures. On entry to a procedure, a new C scope \nis opened which declares the procedure s private parameter variables. Then the relevant global parameter \nvariables are copied into the private variables. Since procedure calls arguments are simple expressions, \nthere are no nested procedure calls in So. Therefore the arguments of a call can be evaluated without \nreferring to the global parameter vari\u00adables. Thus the construction of an argument list is straightforward \ngenerate code to evaluate the simple argument expressions and as\u00adsign the result to the respective global \nparameter variables. Finally, control is transferred to the next procedure by a got o. The translation \nof simple expressions SE is an assignment of its value to anew temporsxy variable. Temporary variables \nare also locrd variables of program, but global to all procedures. Each tem\u00adporary variable is defined \nand used exactly once. We rely on the C compiler s register allocator to merge variables (global param\u00adeter \nvariables, procedure argument variables, and temporaries) if their life ranges are disjoint. The evaluation \nis sequentialized us\u00ading C s sequential evaluation operator (expr, expr). Thus the result of the translation \nis a C expression. All other expressions E are translated into C statements. For a simple expression \na return statement is generated which terminates the execution of program. The most important interface \nbetween the tail-recursive inter\u00adpreter and the translation to C is the closure representation. The interpreter \ntreats closures as an abstract datatype with operations make-closure, closure-label, and closure-freeval \nwith the obvious interpretations. These operations are propagated to resid\u00adual programs. The translator \nto C is free to choose an efficient implementation for closures. The current implementation uses a flat \nvector representation. Note that the C code ASO performs a se\u00ad quential dispatch on closure applications \nexactly like the Scheme input programs. It might be desirable to perform closure applica\u00ad tion by sn \nindirect goto statement as allowed by GCC [41]. How\u00ad ever, since sequential dispatch is inherent in our \napproach, it would seem difficult to achieve this by straightforward means. We represent Scheme data \nobjects by a C union, and we em\u00ad ploy the Boehrn garbage collector for C [6]. There is no coopera\u00ad tion \nbetween the translation and the garbage collector. 210 p G Env = Variable+ ValDesc o G CVEnv = ConfigVariable \n+ Value 7 G Context = ValDesc* s [-] : SimpleExpression + Env + CVEnv + (ValDesc x CVEnv) q-] : ValDesc \n+ CVEnv + Value &#38; [-] : Expression + CVEnv + Context + Value c -: ValDesc + CVErw + Context + Value \ns* [V]pcr = (pv,u) S*[K]pa = (quote(K), m) S* [(eons SEI SE2)]p0 = let (descl, cl) = S* [S.El]pCJ (desc2, \nm2) = s* [SE2JX71 in (cons (deml, descz), 02) S [(car Sl?)]pa = let (ahc, u ) = S [SE@ in (desc = cons \n(descl, desc2)) + (descl , u ) / leti =jhxh(a) in (cv(i),a[i I+ O[car](Z)[desc]a )]) S [(cdr SE) pa = \nanalogous ) S* [(lambda (V) E)]po = let Vi... V. =f?eevam(t) in clos(t, pVI . . . pVn) S [(0 EI . . \n. EJ]pa = (SE, . . . SE~ StStiC) + (quote (O[O](D[&#38;scl]a,... , D~descnjo)), 0) I let i =<@sh(cr) \nin (cv(i), u[i I+ OIO](D[descl]a, . . . , D@hCn]CJ)]) D[quote(K)]u = K[K] D[cons(descl, descz)]a = U[cons](D[descl]a, \nD[descz]a) D[clos(t!, a%cl . . . descw)]a = (t!, D[akcl]a... D[descn]a) D[cv(i)]u = a(i) t [s??ljpcr~ \n= let (&#38;sc, u ) = S [SE]pU in C desc at-y &#38; [(if SE El Ez)]pcq = let (ahc, u ) = S* [SE]pU in \n(SE static) (descn, u,,) = S*[SE.]pa.-l in &#38; [#(P)][V1 + descl, . . . , V~ I+ desc~]un7 &#38; [(SE \nE)]p07 = let (desc, a ) = S [SE]pa~ in &#38; [E]pu (desc : 7) c ak?sca(c: -/) = (c= clos(l, descl . . \n. de$c~)) + let (Iambd;(v) E) = 4(/) . . . Vn = fwevars(e) in &#38;e[.E][V -. . . , Vn +) desco]rq desc, \nV1 I+ G%SC1,I let (e,vl. ..vn) = D[c]a (lambd;(V) E) = +(~) . . . V. = fr.swars(l) il = jii;y descl = \na] = C7[il * Vl] = @d+7._*) des~ = Cv(in) Un = cr.-l [in * v.] in &#38; [E][V I+ desc, V1 I+ &#38;,scl, \n. . . , Vn I+ U2scn]an+ C de$ca[] = D[desc]u Figure 7: IWo-level interpreter 5.2 By Partial Evaluation \nhigher-order to first-order interpreter which has been developed in Section 4, and ~nt e, s hypothetical \ninterpr.tor for first-order tail- The recent advent of C specializes [1,2] facilitates a development \nrecursive Scheme written in C. Our tools are the compiler generator which cuhninates in a complete compiler \nwritten entirely in C. As cogen derived by self-application from the Unmix specislizer and ingredients \nwe only have to provide two interpreters, int-s, the the compiler generator C-mix [2]. Let us denote \nexecution of 211  Scheme programs and [.]c execution of C programs. First we apply gen-s = [cogen]~ \nint-s to obtain a program generator which turns higher-order Scheme programs into first-order tail-recursive \nScheme F. Nextj we apply gen-s to itself, gen-s-ft = [gen-sjs gen-s, and obtain the higher-order to \nF program generator, but now writ\u00adtenin F ! Now we start on the C end of the translation. An F+C com\u00adpiler \n(written in C) is theresnlt of an application of C-mix to int-c: gen-c = [C-mix]c int-c. We can now \ntranslate gen-s-f t to C by using the compiler just constructed: gen-s-ft-c = [gen-c]c gen-s-ft. It \nremains to compose the programs gen-c and gen-s-f t-c to obtain a full Scheme to C compiler written in \nC: scheme->c = gen-c o gen-e-ft-c Performing the composition merely consists in merging the print routine \nof gen-c with the parser of gen-s-f t -c. In essence, we have seen that a Scheme-to-C compiler (written \nin C) can be generated by partial evaluation for the price of writing two interpreters, int-s and int-c: \nscheme->c = int-s + int-c + partial evaluation. The ideas presented in this section have not been realized \nin prac\u00adtice, due to the fact that no C specirdizer is publicly available as of yet. Experimental Results \n We have run some preliminary benchmarks which indicate that the performance of OUTapproach is comparable \nto other Scheme com\u00adpilers which generate C code. Our benchmarks are a program com\u00adputing derivatives \nderiv from the Gabriel benchmark suite [22], the Takeuchi function tak, a CPS version of it cpstak, a \nver\u00adsion of it using lists instead of integers takl (also taken from the Gabriel suite) a version of \nthe Fibonacci function involving clo\u00adsures fibclos, a suite of calls to cps-append, and a program solving \nthe 10-queens problem queens. Figure 8 shows the timings of the benchmarks as compared with Hobbit 4d \n[43], an optimiz\u00ading compiler which produces code for the scm runtime, used with maximum optimization \nand fixnum arithmetic. Our versions of the benchmarks were sdl run using the offline generalization strategy. \nWe Hobbit deriv 2420 390 tak 5820 810 cpstak 6400 6490 t akl 220 870 fibclos 15820 19480 cps-append 5480 \n36340 queens 8110 2370 Figure 8: Benchmarks (timings in milliseconds) 7 Related Work Tbrchin [47] shows \nthat the interpretive approach can perform pow\u00aderful transformations. Gluck and J@gensen [24, 25] use \nthe in\u00adterpretive approach to generate a deforested and a supercompiler. However, they only deal with \nfirst-order languages. Past attempts at compilers for higher-order languages by partial evaluation have \nproduced higher-order target code because they are written in high\u00ader-order languages. Bondorf [7] studies \nthe automatic generation of a compiler for a lazy higher-order functional language from an interpreter. \nJwgensen shows that optimizing compilers for real\u00adistic functional languages can be generated by rewriting \nan inter\u00adpreter [30, 31]. Consel and Danvy [17] use partial evaluation to compile Algol to tail-recursive \nScheme. However, they attribute their success to sophisticated features of the partial evaluator they \nuse, Schism, such as partially static data structures and higher-order functions. Burke and Consel [12] \ntranslate Scheme into low-level stack-machine code by multiple interpretive passes, starting from a denotational \nsemantics for Scheme. However, they also make ex\u00adtensive use of higher-order features of the partial \nevaluator. The first mention of higher-order removal or defunctionaliza\u00adtion appears in work of Reynolds \n[35]. Compilers for functional languages [42, 4,3, 20] usually achieve closure conversion with a direct \nnon-optimizing transformation algori~ and employ CPS conversion to transform programs into tail form. \nChin and Dar\u00adlington [13, 14] give a higher-order removal algorithm for lazy functional languages. However, \nthe resulting progmrn may still be higher-order-the algorithm does not perform closure conver\u00adsion. The \ncompilation of higher-order languages via a C compile-r has been used successfully in several projects, \nsnch as ssrd2c [44], Hobbit [43], Bigloo [39], and the Glasgow Haskell Compiler [34]. In particnh, srn12c \nalso translates tail-recursive intermediate code obtained from a CPS trausformation, but uses a function \ndispatcher for handling control. 8 Conclusion The tests were run on an IBM PowerPC/250. The f ibclos \nand cps-append benchmarks indicate that our approach deals especially well with higher-order code. For \nthe first\u00adorder code in t ak, deriv, and queens, OUTapproach introduces evaluation contexts and thus \nclosures whereas Hobbit can use the native C stack to some advantage. Note that we have spent no effort \nwhatsoever on tuning either the resulting first-order Scheme code, or the translation to C. We believe \nthat further optimization wilI result in an additional substantial performance increase. Also, ri\u00adsing \nthe online generalization strategy, the cpst ek benchmark ran roughly 3 times faster. Our compiler produces \nquite compact stand-alone executable. The complete benchmark suite yields a binary well under 200 Kilo\u00adWe \nhave used the interpretive approach to generate the middle end of a compiler for a strict higher-order \nfunctional language from an interpreter. We achieve closure conversion and conversion to tail form by \napplying the respective transformations on a straight\u00adforward interpreter manually. Otfline partial evaluation \nturns the interpreter into an automatic transformer by virtue of the language bytes-including the Boehm \ncollector. The programs associated with the optimizing compiler to tail\u00ad recursive Scheme take up less \nthan 70 Kilobytes. The compiler to C takes up a mere 10 Kilobytes. preservation property. Adding constant \npropagation in static data to our interpreter then turns the simple transformer into an optimizer and \nspecialirer thanks to the specialism projections. The transla\u00adtion also makes optirnizations present \nin the partial evaluator such as post-unfolding and arity raising accessible to the optimized pro\u00adgmms, \nIn addition, we have presented a translator of the resulting code into low-level C. We have formulated \nthe language preservation property, and put it to use for the optimizing compilation of a higher-order \nlanguage into C with little conceptual effort. We consider this a successful 212 bootstrapping process. \nOur results prove that partial evaluation is a [17] Charles Consel and Olivier Danvy. Static and dynamic \nse\u00adpractical approach to the generation of optimizing compilers. mantics processing. In Symposium on \nPrinciples of Progra\u00admmingLanguages 91, pages 14-24. ACM, January 1991. References [18] Charles Consel \nand Olivier Danvy. Thtorial notes on partial [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] \n[14] [15] [16] Lars Ole Andersen. Self-applicable C program specialization. In Consel [15], pages 54-61. \nReport YfiEU/DCSiRR-909. Lam Ole Andersen. Ptngrmn Analysis and Specialization for the CPmgramming Language. \nPhD thesis, DIKU, University of Copenhagen, May 1994. (DIKU report 94/19). Andrew W. Appel. Compiling \nwith Continuations. Cam\u00adbridge University Press, 1992. Andrew W. Appel and Ikevor Jim. Continuation-passing, \nclosure-passing style. In Pmt. 16th ACMSymposium on Prin\u00adciples of Programming Languages, pages 293 302, \n1989. Dines Bj@mer, Andrei P. Ershov, and Neil D. Jones, editors. Partial Evaktion and Mixed Computation. \nNorth-Holland, 1987. Proceedings of the JFfP 1122Workshop on Partial Eval\u00aduation and Mixed Computation. \nHans-J. Boehm, Alan J. Demers, and Scott Shenker. Mostly parallel garbage collection. In Proc. oftheACMSIGPLAN \n91 Conference on Programming Language Design and Imple\u00admentation, pages 157-164, Ibronto, June 1991. \nACM. SIG-PLAN Notices, v26, 6. Anders Bondorf. Automatic autopmjection of higher order recursive equations. \nIn Jones [27], pages 70-87. LNCS 432. Anders Bondorf. Automatic autoprojection of higher order re\u00adcursive \nequations. Science of Computer Programming, 173\u00ad34, 1991. Anders Bondorf. Similix 5.0 Manual. DIKU, University \nof Copenhagen, May 1993. Anders Bondorf and Olivier Danvy. Automatic autoprojection of recursive equations \nwith global variables and abstract data types. Science of Computer Programming, 19(2):151-195, 1991. \nAnders Bondorf and Jesper J@gensen. Efficient analyses for realistic off-line partiaJ evaluation. Journal \nof Functional Pro\u00adgramming, 3(3):315-346, July 1993. E. David Burke and Charles Consel, Compiling scheme \npro\u00adgrams via multi-pass partial evaluation. Technical report, Ore\u00adgon Graduate Institute of Science \nand Technology, 1994. Wei-Ngan Chin. Fully lazy higher-order removal. In Consel [15], pages 38-47. Report \nY~EU/DCS/RR-909. Wei-Ngan Chin and John Darlington. Higher-order removal transformation technique for \nfunctional programs. In Pmt. of 15th Austrrdian Computer Science Conference, pages 181\u00ad 194, Hobart, \nTasmani~ Januwy 1992. Australian CS Comm Vol 14, No 1. Charles Consel, editor. Workshop Pardal Evaluation \nand Semantics-Based Program Maniptdation *92, San Fkmciaco, CA, June 1992. Yale University. Report YALEU/DCS/RR\u00ad \n909. Charles Consel. A tour of schism. In Symp. Partial lhdua\u00adtion and Sem&#38; cs-Based Program Manipuktion \n93, pages 134-154, Copmhagen, Denmark June 1993. ACM. evrduation. In Symposium on Principles of Pn2gramming \nLan\u00adguages 93, pages 493-501, Charleston, January 1993. ACM. [19] Functional Programming Languages and \nComputer Arrhitec\u00adtute, London, GB, 1989. [20] Daniel P. Friedman, Mitchell Wand, and Christopher T. \nHaynes. Essentkds of programming languages. MIT Press, Cambridge, MA, 1992. [21] Y. Futamura. Partial \nevaluation of computation process-an approach to a compiler-compiler. Systems, Computers, Con\u00adtrols, \n2(5):45-50, 1971. [22] Richard P. Gabriel. Pe#ormance and EvoJuation of Lisp Sys\u00adtems. MIT Press, Cambridge, \nMA, 1985. [23] Robert Gltick. On the generation of specializes. Journal of Functional Programming, 4(4):499-514, \nOctober 1994. [24] Robert Gltick and Jesper J@gensen. Generating optimizing specializes. In IEEE International \nConference on Computer Lunguages, pages 183-194. IBEE Computer Society Press, 1994. [25] Robert Gliick \nand Jesper J@rgensen. Generating transformers for deforestation and supercompilation. In B. Le Charlier, \nedi\u00adtor, Static Analysis, volume 864 of Lecture Notes in Computer Science, pages 432-448. Springer-Verlag, \n1994. [26] Neil D. Jones. Challenging problems in partial evaluation and mixed computation. In Bj@mer \net al. [5], pages 1-14. Pro\u00adceedings of the IFIP TC2 Workshop on Partial Evaluation and Mixed Computation. \n[27] Neil D. Jones, editor. Proc. of the 3rri European Sympo\u00adsium on Programming 1990, Copenhagen, Denmark, \n1990. Springer-Verlag. LNCS 432. [28] Neil D. Jones, Carsten K Gomard, and Peter Sestoft. Par\u00adtial Evah@ion \nand Automatic Program Generation. Prentice-Hal~ 1993. [29] Neil D. Jones, Peter Sesto&#38; and Harald \nS@ndcrgaard. An experiment in partial evaluation: The generation of a com\u00adpiler generator. In J.-P. Jouannaud, \neditor, Rewriting Tech\u00adniques and Applications, pages 124-140, Dijon, France, 1985. Springer-Verlag. \nLNCS 202. [30] Jesper J@gensen. Compiler generation by partial evaluation. Master s thesis, DIKU, University \nof Copenhagen, 1991. [31] Jesper Jorgensen. Generating a compiler for a lazy language by pmtial evaluation. \nIn Symposium on Principles of Pr\u00adogramming Languages 92, pages 258 268, ACM, ACM, Jan-LUUy 1992. [32] \nJohn Launchbury. Projection Factorisations in Partial Ewd\u00aduation, volume 1 of Distinguished Dissertations \nin Computer Science. Cambridge University Press, 1991. [33] Torben X. Mogensen. Separating binding times \nin language specifications. In FPCA1989 [191. pages l&#38;25\u00ad 213 [34] Simon L Peyton Jones, Cordelia \nHsl~ Kevin Hammond, Will Partain, and Philip Wsdler. The Glasgow Haskell compile~ a technical overview. \nIn Proceedings of the UK Joint Frame\u00adwork for Information Technology (JFIT) Technical Confer\u00adence, Keele, \n1993. [35] John C. Reynolds. Definitional interpreters for higher-order progr amming. In ACM Annual Conference, \npages 717-740, Jllly 1972. [36] Sergei A. Romsnenko. A compiler generator produced by a self-applicable \nspecislirer can have a surprisingly natural and understandable strncture. In Bj@mer et al. [5], pages \n445-464. Proceedings of the IFIP TC2 Workshop on Partial Evaluation and Mixed Computation. [37] Sergei \nA. Romanenko. kity raiser and its use in program specialization. In Jones [27], pages 341-360. LNCS 432. \n[38] Erik Rnf. Topics in Online Partied Evah@ion. PhD thesis, Stanford University, Stanford, CA 94305-4055, \nMarch 1993. Technical repxt CSL-TR-93-563. [39] Manual Serrsno. Bigloo user s manual. Technical report, \nIN-RIA, 1994. (to appear). [40] Peter Sestoft. Replacing function parameters by global vari\u00adables. In \nFPCA1989 [19], pages 39-53. [41] Richard M. Stalhnsn. Using GNU CC, November 1995. (part of the GCC distribution). \n[42] Guy L. Steele. Rsbbik a compiler for Scheme. Technical Report AI-TR-474, MIT, Cambridge, M&#38; \n1978. [43] Tanel Tsmmet. Lambda-lifting as an optimization for com\u00adpiling scheme to C. available as f \ntp: //www. cs. chalmers. edu/pub/users/temmet /wwu/hobbit. ps. [44] D. Tsrditi, A. Achary~ and P. Lee. \nNo assembly required: compiling Standard ML to C. Technical Report CMU-CS\u00ad90-187, School of Computer \nScience, Carnegie Mellon Uni\u00adversity, November 1990. [45] Peter Thiemsnn. Higher-order redundancy elimination. \nIn Pe\u00adter Sestoft and Herald S@ndergaard, editors, Workxhop Partial Evcduation and Semantics-Based Prvgram \nManipulation 94, pages 73-84, Grlando, Fla., June 1994. ACM. [46] Peter Thiemsnn and Robert Gliick. The \ngeneration of a higher-order online partial evaluator. In Mssato Takeichi, ed\u00ad itor, Fuji Workshop on \nFunctional and Logic Programming, pages 239-253, Fuji Susouo, Japan, July 1995. World Scien\u00adtific Press, \nSingapore. [47] Vslentin F, Turchin. Program transformation with metasystem transitions. Joumd of Functional \nProgramming, 3(3):283\u00ad313, July 1993. [48] Daniel Weise, Roland Conybesre, Erik Rnf, and Scott Selig\u00admsn. \nAutomatic online partial evaluation. In Con$ Func\u00adtional Programming Languages and Computer Axhitecture \n91, pages 165-191, Cambridge, September 1991. ACM.  \n\t\t\t", "proc_id": "231379", "abstract": "Two key steps in the compilation of strict functional languages are the conversion of higher-order functions to data structures (<i>closures</i>) and the transformation to tail-recursive style. We show how to perform both steps at once by applying first-order offline partial evaluation to a suitable interpreter. The resulting code is easy to transliterate to low-level C or native code. We have implemented the compilation to C; it yields a performance comparable to that of other modern Scheme-to-C compilers. In addition, we have integrated various optimizations such as constant propagation, higher-order removal, and arity raising simply by modifying the underlying interpreter. Purely first-order methods suffice to achieve the transformations. Our approach is an instance of semantics-directed compiler generation.", "authors": [{"name": "Michael Sperber", "author_profile_id": "81100100127", "affiliation": "Wilhelm-Schickard-Institut f&#252;r Informatik, Universit&#228;t T&#252;bingen, Sand 13, D-72076 T&#252;bingen, Germany", "person_id": "PP14044834", "email_address": "", "orcid_id": ""}, {"name": "Peter Thiemann", "author_profile_id": "81100458917", "affiliation": "Wilhelm-Schickard-Institut f&#252;r Informatik, Universit&#228;t T&#252;bingen, Sand 13, D-72076 T&#252;bingen, Germany", "person_id": "PP39043747", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/231379.231419", "year": "1996", "article_id": "231419", "conference": "PLDI", "title": "Realistic compilation by partial evaluation", "url": "http://dl.acm.org/citation.cfm?id=231419"}