{"article_publication_date": "05-01-1996", "fulltext": "\n GUM: a portable parallel implementation of Haskell PW Trinder K Hammond JS Mattson Jr * AS Partridge \nt SL Peyton Jones $ Department of Computing Science, Glasgow University Email: {trinder,kh,simonpj} @dcs.glasgow.ac.uk \nAbstract GUM is a portable, parallel implementation of the Haskell functional language. Despite sustained \nresearch interest in parallel functional programming, GUM is one of the first such systems to be made \npublicly available. GUM is message-based, and portability is facilitated by us\u00ading the PVM communications \nharness that is available on many multi-processors. As a result, GUM is available for both shared-memory \n(Sun SPARCserver multiprocesscu-s) and distributed-memory (networks of workstations) archi\u00adtectures. \nThe high message-latency of distributed machines is ameliorated by sending messages asynchronously, and \nby sending large packets of related data in each message. Initial performance figures demonstrate absolute \nspeedups relative to the best sequential compiler technology. To im\u00adprove the performance of a parallel \nHaskell program GIJM provides tools for monitoring and visualizing the behaviour of threads and of processors \nduring execution. 1 Introduction GUM (Graph reduction for a Unified Machine model) is a portable, parallel \nimplementation of the non-strict purely\u00adfunctional programming language Haskell. Despite hun\u00addreds of \npapers, dozens of designs, and a handful of real single-site implementations, GUM is one of the first \nsuch systems to be made publicly available. We believe that this is partly because of the diversity of \nparallel machine archit\u00adectures, but also because the task of implementing a par\u00adallel functional language \nis much more substantial than it * Author s present address: Hewlett-Packard, California Language Laboratory \nEmail JmattsOn@cup.hp. corn. tAuthorls present address. Department of CbmpUkr %ma?, uni\u00ad versity of Tasmania. \nEmad. A.S Partridge@cs utas edu. au. $Thls work supported by an SOED personal research felh3wshlP M \nfrom the Royal Society of Edinburgh, and the UK EPSRC AQUA and Parade grants. Pefmiesion to make digitaihard \noopy of part or all of this work for personal or dassmom use is granted without fee provided that copies \nare not made or distributed for profit or commercial advantage, the copyright notice, thetitle of the \nublkatbn and its date appear, and notice is given that i copying IS y permission of ACM, Inc. To copy \notherwise, to republish, to poston servers,orto redistributeto Iis(s,requirespriorspecificpermission \nandfor a fee. PLDI 96 5/96 PA, USA 01996 ACM 0-89791-795-2/%/0005 ...$3.50 first appears. The goal of \nthis paper is to give a technical overview of GUM, highlighting our main design choices, and present \npreliminary performance measurements. GUM has the following features: GUM is portable. Itis message \nbased, and uses PVM [21], a communication infrastructure available on almost every multiprocessor, including \nboth shared\u00admemory and distributed-memory machines, as well as networks of workst ations. The basic assumed \narchit ec\u00adture is that of a collection of processor-memory units (which we will call PEs) connected by \nsome kind of network that is accessed through PVM. PVM imposes its own overheads, but there are short-cuts \nthat can be taken for communication between homogeneous ma\u00adchines on a single network. In any case, for \nany pwtic\u00adular architecture a machine-specific communications substrate could readily be substituted. \n GUM can deliver szgnijicant absolute speedups, rela\u00adtive to the best sequential compiler technology. \nNeed\u00adless to say, this claim relates to programs with lots of large-grain parallelism, and the name of \nthe game is seeing how far it extends to more realistic programs. Nevertheless, such tests provide an \nimportant sanity check: if the system does badly here then all is lost. The speedups are gained using \none of the best avail\u00adable sequential Haskell compilers, namely the Glasgow Haskell Compiler (GHC). Indeed \nGUM is just a new runtime system for GHC. The sequential parts of a pro\u00adgram run as fast as if they were \ncompiled by GHC for a sequential machine, apart from a smalf constant-factor overhead (Section 3.1). \n GUM provides a suite of tools for monitoring and vtsu\u00adaking the behavaour of programs. The bottom line \nfor any parallel program is performance, and performance can only be improved if it can be understood. \nIn ad\u00addition to conventional sequential tools, GUM provides tools to monitor and visualise both PE and \nthread ac\u00adtivity over time. These tools are outside the scope of this paper, but are discussed in [9]. \n GUM supports independent local garbage collection, within a singie global virtual heap. Each PE has \na local heap that implements part of the global virtual heap. A two-level addressing scheme distinguishes \nlo\u00adcal addresses, within a PE s local heap, from global addresses, that point bet ween local heaps. The \nman\u00ad  agement of global addresses is such that each PE CZUL garbage-collect its local heap without synchronizing \nwith other PEs, a property we found to be crucial on the GRIP multiprocessor [22].  Thread distribution \nis per~ormed lazily, but data dis\u00adtribution is performed somewhat eagerly. Threads are never exported \nto another PE to try to balance the load. Instead, work is only moved when a processor is idle (Section \n2.2). Moving work prematurely can have a very bad effect on locality. On the other hand, when replying \nto a request for a data value, a PE packs (a copy of) nearby data into the reply} on the grounds that \nthe requesting PE is likely to need it soon (Section 2.4). Since the sending PE retains its copy, locality \nis not lost.  Ml messages are asynchronous. The idea which is standard in the multithreading community \n[I] is that once a processor has sent a message it can forget all about it and schedule further threads \nor messages without waiting for a reply (Section 2.3.4). Notably, when a processor wishes to fetch data \nfrom another processor it sends a message whose reply can be ar\u00adbitrarily delayed for example, the data \nmight be under evaluation at the far end. When the reply fi\u00adnally does arrive, it is treated as an independent \nwork item.  Messages are sent asynchronously and contain large amounts of graph in order to ameliorate \nthe effects of long-latency distributed machines, Of course there is no free lunch. Some parallel Haskell \nprograms may work much less well on long-latency machines than short-latency knows to what extent. One \nmerit of having framework is that we may hope to identify GUM is freely available by FTP, as part Haskell \nCompiler (release 0.26 onwards). ones, but nobody a single portable this extent. of the Glasgow It is \ncurrently ported to networks of Sun SPARCS and DEC Alphas, and to Sun s symmetric multiprocessor SPARCserver. \nOther ports are in progress. The remainder of this paper is structured as follows. Section 2 describes \nhow the GUM run-time system works. Section 3 gives preliminary performance results. Section 4 discusses \nrelated work. Section 5 contains a discussion and outlines some directions for development. 2 How GUM \nworks The first action of a GUM program is to create a PVM man\u00adager task, whose job is to control startup \nand termination. The manager spawns the required number of logical PEs as PVM tasks, which PVM maps to \nthe available processors. Each PE-task then initialises itse~ processing runtime ar\u00adguments, allocating \na local heap etc. Once all PE-tasks have initialised, and been informed of each others identity, one \nof the PE-tasks is nominated as the main Pi3. The main PE then begins executing the main thread of the \nHaskell pro\u00adgram. The program terminates when either the main thread com\u00adpletes, or an error is encountered. \nIn either case a FINISH 80 message is sent to the manager task, which in turn broad\u00adcasts a FINISH message \nto all of the PE-tasks. The man\u00adager waits for each PE-task to respond before terminating the program. \nDuring execution each PE executes the following scheduling loop &#38;til it receives a FINISH message. \n- I Main Scheduler: 1.Perform local garbage collection, if necessary (Sec\u00adtion 2.3). 2. Process any incoming \nmessages from other PEs, possibly sending messages in response (Sections 2.2 and 2.3.4). 3. If there \nare runnable threads, run one of them (Sec\u00adtion 2.1). 4. Otherwise look for work (Section 2.2).  I \nThe inter-PE message m-otocol is comrdetelv asynchronous. ., . When a PE sends a message it does not \nawait a reply; in\u00adstead it simply continues, or returns to the main scheduler. Indeed, sometimes the \nreply may be delayed a long time, if (for example) it requests the value of a remote thunk that is being \nevaluated by some other thread. These techniques are standard practice in the multithreadlng community \n[1]. 2.1 Thread Management A thread is a virtual processor. It is represented by a (heap\u00adallocated) Thread \nState Object (TSO) containing slots for the thread s registers. The TSO in turn points to the thread \ns (heap-allocated) Stack Object (SO). As the thread s stack grows, further Stack Objects are allocated \nand chained on to the earlier ones. Each PE has a pool of runnable threads or, rather, TSOS called its \nrunnable pool, which is consulted in step (3) of the scheduling loop given earlier. Currently, once a \nthread has begun execution on a PE it cannot be moved to another PE. This makes programs with only limited \nparallelism vul\u00adnerable to scheduling accidents, in which one PE ends up with several runnable (but immovable) \nthreads, while oth\u00aders are idle. In the future we plan to allow runnable threads to migrate. When a thread \nis chosen for execution it is run non\u00ad preemptively until either space is exhausted, the thread blocks \n(either on another thread or accessing remote data), or the thread completes. Compared with fair scheduling, \nthis has the advantage of tending to decrease both space usage and overall run-time [4], at the cost \nof making con\u00adcurrent and speculative execution rather harder. 2.1.1 Sparks Parallelism is initiated \nexplicitly in a Haskell program by the par combinator. At present these combinators are added by the \nprogrammer, though we would of course like this task to be automated. The par combinator implements a \nform of parallel composition. Operationally, when the expression x par e is evaluated, the heap object \nreferred to by the variable x is sparked, and then e is evaluated. Quite a com\u00admon idiom (though by no \nmeans the only way of using par) is to write letx=fabinx par(e where e mentions x. Here, a thunk (or \nsuspension) represent\u00ading the call f a b is allocated by the let and then sparked by the par. It may \nthus be evaluated in parallel with e. Sparkhg a thunk is a relatively cheap operation, consisting only \nof adding a pointer to the thunk to the PE s spark pool, A spark is an indication that a thunk might \nusefully be evaluated in parallel, not that it must be evaluated in parallel. Sparks may freely be discarded \nif they become too numerous. A sparked thunk is similar to a future, as used in MultiLisp and TERA C \n[19]. 2.1.2 Synchronisation It is obviously desirable to prevent two threads from evaluat\u00ading the same \nthunk simultaneously, lest the work of doing so be duplicated. This synchronisation is achieved as follows: \n1.When a thread enters (starts to evaluate) a thunk, it overwrites the thunk with a black hoie (so called \nfor historical reasons)l. 2. When a thread enters a black hole, it saves its state in its TSO, attaches \nits TSO to the queue of threads blocked on the black hole (the black hole s blocking queue), and enters \nthe scheduler. 3. When a thread completes the evaluation of a thunk, it overwrites the latter with its \nvalue (the update opera\u00adtion). When it does so, it moves any queued TSOS to the ruunable pool.  Notice \nthat synchronisation costs are only incurred if two threads actually collide. In particular, if a thread \nsparks a sub-expression, and then subsequently evaluates that sub\u00adexpression before the spark has been \nturned into a thread and scheduled, then no synchronisation cost is incurred. In effect the putative \nchild thread is dynamically inlined back into the parent, and the spark becomes an orphan. 2,2 Load distribution \nIf (and only if) a PE has nothing else to do, it tries to schedule a spark from its spark pool, if there \nis one. The spark may by now be an orphan, because the thunk to which it refers may by now be evaluated, \nor be under evaluation 1In fact, thunks are only overwritten wlt h black holes when a thread context \nsw]tches. The advantage of this lazg Mack-hol!ng is that many thunks may have been entered and updated \nwithout ever being black-holed, PEA PEB PEC Figure 1: Fish/Schedule/Ack Sequence by another thread. \nIf so, the PE simply discards the spark and tries the next in first-in first-out (FIFO) order. FIFO gives \ngood results for divide-and-conquer programs because large-grain threads are given priority. If the PE \nfinds a useful spark, it turns it into a thread by allocating a fresh TSO and S02, and starts executing \nit. If there are no local sparks, then the PE seeks work from other PEs, by launching a FISH message \nthat swims from PE to PE looking for available work. Initially only the main PE is busy has a runnable \nthread and all other PEs start fishing for work as soon as they begin execution. When a FISH message \nis created, it is sent at random to some other PE. If the recipient has no useful sparks, it in\u00adcreases \nthe age of the FISH, and sends the FISH to an\u00adother PE, again chosen at random. The age of a FISH lim\u00adits \nthe number of PEs that a FISH visits: having exceeded this limit, the last PE visited returns the unsuccessful \nFISH to the originating PE. On receipt of its own, starved, FISH the originating PE then delays briefly \nbefore launching an\u00adother FISH. The purpose of the delay is to avoid swamping the machine with FISH messages \nwhen there are only a few busy PEs. A PE only ever has a single FISH outstanding. If the PE that receives \na FISH has a useful spark (again located by a FIFO search), it sends a SCHEDULE mes\u00adsage to the PE that \noriginated the FISH, containing the sparked thunk packaged with nearby graph, as described in Section \n2.4. The originating PE unpacks the graph, and adds the newly acquired thunk to its local spark pool. \nAn ACK message is then sent to record the new location of the thunk(s) sent in the SCHEDULE (Section \n2.4). Note that the originating PE may no longer be idle because, before the SCHEDULE arrives, another \nincoming message may have unblocked some thread. A sequence of messages initiated by a FISH is shown \nin Figure 1. 2:3 Memory Management Parallel graph reduction proceeds on a shared program/data graph, \nso a primary function of GUM is to manage the vir\u00adtual shared memory in which the graph resides. Since \nwe know exactly when we discard TSOS and SOS, and they are relatively large, we keep them on a free list \nso that we can avoid chompmg through heap when executing short-llved tasks. 2.3.1 Local Addresses Since \nGUM is based on the Glasgow Haskell Compiler, most execution is carried out in precisely the same way \nas on a uniprocessor. In particular: Each PE has its own local heap.  New heap objects are allocated \nfrom a contiguous chunk of free space in this local heap.  The heap-object addresses manipulated by \nthe com\u00adpiled code are simply one-word pointers within the lo\u00adcal heap which we term local addresses. \n Each PE can verform local oarba~e collection indepen\u00addently of all ~he other PEsT Thi~ crucial propert~ \nal\u00adlows each PE cheaply to recycle the litter generated by normal execution.  Sometimes, though, the \nrun-time system needs to move a heap object from one PE s local heap to another s. Forex\u00adample, when \nPE C in Figure 1 with plenty of sparks receives a FISH message, it sends one of itssparkedthunks to A, \nthe originating PE. When a t hunk is moved in this way, the original thunk is (ultimately) overwritten \nwith a FetchMe object, cent aining the global address of the new copy on A. Why does the thunk need to \nbe overwritten? It would be a mistake simply to copy it, because then both A and C might evaluate it \nseparately (remember, there might be other local pointers to it from C s heap). 2.3.2 Global Addresses \nAt first one might think that a global address (GA) should consist of the identifier of the PE concerned, \ntogether with the local address of the object on that PE. Such a scheme would, however, prevent the PEs \nfrom performing compact\u00ading garbage collection, since that changes the local address of most objects. \nSince compacting garbage collection is a crucial component of our efficient compilation technology we \nreject thk restriction. Accordingly, we follow standard practice [1~] and allocate each globally-visible \nobject an immutable local zdentijier (typically a natural number). A global address consists of a (PE \nidentifier, local identifier) pair. Each PE maintains a Global Indirection Table, or GIT, which maps \nlocal identi\u00adfiers to the local address of the corresponding heap object. The GIT is treated as a source \nof roots for local garbage collection, and is adjusted to reflect the new locations of local heap objects \nfollowing local garbage collection. We say that a PE owns a globally-visible object (that is, one possessing \na global address) if the object s global address contains that PE s identifier. .4 heap object is globalised \n(that is, given a global address) by allocating an unused local identifier, and augmenting the GIT to \nmap the local identifier to the object s address. Of 3The alert reader WI1l have noticed that we will \nneed some mech\u00adanism for recovering and re-usmg local Identifiers, a matter we will return to shortly \ncourse, it is possible that the object already has a global ad\u00address. We account for this possibility \nby maintaining (sepa\u00adrately in each PE) a mapping from local addresses to global addresses, the LA+ GA \ntable, and checking it before glob\u00adalking a heap object. Naturally, the LA -+ GA table has to be rebuilt \nduring garbage collection, since objects local addresses may change. A PE may also hold copies of globally-visible \nheap objects owned by another PE. For example, PE A may have a copy of a list it obtained from PE B. \nSuppose the root of the list has GA (B,34). Then it makes sense for A to remember that the root of its \ncopy of the list also has GA (B,34), in case it ever needs it again. If it does, then instead of fetching \nthe list again, it can simply share the copy it already has. We achieve this sharing by maintaining (in \neach PE) a mapping from global addresses to local addresses, the PE s GA+ LA table. When A fetches the \nlist for the first time, it enters the mapping from (B, 34) to the fetched copy in its GA+ LA table; \nthen, when it needs (B ,34) again it checks the GA+ LA table first, and finds that it already has a local \ncopy. To summarise, each PE maintains the following three ta\u00adbles. In practice the tables are coalesced \ninto a single data structure. s Its GIT maps each allocated local identifier to its local address. c \nIts GA ~ LA table maps some fo rezgraglobal addresses (that is, ones whose PE identifier is non-local) \nto their local counterparts. Notice that each foreign GA maps to precisely one LA. Its LA + GA table \nmaps local addresses to the corre\u00adsponding global address (if any).  2.3.3 Garbage collection Thk scheme \nhas the obvious problem that once an object has an entry in the GIT it cannot ever be garbage collected \n(since the GIT is used as a source of roots for local garbage collection), nor can the local identifier \nbe re-used. Again following standard practice, e.g. [14], we use we~ghted refer\u00adence co thing [2, 24] \nto recover local identifiers, and hence the objects they identify. We augment both the GIT and the GA+ \nLA table to hold a weight as well as the local address. The invariant we main\u00adtain is that for a given \nglobal address, G, the sum of: G s weight in the GA + LA tables of ali foretgn PEs, and  G s weight \nin its owner s GIT, and  the wetght attached to any Gs inside any in-j%ght mes\u00adsages  is equal to \nMax Weight, a fixed constant. With thk invariant in mind, we can give the following rules for address \nmanage\u00adment, which are followed independently by each PE: 1.Any entries in a PE s GIT that have weight \nMazWeight can be discarded, and the local identifier made avail\u00adable for re-use. (Reason: because of \nthe invariant, no other PEs or messages refer to this global address.) All the other entries must be \ntreated as roots for local garbage collection, 2. A PE can choose whether or not the local addresses \nin its GA -+ LA table are treated as roots for local garbage collection. If it has plenty of space available, \nit can treat them as roots, thereby preserving local copies of global objects in the hope that they will \nprove useful in the future. If instead the PE is short of space, it refrains from treating them as roots. \nAfter local garbage collection is complete, the GA + LA table is scanned. Any en\u00adtries whose local object \nhas (for some other reason) been identified as live by the garbage collector are redirect ed to point \nto the object s new location. Any entries whose object is dead are discarded, and the weight is returned \nto the owning PE in a FREE mes\u00adsage, which in turn adds the weight in the message to its GIT entry (thereby \nmaintaining the invariant).  3. If a PE sends a GA to another PE, the weight held in the GIT or GA+ \nLA table (depending on whether the GA is owned by this PE or not) is split evenly between the GA in the \nmessage and the GA remaining in the table. The receiving PE adds the weight to its GIT or GA -+ LA table, \nas appropriate. 4. If the weight in a GA to be sent is 1 it can no longer be split, s; instead a new \nGA is allocated. The II:W GA maps to the same local address in the GIT. The new and old GAs are aliases \nfor the same heap ob\u00adject, which is unfortunate because it means that some sharing is not preserved. \nTo prevent every subsequent shipping of the GA from allocating a new GA, we iden\u00adtify the new GA, with \nweight to give away, as the pre\u00ad~emeci GA. LA -+ GA lookup always returns the pre\u00adferred GA.  The only \ngarbage not collected by this scheme consists of cycles that are spread across PEs. We plan ultimately \nto recover these cycles too, by halting all PEs and performing a global collective garbage collection, \nbut we have not yet even begun its implement ation. In practice, local garbage collection plus weighted \nreference counting seems to recover most garbage. 2.3.4 Distributing Data Global references are handled \nby special Fetch-Me objects. When a thread enters a Fetch-Me the following steps are carried out: 1. \nThe Fetch-Me object is globalised, i.e. given a new local GA. It will already have a foreign GA, namely \nthe GA of the remote object, so this step creates a temporary alias for it. 2. The Fetch-Me object is \noverwritten with a Fetching object.  PEA FETCH(B36,A21) PE B r\\ RESUME(A21,<packets) < B36 m Figure \n2: Fetch/Resume/Ack Sequence 3. The demanding thread is blocked, by queueing its TSO on a blocking queue \nattached to the Fetching object. 4. A FETCH message is sent to the PE that owns the foreign GA of the \nFetch-Me. 5. The PE then returns to the main scheduler: i.e it may run other threads, garbage collect \nor process messages while awaiting the response to the FETCH, Any sub\u00adsequent thread that demands the \nsame foreign object will also join the queue attached to the Fetching ob\u00adject.  On receipt of a FETCH \nmessage, the target PE packages up the appropriate object, together with some nearby graph, and sends \nthk in a RESUME message to the originator. When the RESUME arrives, the originating PE unpacks the graph, \nrestarts the thread(s) that were blocked on the Fetching obiect, and redirects the Fetching ob iect to \npoint to the roo~ of &#38;is graph4. Having done this: an-ACK message is returned to the PE that sent \nthe RESUME (the following section explains why). Figure 2 depicts the whole process.  2.4 Packing/Unpacking \nGraph When an object is requested we also speculatively pack some nearby reachable graph into the same \npaclcet, with the ob iect of reducing the number of explicit FETCH messages th~t need to be ~ent. The \nobjective-is to increase throu~h\u00adput over high-latency networks by sending fewer, larger mes\u00adsages. Packing \narbitrary graph is a non-trivial problem, and [9] discusses related work and the algorithm and heuristics \nused in GUM. Packing proceeds object by object, in a breadth-first traver\u00adsal of the graph. As each object \nis packed it is given a global address, if necessary, and its location in the packet is recorded in a \ntable, so that sharing and cycles within the packet are preserved. We stop packing when either all reach\u00adable \ngraph has been packed, or the packet is full. Once the packet is full, the links from packed objects \nto local heap objects are packed as Fetch-Mes. Unpacking traverses the packet, reconstructing the graph \nin a breadth-first fashion. As each object is unpacked the GA -+ LA table is interrogated to find existing \nlocal copies of the object. If no copy exists, then the GA+ LA and 4Actually, It 1spossible that the \nRESUME might include in its nearby graph some objects for which there are other Fetch-Me or Fetching \nobJects on the recipient PE. If so, they are each redirected to point to the appropriate object in the \nnewly-received graph, and any blocked threads are rest arted. LA -+ GA tables are updated appropriately. \nHowever, if there is already a local copy of the object, care is taken to choose the more defined object. \nIn particular, an incom\u00ading normal-form object is preferred to an existing Fetch-Me. The weight of the \nincoming GA is added to the weight of the existing reference. The duplicate is overwritten by an indirection \nto the more defined object. While objects representing (normal form) values are freely copied, care is \ntaken to ensure that there is only ever one copy of a thunk, which represents a potentially unbounded \namount amount of work. An ACK message is sent after un\u00adpacking a packet to indicate the new location \nof any thunks in the packet. 3 Preliminary Results Thk section reports results of experiments performed \nto ver\u00adify that the basic mechanisms in GUM are working properly, and also to perform preliminary performance \nevaluation and tuning. The results should be viewed as indicative that speedups are possible using GUM, \nrather than conclusive evidence that GUM speeds-up real programs. 3.1 Single Processor Efficiency The \nfirst experiment investigates the underlying costs of pm\u00adallel evaluation, compared with sequential evaluation, \non a single processor. Section 3.3 investigatees parallel overheads on multiple processors. Unless the \noverhead on a single pro\u00adcessor is small, we cannot hope to achieve good absolute speed-ups, i.e. speed-ups \nrelative to a good sequential im\u00adplementation. The single-processor overhead can be cate\u00adgorised as follows. \n There is a more-or-less fixed percentage overhead on every program regardless of its use of pamdlelism. \nAn example of these overheads is that GUM must test each new object to see whether it is under evaluation \nalready.  There are overheads introduced by every spark site in the program, as described below.  \nWe investigate these overheads using a single processor ex\u00adecuting a divide-and-conquer factorial. This \ntoy program is a good stress-test for the second overhead, because when compiled for sequential execution \nall of the values used in the main loop are held in registers. However, in its paralIel form, the compiler \nis obliged to insert code to build a heap object for each spark site. If the program is written in the \nusual naive way, each thread does very little work before sparking another thread, and the overheads \nof parallelism are high. The version of divide-and-conquer factoriaf that we use, parf act, has an explicit \ncut-off parameter: if the problem size is smaller than the cut-off then it is solved using purely sequential \ncode: otherwise, the parallel code is used. In the next experiment (Section 3.2 ) the cut-off is varied \nto inves\u00adtigate how well GUM copes with various size threads. An interested reader can find the Haskell \nprogram in Appendix A. We report all speedups in thk paper relative to a fast se\u00adquential version of \neach program compiled using GHC with full optimisation. The following table reports parallel over\u00adheads \nusing parfact on one processor from two Spare-based architectures. The Sun multiprocessor has six Spare \n10 pro\u00adcessors and communicant es by shared memory segments. The Sun 4/15s are connected to a common \nethernet segment. A memory-resident 4Mb heap is used on both machines. The figures reported are the mean \nof at least three runs of the program on a lightly-loaded, but not necessarily completely idle, machine. \nUnix scheduling introduces some variabil\u00adityy into the results, and hence they are reported to only 2 \nsignificant figures. The GUM runtimes are presented as a percentage efficiency, i.e. the sequential runtime \ndivided by the GUM runtime. SINGLE-PROCESSOR EFFICIENCY Platform seq. seq-par par\u00ad par\u00ad runtime efficiency \nworst best Sun 4/15 43.2s 93~o 3570 92% SunMP 35.9s 90% 36% 92% The seq. runt i me column gives the \nelapsed or wall-clock runtime in seconds when the fully optimised sequential ver\u00adsion of the program \nis run under the standard (sequential) runtime system. The seq-par efficiency column of the table gives \nthe ef\u00adficiency when the sequential version of the program is run under GUM on a single processor. The \nGUM runtimes are elapsed time, but exclude the startup time because it is a small (e.g. 0.6s elapsed), \nfixed period independent of the program being run. The increased runtime shows that the overhead imposed \nby GUM on all code, including sequential, is in the region of 10~o. The par-worst column of the table \ngives the efficiency when the parallel version of the program is run under GUM on a single processor, \nwith the finest possible grain of paral\u00adlelism. That is, the cut-off is set to 1, and the program is \nforced to create a heap object to spark in every recursive call. The overheads are high, but it should \nbe remembered that parf act is very much a worst case. In most programs that do real work, there will \nalready be a heap object at most of the spark sites and the cost of the sparks will be quite low. The \npar-best column of the table gives the efficiency when the parallel version of the program is run under \nGUM on a single processor, with an 1good grain of parallelism. In the next section we discover that choosing \na cut-of value of 8192 produces a good thread-size for GUM on both architectures. The behaviour of parfact \nwith cut-off 8192 is more typical of parallel programs in that it creates some heap objects in order \nto spark them, but also has large sections of sequential code. 600 t 10 1000 1Coooo 10000000 Cut-orf \nFigure 3: parfact speedups on Ethernetted Sun 4/15:; 2J . #.+.. +..+..+.-+.+..+..+..+..-+..+..+..+-\u00ad \n*.. ,,< ..:% ..+/ 1, o~ 10 1000 100000 10000000 cut-off Figure4: parfact speedups on SunMP  3.2 Granularity \ninvestigation In the following experiment we investigate the minimum acc\u00adeptable grain-size for two dKferent \narchitectures, again uE\u00ading parf act. Figure 3 shows the absolute speedups obtained for parf act with \ndifferent cut-off values and different numb\u00aders of processors on the network of Sun 4/ 15s. Figure 4 \nshows results from the same experiments run on the Sun multiprocessor. The speedups shown in these figures \nare median speedups obtained over 4 runs. The maximum speedup curves (not given here) are smooth, but \nthe median curves are not smooth because (1) the net work and the processors were lightly-loaded, but \nthere was no way of preventing other peo\u00adple using them while the experiments were being run; and (2) \nthe load distribution is sometimes poor because PEs fish for work at random, and without thread-migration \none pro\u00adcessor may end up with a number of runnable threads while another has none, The peak speedup \nachieved on the SunMP with 6 proces\u00ad sors was 5.1, at a cut-off value of 128. For the Ethernet\u00ad ted Sun \n4/15s, the peak speedup with 6 processors was 4.4, at a cut-off value of 8192. The thread size, or granularity, \n 1 . A 300 ,/\u00ad ,,. - ;.. -, .-,., 200 m,-\u00ad., . 100 V owl1 I 2 68 Num4bar.10,...,s.,, Figure 5: Linearity \nof parf act speedups corresponding to a cut-off value of 8192 is about 45ms for the Ethernet ted SparcClassic \nsystem. For the SunMP, the thread size corresponding to a cut-off value of 128 is about 0.6ms. Since \nat both these cut-off values there are still po\u00adtentially thousands of parallel threads, this is a reasonable \nindication of the finest grain size that can be tolerated by each platform. For both machines, a good \nthread size is independent of the number of processors. Furthermore, a good speedup is not dependent \non getting exactly the right thread size: good speedups are achieved for cut-off values between 128 and \n8192. 3.3 Multiprocessor Efficiency The results from Section 3.2 assure us that GUM is not grossly inefficient \non a single processor. With multiple PEs, what are the overheads of communication, scheduling mul\u00adtiple \nthreads and managing global data? The linearity of speedup as more processors are added gives a good \nmeasure of the parallel efficiency of an implementation. Figure 5 plots the speedups obtained for the \nparf act program with good grannfarity on both architectures, i.e. using a cut-off of 8192. As expected, \nefficiency falls with more processors because of greater communication and increased volumes of globaf \ndata. Because of the high communication cost, the ethernet\u00adted Sun 4/15s lose efficiency more quickly \nthan the SunMp. With six processors efficiency in the Sun 4/15 network has fallen from 92% to 73%, and \nin the SunMp from 92% to 85~o. 3.4 The effect of packet size To investigate the effect of packet size \nwe use a program that generates a list of integers on one processor, and consumes the list (summing it) \non another. Figure 6 shows the abso\u00adlute runtimes for bulktest when run on a pair of Sun 4/15 workstations \nconnected to the same segment of Ethernet. The x-axis shows varying packet sizes, while the multiple \nwith 12 processors. 1r)oooooo ~; z 256 Q-\u00ad10 24 .. ..-. 8192 ---\u00ad 16384 ---\u00ad 32766 * ,. 60000 -+ I I10 \nI 10 1000 100000 Packet size (words) Figure 6: bulkt est rzzntimes on Ethernetted Sun 4/15s plots are \nfor different list lengths, as set by a command-line argument. We make the following observations on \nthe results. The time required to communicate very long lists (in excess of 8000 elements) is predictable, \nand reduces as the packet size in\u00adcreases. The time required to communicant e short lists (less than \n8000 elements) is chaotic, but nevertheless quite small; this is probably due to the random nature of \nthe Ethernet. Most of the benefit of bulk fetching is achieved with packet sizes of about 4K words. Larger \npacket sizes improve per\u00adformance sli~htlv for this ex~eriment. but for more realistic . programs they \nmay prove detrimental. This result is in close agreement wit h the PVM example program timing. 4 Related \nWork There has been much work on parallel functional program\u00adming. Hammond [8] provides a historical \noverview and in\u00adtroduces the principal approaches that have been taken at both the language and implementation \nlevels. This sec\u00adtion describes the implementations that are most closely related to GUM: those based \non compiled graph reduction for non-strict purely-functional languages. Less closely re\u00adlated are the \nvariety of implementations for strict functional languages, including Lisp derivatives such as Qlisp \n[7] or Mu1-T [12], and dataflow languages such as Sisal [16]. 4.1 Shared-Memory Implementations Shared-memory \nimplementations of non-strict functional languages have been quite successful, often showing good relative \nspeedup for limited numbers of processors on sim\u00adple programs. One of the first successful implementations \nwas Buckwheat [6], which ran on the Encore Multimax. This used a fairly conventional stack-based implementation \nof compiled graph-reduction, with a single shared heap, and a two-level task queue, which aimed to reduce \nmemory con\u00adtention. To avoid duplicating work, each thunk was locked when it was entered, an expensive \noperation on a shared\u00admemory machine. Relative speedups of 6 to 10 were achieved The < v, G >-Machine \n[3] was based on the sequential Chalmers G-Machine compiler, and ran on a 16-processor Seauent Svmmetrv. \nThere was no stack. but instead thunks .. were built with enough space to hold all necessary argu\u00adments \nplus some local workspace for temporary variables. As with Buckwheat, a single shared heap was used, \nwith thunks locked on entry. Garbage collection was implemented using a global stop-and-copy policy. \nThe spark creation pol\u00adicy was similar to that for GUM, but only a single global spark pool was provided. \nThere are several obvious prob\u00adlems with this scheme: the global spark pool is a memory hot-spot; the \nlack of an explicit stack means cache locality is lost; and garbage collection requires int m--processor \nsynchro\u00adnisation. As a consequence absolute speedups achieved were a factor of 5 to 11 on a 16-processor \nconfiguration. Mat t son observed similar problems with a similar shared-memory im\u00adplementation of the \nSTG-Machine on a 64-processor BBN Butterfly [18]. The more recent GAML implementation is an attempt to \naddress some of the shortcomings of the original <v, G>-Machine. GAML introduces the notion of possibly \nshared nodes, which are the only nodes that must be locked. It also uses a linked list of stack chunks \nsimilar to those we use in GUM. Garbage collection is done in parallel, with all processors synchronizing \nfirst. Control of parallelism is bv load-based inliniruz which mav lead to starvation. and should be \nused only on programs that are coarse-grained or continuously generate parallelism. On the Sequent Balance, \nGAML achieves relative speedups of between 3.3 and 5.8 for small programs [17]. WYBERT [13] is based \non the FAST/FCG sequential com\u00adpiler, and runs on a 4-processor Motorola HYPERmodule. Rather than defining \na general primitive for parallelism, the implementation uses an explicit divide-and-conquer skele\u00adton. \nThis limits the programs that can be run to those suit\u00ading a single, albeit common, paradigm. The cost \nof locking is avoided entirely by ensuring that shared redexes cannot arise! This is achieved by eagerly \nevaluating all shared data before a task is created. A secondary advantage is that a task can perform \nindependent garbage collection since no remote processor can refer to any of its data. Relative speedups \nare fairly good: between 2.4 and 4 on 4 proces\u00adsors. 4.2 Distributed-Memory Implementations There have \nbeen several Transputer-based distributed\u00admemory implement ations, and a few on other architectures. \nAlfalfa was a distributed-memory implementation for the Intel iPSC, similar to, but predating Buckwheat \n[5], Unfor\u00adtunately, the communication overhead on this system was high, and performance results were \ndisappointing: relative speedups of around 4 to 8 being achieved for 32 processors. Like WYBERT, ZAPP \n[15] aims to implement only divide\u00adand-conquer parallelism, using an explicit fork-and-join skeleton. \nOnce generated, tasks can either be executed on the processor that generated them or stolen by a neighbori\u00adng \nprocessor. There is no task migration, so the program retains a high degree of locality. A simple bulk-fetching \nstrategy is implemented, with all necessary graph being ex\u00ad ported when the task that needs it is exported. \nPerfor\u00ad mance results on the transputer were impressive for the few programs that were tried, with relative \nspeedups generally improving as the problem size increased, up to 39.9 on 40 transputers for naive Fibonacci. \nThe HDG-Machine [11] uses a packet-based approach to memory allocation that is similar to that of the \n<v, G>-Machine, but with a distributed weighted reference-counting garbage collection scheme [14]. Task \ndistribution is similar to ZAPP. Only incremental fetching strategies were tested with this scheme, though \npresumably a bulk fetching strat\u00adegY would also be possible. Relative speedup for naive Fi\u00adbonacci was \n3.6 on 4 transputers. Concurrent Clean runs on transputers and networks of Mac\u00adint oshes [20]. Like GUM, \nit is stack-based, and uses tables of in-pointers to allow independent local garbage collec\u00adtion. A bulk \ngraph-fet thing algorithm is implemented, but in contrast to GUM, there is no limit on the size of graph \nthat will be sent, and graph is reduced to normal form before it is transferred. In contrast to the GUM \nfishing strategy, tasks are statically allocated to processors by means of an\u00adnotations. Relative speedups \nof 8.2 to 14.8 are reported for simple benchmarks on a 16-processor Transputer sys\u00adtem [10]. 4.3 GRIP \nGUM s design is a development of our earlier work on the GRIP multiprocessor [22]. GRIP smemory wasdivided \ninto fast unshared memory that was local to a PE, with sepa\u00adrate banks of globally addressed memory that \ncould be ac\u00adcessed through a fast packet-switched network. Objects were fetched from global memory singly \non demand rather than using GUM-style bulk fetching. While GUM no longer lhas two kinds of memory, we \nhave retained the two-level sep\u00adaration of local and global heap that permits each PE to garbage collect \nindependently. Another potential advantage is that purely local objects might be held in faster unshared \nmemory on a shared-memory machine. On GRIP, in addition to the spark pool on each PE, spe\u00adcial hardware \nmaintained a distributed global spark pool. GRIP s scheme had the advantage that PEs never processed \nFISH messages unnecessarily, but, because local memory was unshared, a sparked thunk could only be exported \nafter all graph reachable from it had also been exported. The work-stealing scheme used in GUM avoids \nthis problem. Relative speedups on GRIP were generally good, for exam\u00adple, a parallel ray tracer achieved \nspeedups of around 14 on 16 processors. 5 Discussion and Further Work We have described a portable, parallel \nimplementation of Haskell, built on the PVM communications har\u00adness. GUM is currently on public a-release \nwith ver\u00adsion 0.26 (and onwards) of the Glasgow Haskell colM\u00adpiler. Further information is available \non the internet at http://www.dcs.gla.ac.uk/fp/software/ghc.html. It is quite ambitious to target such \na variety of architectures, and it is not obvious that a single architectural model will suffice for \nall machines, even if we start from such a high\u00adlevel basis as parallel Haskell. We do however believe \nthat it is easier and more efficient to map a message-based pro\u00adtocol onto a shared-memory machine than \nto map a shared\u00admemory protocol onto a dist ribut cd-memory machine. A port of GUM to a CM5, a distribut \ncd-memory machine with 512 Spare processors, is nearing completion at Los Alamos National Laboratory. \nWe hope that experiments with GUM on shared-memory and distributed-memory ma\u00adchines will reveal how realistic \na single implementation is for both classes of architecture. The performance figures given here are indicative \nrather than conclusive. They show that we have not fallen into the trap of building a parallel system \nwhose performance is fundamentally slower by a large factor than the best unipro\u00adcessor compilation technology. \nThey do not, however, say much about whether real programs can readily be run with useful speedups. Indeed, \nwe believe that considerable work is required to tune the existing system. The development of GUM is \nbeing driven by users who want parallel Haskell to make their programs run faster. The two main users \nare a 30K-line natural language processor, and a user writing complex database queries. Both are in the \npreliminary stages of parallelising their applications. While we have initially targeted PVM because \nof its wide availability this is not a fixed decision and our implementa\u00adtion is designed to be easily \nre-targeted to other message\u00adpassing libraries such as MPI. Indeed the CM5 implementa\u00adtion uses the CMMD \nmessage-passing library native to the machine. We would lilce to measure the speedups can be ob\u00adtained \nby using machine-specific communication primitives, particularly on shared-memory machines. The GUM implementation \ncould be improved in many ways. The load management strategy could be made less naive. In the medium \nterm, the addition of multiple-packet messages and distributed garbage collection for cyclic graphs would \nincrease the number of programs that could be run, and thread migration would improve the ability of \nthe system to cope with arbitrarily partitioned programs. In the longer term, we plan to investigate \nadding speculative evaluation and support for explicit concurrent processes [23]. We hope that the public \navailability of the system will encourage oth\u00aders to join us in these developments. References [1] Arvind \nand Iannucci RA, Two Fundamental Issues in Multiprocessing , Proc DFVLR Conference on Paral\u00adlel Processing \nin Science and Engineering, Bonn-Bad Godesberg (June 1987). [2] Bevan DI, Distributed Garbage Collection \nusing Ref\u00aderence Counting , Proc PARLE, deBakker JW, Nij\u00adman L and Treleaven PC (eds), Eindhoven, Netherlands \n(June 1987), [3] Augustsson L, and Johnsson T, Parallel Graph Re\u00adduction with the <u, G>-Machine , Proc. \nFPCA 89, London, UK, (1989), pp. 202 213. [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14] [15] [16] \n[17] [18] Burton FW, and Rayward-Smith VJ, Worst Case Scheduling for Parallel Functional Programming \n, Journal of Functional Programming, 4(1), (January 1994), pp. 65 75. Goldberg B, and Hudak P, Alfalfa: \nDistributed Graph Reduction on a Hypercube Multiprocessor , Proc. Workshop on Graph Reduct20n, Fasel \nRMKJF (cd.), Santa F4, NM, Springer Verlag LNCS 279, (1986), pp. 94 113. Goldberg BF, Buckwheat: Graph \nReduction on a Shared Memory Multiprocessor , Proc. ACM Conf. on Lisp and Functional Programming, Snowbird, \nUtah, (1988), ppp. 40-51. Goldman R, and Gabriel RP, Qlisp: Parallel Process\u00ading in Lisp , 1131311 So~tware, \npp. 51 59, (1989). Hammond K, Parallel Functional Programming: an Introduction , Proc. PASCO 94, Linz, \nAustria, World Scientific, (September 1994), pp. 181-193. Hammond K, Mattson JS, Partridge AS, Peyton \nJones SL, and Trinder PW GUM: a portable Parallel Im\u00adplementation of Haskell , Proc 7th. IntL Workshop \norz Implementation of Functional Languages, Bastad, Swe\u00adden (September 1995). Kesseler M, iReducing Graph \nCopying Costs Time to Wrap it up , Proc. PASCO 94 First Intl. Symposium on Parallel Symbolic Computation, \nHagen-berg/Linz, Austria, World Scientific, (September 1994), pp, ~44_2~3 Kingdon H, Lester D, and Burn \nGL, [The HDG-Machine: a Highly Distributed Graph Reducer for a Transputer Network , The Computer Journal, \n34(4), (April 1991), pp. 290-302. Kranz DA, Halstead RH, and Mohr E, MuLT: a High-Performance Parallel \nLisp , Proc. PLDI 89, Portland, Oregon, (1989), pp. 81-90. Lzmgendoen K, Graph Reduction on Shared Memory \nMultiprocessors , PhD Thesis, University of Amster\u00addam, 1993. Lester D An Efficient Distributed Garbage \nCollection Algorithm , Proc. PA RLE 89, LNCS 365, Springer Verlag, (June 1989). McBurney DL, and Sleep \nMR, Transputer Based Ex\u00adperiments with the ZAPP Architecture , Proc. PARLE 87, LNCS 258/259, Springer \nVerlag, (1987), pp. 242\u00ad 259. McGraw J, Skedzielewski S, Allan S, Odehoeft R, Glauert JRW, Kirkham C, \nNoyce W, and Thomas R, SISAL: Streams and Iteration in a Single-Assignment Language: Reference Manual \nVersion 1.2 , Manual M\u00ad146, Rev. 1, Lawrence Livermore National Laboratory, (March 1985). Maranget L, \n{GAML: a Parallel Implementation of Lazy ML Proc. FPCA 91, Springer Verlag LNCS 523, p.lo2-123, (1991). \nMattson JS, An Effective Speculative Evaluation Tech\u00ad nique for Parallel Supercombmator Graph Reduction, \nPhD thesis, Dept. of Computer Science and Engineer\u00ad ing, University of California, San Diego, (1993). \n [19] Mohr E, Kranz DA, and Halstead RH, Lazy Task Cre\u00adation a Technique for Increasing the Granularity \nof Parallel Programs , IEEE Transactions on Parallel and Distributed Systems, 2(3), (July 1991), pp. \n264-280. [20] N6cker EGJMH, Smetsers JEW, van Eekelen MCJD and Plasmeijer MJ, Concurrent Clean , Proc. \nPARLE 91, Springer Verlag LNCS 505/506, pp. 202 220, (1991). [21] Oak Ridge National Laboratory, University \nof Ten\u00adnessee, Parallel Virtual Machine Reference Manual, Version 3.2 , (August 1993). [22] Peyton Jones \nSL, Clack C, Salkild J, High\u00ad performance parallel graph reduction , Proc PA RLE 89, Springer Verlag \nLNCS 365 (June 1989). [23] Peyton Jones SL, Gordon AD, and Finne SO, Concur\u00adrent Haskell , Proc. ACM \nSymposium on Principles of Programming Languages, St Petersburg Beach, Florida, (January 1996). [24] \nWatson P and Watson 1, An Efficient Garbage Col\u00adlection Scheme for Parallel Computer Architectures , \nProc PARLE, deBakker JW, Nijman L and Treleaven PC (eds), Eindhoven, Netherkmds (June 1987). 5.1 Appendix: \nParallel Factorial module Main (main) where import Parallel pfc :: Int -> Int -> Int -> Int pfcxyc Iy-x>c=fl \npar (f2 seq (fl+f2)) l~==y =x I otherwise = pf x m + pf (m+l) y where m = (x+y) div 2 fl=pfcxmc f2 \n=pfc (m+i) y c  pf:: Int -> Int -> Int pfxy Ix<y =pfxm+pf(m+l)y I otherwise = x where m = (x+y) div \n2  parfact x c =pfclxc main = getArgs exit ( \\ [al, a2] -> let x = fst (head (readDec al)) c = fst (head \n(readDec a2) ) in appendChan stdout (show (parfact x c)) exit done)   \n\t\t\t", "proc_id": "231379", "abstract": "GUM is a portable, parallel implementation of the Haskell functional language. Despite sustained research interest in parallel functional programming, GUM is one of the first such systems to be made publicly available.GUM is message-based, and portability is facilitated by using the PVM communications harness that is available on many multi-processors. As a result, GUM is available for both shared-memory (Sun SPARCserver multiprocessors) and distributed-memory (networks of workstations) architectures. The high message-latency of distributed machines is ameliorated by sending messages asynchronously, and by sending large packets of related data in each message.Initial performance figures demonstrate absolute speedups relative to the best sequential compiler technology. To improve the performance of a parallel Haskell program GUM provides tools for monitoring and visualising the behaviour of threads and of processors during execution.", "authors": [{"name": "P. W. Trinder", "author_profile_id": "81100457538", "affiliation": "Department of Computing Science, Glasgow University", "person_id": "PP39043677", "email_address": "", "orcid_id": ""}, {"name": "K. Hammond", "author_profile_id": "81100001518", "affiliation": "Department of Computing Science, Glasgow University", "person_id": "PP31080844", "email_address": "", "orcid_id": ""}, {"name": "J. S. Mattson", "author_profile_id": "81100491703", "affiliation": "Hewlett-Packard, California Language Laboratory and Department of Computing Science, Glasgow University", "person_id": "P125349", "email_address": "", "orcid_id": ""}, {"name": "A. S. Partridge", "author_profile_id": "81392601183", "affiliation": "Department of Computer Science, University of Tasmania and Department of Computing Science, Glasgow University", "person_id": "P7035", "email_address": "", "orcid_id": ""}, {"name": "S. L. Peyton Jones", "author_profile_id": "81100271851", "affiliation": "Department of Computing Science, Glasgow University", "person_id": "PP39077294", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/231379.231392", "year": "1996", "article_id": "231392", "conference": "PLDI", "title": "GUM: a portable parallel implementation of Haskell", "url": "http://dl.acm.org/citation.cfm?id=231392"}