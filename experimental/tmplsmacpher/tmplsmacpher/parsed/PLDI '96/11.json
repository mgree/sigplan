{"article_publication_date": "05-01-1996", "fulltext": "\n Practical Program Analysis Using General Purpose Logic Programming Systems A Case Study* Steven Dawsont \nC.R. Ramakrishnan David S. Warren dawson@csl .sri. com cram@cs. sunysb. edu warren@cs.sunysb .edu Department \nof Computer Science SUNY at Stony Brook Stony Brook, NY 11794-4400 Abstract Many analysis problems can \nbe cast in the form of evaluating minimal models of a logic program. Although such formula\u00adtions are \nappealing due to their simplicity and declarative\u00adness, they have not been widely used in practice because, \neither existing logic programming systems do not guarantee completeness, or those that do have been viewed \nas too inef\u00adficient for integration into a compiler. The objective of this paper is to re-examine this \nissue in the context of recent ad\u00advances in implementation technologies of logic programming systems. \nWe find that such declarative formulations can indeed be used in practical systems, when combined with \nthe ap\u00adpropriate tool for evaluation. We use ezisting formulations of analysis problems groundless analysis \nof logic pro\u00adgrams, and strictness analysis of functional programs in this case study, and the XSB system, \na table-baaed logic programming system, as the evaluation tool of choice. We give experimental evidence \nthat the resultant groundless and strictness analysis systems are practical in terms of both time and \nspace. In terms of implementation effort, the an\u00adalyzers took less than 2 man-weeks (in total), to develop, \noptimize and evaluate. The analyzer itself consists of about 100 lines of tabled Prolog code and the \nentire system, includ\u00ading the components to read and preprocess input programs and to collect the analysis \nresults, consists of about 500 lines of code. Introduction Modern compilers perform a variety of analyses \nto gather information about the input program for use in program op\u00adtimization. For instance, in imperative \nlanguages, inter-as well as intra-procedural dat aflow analysis techniques col\u00adlect information about \nvariable usage, which is then used to optimize storage allocation (see [1]); abstract interpreta\u00ad This \nwork was supported in part by NSF grants CCR-9404921, CDA-9303181, CDA-9504275 and INT-9314412. t Current \nAddress: SRI International, 333 Ravens wood Ave., Menlo Park, CA 94025-3493. Permissionto nleks digNaWardcopy \nof part or all of this work for personal or daasroom w ia granted without fee provided that copies are \nnot made or distributed for profit or commercial advantage, the copyright notice, the title of the ublication \nand its date appear, and noti~ is given that copyingis typerrniseionofACM, Inc.Tocopyotherwise,torepublish,to \npostonservers,ortoredistributetoIisk, requirespriorspecificpermission andor a fee. PLDI 9S 5/96 PA, USA \nO 1996 ACM o-89791 -795-XWO005...$3,5O tion techniques (e.g., [12]) detect loop invariants, optimize \narray bounds checking and promote code hoisting. In func\u00adtional programming, a large number of analyses \nhave been recognized as useful: strictness analysis [26], binding time analysis [20] and update analysis \n[19], to name just a few. Compiler writers for logic languages have long realized the importance of information \nsuch as groundless, freeness and types for performing routine aa well as sophisticated opti\u00admization \n[13, 41]. Many of these analyses can be cast in the form of evaluating the minimal model of a (constraint \n) logic program (see e.g., [39, 9]). Although such formulations are appealing due to their simplicity \nand declarativeness, they have seldom been used in practice. The main reason for this gap is that, until \nvery recently, there have been few logic programming tools that offered the two fundamental ingre\u00addients \nneeded to make such formulations usable in a practi\u00adcal compiler: completeness (the ability to find the \nminimal model whenever one exists) and efficiency. Lacking this, such formulations have been deemed impractical. \nRecent developments in the area of logic program imple\u00admentation have made available a few systems that \nguaran\u00adtee completeness while offering good performance. Efficient systems such as Coral [29], which \nare based on complete (bottom-up) evaluation strategies, have emerged from the deductive database community. \nThe performance of these systems has been considerably improved upon by the XSB system [36], XSB combines \nthe benefits of the WAM tech\u00adnology [2] a well-optimized engine developed primarily for Prolog program \nexecution with the completeness gnar\u00adant eed by the use of extension tables. Although resolution methods \nsuch as OLDT [38], and SLG [7] (on which XSB is based) have been known for some time, it is only now \nthat the technology is mature enough to offer an implementation with good raw performance [35]. Availability \ny of such sys\u00adtems forces one to re-evaluate the question: can one obtain practical implementations of \nprogram analysis from a logical formulation with minimal programming effort ? We find that it is indeed \npossible to use declarative for\u00admulations in practical systems, when combined judiciously with the appropriate \ntools of evaluation. In this paper, we describe the implementation of logical formulations of two existing \nanalysis techniques: groundless analysis of logic programs using the Prop-domain [23] and strictness \nanal\u00adysis of functional programs based on demand propagation [37]. We use the XSB system, currently the \nfastest system available that guarantees completeness, as the evaluation tool. 117 Our experiments, \nresults of which are presented here, provide evidence of the practicality of the resulting ground\u00adless \nand strictness analysis systems: the total times for anal\u00adysis (including preprocessing, evaluation and \nresult collec\u00adtion) are significantly less than the compilation time. More\u00adover, the groundless analyzer \nis competitive with the fastest known implementation of the same analysis constructed us\u00ading special-purpose \nanalysis tools. It should, however, be noted that to obtain efficient analyzers, the logical rules were \ncoded so as to take advantage of the underlying evaluation mechanism. The techniques used to encode the \nrules, al\u00adthough relatively straightforward, are noteworthy since they often result in significant performance \ngains. In both groundless and strictness analyses, we find that preprocessing dominates the cost of analysis. \nMost of the published literature on program analysis ignores preprocess\u00ading costs, but our experiments \nsuggest that the evaluation times are small enough that reducing the preprocessing times is a problem \nof significant practical import ante. Apart from analysis time, the analyzers have very low space require\u00adments \n an important metric that impacts the practicality of the system. In terms of programming effort, the \nanalyzers took less than 2 man-weeks to build. The entire system con\u00adsists of about 500 lines of tabled \nProlog code, with the code for the analysis phase accounting for less than 100 lines. It is very encouraging \nto find that an easily verifiable logical formulation is no longer confined to being just a prototype \nbut can be directly used to build a practical system. AS of the time of this writing, we have but prelimin=y \nexperiment al evidence about the practicality y of perform\u00ading dat aflow analysis of imperative programs \nusing a gen\u00aderal purpose logic programming system. The shape anal\u00adysis for programs with destructive \nupdates [33] has been implemented in XSB very recently, and exhibits good per\u00adformance. However, the \npracticality of this implementation (in terms of how it compares to special-purpose dataflow analyzers) \nremains to be established. Nevertheless, the fig\u00adures in [31] indicating the relative performance of \ndat aflow analysis implemented using Coral with respect to that in C, and the relative performance of \nshape analysis in Coral and XSB lead us to believe that the practicality results will carry over to datailow \nanalysis of imperative programs as well. We discuss this issue at greater length in Section 7. All analyses \nmodeled in this paper are performed over finite abstract domains. Analysis over infinite domains will \nrequire support for on-the-fly approximation operations such as widening [12]. Moreover, the system used \nfor this case study, XSB, currently handles term-equality constraints only. The mechanisms needed to \nmake the implementations of analyses based on infinite domains and/or other constraint domains as straightforward \nand efficient as those reported in this paper are discussed in Section 6. The rest of the paper is organized \nas follows. In the next section we discuss the advantages of logic-based for\u00ad mulation of program analysis \nand provide a brief overview of tabled evaluation and its implementation. The formula\u00ad tion of groundless \nand strictness analyses forms the topic of Section 3. In Section 4 we describe the implementation and \nperformance of the analyses. In Section 5 we discuss the implementation of analyses using non-enumerative \nrep\u00ad resentations, to illustrate the power of combining tabling with meta-programming. Optimizations \nand extensions to the system are described in Section 6. We compare this work with current literature \nin Section 7. Concluding re\u00ad marks appear in Section 8. 2 Background Analysis methods are usually described \nin terms of seman\u00adtic equations whose fix point represents the program prop\u00aderty under consideration. \nThese semantic equations can be viewed as a (constraint) logic program, where each equation is translated \ninto a Horn clause. Many program analysis methods have been explicitly formulated as logic programs. \nFor instance, a formulation of flow analysis problems as a logic program is presented in [39, pages 984 \n987]. Not sur\u00adprisingly, there have been many proposals to formulate logic program analysis in this form \n(see, e.g., [9]); some carry this approach a step further, to the level of implementation (e.g., [8]). \nThe main advantage of this approach is that it sepa\u00adrates the concepts of formulation from the implementation \ndetails, yielding the following benefits: The logic program is a straightforward representation of the \nabstract semantic equations. Hence, once the soundness of the analysis is established (at the level of \nthe semantic equations), and given a sound and com\u00adplete implementation to evaluate the logic program, \nthe overall correctness of the analysis is readily estab\u00adlished.  The interface bet ween the formulation \nand its imple\u00admentation is a well-established language. This allows the implementor to choose any complete \nlogic pro\u00adgramming system as the evaluation engine.  Active research in the logic programming area to \nen\u00adhance the power and performance of thes; systems im\u00admediately benefits the implement ation of analyses. \n There are two strategies for complete evaluation of logic programs that are available in existing systems: \nbottom-up evaluation (used, e.g., in Coral), and top-down tabled eval\u00aduation (used, e.g., in XSB). Below, \nwe briefly review the mechanism of tabled evaluation, the evaluation method used in this case study. \nFor an exposition of bottom-up analysis methods see [39]. Tabled Evaluation At a high level, a top-down, \ntabled eval\u00aduation engine evaluates programs by recording subgoals (re\u00adferred to as calls) and their \nprovable instances (referred to as am wers ) in a table. Predicates may be marked as either tabled or \nnon tabled. A program is evaluated as follows. For nont abled predicates, the subgoal is resolved against \npro\u00adgram clauses. For tabled predicates, if the subgoal is already present in the table, then it is resolved \nagainst the answers recorded in the table; otherwise the subgoal is entered in the table and its answers, \ncomputed by resolving the sub\u00adgoal against program clauses are also entered in the tablel. The answer \nentries are associated with the corresponding subgoal entries. For both tabled and nontabled predicates, \nprogram clause resolution is carried out using SLD. The XSB System The tabled evaluation engine of XSB \nsystem provides an efficient fix-point algorithm that t ermi\u00adnates for finite domains. This means that \nthe system can be directly used to compute fix points for Galois-connection lIn XSB, the presence of \na subgoal in the table is tested in the engine by searching for a variant of the subgoal in the table. \nTwo terms tland t2are variants of each other if they are identical up to variable nammg. Only umque answers \nare entered in the table, and duplicates are filtered out using variant checks. 118 based analyses \nwhere all approximations are performed a priori. Moreover, being a full Prolog system, XSB permits metaprogramming. \nThis facility can be used to perform ap\u00adproximations during the course of analysis a feature that is \nnecessary for implementing infinite domain analyses. The XSB system permits dynamic compilation, mainly \nin the form of assert. The primitives for dynamic compi\u00adlation in XSB are, in general, faster than the \ncorresponding primitives provided by other Prolog systems such as SICS\u00adtus and Quintus Prolog. Dynamic \ncompilation is an impor\u00adtant feature affecting the practicality of analyzers built using XSB, since it \nresults in much lower preprocessing overheads compared to full compilation, while adding little to the \neval\u00aduation time. The preprocessing times, which dominate the total analysis times, are low enough that \nanalyzers built us\u00ading dynamic compilation are significantly fast er than their fully compiled counterparts. \n3 Formulation of Finite-domain Analyses In this section, we describe the formulation of two simple tinite \ndomain analyses groundless analysis of logic pro\u00adgrams based on the Prop-domain [23] and strictness \nanalysis of fictional programs based on demand propagation [37]. 3.1 Groundless Analysis using the Prop-domain \nThe Propdomain [23] is a simple, yet effective abstract do\u00admain to compute groundless properties of logic \nprograms. The substitutions in the concrete program are represented in the l+o~domain by boolean formulae \nover the variables in the substitution, with the connective A, V and ~. For instance, a concrete substitution \n{X + t} is mapped to the formula X % (YI A YZ A ...Y~), where {Y1, Y2,..., Y~} is the set of variables \nin t.Substitutions over multiple vari\u00adables and sets of substitutions are represented by conjunc\u00adtion \nand disjunction of formulae representing individual sub\u00adstitutions. This domain has been used in several \nprevious implementations (e.g., [8, 40]) and has been shown to yield accurate results for both offline \nand online analyses. Following [8], we represent the boolean formulae by their truth tables. For instance, \nXl @ X2 A Xa is repre\u00adsented by a predicate iff (Xl, X2 j X3) whose success set is {(true, true, true) \n~ (false, false, true), (false, true, false), (false, false, false)}. Disjunction and conjunction are \nsimply the union and join of the success sets. This leads to a simple transformation, given in Fig\u00adure \n1, which maps a given logic program P to an abstract program P that computes the groundless properties \nof the predicates in P. The transformation is such that the out\u00adput .groundness of a predicate p in P \nis represented by the success set of the corresponding predicate gpP in P . Each variable X in the source \nprogram is associated with a unique variable TX in the target program. An example program, append, is \ngiven in Figure 2a and the corresponding abstract program is given in Figure 2b. Based on the definition \nof if f, it is easy to verify that the success set of gp-ap(X, Y,Z) is { (true, true, true) , (true, \nfalse, false) , (false, true, false) , (false, false, false) }, the truth table for the formula XAY + \nZ which represents the output groundless of ap. Input and Output Groundless The translation rules in \nF@re 1 can be used directly to obtain a program P whose minimal model computes the output groundless \nproperties of a given program P. In order to obtain the inputground\u00adless, we can deiiue a second transformation \nbased on the Magic Sets transformation [3, 34] (such as the one given in [8]) such that the minimal model \nof the resultant program P represents the input groundless of the given program P. However, table-driven \nmethods, such as OLDT and SLG, record all the subaoals (calls) encountered durimz evaluation. .J, , \nas well as their answers (returns). If an implementation of these resolution methods selects the literals \nin left-to-right order, the calls of P capture the input groundless of P. Since the calls are anyway \nrecorded, we do not have to pay an additional price for obtaining input modes. This prop\u00adert y is exploited \nby engines designed for top-down abstract interpretation, such as GAIA [22], and can also be readily \nexploited by evaluating P using the XSB system. 3.2 Strictness Analysis based on demand propagation \nA function ~ is said to be strict in its i-th argument iff eval\u00aduation of $(el, . . . , ei, . . . . en) \nfails to terminate whenever evaluation of e, is nonterminating. It follows that $ is strict in its i-th \nargument iff e, needs to be evaluated in every terminating application ~(el, . . . . e,, . . . . en) \nof ~. Since ex\u00adpressions in a lazy functional program are evaluated only if necessary, the process of \nevaluation can be considered as a form of demand flow. When an application of ~ needs to be evaluated, \nwe say that there is a demand on the output of j; if $ is strict in its z-th argument, then ~ transforms \na de\u00admand on its output to a demand on its i-th argument. We encode the strictness equations from [37], \nconsidering the following demand extents: normal form demand denoted by e, head normal form demand denoted \nby d and null demand denoted by n.2 For each function ~ in the input program, we derive a predicate Spf \nthat models the propagation of demand by an application of f. For a function ~(x) we derive a predi\u00adcate \nsp (D, X) such that the substitutions of X represent the deman d s on the variable x whenever the demand \non the output of an application of ~ is D. For instance, the strict\u00adness of the list building operator \n: may be defined using a predicate sp-cons (D, X, Y) such that sp.cons (e, X, Y) succeeds only with X \n= e and Y = e; sp-cons (d, X, Y) and sp_cons (n, X, Y) succeed for any values of X and Y. For each user-defined \nfunction ~, we derive a clause defin\u00ading SP ~ corresponding to each equation defining j, based on the \ndemand propagation properties of the two basic con\u00adstructs: function composition (application) that is \nused to define expressions, and pattern matching that guides the se\u00adlection of the appropriate equation. \nLet ~(g(z)) be a func\u00ad tion application on the rhs of some equation. The strict\u00adness property of this \napplication is given by the conjunction Spt (D, D1 ) , Spg (Dl, X), where D represents the demand on \n~(g(x) ), X represents the demand on x and D1 represents the demand placed on g(x) by the application \nof ~. The demands on the variables of a rhs expression are transferred to the demands on the arguments \nof the lhs by interpreting the pattern matching operation. For instance, consider a position on the lhs \nwith a pattern x : ZS. If the demand flow is such that the evaluation of the rhs expression places an \ne-demand on both z and XS, then we can conclude that the evaluation of this equation places an e-demand \non x : XS. 2It should be noted that the strictness analysis of [37] used here generalizes Mycroft s strictness \nanalysis [26] to non-flat domains. 119 P ~(tl, t2,..., tn)c]c] + gpP(Xl, X2,..., Xn) :\u00ad &#38; [tl] XI, \n~ [tz] x2,..., ~ [t~]Xn,L[c] D. f[t] a-+ iff (a, al, cw,....ak ) where{ al, 042, . . . , a,}= Vars(t) \n z [1,,12] -+ L [11], z [12] L [q(tl, t2, . . ..tk)] + let {a,, a,,... }ak} + GetNewVariableso in s \n[tl]cl ~,Crz, ..., [tk] O k, &#38; [t2] s gpq(al,crz, . . ..ak) z[x=t] + s [t]7X Figure 1: Formulation \nof Groundless Analysis gp-ap(Xl ,X2,X3) iff(Xl) , ap([l, Ys, ys). iff (X2 ,X3) . ap([XIXsl, Ys, [X IZsl) \n:-gp-ap(Xl ,X2,X3) :- ap(Xs, Ys, Zs). iff (Xl ,X,XS) , iff(X3, X,Zs) , gp-ap(Xs ,X2 ,Zs) . (a) (b) Figure \n2: The ap program (a), and its abstraction gp-ap (b). The effect of mat thing an input expression with \nthe pattern evaluation extents flow bot t em-up through the pattern in z : xs can be described by using \na predicate pm-cons (D, X, the lhs. The order of literals encodes this flow information, Xs ) such that \npm-cons (D, e, e) succeeds only with D = e, and significantly improves the eficiency of the resultant \npro\u00adwhile pm-cons (D, X, Xs ) succeeds with D = d whenever ei-gram by reducing backtracking. ther X or \nXs is not bound to e. The construction of strictness predicates from an input 4 Implementation and Experimental \nResults functional program is given in Figure 3. As in Figure 1, each variable z in the source program \nis associated with a Once the Horn clauses have been generated according to unique variable I-Z in the \ntarget program. In addition to the the analysis formulation of the previous section, they can Horn clauses \ngenerated by the rules in Figure 3, for each be evaluated directly to produce the results of the analysis \nuser-deiined function symbol ~ we derive one clause spf (n, by any logic programming system that guarantees \ncomplete\u00ad xl, x2, . . . . h) . to handle the propagation of n-demand ness. The straightforward approach \nis to compile the clauses that arises due to non-strictness of functions. just as any other logic program. \nHowever, for practical anal-The strictness predicates thus derived for the example ysis, we must consider \nall of the costs involved, including the program in Figure 4a is given in Figure 4b. The query time required \nto prepare the rules for evaluation. We assess sp.ap (e, X, Y) has only one solution, with X = e and \nY the performance of the analysis implementations using the = e, indicating that the function ap is ee-strict \nin both its following metrics corresponding to the different phases of arguments. On the other hand, \nsp-ap (d, X, Y) has two analysis. solutions, {X = e, Y = d} and {X = d, Y = n}, indicating that up is \nold-strict in the first argument, but not in the second Preprocessing time The total time required to \nprepare argument. the source program for analysis, including the time required to transform the program \ninto the logical rules to be evaluated and the time to compile the logical Efficiency Issues In the \nformulation of strictness, we have rules for evaluation (e.g., full compilation into WAM treated the \noutput program as a definite logic program, and code, or ssserting the rules as dynamic code). correctness \nwas based on the minimal-model semantics. In essence, the order of the lit erals in the rhs of clauses, \nas well Analysis time The total time required to evaluate the log\u00ad as the order of clauses themselves, \nis irrelevant to the sound\u00adical rules to yield their minimal model. ness of the analysis. However, note \nin Figure 3 that, apart from the fact that &#38; generates sp predicates while P gener-Collection time \nThe time required to extract the results ates pm predicates, ~ and P differ only in the order of the \nof the analysis. literals in the output conjunction. This follows from the ob\u00ad 3In a goal-oriented set-at-s-time \nevaluation (ss in bottom-up eval\u00ad servation that while the demand flows top-down through the uation \nwith Magic Sets), the reduction in backtracking corresponds rhs expressions (i. e., from an expression \nto its components), to reduction in the size of the joins, 120 SPf(D, xl, . . .. Xn). &#38; [e] D, \nP [tl]Xl, P [tz]X2,..., P [tn] Xn. let {a,, cY2,..., CV1} + GetNew Variubleso in spf(cY,al,cY2, . . . \n,ffl), 8 [cl] CVI, t ~e2_j CV2,..,, &#38; [e~l CYJ t[z] a-+ 7==0 P [c(tl, t2,.,., tk)] c1 + let {crl, \na2,... , CYk} + GetNewVariableso in P [tl] al, P [t2] Crz,..., P [tk] CYk, prnC(a,w,cz2, .,. ,ak) T.=CY \n Figure 3: Formulation of Strictness Analysis sp-ap(D, Xi, X2) :-Tys =D, pm-nil (Xl), Tys = X2. ap(nil, \ngs) = gs sp_ap(D, Xl, X2) :\u00ad ap(z :zs, ys) = x :ap(xs, gs) sp-cons(D,Dl,D2), Tx = Dl, sp-ap(D2,Txs ,Tys), \npm-cons(Xl, Tx, Txs), Tys = X2.  sp-ap(n,Xl,X2) . (a) (b) Figure4: The apperad program: (a) Concrete \nprogram, and (b) Abstract program The total of the three times listed above is the overall anal\u00adysis \ntime, and this will betheprimary indicator of theprac\u00adticality of our analysis implementations. When \nall of the above costs are taken into account, it is not clear that full compilation of the rules for \nevaluation is the best approach. An alternative is to compile the rules dynamically (assert) andinterpret \nthem (through call/iin Prolog). Although it may not be obvious at first, the lat\u00ad ter method turns out \nto be the better choice. As will be shown in Sections 4.1 and 4.2, preprocessing time is gener\u00ad ally \nmuch greater than analysis (evaluation) time, so that keeping preprocessing time as low as possible \nis critical to good overall analysis times. When reconsider that theolo\u00adgical rules generated for logic \nprogram analysis resemble the original logic program, it becomes apparent that compiling those rules \nfor fastest evaluation may itself take as much time (or even more, when initial transformation time is \nin\u00adcluded) decompiling the original program itself. By loading the analysis rules as dynamic code, preprocessing \ntime is reduced substantially, at some cost in evaluation time due to the overhead of interpretation \n(call\\ l). However, even using this interpretation approach, the evaluation times we observe are generally \nlow compared to preprocessing time. The last performance metric, collection time, is inde\u00adpendent of \nthe evaluation approach. For either full com\u00adpilation or interpretation the calls occurring during evalua\u00adtion \nand the computed returns are stored in a table using the same tabling mechanisms. When the analysis phase \nis completed, there maybe multiple calls and/or answers that must be combined into unique analysis results. \nFor exam\u00adple, in the Prop formulation of groundless analysis, if the return table for some predicate \np/3 contained two answers p(true ,false, true) and p(true, true, false), they would be combined into \nthe single result p (true, f alse, f alse). At first glance, our use of an enumerative representa\u00adtion \nof boolean formulae may seem to be inefficient. Many implement ations [10, 40] use Bryant s Decision \nDiagrams (BDDs) [6] to represent boolean formulae compactly. How\u00adever, experimental results show that \nour analysis times are very competitive. This effect is due to the underlying en\u00adgine which computes \nfix points incrementally: computing in one iteration using the change in results (delta-sets, in de\u00ad \nductive database terms) from the previous iteration, The apparently inefficient represent ation we use \nactually allows for efficient computation of the delta-sets. 4.1 Performance of Groundless Analysis \nTable 1 shows performance measurements for Prop-based groundless analysis in XSB on a set of benchmarks \nfrom [40]4. The column labeled Compile time increase shows the ratio of total analysis time to total \ncompilation time (with no analysis) in XSB, and indicates the increase in 4Measurements were taken on \na Sun SPARCSt at ion 10/30, with 64M memory, running SunOS version 4.1.3 and XSB version 1.4.2, The machine \nconfiguration was chosen to match that used by [40], 121  Program Program size Time sec.) Compile time \nTable space (lines) Preproc. Analysm ColIectlon Total increase (70) (bytes) Cs 182 0.31 0.11 0.15 0.57 \n22.1 8056 Disj 172 0.27 0.03 0.10 0.40 26.9 5768 Gabriel 122 0.20 0.05 0.11 0.36 43.6 6912 Kalah 278 \n0.48 0.06 0.23 0.77 37.4 10580 Peep 369 0.84 0.16 0.09 1.09 23.4 5800 PG 53 0.10 0.01 0.02 0.13 31.0 \n2332 Plan 84 0.14 0.01 0.03 0.18 30.8 2888 Pressl 349 0.62 0.38 0.82 1.82 59.5 29400 Press2 351 0.60 \n0.41 0.83 1.84 60.7 29400 QSort 21 0.04 0.00 0.01 0.05 33.3 916 Queens 33 0.04 0.00 0.01 0.05 27.8 976 \nRead 443 0.72 0.60 0.70 2.02 64.4 26528 Table 1: Performance of Prop-based groundless analysis in XSB \nSystem ] CS I DLs.j I Gab. \\ Kalah I Peep I PG I Plan I Pressl I Press2 I QSort I Queens I Read XSB \nI0.57 I 0.40 I 0.36 \\ 0.77 I 1.09 I0.13 I I GAIA I 1.34 ] 1.01 0.47 I 0.93 \\ 1.16 I 0.16 I Table 2: Comparison \nof XSB and GAIA compilation time that could be expected if groundless anal\u00adysis were included as a phase \nof compilation. In all cases, total analysis time is less than compilation time, indicating that simple, \nhigh-level analysis implementations can indeed be practical. The table space used during analysis is \ngiven in the last column, and affirms the practicality y of the analysis. Table 2 compares the total \nanalysis time for XSB with analysis times reported for GAIA [40] on the same bench\u00admarks. It should be \nnoted that the times given for XSB include the time needed to extract all results from the in\u00adt ernal \nrepresentation (the tables), while those for GAIA do not. The results obtained on the two systems are \nidenti\u00adcal, since they implement the same analysis. It is indeed encouraging to note that our high-level \nimplementation in a general purpose system not only performs well enough to be practical, but compares \nvery well with a fast, highly optimized C-based system designed specifically for abstract interpret ation. \n 4.2 Performance of Strictness Analysis Table 3 reports the performance of strictness analysis in XSB5. \nThe programs were taken from the benchmarks for EQUALS [21], and include those translated from the bench\u00admarks \nin [16]. The preprocessing times include the time to compute and output the logic rules for strictness \nfrom EQUALS programs and to read and load these rules into the XSB system. Thus the total time represents \nthe increase in compilation time due to strictness analysis. The analyzer processes about 200 to 350 \nsource lines per second. The to\u00adtal time to perform the analysis is about 570 of time taken by ghc, the \nGlasgow Haskell Compiler, to compile (without analysis or optimization) an equivalent program written \nin Haskell. The speed of the analysis and its space behavior provide strong evidence of its practicality. \nObserve from the table that the preprocessing times once again dominate the total analysis times, except \nfor the pro\u00adgram pcprove. The analysis times are higher for pcprove 5Measurements were taken on a Sun \nSPARC LX with 64M mem\u00adory, running SunOS version 5.4 and XSB version 1.4,2, 0.18 I 1.82 I 1.84 1 0.05 \nI 0.05 2.02 0.12 I 5.96 I 6.03 I 0.05 I 0.04 1 1.66 1 for Prop-based groundless analysis mainly due to \na characteristic of the formulation (see Fig\u00adure 3): deep nesting of function applications result in \nex\u00adcessively long Horn clauses. This, in conjunction with the enumerative definition of base functions, \nleads to deep back\u00adtracking. However, that the occurrence of demand variables is highly localized means \nthat tabling intermediate results (thereby eliminating the existentially quantified demand variables) \nwill reduce backtracking, thereby potentially im\u00adproving analysis times. Such an optimization, called \nsupple\u00adrn entarg magic sets [4] is performed in deductive databases, and XSB offers an analogous (compile-time) \noptimization called supplementary tabling. However, the effectiveness of this optimization in reducing \nanalysis time remains to be established. 5 Beyond Enumerative Analyses The underlying representation \nused in the implementations of groundless and strictness analyses is enumerative. Al\u00adthough enumerative \nrepresentations may be efficient for small domains, performance may significantly degrade when the domain \nsizes increase. The results of [10] show that even when a particularly efficient enumerative representation, \nsuch as a BDD [6] is used, analysis times increase with increases in domain size. In contrast, analyses \nbased on non-enumerative represent ations such as symbolic constraints show little re\u00adduction in performance \ndue to increase in the size of the underlying domain [27]. In this section, we describe how such an analysis \ncan be implemented efficiently in XSB. The abstract domain is the set of all terms of depth k or less, \nconstructed using the function symbols that occur in the program, a special O-ary symbol -y and a countable \nset of variables. The symbol y is used to represent the set of all ground terms. The concretization function \nmaps each abstract term tto the set of concrete terms S, such that for each s 6 S, either (i) t is a \nvariable, or (ii) t= v and sis ground, or (iii) the roots of tand s are identical and each subt erm of \ns is a concretization of the corresponding sub\u00adterm in t. The abstract term can be viewed as a constraint, \nwith the symbol y representing a membership constraint 122 Program Program size Times sec. ) Table space \n(lines) X@EG-CollectIon IT Otal (bytes) eu 67 0.03 0.01 0.16 2852 event 384 0.67 0.63 0.08 1.38 22056 \nfft 343 0.63 0.19 0.06 0.88 15780 listcornpr 241 0.75 0.07 0.02 0.84 4688 mergesort 65 0.11 0.02 0.01 \n0.14 2332 nq 90 0.20 0.12 0.02 0.34 8912 odprove 160 0.39 0,17 0.02 0.58 3776 pcprove 595 1.01 1.60 0.10 \n2.71 25972 quicksort 70 0.10 0.03 0.01 0.14 2660 strassen 93 0.09 0.08 0.01 0.18 2760 1 Table 3: Performance \nof Strictness Analysis in XSB Program Time ,,cc.)70 Compile ~ ~ Xiajm CollectIon Tmr Time (bytes) Cs \n0.16 0.03 0.07 0.26 16 12988 Disj 0.14 0.03 0.06 0.23 23 9552 Kalah 0.24 0.05 0.11 0.40 29 17068 Peep \n0.44 0.08 0.05 0.57 18 12784 PG 0.05 0.01 0.02 0.08 29 4136 Plan 0.08 0.01 0.02 0.11 29 5324 QSort 0.02 \n0.01 0.02 0.05 56 1684 IJueens 0.03 0.00 0.01 0.04 33 1740 Read 0.36 0.25 0.43 1.04 50 52508 Table 4: \nPerformance of groundless analysis (constraining a term to the set of all g-round terms), and the other \nsymbois representing equality ~onstraints. Abstract unification unifies two abstract terms up to depth \nk, with -y unifying with any ground term. Note that abstract unification is different from the unification \nopera\u00adtion provided by the underlying engine; hence we have to implement abstract unification at a higher \nlevel. Table 4 shows performance measurements for groundless analysis with tem depth abstraction in XSB6 \non the set of bench\u00admarks used in Section 4.1. 6 Discussion 6.1 Analyses over Infinite Domains It should \nbe noted that the abstract domains used in all analyses described thus far are finite. Below, we describe \nstrategies to handle infinite domains, including those with infinite ascending chains. Widening Consider \nthe evaluation of the fix point of a func\u00adtional F, over a domain D. An iterative technique to find fix \npoints computes a sequence of ,iterates XO, Xl,... such that XO = l.~ and Xi+l = &#38; (X ). The iteration \nprocess terminates when X = Xi+l. If the domain D has infinite ascending chains, iteration may not terminate \neven when F is monotonic. For analysis over such domains, termination is ensured by using on-the-fly \napproximation operations such as widening [12]. Intuitively, we use a widening op\u00aderat or to accelerate \nthe convergence of fix-point iteration 6 Me=ur,ment, ~er~ t~ken cm a Sun SPARCSt ation Zo with 64M memory, \nrunning SunOS version 5.3 and XSB version 1.4.2. with term depth abstraction in XSB secruences by extrap \nelating the it erat es. The new iteration sequence is such that Xi;l = Xi V F(X ), where V is the widening \noperator. See [11] for a formal definition of widening operation and the requisite properties that ensure \ntermination of the iteration sequence. In the context of tabled evaluation, widening operations require \n(1) the knowledge of other returns already present in the table, and (2) a mechanism to modify any or \nall of the re\u00adturns in the table. Although this can be accomplished using existing low level system primitives, \na high level abstraction of widening will enable infinite domain analyses to be imple\u00admented as directly \nae finite domain analyses. Operations for aggregation over sets, provided by many deductive database \nsystems, have exactly the same requirements, and we believe that widening can be implemented along the \nsame lines. Constraints In a logic programming system, constraint op\u00aderations can be implemented using \nthe programming prim\u00aditives provided by the system. Thus, the programmability of the system not only \nprovides a way to control the ef\u00adficiency of the resulting analyses (see Section 3), but also permits \nimplementation of a larger class of analyses. A case in point is the groundless analysis using a constraint-baaed \nrepresentation, described in Section 5. It should be noted that the methods described in this paper \nare not restricted to analyses where the size of con\u00adstraints are fixed a priori. For instance, consider \nHindley-Milner type analysis [18, 24] for functional programs, where the type of functions in the input \nprogram is formulated as the solution to type equations, which are equations over the domain of equality \nconstraints, The type equations are nonrecursive, since any recursion in the input program is eliminated \nusing an explicit fix-point operator. Note that 123 tabled evaluation is not needed to solve the nonrecursive \ntype equations. Indeed, the only requirement is that occur\u00adcheck be performed by the unification operation. \nIn case the unification operation provided by the underlying engine does not meet this condition, unification \nwith occur-check can be easily programmed at a higher level. In fact, the ab\u00adstract unification operation \nof the depth-bounded constraint analysis of Section 5 performs occur-check. Thus, a straight\u00adforward \nimplementation of the logical formulation is not lim\u00adit ed to only finite-domain analyses. However, whether \nsuch an implementation of the Hindley-Milner type analysis can be practical remains to be seen.  6.2 \nImpact of Engine Optimization Answer Collection Using generic aggregation operations, such as those typically \nprovided by deductive database sys\u00adtems, answer collection can be implemented very simply and at a high \nlevel. Efficiency of these operations depends on the tabling primitives provided by the system, as well \nas the scheduling strate~v used to return answers to tabled predicates. W; are in~~stigating the impact \nof breadth-fist scheduling strategies on aggregation. It should be noted that such strategies are being \nsought mainly to improve the efficiency of the system for database programs [15]. This il\u00adlustrates one \nof the advantages in formulating analysis prob\u00adlems as logic programs: the analyzer can benefit directly \nfrom optimization to the evaluation techniques motivated by the application of the underlying system \nto problems in other areas. Bottom-up Analysis Top-down analyses benefit from a goal\u00addirected evaluation \nstrategy. This benefit is clear from the implementation: the tabling mechanisms enable the com\u00adputation \nof input and output modes in one analysis pass without requiring transformations such as Magic (e.g., \n[8]). However, such goal orientation may lead to inefficiencies in bottom-up analyses, especially when \nvariant checks are used for tabling. For instance, while evaluating logic programs derived for bottom-up \nanalysis, open calls (i.e., calls of the form P(XI, X2 ), where Xl and X2 are variables) are even\u00adtually \nmade. Moreover, many particular calls, (i. e., of the form p(tl, tz ) where tlor .tz are non-variables) \nmaybe gener\u00adated. For each predicate in the program, two cases naturally arise: (1) the open call is \nencountered before any particular call (forward subsurnption), and (2) some particular calls are encountered \nbefore the open call ( ba ckzsa rd subsurnp\u00ad tion). In a tabling system based on variant checks, answers \nfor particular calls are recomputed instead of reusing the answers for the open call. Recently, we completed \nthe implementation of an engine that exploits forward subsumption at a relatively low cost [30]. At present, \nit is unclear whether backward subsump\u00adtion can be exploited as effectively. Nevertheless, bottom-up \ncomputations can be performed efficiently by using a simple strategy that uses forward subsumption as \nfollows. On the first call to a tabled predicate, we generate am open call, since we know that open calls \nwill eventually be made. For\u00adward subsumption can now be used to return the answers to any specific call. \nThe effectiveness of this strategy is cur\u00adrently under evaluation. 7 Related Work As mentioned in Section \n2, a number of analyses for logic programs have been formulated in terms of rules. Of di\u00adrect relevance \nare the works where such a formulation has been used in an implementation, as in [14, 8]. The use of \nextension tables to compute fix points, and the approach of analyzing the program by executing an abstract \nprogram were presented in [14]. However, the technology was not mature enough at that time to study the \npracticality of the approach. Our formulation of the l-%o~domain analysis is based on the formulation \nin [8], where magic-set transforma\u00adtion was used to obtain call patterns. In contrast, we obtain both \ncall and answer pattens by simply executing the ab\u00adstract program on a (complete) top-down engine. Our \nim\u00adplementation establishes that analyses derived in this man\u00adner are practical, and paves way for further \ninvestigation of factors affecting practicality y, such as seeking the right bal\u00adance between compilation \nand interpretation to reduce total analysis time. In [17], analyses are formulated in terms of solving \nSet Constraints, and implemented using a special-purpose con\u00adstraint solver. In [10], groundless and \ntype analysis of logic programs have been formulated as a constraint solving prob\u00adlem, and implemented \nusing Toupie, a finite domain con\u00adstraint solver. In the area of logic programming, many anal\u00adyses for \nlogic programs have been implemented based on the framework in [5]; generic tools for abstract interpretation, \nsuch aa GAIA [22] and PLAI [25] have been implemented and have evolved into well-optimized systems. In \ncontrast, in this paper, we investigated the use of a general purpose logic programming system for solving \nanalysis problems (ir\u00adrespective of the application language) and its practicality. In [31, 32], dataflow \nproperties for imperative programs have been formulated as database facts. The demand canal\u00adysis problem \nis formulated as a query solved over such a set of facts. Results in [31] suggest that a general purpose \nsystem (Coral) takes about 6 times longer to evaluate the queries, compared to a special purpose demand \nalgorithm implement ed in C. In [35] it is reported that XSB is roughly a order of magnitude faster than \nCoral. In particular, for the sample demand analysis program given in [32] we find that it is indeed \nthe case, leading us to believe that XSB can be used to construct practical dataflow analyzers7. Fiu-ther \ninvestigation is clearly needed. 8 Conclusion Many program analysis problems can be cast in the form \nof evaluating minimal models of a logic program. The re\u00adsults of the paper strongly suggest that practical \nanalyzers can be built from such declarative formulations with min\u00adimum effort using general purpose \nlogic programming sys\u00adtems. Furthermore} we find that these systems offer suffi\u00adcient expressive power \nto formulate many common analyses. Nevertheless, as with any general purpose system, carefully exploiting \nthe capabilities of the system is crucial to attain good performance. There is reason to believe that \nthe re\u00adsults of this paper will carry beyond functional and logic program analyses. Such extensions are \nthe subject of ongo\u00ading research. 7It must be noted that the results presented in this paper were taken \nusing XSB v 1.4.2, which offers significant performance improve\u00adment over earlier releases due to optimized \ntabling primitives (see [2s]). 124 References [16] P.H. Hartel and K.G. Langendoen. Benchmarking im\u00ad \n[1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14] [15] plementations of lazy functional languages. \nIn Sympo- A.V. Aho, R. Sethi, and J.D. Unman. Compilers sium on Functional Programming Languages and \nCom-Principles, Techniques, and Tools. Addison Wesley, puter Architecture, pages 341-349. ACM Press, \n1993. 1988. [17] N. Heintze. Set Based Program Araal@s. PhD thesis, H. Ait-Kaci. Warren s Abstract Machine: \nA Tutorial Carnegie Mellon University, 1992. Reconstruction. MIT Press, Cambridge, Maas., 1991. [18] \nJ. Hindley. The principal type-scheme of an object in F. Bancilhon, D. Maier, Y. Sagiv, and J. Unman. \nMagic combinatory logic. Transactions of the American Math-Sets and other strange ways to implement logic \npro-ematical Society, 146:29 60, 1969. grams. In ACM Symposium on Principles of Database Systems, pages \n1 15. ACM Press, 1986. [19] P. Hudak. A semantic model for reference counting and its abstraction. In \nS. Abramsky and C. Hankin, editors, C. Beeri and R. Ramakrishnan. On the power of magic. Abstract Interpretation \nof Declarative Languages, pages In ACM Symposium on Principles of Database Systems, 45 62. Ellis Horwood, \n1987. pages 269 283. ACM Press, 1987. [20] N.D. Jones. Automatic program specialization: A re- M. Bruynooghe. \nA practical framework for the abstract examination from basic principles. In Partial Evacu\u00adinterpretation \nof logic programs. Journal of Logic Pro-ation and Mixed Computation, pages 225 282. North\u00adgramming, 10:91 \n124, 1991. Holland, 1988. R.E. Bryant. Symbolic boolean manipulation with or-[21] 0. Kaser, C.R. Ramakrishnan, \nI.V. Ramakrishnan, and dered binary-decision diagrams. ACM Computing Sur-R.C. Sekar. EQUALS a parallel \nimplementation of oeys, 24(3):293 318, 1992. a lazy language. Journal of Functional Programming, To appear.W, \nChen and D.S. Warren. Query evaluation under the well-founded semantics. In ACM Symposium on [22] B. \nLe Charlier and P. Van Hentenryck. Experimen-Principles of Database Systems. ACM Press, 1993. tal evaluation \nof a generic abstract interpretation algo\u00adrithm for PROLOG. ACM Transactions on Program- M. Codish and \nB. Demo en. Analysing logic programs ming Languages and Systems, 16(1):35 101, January using Prop -ositional \nlogic programs and a Magic 1994. wand. In International Logic Programming Symposium, pages 114 129. MIT \nPress, 1993. [23] K. Marriot and H. Sondergaard. Notes for a tutorial on abstract interpretation of logic \nprograms (unpub- P. Codognet and G. Fil&#38; Computations, abstractions lished). In North American Conference \non Logic Pro\u00adand constraints. In International Conference on Com\u00ad gramming, 1989. puter Languages, pages \n155-164. IEEE Press, 1992. [24] R. Milner. A theory oft ype polymorphism in program-M-M. Corsini, K. \nMusumbu, A. Rauzy, and B. Le ming. Journal of Computer System Sciences, 17:348-Charlier. Efficient bottom-up \nabstract interpretation 375, 1978. of Prolog by means of constraint solving over sym\u00ad bolic finite domains. \nIn International Symposium on [25] K. Muthukumar and M. Hermenegildo. Compile-time Programming Language \nImplementation and Logic Pro-derivation of variable dependency using abstract inter\u00adgramming, number \n714 in Lecture Notes in Computer pretation. Journal of Logic Programming, 13:315-347, Science, pages \n75 91. Springer Verlag, 1993. 1992. P. Cousot and R. Cousot. Abstract interpretation and [26] A. Mycroft. \nAbstract Interpretation and Optimizing application to logic programs. Journal of Logic Pro-Transformations \nfor Applicative Programs. PhD the\u00ad gramming, 13:103-179, 1992. sis, University of Edinburgh, 1981. P. \nCousot and N. Halbwachs. Automatic discovery of [27] C.R. Ramakrishnan, I.V. Ramakrishnan, and R.C. linear \nrestraints among variables of a program. In A CM Sekar. A symbolic constraint solving framework for Symposium \non Principles of Programming Languagesl analysis of logic programs. In ACM Symposium on pages 84-96. \nACM Press, 1978. Partial Evaluation and Semantics-based Program Ma\u00adnipulation, pages 12 23. ACM Press, \n1995. S. Debray. Static inference of modes and data depen\u00ad dencies in logic programs. ACM Transactions \non Pro-[28] I.V. Ramakrishnan, P. Rae, K. Sagonas, T. Swift, and gramming Languages and Systems, 11(3):418 \n450, July D.S. Warren. Efficient tabling mechanisms for logic pro\u00ad1989. grams. In International Conference \non Logic Program\u00ad ming, pages 697 711. MIT Press, 1995. S. Debray and D.S. Warren. Automatic mode infer\u00adence \nfor Prolog programs. In Proceedings of the Third [29] R. Ramakrishnan, P. Seshadri, D. Srivastava, and \nSymposium on Logic Programming, pages 78-88, 1986. S. Sudarshan. The Coral user s manual. Technical report, \nComputer Sciences Department, Univerit y of J. Freire, T. Swift, and D.S. Warren. Taking 1/0 seri- Wisconsin, \nMadison, 1993. ously: Resolution reconsidered for disk. Technical re\u00adport, Department of Computer Science, \nSUNY, Stony Brook, 1996. 125 [30] P. Rae, C.R. Ramakrkhmm, and I.V. Ramakrishnan. A thread in time saves \ntabling time. Technical report, Department of Computer Science, SUNY, Stony Brook, 1996. [31] T. Reps. \nDemand interprocedural program analysis us\u00ad ing logic databases. In R. Ramakrishnan, editor, Ap\u00adplications \nof Logic Databases. Kluwer Academic, 1994. [32] T. Reps. Shape analysis as a generalized path prob\u00adlem. \nIn ACM Symposium on Partial Evaluation and Semantics-based Program Manipulation, pages 1 11. ACM Press, \n1995. [33] T. Reps, M. Sagiv, and R. Wilhelm. Solving shape\u00adanalysis problems in languages with destructive \nupdat\u00ading. In ACM Symposmrn on Principles O! Programming Languages. ACM Press, 1996. [34] R. Rohmer, \nR. Lescoeur, and J.-M. Kersit. The Alexar\u00adder method, a technique for the processing of recursive axioms \nin deductive databases. New Generation Com\u00adputing, 4(3):273 285, 1986. [35] K. Sagonas, T. Swift, and \nD.S. Warren. XSB as an efficient deductive database engine. In ACM SIGMOD Symposium on Management of \nData. ACM Press, 1994. [36] K. Sagonas, T. Swift, and D.S. Warren. The XSB pro\u00adgrammer s manual, Version \n1.4.2. Technical report, De\u00adpartment of Computer Science, SUNY, Stony Brook, 1995. [37] R.C. Sekar and \nI.V. Ramakrishnan. Fast strictness analysis based on demand propagation. ACM Transac\u00adtions on Programming \nLanguages and Systems, 17(6), November 1995. [38] H. Tamaki and T. Sate. OLDT resolution with tabu\u00adlation. \nIn International Conference on Logic Program\u00adming, pages 84 98. MIT Press, 1986. [39] J.D. Unman. Principles \nof Database and Knowledge\u00adbase SyStems, Volume 11. Computer Science Press, 1989. [40] P. Van Hentenryck, \nA. Cortesi, and B. Le Charlier. Evaluation of the domain Prop. Journal of Logic Pro\u00adgramming, 23(3):237 \n278, 1995. [41] P. Van Roy, B. Demoen, and Y. D. Willems. Improv\u00ading the execution speed of compiled \nProlog with modes, clause selection and determinism. In Theory and Prac\u00adtice of Software Development, \npages 111 125, March 1987. 126   \n\t\t\t", "proc_id": "231379", "abstract": "Many analysis problems can be cast in the form of evaluating minimal models of a logic program. Although such formulations are appealing due to their simplicity and declarativeness, they have not been widely used in practice because, either existing logic programming systems do not guarantee completeness, or those that do have been viewed as too inefficient for integration into a compiler. The objective of this paper is to re-examine this issue in the context of recent advances in implementation technologies of logic programming systems.We find that such declarative formulations can indeed be used in practical systems, when combined with the appropriate tool for evaluation. We use <i>existing</i> formulations of analysis problems --- groundness analysis of logic programs, and strictness analysis of functional programs --- in this case study, and the XSB system, a table-based logic programming system, as the evaluation tool of choice. We give experimental evidence that the resultant groundness and strictness analysis systems are practical in terms of both time and space. In terms of implementation effort, the analyzers took less than 2 man-weeks (in total), to develop, optimize and evaluate. The analyzer itself consists of about 100 lines of tabled Prolog code and the entire system, including the components to read and preprocess input programs and to collect the analysis results, consists of about 500 lines of code.", "authors": [{"name": "Steven Dawson", "author_profile_id": "81100534830", "affiliation": "SRI International, 333 Ravenswood Ave., Menlo Park, CA and Department of Computer Science, SUNY at Stony Brook, Stony Brook, NY", "person_id": "PP14185719", "email_address": "", "orcid_id": ""}, {"name": "C. R. Ramakrishnan", "author_profile_id": "81332522509", "affiliation": "Department of Computer Science, SUNY at Stony Brook, Stony Brook, NY", "person_id": "PP40036402", "email_address": "", "orcid_id": ""}, {"name": "David S. Warren", "author_profile_id": "81452607732", "affiliation": "Department of Computer Science, SUNY at Stony Brook, Stony Brook, NY", "person_id": "PP14210981", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/231379.231399", "year": "1996", "article_id": "231399", "conference": "PLDI", "title": "Practical program analysis using general purpose logic programming systems&#8212;a case study", "url": "http://dl.acm.org/citation.cfm?id=231399"}