{"article_publication_date": "05-01-1996", "fulltext": "\n Source-Level Debugging of Scalar Optimized Code Ali-Reza Adl-Tabatabai* 1School of Computer Science \nCarnegie Mellon University Pittsburgh, PA 15213 Abstract Although compiler optimizations play a crucial \nrole in the performance of modern computer systems, debugger tech\u00adnology has lagged behind in its support \nof optimization,, Yet debugging the unoptimized translation is often impossible or futile, so handling \nof code optimizations in the debugger is necessary. But compiler optirnizations make it difficult to \nprovide source-level debugger functionality: Global op\u00adtimization can cause the runtime value of a variable \nto be inconsistent with the source-level value expected at a break\u00adpoint; such variables are called endangered \nvariables. A, de\u00adbugger must detect and warn the user of endangered variables otherwise the user may \ndraw incorrect conclusions about the program. This paper presents a new algorithm for detecting variables \nthat are endangered due to global scalar optimiza\u00adtion. Our approach provides more precise classifications \nof variables and is still simpler than past approaches. We have implemented and evaluated our techniques \nin the context of the cmcc optimizing C compiler. We describe the compiler extensions necessary to perform \nthe required bookkeeping of compiler optimization. We present measurements of the effect of optimizations \non a debugger s ability to present the expected values of variables to the user. Introduction Designing \na source-level debugger for globally optimized code is still an open problem and users must choose be\u00adtween \ndebugging or optimization, Compiler implementors This research was sponsored in part by the Advanced \nResearch Projects Agency/ITO monitored by SPAWAR under contract NO0039-93-C-0152 and in part by donations \nfrom Intel Corp. and Motorola Inc. Permission to make digitalhrd copy of part or all of this work for \npetsonal or classroom use is ranted without fee provided that copies are not made or distributed for \npro #t or commercial advanta e, the copyfight notice, the title of the publication and its date appear, \nan J notice is given that orrpying is by permission of ACM, Inc. To copy otherwise, to republish, to \npost on servers, or to redistribute to lists, requires prior specific permission and/or a fee. PLDI 96 \n5/96 PA, USA @ 1998 ACM 0-89791 -7954YWO005...$ 505O and Thomas Grossl 2 21nstitut fur Computer Systeme \nETH Ziirich CH 8092 Zurich have generally avoided supporting the debugging of opti\u00ad mized code in the \npast, although some systems provide de\u00ad bugger mechanics without warranties for the debugging of optimized \ncode. To cite from the description of the options in the C manual of a major Unix vendor: -g3 Have the \ncompiler produce additional symbol table information for fall symbolic debugging for fully optimized \ncode. This option makes the debugger inac\u00adcurate. For non-Unix systems, the situation is no better (to \ngive an example from the PC/Macintosh world): Enable Debugging turns off all optirnizations. When the \ncompiler optimizes it sometimes rearranges object code so that it does not correspond exactly to the \nsource code. This rearrangement may confuse the debugger s source code view. A source-level debugger \nmust solve two types of prob\u00adlems: First, the debugger must map a source statement to an instruction \nin the object code to set a breakpoint and map an instruction to the source code to report a fault or \nuser inter\u00adrupt (code location problems [26]). Second, the debugger must retrieve and display the values \nof source variables in a manner consistent with what the user expects with respect to the source statement \nwhere execution has halted (data-value problems [26]). When a program has been compiled with optirnizations, \nmappings between breakpoints in the source and object code become complicated, and values of variables \ncan be inaccessible in the runtime state or inconsistent with what the user expects. This paper presents \na solution to deal with the the data\u00advalue problems caused by optimizing compiler transforma\u00adtions. Our \nfocus are the global and local scalar optimization that are included in current state-of-the-art compilers. \nThere are two novel aspects. First, we develop a unified model of the data-value problem caused by global \nscalar optirniza\u00adtions; Figure 1 sketches the questions that a debugger must address. Then we present \nan approach based on data-flow analysis to analyze and propagate the effects of optimizing transformations. \nThese algorithms have been implemented in an optimizing C compiler: the data-flow analysis required to \nsupport the debugger is similar to the data-flow analysis performed for global optimization and in our \ncompiler uses the same modules. Compiler extensions are only necessary to generate the information required \nto analyze the effect of optimizing transformations on the data-value problems. Moreover, our algorithms \nwork on a single representation of a program; this is in contrast to other approaches (e.g., [24]) that \nkeep around a copy of the original source program repre\u00adsentation. Our paper concludes with an empirical \nevaluation; we report on measurements for the eight C programs of the SPEC92 suite. 1.1 The data-value \nproblem We summarize here the terminology; for examples and mo\u00adtivation we defer to the references. When \nglobal register allocation is performed, the register assigned to a variable V may be shared with other \nvariables as well as temporaries. Thus at a breakpoint there may be no value of V available; V is nonresident \n[3] if the value in the register assigned to V may be the value of some vari\u00adable other than V. The debugger \nreports that the value of a nonresident variable V is unavailable since the value in the register assigned \nto V does not correspond to a meaningful source-level value of V [3]. If at a breakpoint a variable V \nis resident, then the value in the runtime location of V corresponds to some source-level value of V. \nThis value is referred to as the actual value of V, while the value that the user expects V to have, \nbased on the context of the source breakpoint statement, is the expected value of V. Since the actual \nvalue of a variable V is a source-level value of V, it is meaningful to display this value to the user. \nHowever, if optimizations have moved or eliminated assignment expres\u00adsions, the actual value of a variable \nV may not correspond to the expected value of V, in which case V is endangered [19, 13, 2] and additional \ninformation must be provided to the user. There are two mutually exclusive classes of endangered variables: \nnoncurrent variables and suspect variables. Some\u00adtimes the debugger can tell that the actual value of \nV definitely does not correspond to the expected value of V, in which case the actual value of V is displayed \nto the user with a warning that V is noncurrent [19, 13, 2]. However, there are situa\u00adtions when a debugger \ncannot tell whether the actual value of V corresponds to the expected value of V, in which case the user \nis warned that V is suspect [2]. Suspect variables are caused by ambiguities due to either multiple paths \nreaching a breakpoint [1], or pointer assignment that are executed out of order [2]. If a variable V \nis endangered, the debugger can provide additional information about V to the user, such as the source \nassignment expression(s) that (may have) assigned the actual value of V, or the source assignment expression(s) \nthat (possibly) should have assigned the expected value of V. If the debugger can positively determine \nthat the actual value of V corresponds to the expected value of V, then V is current and the actual value \nof V is displayed with\u00adout warnings. The techniques described in this paper allow the debugger to detect \nwhether a variable is noncurrent, sus\u00adpect or current, and convey to the user in source terms how optimizations \nhave affected the value of a variable.  1.2 Debugger model Our model of debugging assumes that the debugger \nis non\u00adinvasive [3, 2]; the code generated by the compiler and de\u00adbugged by the user is the default code \ngenerated with op\u00adtimization enabled. The compiler is not allowed to insert additional instructions into \nthe object code to enable or sim\u00adplify source-level debugging; we are interested in debugging the optimized \nversion of a program. Figure 1 orders the states of a variable with respect to de\u00adbugging. The debugger \ncan determine whether a variable should contain a source-level value by performing simple reaching analysis. \nThis analysis helps, since a register pro\u00admoted variable that is uninitialized is likely to be nonresi\u00addent, \nand the debugger can provide a better classification by informing the user that the variable is uninitialized. \nIf a vari\u00adable is initialized, the debugger detects whether the variable is resident. If the variable \nis resident, the analysis described in this paper determines whether the variable is noncurrent, suspect, \nor current, and its value is shown to the user, possibly with a warning. This approach never misleads \nthe user; an endangered values is always accompanied by a warning (that the value is noncurrent or suspect). \nNo + Warn V is ;,::&#38;g#&#38;&#38;U&#38; noncurrent . .,..-... : , .. . . Yes A Show V S value No \nWam Vis ~ - suspect Show V without warnings Figure 1: Outline of our algorithm; shaded part corresponds \nto the novel aspects discussed in this paper, The above approach presents a baseline that can be im\u00adproved, \nfor example, by using runtime values to reconstruct a variable s expected value (recoveV [19, l]). Runtime \nvalues can also be used to differentiate between suspect and current values (e.g., by determining which \npath was taken to reach a specific breakpoint; an example is provided in Section 2.2). However, all of \nthese refinements are improvements to this overall model, which therefore plays a central role in the \norganization of a debugger.  1.3 Prior work There exists a small body of literature on debugging opti\u00admized \ncode, starting with Hennessy s paper [19], which de\u00adfined some of the basic terms (e.g., endangered, \nnoncurrent). DOC [14] and CXdb [8] are examples of two real debug\u00adgers for optimized code. These debuggers \ndetect whether a variable is nonresident (using a conservative approach based on a variable s live range \n[3]). CXdb only detects whether a variable is resident; it is up to the user to determine the correspondence \nbetween a variable s actual value and scmrce code values. DOC can detect variables that are endangered \ndue to instruction scheduling, but cannot deal with data-value problems caused by global optimization. \nThe shortcomings and constraints of these debuggers are discussed in detail in [3, 2, 1]. Our own earlier \nwork [3, 2] concentrates on the effects of global register allocation and local code scheduling. We do \nnot discuss these aspects any further in this paper, as urlder\u00adlined by Figure 1. This paper concentrates \non a framework for the source-level debugging of locally and globally optimized code and discusses how \nto deal with endangered variables caused by these optimization. Except for the italicized en\u00adtries, we \ndiscuss the optimization of Table 1 here for the first time. Register optimization are included in our \ncom\u00adpiler (otherwise, we would not claim to have an optimizing compiler) and therefore included in our \nevaluation. Other researchers that have investigated the problem ofde\u00ad tecting endangered variables caused \nby global optirnizations are Copperman [13] and Wismueller [24]. Both of tlhese works provide formal \nframeworks but do not specify how their solutions can be extended to the problems faced by a real compiler. \nCopperman s approach [13] is based on data-ilow analysis of intermediate representations of the program. \nThis representation captures the effects of transformations (global optimizations), but does not cover \nall aspects of the transla\u00ad tion (e.g., register allocation) and does not deal with faults and user interrupts. \nWithout an implementation, it is difficult to evaluate the practicality of Coppemmn s approach. Wis\u00ad \nmueller [24, 25] concentrates only on detecting whether the expected value of a variable can be displayed \nto the user;, his algorithms do not distinguish between nonresident, suspect, and noncurrent variables. \nBoth [13] and [24] assume that the compiler can mangle the source code arbitrarily, resulting in an arbitrarily \ndifficult problem and in a solution that is difficult to both understand and implement. Our work takes \nadvantage of the fact that Table 3: Performance of optimized code generated by cmcc, relative to optimized \ncode generated by gc c (version 2.3.2) and MIPS cc on a DECstation 5000/200. program gcc -02 cc -02 li \n0.98 1.05 eqntott 1.13 1.09 espresso 1.06 1.05 gcc 1.02 0.89 alvinn 1.06 0.94 compress 0.84 0.95 ear \n1.07 0.95 Sc 1.09 1.03 optimization do not transform code arbitrarily; there are a number of invariants \nthat are preserved when compilers transform programs. For example, if code is hoisted to a different \nbasic block, the basic block is post-dominated by the original block; this limits the range of breakpoints \nwhere a variable is endangered. Or, if an expression is eliminated due to redundancy, the value must \nbe available somewhere, and the debugger can provide this value to the user. [13] and [24] fail to take \nthese constraints into account. Another major difference between the earlier work by Cop\u00adperman and Wismueller \nis that they attempt to capture a summary effect of all optimizations. Then they attempt to relate the \noptimized code back to the source code. In contrast, our approach models each optimization step (of Table \n1); the compiler propagates the information about the effect of these optimization steps through all \nall optimization phases. This issue is discussed in more detail in Section 3.  1.4 Experimental framework \nThe algorithms described in this paper are implemented in the cmcc compiler, CMU S optimizing C compiler. \ncmcc uses the ICC ANSI C front end [18]. Table 1 lists the op\u00adtimization performed by cmc c. These optimization \nare based mostly on the standard bit-vector algorithms described in [12]. Our implementations of partial \nredundancy elimina\u00adtion, strength reduction and partial dead code elimination are based on the algorithms \ndescribed in [21], [20] and [22]. The global register allocator is a Chaitin-style register allocator \n[9] with the improvements described in [7]. So far, we have retargeted cmcc to the MIPS, SPARC, DLX, \nand iWarp ar\u00adchitectures. We used the MIPS code generator to gather the results that we report in this \npaper. In this paper, we base our empirical evaluation on the set of eight C programs from SPEC92, Table \n2 shows the sizes of these programs and statistics relevant to source-level debugging. The third and \nand fourth column of this table show the total number of source-level breakpoints in each program and \nthe average number of breakpoints per function. Loop unrolling and peeling Linear function test replacement \nInduction variable expansion Induction variable simplification Constant propagation and folding Induction \nvariable elimination Assignment propagation Partial dead code elimination Dead assignment elimination \nPartial redundancy elimination Strength reduction Branch optimizations Global register allocation (using \ngraph coloring) Register coalescing Instruction scheduling Table 1: Optimizations performed by cmcc. \nLines Total source Breakpoints Variables per Program of code breakpoints per function breakpoint li 7741 \n2594 7.4 5.2 eqntott 3483 1267 21.6 5.1 espresso 14842 7424 21.5 9.4 gcc 102389 28433 20.7 9.3 alvinn \n322 140 8.3 6.3 compress 1503 429 26.9 5.8 ear 4466 1108 11.8 6.9 Sc 8491 3400 23.1 7.1 Table 2: Programs \nused in this study. The last column shows the average number of local variables that were in scope at \neach source-level breakpoint. Table 3 shows that the optimized code generated by cmcc is roughly of the \nsame quality as the optimized code produced by gcc and the native MIPS cc compiler. (This table presents \nthe perfommnce relative to these compilers; a number of less than one means that cmcc produces better \ncode.)  2 Our approach Since the debugger interacts with the user, only values of source program variables \nare of interest; compiler-internals (temporaries) are never visible to the user. Therefore, data\u00advalue \nproblems cart be handled by restricting attention to transformations that affect assignments to source-level \nvari\u00adables. Many scalar optimizations, such as strength reduc\u00adtion, constant folding and constant propagation \n[4] do not directly affect assignments to source variables 1. Other op\u00adtimization, like loop induction \nvariable strength reduction and elimination, allow the debugger to infer source values from compiler \ntemporaries that replace eliminated variables. There are other situations where the overall effect of \na se\u00adries of transformations is the replacement of a source-level variable with a compiler temporary, \nagain allowing the de\u00adbugger to infer values from compiler temporaries. Control 1Their effects maybe \nindirect, for instance, constant propagation or copy propagation may eliminate all uses of an assignment \ns left hand side, thus subjecting the assignment to elimination by dead code elimination. flow transformations, \nsuch as loop unrolling and code repli\u00adcation, change the control flow graph by duplicating basic blocks. \nThese transformations, however, do not reorder the execution of source-level assignments and thus do \nnot cause data-value problems. Compiler transformations cause endangered variables by either eliminating \nor moving assignments to source variables. An assignment A may be eliminated because it is either dead \n(i.e., the value computed by A is never used), or available (i.e., the value computed by A is already \ncomputed on all paths leading to A). Code motion algorithms move an ex\u00adpression either upwards against \nthe direction of control flow (hoisting), or downwards towards the direction of control flow (sinking). \nCode motion is constrained by safety con\u00adsiderations: a computation cannot be introduced into a path \nwhere it did not exist before. Therefore, code hoisting op\u00adtimization copy an expression E from a block \nB to one or more blocks that are post-dominated by B, while code sinking optimization move an expression \nE from a block B to one or more blocks that are dominated by B. By taking advantage of these code motion \ninvariants, our algorithms are greatly simplified. In fact, it is these invariants that have allowed \nus to produce a solution to the problem that is signifi\u00adcantly simpler than the approaches described \nin [25] and [13]. Examples of code hoisting optimization include partial re\u00addundancy elimination [23, \n12, 15, 21] and global instruction scheduling algorithms that perform non-speculative hoisting of instructions \n[5]2. Examples of code sinking optimiza\u00ad 2ne ~gofithms described in this paper have been extenrfedto \ncleatWifi tions include partial dead code elimination [22], unspecula\u00adtion [17], global instruction \nscheduling algorithms that sink code past conditional branches [10], superblock dead code elimination \n[11] and forward propagation [6], 2.1 Core optimization Code motion and elimination are related, since \nsome code motion algorithms operate by computing the set of program points where insertions of expressions \nrender other expres\u00adsions either dead or available [22, 23]. If the debugger can detect endangered variables \ncaused by code hoisting and dead code elimination, then we have the foundation to debug op\u00adtimized code, \nsince these two transformations capture the effects of the elimination and movement transformation dis\u00adcussed \nabove (including code sinking). Code hoisting causes endangered variables by hoisting an expression that \nassigns to a source-level variable V, thus causing V to be updated prematurely. Dead code elintina\u00adtion \ncauses endangered variables by eliminating updates to variables. In the case of an endangered variable \nV caused by dead code elimination, the expected value of V is the value that would have been assigned \nby a source-level assignment Ed that was eliminated by dead code elimination, while the actual value \nof V is the value assigned by a source-level as\u00adsignment other than Ed. When both dead code elimination \nand code hoisting have been performed, it is possible that the expected value of a variable V stems from \na dead assign\u00adment, while the actual value of V stems from an assignment that has been executed prematurely \ndue to code hoisting; in this case V is endangered due to code hoisting. Because of control flow ambiguities, \nboth transformations may c;ause variables to be suspect; that is, the debugger may sometimes only be \nable to tell that a variable is possibly noncurrent due to code hoisting or dead code elimination. We \nnow provide an example to clarify these concepts.  2.2 Illustration of the effect of code hoisting Figure \n2 shows the intermediate representation (IR) flow graph of a program fragment. Code hoisting has inserted \nexpression EB inside block B2, rendering expression .&#38; in block B3 redundant. We refer to expressions \nthat are in\u00adserted by code hoisting (e.g., E3) as hoisted expressions, while expressions that are made \nredundant by insertions and thus eliminated from the program (e.g., ,?32)are referred to as redundant \nexpressions. If a hoisted expression Eh is inserted to make one other expression E, redundant, then Er \nis re\u00adferred to as the redundant copy RedCopy(Eh ) of the hoisted expression Eh. In Figure 2, Ez = Recopy. \nFigure 2 shows three possible breakpoints that may oc\u00ad cur: Bkpt 1, Bkpt2, and Bkpt3. At Bkpt 1, x is \ndefinitely noncurrent, since the actual value of x is different from the speculativecodehoistingtransformations[1]. \nEO: X= U-V ~. E3 inserted by E3: X=Y+Z El: X=Y+Z ~1 code hoisting Bkpt 1 x noncurrent at Bkptl B3 Bkpt2 \nx suspect at Bkpt2 Ez deleted because E&#38;x+z., available Bk&#38;W3 x current at Bkpt3 u Figure 2: \nExample of code hoisting. expected value of x due to E3 prematurely assigning to x the source-level value \nassigned by Ez. If the user queries the value of x at this breakpoint, the debugger can display the actual \nvalue of x and warn the user that this value is the value assigned by the source-level assignment E2, \nwhich has executed prematurely. At breakpoint Bkpt2, x is current if execution has reached block B3 from \nblock B 1 and noncur\u00adrent if execution has reached from block B2. In the absence of knowledge regarding \nexecution history, the debugger can\u00adnot determine whether execution has reached B 3 via B 1 or B2, and \ntherefore the debugger cannot determine whether x is current or noncurrent and must report x as being \nsuspect at breakpoint Bkpt2. If the user queries the value of x at this breakpoint, the debugger can \ndisplay the actual value of x and warn the user that this value may be from expression E2, which may \nhave executed prematurely at block B2. The user may be able to determine whether block B2 has exe\u00adcuted \n(e.g., based on the values that determine the outcome of block BO S conditional branch) and thus whether \nEz has indeed executed prematurely. Note that the compiler can instrument the object code to collect \nruntime information, allowing the debugger to determine which path was taken to B3 and thus reporting \nx as either noncurrent or current. This is disallowed, however, in our non-invasive debugger model. Now \nconsider breakpoint Bkpt3. At this breakpoint, the expected value of x is the value assigned by E2. The \nactual value of x is the value assigned by either El or E3, depending on the path traversed to this breakpoint. \nThe values of either El or E3 are the same as the value that would have been assigned by E2 (otherwise \nEz would not have been eliminated due to redundancy), and thus x is now current.  2.3 Detecting endangered \nvariables causedby code hoisting The key idea is to determine if there exists a path to a break\u00adpoint \nthat includes a hoisted expression that is not followed (onthesatne path) bya redundant copy. Once execution \nhas progressed to the point that all paths include a redundant copy, the variable is current, since the \nactual value of the vari\u00adable is the expected value. Path problems are easily solved by an appropriate \ndata-flow framework. The data-flow analysis used to detect endangered variables due to code hoisting \nis similar to the reaching definitions analysis [4]. After the execution of a hoisted expression Eh that \nassigns to a variable V, the actual value of V corresponds to the source-level value that would have \nbeen assigned by the redundant expression RedCopy(E~ ); for example, in Figure 2, the actual value of \nx at breakpoint Bkpt 1 is the value assigned by the hoisted assignment EB, which is the source-level \nvalue that would have been assigned by Ez. Let P = (start ,..., O) be an execution path traversed to \na breakpoint l?. If at B the actual value of a variable V is the value assigned by a hoisted expression \nEh, then V is non\u00adcurrent at B if the expected value of V does not correspond to the source-level value \nthat would have been assigned by RedCopy(Eh ). Therefore, if Eh reaches along P and after the last occurrence \nof Eh, P does not include RedCopy(E~), then V is noncurrent. Or, expressed positively, V is current whenever \na path P includes RedCopy(Eh ), and Eh does not occur on P after RedCopy(Eh ). For instance, in Figure \n2, E3 reaches Bkptl and Bkpt3. x is noncurrent at Bkptl, since any path taken to Bkptl does not go through \nEz, and current at Bkpt3, since all paths taken to Bkpt3 go through Ez. Therefore, given paths along \nwhich a hoisted expression Eh reaches, those paths that do not gothroughRedCopy (E~) are distinguished: \nDefinition 1 A redundant expression E, hoist reaches along a path P = (start, . . . . 0), ifthere exists \na hoisted expression Eh such that Er = RedCopy(Eh ), and Eh reaches along P, and Er does not occur after \nthe last occurrence of Eh alovg P. Note that hoist reach is a property of redundant expressions only \n(i.e., expressions that are eliminated by partial redun\u00addancy elimination). In Figure 2, the redundant \nexpression Ez hoist reaches Bkpt2 on the path from block B2. Ez does not hoist reach on paths that reach \nvia block B 1. Lemma 1 Let ET be a redundant assignment expression that assigns to a variable V. If E? \nhoist reaches along a path P = (start, . . . . O) and P is the execution path traversed to a breakpoint \nB, then V is noncurrent at B due to the premature execution of ET 3, swe ~tit tie ~roofs of lemmas as \nthey are str~ght fo~~d. Since the debugger does not know which execution path was actually taken to \nreach a breakpoint, it must consider all possible paths. The following lemmas describe the two cases \nwhere a redundant assignment expression hoist reaches along all or only some of the paths that lead to \na point O, where a breakpoint has occurred. Let E, be a redundant assignment expression that assigns \nto a variable V: Lemma 2 If E, hoist reaches along all paths leading to a point O, then at any breakpoint \noccurring at O, V is noncurrent due to the premature execution of ET. Lemma 3 ~ Ev hoist reaches along \nat least one but not all paths leading to a point O, then at any breakpoint occurring at O, V is suspect \ndue to the possible premature execution of E,, In Figure 2, x is noncurrent at Bkpt 1, since the redundant \nassignment expression E2 hoist reaches on all paths to Bkpt 1. At Bkpt2, x is suspect since E2 hoist \nreaches on only some paths. At Bkpt3, x is current since no expressions that assign to x hoist reach. \nDetecting whether a redundant expression hoist reaches along all or only some paths can easily be done \nusing data\u00adflow analysis. This data-flow analysis is performed on the final instruction-level intermediate \nrepresentation of a pro\u00adgram, that includes annotations describing the effects of op\u00adtimization. In Section \n3, we describe how this representation can be built and maintained by the compiler. The hoist reach data-flow \nattribute of an assignment expression E is gener\u00adated by any code inserted by code hoisting that computes \nE. The hoist reach of E is killed by any eliminated redundant code that also computes E. Two flow analyses \nare done to determine whether an assignment expression hoist reaches on some or all paths to a breakpoint. \nTypically, a variable is en\u00addangered over a small region of the program, and only a few variables are \nendangered. Therefore, an efficient implemen\u00adtation of our analyses can be based on slotwise analysis \n[16]. (For more implementation details, see [1].) Note that the data-flow analysis does not need to determine \nwhich instance of an expression hoist reaches, but rather that some expres\u00adsion hoist reaches; that is, \nthe compiler need not determine that E2 = Recopy, but rather that E3 is a hoisted in\u00adstance of E, and \nthat Ez is redundant. Note also that given a redundant expression Er that is the redundant copy of a \nhoisted expression Eh, E. post-dominates Eh, and thus the hoist reach of E. is eventually killed on any \npath leading from Eh. Therefore the region of endangerment caused by code hoisting is limited. 2.4 Detecting \nendangered variables caused by dead code elimination The program fkagment of Figure 3 is used to demonstrate \nthe effects of dead code elimination on debugging. In this figure, E. deleted BO because dead x noncurrent \nB1 B2 Bk@3 x noncurrent at Bkpt3 BkDt 2 E2: X=Y+Z E2inserted by code sinking x noncurrent BkQt 4 Q x \ncurrent at Bkpt2 \\ T at Bkpt4 B3 x suspect Bk@ 5 at Bkpt5 El: X= U-V x current B~t 6 at Bkpt6 Figure \n3: Example of dead code elimination. assignment sinking has inserted E2 and deleted expression Eo. At \nbreakpoint Bkpt 1, x s expected value is the vah.w that would have been assigned by Eo, while x s actual \nvalue is the value assigned by the last assignment that was executed prior to this program fragment. \nTherefore, the actual value of x is stale, and x is noncurrent. x is similarly noncurrent at Bkpt2 and \nBkpt3. At Bkpt4, x is current since expression Ez assigns the expected value of x (i.e., the value that \nwould have been assigned by Eo). At Bkpt5, x is noncurrent if execution has reached this breakpoint from \nblock B 1 and current if execution has reached from block B2. Therefore, x is suspect at Bkpt5. Finally, \nat Bkpt6, both the expected and actual values of x are from El, and thus x is current. Unlike the hoist \nreaching data-flow algorithm where we solved for whether a redundant IR expression is hoist reach\u00ading, \nthe data-flow algorithm for detecting endangered mri\u00adables caused by dead code elimination solves for \nwhether Lemma 4 If a variable V is dead reaching along a path P = (start, . . . . O), and P is the execution \npath traversed to a breakpoint B, and V is not noncurrent due to the premature execution of a redundant \nassignment, then V is noncurrent at B because the actual value of V is stale. Lemma 5 If a variable V \nis dead reaching along all paths leading to a point O, then V is noncurrent at any breakpoint occurring \nat O. Lemma 6 If a variable V is dead reaching along at least one but not all paths leading to a point \nO, then V is suspect at any breakpoint occurring at O. In Figure 3, x is dead reaching along all paths \nleading to Bkpt 1, Bkpt2 and Bkpt3, and thus x is noncurrent at these breakpoints. At Bkpt5, x is dead \nreaching only along those paths that pass through B 1 and thus x is suspect. At Bkpt6, x is not dead \nreaching and thus x is current. The data-flow analyses to detect whether a variable is dead reaching \non only some or all paths can be derived in a straight forward manner from the above definitions and \nlemmas. The dead reach of a variable V is generated by a dead assignment to V and killed by any other \nkind of assignment to V[ 1].  2.5 Recovery If dead code elimination eliminates an assignment to a vari\u00adable \nV, it may be possible to recover the expected value of V from the values of compiler temporaries. Consider \nthe example in Figure 4(a). The right hand side of the expres\u00adsion X=Y+ z at statement S1 is propagated \nto the two uses of x at statements S2 and S3. After this assignment prop\u00adagation, no uses of x remain \nand S1 is eliminated (Figure 4(b)). Common subexpression elimination detects the com\u00admon subexpression \ny+ z, replacing the two computations of y+z with fetches from the temporary trap (Figure 4(c)). a van \nable V is endangered due to the elimination of slome dead assignment to V. After execution passes through \na dead assignment to a variable V, the actual value of V becomes stale until another assignment to V \nis executed. For example, in Figure 3, x becomes noncurrent after Eo, until after the as\u00adsignments El \nand E2. Therefore, we distinguish those paths where a variable s value is stale due to a dead assignment: \nDefinition 2 A variable V is dead reaching along a path P = (start, .... O), if there exists a dead assignment \nexpres\u00adsion Ed that assigns to V such that Ed occurs in P and no assignments to V occur along P after \nthe last occurrence of Ed. If a variable V is dead reaching along a path P and P is the execution path \ntraversed to a breakpoint B, then V is clearly noncurrent at B: s]: X=y+ z tmp=y+ z S2: ..x. . Sz: .. \nY+z. . S2: ..tmp. . S3: ..x. . S3: .. Y+z. . S3: ..tmp. . (a) (b) (c) Figure 4: Recovery example (a) \nOriginal source program (b) After copy propagation and dead code elimination (c) After common subexpression \nelimination. In cmcc, assignment propagation is performed to improve partial redundancy elimination [12, \n6] and the situation de\u00adscribed above occurs quite often. The final effect of this series of transformations \nis that the source-level variable x is replaced with tmp. If the user queries the value of x at a breakpoint \nthat occurs after statement S1, the debugger can display the value of tmp, since these two variables \nare aliased. This is one form of recovery, where the debugger re\u00adconstructs the expected value of a variable \nfrom other runtime values. Recovery is performed by checking each expression E inserted by code replacement \ntransformations (Section 3 de\u00adscribes how we keep track of such transformations). If E replaced a fetch \nfrom a source-level variable V in the orig\u00adinal program, then the value computed by E aliases V, and \nV can be recovered from the storage location holding the value of E. E may be a constant, a fetch from \na temporary, or some more general computation such as addition. In the case that E is a fetch from a \ntemporary T, we generate the residence [3] of V in the storage location assigned to T. If E is a constant, \nwe generate a special constant residence for V, indicating that the value of V is a constant. If E is \nneither a constant nor a fetch, then we generate the residence of V in the storage location assigned \nto the result register of the instruction that computes E s value. In all cases, the dead reach of V \nis killed by E. A similar approach is used to recover the value of a source-level induction variable \nthat is replaced by a strength-reduced expression. Linear function test replacement [12] replaces a loop \ntest involving a source\u00adlevel variable with a compiler synthesized tempormy. The source-level induction \nvariable V can then be eliminated if all other uses of V have been eliminated (most likely by strength \nreduction).  3 Tracking compiler transformations To allow the debugger analyses described in Section \n2, the compiler must perform bookkeeping to record the effects of optimizations in the program representation. \nIn the cmcc compiler, this bookkeeping is performed by annotating the nodes of cmcc s IR. These annotations \nrecord whether an operation was inserted by optimizations (and if so by which). Bookkeeping also inserts \nspecial IR marker nodes to mark points of interest to the debugger. These annotations and markers are \nignored by optimizations and optimization are not constrained in anyway. This is in contrast to the approach \ndescribed in [24] where a representation of the original source program is kept as a copy, and links \nare maintained between the intermediate representation used for optimizations and the original representation \n(e.g., an abstract syntax tree). The different ways in which global optimizations may transfonu a program \nand the manner in which bookkeeping is performed for these transformations, are as follows: Code insertion \nCode motion and common subexpression elimination transformations insert new code into the program representation. \nExpressions that are inserted by code hoisting or code sinking are marked as hoisted or sunk expressions. \nAssignment expressions that are marked as hoisted will generate the hoist reach of vari\u00adables. Code replacement \nCopy propagation and redundancy elim\u00adination replace one expression with another. Copy propagation replaces \na reference to a variable with a propagated expression, while redundancy elimination replaces an available \nexpression with a fetch from a compiler temporary. When an expression E replaces a fetch from a variable \nV, a reference to V is kept in E. This information is needed only for recovery (as described in Section \n2.5) and can otherwise be omitted. Code deletion Dead code elimination and partial redun\u00addancy elimination \ndelete assignment expressions that are dead or available. When an assignment to a variable V is eliminated, \nit is replaced with a special IR marker node, unless this assignment has been previously marked as sunk \nor hoisted. A marker node contains a reference to the variable V and an indication why the assignment \nto V was eliminated (i.e., whether the assignment was dead or available). Markers are ignored by optimization \nphases and are used only for the debugger analysis algo\u00adrithms. Markers that indicate an available assignment \nto a variable V will kill the hoist reach of the assign\u00adment, while markers indicating a dead variable \nV will generate the dead reach of V. Code duplication Control flow optimizations such as loop peeling \nduplicate code. Code duplication, however, does not create data-value problems since no move\u00adment or \nelimination of assignments occur. Therefore, the effects of this transformation need not be recorded. \nHowever, marker nodes inside a block B must also be duplicated when I? is duplicated. Moreover, if an \nIR node containing debugging annotations is duplicated, the annotations must be duplicated along with \nthe node. Basic block deletion A block of code can be eliminated if the optimizer determines that this \ncode is unreachable. This transformation usually occurs after a conditional branch is folded. Since the \ncode that is eliminated would not have executed in the original program, this transformation does not \ncause data-value problems and its effects need not be recorded. Basic blocks can also be deleted because \nthey become empty after other optimizations, or because they contain only unconditional branches (and \nare deleted by branch chaining). If a deleted basic block contains any infor\u00admation relevant to debugging \n(i.e., markers), then such information must be retained and is transfemed to the deleted block s successor. \nBasic block insertion Edge splitting and preheader inser\u00adtion insert new (empty) basic blocks into the \nprogram representation. These transformations do not cause data-value problems4. 4Code duplication, basic \nblock deletion and basic block insertion create Only after the final object code is produced are all \nopti\u00admization exposed, and thus the analyses for detecting endan\u00adgered variables are performed on an \ninstruction-level repre\u00adsentation of the program [3, 2]. Like most compilers, cmcc has a two-level intermediate \nrepresentation consisting of a machine-independent IR used for global optimizations (e.g., partial redundancy \nelimination), and an instruction-level rep\u00adresentation used for machine-dependent optimization ((e.g., \nregister allocation and instruction scheduling). Most of the bookkeeping is performed on the machine-independent \nIR (since most optimizations operate on this IR), and the anno\u00adtations and markers are passed along to \nthe instruction-level representation as the program representation is lowered. This is similar to passing \nhigh-level information such as aliasing information along to a compiler back end for use by an in\u00adstruction \nscheduler. During code selection, annotations are transferred from nodes in the machine-independent IR, \nto the selected instructions. IR marker nodes are lowered to special marker instructions, that convey \nessentially the same information as the IR marker nodes. Instructions are also an\u00adnotated with information \nindicating which instructions cor\u00adrespond to source-level assignments. Additional information is passed \nalong for detecting endangered variables caused by instruction scheduling, as described in [2, 1]. Experimental \nresults To better understand the effect of global optimizations on source-level debugging, we instrumented \nour algorithms to count the number of variables that are endangered at each breakpoint. The charts in \nthis section show the average nulm\u00adber of variables that are uninitialized, current, endangered, and \nnonresident at a breakpoint 5. These numbers were col\u00adlected by counting the number of variables in each \ncategory, for each possible breakpoint in the source program, and ,av\u00aderaging the results by the number \nof breakpoints. (These static numbers assume that all breakpoints are equally likely. We note that a \nlong-term user study that records actual usage patterns is still outstanding.) Our measurements showed \nthat although there are a large number of global variables that can be queried at each bre&#38;\u00adpoint, \nvery few global variables are endangered on the aver\u00adage. Therefore, our figures depict only the results \nfor local variables. Code hoisting did not affect source-level debugging for these programs, and the \nmeasurements in this section show endangerment caused by elimination and sinking of assign\u00adments. The \ncrncc optimizer hoisted mainly address compu\u00adtations. The few source-level assignments that were hoisted \ncode location problems, since they affect setting and reporting of break\u00adpoints. Code location problems \nare discussed in [26] and [1]. 5We se a ~mt of the nonresidency algofithm described in [31. This algorithm \nwas modified to handle live range splitting [1]. Program I % Suspectl E I Sc 9.6% Table 4: Percentage \nof endangered variables that are suspect in Figure 5(a). were also partially dead, and so the subsequent \npartial dead code elimination phase sunk the hoisted assignments down past their original locations, \nto points where they were less frequently executed. Aggressive global scheduling may increase the number \nof source-level assignments that are hoisted. Figure 5(a) shows the results when the programs are com\u00adpiled \nwith global optimization but without global register allocation. Since register allocation is not performed, \nnon\u00adresident variables cannot occur. On average, only about 10-30% of the variables are endangered at \neach breakpoint. Table 4 shows the percentage of endangered variables that are suspect. This table shows \nthat the majority of endan\u00adgered variables are noncurrent. Figure 5(b) shows the results when the programs \nare com\u00adpiled with global optimizations and with register allocation. About half the variables are current \nor uninitialized; these are the good cases, since the debugger can provide accu\u00adrate and meaningful information \nto the user. It is interesting to note that almost all the variables that cause problems for the debugger \nare nonresident. It is worthwhile to compare Figures 5(a) and 5(b): adding global register allocation \ndecreases the number of current variables (the number of uninitialized ones is obviously un\u00adaffected), \nand there are only a few endangered variables; nonresident variables complicate the life of the debugger. \nThis result is not surprising, since we expect the register al\u00adlocator to reuse registers assigned to \ndead variables. (Note that on a machine like the MIPS R3000, there are only 26 integer and 16 floating \npoint registers available for register allocation.) This result suggests that if register allocation \nis performed with dead code elimination, the effects of dead code elimination are manifested in the form \nof nonresident variables, rather than endangered variables. Note that these results are no indication \nof how often a debugger will be able to respond with a variable s expected value during a debugging session \nsince we report an average result for all possible breakpoints. Such measurements would require a user \nstudy. The numbers presented above, however, do give an indication of how different optimization may \nI ~ Uninitialized W Current Endangered H Nonresident I 10 Ii espresso eqntott alvinn ear gcc compress \n(a) Sc li espresso eqntott alvinm ear gcc compress (b) Sc Figure5: Average number oflocdvmiables atabre~point \n(a) Globdoptitizations only(b) Globdoptifizations mdregister allocation. affect a debugger s ability \nto recover a variable s expected value. We conclude from our results that a debugger s ability to retrieve \na source-level value will more likely be affected by nonresident variables than endangered variables, \nif opti\u00admization are performed in conjunction with global register allocation. Moreover, code hoisting \ndoes not cause much of a problem in practice, since this transformation very rarely results in a hoisted \nsource-level assignment. Conclusions In this paper we presented a concise model for the data-value \nproblem and described a simple approach to detecting endan\u00adgered variables caused by global scalar optimization. \nThe solution described in this paper is cast as a data-flow anal\u00adysis problem. Since data-flow analysis \nis a well-understood technique, there are limited obstacles to overcome for an implementation of these \ntechniques in a compiler/debugger. Moreover, the analyses are very similar to other analyses that are \ndone by the compiler and can thus take advantage of an infrastructure that is already present. This is \nin con\u00adtrast to other approaches that require specialized data-flow analyses and program representations \n[13, 24]. To gather the information required for our data-flow analysis, the program intermediate representation \nis annotated during optirnizations to mark hoisted and sunk assignments, and additional mark\u00aders are \ninserted to indicate points from which source-level assignments are eliminated. The data-flow analysis \ncan be performed either by the compiler after optimizations and code generation, or by the debugger, \nNeither the execution time of the analysis phase nor the storage requirements are significant. We have \nused our implementation to measure the effects of optimization on a source-level debugger s ability to \nre\u00adtrieve variable values. Measurements show that a debugger is more likely to be affected by register \nallocation than by other global optimization. Furthermore, hoisting of assignments almost never occurs. \nTherefore, a debugger can take a con\u00adservative approach to detecting endangered variables caused by code \nhoisting (e.g., a hoisted assignment can cause a variable to become nonresident). Hence, a combination \nof residence detection and our simple data-flow algorithm for detecting endangered variables caused by \ndead code elimina\u00adtion is good enough for most practical situations. Moreover, since assignments are \nalmost never hoisted, the code location issue of syntactic versus semantic breakpoints [26, 13, 24] is \nnot important; the simple syntactic breakpoint model is good enough for a useful debugger. There are \nthree noteworthy aspects of our approach that al\u00adlow us to proceed in solving a problem that researchers \nhave struggled with in the past. First, our approach concentrates on two principal global scalar optimizations: \ncode hoisting and dead code elimination. Other global optimization either can be expressed in terms of \nthese optimizations, or do not cause data-value problems. Second, our approach takes advantage of invariants \nmaintained by these two transformations, This makes the problem tractable and enables us to provide addi\u00adtional \ninformation to the user by conveying the actual value of a variable in source terms. Third, our approach \nis inte\u00adgrated with other implemented solutions to problems caused by local instruction scheduling and \nglobal register allocation, described in [2] and [3]. Thus, we are able to address a wide range of common \nglobal and local scalar optimizations in\u00adcluded in most research and production optimizing compilers \nof the last decade. Acknowledgments We thank Ken Lueh for his many contributions to the cmcc compiler. \nWe appreciate comments, suggestions, and en\u00ad couragement by John Ruttenberg and Fred Chow, Si [icon Graphics, \nGeoff Lowney, DEC, and Bruce Olson, HP. References [1] A. Adl-Tabatabai. Source-Level Debugging of Globally \nOpti\u00admized Code. PhD thesis, Carnegie Mellon University, 1996. [2] A. Adl-Tabatabai and T. Gross. Detection \nand recovery of endangered variables caused by instruction scheduling. In Proc. ACM SIGPLAN 93 Con~ on \nPrag. Lunguage Design and Implementation, pages 13-25. ACM, June 1993. [3] A. Adl-Tabatabai and T. Gross. \nEvicted variables and the in\u00adteraction of global register allocation and symbolic debugging. In Conj \nRecord of the 20th AnnualACM Symp. on Principles of Prog. Lag., pages 371-383. ACM, January 1993. [4] \nA. V. Aho, R. Sethi, and J. D. Unman. Compilers Principles, Techniques, and Tools. Addison-Wesley, 1986. \n[5] D. Bernstein and M. Rodeh. Global instruction scheduling for superscalar machines. In Proc. ACM SIGPL4N \n91 Con$ on Prog. Language Design and Implementation, pages 241--255. ACM, June 1991. [6] P. Briggs and \nK. Cooper. Effective partial redundancy elimi\u00adnation. In Proc. ACM SIGPLAN 94 Confi on Prog. Language \nDesign and Implementation, pages 159 170. ACM, June 1994. [7] P.Briggs, K. D. Cooper, K. Kennedy, and \nL, Torczon, Colc}ring heuristics for register allocation. In Proc. ACM SIGPLAN 89 Conf on Prog. Language \nDesign and Implementation, pages 275-284. ACM, July 1989. [8 G. Brooks, G. Hansen, and S. Simmons. A \nnew approach to debugging optimized code. In Proc. ACM SIGPLAN 92 Con~ on Prog. Lunguage Design and Implementation, \npages 1-11. ACM, June 1992. [9 G. J. Chaitin. Register allocation and spilling via graph color\u00ading. In \nProc. ACM SIGPLAN 1982 Symp. on Compiler Con\u00adstruction, pages 98-105, June 1982. In SIGPLAN Notices, \nv. 17, n. 6. [10] P. P. Chang, S. A. Mahlke, W. Y. Chen, N. J. Warter, and W. W. Hwu. Impact: An architectural \nframework for multiple\u00adinstruction-issue processors. In Proc. 18th Intl. Symp. on Com\u00adputer Architecture, \npages 266-275. ACMAEEE, May 1991. [11] P. P. Chang, S. A. Mahlke, and W. W. Hwu. Using profile information \nto assist classic code optimization. Software Practice and Experience, 21(12): 1301-1321, Dec 1991. [12] \nE Chow. A Portable, Machine-independent Global Optimizer Design and Measurements. PhD thesis, Stanford \nUniversity, 1984. [13] M. Copperrnan. Debugging optimized code without being misled. ACM Trans. on Prog. \nLang. Syst., 16(3):387427, May 1994. [14] D. S. Coutant, S. Meloy, and M. Ruscetta. Dot: A practical \nap\u00adproach to source-level debugging of globally optimized code. In Proc. ACM SIGPLAN 88 Conf on Prog. \nLanguage Design and Implementation, pages 125-134. ACM, June 1988. [15] D. M. Dhamdhere. Practical adaptation \nof the global optimiza\u00adtion algorithm of morel and renvoise. ACM Transactions on Programming Lunguages \nand Systems, 13(2):291-294, April 1991. [16] D.M. Dhamdhere, B.K. Rosen, and F.K. Zadeck. How to an\u00adalyze \nlarge programs efficiently and informatively. In Proc. ACM SIGPLAN 92 Conf on Prog. Language Design and \nIm\u00adplementation, pages 212 223. ACM, June 1992. [17] D. Ebcioglu, R. Groves, K. Kim, G. Silbennan, and \n1. Ziv. Vliw compilation techniques in a superscalar environment. In Proc. ACM SIGPJL4N 94 Conf on Prog. \nLanguage Design and Implementation, pages 36-48. ACM, June 1994. [18] C. Fraser and D. Hanson. A Retargetable \nC Compiler: Design and Implementation. BenjarninlCumrnings, 1995. [19] J. L. Hennessy. Symbolic debugging \nof optimized code. ACM Trans. on Prog. Lang. Syst., 4(3):323 344, July 1982. [20] J. Knoop, O. Ruthing, \nand B. Steffen. Lazy strength reduction. Journal of Prog. Languages, 1(1):71 91, 1993. [21] J. Knoop, \nO. Ruthing, and B. Steffen. Optimal code mo\u00adtion: Theory and practice. ACM Trans. on Prog. f.ung. Syst., \n16(4) :1117-1 155, July 1994. [22] J. Knoop, O. Ruthing, and B. Steffen. Partial dead code elimi\u00adnation. \nIn Proc. ACM SIGPLAN 94 Con. on Prog. Language Design and Implementation, pages 147-158. ACM, June 1994. \n[23] E. Morel and C. Renvoise. Global optimization by suppres\u00adsion of partial redundancies. Communications \nof the ACM, 22(2):96-103, Feb 1979. [24 R. Wismueller. Debugging of globally optimized programs using \ndata flow analysis. In Proc. ACM SIGPLAN 94 Conf on Prog. Lunguage Design and Implementation, pages 278 \n289. ACM, June 1994. [25 R. Wismueller. Quellsprachorientiertes Debugging von op\u00adtimierten Programmed. \nPhD thesis, Technische Universitaet Muenchen, Munich, Germany, Dec. 1994. (in German). Pub\u00adlished (1995) \nby Shaker Verlag, Aachen (Germany), ISBN 3-8265-0841-6. [26] P, Zellweger. Interactive Source-Level Debugging \nof Opti\u00admized Programs. PhD thesis, University of California, Berke\u00adley, May 1984. Published as Xerox \nPARC Technical Report CSL-84-5. \n\t\t\t", "proc_id": "231379", "abstract": "Although compiler optimizations play a crucial role in the performance of modern computer systems, debugger technology has lagged behind in its support of optimization. Yet debugging the unoptimized translation is often impossible or futile, so handling of code optimizations in the debugger is necessary. But compiler optimizations make it difficult to provide source-level debugger functionality: Global optimizations can cause the runtime value of a variable to be inconsistent with the source-level value expected at a breakpoint; such variables are called <i>endangered</i> variables. A debugger must detect and warn the user of endangered variables otherwise the user may draw incorrect conclusions about the program. This paper presents a new algorithm for detecting variables that are endangered due to global scalar optimization. Our approach provides more precise classifications of variables and is still simpler than past approaches. We have implemented and evaluated our techniques in the context of the cmcc optimizing C compiler. We describe the compiler extensions necessary to perform the required bookkeeping of compiler optimization. We present measurements of the effect of optimizations on a debugger's ability to present the expected values of variables to the user.", "authors": [{"name": "Ali-Reza Adl-Tabatabai", "author_profile_id": "81100032153", "affiliation": "School of Computer Science, Carnegie Mellon University, Pittsburgh, PA", "person_id": "PP14023844", "email_address": "", "orcid_id": ""}, {"name": "Thomas Gross", "author_profile_id": "81332502168", "affiliation": "School of Computer Science, Carnegie Mellon University, Pittsburgh, PA and Institut f&#252;r Computer Systeme, ETH Z&#252;rich CH 8092 Z&#252;rich", "person_id": "PP39072612", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/231379.231388", "year": "1996", "article_id": "231388", "conference": "PLDI", "title": "Source-level debugging of scalar optimized code", "url": "http://dl.acm.org/citation.cfm?id=231388"}