{"article_publication_date": "05-01-1996", "fulltext": "\n Optimizing ML with Run-Time Code Generation* Peter Lee Mark Leone School of Computer Science Carnegie \nMellon University Pittsburgh, Pennsylvania 15213-3891 petel@cs. emu. edu mleone~cs. cmu. edu Abstract \nWe describe the design and implementation of a compiler that automatically translates ordinary programs \nwritten in a subset of ML into code that generates native code at run time. Run-time code generation \ncanmake useof values and invariants that cannot be exploited at compile time, yielding code that is often \nsuperior to statically optimal code. But the cost of optimizing and generating code at run time can be \nprohibitive. Redemonstrate howcompile-time special\u00adization can reduce the cost of run-time code generation \nby an order of magnitude without greatly tiecting code quality. Several benchmark programs are examined, \nwhich exhibit an average cost of only six cycles per instruction generated at run time. Introduction \n In this paper, we describe our experience with a prototype system for run-time code generation. Our \nsystem, called FABIUS, is a compiler that takes ordhmry programs written in a subset of ML and automatically \ncompiles them into native code that generates native code at run time. The dy\u00adnamically generated code \nis often much more efficient than statically generated code because it is optimized using run\u00adtime values. \nFurthermore, FABIUS optimizes the code that dynamically generates code by using partial evaluation tech\u00adniques, \ncompletely eliminating the need to manipulate any intermediate representation of code at run time. This \nresults in extremely low overhead: the average cost of run-time code generation is about six instructions \nexecuted per generated instruction. Although not every program benefits from run-time code generation, \nwe have had little trouble finding realistic pro\u00adgrams that run significantly faster-sometimes by more \nthan *This research was $ponsored in part by the Advanced Research Projects Agency CSTO under the title \nThe Fox Project: Advanced Languages for Systems Software, ARPA Order No. C533, issued by ESC/ENS under \nContract No. F1962S-95-C-O050. The views and conclusions contained in this document are those of the \nauthors and should not he interpreted as representing the official policies, either expressed or implied, \nof the Advanced Research Projects Agency or the U.S. Government. Permissionto make digiWlwcf copy of \nPM or all of this work for petsonal orclassroomusaisgrantedwithout fee providedthat oopiesare not made \nor distributed for profit or commercial advanta e, the copyright notice, the tiffe of the ublication \nand ita date appear, an $ notice is given that is\\ ypermissionofACM,Ino.Tocopyotherwise,torepublish,to \n?o% servers, or to redmtributeto lists, requires prior spsroifio permission E30r a fee. a factor of four \nwhen run-time code generation is used. In some cases, the ML programs compiled by FABIUS even out\u00adperform \ncorresponding C programs. For example, the BSD packet filter interpreter, which the BSD operating-system \nkernel uses for fast selection of network packets on behalf of user processes [29], runs nearly 40% faster \non typical inputs when translated from C! to ML and compiled by FABIUS. This paper describes our early \nexperience with the FABIUS compiler, with a particular emphasis on low-level optimiza\u00adtion and code-generation \nissues. We begin with a brief dk\u00adcussion of previous approaches to run-time code generation. Then we \ndescribe, using a running example, some of the compilation strategies implemented in FABIUS. Of course, \nit will not be possible to catalog all of our design decisions in this paper, but the example should \nilluminate some key problems, insights, and implementation techniques. Next, we present the results (both \ngood and bad) of evaluating FABIUS using several benchmark programs. We focus ini\u00adtially on two programs: \nmatrix multiplication and network packet filtering. Besides showing good results, these pro\u00adgrams vividly \nillustrate the potential of this technology. We then discuss briefly the performance of several other \nbench\u00admarks and mention the particular lessons learned from each. Finally, we describe current related \nwork in detail, and con\u00adclude with a number of directions for further research. 2 Previous Approaches \nLike the informal principle of the time-space tradeoff, there is also a tradeoff between general-purpose \nand special-pur\u00adpose programs. General-purpose programs are desirable and even necessary for many applications. \nAs a simple exam\u00adple, consider the choice between a general numerical-analysis routine and one that has \nbeen written specially for sparse in\u00adputs. The special version can be much faster, but might not work \nwell in cases where the inputs are dense. Since gener\u00adality usually incurs a penalty in run-time performance, \npro\u00adgrammers still take the trouble to write efficient specialized programs and use them whenever possible. \nUnfortunately, it is not always possible to predict ahead of time whether the specialized versions can \nbe used. Of course, these tradeoffs are aa old as computer pro\u00adgramming, and so is the idea of attacking \nthem with general\u00adpurpose programs that generate special-purpose code at run time. For example, in 1968 \nKen Thompson implemented a search algorithm that compiled a user-supplied regular ex\u00adpression into an \nexecutable finite-state machine in the form of native code for the IBM 7094 [35 J. This program was both \nPLDI S6!Y96 PA, USA 01996 ACM o-69791-795-2 WO005...$505O general (because it accepted any regul= expression) \nand fast (because it generated special-purpose code quickly). Run-time code generation also led to notable \nperformance improvements in the areas of operating systems [4, 8, 26, 33, 36], method dispatch in object-oriented \nsystems [1, 9, 13, 20}, instruction-set simulation [10], graphics [14, 31], and many other applications. \nWith the emergence of highly dk.tributed and Web computing, more applications demand software that is \ngeneral-purpose, safe, and highly compos\u00adable. This trend has prompted increased interest in run\u00adtime \n(and link-time) optimization and code generation as a way to regain the performance advantage enjoyed \nby special\u00adpurpose, monolithic systems [5]. Additional arguments for run-time code generation have been \nwell-summarized by Kep\u00adpel and colleagues [23, 24] 2.1 General-Purpose Dynamic Compilation Despite the \nincreasing attention, researchers have done rela\u00adtively little to automate and optimize the process of \nrun-time optimization and compilation. Indeed, existing approaches, although quite effective for some \napplications, are them\u00adselves rather specialized and require significant effort on the part of the programmer. \nA standard approach is to provide programming support for constructing representations of programs at \nrun time, and for invoking an optimizing compiler to transform such representations into native code. \nThis approach is relatively easy to implement, and systems such as DCG [17] based on this approach have \nbeen shown to be useful in interesting applications. Using DCG, a C programmer can write a pro\u00adgram that \nconstructs intermediate code trees and then in\u00advokes a compiler back-end to translate them into optimized \nnative code that can be dynamically linked and executed. Lisp systems have long provided programmer support \nfor this style of programming, using backquoted S-expressions and eval. The advantage of S-expressions \nis that the pro\u00adgrammer can specify the construction of Lisp terms at the source level, rather than as \nintermediate code trees. A sim\u00adilar features has also been implemented as an extension of Standard ML \n[3]. Recently, Engler and colleagues added support for this style of run-time code generation on top \nof DCG, in a system called C [16], with good results. An even better approach is to design a programming \nlanguage so that run-time code generation can be performed without programmer assistance. For example, \nin an implementation of SELF [9], the system provides run-time compilation of type-specialized versions \nof methods by simply postponing most aspects of optimization and compilation until run time. The problem \nwith this basic approach is that the cost of code generation at run time can be high, to the point that \nthe improvements gained by delaying compilation to run time are eliminated by the cost of run-time compila\u00ad \n tion. For example, DCG S reported overhead of generating an instruction at run time is about 350 instructions \nper in\u00ad struction generated [17],  2.2 Templates It is possible to reduce the cost of run-time code \ngenera\u00adtion by pre-compiling (as much as possible) the code that will be generated at run time. Previous \nwork has focused on the use of templates, which are seqtiences of machine in\u00adstructions containing holes \nin place of some values. Code is generated simply by copying templates and instantiating holes with values \ncomputed at run time; templates may also be concatenated to effect loop unrolling and function inlin\u00ading. \nSeveral systems have used this approach with great success, such as Pike, Locanthi, and Reiser s bitblt \ncom\u00adpiler [31] and in Maasalin and Pu s Synthesis kernel (27]. Until very recently, the use of templates \nhas imposed a significant burden on programmers. Templates and the code that instantiates them are typically \nconstructed manually, which is non-portable and error prone. Recent work has ex\u00adplored the automatic \nderivation of templates from programs written in higher-level languages. For example, the Tempo system \n[11, 36] uses gcc to create machine code templates corresponding to portions of ordinary C programs that \nare classified as dynamic by a binding-time analysis. Chambers and colleagues adopt a similar approach, \nusing the Multiflow compiler to generate templates for programmer-delimited dynamic regions in Modula-3 \nprograms [4, 8]. A major drawback of templates is that they severely limit the range of optimizations \nthat may be applied at run time. A template fixes a particular instruction schedule and chooses fixed \nencodings for instructions, which precludes op\u00adtimization such as run-time code motion and instruction \nselection. It is possible to pre-compile several alternative templates for the same code sequence and \nchoose between them at run time, but to our knowledge this approach has never been attempted in practice. \nFurthermore, instantiat\u00ading templates involves a certain amount of overhead, because a template is essentially \na low-level intermediate represen\u00adtation of a code sequence. Instructions must be copied from the template \nduring code generation, and table lookup is required to locate and instantiate holes. As we shall demon\u00adstrate, \nthis interpretive overhead can be largely eliminated.  2.3 Specialized Code Generators We advocate a \nmore general approach to reducing the cost of run-time code generation that is based on ideas from the \nliterature on partial evaluation [21]. A partial evaluator is a function (called mix for historical reasons) \nthat takes the code of a two-argument function, j, and the value of its first argument, z, and returns \noptimized code for the result of parhally applying f to x: miz(j, z) = f. The result of this optimization \nis code for a specialized one\u00adargument function, fz, that when given an argument, y, computes the same \nresult as f(z, y), but which does so with\u00adout repeating comput at ions that depend only on x. In a similar \nway, partial evaluation can be used to reduce the cost of performing specialization and code generation \nat run time. If the text of a function f is known at compile time, the (run-time) computation of ~z can \nbe optimized by specializing miz to f at compile time: mix(mix, f) = mzxf The result is the code for \na specialized code generator, mizf, called a generating extension, that can be used to compute fz without \nthe overhead of processing the text of f, but retaining all of the optimizations. In essence, the code \nfor ~ is pre-compiled away, so that it is not needed when the argument x is later supplied. The FABIUS \ncompiler is not a partial evaluator, but it does create specialized run-time code generators (aa gener\u00ad \nating extensions) that do not manipulate templates nor any other intermediate representation of code \nat run time. A similar approach has also been employed recently by the tcc compiler for C [15, 32]. In \ncontrast to t cc, FABIUS achieves run-time code generation from completely ordinary ML pro\u00adgrams (rather \nthan depending on language extensions), and is more systematic in its use of well-known techniques and \nheuristics from partial evaluation. These include memoriz\u00adation of specialization, unfolding of static \nconditionals, and the residualization of static values in dynamic contexts [7]. 3 The FABIUS Compiler \nWe have previously given a high-level overview of our ap\u00adproach, which we call deferred compilation [25]. \nThe main goals of deferred compilation are to minimize the costs of run-time code generation while allowing \na wide range of op\u00adtimization in both the statically and dynamically generated code. The goal of the \nFABH.# compiler is to determine the feasibility of this approach, and to provide a way to test various \ndesign choices. I?ABIUS compiles a pure, first-order subset of ML with in\u00adtegers, reals, vectors, and \nuser-defined datatypes. We chose this language primarily for two reasons. First, we are in\u00advolved in \na more general project to investigate the practi\u00adcality of ML for systems programming (specifically, \nnetwork protocols) [6]. Thus, we have a special interest in the opti\u00admization of ML programs. Second, \nML provides good sup\u00adport for expressing staged computations in a way that might be usefully exploited \nby run-time code generation. When a function j of type ~1 + (1-2 A 73) is applied to an argu\u00adment x : \n~1, it may be profitable to generate optimized code for the result of ~(z) rather than simply building \na closure. Even though our current language is first-order, we allow function definitions to use currying \nto express staging, and then compile such functions into run-time code generators. The restriction to \na pure subset of ML is not fundamental to our approach; it merely eliminates the need to analyze evaluation-order \ndependencies, which simplifies implemen\u00adtation. Also, the lack of mutable data structures avoids any \nneed for an analysis of sharing or aliasing, and furthermore allows run-time optimizations to copy values \nfreely. 3.1 A Simple Example We shall use a simple example, an integer dot-product fuac\u00adtion, to demonstrate \nhow FABIUS constructs specialized run\u00adtime code generators. The dot product of two vectors is sim\u00adply \nthe sum of the products of their corresponding elements. In ML, it is typically implemented using a tail-recursive \naux\u00ad iliary function, as follows: fun dotprod (v1, v2) = loop (v1, v2, O, length VI, O) and 100P (v1 \n, v2, i, n, sum) = if i =n then sum else loop (vi, v2, i + 1, n, sum + (v1 sub 1) * (v2 sub i)) 1The \ncompiler i,q named after Quint us Fabius kfaximus, whO as a Roman general best known for his defeat of \nHannibal in the Sec\u00adond Punic War. His primary strategy was to delay confrontation; repeated small attacks \neventually led to victory without a single de\u00adcisive conflict. The auxiliary function, loop, is parameterized \nby the vectors V1 and v2, an index i, the length of the vectors n, and an accumulating parameter sum \nthat tallies the result. The dot-product operation is an especially attractive can\u00addidate for run-time \ncode generation because it is frequently the innermost loop of long-running numerical computations. Matrix \nmultiplication, for example, is often implemented as a triply nested loop: the outer two loops select \na vector from each matrix and the inner loop computes their dot product. The vector selected by the outermost \nloop will be used to compute many dot products, so it may be profitable to cre\u00adate a specialized function \nfor each such vector. To see more clearly the potential for a code improvement, suppose the value of \nthe vector VI is fixed. Because the loop index i depends only on the length of vi, which is fixed, the \nloop can be completely unrolled. Then, constant propaga\u00adtion and constant folding can eliminate the computation \nof (vl sub i). Because subscripting in ML requires bounds checking, these optimizations can yield a substantial \nbene\u00adfit. Finally, if VI is sparse, a simple reduction of strength eliminates the multiplication, addition, \nand subscripting of V2 whenever (vI sub i) is zero. All these optimizations can be performed only at \nrun time, once the values of VI, i, and n have been computed. The fist step taken by FABIUS is to identify \nthe early computations that can be performed by the statically gen\u00aderated code and the late computations \nthat must appear in the dynamically generated code. This is a difficult task to automate because it requires \nglobal knowledge of the im\u00adplicit staging of computations in a program. 2 For the mo\u00adment, FABIUS relies \non the programmer to provide simple hints that allow a local analysis to uncover the required in\u00adformation. \nSyntactically, these hints are given by declaring functions so that they take their arguments in curried \nform. When a curried function is applied to its first argument referred to as the ~fearly argument-it \ninvokes a code gen\u00aderator to create a specialized function that is parameterized by the remaining late \narguments. FABNJS employs a de\u00adpendency analysis to extend this classification of function parameters \nto the body of the function, thereby producing an annotation of each subexpression as being either early \nor late. For example, the dot-product function is annotated as follows, where an overline indicates an \nearly annotation and an underline indicates a late annotation: fun dotprod Z ~ = T6@ (m, 0, length vI) \n(M, Q) snd loop (n, 1, ii) (V2, ~) = = ~ then sum else G@ (iZ,-H, ii) (v2, sum + ~i) * (v2 sub i)) Note \nthe use of a curried definition to express the staging of computation in the loop function. The variables \ni and n are specified as early parameters, because they depend only on the length of V1, so the conditional \ntest is also early, and so on. Function applications have also been given early anno\u00adtations, which is \ntaken as an indication that they should be inlined at run time. Determining the treatment of function \ncalls is actually a rather subtle problem, which we do not discuss here. (See [25] for details.) In particular, \nthis staging analysis is more difficult than binding\u00adtime analysis because an initial division of program \ninputs is not supplied by the programmer [25]. After annotating the program, FABIUS compiles it into \na register-transfer language, also containing early and late annotations, which in turn is translated \ninto a representation of annotated MIPS assembly code. The annotated MIPS code for the dot-product loop \nis shown below. (In the inter\u00adests of clarity, we have substituted symbolic register names and omitted \nthe early annotations.3 ) loop: beq $i, $n, LI Sll $il, $i, 2 , Shift to scale index addu $pl, $vI, $il \n; Compute address lW $X1> ($pl) > Fetch number Sll $i2, $i, 2 , Shift to scale index &#38; ~, $i2($p2) \n; Fetch, immed. offset !@.1 $prod, $x~@ ~ $sum, $sum, $prod _ addi $i, $i, 1 loop Ll: -$result, $sum \nj $h-a Return j This code is actually a specialized run-time code genera\u00adtor. The early instructions \nare simply executed, but the late instructions are emitted into a dynamic code segment, as explained \nbelow. For example, when supplied with the vector V1 = [1, 2, 31, the following dot-product function \nis dynamically created: lW $x2, o ($V2) rrmlt $prod, 1, $x2 add $sum, $sum, $prod lW $x2, 4($v2) mult \n$prod, 2, $x2 add $sum, $sum, $prod lw $x2 , 8($v2) mult $prod, 3, $x2 add $sum, $sum, $prod move $result, \n$sum Even though this code is generated in a single pass by a relatively simple code generator, it is \nextremely efficient. All the loop operations are early, so the generated code is completely unrolled. \nAll the operations involving the vector V1 are performed by the code generator, amd its elements appeax \nas immediate in the emitted code. And because the index (i) is early, the byte offsets used to access \nV2 are hard-wired into the generated code.  3.2 Emitting Native Code Emitting instructions is quite \nsimple. FABWS adds initial\u00adization code to the compiled program to allocate a dynamic code segment; a \ndedicated register, called the code pointer ($cP), tracks the address of the next available word. Each \nlate instruction is translated into code that constructs the native encoding of the instruction, writes \nit to the dynamic code segment, and advances the code pointer,4 For example, 3There are other simplifications \nas well: we omitted the code for subscript checkhg and run-time instruction selection, and used a sin\u00ad \ngle pseudo-instruction for multiplication that actually requires mul\u00ad tiple instructions to implement \non the MIPS. Also, untagged inte\u00ad gers are used in this and all other examples (see Section 5). The sstute \nreader may notice that this code is not optimal: it could be improved slightly by common subexpression \nelimination and better pointer arithmetic. 4For ~~ciencY, multiple updates of the code pointer within \na basic block are coalesced. ~ $sum, $sum, $prod is emitted as follows: li $tO , x852020 ; Instruction \nencoding Sw $to, ($cp) , Write to code segment addiu $cp, $cp, 4 Advance code pointer Notice that no \nintermediate representation is required at run time. Of course, emitting code and then immediately executing \nit involves some subtle interactions with the cache\u00admemory system and the instruction prefetching mechanism, \nwhich we discuss in Section 3.4. Overall, the cost of code emission is about six instructions per instruction \ngenerated, as additional overhead is incurred by run-time instruction selection (described below) and \na memorization mechanism (discussed in Section 3.5). Rather than loading a 32-bit immediate value (which \nusually requires two cycles on the MIPS), the instruction encoding could instead be copied from a template. \nDoing so is slightly more complicated, because a pointer to the template must be maintained in a register. \nAlso, not all emitted instructions have a fixed encoding. For example, the following load instruction \n&#38; &#38;&#38;, $i2($p2) has an immediate offset that is specified by the value in $i2. It is emitted \nas follows: lui $to , X8C66 ; Upper half is opcode or $tO, $tO, $i2 ; Load half is offset Sw $to, ($cp) \naddiu $cp, $cp, 4 However, because the MIPS restricts immediate values to 16 bits, this encoding is valid \nonly when the value in $i2 is relatively small. The next section describes how FABIUS employs run-time \ninstruction selection to determine when such instructions can be used.  3.3 Run-Time Instruction Selection \nMuch of the benefit of run-time code generation can be viewed as a kind of run-time loop invariant removal: \nthe ex\u00adecution frequency of eaxly computations is reduced in a way that cannot be achieved by compile-time \noptimizations. An\u00adother benefit of run-time code generation arises when early values are used in late \ncomputations. In such cases, data\u00addependent optimizations can yield code that is superior to statically \noptimized code. In the preceding dot-product example, the use of an im\u00admediate-offset load instruction \nto load a value from V2 is cor\u00adrect only when the byte offset of the element can be specified by a 16-bit \nimmediate value. Otherwise, code must be gen\u00aderated to load the offset into a register (which requires \ntwo instructions on the MIPS), followed by a register-register addition and a load. FABWS therefore translates \n&#38; ~, $i2($p2) into the code shown in Figure 1. FABIUS implements a number of similar forms of run\u00adtime \ninstruction selection. Of course, the benefits of such optimizations must be weighed against their cost, \nbut in our experience their cost is quite low (because no interme\u00addiate representation is manipulated) \nand their benefit is of\u00adten very significant. For example, run-time strength reduc\u00adtion greatly improves \nthe performance of dot product on li $tO, x8000 Scale i2 for addu $tO, $i2, $tO 1 unsigned comparison \nli $tl, Xffff Situ $to, $ti, $to ; Does i2 fit in 16 bits? $tO, $0, L1 $tO, $12, 16 ; Get upper halfofi2 \n@, $to ; Load upper half $tO, $i2, xffff ; Getlower halfof i2 @, $to  Load lower half $p2, $V2, $to \ng, z2) j L2 Ll: $&#38;, $i2($v2) Figure 1: Specialized run-time instruction selection sparse input \nby completely eliminating the computation of sum + ~1 ) * (v2 sub i) whenever (vI sub i) is zero. As \nwe demonstrate e in Section 4.1, this optimization allows the performance of a general-purpose ML implemen\u00adtation \nof matrix multiply to compete favorably with C code designed specifically for sparse matrices. 3.4 Instruction \nCaching and Prefetching Unfortunately, modern cache architectures are not designed with run-time code \ngeneration in mind. Most memory sys\u00adtems employ separate data and instruction caches, and many do not \ninvalidate cached instructions when Memorywriter occur, because itisaasumed that self-modifying code \nisrare. The run-time code generators created by FABIUS do not modify existing code, but it is crucial \nthat they be able to reuse code space. Thus, invalidating or updating the instruction cache is necessary \nto ensure that it remains co\u00adherent with memory. Coherency mechanisms vary, but portability does not \nap\u00adpear to be a major concern [22]. The cost of instruction\u00adcache invalidation varies widely, however, \nOn the DEC\u00adstation 5000/200, flushing the instruction cache requires a kernel trap plus approximately \n0.8 nanoseconds per byte flushed [22]. Fortunately, it is relatively easy to amortize this cost. Because \nF.mms-generated code generators do not modify existing code, it is not necessary to flush individual \nwords from the instruction cache as new instructions are written. Instead, when code is garbage collected \nthe freed space can be invalidated in a single operation. We have encountered several other technical \nchallenges related to instruction caching. For example, w we have shown in earlier sections, run-time \nonstants are com\u00admonlypropagated into run-time generated code, where they appear aa immediate values. \nPointers toheap-allocated struc\u00adtures may also be propagated, so a garbage collector must update or invalidate \ncode if the referenced data is copied to a different location. This might be difficult, however; on the \nMIPS a 32-bit pointer is split into two 16-bit immediate val\u00adues whose location in the dynamically generated \ncode can be unpredictable. Instruction prefetching also complicates matters, because programs compiled \nby FABIUS interleave code generation with the execution of generated code. Because instructions are usually \ncached on a line-by-line basis, several locations immediately preceding and following a run-time-generated \nfunction may be cached when it is executed. But the instruc\u00adtions that will appear in these locations \nmay not yet have been written, so the cache will become incoherent. FABIUS solves this problem by aligning \neach newly generated func\u00adtion to the start of an instruction-cache line.  3.5 Other Issues In this \nsection we mention some additional complexities that we have encountered in the implementation of FABIUS. \nDue to space constraints, we can do little more than hint at the solutions we have adopted and possible \nalternatives. The dot-product example shown previously is actually quite misleading in its simplicity. \nFor example, the treat\u00adment of function calls was simplified by the use of inlining: the tail-recursive \ncall in the loop function was merely com\u00adpiled into a jump to the start of the code generator. Curried \napplications that are not inlined are instead compiled into two calls, the first to a rnemoized code \ngenerator and the second to the address returned by the first call. Memo\u00adrization is currently implemented \nusing a per-procedure log that records the entry points of previously optimized code and the values that \nwere used to optimize it. For efficiency, values are compared using pointer equality to determine whether \ncode can be reused. However, this strategy causes unacceptable duplication of effort in some benchmwks \nbe\u00adcause structurally equivalent values fail to match, One-pass run-time code generation is fundamental \nto our approach. As we have demonstrated, FABIUS is able to com\u00adpletely eliminate the overhead of processing \nintermediate code at run time by hard wiring optimizations and instruc\u00adtion encodings into run-time code \ngenerators. However, this approach compromises our ability to generate high-quality code. For example, \nit is surprisingly difficult to avoid creat\u00ading jumps to jumps when generating code for conditionals \nat run time. Other desirable optimization, such as instruc\u00adtion scheduling, are difficult to accomplish \nin a single pass because the structure of the code is often determined by dynamic inlining. Improved \nregister usage is a benefit of run-time code gen\u00aderation that bears mention. The use of symbolic register \nnames obscures this effect in the preceding examples, but it can be observed by considering their live \nranges. Although early and late instructions are textually adjacent, they oc\u00adcupy different live ranges \nand may therefore use overlapping register assignments. For example in the dot-product loop, FABIUS assigns \nthe variables VI and V2 to the same register. Currently all register assignments are determined at compile \ntime, which makes instruction encodings easy to construct at run time. However, this practice defeats \nsome of the advantages of run-time inlining: the registers that are free at one call site may be unavailable \nat another. FABIUS is forced to insert spill code around inlined code in several of the benchmarks we \nhave examined. We propose an efficient solution to this problem in (25]. 4 Preliminary Results To evaluate \nthe performance of FABIUS, we have experi\u00admented with a number of small and medium-sized bench\u00admark programs. \nBecause the current FABIUS prototype does not yet have a gaxbage collector, the measurements we re\u00adport \nhere do not reflect the cost of reclaiming data and code space. Also, for reasons discussed in Section \n5, we have not yet implemented a tagging scheme for integers and point\u00aders. We note, however, that these \nissues probably have only a small effect on the measurements. For example, garbage collection of the \ncode heap is unnecessmy in the packet-filter benchmark, and thus its absence is of little consequence. \nThese issues are discussed further in Section 5. Time (s) 18 I I I I i I I I 16 - I Fabius, No RTCG 14 \n- 12 - 10 - / -I Fabius, dense Special-purpose C, dense 8 - : _ Conventional C 6 - 4 - 2 - Fabius sparse \nSpeciai-purpose C, sparse 20 40 60 80 100 120 140 160 180 200 Dimension (n) Figure 2: Time to multiply \ntwo n x n matrices (dense and sparse) We executed the benchmark programs on an unloaded DECstation 5000/200 \nthat was equipped with a timing board providing an accurate 12.5 MHz memory-mapped clock. Thk machine \nwas not run in single-user mode, and so transient lbads caused occasional variation of execution times. \nTo minimize this effect, we selected the minimum elapsed time observed during five iterations of each \nbenchmark trial. 4.1 Matrix Multiplication We implemented a simple integer matrix multiplication rou\u00adtine \nin ML using the dot product function described in Sec\u00adtion 3.1 and evaluated its performance, with and \nwithout run-time code generation, on both dense and sparse matri\u00adces. For comparison purposes, we also \nevaluated the perfor\u00admance of two matrix mult implication algorithms implemented in C. One was a conventional \ntriply nested loop (in row\u00admajor order) and the other was a special-purpose algorithm based on indirection \nvectors [17], which is well-suited to sparse matrices that lack predictable characteristics. The ML implementation \nwas purely functional and used dynamically dimensioned arrays, which were implemented using immutable \none-dimensional vectors; each subscript op\u00ad eration included two bounds checks. Both C implementa\u00ad tions \nused statically allocated two-dimensional arrays (with\u00ad out bounds checking), and were compiled by gcc \n(version 2.3.2) with -02 optimization. The input matrices contained untagged pseudo-random 16-bit integers; \nsparse matrices were 90~o zero, and the non-zero elements were randomly located. Figure 2 compares the \noverall execution times, includ\u00ading time spent generating code, of the ML code compiled by FABrUSwith \nthe C code, on both dense and sparse matri\u00adces. Without run-time code generation the ML code was nearly \ntwice as slow as the conventional C code, mainly because of array-bounds checking. Run-time code gener\u00adation \nsignificantly improved its performance on both dense and sparse inputs, even for small inputs. On dense \ninput the FABIuS-generated code matched the performance of the special-purpose C code and was 107o slower \nthan the con\u00adventional C code on 200 x 200 matrices. On sparse input the FABIUS-generated code was a \nfactor of 4.5 faster than the conventional C code on 200 x 200 matrices, and only 39yo slower than the \nspecial-purpose C code. Similar improve\u00adments were also observed for floating-point matrix multiply. \nThe time spent generating and managing code at run time was insignificant, representing less than one \npercent of the overall execution time. The cost of run-time code gener\u00adation was recovered even for small \nmatrices: the break-even point for n x n dense matrices was n = 20, and the break\u00adeven point for sparse \nmatrices was n = 2. An average of 4.7 instructions were required to generate an instruction at run time. \nSpace usage was significant, however. At n = 200, each dot-product function created at run time required \n6.25 kilobytes (although better scheduling could reduce this by 25%). Efficient reuse of code space is \nclearly a requirement: wit bout it, the total memory footprint is over a megabyte for large matrices. \n 4.2 Packet Filtering A packet filter is a procedure invoked by an operating system kernel to select \nnetwork packets for delivery to a user-level process. To avoid the overhead of context switching on every \npacket, a packet filter must be kernel resident. But kernel residence has a dk.tinct disadvantage: it \ncan be difficult for user-level processes to specify precisely the types of packets they wish to receive, \nbecause packet selection criteria can be quite complicated. Many useless packets may be delivered as \na result, with a consequent degradation of performance. A commonly adopted solution to this problem is \nto pa\u00adrametrize a packet filter by a selection predicate that is dynamically constructed by a user-level \nprocess [30, 29]. A selection predicate is expressed in the abstract syntax of a safe or easily verified \nprogramming language, so that it can be trusted by the kernel. But this approach has sub\u00adstantial overhead: \nthe selection predicate is reinterpreted every time a packet is received. Run-time code generation can \neliminate the overhead of interpretation by compiling a selection predicate into trusted native code. \nMore generally, run-time code generation can allow a kernel to efficiently execute agents supplied by \nuser-level processes while avoiding context switches. Such an approach has also been investigated by \nothers [5, 18]. To investigate the feasibility of this idea, we implementetl the BSD packet filter language \n[29] using FABIUS and com\u00adpared its performance to BPF, a kernel-resident interpreter implemented in \nC [28]. The interpreter shown in Figure 3 is a simple ML function, called eval, that is parameterized \nby the filter program, a network packet, and variables that encode the machine state. Note that eval \nis curried: when applied to a filter program and a program counter, the re\u00adsult is a finci!ion that is \nparameterized by the machine state and the packet. As with the matrix-multiply example, eval is written \nas an ordinary ML function, with no explicit indi\u00adcation of run-time code generation (in fact, it is \nstill a legal ML program) and no specification of staging of computa\u00adtions beyond the currying of its \narguments. FABIUS compiles eval into a run-time code generator that evaluates all of the operations involving \nthe filter pro\u00adgram (such as instruction decoding) and generates code for all operations involving the \npacket data and the machine state. For example, consider the following filter program, which selects \npackets whose Ethernet type field specifies an 1P packet: LD 4 ; Accum. gets 5th pkt word. RSH 16 ; Shift, \nyielding type field. JEQ ETH.IP , L1 ; Is this an 1P packet? RET i ; If so, accept. L1: RET O The following \nMIPS code is generated for this packet filter at run time: 0 move St, 4 slt $t, $t, $n ; Is offset < \npkt size? beq $t, $0, L1 lW $a, i6($p) ; Accum. gets 5th pkt word. srl $a, $a, 16 ; Shift, yielding type \nfield. move $t, ETH-IP beq $a, $t, L2 ; Is this an 1P packet? move $r, O ; Reject if not. j L3 L2 : \nmove $r, 1 ; Else accept. L3:j L4 Li : li $r, -1 ; Indexing error. L4: jr $ra  This code is not quite \noptimal because the run-time code generator has failed to eliminate two jumps whose targets are jumps. \nBut it does demonstrate several interesting run\u00adtime optimizations: The most beneficial optimization \nis a kind of run-time loop invariant removal: all operations involving the packet filter and the program \ncounter, such as instruc\u00adtion fetching and decoding, have been performed by the code generator. s~ymboli~ \n~egi~ter ~am~s have been added fOr ClaritY: $a cOntain$ the accumulator, $P is a pointer to the Packet, \n% is the number of words in the packet, $r is the result register, and $t is a temporary. Delay slots \nare omitted; all are filled with nop instructions. Run-time constant propagation has embedded val\u00adues \nfrom the filter program (such as the load offset, the shift amount, and the ETH.IP constant) as immediate \nin the run-time generated code.  Run-time constant foldhg has also occurred: be\u00adcause the load offset \nis a run-time constant, the code generator has folded the index scaling implicit in pkt sub k. The resulting \nbyte offset is used in a load-immediate instruction.  Run-time inlining has eliminated all of the tail-recur\u00adsive \ncalls in eval~ These jumps are performed by the code generator, not the generated code.  Larger packet \nfilter programs show similar improvements. Figure 4 compares the overall execution times of the FABIUS \npacket filter implementation (including time spent generat\u00ading code) to the BPF implementation in C, \nusing a packet filter that selects non-fragmentary TCP/IP packets destined for a Telnet port. This packet \nfilter must parse the 1P header, the length of which may vary, to determine the lo\u00adcation of the TCP \ndestination port. To reliably compare execution times, we obtained five sample packet traces by eavesdropping \non a busy CMU network, and we averaged execution times over these traces as a precaution against abnormal \npacket mixes. We modified the BPF interpreter to read packets from these trace files, compiled it with \n-02 optimization, and executed it in user mode. The timings re\u00adported here exclude the time required \nto read packets from the trace files. As the figure demonstrates, the time spent generating code at run \ntime was quickly repaid: the FABIUS implemen\u00adtation broke even with BPF after approximately 250 pack\u00adets. \nAfter only 1000 packets, execution time was reduced by 30.3%. The time spent generating code was brief, \nto\u00adtalling 1.3 ms. Instrumentation revealed that the run-time code generator executed an average of only \n5.6 instructions per instruction generated. The run-time generated code re\u00adquired an average of 8.3 ps \nto process a packet, whereas BPF averaged 13.7 ps per packet. Space usage was insignificant. The abstract \nsyntax of the packet filter program occupied 34 words in both imple\u00admentations, and the FABIUS implementation \ngenerated 85 instructions at run time. The run-time code generator per\u00adformed a small amount of heap \nallocation (43 words), but none was required by the run-time generated code. Stack us\u00adage was also insignificant, \nbecause tail-recursive calls were compiled into jumps. 4.3 Additional Benchmarks We now briefly summarize \nour experience with seven ad\u00additional benchmark programs written in ML. Some of the benchmarks are trivial \nin size, such as a three-line function for determining set membership, but are significant because of \ntheir widespread use in ML programs. Others represent real-world applications, such as a program that \ndetermines the three-dimensional structure of RNA molecules. Figure 5 shows the overall execution times \nof six of the seven benchmarks for varying input sizes (the input size of the seventh benchmark cannot \nbe varied). In the interest of brevity, we simply compare the performance of FABIUS\u00ad generated code with \nand without run-time code generation (using curried and uncurried functions, respectively), and we do \nnot report details of space usage, which was generally val eval : int vector * int -> lnt * lnt * lnt \nvector * int vector -> int fun eval (filter, PC) (a, x, mem, pkt) = let val pc =pc +2 in if pc >= length \nfilter then 1 else let val instr = filter sub pc val opcode = instr >> 16 in (* Load immediateinto accumulator. \n*) pa&#38;etwordat,offset if opcode = LD-ABS then let val k = filter sub (pc+l) in if k >= length pkt \nthen 1 else eval (filter, pc) (pkt sub k, x, mem, pkt) end ... (* Jump if accumulator equals immediate. \n*) else if opcode = JEQ-K then ifs= filter sub (pc+l) then eval (filter, pc + ((instr >> 8) andb 255)) \n(a, x, mem, pkt) else eval (filter, pc + (instr sndb 255)) (a, x, mem, pkt) ... end end Figure 3: Packet \nfilter interpreter The BSD packet filter language is comprised of RISC-Iike instructions for a simple \nabstract machine with an accumulator, an index register, and a small scratch memory. The abstract machine \nis made safe by confining memory references to the packet data and scratch memory and by forbidding backwards \njumps. Instructions are encoded as pairs of 32\u00adbit words. The ilrst word specifies a 16-bit opcode and, \noptionally, a pair of 8-bit branch offsets. The second word specifies an immediate valuethat can beusedfor \nALUoperations and memory indexing. Time (ins) 14 ~gcc -02 12 10 : FABIUS 8 6 oJ--- \u00ad 1 I I I I I I \nI I 10 100 200 300 400 500 600 700 800 900 1000 Number of Packets Figure 4: Run-time code generation \nfor a packet filter Without RTCG ~ With RTCG ~....~ Time (s) 5(a) Cor@gate Gradient Time (ins) 3.0 \n~ I I I I I I I I I 50 I I 45 - 40 - 35 - 30 - 25 - 20 - 15 - 10 - 5 - 4 n 20 40 60 Dimension (n) Time \n(ins) 5(c) Association List Lookup Time (ins) 9- II 8\u00ad7\u00ad6\u00ad5\u00ad4\u00ad3\u00ad lt II   L _ J 2 01 20 40 60 80 \n100 120 140 160 180 200 20 40 60 Attempted Iookups Time (s) 5(e) Game of Life Time (s) 80 6[ III II \n70 \u00ad 5 60 \u00ad4 50 \u00ad 1 40-3\u00ad30 \u00ad 2\u00ad20 \u00ad1 10 \u00ad 00 1 2 3 4 5 100200300 Glider guns Figure 5: Additional \nBenchmarks 5(b) Regexp Matching III III I ) 80 100 120 140 160 180 200 Attempted matches 5(d) Set Membership \nIIIIII -i I II IIII 80 100 120 140 160 180 200 Membership tests 5(f) Insartion Sort I I I I I I1 .0 \n.4 t .@ . .0 . a .. .0 400 500 600 700 800 900 1000 Words sorted  li $tmp, 3 beq $arg, $tmp, L5 li $%rnp, \n2 beq $arg, $tmp, L3 li $tmp, 1 beq $arg, $tmp, L1 . . . . ; Else signal an error. Ll: li $result, 1 \nL2: j L4 L3: li $result, 2 L4: j L6 L5: li $result, 3 L6: jr $ra Figure 6: An executable association \nlist low. Thetotal cost ofrun-time code generation varied from one benchmark to the next, but its relative \ncost was low, averaging roughly six cycles per generated instruction. The fist benchmark (Figure 5a) \nisaniterative solver for sparse linear systems of equations, adapted from [37]. It is based on the conjugate \ngradient method, which finds so\u00adlutions to systems of equations of the form Ax = b, where A is a square, \nsymmetric, positive-definite matrix of double\u00adfloating-point values. This program is amenable to run-time \ncode generation because the coefficient matrix, A, is mul\u00adtiplied with the gradient direction vector \non each iteration, but never varies. The system of equations used as input was tri-diagonal, and as the \ngraph demonstrates, run-time code generation was able to exploit this sparsity, yieldlng a 2.4-fold speedup \nfor a system of 200 equations. Because the cost of run-time code generation was low, performance on smaller \nsystems of equations was also superior to the stati\u00ad cally optimized code. Figure 5(b) demonstrates the \nimprovement obtained in a regular-expression matching algorithm using run-time code generation. Unlike \nThompson s compiler for regulw expres\u00adsions [35], our program is a simple bxktracking interpreter. FABIUS \ncompiled it into a code generator that, given a reg\u00adular expression, creates a finite state machine (in \nnative MIPS code). The input was a regular expression describing words containing the five vowels in \norder (e.g. facetious ), matched against the first n lines of /usr/diet/words. A significant speedup \nwas observed even for small input sizes: the cost of run-time code generation waa recovered after only \n20 matches, and a 3.4-fold speedup was attained for 200 mat ches. Association lists are a common data \nstructure in ML pro\u00adgrams; they are often used as symbol tables, mapping keys of one type to values of \nanother. Looking up a value in an association list can be time consuming: two memory ac\u00ad cesses are required \nto fetch the key of the entry at the head of the list, and if it does not match an additional mem\u00adory \naccess is required to locate the next entry. Figure 5(c) demonstrates that run-time code generation can \namortize this overhead when multiple lookups are performed on the same aasociat ion list. FABKJS automatically \ncompiles a cur\u00adried lookup function into a code generator that, in essence, creates executable data structures \n[26] that require no mem\u00adory accesses; a sample of the lookup code generated at run time is contained \nin Figure 6. The task of determining set membership is also well suit\u00aded to run-time code generation, \nbecause repeated member\u00adship tests on the same set require duplicate effort that can be amortized. Figure \n5(d) demonstrates the effect of run-time code generation on a simple curried membership function written \nin ML, and Figure 5(e) shows the performance im\u00adprovement observed in a commonly used ML benchmark, Conway \ns game of life (adapted from [2]), which uses a set to record the locations of living cells. Run-time \ncode generation does not always improve per\u00adformance, of course. One promising application we consid\u00adered \nwas an insertion sort, which repeatedly chooses a string from the head of a list and compares it to the \nrest of the strings in the list until a lexically greater one is found. If the lexical comparison function \nis curried, FABIUS compiles it into a code generator that performs all of the subscript\u00ading operations \non the first string and unrolls the comparison loop, yielding a nest of comparisons involving constant \nchar\u00adacters. As Figure 5(f) demonstrates, however, optimizing the lexical comparison function at run \ntime does not im\u00adprove performance, despite using input that requires many repeated comparisons to sort \n(in this case, a reverse-sorted prefix of /usr/diet /words). In practice, most lexical com\u00adparisons do \nnot examine more than two or three characters of each string, so the time spent generating code for the \nremaining characters is wasted. We also experimented with the Pseudoknot benchmark [19], which is a floating-point \nintensive program that at\u00adtempts to determine the three-dimensional structure of por\u00adtions of RNA molecules \nusing constraint satisfaction and backtracking search. Each step of the search investigates possible \nplacements of a nucleotide by checking whether these placements violate certain constraints, which are \nspec\u00adified as input to the program. The constraint-checking step is actually unnecessary for most nucleotides, \nbut the general\u00adpurpose nature of the program does not allow this property to be exploited by compile-time \noptimization. We used FABIUS to create a specialized search function that performs constraint checking \nonly when truly necessary. However, do\u00ading so did not have the desired effect: overall execution time \nwas reduced by approximately five percent (the problem size was fixed). This appears to be due to a peculiar \nproperty of the problem domain: nucleotides that do not require con\u00adstraint checking have very few possible \nplacements, so the overhead eliminated by run-time code generation was not large enough to repay the \ncost of run-time optimization. Even when no significant improvements are to be had, the low cost of run-time \ncode generation means that compa\u00adrable performance is still possible. This greatly reduces the risk of \nautomating the staging decisions in the compiler. 5 Conclusions and Future Work We have implemented an \napproach to run-time code gen\u00aderation that is both principled and practical. The use of ML allows the \ncompiler to perform run-time optimizations with little effort on the part of the programmer. It also \nfa\u00adcilitates a substantial use of partial evaluation techniques to optimize the optimization process. \nIndeed, keeping the cost of run-time code generation low appears to be critical for languages such as \nML, because typical programs do not manipulate large data sets and cannot easily amortize the cost incurred \nby a general-purpose run-time compiler. Further experimentation will be required to fully evalu\u00adate \nthe progress that has been made thus far, and we have identified numerous areaa for further research. \nWe are cur\u00adrently extending the prototype FABIUS compiler to support a richer source language, includlng \nmutable data structures and higher-order functions. We also plan to investigate the feasibility of run-time \nregister assignment, scheduling, and other run-time optimizations. We have postponed implementing a major \ncomponent of the FABIUS system, a tagging scheme for integers and point\u00aders, because of recent advances \nin type-directed compilation. Existing ML compilers commonly use tags to distinguish between integers \nand pointers to support efficient garbage collection, but this approach has undesirable side effects: \nintegers are restricted to 31 bits and numerous tagging and unt agging operations are involved in arithmetic \noperations. The TIL compiler project [34] has shown that a principled use of types at compile time and \nrun time permits most ML code to be compiled into tag-free, monomorphic code. Hence, we have focused \non efficiently compiling such code and have ignored the problem of tagging. Our experience with benchmark \nprograms haa led us to the conclusion that this technology is usable but not yet foolproof. The use of \na high-level programming language greatly reduces the programmer effort required to make use of run-time \ncode generation, but it can be difficult for the programmer to predict program behavior. Memorization \ncan be expensive and is sometimes ineffective, and the heuristic we employ to control run-time inlining \noccasionally leads to over-specifllzation or under-specialization. Recent advances in type theory have \nsuggested a mechanism for providing better feedback to programmers [12]. Ultimately, the feasibility \nof deferred compilation will have to be demonstrated on larger, more realistic programs. It is encouraging \nto see that a prototype such as FABIUS can already achieve good results. References [1] AGESEN, O., AND \nHOLZLE, U. Type feedback vs. concrete type inference: A comparison of optimiza\u00adtion techniques for object-oriented \nlanguages. In OOP-SLA 95 Conference on Object-Oriented Programming Systems, Languages, and Applications \n(October 1995). [2] APP~L, A, W. Compiling with Continuations. Cam\u00adbridge University Press, New York, \n1992. [3] APPEL, A. W., AND MACQUEEN, D. B. Separate compilation for Standaxd ML. In PLDI 94 Conference \non Programming Language Design and Implementation (June 1994), pp. 13-23. [4] AUSLANDER, J., PHILIPOSE, \nM., CHAMBERS, C., EG-GERS, S. J., AND BERSHAD, B. N. Fast, effective dy\u00adnamic compilation. In PLDI 96 \nConference on Pro\u00adgramming Language Design and Implementation (May 1996). [5] BERSHAD, B., SAVAGE, S., \nPARDYAK, P., SIRER, E. G., BECKER, D., FIUCZYNSK1, M., CHAMBERS, C., AND EGGERS, S. Extensibility, safety \nand performance in the SPIN operating system. In Sgmposiwn on Oper\u00ad  ating System Principles (December \n1995), pp. 267-284. [6] BIAGIONI, E., HARPER, R., AND LEE, P. Signatures for a protocol stack: A systems \napplication of Standard ML. In Conference on Lisp and Functional Program\u00adming (June 1994). [7] BONDORF, \nA., AND DANVY, O. Automatic autopro\u00adjection of recursive equations with global variables and abstract \ndata types. Sci. Comput. Programming 16, 2 (September 1991), 151-195. [8] CHAMBERS, C., EGC?ERS, S. J., \nAUSLANDER, J., PHILI-POSE, M., MOCK, M., AND PARDYAK, P, Automatic dynamic compilation support for event \ndispatching in extensible systems. In WCSSS 96 Workshop on Com\u00adpiler Support for System Software (February \n1996). [9] CHAMBERS, C., AND UNGAR, D. Customization: Op\u00adtimizing compiler technology for SELF, a dynamic\u00adally-typed \nobject-oriented programming language. In PLDI 89 conference on Programming Language Design and Implementation \n(June 1989), pp. 146-160. 10] CMELIK, R. F., AND KEPPEL, D. Shade: A fast instruction-set simulator for \nexecution profiling. Tech. Rep. 93-06-06, Department of Computer Science and Engineering, University \nof Washington, June 1993. 11] CONSEL, C., AND NOEL, F. A general approach to run-time specialization \nand its application to C. In POPL 96 Symposium on Principles of Programming Languages (January 1996), \npp. 145-156. [12] DAVIES, R., AND PFENNING, F. A modal analysis of staged computation. In POPL 96 Symposium \non Principles of Programming Languages (January 1996), pp. 258-270. [13] DEUTSCH, L., AND SCHIFFMAN, \nA. M. Efficient im\u00adplementation of the Smalltalk-80 system. In POPL 84 Symposium on Principles of Programming \nLanguages, Salt Lake City (January 1984), pp. 297-302. [14] DRAVES, S. Compiler generation for interactive \ngraph\u00adics. In Dagstuhl Seminar on Partial Evahation (Febru\u00adary 1996). [15] ENGLER, D. R. VCODE: A retargetable, \nextensible, very fast dynamic code generation system. In PLDI 96 Conference on Programming Language Design \nand Im\u00adplementation (May 1996). [16] ENGLER, D. R., HSIEH, W. C., AND KAASHOEK, M. F. C: A language for \nhigh-level, efficient, and machine\u00adindependent dynamic code generation. In POPL 96 Symposium on Principles \nof Programming Languages (January 1996), pp. 131-144. [17] ENGLER, D. R., AND PROEBSTING, T. A. DCG: \nAn efficient, retarget able dynamic code generation system. In Conference on Architectural Support for \nProgram\u00admiflg Languages and Operating Systems (A SPLOS VI) (October 1994), ACM Press, pp. 263-272. [18] \nENGLER, D. R., WALLACH, D., AND KAASHOEK, M. F. Efficient, safe, application-specific message processing. \nTeGhnical Memorandum MIT/LGS/TM533, MIT Lab\u00adoratory for Computer Science, March 1995. [19] FEELEY, M., \nTURCOTTE, M., AND LAPALME, G. Us\u00ading Multilisp for solving constraint satisfaction prob\u00adlems: An application \nto nucleic acid 3D structure de\u00adtermination. Lisp and Symbolic Computation 7 (1994), 231-247. [20] HOLZLE, \nU., AND UNGAR, D. Optimizing dynamically\u00addispatched calls with run-time type feedback. In PLDI 94 Conference \non Programming Language Deszgn and Implementation (June 1994). [21] JONES, N. D., GOMARD, C. K., AND \nSESTOFT, P. Partial Evaluation and Automatic Program Generation. Prentice-Hall, 1993. [22] KEPPEL, D. \nA portable interface for on-the-fly instruc\u00adtion space modification. In Conference on Architec\u00adtural \nSupport for Programming Languages and Operat\u00ading Systems (April 1991), pp. 86-95. [23] KEPPEL, D., EGGERS, \nS. J., AND HENRY, R. R. A case for runt ime code generation. Tech. Rep. 91-11\u00ad04, Department of Computer \nScience and Engineering, University of Washington, November 1991. [24] KEPPEL, D., EGGERS, S. J., AND \nHENRY, R. R. Eval\u00aduating runtime-compiled value-specific optimization. Tech. Rep. 93-11-02, Department \nof Computer Science and Engineering, University of Washington, November 1993. [25] LEONE, M., AND LEE, \nP. Lightweight run-time code generation. In PEPM 94 Workshop on Partial Evalua\u00adtion and Semantics-Based \nProgram Manipulation (June 1994), Technical Report 94/9, Department of Com\u00adputer Science, University \nof Melbourne, pp. 97-106. [26] MASSALIN, H. Synthesis: An Ejjlicient Implementation of Fundamental Operating \nSystem Services. PhD thesis, Department of Computer Science, Columbia University, 1992. [27] MASSALIN, \nH., AND Pu, C. Threads and input/output in the Synthesis kernel. In ACM Symposium on Oper\u00adating Systems \nPrinciples (1989), pp. 191 201. [28] MCCANNE, S. The Berkeley Packet Filter man page. BPF distribution \navailable at f tp: //ftp. ee. lbl. gov. [29] MCCANNE, S., AND JACOBSON, V. The BSD packet filter: A new \narchitecture for user-level packet capture. In Winter 1993 USENIX Conference (January 1993), USENIX Association, \npp. 259-269. [30] MOGUL, J. C., RASHID, R. F., AND ACCETTA, M. J. The packet filter: An efficient mechanism \nfor user-level network code. In ACM Symposium on Operating Sys\u00adtems Principles (November 1987), ACM Press, \npp. 39 51. An updated version is available as DEC WRL Re\u00adsearch Report 87/2. [31] PIKE, R., LOCANTHI, \nB., AND REISER, J. Hard\u00adware/software trade-offs for bitmap graphics on the Blit. Software Practice \nand Experience 15, 2 (Febru\u00adary 1985), 131 151. [32] POLETTO, M., ENGLER, D. R., AND KAASHOEK, M. F. \nt cc: A template-baaed compiler for C. In WCSSS 96 Workshop on Compiler Support for System Software (February \n1996), pp. 1-7. [33] Pu, C., AUTREY, T., BLACK, A., CONSEL, C., COWAN, C., INOUYE, J., KETHANA, L., WALPOLE, \nJ., AND ZHANG, K. Optimistic incremental specializa\u00ad tion: Streamlining a commercial operating system. \nIn Symposium on Operating Systems Principles (Decem\u00adber 1995). [34] TARDITI, D., MORRISETT, J. G., CHENG, \nP., STONE, C., HARPER, R., AND LEE, P. TIL: A type-directed optimizing compiler for ML. In PLDI 96 Conference \non Programming Language Design and Implementation (May 1996). [35] THOMPSON, K. Regulm expression search \nalgorithm. Communications of the Association for Computing Ma\u00adchinery 11, 6 (June 1968), 419 422. [36] \nVOLANSCHI, E.-N., MULLER, G., AND CONSEL, C. Safe operating system specialization: the RPC case study. \nIn WCSSS 96 Workshop on Compiler Support for System Software (February 1996). [37] WAINWRIGHT, R. L., \nAND SEXTON, M. E. A study of sparse matrix representations for solving linear sys\u00adtems in a functional \nlanguage. Journal of Functional Programming 2, 1 (January 1992), 61-72. \n\t\t\t", "proc_id": "231379", "abstract": "We describe the design and implementation of a compiler that automatically translates ordinary programs written in a subset of ML into code that generates native code at run time. Run-time code generation can make use of values and invariants that cannot be exploited at compile time, yielding code that is often superior to statically optimal code. But the cost of optimizing and generating code at run time can be prohibitive. We demonstrate how compile-time specialization can reduce the cost of run-time code generation by an order of magnitude without greatly affecting code quality. Several benchmark programs are examined, which exhibit an average cost of only six cycles per instruction generated at run time.", "authors": [{"name": "Peter Lee", "author_profile_id": "81100384353", "affiliation": "School of Computer Science, Carnegie Mellon University, Pittsburgh, Pennsylvania", "person_id": "PP39040384", "email_address": "", "orcid_id": ""}, {"name": "Mark Leone", "author_profile_id": "81341492819", "affiliation": "School of Computer Science, Carnegie Mellon University, Pittsburgh, Pennsylvania", "person_id": "PP42052703", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/231379.231407", "year": "1996", "article_id": "231407", "conference": "PLDI", "title": "Optimizing ML with run-time code generation", "url": "http://dl.acm.org/citation.cfm?id=231407"}