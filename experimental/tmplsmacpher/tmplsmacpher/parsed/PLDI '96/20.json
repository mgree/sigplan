{"article_publication_date": "05-01-1996", "fulltext": "\n Data Specialization Todd B. Knoblock and Erik Ruf Microsoft Research One Microsoft Way, Redmond, WA \n98052 USA {toddk, erikruf}@microsof t.com Abstract Given a repeated computation, pad of whose input \ncontext remains invariant across all repetitions, program staging im\u00adproves performance by separating \nthe computation into two phases. Anearly phase executes only once, performing corn\u00adputations depending \nonly on invariant inputs, while a late phase repeatedly performs the remainder of the work given the \nvarying inputs and the results of the early computations. Common staging techniques based on dynamic \ncompi\u00adlation statically construct anearly phase that dynamically generates object code customized for \na particular input con\u00ad text. In effect, theresults of the invariant computations are encoded as the \ncompiled code for the late phase. This paper describes an alternative approach in which the results \nof early computations are encoded as a data structure, allowing both the early and late phases to be \ngenerated statically. By avoiding dynamic code manipu\u00adlation, we give up some optimization opportunities \nin ex\u00adchange for significantly lower dynamic space/time overhead and reduced implementation complexity. \nIntroduction Program staging transformations capitalize on the fact that the inputs to a computation \noften become known in a partic\u00adular order, or vary at differing frequencies. If we can estab\u00adlish that \na particular portion of the input context remains invariant over multiple executions of the comput at \nion, then we can use this information to execute invariant subcompu\u00adtations only once, and repeat only \nthose subcomputations that depend on varying inputs. Doing so improves perfor\u00admance when the time savings \nfrom repeated execution of the optimized computation exceed the cost of performing the optimization dynamically. \nIn simple cases of staging, such as loop invariant code motion [ASU86], a compiler can automatically \nrecognize the frequency with which the inputs (variable definitions) to a computation (loop body) are \naltered, determine which sub\u00adcomputations depend only on invariant inputs, and hoist them to an appropriate \nlocation. In more complex situ\u00adations, some degree of programmer assistance is required. Permissionto \nmake digitabhard copy of part or all of this work for paraonaI orolassroomuseis rantedwithoutfeeprovidedthatcopiesarenotmade \nor distributed for pro t of commercial atvanfa ie the copyright notice, the titleof the publicationand \nitsdate appear,ani noboeisgiventhat oopyingisbypermissionofACM, Inc.To copyotherwise,to republish,to \npoetonservers,orb risdisfributetolists,requirespriorspedfiopermission @or a fee. Typically, the programmer \nstatically partitions the input context into fixed and varying subparts, and invokes the staging transformation \nto obtain code for the early (invari\u00adant subcomputations) and late (varying subcomputations) phases. \nDuring program execution, the program invokes the early phase when the fixed inputs become known, and \nthe late phase whenever the varying inputs change. It is the pro\u00adgrammer s responsibility to enforce \nthe invariant that fixed inputs will not change across invocations of the late phase; the early phase \nmust be reinvoked whenever the fixed inputs are altered. A number of staging techniques [MP89, KEH93, \nLL94, EP94, CN96, APC+96] are based on dynamic compilation. These approaches range from runtime instantiation \nof manu\u00adally generated machine-code templates to runtime execution of full-blown optimizers and code \ngenerators, with a variety of template-and compilation-based mechanisms lying be\u00adtween these two extremes. \nDespite their differences, these systems share a fundamental characteristic: they all express the output \nof the early phase as object code, an approach we will call code specialization. A typical code specialization \n.\u00adarchitecture has the following type signature: Fragment x Input-Partition + dynamically generated code \n(Fixed-Inputs -+ ~). statically generated dynamic optimizer The programmer selects a program fragment \nto be op\u00adtimized. and Partitions its inrmts into those which are to be held fixed ~cross invocatio&#38; \nof the optimized code, and those which may vary. Statically applying the compiler to a program fragment \nand an input partition yields object code for a runtime optimizer. 1 This optimizer is dynamically in\u00advoked \non the fixed subset oft he fragment s inputs, producing optimized object code. This optimized code is \nthen (repeat\u00adedly) invoked on the remaining input, achieving the same effect as the original program \nfragment, but more quickly. Allowing the runtime optimizer to generate arbitrary object code can achieve \na high degree of optimization; code spe\u00adcializes often eliminate branches, unroll loops, and produce \nimproved instruction schedules in addition to folding oper\u00adat ions involving fixed input values. Aggressive \nopt imizat ion lIn some template-based systems, this step is performed manually by the programmer, who \nuses the input partition to construct a set of templates and code to instantiate them at runtirne. PLDI \n96 W6 PA, USA 01996 ACM 0-69791-795-296/0005...$3.50 may require that the optimized code be executed \nmany times to repay the cost of generating it. In this paper, we describe a more limited approach, data \nspecialization, in which the dynamic optimizer does not emit object code, but instead emits a cache of \nspecialized data values. Along with the fragment s inputs, this cache is then (repeatedly) passed as \ninput to statically-generated code that performs the remainder of the fragment s computation and returns \nthe final value: Fragment x Input-Partition + (Fixed-Inputs + Cache) x statically generated cache loader \n(Cache x VaTymg-Inputs --+ Result). statically generated cache reader Statically applying the compiler \nyields two object codes: an optimizer, or cache loader, that computes the cache, and an execution engine, \nor cache reader, that uses the cache to compute the final output, Because the cache loader and reader \nare generated directly from the fragment and input partition without knowledge of the fixed data values, \ndata specialization cannot, in general, achieve the same degree of optimization as code specialization. \nFor example, it cannot eliminate branches or unroll loops, unless the same elimina\u00adtion/unrolling can \nbe used for all possible values of the fixed inputs. Data specialization trades this generality for several \nother other desirable characteristics: Rapid payback: Cache loading is very inexpensive, and is typically \namortized away after only two executions of the cache reader.  Low space overhead: Caches are typically \nquite small (tens of bytes), allowing data specialization to be used in applications requiring very large \nnumbers of special\u00adizations,  Simple implementation: Data specialization can be implemented entirely \nvia source-to-source program transformation, all of which can be performed stati\u00adcally given only the \nprogram and input partition. The transformation is thus completely portable, and the transformed code \nrequires no additional runtime sup\u00adport.  The remainder of this paper describes the implementa\u00ad tion \nand use of an instance of data specialization based on caching the values of certain invariant expressions. \nOther implementations are possible, as the signature above merely requires us to construct the late phase \nstatically. We envi\u00ad sion placing other kinds of useful information in the cache (cf., Section 7.2). \nWe begin, in Section 2, by giving a small example. Sec\u00ad tion 3 describes basic algorithms for constructing \ncache loader and reader code from an arbitrary program fragment, while Section 4 treats some more advanced \naspects of this transformation, In Section 5, we describe our prototype data specialize for a subset \nof C and show its performance and overhead on a family of graphics programs. We conclude with discussions \nof related and future work. dotprod(xl, yl, z1, x2, y2, 22, scale) { if (scale != O) return (xl*x2 + \nyl*y2 + 21*22) / scale; else return ERROR; } Figure 1: A dot product program. cache loader: dotprod-load(xl, \nyl, { if (seals != O) return ((cache-> / scale; elss z1, slotl x2, = y2, xI*x2 22, scale, + yl*y2) cache) \n+ 21*22) } return ERROR; cache reader: dotprod-read(xl, yl, { if (scale != O) return (cache\u00ad>else ZI, \nslotl x2, + z y2, I*z2) 22, scale, / scale; cache) return ERROR; 3 Figure 2: Cache loader and readsr \nprograms for {.s1, Z2} varying. 2 Example Consider the simple program fragment shown in Figure 1. If \nwe expect to repeatedly execute this code while varying only the .s coordinates, we may benefit by precomputing \nand caching the values of computations not depending on z 1 or 22, namely (scale ! =0) and (xl*x2+yl*y2). \nFor the sake of efficiency, our implementation constructs a loader and reader with signatures that differ \nslightly from those in the introduction in that (1) the loader and reader both receive all of the fragment \ns inputs as parameters, and (2) the loader returns both a cache and a result value: Fragment x Input-Partition \n+ (All-Inputs +-Cache x Result) x (Cache x All-Inputs -+ Result). Case (1) allows the optimization of \nnot caching information that can be cheaply recomputed from the fixed inputs, while (2) reduces overhead \nby allowing some computations in the loader to be used both for cache construction and result generation. \nThus, the loader is essentially an instrumented version of the original fragment, while the reader is \nan op\u00adtimized version. We also make use of heuristics; e.g., the loader does not cache (stale ! =0) , \nas the relational opera\u00adtion is likely to be cheaper than a memory reference, but does cache the result \nof (xl*x2+yl*yz) ; the reader then ref\u00aderences the cached value instead of repeatedly computing (xl*x2+yl*y2) \n. Invoking our data specialize on the fragment of Figure produces the loader and reader code of Figure \n2. These frag\u00adments illustrate two features of data specialization. First, because the loader and reader \nare constructed solely from the input partition by a process which does not have access to the value \nof scale, the conditional cannot be folded out, and appears in the reader. Second, the cache is small, \ncon\u00adtaining only one value, and its initialization is very simple, adding only one assignment expression \nto the original pro\u00adgram, A code specialize could eliminate the conditional, but would generate a larger \nspecialization containing not only the value of (xl*x2+yl*y2), but also opcodesfor addi\u00adtion, multiplication, \nand division. For this trivial program, we achieve a modest speedup (11% when scale is nonzero, O% otherwise) \nby trading two multiplications and an addition for a memory reference. Had the basic operations been \nmore expensive (e.g., matrix mul\u00adtiplications), the speedup would have been higher. Equally import ant \nly, the startup cost is low (5.5% when scale is nonzero, 0~0 otherwise). Thus, we achieve breakeven when\u00adever \nthe original fragment is executed at least twice. Specialization In this section, we describe an algorithm \nfor data special\u00adization based on precomputing and caching the values of invariant expressions. This \nalgorithm constructs the cache loader and cache reader from an imperative program frag\u00adment expressed \nas an abstract syntax tree and an input par\u00ad tition describing which variables are fixed on entry to \nthe fragment. Specialization begins with analyses that anno\u00ad tate every term in the fragment with one \nof the following labels. Statzc: the term only needs to be evaluated in the loader. Cached: the loader \nmust evaluate this term and load the resulting value into the cache. The reader does not evaluate this \nterm, but instead reads the value from the cache. Dynanuc: both the loader and reader must evaluate \nthis term (although its subterms may be recursively trans\u00adformed). The intuition behind these labels \nis that a term must be dynamic (i. e., appear in the reader) if its value depends in any way upon a varying \ninput, or if its effects are visible to another dynamic term. A term is cached if it is not dynamic, \nbut has a dkect dynamic consumer. If neither of these applies, then a term is static, and is omitted \nfrom the reader, The cached terms represent the maximal non\u00addynam~c terms. All of the dynamic terms are \nseparated from the static terms by a frontier of cached terms. The loader and reader communicate only \nvia these cached terms. The conditions of this intuitive specification can be sep\u00adarated into two categories: \n(1) dependence upon the values of varying inputs and (2) consumption by dynamic terms. Our algorithm \nseparates these conditions into two separate passes called dependence analysis and caching analysis. \nA third consideration, the efficient use of cache space, interacts with caching analysis, but will be \ndeferred until Section 4.3 to avoid complicating the discussion. Once these analyses are complete, a \nsimple splitting transformation is used to construct the cache loader and reader. 3.1 Dependence Analysis \nDependence analysis determines, for each program term, whether its result value (and, in the case of \nside-effecting terms, its effects), may depend upon the value of any of the varying inputs. Such dependent \nterms will have to be evaluated each time the reader is executed, while it may be possible to evaluate \nindependent terms only once and cache their results. Dependence cannot be computed precisely; our approximation \nwill (safely) err on the side of overestimating the set of dependent terms. Given standard data dependence \nand control dependence information, computing our dependence relation is straight\u00adforward. A term is \ndependent if 1. it is a member of the varying part of the input parti\u00adtion, 2. it has a dependent operand, \n 3. it is reached by a dependent definition, or 4. it is conditionally reached by a definition along \na path that is control dependent on a dependent predicate.  Cases (1) through (3) are straightforward: \nif a term di\u00adrectly or indirectly consumes the value of a varying input, it is dependent. Case (4) handles \nthe situation in which a variable is conditionally set to one of multiple indepen\u00addent values, but the \ncondition governing the choice cannot be evaluated given only the fixed input. This case is easy to recognize \nin a language having only structured control constructs because each join point corresponds to a single \nconditional, enabling us to force the appropriate variables (those modified in the single-entry, single-exit \nregion as\u00adsociated with the join) to become dependent at the join point. In unstructured code, this problem \nbecomes signifi\u00adcantly more difficult; Auslander et al. [APC+ 96] handle thk case by performing a reachability \nanalysis concurrently with dependence analysis. Our implementation of dependence analysis is a straight\u00adforward, \nworst-case quadratic-time solution based on ab\u00adstract interpretation. We initially assume that all terms \nother than the varying inputs are independent, then itera\u00adtively propagate the dependence constraints \nlisted above. Other techniques for binding time analysis of imperative programs such as program slicing \n[DRH95J and type infer\u00adence [And94] could also be applied here. In the example program of Figure 1, the \nreferences to variables z 1 and 22 are marked as dependent, as are the multiplication z I*Z2 and the \nsurrounding addition and di\u00advision. All other terms are marked as independent. 3.2 Caching Analysis \nCaching analysis determines the structure of the cache loader and cache reader by annotating each program \nterm as static, cached, or dynamic. This annotation is partially determined by the dependence analysis, \nwhich identifies terms whose value or effects may be influenced by the varying inputs. Since the execution \nof such dependent terms must be de\u00adlayed until the values of the varying inputs are available, all dependent \nterms must appear in the cache reader; t bus, the caching analysis annotates them as dynamic. The an\u00adnotation \nis not completely determined by the dependence analysis due to two concerns: Structural concerns: terms \nin the reader may require the values or effects of other, possibly independent, terms, in order to execute. \nThe caching analysis en\u00adsures that the requisite context will be available by caching the value or delaying \nthe effects of such refer\u00ad enced terms, This concern is similaz to the congruence criterion in offline \npartial evaluation [JGS93]. Any consistent cache labeling must satisfy the following system of constraints, \nwhere the functions Dependent, Static, Cached, and Dynamic are predicates on terms. Other predicates \nwill be defined in the text accompanying each rule. 1. Dependent(t) + Dynamic(t) 2. Has GlobalEffect \n(t) + Dynamic(t) 3. UnderDependent Control(t) + Dynamic(t)  Rules 1 3 are base cases that force certain \nterms to be labeled as dynamic; all three rules depend only on the program fragment and the dependence \nanalysis. Terms whose value or effect depends on the varying portion of the input must be dynamic (Rule \n1), as must those that read or write global state such as input/output or volatile storage (Rule 2). \nTo avoid hoisting that could cause the loader to perform unnecessary computations, Rule 3 requires that \nany term whose execution is guarded by a dependent predicate be dynamic. Implementations willing to tolerate \nsuch speculation may choose to weaken this rule. 4. IsRef(t) A Dynamic(t) + Vt G Defs(t) Dynamic(t ) \n 5. Dynamic(t) + Vt c Guards(t) Dynamic(t )  Rules 4 and 5 handle cases where forcing a term t to appear \nin the reader (by labeling it as dynamic) requires that other terms defining the execution context of \nt also appear in the reader. If a variable reference appears in the reader, all definitions reaching \nthe reference must also appear (Rule 4). Similarly, all control constructs guarding a dynamic term must \nalso be dynamic (Rule 5). 6. Dynamzc(t) -+ W E Value Operands(t) (lDynamic(t ) A Single Valued(t ) A \n~ Trivial(t ) --+ Cached (t )) 7. Dynamic(t) + Vt G Value Operands(t) (~ Cached(t ) +-Dynamic(t )) \n Rules 6 and 7 construct the frontier of cached terms that defines the interface between the loader and \nreader. The basic idea is that, if a term appears in the reader, then it needs to obtain reader-time \nvalues for all of its value-producing operands, either by executing those operands or by retrieving their \nvalues from the cache. Operands executed solely for their effects can be ignored, as Rules 1 and 4 will \nadd them as necessary. Rule 7 permits any operand to be annotated as dynamic, while Rule 6 permits caching \nany operand that meets three restrictions: It isn t already dynamic. . It returns a single value during \nthe execution of the fragment. This category includes all expressions not inside loops, and all expressions \nthat are invariant in all enclosing loops. This restriction ensures that a single cache slot will correctly \nsummarize the value of the operand. It is sufficiently nontrivial. For example, constants and expressions \nwith very low execution costs are not cached. 8. ~(Dynamic(t) V Cached(t)) -+ Static(t) Rule 8 ensures \nthat all terms are labeled. Nodes that are neither dynamic nor cached become static, and need not appear \nin the reader. Figure 3: Consistency constraints for caching analysis PolicY concerns: not all independent \nterms need to be cached. For example, a t erm s value may not be used by the reader, or reevaluating \nthe term may cost less than a memory reference to the cache. The caching analysis avoids these cases \nwhile ensuring that the structural concerns are satisfied. Our caching analysis works by representing \nthese con\u00adcerns as a system of constraints limiting the possible label\u00adings of the program terms. We \nthen find a solution to these constraints under the criterion that the reader contain as few computations \nas possible. Figure 3 gives a system of con\u00adsistency constraints for caching annotations. The idea is \nto find a basis set of terms that must appear in the reader, then ensure that all of the (transitive) \ndata and control depen\u00addence predecessors of such terms will be represented in the reader (either by \nthemselves or some cached value). Rules 1 3 identify the basis set: namely, dependent terms, terms with \nglobal side-effects, and terms whose execution in the reader would lead to speculation. The closure over \ndepen\u00addence is implemented ~y Rules 4 7. Rule 4 ensures tha{any definitions reaching a dynamic term are \ndynamic, Rule 5 en\u00adsures that any control constructs guarding a dynamic term are dynamic, and Rules 6 \nand 7 ensure that the operands to any dynamic term are either cached or dynamic. By only caching terms \nwith dynamic consumers, we satisfy the struc\u00adtural requirement that a fringe of cached terms separate \nall static and dynamic terms, and the policy requirement every data value in the cache have at least \none consumer in the reader. The distinction between the dependent and dynamic an\u00adnotations is an important \none because it allows us to force a computation into the reader while still making use of it in the loader. \nConsider the case of an independent assign\u00adment statement which reaches multiple variable uses, some \nof which are dependent. The assignment must appear in the reader so that the dependent use will access \nthe proper value. If we were to implement this by making the assignment statement dependent, the definition \nof dependence would force all of its uses to become dependent, and we would be required to evaluate the \nindependent variable uses (and any independent computations enclosing those uses) in the reader. We \nfind a solution to the constraints of Figure 3 by treat\u00ading them as rewrite rules. We use Rule 8 to initially \nlabel all terms ae static. Rules 1 3 rely only on dependence infor\u00admation and can be executed once. Rules \n4 7 are executed on a demand-driven basis (whenever we label an expression as dynamic, we check to see \nwhich of these rules apply). We resolve conflicts between rules 6 and 7 by always attempting to apply \nrule 6 first; this gives us our preference for caching terms over executing them in the reader. This \nalgorithm requires time proportional to the size of the program, since each term can be labeled at most \nthree times. If we consider the annotations as elements of an ordering dynamic > cached > static, then \nthe algorithm is monotonic. This makes the algorithm restartable, in that we can at any time relabel \nany expression with label 1 with a new label 1 >1, continue the execution of Rules 5 7, and obtain the \nsame result as if the expression initially had label 1 . We employ this observation in Section 4.3 where \nwe use it in an algorithm that limits the amount of memory consumed by the cache. In the example of Figure \n1, the term (xl*x2+yl*y2) is marked as cached, with all of its subterms marked as static. Everything \nelse is marked as dynamic ((scale! =0) is dy\u00adnamic because it is trivial).  3.3 Splitting Transformation \nOnce the caching analysis is complete, we traverse the an\u00adnotated fragment and emit the cache loader \nand the cache reader, The splitting transform proceeds via a simple case analysis based upon the caching \nannotation at each term. Stattc: the recursively split term is added to the loader. Nothing is added \nto the reader for this term. Cached: the recursively split term is added to the loader, along with an \nassignment to the corresponding cache slot. The reader receives a term that reads from the corresponding \ncache slot. Dynamic: the recursively split term is added to both the loader and the reader. If the cache \nfrontier contains n terms, the size of the loader is that of the original fragment plus n assignments \nused to load the cache. The reader is smaller than the origi\u00ad nal fragment, as n terms of the fragment \nhave been replaced by references to cache slots. In practice, the sum of the loader and reader sizes \nhas been less than twice the size of the fragment. In the example of Figure 1, the cached term (x1*x2 \n+YI*Y2) is wrapped with a cache assignment in the loader, and is replaced by a cache access in the reader. \nThe static terms x1*x2 and y1*y2 appear only in the loader. The remaining terms are dynamic, and appear \nin both phases. 4 Advanced Specialization In this section we describe a number of optimizations and re\u00adfinements \nto the specialization algorithms described in Sec\u00adtion 3. Most of the worked described in this section \nis aimed x = f(1) if (p) x = g(2); if (q) h(x) ; z=~; Figure 4: Example code where caching both references \nof variable x would be redundant. The predicates p and q are assumed to be independent, the expressions \nf (i) and g(2) are marked inde\u00adpendent and static, h(x) k dgnarnic, and the two uses of x are marked \ncached. cache loader: x = f(l); if (p) x = g(2); if (q) h(cache->slotl=x) ; z = (cache-> slot2=x) ; cache \nreader: if (q) h(cache->slotl) ; z = cache-> slot2; Figure 5: Unoptimized cache loader and cache reader \nfragments. at minimizing the size of the cached data. This is important to the practical applicability \nof data specialization. 4.1 Using SSA to Improve Caching of Variables The analysis described above determines \nwhich expressions should be cached. The splitting transformation then substi\u00adtutes load and cache read \ninstructions at the location of the expression in the original program. Consider the special case where \nthe expression is a simple variable reference. Notice that if the same variable with the same reaching \ndefinitions occurs twice, and is marked as cached in both instances, the result would be to cache that \nvalue twice. In the example of Figure 4, naively applying the previous algorithm would result in a redundant \ncache slot being assigned to the second use of x as shown in Figure 5. If slot 1 is filled at all, it \nis filled with the same value as is slot 2. To avoid this problem, we preprocess the pro\u00ad gram to produce \nunique definitions at the join points of the control flow graph. The result is analogous to static single \nassignment (SSA) form {CFR+91 ]. In particular, we sh~e the property with SSA that every variable reference \nexcept for those in the introduced assignments (our analog of the phi nodes of SSA) have exactly one \nreaching definition. We produce this form via a source to source transform on the program before performing \nthe specialization analyses. Starting at each control flow split, we analyze the branches for possible \neffects to variables. At the join point, we insert statements of the form V=V for each variable that \nmay have been affected within the control term. We then disallow caching of individual variables except \nthose introduced via cache loader: while (cache size > bound) do x = f(l); if (p) x = g(2); x = (cache-> \nslotl=x) ; if (q) h(x) ; 2=X; cache reader: x = ~a~he->~l~tl ; if (q) h(x) ; ~=~; Figure 6: Improved \ncache loader and cache reader fragments. the transformation (i.e., the phi nodes). The result for our \nexample code fragment is shown in Figure 6. In practice, this optimization typically has only minor effects. \nHowever, in a few programs, it has reduced the size of the cached data to as little as half the original \nsize. 4.2 Associative rewriting Consider the expression (xl*x2+yi*y2+zl* z2) where xl and x2 are dependent. \nIf the addition operator associates to the left, both additions will be dependent, while if it associates \nto the right, only the first one will be. Our implementation optionally reassociates expressions to maximize \nthe size of independent terms, increasing the number of comput at ions that can be performed in the loader. \n2 This operation is similar to binding time improvement techniques used in offline partial evaluation, \nand to the rank-ordered reassoci\u00adation used in code motion optimizations [ASU86, BC94]. 4.3 Cache size \nlimiting Each program term annotated as cached represents an inter\u00ad mediate result value that will be \ncomputed by the loader, placed in the cache, and later used by the reader instead of executing the term. \nThus, caching a term exchanges the time cost of executing the term for the space cost of storing its \nresult value. The analysis described in Section 3.2 avoids caching terms that are inexpensive to execute, \nbut treats space as an infinite resource; any nontrivial term that can usefully be cached will be. This \nis unrealistic; we must en\u00ad sure that the caches of all simultaneously live specializations fit in physical \nmemory, as paging in a cached value is almost certainly slower than recomputing it. The goal of cache \nlimiting is to minimize the amount of computation in the reader given a bound on the size of the cache. \nWe approximate the cost of not caching each cached term, and relabel the lowest-cost cached term to dynamic, \nrepeating this process until the cache size falls below the specified bound: 20f course, computer arithmetic \ndoes not obey the usual math\u00adematical associatlvity rules. However, in many applications, this is not \nsignificant. In those where it is, this feature may be turned off. compute cost of not caching each cached \nterm let victim = the minimum-cost cached term label victim as dynamic reestablish constraints 4-7 of \nFigure 3 Relabeling a term as dynamic requires that we reestab\u00adlish constraints 4 7 of Figure 3 to ensure \nthat the reader will contain the necessary execution context for the newly dynamic term. This may widen \nthe cache frontier, increas\u00ading the amount of cache space required. Even though the total cache size \ndoes not necessarily decrease on each itera\u00adtion of the loop, the loop will eventually terminate because \neach term is relabeled at most twice (from static to cached, or cached to dynamic). The central task \nis choosing the victim term from the frontier of cached terms. We would like to keep cache el\u00adements \nthat are expensive to compute, and make dynamic those with the least utility (perhaps weighted by size). \nFur\u00adther complicating this decision is that the cost and utility of the cache elements are not independent, \nbut depend upon what is already being cached versus computed dynamically. Our heuristic begins by statically \napproximating the execu\u00adtion cost of every program term (cf., [WMGH94]), combin\u00ading the following factors: \n a static cost value for the term s operator (for example, the costof+is1,the costof/is9),  the sum \nof the costs of computing all subterms,  for terms in loops, a multiplier (5),  for terms guarded by \nconditionals, a divisor (2).  Given execution cost estimates for each term, we ap\u00ad proximate the cost \nof not caching a given term on the cache frontier as the term s execution cost plus the transitive ef\u00ad \nfect from rules 4 7 of Figure 3. These costs include those of definitions of variables referenced by \nthe term, and of re\u00ad quired guards that are not already dynamic (the marginal cost of computing an already \ndynamic guard is zero). After the minimum-cost term has been relabeled as dynamic, we efficiently reestablish \nthe consistency constraints by restart\u00ad ing the constraint solver of Section 3.2, and check to see if \nthe desired bound has been achieved. Although this algorithm is decidedly approximate, it ap\u00ad pears, \nin practice, to preserve the most important elements on the cache frontier. Sample results of this cache \nlimiting algorithm are presented in Section 5.4. 5 Results In this section, we present empirical results \nobtained with our data specialize. This system processes a subset of the C language without pointers \nor goto, and assumes that the fragment to be specialized is a single nonrecursive proce\u00ad dure. These \nrestrictions simplify the computation of con\u00ad trol and data dependence, and eliminate the need for an \nalias analysis, but otherwise do not impact the specializa\u00ad tion algorithms. All measurements were conducted \nusing the Microsoft Visual C++ compiler version 4.0 on an Intel Pentium/ 100 CPU with 64 megabytes of \nphysical memory. Our benchmarks are shading procedures, or shaders, be\u00ad longing to the interactive graphics \nrendering system de\u00ad scribed in [GKR95]. A shader computes the color value for 120 -.-- ..*-.-F. _ that \ntakes only a few seconds per input partition. + Speedup wB u Median Speedup 100 B~ [+ 123456789 10 Shader \nFigure 7: Speedup for all input partitions of ten sample shading procedures. Each shader is specialized \non multiple input par\u00adtitions (one per control parameter); these are displayed in the y direction above \neach shader number. Note that multiple in\u00adput partitions having the same speedup are displayed as a single \npoint. . ...... +-. ..... -. - . .. + Cache Size rn %Median Cache Siza I 12345678 9 10 Shader Figure \n8: Single-pixel cache sizes for all input partitions of ten sample shading procedures, Note that multiple \ninput partitions having the same cache size are displayed as a single point. an image pixel given the \npixel coordinates, various rendering information specific to the pixel, and shader-specific control parameters \nprovided by the user via a graphical interface. The graphical interface restricts the user to modifying \na sin\u00adgle control parameter at a time, allowing us to specialize a shader on all of its inputs except \nfor the control parameter being modified, and reuse the specialization (array of per\u00adpixel caches) so \nlong as the user continues to modify the same parameter. Because the fixed inputs include per-pixel rendering \ndata, we may construct as many as 106 simulta\u00adneously live caches for a single image, but we require \nonly one loader/reader code pair per input partition. A typical shader has on the order of 10 cent rol \nparameters, requiring 10 loader/reader pairs. We construct, compile, and link this code statically at \nthe time a shader is installed, an operation In the sections that follow, we present results for ten \nshading procedures (some derived from examples in [Ups89, Smi90, GKR95], the remainder written by the \nauthors) rep\u00adresenting a variety of styles and complexity levels. These range in size from 50 to 15(I \nlines of C code, and invoke a small mathematical library that supports vector and ma\u00adtrix operations \nas well as noise functions. We ported these shaders to our system as directly as possible without opti\u00admizing \nthem for specialization. 5.1 Speedup Figure 7 graphically depicts the asymptotic speedup achieved by \nspecializing each of the 10 shaders with respect to input partitions holding all but one control parameter \nat a time fixed, yielding a total of 131 distinct input part i\u00adtions, These values represent the average \nof multiple runs on a single image, with varying control parameter values. The speedups vary widely both \nbetween shaders and be\u00adtween input partitions of a single shader, but are alway at least 1.OX. The high \nvariance can be explained by the fact that the number and complexity of computations depending on the \nvarying parameter (and thus the amount of work that must be performed by the reader) is different for \neach input parti\u00adtion. For example, shaders 3, 4, and 5 invoke expensive frac\u00adtal noise functions; if \nthe varying control parameter does not affect the input to the noise function, the noise value can be \ncached, and speedups as high as 100x are achieved. If, how\u00adever, the noise function input is affected, \nthe reader is forced to repeat this expensive computation every time the control parameter is altered, \nlowering the achievable speedup by ap\u00adproximately 50%. Simpler, non-iterative shaders such as 1, 6, 7, \nand 8 contain fewer expensive computations and thus exhibit lower speedups. However, their speedups still \nvary across input partitions: for example, changing the ambient light parameter (a simple scaling factor \napplied to the final color value) typically requires only a few multiplications, while altering the location \nof the light source affects virtu\u00adally all of the shader s computations, including a number of coordinate \ntransformations. Thus, a higher speedup is achieved for the ambient light parameter than for the light \nposition parameters. 5.2 Overhead The speedups described in the previous section are asymp\u00adtotic, and \ndo not reflect the additional cost of the extra cache loading operations that must be executed each time \na new cache is required. For the user to benefit in practice, the loading cost must be amortized over \nmultiple uses of the cache (i. e., several successive changes to a single shading parameter). Fortunately, \nthis overhead is extremely 10W of the 131 loader/reader pairs we constructed, 127 (97Yo) reached breakeven \nat two uses,3 3 required three uses, and 1 required 17 uses. It is also worth noting that our speedup \nand overhead figures are truly per-pixel statistics; we are not relying on a large image size to amortize \ncosts. 3This means that the total time to shade a pixel twice using the loader/reader paradigm was no \nmore than that required to shade that pixel twice using the original shading code. 70 -\u00ad ure 10 displays \nthe same information where each input par- T 60 t r- - : 10 .*L\u00ad -.. f OY I,,I 048 1216202428 Cache \nSize (bytes) Figure 9: Speedup factor versus cache size for input partitions of shader 10. ture this \nsecond effect. This gradual degradation cannot al\u00ad ways be achieved; for example, in the specialization \nin which ring scale is varying, reducing the cache size limit from 16 100% bytes to 12 bytes leads to \na 95% reduction in speedup inde\u00adpendent of which values are cached. 90% 80% 6 g 70% u Data $ 60% tion \nwill : 50~o --M-&#38;eo *bluel other staging strategies: runtime -+-nngscale ~ 40% roughness cremental \nprogram execution. We ks , kd implementation techniques to their 1? 30% ambient evaluation. tighlx 20% \n~.. . I,ghty +4 Ilghlz ~mean 6.1 Runtime Code Generation L-1 1o% Figure cache Figure I ( ,,I,I1I o% \n, 04 8 1216202428323640 Cache Size (bytes) 10: Percentage of maximum speedup achieved versus size for \ninput partitions of shader 10. Memory Usage 8 shows the number of bytes of cache space required for each \npixel under each specialization of our sample shad\u00ading procedures. As with the speedup figures, the cache \nsize varies widely from specialization to specialization even for a single shading procedure. The overall \nmean and median cache sizes were 22 and 20 bytes, respectively. In every case, multiplying the cache \nsize by the number of caches constructed (307,200 caches for a 640-by-480 image), yields a total space \nusage well within the physical memory size of a typical workstation.  5.4 Reducing Memory Usage The \ncache limiting algorithm of Section 4.3 allows us to trade decreased cache size for decreased speedup \nby forcing more computations into the reader. Figure 9 demonstrates the absolute speedup factors achieved \nfor various cache size limits applied to all 14 input partitions of shader 10. Fig\u00adtition s maximum speedup \nis normalized cache size limit is reduced from 40 bytes speedups are reduced,4 but a large fraction is \nachieved even when the cache is greatly Overall, 70% of the performance is retained is limited to 2070 \nof the maximum, while to 100%. As the to O bytes, the of the speedup reduced in size. when the cache \n90% is achieved when the limit Two effects I input partitions so they are not their natural tations \nare more is raised to 3070. contribute to this result. First, many of the require fewer than 40 bytes \nof cache space, affected until the limit is moved to below cache size. Second, some cacheable compu\u00adexpensive \nthan others; often, once the most critical computations have been cached, additional caching yields only \na minor improvement. For example, the first 4- I byte floating-point value cached by the specialization \nfor the 32 36 40 input partition wit h parameter 1 ight x varying accounts for 65% of that specialization \ns speedup, even though maximum speedup requires 40 bytes. Our heuristic attempts to cap- Related Work \nspecialization can be viewed [JS86] that moves computations be executed less often. In this as a staging \ntransforma\u00ad to contexts where they section, we describe two code generation and in\u00adalso relate several \nof our counterparts in partial Several approaches to dynamic optimization rely on the fast instantiation \nof precomputed object code templates. Massalin and Pu [MP89] used hand-generated templates; Consel and \nNoel [CN96] automatically ate portable templates at the source code level tract them from the resulting \ncompiled code using specific techniques. Auslander et al. [APC+96] template-based compiler for an annotated \nversion system uses analyses similar to ours to construct phase that fills in a run-time constants table \nour cache. Our systems differ in that we construct reader that references the cache, while Auslander \nassembly gener\u00adand ex\u00ad machine\u00adpresent a of C; this an early similar to a single et al. use the cached \ndata to instantiate immediate values in dynami\u00adcally generated code. Non-template systems optimize and \ngenerate code from an intermediate form at runtime. Tools for this purpose in\u00adclude DCG [EP94] and (C \n[EHK96], which have achieved speedups as high as 50x, but require tens to hundreds of dynamic instructions \nto emit a single optimized instruction. Leone and Lee [LL94] used partial evaluation to compile out the \nintermediate form, generating a custom optimizer 4The few small increases in speedup as cache size is \ndecreased have been verified to be due to timing imprecision and processor cache ef\u00adfects rather than \npoor choices made by the heuristic, as the generated code was identical. The error appears artificially \nlarge for parameters lightx, lighty, and lightz in Figure 10 because their speedup range is smaller than \nthose of other parameters. that directly emits object code. Keppel et al. [KEH93] com\u00adpared assembly-level \nand compiler intermediate representa\u00adtion (IR) level template compilers for a variety of workloads, and \ncomputed conservative amortization intervals of 10-1000 uses for assembly templates and 1000-infinite \nuses for IR templates, General partial evaluators [JGS93, Ruf93, And94, Osg93] can yield highly specialized \ncode, but existing systems are impractical for runtime use due both to their slowness and to the cost \nof dynamically compiling the high-level code they generate, Other dynamic compilation systems [DS84, \nCha92] concentrate more on optimizing language features such as dynamic dispatch than on staging user-level \ncompu\u00adtations. An alternate formulation of partial evaluation known as mixed computation [Ers77, Bu184] \nhas a signature somewhat similar to that of data specialization, in that the transfor\u00admation emits both \nspecialized code and specialized data. The difference is that our approach emits code statically and \ndata dynamically, while systems based on mixed com\u00adputation emit both simultaneously, requiring them \nto pay the cost of dynamic code generation if used at runtime. 6.2 Incremental Program Execution Incremental \nprogram execution techniques effectively stage programs by caching intermediate results for re-use in \nsub\u00adsequent executions. Systems that cope with arbitrary in\u00adput changes by dynamically checking dependence \n[PT89, Ho092] avoid more computations than data specialization does, but they lose the efficiency we \ngain from compiling away the dependence in advance. Liu and Teitelbaum [LT95a, LT95b] present algorithms \nfor statically deriving an incremental version of a pure func\u00adtional program under some input change, \nThe basic idea is to identify computations in a program whose values can be profitably reused when the \nprogram is reexecuted under the input change, and to cache these values instead of recom\u00adputing them. \nBy using a programmatic description of the input change and taking advantage of algebraic identities, \nthis method can support forms of reuse which our purely dependence-based algorithm cannot. For example, \nusing the cached value of the expression jibonacci (z 1) in place of the expression jibonacci (z 2) \nunder the input change (kc.z + 1) yields a form of finite differencing. Sundaresh and Hudak [SH91] derive \nincremental versions of functional programs by expressing the program as the composition of a number \nof residual program fragments, each of which depends on a different projection of the pro\u00adgram s inputs. \nWhen an input is changed, the system re\u00adbuilds the corresponding fragment, merges the code for all fragments, \nand executes the result. This is a form of code specialization as it requires the dynamic construction \n(and compilation) of code whenever an input is changed. 6.3 Partial Evaluation Several aspects of the \nimplementation of data specialization described in Section 3 are similar to techniques used in mr\u00adtial \nevaluation. Our dependence annotat~on is simil~ to the binding time attribute computed by offline partial \neval\u00aduators [JGS93], in that both involve transitive data depen\u00addence on distinguished inputs. The approaches \ndiffer in that we ..parate ..mantic (dependence) information from policy (caching) information, while \nbinding time analyzers typi\u00adcally mix both in the binding time attribute. We have found that the latter \napproach can introduce false dependence. For example, our caching analysis can label a term as dy\u00adnamic \nwithout forcing its consumers to be dynamic, while a BTA-based approach (in which dependent z dynamic) \nwould unnecessarily force all of the term s consumers into the reader. We believe that current partial \nevaluators for im\u00adperative languages [Osg93, And94] have not yet experienced this problem because they \ndo not perform flow-sensitive binding time analysis. Program bifurcation [Mog89, DNBDV91] factors each \nof a program s functions into two new functions: (1) a function which takes as input only independent \nvalues, and produces the independent portion of the result, and (2) a function which takes both independent \nand dependent inputs and produces only the dependent portion of the result. An effect similar to data \nspecialization could be obtained by caching certain intermediate results of type (1) functions and using \nthe corresponding values in the type (2) functions. Our splitting pass, which traverses the annotated \npro\u00adgram fragment and emits the loader and reader code, can be viewed as two nonstandard semantic interpretations \nof an action tree [CD90, CN96]. Consel s evaluate action denotes maximal-sized independent subtrees and \nthus cor\u00adresponds (in the absence of speculation avoidance and cache size limiting) to our cached annotation, \nBecause we require that a single reader suffice for all potential values of the nonvarying inputs, all \nthree of Consel s rebuild, reduce, and identity annotations correspond to the dynamic an\u00ad notation \nin our system. 7 Future Work We foresee a number of ways to extend this work, both in terms of the present \nimplementation and the broader frame\u00adwork. We are also actively seeking other applications that will \nbenefit from our approach. 7.1 Extending the Implementation Expressing our transformation in terms of \nexpressions (ab\u00adstract syntax trees) is convenient for expository purposes but difficult to implement, \nparticularly in the face of side effects and nonlocal control transfers. We expect to move to a control \nflow graph representation in the near future. We would like to explore the costs/benefits of allow\u00ading \nspeculation in the loader. Because the load-time over\u00adhead is presently very low, we can probably afford \nthe time overhead of extra, potentially-unused computations in the loader; other potential problems include \nthe additional cache space required to store the result values and the extra work required to trap and \nhandle exceptions. In our current architecture, we perform staging offline as a source-to-source transformation; \nthis limits the number of distinct input partitions we can handle, By computing the necessary control \nand data dependence information of\u00adfline (e.g., manually staging our staging transformation!), we may \nbe able to perform our analyses and transforma\u00adtions (including code generation) dynamically. 7.2 Extending \nthe framework Reifying the result of the early phase of a staged program as a data structure need not \nlimit us to caching intermediate results. For example, we might choose to combine the result of several \ncontrol transfers into a single index into a lookup table, and cache only the index value. We could also \nspecu\u00adlatively construct multiple specialized cache readers targeted to particular fixed input values \nand select among them using a dispatch code passed in the cache. We might also find it fruitful to explore \npoints in the spectrum between data specialization (which emits no code at runtime) and code specialization \n(which expresses all early results as runtime-generated code). This would allow us the optimization benefits \nof code specialization for long-lived, often-reused contexts, while retaining the low space/time overhead \nof data specialization for more ephemeral contexts.  7.3 New Applications To date, we have only experimented \nwith programs from a single domain, namely local (per-pixel) shading comput a\u00adtions in graphics rendering, \nwhich have the characteristics: 1. a repeated computation him an input context whose components vary \nat different rates, 2. a sufficient fraction of the computation depends only on a subset of the inpute, \n 3. the invariant portion of the computation can be use\u00adfully encoded in the form of cached intermediate \nresult values, and 4. the s~ace/time overhead recmirements of the applica\u00adtion ~rec~ude the use of the \nmore general code sp~cial\u00adization staging technique.  We expect that other applications also meet these \ncri\u00adteria, and will benefit from our technique. In particular, item (3) suggests that we concentrate \non numeric applica\u00adtions where significant effort goes into the production of a small number of values, \nrather than on interpreter-like ap\u00adplications where the early computation primarily performs dispatch \nrather than computing values. Item (4) suggests applications that either require a large number of simul\u00adtaneous \nspecializations, such as image processing, or those where the repetition count is likely to be low, such \nas those where the fixed parameters are derived from interactive user input. 8 Conclusion We have presented \na new technique, data specialization, for improving the performance of program fragments by auto\u00ad matically \nrestaging them into an optimizer and an execution engine that communicate via a cache of data values. \nThis approach complements existing techniques based on runtime code generation; although it achieves \na somewhat lower de\u00ad gree of optimization, its very low overhead in time and space make it attractive \nin applications where existing techniques cannot be profitably applied. Acknowledgements Brian Guenter \nfirst suggested that we apply this technique to shading, and collaborated on the implementation of the \ninteractive shading system described in Section 5. Bruce Duba, Linda O Gara, Thomas Reps, Daniel Weise, \nand the anonymous referees provided helpful comments on drafts of this paper. References [And94] [APC+96] \n[ASU86] [BC94] [Bu184] [CD90] [CFR+91] [Cha92] [CN96] [DNBDV91] [DRH95] [DS84] [EHK96] Lars Ole Andersen. \nProgram Analysis and Spe\u00adcialization for the C Programming Language. PhD thesis, DIKU, University of \nCopenhagen, Denmark, 1994. DIKU Research Report 94/19. Joel Auslander, Matthai Phillipose, Craig Cham\u00adbers, \nSusan J. Eggers, and Brian N. Bershad. Fast, effective dynamic compilation. In Proceedings of the SIGPLAN \n96 Conference on Programming Language Design and Implementation. ACM, May 1996. This volume. Alfred V. \nAho, Ravi Sethi, and Jeffrey D. Unm\u00adan. Compilers; princzplesj techniques, and tools. Addison-Wesley, \nReading, MA, 1986. Preston Briggs and Keith D, Cooper. Effective par\u00adtial redundancy elimination In Proceedings \nof the SIGPLAN 94 Conference on Programming Lan\u00adguage Design and Implementation, pages 159 170, June \n1994. M.A. Bulyonkov Polyvariant mixed computation for analyzer programs. Acts Informatica, 21:473\u00ad484, \n1984. Charles Consel and Olivier Danvy. From interpret\u00ading to compiling binding times. In N. Jones, ed\u00aditor, \nProceedings of the 3rd European Symposnsrn on Programming, pages 88 105. Springer-Verlag, LNCS 432, 1990 \nRon Cytron, Jeanne Ferrante, Barry K. Rosen, Mark N. Wegman, and F. Kenneth Zadeck. Ef\u00adficiently computing \nstatic single assignment form and the control dependence graph. ACM 71-ans\u00adactions on Programmmg Languages \nand Systems, 13(4):451-490, October 1991. Craig Chambers. The Deszgn and Implementation of the Self Compder, \nan Optimizing Compiler for Object-Or~ented Programming Languages. PhD the\u00adsis, Stanford University, March \n1992, Published as technical report STAN-CS-92-1420. Charles Consel and Francois Noel. A general ap\u00ad \nproach to run-time specialization and its applica\u00ad tion to C. In Proceedings 23rd ACM SIGPLAN- SIGA CT \nSymposium on Pr~nciples of Programming Languages, pages 145 156, ACM, January 1996. Anne De Niel, Eddy \nBevers, and Karel De Vlam\u00adinck. Program bifurcation for a polymorphically typed functional language. \nIn Partzal Evaluation and Semantics-Based Program Manipulation, New Havenj Connecticut (Stgplan Noihces, \nvol. 26, no. 9, September 1991), pages 142 153. New York: ACM, 1991. Manuvir Das, Thomas Reps, and Pascal \nVan Hen\u00ad tenryck. Semantic foundations of binding time anal\u00ad ysis for imperative programs. In Partial \nEvalua\u00ad tion and Semantics-Based Program Manipulation, La Jolla, Caltforrua, June 1995. New York: ACM, \n1995. L. Peter Deutsch and A. M. Schiffman. Efficient implementation of the Smalltalk-80 system. In Pro\u00adceedings \nof the Eleventh Annual ACM Symposium on Principles of Programming Languages, pages 297 302. ACM, 1984. \nDawson Engler, Wilson C, Hsieh, and M. Frans Kaashoek. Cl a language for high-level, efficient, and machine-independent \ndynamic code genera\u00adtion. In Proceedings 23rd ACM SIGPLAN-SIGA CT Symposzum on Principles of Programming \nLan\u00adguages, pages 131 144. ACM, January 1996. [EP94] Dawson R. Engler and Todd A Proebsting. DCG: An \nefficient, retargetable dynamic code generation system. In Proceedings of the Sizth Interrwtional Conference \non Architectural Support for Pr-ogram\u00adming Languages and Operating Systems, pages 263 272, October 1994. \n[Ers77] Andrei ciple. April P, Ershov. Information 1977. Onthepartial Processing computation prin-Letters, \n6(2):38-41, [GKR95] Brian Guenter, Todd B. Knoblock, Specializing shaders, In ACM (Computer Graphics \nProceedings, ence Series), pages 343 349, 1995, and Erik Ruf. SIGGRAPH 95 Annual Confe~\u00ad [Ho092] Roger \nHoover. Alphonse: incremental computation as a programming abstraction, In Proceedings oj the SIGPLA \nN 92 Conference on Programming Lan\u00adguage Design and Implementation, pages 261 272, San Francisco, CA, \nJune 1992. [JGS93] Neil D. Jones, Carsten to ft. Partial Evaluation Genemtzon. Englewood 1993. K. Gomard, \nand Peter Ses\u00adand Automatic Program Cliffs, NJ: Prentice Hall, [JS86] Ulrik J@rring and William L. Scherlis. \nCompilers and staging transformations. In Thirteenth ACM Symposium on Principles of Programming Lan\u00adguages, \nSt. Petersburg, Florida, pages 86-96. New York: ACM, 1986. [KEH93] David Keppel, Suean J. Eggers, and \nRobert R, Henry. Evaluating runtime-compiled value-specific optimization. Technical Report UWCSE 93-11-02, \nUniversity of Washington Department of Computer Science and Engineering, 1993. [LL94] Mark Leone and \nPeter Lee. Lightweight run\u00adtime code generation. In Partial Evaluation and Semantics-Based Program Manipulation, \nOrlando, Florida, June 1994 (Technical Report 94/9, De\u00adpartment of Computer Science, University of Mel\u00adbourne), \npages 97 106, 1994. [LT95a] Yanhong A. Liu and Tim Teitelbaum. Caching inter\u00admediate results for program \nimprovement. In Par\u00adtial Evaluation and Semantics-Based Program Ma\u00adnipulation, La Jollaj California, \nJune 1995, pages 190-201. ACM, 1995. [LT95b] Yanhong A. Liu and Tim Teitelbaum. atic derivation of incremental \nprograms. Computer Programming, 24:1-39, 1995. System-Science of [Mog89] Torben Mogensen. Separating \nbinding times in lan\u00adguage specifications. In Fourth International Con\u00adference on Functional Programming \nLanguages and Computer Architecture, London, England, Septem\u00adber 1989, pages 14 25. Reading, MA: Addison- \nWesley, 1989. [MP89] Henry Massalin and Calton Pu. Threads and in\u00adput/output in the Synthesis kernel. \nIn Proceedings of the 12th ACM Symposium on Operating Systems Principles, pages 191-201, December 1989. \n[osg93] Nathaniel David Osgood. PARTICLE: an automatic program specialization system for imperative and \nlow-level languages. Master s thesis, MIT, Septem\u00adber 1993. [PT89] William Pugh and T. Teitelbaum. Incremental \ncom\u00adputation via function caching. In Proceedings of the Sixteenth Annual ACM Symposium on Principles \nof Programming Languages, pages 315 328, Austin, TX, January 1989. [Ruf93] Erik Ruf. Topics in Online \nPartial Evaluation. thesis, Stanford University, California, April Published as technical report CSL-TR-93-563. \nPhD 1993. [SH91] R.S. Sundaresh and Paul Hudak, A theory of incre\u00admental computation and its application. \nIn Proceed\u00adings of the Eighteenth Annual ACM Symposium on Principles of Programming Languages, pages \n1-13, January 1991, [Smi90] Alvy Ray Smith. Unpublished notes. 1990. [Ups89] Steve Upstill. Addison-Wesley, \nThe 1989. RenderMan Companion, [WMGH94] Tim and A. Wagner, Vance Maverick, Susan Graham, Michael A. Harrison. \nAccurate static esti\u00ad mators for program optimization. In Proceedings of the SIGPLA N 94 Conference on \nProgramming Language Design and Implementation, pages 85-96, June 1994.    \n\t\t\t", "proc_id": "231379", "abstract": "Given a repeated computation, part of whose input context remains invariant across all repetitions, program staging improves performance by separating the computation into two phases. An early phase executes only once, performing computations depending only on invariant inputs, while a late phase repeatedly performs the remainder of the work given the varying inputs and the results of the early computations.Common staging techniques based on dynamic compilation statically construct an early phase that dynamically generates object code customized for a particular input context. In effect, the results of the invariant computations are encoded as the compiled code for the late phase.This paper describes an alternative approach in which the results of early computations are encoded as a data structure, allowing both the early and late phases to be generated statically. By avoiding dynamic code manipulation, we give up some optimization opportunities in exchange for significantly lower dynamic space/time overhead and reduced implementation complexity.", "authors": [{"name": "Todd B. Knoblock", "author_profile_id": "81100628296", "affiliation": "Microsoft Research, One Microsoft Way, Redmond, WA", "person_id": "P283183", "email_address": "", "orcid_id": ""}, {"name": "Erik Ruf", "author_profile_id": "81100400397", "affiliation": "Microsoft Research, One Microsoft Way, Redmond, WA", "person_id": "P78858", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/231379.231428", "year": "1996", "article_id": "231428", "conference": "PLDI", "title": "Data specialization", "url": "http://dl.acm.org/citation.cfm?id=231428"}