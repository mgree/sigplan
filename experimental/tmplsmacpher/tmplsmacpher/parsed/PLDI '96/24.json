{"article_publication_date": "05-01-1996", "fulltext": "\n Replay For Concurrent Non-Deterministic Shared-Memory Applications Mark Russinovich and Bryce Cogswell \nDepartment of Computer Science University of Oregon Eugene, OR 97403 {mer, cogswell}(?ks.uoregon.edu \nReplay of shared-memov program execution is desirable in many domains including cyclic debugging, fault \ntolerance and pe~ormance monitoring. Past approaches to repeatable execution have focused on the problem \nof re\u00adexecuting the shared-memory access patterns in parallel programs. With the proliferation of operating \nsystem supported threads and shared memory for unipmcessor programs, there is a clear need for eficient \nreplay of concurrent applications. The solutions for parallel systems can be performance prohibitive \nwhen applied to the uniprocessor case. We present an algorithm, called the repeatable scheduling algorithm, \ncombining scheduling and instruction counts to provide an invariant for eflicient, language independent \nreplay of concurrent shared-memory applications. The approach is shown to have trace overheads that are \nindependent of the amount of sharing that takes place. An implementation for cyclic debugging OR Mach \n3.0 is evaluated and benchmarks show typical performance overheads of around 10%. The algorithm implemented \nis compared with optimal event-based tracing and shown to do better with respect to the number of events \nmonitored or number of events logged, in most cases by several orders of magnitude. Keywords: Non-determinism, \nshared memory, repeat\u00adable execution, instruction counter Thk work was sponsored in part by the Office \nof Naval Research under NOO014-96-1-0205. The views, opinions, and/or findings contained in this report \nare those of the Authors, and shall not be construed as an official position, poiicy or decision by any \nof the sponsoring agencies or the University of Oregon, unless desig\u00ad nated by other documentation. \nPermiSion to make digitakhard mpy of part or ail of this work for personal or d,aasroom usa is ranted \nwithout fee provided that copies are not made or dtstnbuted for pro I t or commercial advantage, the \ncopyright notice, the title of the publication and its date appear, and notica is given that copying \nis by permission of ACM, inc. To copy otherwise, to republish, to post on servers, or tu redistribute \nto lists, requires prior speoific permission andor a fee. PLDI 96 5/96 PA, USA 0 1996AM O-69791 -7954Y9610005... \n$505O 1.0 Introduction Repeatable execution is a crucial component of cyclic debugging, performance \nmonitoring and fault tolerance based on journaling. For example, in cyclic debugging bugs are located \nby repeatedly running an application and then replaying executions that exhibit erroneous behavior in \norder to isolate the cause of the problem. In fault tolerance, programs running on a system that has \nfailed can be recov\u00adered by starting them from a saved earlier state and bringing them up to the state \nthat existed at the time of the failure by recreating inputs as well as other external events. The most \ndifficult types of programs to replay are those that contain non-determinism. Non-determinism is present \nif an application, given the same inputs, can obtain different states and possibly produce different \noutputs across multi\u00adple executions. Sources of non-determinism include asyn\u00adchronous interrupts, concurrent \nor parallel access to shared data, and dependence on external conditions such as time. In many cases \nincreased performance can be obtained by reducing the amount of synchronization among processes sharing \ndata, which leads to the introduction of non-deter\u00administic access patterns. At other times a form of \nnon-deter\u00adminism, called a race, is inadvertently created through the incorreet implementation or omission \nof synchronization. A great deal of research has focused on ways to replay pro\u00adgram exeeution, with the \nmajority focusing on replaying the non-determinism created through shared-memory access in parallel programs. \nRecently, however, uniprocessor operat\u00ading system support for shared memory and light-weight threads \nhas become commonplace [4][7] [16] [15], leading to a need for efficient mechanisms that enable the replay \nof concurrent shared-memory programs. Existing solutions for replaying shared-memory non-determinism \nare all based on the principle that a shared-memory access is an event, and that an application execution \ncan be recreated by recording and replaying the order of the events executed by the appli\u00adcation. While \nthis approach does enable the design of con\u00adcurrent systems with repeatable execution, the overheads \nincurred are often prohibitive. For example, a concurrent program running on a 20 MIPS processor where \n2% of instructions executed are interleaved shared-memory accesses, and where each trace record is 10 \nbytes in size, will generate trace information at the rate of 2MB/s of exe\u00adcution. Tracing, and even \nmonitoring, each shared-memory access can also degrade performance significantly. Further, since the \namount of tracing done depends on the applica\u00adtion s sharing characteristics, the overheads obtained \ncan make the approaches practical for only a restricted class of programs. Another disadvantage of event-based \napproaches is that the system must be aware of every memory access that is poten\u00adtially shared. In most \ncases, this means one of three things: that the shared accesses are well defined at the language or operating \nsystem level; that the trace process is further aug\u00admented to detect sharing dynamically; or that all \nmemory accesses must be considered shared accesses. Each case has drawbacks that can narrow the range \nof applications to which the approach can be effectively applied. In thk paper we present an algorithm \ncalled repeatable scheduling for recording and replaying the execution of shared-memory applications \nthat has several major advan\u00adtages over existing approaches. The technique is based on the fact that \nrepeatable execution can be obtained by recre\u00adating the scheduled behavior of an application at instruction \nlevel granularity. This is accomplished through the use of an instruction counter [1] [11] and a mechanism \nthat makes the application s schedule visible to the replay system. The first advantage of our approach \nover the best event\u00adbased techniques is that trace generation rates can be reduced by several orders \nof magnitude. Because trace information is created only at scheduling points the amount of data generated \nand the frequency of such points is gener\u00adally dependent only on the time-slice of the scheduler. Typi\u00adcal \ntime-slices are on the order of 1-2ms, so for the 20 MIPS computer example mentioned earlier, and in \ngeneral any computer, trace generation rates are fixed and on the order of l-2KB/s. The second advantage \nof the approach is that it is language independent, No information about shared\u00admemory accesses or memory \nlayout is necessary, so the pro\u00adgram does not have to be instrumented at the access level and no run-time \noverhead is incurred for shared accesses. Finally, programs are instrumented with a software instruc\u00adtion \ncounter, a technique that requires minimal analysis of program structure, making it simpler to implement \nthan many existing event-based analysis techniques. The remainder of this paper is organized as follows: \nIn Sec\u00adtion 2.0 we present related work in the area of repeatable execution. Section 3.0 discusses our \napproach, known as Repeatable Scheduling, and in Section 4.0 a replay system aimed at cyclic debugging \nthat has been implemented using our approach is described. Section 5.0 provides a quantita\u00adtive comparison \nof repeatable scheduling with the most effi\u00adcient event-based algorithm. We summarize and conclude in \nSection 6.0.  2.0 Related Work Past approaches to replaying shared-memory applications have focused \non recording how processes interact. The most straight-forward way of recording an execution is to save \nthe value of every shared read operation [14]. The obvious draw-back of thk approach is that if sharing \nis common, trace log size will rapidly explode. In order to minimize the amount of data saved, most approaches \nsave the order of shared accesses, rather than the content. Approaches that save order can be divided \ninto two groups: those that assume coarse-grained sharing through synchro\u00adnization or objects, and those \nthat monitor every shared\u00admemory access. An object oriented treatment of shared memory is seen in [3], \n[5], [8], and [9]. While this scheme can lead to substantial reductions in trace size and overhead, it \nrequires that the objects be implemented correctly so that they do not introduce race conditions. In \nsome cases the object based systems can be made to treat each shared\u00admemory access individually as shared \nobjects, but this can lead to high overheads since they are not designed for fine\u00adgrained operation. \nRecording the order of data synchronization is the strategy seen in [18]. A drawback of this technique \nis that it is not applicable in the debugging domain since applications must be free of races for successful \nreplay to be guaranteed. An approach that is designed for the monitoring of every shared-memory access \nis [12], The method optimizes the amount of ordering information saved by dynamically per\u00adforming a transitive \nreduction on the shared-memory access patterns so that the recorded ordering is optimal. This is shown \nto reduce trace overheads by one to two orders of magnitude over approaches such as [8]. One potential \nissue is the amount of run-time space overhead incurred by the algorithm since a dependency vector must \nbe maintained for each shared-memory location. This can easily raise the memory requirements of an application \nby an order of mag\u00adnitude if there are large amounts of shared data. There are several shortcomings common \nto these approaches. Object based systems require support either at the language or system level. Applications \nwritten without using the designated object model cannot be re-executed under these systems. In addition, \nthe fact that object encap\u00adsulation is assumed to be race-free can make these systems inapplicable in \nthe debugging domain. The approaches that monitor every shared-memory access require that the shared-memory \naccesses be visible to the system and distin\u00adguishable from accesses to private data. In some systems \nshared data is explicitly defined through language or system constructs, but often data sharing is not \nobvious and can be defined at run-time. For example, many current operating systems support multithreaded \nexecution. Applications written for this model of computation have threads of con\u00adtrol that implicitly \nshare their entire address space, requiring monitoring of the complete address space even though only \na few locations may be truly shared. Static analysis of these applications alone is not sufficient to \ndifferentiate shared from non-shared data since pointer dereferences resolved at run-time can determine \nthe difference. None of the existing solutions for replay can be effectively applied for practical replay \nof such applications.  3.0 Repeatable Scheduling While most existing solutions to shared-memory replay \nare designed for parallel systems, our work concentrates on uni\u00adprocessor concurrent execution. In a \nconcurrent system run\u00adning on a uniprocessor only one process or thread of control will be executing \nat any given point in time. Non-determin\u00adism arises in applications that share memory because the interleaving \nof accesses to the shared data by different pro\u00adcesses or threads may be different across executions \nof the application given identical inputs. The key to efficient repeatable execution of such applications \nis the realization that the interleaving is controlled by the system scheduler. To precisely repeat the \nexecution of a non-deterministic shared-memory application the coarse-grained interleaving characteristics \ncan be recreated by repeating the same schedule, ensuring that context switches occur at exactly the \nsame instructions during replay as during the original exe\u00adcution. This section first presents the assumptions \nmade throughout this work, and then describes the basic approach to repeat\u00adable scheduling. Throughout \nthis section and the rest of the paper we refer to executions of an application that are meant to exactly \nrepeat some other execution as repeat executions or replay executions. The execution that is being used \nas the basis for the repetition is known as the original execution. 3.1 System Model This work assumes \nthat applications consist of one or more processes or threads. There are no restrictions on the amount \nof address space shared among threads or processes and this can vary between specific threads or processes. \nFurther, memory sharing can be local between two or more processes or threads, or global across all the \nprocesses and threads. There are no assumptions regarding the visibility or ordering .of shared-memory \naccesses. Since synchronization of shared-memory accesses is not assumed, non-determin\u00adism based on shared-memory-access \nordering or the arrival of asynchronous events can be present in the application. Applications run under \nthe control of a replay system, which is in charge of both recording and playing back an applica\u00adtion \ns behavior. It is assumed that the replay system can be notified of context-switches within an application \nbefore a newly scheduled process or thread is allowed to execute. It is also assumed that there is a \nway for the replay system to control the scheduling of processes and threads. Basic sup\u00adport for this \nassumption is provided on most systems with sleep and wakeup system calls. Finally, we address issues \ninvolving replaying shared-mem\u00adory access only. The recreation of an external environment required for \nreplay, such as input, the state of other applica\u00adtions or hardware for example, are outside the scope \nof this work.  3.2 How the Repeatable Scheduling Algorithm Works The idea behind the repeatable scheduling \nalgorithm is partly based on past work in repeatable execution that focuses on replaying asynchronous \nevents [5][1 1]. In the previous work, the model is that of a single process where non-determinism is \npresent in the form of asynchronous interrupts. Such interrupts can be delivered to a process at any \ninstruction, and correct replay of a process that has received asynchronous events requires that the \nevents be replayed at the same instruction in each repeated execution. This can be accomplished by using \nan instruction counter, as in [11], to record how many instructions have executed between events and \nthen controlling the number of instruc\u00ad tions executed in a replay before an event is recreated. An instruction \ncounter implemented in sotlware is a mem\u00adory location or dedicated register, initialized to its minimum \nvalue at the start of original execution. The counter is incre\u00admented each time the application makes \na backward control transfer. Thus, for each instruction in the execution sequence there is a unique counter \nvalue and instruction pointer pair that can be used to identify the precise location in an execution \nwhere an asynchronous event was deliv\u00adered. When an asynchronous event occurs the replay system saves \nthe instruction count and the instruction pointer of the thread or process being interrupted in a log. \nDuring replay execution, the counter is initialized to the negative of the value stored in the log, and \nwhen the counter increments to zero a routine is called which places a breakpoint on the instruction \nassociated with that count. When the process or thread hits the breakpoint the asynchronous event is \nreplayed. The next counter and pointer pair is then read from the log to determine when to replay the \nnext event. In our algorithm non-determinism can be introduced through shared-memory accesses, as discussed \npreviously. On a uniprocessor the ordering of shared-memory accesses is ultimately determined by how \nthe processes or threads of an application are scheduled. We have therefore extended the notion of an \nasynchronous event in [11 ] to include pro\u00adcess or thread preemptions performed by the scheduler. In \nthe original execution an instruction count and unique iden\u00adtifier of the current process or thread is \nsaved at each pre\u00ademption point in the application, recording the exact place in the instruction stream \nwhere the preemption occurs. In a replay execution, the saved preemption information is read and used \nto force preemptions at the same places in the exe\u00adcution. Consider the example shown in Figure 1(A). \nThe applica\u00adtion that is to be replayed consists of three threads which are scheduled as shown. At each \nof the preemption points the instruction count, instruction pointer and the identifier of the thread \nbeing preempted are saved to a log. In the replay, shown in Figure 1(B), the replay system starts the \nre-execu\u00adtion by putting threads 2 and 3 to sleep, but lets thread 1 run until it has executed the same \nnumber of instructions as it did in the original execution before the first preemption point. When that \npreemption point is reached, the replay system is notified and it puts thread 1 to sleep. The replay \nscheduler then makes the same scheduling decision as in the original run so it again wakes up thread \n2. A similar proce\u00addure is followed at the second preemption point. Thread IIIIIUIIIIIIIIU I Thread \n2 !,,,,,,,,,,,,,8,,,,,, ,,,,,,, II Ipreemptive context switch Thread 1 4.1 Mach 3.0 Replay To make our \nimplementation sible we have made it part Mach system library. The System Design as general and portable \nas pos\u00adof a new debug version of the steps necessary to prepare an application for repeatable execution \nare shown in Figure 2. The application is first passed through a preprocessor that instruments the assembly \nlanguage output of the compiler with a software instruction counter. The application is then linked with \nthe debug library (which is also instrumented) in lieu of the standard library, After the program is \nrun, it can be forced to repeat non-deterministic behavior by speci\u00adfying a command line flag along with \nthe name of the log file of the original run. By running the application under an existing debugger, \nsuch as GNU s gdb for Mach 3.0, bugs can be isolated and identified. 4 Assembly Files + 111111111111111111111111111 \n  ;;yJ~ ~11,11,1111,1,1111,,, I ktramcmted Object h (9CI 0 @ Repkzysystem wakes up thread 1, ensures \nthrewk 2 and 3 are sleeping @ Replay system puts thread 1 to sleep, wakes up thread 2 ~ Replay system \nputs thread 2 to sleep, wakes up thread 3 (B) Replay Execution Figure 1. Example of repeatable scheduling \n The two requirements that make a replay system possible are that the application s schedule is visible \nto the replay system and that an instruction counter is provided to mea\u00adsure the progress of the application. \n4.0 Application for Debugging Debugging concurrent and parallel programs can be espe\u00adcially difficult \nbecause of non-determinism in shared-mem\u00adory access, both intentional and inadvertent. To address the \nproblem we have implemented a replay system for concur\u00adrent Mach 3.0 [16] applications that require repeatable \nexe\u00adcution for debugging purposes. Mach provides both explicit shared memory with virtual memory control \nprimitives, and implicit shared memory through the use of multithreading, making existing event-based \nreplay techniques unsuitable. ............................................ L Repeatable Application \nFigure 2. Steps in preparing an application for repeatable execution  4.2 The Preprocessing Stage: Implementing \nThe Instruction Counter Applications being readied for repeatable execution are first compiled with the \nGNU gcc compiler, which allows one to specify that a particular register be left unused in the assem\u00adbly \noutput. The assembly-language output of this compila\u00adtion is fed to the replay preprocessor, which performs \nthe software instruction counter instrumentation using the dedi\u00adcated register as the instruction counter. \nBackward branches and calls are located, with code such as the following pseudocode fragment added at \neach occurrence: increment count registec if(count register == O)call overflowo; The preprocessor also \nreplaces the application s main ( ) function with a wrapper in the debug library that is used to initialize \nthe replay system. This function also allows for command line flags to be passed to the replay system \nand is further described below. The preprocessor is implemented with an Awk script [1] that is about \n100 lines long. 4.3 The Debug Library: Scheduling the Application Once the application has been instrumented, \nit must be linked with instrumented versions of the system libraries such as 1 ibc. a. Normally, Mach \nprograms are also linked with the 1 ibmach. a library, but when repeatable execu\u00adtion is desired the \napplication is instead linked with the replay version of the Mach library called 1 ibreplay. a. This \nversion of the Mach library is instrumented and moni\u00adtors or recreates the scheduling of applications. \nThe deci\u00adsion to implement the scheduler as a library limits the replay to single-task applications, \nbut it is the most portable approach to making an application s schedule visible to the replay system. \nWhen an application begins executing it consists of one Mach thread running in one task (a task is essentially \nan address space -one thread running in a task is equivalent to a Unix process). If the command line \nflags indicate that the run is an original execution the replay system prepares itself for logging, but \nremains dormant until the application becomes multithreaded through the creation of additional threads. \nAt that time the replay system spawns a scheduling thread that runs at a higher priority than the application \nthreads and begins controlling the scheduling of the applica\u00ad tion. The replay-system scheduler keeps \ntrack of the threads that exist throughout the application s execution. At any point in time the scheduler \nallows only one thread to be active and therefore runnable by the Mach scheduler. When threads are preempted, \nthe scheduler records the thread s identifier, instruction count value and instruction pointer before \nwak\u00ading up the next thread. Records are logged to a tile via the standard C library s stream I/O facilities. \nThe built-in buffer size of 1KB means that a flush of file data, through a system call, occurs about \nonce every second. To replay an execution the user must specify a replay logon the application s command \nline. The replay scheduler reads preemption records from the log and instead of preempting based on time-slices, \nit uses the instruction counter to deter\u00admine when to force preemptions. Because the scheduler is deterministic, \nit performs the same scheduling decisions as it did in the original run of the application, meaning that \nno information is logged during context-switches that occur due to blocking (since the succeeding thread \nwill be the same during both the original and replay runs). This ensures that the only scheduling decisions \nthat must be logged are those due to time-slice preemptions.  4.4 Mach 3.0 Replay System Performance \nThe replay system is implemented on Mach 3.OAJX (UNIX 4.3 BSD) running on a 90MHz Pentium processor with \n16MB of memory. Two synthetic workloads and the SPLASH-2 benchmark kernels are used to demonstrate the \nperformance of the replay system, All programs are multi\u00adthreaded single task applications written using \nthe Mach C-Threads library, The first application is a simple counting program where multiple threads \nincrement a shared counter until it reaches a specified value. There is no synchronization in the incre\u00adment \nloop so the order in which the threads perform the increment is non-deterministic. Therefore, the scheduler \ns time-slicing policy will determine the order and frequency with which threads increment the counter. \nThis application demonstrates the overhead incurred by the software instruc\u00adtion counter code in small \nloops. The second synthetic application is a standard multi\u00adthreaded matrix multiply algorithm. A user-specified \nnum\u00adber of threads are created, and assigned blocks of the matrices to multiply independently. This application \ncon\u00adtains no synchronization and exhibits the same manner of context-switching behavior as the first \napplication. How\u00adever, the number of shared variables accessed depends on the sizes of the matrices being \nmultiplied. This application demonstrates that for repeatable scheduling, overheads are independent of \nthe amount of data sharing performed by an application. The SPLASH-2 (Stanford Parallel Applications \nfor Shared-Memory) benchmark suite [19] is used to quantify the expected behavior of repeatable scheduling \nin real applica\u00adtions. Because the SPLASH applications are not intended to run on a uniprocessor, they \nin many ways exhibit behavior that is not uniprocessor friendly, such as making free use of synchronization \namong threads. For this reason, we expect that results obtained from SPLASH-2 pessimistically por\u00adtray \nthe overhead of the repeatable scheduling approach compared to its use on actual shared-memory and multi\u00adthreaded \nprograms targeted for uniprocessors. All four of the SPLASH-2 executions use 8 threads. Results for both \nthe synthetic workloads and the SPLASH bench\u00admarks are given in Table 1. The Native column contains execution \ntime using the standard Mach and C-Threads libraries under the control of the Mach system scheduler. \nThe Instrumented column shows execution time when applications are instrumented and run using the repeatable \nscheduling scheduler, and the Replay column gives time for subsequent replayed executions. The final \ncolumn is the size of the trace logs generated for the instrumented execu\u00adtions. The table demonstrates \nthat performance degradation due to instrumentation and scheduling is around 10-15%. In one instances, \nCholesky, the performance with instrumentation Application Native Execution (s) Instrumented Execution \nTme Ovrhd (s) Replay Execution Time Ovrhd (s) Ikace Size Synthetic count 8.47 9.16 8.1% 9.44 11.5% llKB \nWorkloads matrix multiply 27.59 30.05 8.9% 31.35 13.6% 26KB Radix 4.94 5.22 5.7% 5.37 8.7% 6KB Splash \nKernels LU LU ~T -contiguous -non-contiguous 10.58 16.08 1.65 12.33 17.52 1.82 16.5% 9.0% 10.3% 12.20 \n17.79 1.85 15.370 10.6% 12.170 14KB 20KB lKB Cholesky 1.66/16.48 I 2.63 na 2.66 na 2KB TABLE Application \n1. Performance of Mach 3.0 Replay II%of Overhead: Instruction Counter System % of Overhead: Scheduler+Logging \n I Synthetic count Workloads matrix multiply-. I Radw II Splash Kernels ~: 1 FFT II TABLE 2. Breakdown \nof Mach is superior to the native system because the imposed sched\u00aduler uses a different, occasionally \nsuperior, scheduling algo\u00adrithm than the native scheduler. In this case two measurements are shown for \nnative execution: time for a run with a single thread, on the left, and for a run with the same number \nof threads as was used for the instrumented execution on the right. Trace size generation rates are fairly \nconstant at about 1KB per second across all the applications measured. This result is significant in \nlight of the fact that it is independent of the amount of shared accesses in the application. For example, \nLU-contiguous executes 93 million shared reads and 44 mil\u00adlion shared writes, all of which require atomic \nmonitoring by event-based replay algorithms. Under repeatable sched\u00aduling only 14KB of trace is generated \nwith an execution overhead of about 15 percent. During the replay of a run the performance overhead of \nthe instrumentation is typically slightly larger than for the origi\u00adnal run. This is due to the fact \nthat during a replay the scheduling decisions made by Mach may need to be over\u00adridden by the replay mechanism, \ncausing needless context switches. 87 13 39 61 75 38 4159 3.0 Replay System Overhead A breakdown of \nthe components that makeup the overhead is shown in Table 2. By timing executions of instrumented code \nand comparing it with the performance of instrument\u00aded+scheduled code the amount of overhead attributed \nto the software instruction counter overhead is due to scheduling that control the progress of writing \nlog records. Overhead rises in programs with tight is obtained. The rest of the events, such as system \ncalls threads, and the overhead of due to the instruction counter loops where the instruction counting \ncode accounts for a significant percentage of the code in the loop. This is seen most clearly in the \ncount application, which is essentially a small loop. Overhead due to scheduling rises in programs that \nperform frequent syn\u00adchronization operations that lead to non-preemptive context switches. The LU-contiguous \napplication performs more synchronization than the other applications and this is reflected in the scheduling \noverhead. A graph of the over\u00adheads with the overhead breakdown is shown in Figure 3. 5.0 Comparison \nwith Optimal Event-Based Replay We present in this section a comparison of the repeatable\u00adscheduling \nalgorithm to the optimal parallel trace algorithm Optimal Trace Repeatable Scheduling Application Best \nCase Worst Case count 1 matrix multiply 160K Radix lM LU-contiguous 262K FFT 64K Cholesky 4128 TABLE \n3. Comparison of Events Logged Application count matrix multiply Radix LU-contiguous FFT Cholesky II \nTABLE 4. Comparison of Events Monitored of [12]. Though the optimal trace algorithm supports replay on \ntrue parallel as well as sequential machines, to this date it is also the optimal approach for replay \nof concurrent sys\u00adtems. The optimal trace algorithm, while requiring more complex run-time monitoring \nof shared accesses than other event-based replay methods, dynamically detects race con\u00additions in order \nto generate the minimal amount of trace information. Race-condition detection is performed by maintaining \ndependency vectors for each shared-memory location that indicate the order of thread read and write accesses \nto the location. The comparison here is made not on raw performance, but rather on the criteria of number \nof trace records generated, and number of events monitored during execution. Both algorithms require \napproximately the same amount of information in a trace record, so comparing the number of records reflects \nthe relative trace log size. The repeatable scheduling algorithm requires that a scheduling decision \nbe made whenever system calls block, so we treat such poten\u00adtially-blocking calls as monitored events. \nIn the optimal\u00adtrace algorithm, all shared-memory accesses are monitored to determine minimal sequencing \ninformation. To make the comparison as favorable as possible for the optimal-trace algorithm, we assume \nthat monitoring is performed only on the data actually being shared by the algorithm imple\u00admented in \neach test program, rather than the entire address 10M 2K 64M 12K 19M 226 137M 2.2K 6.9M 154 99M 2.4K \n For Optimal Tkace and Repeatable Scheduling Optimrd Repeatable hate Scheduling 10M 2032 64M 12K 19M \n8130 137M 39K 6.9M 2810 99M II 1.9M 1 For Optimal Trace and Repeatable Schedulhg space of the threads, \nas is actually the case. Further, we do not model the optimal-trace algorithm s run-time space overhead \nthat results from the fact that dependency vectors must be maintained for each shared-memory location. \nThe dependency vector overhead can raise the memory require\u00adments of an application like matrix multiply \nby an order of magnitude, which can adversely affect performance by increasing page faults and cache \nmisses. The programs we have chosen for our analysis are the same as those used in the performance measurements. \nHowever, we use reported statistics [19], rather than actual measure\u00adments for the comparison. because \ndata such as the number of shared accesses is difficult to obtain otherwise. Table 3 shows the number \nevents logged for the benchmark programs for the optimal-replay and repeatable-scheduling algorithms, \nBecause for optimal replay the precise number of events logged depends upon the interleaving of threads \nduring execution, we instead show the best and worst case scenarios. The best case numbers for optimal \nreplay assume that each global variable is accessed only once during the entire execution, so this represents \nan extremely favorable assumption for the minimum number of events logged. The typical case is closer \nto the worst case number, in which every access to a shared variable generates a log entry. The column \nfor repeatable scheduling shows only a single num\u00ad ber, because the number of events is strictly a function \nof execution time (i.e., number of preemptive context switches) which is fairly constant across runs \nand possible thread interleaving. \u00ad 20.0 o Instruction Counter o Scheduling/Logging  Replay (total) \n1 15.0 g . 10,0 5.0 0.0 Figure 3. Breakdown of Overhead During Execution and Replay The table demonstrates \nthat the repeatable-scheduling algo\u00adrithms typically produces event logs several orders of mag\u00adnitude \nsmaller than the optimal-replay algorithm, even under best-case conditions. During worst case conditions \nthe difference often grows by several orders of magnitude again. Repeatable scheduling allows even large \napplications to be monitored and replayed without requiring excessive amounts of disk space for storing \nlog files. Table 4 shows the number of events that must be monitored during execution under repeatable \nscheduling and optimal replay. Optimal replay must monitor every shared variable access in order to check \nwhether it is being accessed by the same thread as its previous access. On the other hand, repeatable \nscheduling must monitor all events in which a scheduling decision can possibly be made. This set is com\u00adposed \nof all preemptions plus all blocking events, due to synchronization or otherwise. As with Table 3, one \nsees that the repeatable scheduling approach yields overheads several orders of magnitude smaller than \ndoes optimal replay, since there are typically thousands of shared variable accesses for every preemption \nor blocking event. The smaller amount of monitored activity under repeatable scheduling results in less \noverhead during execution, and increased overall per\u00adformance. While repeatable scheduling monitors fewer \nevents than does optimal replay, it should be pointed out that, depending on the implementation, the \noverhead required for monitor\u00ading a single event may be larger under repeatable schedul\u00ading. For example, \nour current implementation requires several system calls during each monitored event, while optimal replay \nmay or may not (depending on the target sys\u00adtem and implementation). However, with a small amount of \noperating system support the overhead for repeatable sched\u00aduling can be reduced to a single system call \n[17]. In addi\u00adtion, because the number of monitored events is so much smaller than with optimal replay, \nwe expect overall perfor\u00admance under repeatable scheduling to be greater in spite of any increased per-event \noverhead. 6.0 Summary We have presented an algorithm called repeatable schedul\u00ading for repeating the \nexecution of concurrent non-determin\u00adistic shared-memory applications. The algorithm has several major \nadvantages over existing replay algorithms. Fkst, unlike existing event-based techniques, there is no \nneed to differentiate shared-memory accesses from non\u00adshared accesses. The trace log generation rate \nis fixed, small, and is independent of the amount of sharing or syn\u00adchronization present in the application \nbeing replayed. The number of monitored events is also independent of the num\u00adber of shared-memory accesses. \nFinally, the algorithm is simple to implement and has minimal impact on application memory requirements. \nWe have implemented the algorithm in the context of a debugging replay system for multithreaded Mach \n3.0 appli\u00adcations and shown that the repeatable scheduling technique is an efficient way to debug non-deterministic \nconcurrent programs. In addition, comparisons of repeatable schedul\u00ading with the optimal event-based \nreplay algorithm indicate that repeatable scheduling does significantly better in both number of events \nmonitored and the number of events logged for asynchronous, as well as synchronous, applica\u00adtions. Future \nwork includes performance enhancements to the repeatable scheduling implementation as well as its applica\u00adtion \nin the fault-tolerance domain, Exploration into the use of repeatable scheduling for debugging parallel \napplications on a uniprocessor is also being considered. Because varying the time-slices in the scheduling \nalgorithm provides the ability to control the amount of non-determinism present in an execution, repeatable \nscheduling may provide an effi\u00adcient way to debug parallel programs with control over the non-determinism \nversus performance and trace size trade\u00adoff. Many newer processors directly support instruction count\u00ading \nin hardware, potentially eliminating the performance overhead of instrumentation. Ongoing work leverages \nthis by moving parts of the replay mechanism into the underly\u00ading OS [17], allowing both high-performance \nand complete application-transparency. 7.0 References [1] A. Aho, B. Kernighan and P. Weinberger, The \nAWK Programming Language, Addison-Wesley, Reading, MA. 1988. ,nm, r.-., . ,.. - T-. .J .-. .!7 [2] -A \nn ana IS. N. ~ocantru, ueap mraware 1. A. wrgm sup\u00adport for Software Debugging and Profiling, in Proc. \nSymp. on Architectural Support for Prog. Lang. and Operating Syst., Palo Alto, CA, Oct. 1987, pp. 82-83. \n [3] R. H. Carver and K. C. Tai, Reproducible Testing of Concurrent Programs Based on Shared Variables; \nin Proc. 6th Int. Conf on Distributed Computing Systems, Boston, MA., May 1986, pp. 428-432. [4] H. Custer, \nInside Windows NT; Microsoft Press, Red\u00admond, WA, 1993. [5] P. Dodd and C. Ravishankar, Monitoring and \nDebug\u00adging Distributed Real-Time Programs, So@are Prac\u00adtice and Experience, Vol. 22(10), Oct. 1992, pp. \n863\u00ad 877. [6] M. Johnson, Some Requirements for Architectural Support of Software Debugging; in Proc. \nof the Symp. on Architectural Support for Prog. Lang. and Operat\u00ading Syst., Palo Alto, CA, Mar. 1982, \npp. 140-148, [7] A. King, Inside Windows 95, Microsoft Press, Red\u00admond, WA, 1994. [8] T. J. LeBlanc and \nJ. M. Mellor-Crummey, Debugging Parallel Programs with Instant Replay. IEEE Trans. on Computers, Apr. \n1987, pp. 471-482. [9] C. Lln and R. LeBlanc, Event-Based Debugging of Object/Action Programs, in Proc. \nof the ACM SIG-PL4N/SIGOPS Workshop on Parallel and Distributed Debugging, 1988, pp. 23-34. [10] C. E. \nMcDowell and D. P. Helbold, Debugging Con\u00adcurrent Programs, ACM Computing Surveys, Dec. 1989, pp. 593-622. \n[11] J. M. Mellor-Crammey and T. J. LeBlanc, A Software Instruction Counter, in Proc. Symp. on Architectural \nSupport for Prog. Lung. and Operating Syst., Palo Alto. CA. Am. 1989. m. 78-86. [12] R. Netzer, ~ptimal \nl&#38;icing and Replay for Debugging Shared-Memory Parallel Programs; in Proc. ACM/ ONR Workshop on Parallel \nand Distributed Debug\u00adging, May 1993, pp. 1-11. [13 R. Netzer and B. Miller, On the Complexity of Event \nOrdering for Shared-Memory Parallel Program Execu\u00adtions, in Proc. Int. Conf on Parallel Processing, 1990, \npp. 93-97. [14] D. Pan and M. Linton, Supporting Reverse Execution of Parallel Programs, in Proc. SIGPLAN/SIGOPS \nWorkshop on Parallel and Distributed Debugging, May 1988, pp. 124-129. [15] M. L. Powell, et. al., SunOS \nMultithreaded Architec\u00adture; Sun Microsystems White Paper, Sun Microsys\u00adtems, Cupertino, CA, June 1995. \n[16] R. Rashid, et. al., Mach: A Foundation for Open Sys\u00adtems, in Pmt. 2nd Workshop on Workstations and \nOperating Syst., Sept. 1989, pp. 27-29. [17] M. Russinovich and B. Cogswell, Operating System Support \nfor Replay of Concurrent Non-Deterministic Shared Memory Applications, in Bulletin of the Tech\u00adnical \nCommittee on Operating Systems and Applica\u00adtions Environments (TCOS), IEEE Computer Society, Whter 1995, \nVol. 7, No. 4, pp. 15-19. [18] K. C. Tai, R. H. Carver, and E. E. Obaid, Debugging Concurrent Ada Programs \nby Deterministic Execu\u00adtion, IEEE Trans. on So@are Engineering, Jan. 1991, pp. 45-63. [19] S. C. Woo, \nM. Ohara, E. Torrie, J. P. Singh, and A. Gupta, The SPLASH-2 Programs: Characterization and Methodological \nConsiderations, in Proc. of the 22nd International Symposium on Computer Architec\u00adture, June 1995, pp. \n24-36.  \n\t\t\t", "proc_id": "231379", "abstract": "Replay of shared-memory program execution is desirable in many domains including cyclic debugging, fault tolerance and performance monitoring. Past approaches to repeatable execution have focused on the problem of re-executing the shared-memory access patterns in parallel programs. With the proliferation of operating system supported threads and shared memory for uniprocessor programs, there is a clear need for efficient replay of concurrent applications. The solutions for parallel systems can be performance prohibitive when applied to the uniprocessor case. We present an algorithm, called the repeatable scheduling algorithm, combining scheduling and instruction counts to provide an invariant for efficient, language independent replay of concurrent shared-memory applications. The approach is shown to have trace overheads that are independent of the amount of sharing that takes place. An implementation for cyclic debugging on Mach 3.0 is evaluated and benchmarks show typical performance overheads of around 10%. The algorithm implemented is compared with optimal event-based tracing and shown to do better with respect to the number of events monitored or number of events logged, in most cases by several orders of magnitude.", "authors": [{"name": "Mark Russinovich", "author_profile_id": "81100337968", "affiliation": "Department of Computer Science, University of Oregon, Eugene, OR", "person_id": "PP31092560", "email_address": "", "orcid_id": ""}, {"name": "Bryce Cogswell", "author_profile_id": "81541154356", "affiliation": "Department of Computer Science, University of Oregon, Eugene, OR", "person_id": "P34085", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/231379.231432", "year": "1996", "article_id": "231432", "conference": "PLDI", "title": "Replay for concurrent non-deterministic shared-memory applications", "url": "http://dl.acm.org/citation.cfm?id=231432"}