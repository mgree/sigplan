{"article_publication_date": "05-01-1996", "fulltext": "\n Printing Floating-Point Numbers Quickly and Accurately Robert G. Burger* R. Kent Dybvig Indiana University \nComputer Science Department Lindley Hall 215 Bloomington, Indiana 47405 (812) 855-3608 {burger, dyb}@cs.indiana. \nedu Abstract This paper presents a fast and accurate algorithm for print\u00ading floating-point numbers in \nboth free-and fixed-format modes. In free-format mode, the algorithm generates the shortest, correctly \nrounded output string that converts to the same number when read back in, accommodating what\u00adever rounding \nmode the reader uses. In fixed-format mode, the algorithm generates a correctly rounded output string \nusing special # marks to denote insignificant trailing digits. For both modes, the algorithm employs \na fast estimator to scale floating-point numbers efficiently. Keywords: floating-point printing, run-time \nsystems 1 Introduction In this paper we present an efficient floating-point printing algorithm, which \nsolves the output problem of converting floating-point numbers from an input base (usually a power of \ntwo) to an output base (usually ten). The algorithm supports two types of output, free for\u00admat and fixed \nformat. For free-format output the goal is to produce the shortest, correctly rounded output string that \nconverts to the same internal floating-point number when read by an accurate floating-point input routine \n[1]. For ex\u00ad ample, % would print as 0.3 instead of 0.2999999. The algorithm accommodates any input rounding \nmode, includ\u00ad ing IEEE unbiased rounding, for example. For fixed-format output the goal is to produce \ncor\u00ad rectly rounded output to a given number of places with\u00ad out garbage digits beyond the point of \nsignificance. For example, the floating-point representation of ~ might print as 0.3333333148 even though \nonly the first seven digits are significant. The algorithm uses special # marks to denote in\u00ad significant \ntrailing digits so that ~ prints as O. 3333333###. These marks are useful when printing denormalized \nnum\u00ad bers, which may have only a few digits of precision, or when printing to a large number of digits. \nOur algorithm is based on an elegant floating-point print\u00ad ing algorithm developed by Steele and White \n[5]. Their al\u00ad Supported in part by a National Science Foundation Graduate Research Fellowship  Permiaaion \nto make digitahlwird copy of part or all of this work for personal or classroom uea is granted without \nfee provided that copies are not made or distributed for profit or mmmercial advantage, the copyright \nnotice, the title of the publication and its date appear, and notice is given that copying is by permission \nof ACM, Inc. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires \nprior specific permission andlor a fee. PLDI 96 5/96 PA, USA 01996 ACM 0-69791 -795-296/0005... $3.50 \ngorithm also supports both free-and fixed-format output, although it does not properly handle input rounding \nmodes or distinguish between significant and insignificant trailing zeros. Furthermore, it is unacceptably \nslow for practical use. An important step in the conversion algorithm is to scale the floating-point \nnumber by an appropriate power of the output base. Steele and White s iterative algorithm re\u00adquires O(llog \nz I) high-precision integer operations to scale x, which results in poor performance for floating-point \nnum\u00adbers with very large and very small magnitudes. We devel\u00adoped an efficient estimator that always \nproduces an estimate within one of the correct power, so our algorithm scales all floating-point numbers \nin just a few high-precision integer operations. Section 2 develops a basic floating-point printing al\u00adgorithm \nin terms of exact rational arithmetic. Section describes an implementation of the algorithm using high\u00adprecision \ninteger arithmetic and our efficient scaling-factor estimator. Section 4 extends the algorithm to handle \nfixed\u00adformat output and introduces # marks. Section 5 summa\u00adrizes our results and discusses related work. \n2 Basic Algorithm In describing the basic algorithm, we first explain how floating-point numbers are \nrepresented, using the IEEE doubIe-precision floating-point specification as an exam\u00adple [3]. Second, \nwe develop an output algorithm based on a key feature of the representation, the gaps between floating\u00adpoint \nnumbers. Finally, we prove that our algorithm gen\u00aderates the shortest, correctly rounded output string \nfrom which the original floating-point number can be recovered when input. 2.1 Float ing-Point Represent \nat ion An important goal in the design of floating-point numbers is to provide a representation that \ncan approximate real num\u00adbers to a certain number of digits of accuracy. Consequently, a floating-point \nrepresentation embodies the notions of the first few significant digits and the location of the decimal \npoint. A floating-point number is modeled mathematically by a mantissa, which corresponds to the first \nfew significant digits, and an exponent, which corresponds to the location of the decimal point. For \nexample, suppose w is a floating\u00adpoint number in base b (usually two). The mantissa, ~, and exponent, \ne, are base-b integers such that v = f x be and 1~1 K b , where p is the fixed size of the mantissa \nin base-b digits. Moreover, o is called normalized if b -l ~ 1~1, i.e., the mantissa begins with a non-zero \ndigit. An un-normalized, non-zero floating-point number can be normalized by shifting the mantissa to \nthe left and re\u00adducing the exponent accordingly. Because the exponent has a fixed size, however, some \nnumbers cannot be normalized. Such numbers are called denormalized floating-point num\u00adbers. If the input \nbase is two, the mantissa of normalized, non\u00adzero numbers always begins with a one. Consequently, this \ninitial bit is often omitted from the representation and is called the hidden bit. These representations \noften reserve an exponent bit pattern to signal denormalized numbers, The IEEE specification [3] also \nprovides representations for 0.0, positive infinity (+inf), negative infinity (-inf), and not a number \n(NaN). An IEEE double-precision floating-point number, v, is represented as a 64-bit datum composed of \nthree fields: a one-bit sign, an eleven-bit unsigned biased exponent (be), and a 52-bit unsigned mantissa \n(m) with a hidden bit. If 1 ~ be <2046, v is a normalized floating-point number whose value ~s sign (252 \n+ m) x 2be-1075. If be = O, v is a denormalized floating-point number whose value is sign m x 2-1074, \nwhich includes +0.0 and 0.0. If be = 2047 and m = O, v is +inf or -inf, depending on the sign. If be \n= 2047 and m# O,v isNaN. This type of representation produces uneven gaps be\u00adtween floating-point numbers. \nFloating-point numbers are most dense around zero and decrease in density as one moves outward along \nthe real number line in either direction. Given a floating-point number, v, it is useful to define its \nfloating-point successor, denoted by v+, and predecessor, denoted by v-. All real numbers between ~ and \n* round to v. Suppose v = f x be as before. We consider the case where ~ > O; the case for f <0 is completely \nanalogous. For all + is (f -+ 1) x be. If f + 1 no Ion er fits in the fixed-size 1 ;a~tissa, i.e., if \nf + 1 = bp, then v is bp-l x be+l. If e is the maximum exponent, v+ is +inf. For most v, v-is (~ 1) \nx be. For the remaining v, the gap is narrower. If ~ = b -l and e is greater than the minimum exponent, \nv-is (b -1) x be l. 2.2 Algorithm We now develop an algorithm that takes advantage of the gaps between \nfloating-point numbers in order to produce the shortest, correctly rounded output string from which the \noriginal floating-point number can be recovered when input. For purposes of discussion, we limit the \ninput to posi\u00ad tive floating-point numbers. Given a positive floating-point number v in terms of its \nmantissa and exponent, the algo\u00ad rithm uses v and v+ to determine the exact range of values that would \nround to v when input. Because input rounding algorithms use different st rat egies to break ties (e.g., \nround up or round to even), we initially assume that neither end point of the rounding range can be \nguaranteed to round to v when input. In Section 3 we show how to relax this con\u00ad straint based on knowledge \nof a particular input rounding algorithm. The algorithm uses exact rational arithmetic to perform its \ncomputations so that there is no loss of accuracy. In order to generate digits, the algorithm scales \nthe number so that it is of the form 0.d1cf2 . . .. where dl, dz, . . .. are base-B digits. The first \ndigit is computed by multiplying the scaled number by the output base, B, and taking the integer part, \nThe remainder is used to compute the rest of the digits using the same approach. The rounding range determines \nwhen the algorithm stops generating digits. After each digit is generated, the algo\u00adrithm tests to see \nif either the resulting number or the re\u00adsulting number with the last digit incremented is within the \nrounding range of v. If one or both of these numbers is within range, the number closer to v is chosen. \nIn the case of a tie, any strategy can be used to decide, since both possi\u00adbilities would round to v \nwhen input. By testing the output number at each digit, the algorithm produces the shortest possible \noutput string that would correctly round to v when input. Moreover, it generates digits from left to \nright with\u00adout the need to propagate carries. The following is a more formal description of the algo\u00adrithm. \nWe use Lzj to denote the greatest integer less than or equal to z, [zl to denote the least integer greater \nthan or equal to z, and {z} to denote z -Lzj. We always indicate multiplication with the x sign, because \nwe use juxtaposition to indicate digits in a place-value notation. Input: output base B and positive \nfloating-point number v = f x be of precision p >0 Output: V = O.dldz . . . d~ x B~, where dl,..., dn \nare base-B digits and n is the smallest integer such that: (1) ~ < V < ~, i.e., V would round to v when \ninput, regardless of the input rounding algorithm, and (2) IV VI S ~, i.e., V is correctly rounded. \n Procedure: 1. Determine v-and v+, the floating-point predecessor and successor of v, respectively. \nv-be ife= min. exp. or ~ # bp-l v- = v -be-l if e > min. exp. and f = bp-l { V+=v+be Let high = ~ and \nlow = ~. All numbers be\u00adtween low and high round to v, regardless of the input rounding algorithm. \\I \nD v-10W v high v+ 2. Find the smallest integer k such that high S Bk; i.e., k = pog~ highl. k is used \nto scale v appropriately. 3. Let go = &#38;. Generate digits as follows:  dl=Lqox Bj ql={qox~} d2=[q1x \nBJ qz={qlx B} 4. Stop at the smallest n for which (1) O.dl . . . dn x Bk > low, i.e., the number output \nat point n would round up to v, or (2) O.d, . . . d~_l[d~+l]l x B~ < high, i.e., increment\u00ading digit \ndn would make the number round down to v. If condition (1) is true and condition (2) is false, return \nO.dl . ..dnx Bk. If condition (2) is true and condition (1) is false, return b.dl . . . dn-l[d~+l] X \nBk. If both conditions are true, return the number closer to v. If the two are equidistant from v, use \nsome strategy to break the tie (e.g., round up).  2.3 Correct ness We now prove that our algorithm is \ncorrect. We begin by showing that the algorithm generates valid base-B digits, the first of which is \nnon-zero, and that there is no need to propagate carries in the case of incrementing the last digit. \nBecause O~ qi < 1for all O~ i < n, all the d~are valid base-B digits. Termination condi~ion (2) guarantees \nthat if d~ is incremented, no carry will be introduced; for if there were a carry, termination condition \n(2) would have held at the previous step. By the minimality of k and termi\u00adnation condition (2), the \nfirst digit must be non-zero. (See Theorem 1 in Appendix A for a complete proof.) Next we show that output \ncondkion (1) holds, i.e., the algorithm always terminates with a number that correctly rounds to v when \ninput, satisfying the goal of information preservation, In order to prove this, we first prove an in\u00advariant \nof the digit-generation loop by induction: for all i, O.dl . ..d. xBk+qix Bk- = v. In other words, the \nnumber generated at step i is qi x Bk-i below v. (See Lemma 2 in Appendix A.) This invariant leads to \na more concise version of the termination conditions (see the corollary to Lemma 2 in Appendix A): (1) \nqnx Bk-n <v low, and (2) (1 -q.) x Bk-n < high -V.  Since O ~ qn < 1 and Bk- becomes arbitrarily close \nto zero as n increases, termination condition (1) eventually holds; thus, the algorithm always terminates. \nMoreover, the invariant and the above termination conditions guarantee that the algorithm terminates \nwith a number strictly be\u00adtween low and high. (See Theorem 3 in Appendix A.) Having shown that the output \nrounds to v when in\u00adput, we now show that the last digit of the output is cor\u00adrectly rounded, i.e., output \ncondition (2) holds. Because the algorithm chooses the closer of O.dl . . . dn x Bk and O.dl ,,. d~-1 \n[dn+l] x Bk, the last digit is correctly rounded. (See Theorem 4 in Appendix A.) Finally, we show that \nno shorter output string rounds to v when input. Equivalently, no (n -1)-digit num\u00adber (trailing zeros \nare allowed) also rounds to v when in\u00adput. Suppose such a number, V , exists. Using the in\u00advariant, one \ncan easily show that O.dl . . . dn-1 x Bk and O.dl . . . dn-z [dn-1+1] x Bk are the two (n -I)-digit \nnum\u00adbers closest to v. Without loss of generality, we assume that d{ The notation O.dI . . . dn_l [cL \n+1] denotes ~ + ~ ~. Infor\u00ad 4=1 really, this represents the number formed by incrementing the last digit. \nV is one of them. If V is the first, termination condition (1) would have held at step n -1, a contradiction. \nIf V is the second, termination condition (2) would have held at step n -1, a contradiction. Therefore, \nno shorter output string rounds to v when input. (See Theorem 5 in Appendix A.) 3 Implementation The \nbasic output algorithm presented in the preceding sec\u00adtion can be implemented directly in Scheme using \nbuilt-in exact rational arithmetic. The resulting code, however, runs slowly, especially for floating-point \nnumbers with Iarge ex\u00adponents. The two main sources for the inefficiency are the high-precision rational \narithmetic and the iterative search for the scaling factor k. Because the algorithm does not need the \nfull generality of rational arithmetic (i. e., there is no need to reduce fractions to lowest terms or \nto maintain separate denominators), it is more efficient to convert the algorithm to use high-precision \ninteger arithmetic with an explicit common denominator. In this section we modify the algorithm to use \nhigh-precision integer arithmetic and a fast estimator for determining k,  3.1 Integer Arithmetic In \norder to eliminate the high-precision rational arithmetic, we introduce an explicit common denominator \nso that the algorithm can use high-precision integer arithmetic. We also make use of the more concise \ntermination conditions given in the preceding section. Procedure: 1, Initialize T, s, m+, and m-such \nthat v = $, ~ = = ~ according to Table 1,~, and~ 2. Find the smallest integer k such that ~ ~ Bk; i.e., \n .. k= log~q .  3. Ifk~O, letro=T, so=sx Bk, m~=m~, and  m; =m-. Ifk<O, letro=T xB-k, so=s, m~ =m+x \nB-k, -k andm~=m x B . Generate digits w follows:  = 1%3 1={%} l=sO ml =m~x B m~=m~xB 2=1%3 ={*} m2 \n=m~x B Invariants: .i (1) v=~x Bk- +~dix Bk\u00ad i=l v v m: (2) ~ =~ xBk-n (3) ~ =~ xBk-n  e>O e<O f = \n(y-l f # w- e=minexporf #&#38; -* e > min exp and f = &#38;-l 1 fxbex2 fxbe+lx2 fx2 fxbx2 b-e+l s \n2 bx2 b-e X2 x2 m+be be+l 1 b m-bebe 1 1 Table 1: Initial values of r, s, m+, and m- Stop at the smallest \nn for which or larger than the true logarithm, a small constant (chosen to be slightly greater than the \nlargest possible error) is sub\u00ad (1) Tn< m;, or tracted from the floating-point logarithm so that the \nceiling (2) ~.+ m; < s. of the result will be either k or k -1. Consequently, the estimate must be checked \nand adjusted by one if necessary. If condition (1) is true and condition (2) is false, return Figure \n2 shows Scheme code that finds k and scales the O.dl . ..dmx B~. numbers using just a few high-precision \ninteger operations. The new scale procedure takes an additional argument, v. If condition (2) is true \nand condition (1) is false, return The code uses a table to look up the value of 10k for O < k < O.dl \n. . .dn.-l[d~+l] X Bk. 325, which is sufficient to handle all IEEE double-pre~kion If both conditions \nare true, return the number that floating-point numbers. It also uses a table to look up the is closer \nto v, using some strategy to break ties. If value of * for 2~ B ~ 36in order to speed up the T~ X 2< \ns~, O.dl ...d~ x Bk is closer. If rn x 2> Sn, computation of logB v. O.dl . . . d~-l[d~+l] x Bk is closer. \n If the cost of the floating-point logarithm function is fairly high, it maybe more efficient to compute \na less accu-The invariants, which can be verified by a straightfor\u00adrate approximation to the logarithm. \nBecause in almost all ward proof by induction, are useful in establishing the equiv\u00adfloating-point representations \nthe input base, b, is two (or a alence of this algorithm with the basic algorithm proved cor\u00adpower of \ntwo), we assume that b = 2 for our discussion of rect in Section 2.3. logarithm estimators. We also assume \nthat B >2, because If the input routine s rounding algorithm is known, V there is no reason to use a \nconversion algorithm if the output may be allowed to equal low or high or both. If low would base is \nthe same as the input base. round up to v when input, termination condition (1) would Since v=fx2 , logzv=logzfi-e.Ifwecomputebe \nT. ~ m;. If high would round down to v when input, the integer s and floating-point number x such that \nv = termination condition (2) would be T. + m$ ~ s., and k zx2sand 1s x<2,wegetlog2v=logzz+s,where would \nbe the smallest integer such that ~ < Ilk (i. e., O s logz x < 1. In other words, s is the integer part \nof the baae-2 logarithm of v. Let len(f ) be the length of f in k=l+ logB+ ). \\ J bits. Then s = e /en(f)+ \n1. For normalized floating-point For IEEE unbiased rounding, if the mantissa, f, is even, numbers, we \nhave s = e -p +1. then both low and high would round to v; otherwise, nei- In order to estimate logB \nv = ~, we use ~. This ther low nor high would round to v. For example, 1023 estimate never overshoots \nlogB v, and it undershoots by no falls exactly between two IEEE floating-point numbers, the more than \n~ <0.631. Once again, floating-point arith\u00ad smaller of which has an even mantissa; thus, 1023 rounds \nto the smaller when input. By accommodating unbiased metic does not compute the exact value of ~, so \nwe sub\u00ad rounding, the algorithm prints this number as le23 instead tract a small constant in order to \npreserve the property that of 9. 999999999999999 e22. the estimate never overshoots. Assuming the estimate \nis Figure 1 gives Scheme code for the algorithm. It uses an computed using IEEE double-precision floating-point \narith\u00aditerative algorithm (scale) similar to the one presented in [5] metic, ~ can be represented with \nan error of less than to find k. It assumes the input routine uses IEEE unbiased 10-14. Sinces is between \n1074 and 1023, the floating-point rounding. In the case of a tie in determining d., it always result \nofs X &#38; haa an error of less than 10-1O. Because rounds up by choosing d~ + 1. The function flonumd \ndigits our estimate never overshoots k and the error is less than returns a pair whose first element \nis k and whose second one, iskork -1. This result also holds if k is element is the list of digits. \nlog: B [1 1+ logB* . 3.2 Efficient Scaling 1 J Whereas the floating-point logarithm estimate was al-Steele \nand White s iterative algorithm requires 0(1 log vI) most always k, our simpler estimate is frequently \nk 1. high-precision integer operations to compute k, TO, so, m$, Having the estimate off by one introduces \nextra overhead, and m;. An obvious alternative is to use the floating-point but this overhead can be \neliminated. When the estimate is logarithm function to approximate k with DogB v] and then k -1, jizup \nmultiplies s by B and then calls generate to use an efficient algorithm to compute the ap ropriate power \ngenerate the digits. on entry to generate, T, m+, and m\u00ad -1 of B by which to multiply either s or r, \nm , and m-. Be-are multiplied by B. By moving these multiplications back cause the floating-point logarithm \nmay be slightly smaller into the call sites of generate, the multiplications can be (define flonumddigits \n(lambda (v f e rein-e p b l?) (let ([round? (men? f)]) (if (>= eO) (if (not (= f (e$pt b (-p l)))) (let \n([be (ezpt b e)]) (scale (*j be 2) 2 be be O B round? round?)) (let* ([be (ezpt b e)] [bel (* be b)]) \n(scale (*f bel 2) (* b 2) bel be O 1? round? round?))) (if (or (= e nzin-e) (not (= f (apt b (-p l))))) \n(scale (*~ 2) (* (ezpt b (-e)) 2) 110 B round? round?) (scale (x ~ b 2) (* (ezpt b (-1 e)) 2) b 10 B \nround? round?)))))) (define scale (lambda (r s rn+ m-k B low-ok? high-ok?) (cond [((if high-ok? >= >) \n(+ r m+) s) ; k is too low (scale r (* s B) m+ m-(+ k 1) B low-ok? high-ok?)] [((if hi9h-ok? < <=) * \n(+ r m+) B) s) ; k is too high $ (scale (* r B) s (*m ~) (* m-B) ( k 1) B low-oh? high-ok?)] [else ; \nk is correct (cons k (generate r s m+ m-B low-ok? high-ok?))]))) (define generate (lambda (r s m+ m-B \nlow-ok? high-ok?) (let ([g-r (quotient-remainder (*r B) s)] [m+ (*m+ B)] [m-(*m-B)]) (let ([d (car q-r)] \n[T-(cdr q-r)]) (let ([tcl((if low-ok? <= <) r m-)] [tc2 ((if high-ok? >=>) (+ r m+) s)]) (if (not tcl) \n(if (not tc2) (cons d (generate r s m+ m-B low-ok? high-ok?)) (list (+ d l))) (if [no: $.2) (if [;s~;; \n2) s) (list (+ d l)))))))))) Figure 1: Scheme code that implements the basic conversion algorithm with \nan iterative scaling procedure and IEEE unbiased rounding (round to even). For other rounding modes, \nscale and generate may be called with different values for tow-ok? and high-ok?. eliminated in jismp \nwhen the estimator returns k 1. The result is that there is no penalty for an estimate that is off by \none. Figure 3 gives a Scheme implementation of our es\u00adtimator and the modified digit-generation loop. \nIt modifies the original scale function to take additional arguments f and e, and it uses a table to \nlook up the value of ~ for 2sB~36. Table 2 gives the relative CPU times for Steele and White s iterative \nscaling algorithm [5] and the floating-point logarithm scaling algorithm with respect to our simple es\u00adtimate \nand scaling algorithm. The timings were performed using Chez Scheme on a DEC AXP 8420 running Digital \nUNIX V3.2C. The input was a set of 250,680 positive nor\u00admalized IEEE double-precision floating-point \nnumbers, and the output base was ten. This set was generated accord\u00ading to the forms Schryer developed \nfor testing floating-point I Scalino Alaorithm I Relative CPU Time [ Table 2: Relative CPU times for \nthree different scaling al\u00adgorithms units [4]. As expected, the timings show that the iterative scaling \nalgorithm is almost two orders of magnitude slower than either estimate-based algorithm, (define scale \n(lambda (r s m+ m-k B low-ok? high-ok? v) (let ([est (inezact~ezact (ceiling (-(logll B u) le-10)))]) \n (if (>= est O) (@p r (* s (ezptt B e$t)) m+ m-est B low-ok? high-ok?) (let ([scale (ezptt B (-est))]) \n(jisup (* r scale) s (* m+ scale) (* m-scale) est B low-ok? high-ok?)))))) (define jixup (lambda (r s \nm+ m-k B low-ok? high-ok?)  (if ((if high-ok? >= >) (+ r m+) s) ; too low? (cons (+ k 1) (generate r \n(* s B) m+ m-B low-ok? high-ok?)) (cons k (generate r s m+ m-B low-ok? high-ok?))))) (define esptt (let \n([tatde (make-vector 326)]) (do ([kO(+ k1)][v1(* v10)]) ((= k 326)) (veci!or-setf table k w)) (lambda \n(B k) (if (and (= B 10) (<= O k 325)) (uector-ref table k) (ezpt B k))))) (define logB (let ([table \n(make-vector 37)]) (do ([B 2(+ B l)]) ((= B 37)) (vector-set! table B (/ (log B)))) (lambda (B z) (if \n(<= 2B 36) (* (log z) (vector.ref table B)) (/ (~09 Z) (log B))))))  Figure 2: Scheme code that uses \nthe floating-point logarithm function to estimate k and then adjusts the result to the exact value of \nk Fixed-Format Output Up to this point we have addressed the free-format output problem. We now describe \nhow to modify the basic algo\u00adrithm to generate fixed-format output, A key property of the output conversion \nalgorithm is its use of the rounding range of v, determined by computing v+ and v-. For fixed\u00adformat \noutput, this range is conditionally modified to indi\u00adcate the requested precision. If a floating-point \nnumber has enough precision to be printed to the given digit position, the rounding range is expanded \nso that the output will stop at the given position. If a floating-point number has insuffi\u00adcient precision, \nthe rounding range is not expanded, and the output will contain tI marks past the last significant digit, \nThere are two ways of specifying how many digits to print in fixed-format mode: by absolute digit position \nand by relative digit position. An absolute digit position is the distance from the radix point in baae-B \ndigits at which one wants the output to stop. A relative digit position is the number of base-B digits \nto print. Suppose an absolute digit position is given. Let j be the digit position and v be a positive \nfloating-point number. In order for the output, V, to be correctly rounded, v \u00ad @<v <v++. Because of \nthe gaps in the floating\u00adp;int representation, all numbers between ~ and ~ are indistinguishable from \nv. The algorithm uses the larger range in order to determine when to stop generating digits. In other \nwords, let low be the lesser of ~ and v ~, and let high be the greater of ~ and v + ~. After low and \nhigh are computed, the scaling factor k is determined as before. If the,end point high is in the rounding \nrange (i. e., if high = v + ~), k is the smallest integer such that high < Bk; i.e., k = 1+ llog~ highJ. \nOtherwise, k is the smallest integer such that high s Bk; i.e., k = flog~ highl. The digits are generated \nas before. Termination condi\u00ad tion (1) is extended to include equality when the end point low is in the \nrounding range. Similarly, termination condi\u00ad tion (2) is extended to include equality when the end point \nhigh is in the rounding range. Let n be the smallest integer for which one of the ter\u00admination conditions \nholds. As before, digit d~ is incre\u00admented when O.dl . . . dn-l[d~+l] x Bk is closer to v than O.dl ,.. \nd~ xBk (or possibly in the case of a tie). If j = k-n, the algorithm stopped at the desired digit position, \nso the algorithm simply returns the result. Because of the way we defined the termination conditions, \nthe algorithm cannot generate too many digits. Therefore, if j # k -n, j < k -n, so the algorithm must \ngenerate the remaining digits. Unfortunately the algorithm cannot simply print fl marks 113  (define \nscale (lambda (r s m+ m-k B low-ok? high-ok? j e) (let ([A (inesxzct-ezact (ceiling (-(*(+ e (k j) -1) \n(imJlo920f l?)) le-10)))]) (if (>= est O) (jimq r (* s (ezptt B est)) m+ m-est B low-ok? high-ok?) (let \n([scale (ezptt B (-e.st))]) (fiimp (* r scale) s (*m+ scale) (*m-scale) est B low-ok? high-ok?)))))) \n(define jixup (lambda (r s m+ m-k B low-ok? high-ok?) (if ((if high-ok? >= >) (+ r m+) s) ; too low? \n(cons (+ k 1) (generate r s m+ m-B low-ok? high-ok?)) (cons k (generate (* r B) s (* m+ 1?) (*m-B) B \ntow-ok? high-ok?))))) (define generate (lambda (r s m+ m-B low-ok? high-ok?) (let ([q-r (quotient-remainder \nr s)]) (let ([d (car q-r)] [r (Cdr q-? )]) (let ([tcl ((if low-ok? <=<) r m-)] [tc2 ((if high-ok? >=>) \n(-I-T m+) s)]) (if (not tcl ) (if (not tc2) (cons d (generate (* r 1?) s (*m+ B) (*m-B) B low-ok? high-ok?)) \n(list (+ d l))) (if ~::j &#38;) (if (< (* r2)s) (list d) (M (+ d l)))))))))) (define invlog20f (let \n([table (make-vector 37)] [log2 (log 2)]) (do ([B 2(+ B l)]) ((= B 37)) (vector-set! table B (/ log2 \n(log B)))) (lambda (B) (if (<= 2B 36) (vector-ref table B) (/ log2 (log B))))))  Figure 3: Scheme code \nthat uses our fast estimator and modified digit-generation loop from here until position j. Suppose 100 \nwere printed to absolute position O, for example. Termination condition (1) would hold after generating \nthe first digit, but the remaining digit positions are significant and must therefore be zero, not #. \nConsequently, the algorithm must generate zeroes as long as they are significant and then generate # \nmarks. A digit is insignificant when it and all the digits after it can be replaced by any base-B digits \nwithout altering the value of the number when input. In other words, a digit is insignificant if incrementing \nthe preceding digit does not cause the number to fall outside the rounding range of v. If low = v ~ and \nhigh = v+ ~, the remaining digit positions are all significant, so the algorithm fills them with zeroes \nand returns. Otherwise, the precision of the output is limited by the floating-point representation. \nThe algorithm generates zeroes until incrementing the preced\u00ading digit would result in a number less \nthan or equal to high, at which point it fills the remaining digit positions with # marks. For example, \nwhen printing 100 in IEEE double-precision to digit position 20, the algorithm prints 100. ooooooooooooooo# \n####. Now suppose a relative position is given instead. Let i >0 be the number of digits requested. In \norder to compute the correspondkg absolute digit position, j, the algorithm first computes the absolute \nposition of the first digit. Unfor\u00adtunately, the position of the first digit, k-1, may depend on the \nupper bound of the rounding range of v, which in turn may depend on j. This cycle is resolved by using \nan initial estimate for k that does not depend on j and then refining it when necessary. The initial \nestimate, ~, is logB ~ I1 which can be computed efficiently using the techniques d&#38; &#38;. scribed \nin Section 3.2. If v+ < Bk, the initial estimate 2 was correct, so k = k; otherwise, the initial estimate \nwas off by one, so k = k + 1. At this point the algorithm proceeds 114 Incowect 242 HP 9000 1.61 2.19 \n317 Linux 1.63 0.58 0 RS/6000 1.75 4.46 0 SGI 32 1.61 5.69 186 SGI 64 1,81 3.12 6280 Solaris 1.59 0.68 \n0 Sun4c 1.66 0.54 0 Sun4d 1.62 0.38 0  ~ Geom. mean I 1.66 I 1.51 I N/A Key: Aipha AXP DEC AXP 8420, \nDigital UNIX V3.2C HP 9000 HP 9000/715/E, HP-UX A.09.05 Linux-AMD 80486DX2/80, Linux 1.3.32 RS/6000-lBM \nRS/6000 7013/560, AIX 3.2 SGI 32 SGI IP22, IRIX 5.3 SGI 64 SGI IP21, IRIX64 6.1 Solaris Sun SPARCstation \n2, SunOS 5.5 (Solaris) Sun4c Sun SPARCstation 2, SunOS 4.1.3 Sun4d Sun SPARCstation 5, SunOS 4.1.3 Table \n3: Ratio of CPU time for free-format versus straight\u00adforward fixed-format, fixed-format versus printf, \nand the count of incorrectly rounded output from printf on 250,6$0 floating-point numbers az though it \nwere given the absolute digit position k -i. The rational arithmetic used in fixed-format printing can \nbe converted into high-precision integer arithmetic by intro\u00adducing a common denominator as before. Because \nthere are several more cases to consider, however, the resulting code is lengthy and has therefore been \nomitted from this paper. Conclusion We have developed an efficient algorithm for converting floating-point \nnumbers from an input base to an output base. For free-format output, it provably generates the shortest, \ncorrectly rounded number that rounds to the orig\u00adinal floating-point number when input, taking the input \nrounding algorithm into account if desired. For iixed-format output, it generates a correctly rounded \nnumber with # marks in the place of insignificant trailing digits. These # marks are useful when the \nrequested number of digits may exceed the internal precision. Our algorithm employs a fast estimator \nto compute scaling factors. By modifying our aJ\u00adgorithm slightly, we eliminated the penalty of having \nthe estimate off by one, which enabled us to make our estimator very inexpensive. We have compared an \nimplementation of our free-format algorithm for baae-10 output against an implementation of a straightforward \nfixed-format algorithm on several differ\u00ad ent systems. For this test, we used a set of 250,680 posi\u00ad \ntive normalized IEEE double-precision floating-point num\u00ad bers [4]. The fixed-format algorithm printed \nthem to 17 significant digits, the minimum number guaranteed to dis\u00ad tinguish among IEEE double-precision \nnumbers. In all cases the numbers were printed to /dev/null in order to factor out 1/0 performance. The \naverage number of digits needed is 15.2, so the free-format algorithm has no particular ad\u00advantage over \nthe tixed-format algorithm. Table 3 shows that our free-format algorithm takes 66% more CPU time on average \nthan the straightforward fixed\u00adformat algorithm. To provide a basis of comparison against a standard \nthed-format algorithm for each system, the ta\u00adble also compares the C library s printf function against \nthe straightforward tixed-format algorithm and gives the num\u00adber of floating-point numbers that were \nrounded incorrectly by printf. For the systems where printf is considerably faster, we suspect that our \nimplementation could be tuned to achieve comparable results. (In particular, our current implementation \nuses 64-bit arithmetic and performs poorly on systems without efficient 64-bit support.) While the cost \nof free-format output may be significant for some applica\u00adtions, the cost is justified for many others \nby the reduced verbosity of free-format output. Our algorithm is based on Steele and White s conversion \nalgorithm [5]. Ours is dramatically more efficient, primar\u00adily due to our use of a fast estimator for \ncomputing scaling factors. Their algorithm does not distinguish between signif\u00adicant and insignificant \ntrailing zeros, nor does it take into ac\u00adcount input rounding modes. In addition, their fixed-format \nalgorithm introduced a slight inaccuracy in the computation of the rounding range. David Gay independently \ndeveloped an estimator similar to ours [2], It uses the first-degree Taylor series to estimate loglo \nv. Although our estimator is less accurate than his, it is less expensive aa well, requiring two rather \nthan five floating-point operations. Furthermore, since our scaling algorithm incurs no additional overhead \nwhen the estimate is off by one, the loss of accuracy is unimportant, and scaling is more efficient in \nall cases. Gay also developed an excellent set of heuristics for de\u00adtermining when more efficient digit-generation \ntechniques can be employed for fixed-format output. In particular, he showed that floating-point arithmetic \nis sufficiently accurate in most cases when the requested number of digits is small. The fixed-format \nprinting algorithm described in thk paper is useful when these heuristics fail. An implementation of \nthe algorithms described in this paper is available from the authors. A version of the free\u00adformat algorithm \nhaa been used in Chez Scheme since 1990; in fact, the ANSI/IEEE Scheme standard requirement for accurate, \nminimal-length numeric output and the desire to do so as efficiently as possible in Chez Scheme motivated \nthe work reported here. References [1] William D. Clinger. How to read floating-point numbers accurately. \nACM SIGPLAN 90 Conference on Program\u00adming Language Design and Implementation, 25(6):92\u00ad101, June 1990. \n[2] David M. Gay. Correctly rounded binary-decimal and decimal-binary conversions. Numerical Analysis \nManuscript 90-10, AT&#38;T Bell Laboratories, Murray Hill, New Jersey 07974, November 1990. [3] IEEE \nstandard for binary floating-point arithmetic. ANSI/IEEE Std 754-1985, Institute of Electrical and Electronics \nEngineers, New York, 1985. 115  [4] N. L. Schryer. A test of a computer s floating-point arith\u00admetic \nunit. In W. Cowell, editor, Sources and Develop\u00adment of Mathematical Software. Prentice-Hall, 1981. [5] \nGuy L. Steele Jr. and Jon L. White. How to print floating-point numbers accurately. ACM SIGPLAN 90 Conference \non Programming Language Design and Im\u00adplementation, 25(6):112 126, June 1990. A Proofs of Correctness \nThis section presents correctness proofs of the free-format printing algorithm described in Section 2.2. \nSee Section 2.3 for a less formal presentation. Theorem 1: Each d; is a valid baae-B digit, dl >0, and \nif dn is incremented, no carry is generated. Proof: O< gO = * < 1since O< v< high $ Bk. For i ~ 1, 0 \n~ qi < 1by definition, Thus for all z~ O, 0 ~ qj x B < B, so di+l = Lqi x B] is a valid base-B digit. \nSuppose dl = O. Then O.[dl+l] x Bk = Bk-l < high by the minimality of k, so termination condition (2) \nholds. Termination condition (1) cannot hold since O.dl x Bk = O. Thus digit dl will be incremented to \n1. Suppose dl = B 1. Then O.[dl+l] x 13k = Bk ~ high, so termination condition (2) will not hold, and \ndigit dl will not be incremented. Assume by way of contradiction that final digit dn (n > 1) is incremented \nto B, which would introduce a carry. Then O.dl . . . dn-l[dn+l] X Bk = O.dl . . .dw-.z[dn-l+l] x Bk < \nhigh, so termination condition (2) would have held at step . ., n 1, a contradiction. Ii n Lemma 2: \nv= qn x Bk-n + ~di X Bk-i i=l Proof: By induction on n. Basis: v = go x Bk by definition of qo. Induction: \nSuppose the result holds for n. v= q. x Bh + ~di XBk-i i= 1 n ~{ x @k-i = (q. x B) X Bk-f + ) + x ,=1 \n= (h x B] + {q. XB}) X Bk-( +k) + n di x Bk-i E i=l n k (n+l) + ~di &#38;-~ x = (dn+l + @L+l) x B n+l \n= qn+l x &#38;-(n+l) + di X z i=l 1 Corollary: The following conditions are equivalent to the termination \nconditions: (1) q. XBk- < V-tOW (2) (1 qn) x Bk-n < high V Theorem 3: (Information Preservation) The \nalgorithm al\u00adways terminates with low < V < high. Proofi By Lemma 2, termination condition (1) is equivalent \nto q. x Bk- < v low. Since O ~. q~ < 1, the left-hand side becomes arbitrarily small as n increases, \nso there will be some n for which the a3gorithm terminates. Suppose the algorithm stops at point n. There \nare two .. cases to consmer: 1. V = O.dl . ..dnx Bk> low By Lemma 2, V = v qn x Bk-n. Since O~ q. <1, \nlow <V~v<high. 2. V = O.dl . . .dn-l[d~+l] x Bk < high  By Lemma2, V = v+(l-qn)xBk-n. Since O s q. \n<1, !OW< V< V < high. In both cases low < V < high. 1 Bk-n Theorem 4: (Correct Rounding) IV VI < \n~ Proofi There are two cases to consider: 1. V= O.dl... d~x Bk Since dn was not incremented, O.dl... \ndn-1 [d~+l] x Bk was no closer to v than V. Thus O.dl . . . dn-l[d.+1] X Bk -v=(v+Bk- )-v~ Bk-n v -V, \nwhich is equivalent to v V s ~. 2. V= O.dl . . . dn-l[dn+l] x Bk Since dn was incremented, O.dl . . \n. d~ x Bk was no closer tovthan V. Thus v-O.dl. ..dnx Bk=v -(V Bk-n Bk-n) ~ V -v, which is equivalent \nto V -v ~ ~. Dk-fI In both cases IV VI< ~. 0 Theorem 5: (Minimum-Length Output) There is no (n 1)-digit \nbase-B number V such that low < V < high. Proofi Assume by way of contradiction that V ex\u00adists. By Lemma \n2, v -O.dl . ..dm_l X Bk = qn_l X Bk-(?l-1) , SO V O.dl...dl-l X Bk < &#38; and O.dl . ..dn_z[d~-l+l]x \nBk v < ~. Thus O.dl . ..dn_lx Bk and O.dl . . . dn_z [d~-l+l XBk are the two closest (n l)- J digit \nbase-B numbers to v. Consequently, there are two cases to consider: 1. V = O.dl ...dn_l X Bk Since the \nalgorithm dld not stop at point n -1, V = O.dl . . . d~-1 $ low, a contradiction. 2. V =O.dl . . . dn-z[d~.-l+l] \nX Bk  Since the algorithm did not stop at point n -1, V = O.dl . . . dn-z[dn-l-i-l] x Bk ~ high, a contradiction. \n0 2Note that if incrementing the last digit introduces a carry, the resulting number may extend to the \nleft by one digit. This does not cause a problem, however, since the last digit would he O and can be \neliminated. 116  \n\t\t\t", "proc_id": "231379", "abstract": "This paper presents a fast and accurate algorithm for printing floating-point numbers in both free- and fixed-format modes. In free-format mode, the algorithm generates the shortest, correctly rounded output string that converts to the same number when read back in, accommodating whatever rounding mode the reader uses. In fixed-format mode, the algorithm generates a correctly rounded output string using special # marks to denote insignificant trailing digits. For both modes, the algorithm employs a fast estimator to scale floating-point numbers efficiently.", "authors": [{"name": "Robert G. Burger", "author_profile_id": "81100576913", "affiliation": "Indiana University Computer Science Department, Lindley Hall 215, Bloomington, Indiana", "person_id": "PP14199444", "email_address": "", "orcid_id": ""}, {"name": "R. Kent Dybvig", "author_profile_id": "81100181541", "affiliation": "Indiana University Computer Science Department, Lindley Hall 215, Bloomington, Indiana", "person_id": "PP14073331", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/231379.231397", "year": "1996", "article_id": "231397", "conference": "PLDI", "title": "Printing floating-point numbers quickly and accurately", "url": "http://dl.acm.org/citation.cfm?id=231397"}