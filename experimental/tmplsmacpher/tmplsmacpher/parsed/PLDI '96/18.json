{"article_publication_date": "05-01-1996", "fulltext": "\n Flow-directed Inlining Suresh Jagannathan and Andrew Wright NEC Research Institute 4 Independence Way \nPrinceton, NJ 08540 {suresh,wright} @research. nj.nec.com Abstract procedure body. Second, by merging \nthe procedure body with its calling cent ext, inlining enables other optimizations to specialize the \ncaller and callee together. A flow-directed inlining strategy uses information derived from control-flow \nanalysis to specialize and inline procedures The same mechanisms that make functional and object\u00ad for \nfunctional and object-oriented languages. Since it uses oriented languages more expressive conspire to \nmake inlining control-flow analysis to identify candidate call sites, flow-more difficult. In functional \nlanguages, the called function directed inlining can inline procedures whose relationships may be computed \nby a complicated expression that returns totheir call sites are not apparent. For instance, procedures \na procedure value. In object-oriented languages, the value defined in other modules, passed as arguments, \nreturned as of a virtual method is based on the types of its argument values, or extracted from data \nstructures can all be inlined. objects. Thus the procedure or method applied at a partic\u00ad Flow-directed \ninlining specializes procedures for particular ular call site may not be obvious. An inlining algorithm \nfor call sites, and can selectively inline a particular procedure these kinds of languages is unlikely \nto be very effective if it at some call sites but not at others. Finally, flow-directed does not take \na program s control-and data-flow properties inlining encourages modular implementations: control-flow \ninto consideration. analysis, inlining, and post-inlining optimizations are all or\u00ad Flow-directed inlining \nuses an approximation of a program s thogonal components. Results from aprototype implemen\u00adcontrol-and \ndata-flow to identify call sites where procedures tation indicate that this strategy effectively reduces \nproce\u00admay be inlined, and to estimate a procedure s size when dure call overhead and leads to significant \nreduction in exe\u00adspecialized for a particular call site. Flow-directed inlining cution time. has several \nimportant attributes: 1 Introduction Generality. All user-defined procedures are candidates for inlining. \nInlining is not limited to system-defined primitives, global procedures, specially marked proce- Functional \nlanguages like Scheme [7] or ML [17], and object\u00addures, or procedures that have no free variables. oriented \nlanguages like Self [5] or Java [24], provide abstrac\u00adtion through the use of first-class procedures \nand objects. Since inlining decisions are driven by a global control-These mechanisms encourage modular \nprogramming and en-flow analysis, inlining can be performed at call sites able elegant programming paradigms. \nHowever, expressivity where the called procedure is not syntactically obvi\u00adof this kind haa often come \nat the price of poor performance. ous. For higher-order languages, our system is capa-A significant factor \ncontributing to this performance loss is ble of inlining procedures passed as arguments, im\u00adthe overhead \nof pervasive procedure calls and method dis-ported from modules, extracted from data structures, patches. \nIn the absence of any compile-time optimizations, or encapsulated as methods within objects. For object\u00ad \nthese overheads can be significant, especially for programs oriented languages, flow-directed inlining \ncan optimize that make liberal use of data and control abstractions. calls to virtual methods. Thus, \nflow-directed inlining minimizes the penalty for liberally using higher-orderInlining is an optimization \nthat trades code space for time procedures and objects to define layers of abstraction by replacing a \nprocedure call with the called procedure s and encapsulation.body. Inlining has two important benefits. \nFirst, it elimi\u00ad nates procedure call overhead. This overhead includes the Selectivity. Inlining decisions \nare highly selective. A cost of passing arguments, saving and restoring registers, particular procedure \nmay be inlined at some call sites, building return linkage information, and branching to the but not \nat others. The decision to inline at a call site is baaed on an estimate of the procedure s size when \n specialized for that call site. Flow analysis facilitates PWmiSSiOtlto make digitalhard oopy of part \nor all of this work for personal constructing different size estimates for a procedure at or oksroom \nuse is ranted without fee provided that copies are not made different call sites based on the portions \nof the proce\u00ad i or distributed br pro t or oommerciai advanta e, the copyright nothx, the title of the \nubliitkm and its date appear, an i notice is given that dure that may be reached from those call sites. \nFor cowk w is ! Y Dermiaaionof ACM. km. TO COPYMwwi-, to fapubfieh, b example, large procedures can be \naggressively inlined poet on savers, or to redistribute to Iisk, requires prior specific permission andktr \na fee. PIJX 96 5/96 PA, USA @ 1996 ACM 0-S9791-7954YWO005...$505O 193 when determination of conditional \ntests in their bodies permits entire blocks of code to be pruned. Modularity. The structure of the flow \nanalysis is in\u00addependent of the design of the inlining algorithm. In\u00adlining decisions are in turn made \nindependently of any subsequent optimizations. Thus, different flow analy\u00adses can be substituted without \nnecessitating changes to the inlining algorithm, and different inlining algo\u00adrithms can be incorporated \nwithout modifying the flow analysis. This orthogonality makes it simple to up\u00adgrade a compiler that uses \nour strategy to include more flow powerful analysis techniques as they become avail\u00adable, or to incorporate \nnew inlining strategies without modifying existing optimizations or analyses. We have implemented a \nsource-to-source inlining optimizer for RARS Scheme programs [7]. Initial experimental results from our \nprototype are very encouraging. For the programs in our benchmark suite, flow-directed inlining improves \nex\u00adecution times by an average of 25~0. For several of these benchmarks, performance doubles. On average, \ncode sizes remain roughly the same as the original program. Based on these results, we believe flow-directed \ninlining is an effective optimization technique for functional and object-oriented languages. The remainder \nof the paper is organized as follows. Sec\u00adtion 2 gives an informal overview of flow-directed inlining. \nSection 3 gives a formal description of the algorithm. We present performance results in Section 4, and \nplace flow\u00addirected inlining in the context of related work in Section 5. 2 Overview Flow-directed inlining \nconsists of three steps: control-flow analysis, inlining, and local optimization. 2.1 Flow Analysis We \nfirst perform a polyvariant [4, 16] control-flow analysis over the input program. Polyvariance is important \nfor two reasons. First, because it disambiguates different uses of a procedure at different call sites, \npolyvariance yields very precise flow information. This enables many call sites to be identified as candidates \nfor inlining. Second, because it analyses a procedure with respect to a specific call site, polyvariance \nenables specialization of the procedure for the call sites where it is inlined. Inlining can be performed \nat a call site only if there is a unique procedure applied at that site. For example, in (define g (J \n(f X) (f x))) the call site (f x) is considered a candidate for inlining only if flow analysis determines \nthat there is a single abstract value associated with f in the call. This value must be an abstract closure. \nAn abstract closure corresponds to a set of exact closures. These exact closures may be closed over different \nenvironments, but they must all share the same code. In other words, for f to be inlined, all calls to \ng must supply closures that are created from evaluation of the same J-expression in the program. To illustrate, \nconsider the following object-oriented program fragment: (define make-network (A ( args) (i (msg) (case \nmsg ( (open) (~ (addr) open a new port)) ((close) (A (port) close a port)) ((send) (J (msg port) send \nmsg to port ) ) ((receive) (A (port) receive from port ) ) . ..)))) A network is a procedure that dispatches \non a request. A network is created by calling make-network with a set of arguments. Given a network N, \nthe expression ((N open) http://www.foo.com ) invokes a procedure to open a connection to the specified \nnetwork. The flow analysis will associate N with the proce\u00ad dure (J (msg) (case msg . ..)) and (N open) \nwith the procedure (J (addr) open a new port). (*) This approach to identifying potential inline sites \nis quite different from traditional approaches to inlining that rely on syntactic heuristics. For example, \nin the expression (let ((f (A (N) ... ((N open) http://www.foo.com ) . ..))) ... (f (make-network m-gs)) \naconventional inlining optimizer will inline procedure (*) at ( . . . http://www,foo.com ) only if it \nalso irdines f, per\u00adforms significant simplification, andrepeats the inline algo\u00adrithm on the simplified \nprogram. In contrast, flow-directed inlining may treat the call (. . . http:/ /www. foo. corn !) as a \ncandidate for inlining even if f is not inlined. All inline decisions aremade prior to any simplification. \n 2.2 Inlining The inlining algorithm proceeds depth-first over the pro\u00adgram, inlining within a procedure \nP before inlining P itself. Closures at candidate call sites whose costs lie below a fixed threshold \nare inlined. Large procedures are not penalized by prohibiting inlining within their bodies. Even if \na pro\u00adcedure P is too big to be inlined at any of its call sites, procedures applied within P can still \nbe inlined within P s body provided the constraints described above are satisfied. The algorithm builds \na loop when inlining a call site would otherwise lead to infinite unfolding. 194 (define nap (A (f al \n. ergs) (letrec ([mapl (A (f 1) (if (null? 1) () (cons (f (earl)) (mapi f (cdr 1)))))1 [map* (~ (lists) \n(if (null? (car lists)) () (cons (apply f (map car lists)) (map* (map cdrlists)))))l) (if (null? ergs) \n(napl f al) (map* (cons al args)))))) Figure 1: Scheme s map procedure ((J (fal . args ) (letrec ([mapl \n(A (f 1) ...)1 [map* (A (lists) . . . )1) (letrec ([mapl (A (f 1) (if (null? 1) () (cons (car (car 1)) \n(mapl f (curl)))))]) (mapl f al)))) car m) Figure 2: Result of inlining (map car m) prior to simplification. \n(letrec ([mapl (A (1) (if (null? 1) () (cons (car (car 1)) (mapl (cdr 1)))) )1) (mapl m)) Figure 3: \nResult of inlining (map car m) after simplification. To illustrate the inlining algorithm, consider Fig. \n1 which provides animplementationofScheme s mapprocedure. Two local procedures are defined inmap s body: \nrnapl is called when map is invoked with a unary procedure, and map* is invoked for all other cases. \nBecause mapl operates over pro\u00adcedures whose arity is known, it is more efficient than map* whose implementation \nuses an expensive apply operator to handle the variable-arity case. Flow analysis helps compute reasonably \nprecise cost esti\u00admates. Consider the call (map car m) . The cost of inlining map at this call siteis \ndependent on how much specialization can be performed at the inlined site. Because flow-directed inlining \nuses results from a polyvariant flow analysis, it can tailor its cost estimate based on the specific \ncontext in which a call occurs. In this call, flow analysis reveals that the conditional test (null? \nargs) will be true, and hence the else-branch will not be taken. Thus, the cost estimate for this call \nneed not include the else branch or the definition of map*. If map is inlined, the outer conditional \ntest, the defi\u00adnition of map*, and its call will all be pruned inthe inlined copy. Fig. 2 shows the result \nof inlining (map car m) prior to simplification.  2.3 Simplification After inlining, &#38;substitution, \ndead-code elimination, and local code transformations are performed. These transfor\u00admationsinclude restructuring \nprocedure definitions andcalls to eliminate unused formal parameters. These optimizations are all syntactic \nand use no flow information. The simpli\u00adfier performs no transformations that violate the results of \nthe flow analysis. Hence, other optimizations could use flow information generated for the original program \nwhen oper\u00adating over the inlined version. Fig. 3 shows the result of applying the simplifier tothe inlined \ncode for (map car m). 195 3 Flow-directed inlining In this section we give a precise account of our \nflow-directed inlining strategy and the flow analysis that drives it. We have used this same analysis \nto eliminate run-time checks from dynamically typed programs [15]. 3.1 Source language The abstract syntax \nfor our Scheme-like source language has labeled expressions eL of the form e ::= C lf \\z (p e$... ek)f \n(call e$ e! ...e~)~ (begin e~ . . . e% ) (if e; e; e$)i (let ((ml e~)... (zn ey)) e$)~ (letrec ((YI $ \n)... (yn k )) 6$)1 (cons e! e$ ) (car e~)~ (cdr e~)~ (set-car! ef e~)i (set-cdr! e; e$)t f ::= (A (xl \n. ..ZW) ef)i where c c Const are constants, p 6 Prim are primitives, x, Y, z ~ Var are va~iables, and \n1 c Label are labels. La\u00adbels, which are not present in the concrete syntax, are dis\u00adcussed below. A \nA-term constructs an anonymous proce\u00addure; terms beginning with p are uses of primitive opera\u00adtions; \ncall introduces a procedure call; begin and if are sequencing and conditional operations; let provides \nnon\u00adrecursive local bindings; letrec constructs named recursive procedures; cons constructs mutable pairs; \ncar and cdr are the first and second projections on pairs, and set-car! and set -cdr ! mutate the components \nof pairs. Free variables (F V) and bound variables (EN ) are defined as usual for a lexically scoped \nlanguage [3], with let binding its variables XI . . . X. in eb and letrec binding its variables in all \nof ~1 . . . ~n and eb. The bound expressions jl . . . .f~ of a let re c-expression must be procedures. \nClosed programs are expressions with no free variables. We sometimes distinguish different uses of variables \nas fol\u00adlows. First, a meta-variable named y must be bound by a letrec-expression. Second, a subscript \non a variable occur\u00adrence indicates how the variable is bound. A variable occur\u00ad rence like XIA1 indicates \nthat x is bound by a ~-expression. A variable occurrence like x~l, ~ or y~l,~ indicates that the vari\u00adable \nis bound by the let-or letrec-expression with label 1 , except that recursive variable occurrences within \nthe bind\u00adings of a letrec-expression appear as yfrecl. The following expression illustrates the use of \nthese conventions: (let ((z O)) (letrec ((y (J (z) (call y[,.cl 21x1)))) (call YII,I ZP,I))12 )1 In order \nto unambiguously name different components of a program,weassume programs have two properties. First, \nsince we use labels to identify subterms of a program, each subterm of a program must have a unique label. \nSecond, since we refer to a specific variable by its name, we assume that all free and bound variables \nin a program are distinct. This condition can be met by renaming bound variables ap\u00adpropriately. Like \nScheme, this source language is latently typed no static typing discipline is imposed on programs. Like \nboth Scheme and ML, this source language has an ordinary call\u00adby-value semantics. 3.2 Flow analysis \nFlow analysis determines the sets of values that may be bound to variables and returned from subexpressions. \nWe name these sets abstract values; in other words, a single ab\u00adstract value represents a set of exact \nvalues. Simple mono\u00advariant flow analyses like OCFA [23] and SBA [13] associate a single abstract value \nwith each variable and subexpression. Polyvariant analyses [4] associate multiple abstract values with \neach variable and expression, distinguishing them by contours that abstract the program s run-time state. \nWe call these variable-contour pairs and label-contour pairs pro\u00adgram points. Let A value be some set \nof abstract values and Contour be some set of contours. A jlow analysis of a closed program P is a function \nF : ( Var-x Contour) + (Label x Contour) S Avalue that maps the program points of P to abstract values. \nF(z7 K) identifies the values that may be bound to x in contour K. F(Z, K) identifies the values that \nmay be returned by the subexpression labeled 1 in context K. Abstract values are defined as follows: \na c Avalue = Aconst + Aclosure + Apair T c Aconst = {true, false, nil, number,. ..} (1, p, rG)~ G Aclosure \n= Label x Aenv x Contour (l, fc)r 6 Apair = Label x Contour\u00ad p 6 Aenv = Var 5-Contour K E Contour An \nabstract value a is a set of abstract constants, abstract closures, and abstract pairs. Abstract constants \nlike true, false, and nil each denote a single exact value, while ab\u00adst ract constants like number denote \na set of exact values. An abstract closure (1, p, K) ~ identifies procedures created from the ~-expression \n(~ (Z I . . . w~ ) eb)~. The contour K of an abstract closure, paired with an argument x, or a label \nof a sub expression of e~, determines the program points for the body of the abstract closure. Thus two \nabstract closures that share the same label but use different contours will have different program points. \nThe abstract environment p of an abstract closure records the contours in which its free vari\u00adables are \nbound. An abstract pair (1, K)* identifies pairs constructed from (cons el ez ) ~ in contour K. Contours \nabstract different execution contexts and are used to distinguish argument bindings and subexpression \nresults 196 arising at different points in a program s execution. The set of contours must be finite \nto ensure that the analysis com\u00adputes finite information, i.e., that the analysis terminates. Since the \nsubsets of variables and labels that a program uses are also finite, a flow analysis can yield only finitely \nmany abstract values for a particular straint, the choice of the contour the analysis. Before discussing \nsuitable for flow-directed inlining, proper. Given a program P and a finite program. Within this con\u00adset \ngoverns the precision of a particular set of contours we introduce the analysis function F of appropriate \ntype, F is a flow analysis or ftow graph for P if it satisfies the relation A in Fig. 4. The rules for \nconstants, primi\u00adtive applications, and A-expressions force the corresponding program points or flow \ngraph nodes to include appropri\u00adate values. The function Abstract ValOf maps a constant to its abstract \nvalue; for example, Abstmct ValOf (#f ) = fa Ise and Abstract ValOf (5) = num her. The function AbstractRe\u00adsultOf \nmaps a primitive to its abstract result; for example, AbstractResultOf (+) = num her. The remaining rules \nintroduce constraints or edges between nodes. The rule for variables constrains the abstract eval\u00aduation \nof a variable z to include the abstract value bound to it in contour p(x). For each abstract closure \nthat can arise in the function position of a call, the application rule introduces two kinds of constraints. \nThe first kind, ~(lj, K) S F (xj, K ), requires the formal parameters of the abstract closure to include \nthe abstract values of the ac\u00adtual parameters. The second kind, ~(lb, K ) ~ F(i, K), re\u00adquires the application \nnode to include the abstract value arising from the evaluation of the body of the abstract clo\u00adsure. \nIn the figure, the notation p[m . . z~ I+ K] means the functional update or extension of the environment \np at each of q . . . zn to K. The constraint in the rule for begin returns the abstract value of the \nlast subexpression as the result of the begin expression. In the rule for conditional expressions, evaluation \nof the then-clause (resp. else-clause) is conditional upon true (resp. fa k.e) arising as a possible \nvalue of the test. If the abstract value of the test includes both true and fa Ise, both the then-clause \nand the else-clause contribute to the result of the conditional expression. If the test diverges and \nits abstract evaluation yields the empty set, neither the then-clause nor the else-clause is evaluated. \nNone of the rules in the top part of Fig. 4 construct new con\u00adtours. To provide polyvariance, the rules \nin the second sec\u00adtion of the figure construct new contours for let-bound clo\u00adsures in a manner that \nmimics the actions of a polymorphic type system. Hence we call this form of polyvariance poly\u00admorphic \nsplitting [15]. Under polymorphic splitting, con\u00adtours are finite strings of labels. The el subexpression \nof a let-expression evaluates in a contour K : 1 obtained by appending 1 to the label string K. This \ncontour is cap\u00adtured in any abstract closures created by el. Since only let-expressions extend contours, \nthe nesting depth of let\u00adexpressions within their el subexpressions bounds the length of contours. At \na subsequent use of the let-expression s variable, say z-f], the contours in abstract closures bound \nto z are modified by replacing 1 with 1 (the notation K[l\u00b0 /1] in the figure means the contour derived \nby replacing Z with 1 in K). In this manner, different uses of the same abstract closure evaluate in \ndifferent contours. The figure presents a rule for let-expressions with a single variable; the extension \nof this rule to multiple variables is straightforward, and is omitted here. To illustrate polymorphic \nsplitting, expression: (let ((f (J (x) (begin (f2 #t) (+ (f The abstract closure constructed 1 is the \nA-expression s label, 0 is and the [0] is contour consisting expression. At f 2, this closure is the \napplication (f 2 #t) binds x At f 3, f s closure is split to (1,0, tion (f 3 O) binds x to {num her} \nconsider the following x)1)) o) 1)))0 for f is (1,0, [0])~ where the empty environment, of the label \nof the let\u00adsplit to (1,0, [2])~, hence to {true} in contour [2]. [3])~, hence the applica\u00adin contour \n[3]. Since the two applications of f evaluate in different contours, the ap\u00adplication (f 3 O) yields \nthe abstract value {num her}. Were the two applications evaluated in the same contour, (f3 O) would yield \nthe less precise result {n um her, true}. For ad\u00additional explanation and examples of polymorphic splitting, \nwe refer the reader to an earlier paper [15]. The third part of Fig. 4 presents a polymorphic splitting \nrule for letrec-expressions. As in a typical polymorphic type system, only occurrences of variable y \nin the body eb of (letrec ((y f)) eh) are polyvariant. We cdl these non\u00adrscursive occurrences, while \nthe occurrences of y within f are recursive occurrences. Recursive uses of y evaluate in the most recent \ncontour of a non-recursive use of y. To illustrate, consider the following expression: (letrec ( (last \n(begin (lastz (last3 The abstract closure 1 is the J-expression s sure for last is split abstract evaluation \nlast yield the same (A (1) (if (cons (cons created label. to yield of (last abstract (null? (cdr 1)) \n(car 1) (last (cdr 1)))) 1)) 1 (cons 2 ()))) a (cons b ()))))) for last is (1, (b, []) ~ where At lastz, \nthe abstract clo\u00ad(1, [last I+ [2], [2])~. During 2 . . . ), recursive references to closure (1, [last \n* [2], [2])x. Similarly, during abstract evaluation of (last3 . ..). the ab\u00adstract closure for last is \nsplit to yield (1, [last I+ [3], [3])~, and recursive references to last yield the abstract closure (1, \n[last * [3], [3])>. In each case, the recursive calls evalu\u00adate in the same contour as the outermost \ncall. Hence last s argument 1 is bound in different contours for the entire eval\u00aduation of each recursive \napplication of last. The last part of Fig. 4 presents rules for construction, pro\u00adjection, and mutation \nof pairs. The rules merge values in the store only when the program s control flow makes this necessary. \nFor a given program, there are relation A. We want the least to prove that for any program and is unique. \nNor is it difficult that constructs this flow graph many flow graphs that satisfy such graph. It is not \ndifficult P the least flow graph exists to implement an algorithm [15].   197 AbstractValOf (c) E \nF(l, K) Abstractl?esvlt Of (p) E F(1, K) and for all i (t, p, m)> e F(1> K) F(z, p(z)) G F(l, K) for \nall i = O.. .n A[e\\]p~ and for all (1 , p , fi )~ c F(lO, K) where (~ (m F(lj, K) ~F(~jjK ) forallj=l... \nnand A[ef]p [zl . . . X. s K ]K and F(lE,, K ) ~ F(Z, K) for all i = 1.. .n d[e~]p~ and F (1~, K) ~ \nA[e~ ]PK and (true 6 F(ll, K) a d[e$]p~ and F(lz, K) (false 6 F(lI, K) * A[e$]pc and ~(Z3,K) for all \n(1 , K )Z G F (11, K) where (cons e~ F(12, K) s F(h, K ) Figure 4: Relation A defining flow analysis \nF of program = 1... n .4[e\\ ]pK ...x~) e$)~ c P F(Z, K) g F(~, K)) and g ~(1, K)) e~) GP P. 198 While \nour implementation of flow-directed inlining uses poly\u00ad morphic splitting, adapting our inlining strategy \nto other polyvariant analyses is straightforward. 3.3 Identifying inlining sites A polyvariant flow \nanalysis determines abstract values for the function position of each call site. If the function po\u00adsition \nfor a particular call site contains the same single ab\u00adstract closure in every contour, it is possible \nto specialize and inline the procedure at that call site. This leads to the following inlining condition. \nInlining Condition 1. If (call e$ el . . . eP) ~ is a call site such that u I (lo,l+ = 4,} {(%/% K ~ \nContour then (~ (z1 . . ,z~) eb)m can be specialized to contour K and inlined at call site 1, provided \nn = p. The proviso n = p ensures that we do not eliminate ap\u00adplications which raise an error due to an \nargument count mismatch.1 We describe specialization below. For now, let (~ (xl . . . z~ ) e!) be the \nspecialized procedure. If condi\u00adtion 1 holds, we can inline the specialized procedure by re\u00adplacing the \ncall at 1 with (call (A (w Z1.. .zn) ej) eO el... eP) where w is a fresh variable. Adding the extra \nargument w ensures that our transformation preserves termination and side effect behavior that may result \nfrom evaluating eo. Moreover, as we describe in Section 3.5, the extra argu\u00adment will also be used to \nensure proper access to the inlined procedure s free variables. Simple local transformations now suffice \nto eliminate the procedure call overhead. 3.4 Specializing inlined procedures Inliniru.z Condition 1 \ndetermines an initial set of call sites where inlining can be performed. For such a call site 1, we can \nspecialize the unique procedure m called at 1. Spe\u00adcialization involves (i) pruning code within m that \nis never reached from call site 1, and (ii) recursively inlining at call sites within m. Pruning unreachable \ncode is most important for conditional expressions. Let (m, p, K)~ be the unique closure called at 1. \nTo prune unreachable code at a conditional expression (if e! ez es) within m, we use contour K, to find \nan upper bound F(1I, K) on the set of values that the test expression el can vield when m is called from \n1. If F(1I, K) does not include-true, the consequent e; can be pruned. If F(1I, K) does not include false, \nthe alternative eq can be pruned. If F(1I, K) includes neither true nor false, both ez and es can be \npruned. If the arities do not match, we leave the application done. We could also issue a warning, raise \na compile-time error, or insert a call to an error handler. Pruning is also possible for most other expression \nforms. For example, assuming subexpressions are evaluated from left to right in an application, all subexpressions \nto the right of a divergent subexpression whose abstract value is empty can be pruned. To simplify the \npresentation, we have included pruning only for conditionals. Our implemented algorithms for flow analysis \nand inlining include pruning wherever pos\u00adsible. The second kind of specialization is recursive inlining \nat call sites within a procedure that is being inlined. Inlining Con\u00addition 1 identifies call sites where \ninlining is possible within the unspecialized procedures of the original program. To identify candidates \nfor inlining within a specialized proce\u00addure, we employ a similar condition that takes into account the \nspecialization contour. Inlining Condition 2. Let (m, p, K)~ be an abstract CiO\u00ad sure that is being inlined \nat call site 1 and specialized to con\u00adtour K. A call site (call 2? 61... ~~)1 within m cart be inlined \nin the specialized version of m if F(io, K) = {(rii$~)A} and closure m has arity $. That is, we can \ninline A at call site ~ within the specialized version of m if the flow analysis identifies a unique \nabstract closure at program point (~0, K). Support for recursive inlin\u00ading follows naturally from a polyvariant \nflow analysis. 3.5 Inlining procedures with free variables Our discussion of inlining has so far neglected \nan important issue in a language with higher-order procedures and lexical scoping. How do inlined procedures \ngain access to the values of their free variables? To answer this question, we define a target language \nthat adds an ordered list of free variables [z1 . . . z~] to each A\u00adexpression: (~ [ZI . ..2%] (Z1... \nzn) eb) The target language also includes an expression form (cl-ref el n) to access the nih element \nof a closure s free variable list. The cl-ref operator (short for closure-reference) requires el to evaluate \nto a closure. When a procedure with free variables is inlined, all refer\u00ad ences to these variables are \nreplaced with cl-ref operations. To illustrate, suppose that at (call eo el . . . en ) we inline (A (zI... \nz~ ) eb) which has {z1 . . . z~ } free. The original J-expression is replaced with We replace the call \nwith (call (J (w z~. ..zn) e:) eo el.. .en) 199 where w is a new variable and ej is a specialized copy \nof eb smaller than some threshold $ize2. In the next section, we with references to zi replaced by (cl-ref \nw i). present performance results for various inlining thresholds. The cl-ref operation is easy to implement \nin a compiler that uses flat closures [2]. A closure record consists of a code pointer and the values \nof the variables in the free variable list for that procedure. A compiler that uses linked closures may \nrequire additional information in the cl-ref operation to indicate the forms of the linkage structures. \n 3.6 An algorithm Fig. 5 collects the above observations into a complete inlin\u00ading algorithm. Given a \nflow analysis F for a program P, the transformation Z[e]tcp takes a subexpression e of P, a contour K, \nand a loop map p, and produces an equivalent inlined and specialized expression. The initial contour \nfor P is the special contour ? and the initial loop map is the empty map. The contour ? denotes the union \nof all possible contours, as in Inlining Condition 1. In other words, F(l, ?) = U F(l,/c). ICEContotw \n The contour parameter K of the algorithm selects a par\u00adticular context in which to specialize subexpressions \n(see the condition F(10, K) = {(t , p , K )~} in the rule for ap\u00adplications, and the various conditions \nin the rule for if\u00adexpressions). Since the original copy of a A-expression must not be specialized to \nany specific contour, the transforma\u00adtion of its body is performed in the special contour ?. To prevent \nuncontrolled unwinding of recursive calls, the loop map p maps label-contour pairs to variables, If, \nwhile inlining procedure 1 in contour K , the algorithm encounters another call site for 1 in contour \nK (see the second case of the rule for applications), it constructs a loop by binding a fresh variable \nto the inlined procedure at the initial call site with letrec. In fact, the algorithm int reduces a let \nrec for every inlined procedure, whether recursive or not, but subse\u00adquent local simplification removes \nany unnecessary bindings. This method of controlling unwinding avoids unrolling loops. Loop unrolling \ncan be a valuable optimization and would be easy to include in this framework, We have intentionally \navoided unrolling loops in order to isolate the benefits of irdining. 3.7 Making inlining decisions \nInlining every call site that is a candidate for inlining is im\u00ad practical sa it can lead to exponential \ncode growth. To limit irdining to those sites where it is likely to be most profitable, Fig. 5 requires \nthe predicate h-dine? to hold for the proce\u00addure body when specialized for the contour and loop map under \nconsideration. This predicate estimate the sizes of the generated code for the inlined procedure at a \nparticular call site, and limits inlining to cases where the generated code is 3.8 Local Simplification \n After inlining, we perform some simple optimization that are based purely on local syntactic criteria. \nThese include p. reductions that do not significantly increase code size, sim\u00adple constant propagation \nand constant folding, eliminating unused bindings, and discarding purely functional expres\u00adsions whose \nresult is never used. 4 Performance We have implemented a source-to-source flow-directed in\u00adlining optimization \nfor the full R4RS Scheme language [7]. Given a Scheme program and an inline threshold T, our op\u00adtimizer \ninlines procedures whose specialized size is estimated to be less than T. We use Chez Scheme [11] to \ncompile the optimized programs to native code. Without access to Chez s internal data representations, \nit is impossible to implement cl-ref with the same efficiency as a variable reference. Indeed, using \na faithful implementation of the algorithm from the previous section, we found the pre\u00admium for accessing \nvariables via cl-ref masked the benefit of inlining. Therefore, the results presented in this section \nreflect an inlining strategy in which the Inline ? predicate requires inlined procedures be closed up \nto top-level vari\u00adables. Under this constraint, the inlining algorithm never generates cl-ref operations. \nThe algorithm will still allow a procedure P that has a free variable z to be inlined at call site G \nif (i) z occurs in a conditional branch which can be eliminated in the specifllzed copy of P at C, or \n(ii) z refers to a procedure that that will be inlined. In either case, a free variable in the source \ndoes not appear in the specialized version. Surprisingly, this inlining strategy leads to consis\u00adtent \nperformance improvements for all programs we have tested. For many of the benchmarks in our test suite, \nthe performance improvements are significant. We would expect even greater improvements with an efficient \nimplementation of cl-ref since this would enable inlining open procedures. We applied our optimizer to \nthe benchmarks listed in Ta\u00adble 1. These benchmarks are characteristic of a wide range of Scheme programs, \nand none were written with knowledge of the inlining strategy used. The program Lattice enumerates the \nlattice of maps be\u00adtween two lattices, and is mostly first-order. Boyer is a term-rewriting theorem prover. \nGraphs counts the number of directed graphs with a distinguished root and k vertices, each having out-degree \nat most 2. It makes extensive use of higher-order procedures, and is written in a continuation\u00adpaasing \nstyle. Given an n x n random matrix &#38;f with {+1, -1} entries, Matrix tests whether A4 is maximal \namong all 2The algorithm in Fig. 5 requires constructing the specialized ver\u00ad&#38;rn of the procedure \nbody before deciding if it should be inlined. Our implementation uses an equivalent but mwe efficient \nalgorithm that estimates the size of the specialized procedure without actually constructing it. 200 \n Z[(call e$ el... en)]rcp Z[(begin el...e~)]~p Z[(let ((z cl)) e2)1]6p Z[(letrec ((y ef)) e2)*]q4 Z[(cons \nel ez)]tcp Z[(car el)]~p Z[(cdr el)]tcp Z[(set-car! el ez)]~p Z[(set-cdr! el ez)]~p  c (p Z[e*]Kp. \n..Z[en]tsg) (A [Z1...zm] ccl . ..z~) ~[eb]?p) where [zl. ..z~]=FV ((A (q. ..zn) if eo #(A...) (letrec \nand F(h, K) = {(1 ,/, IC )A} ((y (A (w Z1.. .zn) and (1 , K ) @ Dom(p) (let ((ZI (cl-ref w l))) and \nrd~ne? (~[eb]K # ) . where (A (xl . ..Zn) eb)( 6P ((~m (cl-ref w m))) y, w fresh = T[eb]K P ) ) ) ) P \n= P[(~ , ~ ) * VI (call Y Z[eO]KK .Z[ellKp .4. ~[enlfiP)) [.Zl . . ..%] =~v((~ (Xl...%) Iz if F(lo, \nK) = {(1 , P , J)A} (call yZ[eO]Kp Z[el]K#. ..Z[en]fc~) and p(l ,lc )=y (call Z[eO]sp Z[el]~p. ..Z[en]xp) \notherwise = (begin Z[el]%&#38;. . . ~[e~]~p) (if Z[e,]KKZ[e2]~p Z[e3]~p) if{true, false} ~F(ll, K) (begin \nZ[el]tcp Z[ez] KP) if {true} ~F(ll,K) (begin X[el]fip Z[e~]xp) if {false} ~F(lI, K.) Z[el~Kp otherwise \n= (let ((x T[el](K:l)p)) ~[e21w) = (letrec ((y ~[el]~p, )) ez) where P =P[(lI, K) *Y] = (cons Z[el]Kp \nZ[ez]xp) = (car ~[el]~p) = (cdr Z[el]tcfl) = (set-car! Z[el]Kp Z[e2]@ = (set-cdr! Z[el]w Z[e2]@ Figure5: \nAflow-directed inliningalgotithm eb)) eb)) 201 Program Source Analysis Ratio of object code size Lines \nTimes to original object code size 11 - - - ---- - -\u00ad (in sees. ) II for vari&#38;s inliiin thresholds \n50 100 200 500 1000 Lattice 207 .2 1.02 1.02 1.06 1.69 3.09 Boyer 381 1.4 .95 1.05 1.10 .97 .95 Graphs \n489 .3 .79 .87 .83 .81 .80 Matrix 527 .4 .93 .97 .96 1.13 1.21 Maze 568 .5 .94 .98 .96 .93 .89 Splay \n934 4.0 .94 .94 .95 .95 .95 Nbody 964 2.6 1.25 1.53 1.66 2.29 2.62 Dynamic 2046 110.3 1.52 1.65 2.36 \n2.48 2.48 Table 1: Benchmark programs. matrices of the same dimension obtainable by simple re\u00adordering \nof rows and columns, and negation of any subset of rows and columns. Like Graphs, this program is writ\u00adten \nin continuation-passing style. Maze generates a random maze and computes a path through it using a union-find \nalgorithm. It makes extensive use of records [25] and uses Scheme s call-with-current-cent inuat ion \noperator, but is primarily a first-order program. Splay is an implementa\u00adtion of splay trees. It makes \nextensive use of higher-order procedures and pattern matching macros [25]. N-Body is a Scheme implementation \n[26] of the Greengard multipole algorithm [12] for computing gravitational forces on point\u00admasses distributed \nuniformly in a cube. Dynamic is an im\u00ad p Cementation of a tagging optimization algorithm [14] for S theme. \nIt is primarily a first-order program, but has com\u00ad p~ex control-flow, and many deeply-nested conditional \nex\u00ad pressions. The first column of Table 1 indicates the size of each pro\u00adgram in lines of source code \nafter prepending necessary li\u00adbrary procedures, removing comments, expanding macros, and performing local \nsimplifications as described in Sec\u00adtion 3.8. The next column indicates the time taken by the flow analysis \nto analyze the program. The remaining columns indicate code sizes after inlining for various size thresholds. \nFor example, the call (map car m) from Figs. 1, 2, and 3 is inlined at thresholds above 60. Code size \nratios less than one indicate that the inlined pro\u00ad gram is smaller than the original. For example, at \nthreshold lI](JJ, the inlined version of Graphs is still 2070 smaller than the original program. For \nmost benchmarks, object code size grows quite slowly as the inlining threshold increases. There are two \nreasons for this. First, Scheme programs typ\u00ad ically consist of many small procedures, so higher inlining \nthresholds are not likely to expose many more opportunities for inlining. Second, even when relatively \nlarge procedures become candidates for inlining, specialization and local sim\u00ad plification significantly \nreduce the size of the inlined copy. Fig. 6 presents execution times for these benchmarks un\u00adder different \ninlining thresholds. These times were gathered on an SGI 150 MHz MIPS R4400 workstation. The exe\u00adcution \ntime for each inlined program is normalized to the execution time of the original program after local \nsimplifi\u00adcation (i.e., threshold O) when run under Chez Scheme 5.Oa st its highest optimization level. \nAt this level, the compiler will inline calls to primitives and small built-in procedures. Furthermore, \nthe generated code is unsafe: inlined primi\u00adtives do not perform any type or bounds checking. Thus, performance \nimprovements shown in these graphs measure gains above what an aggressive optimizing compiler would ordinarily \nachieve. The graphs in Fig. 6 separate execution time into mutator time and collector time. The axis \non the left of each graph measures normalized total execution time, which is indicated by the total heights \nof the bars. The axis on the right of each graph measures mutator execution time, which is indicated \nby the tops of the dark bars. Mutator time is the time taken by the application when garbage collection \nis discounted. The light part of each bar indicates time spent performing garbage collection. For example, \nat size threshold 200, Maze shows a 40% performance improvement in total execution time. Discounting \ngarbage collection time, which inlining cannot (directly) affect, mut ator time has been improved by \nnearly 60~0. For all of the programs we have tested, flow-directed inlin\u00ading improves execution times. \nFurthermore, as size thresh\u00adolds are increased, performance either increases or remains roughly constant. \nThis indicates that our size metric is be\u00adnign, and inlining decisions rarely impact performance neg\u00adatively. \nOn average, the best performance occurs at size thresholds between 200 and 500. At smaller thresholds, \nour implementation inlines small library procedures like cadr or a specialized version of map, but is \nunlikely to inline non\u00adtrivial user-defined procedures. At higher thresholds, more user-defined procedures \nare inlined, and more opportuni\u00adties for specialization of these procedures become apparent. Few additional \nprocedures are inlined at thresholds above 500; thus, we see little further performance improvement. \nBoth Nbody and Matrix show performance improvement of only roughly 10%. A relatively large percentage \nof these program s execution time is spent in tight loops, and not in calls to out-of-line procedures. \nHence, inlining exposes few optimization opportunities. On the other hand, Maze and Boye r both make \nmany out-of-line calls relative to the overall computation. Inlining these calls removes significant \noverhead in these benchmarks. The same conclusion holds to a slightly lesser extent for Graphs and Lattice. \n202 .._...  - ---\u00ad 1.0 g -0.8 $ 0.8 ~ . -0.6$ . -().4 5 g . -0.2 * o 50 100 200 500 0.() 1000 E g \n- 0 50 100 200 500 1000 inlining threshold inlining threshold Lattice Boyer ii! 1.0 + 0.8 ~ 0.6$ 0.4: \n1.0 0.8 0.6 0.4 0.2 J 0.2 o 50 100 200 500 1000 0.0 E e I 0.0 0 50 100 200 50Q 1000 z inlining threshold \ninlining threshold Graphs Matrix 1.0 0.8 i ~ ~ 1.0 ~ -  0.8\u00ad ~. 0.6 : 0.6\u00ad - 0.4 j 044\u00ad -0.4 E 0.2 \n~ 0.2\u00ad  -0.21 0,0 0 50 100 200 500 inlining threshold 1000 8 g 0.0 o 50 100 200 500 inlining threshold \n1000 0.0 i! E Maze Splay EJ 1.0 , ,-7, $ $ 2 g : .s ~ 1.0 0.8 0.6 0.4 0.2 $ 1,0 + ()+8 ~ 0.6 ~ 0.4 z \n().2 ~ 0 50 10Q 2(KI 500 inlining threshold 1000 g c = 0.0 o 50 100 200 500 inlining threshold 1000 0.0 \n] c Nbody Dynamic Figure6: Execution times under different inlining thresholds. Threshold Oindicates \nno inlining. Thedark section ofeach bar indiates mutator time; the light section indicates collector \ntime. 203 There is little change in collection time for most of the benchmarks in Fig. 6. This implies \nthat inlining does not generally alter the amount of data allocated or the lifetime of objects. Indeed, \nfor most benchmarks, overall allocation remains roughly constant. However, for Graphs, there is a dramatic \nincrease in collection time (and allocation) at thresholds 50 and 100. We have three conjectures for \nthis behavior. First, inlining may cause more closures to be al\u00adlocated. When a procedure is inlined, \nany local procedures within it are copied, and hence will construct new closures. Also, recall that a \nlocal letrec is introduced for every in\u00adlined procedure. If some of these procedures are not com\u00adpiled \nas tail-recursive loops, closures will be allocated for them as well. Second, inlining may increase closure \nallo\u00adcation costs. Chez Scheme uses a flat closure representa\u00adtion. As a result, inlining will increase \nthe cost of allocating closures by introducing additional free variables into proce\u00addures. Third, with \nChez Scheme s closure representation, introducing additional free variables into procedures may cause \ndata to be retained longer than it would be otherwise. We suspect that using Shao and Appel s safe for \nspace com\u00adplexity rule [21] would ameliorate some of these problems. At higher thresholds, the collection \ntimes for Graphs drop. More opportunities for simplification are exposed which pre\u00adsumably eliminate \nthe troublesome closures. 5 Related work Two independent topics related to our work have been in\u00advestigated \nby other researchers flow analysis and inlining. 5.1 Flow analysis The literature on flow analysis for \nfunctional and object\u00adoriented languages is vast [13, 18, 22]. We limit our dis\u00adcussion of flow analysis \nto systems with polyvariance, which we believe is key to building optimizations that scale with program \nsize. A well-known approach to polyvariance proposed by Shiv\u00aders [22] is to distinguish different procedure \ncalls by call site. In a framework similar to ours, contours are finite strings of call site labels. \nAn IV-CFA analysis uses the most recent N call sites to distinguish different invocations of a proce\u00addure. \nWhile call-string based analyses are highly precise, they also appear to be quite expensive to compute \n[15]. In general, polymorphic splitting provides as much, if not more, accuracy than polyvariant call-string \nanalyses, but at signif\u00adicantly lower cost. 5.2 Inlining Cooper, Hall, and Torczon [8, 9] studied the \neffects of in\u00adlining for Fortran programs. They determined inlining sites by hand, and were able to eliminate \nthe majority of dy\u00adnamically executed procedure calls from their benchmark programs. Furthermore, they \nfound that the opportunities revealed by inlining mitigated object code growth~bject code grew by only \n10~o when inlining doubled the size of the source code. However, they were unable to demon\u00adstrate consistent \nspeedups for the inlined programs. They speculate that inlining did enable useful optimizations, but \nthe speedup from these optimizations was masked by effects like increased register pressure, and pessimistic \nassumptions made by the compiler about aliasing which lead to poor in\u00ad struction scheduling. Several \ndifferences between Scheme and Fortran may explain our better results. First, our inlined programs have \nsmaller procedures than the inlined Fortran programs, so register pressure is less significant. Second, \nScheme compilers cannot optimistically assume that parameters to procedure calls do not alias. Hence \ninlining introduces no additional require\u00adments on instruction scheduling. Finally, and most impor\u00adtantly, \nas Scheme programs tend to have more procedure calls, there is more procedure call overhead to remove. \nChang et al. [6] constructed an automatic inlining optimiza\u00adtion for C programs. As with Cooper et al. \ns experiments with Fortran, Chang et al. were able to eliminate the ma\u00adjority of procedure calls while \nincreasing object code size by only 16~o. They obtained consistent performance improve\u00adments that averaged \nabout 10~o. Much attention has been devoted to reducing the overhead of dynamically bound method dispatches \nin object-oriented languages. Recently, static analyses have been applied to either inline most dispatches \nor replace them with direct procedure calls [1, 10, 19, 20]. But with the exception of Cecil where dispatching \nconsumes a large fraction of execu\u00adtion time [10], speedups are minimal at best. We suspect that these \nsystems fail to improve execution times because their compilers do not use the larger contexts constructed \nby inlining to enable further optimization. Functional language compilers like Standard ML of New Jer\u00adsey \n[2] use synt attic heuristics to guide inlining decisions. A call site is a candidate for inlining if \nthe called procedure is syntactically evident; typically, the function expression is a variable that \nis bound to a A-expression by let. Inlining is intertwined with other synt act ic optimizations, so additional \ninlining candidates can arise as optimization proceeds. In contrast, flow-directed inlining separates \ninlining decisions from post-inlining simplifications. Heuristics that estimate object code size, similar \nto ours, are used to control code growth. But these heuristics are necessarily more conserva\u00adtive, as \nthey cannot take into account future simplifications that may take place if inlining is performed. Furthermore, \nbecause candidates for inlining are identified syntactically, procedures used in a higher-order manner \nwill not be inlined unless other simplifications reduce the higher-order uses to syntactic ones. We believe \nthat for programs which make heavy use of data and procedural abstraction, flow-directed inlining is \nlikely to identify more profitable sites for inlining. 6 Future Work We have used the same flow analysis \ndescribed in this paper to eliminate run-time checks from Scheme programs [15]. We plan to combine our \ninlining and run-time check opti\u00admization along with other optimizations that use the same flow information. \nThis combination should yield significant performance improvements without compromising safety. 204 \n[14] HENGLEIN, F. Global Tagging Optimization by Type Inference. In ACM Symposium on Lisp and Functional \nProgramming (1992), pp. 205-215. [15] JAGANNATHAN, S., AND WRIGHT, A. Effective Flow-Analysis for Avoiding \nRuntirhe Checks. In Second International Symposium on Static Analysis (1995), pp. 207 225. Springer-Verlag \nLNCS 983. [16] JONES, N., AND MUCHNICK, S. Flow Analysis and Op\u00adtimization of Lisp-like Structures. In \n6th ACM Sym\u00adposium on Principles of Programming Languages (Jan\u00aduary 1979), pp. 244 256. [17] MILNER, \nR., TOFTE, M., AND HARPER, R. The DejL nition of Standard ML. MIT Press, 1990. [18] PALSBERG, J., AND \nSWARTZBACH, M. I. Object-Oriented Type Inference. In ACM Conference on Object-Oriented Programming, Systems, \nLanguages, and Applications (1991), pp. 146 161. [19] PANDE, H. D., AND RYDER, B. G. Static Type De\u00adtermination \nfor C++. In Usenix C++ Conference Pro\u00adceedings (1994), pp. 85 97. [20] PLEVYAK, J., AND CHIEN, A. Precise \nConcrete Type Inference for Object-Oriented Languages. In ACM Conference on Object-Oriented Programming, \nSystems, Languages, and Applications (1994), pp. 324-340. [21] SHAO, Z., AND APPEL, A. Space-Efficient \nClosure Rep\u00adresentations. In ACM Symposium on Lisp and Func\u00adtional Programming (1994), pp. 150 161. [22] \nSHIVERS, O. Control-Flow Analysis of Higher-Order Languages or Taming Lambda. PhD thesis, School of Computer \nScience, Carnegie-Mellon University, 1991. [23] SHIVERS, O. The Semantics of Scheme Control-Flow Analysis. \nIn ACM SIGPLAN Symposium on Partial Evaluation and Semantics-Based Program Manipula\u00adtion (1991), pp. \n190 198. [24] SUN MICROSYSTEMS, INC. The .lavu Language Specifi\u00adcation. Mountain View, Calif., 1995. \n[25] WRIGHT, A. K., AND DUBA, B. F. Pattern Match\u00ading for Scheme. Unpublished document available from \nhttp://www.neci. nj.nec.com\\homepages/wright.html, 1993. [26] ZHAO, F. An O(N) Algorithm for Three-Dimensional \nN-Body Simulations. Master s thesis, Department of Electrical Engineering and Computer Science, Mas\u00adsachusetts \nInstitute of Technology, 1987. [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] Acknowledgments \nWe are grateful to Henry Cejtin, Richard Kelsey and the PLDI referees for many helpful suggestions. References \nAGESEN, O., AND HOLZLE, U. Type Feedback vs. Type Inference: A Comparison of Optimization Techniques \nfor Object-Oriented Languages. In ACM Conference on Object-Oriented Programming, Systems, Languages, \nand Applications (1995), pp. 91 107. APPEL, A. Compiling with Continuations. Cambridge University Press, \n1992. BARENDREGT, H. The Lambda Calculus. North-Holland, Amsterdam, 1981. BULYONKOV, M. Polyvariant Mixed \nComputation for Analyzer programs. Acts Znforrnat!ca 21 (1984), 473\u00ad 484. CHAMBERS, C., AND UNGAR, D. \nCustomization: Opti\u00admizing Compiler Technology for SELF, A Dynamically-Typed Object-Oriented Programming \nLanguage. In ACM SIGPLAN 89 Conference on Progmmming Lan\u00adguage Design and Implementation (New York, June \n1989), ACM, pp. 146-160. CHANG, P. P., MAHLKE, S. A., CHEN, W. Y., AND HWU, W.-M. W. Profile-Guided Automatic \nInline Ex\u00adpansion for C Programs. Software Pmctice and Expe\u00adrience (May 1992), 349 369. CLINGER, W., \nAND REES, EDITORS, J. Revised4 Re\u00adport on the Algorithmic Language Scheme. ACM Lisp Pointers ~, 3 (July \n1991). COOPER, K., HALL, M., AND TORCZON, L. An Exper\u00adiment with Inline Substitution. Software: Pmctice \nand Experience 21, 6 (1991), 581-601. COOPER, K., HALL, M., AND TORCZON, L. Un\u00adexpected Side Effects \nof Inline Substitution: A Case Study. ACM Letters on Programming Languages and Systems 1, 1 (1992), 22-32. \nDEAN, J., CHAMBERS, C., AND GROVE, D. Selective Specialization for Object-Oriented Languages. In ACM \nConference on Programming Language Design and Im\u00adplementation (1995), pp. 93 102. DYBVIG, K. The Scheme \nProgmmming Language. Prentice-Hall, Inc., 1987. GREENGARD, L. The Rapid Evaluation of Potential Fields \nin Particle Systems. ACM Press, 1987. HEINTZE, N. Set-Baaed Analysis of ML Programs. In ACM Sympostum \non Lisp and Functional Programming (1994), pp. 306-317. 205  \n\t\t\t", "proc_id": "231379", "abstract": "A <i>flow-directed inlining</i> strategy uses information derived from control-flow analysis to specialize and inline procedures for functional and object-oriented languages. Since it uses control-flow analysis to identify candidate call sites, flow-directed inlining can inline procedures whose relationships to their call sites are not apparent. For instance, procedures defined in other modules, passed as arguments, returned as values, or extracted from data structures can all be inlined. Flow-directed inlining specializes procedures for particular call sites, and can selectively inline a particular procedure at some call sites but not at others. Finally, flow-directed inlining encourages modular implementations: control-flow analysis, inlining, and post-inlining optimizations are all orthogonal components. Results from a prototype implementation indicate that this strategy effectively reduces procedure call overhead and leads to significant reduction in execution time.", "authors": [{"name": "Suresh Jagannathan", "author_profile_id": "81100208907", "affiliation": "NEC Research Institute, 4 Independence Way, Princeton, NJ", "person_id": "PP39032551", "email_address": "", "orcid_id": ""}, {"name": "Andrew Wright", "author_profile_id": "81100660256", "affiliation": "NEC Research Institute, 4 Independence Way, Princeton, NJ", "person_id": "PP14226681", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/231379.231417", "year": "1996", "article_id": "231417", "conference": "PLDI", "title": "Flow-directed inlining", "url": "http://dl.acm.org/citation.cfm?id=231417"}