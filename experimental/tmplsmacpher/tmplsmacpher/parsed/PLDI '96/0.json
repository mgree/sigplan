{"article_publication_date": "05-01-1996", "fulltext": "\n Software Pip~elining Showdown: Optimal vs. Heuristic Methods in a Production Compiler John Ruttenberg*, \nG. R. Gao ~, A. Stoutchinin~, and W.Lichtenstein* *Silicon Graphics Inc., 2011 N. Shoreline Blvd., Mountain \nView, CA 94043 ~McGill University School of Computer Science, 3480 University St., McConnell Building, \nRoom 318, Montreal, Canada H3A2A7 Authors email: rutt,wdl@sgi.com gao,stoafjcs.mcgill.ca Abstract This \npaper is a scientific comparison of two code generation tech\u00ad niques with identical goals generation \nof the best possible soft\u00ad ware pipelined code for computers with instruction level parallelism. Both \nare variants of modulo scheduling, a framework for generation of soflware pipelines pioneered by Rau \nand Glaser [RaG181 ], but are otherwise quite dissimilar. One technique was developed at Silicon Graphics \nand is used in the MIPSpro compiler. This is the production compiler for SG1 S systems which are based \non the MIPS R8000 processor [Hsu94]. It is essentially a branch-and-bound enumeration of possible sched\u00ad \nules with extensive pruning. This method is heuristic becaus(s of the way it prunes and also because \nof the interaction between reg\u00ad ister allocation and scheduling. The second technique aims to produce \noptimal results by formulat\u00ad ing the scheduling and register allocation problem as an integrated integer \nlinear programming (ILP1 ) problem. This idea has received much recent exposure in the literature [AlGoGa95, \nFeautner94, GoAlGa94a, GoAlGa94b, Eichenberger95], but to our knowledge all previous implementations \nhave been too preliminary for detailed measurement and evaluation. In particular, we believe this to \nbe the first published measurement of runtime performance for ILP based generation of software pipelines. \nA particularly valuable result of this study was evaluation of the heuristic pipelining technology in \nthe SGI compiler. One of the motivations behind the McGill research was the hope that optimal software \npipelining, while not in itself practical for use in product\u00ad ion compilers, would be usefhl for their \nevaluation and validation. Our comparison has indeed provided a quantitative validation of the SGI compiler \ns pipeliner, leading us to increased confidence in both techniques. 1. It is unfortunate that both instruction \nLevel Parallelism and Integer Linear Programming are abbreviated ILP in the literature. To clarifi, we \nalways use the abbreviation ILP to mean the latter and will suffer thrcugh describing instruction level \nparallelism by its fill name. Permission to make digitalhrd COPYof part or all of this work for personal \nor elasaroom use is ranted without fee provided that copies are nc)t made or distributed for pm 1t or \neommemial advanta e, the copyright notice, the Me of the ubiioation and its date appear, an ! notice \nis given that copying tIS y permission of ACM, Ino. To copy othenvise, to republish, to past on sswsra, \nor to mdistributs to lists. requires prior speoific parmiaaion ancf/or a fee. Little work has been done \nto evaluate and compare alterna\u00ad tive algorithms and heuristics for modulo scheduling from the viewpoints \nof schedule quality as well as computational complexity. This, along with a vague and unfounded percep\u00ad \ntion that modtdo scheduling is computationally expensive as well as dljicult to implement, have inhibited \nits incorpora\u00ad tion into product compilers. B. Ramakrishna Rau [Rau94] 1.0 Introduction 1.1 Software \npipelining Software pipelining is a coding technique that overlaps operations from various loop iterations \nin order to exploit instruction level parallelism. In order to be effective software pipelining code \nmust take account of certain constraints of the target processor, namely: 1. instruction latencies, \n2. resource availability, and 3. register restrictions.  Finding code sequences that satisfy the constraints \n(1) and (2) is called scheduling. Finding an assignment of registers to program symbols and temporaries \nis called register allocation. The primary measure of quality of software pipelined code is called the \n11, short for iteration or initiation interval. In order to generate the best pos\u00adsible software pipelined \ncode for a loop, we must find a schedule with the minimum possible II and also we must have a register \nallocation for the values used in the loop that is valid for that schedule. Such a schedule is called \nrate--optimal. The problem of finding rate-optimal schedules is NP--complete [GaJo79], a fact that has \nled to a number of heuristic techniques [DeTo93, GaSc9 1, HuffP3, MoEb92, Rau94, Warter92, Lam88, AiNi88] \nfor genera\u00adtioti of optimal or near optimal soflware pipelined code. [RaFi93] contains an introductory \nsurvey of these methods. 1.2 The allure of optimal techniques Recently, motivated by the critical role \nof software pipelining in high performance computing, researchers have become interested in non-heuristic \nmethods, ones that guarantee the optimality of the solutions they find. The key observation is that the \nproblem can be formulated using integer linear programming (ZLP), a well known framework for solving \nNP-complete problems [Altman95, AlKePoWa83]. Given such a formulation, the problem can be given to one \nof a number of standard ILP solving packages. Because this framework has been effective in solving other \ncom\u00adputationally difficult problems [NeWo88, Nemhauser94, Pugh91,BiKeKr94], it is hoped that it can also \nbe useful for soft\u00ad ware pipelining. PLDI 96506 PA, USA O 1996 ACM O-89791 -795+YWOW5...$505O 1.3 The \nshowdown The rest of this paper presents a detailed comparison of two soft\u00adware pipelining implementations, \none heuristic and one optimal. The heuristic approach is represented by the Silicon Graphics MZPSpro \ncompiler. The optimal approach is represented by MOSZ an ILP based pipeliner developed at McGill University. \nWe adapted the MOST scheduler to the MIPSpro compiler so that it could act as an exact fictional replacement \nfor the original heu\u00adristic scheduler. This work allowed us to perform a very fair and detailed comparison \nof the two approaches. What did we discover? In short, the SGI heuristics were validated by this study. \nComparison with ,MOST was an effective way to evaluate SGI S production quality pipeliner. In particular, \nwe dis\u00adcovered that the optimal approach is only very rarely able to find a better II than the heuristic \napproach and benchmarks compiled with the optimal techniques did not clearly perform better than those \ncompiled with the hefistic pipeliner. In the remainder of this paper we describe the two approaches in \ndetail, present our experimental results, draw some conclusions from these results, and try to shed some \nintuitive light on the results.  2.0 The heuristic approach software pipelining at Silicon Graphics \n 2.1 MIPSpro compiler Starting in 1990, Silicon Graphics designed a new microprocessor aimed at the supercomputing \nmarket called R8000 [HSU94] and shipped with SGI S Power Challenge and Power Indigo2 comput\u00aders. It \nis an in+rder 4-issue superscalar RISC processor featuring fully pipelined floating point and memory \noperations. As part of this project, a new compiler was implemented that had the basic goal of generating \nvery high quality software pipelined inner loop code. This compiler shipped under the name MIPSpro Compiler \nwith R8000 based systems starting in late summer of 1994. At the time the system shipped, it had the \nbest reported perfomxmce on a number of important benchmarks, in particular for the floating point SPEC92. \nOur measurements show the compiler s software pipelining capabilities play the central role in delivering \nthis per\u00adformance on the R8000, (See Figure 2.) The MIPSpro compiler performs a rich set of analysis \nand optimi\u00adzation before its software pipelining phase. These fall into three distinct categories: 1. \nhigh level loop analysis and transformations, including: a. array dependence analysis, b. loop interchange, \nand c. outer loop unrolling;  2. classical intermediate code optimizations, such as: a. common subexpression \nelimination, b. copy propagation, c. constant folding, and d. strength reductioru  3. special inner \nloop optimizations and analysis in preparation for software pipelining, in particular: a, if-conversion \nto convert loops with internal branches to a form using conditional moves [AlKePoWa83, DeTo93], b. interleaving \nof register recurrences such as summation or dot products, c. inter iteration common memory reference \nelimination, and d. data dependence graph construction.  2.2 Modulo scheduling The framework for the \nMIPSpro compiler s pipeliner is modulo scheduling, a technique pioneered by Bob Rau and others and very \nwell described in the literature. [Larn88, RaFi93, Lam89]. Modulo schedulers search for possible schedules \nby first fixing a iteration interval (II) and then trying to pack the operation in the loop body into \nthe given number of cycles. If this is unsuccessful for one of a number of reasons, a larger II may be \ntried. The SGI scheduler contains a number of extensions and elabora\u00adtions, Five of the most important \nare: 1. binary instead of linear search of the 11s, 2. brancbsm&#38;bound ordering of the search for \na schedule with a great deal of heuristic pruning, 3. use of multiple heuristic orderings in (2) to \nfacilitate register allocation, 4. generation of spill code when required to alleviate register pressure, \nand 5. pairing of memory references to optimize memory system utilization.  As we describe SGI S pipeliner \nin the following sections, it will be useful to keep these in mind.  2.3 Modulo scheduling using binary \nsearch The SGI modulo scheduler searches the space of possible 11susing a binary instead of linear search. \n1 This has no measurable impact on output code quality, but can have a dramatic impact on compile speed. \nThe search is bounded fi-om below by MinII, a loose lower bound based on resources required and any dependence \ncycles in the loop body [RaG18 1]. The search is bounded fi-om above by an arbitrary maximum MaxII = \n2MinII. We set this maximum as a sort of compile speed circuit breaker under the assumption that software \npipelining has little advantage over traditional scheduling once this bound is exceeded. In practice, \nthis has proven to be a reasonably accurate assumption. Our search also makes the heuris\u00adtic assumption \nthat if we can find a schedule at II we will also be able to find one at II+ 1. Although it is possible \nto find counter examples of this in theory, we have yet to encounter one in the course of fairly extensive \ntesting. The search is designed to favor the situation where we can sched\u00ad ule at or close to MinII, \nas these cases are overwhelmingly com\u00admon. A simple binary search would be wasteful in cases where a \nschedule could be found within a few cycles of the MirdI. Instead, we use a search with two distinct \nphases: 1. Exponential buckoff Initially, the search attempts to establish an upper bound at which a \nschedule exists. During this phase we perform an exponential backoff from MinII, 1. The use of binary \nsearch in this context has a fairly long history in the literature. Touzems described the AP 120 and \nFPS 164 compiler and explains how binary search 1sused [Tou84]. Lam pointed out that being able to find \na schedule at II does not imply being able to find a schedule at 11+1and used this to explain why her \ncompiler used linear search [Lam88]. searching successively: MinII, MinII+l, MinII+2, MinII+4, MinII+8,... \nuntil we either fmd a schedule or exceed MaxII. If a schedule is found with II s MinII+2, there are no \nbetter 11s left to search and the schedule is accepted. If no schedule is found, our modulo scheduling \nattempt has failed. (But we may still be able to find a software pipelined schedule by introducing spills \nand trying again. Section 2.8.) 2. Binary search If phase 1 is successful and a schedule is found, but \nwith II > MinII+2, a binary search is used over the space of feasible 11s.Information from phase 1 allows \nus to tighten both the upper and lower bounds for the search, For the lower bound, we use the largest \nII for which phase 1 tried to schedule and failed. For the upper bound, we use the II for which phase \n1 succeeded in finding a schedule. These bounds are tighter than MinII and MaxII which are used to bound \nphase 1. In special situations the two phase approach is abandoned in fawor of simple binary search. \nThis is done when there is reason tc) be pessimistic about being able to software pipeline. In particular, \nsimple binary search is used exclusively after spills are introduced into the code. (See Section 2.8.) \n 2.4 Enumerating possible schedules with pruning The process of searching for a schedule at an given \nII is viewed as an enumeration of the possible schedules for that II. This enumera\u00adtion can be accomplished \nwith a branch and-bound algorithm whose outlines should be familiar. The algorithm is presented in its \nfilly exponential form in Figure 1. This is an exponential algorithm and is not practical in its unpruned \nform. In order to make it useful, it is necessary to prime away much of the search. Fortunately, this \ncan be done without losing too much, since much of the backtracking it does is extremely unlikely to \naccomplish anything. For example, consider two operations with unrelated resource requirements which \nare unrelated by data precedence constraints. Suppose one of these has been scheduled and the second \none fails to schedule. What good can possibly come of allowing a backtrack from one to the other? They \nhave nothing to do with one another and moving one of them in the schedule cannot make it possiblle to \nschedule the other. Or consider the example of two fully pipelined operations with identical resource \nrequirements and unrelated data precedence. Suppose the second (on the priority list) of these fails \nto schecktle. Can it help to move the first one? Any other place we put it is a place that second operation \nwould have found if still available. The assumption that two operations are unrelated by data prece\u00addence \nis less important than might appear at first. Because we are modulo scheduling, any two operations that \nare not in the same strongly connected component can occupy any tNo cycles inl the schedule; it s just \na matter of adjusting the pipestages. (See Section 2.5 below.) Step (4) in Figure 1 guides the backtracking \nof the algorithm. We will say that it chooses a catch point for the backtracking. The catch point is \na scheduled operation that will advance to the next cycle of its legal range after all the operations \nfollowing it on the priority list have been unscheduled. Pruning is accomplished by strengthening the \nrequirements about which operations can be catch points. 1. Make a list of the operations to be scheduled, \nLO..Ln.l .  2. Afler LO..Li.l have been scheduled, attempt to schedule Li as follows:  a. Calculate \na legal range of cycles in which Li may be scheduled. If Li is not part of a nontrivial strongly connected \ncomponent in the data precedence graph, its legal range is calculated by considering arty of its direct \npredecessors and successors that have already been scheduled. Li must be scheduled late enough so that \nprecedence arcs from its predecessors are not violated and early enough so any arcs to its succes\u00adsors \nare not violated. For operations that are part of nontrivial strongly connected components, we need to \nconsider all scheduled members of Li s strongly connected component. A longest path table is kept and \nused to determine the number of cycles by which two members must precede or follow each other. The legal \nrange is cut off to be no more than II cycles, since searching more than this number of cycles will just \nreconsider the same schedule slots pointlessly. b. Consider the cycles in Li s legal range in order. \nIf a cycle is found in which Li maybe scheduled without violating resource constraints, schedule Li in \nthat schedule. If no such cycle is found, we have failed in this attempt to schedule Li. 3. If(2) is \nsuccessfid, resume scheduling the next element of L in step (2) or if all the operations are scheduled, \ntermi\u00adnate with success, $. If(2) is tmsuccessfi-d, find the largest j <i, such that Lj s legal range \nis not exhausted. Unscheduled all the opera\u00adtions Lj..Li-l If there is no such j, the entire scheduling \nattempt has failed. Otherwise, set i = j and resume sched\u00aduling at step (2) with the next cycle of Li \ns legal range. FIGURE 1, Branch-and-bound enumera\u00adtion of all possible schedules at a given II The SGI \npipeliner prunes by imposing the following constraints on backtracking: 1. Only the first listed element \nof a strongly connected compo\u00adnent can catch. 2. Operation j may catch the backtrack caused by the schedul\u00ading \nfailure of operation i if the resource required of i and j are non-identical and unscheduling j makes \nit possible to schedule i. 3. If no catch point is found under constraint (2) a slightly looser constraint \nis used. Under this constraint, Operation j may catch the backtrack of operation i even if i and j have \nidentical resources, Unscheduled all the operations starting with j on the priority list. If i can now \nbe scheduled but in a dzjjlerent schedule slot than j, j may catch the backtrack.  2.5 Adjusting the \npipestages Our branchd-bound algorithm does not require that the priority list be a strict topological \nordering of the data dependence graph. The legal range calculation in Figure 1 only takes account of \nsuch predecessors and successors of an operation as are actually sched\u00ad uled already, i.e., ahead of \nthe current operation on the priority list. Why doesn t this result in schedules that violate data precedence \nconstraints? The answer is that it does. A simple postpass is used to compen\u00adsate, ensuring validity. \nThe postpass performs a single depth first search of the strongly connected component tree starting with \nthe roots (operations with no successors, such as stores) and proceed\u00ading to predecessors. This visitation \nis topological, When each strongly connected component is visited, all its successors have already been \nvisited, and their times in the schedule made legal in terms of their successors. Now it may be required \nto move the component to an earlier time in the schedule in order to make its results available on time \nto its predecessors. This can be done without changing the resource requirements of the schedule as a \nwhole by moving the component by multiples of II. Once this has been done for every strongly connected \ncomponent, we have a schedule that is valid both in terms of its resource and data prece\u00addence constraints. \nOf course, this process may have the negative effect of lengthen\u00ading live ranges. It is better ilom a \nregister allocation perspective to use a topological ordering for the schedule priority list. But some\u00adtimes \nthat is untenable in terms of actually finding a schedule. For\u00adtunately the scheme of using multiple \npriority heuristics ensures that both topological and non-topological approaches can be applied to every \npiece of code, in search of what works best. (See Section 2.7.)  2.6 Register allocation and modulo \nrenaming Once a legal schedule is found, an attempt is made to allocate reg\u00adisters for it using fairly \nwell known techniques. The R8000 has no explicit support for software pipelining. In particular, it has \nonly conventionally accessed registers. Lam describes a technique called modulo renaming that allows \neffective pipelining with con\u00adventional architectures by replicating the software pipeline and rewriting \nthe register references in each replicated instance [Lam89]. The SGI pipeliner borrows this technique. \nThe modulo renamed live ranges so generated serve as input of a standard glo\u00adbal register allocator that \nuses the Cluzitin-Briggs algorithm with minor modifications. [BrCoKeTo89, Briggs92].  2.7 Multiple scheduling \npriorities and their effect on register allocation Early on the SGI team discovered that the ordering \nof operations on the schedule priority list has a significant impact on whether the schedules found can \nbe register allocated. On reflection this is not very surprising. For example, a priority list that is \na backward topological sort of the data dependence graph has the advantage that each operation is considered \nfor scheduling only afier any of its uses have already been placed in the schedule. When this is the \ncase, we know the latest point in the schedule at which it is valid to place the operation and still \nhave its result available in time for its uses. We can shorten live ranges by trying to place each operation \nas low in the schedule as possible, given this limitation. A more interesting discovery was that different \nsearch orders seem to work well with different loop bodies. No single search order works best with all \nloop bodies. Backward topological search order, as described in the previous paragraph, works well in \nmany cases since it tends to group the operands of each operation close together shortening live ranges \nfrom their beginnings. On the other hand, we have found cases where forward topological search order \nworks better. The reason is exactly symmetric to the one given in the previous paragraph. Forward orders \nallow us to group all the uses of each operation close together and as high as possible in the schedule, \nthus shortening live ranges from their ends. This can be particularly useful in some codes with common \nsubexpressions of high degree. In some cases, shortening the live ranges has to take aback seat to producing \ncode at all. In loops with many operations that are not fully pipelined or large strongly connected components, \nit is cru\u00adcial to move these to the head of the scheduling list. What scheduling heuristic could take \naccount of so many different factors? Perhaps it would be possible to devise such a heuristic, but instead \nof searching for the one right heuristic, the SGI team took a different approach. A number of different \nscheduling list heuristics would be tried on every loop, ordered by likelihood to succeed. In common \ncases only one heuristic would be tried. Sub\u00adsequent heuristics need only do well in cases where the \nother are weak. The MIPSpro pipeliner uses 4 scheduling priorities. Experimental results show that lower \nquality results are obtained if any one is omitted. (See Section 4.2). Here is a brief description of \nthe two primary ordering heuristics. 1. Folded depthjrst ordering In the simple cases, this is just \na depth first ordering starting with the roots (stores) of the calculation. But when there are difficult+o-schedule \nopera\u00adtions or large strongly-connected components, they are folded and become virtual roots. Then the \ndepth first search proceeds outward from the fold points, backward to the leaves (loads) and forward \nto the roots (stores). 2. Data precedence graph heights The operations are ordered in terms of the \nmaximum sum of the latencies along any path to a root.  These two fundamental heuristics are modified \nin one or both of two ways to derive additional heuristics: 1. Reversal the list can be reversed. Forward \nscheduling is particularly usefid with the heights heuristic. 2. Ajinal memory sort-pulling stores with \nno successors and loads with no predecessors to the end of the list.  The four heuristics actually \nin use in the MIPSpro compiler are: 1. FDA4S folded depth first ordering with a final memory sort, \n2. FDNMS folded depth first ordering with no final memory sort, 3. HMS heights with a final memory \nsort, and 4. RHMS reversed heights with a final memory soti,  See Section 4.2 for experimental results \nthat show the complemen\u00adtary effect of these four. 2.8 Spilling The SGI pipeliner has the ability to \nspill in order to alleviate regis\u00adter pressure. If a modulo scheduling pass is unsuccessful because of \nfailure to register allocate, spills and restores are added to the loop and another attempt is made. \nSpills are added exponentially; the first modulo scheduling failure results in a single value being spilled; \nthe second failure spills two additional values; the third spills 4 more, and so on. The process is capped \nat 8 modulo sched\u00aduling failures, implying that up to 255 values maybe spilled before giving up. In practice \nthis limit is never reached. Spill candidates are chosen by looking at the best schedule that failed \nto register allocate. For each live range, a ratio is calculated: the number of cycles spanned divided \nby the number of references. The ~eater this ratio, the greater the cost and smaller the benefit of keeping \nthe value in a register. Thus values with the largest ratios are spilled first. 2.9 Memory bank optimization \nThe MIPS R8000 is one of the simplest possible implementations of an architecture supporting more than \none memory reference per cycle. The processor can issue two references per cycle, and the memory (specifically \nthe second level cache which is directly accessed by floating point memory references) is divided into \ntwo banks of double-words, the even address bank, and the odd address bank. If two references in the \nsame cycle address opposite banks, then both references are serviced immediately, If two references in \nthe same cycle both address the same bank, one is serviced imme\u00addiately, and the other is queued for \nservice in a l-element queue called the bellows. If this hardware configuration cannot keep up with the \nstream of references, the processor stalls. In the worst case there are two references every cycle all \naddressing the same bank, and the processor stalls once on each cycle, so that it enck up running at \nhalf speed. Scheduling to avoid memory bank conflicts must address two issues, which are independent \nof the details of any particular banked memory system. First, at compile-time we rarely have complete \ninformation on the bank assignments of references. For example the relative banks of consecutive elements \nof a row of two-dimensional Fortran array depend on the leading dimensicm of the array, which is often \nnot a compile-time constant. Second, modifying a schedule to enforce rules about the relative bank assignments \nof nearby references may increase register pressure, and therefore increase the length of the schedule \nmore than the savings in memory stalls shortens the schedule. The MIPSpro heuristic attempts to find \nknown even-odd pairs of references to schedule in the same cycle it does not model the bellows feature \nof the memory system. Before scheduling begins, but after priority orders have been calculated, for each \nmemory reference m it forms the prioritized list L(m) of all other references m for which (m, m ~ is \nknown to be an even-odd pair. If L(w) is no~mpty then we say m is pairable. Given the iteration interval, \nthe number of memory references, and the number of pairable ref\u00aderences, we can tell how many references \nshould ideally be sched\u00aduled in known pairs, and how many references will have to be scheduled together \neven though they may turn out at rnntime to reference the same bank. Until enough pairs have been scheduled \ntogether, whenever the compiler schedules a pairable memory ref\u00aderence m, it immediately attempts to \nschedule the first possible unscheduled element m of L(m) in the same cvcle as m. If this ,. fails, it \ntries in order: the following 1. try to schedule another element of L(m) in the same cycl[e as m, or \n 2. try scheduling m in a different cycle, or 3. backtrack and try changing the scheduling of earlier \nopera\u00adtions in scheduling priority order.  Note that this process may change the priority ordering \nof schedul\u00ading, since memory reference with higher priority than m are passed over in favor of scheduling \nm with m. The MIPSpro scheduler has two methods of controlling the impact of memory bank scheduling on \nregister pressure. First, it measures the amount that priorities are changed due to pairing attempts \ndur\u00ading the entire scheduling process. If this measurement is large enough, and if register allocation \nfails, it tries scheduling again with reduced pairing requirements. Second, in the adjusting pipe stages \npart of scheduling (see Section 2.5), preserving pairing may require issuing a load one or more pipe \nstages earlier than would be necessary for simple legality of a schedule, Again, register :dlo\u00adcation \nhistory is used to guide policy if there has been trouble allocating registers, the scheduler is less \nwilling to add pipe stages to preserve pairing. Finally, since the minimal II schedule found first may \nnot be best once memory stalls are taken into account, the algorithm makes a small exploration of other \nschedules at the same or slightly larger II, searching for schedules with provably better stalling behavior. \n 3.0 Software pipelining at McGill University  the optimal approach 3.1 Development of the ILP formulation \nThe interest in software pipelining at McGill stemmed from work on register allocation for loops on dataflow \nmachines. This work culminated in a mathematical formulation of the problem in a lin\u00adear periodic form \n[GaNi9 1,NiGa92]. It was soon discovered that this formulation can also be applied to software pipelining \nfor con\u00adventional architectures. This formulation was then used to prove an interesting theoretical result: \nthe minimum storage assignment problem for rate-optimal software pipelined schedules can be solved using \nan efficient polynomial-time method provided the tar\u00adget machine has enough functional units so resource \nconstraints can be ignored [NiGa93]. In this framework, FIFO buffers are used to model register requirements. \nA graph coloring method can be applied subsequently on the obtained schedule to further decrease the \nregister requirements of the loop. This is referred to as the integrated formulation and results in rate-optimal \nschedules with minimal register usage. Subsequently, the McGill researchers extended their ftamework \nto handle resource constraints, resulting in a unified ILP formulation for the problem for simple pipelined \narchitectures ~iGa92,GoAlGa94a]. The work was subsequently generalized to more complex architectures \n[AlGoGa95]. By the spring of 1995, this work was implemented at McGill in MOST, the Modulo Scheduling \nToolset, which makes use of any one of several exter\u00adnal ILP solvers. MOST was not intended as a component \nof a pro\u00adduction compiler, but rather as a standard of comparison. Being able to generate optimal pipelined \nloops can be useful for evaluat~ ing and improving heuristics for production pipeliners (as this study \ndemonstrates). Because the McGill work is well represented in recent publica\u00adtions, we omit the details \nof the ILP formulation. The interested reader should consult [AlGoGa95] and then the other cited publi\u00adcations. \n 3.2 Integration with the MIPSpro compiler The research at McGill left many open questions. Although \nthe formulation of the software pipelining problem in ILP was appeal\u00ading, could it also be usefhl? The \nMcGill team did not believe that it could be part of a production compiler due to its exponential run \ntime, but did expect that it could be used to evaluate such a com\u00adpiler. How would it compare with a \nmore specialized heuristic implementation? It was bound to be slower, perhaps even much slower; but how \nmuch better would its results be? Because heuris\u00adtic approaches can have near linear rnming time, they \nwould cer\u00adtainly be able to handle larger loops. How much larger? MOST was not a full software pipelining \nimplementation; its out\u00adput was a set of static quality measures, principally the II of the schedule \nfound and the number of registers required, not a piece of runable code. It only targets were models \nthat exhibited certain interesting properties, never a real commercial high performance processor. How \nwell would it work with when targeted to a real processor? Were there any unexpected problems standing \nin the way of a full implementation, one that would generate runable code? The opportunity to embed MOST \nin the MIPSpro compiler seemed perfect to answer these questions. In that context, MOST would enjoy a \nproven pipelining context a full set of optimiza\u00adtion and analysis before pipelining and a robust post \nprocessing implementation to integrate the pipelined code correctly back into the program. It was particularly \nattractive to reuse the postprocess\u00ading code. Although modulo renaming, generation of pipeline fill and \ndrain code, and other related bookkeeping tasks may seem the\u00ad oretically uninteresting, they account \nfor a large part of the job of implementing a working pipeliner. In the MIPSpro compiler, this postprocessing \naccounts for 18 XOof the lines of code in the pipe\u00ad liner about 6,000 out of 33,000 lines total in the \npipeliner. MOST was successfully embedded in the MIPSpro compiler over the summer of 1995.  3.3 Adjustment \nof McGill approach Due to this study Over the course of this study, the McGill team found it necessary \nto adjust their approach in order to conduct a useful evaluation of SGI S production pipeliner. Sometimes \na rat~ptimal schedule with minimal register usage cannot be found in a reasonable amount of time. For \nthe purpose of this study, we used 3 minutes as a limit on searches for rattiptimal schedules with minimal \nreg\u00ad ister usage. Increasing this limit doesn t seem to improve the results very much. When no optimal \nschedule is found within the given time limit, it is necessary to derive a good but not necessar\u00adily \noptimal schedule. As a result, the following adjustments were made to MOST during this study: 1. Obtain \na resource-constrained schedule)rst. Using the ILP formulation of the integrated register allocation \nand schedul\u00ading problem was just too slow and unacceptably limited the size of loop that could be scheduled. \nFor example, finding a schedule for the large N3 loop in the tomcatv benchmark was far beyond the reach \nof the integrated formulation. The ILP formulation of resourc~onstiained scheduling could be solved considerably \nfaster. This formulation finds schedules satisfying resource constraints, but does not address register \nallocation. When it cannot be solved within reasonable time limits, we would also not be able to solve \nthe integrated problem, so separating the problems seems to serve as a use\u00ad ful filter against unnecessary \ncompile time usage. 2. Adjustment of the objective function Often it was not fea\u00ad sible to use the register \noptimal formulation (involving col\u00ad oring) reported in [AlGoGa95]. Instead, the ILP formulation used \nminimized the number of buffers in the sotlvvare pipe\u00adline. This objective function directly translates \ninto the reduction of the number of iterations overlapped in the steady state of the pipeline. 1 The \nILP solver for this phase was restricted to a fixed time limit. After that, it would accept the best \nsuboptimal solution found, if any. 3. Multiple priority orders One surprising result of this study was \nthe discovery that the same multiple priority order heuristics that were used in the SGI pipeliner are \nalso very useful in driving the McGill solver. The priority order in which the ILP solver traverses the \nbranch-and-bound tree is by far the most important factor affecting whether it could solve the problem. \nMOST thus adopted SGI S policy of try\u00ading many different priority orders in turn until a solution is \nfound.2 These adjustments have proved to be very important for our study, They enable MOST to generate \nR8000 code and greatly extend the size and number of loops it can successfully schedule 1. This is probably \nat least as important a parameter to optimize as reg\u00adister usage as it has a more dkect impact on the \nsize of the pipeline fill and drain code and thus on short trip count performance of the loop. Short \ntrip count performance is the only performance impact of either minimization, Only the II affects the \nasymptotic performance. 2. In fact, the final implementation of MOST embedded in the MIPSpro compiler \nused most of the code from the SGI scheduler directly, replacing only the scheduler proper with MOST. \n  4.0 Experiments and results 4.1 Effectiveness of the MIPSpro pipeliner To lend perspective to the \ncomparative results in Section 4,4, we first present some results to demonstrate that the pipelining \ntech\u00adnology in the MIPSpro compiler is very effective. We use the 14 benchmarks in the SPEC92 floating \npoint suite for this purpose, as SPEC has become an important standard of comparison. (We omit the SPEC92 \ninteger suite because our software pipeliner unfortu\u00adnately has little impact on those benchmarks.) Figure \n2 shows the results for each of the 14 SPEC92 floating point benchmarks with the software pipeliner both \nenabled and disabled, Not only is the overall result very good for a machine with a 75 MHz clock, the \neffectiveness of the pipeliner is dramatic, resulting in more than 35 ?/o improvement in the overall \nSPEC number (calculated as the geometric mean of the results on each benchmark). To be fair, this overstates \nthe case for SGI S scheduling technology because disabling software pipelining does not enable any very \neffective replacement. The MIPSpro compiler leans heavily on software pipelining for floating point performance \nand there has been little work in tuning the alternative paths. In particular, with software pipelining \ndisabled, the special inner loop optimizations and analysis described in Section 2.1 are disabled; without \npipelin\u00ading there is also no if+onversion and vector memory reference analysis, When soflware pipelining \nis disabled a fairly simple list scheduler is used. We are often asked why we consider it important to \nbe able to pipeline large loops. When a loop is large, the argument goes, there should be enough parallelism \nwithin each iteration without having to overlap iterations. Perhaps this is true, but softiarepipe\u00adlining \nas we use the term is much more than just overlapping loop iterations. Rather it is a fundamentally different \napproach to code generation for innermost loops. We are willing to spend much more compile time on them \nthan other parts of programs, We have a suite of special optimizations that we apply to them. We have \na scheduler that backtracks in order to squeeze out gaps that would spice2g6 doduc mdljdp2 wave5 tomcatv \nora alvinn ear mdljsp2 swm256 su2cor hydro2d nasa7 fPPP geomt >tric mean FIGURE 2. SPEC92 floating \npoint benchmarks for the MIPSpro compiler run on an SGI Power Challenge with 75MHz R8000 processors. \nResults are given with software pipelining both enabled and disabled. Results are given as SPECmarks, \ni.e., performance mukiples of VAX 780 mu times. tp# wave5 mdljdp2 doduc spice2g6 0.6 0.7 0.8 0.9 1.0 \n- - RHMS _ FDMS - FDNMS - HMS FIGURE 3. Three of the four priority list heuris\u00adtics help to win at least \none SPEC benchmark. Each bar gives the results of an experiment where all but one heuristic was disabled. \nThe result is reported as the ratio over the numbers reported in Figure 2. otherwise be left in the \nschedule. We have even gone so far (in this paper) as to consider au exponential scheduling ~echnique. \n 4.2 The effect of multiple scheduling priority heuristics Figure 3 shows the results of an experiment \ntesting the effective\u00adness of the multiple scheduling heuristics discussed in Section 2.7. We tried compiling \nthe SPEC benchmarks with each of the heuris\u00adtics alone. No single heuristic worked best with all the \nbench\u00admarks. In fact, three out of the four heuristics were required in order to achieve our best time \nfor at least one of the benchmarks: FDMS hydro2d FDNMS alvinn, ear, swm256 HMS tomcatv, mdljsp2 The \nfourth heuristic, RHMS, is useful for loops not important to any of the SPEC92 floating point benchmarks. \nFor several of the benchmarks, such as wave5, no single heuristic achieves a time equal to the overall \nbest time shown in Figure 2, This is because the benchmarks consist of more than one important loop, \nand within a single benchmark each loop may require a dif\u00adferent heuristic. There are also a few benchmarks, \ne.g., ear, that do better by a few percent when limited to a single heuristic than when the best schedule \nis chosen from among all four heuristics. This is less strange than it may seem at first, The MIPSpro \ncompiler uses only the II of a schedule to measure its quality and ignores some other factors that can \nhave minor impacts on the performance of a sclhed\u00adule. In particular, the overhead to start-up and complete \na 10CJPis ignored when looking for a schedule, but can be relevant to, the performance of short trip \nloops. (See Section 4.6.) hydro2d su2cor swm256 mdlj sp2 ear alvinn ora . tomcatv wave5 mdljdp2 doduc \nspice2g6 geometric mean I I FIGURE 4. Effectiveness of the MIPSpro mem\u00adory bank heuristics Performance \nimprovements from enabling these heuristics .  4.3 Effectiveness of the MIPSpro memory bank heuristics \nWe measured the effectiveness of the MIPSpro compiler s mem\u00adory bank heuristics discussed in Section \n2.9. Figure 4 shows the performance ratio for code compiled with the heuristic enabled over the same \ncode compiled with the heuristic disabled. Two benchmarks stand out as benefiting especially, alvinn \nand mdljdp2, For alvinn, the explanation is fairly simple. This program spends nearly 100\u00b0A of its time \nin two memory bound loops that process consecutive single precision vector elements. Because the R8000 \nis banked on double precision boundaries, the most natural pair\u00adings of memory references can easily \ncause memory bank stalls. In particular, one of the two critical loops is a dot product of two sin\u00adgle \nprecision vectors. Two natural memory reference patterns for this code are: v[i+O], u[i+O] v[i+ I], u[i+l] \n and v[i+O], v[i+ 1] u[i+O], u[i+l].  When (as in this case) both u and v are single precision and \neven aligned, both of these patterns batch 4 references to the same bank within two cycles, causing a \nstall. The memory bank heuristic pre\u00advents this, producing a pattern such as the following memory ref\u00aderence \npattern instead: v[i+OJ, v[i+2] u[i+O], u[i+2]  This pattern is guaranteed to reference an even and \nan odd bank in each cycle and thus to have no stall, For mdljdp2, the situation is different and more \ncomplicated, This loop is not memory bound; it has only 16 memory references out of 95 instructions. \nThe pattern of memory references is compli\u00adcated by the fact that there is a memory indirection and so \nthe exact pattern of memory references cannot be known. Inspection of the generated code reveals that \nwithout memory bank heuristics, memory references with unknowable relative offsets are grouped together \nunnecessarily. The memory bank heuristics prevent that grouping and thus avoid risking stalls by preventing \nany pairing of memory references. 4.4 Comparison between ILP and SGI S heuristics We wanted to answer \nthe question whether an optimal approach could improve performance relative to the pipeliner in the MIPSpro \ncompiler. Remember that the primaty goal of this study is to validate and improve SGI s heuristic techniques. \nThus we wanted to give the ILP approach every possible advantage to expose weaknesses in the production \ncompiler. Unfortunately, there is a problem that could strongly favor the heu\u00adristic pipeliner not every \nloop that can be scheduled by the SGI pipeliner can also be scheduled by the MOST scheduler in reason\u00adable \ntime. This is a particular problem because the penalty for not pipelining can be very high. (See Section \n4. 1.) We addressed this by using the heuristic pipeliner as a backup for MOST. Thus instead of falling \nback to the single block scheduler used when the MIPSpro pipeliner fails to schedule and register allocate \na loop, it instead falls back to the MIPSpro pipeliner itself. In theory, this should reveal only deficiencies \nin the production compiler, never in the ILP pipeliner. fPPPP nasa7 hydro2d su2cor swm256 mdlj sp2 ear \nalvim ora tomcatv wave5 mdljdp2 doduc spice2g6 >tric mean FIGURE 5. Relative performance of ILP sched\u00aduled \ncode over MIPSpro results with and without memory bank pairing heuristics  4.5 Comparison of run time \nperformance The solid bars in Figure 5 show comparative results for the SPEC floating point benchmarks. \nThis data shows the code scheduled by the SGI pipelined code outperforming the optimal code on 8 of the \nbenchmarks. The geometric mean of the suite as a whole is 8V0 better for the heuristic scheduler than \nfor ILP method. On one benchmark, alvinn, the code scheduled by MOST ran 15% slower than the code scheduled \nby the MIPSpro pipeliner. How can this be? The design of the experiment should have pre\u00advented the ILP \npipeliner from ever finding a worse schedule than could be found by MIPSpro. After all, the heuristics \nare available as a fallback when an optimal schedule cannot be found. A large part of the answer is a \ndynamic factor introduced by the memory system of the R8000 and described briefly in Section 2.9. The \nMIPSpro pipeliner uses heuristics to minimize the likelihood of unbalanced memory references causing \nstalls. Currently, the ILP scheduling formulation does not optimize for this hardware. Thus it can generate \ncode which has poorer dynamic performance than that generated by the MIPSpro scheduler. $.......... .. \n... ...................... 0.6 0.7 0.8 0.9 1.0 1.1 1.2 FIGURE 6. Relative performance of ILP sched\u00ad \nuled code over MIPSpro results for each Liver\u00admore kernel. In order to show the size of the dynamic \neffect introduced by the memory system, we report a second set of numbers. These com\u00adpare the ILP results \nto code scheduled by the MIPSpro pipeliner with its memory bank optimization disabled. These results \nare also shown in Figure 5, this time as striped bars. In this compari\u00adson, the ILP code performs within \na range from just a little bit worse to about s~o better than the MIPSpro code. (See Section 4.3 for \nthe direct comparison of MIPSpro results with and without memory bank heuristics.) For two benchmarks, \ntomcatv and alvinn, the ILP does about 5?/o better than the MIPSpro code scheduled with memory bank heuris\u00adtics \ndisabled. Are these places where it is actually generating better code, or is it just another random \nfactor introduced by the memory system? The latter is the case and the ILP code is just generating fewer \nmemory stalls for these programs by chance. We know this because the performance of both programs is \ndominated by just a few loops which: 1. are scheduled by the MIPSpro compiler at their MinII, 2. have \nvery long trip counts (300 for tomcatv, > 1000 for alvinn), and 3. are memory bound or nearly memory \nbound.  In fact these two programs were part of the original motivation for the MIPSpro pipeliner s \nmemory bank optimization and served as a benchmark for tuning it. 4.6 Short trip count performance Both \nthe SGI and the McGill techniques strive to minimize the number of registers used by software pipelined \nsteady states; how\u00adever, the emphasis and motivations are somewhat different. At SGI the motivation was \nto make schedules that could be register allo\u00adcated. Certainly this was important to the McGill team, \nbut there was the additional goal of improving performance even for pipe\u00ad lines scheduled at the same \nII. How well do the two approaches perform in this regard, and how important is it in practice? In fact \nregister usage is only one of a number of factors that can affect the time to enter and exit a pipelined \nsteady state --its over\u00adhead. This overhead is constant relative to the trip count of the loop and thus \nincreases in importance as the trip count decreases g 1;! 19.1 18.3 18,2 18.1 15.2 15.1 14.3 14.2 14.1 \n13 12 11 10 g 7 ~ : -, FIGURE 7. Difference MIPSpro minus ILP in second order static quality measures \nfor each of the loops in the Livermore kernels given as abso\u00adlute register numbers and instruction cycles \n and asymptotically disappears in importance as the trip count increases. If we ignore dynamic factors \nsuch the memory system effect discussed in the previous section, then different schedules of a loop with \nidentical 11sand different registers requirements differ at most in overhead so long as they both fit \nin the machine s avail\u00adable registers. After all, register usage can be compensated by spills and restores \nin the loop head and tail. There are other factors beside register usage that influence pipeline overhead. \nBefore the steady state can execute the first time, the pipeline has to bejilled, and after the last \nexecution of the steady state, the pipeline has to be drained. The number of instructions in the fill \nand drain code is a fimction of how deeply pipelined each instruction in the loop is and whether the \nless deeply pipelined instructions can be executed speculatively during the final few iter\u00adations of \nthe loop. How do the two schedulers compare with regard to the shod trip count performance of the loops \nthey generate? The well know Liv\u00ad ermore Loops benchmark is particularly well suited for this mea\u00ad surement. \nIt measures the performance on each of 24 floating point kernels for short, medium, and long trip counts. \nFigure 6 shows the relative performance of the two approaches on each loop for both the short and long \ntrip count cases. These results show better per\u00adformance for SGI scheduler in nearly all cases with both \nshort and long trip counts. But as we have just seen, these results can be dis\u00adtorted by the effects \nof the machine s memory system. We d like a way to make a more direct comparison. To do this, we looked \nat some static performance information about the individual loops in the benchmark. For all the loops \nin the benchmark, the schedules produced by both pipeliners had identical 11s. Figure 7 shows the relative \nperformance of the two pipeliners in terms ofi 1, register usage measured in total number of both floating \npoint and integer registers used, and 2. overall pipeline overhead, measured in cycles required to enter \nand exit the loop. Overall pipeline overhead counts directly toward performance, while register usage \nis important only is so far as it impacts pipe\u00adline overhead. This chart shows two things quite clearly: \n1. Neither scheduler produces consistent/y better schedules by either measure of overhead. The heuristic \nmethod uses fewer registers in 15 of the 26 loops and requires lower total over\u00adhead in 12. 2. There \nis no clear correlation between register usage and overhead. For 16 of the loops, the schedule with smaller \noverhead didn t use fewer registers.  (1) seems particularly surprising in light of the emphasis on \nregis\u00adter optimality at McGill, but a careful examination of Section 3.3 will shed some light. Even in \nthe best case, the McGill schedules do not have optimal register usage, but only minimal usage of the \nmodulo renamed registers or buffers, ignoring the withiwiteration register usage. Moreover, for the purposes \nof this study, scheduling and buffer minimization were often performed in separate passes so that buffer \nusage is not really optimized over all possible sched\u00adules, but only over a rather small subset. (2) \nshould not be surprising in light of the discussion above. Sav\u00ading and restoring the registers used by \nthe steady state is only one of the things that needs doing in the loop prologue and epilog. An additional \noverlapped iteration in the steady state can have a far larger effect than the use of more registers. \n  4.7 Compile speed comparison As expected, the ILP based pipeliner is much slower than the heu\u00adristic \nbased one. Of the 261 seconds spent in the MIPSpro pipe\u00adliner while compiling the 14 SPEC benchmarks, \n237 seconds are spent in the scheduler proper, with the rest spent in inner loop opti\u00admization and post-pipelining \nbookkeeping. This compares to 67,634 seconds in the ILP scheduler compiling the same code.  5.0 Conclusions \nand future work Only very rarely does the optimal technique schedule and allocate a loop at a lower II \nthan the heuristics. In the course of this study, we found only one such loop. Even in that case a very \nmodest increase in the backtracking limits of the heuristic approach equal\u00adized the situation. The heuristic \ntechnique is much more efficient than MOST. This is especially important for larger loop bodies, where \nits greater effi\u00adciency significantly extends its functionality. In our experiments, the largest loop \nsuccessfidly scheduled by the SGI technique had 116 operations, while the largest optimal schedule found \nby MOST had 61 operations. The ILP technique was not able to guarantee register optimality for many interesting \nand important loops. In order to produce results for these loops, the McGill team had to devise a heuristic \nfallback. This led to the result that neither implementation had consistently better register usage. \nSo long as a software pipelined loop actually fits in the registers, the number of registers used is \nnot an important parameter to opti\u00admize since it is not well related to performance, even for short trip \ncount loops. Future work in this direction should focus on the more complex task of minimizing overall \nloop overhead. We feel that there is much to learn in this area. Inherent in the modulo scheduling algorithm \nis a measure of how the results compare against a loose lower bound on optimality, the MinII (see Section \n2.3.) This has always given us at least a rough idea of how much room there was for improvement in the \niteration intervals of generated schedules. On the other hand, we know very little about the limits on \nthe performance of sofhvare pipelined loops when the trip count is short. Perhaps an ILP formulation \ncan be made that optimizes loop overhead more directly than by optimizing register usage. We were unable \nto compare the MIPSpro memory heuristics with code that optimally uses the machine s memory system. This \ncom\u00adparison would probably be useful for the evaluation and wing of the heuristics. Frankly, we do not \ncurrently understand how well these heuristics work compared to how well they could work. A good solution \nto this problem could have interesting implications ~n design of memory systems, This study had not produced \nany convincing evidence that there is much room for improvement in the SGI scheduling heuristics. But \nwe know there is still room for improvement in several areas, most notably performance of loops with \nshort trip counts and large loops bodies with severe register pressure. Without very signifi\u00adcant strides \ntoward a more efficient implementation of the ILP approach, we do not see how it can help with the latter \nproblem. But short trip count performance is an important problem, espe\u00adcially in light of loop nest \ntransformations such as tiling. And here we feel that the ILP approaches to software pipelining may yet \nhave a significant role to play.  Acknowledgments We wish to express our gratitude to a number of people \nand institu\u00adtions. John Hennessy and David Wall provided invaluable editorial assistance Without their \nhelp, this paper would be much harder to understand. Erik Altman, Fred Chow, Jim Dehnert, Suneel Jain, \nEarl Killian, and Dror Maydan also helped substantially with the proofreading and editing. Monica Lam \nwas the matchmaker of this project; she brought the SGI and McGill teams together in the first place. \nErik Altman, R. Govindarajan, and other people from the ACAPS lab at McGill built the MOST scheduling \ntool. Douglas Gilmore made crucial contributions to the design of the SGI sched\u00aduler, in particular the \nidea of using more than one scheduling heu\u00adristic. Ross Towle was the original instigator of software \npipelining at Silicon Graphics and the father of pratical software pipelining. We received financial \nsupport from Silicon Graphic and Canadian NSERC (Natural Science and Engineering Council). Ashfaq Munshi \nhas been primarily responsible for creating an atmosphere of great productivity and creativity at Silicon \nGraphics where the bulk of this work took place during the summer of 1995. Finally, we must thank the \nmembers of our families for their lov\u00ading support and forbearance. Bibliography [AiNi88] A. Aiken and \nA. Nicolau. Optimal loop paralleliza\u00adtions. In Proc. of the 88 SIGPLAN ConJ on Programming Lan\u00adguage \nDesign and Implementation, pp. 308-317, Atlanta, Georgia, June 1988. [AlKePoWa83] J. R. Allen, K. Kennedy, \nC. Portertield, and J. Warren. Conversion of control dependence to data dependence. In Proc. of the 10th \nAnn. ACM Symp. on Principles of Program\u00adming Languages, pp. 177-189, January 1983. [AlGoGa95] E. R. Altrnan, \nR. Givindarajan~ and G.R. Gao. Scheduling and mapping: Software pipelining m the presence of structural \nhazards. In Proc. of 95 SIGPLAN Con$ on Program\u00adming Language Design and Implementation, La Jolla, Calif., \nJune 1995. [Altman95] E. R. Altman, Optimal Sof~are P@elining with Functional Unit and Register Constraints. \nPh.D. thesis, McGill University, Montreal, Quebec, 1995. [BiKeKr94] R. Bixby, K. Kennedy, and U. Kremer. \nAutomatic data layout using O-1 integer linear programming. In Proc. of Con$ on Parallel Architectures \nand Compilation Techniques, pp. 111-122, August 1994. [BrCoKeTo89] P. Bnggs, K.D. Cooper, K. Kennedy, \nand L. Torczon. Coloring heuristics for register allocation. In Proc. of 89 S IGPLAN Con$ on Programming \nLanguage Design and Im\u00adplementation, pp. 275-284, July 1989. [Briggs92] P. Briggs. Register Allocation \nvia Graph Coloring. Ph.D. thesis, Rice University, Houston, Texas, April 1992. [DeTo93] J.C. Dehnert \nand R.A. Towle. Compiling for Cydra 5. Journal of Supercomputing, v.7, pp.1 81-227, May 1993. [Eichenberger95] \nA. E. Eichenberger, E. S. Davidson, and S, G. Abraham. Optimum modulo schedules for minimum register \nre\u00adquirements. In Proc. of 95 Intl. Corf on Supercomputing, pp. 31-40, Barcelona, Spain, July 1995. [Feautrier94] \nP. Feautrier. Fine-grained scheduling under re\u00adsource constraints. In 7th Ann. Workshop on Languages \nand Compilers for Parallel Computing, Ithaca, N.Y., August 1994. [GaNi91] G, R. Gao and Q. Ning. Loop \nstorage optimization for dataflow machines. In Proc. of 4th Ann. Workshop on Languag\u00ades and Compilers \nfor Parallel Computing, pp. 359-373, August 1991. [GaJo79] M.R. Garey and D.S. Johnson. Computers and \nIntrac\u00adtabili~: A Guideto the Theoty of NP-Completeness, W.H. Free\u00adman and Co., New York, N.Y., 1979. \n[GaSc91] F. Gasperoni and U. Schwiegelshohn, Efficient algo\u00ad rithms for cyclic scheduling. Res. Prep. \nRC 17068, IBM TJ Wat\u00ad son Res. Center, Yorktown Heights, N. Y,, 1991. [GoAlGa94a] R. Govindarajan, E. \nR. Altma:, and G. R. Gao. A framework for rate-optimal resource-constrained soi-lware pipe\u00adlining. In \nProc. of CONPAR94-VAPP VI, no. 854, Lecture Notes in Computer Science, pp. 640-651, Linz, Austria, September \n1994. [GoAlGa94b] R. Govindarajan, E. R. Altman, and G. R. Gao. Minimizing register requirements under \nresource-constrained rate-optimal software pipelining. In Proc. of the 27th Arm. Intl. Symp. on Microarchitecture, \npp. 85-94, San Jose, Calif., No\u00advember-December 1994. [Hsu94] Hsu, P., Designing the TFP Microprocessor, \nIEEE Mi\u00ad cro, April 1994, pp. 23-33. [1-Iuf193] R, A, Huff. Lifetime-sensitive modulo scheduling. In \nProc. of the 93 SIGPLAN Conf on Programming Language De\u00adsign and Implementation, pp. 258-267, Albuquerque, \nN. Mex., June 1993. [Lam88] M. Lam. Software pipelining: An effective scheduling technique for VLIW machines. \nIn Proc. of the 88 SIGPLAN Conf on Programming Lanugage Design and Implementation, pp. 318-328, Atlanta, \nGeorgia, June 1988. 10 [Larn89] M. Lam. A systolic array optimizing compiler. Kluwer Academic Publishers, \n1989. [MoEb92] S. Moon and K. Ebcioglu. An efficient resource-con\u00adstrained global scheduling technique \nfor superscalar and VIJW processors. In Proc. of the 25th Ann. Zntl. Symp. on A4icroarchi\u00adtecture, pp. \n55-71, Portland, Ore., December 1992. ~eWo88] G. Nemhauser and L. Wolsey. Integer and Combina\u00ad torial \nOptimization. John Wiley &#38; Sons, 1988. [Nemhauser94] G. Nemhauser. Theageofoptimization: Solv\u00ading \nlarge-scale real-world problems. Operations Research, 42(1):5-13, January-February 1994. ~iGa92] Q. Ning \nand G. Gao. Optimal loop storage allocation for argument-fetching dataflow machines. International Jow-\u00adnal \nof Parallel Programming, v. 21, no. 6, December 1992. ~iGa93] Q. Ning and G. Gao. A novel framework of \nregister allocation for software pipelining. Zn Conf Rec. of the 20th Ann. ACM SIGPLAN-SIGACT Symp. on \nPrinciples of Programming Languages, pp. 29-42, Charleston, S. Carolina, January 1993. [Pugh91] W, Pugh. \nThe Omega test: A fast and practical integer programming algorithm for dependence analysis. In Proc, \nSW percomputing 91, pp. 18-22, November 1991. [RaG181] B. R. Rau and C. D. Glaser. Some scheduling tec\u00adniques \nand an easily schedulable horizontal architecture for high performance scientific computing. In Proc. \nof the 14 Ann. Mi\u00adcroprogramming Workshop, pp. 183-198, Chatham, Mass,, Clc\u00adtober 1981, [RaFi93] B. R. \nRau and J.A. Fisher. Instruction-level parallel processing: History, overview and perspective. Journal \nof Su\u00adpercomputing, v.7, pp.:9-50, May 1993. [Rau94] B.R. ~au. Iterative modulo scheduling: An algorithm \nfor software pipelining loops. In Proc. of the 27th Ann. Intl. Symp. on A4icroarchitecture, pp. 63-74, \nSan Jose, Calif., No\u00advember-December 1994. [Tou84] R.F. Touzeau. A FORTRAN compiler for the FPS-164 scientific \ncomputer. In Proceedings of the SIGPLAN 84 Sympo\u00adsium on Compiler Construction, pages 48-57, Montreal, \nQue\u00adbec, June 17 22, 1984. [Warter92] N.J. Warter, John W. Bockhaus, Grant E. Haab, and K. Supraliminal. \nEnhanced modulo scheduling for loops with conditional branches. In Proc. of the 25th Ann. Intl. Symp. \non Microarchitecture, pp. 170-179, Portland, Ore., December 1992. \n\t\t\t", "proc_id": "231379", "abstract": "This paper is a scientific comparison of two code generation techniques with identical goals --- generation of the best possible software pipelined code for computers with instruction level parallelism. Both are variants of <i>modulo scheduling</i>, a framework for generation of software pipelines pioneered by Rau and Glaser [RaG181], but are otherwise quite dissimilar.One technique was developed at Silicon Graphics and is used in the MIPSpro compiler. This is the production compiler for SGI's systems which are based on the MIPS R8000 processor [Hsu94]. It is essentially a branch--and--bound enumeration of possible schedules with extensive pruning. This method is heuristic because of the way it prunes and also because of the interaction between register allocation and scheduling.The second technique aims to produce optimal results by formulating the scheduling and register allocation problem as an integrated integer linear programming (<i>ILP</i><sup>1</sup>) problem. This idea has received much recent exposure in the literature [AlGoGa95, Feautrier94, GoAlGa94a, GoAlGa94b, Eichenberger95], but to our knowledge all previous implementations have been too preliminary for detailed measurement and evaluation. In particular, we believe this to be the first published measurement of runtime performance for ILP based generation of software pipelines.A particularly valuable result of this study was evaluation of the heuristic pipelining technology in the SGI compiler. One of the motivations behind the McGill research was the hope that optimal software pipelining, while not in itself practical for use in production compilers, would be useful for their evaluation and validation. Our comparison has indeed provided a quantitative validation of the SGI compiler's pipeliner, leading us to increased confidence in both techniques.", "authors": [{"name": "John Ruttenberg", "author_profile_id": "81100598607", "affiliation": "Silicon Graphics Inc., 2011 N. Shoreline Blvd., Mountain View, CA", "person_id": "P145817", "email_address": "", "orcid_id": ""}, {"name": "G. R. Gao", "author_profile_id": "81100134147", "affiliation": "McGill University - School of Computer Science, 3480 University St., McConnell Building, Room 318, Montreal, Canada H3A2A7", "person_id": "PP39072129", "email_address": "", "orcid_id": ""}, {"name": "A. Stoutchinin", "author_profile_id": "81408596999", "affiliation": "McGill University - School of Computer Science, 3480 University St., McConnell Building, Room 318, Montreal, Canada H3A2A7", "person_id": "PP31096279", "email_address": "", "orcid_id": ""}, {"name": "W. Lichtenstein", "author_profile_id": "81100312622", "affiliation": "Silicon Graphics Inc., 2011 N. Shoreline Blvd., Mountain View, CA", "person_id": "P294345", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/231379.231385", "year": "1996", "article_id": "231385", "conference": "PLDI", "title": "Software pipelining showdown: optimal vs. heuristic methods in a production compiler", "url": "http://dl.acm.org/citation.cfm?id=231385"}