{"article_publication_date": "05-01-1996", "fulltext": "\n TIL: A Type-Directed Optimizing Compiler for ML D. Tarditi, G. Morrisett, P. Cheng, C. Stone, R. Harper, \nand P. Lee School of Computer Science Carnegie Mellon University 5000 Forbes Avenue Pittsburgh, PA 15213-3891 \nIntroduction We are investigating a new approach to compiling Standard ML (SML) based on four key technologies: \ninterwional poly\u00admorphism [23], nearlg tag-free garbage collection [12, 46, 34], conventional functional \nlanguage optimization, and loop op\u00ad timization. To explore the practicality of our approach, we have \nconstructed a compiler for SML called TIL, and are thus far encouraged by the results: On DEC ALPHA work\u00adstations, \nprograms compiled by TIL are roughly three times faster, do one-fifth the total heap allocation, and \nuse one\u00adhalf the physical memory of programs compiled by SML of New Jersey (SML/NJ). However, our results \nare still pre\u00adliminary we have not yet investigated how to improve compile time; TIL takes about eight \ntimes longer to compile programs than SML/NJ. Also, we have not yet implemented the full module system \nof SML, although we do provide sup\u00adport for structures and separate compilation. Fb.mlly, we expect the \nperformance of programs compiled by TIL to im\u00adprove significantly as we tune the compiler and implement \nmore optimizations. Two key issues in the compilation of advanced languages such as SML are the presence \nof garbage collection and type variables. Most compilers use a universal representation for values of \nunknown or variable type. In particular, values are forced to fit into a tagged machine word; values \nlarger than a machine word are represented as pointers to tagged, heap\u00ad allocated objects. This approach \nsupports fast garbage col\u00ad lection and efficient polymorphic functions, but can result in inefficient \ncode when types are known at compile time. Even with recent advances in SML compilation, such as Leroy \ns representation analysis [28], values must be placed in a uni\u00adversal representation before being stored \nin updateable data This research was sponsored in part by the Advanced Research Projects Agency CSTO \nunder the title The Fox Project: Advanced Languages for Systems Software , ARPA Order No. C533, issued \nby ESC/ENS under Contract No, F19628-95-C-0050, and in part by the National Science Foundation under \nGrant No. CCR-9502674, and in part by the Isaac Newton Institute for Mathematical Sciences, Cam\u00adbridge, \nEngland. David Tarditi was also partly supported by an AT&#38;T Bell Labs PhD Scholarship. The views \nand conclusions con\u00adtained in this document are those of the authors and should not be in\u00adterpreted ss \nrepresenting official policies, either expressed or implied, of the Advanced Research Projects Agency, \nthe U.S. Government, the National Science Foundation or AT&#38;T. Permissionto make digitablmrd copy \nof part or all of this work for personal of dassroorn use is ranted without fee provided that oopiee \nare not made ie, the copyright notice, the or distributed for pm t or commercial dvanta title of the \npubliition end ik date appear, en i notice is given that copyingi$bypefmi$$km OfACM,Ino. TO COPYOthSfWiSS, \nto republish,to postonservers,ortoIwfietributetoMs, requiresprforspeoifiopermission antior a fee. structures \n(e.g., arrays) or recursive data structures (e.g., lists). hztensional polymorphism and tag-free garbage \ncollection eliminate the need to use a universal representation when compiling polymorphic languages. \nTIL uses these technolo\u00adgies to represent many data values naturally . For ex\u00adample, TIL provides tag-free, \nunallocated, word-sized in\u00adtegers; aligned, unboxed floating-point arrays; and unallo\u00adcated multi-argument \nfunctions. These natural representa\u00adtions and calling conventions not only improve the perfor\u00admance of \nSML programs, but also allow them to interoperate with legacy code written in languages such as C and \nFortran. When types are unknown at compile time, TIL may produce machine code which is slower and bigger \nthan conventional approaches, This is because types must be constructed and passed to polymorphic functions, \nand polymorphic functions must examine the types at run-time to determine appropri\u00adate execution paths. \nHowever, when types are known at compile time, no overhead is incurred to support polymor\u00adphism or garbage \ncollection. Because these technologies make polymorphic functions slower, it becomes important to eliminate \nas many polymor\u00adphic functions at compile time as is possible. Mining and uncurrying are well-known techniques \nfor eliminating poly\u00admorphic and higher-order functions. We have found that for the benchmarks used here, \nthese techniques eliminate all polymorphic functions and all but a few higher-order func\u00adtions when programs \nare compiled as a whole. We have also found that applying traditional loop op\u00adtimization to recursive \nfunctions, such as common sub\u00adexpression elimination and invariant removal, is important. In fact, these \noptimization reduce execution time by a me\u00addian of 39%. An important property of TIL is that all optimizations \nand the key transformations are performed on typed inter\u00admediate languages (hence the name TIL). Maintaining \ncor\u00adrect type information throughout optimization is necessary to support both intensional polymorphism \nand garbage col\u00adlection, both of which require type information at run time. By using strongly-typed \nintermediate languages, we ensure that type information is maintained in a principled fash\u00adion, instead \nof relying upon ad hoc invariants. In fact, us\u00ading the intermediate forms of TIL, an [untrusted compiler \ncan produce fully optimized intermediate code, and a client can automatically verify the type integrity \nof the code. We have found that this ability has a strong engineering benefit: type-checking the output \nof each optimization or transfor\u00admation helps us identify and eliminate bugs in the compiler. PLDI 965J \n96 PA, USA Q 19W ACM O-39731 -7WWWOO05...$505O In the remainder of this paper, we describe the technolo\u00adgies \nused by TIL in detail, give an overview of the structure of TIL, present a detailed example showing how \nTIL com\u00adpiles ML code, and give performance results of code pro\u00adduced by TIL. 2 Overview of the Technologies \nThis section contains a high-level overview of the technolo\u00adgies we use in TIL. 2.1 Intentional Polymorphism \nIntensional polymorphism [23] eliminates restrictions on data representations due to polymorphism, separate \ncompilation, abstract datatypes, and garbage collection. It also supports efficient calling conventions \n(multiple arguments passed in registers) and tag-free polymorphic, structural equality. With intensional \npolymorphism, types are constructed and paased as values at run time to polymorphic functions, and these \nfunctions can branch based on the types. For example, when extracting a value from an array, TIL uses \na typecase expression to determine the type of the array and to select the appropriate specialized subscript \noperation: fun sub[a] (x:a array, i: int) = typecase a of int => intsub(x, i) I float => floatsub(x, \ni) I ptr(~) => ptrsub(x, i) If the type of the array can be determined at compile-time, then an optimizer \ncan eliminate the t ypecase: sub[float] (a, 5) + floatsub(a, 5) However, intensional polymorphism comes \nwith two costs, First, we must construct and pass representations of t ypes to polymorphic functions \nat run time, Furthermore, we must compile polymorphic functions to support any possible rep\u00adresentation \nand insert typecase constructs to select the ap\u00adpropriate code paths. Hence, the code we generate for \npoly\u00admorphic functions is both bigger and slower, and minimizing polymorphism becomes quite important. \nSecond, in order to use type information at run time, for both intensional polymorphism and tag-free \ngarbage collec\u00adtion, we must propagate types through each stage of compi\u00adlation. To address this second \nproblem, almost all compila\u00adtion stages, including optimization and closure conversion, are expressed \nas type-directed, type-preserving translations to strongly-typed intermediate languages. The key difficulty \nwith using typed intermediate lan\u00adguages is formulating a type system that is expressive enough to statically \ntype check terms that branch on types at run time, such as sub. The type system used in TIL is based \non the approach suggested by Harper and Morrisett [23, 33]. Types themselves are represented as expressions \nin a simply\u00adtyped A-calculus extended with an inductively generated base kind (the monotypes), and a \ncorresponding induction elimination form. The induction elimination form is es\u00adsentially a Typecase at \nthe type level; this allows us to write type expressions that track the run-time control flow of term-level \ntypecase expressions. Nevertheless, the type system used by TIL remains both sound and decidable. This \nimplies that at any stage during optimization, we can auto\u00admatically verify the type integrity of the \ncode. 2.2 Conventional and Loop-Oriented Opti\u00admization Program optimization is crucial to reducing the \ncost of in\u00adtentional polymorphism, improving loops and recursive func\u00adtions, and eliminating higher-order \nand polymorphic func\u00adtions. TIL employs optimizations found in conventional functional language compilers, \nincluding irdining, uncurry \u00ading, dead-code elimination, and constant-folding. In addi\u00adtion, TIL does \na set of generalized loop-oriented optimiza\u00adtion to improve recursive functions. These optimizations \nin\u00adclude common-subexpression elimination, invariant removal, and array-bound check removal. In spite \nof the large num\u00adber of different optimizations, each optimization produces type-correct code. TIL applies \noptimizations across entire compilation units. This makes it more likely that inlining and uncurrying \nwill eliminate higher-order functions, which are likely to interfere with the loop-orient ed opt imitations. \nSince the optimiza\u00adtion are applied to entire compilation units (which may be whole programs), we paid \nclose attention to algorithmic effi\u00adciency of individual optimization passes. Most of the passes have \nan O(N logiV) worst-case asymptotic complexity (ex\u00adcluding checking types for equality ), where N is \nprogram size.  2.3 Nearly Tag-l?ree Garbage Collection Nearly tag-free garbage collection uses type \ninformation to eliminate data representation restrictions due to garbage collection. The basic idea is \nto record enough represen\u00adtation information at compile time so that, at any point where a garbage collection \ncan occur, it is possible to deter\u00admine whether or not values are pointers and hence must be traced by \nthe garbage collector. Recording the information at compile time makes it possible for code to use untagged \nrepresentations. Unlike so-called conservative collectors (see for example [10, 14]), the information \nrecorded by TIL is sufficient to collect all unreachable objects. Collection is nearly tag-free because \ntags are placed onlv on heamallocated data structures (records and arrays); values in re-~sters, on the \nstack, and within data structures remain tagless. We construct the tags for monomorphic records and arrays \nat compile time. For records or arrays with unknown component types, we may need to construct tags partially \nat run time. As with other polymorphic oper\u00adations, we use intensional polymorphism to construct these \ntags. Registers and components of stack fi-ames are not tagged. Instead, we generate tables at compile \ntime that describe the layout of registers and stack frames. We associate these tables with the addresses \nof call sites within functions at compile time. When garbage collection is invoked, the col\u00adlector scans \nthe stack, using the return address of each frame as an index into the table. The collector looks up \nthe lay\u00adout of each stack-frame to determine which stack locations to trace. We record additional liveness \ninformation for each variable to avoid tracing pointers that are no longer needed. This approach is well-understood \nfor monomorphic lan\u00adguages requiring garbage collection [12]. Following Tolmach [46], we extended it \nto a polymorphic language as follows: when a variable whose type is unknown is saved in a stack frame, \nthe type of the variable is also saved in the stack frame. However, unlike Tolmach, we evaluate substitutions \n 182 of ground types for type variables eagerly instead of lazily. This is due in part for technical \nreasons (see [33, Chapter 7]), and in part to avoid a class of space leaks that might result with lazy \nsubstitution. 3 Compilation Phases of TIL Figure 1 shows the various compilation phases of TIL. The phases \nthrough and including closure conversion use a typed intermediate language, The phase after closure conversion \nuse an untyped language where variables are annotated with garbage collection information. The low-level \nphases of the compiler use languages where registers are annotated with garbage collection information. \nThe following sections describe the phases of TIL and the intermediate languages they use in more detail. \n3.1 Front-end The first phase of TIL uses the front-end of the ML Kit compiler [8] to parse and elaborate \n(type check) SML source code. The Kit produces annotated abstract syntax for all of SML and then compiles \na subset of this abstract syntax to an explicitly-typed core language called Lambda. The com\u00adpilation \nto Lambda eliminates pattern matching and various derived forms. We extended Lambda to support signatories, \nstructures (modules), and separate compilation. Each source module is compiled to a Lambda module with \nan explicit list of imported modules and their signatures. Imported signa\u00adtures may include transparent \ndefinitions of types defined in other modules; hence TIL supports a limited form of translu\u00adcent [22] \nor m cmijest types[29]. Currently, the mapping to Lambda does not handle signatures, nested structures, \nor functors. In principle, however, all of these constructs are supported by TIL s intermediate languages. \n3.2 Lmli and Type-Directed Optimization ML 23], is an intensiOndlY polyrnor- Lmli, which stands for ~, \n[ phic language that provides explicit support for construct\u00ading, passing, and analyzing types at run-time. \nWe use these constructs in the translation of Lambda to Lmli to provide efficient data representations \nfor user-defined datatypes, multi\u00adargument functions, tag-free polymorphic equality, and spe\u00adcialized \narrays. After the conversion from Lambda to Lmli, TIL performs a series of type-directed optimizations. \nSML provides only single-argument functions; multiple arguments are passed in a record. The first optimization, \nargument flattening, trans\u00adlates each function which takes a record as an argument to a function which \ntakes the components of the record as mul\u00adtiple arguments. These arguments are passed in registers, avoiding \nallocation to create the record and memory op\u00aderations to access record components. If a function takes \nan argument of variable type a, then we use t ypecase to determine the proper calling convention, according \nto the instantiation of a at run time. As with functions, datatype constructors in SML take a single \nargument. For example, the cons data construc\u00adtor (: : ) for an a list takes a single record, consisting \nof an a value and an a list value. Naively, such a construc\u00ad tor i. represented a= a pair conakking of \na tag (e.g., cons), and a pointer to the record containing the CYvalue and the a list value. The tag \nis a small integer value used to distin\u00adguish among the constructors of a datatype (e.g., nil vs. : :). \nConstructor flattening rewrites all constructors that take records as arguments so that the components \nof the records are flattened. In addition, constructor flattening eliminates tag components when they \nare unneeded. For example, cons applied to (hd, tl ) is simply represented as a pointer to the pair (hd, \nt 1), since such a pointer can always be distinguished from nil. If the constructor takes an argu\u00adment \nof unknown type, then we use t ypecase to determine the proper representation, according to the instantiation \nof a at run time. Because lists are used often in SML, the SML/NJ com\u00adpiler also flattens cons cells \n(and other constructors). How\u00adever, in violation of the SML Definition [31], SML/NJ pre\u00advents programmers \nfrom abstracting the type of these con\u00adstructors, in order to prevent representation mismatches be\u00adtween \ndefinitions of abstract datatypes and their uses [3]. In cent rast, TIL supports fully abstract datat \nype components, but uses intentional polymorphism to determine representa\u00adtions of abstract datatypes, \npotentially at run time. In addition to specializing calling conventions and datatypes, the conversion \nfrom Lambda to Lmli makes polymorphic equality explicit as a term in the language. Also, arrays are specialized \ninto one of three cases: int arrays, float arrays, and pointer arrays. Intentional polymorphism is used \nto select the appropriate creation, subscript, and update oper\u00adations for polymorphic arrays. Finally, \nTIL boxes all floating point values, except for values stored in floating-point arrays. We chose to box \nfloats to make record operations faster, since typical SML code manipulates many records but few floats. \nThe is\u00adsue is that floating-point values are 64 bits, while other scalars and pointers are 32 bits. If \nfloats were unboxed, then record offset calculations could not always be done at com\u00adpile time. Fortunately, \nthe optimizer later eliminates un\u00adnecessary box/unbox operations during the constant-folding phase, so \nstraight-line floating point code still runs fast. In all, the combination of t ype-directed optimizations \nre\u00adduce running times by roughly 40% and allocation by 50% [33, Chapter 8]. However, much of this improvement \ncan be realized by other techniques; For example, SML/NJ uses Leroy s unboxing technique to achieve comparable \nimprove\u00adments for calling conventions [42]. The advantage of our approach is that we use a single mechanism \n(intentional polymorphism) to specialize calling conventions, flatten con\u00adstructors, unbox floating-point \narrays, and eliminating tags for both polymorphic equality and garbage collection. 3.3 Optimizations \nTIL employs an extensive set of optimizations. The opti\u00admization include most of those typically done \nby compilers for functional languages. They also include loop-oriented optimizations, such as invariant \nremoval, applied to recur\u00adsive functions. TIL first translates Lmli to a subset of Lmli called Bform. \nBform, based on A-Normal-Form [18], is a more regular in\u00adtermediate language than Lmli that facilitates \noptimization. The translation from Lmli names all intermediate computa\u00adtions and binds them to variables \nby a let-construct. It also names all potentially heap-allocated values, including strings, records and \nfunctions. Finally, it allows nested . .4 parse, elaborate, eliminate .o Front end ** pattern matching \n.* .* e , introduce intensional polymorphism, .** Conversion to Lmli Z . choose data representations \nL/ , 4 ** ,e  * flatten args, Typed intermediate flatten constructors, languages box floats 1 88 r\\ \n88 do inlining, uncurrying, constant\u00ad ~ Conventional and \\ . folding, CSE, invariant removal, etc. close \nfunctions, choose environment representations   v calculate gc info for variables, choose representation \nfor types choose machine representation for variables, introduce tagging for records and arrays R@tersa~:ted\u00ad \nwith gc info do graph-coloring register allocation, construct tables for gc ****.e Figure 1: Phases \nof the TIL compiler 184 let expressions only within switches (branch expressions). Hence, the translation \nfrom Lmli to Bform linearizes and names nested computations and values. After translation to Bform, TIL \nperforms the following conventional transformations: alpha-conversion: Bound variables are uniquely re\u00adnamed. \ndead-code elimination: unreferenced, pure expres\u00ad sions and functions are eliminated. uncurrying: Curried \nfimctions are transformed to multi-argument functions, when possible. constant folding: Arithmetic operations, \nswitches, and typecases on constant values are reduced, as well as projections from known records. sinking: \nPure expressions used in only one branch of a switch are pushed into that branch. However, such expressions \nare not pushed into function definitions. inlining: Non-escaping functions that are called only once \nare always inlined. Small, non-recursive functions are inlined in a bottom-up paas. Recursive finctions \nare never (directly) irdined. inlining switch cent inuations: The continuation of a switch is inlined \nwhen all but one brzmch raises an exception. For example, the expression let x = if y then e2 else raise \nes in ed end is transformed to if y then let x = ez in ed end else raise es. This makes expressions in \nez available within ed for optimizations like common sub-expression elimination. minimizing fix: Mutually-recursive \nfunctions are bro\u00adken into sets of strongly connected components. This improves irdining and dead code \nelimination, by sepa\u00adrating non-recursive and recursive functions. In addition to these standard functional \nlanguage trans\u00ad formations, TIL also applies loop-oriented optimization to recursive functions: . common \nsubexpression elimination (CSE): Given an expression let x =el in ez end if el is pure or the only effect \nit may have is to raise an exception, then all occurrences of el in ez are replaced with x. The only \nexpressions that are excluded from CSE are side-effecting expressions and fimction calls. . eliminating \nredundant switches: Given an expres\u00adsion let x=if zthen let y Gif z ttien el else e2 in ... the nested \nif statement is replaced by el, since z is always true at that point. . invariant removal: Using the \ncfl maDh, we calcu\u00adlate the nesting depth of eac~ function: (Nesting-depth is analogous to loop-nesting \ndepth in languages like C.) TIL assigns a let-bound variable and the expression it binds a nesting depth \nequal to that of the nearest enclosing function. For every pure expression e, if all free variables of \ne have a nesting depth less than e, TIL moves the definition of e right after the definition of the free \nvariable with the highest lexical nesting depth. hoisting: All constant expressions are hoisted to the \ntop of the program. An expression is a constant ex\u00adpression if it uses only constants or variables bound \nto constant expressions. eliminat ine redundant comparisons: A set of sim\u00adple arithmetic relations of \nthe f~rm x < g is propagated top-down through the program. A rule-of-signs ab\u00adstract interpretation is \nused to determine signs of vari\u00adables. This information is used to eliminate array\u00adbounds checks and \nother tests. TIL applies the optimizations as follows: first, it per\u00adforms a round of reduction optimizations, \nincluding dead\u00adcode elimination, constant folding, inlining functions called once, CSE, eliminating redundant \nswitches, and invariant removal. These optimization do not increase program size and should result in \nfaster code. R iterates these optimiza\u00adtion until no further reductions occur. Then it performs switch-continuation \ninlining, sinking, uncurrying, compar\u00adison elimination, fix minimizing, and inlining. The entire process, \nstarting with the reduction optimization, is iter\u00adated two or more times. 3.4 Closure conversion TIL \nuses a type-directed, abstract closure conversion in the style suggested by Minamide, Morrisett, and \nHarper [32] to convert Lmli-Bform programs to to Lmli-Closure programs. Lmli-Closure is an extension \nof Lmli-Bform that provides constructs for explicitly constructing closures and their en\u00advironments. \nFor each escaping Bform function, TIL generates a closed piece of code, a type environment, and a value \nenvironment. The code takes the free type variables and free value vari\u00adables of the original function \naa extra arguments. The types and values corresponding to these free variables are placed in records. \nThese records are paired with the code to form an abstract closure. TIL uses a flat environment representation \nfor type and value environments [5]. For known functions, TIL generates closed code but avoids creating \nenvironments or a closure. Following Kranz [27], we modify the call sites of known functions to paas \nfree variables as additional arguments. TIL closes over only variables which are function argu\u00adments \nor are bound within functions. The locations of other top-level variables are resolved at compile-time \nthrough traditional linking, so their values do not need to be stored in a closure. 3.5 Conversion to \nan untyped language To simplify the conversion to low-level assembly code, TIL translates Lmli-Closure \nprograms to an untyped language called Ubform. Ubform is a much simpler language than Lmli, since similar \ntype-level and term-level constructs are collapsed to the same term-level constructor. For exam\u00adple, \nin the translation from Lmli-Closure to Ubform, TIL replaces type case with a conventional switch expression. \nThis simplifies generation of low-level code, since there are many fewer cases. TIL annotates variables \nwith representation information that tells the garbage collector what kinds of values variables must \ncontain (e.g., point ers, integers, floats, or pointers to code). The representation of a variable z \nmay be unknown at compile time, in which case the representation informa\u00adtion is the name of the variable \ng that will contain the type of z at run time. 3.6 Conversion to RTL Next TIL converts Ubform programs \nto RTL,a register-transfer language similar to ALPHA or other RISC-style assembly language. RTL provides \nan infinite number of pseudo-registers each of which is annotated with representation informa\u00adtion. Representation \ninformation is extended to include locatives, which are pointers into the middle of objects. Pseudo-registers \ncontaining locatives are never live across a point where garbage collection can occur. RTL also provides \nheavy-weight function call and return mechanisms, and a form of inter-procedural goto for implementing \nexceptions. The conversion of Ubform to RTL decides whether Ub\u00adform variables will be represented as \nconstants, labels, or pseudo-registers. It also eliminates exceptions, inserts tag\u00adging operations for \nrecords and arrays, and inserts garbage collection checks. 3.7 Register allocation and assembly Before \ndoing register allocation, TIL converts RTL programs to ALPHA assembly language with extensions similar \nto those for RTL. Then TIL uses conventional graph-coloring register allocation to allocate physical \nregisters for the pseudo\u00adregisters. It also generates tables describing layout and garbage collection \ninformation for each stack frame, as de\u00adscribed in Section 2.3. Finally, TIL generates actual ALPHA assembly \nlanguage and invokes the system assembler, which does instruction scheduling and creates a standard object \nfile. 4 An example This section shows an ML function as it passes through the various stages of TIL. \nThe following SML code defines a dot product function that is the inner loop of the integer matrix multiply \nbenchmark: val sub2 : ) a array2 * int * int -> a fum dot (cnt ,sum) = M cnt <bound then let val som \n=sum+sub2 (A, i, cnt) *sub2(B ,cnt, j) in dot (cnt+l ,sum~ ) end else sum The function sub2 is a built-in \n2-d array subscript function which the front end expands to fun sub2 ({columns ,rows, v} , s : int, t: \nint) = if s <0 orelse s>=rows orelse t<O orelse t>=columns then raise Subscript else unsafe.subl(v, s \n* columns + t) Figures 2 through 7 show the actual intermediate code created as dot and sub2 pass through \nthe various stages of TIL. For readability, we have renamed variables, erased type information, and performed \nsome minor optimizations, such as eliminating selections of fields from known records, Figure 2 shows \nthe functions after they have been con\u00adverted to Lmli. The sub2 function takes a type as an ar\u00adgument. \nA function parameterized by a type is written as At., while a function parameterized by a value is written \nas Ji. In the dot function, the sub2 function is first applied to a tgpe and then applied to its actual \nvalues. Each function takes only one argument, often a record, from which fields are selected. The quality \nof code at this level is quite poor: there are eight function applications, four record construc\u00adtions, \nand numerous checks for array bounds, Figure 3 shows the Lmli fragment after it has been con\u00advert ed \nto Lmli-Bform. Fhnctions have been transformed to take multiple arguments instead of records and every \ninter\u00admediate e computation is named. Figure 4 shows the Lmli-Bform fragment after it has been optimized. \nAll the function applications in the body of the loop have been eliminated. psub-ai (av, a) is an applica\u00adtion \nof the (unsafe) integer array subscript primitive. All of the comparisons for array bounds checking have \nbeen safely eliminated, and the body of the loop consists of 9 expres\u00adsions. This loop could be improved \neven further; we have yet to implement any form of strength reduction and induction variable elimination. \nFigure 5 shows the Lmli-Bform fragment after it has been convert ed to Ubform. Each variable is now annotated \nwith representation intormatio n, to be used by the garbage col\u00adlector. INT denotes integers and TRACE \ndenotes pointers to tagged objects. The function is now closed, since it was closure converted before \nconverting to Ubform. Figure 6 shows the Ubform fragment after it has been converted to RTL. Every pseudo-register \nis now annotated with precise representation information for the collector. The representation information \nhas been extended to in\u00adclude LOCATIVE, which denotes point ers into the middle of tagged objects. Locatives \ncannot be live across garbage\u00adcollection points. The (*) indicates points when-e the psub.ai primitive \nhas been expanded to two RTL instructions. This indicates that induction-variable elimination would also \nbe profitable at the RTL level, The return instruction s operand is a pseudo-register containing the \nreturn address. Figure 7 shows the actual DEC ALPHA assembly lan\u00adguage generated for the dot function. \nThe code between Li and L3 corresponds to the RTL code. The other code i= epilo=e and prolowe cede f~r \nenterka and .xitk~ the function. Note that no tagging operations occur anywhere in this function. 5 \nPerformance In this section, we compare the performance of programs compiled by TIL against programs \ncompiled by the SML/NJ sub2 = let fix f =Aty . let fix g = hrg. let a = (#O arg) s = (#l arg) t = (*2 \nerg) columns = (#O a) rows = (#l a) v = (#2 a) check = let testl = plst.i(s in Switch-enum test ,0) of \nfix dot = Acnt ,sum. let test r = = plst-i (cnt ,bound) Switch-enum test of 1 => A. 1 => IO=>A. A.enum(l) \nlet a b = = tl + cnt psubai(av ,a) let in test2 = pgte.i(s, Switch-enum test2 1 => A.enum(l) IO=>A. rows) \nof L e f = = = columns * cnt j+c psub-ai(bv, d) b*e let in I end test3 = plst.i(t ,0) Switch.enum test3 \nof 1 => J.enum(l) O => ~. pgte-i (t, columne) I in end O => E = sum+f h = l+cnt i = dot(h i A.sum ,g) \nin end end Switch.enum check of in end r 1 => A. raise Subscript end 1 0 => A .unsafe-subi [tY] {v, t \n+ s  colu~s} Figure 4: Lmli-Bform after optimization in g in f end fix dot= Ai let cnt = (*O i) sum \n= (M i) d = plst-i(cnt ,bound) in Switch.enum d of 1 => A.let eumy = sum + ((sub2 Cintl) {A, i,cnt}) \n* ((sub2 [int] ) {B,cnt, j}) in dot{cnt+l, sum~} fix dot = end Abound: INT, columns: IIJT ,bv :TRACE \n,av: TRACE, tl: IMT, I o => A.sum j: IIJT,cnt: IBT, sum:IET. end let test: IMT = pgtt-i(bound, cnt) r: \nIIJT = Switchint test of Figure 2: After conversion to Lmli 1 => let a: I?JT = tl + cnt sub2= fix dot \n. . . = Acnt, eum. let test = plst.i (cnt, bound) r . Switch.enum test of 1 => A+ let a = sut12 [intl \nb = a(A, i,cnt) c = sub2 Lint] d = c(B, cnt, j) e = &#38;d f= sum+e g= cnt+l h = dot(g, f) in end b: \nIMT c: I?JT d:lET e: IIJT f:IBT g: IET h: IMT i:IUT in i end I O => sum r : IMT = = = = = = = = psub-ai(av \ncolumns * j+c peubai(bv, b*e SUel+f l+cnt dot (bound, av, tl, ,a) cnt d) columns j,h, g) ,bv, in h I \nend O => A.sum Figure 5: After conversion to Ubform in r end Figure 3: Lmli-Bform before optimization \n dot ( ( [bound(IMT) , columns (I?JT) ,bv(TRACE) , av(TRACE) ,tl(IIJT) ,j(IHT) ,cnt(Il!JT) , sum(IMT)l \n.rl)) { LO: &#38; b;ii; (IIiT) , cnt(IIJT) , test(IET) ;ne test(IIJT) ,Ll mv sum(IET) ,result (IHT) br \nL2 L1 : addl tl(IET) , cnt(INT) , a(IMT) (*) s4add a(IMT) , av(TRACE) , t2 (LOCATIVE) (*) ldl b(Il!T) \n, O(t2(LOCATIVE) ) mull columns (IIJT) , cnt(I!JT) , c (IMT) addl j(IIiT) , c(IIJT) , d (IMT) (*) s4add \nd (IIJT) , bv(TRACE) , t3(LllCATIVE) (*) ldl e (IIJT) , o(t3 (LOCATIVE)) mull/v b (IBT) , e (IMT) , -f(rMT) \naddl/v sum(IIIT) , f (IET) , g (IMT) addl/v cnt(IMT) , 1 , h (IHT) t rapb mv h (IMT) ,cnt(IllT) mv g \n(I MT) ,sum(IHT) br LO L2 : return retreg(LABEL) } Figure 6: After conversion to RTL compiler. We measure \nexecution time, heap allocation, phys\u00adical memory requirements, executable size, and compile time. We \nalso measure the effect of loop optimizations. Further performance analysis of TIL appears in Morrisett \ns [33] and Tarditi s theses [45]. 5.1 Benchmarks Table 1 describes the benchmark programs, which range \nin size from 62 lines to about 2000 lines of code. Some of these programs have been used previously for \nmeasuring ML per\u00adformance [5, 16]. The benchmarks cover a range of appli\u00adcation areas including scientific \ncomputing, list-processing, systems programming, and compilers. We compiled the programs as single closed \nmodules. For Lexgen and Simple, which are standard benchmarks [5], we eliminated functors by hand because \nTIL does not yet sup\u00adport the full SML module language. Because whole pro\u00adgrams were given to the compiler, \nwe found that the opti\u00admizer naturally eliminated all polymorphic fwctions. Con\u00adsequently, for this benchmark \nsuite, there was no run-time cost to support intensional polymorphism. We extended the built-in ML types \nwith safe 2-dimensional arrays. The 2-d array operations do bounds checking on each dimension and then \nuse unsafe l-d array operations. Arrays are stored in column-major order. 5.2 Comparison against SML/NJ \nWe compared the performance of TIL against SML/NJ in several dimensions: execution time, total heap allocation, \nphysical memory footprint, the size of the executable, and compilation time. For TIL, we compiled programs \nwith all optimizations enabled. For SML/NJ, we compiled programs using the de\u00adfault optimization settings. \nWe used a recent internal release of SML/NJ (a variant of version 1.08), since it produces code that \nis about 35% faster than the current standard release (0.93) of SML/NJ [41]. . ent Lv2851-dot-205965 \n# arguments : [$bound, $01 [$columns, $11 [$bv, $21 # [$av, $31 [$tl ,$41 [$J ,$51 # [$cnt, $6] [$sum, \n$7] # results : [$result, $01 # return addr : [$retreg,$26] * destroys : $0$1 $2$3$4 $% $6$7 $27 Lv285i-dot-205955 \n: mask (1 << 26) , -32 . frame $sp, 32, $26 . prologue 1 ldgp $gp, ($27) lda $Sp, -32($sP) Stq $26, ($sp) \nStq $8, 8($sP) Stq $9, 16($sP) mov $26, $27  Li : cmplt $6, $0, $8 bne $8, L2 mov $7, $1 br $31, L3 \n L2 : addl $4, $6, $8 s4addl $8, $3, $8 ldl $8, ($8) mull $1, $6, $9 add 1 $5, $9, $9 s4addl $9, $2, \n$9 ldl $9, ($9) mullv $8, $9, $8 addlv $7, $8, $7 addlv $6, 1, $6 trapb br $31, LI  L3 : mov $1, $0 \nmov $27, $26 ldq $8, 8($sP) ldq $9, 16($sP) lda $SP, 32($sP) ret $31, ($26) , 1 . end Lv2851-dot-205955 \nFigure 7: Actual DEC ALPHA assembly language Program lines Description Checksum 241 Checksum fragment \nfrom the Foxnet [7], doing 5000 checksumson a 4096-byte array. FFT 246 Fast fourier transform, multiplying \npolynomialsup to degree 65,536 Knuth-Bendix 618 An implementation of the Knuth-Bendix completion algorithm. \nLexgen 1123 A lexical-analyzer generator [6], processing the lexical description of Standard MT,. Life \n146 The game of Life implemented using lists [39]. Matmult 62 Integer matrix multiply, on 200x200 integer \narrays. PIA 2065 The Perspective Inversion Algorithm [47] deciding the location of an object in a perspective \nvideo image. Simple 870 A spherical fluid-dynamics program [17], run for 4 iterations with grid size \nof 100. Table 1: Benchmark Programs TIL always prefixes a set of operations on to each mod\u00adule that it \ncompiles, in order to facilitate optimization. This 1259 1 ~inline prelude contains 2-d array operations, \ncommonly\u00adused list functions, and so forth. To avoid handicapping SML/NJ, we created separate copies \nof the benchmark pro\u00adgrams for SML/NJ, and placed equivalent prelude code at the beginning of each program \nby hand. 75Yo-- Since TIL creates stand-alone executable, we used the exportFn facility of SML/NJ to \ncreate stand-alone programs. The exportFn function of SML/NJ dumps part of the heap 5oY.-\u00adto disk and \nthrows away the interactive system. We measured execution time on DEC ALPHA AXP 3000/\u00ad2s%-\u00ad 3OOLX workstations, \nrunning OSF/1, version 2.o, using the UNIX getrusage function. For SML/NJ, we started timing after the \nheap had been reloaded. For TIL, we measured the entire execution time of the process, including load \ntime. We Ckmmm FFT KB Iexg.n L*fe Mmult PIA SIMPLE made 5 runs of each program on an unloaded workstation \nand chose the lowest execution time. Each workstation had Figure 8: TIL Execution Time Relative to SML/NJ \n96 MBytes of physical memory, so paging was not a factor in the measurements. We measured total heap \nallocation by instrumenting the physical memory used. On average, TIL programs use half TIL run-time \nsystem to count the bytes allocated. We used the memory used by SML/NJ programs. We see that floating\u00adexisting \ninstrumentation in the SML/NJ run-time system. point programs use the least amount of memory relative \nto We measured the maximum amount of physical memory comparable SML/NJ programs. We speculate that this \nis due to TIL s ability to keep floating values unboxed whenduring execution using getrusage. We used \nthe size pro\u00ad gram to measure the size of executable for TIL. For SML/NJ, stored in arrays. we used the \nsize program to measure the size of the run-TIL stand-alone programs are about half the size of stand\u00adtime \nsystem and then added the size of the heap created alone heaps and the runtime system of SML/NJ. The \ndiffer\u00ad ence in size is mostly due to the different sizes of the runtime time, including time to assemble \nfiles produced by TIL. systems and standard libraries for the two compilers. (TIL s Figures 8 through \n11 present the measurements. For runtime system is about lOOK, while SML/NJ s runtime is each benchmark, \nmeasurements for TIL were normalized to about 425K. ) The program sizes for TIL confirm that gener\u00ad by \nexportfi. Finally, we measured end-to-end compilation ating tables for nearly tag-free garbage collection \nconsumes is the 100~o mark on all the graphs. a modest amount of space, and that the inlining strategy \nthose for SML/NJ and then graphed. SML/NJ performance Figure 8 presents relative running times. On average, \nused by TIL produces code of reasonable size. programs compiled by TIL run 3.3 times faster than pro-Figure \n11 compares compilation times for TIL and SML/NJ. SML/NJ does much better than TIL when it comes to com\u00adgrams \ncompiled by SML/NJ. In fact, all programs except pilation time, compiling about eight times faster. However, \nKnuth-Bendix and Life are substantially faster when com\u00ad piled by TIL. We speculate that less of a speed-up \nis seen we have yet to tune TIL for compilation speed. for Knuth-Bendix and Life because they make heavy \nuse of list-processing, which SML/NJ does a good job of compiling.  5.3 Loop-Oriented Optimization Figure \n9 compares the relative amounts of heap alloca\u00adtion. On average, the amount of data heap-allocated by \nthe We also investigated the effect of the loop-oriented optimiza-TIL program is about 17% of the amount \nallocated by the tion (CSE, invariant removal, hoisting, comparison elimi\u00adSML/NJ program. This is not \nsurprising, because TIL uses nation, and redundant switch elimination). For each bench\u00ad a stack while \nSML/NJ allocates frames on the heap. mark, we compared performance with the loop optimiza-Figure 10 presents \nthe relative maximum amounts of tion against performance without the loop optimizations. Ck,um FFT KB \nLexgem Life Mm.lt PIA SIMPLE Figure 9: TIL Heap Allocation Relative to SML/NJ 100% 75% .50% 257. Cksum \nFFT KB Lexg. r, Life Mmult PIA SIMPLE Figure 10: TIL Physical Memory Used Relative to SML/NJ 2000$7 \n/ 1.500% - looo% - 500f%\u00ad - 100 Y.- Ck, um FFT KB Lexgen Life Mmult PIA SIMPLE Figure 11: Til Compilation \nTime Relative to SML/NJ Exec timeOPt/Exec time~~~Pt o Heap Allocop~/Heap Alloc~~~Pt 125%  100% 75% \nI 507. 25% . Cksum FFT KB Lexge II Life Mrmult PIA SIMPLE Figure 12: Effects of Loop Optimizations \nFigure 12 presents the ratios of execution time with the loop optimizations to execution time without \nthe loop optimiza\u00adtion, and similar ratios for total heap allocation. The loop optimizations reduce execution \ntime by O to 83Y0, with a median reduction of 39 %. The effect on heap allocation ranges from an increase \nof 20~o to a decrease of 96.5Y0, with a median decrease of 10~o. For matmult, the matrix multiplication \nfunction is small enough that the optimizer inlines it, making the array di\u00admensions known. If the array \ndimensions are held unknown, then the loop optimizations speed up matmult by a factor of 2.5. 6 Related \nWork Morrison et al. used an ad-hoc approach to implement polymorphism in their implementation of Napier \n88 [35]. In particular, they passed representations of types to polymor\u00adphic routines at run-time to \ndetermine behavior. However, to our knowledge, Napier 88 did not use types to implement tag-free garbage \ncollection, Also, there is no description of the internals of the Napier 88 compiler, nor is there an \nac\u00adcount of the performance of code generated by the compiler. Peyton Jones and Launchbury suggested \nthat types could be used to unbox values in a polymorphic language [26]. However, they only supported \na limited set of unboxed types (ints and floats) and restricted these types from in\u00adstantiating type \nvariables. Later, Leroy suggested a gen\u00aderaJ approach for unboxing values based on the ML type system \n[28]. Leroy s approach has been extended and implemented elsewhere [38, 24, 42], including the SML/NJ \ncompiler. It does not support unboxed array components nor flattened, recursive datatypes. Tolmach [46] \ncombined Leroy s approach with tag-free garbage collection. However, he used an ad hoc approach to propagate \ntype information to the collector. Other researchers have suggested that polymorphism should be eliminated \nentirely at compile time [9, 25, 21], in the style of c++ templates [44]. This prevents separate compilation \nof a polymorphic definition from its uses. In contrast, in\u00adtentional polymorphism, and in particular \nthe intermediate forms of TIL, support separate compilation of polymorphic definitions, though we have \nyet to take advantage of this. References Tag-free garbage collection was originally proposed for monomorphic \nlanguages like Pascal, but has been used else\u00ad [1] where [12, 11, 48, 15]. Bntton suggested associating \ntype information with return addresses on the stack [12]. Appel suggested extending this technique to \nML by using unifica\u00ad [2] tion [4]. Goldberg and Gloger improved Appel s algorithm [20, 19]. None of the \nunification-based algorithms were im\u00adplemented due to the complexity of the algorithms and the [3] overhead \nof performing unification during garbage collec\u00adtion. [4] Aditya, Flood, and Hicks used type-passing \nto support fully tag-free garbage collection for Id [1]. Independently, [5] Tolmach [46] implemented \na type-passing garbage collec\u00adtion algorithm for ML. Our approach differs from others [6] by using nearly \ntag-free collection. In particular, records and arrays on the heap are tagged. Another difference is \nthat we calculate type environments eagedy, while the other [7] implementations construct type environments \nlazily during garbage collection. Loop-oriented optimizations are well-known for imper-[8] ative languages \n[2]. However, few results are reported for Lisp, Scheme, and ML. Appel [5] and Serrano [40] report common-subexpression \nelimination optimizations similar to [9] ours. Appel found that CSE was not useful in the SML/NJ compiler. \nSerrano restricted CSE to pure expressions, while our CSE handles expressions which may raise exceptions. \n[10] [11] 7 Conclusions and future work Our results show that for core-SML programs compiled as [12] \na whole, intensional polymorphism can remove restrictions on data representation, yet cost literally \nnothing due to the effectiveness of optimization. They also show that loop op\u00ad [13] timization can improve \nprogram performance significantly. These results suggest that ML can be compiled as well as conventional \nlanguages such as Pascal. TIL produces code that is similar in many important respects to code produced \n[14] by Pascal and C compilers. For example, most function calls are known, since few higher-order functions \nare left, integers are untagged, and most code is monomorphic, There are numerous areas that we would \nlike to investi\u00adgate further. We would like to explore the effect of separate compilation. With separate \ncompilation, polymorphic func\u00ad [15] tions may be compiled separately from their uses, leading to some \ncost for int ensional polymorphism. We would like to measure this cost and explore what kinds of optimizations \ncan reduce it. Another direction we would like to investigate is how this [16] approach performs for \nlarger programs. We would like to add support for more of the ML module system, since large ML programs \nmake extensive use of the module system. We [17] would also like to improve TIL s compile times, so that \nlarge programs can also be compiled as a whole. Finally, we would like to continue improving the per\u00adformance \nof ML programs. We would like to extend our register allocation strategy along the lines of Chow [13] \nor Steenkiste [431. We would also like to investigate more [18] loop optimizations, such as strength-reduction, \ninduction\u00ad variable elimination, and loop unrolling. On a more specu\u00adlative note, we would like to explore \nstack allocation of data [19] structures. Shail Adltya, Christine Flood, and James Hicks. Garbage collection \nfor strongly-typed languages using run-time type reconstruction. In LFP 94 [30], pages 12 23. Alfred \nV. Aho, Ravi Sethi, and Jeffrey D. Unman. Com\u00adpilers: Principles, Techniques, and Tool$, Addison Wesley \nPublishing Company, 1986. Andrew Appel. A critique of Standard ML. Journal of Func\u00adtional Programming, \n3(4):391 429, October 1993. Andrew W. Appel. Runtime tags aren t necessary. Lisp and Symbolic Computation, \n(2):153-162, 1989. Andrew W. Appel. Compiling with Continuations. Cam\u00adbridge University Press, 1992. \nAndrew W. Appel, James S. Mattson, and David Tardlti, A lexical analyzer generator for Standard ML. Distributed \nwith Standard ML of New Jersey, 1989. Edoardo Biagioni, Robert Harper, Peter Lee, and Brian Milnes. Signatures \nfor a network protocol stack: A systems application of Standard ML. In LFP 94 [30], pages 55-64. Lars \nBirkedal, Nick Rothwell, Mads Tofte, and David N. Turner. The ML Kit, Version 1. Technical Report 93/14, \nDIKU, 1993. Guy E. Blelloch. NESL: A nested data-parallel language (version 2.6). Technical Report CMU-CS-93-129, \nSchool of Computer Science, Carnegie Mellon University, April 1993. Hans-Juergen Boehm. Space-efficient \nconservative garbage collection. In PLDI 93 [36], pages 197-206. P. Branquart and J. Lewi. A scheme for \nstorage allocation and garbage collection for Algol-68. In A lg 01-68 Implementa\u00adtion. North-Holland \nPublishing Company, Amsterdam, 1970. Dianne Ellen Britton. Heap storage management for the programming \nlanguage Pascal. M-ter s thesis, University of Arizona, 1975. Fred C. Chow. Minimizing register usage \npenalty at proce\u00addure calls. In Proceedings oj the ACM SIGPLAIV 88 Con\u00adjerence on Programming Language \nDesign and Implementa\u00adtion, pages 85 94, Atlanta, Georgia, June 1988. ACM. A, Demers, M. Weiser, B. Hayes, \nH. Boehm, D. Bobrow, and S. Shenker. Combining generational and conservative garbage collection: Framework \nand implementations. In Conference Record of the 17th Annual ACM SIGPLAN-SIGA CT Symposium on Principles \nof Programming Lan\u00adguages, San Francisco, California, January 1990. ACM. Amer Diwan, Eliot Moss, and \nRichard Hudson. Compiler support for garbage collection in a statically typed language. In Proceedings \nof the ACM SIGPLAN 92 Conference on Programming Language Design and Implementation, pages 273 282, San \nFrancisco, CA, June 1992. ACM. Amer Diwan, David Tarditi, and Eliot Moss. Memory-System Performance of \nPrograms with Intensive Heap Al\u00adlocation. Transactions on Computer Systems, August 1995. K. Ekanadham \nand Arvind. SIMPLE: An exercise in fu\u00adture scientific programming. Technicat Report Computation Structures \nGroup Memo 273, MIT, Cambridge, MA, July 1987. Simultaneously published as IBM/T. J. Watson Re\u00adsearch \nCenter Research Report 12686, Yorktown Heights, NY. Cormac Flanagan, Amr Sabry, Bruce F. Duba, and Matthiss \nFelleisen. The essence of compiling with continuations. In PLDI 93 [36], pages 237-247. Benjamin Goldberg. \nTag-free garbage collection in strongly typed programming languages. In Proceedings of the ACM SIGPLAN \n91 Conference on Programming Language De\u00adsign and Implementation, pages 165 176, Toronto, Canada, June \n1991. ACM. [20] [21] [22] [23] [24] [25] [26] [27] [28] [29] [30] [31] [32] [33] [34] [35] [36] [37] \nBenjamin Goldberg and Michael Gloger. Polymorphic type reconstruction for garbage collection without \ntags. In Pro\u00ad ceedings o.f the 1992 A CM Conference on Lisp and Func\u00adtional Programming, pages 53-65, \nSan Francisco, California, June 1992. ACM. Cordelia Hall, Simon L. Peyton Jones, and Patrick M. San\u00adsom. \nUnboxing using specialisation. In D. Turner K. Ham\u00admond, P.M. Sandom, editor, Functional Programming, \n1994. Springer-Verlag, 1995. Robert Harper and Mark Lillibridge. A type-theoretic ap\u00adproach to Klgher-order \nmodules with sharing. In POPL 94 [37], pages 123-137. Robert Harper and Greg Morrisett. Compiling polymor\u00adphism \nusing intentional type analysis. In Conference Record oj the 22nd Annual ACM SIGPLAN-SIGA CT Symposium \non Principles oj Programming Languages, pages 130 141, San Francisco, California, January 1995. ACM. \nRitz Henglein and Jesper J@gensen. Formally optimal box\u00ading. In POPL 94 [37], pages 213 226. M.P. Jones. \nPartial evaluation for dictionary-free overload\u00ading. Research Report YALEU/DCS/RR-959, Yale Univer\u00adsity, \nNew Haven, Connecticut, USA, April 1993. Simon Peyton Jones and John Launchbury. Unboxed values as first-class \ncitizens. In Proceedings o.f the Conjere race on Functional Programming and Computer Architecture, vol\u00adume \n523 of Lecture Notes on Computer Science, pages 636 666. ACM, Springer-Verlag, 1991. David Kranz, l%chard \nKelsey, Jonathan Rees, Paul Hudak, James Philbin, and Norman Adams. ORBIT: An Optimizing Compiler for \nScheme. In Proceedings oj the SIGPLAN 86 Symposium on Compiler Construction, pages 219-233, Palo Alto, \nCalifornia, June 1986. ACM. Xavier Leroy. Unboxed objects and polymorphic typing. In Conference Record \noj the 19th Annual ACM SIGPLAN-SIGA CT Symposium on Principles oj Programming Lan\u00adguages, pages 177 188, \nAlbuquerque, NM, January 1992. ACM. Xavier Leroy. Manifest types, modules, and separate compi\u00ad lation. \nIn POPL 94 [37], pages 109-122. Proceedings oj the 1994 AC&#38;f Conference on Lisp and Func\u00ad tioraa~ \nProgramming, Orlando, Florida, June 1994. ACM. Robin Milner, Mads Tofte, and Robert Harper. The Defini\u00ad \ntion oj Standard ML. MIT Press, 1990. Y. Minamide, G. Morrisett, and R. Harper. Typed clo\u00adsure conversion. \nIn Conference Record oj the 23rd Annual ACM SIGPLAN-SIGA CT Symposium on Principles of Pro\u00adgramming Languages, \nSt. Petersburg, Florida, January 1996. ACM. Greg Morrisett. Compiling with Types. PhD thesis, School \nof Computer Science, Carnegie Mellon University, Pittsburgh, PA, December 1995. Published as Technical \nReport CMU\u00ad CS-95-226. Greg Morrisett, Matthias Felleisen, and Robert Harper. Ab\u00adstract models of memory \nmanagement. In ACM Conje r\u00adence on Functional Programming and Computer Architec\u00adture, pages 66-77, La \nJolla, June 1995. R. Morrism, A. Dearle, R, C, H, Connor, and A, L, Brown. An ad hoc approach to the \nimplementation of polymorphkm. ACM fiansactions on Programming Languages and Sy8\u00adtems, 13(3):342-371, \nJuly 1991. Proceedings oj the ACM SIGPLAN 99 Conference on Pro\u00ad gramming Language Design and Implementation, \nAlbu\u00ad querque, New Mexico, June 1993. ACM. Conference Record oj the 21st Annual ACM SIGPLAN- SIGA CT \nSymposium on Principles oj Programming Lan\u00ad guages, Portland, Oregon, January 1994. ACM. [38] Eigil \nRosager Poulsen. Representation analysis for efficient implemental ion of polymorphism. Technical report, \nDepart\u00adment of Computer Science (DIKU), University of Copen\u00adhagen, April 1993. Master Dissertation. [39] \nChris Reade. Elements oj Functional Programming. Addison-Wesley, Reading, Massachusetts, 1989. [40] Manual \nSerrano and Pierre Weis. 1+1 = 1: an optimizing CAML compiler. Technical Repoti 2264, INRIA, June 1994. \n[41] Zhong Shao. Compiling Standard ML jor Eflcient Execu\u00adtion on Modern Machines. PhD thesis, Princeton \nUniversity, Princeton, New Jersey, November 1994. [42] Zhong Shao and Andrew W. Appel. A type-based compiler \nfor Standard ML. In Proceedings oj the ACM SIGPLAN 95 Conference on Programming Language Design and Imple\u00admentation, \npages 116429, La Jolla, California, June 1994. ACM. [43] Peter Steenkiste. Advanced register allocation. \nIn Peter Lee, editor, Topics in Advanced Language Implementation. MIT Press, 1990. [44] Bjarne Stroustrup. \nThe C++ Programming Language, 2nd Edition. Addison-Wesley, 1991. [45] David R. Tarditi. Optimizing ML. \nPhD thesis, School of Computer Science, Carnegie Mellon University, 1996. Forth\u00adcoming. [46] Andrew Tolmach. \nTag-free garbage collection using explicit type parameters. In LFP 94 [30], pages 1-11. [47] Kevin G. \nWaugh, Patrick McAndrew, and Greg Michelson. Parallel implementations from function prototypes: a case \nstudy. Technical Report Computer Science 90/4, Heriot-Watt University, Edinburgh, August 1990, [48] P.L. \nWodon. Methods of garbage collection for Algol-68. In Algol-68 Implementation.-North-Holland PubIishhg \nCom\u00adpany, Amsterdam, 1970.  \n\t\t\t", "proc_id": "231379", "abstract": "", "authors": [{"name": "D. Tarditi", "author_profile_id": "81100531922", "affiliation": "School of Computer Science, Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh, PA", "person_id": "P56727", "email_address": "", "orcid_id": ""}, {"name": "G. Morrisett", "author_profile_id": "81339518683", "affiliation": "School of Computer Science, Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh, PA", "person_id": "PP39080790", "email_address": "", "orcid_id": ""}, {"name": "P. Cheng", "author_profile_id": "81451593218", "affiliation": "School of Computer Science, Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh, PA", "person_id": "PP35042918", "email_address": "", "orcid_id": ""}, {"name": "C. Stone", "author_profile_id": "81409592706", "affiliation": "School of Computer Science, Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh, PA", "person_id": "P38903", "email_address": "", "orcid_id": ""}, {"name": "R. Harper", "author_profile_id": "81100140064", "affiliation": "School of Computer Science, Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh, PA", "person_id": "PP39072917", "email_address": "", "orcid_id": ""}, {"name": "P. Lee", "author_profile_id": "81100384353", "affiliation": "School of Computer Science, Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh, PA", "person_id": "PP39079421", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/231379.231414", "year": "1996", "article_id": "231414", "conference": "PLDI", "title": "TIL: a type-directed optimizing compiler for ML", "url": "http://dl.acm.org/citation.cfm?id=231414"}