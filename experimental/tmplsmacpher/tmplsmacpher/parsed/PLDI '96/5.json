{"article_publication_date": "05-01-1996", "fulltext": "\n Commutativity Analysis: A New Analysis Framework for Parallelizing Compilers* Martin C. Rinard (martin@ \ncs.ucsb.edu) Pedro C. Diniz (pedro@cs.ucsb. edu) Department of Computer Science University of California, \nSanta Barbara Santa Barbara, CA 93106 Abstract This paper presents a new analysis technique, commutativity \nsnsl\u00ad ysis, for automatically psrallelizing computations that manipulate dynamic, pointer-based data \nstructures. Commutativity analysis views the computation as composed of operations on objects. It then \nanalyzes the program at this granularity to discover when op\u00ad erations commute (i.e. generate the same \nfinal result regardless of the order in which they execute). If all of the operations re\u00ad quired to perform \na given computation commute, the compiler can automaticrdly generate parallel code. ~ We have implemented \na prototype compilation system that uses commutativit y analysis asits primary analysis framework. We \nhave used this system to automatically psrallelize two complete scientific computations: the Barnes-Hut \nN-body solver and the Water code. This paper presents performance results for the generated parallel \ncoderunning ontheStanfordDASH machine. Theseresultsprovide encouraging evidence that commutativity analysis \ncan serve as the basis for a successful parsllelizing compiler. Introduction Parsllelizing compilers \npromise to drsrnaticrdly reduce the difficulty of developing software for parallel computing environments. \nExist\u00ading parallelizing compilers use data dependence analysis to detect independent computations (two \ncomputations are independent if neither accessesdata that the other writes), then generate code that \nexecutes these computations in parallel. In the right context this approach works well researchers have \nsuccessfully used data dependence analysis to parallelize computations that manipulate dense arrays using \naffine accessfunctions [1, 27, 11, 15]. But data dependence analysis is, by itself, inadequate for computations \nthat manipulate dynamic, pointer-based data structures. Its limitations include a need to perform complicated \nanalysis to extract global properties of the data structure topology and an inherent inability to parallelize \ncomputations that manipulate graphs [2]. *The fmt author k suppotied m psrt by an Alfred P. Slosn Resesrch \nFellowship The secondauthoris sponsoredby thePRAXIS XXI progrrunadministratedby JNICT JuntaNacionzddeInvestiga@oClentificaeTecno16gicafrom \nPortugal,sndholdsa Fulbrighttravelgram, Permission to make digitabtvard 00PY of part or ail of this \nwork for personal or classroom use is granted without fee provided that copies are not made or distributed \nfor profit or commercial advantage, the copyright notice, the title of the ublication and its date appear, \nand notice is given that copying !IS y permission of ACM, Inc. To copy otherwise, to republish, to post \non servers, or to redistribute to lists, requires prior specific permission andlot a fee. PLDI 965196 \nPA, USA GI 1996 ACM 0-89791 -795-296/0005... $3.50 We believe the key to automatically psrallelizing \ndynamic, pointer-based computations is to recognize and exploit commut\u00ading operations, or operations \nthat generate the same final result regardless of the order in which they execute. Even though tradi\u00adtional \ncompilers have not exploited commuting operations, these operations play an important role in other areas \nof parallel com\u00adputing. Explicitly parallel programs, for example, often use locks, monitors andcritical \nregions to ensurethat operations executeatom\u00adically [20, 35]. For the program to execute correctly, the \nprogram\u00admer must ensure that all of the atomic operations commute. Four of the six parallel applications \nin the SPLASH benchmmk suite [34] and three of the four parallel applications described in [32] rely \non commuting operations to expose the concurrency and generate correct parallel execution. This experience \nsuggeststhat compilers will be unable to parallelize a wide range of computations unless they go beyond \ndata dependence analysis to recognize and exploit commuting operations. We have developed a new analysis \nframework called cornmu\u00adtativity analysis. This framework is designed to automaticrdly rec\u00adognize and \nexploit commuting operations to generate parallel code. It views the computation as composed of srbitrrry \noperations on arbitrary objects. It then analyzes the computation at this gramt\u00adlsrity to determine if \noperations commute. If all of the operations in a given computation commute, the compiler can automatically \ngenerate parallel code. Even though the code may violate the data dependence of the original serial program, \nit is still guaranteed to generatethe sameresult. We have built a complete prototype compilation system \nbased on commutativity analysis. This compilation system is designed to automaticrdly psrsllelize unannotated \nprograms written in a subset ofC++.Thedynamicnatureofourtargetapplication setmeansthat the compiler must \nrely on a run-time system to provide basic task management functionality such as synchronization and \ndynamic load balancing. We have implemented a run-time system that pro\u00advides this functionality. It currently \nruns on the Stanford DASH multiprocessor [22] and on multiprocessors from Silicon Graphics. We have used \nthe compilation system to automatically paral\u00adlelize two complete scientific applications: the Barnes-Hut \nN-body solver [3] and the Water code [39]. The Barnes-Hut is representative of our target class of dynamic \ncomputations: it performs well be\u00adcauseitusesapointer-baseddatastructure(aspacesubdivision tree) to organize \nthe computation. The Water code is a more traditional scientific computation that organizes its data \nas arrays of objects representing water molecules. We have collected performance re\u00adsults for the generated \nparallel code running on the Stanford DASH machine. These results indicate that commutativity anrdysis \nmay beabletoserveasthebasisforasuccessfidparrdlelizing compiler. This paper makes the following contributions: \n It describes an new analysis framework, commutativit y tmal\u00adysis, that can automatically recognize and \nexploit commuting operations to generate parallel code.  It describes extensions to the basic commutativity \nanalysis framework. These extensions significantly extend the range of programs that commutativity analysis \ncan effectively par\u00adrdlelize. . It presents several analysis algorithms that a compiler can use to automatically \nrecognize commuting operations, discover parallelizable computations and generate parallel code.  It \npresents performance results for automatically parrtllelized versionsoftwoscientificcomputations, Theseresultssupport \nthe thesis that it is possible to use commutativity rmalysiIsas the basis for a successful parallelizing \ncompiler,  Although we designed commutativity analysis to parallelizc se\u00adrial programs, it may also \nbenefit other areasof computer science. For example, commuting operations allow computations on the persistent \ndata in object-oriented databasesto execute in parallel. Transaction processing systems can exploit commuting \noperations to use more efficient locking algorithms [37]. Commuting opera\u00adtions make protocols from distributed \nsystems easier to implement efficiently; the corresponding reduction in the size of the associ\u00adated state \nspace may make it easier to verify the correctness of the protocol. In all of these cases the system \nrelies on commut\u00ading operations for its correct operation. Automatically recognizing or verifying that \noperations commute may therefore increase the efficiency, safety audlor reliability of these systems. \nThe remainder of the paper is structured as follows. Section 2 presents an example that shows how commuting \noperations enable parallel execution. Section 3 presents an overview of the commu\u00adtativity analysis framework. \nSection 4 presents the analysis algo\u00adrithms that the compiler uses, Section 5 describes how the compiler \ngenerates parallel code, Section 6 presents the experimental per\u00adformance results for two automatically \nparallelized applications. Section 7 describes some directions for future research. We survey related \nwork in Section 8 and conclude in Section 9. 2 An Example This section presents au example that shows \nhow commuting oper\u00adations enable parallel execution. The vis it operation in Figure 1 serially traverses \na graph. When the traversal completes, each node s sum instance variable contains the sum of its original \nvalue and the values of the val instance variables in all of the nodes tlhat directly point to that node. \nThe example is written in C++. The traversal generates one visit operation for each edge in the graph. \nEach operation traverses a graph node; this node is called the receiver of the operation. Each visit \noperation takes as a parameter p the vahte of the instance variable va 1 of the node that points to the \nreceiver. The operation first adds p into the running sum stored in the receiver s sum instance variable. \nIt then checks the receiver s mark instance variable to see if the traversal has already visited the \nreceiver. If not, the operation marks the receiver, then recursively invokes the vis it operation for \nall of the nodes that the receiver points to, The way to parallelize the traversal is to execute the \ntwo recur\u00adsive vis it operations concurrently, But this parallelization may violate the data dependence, \nThe serial computation executes tall of the ~cceweg generated by the left trawrtml before all of the \nac\u00adcessesgenerated by the right traversal. If the two traversals visit tlhe class graph { boolean mark; \nint val, sum; graph *left; graph *right; }; graph: :visit(int p) ( sum =sum +p; if (!mark) { mark = TRUE; \nif (left != NULL) left->visit(val) ; if (right != NULL) right->visit (val) ; 1 1 Figurel: Serial Graph \nTraversal samenode, in the parallel execution the right traversal may visit the node before the left \ntraversal, changing the order of reads and writes to that node. This violation of thedata dependences \nmay generate cascading chmgesin theoverdl execution of thecomputation. Be\u00adcauseofthemarking algorithm, \nanodeonly executestherecursive calls the first time it is visited, Iftheright traversal reaches anode \nbefore the left traversal, the parallel execution may also change the order in which the overall traversal \nis generated, In fact, none of these changes affects the overall result of the computation. It is possible \nto automatically parallelize the com\u00adputation even though the resulting parallel program may generate \ncomputations that differ substantially from the original serial com\u00adputation. The key property that enablestbe \nparrtllelization is that the parallel computation generates the same set of visit operations as the serial \ncomputation and the generated visit operations can execute inauy order without affecting the overall \nbehavior of the traversal, Given this commutativity information, the compiler can auto\u00admagically generate \ntheparallel visit operation in Figure2. The top level visit operation first invokes the parallel.visit \noperation, then invokes the wait construct, which blocks until all parallel tasks created by the current \ntask or its descendant tasks finishes. Theparallel.vis itoperation executes therecursive calls concurrently \nusing the spawn construct, which creates a new task foreach operation. Astraightforward application oflazy \ntask creation techniques [23] canincrease thegrauularityof the result\u00ading parallel computation. The compiler \nalso augments each graph node with a mutual exclusion lock mut ex. The generated parallel operations \nuse this lock to ensure that they execute atomically. 3 Basic Concepts Commutativity analysis exploits \nthe structure present in object\u00adbasedprograms to guide the parallelization process. In this section we \npresent the basic concepts behind this approach. 3.1 Model of Computation We explain the basic model \nof computation for commutativity anal\u00adysis as applied to pure object-based programs, Such programs structure \nthe computation a~Q~equsfice of operations on objects. Each object implements its state using a set of \ninstance variables. class graph { lock mutex; boolean mark; int val, Sum; graph *left; graph *right; \n}; graph: :visit(int p) { this->parallel_visit (P) ; waito ; } graph: :parallel_visit(int p) { mutex.acquireo \n; sum = sum +p; if (!mark) { mark = TRUE; mutex.releaseo ; if (left != NULL) spawn(left->parallel_visit \n(val) ) ; if (right != NULL) spawn(right->parallel_visit (val) ) ; } else { mutex.releaseo ; } } Figure \n2: Parallel Graph Traversal Each instance variable can be either a nested objector a primitive type from \nthe underlying language such as an integer, an array of doubles or a pointer to an object. In the example \nin Figure 2 each graph node is an object. Programmers define operations by writing methods. Each op\u00ad \neration corresponds to a method invocation: to execute an opera\u00ad tion, the machine executes the code \nin the corresponding method. Each operation has a receiver object and several parameters that are passedby \nvalue to the operation. When an operation executes it can accessthe parameters, invoke other operations \nor accessthe instance variables of the receiver (unless the instance variable is a nested object, in \nwhich case the operation can only manipulate the nested object indirectly by invoking operations with \nthe nested object as the receiver). Commutativity analysis is designed to work with separable op\u00ad erations. \nAn operation is separable if it can be decomposed into an object section and an invocation section. The \nobject section performs all accessesto the receiver. The invocation section in\u00ad vokes other operations \nand does not accessthe receiver. It is of course possible for 10CSIvariables to carry values computed \nin the object section into the invocation section, and both sections can accessthe parameters. The motivation \nfor separability is that the commutativity testing algorithm (which determines if operations commute) \nrequires that each operation s accessesto the receiver execute atomically with respect to the operations \nthat it invokes. Separability ensures that the actual computation obeys this con\u00ad straint. Separability \nimposes no expressibility limitations it is possible to automatically decompose any method into a collection \nof methods whose invocations are all separable via the introduction of auxiliary methods. 3.2 Extents \nA policy in the compiler must choose computations to attempt to parallelize. The current policy is that \nthe compiler analyzes one computation for each method; that computation consists of all op\u00aderations either \ndirectly or indirectly executedasaresult of executing the given method. The compiler computes a conservative \napprox\u00adimation to the set of executed operations. This approximation is called the extent of the method. \nIf the compiler can verify that all pairs of operations in an extent commute, it marks the method that \ngenerated the extent as a parallel method. If some of the pairs may notcommute.it marksthemethod asaserialmethod. \n 3.3 Commutativity Testing The foundation of commutativity analysis is a set of conditions that the compiler \ncan use to test if two operations A and B commute. These commutativity testing conditions must consider \ntwo execu\u00adtion orders: the execution order A;B in which A executes first then B executes, and the execution \norder B;A in which B executes first then A executes. The two operations commute if they meet the followingcommutativity \ntestingconditions: b Instance Variables: The new value of each instance variable of the receiver objects \nof A and B under the execution order A;B must be the same as the new value under the execution order \nB;A. Invoked Operations: The multiset ---., of operations dirertlvinvoked by ~ither A or B under the \nexecuti&#38; order A;B must be the same as the multiset of operations directly invoked by either A or \nB under the execution order B ;A. Both commutativity testing conditions are trivially satisfied if the \ntwo operations have different receivers or if neither operation writes an instance variable that the \nother accesses in both of these casesthe operations are independent, If the operations may not be independent, \nthe compiler reasonsabout the values computed in the two execution orders. We illustrate this concept \nby applying it to the sum instance variable in the example in Figure 1. We assume two invocations r->visit(pl) \nand r->visit(p2) ofthe visit operation. r->vis,it(pl) has pammeterpl, r->visit(p2) has parameter p2 and \nboth operations have the same receiver r. Table 1 contains the two expressions denoting the new values \nof sum under the two execution orders. In these expressions sum represents the old value of the sum instance \nvariable before either operation executes. It is possible to determine by algebraic reason\u00ad ing that \nboth expressions denote the same value.1 The compiler can use a similar approach to discover that the \nvalues of the other instance variables are the same in both execution orders and that together the operations \nalways directly invoke the same multiset of operations. Execution Order NewValueofsum r->visit(pl);r->visit(p2) \n(sum+pl)+p2 x =-visit(p2);=->vieit(pl) (s3urn+p2)+pl Table 1: New Values of sum Under Different Execution \nOrders 1Weignoreherepotentialaaornaliescausedbythefimterepresentationof numbers. A compilerswitchthatdisablestheexploitationof \ncommutativityarrdassociativiryfor operatorssuchas+WI1lallowtheprogrammerto preventthecompilerfrom performing \ntransformationsthatmaychangetheorderin whichtheparallelprogramcombmesthe summarrds 3.4 Symbolic Execution \nThe compiler uses symbolic execution [19] to extract the expres\u00adsions that denote the new values of instance \nvariables and the mul\u00adtiset of invoked operations. Symbolic execution simply executes the methods, computing \nwith expressions instead of values, It maintains a set of bindings that map variables to the expressions \nthatdenotetheirvaluesandupdatesthebindings asitexecutesthe methods. In certain circumstances the compiler \nmaybe unable to extract expressions that precisely represent the values that an operation computes. In \nthe current compiler this may happen, for example, if the method contains unstructured flow of control \nconstructs such as got o constructs. In this case the compiler marks the method as unanalyzable; the \ncommutativity testing phase conservatively assumesthat invocations of unanalyzable methods commute with \nno operation. 3.5 Extensions We have found it usefil to extend the analysis framework to handle severrd \nsituations that fall outside the basic model of computation outlined in Section 3.1. These extensions \nsignificantly increase the range of programs that the compiler can successfully analyze. 3.5.1 Extent \nConatantx All of the conditions in the cornmutativity testing rdgorithm clheck expressions for equality. \nIn certain casesthe compiler maybe able to prove that two values are equal without representing the values \nprecisely in closed form. Consider the execution of an operation in the context of a given extent. If \nthe operation reads a variable that none of the operations in the extent write, the variable will have \nthe samevalue regardless of when the operation executesrelative to all of the other operations in the \nextent. We call such a variable an extent constant variable. If the operation computes a value that does \nnot depend on state modified by the other operations in the extent, the value will be the same regardless \nof when the operation executes relative to all of the other operations in the extent. In this casethe \ncompiler can represent the value with an opaque constant instead of attempting to derive a closed form \nexpression. We call such a value an extent constant value, the expression that generated it an extent \nconstant expression and the opaque constant an extent constant. Extent constants improve the analysis \nin several ways: They support operations that directly accessglobal variables and instance variables \nof objects other than the receiver of the operation, The constraint is that such variables must be extent \nconstants.  They improve the efficiency of the compiler by supporting compact representations of expressions. \nThese representa\u00adtions support efficient simplification and equality testing al\u00adgorithms.  They extend \nthe range of constructs that the comr)iler can effe&#38;ively analyze t: include otherwise unanalyz~ble \ncon\u00adstructs that only accessextent constants.  The compiler relaxes the notion of separability to allow \nthe invoca\u00adtion sectiontocomputeextentconstantvalues,evenif theinvocation section must accessinstance \nvariables to do so. 3.5.2 Auxiliary Operations For modularity purposes programmers often encapsulate \nthe com\u00adputation of values inside an operation. The caller obtains the com\u00adputed values either as the \nreturn value of the operation or via local variables passed by reference into the operation. We call \nsuch operations auxiliary operations. Integrating such operations into their callers for analysis purposes \ncan improve the effectiveness of the commutativity testing algorithm. The integration coarsens the granularity \nof the analysis, reducing the number of pairs that the algorithm tests for commutativity and increasing \nthe ability of the compiler to recognize parallelizable computations. Because auxil\u00adiary operations are \nconceptually part of the operation that invokes them, the compiler relaxes the notion of separability \nto allow the object section to invoke auxiliary operations. 4 Analysis Algorithms Inthissectionwepresentanalysisalgorithms \nthatacompiler canuse to reaEzethe basic approach outlined in Section 3. The algorithms use the type information \nto characterize how method invocations accessdata. This data accessinformation is then used to identify \nextent constant variables, auxiliary operations and independent op\u00aderations. The basic assumption behind \nthis approach is that the program does not violate its type declarations. Forpresentation purposesweassumethat \neachinstancevariable is either a primitive type from the underlying language; the algo\u00adrithms generalize \nto handle nested objects. Also for presentation purposesweassumethatnooperation returnsavalue(it canachieve \nthe same functionality using reference parameters); the algorithms generalize to handle auxiliary operations \nwith return values.  4.1 Overview Given a method, the compiler first identifies the set of variables \nthat the computation rooted at the method reads but does not write. This set is called the set of extent \nconstant variables; the compiler uses this set to identify auxiliary operations and extent constant expressions. \nThe compiler then performs a depth-first search of the call graph, identifying call sites that always \ninvoke auxiliary operations and computing the extent of the method. The presented algorithms represent \nthe extent as a set of methods. When possi\u00adble the implemented compiler also extracts an expression for \neach parameter of a potentially invoked method that denotes that pa\u00adrameter s value. These expressions \nimprove the precision of the commutativity testing phase they increase the compiler s ability to prove \nthat expressions involving the parameters denote identical values. Once it has extracted the extent, \nthe compiler verifies that ail of the operations in the extent are separable and do not violate several \nreference parameter usage constraints. It also checks that none of the operations perform any input or \noutput or create new objects. Finally, it tests that all pairs of operations in the extent commute. If \nso, it is possible to parallelize the original method. Figure 3 presents the algorithm; in the succeeding \nsections we discuss each of the routines that it usesto perform the analysis, 4.2 Basic Functionality \nWe first describe the basic functionality that provides the foundation for the analysis algorithms. The \nprogram defines a set of classes cl G CL, instance variables v G V, local variables 1 6 L, methods m \nc M, call sitesc c C,primitive typesp E P andformal reference parameters r c R, The compiler considers \nany parameter whose declaredtypeisapointertoaprimitive tYPe,anarraYofPrimitive isParallel(frr) types \nor a ;eference (in the C++ sense) to a primitive type to be a reference parameter. If a parameter s declared \ntype is a pointer or reference to a class, it is not considered to be a reference parameter. Given a \nmethod, calls ites : M + 2C returns its set of call sites and referencel?areuneters : M + 2R returns \nits set of formal reference parameters. Given a call site, method : C + M returns the invoked method. \nmap(~, A) = {f(a).a G A}. The analysis representsmemory locations using storagedescrip\u00adtorssc S= R U \nL U P U CL x V. Z istheidentity function on S. type : S + P gives the type of a storage descriptor. Bindings \nb : B = R + S represent bindings of formal reference parameters to storage descriptors. Given a binding \nb, the extension ~ of b to S is defined by ~(s) = s whens @R and b(s) otherwise. Given a crdl site and \na binding, bind : C x B + B represents the binding of formal to actual parameters that takes place when \nthe method at the call site is invoked in the context of the given binding. 1i f t : S + PUCLX Vtranslates \nlocal variables andparameters to their primitive types. The definition is 1 i f t(s) = type(s) when s \nc R U L and s otherwise. There is a partial order ~ on S. Conceptually S1 ~ sz if the set of memory locations \nthat S1 represents is a subset of the set of memory locations that sz represents. (c1l,v) < (cIz,v) if \nc1linherits from CIZandS1~ szif type(sl) = s2. The compiler performs some local analysis on each method \nto extract several functions that describe the way the method accesses memory. Given a method and a binding, \nread : MXB+2S returns a set of storage descriptors that represent how the method reads data, For example, \n(cl, v) c read(m, b) if the method m in thecontextofthebinding breadstheinstancevariablevinanobject of \nclass cl. r G read(m, b) if m reads the reference parameter r in the context of the binding b. Similarly, \nwrite :MXB--+2S represents how the method writes data. dep : C + 2s represents the memory locations that \nthe surrounding method readsto compute the values in the reference parameters at the given call site. \n 4.3 Extent Constant Variables The extentConstantVar iables routine in Figure 4 com\u00adputes the set of \nextent constant variables for a given method. The transit iveE f f ec ts routine performs the core computation, \nusing abstract interpretation to compute a read set and write set of storage descriptors that accurately \nrepresent how the computation may read and write data. f i 1t er prunes the read set so that it only \ncontains storage descriptors that represent memory locations that the computation does not write. 4.4 \nExtents and Auxiliary Operations Given amethod andasetof extentconstantvariables,thealgorithm in Figure \n5 computes the set of call sites that always invoke auxil\u00ad iary operations; each such call site is called \nan auxiliary call site. The algorithm also computes the extent of the method. Note that auxiliary operations \nare not included in the extent. The algorithm performs a depth first search of the call graph, terminating \nsearch paths when it encounters a call site that always invokes an auxiliary operation. The need to accurately \nrepresent the values that auxiliary operations write into reference parameters partly determines the \nconditions that they must satisfy. The current condition is that auxilia-y operations only compute extent \nconstant values, which allows the compiler to represent the values in refer\u00ad ence parameters using extent \nconstants. An interprocedurrd sym\u00ad bolic execution algorithm would relax this condition by allowing ec \n= exten tConstantVar iables(m); (exr, am) = extent(m, cc); if (!checkReferenceParameters(fn, ext)) return \nfake; ms = {m} U map(rnethod, .eM); forafl ml E ms if (!separable(ml, awe, cc)) return false; if (mayPerformIO(ml)) \nreturn fdsq if (mayCreateObj ect(ml)) return false; forall (ml, mz) G ms X ms if (!commute(ml, rrzz,ax, \ncc)) return false; return tnre; Figure 3: Algorithm to Recognize Parallel Methods extentConstantVariables( \nnz) (d, wr) = transitiveEf f ects(rn); rd = map(lift, rd); wr = map(lift, wr); (rd, UW) = f ilter(rd, \nwr); return rd; transit iveEf f ects(rn) rd=O; wr=O; visited = 0; current = {(m, Z)}; while (current \n# 0) nezt = 0; for afl (m , b) c current for all c 6 callsites(rn ) next = nezt U {(method(c), bind(c, \nb))}; rd = rd U read(rn , b); wr = wr U write(m , b); visited = visited U current; current = next visited; \nrd=rd L; WT=WT k, return(rd, wr); f ilter(rd, wr) foralls~rd foralls Ewr if(s S s ) rd = ? d {S}; wr \n= wr U{s}; elseif (s + s) rd= rd {s}; return(rd, WT); Figure 4: Extent Constant Variables Algorithm \n global visited = 0; extent(m, ec) ext= 0;aux = 0; for all c E callsites(m) if (c @ visited) visited \n= visited U lcj; (rd, WT) = transi(i~eEf f ects(method(c)); rd = map(bind(c, T), rd); WT = maD(bind(c, \nZ), wr); d = dep(c); if(wr QL&#38;&#38;rd~ec UL&#38;&#38;d ~ec)  aux = aux U {c}; else exf = ext U extent \n(method(c), ec); retunr (exr, au.+; Figure 5: Extent Algorithm checkReferenceParameters(m, e.u) if \n(referenceParameters(m) # 0) retain false; forallccext r D receiver(c); rn~=rnethod(c); foralls E map(bind(c, \nZ), referenceparameters( m )) if (mayPoint Into(s, r)) returnftike; (nit wr ) = transit iveEf fects(m \n); WT = map(bimd(c, z), WT); if (WT ~ CL. x V) retnnt false; returntrue; mayPoint Into(s, ? ) retunt \n!(s 6 L.&#38;&#38; type(s) C P); Figure 6: Check Reference Parameters Algorithm the compiler to extract \na more precise representation for the values computed in auxiliary operations, The compiler also checks \nthat auxiliary operations write all their return values into locrd variables of the caller. The compiler \ncan therefore omit auxiliary operations from the commutativity testing phase.  4.5 Reference Parameter \nChecks The compiler performs several checks to ensure that the symbolic execution operates correctly. \nTo preserve the property that refer\u00adence parameters always hold extent constant values, the compiler \nenforces the constraint that none of the operations in the extent write their reference parameters. To \nhelp ensure that the symbolic execution builds expressions that correctly denote the new values of instance \nvariables, it also enforces the constraint that none of the methods are invoked with a reference parameter \nthat points into the receiver, The current policy requires that all reference parameters be local variables \nof primitive types. Figure 6 presentsthe algorithm that checks these two restrictions. 4.6 Separability \nThe compiler must also check that each operation in the extert is separable. It therefore scanseach method \nto make sure that it never accessesan instance variable after it executes a call site that may invoke \nan operation in the extent, As part of the separability test it also makes sure that the method only \nwrites local variables or instance variables of the receiver and only reads parameters, lo\u00adcal variables, \ninstance variables of the receiver or extent constant variables, The separability of a method invocation \nmay depend on the set of auxiliary call sites and the set of extent constant vari\u00adables. The separabilityy \ntesting routine, s eparabl e(nz, aux, ec), therefore takesthesetwo setsasparameters. 4.7 Commutativity \nTesting The cotnrmrtativity testing algorithm presented in Figure 7 deter\u00admines if all invocations of \ntwo methods in the context of a given set of auxiliary crdl sites and extent constant variables commute. \nThe algorithm first checks to see if the invocations are always in\u00addependent. The check can be performed \nusing the type system if the classesof the receiver types are different and neither inherits from the \nother, the two methods can never execute with the samere\u00adceiver. The compiler can also analyze the instance \nvariable usageto check that neither method writes an instance variable that the other accesses.The algorithm \nnext checks if it can symbolically execute commute(ml, mz, aux, ec) if (independent(ml, mz)) returntrue; \nif (!analyzable(ml, au+ cc)) returnfalse; if (!analyzable(mz, at+ cc)) return false; (il,nl) = symbol \nicallyExecute(ml, mz, aux, cc); (iz, nz) = symbol icallyExecute(m2, ml, aux, cc); for all v c instanceVariables( \nreceiverClass(ml)) if ~~;:(sirnplif~(il (v)), sirnpli fy(i2(v)))) if ~~::::ee(~implif y(nl), simplif \ny(nz))) returntrue; Figure 7: Commutativity Testing Algorithm each of the methods. If not, it conservatively \nassumesthat invoca\u00adtions of the two methods may not commute. If it cart symbolically executethemethods,itdoesso,thensimplifies \ntheresultingexpres\u00adsions and compares corresponding expressions for equrdity. If all of the expressions \ndenote the same value, the operations commute. The compiler hastwo routines that deal with the symbolic \nexecu\u00adtion: anal yzable(nr, aux, ec), which determines if it is possible to symbolically execute a method, \nand S@Ol ical lYEXecute(ml, mz, aux, cc), which actually per\u00adforms the execution, The result of the execution \nis a pair (i, n), where i(v) is the expression denoting the new value of the instance variable v and \nn is a mukiset of m expressions denoting the multiset of directly invoked operations. Because the symbolic \nexecution de\u00adpends on the set of auxiliary call sites and the set of extent constant variables, both \nroutines take thesetwo setsasparameters. 4.8 Symbolic Analysis To test that method invocations always \ncommute, the compiler must represent and reason about the new values of the receiver s instance variables \nand the mukiset of operations directly invoked when the two methods execute. The compiler represents \nthe new values and multisets of invoked methods using symbolic expressions. Fig\u00adure 8 presents the symbolic \nexpressions that the compiler uses. These expressions include standard arithmetic and logical expres\u00adsions, \nconditional expressions and expressions that represent values computed in simple for loops. The compiler \nuses extent constants e C E to represent vrdues computed in auxiliary operations. @ represents an arbitrmy \nbinary operatoc G represents an arbitrary unstry operator. The compiler usesEXexpressionstorepresentinstancevariable \nvaluesandmul\u00adtisets of MX expressions to represent invoked methods. exCEX ::= EX@EXl@EXl i f (EX, EX, \nEX)lvle mxGMx :: = EX->op(EX)[if (EX, MX)l f or(kEX,kEX,~+= EX)MX Figure 8: Expressions for Symbolic \nAnalysis 4.8.1 Symbolic Execution The symbolic execution algorithm can operate successfully only on \na subsetof the constructs in the language. To execute an assignment statement, the algorithm symbolically \nevaluates the expression on the right hand side using the current set of bindings, then binds the computed \nexpression to the variable on the left hand side of 59 the assignment. It executesconditional statementsby \nsymbolically 5.1 Parallel Loops executing the two branches, then using conditional expressions to combine \nthe results, It executes auxiliary operations by internally generating anew extent constant for each \nreference parameter, then binding each reference parameter to its constant. The symbolic execution does \nnot handle loops in a general way. If the loop is in the following form, where exl is the upper bound \nof the array v and exz is an extent constant expression, the rdgorithm can represent the new value of \nv. for(l = Q 1< exl; 1+ +) v[Zl = v[~ 69 exz; If the loop is in the following form, where exl, ..., ex~ \nare all extent constant expressions, the algorithm can represent the invoked set of methods. for(l = \nexl; 1< exz; 1+ = ex3) ex4->op(ex5, . . . , exn); The algorithm cannot currently represent expressions \ncomputed in loops that are not in one of these two forms. We expect to enhance the algorithm to recognize \na wider range of loops. For analysis purposes the compiler can also replace unanalyzable loops with tail \nrecursive methods that perform the samecomputation,  4.8.2 Expression Simplification and Comparison \nThe expression simplifier is organized as a set of rewrite rules designed to reduce expressions to a \nsimplified form for comparison. The comparison itself consists of a simple isomorphism test. The compiler \ncurrently applies simple arithmetic rewrite rules such as exl exz ~ exl + ( exz), - ex ~ ex and exl \nx (exz + exs) * exl x exz + exl x exs. The simplifier also applies rules such as ( (exl +exz) +ex3) ~ \n(exl +ex2.+exq) that convert binary applications of commutative and associative opera\u00adtors to n-ary applications. \nIt then sorts the operands according to an arbitrary order on expressions. This sort facilitates the \neventual expression comparison by making it easier to identify isomorphic subexpressions. We have also \ndeveloped rules for conditional and array expressions [30]. In the worst case the expression manipulation \nalgorithms may take exponential running time. Like other researchers applying similar expression manipulation \ntechniques in other analysis con\u00adtexts [6], we have not observed this behavior in practice. Finally, \nit is undecidable in general to determine if two expressions always denote the same value [18]. We therefore \nfocus on developing algorithms that work well for the casesthat occur in practice. 5 Code Generation \n The compiler applies an optimization that exposesparallel loops to the run-time system. If a for loop \ncontains nothing but invocations of parallel versions of methods, the compiler generatesparallel loop \ncode instead of code that serially spawns each invoked operation. The generated code can then apply standard \nparallel loop execution techniques; it currently uses guided self-scheduling [26].  5.2 Suppressing \nExcess Concurrency In practice parallel execution inevitably generates overhead in the form of synchronization \nand task management overhead. If the compiler exploits too much concurrency, the resulting overhead may \noverwhelm the performance benefits of parallel execution. The compilerusesaheuristicthatattemptstosuppresstheexploitation \nof unprofitable concurrency; this heuristic suppressesthe exploitation of nested concurrency within parallel \nloops. To apply the heuristic, the compiler generates a third version of each parallel method, the mutex \nversion. Like the parallel version, the mutex version uses the mutual exclusion lock in the receiver \nto make the object section execute atomically. But the generated invocation section serially invokes \nthe mutex versions of all invoked methods. Any computation that starts with the execution of a mutex \nversion therefore executes serially, The inserted synchronization constructs allow the mutex versions \nof methods to safely execute concurrently with parallel versions. The generated code for parallel loops \ninvokes the mutex versions of methods rather than the parallel versions. Each iteration of the loop therefore \nexecutes serially. The heuristic trades off parallelism for a reduction in the con\u00adcurrency exploitation \noverhead. While it works well for our current application set,in somecasesit may generateexcessively \nsequential code. In the future we expect to tune the heuristic and explore effi\u00adcient mechanisms for \nexploiting the concurrency in nested parallel loops. 5.3 Local Variable Lifetimes The compiler must \nensure that the lifetime of an operation s activa\u00adtion record exceeds the lifetimes of all operations \nthat may access the activation record, The compiler currently uses a conservative strategy: if a method \nmay pass a local variable by reference to an operation or create a pointer to a local variable, the compiler \nserializes the computation rooted at that method. At auxiliary call sites the generated code invokes \nthe originaJ version of the invoked method, at other call sites it invokes the mutex version. This code \ngeneration strategy also ensuresthat operations observe the correct values of local variables written \nby multiple auxiliary operations. If the analysis marks a method as parallel, the compiler generates \ntwo versions of the method: a serial version and a parallel version. Methods marked as serial invoke \nthe serial version of any parallel method that they invoke. The generated code for the serial version \nof the parallel method simply invokes the parallel version, then blocks until the generated parallel \ncomputation terminates. It then returns back to the caller. To generate code for the parallel version, \nthe compiler first generates the object section of the method. The generated code acquires the mutual \nexclusion lock in the receiver when it enters the object section, then releases the lock when it exits. \nThe generated code for the invocation section invokes the parallel version of each invokedmethod,usingthespawn \nconstructtoexecutetheoperation in parallel, Auxiliary operations are an exception to this code generation \npolicy; they execute serially with respect to the caller. 5.4 Lock Optimizations Lock constructs are \na significant potential source of overhead, The code generator therefore applies several optimizations \ndesigned to reduce the lock overhead. 5.4.1 Lock Elimination If an operation s object section only computes \nextent constant val\u00adues, the compiler generates no lock constructs the operation executes atomically \neven without synchronization. If none of the parallel operationswith receiversfrom agivenclassusesthemutual \nexclusion lock, the compiler omits the lock from the class declara\u00adtion.  5.4.2 Lock Hoisting The program \ncontains no typedef or union types. The lock hoisting optimization eliminates lock constructs associated \nwith nested objects and coarsensthe lock granularity by generating a single lock acquireh-elease pair \nfor multiple operations that ac\u00adcess the same object. The optimization is based on the following observatiorx \nAssume that all operations o with receiver r in a given extent acquire r s lock and transitively only \ninvoke operations that have either ~ or nested objects of r as a receiver. In this casethe compiler can \ngenerate code for rdl of the corresponding methods that holds the lock for both the object and the invocation \nsections and invokes the original version of each invoked method. Thk version executes serially with \nno lock operations. In effect, the generated code usesthe lock on the enclosing object to ensure the \natornicity of operations that accessnested objects. It also generatesonly one lock acquirehelease pair \nfor computations rooted at outermost operations that meet the condition in the observation. Lock hoisting \ntrades off concurrency for a reduction in the lock overhead. In some casesthe optimization may generate \ncode that performs poorly becauseof excessiveserialization. While we may refine the optimization in the \nfuture, it works well for our current set of applications. 6 Experimental Results We have developed a \nprototype compiler based on the analysis al\u00adgorithms in Section 4. We have used this compiler to automatically \nparallelize two applications: the Barnes-Hut hierarchical N-body solver [3] and the Water [34] code,2 \nExplicitly parallel versions of the applications are available in the SPLASH [34] and SPLASH\u00ad2 [39] benchmark \nsuites, This section presents performance results for both the automatically parallelized and explicitly \nparallel ver\u00adsions on a 32 processor Stanford DASH machine [22] running a modified version of the IRIX \n5.2 operating system. The programs were compiled using the IRIX 5.3 CC compiler at the -02 opti\u00admization \nlevel.  6.1 The Compilation System Thecompiler isstructuredasasource-to-sourcetranslatorthattakes a \nserial program written in a subset of C++ and generates an ex\u00adplicitly parallel C++ program that performs \nthe same computation. We use Sage++ [7] as a front end. The analysis phase consists of approximately \n14,000 lines of C++ code, with approximately 1,800 devoted to interfacing with Sage++. The generated \ncode contains calls to a run-time library that provides the basic concurrency mrm\u00adagement and synchronization \nfunctionality, The library consists of approximately 5,000 lines of C code. The current version of the \ncompiler imposes several restrictions on the dialect of C++ that it accepts, The goal of these restrictions \nis to simplify the implementation of the prototype while providing enough expressive power to allow the \nprogrammer to develop clean object-based programs. The specific restrictions are: Theprogramhasnovirtual \nmethodsanddoesnotuseoperator or method overloading. The compiler imposes this restriction to simplify \nthe extent computation.  The program usesneither multiple inheritance nor templates.  The sequentialsourcecodesand \nautomaticallygeneratedparatlel codescanbe foundathttp: / /www.cs .ucsb. edu/wpedro/CA/apps. . Global \nvariables cannot be primitive data types; they must be class types. In addition to theserestrictions \nthe compiler assumesthat the pro\u00adgram has been type checked and does not violate its type declara\u00adtions. \n 6.2 Barnes-Hut Barnes-Hut is representative of our target class of applications. It performs well in \npart because it employs a sophisticated pointer\u00adbased data structure: a space subdivision tree that dramaticrdly \nimproves the efficiency of a key phase in the algorithm. Although it is considered to be an important, \nwidely studied computation, all previously existing parallel versions were parallelized by hand using \nlow-level, explicitly parallel programming systems [31, 33]. We are aware of no other compiler that is \ncapable of automatically parallelizing this computation. The space subdivision tree organizes the data \nas follows. The bodies are stored at the leaves of the tree; each internrd node rep\u00adresents the center \nof mass of all bodies below that node in the tree. Each iteration of the computation first constructs \na new spacesubdi\u00advision tree for the current positions of the bodies, It then computes the center of \nmass for all of the internal nodes in the new tree. The force computation phase executes next; this phase \nuses the space subdivision tree to compute the totrd force acting on each body. The finrd phaseusesthe \ncomputed forces to update the positions of the bodies. 6.2.1 The Serial C++ Code We obtained serial C++ \ncode for this computation by acquiring the explicitly parallel C version from the SPLASH-2 benchmark \nset, then removing the parallel constructs to obtain a serial version written in C. We then translated \nthe serial C version into C++. The goal of the translation process was to obtain a clean object-based \nprogram that conformed to the model of computation presented in Section 3. As part of the translation \nwe eliminated several computations that dealt with parallel execution. For example, the parallel ver\u00adsion \nused costzones partitioning to schedule the force computation phase [33]; the serial version eliminated \nthe costzones code and the associated data structures. We also split a loop in the force computation \nphase into three loops. This transformation exposed the concurrency in force computation phase, enabling \nthe compiler to recognize that two of the resulting three loops could execute in parallel. As part of \nthis transformation we also introduced a new instance variable into the body class. The new variable \nholds the force acting on the body during the force computation phase. When we ran the C++ version, we \ndiscovered that abstractions introduced during the translation processdegraded the serial perfor\u00admance. \nWe therefore hand optimized the computation by removing abstractions in the performance critical parts \nof the code until we had restored the original performance. These optimizations do not affect theparallelization; \nthey simply improve thebaseperformance of the computation. 6.2.2 Application Statistics ThefinsdC++versionconsistsofapproximately \n1500linesofcode; the explicitly parallel version consists of approximately 1900 lines of code. The compiler \ndetects five paratlel loops in the C++ code. Two of the loops are nested inside other parallel loops, \nso the heuristic described in Section 5.2 suppresses the exploitation of concurrency in these loops, \nThe generated parallel version contains three parallel loops. Table 2 presents several analysis statistics. \nFor each parallel extent it presents the number of auxiliary call sites in the extent, the number of \nmethods in the extent, the number of independent pairs of methods in the extent and the number of pairs \nthat the compiler had to symbolically execute. AU of the parallel extents have a significant number of \nauxiliary call sites; the compiler wouldbeunabletoparallelize anyoftheextentsifthecommutativity testing \nphase included the auxiliary operations. Most of the pairs of invoked methods in the extent are always \nindependent, which means that the compiler has to symbolically execute relatively few pairs. Pamtlel \nAuxitiary Operation Extent independent Symbiotically Extent Call Sies Size Pairs ExecutedPairs VelocitvI \n5 3 5 1 Force 9 617 4 PositionI 8 3 5 1 Table2: Analysis Statistics for Barnes-Hut  6.2.3 Performance \nResultS and Analysis Table 3presents theexecution times for Barnes-Hut. To eliminate cold start effects, \nthe instrumented computation omits the first two iterations. Inpractice thecomputation would perform \nmany itera\u00adtions and the amortized overhead of the first two iterations would be negligible. Thecolumn \nlabeled Serial contains the execution time fortheserirtl C++program. Thisprogram contains only sequential \nC++ code and executes with no parallelization or synchronization overhead. The rest of the columns contain \nthe execution times for the automatically parallelized version. Figure 9 presents the speedup for the \ncomputation. The computation scales reasonably well, exhibiting speedupsof between 11 and 12 out of 16 \nprocessors and 17 and 18 out of 32 processors. Number Processors ofBodies salI1I2I4I8I16 32 8192 %0 63.4 \n319 15.8 8.8 5.3 3.6 16384 146.9 151,8 I 79.9 I 39.0 I 21.9 I 13.2 87 I Table 3: Execution Times for \nBarnes-Hut (seconds) 32 32 , 28 ? 28 , i1 24 , 24 . 920 920 ~:: . ~;; . . , 84 8, 4 4 //, 1<( 0 0 \n048121620242832 048121620242832 Nrrmtwofprocessors Numberof Processors Barnes-Hut(8192bodies) Barnes-Hut(16384bodes) \nFigure9: Speedup for Barnes-Hut We start our analysis of the performance with the parallelism coverage \n[15], which measures the amount of time that the serial computation spends in parallelized sections. \nTo obtain good paral\u00adlel performance, the compiler must parallelize a substantial part of the computation. \nBy Amdahl s law any remaining serial sections of the computation impose an absolute limit on the parallel \nper\u00adformance. For example, even if the compiler parallelizes 90% of the computation, the parallel computation \ncan run at most 10 times faster than the serial computation. Table 4 presents the parallelism coverage \nfor Barnes-Hut; these statistics show that the compiler is able to parallelize almost all of the computation. \nNmnber SerialCompute Time in Parallelized Parallelism of Bodies Time (seconds) Sections(seconds) Coverage \n8192 64.14 62.87 98.02% 16384 153.20 148.34 96.83% Table 4: Parallelism Coverage for Barnes-Hut Good \nparallelism coverage is by itself no guarantee of good parallel performance. To exploit parallelism, \nthe compiler in\u00adevitably introduces synchronization and concurrency management overhead. If the granularity \nof tbe generated parrdlel computation is too small to successfully amortize the overhead, the parallel \npro\u00adgram will perform poorly even if it has good parallelism coverage. A standard problem with traditional \nparallelizing compilers, for ex\u00adample, hasbeen the difficulty of successfully amortizing the barrier \nsynchronization overhead at each parallel loop [36]. Our prototype compiler introduces four sources of \noverhead when it generates parallel code: Loop Overhead: Theoverheadgeneratedbytheexecutionof a parallel \nloop. Sources of this overhead include the commu\u00adnication at the beginning of the loop to inform all \nprocessors of the loop s execution and barrier synchronization at the end of the loop.  Chunk Overhead: \nThe overhead associated with acquiring a chunk of parallel loop iterations. Sources of this overhead \ninclude the computation that determines how many iterations the processor will take, the update of a \ncentralized counter that records which iterations have yet to be assigned to a specific processor for \nexecution and the lock constructs that make the chunk acquisition atomic.  Iteration Overhead: The overhead \ngenerated by the execu\u00adtion of one iteration of a parallel loop. This includes function call and argument \nunpacking overhead.  Lock Overhead: The overhead generated by the lock con\u00adstructs automatically inserted \nint~ methods ~o make opera\u00adtions execute atomically.  We developed a benchmark program to measure the \ncost of each sourceofoverhead.Table5presentstheresults. Theloop overhead increaseswith the number of \nprocessors;the table presentsthe loop overhead on 32 processors Loop Overhead Chrmk Iteration Lock On32Processors \nOverhead Overhead Overhead 211 30 0.38 5.1 Table 5: Parallel Construct Overhead (microseconds) For each \nsource of overhead the applications execute a corre\u00adsponding piece of useful work the loop overhead \nis amortized by the parallel loop, the chunk overhead is amortized by the chunk of iterations, the iteration \noverhead is amortized by the iteration and the lock overhead is amortized by the computation between \nlock acquisitions. The relative size of each piece of work determines if the overhead will have a significant \nimpact on the performance, Tables6and11presentthemeansizesofthepiecesof useful work. The numbers in the \ntables are computed as follows: Loop Size: The time spentin parallelized sectionsdivided by the number \nof executed parallel loops, A comparison with the Loop Overhead number in Table 5 shows that the amortized \nloop overhead is negligible.  Chunk Size: The time spent in parallelized sections divided bythetotalnumber \nofchunks, Becausethenumberofchunks tendstoincreasewith thenumberofprocessors,wereportthe chunk size at \n32 processors. A comparison with the Chunk Overhead in Table 5 shows that the amortized chunk overhead \nis negligible. . Iteration Size: The time spent in parallelized sectious di\u00advided by the total number \nof iterations in executed parallel loops. A comparison with the Iteration Overhead in Ta~ble5 shows that \nthe amortized iteration overhead is negligible.  Task Size: The time spent in parallelized sections \ndivided by the number of times that operations acquire a lock. A comparison with the Lock Overhead in \nTable 5 shows that the amortized lock overhead is negligible.  Number Loop CharrkSize Iteration Task \nof Borties Size 32 Processors Size Size 8192 10.5 x 106 68.7 X Id 1.28 X 103 1.28 X 10:V 16384 24.7 X \n106 134 x 103 1.51 x 103 1.51 x 10 Table 6: Granularities for Barnes-Hut (microseconds) We instrumented \nthe generated parallel code to measure how much time each processor spends in different parts of the \nparallel computation. The instrumentation breaks the execution time down into the following categories: \n Parallel Idle: The amount of time the processor spendsidle while the computation is in a parallel section. \nIncreases in the load imbalance show up as increases in this component.  Serial Idle: The amount of \ntime the processor spends idle when the computation is in a serial section. Currently ev\u00adery processor \nexcept the main processor is idle during the serial sections. This component therefore tends to increase \nlinearly with the number of processors, since the time the main processorspendsin serialsectionstendsnottoincrease \ndramatically with the number of processors.  Blocked: The amount of time the processor spends wait\u00ading \nto acquire a lock that an operation executing on another processor has already acquired. Increases in \ncontention for objects are reflected in increases in this component of the time breakdown,  Parallel \nCompute: The amount of time the processor spends performing useful computation during aparallel sectionof \nthe computation. This component also includes the lock over\u00adhead associated with an operation s first \nattempt to acquire a lock, but does not include the time spent waiting for another processor to release \nthe lock if the lock is not available. In\u00adcreasesin the communication of application data during the \nparallel phasesshowup asincreasesin this component.  . Serial Compute: The amount of time the processor \nspends performing useful computation in a serial section of the pro\u00adgram. With the current parallelization \nstrategy, the main processor is the only processor that executes any useful work in a serial part of \nthe computation. Given the execution time breakdown for each processor, we compute the cumulative time \nbreakdown by taking the sum over all processors of the execution time breakdown at that processor. Figure \n10presentsthecumulative timebreakdownsasafunction of the number of processorsexecuting the computation. \nThe height of eachbar in the graph representsthe total processing time required to executetheparallelprogram,thedifferent \ngrayscaleshadesineach bar represent the different time breakdown categories, If a program scales perfectly \nwith the number of processors then the height of the bar will remain constant asthe number of processorsincreases. \n120 303 ~m o E,ue 103 7 50 i ! E!! L:d -so t~ jz~ o Blccked $150 , &#38;m&#38;k;E $60 :40 ~ Y m g L. \n::: . 220 = 50 d d o 0 124 8162432 1 248162432 Numter of Prwemm Numta of kkccemm Barnes-Hut(8k bodies) \nBarnes-Hut(16kbodies) Figure 10 Cumulative Time Breakdowns for Barnes-Hut These graphs show that the \nlimit on the performance is the time spentintheserialphasesofthecomputation at32processorsthe serial \nidle time accounts for approximately 40% of the cumulative compute time for both applications. 6.2.4 \nComparison with the Explicitly Parallel Version Table 7 contains the execution times for the explicitly \nparallel ver\u00adsion of Barnes-Hut. For small numbers of processors the automat\u00adically parallelized and \nexplicitly parallel versions exhibit roughly comparable performance. For larger numbers of processors \nthe ex\u00adplicitly parallel version performs significantly better at 32 pro\u00adcessorsit runs 50% faster for \n8192 bodies and 70% faster for 16384 bodies than the automatically parallelized version. The largest \ncontribution to the performance difference is that the explicitly par\u00adallel version builds the space \nsubdivision tree in parallel, while the automatically parallelized version builds the tree serially. \nThe ex\u00adplicitly parallel version also usesan application-specific scheduling algorithm called costzones \npartitioning in the force computation phase [33]. This algorithm provides better locality than the guided \nself-scheduling algorithm in the automatically parallelized version. Number Processors ofBodies 1 2 4 \n8 I I I 116132. 8192 73.1 35.2 16.8 8.4 4.4 ] 2.4 16384 154.0I 77,4 ] 36.8 I 19.2 I 9.7 I 5.1 Table \n7: Execution Times for Explicitly Parallel Barnes-Hut (sec\u00adends)  6.3 Water The main data structure \nin Water is an array of molecule objects. Almost all of the compute time is spent in two 0(n2) phases. \nOne phase computes the total force acting on each molecule; the other phase computes the potential energy \nof the collection of molecules. 6.3.1 The Serial C++ Code The original source of Water is the Perfect \nClub benchmark MDG, which is written in Fortran. Several students at Stanford University Number of Processors \nII Molecules s 2 4 16 32 ~ 41.0 I 21.3 16.2 ~ 13,s 512 158.1 167.5 90.4 I 46.7 I 31.5 29.9 I 37.8 Table \n9: Execution Times for Water (seconds) 32 32 , , 2s 28. . i , 1 , 24 24 translated this benchmark from \nFortran to C aspart of aclassproject. We obtained the serial C++ version by translating this existing \nserial C version to C++. As part of the translation processwe converted the O(n2 ) phases to use auxiliary \nobjects tailored for the way each phase accesses data.Beforeeachphasethecomputation loadsrelevantdataintoan \nauxiliary object; attheendofthephasethecomputation unloadsthe computed values from the auxiliiwy object \nto update the molecule objects. This modification increases the precision of the data usage analysis \nin the compiler, enabling the compiler to recognize the concurrency in the phase. 6.3.2 Application \nStatistics The final C++ version consists of approximately 1850 lines of code, the serial C version consists \nof approximately 1220 lines of code and the explicitly parallel version in the SPLASH benchmark suite \nconsists of approximately 1600 lines of code. Much of the extra code in the C++ version comes from the \npervasive use of classes and encapsulation. Instead of directly accessing many of the data structures \n(asthe C versions do), the C++ version encapsulatesdata in classes and accessesthe data via accessor \nmethods. The class declarations and accessormethod definitions significantly increase the size of the \nprogram. The use of a vector class instead of arrays of doubles, for example, added approximately 230 \nlines of code. The analysis finds a total of seven parallel loops. TWOof the loopsarenestedinsideotherparallel \nloops,sothegeneratedparallel version contains five parsllel loops. Table 8 contains the analysis statistics; \nas for the Barnes-Hut all of the extents contain auxil\u00adiary operation call sites and most of the pairs \nin the extents are independent. Parallel I Auxiliary Okration Extent Car-lSl;es Vktnal 9 I Energy 1 \nLoading 5 Forces 3 Momenta 2 m Table 8: Analysis Statistics for Water 6.3.3 Performance Resulta and \nAnalysis Table 9 contains the execution times for Water. The measured computation omits initial and final \nI/O. In practice the computation would execute many iterations and the amortized overhead of the I/O \nwould be negligible. Figure 11 presents the speedup curves. Water initially performs well (the speedup \nover the sequential C++ version at 8 processors is approximately 4.5 for 343 molecules and 5 for512 molecules), \nbut it does not scale beyond 8 processors. Table 10, which presents the parallelism coverage for this \nap\u00adplication, shows that the compiler parallelizes almost all of the  !LL-lL!_\u00ad 048121620242S32 048121620242832 \nNumber of processors Numbsr of Processors Water(343molecules) Water(512molecules) Figure 11: Speedup \nfor Water computation. Table 11 shows that all of the sources of overhead are negligible except for the \nlock overhead. The lock overhead by itself, however, does not explain the lack of scalability. Number \nof Serial Compute Time in Parallelized Parallelism Molecules Time (seconds) Sections (seconds) Coverage \n343 70.89 69.97 98.70% 512 154.29 152.85 99.07% Table 10: Parallelism Coverage for Water Numberof Loop \nChunkSize Iteration Task Molecules Size 32 Processors Size Size 343 3.50 x 106 34.0 x 103 10.2 x 103 \n74.2 512 7.64 x 106 69.1 X 103 14.9 x 103 72.8 Table 11: Granularities for Water (microseconds) Figure \n12, which presents the cumulative time breakdowns for Water, clearly shows why Water fails to scale beyond \neight pro\u00adcessors. The fact that the blocked component grows dramatically while all other components \neither grow relatively slowly or remain constant indicates that contention for objects is the primary \nsource of the lack of scalability. It should, in principle, be possible to auto\u00admatically eliminate the \ncontention by replicating objects to enable conflict-free write access. We expect that this optimization \nwould dramatically improve the scalability. 6.3.4 Comparison with the Explicitly Parallel Version The \nSPLASH parallel benchmark set contains an explicitly parallel version of Water; Table 12 contains the \nexecution times for this ver\u00adsion. Unlike the automatically parallelized version, the explicitly parallel \nversion scales reasonably well to 32 processors. We at\u00adtribute this difference to the fact that the explicitly \nparallel version replicates several data structures, eliminating the contention that limits the performance \nof the automatically parallelized version. 14s0 o p&#38; a 12C0 i 1 o E* Km p z o Blw ked ~sw ~3c13 , \nPdde z! ~LUsl* ,:2ml g j4cQ . $: . [1 jml # ~ 21N o 0 1 248162432 124 8162432 Number of Pm%wm Number \nof hccewm Water(343molecules) Water(512molecules) Figure 12: Cumulative Time Breakdowns for Water  \n6.4 Caveats The goal of our project is to enable programmers to exploit both the performance advantages \nof parallel execution and the substantial programming advantagesof the sequential programming paradigm. \nWe view the compiler as a tool that the programmer usesto achieve reliable parallel execution with a \nminimum of effort, We expect that the programmer will need a reasonable understanding of the compiler \ns capabilities to use it effectively. In particular, we do not expect to develop a compiler capable of \nautomatically parallelizing a wide range of existing dusty deck programs. Several aspects of our experimental \nmethodology reflect this perspective. As part of the translation process from C to C++ we ensured that \nthe C++ program conformed to the model of computa\u00adtion that the compiler was designed to analyze. We \nbelieve that this approach accurately reflects how parallelizing compilers in general will be used in \npractice. We expect that programmers may have to tune their programs to the capabilities of the compiler \nto get good performance. The experience of other researchers supports this hypothesis [4, 5]. For our \ntwo applications it was relatively straightforward to produce code that the compiler could successfully \nanalyze. Almost all of the translation effort was devoted to expressing the computa\u00adtion in a clean object-based \nstyle with classes,objects and methods instead of structures and procedures. The basic structure of both \napplications remains intact in the final C++ versions, and the C++ versions have better encapsulation \nand modularity properties than the C versions. We selected Barnes-Hut and Water in part because other \nre\u00adsearchershad developed explicitly parallel versions that perfonmed well. We therefore knew that it \nwas possible in principle to p,aral\u00adlelize the applications, The question was whether commutativity analysiswouldbeabletoautomatically \ndiscoverandexploitthecon\u00adcurrency. In general we expect programmers to use the compiler to parallelize \napplications that have enough inherent concurrency to keep the machine busy. 7 Future Research The ideas \nand results in this paper suggestmany possible directions for future research. In this sensewe believe \nthe paper is only a start: Number of Processors IIIII Molecules 1 2 4 8 16 32 343 75.68 I 41.93 I 20,13 \n10.35 5.39 I 3.39 512 166.54 I 81.98 I 40.33 I 2122 ] 11.46 ] 6.76 Table 12: Execution Times for Explicitly \nParallel Water (seconds) the culmination and ultimate evaluation of commutativity analysis stilllieaheadofus.Inthissectionwebriefly \nmentionseveralfuture research directions, 7.1 Relative Commutativity The current formulation of commutativity \nanalysis is absolute. Dur\u00adingtheexecutionofaparallelized sectionofcode,thedatastructures in the parallel \nand serial versions may diverge. But the compiler guaranteesthat by the end of the parallel section the \ndata structures in the two versions have converged to become identical. This formulation is obviously \noverly conservative. To preserve the semantics of the serial program, it is sufficient to preserve the \nproperty that the parallel and serial computations generate data structures that are equivalent with \nrespect to the rest of the com\u00adputation, For example, the output of the explicitly parallel tree construction \nalgorithm in the Barnes-Hut depends on the relative execution speedof the different processors: different \nexecutions on the sameinput may generate different data structures. But because all of these data structures \nare equivalent with respect to the rest of the program, the program as a whole executes determitistically. \nIt maybe possible to extend the commutativity analysis frame\u00adwork to automatically generate parallel \ncode for algorithms such as the tree construction algorithm in Barnes-Hut. The current formula\u00adtion works \nwell for data structure traversal algorithms; the compiler may need to extend the framework to recognize \noperations that commute relative to the rest of the computation if it is to effectively parrrllelize \ndata structure construction algorithms. 7.2 Analysis Granularity Our experience with auxiliary operations \nshows that the correct analysis granularity does not always correspond to the granularity of methods \nin the source program. In both Water and Barnes-Hut the method granularity is too fine: for the analysis \nto succeed it must coarsen the granularity by concepturdly integrating auxiliaxy operations into their \ncallers. We expect a generalized concept of auxiliary operations to eventually emerge, with the compiler \npro\u00admoting the successof the analysis by partitioning the program at an appropriate granularity. Several \nissues confront the designer of a portioning algorithm, First, the analysis granularity interacts with \nthe locking algorithm, If the analysis is performed at the granularit y of computations that manipulate \nmultiple objects, the generated code may need to hold multiple locks to make the computation atomic. \nThe need to acquire these locks without deadlock may complicate the code generation algorithm. This issue \ndoes not arise in the current compiler because itanalyzesthecomputation atthegranularity ofoperationsonsingle \nobjects and generates code that only holds a single lock at a time. There is tradeoff between increased \ngranularity and analyzabil\u00adity: increasing the analysis granularity may make it difficult for the compiler \nto extract expressionsthat accurately denote the computed values. In some cases,the compiler may even \nneed to analyze the program at a finer granularity than the method granularity. This can happen, for \nexample, if a method contains an otherwise unan- alyzable loop. Replacing the loop with a tail-recursive \nmethod and analyzing the computation at that finer granularity may enable the analysis to succeed. Finally, \ncoarsening the analysis granularity may waste concur\u00adrency. The compiler must ensure that it analyzes \nthe computation at a granularity fine enough to expose a reasonable amount of con\u00adcurrency in the generated \ncode. 7.3 A Message-Passing Implementation they manipulate. An example of such a computation is a query \nthat The current compiler relies on the hardware to implement the ab\u00adstraction of shared memory. It is \nclearly feasible, however, to gen\u00aderate code for message-passingmachines. The basic required ftmc\u00adtionality \nis a software layer that usesmessage-passingprimitives to implement the abstraction of a single shared \nobject store [28, 32]. The key question is how well the generated code would perform on such a platform. \nMessage-passing machines have traditionally suf\u00adfered from much higher communication costs than shared-memory \nmachines. Compilation research for message-passing machines has therefore emphasized the development \nof data and computation placement algorithms that minimize communication [17]. Given the dynamic nature \nof our target application set,the compiler would havetorelyondynamic techniquessuchasreplication andtaskmi\u00adgration \nto optimize the locality of the generated computation [9, 29].  7.4 Pointer Analysis The algorithms \nin Section 4 perform a data usage analysis at the granularity of the type system. An obvious alternative \nis to use pointer analysis [12, 38] to identify the regions of memory that eachoperation may access.A \nmajor advantageof this approachfor non type-safe languages like C and C++ is that it would allow the \ncompiler to analyze programs that may violate their type dechua\u00adtions. It would also characterize the \naccessedregions of memory at a finer granularity than the type system, which would increase the precision \nof the data usage analysis. One potential drawback is a complication of the compiler. The use of pointer \nanalysis would also increase the amount of code that the compiler would have to analyze. Before the compiler \ncould parallelize a piece of code that manipulated a given pointer-based data structure, it would have \nto analyze the code that built the data structure. 8 Related Work This section briefly surveys previous \nresearch in the area of par\u00adallelizing compilers for computations that manipulate irregular or pointer-based \ndata structures. We also discuss reduction analysis research. 8.1 Data Dependence Analysis Research on \nautomatically parallelizing serial computations that manipulate pointer-based data structures has focused \non techniques that precisely represent the run-time topology of the heap [16, 21, 10, 25]. The idea is \nthat the analysis can use this precise rep\u00adresentation to discover independent pieces of code. To recognize \nindependent pieces of code, the compiler must understand the global topology of the manipulated data \nstructures [16,21]. It must there\u00adfore analyze the code that builds the data structures and propagate \nthe results of this analysis through the program to the section that usesthe data. A limitation of thesetechniques \nis aninherent inabil\u00adit y to parallelize computations that manipulate graphs. The aliases present in \nthese data structures preclude the static discovery of in\u00addependent pieces of code, forcing the compiler \nto generate serial code. Commutativity analysis differs substantially from data depen\u00addence analysis \nin that it neither depends on nor takes advantage of the global topology of the data structure. This \nproperty enables commutativity analysis to parallelize computations that manipulate graphs. It also eliminates \nthe need to analyze the data structure construction code. Commutativity analysis may therefore be ap\u00adpropriate \nfor computations that do not build the data structures that manipulates persistent data stored in an \nobject-oriented database. But insensitivity to the data structure topology is not always an advantage. \nStandard code generation schemes for commutativity analysis insert synchronization constructs to ensure \nthat operations executeatomically. Theseconstructsimposeunnecessaryoverhead when the operations access \ndisjoint sets of objects. If a compiler can use data dependence analysis to recognize that operations \nare independent, it can generate parallel code that contains no syn\u00adchronization constructs. In the long \nrun we believe parallelizing compilers will incorporate both commutativity analysis and data dependence \nanalysis, using each when it is appropriate. 8.2 Reductions Several existing compilers can recognize \nwhen a loop performs a reduction of many values into a single value [14, 13, 24, 8]. These compilers \nrecognize when the reduction primitive (typically addi\u00adtion) is associative. They then exploit this algebraic \nproperty to eliminate the data dependence associated with the serial accumu\u00adlation of values into the \nresult. The generated program computes the reduction in parallel. Researchershave recently generalized \nthe basic reduction recognition algorithms to recognize reductions of arrays instead of scalars. The \nreported results indicate that this opti\u00admization is crucial for obtaining good performance for the measured \nset of applications [15], There are interesting connections between reduction analysis and commutativit \ny analysis. Many (but not all) of the computations that commutativity analysis is designed to handle \ncan be viewed as performing multiple reductions concurrently across a large data structure, The need \nto exploit reductions in traditional data parallel computations suggeststhat lessstructured computations \nwill require generalized but similar techniques. 9 Conclusion The difficulty of developing explicitly \nparallel software limits the enormous potential of parallel computing. The problem is espe\u00adcially acute \nfor irregular, dynamic computations that manipulate pointer-based data structures such as graphs. Commutativity \nanal\u00adysis addressesthis problem by promising to extend the reach of parallelizing compilers to include \npointer-based computations. We have developed a parallelizing compiler that uses commu\u00adtativity analysis \nas its main analysis paradigm. We have used this compiler to automatically parallelize two complete scientific \nappli\u00adcations, The performance of the generated code provides encour\u00adaging evidence that commutativity \nanalysis can serve as the basis for a successful parallelizing compiler. Acknowledgements The authors \nwould like to thank the anonymous referees for their many vrduable comments. References [1] U. Banerjee.Dependence \nAnalysis for Supercomputing. Kluwer Aca\u00addemic Publishers, Boston, MA, 1988. [2] U. Bauerjee,R. Eigenmaun,A. \nNicolau, and D. Padua. Automatic programparatlelization. Proceedings of the IEEE, 81(2):21 1-243, Febmary1993. \n[3] J.BarnesandP,Hut. A hierarcbicrdO(NlogN)force-calculationalgo\u00adrithm Natare, pages446-449, December \n1976. [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14] [15] [16] [17] [18] [19] [20] M. Berry, D. Chen, \nP. Koss, D. Kuck, S. Lo, Y. Paog,L. Pc~inter, R. Roloff, A. Sameh,E. Clementi, S. Chin, D. Schneider,G. \nFox, P.Messin~D.Walker,C.Hsiung,J.Schwarzmeier,K. Lue, S.Orszag, F. Seidl, O. Johnson, R, Goodmm, and \nJ. Martin. The Perfect C1ub benchmarks: Effective performance evaluation of supercomputers. ICASE Report \n827, Center for Supercomputing Research aod Devel\u00adopment, University of Illinois at Urbana-Champaign, \nUrban&#38; IL, May 1989. W. Blume and R. Eigenmarm. Performance analysis of paraflelizing compifers on \ntJrePerfect Benchmmks programs. IEEE Transactions on Parallel and Distributed Systems, 3(6):643+56, November \n1992. W. Blume and R. Elgemnamt. Symbofic range propagation. Irt Pro\u00adceedings of the 9th International \nParallel Processing Symposium, pages 357-363, Santa Barbara, CA, April 1995. F. Bodin, P.Beckman, D. \nGarmon, J, Gotwafs, S. Narayrrn&#38; S. Srinivas, and Beata Wirmicka. Sage+ An object-oriented toolkit \nand class libmry for buildbrg Fortran and C++ structuring tools. In Proceedings of the Object-Oriented \nNumerics Conference, 1984. D. Callahan. Recognizing and pamflelizing bounded recurrences. In Proceedings \nof the Fourth Workshop on Languages and Compilers for Parallel Computing, Santa Clara, CA, August 1991. \nM. Carlisle and A. Rogers. Software caching aod computation rr@ra\u00adtion in Olden. In Proceedings of the \nFlflh ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, Santa Barbar&#38; CA, \nhdy 1995. D. Chase, M. Wegmarr, and F. Zadek. Anafysis of pointers and stmc\u00adtures. In Proceedings of \nthe SIGPL4N 90 Conference on Program Language Design and Implementation, White Plains, NY June 1990. \nR. Eigenrmutn, J. Hoeflinger, Z. LI, and D. Padua. Experience in the automatic pamflelization of four \nPerfect benchmark programs. In U. Banerjee, D. Gelernter, A. N1colau, and D. Padua, editors, Lang\u00aduages \nand Compilers for Parallel Computing, Fourth International Workshop, Santa Clar&#38; CA, August 1991. \nSpringer-Verlag. M. Emami, R. Ghiy~ and L. Hendren. Context-sensitive intezpro\u00adcedurrd points-to anafysis \nin the presence of function pointers. In Proceedings of the SIGPL4N 94 Conference on Program Language \nDesign and Implementation, Orlando, FL, June 1994. A. Fisher and A. Ghtdoum. Pamflefizing complex scans \nand reduc\u00adtions. In Proceedings of the SIGPLAN 94 Conference on Program Language Design and Implementation, \nOrlando, FL, June 1994. A. Ghuloum and A. Fisher. Fattening and parallefizing irqydar, recurrent loop \nnests. In Proceedings of the F@h ACM SIGPLAN Symposium on Principles andPractice of Parallel Programming, \nSanta BarbarA CA, July 1995. M.W. Hafl, S.P. Amamsinghe, B.R. Murphy, S. Liao, and M. S..Lam. Detecting \ncoarse-grain parallelism using an interprocedoral parafieliz\u00ading compiler. In Proceedings of Supercomputing \n95, December 1995. L. Hendren, J. Hmmnel, and A. Nicolau. Abstractions for recursive pointer data structures: \nImproving the rurafysis and transformation of imperative programs. In Proceedings of the SIGPLAN 92 Conference \non Program Language Design and Implementation, San Francisco, CA, June 1992, S. Hwanandani, K. Kennedy, \nand C. Tseng. Compiling Fortran D for MIMD distributed-memory machines. Communications of ?he ACM, 35(8):66-80, \nAugust 1992.  0. Ibarra,P.Diniz, andM.Rinard.Onthecomplexity ofcommutativity anafysis. In Proceedings \nof the 2nd Annual International Computing and Combinatorics Conference, Hong Kong, June 1996. R. Kemmerer \nand S. Eckmamr. UNISEX: a UNIx-based Symbofic EXecutor for pascaf. SoJlware-Practice and E.qrerience, \n15(5):439\u00ad458, Mrry 1985. Butler W. Lampson and David D. Redell. Experience with processes nrrd monitors \nin Mesa. Communications of rtze ACM, 23(2):105-117, February 1980. [21] J. Larus and P. Hilfinger. Detecting \nconflicts between structure ac\u00adcesses. In Proceedings of the SIGPLAN 88 Conference on Program Lunguage \nDesign and Implementation, Atfao@ GA, June 1988. [22] D. Lenoski. The Design and Analysis of DASH: A \nScalable Directory-Based Multiprocessor. PhD thesis, Stanford, CA, February 1992. [23] E. Mohr, D. Kranz, \nand R. Hafstead. Lazy task creation: a technique for increasing the granularity of paraflel programs. \nIn Proceedings of the 1990 ACM Conference on Lisp and Functional Programming, pages 185 197, June 1990. \n[24] S. Pinter atrd R. Pinter. Program optimization and paraflelization using idioms. In Proceedings \nof the Eighteenth Annual ACM Symposium on the Principles of Programming Languages, Orlando, FL, January \n1991. [25] J. Plevyak, V. Krwarncheti, and A. Chien. Analysis of dynamic stmc\u00adtures for efficient paraflel \nexecution. hr Proceedings of /he Sixth Work\u00adshop on Languages and Compilers for Parallel Computing, Portfattd, \nOR, August 1993. [26] C. Polychronopoulos and D. Kuck. Guided self-scheduling: A pmcti\u00adcal scheduling \nscheme for parallel computers. LEEE Transactions on Parallel and Distributed Systems, pages 1425-1439, \nDecember 1987. [27] W. Pugh and D. Wonnacott. Eliminating fafse data dependence using the Omega test. \nIn Proceedings of the SIGPLAN 92 Conference on Program .Latguage Design and Implementation, San Francisco, \nCA, June 1992. [28] M. Rinard. The Design, Implementation and Evaluation of Jade, a Portable, Implicitly \nParallel Programming Language. PhD thesis, Startford, CA, 1994. [29] M. Rinard. Communication optimization \nfor parallel computing using data accessinformation. In Proceedings of Supercomputing 95. Sarr Diego, \nCA, December 1995. [30] M. Rinard and P. Diniz. Commutativity anafysis: A technique for au\u00adtomatically \nparaflelizing pointer-based computations. In Proceedings of the 10th International Parallel Processing \nSymposium, Honolulu, HI, April 1996. [31] J. K. Srdmon. Parallel Hierarchical N-body Methods. PhD thesis, \nCafifomia Institute of Technology, December 1990. [32] D. Scafesand M. S. Lam. The design and evacuation \nof a shared object system for distributed memory machines. In Proceedings of the First USENLY Symposium \non Operating Systems Design and Implementa\u00adtion, Monterey, CA, November 1994. [33] J. Singh. Parallel \nHierarchical N-body Methods and their Implications for Multiprocessors. PhD thesis, Stanford University, \nFebruary 1993. [34] J. Singh, W. Weber, and A. Gupta. SPLASH: Stanford paraflel appli\u00adcations for shared \nmemory. Computer Architecture News, 20( 1):5-44, March 1992. [35] G, Steele. Making asynchronous paraflefism \nsafe for the world. In Proceedings of the Seventeenth Annual ACM Symposium on the Prin\u00adciples of Programming \nLunguages, pages 2 18 231, San Francisco, CA, January 1990. [36] C. Tseng. Compiler optimization for \neliminating barrier synchro\u00adnization. In Proceedings of the F@h ACM SIGPLAN Symposium on Principles and \nPractice of Parallel Programming, pages 144-155, Santa BarbarA CA, hdy 1995. [37] W. Weihf. Commutativity-based \nconcumcncy control for abstract data types. IEEE Transactions on Computers, 37(12): 1488 1505, Decem\u00adber \n1988. [38] R. Wilson and M. Lam. Efficient context-sensitive pointer anafysis for C programs. In Proceedings \nof the SIGPLAN 95 Conference on Program Language Design and Implementation, La Jofl~ CA, June 1995. [39] \nS.Woo,M.Ohar%E.Tome,J.P.Singh,andA.Gupta.TheSPLASH\u00ad2 programs: Characterization and methodological considerations. \nIn Proceedings of the 22th International Symposium on Computer Archi\u00adtecture, Santa Margherha L@re, Itafy, \nJune 1995.  \n\t\t\t", "proc_id": "231379", "abstract": "This paper presents a new analysis technique, commutativity analysis, for automatically parallelizing computations that manipulate dynamic, pointer-based data structures. Commutativity analysis views the computation as composed of operations on objects. It then analyzes the program at this granularity to discover when operations commute (i.e. generate the same final result regardless of the order in which they execute). If all of the operations required to perform a given computation commute, the compiler can automatically generate parallel code.We have implemented a prototype compilation system that uses commutativity analysis as its primary analysis framework. We have used this system to automatically parallelize two complete scientific computations: the Barnes-Hut N-body solver and the Water code. This paper presents performance results for the generated parallel code running on the Stanford DASH machine. These results provide encouraging evidence that commutativity analysis can serve as the basis for a successful parallelizing compiler.", "authors": [{"name": "Martin C. Rinard", "author_profile_id": "81100087275", "affiliation": "Department of Computer Science, University of California, Santa Barbara, Santa Barbara, CA", "person_id": "P192533", "email_address": "", "orcid_id": ""}, {"name": "Pedro C. Diniz", "author_profile_id": "81377592829", "affiliation": "Department of Computer Science, University of California, Santa Barbara, Santa Barbara, CA", "person_id": "PP39071146", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/231379.231390", "year": "1996", "article_id": "231390", "conference": "PLDI", "title": "Commutativity analysis: a new analysis framework for parallelizing compilers", "url": "http://dl.acm.org/citation.cfm?id=231390"}