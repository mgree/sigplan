{"article_publication_date": "01-23-2013", "fulltext": "\n Checking NFA Equivalence with Bisimulations up to Congruence Filippo Bonchi Damien Pous * CNRS, ENS \nLyon, Universit\u00b4 e de Lyon, LIP (UMR 5668) {.lippo.bonchi,damien.pous}@ens-lyon.fr Abstract We introduce \nbisimulation up to congruence as a technique for proving language equivalence of non-deterministic .nite \nautomata. Exploiting this technique, we devise an optimisation of the classical algorithm by Hopcroft \nand Karp [18]. We compare our approach to the recently introduced antichain algorithms, by analysing \nand re\u00adlating the two underlying coinductive proof methods. We give con\u00adcrete examples where we exponentially \nimprove over antichains; experimental results moreover show non negligible improvements. Categories and \nSubject Descriptors F.4.3 [Mathematical Logic]: Decision Problems; F.1.1 [Models of computation]: Automata; \n D.2.4 [Program Veri.cation]: Model Checking Keywords Language Equivalence, Automata, Bisimulation, Coin\u00adduction, \nUp-to techniques, Congruence, Antichains. 1. Introduction Checking language equivalence of .nite automata \nis a classical problem in computer science, which .nds applications in many .elds ranging from compiler \nconstruction to model checking. Equivalence of deterministic .nite automata (DFA) can be checked either \nvia minimisation [17] or through Hopcroft and Karp s algorithm [2, 18], which exploits an instance of \nwhat is nowadays called a coinduction proof principle [25, 27, 29]: two states recognise the same language \nif and only if there exists a bisimulation relating them. In order to check the equivalence of two given \nstates, Hopcroft and Karp s algorithm creates a relation containing them and tries to build a bisimulation \nby adding pairs of states to this relation: if it succeeds then the two states are equiva\u00adlent, otherwise \nthey are different. On the one hand, minimisation algorithms have the advantage of checking the equivalence \nof all the states at once (while Hopcroft and Karp s algorithm only check a given pair of states). On \nthe other hand, they have the disadvantage of needing the whole au\u00adtomata from the beginning, while Hopcroft \nand Karp s algorithm can be executed on-the-.y [13], on a lazy DFA whose transitions are computed on \ndemand. * Work partially funded by the PiCoq project, ANR-10-BLAN-0305 and by the COGIP (PEPS-CNRS) project. \nPermission to make digital or hard copies of all or part of this work for personal or classroom use is \ngranted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. POPL \n13, January 23 25, 2013, Rome, Italy. Copyright c &#38;#169; 2013 ACM 978-1-4503-1832-7/13/01. . . $10.00 \nThis difference is fundamental for our work and for other re\u00adcently introduced algorithms based on antichains \n[1, 31]. Indeed, when starting from non-deterministic .nite automata (NFA), the powerset construction \nused to get deterministic automata induces an exponential factor. In contrast, the algorithm we introduce \nin this work for checking equivalence of NFA (as well as those in [1, 31]) usually does not build the \nwhole deterministic automaton, but just a small part of it. We write usually because in few bad cases, \nthe algorithm still needs exponentially many states of the DFA. Our algorithm is grounded on a simple \nobservation on deter\u00adminised NFA: for all sets X and Y of states of the original NFA, the union (written \n+) of the language recognised by X (written [ X] ) and the language recognised by Y ([ Y ] ) is equal \nto the lan\u00adguage recognised by the union of X and Y ([ X + Y ] ). In symbols: [ X + Y ] = [ X] + [ Y \n] (1) This fact leads us to introduce a sound and complete proof tech\u00adnique for language equivalence, \nnamely bisimulation up to context, that exploits both induction (on the operator +) and coinduction: \nif a bisimulation R equates both the (sets of) states X1, Y1 and X2, Y2, then [ X1] = [ Y1] and [ X2] \n= [ Y2] and, by (1), we can immediately conclude that also X1 + X2 and Y1 + Y2 are lan\u00adguage equivalent. \nIntuitively, bisimulations up to context are bisim\u00adulations which do not need to relate X1 + X2 and Y1 \n+ Y2 when X1 (resp. X2) and Y1 (resp. Y2) are already related. To illustrate this idea, let us check \nthe equivalence of states x and u in the following NFA. (Final states are overlined, labelled edges represent \ntransitions.) a a  a x a z y u w a v aa The determinised automaton is depicted below. aaa a + a + \n{x} {y} + {z} + {x, y} {y, z} + {x, y, z}  5 61 2 3 4 a {u} + {v, w} + {u, w} + {u, v, w} a aaa Each \nstate is a set of states of the NFA, .nal states are overlined: they contain at least one .nal state \nof the NFA. The numbered lines show a relation which is a bisimulation containing x and u. Actually, \nthis is the relation that is built by Hopcroft and Karp s algorithm (the numbers express the order in \nwhich pairs are added). The dashed lines (numbered by 1, 2, 3) form a smaller relation which is not a \nbisimulation, but a bisimulation up to context: the equivalence of states {x, y} and {u, v, w} could \nbe immediately deduced from the fact that {x} is related to {u} and {y} to {v, w}, without the need of \nfurther exploring the determinised automaton.  Bisimulations up-to, and in particular bisimulations \nup to con\u00adtext, have been introduced in the setting of concurrency theory [25, 28] as a proof technique \nfor bisimilarity of CCS or p-calculus pro\u00adcesses. As far as we know, they have never been used for proving \nlanguage equivalence of NFA. Among these techniques one should also mention bisimulation up to equivalence, \nwhich, as we show in this paper, is implicitly used in the original Hopcroft and Karp s algorithm. This \ntechnique can be brie.y explained by noting that not all bisimulations are equivalence relations: it \nmight be the case that a bisimulation re\u00adlates (for instance) X and Y , Y and Z but not X and Z. However, \nsince [ X] = [ Y ] and [ Y ] = [ Z] , we can immediately conclude that X and Z recognise the same language. \nAnalogously to bisim\u00adulations up to context, a bisimulation up to equivalence does not need to relate \nX and Z when they are both related to some Y . The techniques of up-to equivalence and up-to context \ncan be combined resulting in a powerful proof technique which we call bisimulation up to congruence. \nOur algorithm is in fact just an ex\u00adtension of Hopcroft and Karp s algorithm that attempts to build a \nbisimulation up to congruence instead of a bisimulation up to equivalence. An important consequence when \nusing up to congru\u00adence is that we do not need to build the whole deterministic au\u00adtomata, but just those \nstates that are needed for the bisimulation up-to. For instance, in the above NFA, the algorithm stops \nafter equating z and u + v and does not build the remaining four states. Despite their use of the up \nto equivalence technique, this is not the case with Hopcroft and Karp s algorithm, where all accessible \nsub\u00adsets of the deterministic automata have to be visited at least once. The ability of visiting only \na small portion of the determinised automaton is also the key feature of the antichain algorithm [31] \nand its optimisation exploiting similarity [1]. The two algorithms are designed to check language inclusion \nrather than equivalence, but we can relate these approaches by observing that the two prob\u00adlems are equivalent \n([ X] = [ Y ] iff [ X] . [ Y ] and [ Y ] . [ X] ; and [ X] . [ Y ] iff [ X] + [ Y ] = [ Y ] iff [ X + \nY ] = [ Y ] ). In order to compare with these algorithms, we make explicit the coinductive up-to technique \nunderlying the antichain algo\u00adrithm [31]. We prove that this technique can be seen as a restriction of \nup to congruence, for which symmetry and transitivity are not al\u00adlowed. As a consequence, the antichain \nalgorithm usually needs to explore more states than our algorithm. Moreover, we show how to integrate \nthe optimisation proposed in [1] in our setting, resulting in an even more ef.cient algorithm. In summary, \nthe contributions of this work are: (1) the obser\u00advation that Hopcroft and Karp implicitly use bisimulations \nup to equivalence, (2) an ef.cient algorithm for checking language equiv\u00adalence (and inclusion), based \non a powerful up to technique, and (3) a comparison with antichain algorithms, by recasting them into \nour coinductive framework. Outline Section 2 recalls Hopcroft and Karp s algorithm for DFA, show\u00ading \nthat it implicitly exploits bisimulation up to equivalence. Sec\u00adtion 3 describes the novel algorithm, \nbased on bisimulations up to congruence. We compare this algorithm with the antichain one in Section \n4, and we show how to exploit similarity in Section 5. Sec\u00adtion 6 is devoted to benchmarks. Sections \n7 and 8 discuss related and future works. Omitted proofs can be found in [6]. Notation We denote sets \nby capital letters X, Y, S, T . . . and functions by lower case letters f, g, . . . Given sets X and \nY , X \u00d7 Y is their Cartesian product, X l Y is the disjoint union and XY is the set of functions f : \nY . X. Finite iterations of a function f : X . X x, f n+1 are denoted by fn (formally, f0(x) = (x) = \nf(fn(x))). The collection of subsets of X is denoted by P(X). The (omega) iteration of a function f : \nP(X) . P(X) is denoted by f . (formally, f .(Y ) = n=0 fn(Y )). For a set of letters A, A* denotes the \nset of all .nite words over A; E the empty word; and w1w2 the concatenation of words w1, w2 . A*. We \nuse 2 for the set {0, 1} and 2A* for the set of all languages over A. 2. Hopcroft and Karp s algorithm \nfor DFA A deterministic .nite automaton (DFA) over the alphabet A is a triple (S, o, t), where S is a \n.nite set of states, o: S . 2 is the output function, which determines if a state x . S is .nal (o(x) \n= 1) or not (o(x) = 0), and t : S . SA is the transition function which returns, for each state x and \nfor each letter a . A, a the next state ta(x). For a . A, we write x . x' to mean that w ta(x) = x'. \nFor w . A*, we write x . x' for the least relation .awa'' w' such that (1) x . x and (2) x . x' iff x \n. x'' and x. x. For any DFA, there exists a function [ -] : S . 2A* mapping states to languages, de.ned \nfor all x . S as follows: [ x]](E) = o(x) , [ x]](aw) = [ ta(x)]](w) . The language [ x] is called the \nlanguage accepted by x. Given two automata (S1, o1, t1) and (S2, o2, t2), the states x1 . S1 and x2 . \nS2 are said to be language equivalent (written x1 ~ x2) iff they accept the same language. Remark 1. \nIn the following, we will always consider the prob\u00adlem of checking the equivalence of states of one single \nand .xed automaton (S, o, t). We do not loose generality since for any two automata (S1, o1, t1) and \n(S2, o2, t2) it is always possible to build an automaton (S1 l S2, o1 l o2, t1 l t2) such that the language \naccepted by every state x . S1 l S2 is the same as the language accepted by x in the original automaton \n(Si, oi, ti). For this rea\u00adson, we also work with automata without explicit initial states: we focus \non the equivalence of two arbitrary states of a .xed DFA. 2.1 Proving language equivalence via coinduction \nWe .rst de.ne bisimulation. We make explicit the underlying no\u00adtion of progression which we need in the \nsequel. De.nition 1 (Progression, Bisimulation). Given two relations R, R' . S \u00d7 S on states, R progresses \nto R', denoted R > R', if whenever x R y then 1. o(x) = o(y) and 2. for all a . A, ta(x) R' ta(y). \n A bisimulation is a relation R such that R > R. As expected, bisimulation is a sound and complete proof \ntech\u00adnique for checking language equivalence of DFA: Proposition 1 (Coinduction). Two states are language \nequivalent iff there exists a bisimulation that relates them.  2.2 Naive algorithm Figure 1 shows a \nnaive version of Hopcroft and Karp s algorithm for checking language equivalence of the states x and \ny of a de\u00adterministic .nite automaton (S, o, t). Starting from x and y, the algorithm builds a relation \nR that, in case of success, is a bisimula\u00adtion. In order to do that, it employs the set (of pairs of \nstates) todo which, intuitively, at any step of the execution, contains the pairs '' '' (x, y) that must \nbe checked: if (x, y) already belongs to R, then it has already been checked and nothing else should \nbe done. Other\u00adwise, the algorithm checks if x' and y' have the same outputs (i.e., ' ' if both are .nal \nor not). If o(x) = o(y), then x and y are different. '' '' If o(x) = o(y), then the algorithm inserts \n(x, y) in R and, for ' ' all a . A, the pairs (ta(x), ta(y)) in todo.  Naive(x, y) ( 1 ) R i s e m p \nt y ; todo i s e m p t y ; ( 2 ) i n s e r t (x, y) i n todo; ( 3 ) w h i l e todo i s n o t e m p \nt y , d o  ( 3 . 1 ) e x t r a c t (x ' , y ' ) f r o m todo; ' ' ( 3 . 2 ) i f (x , y ) . R t h e n \nc o n t i n u e ; ' ' ( 3 . 3 ) i f o(x ) = o(y ) t h e n r e t u r n false; ( 3 . 4 ) f o r a l l a \n. A, i n s e r t (ta(x ' ), ta(y ' )) i n todo; ' ' ( 3 . 5 ) i n s e r t (x , y ) i n R; (4) return \ntrue; Figure 1. Naive algorithm for checking the equivalence of states x and y of a DFA (S, o, t); R \nand todo are sets of pairs of states. The code of HK(x, y) is obtained by replacing step 3.2 with if \n(x ' , y ' ) . e(R) then continue. a a,b a,ba . + + a,b x y z x + y z  4 a 52 3 1 1 2 3 a,ba v w \n + u v w a a a u b Figure 2. Checking for DFA equivalence. Proposition 2. For all x, y . S, x ~ y iff \nNaive(x, y). Proof. We .rst observe that if Naive(x, y) returns true then the relation R that is built \nbefore arriving to step 4 is a bisimulation. Indeed, the following proposition is an invariant for the \nloop corre\u00adsponding to step 3: R > R . todo This invariant is preserved since at any iteration of the \nalgorithm, a pair (x ' , y ' ) is removed from todo and inserted in R after checking '' '' that o(x ) \n= o(y ) and adding (ta(x ), ta(y )) for all a . A in todo. Since todo is empty at the end of the loop, \nwe eventually have R > R, i.e., R is a bisimulation. By Proposition 1, x ~ y. We now prove that if Naive(x, \ny) returns false, then x ~ y. Note that for all (x ' , y ' ) inserted in todo, there exists a word ww \n' '' w . A * such that x . x and y . y ' . Since o(x ) = o(y ), '' '' then [ x ]](E) = [ y ]](E) and \nthus [ x]](w) = [ x ]](E) = [ y ]](E) = [ y]](w), that is x ~ y. Since both Hopcroft and Karp s algorithm \nand the one we in\u00adtroduce in Section 3 are simple variations of this naive one, it is important to illustrate \nits execution with an example. Consider the DFA with input alphabet A = {a} in the left-hand side of \nFigure 2, and suppose we want to check that x and u are language equivalent. During the initialisation, \n(x, u) is inserted in todo. At the .rst iteration, since o(x) = 0 = o(u), (x, u) is inserted in R and \n(y, v) in todo. At the second iteration, since o(y) = 1 = o(v), (y, v) is inserted in R and (z, w) in \ntodo. At the third iteration, since o(z) = 0 = o(w), (z, w) is inserted in R and (y, v) in todo. At the \nfourth iteration, since (y, v) is already in R, the algorithm does nothing. Since there are no more pairs \nto check in todo, the relation R is a bisimulation and the algorithm terminates returning true. These \niterations are concisely described by the numbered dashed lines in Figure 2. The line i means that the \nconnected pair is inserted in R at iteration i. (In the sequel, when enumerating iterations, we ignore \nthose where a pair from todo is already in R so that there is nothing to do.) Remark 2. Unless it .nds \na counter-example, Naive constructs the smallest bisimulation that relates the two starting states (see \nProposition 8 in [6]). On the contrary, minimisation algorithms [17] are designed to compute the largest \nbisimulation relation for a given automaton. For instance, taking automaton on the left of Fig\u00adure 2, \nthey would equate the states x and w which are language equivalent, while Naive(x, u) does not relate \nthem.  2.3 Hopcroft and Karp s algorithm The naive algorithm is quadratic: a new pair is added to R \nat each non-trivial iteration, and there are only n 2 such pairs, where n = |S| is the number of states \nof the DFA. To make this algorithm (almost) linear, Hopcroft and Karp actually record a set of equiva\u00adlence \nclasses rather than a set of visited pairs. As a consequence, their algorithm may stop earlier when an \nencountered pair of states is not already in R but in its re.exive, symmetric, and transitive clo\u00adsure. \nFor instance, in the right-hand side example from Figure 2, we can stop when we encounter the dotted \npair (y, w) since these two states already belong to the same equivalence class according to the four \nprevious pairs. With this optimisation, the produced relation R contains at most n pairs (two equivalence \nclasses are merged each time a pair is added). Formally, and ignoring the concrete data structure to \nstore equivalence classes, Hopcroft and Karp s algorithm consists in simply replacing step 3.2 in Figure \n1 with ( 3 . 2 ) i f (x ' , y ' ) . e(R) t h e n c o n t i n u e ; where e: P(S \u00d7 S) . P(S \u00d7 S) is the \nfunction mapping each relation R . S \u00d7 S into its symmetric, re.exive, and transitive closure. We hereafter \nrefer to this algorithm as HK.  2.4 Bisimulations up-to We now show that the optimisation used by Hopcroft \nand Karp corresponds to exploiting an up-to technique . De.nition 2 (Bisimulation up-to). Let f : P(S \n\u00d7 S) . P(S \u00d7 S) be a function on relations on S. A relation R is a bisimulation up to f if R > f (R), \ni.e., whenever x R y then 1. o(x) = o(y) and 2. for all a . A, ta(x) f(R) ta(y).  With this de.nition, \nHopcroft and Karp s algorithm just consists in trying to build a bisimulation up to e. To prove the correctness \nof the algorithm, it suf.ces to show that any bisimulation up to e is contained in a bisimulation. We \nuse for that the notion of compatible function [26, 28]: De.nition 3 (Compatible function). A function \nf : P(S \u00d7 S) . P(S \u00d7 S) is compatible if it is monotone and it preserves progres\u00adsions: for all R, R \n' . S \u00d7 S, R > R ' entails f (R) > f(R ' ) . Proposition 3. Let f be a compatible function. Any bisimulation \nup to f is contained in a bisimulation. Proof. Suppose that R is a bisimulation up to f, i.e., that R \n> f(R). Using compatibility of f and by a simple induction on n, we (R) > fn+1 get .n, fn (R). Therefore, \nwe have  fn fn (R) > (R) , n n in other words, f.(R) = n fn(R) is a bisimulation. This latter relation \ntrivially contains R, by taking n = 0.  We could prove directly that e is a compatible function; we \nhow\u00adever take a detour to ease our correctness proof for the algorithm we propose in Section 3. Lemma \n1. The following functions are compatible: id: the identity function; o: S . 2 is the output function \n(as for DFA), and t : S . P(S)A is the transition relation, which assigns to each state x . S and input \nletter a . A a set of possible successor states. The powerset construction transforms any NFA (S, o, \nt) in the DFA (P(S), o, t) where o: P(S) . 2 and t: P(S) . P(S)A are de.ned for all X . P(S) and a . \nA as follows: f . g: the composition of compatible functions f and g; F : the pointwise union of an \narbitrary family F of compatible o  . .. o(x) if X = {x} with x . S if X = 0 functions: F (R) = f.F \nf(R); (X) = 0 .. . .. .. (X1) + o(X2) if X = X1 + X2 f.: the (omega) iteration of a compatible function \nf. Lemma 2. The following functions are compatible: o ta(x) if X = {x} with x . S if X = 0 a the constant \nre.exive function: r(R) = {(x, x) | x . S}; (X) = 0 tt a(X1) + ta(X2) if X = X1 + X2 the converse function: \ns(R) = {(y, x) | x R y};  the squaring function: t(R) = {(x, z) | .y, x R y R z}.  Intuitively, given \na relation R, (s . id)(R) is the symmetric closure of R, (r .s .id)(R) is its re.exive and symmetric \nclosure, and (r . s . t . id).(R) is its symmetric, re.exive and transitive closure: e = (r . s . t . \nid). . Another way to understand this decomposition of e is to recall that for a given R, e(R) can be \nde.ned inductively by the following rules: x e(R) y x e(R) y y e(R) z x R y rs t id x e(R) x y e(R) x \nx e(R) z x e(R) y Theorem 1. Any bisimulation up to e is contained in a bisimula\u00adtion. Proof. By Proposition \n3, it suf.ces to show that e is compatible, which follows from Lemma 1 and Lemma 2. Corollary 1. For \nall x, y . S, x ~ y iff HK(x, y). Proof. Same proof as for Proposition 2, by using the invariant R > \ne(R) .todo. We deduce that R is a bisimulation up to e after the loop. We conclude with Theorem 1 and \nProposition 1. Returning to the right-hand side example from Figure 2, Hopcroft and Karp s algorithm \nconstructs the relation RHK = {(x, u), (y, v), (z, w), (z, v)} which is not a bisimulation, but a bisimulation \nup to e: it contains the pair (x, u), whose b-transitions lead to (y, w), which is not in RHK but in \nits equivalence closure, e(RHK ). 3. Optimised algorithm for NFA We now move from DFA to non-deterministic \nautomata (NFA). We start with standard de.nitions about semi-lattices, determinisation, and language \nequivalence for NFA. A semi-lattice (X, +, 0) consists of a set X and a binary op\u00aderation + : X \u00d7 X . \nX which is associative, commutative, idempotent (ACI), and has 0 . X as identity. Given two semi\u00adlattices \n(X1, +1, 01) and (X2, +2, 02), an homomorphism of semi\u00adlattices is a function f : X1 . X2 such that for \nall x, y . X1, f(x +1 y) = f(x) +2 f(y) and f (01) = 02. The set 2 = {0, 1}is a semi-lattice when taking \n+ to be the ordinary Boolean or. Also the set of all languages 2A * carries a semi-lattice where + is \nthe union of languages and 0 is the empty language. More generally, for any set X, P(X) is a semi-lattice \nwhere + is the union of sets and 0 is the empty set. In the sequel, we indiscriminately use 0 to denote \nthe element 0 . 2, the empty language in 2A * , and the empty set in P(X). Similarly, we use + to denote \nthe Boolean or in 2, the union of languages in 2A * , and the union of sets in P(X). A non-deterministic \n.nite automaton (NFA) over the input al\u00adphabet A is a triple (S, o, t), where S is a .nite set of states, \nObserve that in (P(S), o, t), the states form a semi-lattice (P(S), +, 0), and o. and t. are, by de.nition, \nsemi-lattices homo\u00admorphisms. These properties are fundamental for the up-to tech\u00adnique we are going \nto introduce; in order to highlight the difference with generic DFA (which usually do not carry this \nstructure), we introduce the following de.nition. De.nition 4. A determinised NFA is a DFA (P(S), o, \nt.) ob\u00adtained via the powerset construction of some NFA (S, o, t). Hereafter, we use a new notation for \nrepresenting states of determinised NFA: in place of the singleton {x}, we just write x and, in place \nof {x1, . . . , xn}, we write x1 + \u00b7 \u00b7 \u00b7 + xn. For an example, consider the NFA (S, o, t) depicted below \n(left) and part of the determinised NFA (P(S), o, t) (right). a a aaa -+++ xyz xy + z x + y x + y + \nz  aa a In the determinised NFA, x makes one single a-transition going into y + z. This state is .nal: \no.(y + z) = o.(y) + o.(z) = o(y)+o(z) = 1+0 = 1; it makes an a-transition into ta(y +z) = ta(y) + ta(z) \n= ta(y) + ta(z) = x + y. The language accepted by the states of an NFA (S, o, t) can be conveniently \nde.ned via the powerset construction: the language accepted by x . S is the language accepted by the \nsingleton {x}in the DFA (P(S), o., t.), in symbols [ {x}] . Therefore, in the fol\u00adlowing, instead of \nconsidering the problem of language equivalence of states of the NFA, we focus on language equivalence \nof sets of states of the NFA: given two sets of states X and Y in P(S), we say that X and Y are language \nequivalent (X ~ Y ) iff [ X] = [ Y ] . This is exactly what happens in standard automata theory, where \nNFA are equipped with sets of initial states. 3.1 Extending coinduction to NFA In order to check whether \ntwo sets of states X and Y of an NFA (S, o, t) are language equivalent, we can simply employ the bisim\u00ad \nulation proof method on (P(S), o, t). More explicitly, a bisimu\u00adlation for an NFA (S, o, t) is a relation \nR . P(S) \u00d7 P(S) on sets of states, such that whenever X R Y then (1) o(X) = o(Y ), and (2) for all a \n. A, ta(X) R t.a(Y ). Since this is just the old de.nition of bisimulation (De.nition 1) applied to (P(S), \no, t), we get that X ~ Y iff there exists a bisimulation relating them. Remark 3 (Linear time v.s. branching \ntime). It is important not to confuse these bisimulation relations with the standard Milner\u00adand-Park \nbisimulations [25] (which strictly imply language equiv\u00adalence): in a standard bisimulation R, if the \nfollowing states x and y of an NFA are in R,  a x1 a y1 . . . y x . . . a xn a ym then each xi should \nbe in R with some yj (and vice-versa). Here, instead, we .rst transform the transition relation into \nx a + x1 + \u00b7 \u00b7 \u00b7 + xn y a + y1 + \u00b7 \u00b7 \u00b7 + ym , using the powerset construction, and then we require that \nthe sets x1 + \u00b7 \u00b7 \u00b7 + xn and y1 + \u00b7 \u00b7 \u00b7 + ym are related by R.  3.2 Bisimulation up to congruence The \nsemi-lattice structure (P(S), +, 0) carried by determinised NFA makes it possible to introduce a new \nup-to technique, which is not available with plain DFA: up to congruence. This technique relies on the \nfollowing function. De.nition 5 (Congruence closure). Let u: P(P(S) \u00d7 P (S)) . P(P(S) \u00d7 P (S)) be the \nfunction on relations on sets of states de.ned for all R . P (S) \u00d7 P (S) as: u(R) = {(X1 + X2, Y1 + Y2) \n| X1 R Y1 and X2 R Y2} . The function c = (r . s . t . u . id). is called the congruence closure function. \nIntuitively, c(R) is the smallest equivalence relation which is closed with respect to + and which includes \nR. It could alterna\u00adtively be de.ned inductively using the rules r, s, t, and id from the previous section, \nand the following one: X1 c(R) Y1 X2 c(R) Y2 u X1 + X2 c(R) Y1 + Y2 We call bisimulations up to congruence \nthe bisimulations up to c. We report the explicit de.nition for the sake of clarity: De.nition 6 (Bisimulation \nup to congruence). A bisimulation up to congruence for an NFA (S, o, t) is a relation R . P (S) \u00d7 P (S) \non sets of states, such that whenever X R Y then 1. o (X) = o (Y ) and 2. for all a . A, ta(X) c(R) \nta(Y ).  We then show that bisimulations up to congruence are sound, using the notion of compatibility: \nLemma 3. The function u is compatible. Proof. We assume that R > R ', and we prove that u(R) > u(R ' \n). If X u(R) Y , then X = X1 + X2 and Y = Y1 + Y2 for some X1, X2, Y1, Y2 such that X1 R Y1 and X2 R \nY2. By assumption, we have o (X1) = o (Y1), o (X2) = o (Y2), and for all a . A, ta(X1) R ' ta(Y1) and \nta(X2) R ' ta(Y2). Since o and t are homomorphisms, we deduce o (X1 + X2) = o (Y1 + Y2), and for all \na . A, ta(X1 + X2) u(R ' ) ta(Y1 + Y2). Theorem 2. Any bisimulation up to congruence is contained in \na bisimulation. Proof. By Proposition 3, it suf.ces to show that c is compatible, which follows from \nLemmas 1, 2 and 3. In the Introduction, we already gave an example of bisimulation up to context, which \nis a particular case of bisimulation up to congruence (up to context corresponds using just the function \n(r . u . id)., without closing under s and t). a . a aaa - +++ xyz xy + z x + y x + y + z  a 4 2 3 \n1 a a u u  aa Figure 3. Bisimulations up to congruence, on a single letter NFA. Naive(X, Y ) ( 1 ) \nR i s e m p t y ; todo i s e m p t y ; ( 2 ) i n s e r t (X, Y ) i n todo; ( 3 ) w h i l e todo i s \nn o t e m p t y , d o ( 3 . 1 ) e x t r a c t (X ' , Y ' ) f r o m todo; ( 3 . 2 ) i f (X ' , Y ' ) \n. R t h e n c o n t i n u e ; ( 3 . 3 ) i f o (X ' ) = o (Y ' ) t h e n r e t u r n f alse ;  (3.4) \nfor all a . A, i n s e r t (ta(X ' ), ta(Y ' )) i n todo; ( 3 . 5 ) i n s e r t (X ' , Y ' ) i n R; \n(4) return true; Figure 4. On-the-.y naive algorithm, for checking the equivalence of sets of states \nX and Y of an NFA (S, o, t). The code for on\u00adthe-.y HK(X, Y ) is obtained by replacing the test in step \n3.2 with (X ' , Y ' ) . e(R); the code for HKC(X, Y ) is obtained by replacing this test with (X ' , \nY ' ) . c(R . todo). A more involved example illustrating the use of all ingredients of the congruence \nclosure function (c) is given in Figure 3. The relation R expressed by the dashed numbered lines (formally \nR = {(x, u), (y + z, u)}) is neither a bisimulation nor a bisimulation aa up to equivalence since y+z \n. x+y and u . u, but (x+y, u) ./e(R). However, R is a bisimulation up to congruence. Indeed, we have \n(x + y, u) . c(R): x + y c(R) u + y ((x, u) . R) c(R) y + z + y ((y + z, u) . R) = y + z c(R) u ((y + \nz, u) . R) In contrast, we need four pairs to get a bisimulation up to e contain\u00ading (x, u): this is \nthe relation depicted with both dashed and dotted lines in Figure 3. Note that we can deduce many other \nequations from R; in fact, c(R) de.nes the following partition of sets of states: {0}, {y}, {z}, {x, \nu, x+y, x+z, and the 9 remaining subsets}.  3.3 Optimised algorithm for NFA Algorithms for NFA can be \nobtained by computing the deter\u00adminised NFA on-the-.y [13]: starting from the algorithms for DFA (Figure \n1), it suf.ces to work with sets of states, and to inline the powerset construction. The corresponding \ncode is given in Figure 4. The naive algorithm (Naive) does not use any up to technique, Hopcroft and \nKarp s algorithm (HK) reasons up to equivalence in step 3.2, and the optimised algorithm, referred as \nHKC in the se\u00adquel, relies on up to congruence: step 3.2 becomes ( 3 . 2 ) i f (X ' , Y ' ) . c(R . todo) \nt h e n c o n t i n u e ;  Observe that we use c(R . todo) rather than c(R): this allows us to skip \nmore pairs, and this is safe since all pairs in todo will eventually be processed. Corollary 2. For all \nX, Y . P(S), X ~ Y iff HKC(X, Y ). Proof. Same proof as for Proposition 2, by using the invariant R > \nc(R . todo) for the loop. We deduce that R is a bisimulation up to congruence after the loop. We conclude \nwith Theorem 2 and Proposition 1. The most important point about these three algorithms is that they \ncompute the states of the determinised NFA lazily. This means that only accessible states need to be \ncomputed, which is of prac\u00adtical importance since the determinised NFA can be exponentially large. In \ncase of a negative answer, the three algorithms stop even before all accessible states have been explored; \notherwise, if a bisimulation (possibly up-to) is found, it depends on the algorithm: With Naive, all \naccessible states need to be visited, by de.ni\u00adtion of bisimulation.  With HK, the only case where some \naccessible states can be avoided is when a pair (X, X) is encountered: the algorithm skips this pair \nso that the successors of X are not necessarily computed (this situation rarely happens in practice it \nactually never happens when starting with disjoint automata). In the other cases where a pair (X, Y ) \nis skipped, then X and Y are necessarily already related to some other states in R, so that their successors \nwill eventually be explored.  With HKC, only a small portion of the accessible states is built (check \nthe experiments in Section 6). To see a concrete exam\u00adple, let us execute HKC on the NFA from Figure \n3. After two iterations, R = {(x, u), (y + z, u)}. Since x + y c(R) u, the algorithm stops without building \nthe states x + y and x + y + z. Similarly, in the example from the Introduction, HKC does not construct \nthe four states corresponding to pairs 4, 5, and 6.  This ability of HKC to ignore parts of the determinised \nNFA comes from the up to congruence technique, which allows one to infer properties about states that \nwere not necessarily encountered be\u00adfore. As we shall see in Section 4, the ef.ciency of antichain-based \nalgorithms [1, 31] also comes from their ability to skip large parts of the determinised NFA.  3.4 Computing \nthe congruence closure For the optimised algorithm to be effective, we need a way to check whether some \npairs belong to the congruence closure of some relation (step 3.2). We present here a simple solution \nbased on set rewriting; the key idea is to look at each pair (X, Y ) in a relation R as a pair of rewriting \nrules: X . X + Y Y . X + Y , which can be used to compute normal forms for sets of states. Indeed, by \nidempotence, X R Y entails X c(R) X + Y . De.nition 7. Let R . P(S)\u00d7P(S) be a relation on sets of states. \nWe de.ne .R . P(S) \u00d7 P(S) as the smallest irre.exive relation that satis.es the following rules: XRY \nXRY Z .R Z ' X .R X + Y Y .R X + Y U + Z .R U + Z ' Lemma 4. For all relations R, the relation .R is \nconvergent. In the sequel, we denote by X.R the normal form of a set X w.r.t. .R. Intuitively, the normal \nform of a set is the largest set of its equivalence class. Recalling the example from Figure 3, the common \nnormal form of x + y and u can be computed as follows (R is the relation {(x, u), (y + z, u)}): x + y \n u . r x + y + u x + u x + y + z + u Theorem 3. For all relations R, and for all X, Y . P(S), we have \nX.R = Y .R iff (X, Y ) . c(R). Thus, for checking (X, Y ) . c(R . todo) we only have to compute the normal \nform of X and Y with respect to .R.todo. Note that each pair of R.todo may be used only once as a rewriting \nrule, but we do not know in advance in which order to apply these rules. Therefore, the time required \nto .nd one rule that applies is in the worst case rn where r = |R . todo| is the size of the relation \nR.todo, and n = |S| is the number of states of the NFA (assuming linear time complexity for set-theoretic \nunion and containment of sets of states). Since we cannot apply more than r rules, the time for checking \nwhether (X, Y ) . c(R . todo) is bounded by r 2 n. We tried other solutions, notably by using binary \ndecision dia\u00adgrams [9]. We have chosen to keep the presented rewriting algo\u00adrithm for its simplicity \nand because it behaves well in practice. 3.5 Complexity hints The complexity of Naive, HK and HKC is \nclosely related to the size of the relation that they build. Hereafter, we use v = |A| to denote the \nnumber of letters in A. Lemma 5. The three algorithms require at most 1 + v\u00b7|R| itera\u00adtions, where |R| \nis the size of the produced relation; moreover, this bound is reached whenever they return true. Therefore, \nwe can conveniently reason about |R|. Lemma 6. Let RNaive , RHK , and RHKC denote the relations produced \nby the three algorithms. We have |RHKC |, |RHK | = m |RNaive | = m 2 , (2) where m = 2n is the number \nof accessible states in the deter\u00adminised NFA and n is the number of states of the NFA. If the algo\u00adrithms \nreturn true, we moreover have |RHKC | = |RHK | = |RNaive | . (3) As shown below in Section 4.2.4, RHKC \ncan be exponentially smaller than RHK . Notice however that the problem of deciding NFA language equivalence \nis PSPACE-complete [24], and that none of the algorithms presented here is in PSPACE: all of them store \na set of visited pairs, and in the worst case, this set can become exponentially large with all of them. \n(This also holds for the antichain algorithms [1, 31] which we describe in Section 4.) Instead, the standard \nPSPACE algorithm does not store any set of visited pairs: it checks all words of length smaller than \n2n . While this can be done in polynomial space, this systematically requires exponential time.  3.6 \nUsing HKC for checking language inclusion For NFA, language inclusion can be reduced to language equiva\u00adlence \nin a rather simple way. Since the function [ -] : P(S) . 2A * is a semi-lattice homomorphism (see Theorem \n7 in [6]), for any given sets of states X and Y , [ X+Y ] = [ Y ] iff [ X] +[ Y ] = [ Y ] iff [ X] . \n[ Y ] . Therefore, it suf.ces to run HKC(X+Y, Y ) to check the inclusion [ X] . [ Y ] . In such a situation, \nall pairs that are eventually manipulated by HKC have the shape (X ' +Y ' , Y ' ) for some sets X ' , \nY ' . Step 3.2 of HKC, where it checks whether the current pair belongs to the congruence closure of \nthe relation, can thus be simpli.ed. First, the pairs in the current relation can only be used to rewrite \nfrom right to left. Second, the following lemma allows one to avoid unnecessary normal form computations: \n Lemma 7. For all sets X, Y and for all relations R, we have X+Y c(R) Y iff X . Y .R. Proof. We .rst \nprove that for all X, Y, X.R = Y .R iff X . Y .R and Y . X.R, using the fact that the normalisation function \n.R : X . X.R is monotone and idempotent. The announced result follows by Theorem 3 since Y . (X + Y ).R \nis always true and X+Y . Y .R iff X . Y .R. At this point, the reader might wonder whether checking the \ntwo inclusions separetly is more convenient than checking the equivalence directly. Hereafter, we show \nthat this is not the case. Lemma 8. Let X, Y be two sets of states; let R. and R. be the relations computed \nby HKC(X+Y, Y ) and HKC(X+Y, X), respec\u00adtively. If R. and R. are bisimulations up to congruence, then \nthe following relation is a bisimulation up to congruence: '' ''' ''' R= = {(X , Y ) | (X +Y , Y ) . \nR. or (X +Y , X ) . R.}. On the contrary, checking the equivalence directly actually al\u00adlows one to skip \nsome pairs that cannot be skipped when reasoning by double inclusion. As an example, consider the DFA \non the right of Figure 2. The relation computed by HKC(x, u) contains only four pairs (because the .fth \none follows from transitivity). Instead, the relations built by HKC(x, x+u) and HKC(u+x, u) would both \ncon\u00adtain .ve pairs: transitivity cannot be used since our relations are now oriented (from y = v, z = \nv and z = w, we cannot deduce y = w). Another example, where we get an exponential factor by checking \nthe equivalence directly rather than through the two in\u00adclusions, can be found in Section 4.2.4. In a \nsense, the behaviour of the coinduction proof method here is similar to that of standard proofs by induction, \nwhere one often has to strengthen the induction predicate to get a (nicer) proof. 4. Antichain algorithm \nIn [31], De Wulf et al. have proposed the antichain approach for checking language inclusion of NFA. \nWe show that this approach can be explained in terms of simulations up to upward-closure that, in turn, \ncan be seen as a special case of bisimulations up to congruence. Before doing so, we recall the standard \nnotion of antichain and we describe the antichain algorithm (AC). Given a partial order (X, g), an antichain \nis a subset Y . X containing only incomparable elements (that is, for all y1, y2 . Y , y1 g y2 and y2 \ng y1). AC exploits antichains over the set S \u00d7 P(S), where the ordering is given by (x1, Y1) g (x2, Y2) \niff x1 = x2 and Y1 . Y2. In order to check [ X] . [ Y ] for two sets of states X, Y of an NFA (S, o, \nt), AC maintains an antichain of pairs (x ' , Y ' ), where x ' is a state of the NFA and Y ' is a state \nof the deter\u00adminised automaton. More precisely, the automaton is explored non\u00addeterministically (via \nt) for obtaining the .rst component of the pair and deterministically (via t ) for the second one. If \na pair such that x ' is accepting (o(x ' ) = 1) and Y ' is not (o (Y ' ) = 0) is encoun\u00adtered, then a \ncounter-example has been found. Otherwise all deriva\u00adtives of the pair along the automata transitions \nhave to be inserted into the antichain, so that they will be explored. If one of these pairs p is larger \nthan a previously encountered pair p ' (p ' g p) then the language inclusion corresponding to p is subsumed \nby p ' so that p can be skipped; otherwise, if p g p1, . . . , pn for some pairs p1, . . . , pn that \nare already in the antichain, then one can safely remove these pairs: they are subsumed by p and, by \ndoing so, the set of visited pairs remains an antichain. Remark 4. An important difference between HKC \nand AC consists in the fact that the former inserts pairs in todo without checking whether they are redundant \n(this check is performed when the pair is processed), while the latter removes all redundant pairs whenever \na new one is inserted. Therefore, the cost of an iteration with HKC is merely the cost of the corresponding \ncongruence check, while the cost of an iteration with AC is merely that of inserting all successors of \nthe corresponding pair and simplifying the antichain. Note that the above description corresponds to \nthe forward antichain algorithm, as described in [1]. Instead, the original an\u00adtichain algorithm, as \n.rst described in [31], is backward in the sense that the automata are traversed in the reversed way, \nfrom ac\u00adcepting states to initial states. The two versions are dual [31] and we could similarly de.ne \nthe backward counterpart of HKC and HK. We however stick to the forward versions for the sake of clarity. \n4.1 Coinductive presentation Leaving apart the concrete data structures used to manipulate an\u00adtichains, \nwe can rephrase this algorithm using a coinductive frame\u00adwork, like we did for Hopcroft and Karp s algorithm. \nFirst de.ne a notion of simulation, where the left-hand side automaton is executed non-deterministically: \nDe.nition 8 (Simulation). Given two relations T, T ' . S \u00d7P(S), T s-progresses to T ' , denoted T >s \nT ' , if whenever x T Y then 1. o(x) = o (Y ) and ' '' 2. for all a . A, x . ta(x), x T ta(Y ). A simulation \nis a relation T such that T >s T . As expected, we obtain the following coinductive proof principle: \nProposition 4 (Coinduction). For all sets X, Y , we have [ X] . [ Y ] iff there exists a simulation T \nsuch that for all x . X, x T Y . (Note that like for our notion of bisimulation, the above notion of \nsimulation is weaker than the standard one from concurrency theory [25], which strictly entails language \ninclusion Remark 3.) To account for the antichain algorithm, where we can discard pairs using the preorder \ng, it suf.ces to de.ne the upward closure function t: P(S \u00d7 P(S)) . P(S \u00d7 P(S)) as tT = {(x, Y ) | .(x \n' , Y ' ) . T s.t. (x ' , Y ' ) g (x, Y )} . A pair belongs to the upward closure tT of a relation T \n. S \u00d7 P(S), if and only if this pair is subsumed by some pair in T . In fact, rather than trying to construct \na simulation, AC attempts to construct a simulation up to upward closure. Like for HK and HKC, this method \ncan be justi.ed by de.ning the appropriate notion of s-compatible function, showing that any sim\u00adulation \nup to an s-compatible function is contained in a simulation, and showing that the upward closure function \n(t) is s-compatible. Theorem 4. Any simulation up to t is contained in a simulation. Corollary 3. For \nall X, Y . P(S), [ X] . [ Y ] iff AC(X, Y ).  4.2 Comparing HKC and AC The ef.ciency of the two algorithms \nstrongly depends on the num\u00adber of pairs that they need to explore. In the following (Sections 4.2.3 \nand 4.2.4), we show that HKC can explore far fewer pairs than AC: when checking language inclusion of \nautomata that share some states, or when checking language equivalence. We would also like to formally \nprove that (a) HKC never explores more than AC, and (b) when checking inclusion of disjoint automata, \nAC never ex\u00adplores more than HKC. Unfortunately, the validity of these state\u00adments highly depends on \nnumerous assumptions about the two algorithms (e.g., on the exploration strategy) and their potential \nproofs seem complicated and not really informative. For these rea\u00adsons, we preferred to investigate the \nformal correspondence at the level of the coinductive proof techniques, where it is much cleaner.  4.2.1 \nLanguage inclusion: HKC can mimic AC As explained in Section 3.6, we can check the language inclusion \nof two sets X, Y by executing HKC(X+Y, Y ). We now show that for any simulation up to upward closure \nthat proves the inclusion [ X] . [ Y ] , there exists a bisimulation up to congruence of the same size \nwhich proves the same inclusion. For T . S \u00d7 P(S), let TT. P(S) \u00d7 P(S) denote the relation {(x + Y, Y \n) | x T Y }. Lemma 9. We have ttT . c(TT). Proof. If (x + Y, Y ) . ttT , then there exists Y ' . Y such \nthat (x, Y ' ) . T . By de.nition, (x + Y ' , Y ' ) . TTand (Y, Y ) . c(TT). By the rule (u), (x + Y \n' + Y, Y ' + Y ) . c(TT) and since Y ' . Y , (x + Y, Y ) . c(TT). Proposition 5. If T is a simulation \nup to t, then T is a bisimulation up to c. ' .(t Proof. First observe that if T >s T , then TT> u T '). \nThere\u00ad fore, if T >s tT , then TT> u .(ttT ). By Lemma 9, TT> . u (c(TT)) = c(TT). (Note that transitivity \nand symmetry are not used in the above proofs: the constructed bisimulation up to congruence is actually \na bisimulation up to context (r . u . id)..) The relation TTis not the one computed by HKCsince the former \ncontains pairs of the shape (x + Y, Y ), while the latter has pairs of the shape (X + Y, Y ) with X possibly \nnot a singleton. However, note that manipulating pairs of the two kinds does not change anything since \nby Lemma 7, (X + Y, Y ) . c(R) iff for all x . X, (x + Y, Y ) . c(R). 4.2.2 Inclusion: AC can mimic \nHKC on disjoint automata As shown in Section 4.2.3 below, HKC can be faster than AC, thanks to the up \nto transitivity technique. However, in the special case where the two automata are disjoint, transitivity \ncannot help, and the two algorithms actually match each other. Suppose that the automaton (S, o, t) is \nbuilt from two disjoint automata (S1, o1, t1) and (S2, o2, t2) as described in Remark 1. Let R be the \nrelation obtained by running HKC(X0+Y0, Y0) with X0 . S1 and Y0 . S2. All pairs in R are necessarily \nof the shape (X+Y, Y ) with X . S1 and Y . S2. Let R . S \u00d7 P(S) denote the relation {(x, Y ) | .X, x \n. X and X+Y R Y }. Lemma 10. If S1 and S2 are disjoint, then c(R) . t(R). Proof. Suppose that x c(R) \nY , i.e., x . X with X + Y c(R) Y . By Lemma 7, we have X . Y .R, and hence, x . Y .R. By def\u00adinition \nof R the pairs it contains can only be used to rewrite from right to left; moreover, since S1 and S2 \nare disjoint, such rewriting steps cannot enable new rewriting rules, so that all steps can be per\u00ad s \nformed in parallel: we have Y .R = X ' . There- X +Y RY .Y ' '' fore, there exist some X , Y ' with x \n. X , X +Y ' R Y ' , and Y ' . Y . It follows that (x, Y ' ) . R, hence (x, Y ) . t(R). Proposition 6. \nIf S1 and S2 are disjoint, and if R is a bisimulation up to congruence, then R is a simulation up to \nupward closure. Proof. First observe that for all relations R, R ' , if R > R ' , then R >s R' . Therefore, \nif R > c(R), then R >s c(R). We deduce R >s t(R) by Lemma 10. a a,b a,b + a,b C x + x1 \u00b7 \u00b7 \u00b7 + xn a,b \na,bb ++ + a,b C y y1 \u00b7 \u00b7 \u00b7 yn a,b a,b a,b ++ + a,b C z z1 \u00b7 \u00b7 \u00b7 zn Figure 5. Family of examples where \nHKC exponentially improves over AC and HK; we have x + y ~ z. 4.2.3 Inclusion: AC cannot mimic HKC on \nmerged automata The containment of Lemma 10 does not hold when S1 and S2 are not disjoint since c can \nexploit transitivity while t cannot. For a concrete grasp, take R = {(x + y, y), (y + z, z)} and observe \nthat (x, z) . c(R) but (x, z) ./t(R). This difference makes it possible to .nd bisimulations up to c \nthat are much smaller than the corresponding simulations up to t, and for HKC to be more ef.cient than \nAC. An example, where HKC is exponentially better than AC for checking language inclusion of automata \nsharing some states, is given in [7].  4.2.4 Language equivalence: AC cannot mimic HKC AC can be used \nto check language equivalence, by checking the two underlying inclusions. However, checking equivalence \ndirectly can be better, even in the disjoint case. To see this on a simple example, consider the DFA \non the right-hand side of Figure 2. If we use AC twice to prove x ~ u, we get the following antichains \nT1 = {(x, u), (y, v), (y, w), (z, v), (z, w)} T2 = {(u, x), (v, y), (w, y), (v, z), (w, z)} containing \n.ve pairs each. Instead, four pairs are suf.cient with HK or HKC, thanks to up to symmetry and up to \ntransitivity. For a more interesting example, consider the family of NFA given in Figure 5, where n is \nan arbitrary natural number. Taken together, the states x and y are equivalent to the state z: they recog\u00adnise \nthe language (a+b) * (a+b)n+1 . Alone, the state x (resp. y) recognises the language (a+b) * a(a+b)n \n(resp. (a+b) * b(a+b)n). For i = n, let Xi = x+x1+ . . . +xi, Yi = y+y1+ . . . +yi, and Zi = z+z1+ . \n. . +zi; for N . [1..i], furthermore set N Xi N = x +xj , Y i = y +yj . j.N j.[1..n]\\N In the determinised \nNFA, x + y can reach all the states of the N shape Xi N +Y i , for i = n and N . [1..i]. For instance, \nfor aaab n=i=2, we have x+y . x+y+x1+x2, x+y . x+y+y1+x2, babb x+y . x+y+x1+y2, and x+y . x+y+y1+y2. \nInstead, z reaches only n+1 distinct states, those of the form Zi. The smallest bisimulation relating \nx + y and z is N R ={(Xi N + Y i , Zi) | i = n, N . [1..i]}, which contains 2n+1-1 pairs. This is the \nrelation computed by Naive(x, y) and HK(x, y) the up to equivalence technique (alone) does not help in \nHK. With AC, we obtain the antichains Tx + Ty (for [ x + y] . [ z] ) and Tz (for [ x + y] . [ z] ), where: \nTx = {(xi, Zi) | i = n}, Ty = {(yi, Zi) | i = n}, Tz = {(zi, XN + Y N ) | i = n, N . [1..i]}. i i Note \nthat Tx and Ty have size n + 1, and Tz has size 2n+1-1.  The language recognised by x or y are known \nfor having a minimal DFA with 2n states [19]. So, checking x + y ~ z via minimisation (e.g., [17]) would \nalso require exponential time. This is not the case with HKC, which requires only polynomial time in \nthis case. Indeed, HKC(x+y, z) builds the relation R ' = {(x + y, z)} . {(x + Yi + yi+1, Zi+1) | i < \nn} . {(x + Yi + xi+1, Zi+1) | i < n} which is a bisimulation up to congruence and which only contains \n2n + 1 pairs. To see that this is a bisimulation up to congruence, consider the pair (x+y+x1+y2, Z2) \nobtained from (x+y, z) after reading the word ba. This pair does not belong to R ' but to its congruence \nclosure. Indeed, we have x+y+x1+y2 c(R ' ) Z1+y2 (x+y+x1 R ' Z1) c(R ' ) x+y+y1+y2 (x+y+y1 R ' Z1) c(R \n' ) Z2 (x+y+y1+y2 R ' Z2) (Check Lemma 18 in [6] for a complete proof.) 5. Exploiting Similarity Looking \nat the example in Figure 5, a natural idea would be to .rst quotient the automaton by graph isomorphism. \nBy doing so, we would merge the states xi, yi, zi, and we would obtain the following automaton, for which \nchecking x+y ~ z is much easier. a,b x C a a,b a,b a,b C .+ + + y m1 \u00b7 \u00b7 \u00b7 mn b a,ba,b z C As shown \nin [1, 12], one can actually do better with the an\u00adtichain algorithm, by exploiting any preorder contained \nin language inclusion (e.g., similarity [25]). In this section, we rephrase this technique for antichains \nin our coinductive framework, and we show how this idea can be embedded in HKC, resulting in an even \nstronger algorithm. 5.1 AC with similarity: AC For the sake of clarity, we .x the preorder to be similarity, \nwhich can be computed in quadratic time [14] (or even less [16]): De.nition 9 (Similarity). Similarity \nis the largest relation on states 3 . S \u00d7 S such that x 3 y entails: 1. o(x) = o(y) and a 2. for all \na . A, x ' . S such that x . x ' , there exists some y ' a ' '' such that y . y and x 3 y . One extends \nsimilarity to a preorder 3.. . P(S) \u00d7 P(S) on sets of states, and to a preorder g: . (S \u00d7 P(S)) \u00d7 (S \n\u00d7 P(S)) on antichain pairs, as: X 3.. Y if .x . X, .y . Y, x 3 y , '': '' 3.. (x , Y ) g(x, Y ) if x \n3 x and Y Y . The new antichain algorithm [1], which we call AC , is similar to AC, but the antichain \nis now taken w.r.t. the new preorder g:. Formally, let }: P(S \u00d7 P(S)) . P(S \u00d7 P(S)) be the function de.ned \nfor all relations T . S \u00d7 P(S) by x }T Y if x 3.. ' ' : ' ' Y or (x , Y ) g(x, Y ) for some (x , Y ) \n. T . While AC consists in trying to build a simulation up to t, AC tries to build a simulation up to \n}, i.e., it skips a pair (x, Y ) if either (a) it is subsumed by another pair of the antichain or (b) \nx 3.. Y . Theorem 5. Any simulation up to } is contained in a simulation. Corollary 4. The antichain \nalgorithm proposed in [1] is sound and complete: for all sets X, Y , [ X] . [ Y ] iff AC (X, Y ). Optimisation \n1(a) and optimisation 1(b) in [1] are simply (a) and (b), as discussed above. Another optimisation, called \nOptimi\u00adsation 2, is presented in [1]: if y1 3 y2 and y1, y2 . Y for some pair (x, Y ), then y1 can be \nsafely removed from Y . Note that while this is useful to store smaller sets, it does not allow one to \nexplore less since the pairs encountered with or without Optimisation 2 are always equivalent w.r.t. \nthe ordering g:: Y 3.. Y \\ y1 and, for (Y ) 3.. all a . A, ta ta(Y \\ y1). 5.2 HKC with similarity: HKC \nAlthough HKC is primarily designed to check language equivalence, we can also extend it to exploit the \nsimilarity preorder. It suf.ces to notice that for any similarity pair x 3 y, we have x+y ~ y. Let 3 \ndenote the relation {(x+y, y) | x 3 y}, let r ' denote the constant function to 3, and let c ' = (r ' \n.s.t.u.id). . Accord\u00adingly, we call HKC the algorithm obtained from HKC (Figure 4) by replacing (X, Y \n) . c(R . todo) with (X, Y ) . c ' (R . todo) in step 3.2. Notice that the latter test can be reduced \nto rewriting thanks to Theorem 3 and the following lemma. Lemma 11. For all relations R, c ' (R) = c(R \n. 3). In other words to check whether (X, Y ) . c ' (R . todo), it suf.ces to compute the normal forms \nof X and Y w.r.t. the rules from R . todo plus the rules x + y . y for all x 3 y. Theorem 6. Any bisimulation \nup to c ' is contained in a bisimula\u00adtion. Proof. Consider the constant function r '' : P(P(S) \u00d7 P(S)) \n. P(P(S)\u00d7P(S)) mapping all relations to ~. Since language equiv\u00adalence (~) is a bisimulation, we immediately \nobtain that this func\u00ad '' '' .s.t.u.id). tion is compatible. Thus so is the function c = (r . We have \nthat 3 is contained in ~, so that any bisimulation up to c ' is a bisimulation up to c '' . Since c '' \nis compatible, such a relation is contained in a bisimulation, by Proposition 3. Note that in the above \nproof, we can replace 3 by any other relation contained in ~. Intuitively, bisimulations up to c '' correspond \nto classical bisimulations up to bisimilarity [25] from concurrency. Corollary 5. For all sets X, Y , \nwe have X ~ Y iff HKC (X, Y ).  5.3 Relationship between HKC and AC Like in Section 4.2.1, we can show \nthat for any simulation up to }there exists a corresponding bisimulation up to c ' , of the same size. \nLemma 12. For all relations T . S \u00d7 P(S), }tT . c ' (TT). Proposition 7. If T is a simulation up to }, \nthen TTis a bisimula\u00adtion up to c ' . However, even for checking inclusion of disjoint automata, AC cannot \nmimic HKC , because now the similarity relation allows one to exploit transitivity. To see this, consider \nthe example given in Figure 6, where we want to check that [ z] . [ x + y] , and for which the similarity \nrelation is shown on the right-hand side. Since this is an inclusion of disjoint automata, HKC and AC, \nwhich do not exploit similarity, behave the same (cf. Sections 4.2.1 and 4.2.2). Actually, they also \nbehave like HK and they require 2n+1-1 pairs. On the contrary, the use of similarity allows HKC to prove \nthe inclusion with only 2n + 1 pairs, by computing the Figure 6. Family of examples where HKC exponentially \nimproves over AC , for inclusion of disjoint automata: we have [ z] . [ x+y] .  a,b xC a + x1 a,b + \n\u00b7 \u00b7 \u00b7 a,b + xn. x 3 z a,b yC b a + y1 a,b + \u00b7 \u00b7 \u00b7 a,b + yn. x1 3 . . z1 c . a,b zC a,b + z1 a,b + \u00b7 \u00b7 \n\u00b7 a,b + zn. xn 3 zn a,b following bisimulation up to c ' (Lemma 19 in [6]): R '' = {(z+x+y, x+y)} . \n{(Zi+1+Xi+y+yi+1, Xi+y+yi+1) | i < n} . {(Zi+1+Xi+1+y, Xi+1+y) | i < n} , where Xi = x+x1+ . . . +xi \nand Zi = z+z1+ . . . +zi. Like in Section 4.2.4, to see that this is a bisimulation up to c ' (where \nwe do exploit similarity), consider the pair obtained after reading the word ab: (Z2+x+y+x2+y1, x+y+x2+y1). \nThis pair does not belong to R '' or c(R '' ), but it does belong to ' (R '' ' (R '' c ). Indeed, by \nLemmas 7 and 11, this pair belong to c ) iff Z2 . (x+y+x2+y1).R .: , and we have x+y+x2+y1 (Z1+x+y+y1 \nR '' R .: Z1+x+y+y1+x2 x+y+y1) R .: Z1+X1+y+y1+x2 = Z1+X2+y+y1 (x1 3 z1) R .: Z2+X2+y+y1+x2 (Z2+X2+y \nR '' X2+y) On the contrary, AC is not able to exploit similarity in this case, and it behaves like AC: \nboth of them compute the same antichain Tz as in the example from Section 4.2.4, which has 2n+1-1 elements. \nIn fact, even when considering inclusion of disjoint automata, the use of similarity tends to virtually \nmerge states, so that HKC can use the up to transitivity technique which AC and AC lack.  5.4 A short \nrecap Figure 7 summarises the relationship amongst the presented algo\u00adrithms, in the general case and \nin the special case of language in\u00adclusion of disjoint automata. In this diagram, an arrow X.Y (from \nan algorithm X to Y) means that (a) Y can explore less states than X, and (b) Y can mimic X, i.e., the \nproof technique of Y is at least as powerful as the one of X. (The labels on the arrows point to the \nsec\u00adtions showing these relations; unlabelled arrows are not illustrated in this paper, they are easily \ninferred from what we have shown.) 6. Experimental assessment To get an intuition of the average behaviour \nof HKC on various NFA, and to compare it with HK and AC, we provide some benchmarks on random automata \nand on automata obtained from model-checking problems. In both cases, we conduct the experiments on a \nMacBook pro 2.4GHz Intel Core i7, with 4GB of memory, running OS X Lion (10.7.4). We use our OCaml implementation \n[7] for HK, HKC, and HKC , while we use the libvata C++ library [21] for AC and AC . 1 (To our knowledge, \nlibvata is the most ef.cient implementation currently available for these algorithms, even though this \nlibrary was designed for tree automata rather than plain NFA.) 6.1 Random automata For a given size \nn, we generate a thousand random NFA with n states and two letters. According to [30], we use a linear \ntransi\u00ad 1 More precisely, the upward algorithms provided by default in libvata. General case Disjoint \ninclusion case HKC HKC (5) .(5.3)C (5.3) HKC AC AC ?(3) (4.2) ?(5) C HK 7 AC HKC . AC (4.2.2) .(2.4) \nC Naive HK . Naive Figure 7. Relationship between the various algorithms. Figure 8. Distributions of \nthe number of processed pairs, for the 1000 NFA with 100 states and 2 letters from Table 1. tion density \nof 1.25 (which means that the expected out-degree of each state and with respect to each letter is 1.25): \nTabakov and Vardi empirically showed that one statistically gets more challeng\u00ading NFA with this particular \nvalue. We generate NFA without ac\u00adcepting states: by doing so, we make sure that the algorithms never \nencounter a counter-example, so that they always continue until they .nd a (bi)simulation up to: these \nruns correspond to their worst cases for all possible choices of accepting states for the given NFA.2 \nWe run all algorithms on these NFA, starting from two distinct singleton sets, to measure the required \ntime and the number of processed pairs: for HK, HKC, and HKC , this is the number of pairs put into the \nbisimulation up to (R); for AC and AC , this is the number of pairs inserted into the antichain. The \ntimings for HKC and AC do not include the time required to compute similarity. We report the median values \n(50%), the last deciles (90%), the last percentiles (99%), and the maximum values (100%) in Table 1. \nFor instance, for n = 70, 90% of the examples require less than 155ms with HK; equivalently, 10% of the \nexamples require more than 155ms. (For a few tests, libvata ran out of memory, whence the 8 symbols in \nthe table.) We also plotted on Figure 8 the distribution of the number of processed pairs when n = 100. \nHKC and AC are several orders of magnitude better than HK, and HKC is usually two to ten times faster \nthan AC. Moreover, for the .rst four lines, HKC is much more predictable than AC, i.e., the last percentiles \nand maximal values are of the same order as the median value. (AC seems to become more predictable for \nlarger values of n.) The same relative behaviour can be observed between HKC and AC ; moreover, HKC alone \nis apparently faster than AC . Also recall that the size of the relations generated by HK is a lower \nbound for the number of accessible states of the determinised 2 To get this behaviour for AC and AC , \nwe actually had to trick libvata, which otherwise starts by removing non-coaccessible states, and thus \nre\u00adduces any of these NFA to the empty one.  n = |S| algo. required time (seconds) number of processed \npairs mDFA size 50% 90% 99% 100% 50% 90% 99% 100% 50% HK 0.007 0.022 0.050 0.119 2511 6299 12506 25272 \nAC 0.002 0.003 0.142 1.083 112 245 2130 5208 50 HKC 0.000 0.000 0.000 0.000 21 26 32 63 ~1000 AC 0.002 \n0.002 0.038 0.211 79 131 1098 1926 HKC 0.000 0.000 0.000 0.000 18 23 28 58 HK 0.047 0.155 0.413 0.740 \n10479 28186 58782 87055 AC 0.002 0.003 1.492 4.163 150 285 8383 15575 70 HKC 0.000 0.000 0.000 0.000 \n27 34 40 49 ~6000 AC 0.002 0.003 0.320 0.884 110 172 3017 6096 HKC 0.000 0.000 0.000 0.000 23 29 36 \n44 HK 0.373 1.207 3.435 5.660 58454 164857 361227 471727 AC 0.003 0.004 3.214 36.990 204 298 13801 48059 \n100 HKC 0.000 0.000 0.000 0.001 36 44 54 70 ~30000 AC 0.003 0.004 0.738 6.966 152 211 4087 18455 HKC \n 0.000 0.000 0.000 0.001 31 39 46 64 300 AC HKC AC HKC 0.009 0.001 0.012 0.001 0.010 0.002 0.013 0.001 \n0.028 0.003 0.022 0.002 0.750 0.009 0.970 0.006 562 86 433 76 622 104 484 91 2232 118 920 104 14655 132 \n14160 116 500 AC HKC AC HKC 0.014 0.002 0.025 0.002 0.015 0.005 0.028 0.004 0.039 0.008 0.042 0.007 \n8 0.018 8 0.013 918 130 710 115 986 154 772 136 2571 176 1182 154 8 193 8 169 1000 AC HKC AC HKC 0.029 \n0.007 0.074 0.008 0.031 0.022 0.080 0.019 0.038 0.055 0.092 0.041 8 0.093 8 0.077 1808 228 1409 202 1878 \n271 1488 238 2282 304 1647 265 8 337 8 299 Table 1. Running the .ve presented algorithms to check language \nequivalence on random NFA with two letters. NFA (Lemma 6 (2)); one can thus see in Table 1 that HKC usually \nexplores an extremely small portion of these DFA (e.g., less than one per thousand for n = 100). The \nlast column reports the median size of the minimal DFA for the corresponding parameters, as given in \n[30]. HK usually explores much more states than what would be necessary with a minimal DFA, while HKC \nand AC need much less.  6.2 Automata from regular model-checking Checking language inclusion of NFA \ncan be useful for model\u00adchecking, where one sometimes has to compute a sequence of NFA by iteratively \napplying a transducer, until a .xpoint is reached [8]. To know that the .xpoint is reached, one typically \nhas to check whether an NFA is contained in another one. Abdulla et al. [1] use such benchmarks to test \ntheir algorithm (AC ) against the plain antichain algorithm (AC [31]). We reuse them to test HKC against \nAC in a concrete scenario. We take the sequences of automata kindly provided by L. Holik, which come \nfrom those used in [1] and arise from the model checking of various programs (the bakery algorithm, bubble \nsort, and a producer-consumer system). For all these sequences, we check the inclusions of consecutive \npairs, in both directions. We separate the results into those for which a counter-example is found, and \nthose for which the inclusion holds. The results are given in Table 2. As expected, HKC and AC roughly \nbehave the same: we have inclusions of disjoint automata. HKC is however quite faster than AC : up to \ntransitivity can be exploited thanks to similarity pairs. Like in Table 1, the timings do not include \nthe time required to compute similarity, which is given separately (computed using libvata). Also note \nthat over the 546 positive answers, 368 are obtained immediately by similarity. 7. Related work A similar \nnotion of bisimulation up to congruence has already been used to obtain decidability and complexity results \nabout context\u00adfree processes, under the name of self-bisimulations. Caucal [10] introduced this concept \nto give a shorter and nicer proof of the result by Baeten et al. [4]: bisimilarity is decidable for normed \ncontext-free processes. Christensen et al [11] then generalised the result to all context-free processes, \nalso by using self-bisimulations. Hirshfeld et al. [15] used a re.nement of this notion to get a polynomial \nalgorithm for bisimilarity in the normed case. There are two main differences with the ideas we presented \nhere. First, the above papers focus on bisimilarity rather than lan\u00adguage equivalence (recall that although \nwe use bisimulation re\u00adlations, we check language equivalence since we work on the determinised NFA Remark \n3). Second, we consider a notion of bisimulation up to congruence where the congruence is taken with \nrespect to non-determinism (union of sets of states). Self\u00adbisimulations are also bisimulations up to \ncongruence, but the con\u00adgruence is taken with respect to word concatenation. We cannot consider this \noperation in our setting since we do not have the corresponding monoid structure in plain NFA. Other \napproaches, that are independent from the algebraic struc\u00adture (e.g., monoids or semi-lattices) and the \nbehavioural equiv\u00adalence (e.g., bisimilarity or language equivalence) are shown in [5, 22, 23, 26]. These \npropose very general frameworks into which our up to congruence technique .ts as a very special case. \nTo our knowledge, bisimulation up to congruence has never been proposed as a technique for proving language \nequivalence of NFA. 8. Conclusions and future work We showed that the standard algorithm by Hopcroft \nand Karp for checking language equivalence of DFA relies on a bisimulation up to equivalence proof technique; \nthis allowed us to design a new algorithm (HKC) for the non-deterministic case, where we exploit a novel \ntechnique called up to congruence. We then compared HKC to the recently introduced antichain al\u00adgorithms \n[31] (AC): when checking the inclusion of disjoint au\u00adtomata, the two algorithms are equivalent, in all \nthe other cases  algorithm inclusions (546 pairs) counter-examples (518 pairs) 50% 90% 99% 100% 50% \n90% 99% 100% AC HKC AC HKC sim time 0.036 0.049 0.013 0.000 0.039 0.860 0.798 0.167 0.034 0.185 4.981 \n6.494 1.326 0.224 0.574 5.084 6.762 1.480 0.345 0.618 0.009 0.000 0.012 0.001 0.038 0.094 0.014 0.107 \n0.005 0.193 1.412 0.916 1.047 0.025 0.577 2.887 2.685 1.134 0.383 0.593 Table 2. Timings, in seconds, \nfor language inclusion of disjoint NFA generated from model-checking. HKC is more ef.cient since it can \nuse transitivity to prune a larger portion of the state-space. The difference between these two approaches \nbecomes even more striking when considering some optimisation exploiting sim\u00adilarity. Indeed, as nicely \nshown with AC [1], the antichains ap\u00ad proach can widely bene.t from the knowledge one gets by .rst computing \nsimilarity. Inspired by this work, we showed that both our proof technique (bisimulation up to congruence) \nand our al\u00adgorithm (HKC) can be easily modi.ed to exploit similarity. The re\u00adsulting algorithm (HKC ) \nis now more ef.cient than AC even for checking language inclusion of disjoint automata. We provided concrete \nexamples where HKC and HKC are ex\u00adponentially faster than AC and AC (Sections 4.2.4 and 5.3) and we proved \nthat the coinductive techniques underlying the formers are at least as powerful as those exploited by \nthe latters (Proposi\u00adtions 5 and 7). We .nally compared the algorithms experimentally, by running them \non both randomly generated automata, and au\u00adtomata resulting from model checking problems. It appears \nthat for these examples, HKC and HKC perform better than AC and AC . Our implementation of the presented \nalgorithms is available on\u00adline [7], together with Coq proof scripts, and with an applet making it possible \nto run the algorithms on user-provided examples. As future work, we plan to extend our approach to tree \nau\u00adtomata. In particular, it seems promising to investigate if further up-to techniques can be de.ned \nfor regular tree expressions. For instance, the algorithms proposed in [3, 20] exploit some optimisa\u00ad \ntion which suggest us coinductive up-to techniques. Acknowledgments We are grateful to M. Bonsangue, \nJ. Rutten, and A. Silva for the stimulating discussions that eventually led to this work; and to L. Doyen, \nP. Habermehl, L. Hol\u00b4ik and B. Koenig for their help with antichain algorithms. References [1] P. A. \nAbdulla, Y.-F. Chen, L. Hol\u00b4ik, R. Mayr, and T. Vojnar. When simulation meets antichains. In Proc. TACAS, \nvol. 6015 of LNCS, pages 158 174. Springer, 2010. [2] A. V. Aho, J. E. Hopcroft, and J. D. Ullman. The \nDesign and Analysis of Computer Algorithms. Addison-Wesley, 1974. [3] A. Aiken and B. R. Murphy. Implementing \nregular tree expressions. In FPCA, vol. 523 of LNCS, pages 427 447. Springer, 1991. [4] J. C. M. Baeten, \nJ. A. Bergstra, and J. W. Klop. Decidability of bisim\u00adulation equivalence for processes generating context-free \nlanguages. In Proc. PARLE (II), vol. 259 of LNCS, pages 94 111. Springer, 1987. [5] F. Bartels. On generalized \ncoinduction and probabilistic speci.cation formats. PhD thesis, Vrije Universiteit Amsterdam, 2004. [6] \nF. Bonchi and D. Pous. Extended version of this abstract, with omitted proofs. http://hal.inria.fr/hal-00639716/, \n2012. [7] F. Bonchi and D. Pous. Web appendix for this paper. http://perso.ens-lyon.fr/damien.pous/hknt, \n2012.  [8] A. Bouajjani, P. Habermehl, and T. Vojnar. Abstract regular model checking. In Proc. CAV, \nvol. 3114 of LNCS. Springer, 2004. [9] R. E. Bryant. Graph-based algorithms for boolean function manipula\u00adtion. \nIEEE Trans. Computers, 35(8):677 691, 1986. [10] D. Caucal. Graphes canoniques de graphes alg \u00b4ebriques. \nITA, 24:339 352, 1990. [11] S. Christensen, H. H \u00a8uttel, and C. Stirling. Bisimulation equivalence is \ndecidable for all context-free processes. Information and Computa\u00adtion, 121(2):143 148, 1995. [12] L. \nDoyen and J.-F. Raskin. Antichain Algorithms for Finite Automata. In Proc. TACAS, vol. 6015 of LNCS. \nSpringer, 2010. [13] J.-C. Fernandez, L. Mounier, C. Jard, and T. Jron. On-the-.y veri.ca\u00adtion of .nite \ntransition systems. Formal Methods in System Design, 1 (2/3):251 273, 1992. [14] M. R. Henzinger, T. \nA. Henzinger, and P. W. Kopke. Computing simulations on .nite and in.nite graphs. In Proc. FOCS, pages \n453 462. IEEE Computer Society, 1995. [15] Y. Hirshfeld, M. Jerrum, and F. Moller. A polynomial algorithm \nfor deciding bisimilarity of normed context-free processes. Theoretical Computer Science, 158(1&#38;2):143 \n159, 1996. [16] L. Hol\u00b4ik and J. .a.Sim\u00b4cek. Optimizing an LTS-Simulation Algorithm. Computing and Informatics, \n2010(7):1337 1348, 2010. [17] J. E. Hopcroft. An n log n algorithm for minimizing in a .nite automaton. \nIn Proc. International Symposium of Theory of Machines and Computations, pages 189 196. Academic Press, \n1971. [18] J. E. Hopcroft and R. M. Karp. A linear algorithm for testing equiva\u00adlence of .nite automata. \nTR 114, Cornell Univ., December 1971. [19] J. E. Hopcroft and J. D. Ullman. Introduction to Automata \nTheory, Languages and Computation. Addison-Wesley, 1979. [20] H. Hosoya, J. Vouillon, and B. C. Pierce. \nRegular expression types for XML. ACM Trans. Program. Lang. Syst., 27(1):46 90, 2005. [21] O. Leng \u00b4al, \nJ. Sim \u00b4acek, and T. Vojnar. Vata: A library for ef.cient manipulation of non-deterministic tree automata. \nIn TACAS, vol. 7214 of LNCS, pages 79 94. Springer, 2012. [22] M. Lenisa. From set-theoretic coinduction \nto coalgebraic coinduction: some results, some problems. ENTCS, 19:2 22, 1999. [23] D. Lucanu and G. \nRosu. Circular coinduction with special contexts. In Proc. ICFEM, vol. 5885 of LNCS, pages 639 659. Springer, \n2009. [24] A. Meyer and L. J. Stockmeyer. Word problems requiring exponential time. In Proc. STOC, pages \n1 9. ACM, 1973. [25] R. Milner. Communication and Concurrency. Prentice Hall, 1989. [26] D. Pous. Complete \nlattices and up-to techniques. In Proc. APLAS, vol. 4807 of LNCS, pages 351 366. Springer, 2007. [27] \nJ. Rutten. Automata and coinduction (an exercise in coalgebra). In Proc. CONCUR, vol. 1466 of LNCS, pages \n194 218. Springer, 1998. [28] D. Sangiorgi. On the bisimulation proof method. Mathematical Structures \nin Computer Science, 8:447 479, 1998. [29] D. Sangiorgi. Introduction to Bisimulation and Coinduction. \nCam\u00adbridge University Press, 2011. [30] D. Tabakov and M. Vardi. Experimental evaluation of classical \nau\u00adtomata constructions. In Proc. LPAR, vol. 3835 of LNCS, pages 396 411. Springer, 2005. [31] M. D. \nWulf, L. Doyen, T. A. Henzinger, and J.-F. Raskin. Antichains: A new algorithm for checking universality \nof .nite automata. In Proc. CAV, vol. 4144 of LNCS, pages 17 30. Springer, 2006.     \n\t\t\t", "proc_id": "2429069", "abstract": "<p>We introduce bisimulation up to congruence as a technique for proving language equivalence of non-deterministic finite automata. Exploiting this technique, we devise an optimisation of the classical algorithm by Hopcroft and Karp. We compare our approach to the recently introduced antichain algorithms, by analysing and relating the two underlying coinductive proof methods. We give concrete examples where we exponentially improve over antichains; experimental results moreover show non negligible improvements.</p>", "authors": [{"name": "Filippo Bonchi", "author_profile_id": "81317491714", "affiliation": "CNRS, Lyon, France", "person_id": "P3978029", "email_address": "filippo.bonchi@ens-lyon.fr", "orcid_id": ""}, {"name": "Damien Pous", "author_profile_id": "81430621712", "affiliation": "CNRS, Lyon, France", "person_id": "P3978030", "email_address": "damien.pous@ens-lyon.fr", "orcid_id": ""}], "doi_number": "10.1145/2429069.2429124", "year": "2013", "article_id": "2429124", "conference": "POPL", "title": "Checking NFA equivalence with bisimulations up to congruence", "url": "http://dl.acm.org/citation.cfm?id=2429124"}