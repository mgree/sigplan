{"article_publication_date": "01-23-2013", "fulltext": "\n Cache and I/O Ef.cient Functional Algorithms Guy E. Blelloch Robert Harper Carnegie Mellon University \n guyb@cs.cmu.edu rwh@cs.cmu.edu Abstract The widely studied I/O and ideal-cache models were developed \nto account for the large difference in costs to access memory at different levels of the memory hierarchy. \nBoth models are based on a two level memory hierarchy with a .xed size primary memory (cache) of size \nM, an unbounded secondary memory organized in blocks of size B. The cost measure is based purely on the \nnumber of block transfers between the primary and secondary memory. All other operations are free. Many \nalgorithms have been analyzed in these models and indeed these models predict the relative performance \nof algorithms much more accurately than the standard RAM model. The models, however, require specifying \nalgorithms at a very low level requiring the user to carefully lay out their data in arrays in memory \nand manage their own memory allocation. In this paper we present a cost model for analyzing the memory \nef.ciency of algorithms expressed in a simple functional language. We show how some algorithms written \nin standard forms using just lists and trees (no arrays) and requiring no explicit memory layout or memory \nmanagement are ef.cient in the model. We then describe an implementation of the language and show provable \nbounds for mapping the cost in our model to the cost in the ideal\u00adcache model. These bound imply that \npurely functional programs based on lists and trees with no special attention to any details of memory \nlayout can be as asymptotically as ef.cient as the carefully designed imperative I/O ef.cient algorithms. \nFor example we describe an O( n logM/B n ) cost sorting algorithm, which is B B optimal in the ideal \ncache and I/O models. Categories and Subject Descriptors D.3.1 [Programming Lan\u00adguages]: Formal De.nitions \nand Theory; F.2.2 [Analysis of Algo\u00adrithms and Problem Complexity]: Tradeoffs and Complexity Mea\u00adsures; \nF.3.2 [Logics and Meanings of Programs]: Semantics of Programming Languages General Terms Algorithms, \nDesign, Languages, Performance, Theory. Keywords cost semantics, I/O algorithms 1. Introduction On today \ns computers there is a vast difference in cost for accessing different levels of the memory hierarchy, \nwhether it be registers, Permission to make digital or hard copies of all or part of this work for personal \nor classroom use is granted without fee provided that copies are not made or distributed for pro.t or \ncommercial advantage and that copies bear this notice and the full citation on the .rst page. To copy \notherwise, to republish, to post on servers or to redistribute to lists, requires prior speci.c permission \nand/or a fee. POPL 13, January 23 25, 2013, Rome, Italy. Copyright c &#38;#169; 2013 ACM 978-1-4503-1832-7/13/01. \n. . $10.00 one of many levels of cache, the main memory, or a disk. On current processors, for example, \nthere is over a factor of a hundred between the time to access a register and main memory, and another \nfactor of a hundred or so between main memory and disk, even a solid state disk (SSD). This variance \nin costs is contrary to the standard Random Access Machine (RAM) model, which assumes that the cost of \naccessing memory is uniform. To account for non uniformity several cost models have been developed that \nassign difference costs to different levels of the memory hierarchy. The widely used I/O [2] and ideal-cache \n[9] models both assume a two level memory hierarchy with a .xed size primary memory (cache) of size M, \nan unbounded secondary memory partitioned into blocks of size B. Cost is measured in terms of the number \nof block transfers between primary and secondary memory all other operations are considered free. The \nparameters M and B are considered variables for the sake of analysis and therefore show up in asymptotic \nbounds. Algorithms that do well in these models are often referred to as I/O ef.cient or cache ef.cient \nin this paper we will generically use the term cache ef.cient. The theory of cache ef.cient algo\u00adrithms \nis now well developed (see e.g. the surveys [4, 6, 10, 15, 17, 22]) and the models indeed much more accurately \ncapture the rela\u00adtive cost of algorithms on real machines than the RAM model does. This is true both \nin the context of algorithms that must run off disk when there is not enough main memory, and also in \nthe context of algorithms that can .t in main memory, but not in various levels of the cache. For example, \nthe models properly indicate that a blocked or hierarchical matrix-matrix multiply is much more ef.cient \nthan u s the na\u00a8ive triply nested loop (T nv 3 vs. T(n 3)). In the RAM B M they have equal costs. The \nmodels also indicate that properly imple\u00admented versions of mergesort and quicksort are reasonably cache \nef.cient but that samplesort and multiway mergesort are more ef.\u00adcient, and in fact optimal. Correspondingly \nall the fastest disk sorts indeed use some variant of samplesort or multiway mergesort, as the theory \npredicts [19]. Although the study of cache ef.cient algorithms has been very successful in identifying \nalgorithms that are fast in practice, not sur\u00adprisingly designing and programming algorithms for these \nmodels requires a careful layout of memory and careful management of space. Both temporal and spatial \nlocality is critical in achieving good bounds. Spatial locality is important since memory is moved in \nblocks of size B, corresponding to either cache lines or memory pages. For example although merging two \narrays of integers is rea\u00adsonably ef.cient, the cost of merging two linked lists will depend on how the \nlinks are laid out in memory and needs to be considered with care. Care is also needed when allocating \nand freeing memory since touching unused memory incurs a cache miss. It is therefore important to reuse \nfreed space immediately rather than returning it to a pool which might be evicted by the time it is reused \na generic memory allocator or garbage collection scheme will likely not do the right thing. To properly \nmanage this problem, memory is typically preallocated and fully managed by the user/algorithm designer. \n Needless to say, this form of programming is inconsistent with functional programming, especially when \nusing recursive data types such as lists or trees. However, it is known experimentally that by using \ncertain standard memory allocation schemes purely func\u00adtional programs (no side effects) can be reasonably \ncache ef.cient with regards to both spatial and temporal locality [7, 8, 12, 23]. We give two examples. \nFirstly, consider applying map with some simple function (e.g. increment) over a list of integers, and \nthen applying the same map to the output. If the allocator keeps a pointer that gets incremented on each \nallocation, then after the .rst map all the cells of the list will be allocated adjacently. On the second \nmap since the allocations are adjacent, reading the whole list will only incur O(n/B) cache misses, where \nB is the block size, and evicting the newly generated blocks will also incur only O(n/B) misses. This \ngives O(n/B) cost, which asymptotically matches the cost of an optimal array version in an imperative \nsetting. If the list were in an arbitrary order, the cost would be O(n). All we have done is noted that \nthe temporal locality of the allocations will lead to spatial locality of how they are laid out in memory. \nSecondly, consider a block recursive matrix multiply on two n \u00d7 n matrices. Such an algorithm will never \nrequire more than O(n 2) live space but if recursion stops at problems of a constant size it will allocate \na total of O(n 3) space. Assuming that the max\u00adimum live space .ts within the cache we should be able \nto run our matrix multiply with only O(n 2/B) cache misses, needed for load\u00ading the two matrices and \nstoring the result, but this would require being careful about reusing freed space that is already in \ncache. For\u00adtunately generational garbage collectors have approximately this effect [8]. In particular \nif we make the .rst generation smaller than the size of the cache (M ) then we will reclaim the memory \nwhen\u00adever the allocation area .lls, and reuse memory that is already in cache. This does not quite work \nin general since what is live at the time of the minor collection might get bumped from cache, but it \ngives some indication that it is not hopeless to make the natu\u00adral recursive matrix multiply algorithm, \nas well as similar recursive algorithms, cache ef.cient. We show that one can indeed implement cache-ef.cient \nalgo\u00adrithms in a call-by-value functional setting using recursive data types, and get provably ef.cient \nbounds on cache complexity. In particular we show that one can express algorithms at a high level using \nstandard techniques and achieve optimal asymptotic perfor\u00admance when implemented on the ideal cache. \nOf course we do not expect the algorithm designer to understand the intricacies the garbage collector \nworks in order to analyze their algorithm. Instead our approach consist of providing a reasonably high-level \ncost se\u00admantics that abstracts away from implementation details such as the garbage collection method, \nbut still admits precise analysis of the cost of an algorithm on a two-level memory architecture. We \nthen describe a provably-ef.cient implementation of the language on the ideal-cache model. We show that \nby using this implementa\u00adtion the costs analyzed in the high-level cost model asymptotically match the \nnumber of cache misses in the underlying ideal-cache model. The general idea of using high-level cost \nmodels based on a cost semantics along with a provable ef.cient implementation that maps the cost onto \na lower level machine model has previously been used in the context of parallel cost models [5, 11, 13, \n21]. Our high-level cost model consists of an operational semantics for a call-by-value variant of PCF \nin which we make explicit the allocation of and access to data objects. The store consists of three parts: \na main memory, an allocation cache and a read cache. Both caches have size M and the memory is organized \nin blocks of size B (both measured in terms of abstract data objects). Data can migrate from the allocation \ncache to memory and from memory to the read cache, always in blocks of size B. Allocations are made in \nthe allocation cache, and if the number of live objects in the cache exceeds M, then the B oldest locations \nare evicted to memory as a block, having unit cost. The read cache contains a subset of the memory blocks. \nA read has no cost if its location is in the read or allocation cache, otherwise it requires loading \na block from memory into the read cache, having unit cost, and possibly ejecting an existing block. Hence \nthe only costs are for evicting a block from the allocation cache or loading a block into the read cache. \nSince we are only concerned with measuring the traf.c between main memory and cache memory, garbage collection \nfor main memory is not modeled, but we do account for the detection of live objects, and their migration \nto main memory, when the cache limit is exceeded. The provable implementation uses a generational collector \nto maintain the allocation cache. It uses a nursery of size 2M and al\u00adlocates until the space runs out. \nIt then traces the nursery for the live data. If there is L > M live data, then L - M locations are written \nto memory in blocks of B, leaving the nursery with at most M loca\u00adtions. The implementation allocates \nthe stack in the heap and must amortize the cost of loading old stack frames against other opera\u00adtions \nsince they are not modeled in the high-level cost semantics. We emphasize that the algorithm designer \nneed not know anything about the garbage collector or how the stack is managed to ana\u00adlyze their algorithm; \nthese concepts are only part of the provable implementation. The cost model is described in Section 3 \nand the provable implementation is described in Sections 4 and 5. To demonstrate the utility of our approach, \nin Section 6 we describe some general techniques for analyzing the cost of algo\u00adrithms in our model and \nshow three examples of how to analyze the cost of algorithms in the model: mergeSort, k-way mergeSort \nand matrix multiply. Importantly our results on sorting and matrix multiply match the bounds for algorithms \nimplemented directly in u sus nn n the ideal-cache model (O and O v 3 respec- B logM/B B B N tively). \nThe bounds for sorting are optimal. Because of our provable implementation bounds these results imply \nthat on the ideal-cache model our algorithms written in a functional style using lists and trees are \nasymptotically as ef.cient as the low-level imperative pro\u00adgrams. To analyze the algorithms we introduce \nthe notion of a data structure being compact with respect to a traversal order. This is the way we capture \nthe spatial locality of data structures in a language that has no explicit way to express memory layout. \nRelated Work Although there has been a large amount of experimental work on showing how good garbage \ncollection can lead to ef.cient use of caches and disks ([7, 8, 12, 23] and many references in [14]), \nwe know of none that try to prove bounds for algorithms for functional programs when manipulating recursive \ndata types such as lists or trees. Abello et. al. [1] show how a functional style can be used to design \ncache ef.cient graph algorithms. They however assume that data structures are in arrays (called lists), \nand that primitives for operations such as sorting, map, .lter and reductions are sup\u00adplied and implemented \nwith optimal asymptotic cost (presumably at a lower level using imperative code). Their goal is therefore \nto design graph algorithms by composing these high-level operations on collections. They do not explain \nhow to deal with garbage col\u00adlection or memory management. 2. Background I/O and Caching Models The two-level \nI/O model of Aggarwal and Vitter [2] assumes a memory hierarchy consisting of main memory of size M and \nan un\u00adbounded secondary memory.1 Both memories are partitioned into blocks of size B of consecutive memory \nlocations. All computation must be performed from main memory, which is treated like a stan\u00addard RAM, \nbut there is an additional instruction for moving a block of memory from secondary memory to main memory \nand one for moving the other way. The cost of an algorithm is analyzed in terms of the number of block \ntransfers the cost of operations within the main memory is ignored. Many algorithms can be analyzed in \nthis model and it is perhaps surprising how accurately it is able to capture the relative performance \nof algorithms. In their original work, for example, Aggarwal and Vitter showed tight upper and  u s \nlower bounds for sorting n keys, with I/O cost T n logM/B n . BB This can be seen by noting that at any \ngiven time we only need one but do not require arrays or explicitly memory management. We .rst consider \nmergesort. Throughout our discussion we assume that the elements being sorted each .t in a single machine \nword. All cache ef.cient algorithms we know of for sorting store the input and output elements directly \nin arrays. First consider merging two arrays of keys A and B into an output array C of length n (as usual \nwe assume the inputs are sorted in A and B in increasing order). The standard sequential algorithm for \nmerging starts at the beginning of each array keeping a .nger on each, .nding the the lesser of the two \nkeys at the .ngers, copying this key to C, and incrementing the appropriate .nger. This algorithm has \na cache complexity O(n/B) as long as M = 3B. The two algorithm that match this bound are a multiway merge\u00adblock \nfrom each of A, B and C resident in cache, and that we fully process the block before needing the next \nblock. Therefore every block is only needed once. For mergesort we assume the standard divide-and-conquer \nver\u00ad sion, which recursively sorts each half of the array and then merges the result. Since merging as \ndescribed cannot be done in place we have to be speci.c on how to manage memory. In particular allo\u00ad \ncating a new array for the result and then freeing the two old arrays using a general purpose memory \nallocator will likely not lead to the desired bounds (unless one can ensure special properties of the \nmemory allocator). Instead the algorithm needs to pre-allocate a temporary array of length n and pass \nparts of this array to all sub\u00ad calls. In particular mergesort could take as arguments both the input \narray and an equal length temporary array. The result is returned in the input array and the temporary \narray is used to merge into. Al\u00adsort and a distribution sort, which are the standard algorithms used \nfor disk based sorting, and they both perform signi.cantly better than quicksort or standard mergesort. \nThese algorithms are more ef.cient since they do not need to pass over the data as many times. The I/O \nmodel can capture either the distinction between cache and main memory or between main memory and disk. \nIn the .rst case the memory size corresponds to the cache size and the block size to the cache-line size, \nand in the second case the memory size corresponds to the main memory size and the block size to the \npage size (or whatever the transfer size between the disk and main memory is). One might note, however, \nthat while the I/O model assumes two address spaces and the user explicitly moves data between them, \na machine with caches assumes a single address space and makes its own decisions about what gets evicted \nfrom cache, e.g., using a least recently used (LRU) policy. The ideal-cache model [9] can be used to \nbetter model a cache. It is similar to the I/O model but assumes the primary memory is treated as a cache \nwith an ideal eviction policy. In particular the programmer only accesses one address space and a block \nis brought into the cache when a memory location is accessed whose block is not already in cache. Bringing \nin a block might require evicting another block from the cache. The model assumes that the best decision \nis always made, which is to evict the line used furthest in the future (the optimal off-line replacement \npolicy). Since in practice we don t know the future, this is not possible on-line, but it is proved by \nSleator and Tarjan s seminal work on competitive paging [20] that an LRU policy is always competitive \nwith the optimal strategy (within constant factors in time and cache size). Therefore from a theoretical \npoint of view the models are though these optimizations are relatively obvious and standard to programmers \nof imperative code, we bring them up to emphasize the care that needs to be taken to ensure the cache \nbounds it is not simply an issue of reducing the number of calls to the memory allocator, it can actually \nasymptotically affect the cache bounds. The cache complexity of this mergesort can be analyzed by considering \ntwo cases. The .rst is when the full computation .ts in cache. In this case the two arrays need only \nbe loaded into cache once and all the work can be done in cache. The problem .ts in cache as long as \n2n + log n = M, where the log n accounts for the stack size. The second case is when the problem does \nnot .t in cache. In this case we have to pay for the cache misses on the two recursive calls plus the \ncache misses of the merge. This gives the following recurrence for the cache complexity Q(n): asymptotically \nthe same. In this paper we will be using the ideal\u00ad 2Q( n 2 ) + O( n B ) 2n + log n > M (1) otherwise \n cache model for simplicity although the results are also apply to the Q(n) = O( n B ) I/O model. The \nideal-cache model is often used in the context of cache\u00adoblivious algorithms. These are simply algorithms \nfor which the algorithm does not make any decisions based on the cache parame\u00adters M and B, although \nof course the analysis of cache complexity will depend on M and B. The advantage of cache-oblivious algo\u00adrithms \nis that since they are oblivious to the cache parameters they work across multiple levels of a cache \nhierarchy simultaneously. Most of the algorithms in this paper are cache oblivious, but our k-way mergesort \nis not. We leave it as an open question whether it is possible to develop an I/O-ef.cient cache-oblivious \nsorting algo\u00adrithm in our model. Cache Ef.cient Algorithms We now review some basic well known results \non cache ef.cient algorithms in the imperative setting. The functional algorithms we present in Section \n6 are based on the algorithms described here, 1 Aggarwal and Vitter also considered a version of the \nmodel with parallel disk access, but most interesting results are explained with the single disk version. \nThe solution to this recurrence can be derived by noting that the top log2(2n/M ) levels of the recursion \ndo not .t in memory while the lower levels do. The total cache complexity across each of the upper levels \nis O(n/B) so the total overall cache complex\u00adity is O(n/B log2(n/M )). We note that this does not match \nthe optimal cache complexity for sorting but is signi.cantly better than simply assuming every access \nis a cache miss speci.cally, a fac\u00adtor of B log n/(log n - log M) better. For sorting 1012 words in a \nmemory with 109 words and a block size of 103 words, it is about a factor of about 4000 better. Quicksort \nhas basically the same com\u00adplexity as mergesort, although in the expected case. This is because scanning \nthe input array to split it into the lesser and larger ele\u00adments can be done using two .ngers like in \nmerging so again each block only needs to be loaded once. We now describe a sort that is optimal for \nthe I/O model. The idea is instead of partitioning the input array into two and recursively calling sort \non each, to partition the input array into k parts, sort each part, and then merge all the parts. Since \ninstead of having just two arrays to merge we have k arrays, we require a k-way merge. Without going \ninto too much detail, such a merge can be implemented using one block of memory for each of the inputs \nneeding to be merged as well as one block for the output. We keep a .nger on each input and on each step \nselect the minimum key at the .ngers, move it to the output buffer and increment that .nger. As long \nas all input blocks, the output block and any data for maintaining the .ngers .t in cache, then the k-way \nmerge will run with cache complexity O(n/B), which is the same as the binary merge. Since there will \nbe k input blocks and 1 output blocks, the space needed for the blocks is (k + 1)B. Therefore accounting \nfor overheads everything will .t in cache as long as ckB = M, or equivalently k = M/(cB) for some constant \nc. We therefore pick k to be as large as possible, giving k = M/(cB) As in the two way merge we need \nto be careful about allocation and preallocate temporary arrays to copy the output. We again can analyze \nthe algorithm by considering the case when the problem .ts in memory and when it does not. This gives \nthe recurrence:  n ) ' M Q( )+ O( n nc> M B M/cB B Q(n) = (2) O( n B ) otherwise where c and c' are \nconstants, but n, B and M are variables. This solves to O(n/B logM/B (n/B)). This bound matches the lower \nbound for sorting in the I/O model [2] and hence also the ideal\u00adcache model. The k-way mergesort is therefore \nasymptotically optimal. 3. Cost Semantics In this section we de.ne an evaluation dynamics that assigns \na cost to a complete execution of a program. Following the I/O model, the cost measures the cache complexity, \nwhich is de.ned to be the traf.c caused by the transfer of objects between the main memory and the memory \ncache. Accesses to objects in cache are considered to be cost-free, whereas migration of objects from \ncache into memory and from memory into the cache are charged unit cost. The dynamics is based on a two-level \nmodel of storage that includes a .xed-size allocation cache and a .xed-size read cache together with \na main memory of unbounded size. The evaluation dynamics provides the basis for assessing both the correctness \nand the cache complexity of programs. It is formu\u00adlated at a suf.ciently abstract level to free the programmer \nfrom having to reason directly about the compiler and run-time system, but is suf.ciently concrete as \nto admit an implementation with a provable bound on its cache complexity. Thus, we may achieve the same \noverall results as are obtained using only low-level machine models in previous work on I/O algorithms, \nwhile working at the much more practical level of abstraction offered by functional pro\u00adgramming languages. \nWe give the dynamics of a call-by-value variant of Plotkin s PCF language [18]. The syntax of expressions \nis summarized by the following grammar: e ::= x | z | s(e) | ifz(e; e0; x.e1) |fun(x, y.e) | app(e1; \ne2) The conditional tests whether a number is zero or not, and passes the predecessor to the non-zero \ncase. Functions are equipped with a name for themselves to allow for recursion. The typing rules are \nstandard, and are omitted here for the sake of concision. (See, for example, Chapter 10 of [13].) For \nillustrative purposes natural numbers are treated as heap\u00adallocated data structures of unbounded size \n(as will become evident shortly). It is straightforward to extend the language to account for a richer \nvariety of data structures, including sum, product, .nite sequence, and recursive types, and to account \nfor typical hardware-oriented concepts such as machine words and .oating point numbers. Storage Model \nFollowing Morrisett, et al. [16], the dynamics distinguishes large from small values, with large values \nbeing allocated in memory and represented by a location, and small values being those that are manipulated \ndirectly. In the present case the only small values are locations, but it is also possible to consider, \nfor example, .xed\u00adsized numbers as forms of small value. Correspondingly, all other forms of value (numbers \nand functions) are large. We also allocate stack frames, which reify the control state of evaluation, \nin memory. A memory object is either a large value of a stack frame. The two-level memory model is parameterized \nby two con\u00adstants, the block size B, and the cache size M = c \u00d7 B determined by some constant c representing \nthe number of blocks in the cache. A memory \u00b5 is a .nite mapping assigning a memory object to each of \na .nite set dom(\u00b5) of abstract locations. The memory may grow without bound. (We do not consider here \nthe separate problem of garbage collection for main memory, for which see Morrisett, et al. [16].) As \na technical convenience, we assume that locations are divided into two classes, value locations, l, and \nstack locations, s, and require that a memory map value locations to large values and stack locations \nto stack frames. When the distinction is immaterial, we speak simply of locations and objects in memory. \nA memory \u00b5 comes equipped with an equivalence relation l =\u00b5 l' over dom(\u00b5) specifying that l and l' are \nneighbors in \u00b5. Addi\u00adtionally, we require that each equivalence class in the domain of a memory is of \nsize B. A memory whose domain consists of a sin\u00adgle equivalence class of size B is called a block. The \nneighborhood nbhd(\u00b5, l) of a location l . dom(\u00b5) is the restriction of \u00b5 to the neighbors of l in \u00b5, \na single block. The expansion \u00b5 . \u00df of a mem\u00adory \u00b5 by a block \u00df such that dom(\u00df) n dom(\u00b5) = \u00d8 is the \nmemory \u00b5' that agrees with \u00b5 and \u00df on their respective domains and for which l =\u00b5i l' iff l =\u00b5 l' or \nl =\u00df l'. There are two forms of cache mediating access to memory. A read cache . for a memory \u00b5 is the \nrestriction of \u00b5 to a .nite set of locations of size at most M. The contraction . 8 \u00df of a read cache \n. by a block \u00df . . is the read cache .' such that . = .' . \u00df. A nursery . is a .nite mapping that associates \nan object to each a .nite set dom(.) of locations. A nursery comes equipped with a linear ordering l \n-. l' of dom(.), called the allocation ordering. If l -. l' we say that l is older than l' and that l' \nis newer than l in .. The extension .[l . o] of a nursery . binding a location l /. dom(\u00b5) to an object \no is the nursery .' such that (1) .'(l) = o and .'(l') = .(l') for each l' . dom(.), and (2) l' -.i l \nfor every l' . dom(.). The contraction . 8 \u00df of a nursery . by a block \u00df . . is the restriction of . \nto dom(.) \\ dom(\u00df). The live locations live(R, . ) in a nursery . relative to a subset R . dom(.) consists \nof those locations in dom(.) that are (tran\u00adsitively) reachable from locations in R. The scan scan(R, \n. ) of a nursery . with respect to a subset R . dom(.) is the block \u00df of consisting of the oldest B live \nlocations in live(R, .). (See Mor\u00adrisett, et al. [16] for formal de.nitions of these standard concepts.) \nIt will be an invariant of the dynamics that the nursery contains at most M live objects relative to \nthe roots of the computation. A store s is a triple (\u00b5, ., .) consisting of a memory \u00b5, a read cache \n. for \u00b5, and a nursery . such that dom(.)ndom(\u00b5) = \u00d8. The domain of a store s is de.ned by dom(s) = dom(\u00b5) \n. dom(.). An initial store is a store in which the main memory contains only large values and in which \nthe read cache and allocation area are empty. Evaluation Dynamics The overall goal of the evaluation \ndynamics is to de.ne the evalu\u00adation of a closed expression by an inductive de.nition of a relation between \nan expression and its value, which is always small, and its cost, a non-negative integer. The cost is \ncomputed by tracking the  movement of objects among the components of the store, charg\u00ading one unit \nof cost whenever a block of objects must be moved s @ z .n to or copied from main memory, and charging \nzero cost otherwise. R s ' @ l ' (So, for example, a computation that runs entirely in cache will be \nR s ' s @ z .n @ l ' (3a) assigned zero cost, consistently with the I/O model.) To account  for the \nmemory traf.c involving values, the dynamics makes ex\u00ad s @ s(-) .n i) s ' @ s @ e i ' s ' ' .n i} s '' \n@ l '' R.locs(eR.{s plicit the allocation of objects in the nursery, their eviction to main ii '' '' \n''' '''memory when the capacity of the nursery is exceeded, and their s ) .n s @ s(l R @ l i ii movement \ninto the read cache as they are required by the computa\u00ad ' ) .n+n +n s ''' @ l ''' s @ s(e R tion. To \naccount for the memory traf.c attributable to the implicit (3b) control stack, the dynamics also allocates \n(but does not otherwise use) stack frames, and ensures that any data that would appear in . . . . . . \ns @ ifz(-; e2; x.e3) .n1 s1 @ s1 R.locs(e1) the stack is kept live by the dynamics. i n ' ' 1 These considerations \nlead to the evaluation judgment s1 @ e1 .s1 @ l1 R.{s1} . . . . ii ' ' ' ' s 1 .n s2 @ e2 .n2 s 1 @ \nl 1 s2 @ z R @ l ' s @ e .n R s @ l n1+n1+n1 +n2 s @ ifz(e1; e2; x.e3) .R i ii s ' @ l ' (3c) stating \nthat the expression e, when evaluated with respect to a store s such that dom(s) . locs(e) and to roots \nR . dom(s), results in a modi.ed store s ' , a location l representing the (large) value of . . . . . \n. s @ ifz(-; e2; x.e3) .n1 s1 @ s1 R.locs(e1) i n ' ' s1 @ e1 .1 s1 @ l1 R.{s1} the expression, and a \ncost n representing the cache complexity of the execution. The modi.cations to the store consist of allocations \n... ... s1 ' @ l1 ' .n ii 1 s3 @ s(l '' s3 @ [l '' 1 ) 1 /x]e3 .n3 s ' @ l ' R in the nursery, migrations \nof objects from the nursery to the main memory, and copying of objects from the main memory to the s \n@ ifz(e1; e2; x.e3) . n1+n1+n1 +n3 R i ii s ' @ l ' (3d) read cache. All memory traf.c occurs in blocks \nof B objects, R s ' s @ fun(x, y.e) .n @ l corresponding to loading a cache line or reading a block from \ndisk. R s ' The roots R represent locations that are to be kept live by virtue of s @ fun(x, y.e) .n \n@ l (3e) their being present in the implicit control stack or expression under evaluation. The evaluation \njudgment is de.ned by the rules in Figure 1, . .. . . .. . i n ' ' s @ app(-; e2) .n1 R.locs(e1) s1 @ \ns1 s1 @ e1 . 1 s1 @ l1 R.{s1} ii iii ' ' '' '' ' n 1 1 @ l1 .n s1 @ fun(x, y.e) s1 @ app(l1; -) .1 making \nuse of two auxiliary judgments for reading and allocating objects de.ned in Figure 2. It may be helpful \nto read through the s s2 @ s2 R . .. . .. ''''' '' s2 @ e2 .n2 s s 1, l2/x, y]e .n2 s R.{s2} 2 @ l2 2 \n@ [l R @ l rules once while ignoring all but the evaluation judgments to see i ii iii i n1+n1+n1 +n +n2+n \n1 2 s @ app(e1; e2) .R s ' @ l ' (3f) Figure 1. Cost Dynamics l . dom(.) (\u00b5, ., .) @ l .0 (\u00b5, ., .) \n@ .(l) (4a) l . dom(.) (\u00b5, ., .) @ l .0 (\u00b5, ., .) @ .(l) (4b) l /. dom(.) . dom(.) | dom(.)| = M - B \n(\u00b5, ., .) @ l .1 (\u00b5, . . nbhd(\u00b5, l), .) @ \u00b5(l) (4c) l /. dom(.) . dom(.) | dom(.)| = M \u00df . . (\u00b5, ., .) \n@ l .1 (\u00b5, . 8 \u00df . nbhd(\u00b5, l), .) @ \u00b5(l) (4d) | live(R . locs(o), .)| < M l /. dom(.) (\u00b5, ., .) @ o .0 \nR (\u00b5, ., .[l . o]) @ l (5a) | live(R . locs(o), .)| = M \u00df = scan(R . locs(o), .) l /. dom(.) (\u00b5, ., \n.) @ o .1 R (\u00b5 . \u00df, ., (. 8 \u00df)[l . o]) @ l (5b) Figure 2. Reading and Allocation that the rules de.ne \na conventional eager dynamics for a functional language. On such a reading the root set plays no role, \nand can be ignored. Moreover, the cost assignment has no signi.cance under such a simpli.cation. Next, \nlet us consider the roles of the read judgments s @ l .n .n s ' o @ s ' and the allocate judgments s \n@ v R @ l, where v is a value, in the dynamics. The quoted read judgment states that the result of reading \nlocation l in store s results in the object o and the modi.ed store s ' , and has cost n = 0 or n = 1. \nThe cost is non-zero only if the read causes a block to be loaded into the read cache. The modi.ed store \nrepresents the possible effect of loading a block into the read cache. The quoted write judgment states \nthat allocating the large value v in store s results in a modi.ed store s ' and location l . dom(s ' \n), and has cost n = 0 or n = 1. The cost is non-zero only if the allocation causes the eviction of a \nblock from the nursery in order to maintain the live-size invariant. The read and allocate operations \nin the dynamics record the memory traf.c engendered by the creation and examination of values during \ncomputation. It remains to consider the role of the allocation judgments of the form s @ s .n s ' @ f, \nwhich represent the allocation of a stack R frame in the store at stack location s. The purpose of allocating \nthese frames is purely to ensure that the cost assigned to a com\u00adputation is accurate with respect to \nthe underlying implementation. Although an evaluation semantics has no explicit control stack, it is \nnevertheless the case that an implementation must allocate space for the representation of the control \nstate, and this space alloca\u00adtion does in.uence the cache behavior of the computation. It may not, therefore, \nbe ignored. Our method for accounting for the mem\u00adory effects of the control stack is to allocate explicitly \nframes that would appear in the control stack to ensure that space usage is prop\u00aderly accounted for, \nand that required liveness information (to be detailed shortly) is properly maintained. The frames are \ndenoted as app(-; e2) and app(l1 ' ; -) in the cost dynamics.  With this in mind, let us examine in \ndetail Rule 3f in Figure 1. We are to evaluate and determine the cost of app(e1; e2) in store s with \ngiven roots R. First, we allocate a stack frame s1 representing the pending evaluation of e2 during the \nevaluation of e1. This frame is now considered live, even though it does not appear in any expression \nunder consideration. Accordingly, we evaluate e1 relative to the store containing this frame, treating \nthe just-allocated stack pointer to be live (as indicated by .R.{s1}). This results in a location l1 \n' , which we then read from the store to obtain a function abstraction (as would be guaranteed by the \nstatic type discipline omitted here). We then create another frame s2 corresponding to the suspended \napplication of the function at location l1 ' , and evaluate e2 with this stack pointer considered live \n(as indicated by .R.{s2}) to obtain location l2 ' . Finally, we evaluate the function body, replacing \nthe self variable by l1 ' and the argument by l2 ' . The overall cost of the computation is the sum of \nthe costs of each of these steps, which are given either inductively or by the uses of the read and allocate \njudgments. Observe that this rule properly accounts for tail recursion in that no extra space is held \nduring tail recursive calls (as indicated by .R). It remains to explain the read and allocate judgments \nde.ned in Figure 2. The read judgment assigns cost zero to any read from a location in either the nursery \nor the read cache (Rules 4b and 4a). Such reads have no effects, and hence induce no cache traf.c. A \nread of a location that is only in main memory induces a load of the neighborhood of that location (a \nblock of memory) into the read cache. If there is suf.cient room for it in the read cache, the block \nis added to the cache and the contents is returned, at a cost of one unit (Rule 4c). If there is insuf.cient \nroom in the read cache, a block is selected non-deterministically to be replaced by the required block, \nand once again a unit cost is charged to the read (Rule 4d). At the end of the section we discuss the \nuse of non-deterministic eviction. The allocate judgement de.nes the procedure for creating new objects \nin the store. Of course, new objects are considered newer in the allocation ordering than the objects \nalready present in the nursery. If the new object .ts within the nursery, it is allocated there at zero \ncost (Rule 5a). If the new object will not .t within the nursery, then the block consisting of the oldest \nB live objects in the nursery is evicted to main memory, making room for the newly allocated object; \nsuch an allocation is charged unit cost (Rule 5b). It is important to our method that the oldest objects \nbe evicted from the cache as a block forming the neighborhood of each of its locations. Whether an object \n.ts within the nursery is determined as follows. The nursery is full if the number of live objects in \nit is exactly M. (It is for the sake of assessing liveness that the allocation judgment is parameterized \nby a root set.) Eviction of a block reduces this to at most M - B objects, so that the next B - 1 allocations \nwill not cause an eviction. Thus we are, in effect, charging at most 1/B units of cost to each allocation \n(less if objects die before needing to be evicted). It is essential to our results that the liveness \nof objects in the nursery may be assessed without accessing main memory. Given roots R we need only trace \nobjects in the nursery itself, and need never consider locations lying outside of it. This is ensured \nby two properties of the dynamics. First, since the model is purely func\u00adtional, the dependency graph \nof objects in the nursery is acyclic; an object may only refer to objects allocated earlier in the compu\u00adtation \nas de.ned by the allocation ordering. Second, implicit stack frames are explicitly allocated in the nursery \nto ensure that liveness may be assessed solely by examining the nursery itself, starting from the root \nset. Put another way, an object in the nursery can\u00adnot be live solely because of a pointer from main \nmemory back to the nursery. This property is a consequence of immutability and the explicit allocation \nof stack frames in the semantics. In Section 6 we will make use of a deep copy operation on values of \ncertain types. In the illustrative language considered here this operation is de.nable on natural numbers \nas follows: fun(copy, x.ifz(x; z; x ' .s(app(copy; x ' )))). Calling this function on a number n has \nthe effect of creating a fresh copy of n in the heap. No such operation is de.nable, or required, for \nfunction types. Deep copying is easily extended to product, sum, and inductive types, but would need \nto be provided as a primitive for base types such as .xed precision integers or .oating point numbers. \nDiscussion We brie.y discuss some of the motivation for the decisions we made in formulating the dynamic \nsemantics. The overall goal is to allow a simple analysis for the algorithm developer while capturing \nall the costs needed to prove asymptotic implementation bounds. The separate allocation cache is important \nboth for convenience of analysis and properly accounting for costs. It ensures that all short lived allocations \nnever need to be allocated to memory. For a subcomputation in which the maximum footprint of live data \nallocated .ts in the allocation cache, the user need not worry about any costs for any temporary memory. \nIn a block matrix multiply on n \u00d7 n matrices, for example, once kn2 = M for some small constant k, the \nonly cost that needs be considered is the cost of reading the input and evicting the output. This is \nthe case even though the multiply will allocate a total of T(n 3) space. It is also important that the \npartitioning of locations into blocks is not decided until locations are evicted from the allocation \ncache, which ensures that only live data is ever migrated to memory. If blocking were to be decided on \nallocation, for example, then by the time the objects are evicted most of the objects in a block may \nno longer be live. This would break the bounds we give in Section 6. The cost semantics accounts for \nthe allocation of stack frames in order to account for the space required to manage the control state \nof evaluation. This is particularly important in the case that no allocation is associated with the creation \nof a frame, for then there is no possibility to amortize the space required for the frame against the \nallocated object. Note that the semantics only models the space taken by the frames in the allocation \ncache and the cost of evicting them. It does not model any costs associated with reloading them into \nthe read cache. As described in the next section, in a lower level model this can be amortized against \nthe cost of evicting the frames in the .rst place. It is important that the stack frames be heap allocated. \nA crucial invariant we require is that all live data in the allocation cache can be determined solely \nthrough the caches. If we had a separate stack cache it could allow for the eviction of a stack frame \nthat references data in the allocation cache, breaking the required invariant. There are other techniques \nto handle this problem but we found that allocating the stack frames in the heap is the easiest. Our \nmodel is non-deterministic in the choice of what block is evicted from the read cache in the case of \na read miss. In our provable implementation bounds we show that if there is a (non\u00addeterministic) execution \nthat gives certain cache complexity then we can guarantee those bounds on the ideal cache model (within \nconstant factors). When analyzing an algorithm this allows one to consider any policy for eviction. This \nis possible because the ideal cache makes the optimal decisions and will therefore be at least as good \nas the policy the user assumes. The justi.cation for the ideal cache model is given in Section 2.  4. \nAbstract Machine The abstract cost of a computation assigned by the evaluation dy\u00adnamics given in the \npreceding section is validated in two stages. First, in this section we de.ne an abstract machine with \nan ex\u00adplicit control stack, and show that the evaluation dynamics accu\u00adrately predicts the behavior of \nthe abstract machine with respect to both the outcome and the cost of the computation. Second, in Section \n5 we show how to implement the basic operations of the abstract machine with only a small overhead. Taken \ntogether these two arguments demonstrate that the evaluation semantics provides an accurate model of \nthe cache complexity of a program when im\u00adplemented as described in these two steps. The abstract machine \ntakes the form of a labeled transition system between states of two different forms: 1. Evaluation state: \ns @ k C e, where k . dom(s) is a stack pointer, and locs(e) . dom(s), stating that e is to be evaluated \non stack k relative to store s. 2. Return state: s @ k < l, where k, l . dom(s), stating that small \nvalue l is to be returned to stack k relative to store s.  The control stack is represented by a stack \nlocation, k, that refers to a linked list of frames, either the empty stack, written , or a frame together \nwith another stack location, written f;k. The label on a transition is either 0, 1, or 2, and speci.es \nthe amount of work to be charged for that transition. The rules given in Figure 3 de.ne the abstract \nmachine. Their overall form is standard (see, for example, Chapter 27 of [13]), with the main differences \nbeing that allocation and reading of values is made explicit, just as in the evaluation dynamics, and \nthat the stack is explicitly represented as a linked data structure in the store. The n ' multistep transition \njudgment s -. * s means that there is a .nite, possibly empty, sequence of transitions from s to s ' \nwhose labels sum to n. TH E O RE M 4.1 (Correctness of Evaluation Dynamics). Let s0 be an initial store, \nlet e0 be a closed expression such that locs(e0) . dom(s0). Let the abstract machine be equipped with \none additional block in the read cache, and let k0 be a reserved stack location not used in the evaluation \ndynamics. If s0 @ e0 .n \u00d8 s @ l, then there is an evaluation m' ' s0[k0 . ] @ k0 C e0 -. * s [k0 . ] \n@ k0 < l such that = s ' 1. the results are isomorphic, s @ l ~@ l ', and 2. the cost m is at most 3n. \n The relation s @ l ~= s ' @ l ' states that the reachable graph from l in s is isomorphic to the reachable \ngraph from l ' in s ' . Theorem 4.1 states that the outcome of a computation on the abstract machine \nis the same, up to choice of locations, as the out\u00adcome of the same computation according to the evaluation \ndynam\u00adics. Moreover, the total cost of the machine execution (measured in accordance with the I/O model \ndescribed earlier) is at most a small constant factor larger than the cost assigned by the evalua\u00adtion \ndynamics. The content of the theorem amounts to a proof that the space required by the control stack \nin a computation may be managed so as not to interfere with space usage of the computation itself. The \ncorrectness proof may be decomposed into three major components. The .rst obligation is to relate the \noutcome of the evaluation dynamics to that of the abstract machine, disregarding, for the moment, the \ncost. The required correspondence is proved s @ kC l -0 (6a) . s @ k< l s @ z .n {k} s ' @ l n s @ k \nC z -. s ' @ k< l (6b) s @ s(-);k .n i) s ' @ k ' locs(en s @ k C s(e ' ) -. s ' @ k ' C e ' (6c) {ki} \ns '' s @ k .n s ' @ s(-);k ' s ' @ s(l) .n i @ l ' i n+n s @ k < l ---. s '' @ k ' < l ' (6d) s @ ifz(-; \ne2; x.e3);k .n s ' @ k ' locs(e1) . s ' @ k ' s '' s @ k C ifz(e1; e2; x.e3) -nC e1 (6e) s @ k .n1 s \n' @ ifz(-; e2; x.e3);k ' s ' @ l .n2 @ z n1+n2 s @ k < l ----. s '' @ k ' C e2 (6f) s '' s @ k .n1 s \n' @ ifz(-; e2; x.e3);k ' s ' @ l .n2 @ s(l ' ) n1+n2 . s '' s @ k < l ----@ k ' C [l ' /x]e3 (6g) s @ \nfun(x, y.e) .n {k} s ' @ l ' n s @ k C fun(x, y.e) -. s ' @ k < l ' (6h) s @ app(-; e2);k .n s1 @ k1 \nlocs(e1) n s @ k C app(e1; e2) -. s1 @ k1 C e1 (6i) s @ k .n1 s1 @ app(-; e2);k1 s1 @ app(l1; -);k1 .n2 \ns2 @ k2 locs(e2) n1+n2 s @ k < l1 ----. s2 @ k2 C e2 (6j) s @ k .n1 s1 @ app(l1; -);k2 s1 @ l1 .n2 s2 \n@ fun(x, y.e) n1+n2 s @ k < l2 ----. s2 @ k2 C [l1, l2/x, y]e (6k) Figure 3. Abstract Machine by induction \non the derivation of evaluation judgment. Speci.cally, we prove that if s @ e .n @ l, then for any stack \npointer k, R s ' s @ kC e . * s @ k < l. The proof proceeds along standard lines, as described, for example, \nin Chapter 27 of the second author s textbook [13]. The same choice of locations may be made in the machine \nderivation as were made in the evaluation derivation, because the sequence of value allocations is precisely \nthe same in both forms of dynamics. The next step of the proof is to show that the abstract machine performs \nthe same sequence of value reads in the same order as speci.ed by the evaluation dynamics. This may be \nproved along with the correspondence described in the preceding paragraph. The argument relies on two \nimportant properties of the evaluation dynamics: 1. Any read of a value location is either a read of \na location in the initial store, or a location that was allocated earlier in the evaluation. 2. Stack \nframes are allocated, but never read, in order to ensure that eviction of blocks from the nursery occurs \nin exactly the order imposed by the stack-based abstract machine.   A deterministic nursery eviction \npolicy is required to ensure that the memory reads correspond exactly between the evaluation dynamics \nand the abstract machine. We can assume whatever policy is used the the dynamic semantics is also used \nby the abstract machine. It remains to show that the stack reads employed by the abstract machine do \nnot impose an asymptotically signi.cant cost beyond what is predicted by the evaluation dynamics. Without \nspecial provision, access to the control stack would interfere with the allocation of data in the read \ncache, invalidating the cost given to the computation by the evaluation dynamics. To avoid this we make \nuse of a dedicated read cache block in the abstract machine, which we will call the stack cache block, \nand explicitly manage this cache block as follows. Whenever a stack location is read from main memory, \nits neighborhood is loaded into the stack cache block, evicting the block that currently occupies it. \nWe will argue that the cost of loading the stack cache can be amortized across the execution sequence, \neven if the same block is loaded into the stack cache more than once, a possibility that will be detailed \nshortly. The validity of the argument depends on two special properties of the run-time stack, namely \nthat each allocated frame is read exactly once in a complete computation, and that the preceding stack \nframe is always older than the current one. Given such an amortization, it is then clear that the overall \ncost of execution on the abstract machine is bounded by a small constant factor of the cost ascribed \nto it by the evaluation dynamics, establishing the theorem. To complete the proof, we describe the amortization \nof the cost of stack management in more detail. As an invariant we put a dollar on every memory block \nthat contains a stack frame, except if it is the youngest such block and resides in the stack cache block, \nin which case it has no money associated with it. When the abstract machine evicts a block containing \na stack frame from the allocation cache we spend three dollars one for the eviction itself, one to put \na dollar on the evicted block, and one to put a dollar on the block that is in the cache stack block. \nThis third dollar might be needed to maintain our invariant since that block, if there is one, will no \nlonger be the youngest memory block containing a stack frame. Now when the abstract machine loads a block \ninto the stack cache block from memory we spend its dollar for the load. All blocks with older frames \nhave a dollar on them already by the invariant, so the invariant is maintained. In summary we spend 3 \nblock transfers (worst case) per block that is evicted from the allocation cache. We .nally note that \nthere is no need to explicitly maintain the stack cache block in the abstract machine semantics since \nwe are assuming an ideal cache . Therefore as long as the cache has an extra block available, then the \ncache policy will do at least as well as the one we described. 5. Provable Implementation In this section \nwe describe an implementation of the abstract ma\u00adchine given in Section 4 in the ideal cache model with \nthe same asymptotic cost. The ef.ciency proof for the implementation takes account of two issues that \nare treated abstractly in the evaluation semantics and in the abstract machine. The main issue is how \nto implement the allocation judgment de.ned in Figure 2. Rules 5a and 5b make reference to liveness of \nthe data, and evict a block consisting of the oldest B live objects in the nursery. To ensure that the \npredicted costs are realized in practice we must argue that these conditions can be met by an implementation. \nThe second issue is that we must account for the size of the stored objects (values and frames) that \nmay appear in a computation, and account for the cost of handling these objects in an implementation. \nDe.ne the size of a machine state s0 @ k0 C e0 be the sum of the size of e0 and the size of any function \nin s0. This may be thought of as the size of the program, including any .-abstractions that may be present \nin the initial store. TH E O R EM 5.1. Fix an initial state s0 @ k0 C e0 of size s0, and consider a complete \ncomputation m s0 @ k0 C e0 -. s @ k0 < l with s1 objects in the .nal store, s. This computation be simulated \nin the ideal cache model with cache complexity c\u00d7m for some con\u00adstant c, provided that words are of size \nat least d log(max(s0, s1)) for some d > 0 and that the cache has at least (4M + B) \u00d7 s0 words. (The \nconstants c and d are independent of s0, k0 and e0.) Theorem 5.1 states that the implementation asymptotically \nrealizes the work attributed to the computation by the evaluation semantics, and hence validates the \nalgorithm analysis performed using that semantics. The requirement on the word size in Theorem 5.1 ensures \nthat all objects are addressable by a word-sized pointer, and accounts for the sizes of the objects themselves \nin storage. (A closure can be as large as the initial program.) The requirement on the cache size in \nTheorem 5.1 ensures that we may implement the abstract memory hierarchy with no more than a small constant \nfactor of overhead in a manner that we now describe. (The B \u00d7 s0 additional words account for the stack \ncache described in Section 4; it remains to discuss the implementation of allocation.) The allocation \njudgment de.ned in Figure 2 relies on an assess\u00adment of the live size of the nursery, and on the eviction \nof blocks from the nursery to ensure that the nursery contains no more than M live objects. As we note \nearlier, the liveness of data in the nurs\u00adery may be assessed without reference to the main memory; the \nliveness computation takes place entirely within the cache. Rather than assess liveness, possibly evicting \na block, on each allocation, we instead amortize these costs across multiple allocations accord\u00ading to \nthe following strategy. We reserve 2M \u00d7 s0 words of cache memory for the allocation area to accommodate \nat least 2M ob\u00adjects. Objects are allocated by maintaining a pointer into the nurs\u00adery area, incrementing \nit on each allocation until 2M objects have been allocated, at which point the nursery space is exhausted. \nWhen this occurs, we perform a compacting garbage collection that pre\u00adserves the allocation order of \nobjects, simultaneously evicting as many blocks as necessary to obtain a live size of M objects in the \nnursery. After compaction, allocation continues as before until the nursery is again exhausted. As long \nas there is suf.cient space, allocation takes constant time. When a garbage collection is required, the \ncost may be at\u00adtributed to the allocations of the live data in the nursery, so that in an amortized sense \ngarbage collection is cost-free [3]. It is easy to see that no object is evicted to main memory using \nthis imple\u00admentation that would not have been evicted in the abstract sense. However, the evictions will, \nin general, happen later than predicted by the semantics. As a result, fewer objects may be live at the \ntime of eviction, and so fewer blocks overall may be moved to main memory. As a result of this compression \neffect, two locations that were neighbors in the evaluation semantics may be in two differ\u00adent blocks \nin the implementation. To account for this, two blocks must be loaded to ensure that neighboring objects \nin the semantics are loaded into the read cache together. Thus we require 2M \u00d7 s0 words of cache in the \nideal cache model to account for the M ob\u00adjects in the read cache. With regards to the eviction policy \nfrom the read cache we note that an ideal cache will always choose an optimal policy (furtherst in the \nfuture). It will therefore do at least as well as any policy assumed by the abstract machine. This completes \nthe proof of the implementation bound stated in Theorem 5.1.  6. I/O Ef.cient Algorithms We now describe \nalgorithms analyzed in the model and prove bounds on their cache complexity. In particular we will consider \nmergeSort, k-way mergeSort and a recursive block matrix-matrix multiply. We will show that the k-way \nmergeSort and the matrix\u00admatrix multiply analyzed in our model match the best bounds for the ideal-cache. \nFurthermore the implementations are completely natural functional programs using lists and trees instead \nof arrays. Preliminaries Before describing these algorithms we discuss some general issues and techniques \nthat will be important in the analysis. For simplicity the semantics described in Section 3 only de.nes \nnatural numbers. In the discussion in this section we assume the semantics have been augmented with some \nbasic types including base types that .t in a machine word (machine integers, .oats, and Boolean), sum \ntypes, product types, and recursive types made from products and sums. As with the RAM and I/O models, \nwe assume that for an input of size n that machine words can store O(log n) bits. Hence a machine integer \nwill be bounded by n k for some constant k. Since sorting (and matrix multiply) can be de.ned as higher \norder polymorphic functions we need to be careful about the size of the element type and cache complexity \nof the element function when analyzing overall cache complexity. For this purpose we de.ne the notion \nof a hereditarily .nite (HF) values. At the base any value of basic types that .t in words are HF. Inductively \nany sums or products of HF values are HF. A value of function type is HF iff for every HF argument of \nthe domain type the function yields a HF argument of the range type, using only constant space in the \nprocess. In sorting we assume the elements themselves and the comparison function are hereditarily .nite. \nIn matrix multiply we assume the elements, addition, and multiplication functions are hereditarily .nite. \nIt is important that the data structures that are traversed by our algorithms are laid out in an order \nthat makes accessing them ef.cient. In our model the only way to control the layout of data structures \nis to allocate them in the desired order. We could try to de.ne the notion of a list being in a good \norder in terms of how the list is represented in memory. This is cumbersome and low level. We could also \ntry to de.ne it with respect to the speci.c code that allocated the data. Again this is cumbersome. Instead \nwe de.ne it directly in terms of the cache complexity of traversing the structure. By traversing we mean \ngoing through the structure in a speci.c order and touching (reading) all data in the structure. Since \ntypes such as trees might have many traversal orders, the de.nition is with respect to a particular order \n(e.g. pre-order). For this purpose we de.ne the notion of compact . DE FINI T I ON 6.1. A data structure \nof size n is compact with respect to a given traversal order if traversing it in that order has cache \ncomplexity O(n/B) in our cost semantics with M = kB for some constant k. We can now argue that certain \ncode will generate data structures that are compact with respect to a particular order. To keep data \nstructures compact not only do the top level links need to be accessed in approximately the order they \nwere allocated, but anything that is touched by the algorithm during traversal also needs to be accessed \nin a similar order. For example, if we are sorting a list it is important that in addition to the cons \ncells , the keys are allocated in the list order (or, as we will argue shortly, reverse order is .ne). \nTo ensure that the keys are allocated in the appropriate order they need to be copied whenever placing \nthem in a new list. This copy needs to be a deep copy that copies all components. If the keys are machine \nwords then in practice these might be inlined into the cons cells anyway by a compiler in fact fun traverseL \n[] = [] | traverseL h::R = let val _ = touch h in traverseL R end fun traverseR [] = [] | traverseR h::R \n= let val R = traverseR R val _ = touch h in R end fun map f [] = [] | map f h::R = f(h)::map R fun map \nf [] = [] | map f h::R = let val R = map R in f(h)::R end Figure 4. Traversing a list in two orders, \nand examples of map that use each of the orders. fun mergeSort less [] = [] | mergeSort less [a] = [a] \n| mergeSort less A = let fun merge(A,B) = case (A,B) of ([],B) => B | (A,[]) => A | (Ah::At, Bh::Bt) \n=> if (less(Ah,Bh)) then !Ah::merge(At,B) else !Bh::merge(A,Bt) val (L,H) = split A in merge(mergeSort(L),mergeSort(H)) \nend Figure 5. Code for mergeSort. optimizing compilers such as MLton can even inline product and sum \ntypes. However, to ensure that objects are copied we will use the copy operation described in Section \n3, writing !a to indicate copying of a. Although it might seem that there is only one canonical way to \ntraverse a list, there are actually two. In the .rst all the elements of the list are visited on the \nway down the recursion, and in the second the elements are visited on the way back up. The order in which \nthe elements are visited is reversed. The two versions are illustrated by the code in Figure 4. Fortunately \nif an algorithm is compact for one traversal it is compact for the other. This is because to be compact \nunder either traversal requires that adjacent elements are allocated in the same block (neighborhoods \nin memory), and hence will be ef.cient in both directions. The fact that the orders are effectively equivalent \nis important since it means the model is quite robust relative to programming styles. For example the \ntwo implementations of the map function shown in Figure 4 will both be ef.cient if the list is compact \nwith respect to either traversal. Furthermore the output is compact with respect to either traversal. \nThis is true even though in the .rst case all elements are allocated .rst and then all cons cells, while \nin the second case they are interleaved. Sorting We now consider analyzing sorting in our cost model. \nWe .rst consider mergeSort. We assume a vanilla purely functional version of mergeSort on lists as shown \nin Figure 5. We will not cover the de.nition of split since it is similar to merge. Note the only difference \nfrom a standard mergeSort is the use of the copy (!)  before the Ah and Bh. As discussed earlier this \nis important to ensure the result of the merge is compact. We note that for a list to be compact all \nits elements must be constant size. The interesting aspect of the code is that the cache complexity of \nthis code in our model and hence when mapped onto the ideal cache using the implementation in section \n5 matches the bounds for the array version discussed in Section 2. To analyze the mergeSort we .rst analyze \nthe merge. TH E O RE M 6.2. For a HF function less, and compact lists A and B, the evaluation of (merge \nless A B) starting with any cache e B datatype a pq = Leaf of a list | Node of a * a pq * a pq fun kWayMergeSort \n_ _ [] = [] | kWayMergeSort _ _ [a] = [a] | kWayMergeSort less k L = let fun getMin Leaf [] = NONE | \ngetMin Leaf (a::_) = SOME(a) | getMin Node (a,_,_) = SOME(a) fun join(L,R) = (getMin(L),getMin(R)) of \n(NONE,_) => R state will have cache complexity O n B case (n = |A| + |B|) and will return a compact list \nas a result. | (_,NONE) => L | (la,ra) => Proof. We consider the cache complexity of going down the recur\u00ad \nif less(la,ra) sion and then coming back up. Since A and B are both compact we then Node(la,delMin(L),R) \nneed only put aside a constant number of cache blocks to traverse else Node(lb,L,delMin(R)) each one \n(by de.nition). Recall that in the cost model we have a nursery . that maintains both live allocated \nvalues and place hold\u00aders for stack frames in the order they are created. In merge nothing is allocated \nfrom one recursive call to the next (the cons cells are created on the way back up the recursion) so \nonly the stack frames are placed in the nursery. After M recursive calls the nursery will .ll and blocks \nwill have to be .ushed to the memory \u00b5 (rule 5b in Figure 2). The merge will invoke at most O(n/B) such \n.ushes since only n frames are created. On the way back up the recursion we will generate the cons cells \nfor the list and copy each of the keys (using the !). Note that copying the keys is important so that \nthe result remains compact. The cons cells and copies of the keys will be interleaved in the allocation \norder in the nursery and .ushed to memory once the nursery .lls. Once again these will be .ushed in blocks \nof size B and hence there will be at most O(n/B) such .ushes. Furthermore the resulting list will be \ncompact since adja\u00adcent elements of the list will be in the same block (neighborhood). D We now consider \nmergeSort as a whole. TH E O RE M 6.3. For a HF function less, and compact list A, the evaluation of \n(mergeSort less A) starting with any cache state e B and delMin (Leaf (_::R)) = Leaf(R) | delMin (Node \n(_,L,R)) = join(L,R) fun merge H = case getMin(H) of NONE => [] | SOME(a) => let val r = merge(delMin(H)) \nin !a::r end fun buildPQ [a] = Leaf(a) buildPQ A = let val (L,H) = partition 2 A in join(buildPQ(L),buildPQ(R)) \nend val LL = partition k L val SL = map (kWayMergeSort less k) LL val HL = buildPQ SL in merge HL end \nFigure 6. K-way Merge Sort. will have cache complexity O a compact list as a result. n B log n M (n = \n|A|) and will return Proof. As with the array version, we consider the two cases when the input .ts in \ncache and when it does not. The mergeSort routine never requires more than O(n) live allocated data. \nTherefore when kn = M for some (small) constant k all allocated data .ts in the nursery. Furthermore \nsince the input list is compact, for k ' n = M the input .ts in the read cache (for some constant k '). \nTherefore the cache complexity for mergeSort is at most the time to .ush O(n) items out of the allocation \ncache that it might have contained at the start, and to load the read cache with the input. This cache \ncomplexity is bounded by O(n/B). When the input does not .t two children are joined, which recursively \npulls the value from the child with the smaller root. We don t show the code for partition, which simply \npartitions a list into k equal length parts (within 1). Although the code is somewhat involved the analysis \nof the cache complexity is relatively simple since most of the data allocated for the tree-based priority \nqueue becomes unreachable before it needs to be .ushed to memory. TH E O R EM 6.4. For a HF function \nless, and compact list A, the evaluation of (kWayMergeSort less k A) starting with any cache state and \nwith an appropriate k ( M/B) will have cache u s in cache we have to pay for the merge as analyzed above \nplus complexity O n B logM/B n B (n = |A|) and will return a com\u00ad the recursive calls. This gives the \nsame recurrence as for the array pact list as a result. version (Section 2, equation 1) and hence solves \nto the claimed result. D Proof. As usual once the input size is less than n/c for some constant, the \nwhole problem .ts in cache and we just pay to load We now consider a k-way mergeSort using lists and \na tree based the input and write the output, which will have O(n/B) cache heap for the k-way merging. \nThe code is shown in Figure 6. The complexity if the input and output are compact. When the problem 1 \n sort partitions the list into k parts, sorts each part recursively, builds size does not .t in cache \nwe note that with k M/B for some = c a priority queue (PQ) out of the resulting parts, and pulls keys \none constant c we can .t the head of each recursively solved list in the by one out of the PQ adding \nthem to the output list. The only read cache, assuming each is compact. Therefore traversing all lists \nslightly tricky part is maintaining the priority queue. The idea is will use O(n/B) cache complexity. \nFurthermore the size of the each of the sorted lists is placed at a leaf. When pulling elements priority \nqueue is proportional to k so the live part easily .ts within from the root of the PQ, the value is removed \nfrom the root and the the allocation cache. We have to be careful, however, since the datatype a M = \nLeaf of a | Node of a M * a M * a M * a M fun mmult + * (Leaf a) (Leaf b) = Leaf(*(a,b)) | mmult + * \n(Node(a1,a2,a3,a4)) (Node(b1,b2,b3,b4)) = let fun madd (Leaf a) (Leaf b) = Leaf(+(a,b)) | madd (Node(a1,a2,a3,a4)) \n(Node(b1,b2,b3,b4)) Node(madd(a1,b1),madd(a2,b2), madd(a3,b3),madd(a4,b4)) val mm = mmult + * in Node(madd(mm(a1,b1),mm(a2,b3)), \nmadd(mm(a1,b2),mm(a2,b4)), madd(mm(a3,b1),mm(a4,b3)), madd(mm(a3,b2),mm(a4,b4))) end Figure 7. Matrix \nMultiply. allocation cache is shared with the stack frames, which could eject some of the data allocated \nby the PQ. But since at any given time much less than half of the allocation cache (only k = O(M/B) of \nit) is used by the PQ, we can charge all such ejections against the ejected cache frames (we charge for \nevery cache frame). We can also charge reading them back into read cache against the cache frames. Going \ndown the recursion of the merge therefore requires O(n/B) cache complexity to account for loading the \nk recursively solved lists, and ejecting the n cache frames. Coming back up the recursion again requires \nO(n/B) cache complexity for ejecting the list and the copied keys. The resulting list is compact for \nlist traversal since it is allocated in list traversal order (tail of the list .rst). This gives us the \nrecurrence in equation 2 from Section 2 which solves to the desired result. D Matrix Multiply Our .nal \nexample is matrix multiply. The code is shown in .gure 7 (we have left out checks for matching sizes). \nThis is a block recur\u00adsive matrix multiply with the matrix laid out in a tree. It is therefore an interesting \nexample of a tree data structure. We de.ne compact\u00adness with respect to a preorder traversal of this \ntree. We therefore say the matrix is compact if traversing in this order can be done with cache complexity \nO(n 2/B) for an n \u00d7 n matrix (n 2 leaves). We note that if we generate a matrix in a preorder traversal \nallocat\u00ading the leaves along the way, the resulting array will be compact. TH E O RE M 6.5. For HF functions \n* and +, and compact n \u00d7 n matrices A and B, the evaluation of (mmult + * A B) starting u s with any \ncache state will have cache complexity O nv 3 and B M will return a compact matrix as a result. Proof. \nMatrix addition has cache complexity O(n 2/B) and gen\u00aderates a compact result since we traverse the two \ninput matrices in preorder traversal and we generate the output in the same order. Since the live data \nis never larger than O(n 2) the problem will .t in cache for n 2 = M/c for some constant c. Once it .ts \nin cache the cost is O(n 2/B) needed to the load the input matrices and write out the result. When it \ndoes not .t in cache we have to do 8 recursive calls and four calls to matrix addition. This gives the \nrecurrence. 2 8Q( n ) + O( n 2 ) n > M/c Q(n) = 2 2 B O( n B ) otherwise u s This solves to O nv 3 . \nThe output is compact since each of B M the four calls to madd in mmult allocate new results in preorder \nwith respect to the submatrices they generate, and the four calls are made in preorder. Therefore the \noverall matrix returned is allocated in preorder. D 7. Conclusion The idea of distinguishing the abstract \ncost semantics of language from its concrete implementation originates with Blelloch and Greiner s work \non parallel programming [5, 11]. The chief ben\u00ade.t of their approach is that it provides a useful abstraction \nto the programmer that accounts for the complexity of a program, while simultaneously providing a guide \nto the implementor for how to achieve the complexity bound (with stated overhead). This work extends \nthat methodology to account for the I/O complexity of a program in terms of two parameters, the cache \nblock size (mea\u00adsured in objects) and the number of cache blocks. The programmer reasons at the level \nof the evaluation semantics, the implementor makes use of the provable implementation strategy to realize \nthe predicted complexity. In the present case the essence of the proof is to argue that conventional \nimplementation techniques, which rely on a run-time control stack and copying garbage collection, can \nbe deployed to meet the abstract bounds given by the semantics of a functional language. The separation \nbetween the semantics and its implementations allows the programmer to work at the level of the code \nitself, and avoids having to reason in terms of the details of the compiler and run-time system (or, \neven worse, to be forced to drop down to a C-like level in which the programmer explicit manages storage \nallocation for each application). Using the approach we are able to express algorithms in a standard \nhigh-level functional style using recursive data types (lists and trees), analyze them using a model \nthat captures the idea of a .xed size read and allocation stack, but no details of the run time system, \nand yet match the asymptotic bounds for the ideal cache achieved by designing them using arrays and explicit \nand careful memory management in the imperative setting. For sorting the bounds are optimal. One direction \nfor further research is to integrate (deterministic) parallelism with the present work. Based on previous \nwork we ex\u00adpect that the evaluation semantics given here will provide a good foundation for specifying \nparallel as well as sequential complex\u00adity. One complication is that the explicit consideration of storage \nconsiderations in the cost model given here would have to take ac\u00adcount of the interaction among parallel \nthreads. The amortization arguments would also have to be reconsidered to account for paral\u00adlelism. Another \ndirection is suggested by the special treatment of the run-time stack described in Section 4. The stack \nis, after all, a particular data structure that is used implicitly by each program. This use could be \nmade explicit, in which case it would be useful to understand more generally what properties of it allow \nfor its ef.cient (in terms of cache complexity) implementation. These might well generalize to other \ndata structures, and we it may be useful to develop a type system to capture these special properties. \nFinally, although we are able to generate an optimal cache\u00adaware sorting algorithm it is unclear whether \nit is possible to gen\u00aderate an optimal cache-oblivious sorting algorithm in our model. Acknowledgments. \nThis work is partially supported by the Na\u00adtional Science Foundation under grant number CCF-1018188, \nand by Intel Labs Academic Research Of.ce for the Parallel Algorithms for Non-Numeric Computing Program. \n References [1] J. Abello, A. L. Buchsbaum, and J. Westbrook. A functional approach to external graph \nalgorithms. Algorithmica, 32(3):437 458, 2002. [2] A. Aggarwal and J. S. Vitter. The input/output complexity \nof sorting and related problems. Commun. ACM, 31(9):1116 1127, 1988. [3] A. W. Appel. Garbage collection \ncan be faster than stack allocation. Inf. Process. Lett., 25(4):275 279, 1987. [4] L. Arge, M. A. Bender, \nE. D. Demaine, C. E. Leiserson, and K. Mehlhorn, editors. Cache-Oblivious and Cache-Aware Algorithms, \n18.07. -23.07.2004, volume 04301 of Dagstuhl Seminar Proceedings, 2005. IBFI, Schloss Dagstuhl, Germany. \n[5] G. E. Blelloch and J. Greiner. Parallelism in sequential functional languages. In FPCA, pages 226 \n237, 1995. [6] Y.-J. Chiang, M. T. Goodrich, E. F. Grove, R. Tamassia, D. E. Ven\u00adgroff, and J. S. Vitter. \nExternal-memory graph algorithms. In K. L. Clarkson, editor, SODA, pages 139 149. ACM/SIAM, 1995. ISBN \n0-89871-349-8. [7] T. M. Chilimbi and J. R. Larus. Using generational garbage collection to implement \ncache-conscious data placement. In S. L. P. Jones and R. E. Jones, editors, ISMM, pages 37 48. ACM, 1998. \nISBN 1-58113\u00ad114-3. [8] R. Courts. Improving locality of reference in a garbage-collecting memory management \nsystem. Commun. ACM, 31(9):1128 1138, 1988. [9] M. Frigo, C. E. Leiserson, H. Prokop, and S. Ramachandran. \nCache\u00adoblivious algorithms. In FOCS, pages 285 298. IEEE Computer Society, 1999. [10] M. T. Goodrich, \nJ.-J. Tsay, D. E. Vengroff, and J. S. Vitter. External\u00admemory computational geometry (preliminary version). \nIn FOCS, pages 714 723. IEEE Computer Society, 1993. [11] J. Greiner and G. E. Blelloch. A provably time-ef.cient \nparallel implementation of full speculation. ACM Trans. Program. Lang. Syst., 21(2):240 285, 1999. [12] \nD. Grunwald, B. G. Zorn, and R. Henderson. Improving the cache locality of memory allocation. In R. Cartwright, \neditor, PLDI, pages 177 186. ACM, 1993. ISBN 0-89791-598-4. [13] R. Harper. Practical Foundations for \nProgramming Languages. Cam\u00adbridge University Press, 2013. (Draft available at http://www.cs. cmu.edu/~rwh/plbook/book.pdf.). \n[14] R. Jones and R. Lins. Garbage Collection: Algorithms for Automatic Dynamic Memory Management. Wiley, \n1996. [15] U. Meyer, P. Sanders, and J. F. Sibeyn, editors. Algorithms for Memory Hierarchies, Advanced \nLectures [Dagstuhl Research Seminar, March 10-14, 2002], volume 2625 of Lecture Notes in Computer Science, \n2003. Springer. ISBN 3-540-00883-7. [16] J. G. Morrisett, M. Felleisen, and R. Harper. Abstract models \nof memory management. In FPCA, pages 66 77, 1995. [17] K. Munagala and A. G. Ranade. I/o-complexity of \ngraph algo\u00adrithms. In R. E. Tarjan and T. Warnow, editors, SODA, pages 687 694. ACM/SIAM, 1999. ISBN \n0-89871-434-6. [18] G. D. Plotkin. LCF considered as a programming language. Theor. Comput. Sci., 5(3):223 \n255, 1977. [19] M. Rahn, P. Sanders, and J. Singler. Scalable distributed-memory external sorting. In \nF. Li, M. M. Moro, S. Ghandeharizadeh, J. R. Haritsa, G. Weikum, M. J. Carey, F. Casati, E. Y. Chang, \nI. Manolescu, S. Mehrotra, U. Dayal, and V. J. Tsotras, editors, ICDE, pages 685 688. IEEE, 2010. ISBN \n978-1-4244-5444-0. [20] D. D. Sleator and R. E. Tarjan. Amortized ef.ciency of list update and paging \nrules. Commun. ACM, 28(2):202 208, 1985. [21] D. Spoonhower, G. E. Blelloch, R. Harper, and P. B. Gibbons. \nSpace pro.ling for parallel functional programs. In J. Hook and P. Thiemann, editors, ICFP, pages 253 \n264. ACM, 2008. ISBN 978-1-59593-919\u00ad 7. [22] J. S. Vitter. Algorithms and data structures for external \nmemory. Foundations and Trends in Theoretical Computer Science, 2(4):305 474, 2006. [23] P. R. Wilson, \nM. S. Lam, and T. G. Moher. Caching considerations for generational garbage collection. In LISP and Functional \nProgram\u00adming, pages 32 42, 1992.  \n\t\t\t", "proc_id": "2429069", "abstract": "<p>The widely studied I/O and ideal-cache models were developed to account for the large difference in costs to access memory at different levels of the memory hierarchy. Both models are based on a two level memory hierarchy with a fixed size primary memory(cache) of size M, an unbounded secondary memory organized in blocks of size B. The cost measure is based purely on the number of block transfers between the primary and secondary memory. All other operations are free. Many algorithms have been analyzed in these models and indeed these models predict the relative performance of algorithms much more accurately than the standard RAM model. The models, however, require specifying algorithms at a very low level requiring the user to carefully lay out their data in arrays in memory and manage their own memory allocation.</p> <p>In this paper we present a cost model for analyzing the memory efficiency of algorithms expressed in a simple functional language. We show how some algorithms written in standard forms using just lists and trees (no arrays) and requiring no explicit memory layout or memory management are efficient in the model. We then describe an implementation of the language and show provable bounds for mapping the cost in our model to the cost in the ideal-cache model. These bound imply that purely functional programs based on lists and trees with no special attention to any details of memory layout can be as asymptotically as efficient as the carefully designed imperative I/O efficient algorithms. For example we describe an O(n_B logM/Bn_B)cost sorting algorithm, which is optimal in the ideal cache and I/O models.</p>", "authors": [{"name": "Guy E. Blelloch", "author_profile_id": "81100282539", "affiliation": "Carnegie Mellon University, Pittsburgh, PA, USA", "person_id": "P3977913", "email_address": "guyb@cs.cmu.edu", "orcid_id": ""}, {"name": "Robert Harber", "author_profile_id": "81553124756", "affiliation": "Carnegie Mellon University, Pittsburgh, PA, USA", "person_id": "P3977914", "email_address": "rwh@cs.cmu.edu", "orcid_id": ""}], "doi_number": "10.1145/2429069.2429077", "year": "2013", "article_id": "2429077", "conference": "POPL", "title": "Cache and I/O efficent functional algorithms", "url": "http://dl.acm.org/citation.cfm?id=2429077"}