{"article_publication_date": "01-23-2013", "fulltext": "\n Plan B: A Buffered Memory Model for Java Delphine Demange2 Vincent Laporte2,1 Lei Zhao1 Suresh Jagannathan1 \nDavid Pichardie3,1 Jan Vitek1 1Purdue University 2ENS Cachan Bretagne -IRISA 3INRIA Rennes Abstract Recent \nadvances in veri.cation have made it possible to envision trusted implementations of real-world languages. \nJava with its type-safety and fully speci.ed semantics would appear to be an ideal candidate; yet, the \ncomplexity of the translation steps used in production virtual machines have made it a challenging target \nfor verifying compiler technology. One of Java s key innovations, its memory model, poses signi.cant \nobstacles to such an endeavor. The Java Memory Model is an ambitious attempt at specifying the behavior \nof multithreaded programs in a portable, hardware agnostic, way. While experts have an intuitive grasp \nof the properties that the model should enjoy, the speci.cation is complex and not well-suited for integration \nwithin a verifying compiler infrastructure. Moreover, the speci.cation is given in an axiomatic style \nthat is distant from the intuitive reordering-based reasonings traditionally used to justify or rule \nout behaviors, and ill suited to the kind of operational reasoning one would expect to employ in a compiler. \nThis paper takes a step back, and introduces a Buffered Memory Model (BMM) for Java. We choose a pragmatic \npoint in the design space sacri.cing generality in favor of a model that is fully characterized in terms \nof the reorderings it allows, amenable to formal reasoning, and which can be ef.ciently applied to a \nspeci.c hardware family, namely x86 multiprocessors. Although the BMM restricts the reorderings compilers \nare allowed to perform, it serves as the key enabling device to achieving a veri.cation pathway from \nbytecode to machine instructions. Despite its restrictions, we show that it is backwards compatible with \nthe Java Memory Model and that it does not cripple performance on TSO architectures. Categories and Subject \nDescriptors D.3.1 [Programming Lan\u00adguages]: Formal De.nitions and Theory; F.3.1 [Specifying and Verifying \nand Reasoning about Programs] Keywords Concurrency; Java; Memory Model; Veri.ed Compila\u00adtion 1. Introduction \nFormally veri.ed systems are becoming a reality [18, 20]. Recent successes have shown that it is possible \nto construct reasonably ef.cient compilers along with a machine-checked proof of correct\u00adness. Building \non our experience with the Fiji real-time virtual machine [26] and veri.ed compilation for relaxed memory \narchi\u00adtectures (CompcertTSO) [35], we have embarked on a project to produce a veri.ed platform for a \nvariant of the Java language that abides by the Safety Critical Speci.cation for Java [12]. To build \na verifying compiler requires starting from a formal semantics of the source language and, simultaneously, \ndeveloping and proving the Permission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies are not made or distributed for \npro.t or commercial advantage and that copies bear this notice and the full citation on the .rst page. \nTo copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci.c \npermission and/or a fee. POPL 13, January 23 25, 2013, Rome, Italy. Copyright &#38;#169; 2013 ACM 978-1-4503-1832-7/13/01. \n. . $10.00 x.0 y.0 z.0 r1.z r2.x if (r1 = 1) {x.1; y.1} r3.y else {y.1; x.1} if (r2 = 1 &#38; r3 = \n1) z.1 Figure 1. JMM. x, y and z are shared memory variables, ri are local. In the JMM, no execution \nyields r1 = r2 = r3 = 1. However, manually reordering the writes to x and y so that they are performed \nin the same order in both branches allows the compiler to eliminate the conditional and hoist the assignments \nabove the store to r1, thus making such an execution permissible. Alternative de.nitions of the JMM [5] \nallow this execution, but the justi.cation is complex, involving subtle notions of speculations and non-local \nreasoning over execution traces. correctness of optimizations and transformations, with an associated \nformal operational semantics developed for each of the compiler s intermediate representations all the \nway through to the backend target. Although we expected Java to be a challenge because of the complexity \nof the transformations used in high-performance virtual machines like Fiji, the Java memory model [22] \nalso complicates the task of devising a tractable formal semantics for the source and intermediate representations. \nThe Java Memory Model (JMM) was designed to specify the behavior of concurrent programs across all modern \narchitectures and compiler optimizations currently in use in production virtual machines. The complexity \nunderlying the JMM stems from the need to give a semantics to all programs, even racy ones, and the desire \nto be maximally portable. This degree of generality comes at the price of complexity. When explaining \nthe semantics of racy programs, textbooks [11] elide key JMM notions such as the de.nition of a legal \nexecution. This issue remains the subject of much active research [4, 5, 10, 15, 21]. Consider Fig. 1 \nwhich shows a program where, according to [22], no execution can yield r1 = r2 = r3 = 1. But, if the \nprogrammer were to manually reorder the instructions in the else-branch, a seemingly insigni.cant refactoring, \nthis result would be allowed. Such unintuitive behavior is unfortunate. From the verifying compiler writer \ns point of view, the situation is not much better it is an open problem to write correctness proofs \nfor a compiler with respect to the JMM s current de.nition, especially if the target platform enforces \nits own relaxed (or weak) memory semantics. Indeed, even in the absence of formal proofs, existing Java \ncompilers are not JMM compliant [34]. (And in the process of writing this paper we discovered that our \nown virtual machine was not JMM compliant.) Often the JMM is informally discussed in terms of allowed \ninstruction reorderings combined with sequentially consistent (SC) interleavings. To illustrate, assume \nfor a moment that reordering of independent, non-volatile, statements was allowed, then we could admit \nr1 = r2 = 1 as the value of the program on the left. x.0 r1.x y.1 y.0 r2.y x.1 reordering with- ------. \nspeculative loads x.0 y.1 r1.x y.0 x.1 r2.y This would be true because after reordering, the result \nis an SC execution. With this approach, reasoning would be easy determin\u00ading the validity of an execution \nwould boil down to considering combinations of permitted reorderings. Unfortunately, the JMM reorderings \nare complex, and reasoning about transformations is rarely intuitive. To rule an execution illegal requires \nshowing that it is not an SC execution if the program is data-race free, or that it includes an out-of-thin-air \nread. Formalizing these notions leads to a complex de.nition involving race-committing sequences. This \nis a level of complexity that is challenging for most programmers to tackle. In [5], four test cases \nfrom Pugh et al. [28] were found to be .awed, but a later paper [37], using an automatic veri.cation \ntool, contradicted this interpretation on two of the examples. Such a lack of clarity for programs less \nthan 10 instructions long does not bode well for the viability of the model.  The complexity of the \nJMM arises in large part by its desire to be portable across all platforms and thus hardware agnostic. \nBut, this is a challenging goal given the vastly different relaxed memory subsystems found on modern \narchitectures, and the range of optimizations modern compilers perform, many of which are often ill-understood. \nFor example, operational de.nitions of the Power architecture [31] are substantially different from those \nde.ned for x86 [36]; the former supports out-of-order unbounded speculative executions and subtle notions \nof partial coherence, while the latter is expressed in terms of more intuitive store buffers and limited \nmemory reorderings. It is unclear how one might de.ne a tractable formal language memory model that can \nbe effectively tailored to both. Neither is there a clear speci.cation of the optimizations that ought \nto be supported. Of course, we could greatly simplify the problem by drastically limiting the behaviors \nadmitted by Java with an easily understood albeit more restrictive model such as sequential consistency. \nValid program executions would be limited to those that can be expressed purely in terms of a sequential \ninterleaving of thread actions; but under SC, reordering of shared-memory reads and writes is prohib\u00adited. \nWhile simple to state and easy to understand, we choose to not follow this approach because SC would \nlikely cripple perfor\u00admance of Java programs on all modern microprocessors due to the large number of \nfences needed to enforce SC on relaxed hardware; this would be especially true for racy programs implementing \nso\u00adphisticated lock-free algorithms of the kind found, for example, in optimized libraries such as java.util.concurrent. \nInstead, we propose an alternative memory model that has a tractable semantics and designed to be mapped \nto x86 multiproces\u00adsors that support a Total Store Ordering relaxed memory seman\u00adtics [30, 36]. We introduce \nthe Buffered Memory Model (BMM), a memory model for Java that can be fully characterized in terms of \nthe memory reorderings it allows on top of sequentially con\u00adsistent executions. The BMM comes in two \nforms. The .rst is an axiomatic speci.cation that can be used to relate it to the JMM, and which provides \nan intuitive method to describe valid and invalid program executions. The second form is a fully operational \nabstract machine with write buffers attached to each thread. This seman\u00adtics can be readily used by a \nverifying compiler infrastructure like CompcertTSO. BMM is backwards compatible: existing software that \nhas been written against the JMM can be run directly since the JMM is a superset of the BMM (i.e., every \nlegal BMM execution is legal under the JMM). This ensures that legacy software that has been validated \nand tested with the JMM in mind will remain correct. The contributions of this paper are thus as follows: \n An axiomatic de.nition of the BMM, an alternative memory model for Java programs, fully characterized \nin term of memory event reorderings, and a compliance proof with the JMM.  A formalization of BMMo, \nan operational de.nition of the semantics of Java programs, equipped with a simple mapping to the Total \nStore Order (TSO) memory model enabling it easily be used with the x86 family of multiprocessor architectures \n and with veri.ed compilers like CompcertTSO targeting such platforms. A proof that BMM and BMMo are \nequivalent. We prove the data race freedom (DRF) theorem for BMM and the legality of reordering optimizations \nunder the BMM.  An upper bound on the cost of preserving BMM semantics on a production virtual machine \nrunning on a TSO hardware, benchmark results measuring the cost and overhead of BMM enforcement, compared \nto JMM-compliant implementations, and an illustration of the subtle performance implications of the memory \nmodel with a case study on the implementation of biased locking.  We note that the design rationale \nfor the BMM is very much dictated by our overarching goal of constructing a verifying compiler for Java. \nAs a result, BMM is not intended to be general-purpose, and does not apply directly to all architectures \nand compiler optimizations. Instead, we view it as a starting point for further research on veri.ed compilation \nand memory models. The BMM, the statements of results and key lemmas are ex\u00adpressed in Coq; our proofs \nare rigorous but non-mechanised. The details are available online, along with the version of Fiji and \nLLVM used in our benchmarks: http://r.cs.purdue.edu/bmm 2. Related Work Language Memory Models. Languages \nlike Java and C++ have sophisticated memory models to answer questions related to data visibility and \nupdates for concurrent programs. The JMM does this using committing-sequences, that make it subtle, complex, \nand unsound [10, 34]. Huisman and Petri [15] proved its DRF guarantee. They tackled the inconsistencies \nof [22], related to memory initialization by adding hypotheses. Aspinall and Sev .c.\u00b4ik [5] proposed \nan alternative de.nition of the JMM that does not suffer from these issues. They restrict their de.nition \nto the .nite case, as we do here. Recently, Lochbihler [21] extended the formalization by including in.nite \nexecutions and dynamic allocations. This work proves type-safety for correctly synchronized programs. \nSuch a mechanized proof is a tour de force since it covers a large fragment of Java. However, the proof \nstructure is quite different from the simulation proof we need to perform within the scope of a veri.ed \noptimizing compiler. There has also been recent work on memory models for C++. Boehm and Adve [6] provide \na semantics for data\u00adrace free C++ programs, including a semantics for low-level atomics. In Java, it \nis mandatory to also give well-de.ned semantics to racy programs to avoid security breaches. Weak Memory \nModel Formalizations. Work in this area has fo\u00adcussed primarily on characterizing hardware memory models. \nEarly studies [1, 2, 13] outlined a range of hardware memory models, and attempted to rigorously formalize \nthe vendor s documentations. Alglave et al. [3] de.ned a general framework for formalizing hard\u00adware \nmodels using partial orders. Operational characterizations have been examined in [25, 30, 31]. Burckhardt \net al. [9] de.ne an ex\u00adpressive denotational framework where a memory model is a set of dynamic reorderings, \naggregation or splitting rewriting rules. TSO boils down to a store-load reordering and a store-load \naggregation rule. The BMM follows this line of work by providing provably equivalent axiomatic and operational \nmodels that make it intuitive and suitable to veri.cation. Proofs, Veri.ed Compilation, and Weak Memory \nModels. De.n\u00ading multithreaded semantics in terms of reordering is also the ap\u00adproach taken by Min\u00b4 e \n[24]: the semantics of multithreaded C pro\u00adgrams is de.ned as the interleavings of the programs possibly \nob\u00adtained by the syntactic transformations de.ning the memory model.  From a certi.ed compiler perspective, \nan operational de.nition of the JMM is desirable. There are several attempts to provide such a semantics \n[7, 8, 10, 16] but none of them has been used in a proof assistant. Our operational semantics is inspired \nby the TSO memory model proposed in [35] which has been formalized in Coq. . Sev c.\u00b4ik [32] identi.es \nsome trace transformations that are valid under the JMM. Transformations are however de.ned semantically. \nThis gap is .lled in [33] where the program transformations are proved to be correct, but this is done \nunder a DRF assumption. [33] also identi.es the need for characterizing memory models in terms of the \ntransformations they permit, and this is the goal of the axiomatic formalization of BMM. 3. BMM: Two \nModels for the Price of One While the BMM covers the whole of Java, the most challenging parts of reasoning \nabout concurrent programs are those that manipulate shared memory in a potentially racy way. Following \nthe JMM s design, we separate operations that are purely thread-local from those that deal with shared \nmemory. The BMM is parametrized by an abstract notion of intra-thread semantics to deal with the former. \nWe mention a few of the high-level concepts and our design choices before focusing on the memory model \nproper. (More details about this part of our model is in the online Coq development.) Java Values. The \nJVM operates on two kinds of values: primitives and references. Primitives include long and double values \nthat are 64-bits wide. Updates to these variables is not guaranteed to be atomic on a 32-bit machine. \nOur intra-thread semantics captures this. The treatment of references is delicate. Several instruction \nlike instanceof, checkcast, invokevirtual require reading the class of an object pointed to by a reference. \nThese operations are not relevant to the memory model because they refer to immutable meta-data. We explicitly \navoid reading from shared memory in those cases since that would directly entail reasoning about the \nmemory model. Instead, our semantics attaches type information to references in the spirit of formalizations \nof the bytecode veri.er. As a consequence, the type of an object can be conceptually read without going \nto shared memory. We also give to each thread its own allocation pool. This matches the notion of .eld \ninitialization that is advocated in the JMM. We do not need to initialize objects with a default value \nwhen we allocate them because each memory address has a well-typed initial value from the very beginning \nof the program execution. Final Fields. The JMM provides complex rules for .elds anno\u00adtated final. Compilers \nare allowed to perform aggressive optimiza\u00adtions on reads. The surprising thing is that final .elds may \nchange between reads. This occurs if the programmer leaks an object during its construction. The result \nis a very non-intuitive semantics which stands more as a warning for the programmer who does not follow \na safe publication pattern than a real semantics. Our position is to forbid unsafe publications [14] \n(which happens to be a restriction imposed by Safety Critical Java as well). This allows us to keep an \nintuitive semantics for final .elds and let compilers aggressively optimize them. Class Initialization. \nLazy class loading is an important mecha\u00adnism for concurrent programming; see for example the initialization \non demand holder idiom [27]. Our semantic supports this by tracking the current initialization status \nof each class. Synchronization Actions. Their treatment in the paper is slightly simpli.ed. Our formal \nsemantics needs to handle more synchroniza\u00adtion actions than those explained here. For example, when \nspawn\u00ading a thread, an exception IllegalThreadStateException can occur if the thread has already been \nstarted. It means that, for a given execution and a given thread, there can be only one successful spawn \nevent but several other failed spawn events. A so-called JMM synchronizes-with edge must exist between \nthe successful spawn and all failed spawn events. 3.1 A Reordering-based Memory Model We propose a memory \nmodel that can be fully characterized by reorderings over SC executions and that has a simple speci.cation. \nIn the BMM, (1) all SC executions are allowed and, (2) any execution from which an SC execution can be \nderived by reordering a non-volatile read action with a preceeding non-volatile write action is allowed; \nboth actions must operate over disjoint memory locations and the actions can be separated by a sequence \nof reads that see the write action. Thus, for instance in the following program x.0 y.0 x.1 y.1 r1.y \nr2.x r1 = r2 = 0 is allowed it suf.ces to reorder one of the threads for the result to be permissible \nunder an SC execution. Fig. 2 gives some prohibited executions. Thanks to this reordering-based de.nition, \nall three examples are easily seen to be illegal since no reordering is applicable and no SC execution \ncan exhibit such behaviors. x.0 y.0 x.0 y.0 x.1 r1.y y.1 r2.x x.1 r1.x r2.y y.1 r3.x r1 = 1, r2 = 0 is \nillegal r1 = r2 = 1, r3 = 0 is illegal x.0 y.0 Figure 2. BMM executions. 3.2 A Buffered Operational \nMemory Model While more intuitive for the programmer, the reordering-based de.nition of BMM lacks an \noperational form. What we seek with an operational model is a companion proof technique to provide a \nformal correspondence proof between the different layers of a veri.ed compiler. Operational semantics \nhave been long advocated as a vehicle within which to conduct such proofs by facilitating the use of \nsimulation diagrams. Thanks to their inductive form, they can be used to reason about global program \nbehavior in terms of elementary single steps. Axiomatic models generally do not enjoy the same kind of \nproof technique. In [34], to prove validity of some program transformations, the authors had to reason \non whole pre.xes of traces. This uncomfortable situation comes from the fact that in the JMM, a trace \nof n + 1 execution steps is not easily de.ned in term of its n .rst steps. At the hardware level, operational \nsemantics have been provided in [31, 36]. It is not surprising to see that no proof connects the JMM \nwith any of these operational models. Thus, our second memory model, called BMMo, introduces a store \nbuffer: each hardware thread effectively has a FIFO buffer of pending memory writes, so that reads performed \non different processors can occur before writes have propagated to main memory.  3.3 Reconciling the \nTwo Models While the axiomatic form of BMM has an intuitive de.nition and supports a subset of the executions \npermitted by the JMM, its op\u00aderational variant has been speci.ed to .t cleanly within the store  SC \nC++ JMM BMM DRF theorem / / / / Reordering memory accesses \u00d7 / / . Redundant memory accesses elimination/introduction \n/ / . . Operational semantics / / \u00d7 / Semantics for all programs / \u00d7 / / Programmer can understand the \nsemantics of racy programs / \u00d7 \u00d7 / Sound with respect to JMM (does not break legacy Java) / \u00d7 / / Lock \noptimizations / / . . Table 1. Expressivity and properties of memory models. / if a property holds, \n\u00d7 if it does not, and . if there are restrictions. The models differ in the reordering they permit, how \nthey are formalized, the programs they consider, and their support for legacy code. BMM is weaker than \nthe JMM in terms of allowed reorderings, but its operational semantics is useful for verifying compiler \noptimizations, and its simpler axiomatic version is easier for programmers to understand. Reordering \nmemory accesses is illegal under original JMM [10, 22] but legal under the alternative version of [34]. \nbuffering operations provided by x86-based relaxed memory hard\u00adware. The two models are reconciled with \nan equivalence proof. For this purpose, the BMM reorderings have been carefully cho\u00adsen to align with \nthe behaviors permitted under BMMo. These two semantics have different uses. In Sec. 5.1, we use the \nreordering view to prove the JMM compatibility. In Sec. 8 we use the opera\u00adtional semantics to study \nthe validity of compiler-driven program transformations. Tab. 1 gives an overview of the properties of \nthe BMM. 4. Background on the Java Memory Model We introduce key notions from the JMM. A language memory \nmodel formally speci.es what values can be read by each thread depending upon the writes performed by \nthis thread or others. These interactions are categorized using inter-thread actions. Inter-thread Actions. \nThe shared memory of a program is split into a set of disjoint addresses which are instance .elds, static \n.elds or array positions but not local variables. For each address x . X, we can determine if it is volatile \nor not with the function volatile : X . bool. In the literature, external actions are distinguished from \nother memory actions. In this work, we model external actions using volatile writes 1, that can be identi.ed \nwith the function external : X . bool. 2 We assume a set T of threads, a set L of locks, and a set V \nof values. The set of inter-thread actions is given below, where superscript i denotes the unique identi.er \nof memory actions. A ::= wt i x, v (thread t writes value v to address x) | rt i x (thread t reads from \naddress x) | tl (thread t acquires a lock on monitor l) li | u i tl (thread t releases a lock on monitor \nl) | stt' (thread t creates a new thread t') | bt (thread t starts) | jt it' (thread t detects t' has \nterminated) | et (thread t ends) | w0x (default write action to address x) ' x . X v . V l . L t, t. \nT i . N 1 E.g., a call by a thread t to a function f with arguments args that returns value v is modeled \nas a volatile write to the abstract location f(args). 2 We require that, .x, external(x) . volatile(x) \nAction w0x initializes address x; it has no emitting thread.3 Thread start (bt) and end (et) can happen \nonly once and thus do not require identi.ers. For any action a that is not a default write action, we \nwrite T (a) the emitting thread of this action. For any write action w, we write V (w) the value written \nby that action; initialization actions write a default value according to the type of the related address. \nWe use some notations for sets of actions: Ar = {rt i x | t . T, x . X} (reads) Aw = {wt i x, v; w0x \n| t . T, x . X, v . V} (writes) Ad = {w0x | x . X} (initializations) Ab = {bt | t . T} (begins) As = \n{wt i x, v; rt i x | t . T, x . X, volatile(x)} . {li tl; ut il | t . T, l . L} (synchronizations) '' \n' . {stt; bt; jt i t; et | t, t. T} Ax = {wt i x, v | t . T, external(x)} (external actions) The JMM \nis based on a happens-before model [19]. An execution is described in terms of partial orders between \nmemory actions. The same external behavior may be associated with many different interleavings of thread \nactions. An interleaving can be seen as a total order on actions: this action occurs before that one \naccording to global time . Such an interleaving is in fact a consistent extension of a partial order \ncalled happens before that precisely relates causal dependencies between actions. For example, the program \nFig. 3a may exhibit an interleaving of thread-actions bt1 :: bt2 :: wt1 x, 1 :: rt1 y :: wt2 y, 1 :: \nrt2 x but there is no causal dependency between the read performed in t1 and the one performed in t2. \nFig. 3b presents the causality relation behind such a linear presentation. Each gray region is dedicated \nto the actions owned by a same thread. x.0 y.0 x.1 y.1 r1.y r2.x (a) Code of threads t1, t2. (b) Causality \nrelation (c) Axiomatic execution Figure 3. Happens-before execution with and without write-seen arrows. \nThe unique identi.er is omited here. Any sequence of solid arrows between actions a and b means a should \nhappen before b. Apart from address initialization and thread starts, few actions are actually po constrained \nin this example. We distinguish two kinds of arrows: -. so re.ects program order between actions of a \nsame thread; -. re.ects the synchronization relation between events. In more complex 3 Unlike [5, 22], \nwe stick to the JMM: every address is virtually given a default value at the start of the program, even \nif the corresponding location is not allocated yet.  examples, it may relate an unlock of a monitor \nwith its subsequent lock or the write of a volatile address with a subsequent read. Dotted arrows indicate \nwhich write was seen by each read action. These arrows form what we call an axiomatic execution. The \nwrite seen must satisfy some minimal constraints that we will make clear with the notion of well-formed \nexecution. Notations. When a partial order is total we write it directly as a sequence of elements that \nuniquely characterizes it. When a partial order -o . is a disjoint union (indexed by T) of orders, we \nwrite its restriction on thread t as [-o . ]t. A list of elements can be thought of as a total order. \nWe write a -tr. b when elements a, b are ordered with regards to a list tr. We call any irre.exive transitive \nrelation an order. Two such relations R and R ' are said to be consistent  when they satisfy: .x, y, \n\u00ac(xRy . yR ' x). We write tr .A for the sequence tr .ltered to the elements of the set A. DEFINITION \n4.1 (Axiomatic execution). An execution E is de\u00ad poso scribed by a tuple E = (P, A, -., -., W ) where: \nP is a program and A . A\\ Ad is a set of actions, po -. . A \u00d7 A is the program order, a disjoint union \nof total orders on actions of each thread, so -. . (A . Ad) \u00d7 (A . Ad) is the synchronization order: \nthe union of a total order on A n As of all synchronization actions in A, and the cartesian product Ad \n\u00d7 (A n As),  W . Ar -Aw is a write-seen function that maps each read action r from A to a write action \nw of A . Ad (r and w must operate on the same address).  We now explain how to extract the happens-before \nrelation from the program order and the synchronization order of an execution. DEFINITION 4.2 (Synchronizes-with \nrelation). An action a synchr\u00ad sw onizes-with an action b (written a - . b) in an execution pososo E \n= (P, A, -., -., W ) if a-.b and a, b satisfy one of the fol\u00adlowing conditions: a.Ad and b . A n Ab \n(default writes synchronizes-with starts),  a is a spawn of a thread t and b is the start of the thread \nt,  a is a write to a volatile address x and b is a read from x,  a is an unlock on monitor l and b \nis a lock on monitor l,  a is the end of the thread t and b is a join action on t.  DEFINITION 4.3 \n(Happens-before order). The happens-before or\u00adder of an execution is the transitive closure of the union \nof its synchronizes-with relation and its program order. hbswpo -.= (- . .-.)+ Intra-thread Semantics. \nOur formalization requires an abstract notion of intra-thread semantic state Stateintra and an intra-thread \nlabeled transition relation --._. Stateintra \u00d7 Labelintra \u00d7 Stateintra given to each thread t . T. Transition \nlabels are in the set Labelintra = (A \\ Ar) . (Ar \u00d7 V) . {t }: a thread can either take an action step, \nor a silent step (with label t) that is memory irrelevant. For a read action step, the value read is \npaired with the action in the label. The requirements on the intra-thread semantics are: --._ only relates \nstates of the same thread,  there is an initial state Ready: no transition leads to it and a thread \nt steps from it if and only if it emits the bt action,  non-silent labels are tagged with the emitting \nthread,  there is a .nal state Done: a step of a thread t leads to it if and only if its last transition \nis labeled by et. DEFINITION 4.4 (Intra-traces). Let tr = a1 :: \u00b7 \u00b7 \u00b7 :: an be a sequence of actions \nin set A and let W be a write-seen function on A. Given a thread t . T in program P , tr is an intra-trace \nof t if there exist s0, s1, . . . , sm . Stateintra (m = n) and l = l1 :: \u00b7 \u00b7 \u00b7 :: lm . list(Labelintra) \nsuch that: for all a . {a1, . . . , an}, T (a) = t, s0 is the initial intra-thread state Ready,  for \nall i . {1, . . . , m}, si-1 -_ si,  li -- the projection b1 :: \u00b7 \u00b7 \u00b7 :: bn of l to non-silent labels \nis such that bi = (ai, V (W (ai))) if ai is a read action or bi = ai otherwise. We write P [t] for the \nset of such pairs (tr, W ) for P . DEFINITION 4.5 (Well-formed execution). An execution poso (P, A, -., \n-., W ), is well-formed if A is .nite, sopo -. is consistent with -.,  Locking is proper: for all lock \nactions li tl . A and all threads t ' different from the thread t, the number of lock actions on  so \nl emitted by t ' before li tl in -. is the same as the number of so unlock actions on l emitted by t \n' before li tl in -., and each unlock action u i tl . A occurs after a matching lock action: 'soj j so \n.li tl, .t = t, |{lj t, l | lj t, l-.li tl}| = |{ut, l | ut, l-.li tl}| i poi j j poi tt tt .utl, |{lj \nl | lj l-.utl}| > |{ul | ul-.utl}| popo -. is intra-thread consistent: for all thread t . T, ([-.]t, \nW ) . P [t], so -. is consistent with W : for every read r of a volatile address so x we have W (r)-.r \nand for any write w to x different from sosososo W (r), either w-.W (r)-.r or W (r)-.r-.w, hbhb -. is \nconsistent with W : for all reads r of x, r -. W (r) does not hold and there is no intervening write \nw to x, i.e. such that hbhb W (r) -. w -. r. A distinguished subfamily of well-formed executions is the \nset of Sequentially Consistent axiomatic executions. DEFINITION 4.6 (Sequentially Consistent (SC) execution). \nA well\u00ad poso formed execution E = (P, A, -., -., W ) is SC if there exists a to total order -. on A such \nthat toposo -. is consistent with -. and -.,  for each read action r . A accessing address x, W (r) \nis the  to last write on x before r in -.. The set of well-formed executions of a program forms the \nHappens-Before memory model. It is relatively easy to manipulate but it is not a satisfactory memory \nmodel because it allows out-of-thin\u00adair values [22] and does not ful.ll the DRF theorem. The JMM considers \na subset of this model; these are known as legal executions. The exact de.nition of legal executions \nis subtle. In a nutshell, a well-formed execution E is legal if there exists a sequence E0, E1, . . . \n, En = E of well-formed executions such that in E0, each read sees a write that it does not race with. \nThen progressively, each execution Ei allows some reads through data races but in a well-founded order \nuntil the execution E itself is reached. Thanks to this de.nition, one obtains almost directly that: \nin a DRF program all reads see writes that happen-before them and each execution is sequentially consistent; \nand no out-of-thin-air value can be read, by the causality order on races.  Whereas the DRF guarantee \nof the JMM is easily achieved, the complex de.nition of the race-committing sequence makes it hard to \njustify whether a given reordering is allowed or not. Our approach is to de.ne the BMM with explicit \nreorderings, introduced in the next section. 5. BMM: An Axiomatic Memory Model We now formally de.ne \nthe axiomatic view of our memory model. The semantics is built on top of two simple notions: sequential \nconsistency and reordering of actions. The transformations of executions are expressed with the notion \nof local reordering. DEFINITION 5.1 (Local reordering). Given an execution E = , posoposo (P, A, -., \n-., W ), E ' = (P ' , A, - ., -., W ) is a local re\u00adordering of E from an action list l to a list l ' \nin thread t if , popo [-.]t = a \u00b7 l \u00b7 \u00df and [- .]t = a \u00b7 l ' \u00b7 \u00df, , popo [-.]t, = [- .]t, for all threads \nt ' = t,  for all (tr, W ) . P ' [t] where tr is of the form a \u00b7 l ' \u00b7 \u00df, there exists (a \u00b7 l \u00b7 \u00df, W \n) . P [t],  P [t ' ] = P ' [t ' ] for all thread t ' = t,  l and l ' contain the same set of actions, \n no element of l or l ' is a synchronization action.  t:[l-.l,] Such a reordering is written E -----. \nE ' . po Intuitively, we reorder the intra-thread trace [-.]t by transforming l into l ' . BMM exposes \ntwo reorderings to the programmer. The .rst one is a Write-Read reordering which reorders a read before \na previous adjacent write to a different address. Here is a simple example: WR x.1; r.y ----. r.y; x.1 \nDEFINITION 5.2 (Write-Read reordering). A Write-Read reorder- WRposo ing, E --. E ' , of an execution \nE = (P, A, -., -., W ) with respect to actions w and r in t, is a local reordering E ' such that t:[w::r-.r::w] \nE ---------. E ' where w and r operate on different addresses. Fig. 4 illustrates the use of the Write-Read \nreordering on a classical litmus test program. To understand the BMM semantics, a key ob\u00adservation must \nbe made: the execution on the left is not sequentially consistent but after two WR reorderings we obtain \na sequentially consistent execution. It is then tempting to ask if a BMM execution is any execution that \ncan be transformed into an SC execution after Figure 4. Write-Read reordering example. some WR reorderings. \nUnfortunately, such a de.nition would not allow us to capture executions exhibited by TSO-hardware, so \nwe need to work a bit harder. The program on the top-left part of Fig. 5 illustrates this issue. In this \nprogram, the con.guration r1 =1, r2 = 0, r3 = 1; r4 = 1, r5 = 0 is reachable under a TSO architecture, \nbut it is not a SC execution and no WR reordering can be applied to this program. We introduce a second \ncategory of reorderings that is allowed in BMM that permits such executions. Figure 5. Write-Read-Read \nreordering example. DEFINITION 5.3 (Write-Read-Read reordering). A Write-Read\u00ad poso Read reordering of \nE = (P, A, -., -., W ) w.r.t. a tuple of action (w, rr, r ' ) of A, is a local reordering E ' such that \n, t:[w::ir::r,-.r::w::ir] E -------------. E ' Reads in rr = r1, . . . , rn have w as write-seen, and \nr ' and w target WRR different addresses. Such a reordering is written E ----. E ' . In Fig. 5, we apply \nthis transformations to the previous program. This time, a reordering is possible and leads to an SC \nexecution. Note that WR*R is a generalization of the previous WR reordering. We prove in Sec. 7 that \nthis new reordering exactly captures a TSO operational semantics. Formally, a BMM execution is any execution \nthat can be transformed using WR*R reorderings until reaching an SC execution. DEFINITION 5.4 (BMM executions). \nBMM executions are: n o ' RO' BMM= E | .E , E --. E and E ' is SC ROWRR with --.= (----.) * . We write \nBMM(P ) for the set of executions of a program P . The BMM observable behaviors of a program P is then \nde.ned as the set of sequences of external actions: n o soposo Obs(P ) = -..Ax | (P, A, -., -., W ) . \nBMM(P ) The de.nition of BMM is a least post-.xpoint. Every time we must prove that BMM is included in \nsome set of well-formed executions, we can rely on the following characterization. LEMMA 5.1 (BMM least \npost-.xpoint characterisation). BMM is the least set S of well-formed executions such that  all SC executions \nare in S, t ts(t) --s S is backward-closed by BMM reorderings: for any well-formed tt--[TAU] ROts, b, \nm -. ts[t . s], b, m executions E, E ' such that E --. E ' , if E ' . S then E . S. ts(t) i t r x|V (w) \n---------s Proof. First, BMM satis.es the two above properties. Now, let S w = rdt(b, m, x) \u00acvolatile(x) \n[READ] x|w - --. ts[t . s], b, m i t be a set of well-formed executions satisfying both properties. We \nshow that BMM . S. Let E . BMM. If E is SC, then E . S. r ts, b, m i t ROw --. E ' Otherwise, there \nexists an SC execution E ' such that E x,v ------s \u00acvolatile(x) . ts(t) - [WRITE] But E ' is in S, so \nE is in S too. o w x,v - --. ts[t . s], b[t . (wi i t ts, b, m x, v) :: b(t)], m t We use this lemma \nto show that BMM is a subset of the JMM b(t) = l \u00b7 [wt i x, v] executions and to show the equivalence \nbetween BMM and BMMo. [UNBUFF] x,v) -----. ts, b[t . l], m[x . (wi i t B(w - ts, b, m x, v)] 5.1 BMM \nis a Subset of JMM t The current Java Memory Model de.nes the set of legal executions as a subset of \nall well-formed executions that are justi.able using a sequence of intermediate justi.cations. In order \nto connect our model with the JMM, rather than unfolding the details of this formal de.nition, we rely \non the following JMM properties: JMM accepts all sequentially consistent executions,  JMM allows reordering \nof non-volatile memory accesses hitting different locations [34].  THEOREM 5.2. Let JMM be the set of \nall legal executions permit\u00adted by the JMM. Then, BMM . JMM. Proof. We use here Lemma 5.1. We .rst know \nthat JMM contains all RO . E ' ' SC executions. Then, suppose that E --with E . JMM. In the JMM, reordering \nnon volatile memory accesses hitting different addresses is allowed [34]. We use this property to un-transform \nE ' into E. Hence, E is also in JMM, meaning that JMM is backward\u00adclosed by WR * R. o  5.2 DRF Guarantee \nWe establish that BMM enjoys the important property that any reasonable memory model should have, namely \na data-race-free guarantee -data-race free programs have SC executions only. We .rst de.ne the standard \nnotions of concurrent con.icting memory action, data-race, and data-race-free programs: DEFINITION 5.5 \n(Con.icting actions). Two non-volatile actions a, b . Ar . Aw are con.icting if they target the same \naddress and T (a) = T (b) and at least one of them is a write. poso DEFINITION 5.6 (Data-races). Let \n(P, A, -., -., W ) be a BMM SC execution. Two con.icting actions a, b form a data-race if they hb are \nnot ordered by -.. DEFINITION 5.7 (DRF). A program P is data-race free, written DRF (P ), if all of its \nSC executions are free of data-race. THEOREM 5.3 (DRF guarantee). For all P , if DRF(P ), then for all \nexecution E . BMM(P ), E is SC. Proof. Let P be such that DRF (P ), and E . BMM(P ). By Theo\u00adrem 5.2, \nE . JMM(P ). But JMM satis.es the DRF guarantee [32], so E is sequentially consistent. o 6. BMMo: An \nOperational Memory Model In this section, we provide an operational view of the BMM. The reorderings \nallowed in its axiomatic version can be implemented by a BMMo machine that attaches a write-buffer to \neach running thread. The BMMo semantics is also parametrized by an intra-thread semantics. Hence, we \nneed to consider an extra set of actions: the . ' ' ts, m -.synch ts , m b(t) = [] [SYNCH] . ts, b, m \n-. ts ' , b, m' Figure 6. BMMo machine (labelled transition system). silent actions in Asil that are \neither the unbuffering B(a) of a write action a . Aw \\ Ad by thread T (a) or a silent step tt by thread \nt. Asil ::= B(a) | tt The idea behind BMMo is to provide a generative, operational machine that executes \nan input operational execution, modifying a memory state, made of thread buffers and a shared memory. \nThe input of the BMMo machine is an operational execution, made of a program and a trace of operational \nactions. An operational action a . Aop is either an action in A\\ (Ad . Ar), or a pair in Ar \u00d7 Aw (for \neach read action we record the write action that it sees, and refer to it as its write-seen), or a silent \naction in Asil. DEFINITION 6.1 (Operational execution). An operational execu\u00adtion is a pair (P, tr) where \nP is a program and tr . list(Aop) is .nite and such that no action appear more than once in tr. The BMMo \nmachine is then de.ned by a transition system, parametrized by an intra-thread semantics. We now describe \nits states and transitions. A BMMo state . State is a record ts . T -Stateintra; (intra-thread state \nof threads) b . T -list(Aw \\ Ad); (one buffer per thread) m . X . Aw (one write action per address) The \nstate .rst keeps track of each intra-thread state in Stateintra. Each thread is given a write buffer; \nall non-volatile write actions are .rst written to this buffer. When unbuffered, these writes are committed \nto the shared memory m, that maps addresses to write actions. Given a memory state (buffers b and memory \nm), the BMMo machine speci.es the write action a thread t can read when accessing the address x: ( w \nif w is the .rst write to x in b(t) rdt(b, m, x) = m(x) if there is no write to x in b(t) If a pending \nwrite to this address appears in the buffer of t, we take the most recent in the execution (i.e. the \n.rst in the buffer). Otherwise we consult the memory. If no write has been performed yet at this address \nwe will retrieve the default value for this address. The BMMo machine is de.ned as a labeled transition \nsystem where steps are labeled by operational actions. The salient semantic rules are given Fig. 6. In \nall rules (except BUFF) the BMMo machine makes a step that the intra-thread semantics can match. Rule \nTAU corresponds to a intra-thread silent step (when e.g. the thread operates on registers). The BMMo \nmemory state does not change in this case. On a non-volatile read (READ) the value is obtained from \n r i t w i t x,vx|V (w) ---------s volatile(x) ts(t) -------s [READV] [WRITEV] x,v - --.synch ts[t . \ns], m[x . (wi i t x|w - --.synch ts[t . s], m r w x, v)] ts, m ts, m t ts(t) w = m(x) volatile(x) bt \nstt, ts(t) - ---s ts(t) - ----s t ' . dom(ts) ts(t) i ti t l l - ---s [BEGIN] [SPAWN] [LOCK] l i t bt \n-.synch ts[t . s], m stt, --.synch ts[t . s][t 'l l . Ready], m ts, m ts, m -.synch ts[t . s], m u ts, \nm i ti t , j t et ts(t) - ---s ts(t ' ) = Done ts(t) - ----s ts(t) -----s [END] [JOIN] [UNLOCK] et -.synch \nts[t . s], m j -- i t u - .synch ts[t . s], m i t lt, ts, m .synch ts[t . s], m Figure 7. BMMo machine \n(synchronization actions). ts, m ts, m the memory state using the rd function. For this action, the intra\u00adthread \nevent is a pair rt i x | v composed of a read and a value. The intra-thread semantics accepts any value \nhere but the purpose of the rule is to constrain it using thread-local buffers and shared memory. On \na non-volatile write (WRITE), the write action is put onto the thread s buffer. A write action can be \nunbuffered at any time, in which case the write is committed into shared memory (UNBUFF). All synchronizing \nactions are emitted by threads whose buffers are empty. They are gathered under the SYNCH rule whose \nde.nition (relying on a relation -\u00b7 .synch) is in Fig. 7. Reads from and writes to volatile locations \ndirectly access the memory so that all threads have a consistent view (READV and WRITEV). When a thread \nends (END), its state is kept in the BMMo state, enabling other threads to join it (JOIN), and preventing \nit to from being restarted (SPAWN). We then de.ne a BMMo execution as a constrained operational execution \nthat is accepted by the BMMo machine: the input trace is properly locked and can be executed by the machine, \nwith the intended meaning that the input execution is intra-thread consistent. DEFINITION 6.2 (BMMo execution). \nAn operational execution (P, tr) is in BMMo(P ) if there exists states s0, s1, . . . , sn in State satisfying \nthe following: tr is properly locked (see De.nition 4.5, using -tr. instead of poso -. and -.), s0 is \nan initial state: the memory maps every address to the corresponding default write (.x, m(x) = w0x), \nbuffers are empty and s0.ts is de.ned for exactly one thread, mapping it to the Ready state,  tr = a1 \n:: \u00b7 \u00b7 \u00b7 :: an . list(Aop),  ai for all i . {1, . . . , n}, si-1 -. si. The BMMo behaviors of program \nP are external action traces obtained by projecting all executions of P accepted by the BMMo machine \non Ax: Obso(P ) = {tr .Ax | (P, tr) . BMMo(P )} 7. Equivalence of BMM and BMMo We show that BMM and BMMo \nare equivalent relaxed memory models: they allow the exact same set of behaviors for any program. THEOREM \n7.1. For all program P , Obso(P ) = Obs(P ). The proof relies on an operator . that bridges the gap between \nBMM and BMMo by building axiomatic executions from operational ones. potr for all a, b . A, a-.b iff \nT (a) = T (b) and a -. b, sotr for all a, b . A, a-.b iff a, b . As and a -. b,  for all pairs rt i \nx | w in tr, W (rt i x) = w.  We show that .(BMMo) = BMM. Each inclusion is proved and Theorem 7.1 follows \nfrom the following lemma. LEMMA 7.2. Let Eo = (P, tr) be an operational execution and pososo (P, A, -., \n-., W ) = .(Eo), then tr .Ax = (-..Ax ). so Proof. By de.nition of ., we know that for all a, b . A, \na-.b iff a, b . As and a -tr. b. o We de.ne auxiliary notions needed for the proof. An operational execution \nEo is well-formed if .(Eo) is well-formed, and it is SC. if .(Eo) is SC. Similarly, we de.ne operational \nreorderings relying on .: an execution Eo ' is an operational reordering of WR R Eo if .(Eo) ----. .(Eo \n' ). We abuse notations and write it WR R RO Eo ----. Eo ' , and lift this notion to trace reorderings \n--.. To lighten the notations in this section, we keep implicit the unique identi.er of actions and write \nwx t for wt i x, v (the value written is omitted). When considering operational actions in a trace we \ngenerally omit the write action w attached to a read action. 7.1 .(BMMo) . BMM We prove that every BMMo \nexecution trace can be reordered into a SC trace. The proof relies on a reordering scheme explained in \nthe following lemma. LEMMA 7.3. Let Eo = (P, tr) . BMMo(P ), with tr = a\u00b7[wx t ]\u00b7\u00df an execution such \nthat B(wx t ) . \u00df. We write Wt = {wy t . \u00df | y . X}, write actions belonging to thread t, tt tt Rt = \n{ry . \u00df | wx \u00b7\u00df = .1\u00b7wy \u00b7.2\u00b7ry \u00b7.3, y . X, .1, .2, .3 . list(Aop)}, read actions seeing a write performed \nby t in [wx t ]\u00b7\u00df, \u00df \\ (Wt . Rt) the remaining actions in \u00df. ' ''' Then, there exist P , \u00df1, \u00df2 such \nthat Eo ' = (P , tr ) . BMMo(P ), RO Eo --. Eo ' and tr ' = a \u00b7 \u00df1 \u00b7 [wx t ] \u00b7 \u00df2 where, \u00df1 = \u00df .\u00df\\(Wt.Rt), \n \u00df2 contains the elements of Wt . Rt , t tttt  [wx] \u00b7 \u00df2 matches the pattern (w ; (r ) * ) \u00b7 . . . \n\u00b7 (wxn ; (r ) * ),  x1 x1 xn '' ' for all d, (P, tr \u00b7 d).BMMo(P ) then (P , tr \u00b7 d) .BMMo(P ). Before \ngoing into the detail of the proof, we give an intuition on how DEFINITION 7.1 (. operator). Let Eo = \n(P, tr) be an execution, . posowe use it. This lemma is applied to a part of an execution in which a \nis de.ned as .(Eo) = (P, A, -., -., W ) where write action, performed by t, stays in its buffer. This \nis illustrated in A is the set of non-silent actions in tr, the following .gure, where the grey regions \ndenote subsequences of actions whose owning thread is not t. The bold stroke action w t is x the write \naction that remains in t s buffer until the end of tr. We will use this action as a pivot on all the \nactions performed in the rest of tr, so that the resulting trace tr ' is as illustrated: (a) all grey \nactions are shifted before the pivot, remaining in the same relative order, by changing the interleaving; \n(b) actions of thread t are handled with the WR * R reordering rule. Because write actions of t cannot \nbe moved, they are kept to the right of the pivot. Handling read actions of t is more involved: either \nthey see a write that occurred (necessarily in thread t) after the pivot in tr, and they tt tt are accumulated \nin a pattern (wx1 ; (rx1 ) * ) \u00b7 . . . \u00b7 (wxn ; (rxn ) * ), or they see a write that occurs before the \npivot, and WR * R is applied repeatedly on this pattern, until they are swapped before wx t . Proof. \nLet Eo = (P, tr) . BMMo(P ), with tr = (a \u00b7 [wx t ] \u00b7 \u00df) and B(wx t ) . \u00df. The sub-trace [wx t ] \u00b7 \u00df \ncan be decomposed as t tt [wx] \u00b7 \u00df = \u00dfp \u00b7 \u00dft with \u00dfp = [wy; (ry) * ]+ (we take the shortest \u00dft). We now \nproceed by induction on the length of \u00dft. Base case: \u00dft is empty. We don t need any reordering transfor\u00admation \nto reach the expected pattern.  Inductive case. We proceed by case analysis on the .rst element of \u00dft \n= a :: \u00dft ' and show that a can be either (i) integrated inside the pattern \u00dfp or (ii) be moved before \nthis pattern:  If a = rk t , when k is one of the addresses of \u00dfp. Here, we integrate in \u00dfp by applying \nan WR * R as many times as needed to make it part of the right wk t (rk t ) * pattern such that \u00dfp = \n\u00dfp1 \u00b7 (wk t (rk t ) * ) \u00b7 \u00dfp2 . Note WR * R can be applied because the pattern only concerns thread t, \nand the visibility conditions required by WR * R are ful.lled. The right-most such pattern before a, \ni.e. such that there is no wk t in \u00dfp2 , is the only one that can ensure the write-seen of a to be preserved, \naccording to BMMo. Thus the resulting trace ` \u00b4 ' t t P , a \u00b7 \u00dfp1 \u00b7 (wk(rk) * a) \u00b7 \u00dfp2 \u00b7 \u00dft ' is BMMo. \nIf a = rk t , when k is not any of the addresses of \u00dfp. We apply the same reasoning as in the previous \ncase, rewriting the trace with WR * R, but in this case, this simply amounts to put a before wx t , because \nWR * R will be applied on the whole \u00dfp (and the write-seen is trivially kept valid). If a = B(a ' ), \nthen a ' . a because there is no un\u00ad buffering action in \u00dfp. This unbuffering could have been done just \nbefore wx t . This unbuffering does not modify the visibility constraints of the new trace, as it cannot \noverwrite any of the write actions in \u00dfp. For any trace d, if (P, a \u00b7 \u00dfp \u00b7 [a] \u00b7 \u00dft ' \u00b7 d) . BMMo(P ) \nthen the trace (P, a \u00b7 [a] \u00b7 \u00dfp \u00b7 \u00dft ' \u00b7 d) is also in BMMo (P): any read in d that sees a ' in the .rst \ntrace can still see it in the second trace because no write action in \u00dfp is unbuffered before d. ,, ,, \nt'' t For all other cases (a = rk , where t = t, a = wk , with t '' = t, and a . As), we can just change \nthe interleaving to move a before \u00dfp and conclude easily. For each resulting trace, we show that we are \nunder the induction hypothesis premises. Hence, we conclude by induction. o This reordering scheme is \nextensively used for proving Lemma 7.4, stating that the reordering interpretation of BMM can be simulated \nin the operational world. LEMMA 7.4. Let Eo = (P, tr) . BMMo(P ). Then there exist RO '' '''' ' P , tr \nsuch that Eo --. (P , tr ), with (P , tr ) . BMMo(P ) is SC.. In order to prove the lemma by induction, \nwe need to prove a stronger result, that includes an additional property on the shape of the two operational \ntraces tr and tr ' . This so-called write-swapping property is formally de.ned as follows. DEFINITION \n7.2 (Write-swapping). Let tr and tr ' two sequences of actions. The write-swapping property holds on \n(tr, tr ' ) if for any write actions w1, w2 to the same address, , if w1 -tr. w2 and w2 tr- . w1 (write \nactions have been swapped) then the trace tr is of the form tr = a \u00b7 [w1] \u00b7 \u00df \u00b7 [w2] \u00b7 . and B(w1) . \n\u00df. , if w1 -tr. w2 and w1 tr- . w2 (write actions have not been swapped) then if B(w1) occurs before \nw2 in tr, it is also the case in tr ' . Proof. We prove Lemma 7.4 by strong induction on the size of \nthe execution |tr| = n. Assume the property holds for any integer k < n. Let Eo = (P, tr) . BMMo(P ) \nan execution of size n. We assume n > 0 (the case n = 0 holds trivially). So tr is of the form tr = tr1 \n\u00b7 [a]. By induction on tr1, we get P2 and RO tr2 such that (P, tr1) --. (P2, tr2), with Eo 2 = (P2, tr2) \n. BMMo(P2) n SC., plus a write-swapping on (tr1, tr2). We extend Eo 2 to (P2, tr2 \u00b7 [a]). If action a \nis not a read action, we can conclude directly. Otherwise, a = rx t3 , the extended trace is in BMMo(P2), \nand the write-swapping property holds. It remains to show that it is SC.. If it is not, we proceed by \ncase analysis: Case 1: There is a thread t = t3 whose buffer is not empty at the end of tr2. Formally, \nthere is a write action wy t in tr2 such that B(wy t ) . tr2. tr2.[a] is of the form a \u00b7 [wy t ] \u00b7 \u00df \n\u00b7 [rx t3 ]. Applying Lemma 7.3 on wy t , we get P3 and tr3 such RO that (P2, tr2.[a]) --. (P3, tr3) with \nEo 3 = (P3, tr3) . t3 t t BMMo(P3) and tr3 = a \u00b7 \u00df1 \u00b7 [rx ] \u00b7 [wy] \u00b7 \u00df2 with [wy] \u00b7 \u00df2 tt t3 matching \nthe pattern [w\u00b7 ; (r\u00b7 ) * ]+ . By induction on a \u00b7 \u00df1 \u00b7 [rx ], ` \u00b4 RO we get P4 and tr4 such that P3, \na \u00b7 \u00df1 \u00b7 [r t3 ] --. (P4, tr4), x with Eo 4 = (P4, tr4) . BMMo(P4) n SC., plus a write\u00adswapping property \non the traces. We concatenate the suf.x [wy t ]\u00b7\u00df2 to extend tr4 to an execution in BMMo(P4)nSC.. The \nsequential consistency holds thanks to the pattern of [wy t ] \u00b7 \u00df2. The write-swapping is preserved by \nthe concatenation. Case 2: All threads distinct from t3 have their buffer empty at the end of tr2. Formally, \nfor every write action wy t . tr2 such that t t1 t3 t = t3, B(wy) . tr2. Let wx be the write seen by \nrx . If B(wx t1 ) . tr2, it means that t1 = t3. The trace tr2.[a] is of t1 t3 t1 the form: a \u00b7 [w ] \u00b7 \n\u00df \u00b7 [r ]. We apply Lemma 7.3 on w . xx x The trace a \u00b7 \u00df1 \u00b7 [wx t1 ] \u00b7 \u00df2 we obtain is in BMMo and by \nthe shape of \u00df1 and \u00df2 it is SC.. We must show wx t1 is now the most recent write to x in \u00df2. But any \nother write there t3 t1 would be from thread t1 and rx could thus not see wx . If B(wx t1 ) . tr2, then \nthe trace tr2.[a] is of the form t1 t1 t3 t2 a \u00b7 [wx ] \u00b7 \u00df \u00b7 [B(wx )] \u00b7 . \u00b7 [rx ]. No wx more recent \nthan wx t1 can appear in .: either it is unbuffered in . and it would overwrite wx t1 or it is not unbuffered \nbut then t2 = t3 and t1 t3 t2 wx could not be seen by rx . No unbuffering B(wx ) can either appear in \n. since it would overwrite wx t1 .  x Transformation SC JMM BMM ] = a\u00b7\u00df1 \u00b7[w ]\u00b7\u00df2 \u00b7[B(w )]\u00b7.\u00b7[r x xx \nTrace preserving transformation / / / t1 x Reordering normal memory accesses \u00d7 / . Redundant read after \nread elimination / \u00d7 / t1 t1 t1 t1 )] is SC because a\u00b7[w a\u00b7\u00df1 \u00b7[w ]\u00b7\u00df2 \u00b7[B(w ]\u00b7\u00df\u00b7[B(w )] x xx was. It \nremains to show that all reads in . still see the most t1 Redundant read after write elimination / / \n/recent writes. By induction on tr3 \u00b7 [B(w )] \u00b7 ., an SC x Irrelevant read elimination / / /execution \nis rebuilt, that keeps the same write-seen. Irrelevant read introduction / \u00d7 / o Redundant write before \nwrite elimination / / / Redundant write after read elimination / Finally, we obtain the .rst inclusion \nas a corollary of Lemma 7.4. \u00d7 \u00d7 Roach motel reordering / \u00d7 . COROL L A RY 7.5. .(BMMo) . BMM. Proof. \nLet Eo = (P, tr) . BMMo(P ). By Lemma 7.4, we have RO '' ' Eo --. Eo ' , with Eo ' = (P , tr ) . BMMo(P \n) is SC.. By RO . .(E ' ' de.nition, .(Eo) --o) and .(Eo) is SC. Hence, .(Eo) . BMM(P ). o  7.2 BMM \n. .(BMMo) We use here the post-.xpoint characterization of BMM (Lemma 5.1). We .rst show that .(BMMo) \ncontains all SC axiomatic executions. poso Let E = (P, A, -., -., W ) . BMM be an SC execution. Then \n-. and - there exists a total order to. on A, compatible with -poso. We write / for a valid transformation \nand \u00d7 when it is generally wrong, . for limited applicability: only WR*R applies to normal memory accesses; \na read can be delayed past a lock and a write can take over an unlock. Table 2. Validity of transformations \nin memory models. WR LEM M A 8.1. --. is valid. poso Proof. Let E = (P, A, -., -., W ) be an axiomatic \nexecution WR and Eo = (P ' , tr ' ) an operational execution with E --. .(Eo) = \u00df \u00b7 [rx] \u00b7 j t and Eo \n. BMMo. By hypothesis, we know that tr ' i t y, v] \u00b7 d and . does not contain any action owned by t \nexcept such that all read actions in A see the last write to their address . \u00b7 [w toto -.. We claim \nthat Eo -., i t y, v can be performed some unbuffering actions. The write action w= (P, tr) can be build, \nwith tr .A=w.r.t. j t just after rx since the unbuffering in . are independent of it and to and that \nEo . BMMo(P ). Silent actions are inserted in -. so that each write action is immediately unbuffered, \nand that tr is intra-thread consistent an equivalent condition was required for E . BMM(P ). Finally \n.(Eo) = E. We also show that .(BMMo) no read action in . can see w i t y, v (it is still in its buffer). \nHence the trace \u00df \u00b7 [r j t x] \u00b7 [w i t y, v] \u00b7 . \u00b7 d is still in BMMo for the same value-seen and write-seen \ninformation. After a swap we obtain a i t y, v] \u00b7 [r j t x] \u00b7 . \u00b7 d that belongs to BMMo(P ) (by '' \nis backward-closed by WR * R. Let E and E ' two well-formed trace tr = \u00df \u00b7 [w WR R axiomatic executions \nsuch that E ' . .(BMMo) and E ----. E ' . E ' . .(BMMo) so there exists Eo ' . BMMo such that E ' = .(Eo \n' ). WR * R is a valid transformation under BMMo (see Sec. 8.1), meaning that there exists Eo . BMMo \nsuch that .(Eo) = E. Hence, E . .(BMMo). 8. Validity of Transformations One of the objectives of any \nmemory model is to take into account the reorderings performed by the hardware and to allow compilers \nWR de.nition of --.) and .(P, tr '' ) = E. o WR R LEM M A 8.2. ----. is valid. poso Proof. Let E = (P, \nA, -., -., W ) be an axiomatic execution and let Eo = (P ' , tr ' ) be an operational execution such \nthat WR R E ----. .(Eo) and Eo . BMMo. By hypothesis, the traces tr ' is ' i1 in of the form tr j ti \nt y]\u00b7d. Each = \u00df\u00b7[r x]\u00b7.\u00b7[w y, v]\u00b7.1\u00b7[r y] \u00b7 \u00b7 \u00b7 .n \u00b7[r tt . , .1, . . . , .n does not contain any action \nowned by t except some i t y, v can unbuffering actions. As in the previous proof, the action wto perform \nsome program transformations that deal directly with j t x. All read actions r i1 in be performed just \nafter ry see the y, . . . , r memory accesses or locks. Tab. 2 gives standard transformations and their \nvalidity under various memory models [32, 34]. tti t y, v in tr ' write w and can thus also be performed \nearlier. The trace \u00df\u00b7[r j t i1 in ' x]\u00b7[w i t y, v]\u00b7[r .n \u00b7d is still in BMMo(P y] \u00b7 \u00b7 \u00b7 [r y]\u00b7.\u00b7.1 \u00b7 \n) For a proof of validity we rely on the operational model: we \u00b7 \u00b7 tti t for the same value-seen and \nwrite-seen (the moved reads see wconsider a BMMo trace of a transformed program and show there exists \na valid BMMo trace of the original program with the same y, v directly from t s buffer). We then conclude \nwith a swap: the trace i1 in j t x] \u00b7 . \u00b7 .1 \u00b7 \u00b7 \u00b7 .n \u00b7 '' = \u00df \u00b7 [w i t y, v] \u00b7 [r d belongs y] \u00b7 \u00b7 \u00b7 \n[r y] \u00b7 [r behavior. For a proof of invalidity, we provide a counter-example tr tt to BMMo(P ) and .(P, \ntr '' ) = E. and use the intuitive reordering memory model of BMM: given a o program P and a transformed \nprogram P ', we show that there exists an execution that is valid for P ' but invalid for P (both under \nBMM). This table demonstrate that, despite its restricted set of reorderings, BMM allows useful transformations. \n  8.1 Validity of WR and WR * R Among the set of transformations, the validity of the local reordering \nWR * R is crucial for the memory model inclusion.  8.2 Other Valid Transformations Given E ' =(P ' , \ntr ' ), for each transformation we observe the shape of tr ' and provide a corresponding BMMo trace of \nan untransformed program. We assume that the intra-thread semantics accepts the transformation. All read \nand write actions are non-volatile. Redundant Read after Read Elimination. ' = \u00df \u00b7 [r i t x] \u00b7 . tr \nF -. be- DE FI NI TI O N 8.1 (Valid reordering). A local reordering Trace \u00df \u00b7 [r i t x] \u00b7 [r j t x] \n\u00b7 . is in BMMo. r j t x and r i t x see the same write. tween axiomatic executions is said to be valid \nwith respect to BMMo if for all axiomatic execution E and all operational execu\u00ad FRedundant Read after \nWrite Elimination. tion Eo, E -. .(Eo) and Eo . BMMo implies that there exists ' Eo ' . BMMo such that \nE = .(Eo ' ). tr = \u00df \u00b7 [w i t x, v] \u00b7 . \u00b7 [w i t x, v] \u00b7 [r j t x] \u00b7 . is in BMMo. r j t x sees w \n i t x, v in its buffer.  Irrelevant Read Elimination. Roach-motel: Unlock-read Reordering. Reordering \nreads and unlocks is not al\u00ad ' x . 0 y . 0 tr = \u00df \u00b7 d lowed. Here, the reads cannot both see lock l \nlock l ' the default writes, whereas they could if Trace \u00df \u00b7 [r i t x] \u00b7 . is in BMMo. r i t x sees the \n.rst write to x in the x . 1 y . 1 one of them was hoisted into the critical buffer or pick the current \nwrite attached to x in memory. unlock l unlock l ' r1.y r2.x section and reordered with the preceed\u00ad \ning write. Irrelevant Read Introduction. Roach-motel: Write-lock Reordering. Reordering writes and locks \nis not al-x . 0 y . 0 lowed. The reads cannot both see the de-x . 1 y . 1 fault writes, whereas they \ncould if one lock l lock l ' of the writes was hoisted into the critical r1.y r2.x section and reordered \nwith the next read. unlock l unlock l ' 9. Empirical Evaluation of the BMM ' tr = \u00df \u00b7 [r i t x] \u00b7 d \nTrace \u00df \u00b7 . is in BMMo. The visibility of the other actions is preserved. Redundant Write before Write \nElimination. ' tr = \u00df \u00b7 [w j t y, v] \u00b7 . We have shown that BMM is more restrictive than the JMM. It \nj t We distinguish two cases. If . y, v) \u00b7 .2. The trace = .1 \u00b7 B(w is, therefore, natural to ask how \nsevere these restrictions are in \u00df \u00b7 [w i t y, v ' ] \u00b7 [w j t y, v] \u00b7 .1 \u00b7 B(w j t y, v) \u00b7 .2 i t y, \nv ' ) \u00b7 B(w is in practice, i.e. what is the performance impact imposed by BMM j t y, v BMMo. Adding \nthe unbuffering preserves the visibility of wwhen incorporated within a production virtual machine running \nis never seen. Otherwise, B(w j t y, v) . .. The trace ' since w i t y, v on a TSO architecture. In this \nsection, we present results from a preliminary study that provides a coarse upper-bound approximation \nof the overheads incurred by BMM conformance. Our experiment is as follows. Starting with a production \nvirtual machine, we switch y, v ' ] \u00b7 [w j t y, v] \u00b7 . is in BMMo. w i t y, v ' is never seen. ' tr = \n\u00df \u00b7 [w i t Roach-motel: Read-lock Reordering. A read before a lock can be reordered (no action emitted \nby the thread t in .). the backend from a non-verifying optimizing compiler to one tr ' = \u00df \u00b7 [l i t \nl] \u00b7 . \u00b7 [r j t x] \u00b7 d that preserves TSO semantics. Then, we modify any Java-level optimizations performed \nby the VM to be BMM compliant. We We assume tr ' is properly locked so no synchronization action expect \nthat the performance results are going to be an upper bound on l is in .. Hence the following interleaving \nis a valid trace of on the costs of BMM, since we limited the optimizations performed j t i t the transformed \nprogram: \u00df x] \u00b7 d. Then the trace \u00b7 . \u00b7 [l l] \u00b7 [r by the VM and ensured that they precisely respect \nthe reorderings \u00df \u00b7 . \u00b7 [r j t x] \u00b7 [l i t l] \u00b7 d is in BMMo. allowed by BMM. This enforcement is realized \nthrough the injection of memory fences, operations that effectively .ush the contents Roach-motel: Unlock-write \nReordering. A write after an unlock of store buffers; we make no attempt to optimize or verify the can \nbe reordered. Since the buffer must be empty, the transformed placement of such fences [38]. i t trace \nhas the following shape, where wx, v is the last non silent Speci.cally, we start with the Fiji open-source \nreal-time VM [26]. action of thread t. We selected Fiji because it has competitive performance, we under\u00ad \nstand the optimizations it performs well, and it is representative ' tr = \u00df \u00b7 [w i t x, v] \u00b7 . \u00b7 [B(w \ni t x, v)] \u00b7 d \u00b7 [u j t l] \u00b7 . of a real-world system. The Fiji compiler takes bytecode as input and, \nin the con.guration we use here, transforms it into ANSI C code, which is fed to gcc. The Fiji compiler \nincludes a variety of There is no action emitted by thread t in . except some unbufferings. No action \nin . emitted by the other threads can see the write w i t x, v. classic optimizations as well as Java-speci.c \ntechniques. To achieve Then, changing the scheduling, we can obtain the following BMMo BMM compliance \nin the back-end, we replace gcc with LLVMTSO, traces: \u00df \u00b7 . \u00b7 [w i t x, v] \u00b7 [B(w i t x, v)] \u00b7 d \u00b7 [u \nj t l] \u00b7 .. By hypothesis, an LLVM branch with optimizations either modi.ed or disabled to there is no \naction emitted by the thread t in d; besides there is no preserve compliance with the TSO memory model \n[23]. This is a lock/unlock action on l in d. Therefore changing the scheduling close approximation of \nwhat we would write to support BMM inagain can lead to the following BMMo (properly locked) trace: the \nbackend. Within Fiji, we carefully examined all optimization \u00df \u00b7 . \u00b7 [w i t x, v] \u00b7 [B(w i t x, v)] \u00b7 \n[u j t l] \u00b7 d \u00b7 .. Finally, the following trace passes to ensure compliance with BMM; redundant code \nelimina\u00ad tion (RCE) is the only optimization for which compliance could j t l] \u00b7 is a BMMo execution \nof the untransformed program: \u00df \u00b7 . \u00b7 [u [w i t x, v] \u00b7 [B(w i t x, v)] \u00b7 d \u00b7 ..  8.3 Invalid Transformations \nnot be guaranteed; RCE is performed over local operations as well as heap loads; as a result, the optimization \npermits write-read or read-read operations to be reordered, violating BMM semantics. We The BMM helps \nunderstand why some transformations are invalid. Redundant Write after Read Elimination. Here v is volatile. \nx.0 y.0 x.0 y.0 r1.x x.1 r1.x x.1 y.1 x.r1 v.0 r2.y redund. write-------. after read elim. y.1 v.0 r2.y \nr3.x r3.x r1 = r2 = 0, r3 =1 invalid r1 = r2 = 0, r3 =1 valid On the left, we see why the execution \nis not valid: it isn t SC and no reordering is possible. After the redundant write elimination, the execution \nbecomes valid because the read r3.x can be reordered with the write y.1 to give a SC execution. This \ntransformation thus introduced new behaviors. modi.ed the optimization to disallow processing any heap \nloads. We evaluated the modi.ed system on a variety of benchmarks, including SPECjvm98, SPECjbb2000, \nand a subset of DaCapo2006 (2006-10-MR2 Release) and DaCapo2009 (9.12 Bach Release). The whole benchmark \nsuite is a mix of concurrent and sequential programs, which we think are well suited to exhibit the throughput \noverheads of missed optimization opportunities and super.uous fences. (The concurrent programs we consider \nare mtrt, avora9, jbb2000, lusearch6, and lusearch9). The experiments were run on an 8-Core Xeon 3.16 \nGHz processors and 8 GB memory machine installed with Fedora 13 Linux (kernel 2.6.34). SPECjvm98 was \nexecuted for 15 iterations with the .rst 5 iterations being warm-up. We report the mean of the last 10 \niterations. The standard deviation was negligible. The SPECjbb2000 and DaCapo benchmarks were all executed \nwith their default con.gurations, except that the workload  Figure 8. Fiji VM. Execution time normalized \nto JMM con.guration. See Tab. 3 for con.gurations. Name LLVM RCE Biased-locking JMM (baseline) regular \nregular off BMM tso modi.ed off JMM +BL regular regular on with no fence BMM +BL tso modi.ed on with \n2 fences Table 3. Evaluation con.gurations. was set to the maximum wherever possible and that the number \nof warehouses (for SPECjbb2000) and the number of threads (for to the JSR 133 cookbook and other folklore,4 \nbiased locking is permitted under JMM, but to the best of our knowledge, there is no formal proof on \nthe number of fences required to ensure correctness. The BMM requires two fences, as we will prove next, \none at the lock and one at the unlock. With two fences, the bene.ts of biased locking is 30% at best \n(and on average 6%). Validity of Biased Locking. We explain how to reason about biased locking under \nBMM. Since the implementation of the locking mechanism cannot be described with Java constructs only, \nwe introduce two actions available only at a lower-level language in the DaCapo2009) were set to 8 to \nexercise the maximum concurrency. compilation chain: a memory fence ft and a fake unlock u i t l. The \nWe evaluate how BMM impacts performance by examining memory fence can be emitted by a thread if and only \nif its buffer is performance relative to a system that uses an unmodi.ed version empty. The effect of \nthe action is to ensure the buffer is empty prior of Fiji and LLVM; in this baseline con.guration, there \nare no to any subsequent action being performed.optimizations that are either modi.ed or disabled within \neither Fiji or LLVM; speci.cally, the baseline implementation allows Fiji to ft ts(t) ----s [FENCE] \nperform RCE transformations, and LLVM to perform non-TSO f ts, m -.synch ts[t . s], m t The fake unlock \nmodels the bias revocation; this action is taken intocompliant reorderings. The results are shown in \nFig. 8 with column BMM. The numbers are normalized with respect to this baseline. account as a regular \nunlock in what makes a trace properly locked.First, observe that adding TSO support to LLVM and crippling \nRCE has almost no impact on performance on most of benchmarks with However, it imposes no restrictions \non the state of buffers. l i t u the exception of jack and avrora9; for the former, BMM is 22% ts(t) \n---- -s faster while 16% slower for the latter. The average slowdown is 1%. [FAKEUNLOCK] i t At this \nlevel of abstraction we do not describe how the biased locking In general, these results provide anecdotal \nevidence that BMM is u ts, b, m - . ts[t . s], b, m l essentially performance neutral. On more relaxed \narchitectures the story is, of course, likely to be different. is to be implemented, but the intuition \nis that the conjunction of a fence and a fake unlock corresponds to an unlock. The general caseBiased \nLocking. While inspecting the code of the Fiji VM, we stumbled upon an interesting optimization. Fiji \nhas several imple-of a thread silently reacquiring the same lock can be seen as a trace mentations of \nthe Java concurrency control primitives. One optimiza-transformation (there is no ltl in \u00df and no ltl \nnor utl in .): tion that is supported is called biased locking [17]. Biased locking is an implementation \nof locking operations that is ef.cient when a \u00b7 [l i t l] \u00b7 \u00df \u00b7 [u j t l] \u00b7 . \u00b7 [l k t l] \u00b7 d BL -. \na \u00b7 [l i t l] \u00b7 \u00df \u00b7 [ft] \u00b7 . \u00b7 [ft] \u00b7 d one thread repeatedly acquires and releases the same monitor \n[29], This transformation is indeed valid: consider a BMMo execution '' ' a pattern that is frequent \nin Java. Roughly, a monitor can be biased (P , tr ) with tr = a \u00b7 [l i t l] \u00b7 \u00df \u00b7 [ft] \u00b7 . \u00b7 [ft] \u00b7 \nd. If the intra\u00ad to one thread that can then acquire and release it without any syn\u00ad j t thread semantics \nallows to replace the .rst fence by an action uchronization. However, when a thread needs a lock that \nis biased to k t l, then the untransformed traceand the second one by an action lanother thread, that \nbias needs to be revoked using synchronizing tr = a \u00b7 [l i t l] \u00b7 \u00df \u00b7 [u j t l] \u00b7 . \u00b7 [l k t l] \u00b7 \nd is also BMMo: the FENCEinstructions. As shown in Fig. 8 under column JMM+BL, turning on biased locking \nin Fiji results in a maximum speed up of roughly rule ensures that when the ft action is emitted the \nbuffer of thread t is empty so the UNLOCK/LOCK rules can be applied. Moreover45% (and an average of 8%). \nThis optimization thus appears to be the trace tr is properly locked. The revocation of a biased lockimportant \nto performance. corresponds to the following trace transformation. Fiji s biased locking is implemented \nwithout any CAS or fences. a \u00b7 [l i t l] \u00b7 \u00df \u00b7 [u j t l] \u00b7 . \u00b7 d BL, - . a \u00b7 [l i t l] \u00b7 \u00df \u00b7 [ft] \n\u00b7 . \u00b7 [u j, t l] \u00b7 d Although we believe this is what most of modern Java VM do, its 4 https://blogs.oracle.com/dave/entry/biased_locking_in_ \n cik [34]. According  Since there is no lock on monitor l in ., if the transformed trace is properly \nlocked, then so is the initial one. The action ft ensures t s buffer is empty after \u00df. The UNLOC K rule \ncan thus be applied. 10. Conclusions This work presents BMM, a memory model that has been designed as \npart of a broader undertaking to build a verifying compiler for multithreaded safety-critical Java. The \nkey characteristics required for such a memory model are ease-of-understanding both for pro\u00adgrammers \nand compiler writers, and a practical realization within a compiler framework that does not impose onerous \nrestrictions on important program optimizations. We believe BMM is a promising step in this direction. \nIts axiomatic de.nition is expressed using intuitive and simple memory reordering notions, making it \nsuitable for reasoning about program transformations, while its operational view can conveniently serve \nas a basis for a verifying compiler in\u00adfrastructure. Its backward compatibility with JMM entails that \nwe can use BMM on legacy code. It thus provides a key missing piece for a veri.ed infrastructure for \nJava on the x86 processor family. The question of how to obtain a more relaxed model, one that would \nallow ef.cient implementations on architectures such as Power and ARM, while at the same time remaining \namenable to incorporation within veri.ed compilers, remains a subject for fu\u00adture research. Although \nthese more relaxed platforms still guarantee coherence (all threads must respect a single linear order \nof writes), they allow other non-intuitive behaviors such as out-of-order writes, speculative execution, \netc., as well as having a substantially more complex notion of dependence; these complexities make formal\u00adization \nand axiomatic reasoning challenging [31]. We believe that more experience with BMM, with respect to both \napplications and optimizations, is necessary before we can transplant the intuitions underlying our development \nhere to these other environments. Acknowledgments We would like to thank Hans Boehm, Cliff Click, Doug \nLea, Gustavo Petri, Filip Pizlo, Jaroslav Sev.c.\u00b4ik and Peter Sewell for their useful feedback on this \nwork. We thank Dan Marino for his help with LLVMTSO. References [1] S. V. Adve and K. Gharachorloo. Shared \nmemory consistency models: A tutorial. Computer, 29(12), 1996. [2] S. V. Adve and M. Hill. A Uni.ed Formalization \nof Four Shared-Memory Models. Par. and Distr. Systems, IEEE Transactions on, 1993. [3] J. Alglave, L. \nMaranget, S. Sarkar, and P. Sewell. Fences in Weak Memory Models. In Proc. of CAV, 2010. [4] D. Aspinall \nand J. Sev .c.\u00b4ik. Java Memory Model Examples: Good, Bad and Ugly. In Proc. of VAMP, 2007. [5] D. Aspinall \nand J. Sev .c.\u00b4ik. Formalising Java s Data Race Free Guarantee. In Proc. of TPHOLs, 2007. [6] H.-J. Boehm \nand S. V. Adve. Foundations of the C++ concurrency memory model. SIGPLAN Not., 43, 2008. [7] G. Boudol \nand G. Petri. Relaxed Memory Models: an Operational Approach. In Proc. of POPL, 2009. [8] G. Boudol and \nG. Petri. A Theory of Speculative Computation. In Proc. of ESOP, 2010. [9] S. Burckhardt, M. Musuvathi, \nand V. Singh. Verifying Local Transfor\u00admations on Relaxed Memory Models. In Proc. of CC, 2010. [10] P. \nCenciarelli, A. Knapp, and E. Sibilio. The Java Memory Model: Operationally, denotationally, axiomatically. \nIn Proc. of ESOP, 2007. [11] B. Goetz, T. Peierls, J. Bloch, J. Bowbeer, D. Holmes, and D. Lea. Java \nConcurrency in Practice. Addison-Wesley Longman, 2006. [12] T. Henties, J. Hunt, D. Locke, K. Nilsen, \nM. Schoeberl, and J. Vitek. Java for safety-critical applications. In SafeCert, 2009. [13] L. Higham, \nJ. Kawash, and N. Verwaaland. De.ning and Comparing Memory Consistency Models. In Proc. of PDCS, 1997. \n[14] L. Hubert, T. Jensen, V. Monfort, and D. Pichardie. Enforcing Secure Object Initialization in Java. \nIn Proc. of ESORICS, 2010. [15] M. Huisman and G. Petri. The Java Memory Model: a Formal Explanation. \nIn Proc. of VAMP, 2007. [16] R. Jagadeesan, C. Pitcher, and J. Riely. Generative Operational Semantics \nfor Relaxed Memory Models. In Proc. of ESOP, 2010. [17] K. Kawachiya, A. Koseki, and T. Onodera. Lock \nReservation: Java Locks can Mostly do Without Atomic Operations. In Proc. of OOPSLA, 2002. [18] G. Klein \nand T. Nipkow. A Machine-Checked Model for a Java-like Language, Virtual Machine, and Compiler. ACM Trans. \nProgram. Lang. Syst., 28(4), 2006. [19] L. Lamport. Time, Clocks, and the Ordering of Events in a Distributed \nSystem. Commun. ACM, 21(7), 1978. [20] X. Leroy. A Formally Veri.ed Compiler Back-end. J. Autom. Reason\u00ading, \n43(4), 2009. [21] A. Lochbihler. Java and the Java memory Model a Uni.ed, Machine-Checked Formalisation. \nIn Proc. of ESOP, 2012. [22] J. Manson, W. Pugh, and S. V. Adve. The Java Memory Model. In Proc. of POPL, \n2005. [23] D. Marino, A. Singh, T. D. Millstein, M. Musuvathi, and S. Narayanasamy. A Case for an SC-Preserving \nCompiler. In Proc. of PLDI, 2011. [24] A. Min\u00b4 e. Static Analysis of Run-Time Errors in Embedded Critical \nParallel C Programs. In Proc. of ESOP, 2011. [25] S. Owens, S. Sarkar, and P. Sewell. A Better x86 Memory \nModel: x86-TSO. In Proc. of TPHOLs, 2009. [26] F. Pizlo, L. Ziarek, E. Blanton, P. Maj, and J. Vitek. \nHigh-level Programming of Embedded Hard Real-Time Devices. In Proc. of EuroSys, 2010. [27] W. Pugh. The \nInitialization On Demand Holder idiom, 2004. http://www.cs.umd.edu/~pugh/java/memoryModel/ jsr-133-faq.html#dcl. \n[28] W. Pugh. Causality test cases for the Java Memory Model, 2004. http://www.cs.umd.edu/~pugh/java/memoryModel/ \nCausalityTestCases.html. [29] K. Russell and D. Detlefs. Eliminating Synchronization-Related Atomic Operations \nwith Biased Locking and Bulk Rebiasing. In Proc. of OOPSLA, 2006. [30] S. Sarkar, P. Sewell, F. Z. Nardelli, \nS. Owens, T. Ridge, T. Braibant, M. O. Myreen, and J. Alglave. The Semantics of x86-CC Multiproces\u00adsor \nMachine Code. In Proc. of POPL, 2009. [31] S. Sarkar, P. Sewell, J. Alglave, L. Maranget, and D. Williams. \nUnder\u00adstanding Power Multiprocessors. In Proc. of PLDI, 2011. [32] J. .c\u00b4PhDSev.ik. Program Transformations \nin Weak Memory Models. thesis, The University of Edinburgh, 2009. [33] J. Sev .c.\u00b4ik. Safe optimisations \nfor shared-memory concurrent programs. In Proc. of PLDI, 2011. [34] J. Sev.c.\u00b4ik and D. Aspinall. On \nValidity of Program Transformations in the Java Memory Model. In Proc. of ECOOP, 2008. [35] J. Sev.c.\u00b4ik, \nV. Vafeiadis, F. Z. Nardelli, S. Jagannathan, and P. Sewell. Relaxed-memory Concurrency and Veri.ed Compilation. \nIn Proc. of POPL, 2011. [36] P. Sewell, S. Sarkar, S. Owens, F. Z. Nardelli, and M. O. Myreen. x86-TSO: \nA rigorous and usable programmer s model for x86 multi\u00adprocessors. Commun. ACM, 53(7), 2010. [37] E. \nTorlak, M. Vaziri, and J. Dolby. MemSAT: Checking Axiomatic Speci.cations of Memory Models. In Proc. \nof PLDI, 2010. [38] V. Vafeiadis and F. Z. Nardelli. Verifying fence elimination optimisa\u00adtions. In Proc. \nof SAS, 2011.  \n\t\t\t", "proc_id": "2429069", "abstract": "<p>Recent advances in verification have made it possible to envision trusted implementations of real-world languages. Java with its type-safety and fully specified semantics would appear to be an ideal candidate; yet, the complexity of the translation steps used in production virtual machines have made it a challenging target for verifying compiler technology. One of Java's key innovations, its memory model, poses significant obstacles to such an endeavor. The Java Memory Model is an ambitious attempt at specifying the behavior of multithreaded programs in a portable, hardware agnostic, way. While experts have an intuitive grasp of the properties that the model should enjoy, the specification is complex and not well-suited for integration within a verifying compiler infrastructure. Moreover, the specification is given in an axiomatic style that is distant from the intuitive reordering-based reasonings traditionally used to justify or rule out behaviors, and ill suited to the kind of operational reasoning one would expect to employ in a compiler. This paper takes a step back, and introduces a Buffered Memory Model (BMM) for Java. We choose a pragmatic point in the design space sacrificing generality in favor of a model that is fully characterized in terms of the reorderings it allows, amenable to formal reasoning, and which can be efficiently applied to a specific hardware family, namely x86 multiprocessors. Although the BMM restricts the reorderings compilers are allowed to perform, it serves as the key enabling device to achieving a verification pathway from bytecode to machine instructions. Despite its restrictions, we show that it is backwards compatible with the Java Memory Model and that it does not cripple performance on TSO architectures.</p>", "authors": [{"name": "Delphine Demange", "author_profile_id": "81418599022", "affiliation": "ENS Cachan Bretagne - IRISA, Rennes, France", "person_id": "P3977982", "email_address": "delphine.demange@gmail.com", "orcid_id": ""}, {"name": "Vincent Laporte", "author_profile_id": "81479664863", "affiliation": "ENS Cachan Bretagne - IRISA &#38; Purdue University, Rennes, France", "person_id": "P3977983", "email_address": "vincent.laporte@irisa.fr", "orcid_id": ""}, {"name": "Lei Zhao", "author_profile_id": "81553045356", "affiliation": "Purdue University, West Lafayette, USA", "person_id": "P3977984", "email_address": "zhao54@purdue.edu", "orcid_id": ""}, {"name": "Suresh Jagannathan", "author_profile_id": "81100208907", "affiliation": "Purdue University, West Lafayette, USA", "person_id": "P3977985", "email_address": "suresh@cs.purdue.edu", "orcid_id": ""}, {"name": "David Pichardie", "author_profile_id": "81321496627", "affiliation": "INRIA Rennes &#38; Purdue University, Rennes, France", "person_id": "P3977986", "email_address": "david.pichardie@inria.fr", "orcid_id": ""}, {"name": "Jan Vitek", "author_profile_id": "81100018102", "affiliation": "Purdue University, West Lafayette, USA", "person_id": "P3977987", "email_address": "jv@cs.purdue.edu", "orcid_id": ""}], "doi_number": "10.1145/2429069.2429110", "year": "2013", "article_id": "2429110", "conference": "POPL", "title": "Plan B: a buffered memory model for Java", "url": "http://dl.acm.org/citation.cfm?id=2429110"}