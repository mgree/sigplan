{"article_publication_date": "01-23-2013", "fulltext": "\n The Geometry of Types Ugo Dal Lago Barbara Petit Universit`a di Bologna &#38; INRIA {dallago,petit}@cs.unibo.it \nAbstract We show that time complexity analysis of higher-order functional programs can be effectively \nreduced to an arguably simpler (al\u00adthough computationally equivalent) veri.cation problem, namely checking \n.rst-order inequalities for validity. This is done by giving an ef.cient inference algorithm for linear \ndependent types which, given a PCF term, produces in output both a linear dependent type and a cost expression \nfor the term, together with a set of proof obligations. Actually, the output type judgement is derivable \niff all proof obligations are valid. This, coupled with the already known relative completeness of linear \ndependent types, ensures that no in\u00adformation is lost, i.e., that there are no false positives or negatives. \nMoreover, the procedure re.ects the dif.culty of the original prob\u00adlem: simple PCF terms give rise to \nsets of proof obligations which are easy to solve. The latter can then be put in a format suitable for \nautomatic or semi-automatic veri.cation by external solvers. On\u00adgoing experimental evaluation has produced \nencouraging results, which are brie.y presented in the paper. Categories and Subject Descriptors F.3.2 \n[Logics And Meanings Of Programs]: Semantics of Programming Languages Program Analysis; F.3.1 [Logics \nAnd Meanings Of Programs]: Specifying and Verifying and Reasoning about Programs General Terms Performance, \nTheory, Veri.cation Keywords Functional Programming, Higher-order Types, Linear Logic, Resource Consumption, \nComplexity Analysis 1. Introduction One of the most crucial non-functional properties of programs is \nthe amount of resources (like time, memory and power) they need when executed. Deriving upper bounds \non the resource consump\u00adtion of programs is crucial in many cases, but is in fact an undecid\u00adable problem \nas soon as the underlying programming language is non-trivial. If the units of measurement in which resources \nare mea\u00adsured become concrete and close to the physical ones, the problem becomes even more complicated, \ngiven the many transformation and optimisation layers programs are applied to before being exe\u00adcuted. \nA typical example is the one of WCET techniques adopted in real-time systems [29], which do not only \nneed to deal with how many machine instructions a program corresponds to, but also with how much time \neach instruction costs when executed by possibly Permission to make digital or hard copies of all or \npart of this work for personal or classroom use is granted without fee provided that copies are not made \nor distributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. complex architectures (including caches, pipelining, etc.), a \ntask which is becoming even harder with the current trend towards mul\u00adticore architectures. A different \napproach consists in analysing the abstract com\u00adplexity of programs. As an example, one can take the \nnumber of instructions executed by the program as a measure of its execution time. This is of course \na less informative metric, which however becomes more accurate if the actual time complexity of each \nin\u00adstruction is kept low. One advantage of this analysis is the indepen\u00addence from the speci.c hardware \nplatform executing the program at hand: the latter only needs to be analysed once. A variety of ver\u00adi.cation \ntechniques have been employed in this context, from ab\u00adstract interpretation [20] to type systems [23] \nto program logics [3] to interactive theorem proving1 . Among the many type-based techniques for complexity \nanaly\u00adsis, a recent proposal consists in going towards systems of linear dependent types, as suggested \nby Marco Gaboardi and the .rst au\u00adthor [11]. In linear dependent type theories, a judgement has the form \n$I t : s, where s is the type of t and I is its cost, an estimation of its time complexity. In this paper, \nwe show that the problem of checking, given a PCF term t and I, whether $I t : s holds can be ef.ciently \nreduced to the one of checking the truth of a set of proof obligations, themselves formulated in the \nlanguage of a .rst-order equational program. Interestingly, simple .-terms give rise to sim\u00adple equational \nprograms. In other words, linear dependent types are not only a sound and relatively complete methodology \nfor inferring time bounds of programs [11, 13]: they also allow to reduce com\u00ad plexity analysis to an \narguably simpler (although computationally equivalent) problem which is much better studied and for which \na variety of techniques and concrete tools exist [6]. Noticeably, the bounds one obtains this way translate \nto bounds on the number of steps performed by evaluation machines for the .-calculus, which means that \nthe induced metrics are not too abstract after all. The type inference algorithm is described in Section \n4. The scenario, then, becomes similar to the one in Floyd-Hoare program logics for imperative programs, \nwhere completeness holds [9] (at least for the simplest idioms [7]) and weakest pre\u00adconditions can be \ngenerated automatically (see, e.g., [3]). A bene.t of working with functional programs is that type inference \n the analogue of generating WPs can be done compositionally with\u00adout the need of guessing invariants. \nLinear dependent types are simple types annotated with some index terms, i.e. .rst-order terms re.ecting \nthe value of data .ow\u00ading inside the program. Type inference produces in output a type derivation, a \nset of inequalities (which should be thought of as proof obligations) and an equational program E giving \nmeaning to func\u00adtion symbols appearing in index terms (see Figure 1). A natural thing to do once E and \nthe various proof obligations are available is to try to solve them automatically, as an example through \nSMT solvers. If automatically checking the inequalities for truth does not POPL 13, January 23 25, 2013, \nRome, Italy. Copyright &#38;#169; 2013 ACM 978-1-4503-1832-7/13/01. . . $15.00 1 A detailed discussion \nwith related work is in Section 6.  c  Figure 1. General scheme of the type inference algorithm. succeed \n(which must happen, in some cases), one can anyway .nd useful information in the type derivation, as \nit tells you precisely which data every symbol corresponds to. We elaborate on this issue in Section \n5. But where does linear dependency come from? Linear depen\u00addent types can be seen as a way to turn Girard \ns geometry of in\u00adteraction (or, equivalently, AJM games [1]) into a type system for the .-calculus: the \nequational program one obtains as a result of type inference of a term t is nothing but as a description \nof a to\u00adken machine [14] for t. In presence of linear dependency, any term which can possibly be duplicated, \ncan receive different, although uniform, types, similarly to what happens in BLL [19]. As such, this \nform of dependency is signi.cantly simpler than the one of, e.g., the calculus of inductive constructions. \n2. Linear Dependency at a Glance Traditionally, type systems carry very little information about the \nvalue of data manipulated by programs, instead focusing on their nature. As an example, all (partial \nrecursive) functions from natural numbers to natural numbers can be typed as Nat \u00f1 Nat in the .-calculus \nwith natural numbers and higher-order recursion, also known as PCF [26]. This is not an intrinsic limit \nof the type-based analysis of programs, however: much richer type disciplines have .ourished in the last \ntwenty years [4, 15, 22]. All of them guarantee stronger properties for typable programs, the price being \na more complicated type language and computationally more dif.cult type inference and checking problems. \nAs an example, sized types [22] are a way to ensure termination of functional programs based on size \ninformation. In systems of sized types, a program like t .x..y.add padd x yq psucc yq can be typed as \nNata \u00f1 Natb \u00f1 Nata`2b`1, and in general as Nata \u00f1 Natb \u00f1 NatI, where I e a ` 2b ` 1. In other words, \nthe PCF type Nat is re.ned into NatI (where I is an arithmetical ex\u00adpression) whose semantics is the \nset of all natural numbers smaller or equal to I, i.e. the interval r0, Is D N. The role of size infor\u00admation \nis to ensure that all functions terminate, and this is done by restricting the kind of functions of which \none is allowed to form .xpoints. Sized types are nonlinear: arguments to functions can be freely duplicated. \nMoreover, the size information is only approx\u00adimate, since the expression labelling base types is only \nan upper bound on the size of typable values. Linear dependent types can be seen as a way to inject precision \nand linearity into sized types. Indeed, t receives the following type in dePCFV [13]: caIdaJ Natr0, as \n-Natr0, bs -Natr0, a ` 2b ` 1s. (1) As one can easily realise, NatrK, Hs is the type of all natural baJ \nnumbers in the interval rK, Hs D N. Moreover, s -t is the type of linear functions from s to t which \ncan be copied by the environment J times. The J copies of the function have types obtained by substituting \n0, . . . , J \u00b4 1 for b in s and t . This is the key idea behind linear dependency. The type (1) is imprecise, \nbut can be easily turned into caIdaJ Natra, as -Natrb, bs -Natra ` 2b ` 1, a ` 2b ` 1s, (2) itself a \ntype of t. In the following, the singleton interval type NatrK, Ks is denoted simply as NatrKs. Notice \nthat linear dependency is not exploited in (2), e.g., d does not appear free in Natrbs nor in Natra ` \n2b ` 1s. Yet, (2) precisely captures the functional behaviour of t. If d does not appear free daII in \ns nor in t , then s -t can be abbreviated as s -t . Linear dependency becomes necessary in presence of \nhigher-order functions. Consider, as another example, the term u .x..y. ifz y then 0 else xy u has simple \ntype pNat \u00f1 Natq \u00f1 Nat \u00f1 Nat. One way to turn it into a linear dependent type is the following HKL pNatras \n-NatrIsq -Natras -NatrJs, (3) where J equals 0 when a 0 and J equals I otherwise. Actually, u has type \n(3) for every I and J, provided the two expressions are in the appropriate relation. Now, consider the \nterm v p.x..y.px pred px id yqqq u. The same variable x is applied to the identity id and to the prede\u00adcessor \npred. Which type should we give to the variable x and to u, then? If we want to preserve precision, the \ntype should re.ect both uses of x. The right type for u is actually the following 1ca21 pNatras -NatrIsq \n-Natras -NatrJs, (4) where both I and J evaluate to a if c 0 and to a \u00b4 1 otherwise. If id is replaced \nby succ in the de.nition of v, then (4) becomes even more complicated: the .rst copy of J is fed not \nwith a but with either 0 or a ` 1. Linear dependency precisely consists in allowing different copies \nof a term to receive types which are indexed differently (although having the same functional skeleton \n) and to represent all of them in compact form. This is in contrast to, e.g., intersection types, where \nthe many different ways a function uses its argument could even be structurally different. This, as we \nwill see in Sec\u00adtion 3, has important consequences on the kind of completeness results one can hope for: \nif the language in which index terms are written is suf.ciently rich, then the obtained system is complete \nin an intensional sense: a precise type can be given to every terminat\u00ading t having type Nat \u00f1 Nat. Noticeably, \nlinear dependency allows to get precise information about the functional behaviour of programs without \nmaking the lan\u00adguage of types too different from the one of simple types (e.g., one does not need to \nquantify over index variables, as in sized types). The price to pay, however, is that types, and especially \nhigher-order types, need to be context aware: when you type u as a subterm of v (see above) you need \nto know which arguments u will be applied to. Despite this, a genuinely compositional type inference \nproce\u00addure can actually be designed and is the main technical contribution of this paper.  2.1 Linearity, \nAbstract Machines and the Complexity of Evaluation Why dependency, but specially linearity, are so useful \nfor complex\u00adity analysis? Actually, typing a term using linear dependent types requires .nding an upper \nbound to the number of times each value is copied by its environment, called its potential. In the term \nv from the example above, the variable x is used twice, and accordingly one .nds c a 2 in (4). Potentials \nof higher-order values occurring in a term are crucial parameters for the complexity of evaluating the \nterm by abstract mechanisms [10]. The following is an hope\u00ad fully convincing (but necessarily informal) \ndiscussion about why this is the case. Con.gurations of abstract machines for the .-calculus (like Friedman \nand Felleisen s CEK and Krivine s KAM) can be thought of as being decomposable into two distinct parts: \n First of all, there are duplicable entities which are either copied entirely or turned into non-duplicable \nentities. This includes, in particular, terms in so-called environments. Each (higher\u00adorder) duplicable \nentity is a subterm of the term the computa\u00adtion started from.  There are non-duplicable entities that \nthe machine uses to look for the next redex to be .red. Typically, these entities are the current term \nand (possibly) the stack. The essential feature of non-duplicable entities is the fact that they are \nprogressively consumed by the machine: the search for the next redex somehow consists in traversing the \nnon-duplicable entities until a redex is found or a duplicable entity needs to be turned into a non-duplicable \none.  As an example, consider the process of evaluating the PCF term p.f. ifz pf 0q then 0 else f pf \n0qqpp.x..y.add x yq 3q by an appropriate generalisation of the CEK, see Figure 2. Initially, the whole \nterm is non-duplicable. By travelling into it, the machine .nds a .rst redex u; at that point, 3 becomes \nduplicable. The ob\u00adtained closure itself becomes part of the environment ., and the machine looks into \nthe body of t, ending up in an occurrence of f, which needs to be replaced by a copy of .pfq. After an \ninstantiation step, a new non-duplicable entity .pfq indeed appears. Note that, by an easy combinatorial \nargument, the number of machine steps necessary to reach f is at most (proportional to) the size of the \nstart\u00ading term tu, since reaching f requires consuming non-duplicable entities which can only be created \nthrough instantiations. After a copy of .pfq becomes non-duplicable, some additional nondupli\u00adcable fuel \nbecomes available, but not too much: .y.add x y is af\u00adter all a subterm of the initial term. The careful \nreader should already have guessed the moral of this story: when analysing the time complexity of evaluation, \nwe could limit ourselves to counting how many instantiation steps the machine performs (as opposed to \ncounting all machine steps). We claim, on the other hand, that the number of instantiation steps equals \nthe sum of potentials of all values appearing in the initial term, something that can be easily inferred \nfrom the kind of precise linear typing we were talking about at the beginning of this section. Summing \nup, once a dependently linear type has been attributed to a term t, the time complexity of evaluating \nt can be derived somehow for free: not only an expression bounding the number of instantiation steps \nperformed by an abstract machine evaluating t can be derived, but it is part of the underlying type derivation, \nessentially. As a consequence, reasoning (automatically or not) about it can be done following the structure \nof the program. 3. Programs and Types, Formally In this section, we present some details of dePCF, a \nsystem of lin\u00adear dependent types for PCF [26]. Two versions exist: dePCFN and dePCFV, corresponding \nto call-by-name and call-by-value evalua\u00adtion of terms, respectively. The two type systems are different, \nbut the underlying idea is basically the same. We give here the details of the CB V version [13], which \nbetter corresponds to widespread in\u00ad tuitions about evaluation, but also provide some indications about \nthe CB N setting [11]. 3.1 Terms and Indexes Terms are given by the usual PCF grammar: s, t, u : x | \nn | t u | .x.t | pptq | sptq fix x.t | ifz t then u else s. A value (denoted by v, w etc. ) is either \na primitive integer n, or an abstraction .x.t, or a .xpoint fix x.t. In addition to the usual terms of \nthe .-calculus, there are a .xpoint construction, primitive natural numbers with predecessor and successor, \nand conditional branching with a test for zero. For instance, a simple program computing addition is \nthe following: add fix x. .y..z. ifz y then z else spx pppyqq zq . 3.1.1 Language of Indexes As explained \ninformally in Section 2, a type in dePCF consists in an annotation of a PCF type, where the annotation \nconsists in some indexes. The latter are parametrised by a set of index variables V ta, b, c, . . . \nu and an untyped signature T of function symbols, denoted by f, g, h, etc. We assume T contains at least \nthe arithmetic symbols `, \u00b4, 0 and 1, and we write n for 1 ` \u00a8 \u00a8 \u00a8 ` 1 (n times). Indexes are then constructed \nby the following grammar: \u00ef I, J, K :: a | fpI1, . . . , Inq | \u00ffJ | I,JK, aaI a where n is the arity \nof f in T. Free and bound index variables are de.ned as usual, taking care that all free occurrences \nof a r \u00cfI,K in J are bound in both J and J. The substitution of aaI a a variable a by J in I is written \nItJ{au. Given an equational program E attributing a meaning in Nn \u00e1 N to some symbols of arity n, and \na valuation . mapping index variables to N, the semantics [I]E . of an index I is either a natural number \nor unde.ned. Let us describe how we interpret the last two constructions, namely bounded sums and forest \ncardinalities. r Bounded sums have the usual meaning: aaI J is simply the sum of all possible values \nof J with a taking the values from 0 up to I, excluded. Describing the meaning of forest cardinalities, \non \u00cfI,J the other hand, requires some effort. Informally, the index K a counts the number of nodes in \na forest composed of J trees de\u00adscribed using K. Each node in the forest is (uniquely) identi.ed by a \nnatural number, starting from I and visiting the tree in pre\u00adorder. The index K has the role of describing \nthe number of chil\u00addren of each forest node, e.g. the number of children of the node 0 is Kt0{au. Consider \nthe following forest comprising two trees: 0 8 1 9 11 2 5 6 10 12 3 47 and consider an index K with \na free index variable a such that Kt1{au 3; Ktn{au 2 for n P t2, 8u; Ktn{au 1 when n P  t .f. ifz \npf 0q then 0 else fpf 0q; u p.x..y.add x yq 3; . f \u00de\u00d1 x .y.add x y ; x \u00de\u00d1 x 3 y y. x tu y . a x t \ny argx u y \u00a8 . a x u y funx t y \u00a8 . a x .x..y.add x y y argx 3 y \u00a8 funx t y \u00a8 . a\u00b0 x .y.add x y ; \nx \u00de\u00d1 x 3 y y funx t y \u00a8 . a x ifz pf 0q then 0 else f pf 0q ; . y . a x f 0 ; . y fork x0 ; fpf 0q \n; .y \u00a8 . a x f ; . y argx 0 ; . y \u00a8 fork x0 ; fpf 0q ; .y \u00a8 . a x .y.add x y ; x \u00de\u00d1 x 3 y y argx 0 \n; . y \u00a8 fork x0 ; fpf 0q ; .y \u00a8 . a\u00b0 x add x y ; py \u00de\u00d1 x 0 ; . y, x \u00de\u00d1 x 3 yq y fork x0 ; fpf 0q ; .y \n\u00a8 . a\u00b0 x 3 y fork x0 ; fpf 0q ; .y \u00a8 . a x f pf 0q ; . y . a x f ; . y argx f 0 ; . y \u00a8 . a x .y.add \nx y ; x \u00de\u00d1 x 3 y y argx f 0 ; . y \u00a8 . a x f 0 ; . y funx .y.add x y ; x \u00de\u00d1 x 3 y y \u00a8 . a\u00b0 x 3 y funx \n.y.add x y ; x \u00de\u00d1 x 3 y y \u00a8 . a\u00b0 x 6 y . Figure 2. Evaluation of a term in the CEKPCF abstract machine. \nt0, 6, 9, 11u; and Ktn{au 0 when n P t3, 4, 7, 10, 12u. That is, \u00cf0,2 K describes the number of children \nof each node. Then K a \u00cf0,1 13 since it takes into account the entire forest; a K 8 since it \u00cf8,1 takes \ninto account only the leftmost tree; a K 5 since it takes \u00cf2,3 into account only the second tree of \nthe forest; .nally, a K 6 since it takes into account only the three trees (as a forest) within the \ndashed rectangle. One may wonder what is the role of forest cardinalities in the type system. Actually, \nthey play a crucial role in the treatment of recursion, where the unfolding of recursive calls produces \na tree\u00adlike structure whose size is just the number of times the (recursively de.ned) function will be \nused globally. Notice that [I]I and any natural number cannot be derived from the underlying equational \nprogram. In particular, a forest cardinality may be unde- E . is unde.ned whenever the equality between \nto a natural number in the closed interval rI, Js. The potential of natural number values is not speci.ed, \nas they can be freely dupli\u00ad cated along CB V evaluation. \u00cf0,1 1 .ned even if all its subterms are de.ned: \nas an example I a has no value, because the corresponding tree consists of an in.nite descending chain \nand its cardinality is in.nite. By the way I is the index term describing the structure of the recursive \ncalls induced by the program fix x.x.  3.1.2 Semantic Judgements A constraint is an inequality on indexes. \nA constraint I d J is valid EE . .. duplicable terms. In dePCFV, they are de.ned as follows: A, B : s \n-t ; linear types s, t : ra a Is \u00a8 A | NatrI, Js. modal types Indeed, C BV evaluation only duplicates \nvalues. If such a value has an arrow type, then it is a function (either an abstraction or a .x\u00adpoint) \nthat can potentially increase the complexity of the whole pro\u00adgram if we duplicate it. Hence we need \na bound on the number of times we instantiate it if we want to keep the overall complexity under control. \nThis bound, call the potential of the value, is repre- aaI sented by I in the type ra a Is \u00a8 ps -t q \n(also written s -t). As explained in Section 2, NatrI, Js is the type of programs evaluating  3.2.1 \nSumming types Intuitively, the modal type s ra a Is \u00a8 A is assigned to terms that can be copied I times, \nthe kth copy being of type Atk \u00b4 1{au. For those readers who are familiar with Linear Logic, s can be \nthought of as representing the type At0{au b \u00a8 \u00a8 \u00a8 b AtI \u00b4 1{au. In the typing rules we are going to \nde.ne, modal types need to be manipulated in an algebraic way. For this reason, two operations on modal \ntypes are required. The .rst one is a binary operation Z E E for . and E when both [I]. and [J]As usual, \nwe can derive a notion of equality and strict inequality from d. A semantic judgement is of the form \n. are de.ned, and [I] f; F (E I d J, d [J] on modal types. Suppose that s ra a Is \u00a8 Ata{cu and that \nt ra a Js \u00a8 AtI ` a{cu. In other words, s consists of the .rst I instances of A, i.e. At0{cu b \u00a8 \u00a8 \u00a8 \nb AtI \u00b4 1{cu, while t consists of the next J instances of A, i.e. AtI ` 0{cu b \u00a8 \u00a8 \u00a8 b AtI ` J \u00b4 1{cu. \nTheir sum s Z t is naturally de.ned as a modal type consisting of where F is a set of constraints and \nf is the set of free index variables the .rst I ` J instances of A, i.e. rc a I ` Js \u00a8 A. Furthermore, \nin F, I and J. These semantic judgements are used as axioms in the NatrI, Js Z NatrI, Js is just NatrI, \nJs. A bounded sum operator on typing derivations of dePCF, and the set of constraints F, called modal \ntypes can be de.ned by generalising the idea above: suppose the index context, contains mainly some indications \nof bounds for the free index variables (such as a a K). Such a judgement is valid when, for every valuation \n. : f \u00d1 N, if all constraints in F are that # + \u00ff daa s rb a Js \u00a8 A b ` r Jtd{au{c . r valid for E and \n. then so is I d J. Then its bounded sum r s is just rc a aaI aaI Js \u00a8 A. Finally,  3.2 Types aaI NatrJ, \nKs NatrJ, Ks, provided a does not occur free in J nor in K. Remember that dePCF is aimed at controlling \nthe complexity of programs. The time complexity of the evaluation is thus analysed statically, while \ntyping the term at hand. The grammar for types 3.2.2 Subtyping distinguishes the subclass of linear \ntypes, which correspond to non-Central to dePCF is the notion of subtyping. An inequality rela\u00adduplicable \nterms (see Section 2.1), and the one of modal types, for tion D between (linear or modal) types can be \nde.ned using the f; F (E K d I f; F $E ( D s f; F (E J d H f; F $E t D . f; F $E NatrI, Js D NatrK, \nHs f; F $E s -t D ( -. pa, fq; pa a J, Fq $E A D B f; F (E J d I f; F $E ra a Is \u00a8 A D ra a Js \u00a8 B Figure \n3. Subtyping derivation rules of dePCFV. formal system in Fig. 3. This relation corresponds to lifting \nindex inequalities to the type level. As de.ned here, D is a pre-order (i.e. a re.exive and transitive \nrelation), which allows to cope with ap\u00adproximations in the typed analysis of programs. However, in the \ntype inference algorithm we will present in the next section only the symmetric closure of D, called \ntype equivalence will be used. This ensures that the type produced by the algorithm is precise.  3.2.3 \nTyping A typing judgement is of the form f; F; G $E K t : t , where K is the weight of t, that is (informally) \nthe maximal number of substitutions involved in the CB V evaluation of t (including the potential substitutions \nby t itself in its evaluation context). The index context F is as in a semantic judgement (see Section \n3.1.2), and G is a (term) context assigning a modal type to (at least) each free variable of t. Both \nsums and bounded sums are naturally extended from modal types to contexts (with, for instance, tx : s; \ny : tu Z tx : (, z : .u tx : s Z (; y : t ; z : .u). There might be free index variables in F, G, t \nand K, all of them from f. Typing judgements can be derived from the rules of Figure 4. Observe that, \nin the typing rule for the abstraction (-), I repre\u00adsents the number of times the value .x.t can be copied. \nIts weight (that is, the number of substitutions involving .x.t or one of its sub\u00adterms) is then I plus, \nfor each of these copies, the weight of t. In the typing rule (App), on the other hand, t is used once \nas a function, without been copied. Its potential needs to be at least 1. The typ\u00ading rule for the .xpoint \nis arguably the most complicated one. As a .rst approximation, assume that only one copy of fix x.t will \nbe used (that is, K 1 and a does not occur free in B). To compute the weight of fix x.t, we need to \nknow the number of times t will be copied during the evaluation, that is the number of nodes in the tree \nof its recursive calls. This tree is described by the index I (as explained in Section 3.1.1), since \neach occurrence of x in t stands \u00cf0,1 for a recursive call. It has H b I nodes. At each node b of this \ntree, there is a copy of t in which the a th occurrence of x will \u00cfb`1,abe replaced by the a th son of \nb, i.e. by b ` 1 ` b I. The types thus have to correspond, which is what the second premise of this rule \nprescribes. Now if fix x.t is in fact aimed at being copied K e 0 times, then all the copies of t are \nrepresented by a forest of K trees described by I. For the sake of simplicity, we present here the type \nsystem with an explicit subsumption rule. The latter allows to relax any bound in the types (and the \nweight), thereby loosing some precision in the information provided by the typing judgement. However, \nwe could alternatively replace this rule by relaxing the premises of all the other ones (which corresponds \nto the presentation of the type system given in [13], or in [11] for dePCFN). Restricting subtyping to \ntype equivalence amounts to considering types up to index equality in the type system of Figure 4 without \nthe rule (Subs) this is what we do in the type inference algorithm in Section 4. In this case we say \nthat the typing judgements are precise: DE FIN I T I ON 3.1. A derivable judgement f;F; G $E I t : s \nis precise if $ &#38; f; F $E . D G f; F; . $E J t : t is derivable \u00f9\u00f1 f; F $E s D t % f; F (E I d J \n 3.2.4 Call-by-value vs. Call-by-name In dePCFN, the syntax of terms and of indexes is the same as in \ndePCFV, but the language of types differs: A, B : s -A | NatrI, Js; linear types s, t : ra a Is \u00a8 A. \nmodal types Modal types still represent duplicable terms, except that now not only values but any argument \nto functions can be duplicated. So modal types only occur in negative position in arrow types. In the \nsame way, one can .nd them in the context of any typing judgement, $E f; F; x1 : s1, . . . , xn : sn \nK t : A. When a term is typed, it is a priori not duplicable, and its type is linear. It is turned into \na duplicable term when it holds the argument position in an application. As a consequence, the typing \nrule (App) becomes the most expansive one (for the weight) in dePCFN: the whole context used to type \nthe argument has to be duplicated, whereas in dePCFV this duplication of context is anticipated in the \ntyping rules for values. The readers who are familiar with linear logic, could have noted that if we \nreplace modal types by banged types (and we remove all annotations with indexes), then dePCFN corresponds \nto the target fragment of the CB N translation from simply-typed .-calculus to LL, and dePCFV to the \ntarget of the CB V translation [25]. In dePCFN, the weight K of a typing judgement represents the maximal \nnumber of substitutions that may occur in the C BN evaluation of t. We do not detail the typing rules \nof dePCFN here (they can be found in [11]). However, an important remark is that in dePCFN, just like \nin dePCFV, some semantic judgements can be found in the axioms of a typing derivation, and every typing \nrule is reversible (except subsumption). The type inference algorithm for dePCFV that we present in Section \n4 can be easily adapted to dePCFN.  3.3 Abstract Machines The evaluation of PCF terms can be simulated \nthrough an exten\u00adsion KAMPCF of Krivine s abstract machine [24] (for CB N evalua\u00adtion) or through an \nextension CEKPCF of Felleisen and Friedman s CEK machine [16] (for CB V evaluation). Both these machines \nhave states in the form of processes, that are pairs of a closure (i.e. a term with an environment de.ning \nits free variables) and a stack, representing the evaluation context. In the KAMPCF, these objects are \ngiven by the following grammar: Closures: c : x t ; . y; Environment: . : tx1 \u00de\u00d1 c1; \u00a8 \u00a8 \u00a8 ; xk \u00de\u00d1 cku; \nStacks: p : . | argx t ; . y \u00a8 p | s \u00a8 p | p \u00a8 p | fork xt ; u ; .y \u00a8 p; Processes: P : c p. When the \nenvironment is empty, we may use the notation x t y in\u00adstead of x t ; H y for closures. The evaluation \nrules of the KAMPCF are given in Figure 5. The fourth evaluation rule is said to be an in\u00adstantiation \nstep: the value of a variable x is replaced by the term x maps to in the underlying environment .. The \nCEKPCF, which performs CB V evaluation, is slightly more complex: within closures, the value closures \nare those whose .rst  f; F; G $E t : s f; F $E . D G f; F $E s D t f; F (E I d J pAxq I pSubsq f; F; \nG, x : s $E 0 x : s f; F; . $E J t : t pa, fq; pa a I, Fq; G, x : s $E K t : t f; F; G $E f; F; . $E \n K t : ra a 1s \u00a8 s -t H u : st0{au r p-q pAppq f; F; G $E r .x.t : ra a Is \u00a8 s -t f; F; G Z . $E tu \n: tt0{au aaI I`K K`H aaI f; F; G $E M t : NatrJ, Ks f; pJ d 0, Fq; . $E f; pK e 1, Fq; . $E N u : t \nN s : t pIfq f; F; G Z . $E ifz t then u else s : t M`N f; F; G $E M t : NatrI, Js f; F; G $E M t : \nNatrI, Js pnq psq ppq f; F; G $E n : Natrn, ns f; F; G $E M sptq : NatrI ` 1, J ` 1s f; F; G $E M pptq \n: NatrI \u00b4 1, J \u00b4 1s 0 \u00cfb`1,a pb, fq; pb a H, Fq; G, x : ra a Is \u00a8 A $E t : ra a 1s \u00a8 B pa, b, fq; pa \na I, b a H, Fq $E Bt0{aut I ` b ` 1{bu D A b rJ \u00cf0,a pFixqf; F; G $E r fix x.t : ra a Ks \u00a8 Bt0{aut I{bu \nbaH H` baH J b \u00cf0,K (where H I) b Figure 4. Typing rules of dePCFV. component is a value: v : x v ; \n. y (remember that a value v is of the form n, .x.t or fix x.t). Moreover, environments assign only value \nclosures to variables: . : tx1 \u00de\u00d1 v1; \u00a8 \u00a8 \u00a8 ; xk \u00de\u00d1 vku. The grammar for stacks is the same, with one \nadditional construc\u00adtion (funpvq \u00a8 p) that is used to encapsulate a function (lambda ab\u00adstraction or \n.xpoint) while its argument is computed. Indeed, the latter cannot be substituted for a variable if it \nis not a value. Evalu\u00adation rules for processes are the same as the ones in Figure 5, except that the \nsecond and the third ones are replaced by the following: v argpcq \u00a8 p a c funpvq \u00a8 p v funx .x.t ; \n. y \u00a8 p a x t ; x \u00de\u00d1 v \u00a8 . y p v funx fix x.t ; . y \u00a8 p a x t ; x \u00de\u00d1 x fix x.t ; . y \u00a8 . y argpvq \n\u00a8 p An example of the evaluation of a term by the CEKPCF can be found in Figure 2. We say that a term \nt evaluates to u in an abstract machine when x t y . a x u ; . y .. Observe that if t is a closed term, \nthen u is necessarily a value. We write t N u whenever the KAMPCF \u00f3n evaluates t to u in exactly n steps, \nand t \u00f3n V u when the same holds for the CEKPCF (we may also omit the exponent n when the number of steps \nis not relevant). Abstract Machines and Weight. The weight of a typable term was informally presented \nas the number of instantiation steps in its evaluation. Abstract machines enable a more precise formulation \nof this idea: FACT 1. 1. If t \u00f3 N u, and $E t : A is derivable in dePCFN, I then [I]E is an upper bound \nfor the number of instantiation steps in the evaluation of t by the KAMPCF. 2. If t \u00f3 V u, and $E I t \n: ra a 1s \u00a8 A is derivable in dePCFV, then [I]E is an upper bound for the instantiation steps in the \nevaluation of t by the CEKPCF. This can be shown by extending the notion of weight and of typing judgement \nto stacks and processes [11, 13], and is the main ingre\u00ad dients for proving Intensional Soundness (see \nSection 3.4).  3.4 Key Properties In this section we brie.y recall the main properties of dePCF, arguing \nfor its relevance as a methodology for complexity analysis. We give the results for dePCFV, but they \nalso hold for dePCFN (all proofs can be found in [11, 13]). The Subject Reduction Property guarantees \nas usual that typing is correct with respect to term reduction, but speci.es also that the weight of \na term cannot increase along reduction: PRO P O S ITI O N 3.1 (Subject Reduction). For any PCF-terms \nt,u, if f; F; H $E I t : t is derivable in dePCFV, and if t \u00d1 u in CB V, then f; F; H $E J u : t is also \nderivable for some J such that f; F (E J d I. As a consequence, the weight does not tell us much about \nthe number of reduction steps bringing a (typable) term to its normal form. So-called Intensional Soundness, \non the other hand, allows to deduce some sensible information about the time complexity of evaluating \na typable PCF program by an abstract machine from its dePCF typing judgement. PRO P O S ITI O N 3.2 (Intensional \nSoundness). For any term t, if $E K t : NatrI, Js is derivable in dePCFV, then t evaluates to n in k \nsteps in the CEKPCF, with [I]E d n d [J]E and k d |t| \u00a8 p[K]E ` 1q . Intensional Soundness guarantees \nthat the evaluation of any pro\u00adgram typable in dePCF takes (at most) a number of steps directly proportional \nto both its syntactic size and its weight. A similar the\u00adorem holds when t has a functional type: if, \nas an example, the 1 type of t is Natras -NatrJs, then K is parametric on a and p|t| ` 2q \u00a8 p[K]E ` 1q \nis an upper bound on the complexity of evaluating t when fed with any integer a. But is dePCF powerful \nenough to type natural complexity bounded programs? Actually, it is as powerful as PCF itself, since \nany PCF type derivation can be turned into a dePCF one (for an expressive enough equational program), \nas formalised by the type inference algorithm (Section 4). We can make this statement even more precise \nfor terms of base or .rst order type, provided two conditions are satis.ed: On the one hand, the equational \nprogram E needs to be uni\u00adversal, meaning that every partial recursive function is repre\u00adsentable by \nsome index term. This can be guaranteed, as an example, by the presence of a universal program in E. \n On the other hand, all true statements in the form f; F (E I d J must be available in the type system \nfor completeness to hold. In other words, one cannot assume that those judgements are derived in a given \n(recursively enumerable) formal system, because this would violate G \u00a8odel s Incompleteness Theorem. \nIn fact, in dePCF completeness theorems are relative to an oracle for the truth of those assumptions, \nwhich is precisely what happens in Floyd-Hoare logics [9].   x tu ; . y p a x t ; . y argx u ; . \ny \u00a8 p x .x.t ; . y argpcq \u00a8 p a x t ; px \u00de\u00d1 cq \u00a8 . y p x fix x.t ; . y p a x t ; px \u00de\u00d1 x fix x.t ; \n. yq \u00a8 . y p x x ; . y p a .pxq p x ifz t then u else s ; . y p a x t ; . y fork xu ; s ; .y \u00a8 p \nx 0 ; .1 y fork xt ; u ; .y \u00a8 p a x t ; . y p x n `1 ; .1 y fork xt ; u ; .y \u00a8 p a x u ; . y p x \nsptq ; . y p a x t ; . y s \u00a8 p x pptq ; . y p a x t ; . y p \u00a8 p x n ; . y s \u00a8 p a x n`1 y p x n \n; . y p \u00a8 p a x n\u00b41 y p Figure 5. KAMPCF evaluation rules. PRO P O S IT IO N 3.3 (Relative Completeness). \nIf E is universal, then for any PCF term t, 1. if t \u00f3k V m, then $E t : Natrms is derivable in dePCFV. \nk 2. if, for any n P N, there exist kn, mn such that t n \u00f3kV n mn , then there exist I and J such that \na; H; H $E I t : rb a 1s \u00a8 pNatras -NatrJsq is derivable in dePCFV, with (E Jtn{au mn and (E Itn{au \nd kn for all n P N. The careful reader should have noticed that there is indeed a gap between the lower \nbound provided by completeness and the upper bound provided by soundness: this is indeed the reason why \nour complexity analysis is only meaningful in an asymptotic sense. Sometimes, however, programs with \nthe same asymptotic behavior can indeed be distinguished, e.g. when their size is small relative to the \nconstants in their weight. In the next section, we will see how to make a concrete use of Relative Completeness. \nIndeed, we will describe an algorithm that, given a PCF term, returns a dePCF judgement $E t : t for \nK this term, where E is equational program that is not universal, but expressive enough to derive the \ntyping judgement. To cope with the relative part of the result (i.e., the very strong assumption that \nevery true semantic judgement must be available), the algorithm also returns a set of side conditions \nthat have to be checked. These side conditions are in fact semantic judgements that act as axioms (of \ninstances of the subsumption rule) in the typing derivation. 4. Relative Type Inference Given on the \none hand soundness and relative completeness of dePCF, and on the other undecidability of complexity \nanalysis for PCF programs, one may wonder whether looking for a type infer\u00adence procedure makes sense \nat all. As stressed in the Introduction, we will not give a type inference algorithm per se, but rather \nreduce type inference to the problem of checking the validity of a set of in\u00adequalities modulo an equational \nprogram (see Figure 1). This is the reason why we can only claim type inference to be algorithmically \nsolvable in a relative sense, i.e. assuming the existence of an oracle for proof obligations. Why is \nsolving relative type inference useful? Suppose you have a program t : Nat \u00f1 Nat and you want to prove \nthat it works in a number of steps bounded by a polynomial p : N \u00d1 N (e.g., ppxq 4 \u00a8 x ` 7). You could \nof course proceed by building a dePCF type derivation for t by hand, or even reason directly on the complexity \nof t. Relative type inference simpli.es your life: it outputs an equational program E, a precise type \nderivation for t 1 whose conclusion is a; H; H $E I t : Natras -NatrJs and a set I of inequalities on \nthe same signature as the one of E. Your original problem, then, is reduced to verifying |\u00f9E I Y tI d \nppaqu. This is arguably an easier problem than the original one: .rst of all, it has nothing to do with \ncomplexity analysis but is rather a problem about the value of arithmetical expressions. Secondly it \nonly deals with .rst-order expressions. 4.1 An Informal Account From the brief discussion in Section \n2, it should be clear that devising a compositional type inference procedure for dePCF is nontrivial: \nthe type one assigns to a subterm heavily depends on the ways the rest of the program uses the subterm. \nThe solution we adopt here consists in allowing the algorithm to return partially unspeci.ed equational \nprograms: E as produced in output by T gives meaning to all the symbols in the output type derivation \nexcept those occurring in negative position in its conclusion. To better understand how the type inference \nalgorithm works, let us consider the following term t: uv p.x..y.xpxyqqp.z.spzqq. The subterm u can \nbe given type pNat \u00f1 Natq \u00f1 Nat \u00f1 Nat in PCF, while v has type Nat \u00f1 Nat. This means t as a whole has \ntype Nat \u00f1 Nat and computes the function x \u00de\u00d1 2 \u00a8 x. The type inference algorithm proceeds by giving \ntypes to u and to v separately, then assembling the two into one. Suppose we start with v. The type inference \nalgorithm re.nes Nat \u00f1 Nat into bahpaq s Natrfpa, bqs -Natrgpa, bqs and the equational program Av, which \ngives meaning to g in terms of f: gpa, bq fpa, bq ` 1. Observe how both f and h are not speci.ed in \nAv, because they appear in negative position in s: fpa, bq intuitively corresponds to the argument(s) \nv will be applied to, while hpaq is the number of times v will be used. Notice that everything is parametrised \non a, which is something like a global parameter that will later be set as the input to t. The function \nu, on the other hand, is given type cajpa,bq pNatrppa, b, cqs -Natrqpa, b, cqsq bakpaq - campa,bq Natrlpa, \nb, cqs -Natrnpa, b, cqs. The newly introduced function symbols are subject to the following equations: \njpa, bq 2 \u00a8 mpa, bq; npa, b, cq qpa, b, 2cq; ppa, b, 2cq qpa, b, 2c ` 1q; ppa, b, 2c ` 1q lpa, b, \n2cq. Again, notice that some functions are left unspeci.ed, namely l, m, q and k. Now, a type for uv \ncan be found by just combining the types for u and v, somehow following the typing rule for applications. \nFirst of all, the number of times u needs to be copied is set to 1 by the equation kpaq 1. Then, the \nmatching symbols of u and v are de.ned one in terms of the others:  qpa, 0, bq gpa, bq; fpa, bq ppa, \n0, bq; hpaq jpa, 0q. This is the last step of type inference, so it is safe to stipulate that mpa, 0q \n 1 and that lpa, 0, cq a, thus obtaining a fully speci.ed equational program E and the following type \nt for t: ca1 Natras -Natrnpa, 0, cqs. As an exercise, the reader can verify that the equational program \nabove allows to verify that npa, 0, 0q a ` 2, and that a; H; H $E 2 t : t.  4.2 Preliminaries Before \nembarking on the description of the type inference algo\u00adrithms, some preliminary concepts and ideas need \nto be introduced, and are the topic of this section. 4.2.1 Getting Rid of Subsumption The type inference \nalgorithm takes in input a PCF term t, and returns a typing judgement J for t, together with a set R \nof so\u00adcalled side conditions. We will show below that J is derivable iff all the side conditions in R \nare valid. Moreover, in this case J is precise (see De.nition 3.1): all occurrences of the base type \nNatrI, Js are in fact of the form NatrIs, and the weight and all potentials H occurring in a sub-type \nra a Hs \u00a8 A are kept as low as possible. Concretely, this means that there is a derivation for J in which \nthe subsumption rule is restricted to the following form: f; F $E . G f; F $E s t f; F; G $E t : s \nf; F (E I J I f; F; . $E J t : t The three premises on the right boil down to a set of semantic t ( \njudgements of the form f; F (E Ki Hi (see Figure 3), where the Ki s are indexes occurring in s or . \n(or I itself) and the Hi s occur in t or G (or are J itself). If the equalities Ki Hi can all be derived \nfrom E, then the three premises on the right are equivalent to the conjunction (on i) of the following \nproperties: [Hi]E . is de.ned for any . : f \u00d1 Nsatisfying F (see Section 3.1.2). Given E, this property \n(called a side condition) is denoted by f; F ( Hi \u00d3. Actually the type inference algorithm does not verify \nany semantic or subtyping judgement coming from (instances of) the subsumption rule. Instead, it turns \nall index equiv\u00adalences Hi Ki into rewriting rules in E, and put all side condi\u00adtions f; F ( Hi \u00d3 in \nR. If every side condition in R is true for E, Z we write E , R. Informally, this means that all subsumptions \nassumed by the algorithm are indeed valid. 4.2.2 Function Symbols Types and judgements manipulated by \nour type inference algorithm have a very peculiar shape. In particular, not every index term is allowed \nto appear in types, and this property will be crucial when showing soundness and completeness of the \nalgorithm itself: DE FI NI TI O N 4.1 (Primitive Types). A type is primitive for f when it is on the \nform Natrfpfqs, or A -B with A and B primitive for f, or ra a fpfqs \u00a8 A with a R f and A primitive for \na; f. A type is said to be primitive when it is primitive for some f. As an example, a primitive type \nfor f a; b is cagpa,bq Natrfpa, b, cqs -Natrhpa, b, cqs. Informally, then, a type is primitive when \nthe only allowed index terms are function symbols (with the appropriate arity).  4.2.3 Equational Programs \nThe equational program our algorithm constructs is in fact a rewrit\u00ading program: every equality corresponds \nto the (partial) de.nition of a function symbol, and we may write it fpa1, . . . , akq : J (where all \nfree variables of J are in ta1, . . . , aku). If there is no such equation in the rewriting program, \nwe say that f is unspeci\u00ad.ed. An equational program E is completely speci.ed if it allows to deduce a \nprecise meaning (namely a partial recursive function) for each symbol of its underlying signature (written \nSE ), i.e. none of the symbols in SE are unspeci.ed. In other words: a com\u00adpletely speci.ed equational \nprograms has only one model. On the other hand, a partially speci.ed equational program (i.e. a program \nwhere symbols can possibly be unspeci.ed) can have many models, because partial recursive functions can \nbe assigned to unspeci.ed function symbols in many different ways, all of them consistent with its equations. \nUp to now, we only worked with completely speci.ed programs, but allowing the possibility to have unspeci\u00ad.ed \nsymbols is crucial for being able to describe the type infer\u00adence algorithm in a simple way. In the following, \nE and F denote completely speci.ed equational programs, while A and B denote rewriting programs that \nare only partially speci.ed. DE FIN I T I ON 4.2 (Model of a Rewriting Program). An interpreta\u00adtion \u00b5 \nof A in E is simply a map from unspeci.ed symbols of A to indexes on the signature SE , such that if \nf has arity n, then \u00b5pfqis a term in SE with free variables from ff ta1, . . . , anu. When such an interpretation \nis de.ned, we say that E is a model of A, and we write \u00b5 : E |\u00f9 A. Notice that such an interpretation \ncan naturally be extended to arbitrary index terms on the signature SA, and we assume in the following \nthat a rewriting program and its model have disjoint signatures. DE FIN I T I ON 4.3 (Validity in a Model). \nGiven \u00b5 : E |\u00f9 A, we say that a semantic judgement f; F (A I d J is valid in the model (notation: f; \nF (\u00b5 I d J) when f; F (F I d J where F A Y E Y tfpff q : \u00b5pfq | f is unspeci.ed in Au. This Z de.nition \nis naturally extended to side conditions (with \u00b5 , R Z standing for F , R). Please note that if A is \na completely speci.ed rewriting program, then any model \u00b5 : E |\u00f9 A has an interpretation \u00b5 with an empty \nZ Z domain, and \u00b5 , R iff A , R (still assuming that SA and SE are disjoint). As already mentioned, the \nequational programs handled by our type inference algorithm are not necessarily completely speci.ed. \nFunction symbols which are not speci.ed are precisely those oc\u00adcurring in negative position in the judgement \nproduced in output. This invariant will be very useful and is captured by the following de.nition: DE \nFIN I T I ON 4.4 (Positive and Negative Symbols). Given a prim\u00aditive type t, the sets of its positive \nand negative symbols (denoted  by t ` and t \u00b4 respectively) are de.ned inductively by ear logic. Actually, \nthe best way to present the type inference algo\u00ad` rithm consists in .rst of all introducing four auxiliary \nalgorithms, Natripfqs tiu; each corresponding to a principle regulating the behaviour of the Natripfqs \n\u00b4 H; exponential connectives in linear logic. Notice that these auxiliary ` \u00b4 ` algorithms are the main \ningredients of both dePCFV and dePCFN ra a hpfqs \u00a8 ps -tq s Y t ; type inference. Consistently to what \nwe have done so far, we will \u00b4 `\u00b4 ra a hpfqs \u00a8 ps -t q ; thu Y s Y t . prove and explain them with \ndePCFV in mind. All the auxiliary algorithm we will talk about in this section will take a tuple of Then \nthe set of positive (resp. negative) symbols of a judge\u00ad dePCFV types as .rst argument; we assume that \nall of them have $E t : t is the union of all negative the same skeleton and, moreover, that all index \nterms appearing in (resp. positive) symbols of the si s and all positive (resp. negative) I ment f; F; \npxi : siqidn symbols of t . Polarities in t`, \u00b4u are indicated with symbols like p, q. Given such a p, \nthe opposite polarity is .p. DE FI NI TI O N 4.5 (Speci.ed Symbols, Types and Judgments). Given a set \nof function symbols S, a symbol f is said to be pS, Aq-speci.ed when there is a rule fpfq : J in A such \nthat any function symbol appearing in J is either f itself, or in S, or a symbol that is pS Y tfu, Aztfpfq \n: Juq-speci.ed. Remember that when there is no rule fpfq : J in A the symbol f is unspeci.ed in A. A \nprimitive type s is said to be pp, S, Aq-speci.ed when all function symbols in sp are pS, Aq-speci.ed \nand all symbols in s.p are unspeci.ed. A judgement f; F; G $A t : t is correctly I them are pairwise \ndistinct. Dereliction. Dereliction is the following principle: any duplica\u00adble object (say, of type !A) \ncan be made linear (of type A), that is to say !A \u00d1 A. In dePCF, being duplicable means having a modal \ntype, which also contains some quantitative information, namely how many times the object can be duplicated, \nat most. In dePCFV, dereliction can be simply seen as the principle ra a 1s \u00a8 A \u00d1 At0{au, and is implicitly \nused in the rules (App) and (Fix). Along the type inference process, as a consequence, we often need \nto cre\u00adate fresh instances of dereliction in the form of pairs of types being in the correct semantic \nrelation. This is indeed possible: LEM M A 4.1. There is an algorithm DE R such that given two types \nt (primitive for f) and s (primitive for a, f) of the same speci.ed when t and all types in G are primitive \nfor f, and t is skeleton, D ERpps, t q; a; f; F; pq pA, Rq where: Z 1. for every E E A, if E , p`, N \n, Aq-speci.ed, and all types in G are p\u00b4, N , Aq-speci.ed, and all function symbols in I are pN , Aq-speci.ed \nwhere N is the set of negative symbols of the judgement. In other words, a judgement is correctly speci.ed \nif the underlying equational program (possibly recursively) de.nes all symbols in positive position depending \non those in negative position.  4.3 The Structure of the Algorithm The type inference algorithm receives \nin input a PCF term t and Ejudgement H $returns a dPCFeKof side conditions . We will prove that it is \n, in the sense correctR t : t for it, together with a set that the typing judgement is derivable iff \nthe side conditions hold. R then f; F $E st0{au t ; 2. whenever f; F $E (t0{au . where pr(sq prssq, \nthere is \u00b5 : E |\u00f9 A such that f; F $\u00b5 st0{au (t0{au, Z f; F $\u00b5 t ., and \u00b5 , R; 3. s is pp, t p, Aq-speci.ed \nand t is p.p, s.p, Aq-speci.ed. The algorithm DER works by recursion on the PCF type prssq and has thus \nlinear complexity in |prssq|. The proof of Lemma 4.1 (see [12]), as a consequence, proceeds by induction \non the structure of prssq. Contraction. Another key principle in linear logic is contraction, according \nto which two copies of a duplicable object can actually be produced, !A \u00d1!Ab!A. Contraction is used in \nbinary rules like (App) or (If ), in the form of the operator Z. This time, we need an algorithm C T \nR which takes three linear types A, B and C (all of them primitive for pa, fq) and turn them into an \nequational program and a set of side conditions: C T RppA, B, C q; pI, Jq; a; f; F; pq pA, Rq. The parameters \nI and J are index terms capturing the number of times B and C can be copied. A Lemma akin to 4.1 can \nindeed be Z proved about C TR. In particular, for any E E A, if E , R then f; F $E ra a I ` Js \u00a8 A pra \na Is \u00a8 Dq Z pra a Js \u00a8 Eq (5) for some D and E such that f; F, a a I $E D B and f; F, a a J $E E C. \nDigging. In linear logic, any duplicable object having type !A can be turned into an object of type !!A, \nnamely an object which is the duplicable version of a duplicable object. Digging is the prin\u00adciple according \nto which this transformation is possible, namely !A \u00d1!!A. At the quantitative level, this corresponds \nto splitting a bounded sum into its summands. This is used in the typing rules for functions, (-) and \n(Fix). The auxiliary algorithm corresponding to the digging principle takes two linear types and builds, \nas usual, a rewriting program and a set of side conditions capturing the fact that the .rst of the two \ntypes is the bounded sum of the second: D I GppA, Bq; pI, Jq; f; pa, bq; F; pq pA, Rq. The algorithm \nproceeds as follows: 1. Compute dPCF, a PCF type derivation for t; 2. Proceeding by structural induction \non dPCF, construct a dePCF derivation for t (call it dv) and the corresponding set of side conditions \nR; 3. Returns R and the conclusion of dv. The skeleton prssq (or prAsq) of a modal type s (resp. of \na linear type A) is obtained by erasing all its indexes (and its bounds ra a Is). The skeleton of a dePCF \nderivation is obtained by replacing each type by its skeleton, and erasing all the subsumption rules. \nIn PCF the type inference problem is decidable, and Step 1 raises no dif.culty: actually, one could even \nassume that the type dPCF attributes to t is principal. The core of the algorithm is of course Step 2. \nIn Section. 4.5 we de.ne a recursive algorithm GE N that build dv and R by annotating dPCF. The algorithm \nG E N itself relies on some auxiliary algorithms, which will be described in Section 4.4 below.  All \nalgorithms we will talk about have the ability to generate fresh variables and function symbols. Strictly \nspeaking, then, they should take a counter (or anything similar) as a parameter, but we elide this for \nthe sake of simplicity. Also, we assume the existence of a function apf; T q that, given a set of index \nvariables f and a PCF type T , returns a modal type t primitive for f, containing only fresh function \nsymbols, and such that prt sq T .  4.4 Auxiliary Algorithms and Linear Logic The design of systems \nof linear dependent types such as dePCFV and dePCFN is strongly inspired by BLL, itself a restriction \nof lin\u00adThe correctness of D I G can again be proved similarly to what we did in Lemma 4.1, the key statement \nbeing that for every E E A  Z such that E , R, the following must hold \u00ff\u00ff f; F $E rb a Js \u00a8 A rb a \nJs \u00a8 C aaI aaI for some C such that f; F, a a I, b a J $E C B. Weakening. Weakening means that duplicable \nobjects can also be erased, even when the underlying index is 0. Weakening is useful in the rules (Ax) \nand (n). Once a fresh dePCFV type A is produced, the only thing we need to do is to produce an equational \nprogram A specifying (in an arbitrary way) the symbols in Ap, this way preserving the crucial invariants \nabout the equational programs manipulated by the algorithm. Formally, it means that there is an algorithm \nWEAK such that WE AKpA; f; a; pq A, where A is pp, H, Aq-speci.ed Observe how no sets of constraints \nis produced in output by W EA K, contrarily to DER, C T R, and DI G.  4.5 The Type Inference Procedure \nIn this section, we will describe the core of our type inference algo\u00adrithm. This consists in a recursive \nalgorithm G EN which decorates a PCF type derivation dPCF, producing in output a dePCF judge\u00adment, together \nwith an equational program and a set of side condi\u00adtions. In order to correctly create fresh symbols \nand to format side conditions properly, the main recursive function G E N also receives a set of index \nvariables f and a set of constraints F in input. Thus, it has the following signature: GENpf; F; dPCFq \n pG $I t : t ; A; Rq. We will prove that the the output of GE N satis.es the following two invariants: \n Decoration. dPCF has conclusion prGsq $ t : prtsq.  Polarity. f; F; G $A I t : t is correctly speci.ed \n(see De.ni\u00ad  tion 4.5). The algorithm G EN proceeds by inspecting dPCF in an inductive manner. It .rst \nannotates the types in the conclusion judgement with fresh function symbols to get a dePCF judgement \nJ . Then a recursive call is performed on the immediate sub-derivations of dPCF, this way obtaining some \ndePCF typing judgement Ji. Fi\u00adnally G E N generates, calling the auxiliary algorithms, the equations \non function symbols that allow to derive J from the Ji s, The equations are written in A, and the required \nassumptions of index convergence in R. Decoration and Polarity are the invariants of the algorithm G \nE N. In particular, the auxiliary algorithms are always called with the appropriate parameters, this \nway enforcing Polarity. The algorithm computing GE N proceeds by case analysis on dPCF. We give some \ncases here, the other ones are developed in [12]. Suppose that dPCF is y1 : U1, . . . , yk : Uk $ n : \nNat For each i, let Bi apf; Uiq and Ai WEAKpBi; f; bi; \u00b4q(where all the bi s are fresh). Let ipfq be \na fresh function symbol. Then return pG $0 n : Natripfqs; A; Hq where the hi s are fresh symbols and \nd A pAi Y thipfq : 0uq Y t ipfq : n u; G tyi : rbi a hipfqs \u00a8 Biu1didk. If dPCF is on the form ePCF \n1 : . $ t : U \u00f1 T ePCF 2 : . $ u : U . $ t u : T let pG1 $K t : ra a fpfqs \u00a8 ps1 -s2q; A1; R1q G ENpf; \nF; e 1 PCFq, and pG2 $H u : t; A2; R2q G ENpf; F; ePCF 2 q. Let pB, S q DERpps1, t q; a; f; F; `q. \nWe then annotate T : let t2 apf; T q, and let pC, U q D ERpps2, t2q; a; f; F; \u00b4q. Then we build a context \nequivalent to G1 Z G2: by the decoration property, G1 and G2 have the same skeleton ., so for any y : \nrby a iypfqs \u00a8 By in G1, there is some y : rby a jypfqs \u00a8 Cy in G2 (possibly after some a-conversion). \nThen let Ay appby, fq; prBysqq, and pAy; Ryq C T RppAy, By, Cyq; piypfq, jypfqq; by; f; F; \u00b4q. There \nare .i Gi (for i 1, 2) such that f; F $E ty : rby a T iypfq ` jypfqs \u00a8 Ayuy .1 Z .2, for every E E \nAy such y that E , p Ryq. Thus, let hy s be fresh symbols and Z T y return p. $K`H t u : t2; A; Rq in \noutput, where R R1 Y R2 Y S Y U d` . Y Ry Y tf; F ( iypfq ` jypfq \u00d3u; y A A1 Y A2 Y B Y C Y tfpfq : \n1u d` . Y Ay Y thypfq : iypfq ` jypfqu ; y . ty : rby a hypfqs \u00a8 Ayuy. Assume that dPCF is ePCF : ., \nx : U $ t : T . $ .x.t : U \u00f1 T Let a be a fresh index variable, and ipfq be a fresh func\u00adtion symbol, \nand compute pG, x : s $K t : t; B; S q G ENpePCF, pa, fq, pa a ipfq, Fqq. We build a context equiva\u00ad \nr lent to aaipfq G: for every y : rby a jypa, fqs \u00a8 By P G, let Ay appby, fq; prBysqq, let hypfq be \na fresh symbol, and let pAy, Ryq be D IGppAy, Byq; pipfq, jypa, fqq; f; pa, byq; F; \u00b4q. Then return p. \n$ipfq`r aaipfq K .x.t : ra a ipfqs \u00a8 ps -t q; A; Rq where d \u00ff R S Y pRy Y tf; F (E jypa, fq \u00d3uq; y aaipfq \n\u00ff A B Y YypAy Y thypfq : jypa, fquq; aaipfq . ty : rby a hypfqs \u00a8 Ayuy. LEM M A 4.2. For every f, F, \nand every PCF derivation dPCF, the output of G ENpf; F; dPCFq satis.es Decoration and Polarity.  4.6 \nCorrectness The algorithm we have just .nished describing needs to be proved sound and complete with \nrespect to dePCFV typing. As usual, this is not a trivial task. Moreover, linear dependent types have \na semantic nature which makes the task of formulating (if not proving) the desired results even more \nchallenging. 4.6.1 Soundness A type inference procedure is sound when the inferred type can actually \nbe derived by way of the type system at hand. As already remarked, G E N outputs an equational program \nA which possibly contains unspeci.ed symbols and which, as a consequence, cannot be exploited in typing. \nMoreover, the role of the set of proof obligations in R may be unclear at .rst. Actually, soundness holds \nfor every completely speci.ed E E A which makes the proof obligations in R true:  TH EO R E M 4.3 (Soundness). \nIf dPCF is a PCF derivation for t, then for any f and F, G E Npf; F; dPCFq A pG $I t : t ; A; Rq where \nf; F; G $ where j is a positive symbol while f and g are negative, thus un\u00adspeci.ed in A. A can be appropriately \ncompleted by adding the equations fpaq : 1 and gpa, bq : a to it. This way, we are insisting on the behaviour \nof t when fed with any natural number (represented by a) and when the environment needs t only once. \nHow about complexity analysis? Actually, we are already there: the problem of proving the number of machine \nreduction steps needed N \u00d1 N (where p is, e.g. a polynomial) t : t is correctly speci.ed and for any \nE E A, I by t to be at most p : l Z \u00f9\u00f1 f; F; G $ E I t : t is derivable and precise. becomes the problem \nof checking E , S where E is the E , R Soundness can be proved by induction on the structure of dPCF, \nexploiting auxiliary results like Lemma 4.1.  4.6.2 Completeness But are we sure that at least one type \nderivation can be built from the outcome of G E N if one such type derivation exists? Again, it is nontrivial \nto formulate the fact that this is actually the case. E appropriate completion of A, and S is R Y tpK \n` 1qp|t| ` 2q d ppaqu (Proposition 3.2). Simplifying Equations. Equational programs obtained in output \nfrom CheckBound contains many equations which are trivial (such as fpaq : n or fpaq : gpaq), and as such \ncan be eliminated. Moreover, instances of forest cardinalities and bounded sums can r sometime be greatly \nsimpli.ed. As an example, aa0 J can always be replaced by 0. This allows, in particular, to turn A into \na set of TH EO R E M 4.4 (Completeness). If f; F; . $ t : s is a precise fewer and simpler rules, thus \nfacilitating the next phase. A basic simpli.cation procedure has already been implemented, and is called \nby CheckBound on the output of GE N. However, J dePCFV judgement derivable by dv, then G ENpf; F; prdvsqq \nis of t : t ; A; Rq, and there is \u00b5 : E |\u00f9 A such that the form pG $I Z \u00b5 , R. automatically treating \nthe equational program by an appropriate Completeness can be proved by an induction on the structure \nof dv; its statement, however, needs to be appropriately enriched for induction to work. Again, results \nlike Lemma 4.1 greatly help here: Point 2 states completeness of D E R, and the latter is called by GE \nN many times. A direct consequence of Soundness and Completeness (and the remark on De.nition 4.3) is \nthe following: COROL L A RY 4.5. If a closed term t is typable in PCF with type Nat and a derivation \ndPCF, then G E NpH; H; dPCFq p$I t : Natrfs; E; Rq Z and t is typable in dePCFV iff E , R. 5. Type Inference \nat Work The type inference algorithm presented in the previous section has been implemented in OCA M \nL2. Programs, types, equational pro\u00ad grams and side conditions become values of appropriately de.ned \n A inductive data structures in OC A M L, while the functional nature of the latter makes the implementation \neffort easier. This section is devoted to discussing the main issues we have faced along the process, \nwhich is still ongoing. The core of our implementation is an OCA M L function called CheckBound. Taking \na closed term t having PCF type T in in\u00adput, CheckBound returns a typing derivation dv, an equational \npro\u00adgram E and a set of side conditions R. The conclusion of dv is a dePCF typing judgement for the input \nterm. If T is a .rst-order type, then the produced judgement is derivable iff all the side con\u00additions \nin R are valid. To do so, CheckBound calls (an implemen\u00adtation of) G E N on t and a context f consisting \nof n unconstrained index variables, where n is the arity of t. This way, CheckBound obtains A and R as \nresults, and then proceeds as follows: If T is Nat, then A is already completely speci.ed and Corol\u00adlary \n4.5 ensures that we already have what we need.  If T has a strictly positive arity, then some of the \nsymbols in A are unspeci.ed, and appropriate equations for them need to be added to A. Take for instance \na term s of type Nat \u00f1 Nat. CheckBound(s) returns A, R, and a typing judgement of the form  bafpaq K \nt : Natrgpa, bqs \u00ad a; H; H $ Natrjpa, bqs, prover would of course be desirable. For this purpose, the \npossi\u00adbility for CheckBound to interact with MAU D E [8], a system sup\u00ad porting equational and rewriting \nlogic speci.cation, is currently in\u00advestigated. Checking Side Conditions. As already stressed, once CheckBound \nhas produced a pair pA, Rq, the task we started from, namely com\u00adplexity analysis of t, is not .nished, \nyet: checking proof obligations in R is as undecidable as analysing the complexity of t directly, since \nmost of the obligations in R are termination statements any\u00adway. There is an important difference, however: \nstatements in R are written in a language (the .rst-order equational logic) which is more amenable to \nbe treated by already existing automatic and semi-automatic tools. Actually, the best method would be \nto .rst call as many existing automatic provers as possible on the set of side conditions, then asking \nthe programmer to check those which cannot be proved automatically by way of an interactive theorem prover. \nFor this purpose, we have implemented an algorithm translating a pair in the form pA, Rq into a WH Y3 \ntheory [6]. 6. Related Work Complexity analysis of higher-order programs has been the object of many \nstudies. We can for example mention the proposals for type systems for the .-calculus which have been \nshown to correspond in an extensional sense to, e.g. polynomial time computable functions. Many of them \ncan be seen as static analysis methodologies: once a program is assigned a type, an upper bound to its \ntime complexity is relatively easy to be synthesised. The problem with these systems, however, is that \nthey are usually very weak from an intentional point of view, since the class of typable programs is \nquite restricted compared to the class of all terms working within the prescribed resource bounds. More \npowerful static analysis methodologies can actually be de\u00advised. All of them, however, are either limited \nto very speci.c forms of resource bounds or to a peculiar form of higher-order functions or else they \ndo not get rid of higher-order as the underlying logic. Consider, as an example, one of the earliest \nwork in this direction, namely Sands s system of cost closures [27]: the class of programs that can be \nhandled includes the full lazy .-calculus, but the way complexity is reasoned about remains genuinely \nhigher-order, be\u00ad ing based on closures and contexts. In Benzinger s framework [5] 2 the source code \nis available at http://lideal.cs.unibo.it. higher-order programs are translated into higher-order equations, \n and the latter are turned into .rst-order ones; both steps, and in particular the second one, are not \ncompleteness-preserving. Re\u00adcent works on amortised resource analysis are either limited to .rst-order \nprograms [21] or to linear bounds [23]. A recent pro\u00ad posal by Amadio and R\u00b4 egis-Gianas [2] allows to \nreason on the the cost of higher-order functional programs by way of so-called cost\u00adannotations, being \nsure that the actual behaviour of compiled code somehow re.ects the annotation. The logic in which cost \nannota\u00adtions are written, however, is a higher-order Hoare logic. None of the proposed systems, on the \nother hand, are known to be (rela\u00adtively) complete in the sense we use here. Ghica s slot games [17] \nare maybe the work which is closest to ours, among the many in the literature. Slot Games are simply \nor\u00addinary games in the sense of game semantics, which are however instrumented so as to re.ect not only \nthe observable behaviour of (higher-order) programs, but also their performance. Indeed, slot games are \nfully abstract with respect to an operational theory of improvements due do Sands [28]: this can be seen \nas the coun\u00ad terpart of our relative completeness theorem. An aspect which has not been investigated \nmuch since Ghica s proposal is whether slot games provides a way to perform actual veri.cation of programs, \nmaybe via some form of model checking. As we have already men\u00adtioned, linear dependency can be seen as \na way to turn games and strategies into types, so one can see the present work also as an attempt to \nkeep programs and strategies closer to each other, this way facilitating veri.cation. Another recent \nwork which seems to be quite close to ours is Geometry of Synthesis, in particular when the latter takes \nthe form of type inference [18]. 7. Conclusions A type inference procedure for dePCF has been introduced \nwhich, given a PCF term, reduces the problem of .nding a type deriva\u00adtion for it to the one of solving \nproof obligations on an equational program, itself part of the output. Truth of the proof obligations \ncorrespond to termination of the underlying program. Any type derivation in dePCF comes equipped with \nan expression bound\u00ading the complexity of evaluating the underlying program. Notice\u00adably, proof obligations \nand the related equational program can be obtained in polynomial time in the size of the input PCF program. \nThe main contribution of this paper consists in having shown that linear dependency is not only a very \npowerful tool for the anal\u00adysis of higher-order functional programs, but is also a way to ef\u00adfectively \nand ef.ciently turn a complex problem (that of evaluating the time complexity of an higher-order program) \ninto a much easier one (that of checking a set of proof obligations for truth). Although, as explained \nin Section 5, experimental evaluation shows that proof obligations can potentially be handled by modern \ntools, much remains to be done about the technical aspects of turn\u00ading proof obligations into a form \nwhich is suitable to automatic or semi-automatic solving. Actually, many different tools could con\u00adceivably \nbe of help here, each of them requiring a speci.c input format. This implies, however, that the work \ndescribed in this pa\u00adper, although not providing a fully-.edged out-of-the-box method\u00adology, has the \nmerit of allowing to factor a complex non-well\u00adunderstood problem into a much-better-studied problem, \nnamely veri.cation of .rst-order inequalities on the natural numbers. References [1] Abramsky, S., Jagadeesan, \nR., Malacaria, P.: Full abstraction for PCF. I &#38; C 163(2), 409 470 (2000) [2] Amadio, R.M., R\u00b4egis-Gianas, \nY.: Certifying and reasoning on cost annotations of functional programs. CoRR abs/1110.2350 (2011) [3] \nde Bakker, J.W.: Mathematical Theory of Program Correctness. Prentice-Hall (1980) [4] Barthe, G., Gr \n\u00b4egoire, B., Riba, C.: Type-based termination with sized products. In: CSL 2008. LNCS, vol. 5213, pp. \n493 507. Springer (2008) [5] Benzinger, R.: Automated higher-order complexity analysis. Theor. Comput. \nSci. 318(1-2), 79 103 (2004) [6] Bobot, F., Filliatre, J.C., March \u00b4e, C., Paskevich, A.: Why3: Shepherd \nyour herd of provers. In: First International Workshop on Intermediate Veri.cation Languages. pp. 53 \n64 (2011) [7] Clarke, E.M.: Programming language constructs for which it is im\u00adpossible to obtain good \nhoare axiom systems. J. ACM 26(1), 129 147 (1979) [8] Clavel, M., Dur \u00b4an, F., Eker, S., Lincoln, P., \nMart\u00b4i-Oliet, N., Meseguer, J., Talcott, C.: The Maude 2.0 system. In: RTA 2003. LNCS, vol. 2706, pp. \n76 87 (2003) [9] Cook, S.A.: Soundness and completeness of an axiom system for program veri.cation. SIAM \nJ. on Computing 7, 70 90 (1978) [10] Dal Lago, U.: Context semantics, linear logic and computational \ncom\u00adplexity. In: LICS 2006. pp. 169 178 (2006) [11] Dal Lago, U., Gaboardi, M.: Linear dependent types \nand relative completeness. In: LICS 2011. pp. 133 142 (2011) [12] Dal Lago, U., Petit, B.: The geometry \nof types (long version) (2012), available at http://arxiv.org/abs/1210.6857 [13] Dal Lago, U., Petit, \nB.: Linear dependent types in a call-by-value scenario. In: ACM PPDP 2012. pp. 115 126 (2012) [14] Danos, \nV., Regnier, L.: Reversible, irreversible and optimal lambda\u00admachines. Theor. Comput. Sci. 227(1-2), \n79 97 (1999) [15] Denney, E.: Re.nement types for speci.cation. In: IFIP-PROCOMET. pp. 148 166 (1998) \n[16] Felleisen, M., Friedman, D.P.: Control operators, the SECD-machine and the .-calculus. Tech. Rep. \n197, Computer Science Department, Indiana University (1986) [17] Ghica, D.R.: Slot games: a quantitative \nmodel of computation. In: ACM POPL 2005. pp. 85 97 (2005) [18] Ghica, D.R., Smith, A.: Geometry of synthesis \nIII: resource man\u00adagement through type inference. In: ACM POPL 2011. pp. 345 356 (2011) [19] Girard, \nJ.Y., Scedrov, A., Scott, P.: Bounded linear logic. Theor. Comp. Sci. 97(1), 1 66 (1992) [20] Gulwani, \nS.: Speed: Symbolic complexity bound analysis. In: CAV. pp. 51 62 (2009) [21] Hoffmann, J., Aehlig, K., \nHofmann, M.: Multivariate Amortized Re\u00adsource Analysis. In: ACM POPL 2011. pp. 357 370 (2011) [22] Hughes, \nJ., Pareto, L., Sabry, A.: Proving the correctness of reactive systems using sized types. In: ACM POPL \n1996. pp. 410 423 (1996) [23] Jost, S., Hammond, K., Loidl, H.W., Hofmann, M.: Static determi\u00adnation \nof quantitative resource usage for higher-order programs. In: ACM POPL 2010. Madrid, Spain (2010) [24] \nKrivine, J.L.: A call-by-name lambda-calculus machine. Higher-Order and Symbolic Computation 20(3), 199 \n207 (2007) [25] Maraist, J., Odersky, M., Turner, D.N., Wadler, P.: Call-by-name, call\u00adby-value, call-by-need \nand the linear lambda calculus. Electr. Notes Theor. Comput. Sci. 1, 370 392 (1995) [26] Plotkin, G.D.: \nLCF considerd as a programming language. Theor. Comp. Sci. 5, 225 255 (1977) [27] Sands, D.: Complexity \nanalysis for a lazy higher-order language. In: ESOP 1990. LNCS, vol. 432, pp. 361 376 (1990) [28] Sands, \nD.: Operational theories of improvement in functional lan\u00adguages (extended abstract). In: Functional \nProgramming. pp. 298 311 (1991) [29] Wilhelm, R., Engblom, J., Ermedahl, A., Holsti, N., Thesing, S., \nWhal\u00adley, D., Bernat, G., Ferdinand, C., Heckmann, R., Mitra, T., Mueller, F., Puaut, I., Puschner, P., \nStaschulat, J., Stenstrom, P.: The worst case execution time problem -overview of methods and survey \nof tools. ACM Trans. Embed. Comput. Syst. (2008)    \n\t\t\t", "proc_id": "2429069", "abstract": "<p>We show that time complexity analysis of higher-order functional programs can be effectively reduced to an arguably simpler (although computationally equivalent) verification problem, namely checking first-order inequalities for validity. This is done by giving an efficient inference algorithm for linear dependent types which, given a PCF term, produces in output both a linear dependent type and a cost expression for the term, together with a set of proof obligations. Actually, the output type judgement is derivable iff all proof obligations are valid. This, coupled with the already known relative completeness of linear dependent types, ensures that no information is lost, i.e., that there are no false positives or negatives. Moreover, the procedure reflects the difficulty of the original problem: simple PCF terms give rise to sets of proof obligations which are easy to solve. The latter can then be put in a format suitable for automatic or semi-automatic verification by external solvers. Ongoing experimental evaluation has produced encouraging results, which are briefly presented in the paper.</p>", "authors": [{"name": "Ugo Dal lago", "author_profile_id": "81100620235", "affiliation": "Universit&#224; di Bologna &#38; INRIA, Bologna, Italy", "person_id": "P3977942", "email_address": "dallago@cs.unibo.it", "orcid_id": ""}, {"name": "Barbara Petit", "author_profile_id": "81548027963", "affiliation": "Universit&#224; di Bologna &#38; INRIA, Bologna, Italy", "person_id": "P3977943", "email_address": "petit@cs.unibo.it", "orcid_id": ""}], "doi_number": "10.1145/2429069.2429090", "year": "2013", "article_id": "2429090", "conference": "POPL", "title": "The geometry of types", "url": "http://dl.acm.org/citation.cfm?id=2429090"}