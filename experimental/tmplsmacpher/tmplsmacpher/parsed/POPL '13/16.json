{"article_publication_date": "01-23-2013", "fulltext": "\n The Power of Parameterization in Coinductive Proof Chung-Kil Hur Georg Neis Derek Dreyer Viktor Vafeiadis \nMicrosoft Research MPI-SWS &#38; Saarland University MPI-SWS MPI-SWS gil@microsoft.com neis@mpi-sws.org \ndreyer@mpi-sws.org viktor@mpi-sws.org Abstract Coinduction is one of the most basic concepts in computer \nscience. It is therefore surprising that the commonly-known lattice-theoretic accounts of the principles \nunderlying coinductive proofs are lack\u00ading in two key respects: they do not support compositional reason\u00ading \n(i.e., breaking proofs into separate pieces that can be developed in isolation), and they do not support \nincremental reasoning (i.e., developing proofs interactively by starting from the goal and gen\u00aderalizing \nthe coinduction hypothesis repeatedly as necessary). In this paper, we show how to support coinductive \nproofs that are both compositional and incremental, using a dead simple con\u00adstruction we call the parameterized \ngreatest .xed point. The basic idea is to parameterize the greatest .xed point of interest over the accumulated \nknowledge of the proof so far . While this idea has been proposed before, by Winskel in 1989 and by Moss \nin 2001, neither of the previous accounts suggests its general applicability to improving the state of \nthe art in interactive coinductive proof. In addition to presenting the lattice-theoretic foundations \nof pa\u00adrameterized coinduction, demonstrating its utility on representative examples, and studying its \ncomposition with up-to techniques, we also explore its mechanization in proof assistants like Coq and \nIsabelle. Unlike traditional approaches to mechanizing coin\u00adduction (e.g., Coq s cofix), which employ \nsyntactic guardedness checking , parameterized coinduction offers a semantic account of guardedness. \nThis leads to faster and more robust proof develop\u00adment, as we demonstrate using our new Coq library, \nPaco. Categories and Subject Descriptors D.3.1 [Programming Lan\u00adguages]: Formal De.nitions and Theory; \nF.4.1 [Mathematical Logic and Formal Languages]: Mathematical Logic Keywords Coinduction, simulation, \nparameterized greatest .xed point, compositionality, lattice theory, interactive theorem proving 1. Introduction \nCoinduction is one of the most basic concepts in computer sci\u00adence. Coinductive proofs, especially those \nbased on simulation ar\u00adguments, are relevant in many settings where one wishes to model in.nitary properties \nor recursive behaviors [13, 18]. It is therefore surprising that the commonly-known lattice-theoretic \naccounts of This research was carried out primarily while the .rst author was a post\u00addoctoral researcher \nat MPI-SWS. The second author is currently funded by a Google European Doctoral Fellowship. Permission \nto make digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior speci.c permission and/or a fee. POPL 13, January \n23 25, 2013, Rome, Italy. Copyright c &#38;#169; 2013 ACM 978-1-4503-1832-7/13/01. . . $10.00 the principles \nunderlying coinductive proofs are lacking in two key respects: they do not support compositional reasoning \n(i.e., break\u00ading proofs into separate pieces that can be developed in isolation), and they do not support \nincremental reasoning (i.e., developing proofs interactively by starting from the goal and generalizing \nthe coinduction hypothesis repeatedly as necessary). The Trouble with Compositionality. Consider, for \ninstance, two possibly non-terminating, mutually recursive but loosely coupled functions, f and g, which \nwe would like to show equivalent to another pair of recursive functions, f' and g'. Ideally, we should \nbe able to reason about each function separately. That is, we should be able to prove that f . f' entails \ng . g', and similarly that g . g' entails f . f', and somehow derive from those ' two entailments that \n(f, g) . (f', g). The problem, however, is that this type of circular reasoning (aka rely-guarantee -style \nreasoning) is unsound in general. For example, if f = g and f' = g', then we would be able to derive \nf . f' from a tautology. One way to avoid this kind of unsound circularity is by placing a syntactic \nguardedness restriction on the proofs of each of the entailments, ensuring that their hypotheses are \nonly used after the de.nitions of the functions in their conclusions have been unfolded. This is the \napproach taken, for instance, by Coq s cofix tactic for coinduction [3]. Unfortunately, the limitation \nof using a syntactic criterion is that it is inherently non-compositional: it requires one to have access \nto the proof of each of the component entailments in order to determine whether the whole coinductive \nargument is valid. Moreover, as we explain in more detail in Section 7.3, the syntactic nature of guardedness \nchecking makes proof checking severely inef.cient and interacts poorly with other tactics. What we would \nreally like instead is a more semantic account of guardedness checking, by which the guardedness condition \nis re.ected directly in the statement of the entailment being proved, rather than being relegated to \na syntactic property of the proof itself. The Desire for Incrementality. Consider the transition system \nshown in Figure 1, and suppose we want to show that there is an in.nite path starting from node a. Let \nus .rst try to imitate a model checker and explore all paths starting from a in a depth-.rst fashion. \nFor the .rst step, there is only one choice: the edge a . b. Then at b, we have two choices. Perhaps \nwe can try b . c, but this will soon lead to a dead end, at which point we will have to backtrack. So \nlet us follow the edge b . d instead. Then, we follow d . b and we are back to a node we have already \nvisited. We have discovered a cycle, and thus an in.nite path, reachable from a. Now let us try to do \nthe same proof formally. The set of nodes from which in.nite paths emanate can be de.ned as the greatest \ndef .xed point inf = . step of the following monotone function: def step(X) = {x . Node | .y . X. x . \ny} Our goal then is to show that a . inf . Of course, in this small example, we could just compute inf \ndirectly by iteration and then Node(=x) Gstep({x})  a {a, b, d} b {a, b, d} c {a, b, d, e} d {a, b, \nd} e {a, b, d} f {a, b, c, d, e} Figure 1. A simple transition system and tabulation of Gstep. check \nif a is in it, but suppose we do not want to do this because in practice the transition system may be \nhuge or even in.nite. Instead, we may employ Tarski s .xed-point theorem [22], which says that, to show \na . inf , it suf.ces to .nd a set of nodes X such that a . X and .x . X. .y . X. x . y. For the given \ntransition system, a possible such set is X = {a, b, d}, which cor\u00adresponds to the set of nodes that \nwe followed to exhibit the cycle earlier. The problem, however, is that this proof is rather different \nfrom the model checking one, and actually more dif.cult because it forces us to .gure out what X is up \nfront. What we would re\u00adally like is a way to prove our goal by incrementally expanding the coinduction \nhypothesis from {a} to {a, b} to {a, b, d} as we ex\u00adplore the transition system and see what nodes are \nreachable from a. The validity of such an approach is intuitively obvious, but what is the general lattice-theoretic \nproof principle that justi.es it? Contributions. In this paper, we show how to support coinductive proofs \nthat are both compositional and incremental, using a dead simple construction we call the parameterized \ngreatest .xed point. The basic idea is to parameterize the greatest .xed point of interest over the accumulated \nknowledge of the proof so far . Neither the idea nor the construction behind it is an origi\u00adnal invention \nof ours per se. In 1989, Winskel [23] proposed the same idea for supporting local model checking in the \nmodal \u00b5\u00adcalculus. (His construction, which is slightly different from ours, supports incrementality but \nnot compositionality in our sense of the word but it is straightforward to repurpose his core reduc\u00adtion \nlemma to derive a compositional version of his construction.) Independently, in 2001, Moss [14] presented \na construction that is essentially the same as ours, albeit in a more abstract categori\u00adcal setting. \nHowever, neither of these prior accounts suggests the general applicability of the parameterized greatest \n.xed point to improving the state of the art in interactive coinductive proof. Our goal in the present \npaper is to popularize the idea of param\u00adeterized coinduction and explore its potential as a practically \nuseful tool. More speci.cally, we make the following contributions: We present the parameterized greatest \n.xed point in simple lattice-theoretic terms, and show that it validates several useful principles for \ncompositional, incremental proofs (Section 2). We give representative examples to illustrate the utility \nof these proof principles (Sections 2 and 3).  We show how parameterized coinduction is complementary \nto the traditional approach to simplifying simulation proofs via up-to techniques, and we develop the \nbasic theory of how these approaches compose (Section 4).  We explore the issues that arise in the mechanization \nof parame\u00adterized coinduction in existing interactive theorem provers like Coq and Isabelle (Section \n5). Fortunately, several of these is\u00adsues can be resolved through variations on a somewhat esoteric technique \ncalled Mendler-style recursion (Section 6).  We describe Paco (pronounced pah-ko ), a new Coq library \nwe have developed for parameterized coinduction. Compared to Coq s existing cofix tactic, Paco enables \nfaster and more robust proof development, thanks to its support for semantic, rather than syntactic, \nguardedness checking (Section 7).  Finally, we conclude the paper in Section 8 with a detailed discus\u00adsion \nof related work. The technical development of this paper has been formalized in the Coq proof assistant. \nThat formalization, together with a tutorial for our Coq library, Paco, is available from the Paco website: \nhttp://plv.mpi-sws.org/paco/ 2. Parameterized Coinduction Let us begin by reviewing the basic lattice \ntheory underlying coin\u00adductive de.nitions and their associated standard proof principles. Consider a \ncomplete lattice (C, g, n, U, T, .), and a monotone mon (i.e., order-preserving) function f . C -. C. \nStrictly speaking, for generality, we do not require g to be antisymmetric, and we write = for the intersection \nof g and ; (its inverse), which corre\u00adsponds to = if we have antisymmetry. We say that r is a pre.xed \npoint of f if f(r) g r, and r is a post.xed point of f if r g f(r). Further, we write \u00b5f for f s least \n.xed point and . f for its greatest .xed point, which by Tarski s .xed-point theorem [22] is equal to \nthe join of all post.xed points of f: .f = {r . C | r g f(r)} Tarski s Principle. We are concerned with \nproving statements of the form x g . f . From Tarski s theorem we directly get that post.xed points are \nincluded in the greatest .xed point: x g f (x) =. x g .f (TAR S K I) To prove that x g . f when x g f(x) \nusing this principle, we have to determine a post.xed point of f larger than x up front: x g .f .. .r. \nx g r . r g f(r) This is clearly inconvenient for doing interactive proofs, as it forces one to construct \nthe coinduction hypothesis r up front, instead of allowing r to be generated naturally in the course \nof the proof. Re\u00adcall that in the example of the introduction, although we were only interested in showing \nthat a . . step, we had to pick r := {a, b, d}up front. In large proofs, this quickly becomes a big problem. \nFor example, the ClightTSO-Csharpminor simulation proof in the CompCertTSO veri.ed compiler [20] requires \na simulation relation r that comprises 69 cases, most of which tediously relate interme\u00addiate execution \nstates. Strong Coinduction. Second, there is a slight variant of (TAR S K I), sometimes called the strong \ncoinduction principle [4]: Lemma 1 (Strong coinduction). x g . f .. x g f (x U . f ). Proof. First, we \nhave . f = f(. f ) g f(x U . f ) ( ). The (=.) direction follows directly from ( ). For the (.=) direction: \nfrom x g f(x U . f ) and ( ), we get x U . f g f(x U .f ), i.e., x U . f is a post.xed point of f. So, \nfrom (TA R S KI), x g x U . f g .f . This principle is strong in the sense that it is complete, but it \nstill does not offer us a very useful interactive proof technique. The problem arises if in the course \nof proving that x g f(x U .f ), we ever need to generalize the coinduction hypothesis by adding some \ny to it. The only recourse the strong coinduction principle gives us at this point (if we want to continue \ninteractively with the proof) is to show that y g . f . But of course the proof of that may cycle around, \nforcing us to prove that x g . f , in which case we are stuck. We are therefore forced to restart the \nproof, generalizing the coinduction hypothesis to x U y, i.e., showing that x U y g . f . Parameterized \nCoinduction. Our parameterized coinduction prin\u00adciple gives us a way to avoid restarting the proof by \nmaking explicit the idea of accumulated knowledge. In the course of the proof, we remember the things \nthat we have already claimed are g . f and we can treat those as assumed knowledge in guarded subproofs \n(where guardedness is enforced semantically, not syntactically).  Formally, the idea is that instead \nof dealing with .f directly, we deal instead with some Gf that is parameterized by accumulated mon knowledge. \nThat is, Gf . C -. C, and intuitively, Gf (x) represents our goal of proving that something is in . f \n, under accumulated (or assumed) knowledge x. De.nition 1 (Parameterized greatest .xed point). monmonmon \nWe de.ne G . (C -. C) -. (C -. C): def Gf (x) = . y. f (x U y) Here, and elsewhere, we write .y.a for \n.(.y.a). Note that the .xed point in the de.nition exists because .y. f (x U y) is monotone for monotone \nf. The monotonicity of G and Gf is easy to check. One way of understanding G is pictorially. In the transition \nsystem from the introduction, Gstep(X) describes the set of nodes that either have an in.nite path (i.e., \nare in . step), or else have a non-empty path leading to a node in X. 1 To illustrate, Figure 1 lists \nthe set Gstep({x}) for each node x. Incrementality of Parameterized Coinduction. We begin with the trivial \nobservation that the parameterized greatest .xed point coincides with the standard one if no knowledge \nhas been accumu\u00adlated. Lemma 2 (Initialize). . f = Gf (.). Further, by simply unfolding the .xed point, \nwe obtain the fol\u00adlowing analogue of the strong coinduction principle mentioned pre\u00adviously (Lemma 1), \nexcept that this version can be stated directly as an equality on the parameterized greatest .xed point: \nLemma 3 (Unfold). Gf (x) = f(x U Gf (x)). What Gf allows us to do in addition, that .f does not support, \nis to accumulate knowledge in the following sense: Theorem 4 (Accumulate). y g Gf (x) .. y g Gf (x U \ny). Proof. The (=.) direction follows straight from the monotonicity of Gf . In the (.=) direction, assume \ny g Gf (x U y) (*). Then, Gf (x U y) = f(x U y U Gf (x U y)) .xed point equation g f(x U Gf (x U y)) \nf monotone and (*) = (.z. f (x U z)) (Gf (x U y)) Therefore, as Gf (x U y) is a post.xed point of .y.f \n(x U y), we obtain from (TA R S K I) that Gf (x U y) g Gf (x), which together with (*) entails y g Gf \n(x), as required. Compositionality of Parameterized Coinduction. Our construc\u00adtion also admits a clean \ncompositional rule for combining proofs in the circular rely-guarantee style [8]: g1 g Gf (r1) r1 g r \nU g2 g2 g Gf (r2) r2 g r U g1 (CO M P OSE) g1 U g2 g Gf (r) The rule says that we can prove g1 and g2 \nare correct under assumptions r by proving that g1 is correct under the additional assumption that g2 \nis correct and similarly that g2 is correct under the additional assumption that g1 is. In essence, this \nrule is sound because Gf (r) allows the use of the assumptions r only within a guarded context. 1 The \nfact that the paths to nodes in x must be non-empty is what ensures that x can be used as an assumption \nonly within guarded contexts. Although we have motivated the desire for compositionality separately from \nthe desire for incrementality, it turns out that this (CO M P O SE) rule is equivalent to the accumulation \ntheorem (The\u00adorem 4), which we have already proved, and in fact the two prin\u00adciples are interderivable \nunder no assumptions about how Gf is de.ned (aside from the fact that it is monotone). To see this, let \nus .rst derive (CO M P OSE) from Theorem 4. As\u00adsuming the premises of (COMP O S E) hold, it is clear \nby monotonic\u00adity of Gf that g1 g Gf (r U g1 U g2) and g2 g Gf (r U g1 U g2), so g1 U g2 g Gf (r Ug1 Ug2). \nBy Theorem 4, g1 Ug2 g Gf (r) as de\u00adsired. Conversely, we can derive Theorem 4 from (CO M POS E). As \nbefore, the (=.) direction follows straight from the monotonicity of Gf . In the (.=) direction, assume \ny g Gf (xUy). Then, instan\u00adtiate (COMPO S E) with g1 = g2 = y, r = x, and r1 = r2 = x U y. The conclusion \nyields y = y U y g Gf (x) as desired. A Simple Application of Parameterized Coinduction. We now return \nto the model checking example presented in the intro\u00adduction, and show how to use parameterized coinduction \nto prove a . inf incrementally. Here, we instantiate the principle with the powerset lattice on states \nof the transition system. Thus, showing x . S is equivalent to showing {x} g S in this lattice. a . inf \n= . step .. a . Gstep(\u00d8) initialize .. .y . Gstep(\u00d8). a . y unfold .= b . Gstep(\u00d8) pick y := b .. b . \nGstep({b}) accumulate .. .y . {b} . Gstep({b}). b . y unfold .= d . {b} . Gstep({b}) pick y := d .. d \n. Gstep({b}) since d = b .. .y . {b} . Gstep({b}). d . y unfold .= b . {b} . Gstep({b}) pick y := b As \nyou can see, we can perform the same incremental proof as when model checking the example. Note that \nin the proof we do not necessarily have to accumulate the visited nodes at every step, but rather only \nwhen we think that their addition to the accumulated knowledge will be useful in the remainder of the \nproof. With the same example, we can also illustrate a simple use of compositionality. We can easily \nestablish: b . Gstep({d}) by unfolding &#38; picking y := d d . Gstep({b}) by unfolding &#38; picking \ny := b Thus, by (CO M P O S E), {b, d} . Gstep(\u00d8) = inf . Full Characterization of Gf . Finally, we observe \nthat Lemma 3 and Theorem 4 uniquely determine Gf up to =. Proposition 5. For any G ' such that (i) .x. \nG ' (x) = f (xUG ' (x)) and (ii) .x, y. y g G ' (xUy) =. y g G ' (x), we have G ' = Gf . Proof. Given \nx, G ' (x) g Gf (x) follows by (TAR S K I) from as\u00adsumption (i). To show Gf (x) g G ' (x), it suf.ces \nby (ii) to show Gf (x) g G ' (x U Gf (x)), which, after unfolding on the left and rewriting using (i) \non the right, follows by monotonicity of f. 3. A Simulation Example In this section, we illustrate parameterized \ncoinduction on a slightly larger example that demonstrates the practical motivation for a compositional, \nincremental coinduction principle. Consider the two recursive programs f and g shown in Figure 2. These \nprograms continually poll the user for a new (numerical) input, compute the double of the sum of all \ninputs seen so far,  def restart = .x restart(f). .n. if n > 0 then (output n; restart f (n - 1)) else \nf 0 def f = .x f (n). let v = (output n; input()) * 2 in (if v = 0 then f else restart f ) (v + n) def \ng = .x g(m). output (2 * m); let v = input() in if v = 0 then restart g (2 * m) else g (v + m) Figure \n2. Recursive programs in the simulation example. v ::= n | .x f(x). e . ::= + | - | * | = |= | > | = \ne ::= x | v | e1 . e2 | e1 e2 | if e0 then e1 else e2 | input() | output e K ::= e . | . v | e | \nv | if then e1 else e2 | output q ::= t | in n | out n def .x. e = .x f(x). e where f /. fv(e) def \nlet x = e1 in e2 = (.x. e2) e1 def e1; e2 = let x = e1 in e2 where x /. fv(e2) n1 . n2 . n1 . n2 if n \nthen e1 else e2 . e1 where n = 0 if 0 then e1 else e2 . e2 (.x f(x). e) v . e[(.x f(x). e)/f, v/x] in \nn input() --. n out n output n ---. 0 q q ' K[e] -. K[e ' ] where e -. e Figure 3. Syntax and semantics \nof a tiny programming language. and report the current value of the sum. If at any point, the user provides \nzero as an input, then the programs count down to zero, and start again. The two programs differ in the \nrepresentation of the double of the sum, as well as in their programming style (as the programmer responsible \nfor f followed a somewhat peculiar coding style). Nevertheless, it should be relatively straightforward \nto see that g(m) is equivalent to f(2m). Formally, these programs are written in the minimal program\u00adming \nlanguage de.ned in Figure 3. This is a standard call-by-value .-calculus with integers, recursion, primitives \nfor inputting and outputting integer values, and a right-to-left evaluation order for applications and \narithmetic operations. We give its operational se\u00ad mantics as a labelled transition system, e -qe ', \nwith the labels . recording numerical inputs and outputs. We follow the standard t convention of writing \n. instead of -. for internal transitions. For this language, we can de.ne (weak) similarity as the great\u00adest \n.xed point of the following function on expression relations: def sim(R) = {(e1, e2) | t (.v. e1 = v \n=. e2 . v) -q1 q '' '''' . (.q, e 1. e1 . e =. .e2. e2 . e2 . R(e1, e 2))} t where . is equal to . * \n(the re.exive-transitive closure of .), and q. for q = . is equal to . * -qt. If e1 is a value, we require \ne2 to evaluate to the same value; otherwise, if e1 reduces to some e1 ' , we require that e2 can match \nthat execution step and end up in a R\u00adrelated state.2 Note that we are working with the powerset lattice \nP(exp\u00d7exp) where g = ., U = ., n = n, and . = \u00d8. 2 We chose this simulation de.nition for simplicity. \nWe can also handle more elaborate de.nitions that result in coarser equivalences for values of function \ntype. Our goal is to show that g(m) simulates f(2m), that is: R0 := {(f(2m), g(m)) | m . Z} . .sim While \nthis might appear trivial, proving this formally using Tarski s coinduction principle is extemely laborious. \nWe have to come up with a simulation relation R containing all the intermediate exe\u00adcution steps of f \nand g appropriately matched: just before the out\u00adput, just after the output, just before the input, just \nafter the input, just before the evaluation of the condition, just after the evaluation of the condition, \njust after the choice of the condition, and so on. In total, there are 16 cases. This relation must be \nfully de.ned before we can even start the proof. We remark that up-to techniques [18, 17], the standard \ntool\u00adbox for simplifying simulation proofs, while very helpful in many cases, are of limited use in this \nexample. As the programs strictly alternate internal and external execution steps, the up to reduction \ntechnique can reduce the cases only by a factor of two. Similarly, the up to context technique does not \nhelp us relate the interme\u00addiate states of the calls to f and g because the two functions have rather \ndifferent internal structure. (It does, however, help in reason\u00ading about restart, as we will see in \nSection 4.) 3.1 Proof Sketch using Parameterized Coinduction We now show how to apply parameterized \ncoinduction in order to avoid the explicit, manual generalization of the coinduction hypothesis that \nis required with Tarski s principle. First, let us introduce the following shorthand: def ff(v) = (if \nv = 0 then f else restart f) (v + 2m) def gg(v) = if v = 0 then restart g (2 * m) else g(v + m) We will \nnow prove R0 . Gsim (\u00d8), which by Lemma 2, = .sim. Step 1. By applying Theorem 4 followed by Lemma 3, \nwe get as our goal: R0 . sim(\u00d8 . R0 . Gsim (\u00d8 . R0)) = sim(R0 . Gsim (R0)) This reduces to showing ' \n''' .e2. g(m) . * e2 . (e1, e 2) . R0 . Gsim (R0) where e1 ' = (let v = (output 2m; input()) * 2 in ff(v)). \nWe pick e2 ' = (output 2m; let v = input() in gg(v)) and proceed to show (e1 ' , e 2 ' ) . Gsim (R0). \nStep 2. We now unfold the .xed point (Lemma 3) to get as our goal (e1 ' , e 2 ' ) . sim(R0 . Gsim (R0)), \nwhich means showing '' ' 2 . * out 2m '' '' '' .e2 . e ----. e2 . (e1 , e 2 ) . R0 . Gsim (R0) where \ne '' 1 = (let v = (0; input()) * 2 in ff(v)). Now, we pick e '' 2 = (0; let v = input() in gg(v)) and \nproceed to show '' '' (e1 , e 2 ) . Gsim (R0). Next Steps. Further steps using Lemma 3 eventually lead \nus to a point where we have to prove the following two inclusions: (f(2v + 2m), g(v + m)) . R0 . Gsim \n(R0) (restart f 2m, restart g 2m) . R0 . Gsim (R0). Regarding the former, we observe that the terms are \nrelated by R0 and so we are done. Regarding the latter, we proceed as follows to show that they are related \nby Gsim (R0). Accumulation Step. We realize that (i) we have to increase the knowledge R0 because restart \ncalls itself recursively, and (ii) that before doing so we should generalize the goal to: R1 := {(restart \nf n, restart g n) | n . Z} . Gsim (R0) To this, we now apply the accumulating principle (Theorem 4) and \nget as our new goal: R1 . Gsim (R0 . R1)  Final Steps. We proceed in the same manner as earlier, using \nLemma 3 to step through the code of restart. When arriving at the recursive call in the then-branch, \nwe use the new R1 part of the coinduction hypothesis and are done. When reasoning about the else-branch, \non the other hand, we conclude by appeal to the original R0 part, which fortunately is still around. \nThe bene.t of our approach over the traditional Tarski approach here is that the simulation relation \ndoes not have to be de.ned up front. Instead, the intermediate goals and the quanti.er instanti\u00adations \ncan be generated automatically by an interactive theorem prover using simple tactics. The details for \nachieving this will be presented in Section 5.  3.2 Decomposing the Proof Since parameterized coinduction \nalso supports compositionality, we can factor out the reasoning about restart from the previous proof \ninto a separate generic lemma that may then be reused in other proofs. Lemma 6 (Restart). For all values \nf1 and f2, we have: {(restart f1 n, restart f2 n) | n . Z} . Gsim ({(f1 0, f2 0)}) Its proof follows \nstraightforwardly from the fact that restart applies its function argument only to 0. Instead of proving \nthe previous goal R0 . Gsim (\u00d8) directly, it now suf.ces to prove the following: Lemma 7. R0 . Gsim (R1) \nThen R0 . Gsim (\u00d8) is obtained from Lemmas 6 and 7 by rule (CO M P OSE) (using empty initial assumptions). \nThe proof of Lemma 7 follows the same structure as the one sketched for R0 . Gsim (\u00d8), except that we \nare done after performing what were called above the Next Steps . Remark. Without Gsim we cannot split \nthe proof into separate lemmas about restart and about f and g. For example, while we can prove the following \nstatements: R0 . . sim =. R1 . . sim R1 . . sim =. R0 . . sim we cannot combine the two to derive R0 \n. . sim. This requires a sound form of circular reasoning, such as the one provided by the (CO M P O \nS E) rule. 4. Combination with Up-To Techniques The incremental proof sketched in the previous section, \ndespite being a huge improvement over the one based on (TA R S K I), is still quite tedious to do (at \nleast on paper). Most of the proof involved trivally stepping through f and g. Doing so for f and g is \narguably necessary, because their structure is quite different, but for the proof of Lemma 6 about restart, \nwhere the structure is identical, it seems unnecessary. In this section, we will see that with a bit \nof additional theory, this tedium can be avoided as well. In traditional (bi-)simulation proofs, people \noften employ sim\u00adpli.cation techniques known as up-to functions [19, 17]. Intuitively, an up-to function \nmaps an element r . C to an element r * (typi\u00adcally larger than r) that is still valid as a coinduction \nhypothesis for proving r g . f . mon De.nition 2. (-) * . C -. C is a sound up-to function for mon f \n. C -. C iff r g f (r * ) implies r g . f for all r . C. The standard de.nition (e.g., [17]) does not \nrequire (-) * to be monotone, but this is a very natural condition, which holds of all up-to functions \nin the literature that we are aware of, and is needed for taking .xed points involving (-) * . This raises \nthe question: is parameterized coinduction compati\u00adble with the use of up-to functions? Fortunately, \nthe answer is yes. To see how up-to functions and parameterized coinduction can be combined, we observe \nthat sound up-to functions respect the great\u00adest .xed point: mon Lemma 8. If (-) * is a sound up-to function \nfor f . C -. C, then: * * def * . f g .f where f = .r. f (r ) Proof. We have . f * g f * (. f * ) = f((. \nf * ) * ), which by De.ni\u00adtion 2 implies . f * g . f . Thus, to prove x g .f using parameterized coinduction, \nwe can instead show x g Gf* (.) for any sound up-to function (-) * . How does this interact with compositionality? \nSuppose we have two separate proofs using two up-to functions * and .: x g Gf* (y U r) and y g Gf. (x \nU r) If f* and f. happen to be equal, then we can combine the proofs using (CO M P O S E) as expected. \nHowever, in general (-)* and (-). might be two arbitrarily different up-to functions, and thus we may \nnot be able to apply the composition rule. Fortunately, there is a solution to this dilemma if we con.ne \nourselves to respectful up-to functions: mon De.nition 3. (-) * . C -. C is a respectful up-to function \nfor mon f . C -. C iff for any r, s . C the following holds:3 r g s . r g f(s) =. r * g f(s * ) Lemma \n9. If (-) * is a respectful up-to function for f, then it is also a sound one. (The proof follows that \nin [17].) Respectfulness, although stronger than soundness, is still satis\u00ad.ed by most up-to functions \nof interest, and, crucially, has better compositionality properties. In particular: Proposition 10. def \n1. id = (.x. x) is a respectful up-to function for any f. 2. If u and u ' are respectful up-to functions \nfor f, then so is u.u ' . 3. If each element of a set X is a respectful up-to function for f,  then \nso is X. 4 We can thus de.ne the greatest respectful up-to function for a mon given f . C -. C: def mon \nf = {u . C -. C | u is a respectful up-to function for f} Using Proposition 10.3, it is easy to show \nthat (-) is a respectful up-to function, and that it is the greatest such. Moreover, we have: Lemma 11. \nIf (-) * is a respectful up-to function for f, then: Gf* g Gf Proof. Follows from monotonicity of G and \n(-) * g (-) . Now, using Lemma 11, we can bring the two proofs from above to a common denominator , x \ng Gf (y U r) and y g Gf (x U r), and then compose them to get x U y g Gf (r). 3 As with the de.nition \nof a sound up-to function, Sangiorgi [17] does not assume monotonicity of (-)*, but here he requires \na slightly weaker * property: r r s . r r f(s) =. r * r s . 4 Here and elsewhere, we treat a function \nspace A . C as the complete lattice obtained by canonically lifting C pointwise.  (e, e ' ) . r (IN \nC L) (R E FL) (e, e ') . rctx (e, e) . rctx (e, e ' ) . r (e1, e 1 ' ) . r (e2, e 2 ' ) . r (I F) ' \n'' ) . rctx (if e then e1 else e2, if e then e1 else e2 e1[v/x, .x f(x). e1/f], ' .v.' ' ' ' ' . r (e2, \ne 2) . r e1[v/x ' , .x f (x ). e 1/f ] (A P P V) ) . rctx ((.x f(x). e1) e2, (.x f'(x '). e 1 ' ) e2 \n' Figure 4. A respectful up-to function. Actually, since the greatest respectful up-to function is so \npow\u00aderful, we see no point in ever stating a proof component s contribu\u00adtion involving a different respectful \nup-to function. In other words: state your goal in terms of the greatest one. The following proper\u00adties \nenable the use of zero or more particular respectful up-to func\u00adtions inside the proof of such a goal: \nLemma 12. If (-) * is a respectful up-to function for f, then for any r . C: 1. r g r 2. (r ) * g r Proof. \nFollows from Proposition 10.1 and 10.2, respectively. We remark that the greatest respectful up-to function \nalso allows us to use up-to reasoning at any point in a proof by parameterized coinduction (not just \nafter unfolding). Theorem 13. Gf (r) = (Gf (r)) Proof. The (g) direction holds by Lemma 12.1. The (;) \ndirection is more complicated. By (TA RSK I) it suf.ces to show Gf (r) gf((r U Gf (r) ) ). This follows \nfrom Lemma 12.2 if we can show Gf (r) g f((r U Gf (r) ) ). By respectfulness in turn it suf.ces to show \nGf (r) g (r U Gf (r) ) and Gf (r) gf((r U Gf (r) ) ). Both are not hard to show using Lemma 12.1 and, \nfor the second, monotonicity of f and (-) . Simulation Example. We return to the simulation example pre\u00adsented \nin Section 3, and show how to apply an up to context technique in order to simplify the proof. To reason \nup to contexts, one usually de.nes a context closure operation. We could do this here as well, but, in \ncombination with (-) , we can get away with something simpler: Figure 4 de.nes a function (-)ctx . P \n(exp 2) . P (exp 2). It is straightforward to verify that this is a respectful up-to function for sim \n(the proof can be found in our Coq formalization). Observe, however, that the de.nition is not recursive \nand thus (-)ctx only adds atomic contexts (i.e., contexts whose de.nition does not involve any form of \nrecursion); we will see in a moment how we can nevertheless get the full power of a proper context closure \noperation. The .nal rule (A P P V) is the case for function application. As . sim relates values only \nif they are identical, the simpler rule {if (e1, e 1 ' ) . r and (e2, e 2 ' ) . r then (e1 e2, e1 ' e2 \n' ) . r ctx} while sound, is useless because the .rst assumption requires e1 and e1 ' to evaluate to \nsyntactically the same function. Therefore, A P P V requires the functions to be already values, and \nchecks that their bodies are related whenever the functions are applied to the same arguments. Finally, \nnote that the following rule can be derived from ) . rctx (e1; e2, e 1; e A P P V: ' ' (e1, e 1) . r \n' (e2, e 2) . r ' (SE Q) 2 With the help of this up-to function we will now prove the same two lemmas \nas before (Lemmas 6 and 7), except that we replace Gsim by Gsim . Lemma 14. For all values f1 and f2, \nwe have: {(restart f1 n, restart f2 n) | n . Z} . Gsim ({(f1 0, f2 0)}) Lemma 15. R0 . Gsim (R1). Proof \nof Lemma 14. As in Section 3, we apply the accumulation principle and reason about the .rst two steps \nof execution with the help of Lemma 12.1. Then we obtain the following proof obligation: (e1, e2) . (R \n. Gsim (R)) where: ei := if n > 0 then (output n; restart fi (n - 1)) else fi 0 R := {(restart f1 n, \nrestart f2 n) | n . Z} . {(f1 0, f2 0)} At this point, since (- ) ctx g (-) by Lemma 12.2, we may apply \none of the rules from Figure 4 in order to simplify this goal. We pick I F and hence it remains to show \nthe following three: 1. (n > 0, n > 0) . (R . Gsim (R)) 2. ((output n; restart f1 (n - 1)), (output \nn; restart f2 (n - 1))) . (R . Gsim (R))  3. (f1 0, f2 0) . (R . Gsim (R))  Now, by the same argument, \nwe may apply another of these rules in each case. So, effectively, we can apply an arbitrary number of \nrules and thus did not lose any power by de.ning (-)ctx in terms of atomic contexts only. (1) is solved \nby rule R EFL, and (3) is solved by Lemma 12.1 since the terms are related by R. To show (2), we .rst \napply rules S E Q and RE FL, and then use Lemma 12.1 to reduce the goal to: (restart f1 (n - 1), restart \nf2 (n - 1)) . Gsim (R) This is easily shown by unfolding (Lemma 3), performing a step of computation, \nwhich converts the two instances of n - 1 to n - 1, and then concluding once again with Lemma 12.1. The \nattentive reader may have noticed that we never seem to need rule IN C L. Indeed, this is a side effect \nof reasoning via Lemma 12. Nevertheless, the rule is necessary: without it, (-)ctx would not be respectful. \nProof of Lemma 15. Here, reasoning up to contexts seems not to buy us anything, so we just derive the \ngoal from our old proof (Lemma 7) via Lemma 11. Finally, we can, as before, use (CO M P OSE) to deduce \nthat R0 . Gf (\u00d8). By Lemmas 2, 8 and 9, this implies R0 . . f . 5. Mechanizing Parameterized Coinduction \nIn this section, we discuss at a high level the issues raised by for\u00admalizing our parameterized coinduction \nprinciple from Section 2 in a proof assistant such as Isabelle/HOL or Coq. Details about our Coq implementation, \nas well as examples using it, follow in Sec\u00adtion 7. To establish some common terminology, we say that \na predicate of arity n is a (dependent) function of type .a1:A1. .a2:A2(a1). . . . .an:An(a1, . . . , \nan-1). S where the sort S is impredicative (Prop in Coq, bool in Isabelle). If instead the sort S is \npredicative (Type in Coq, or Set in Agda), we call such objects indexed sets.  Predicates are normally \nused for writing proofs (whose com\u00adputational meaning is not of interest), whereas indexed sets are used \nfor writing programs (whose computational meaning is their main point of interest), and of these two \nactually only predicates form complete lattices. Therefore, in this paper, whose main focus is on coinductive \nproofs, we shall largely ignore indexed sets, and only brie.y discuss them in Section 8. There are two \nways in which a formalization can be done, namely what we call the external and the internal approach. \nThey differ in the way in which the parameterized greatest .xed point is constructed. The external approach \ndevelops a library of complete lattices that uses Tarski s construction for greatest .xed points, and \nthen uses that library to de.ne G. De.ning such a library, however, requires impredicative quanti.cation, \nand so this approach only works for Isabelle and Coq, but not Agda.  The internal approach de.nes G \ndirectly using the proof assis\u00adtant s primitive mechanism for de.ning coinductive types (e.g., Coq s \nCoInductive or Agda s 8 constructor). In Isabelle/HOL and related systems, where coinductive types/relations \nare not primitive, one must instead follow the external approach.  In Coq, where both approaches are \napplicable, the internal ap\u00adproach, besides being more direct, is also easier to use because Coq s automation \nworks much better there. The main problem is that in the external approach, Coq s automation tactics \ndo not know how to unfold the lattice-theoretic constructs, and so the user has to instruct Coq manually \nto do so. We now discuss the two approaches in more detail. The External Approach. In this approach, \none de.nes a generic library of complete lattices and greatest .xed points of arbitrary monotone endofunctions, \nand uses that to construct G and prove its properties. The library can then be instantiated to the application \ndomain at hand. This approach is arguably as general and modular as it gets, and works quite well for \nboth Coq and Isabelle/HOL. The Isabelle im\u00adplementation is actually simpler: Isabelle already has a complete \nlattice theory, which only needs to be extended with our parameter\u00adized coinduction. To apply the library \nde.nitions and lemmas to arbitrary predi\u00adcates, one has to prove that Prop (in Coq, or bool in Isabelle) \nforms a complete lattice and that the space of (dependent) functions to a complete lattice forms again \na complete lattice (the pointwise lift\u00ading). Using type classes (in Isabelle or Coq) or canonical structures \n(only in Coq), one can easily arrange that the appropriate lattice structure for a given predicate is \nautomatically inferred. The Internal Approach. As mentioned above, the internal ap\u00adproach depends on \nhaving primitive support for coinductive types, but if it is available, it can be more a convenient option. \nHowever, the applicability of the internal approach is somewhat limited for two further reasons: 1. The \nonly objects that one can de.ne with the primitive coin\u00adductive de.nition mechanism are predicates (and \nindexed sets). Hence, this approach does not work for arbitrary complete lat\u00adtices. However, it is still \nvery useful in practice, because (i) predicates are already quite expressive, and (ii) we found a clever \ntrick that enables us to extend this approach to re.ned predicates, explained in detail in Section 6.2. \n 2. Moreover, the predicates themselves must have a certain syntac\u00adtic form: all recursive uses of a \n(co-)inductively de.ned object in its de.nition must be strictly positive, i.e., roughly speaking, not \noccur on the left of an arrow. While this syntactic condition  is overly restrictive for predicates, \nit is important for ensuring consistency in the case of indexed sets, and is thus imposed for uniformity \non all coinductive de.nitions. What does this entail for the formalization? Consider the power\u00adset lattice \nfrom Section 3. It satis.es condition (1) above (i.e., it is a predicate type), but condition (2) prevents \none from parameterizing the de.nition of Gf over f. Recall its de.nition: monmonmon 22 22 G . (P(exp \n) -. P (exp )) -. (P(exp ) -. P (exp )) def Gf (x) = .y. f (x U y) This can be translated into Coq as \nfollows: De.nition erel := exp . exp . Prop. CoInductive G (f: erel . erel) (fM: monotonic f) (x: erel) \n(e1 e2: exp) : Prop := | G_fold (IN: f (x U G f fM x) e1 e2). (* REJECTED *) This de.nition, however, \nis rejected because it violates strict pos\u00aditivity: in the type of the constructor argument IN, G occurs \nin the argument of a function application, where the function, f, is a vari\u00adable. This is forbidden, \nintuitively because f could be instantiated to a function that uses its argument on the left side of \nan arrow. Fortunately, there is a simple but clever trick that lets us work around the strict positivity \nrequirement as long as the function in question (here: .y.f (x U y)) is monotone. This trick is based \non Mendler-style recursion and explained in Section 6.1. Conse\u00adquently, in the internal approach, we \ncan also de.ne a library for parameterized greatest .xed points of arbitrary monotone predi\u00adcates up \nto some .xed arity see Section 7.4. 6. Mendler-Style Recursion to the Rescue! Mendler [12] proposed a \nstrongly normalizing calculus featuring an unusual treatment of inductive and coinductive types. In this \nsection, we show how recursion in the Mendler style can be used to address the two issues mentioned in \nSection 5 that come up when mechanizing parameterized greatest .xed points following the internal approach. \nWe .rst review the idea of Mendler-style recursion in the setting of complete lattices and show how it \ncan be employed to overcome Coq s strict positivity restriction in the case of predicates. This observation \nis not novel, and has been made before by Matthes [11]. Second, we generalize this theory in a way that \nyields a method for de.ning .xed points in complete sublattices. In the internal approach, this enables \nus to de.ne re.ned predicates coinductively, while, in the external approach, it aids in avoiding dealing \nwith dependent types. As far as we can tell, both the generalization of the theory and its application \nto mechanization are new. 6.1 Strict Positivization A convenient way to view Mendler s .xed point constructions \nis as ordinary .xed points of monotonized functions. Given a function g . C . C (where C is a complete \nlattice), there are two canoni\u00adcal ways of adapting this function slightly to make it monotone: mon De.nition \n4 (Monotonizations). We de.ne LgJ, gl . C -. C: def LgJ = .x. n{g(y) | y ; x} def gl = .x. {g(y) | y \ng x} Their monotonicity is easy to see, given that for x g x ' we have: {g(y) | y ; x} . {g(y) | y ; \nx ' }and {g(y) | y g x} . {g(y) | y g x ' }. It is also easy to verify that both L-J and -l form a Galois \nmon connection with the canonical embedding of C -. C in C . C, in the way depicted in Figure 5. This \nmeans that  -l . mon C -. C C . C . L-J Figure 5. Monotonization operators and their Galois connections. \n.f . C mon -. C. f g LgJ .. f g g i.e., LgJ is the greatest monotone function below g, and mon .f . \nC -. C. gl g f .. g g f i.e., gl is the least monotone function above g, and thus LgJ g g g gl (which \nexplains our choice of notation). Now, if g is already monotone, then all three are equivalent, and consequently \nso are their least and greatest .xed points: mon Proposition 16. If g . C -. C, then LgJ = g = gl, and \nhence \u00b5LgJ = \u00b5g = \u00b5 gl and .LgJ = . g = . gl. So why is this interesting at all? The point is that monotonizing \nan already monotone function using -l yields the same function in a form that translates to a strictly \npositive one in Coq. To see this, let us apply it to the de.nition of G that we attempted at the end \nof the previous section. Given a monotone function f, there are actually two ways to apply Proposition \n16 to Gf (x). In the .rst, we monotonize f: Gf (x) = Grfl(x) = .z. {f(y) | y g x U z} In the second, \nwe monotonize .z. f (x U z): Gf (x) = .z. f (x U z) = .z. {f(x U y) | y g z} Hence we have two possible \nde.nitions of G in Coq: CoInductive G (f: erel . erel) (x: erel) (e1 e2: exp) : Prop := | G_fold y (LE: \ny . x . G f x) (IN: f y e1 e2). CoInductive G (f: erel . erel) (x: erel) (e1 e2: exp) : Prop := | G_fold \ny (LE: y . G f x) (IN: f (x . y) e1 e2). Note the similarity to the rejected de.nition in the previous \nsection and the absence of the fM assumption. These new de.nitions are well-formed for arbitrary functions \nf (not necessarily monotone), and dropping fM makes them more convenient to work with. Of course, we \nneed to assume monotonicity in statements about G then instead. We prefer the .rst de.nition, because \nthen monotonicity of f is required only for unfolding Gf (the g direction of Lemma 3) and is not needed \nfor the accumulation theorem (Theorem 4). We remark that this trick of monotonizing functions that are \nalready monotone in order to obtain a strictly positive form applies to inductive predicates (de.ned \nusing Coq s Inductive command) as well, but we do not exploit that observation in this development. \n 6.2 Fixed Points in Sublattices The Problem. Recall that, in the internal approach, (co-)inductive de.nitions \nare limited to predicates and indexed sets. Here we show how to broaden this to re.ned predicates. By \nre.ned predicates, we mean objects whose type has the form {x : .a1:A1. . . . .an:An(a1, . . . , an-1). \nProp | P (x)}, i.e., a regular predicate type re.ned by some property P . This is best illustrated with \nan example. Imagine we want to de.ne the greatest .xed point of a function 2 2 mon2 2 f . (P(exp ) . \nP (exp )) -. (P(exp ) . P (exp )). We can easily do this using Coq s CoInductive mechanism since P(exp \n2) . P (exp 2) is naturally expressed as a predicate type (we may have to bring f into strictly positive \nform, of course). Now imagine we want to de.ne the greatest .xed point of a different function 2 mon2 \nmon2 mon2 g . (P(exp ) -. P (exp )) -. (P(exp ) -. P (exp )). mon Note that the complete lattice P(exp \n2) -. P (exp 2) cannot be expressed as a predicate type due to the restriction of the function space. \nInstead, it can be seen as a re.ned predicate type (where P is monotonicity). In the external approach, \nthere is no problem: we can de.ne the complete lattice structure for this type, then use that to write \ndown the faithful de.nition of g, prove that it is monotone, and .nally just apply the greatest .xed \npoint operator to it. But in the internal approach, this is not possible. The best we can do there, so \nit seems, is take the greatest .xed point in the unrestricted function space. However, (i) this assumes \nthat g is well-de.ned and monotone in the larger space, and (ii) even if that .xed point exists, it will \nnot necessarily be the desired one. The Solution. Our solution is as follows: we do indeed take the greatest \n.xed point in the unrestricted space, but only after modify\u00ading the function in a way that ensures (i) \nthat it is well-de.ned and monotone, and (ii) that the result actually coincides with the de\u00adsired greatest \n.xed point in the original restricted space. The math behind this is a generalization of what we saw \nin Section 6.1, and is basically stated in terms of an arbitrary complete lattice and an arbitrary complete \nsublattice thereof that preserves meets and/or joins. Because the theory is so general, it could actually \nalso be used in the external approach, where the (non-negligible) bene.t would be avoiding to work with \n(dependent) subset types. The Theory. Consider two complete lattices B and C. We are interested in the \nscenario where there exists an embedding of B in C, in the following sense: De.nition 5. A function i \n. B . C is an embedding of B in C, written i : B .. B. b gB b ' ). . C, iff .b, b ' .. i(b) gC i(b ' \n(Note that this implies injectivity.) In the case where B is a com\u00adplete sublattice of C, the canonical \ninjection from B to C consti\u00adtutes such an embedding. With the help of an embedding, we can now de.ne \ngeneralized versions of the monotonization operators from Section 6.1: De.nition 6 (Generalized Monotonizations). \nFor i : B .. C and mon a function g . B . B we de.ne LgJi, gli . C -. C: def LgJi = .x. n{i(g(y)) | y \n. B . i(y) ; x} def gli = .x. {i(g(y)) | y . B . i(y) g x} Now, if i preserves meets and joins (e.g., \nbecause C is a function space with a pointwise ordering and B its restriction to monotone functions), \nthen the Galois connections from earlier generalize as well. And, if moreover g is monotone, then the \nleast and greatest .xed points of g, LgJi and gli coincide modulo the embedding. Proposition 17. If .X. \ni(n X) = n(i(X)), the following hold mon for any f . C -. C and g . B . B: def 1. f g LgJi .. |f |g g, \nfor |f|= n{y | i(y) ; f(i(x))}. i i 2. If g is monotone, then \u00b5LgJi = i(\u00b5g) and .LgJi = i(. g).  Proposition \n18. If .X. i( X) = (i(X)), the following hold mon for any f . C -. C and g . B . B: 1. gli = g f .. g \ng |f |i, for |f|i def {y | i(y) g f(i(x))}. 2. If g is monotone, then \u00b5 gli = i(\u00b5g) and . gli = i(. g). \nFinally, observe that the results in Section 6.1 are merely a special case of the results presented here, \nnamely where B = C and i = id, in which case L-Ji = L-J, -li = -l, and | - |i = | - |= id. i The Example. \nTo see what this looks like in practice, let us return mon to the example g from the beginning. Since \nP(exp 2) -. P (exp 2) is a complete sublattice of P(exp 2) . P(exp 2), we have by Propositions 17 and \n18 (for the canonical injection i): . g = .LgJi = . gli We can now mechanize either LgJi or gli. Because \n-li has the added bene.t of yielding a strictly positive form (even if g does not have one), we pick \nthe latter. To see how this translates into Coq, note that mon gli . (P(exp 2) . P (exp 2)) -. P (exp \n2) . P (exp 2) . mon gli = .x. {i(g(y)) | y . P (exp 2) -. P (exp 2) . i(y) g x} and that . gli, by \nunfolding, is equal to: mon {i(g(y)) | y . P (exp 2) -. P (exp 2) . i(y) g . gli} Accordingly, we write: \nCoInductive nu_g (r: erel) (e1 e2: exp): Prop := nu_g_fold (y: erel . erel) (yM: monotonic y) (LE: .r \n, y r . nu_g r ) (IN: g y yM e1 e2). Here, g y yM e1 e2 may look a bit different, depending on how g \nis de.ned (if it is not de.ned explicitly, one can just inline it here). Similar to the .rst part, and \nas is evident from the theory, all this applies to induction (i.e., least .xed points) as well. Remark. \nThe type that we used for illustration, 2 mon2 mon2 mon2 (P(exp ) -. P (exp )) -. (P(exp ) -. P (exp \n)), is highly reminiscent of one that occurs in the meta-theory of Re\u00adlation Transition Systems (RTS) \n[7], a new kind of semantic model that we recently introduced for compositional reasoning about pro\u00adgram \nequivalences in higher-order stateful languages. Reasoning using this method feels very much like a regular \nbisimulation ar\u00adgument: to show the equivalence of two functions, one has to con\u00adstruct a local knowledge \nrelating them and then prove its con\u00adsistency . In our mechanized RTS proofs, we found that explicitly \nde.ning this local knowledge up front was quite a painful experi\u00adence, for essentially the same reasons \nas de.ning the Tarski-style simulation relation was painful in the example in Section 3. It turns out \nthat being a consistent local knowledge can be expressed as being a post.xed point of a certain monotone \nfunction of basically the above type: 2 mon2 mon2 mon2 (P(exp ) -. P (exp )) -. (P(exp ) -. P (exp )) \nBy following the internal approach and using the trick presented here, we were able to bring the bene.ts \nof parameterized coinduc\u00adtion to our RTS framework, while keeping changes to existing def\u00adinitions to \na minimum. 7. Coq Implementation and Evaluation In this section, we discuss our implementation of parameterized \ncoinduction in Coq, and compare it to other approaches, most notably Coq s builtin cofix tactic. Technique \nLines Proof (s) Qed (s) Composes? Tarski 74 8.5 3.5 no Coq s cofix (\u00a77.3) 8 7.9 14.0 no Internal (\u00a77.1) \n8 8.6 5.1 yes External (\u00a77.2) 8 11.8 4.3 yes Table 1. Comparison of the mechanization approaches For \na fair comparison, we have carried out the simulation proof of the example from Section 3 using a number \nof different ap\u00adproaches, and show the results in Table 1. In each case, we report: (a) the number of \nlines of proof and auxiliary lemmas/de.nitions, not counting the lines of generic library lemmas and \ntactics as these can be de.ned once and for all; (b) the time taken to execute the corresponding proof \nscript; and (c) the time taken to check (Qed) that the constructed proof object (from running the script) \nis valid. Tarski: Explicitly de.ne a simulation relation, prove that it is valid (i.e., a post.xed point \nof sim), and then apply (TA R S KI). This requires by far the most human effort, which is partially re\u00ad.ected \nin lines of code needed to de.ne the simulation relation; the proof itself is quite short, and hence \nQed-checking is fast. Co.x: Use Coq s builtin cofix tactic to simulate an incremental proof. We shall \ndiscuss this approach in detail in Section 7.3, but for the moment we remark that proof checking (the \nQed column) for cofix is signi.cantly slower than for all the other approaches, and can become a serious \nusability bottleneck in larger examples. Internal: Use a direct encoding of Gsim following the internal \nap\u00adproach described in Section 5. (See Section 7.1 for the details.) External: Use a library-based de.nition \nof Gsim following the external approach. (See Section 7.2 for the details.) Among these approaches, it \nis quite clear that the internal and external approaches are roughly equivalent, with the internal ap\u00adproach \nbeing somewhat more ef.cient by virtue of using primi\u00adtive de.nitions, and avoiding the use of clever \ninference during type checking (namely, canonical structures or type classes) that the external approach \nrequires. The two are much faster than the cofix approach. Moreover, they enable compositional proof \nde\u00advelopments, unlike the other two approaches. 7.1 Internal Implementation of Parameterized Coinduction \nWe now show what the internal approach to mechanizing parame\u00adterized coinduction (discussed in Section \n5) looks like in practice, by applying it to the example from Section 3. For the sake of a sharper contrast \nwith the external approach, we do not parameterize G (which we could, with the help of the trick from \nSection 6.1), but directly de.ne Gsim : CoInductive psimil r (e1 e2 : exp) : Prop := psimil_fold t (VAL: \n.v : val, e1 = v . e2 . v) q (EXP: .q e1 , e1 -. e1 . q . e2 , e2 . e2 . (psimil r e1 e2 . r e1 e2 \n)). Observe how psimil r corresponds to Gsim (r). Next, we prove the following accumulation property, \nwhich we could in principle instrument Coq to generate and prove automatically. Theorem psimil_acc : \n.l r, (. r , r . r . l . r . l . psimil r ) . l . psimil r. This is a variant of the (.=) direction of \nTheorem 4 in Mendler form , which avoids the explicit join operation and is thus a bit more convenient \nto use interactively than its simpler counterpart. Also, note that the unfold property (Lemma 3) is implicit \nin the de.nition of psimil, and that we do not need to state Lemma 2 either, because we simply work with \nGsim (.) directly.  We can now already state and prove the example from Sec\u00adtion 3: Theorem fg_simulated \n: . n m (EQ: n = 2 * m), psimil bot2 (f @ n) (g @ m). Proof. pcofix CIH using psimil_acc. intros; subst; \ndo 6 psimil_step 1; do 2 psimil_step 0. destruct m0; psimil_step 2; [|eauto]. left; fold restart f g; \ngeneralize (m+(m+0)). pcofix CIH using psimil_acc. intros; do 3 psimil_step 1. destruct n; psimil_step \n1; [eauto|]. do 3 psimil_step 1; eauto. Qed. In this proof script, all tactics except psimil step and \npcofix are standard (i.e., part of Coq). The former matches one step of e1 with n steps of e2. The latter \nassists in applying parameterized coinduc\u00adtion: pcofix rewrites the goal to a form suitable for applying \nthe accumulation theorem, applies the theorem, and then simpli.es the result. For instance, at the beginning \nof the proof (when the goal is simply the theorem statement), applying psimil acc directly is not possible, \nbecause Coq cannot .gure out how to instantiate its l parameter. Hence we invoke pcofix, which .rst transforms \nthe goal into the following equivalent statement: (fun x y . .n m, f @ n = x . g @ m = y . n = 2 * m) \nr psimil bot2 After that, it applies psimil acc (its argument), which now matches the goal, and .nally \npcofix simpli.es the resulting proof state so that the user never sees this explicit function expression: \nr : exp . exp . Prop CIH : .n m, n = 2 * m . r (f @ n) (g @ m) . n m, n = 2 * m . psimil r (f @ n) (g \n@ m) Note that the new goal essentially also appears as the coinduction hypothesis CIH, except that psimil \nhas been removed, and so the hypothesis is semantically guarded. The second use of pcofix corresponds \nto the Accumulation Step of the proof sketch, and results in the following proof state: r : exp . exp \n. Prop CIH : .n m, n = 2 * m . r (f @ n) (g @ m) CIH : .n, r ((restart @ f) @ n) ((restart @ g) @ n) \n . n, psimil r ((restart @ f) @ n) ((restart @ g) @ n) It essentially added the additional assumption \nCIH , but to achieve this, it had quite some cleaning up to do internally: after applying the accumulation \ntheorem, a new r (greater than r) had been introduced, so pcofix used transitivity of g to convert all \nexisting hypotheses involving r into statements about r , which it then .nally renamed to r again. Although \nwe have demonstrated the use of our pcofix tactic on one example, it is not tailored to this particular \nexample rather, it is a general tactic that applies to arbitrary coinductive predicates. It is implemented \nin Ltac with the help of the hpattern [6] library for handling dependent types.  7.2 External Implementation \nof Parameterized Coinduction We implement G from Section 2 using a complete lattice library that provides \na type of complete lattices (cola) and operations such as gfp, g, U (all with the obvious meaning): De.nition \nG {C : cola} (f : C . C) (x : C) := gfp (fun y . f (x U y)). Next, we prove Lemma 3 (here as two separate \nlemmas) and (the interesting direction of) the accumulation property (Theorem 4, again expressed in Mendler-style \nto make it easier to use). Like before, there is no point in de.ning . sim explicitly and proving Lemma \n2, because we can just work with Gsim (.) directly. Lemma G_fold: .(C:cola) (f:C . C), monotone f . . \nr: C, f (G f r U r) r G f r. Lemma G_unfold: .(C:cola) (f:C . C), monotone f . . r: C, G f r r f (G f \nr U r). Theorem G_acc: .(C:cola) (f:C . C), monotone f . . l r, (. r , r r r . l r r . l r G f r ) . \nl r G f r. To use this library, we simply de.ne the generating function sim. Inductive sim r (e1 e2: \nexp) : Prop := sim_fold (VAL: .v : val, e1 = v . e2 t v) (EXP: .q e1 , e1 . e1 . .e2 , e2 e2 . r e1 -qq \ne2 ). Then G sim corresponds to Gsim , where the implicit argument C:cola is automatically inferred \nfrom the type of sim with the help of Coq s canonical structure mechanism. The statement of the example \nfrom Section 3 is the same as in the internal implementation (fg simulated above), except that psimil \nis replaced by (G sim). In the corresponding proof script, the argument to the pcofix tactic changes \nfrom psimil acc to (G acc sim). Similarly, the psimil step tactic is replaced by one that applies (G \nfold sim) instead of psimil fold.  7.3 Coq s cofix Coq s standard coinduction principle is rather syntactic \nand thus quite different from the Tarski principle. It basically works as follows: to show x g . f , \none invokes Coq s builtin cofix tactic, which adds the very same proposition as an assumption to the \nlocal context. Being an ordinary assumption, it can be used at any point in the proof script. However, \nonce the proof is .nished, Coq runs a syntactic check on the proof term and accepts it only if the use \nof the coinductive assumption is guarded [3]. It is possible, although not well known, that one can nest \nuses of cofix and thereby achieve a form of incremental (albeit non\u00adcompositional) coinduction. For example, \nif we take the proof script from Section 7.1 and simply replace the two occurrences of pcofix CIH[ ] \nusing psimil acc with cofix CIH[ ] , we obtain a valid proof! Despite its surprising usefulness in allowing \na form of accu\u00admulation, Coq s cofix approach to coinductive proofs has several important drawbacks due \nto its syntactic nature:5 It is non-compositional. Inside a proof via cofix, the use of normal lemmas \ninvolving the coinduction hypothesis is not permitted by guardedness checking, because they are treated \nopaquely. One can of course mark these lemmas as transpar\u00adent (thereby further slowing down proof checking), \nbut this does not yield proper compositionality: from the statements of the transparent lemmas alone, \none cannot know whether their proofs can be composed together to yield a valid proof.6  It is inef.cient. \nGuardedness checking can be very slow, mainly because it has to reduce proof terms to normal forms, which \nmay be huge. This problem is already apparent from Table 1 and  5 Another critique of Coq s syntactic \nguardedness checking can be found in Barthe et al. [2]. 6 One way to think of this is by the following \nanalogy. Our (C O M P O S E) rule corresponds to the rely-guarantee parallel composition rule [8], whereas \ntransparent lemma application within a cofix-proof corresponds to the earlier non-compositional Owicki-Gries \nrule [15] with the syntactic non\u00adinterference side condition.  becomes aggravated in larger developments. \nIn our accompany\u00ading Coq code, we provide an example from an earlier project, where proof checking takes \n192 seconds due to cofix. Replac\u00ading cofix with pcofix reduces this to 40 seconds. It is not at all \nuser-friendly. The user interface does not indi\u00adcate when exactly in a proof it is safe to use the coinductive \nassumption. Coq provides a designated command for explicitly checking guardness, but, due to the previous \nissue, its repeated use during the proof is often impractical.  It interacts poorly with builtin automation \ntactics. They do not know about guardedness and hence very frequently produce in\u00adcorrect proofs . Usually \nthis happens because automation ap\u00adplies constructors and hypotheses in the wrong order, solving the \ngoal but causing the proof to be rejected later by the guard\u00adedness checker. To make matters worse, as \na consequence of the two previous issues, it is very painful to debug such situations. Consequently, \none has to be extremely careful when using au\u00adtomation.  As a demonstration of automation leading to \na dead end, con\u00adsider the following code: De.nition monotone0 (f: Prop . Prop) := . (p q: Prop), f p \n. (p . q) . f q. CoInductive A f: Prop := foldA (p: Prop) (LE: p . A f) (IN: f (f p)). Goal . (f: Prop \n. Prop) (MON: monotone0 f), . p: Prop, (p . f (f p)) . p . A f. Proof. cofix CIH; intros; eapply foldA; \neauto. Qed. (* REJECTED *) Here, although we explicitly apply the constructor (foldA) .rst, eauto somehow \nmanages to construct an invalid proof term. We can obtain a proper proof by manually applying the coinduction \nhypothesis before letting eauto take over (this is the result of trial and error): cofix CIH; intros; \neapply foldA; [apply CIH|]; eauto. Alternatively, we can just do the proof using our pcofix tactic instead \nof cofix (where A acc is the corresponding accumula\u00adtion lemma): pcofix CIH using A_acc; eauto using \nfoldA.  7.4 Paco: A Coq Library for Parameterized Coinduction In order to make parameterized coinduction \nmore easily applica\u00adble, we have built the Coq library Paco (standing for parameterized coinduction). \nPaco contains internal implementations of parameter\u00adized coinduction for predicates of arity up to 15, \nwith paco{n} f standing for Gf for any monotone function f from predicates of arity n to predicates of \narity n. Besides the pcofix tactic that we have already seen in Sec\u00adtion 7.1, the library provides tactics \nfor folding (pfold) and unfold\u00ading (punfold) the de.nition of Gf , for proving monotonicity of predicates \n(pmonauto) and for simplifying hypotheses by reducing occurrences of r U. to r (pclearbot). It also provides \nmultiplica\u00adtion lemmas (paco{n} mult) of the form Gf (Gf (r)) g Gf (r). These follow easily from Theorem \n4 and monotonicity of Gf , and show that doubly guarded assumptions r are also simply guarded. We have \nfound these multiplication lemmas useful for composing parameterized coinduction proofs. To illustrate \nthe use of our library, we build on the example of the introduction. We represent a graph by a type of \nnodes and a relation, R, characterizing the edges between nodes: Variables (Node: Type) (R: Node . Node \n. Prop). We now de.ne in.nite paths using parameterized coinduction. (We write paco1 and bot1 as the \npredicate is unary.) Inductive step (X: Node . Prop) (x: Node) : Prop := | step_intro : .y, R x y . X \ny . step X x. Hint Constructors step. De.nition infpath := paco1 step bot1. We also prove monotonicity \nof step, and register the correspond\u00ading lemma in the paco hint database that is used by punfold. Lemma \nstep_mon : monotone1 step. Proof. pmonauto. Qed. Hint Resolve step_mon : paco. Further, we de.ne the \npredicate path n x to say that there exists an outgoing path of length n from node x. Fixpoint path n \nx := match n with | O . True | S n . .y, R x y . path n y end. Then we can establish that if an in.nite \npath emanates from x, then so do paths of length n, for any n. This is proved by induction on n and unfolding \nand inverting the de.nition of in.nite paths. Goal . n x, infpath x . path n x. Proof. induction n; intros; \nsimpl; auto. punfold H; inversion H; pclearbot; eauto. Qed. This shows that Paco-style coinductive de.nitions \ncan, after un\u00adfolding, be inverted just as native Coq coinductive de.nitions can. We move on to a more \ninteresting property involving transitive closure that was suggested to us by an anonymous reviewer. \nThe goal is to prove that if there is a predicate P holding of a node x in a graph and that whenever \nP holds of a node there is a non\u00adempty path at the end of which P holds again, then there is an in.nite \npath starting from x. The in.nite path can be constructed by concatenating these non-empty paths, which \nis formally done by an inner induction inside a coinductive proof. Goal . (P: Node . Prop), (. x, P x \n. .y, clos_trans_1n _ R x y . P y) . . x, P x . infpath x. Proof. pcofix CIH; intros P M x Px. destruct \n(M _ Px) as (y &#38; C &#38; Py); clear Px. induction C; pfold; eauto. Qed. Using Paco, the proof is \nstraightforward: destruct instantiates the second assumption and destructs the existential quanti.er \nto ex\u00adpose the transitive closure, upon which an induction is later per\u00adformed, whereas clear Px simply \nforgets Px to avoid confusing the later automation. Moreover, pcofix ensures that the coinduc\u00adtive hypothesis \nCIH is used in a semantically guarded way by con\u00adstruction, thereby placing no further restrictions on \nthe proof. In contrast, carrying out this proof using Coq s builtin cofix tactic is surprisingly dif.cult \nbecause the inner inductive proof turns out to violate Coq s conservative syntactic notion of guardedness, \nthus necessitating ugly workarounds. 8. Discussion and Related Work In this section, we compare to some \nrelated forms of incremental coinduction that have appeared in the literature, including the ear\u00adlier \nversions of our construction due to Winskel [23] and Moss [14]. We conclude with some thoughts about \nCoq.  Local Model Checking and the Reduction Lemma . To our knowledge, the earliest account of the parameterized \ngreatest .xed point is in a 1989 paper by Winskel [23]. That paper, building on prior work of Larsen \n[10] and Stirling and Walker [21], is focused on the speci.c problem of local model checking in the modal \n\u00b5-calculus i.e., deciding whether a particular state or process in a labelled transition system satis.es \nsome recursively-de.ned assertion. However, in the course of attacking this speci.c problem, Winskel \npresents a generally useful construction (on power sets, but easily generalizable to lattices) that, \nwith hindsight, we can analyze and appreciate in a more abstract way. Winskel s key innovation is a parameterized \nrecursive assertion, which he writes as . X {Tr}A. One can understand this assertion as representing \nthe greatest .xed point . X.A under the accumu\u00adlated knowledge Tr, but where and this is the key difference \nfrom our parameterized greatest .xed point the use of the accumu\u00adlated knowledge is not guarded. Winskel \ncorrespondingly provides the following rules for checking recursive assertions incrementally (where p \nfA denotes that process p satis.es assertion A): p . {Tr} p . {Tr} p fA[. X {p, Tr}A/X] p f.X {Tr}A p \nf. X {Tr}A Due to the un-guardedness embodied by the .rst of these rules, Winskel s parameterized .-assertion \ndoes not support composi\u00adtional reasoning along the lines of our CO M P O S E rule. In particu\u00adlar, from \np f. X {q, Tr}A and q f. X {p, Tr}A, one cannot conclude that p f. X {Tr}A or q f. X {Tr}A, since the \npremises hold trivially if p = q. Interestingly, a subsequent paper by Andersen, Stirling and Winskel \n[1], building on Winskel s parameterized .-assertions, presents a compositional proof system for the \nmodal \u00b5-calculus, but they mean compositional an admittedly overloaded term in a very different sense \nfrom us (their compositional concerns the structure of processes). Our parameterized greatest .xed point \nmay be understood as a guarded version of Winskel s. Formally, Winskel s model of . X {Tr}A is synonymous \nwith . X.({Tr} . A). In the notation of our paper, this suggests the following alternative to our Gf \n(x): def Wf (x) = . z. x U f(z) The connection to Gf (x), provable using a few straightforward applications \nof Tarski s principle, is then very simple: Wf (x) = x U Gf (x) Gf (x) = f (Wf (x)) For the purpose of \nproving soundness of local model checking, the lack of guardedness in Wf (x) was not an issue, but for \ncomposi\u00adtional proof development it is. That said, the key technical result that Winskel employs in order \nto prove soundness of his local model checking algorithm, namely a reduction lemma due to Kozen [9], \nis in fact interderivable with our Theorem 4. The reduction lemma states, for monotone f: y g .f .. y \ng f (Wf (y)) To see the connection with Theorem 4, .rst observe that since f(Wf (y)) = Gf (y), the reduction \nlemma can be restated as: y g . f .. y g Gf (y) And since .f = Gf (.), the lemma can thus be seen as \nan in\u00adstantiation of Theorem 4 where x := .. At the same time, Theo\u00adrem 4 can also be seen as an instantiation \nof the reduction lemma! Speci.cally, if (given x) we instantiate the reduction lemma s f with f(x U -), \nit yields y g . z. f (x U z) .. y g . z. f (x U y U z) This is precisely Theorem 4. Incremental Coinduction. \nIn 2010, Popescu and Gunter [16] proposed a proof system for incremental coinduction, tailored to\u00adwards \nbisimilarity in a process calculus, and they established the soundness of their system by a global, monolithic \nargument. Their judgment . f . ' corresponds precisely to . ' g . U Gf (.) (where f is the generating \nfunction of their process bisimilarity). This suggests that our/Winskel s lattice-theoretic account of \nparame\u00adterized coinduction might (a) offer a simpler alternative proof of their logic s soundness, and \n(b) support a shallow embedding of their logic in Isabelle/HOL, which is much more ef.cient than a deep \nembedding since it reuses the proof assistant s underlying in\u00adfrastructure. Finally, we note that because \ntheir proof system (like Winskel s) does not offer a way of expressing guarded entailments, it does not \nadmit a circular compositional rule, such as (CO M P OSE). Circular Coinduction. Rather than developing \na custom proof system, another approach to incremental coinduction that has been tried is to engineer \na tactic that builds the full simulation relation on the .y as the interactive proof progresses [5]. \nThe idea is to use a uni.cation metavariable during the proof and try to show that R0 . ?r and ?r . f(?r). \nTo solve the .rst goal, we set ?r := R0 . ?r ', where ?r ' is a fresh metavariable. Each time we later \nencounter a goal R0 ' . R0 . ?r ', where R0' . R0, we instantiate ' R ' '' '' ?r := 0 . ?r , where ?r \nis another fresh metavariable, and proceed. If this exploration phase ever terminates, then at the end \nthere should be one uninstantiated ?r(n) metavariable left, which we can instantiate to the empty relation, \n\u00d8. In this way, we will have constructed a valid post.xed point of f incrementally, and Coq can thus \nconclude that R0 . . f . Under this approach, even if the construction of the simulation relation can \nbe done incrementally using clever tactics, the whole simulation has to eventually be constructed. As \na result, this ap\u00adproach does not support compositional reasoning. Coen s Principle. Isabelle s standard \nlibrary (theory Inductive) contains an interesting coinduction principle for sets attributed to Martin \nCoen. Generalized to complete lattices, it reads: x g f(\u00b5y. f (y) U x U .f ) =. x g .f (COEN) This principle \nis strictly less useful than our parameterized coinduc\u00adtion. While it lets us remember the initial x \nfor as many unfoldings of .f as desired, it does not let us accumulate further knowledge during the proof, \nnor does it support compositional reasoning. In\u00adterestingly, if we change the least .xed point into a \ngreatest one, then the result, f(.y. f (y) U x U .f ), is equivalent to our Gf . Parametric Corecursion. \nMoss [14] .rst introduced the con\u00adstruction that we use in this paper in a categorical setting, calling \nit parametric corecursion and proving an analogue of Theorem 4. He was chie.y concerned with de.ning \nelements of coinductive sets, rather than constructing proofs of coinductive predicates, and thus did \nnot observe the compositionality of this simple construc\u00adtion nor its practical utility in mechanized \ntheorem proving. More speci.cally, in his work, given sets X and Y of variables, a func\u00adtion of type \nX . Gf (Y ) models a set of equations of the form { x = fx(Y ) }x.X , where Gf (Y ) is the coinductive \nset de.ned by the functor f (Y + (-)) (which corresponds to our Gf constructor) and fx s are formulae \nof some particular form related to f . Then, given a set of recursive equations of type X . Gf (X + Y \n), the variant of Theorem 4 gives its solution for X, which is a set of non-recursive equations of type \nX . Gf (Y ). In particular, when Y is the empty set, X . Gf (\u00d8) determines elements of Gf (\u00d8), the .nal \ncoalgebra of f (i.e., the coinductive set de.ned by f). Below, we present an example showing that this \nconstruction can be useful for programming (guarded) corecursive functions. Take pcotree to be the set \nof parameterized in.nite binary trees with nodes containing natural numbers, de.ned as follows:  CoInductive \npcotree (R: Type) : Type := | tcons (a: nat) (tl: pcotree R + R) (tr: pcotree R + R). We can then de.ne \nthe coiterator combinator coitr of the follow\u00ading type using the construction above (see the website \nfor details): coitr: .L R, (L . cotree (L + R)) . L . cotree R Now we de.ne the in.nite tree fliptree \ncontaining 1 on every left child and 2 on every right child, depicted below: 1 1 2 I \\  I\\ 1 2 1 2 \n. . . . . . . . . . . . Using coitr, we can write the corecursive de.nition of fliptree without using \nCoFixpoint, thus avoiding guardedness checking: De.nition fliptree : cotree False := coitr (fun onetree \n. tcons 1 (inr (inl onetree)) (inl (coitr (fun twotree . tcons 2 (inr (inr (inl onetree))) (inr (inl \ntwotree))) I))) I. Nice as these examples are, we believe that the more important use of Moss s categorical \nconstruction is in building proofs incre\u00admentally and compositionally using tactics, as we have sought \nto demonstrate in this paper. Application to Agda. We remark that the direct internal approach also works \nfor Agda and Coq s indexed sets, because Lemmas 2 3 and Theorem 4 can be generalized to a lattice where \nthe greatest .xed point Gf (x) exists for a particular f (i.e., it does not have to exist for all f as \nin a complete lattice). We can easily show that indexed sets form (non-complete) lattices, as they have \na natural order, .nite products and coproducts. Also, for a strictly positive function f, we can de.ne \nGf (x) using Agda s 8 constructor (or Coq s CoInductive), which gives a greatest .xed point of f on this \nlattice, and we can thus reason about statements of the form y g Gf (x) using our principle. In this \nsetting, however, we can\u00adnot use Mendler-style recursion because it requires impredicative quanti.cation. \nCoinduction vs Induction in Coq. It is instructive to contrast Coq s support for induction and coinduction. \nIn both cases, Coq provides a built-in (co-)recursion combinator that comes with a syntactic guardedness \ncondition. As we have seen in Section 7.3, it is possible (but quite inconvenient) to use corecursion \ndirectly inside a proof via the cofix tactic. For each inductively de.ned type, Coq additionally generates \ninduction lemmas (proved using the aforementioned recursion combinator), which semantically enforce guardedness. \nThese lem\u00admas can conveniently be applied using the induction tactic, ren\u00addering the use of the low-level \nrecursion combinator inside proofs unnecessary for most users. For coinductively de.ned types, however, \nCoq does not gener\u00adate any lemmas, nor does it provide a coinduction tactic anal\u00adogous to its induction \ntactic. Our work can be seen as .lling this gap by providing lemmas such as G unfold and G acc and the \ntactic pcofix. It is also worth pointing out that, unlike the Coq\u00adgenerated principles for induction, \nours are complete: whatever can be proved using cofix can also be proved using G acc &#38; co. Acknowledgements \nWe would like to give special thanks to Deepak Garg for alerting us to the connection with Winskel s \nwork. We would also like to thank Andreas Abel, Fritz Henglein, Neel Krishnaswami, Andy Pitts, and Glynn \nWinskel for helpful discussions, as well as the anonymous reviewers for their constructive feedback. \nReferences [1] H. R. Andersen, C. Stirling, and G. Winskel. A compositional proof system for the modal \n\u00b5-calculus. In LICS, pages 144 153. IEEE Computer Society, 1994. [2] G. Barthe, M. J. Frade, E. Gim\u00b4enez, \nL. Pinto, and T. Uustalu. Type\u00adbased termination of recursive de.nitions. Mathematical Structures in \nComp. Sci., 14(1):97 141, Feb. 2004. [3] E. Gim \u00b4Codifying guarded de.nitions with recursive schemes. \nenez. In Types for Proofs and Programs, volume 996 of LNCS, pages 39 59. Springer, 1995. [4] A. D. Gordon. \nBisimilarity as a theory of functional programming. Theoretical Computer Science, 228(1-2):5 47, 1999. \n[5] D. Hausmann, T. Mossakowski, and L. Schroeder. Iterative circular coinduction for CoCasl in Isabelle/HOL. \nIn FASE, volume 3442 of LNCS, pages 341 356. Springer, 2005. [6] C.-K. Hur. Heq: a Coq library for heterogeneous \nequality, 2010. Presented at Coq-2 workshop. [7] C.-K. Hur, D. Dreyer, G. Neis, and V. Vafeiadis. The \nmarriage of bisimulations and Kripke logical relations. In POPL, 2012. [8] C. B. Jones. Speci.cation \nand design of (parallel) programs. In IFIP Congress, pages 321 332, 1983. [9] D. Kozen. Results on the \npropositional \u00b5-calculus. Theor. Comput. Sci., 27:333 354, 1983. [10] K. G. Larsen. Proof systems for \nHennessy-Milner logic with recur\u00adsion. In CAAP, volume 299 of LNCS, pages 215 230. Springer, 1988. [11] \nR. Matthes. Recursion on nested datatypes in dependent type theory. In Computability in Europe (CiE), \nvolume 5028 of LNCS, pages 431 446. Springer, 2008. [12] N. P. Mendler. Inductive types and type constraints \nin the second\u00adorder lambda calculus. Annals of Pure and Applied Logic, 51(1\u00ad2):159 172, 1991. [13] R. \nMilner. Communicating and Mobile Systems: The Pi-Calculus. Cambridge University Press, 1999. [14] L. \nS. Moss. Parametric corecursion. Theor. Comput. Sci., 260(1\u00ad2):139 163, June 2001. [15] S. S. Owicki \nand D. Gries. An axiomatic proof technique for parallel programs. Acta Informatica, 6:319 340, 1976. \n[16] A. Popescu and E. L. Gunter. Incremental pattern-based coinduction for process algebra and its Isabelle \nformalization. In FOSSACS, pages 109 127, 2010. [17] D. Sangiorgi. On the bisimulation proof method. \nMathematical Structures in Comp. Sci., 8(5):447 479, Oct. 1998. [18] D. Sangiorgi. Introduction to Bisimulation \nand Coinduction. Cam\u00adbridge University Press, 2011. [19] D. Sangiorgi and J. Rutten. Advanced Topics \nin Bisimulation and Coinduction. Cambridge Tracts in Theoretical Computer Science. Cambridge University \nPress, 2011. . [20] J. Sev c.\u00b4ik, V. Vafeiadis, F. Zappa Nardelli, S. Jagannathan, and P. Sewell. Relaxed-memory \nconcurrency and veri.ed compilation. In POPL, 2011. [21] C. Stirling and D. Walker. Local model checking \nin the modal mu\u00adcalculus. In TAPSOFT, Vol.1 (CAAP), volume 351 of LNCS, pages 369 383. Springer, 1989. \n[22] A. Tarski. A lattice-theoretical .xpoint theorem and its applications. Paci.c J. Math., 5(2):285 \n309, 1955. [23] G. Winskel. A note on model checking the modal .-calculus. In ICALP, volume 372 of LNCS, \npages 761 772. Springer, 1989.   \n\t\t\t", "proc_id": "2429069", "abstract": "<p>Coinduction is one of the most basic concepts in computer science. It is therefore surprising that the commonly-known lattice-theoretic accounts of the principles underlying coinductive proofs are lacking in two key respects: they do not support compositional reasoning (i.e. breaking proofs into separate pieces that can be developed in isolation), and they do not support incremental reasoning (i.e. developing proofs interactively by starting from the goal and generalizing the coinduction hypothesis repeatedly as necessary).</p> <p>In this paper, we show how to support coinductive proofs that are both compositional and incremental, using a dead simple construction we call the parameterized greatest fixed point. The basic idea is to parameterize the greatest fixed point of interest over the accumulated knowledge of \"the proof so far\". While this idea has been proposed before, by Winskel in 1989 and by Moss in 2001, neither of the previous accounts suggests its general applicability to improving the state of the art in interactive coinductive proof.</p> <p>In addition to presenting the lattice-theoretic foundations of parameterized coinduction, demonstrating its utility on representative examples, and studying its composition with \"up-to\" techniques, we also explore its mechanization in proof assistants like Coq and Isabelle. Unlike traditional approaches to mechanizing coinduction (e.g. Coq's cofix), which employ syntactic \"guardedness checking\", parameterized coinduction offers a semantic account of guardedness. This leads to faster and more robust proof development, as we demonstrate using our new Coq library, Paco.</p>", "authors": [{"name": "Chung-Kil Hur", "author_profile_id": "81436594089", "affiliation": "Microsoft Research, Cambridge, United Kingdom", "person_id": "P3977947", "email_address": "gil@microsoft.com", "orcid_id": ""}, {"name": "Georg Neis", "author_profile_id": "81442619526", "affiliation": "MPI-SWS &#38; Saarland University, Saarbruecken, Germany", "person_id": "P3977948", "email_address": "neis@mpi-sws.org", "orcid_id": ""}, {"name": "Derek Dreyer", "author_profile_id": "81548019178", "affiliation": "MPI-SWS, Saarbruecken, Germany", "person_id": "P3977949", "email_address": "dreyer@mpi-sws.org", "orcid_id": ""}, {"name": "Viktor Vafeiadis", "author_profile_id": "81100493655", "affiliation": "MPI-SWS, Kaiserslautern, Germany", "person_id": "P3977950", "email_address": "viktor@mpi-sws.org", "orcid_id": ""}], "doi_number": "10.1145/2429069.2429093", "year": "2013", "article_id": "2429093", "conference": "POPL", "title": "The power of parameterization in coinductive proof", "url": "http://dl.acm.org/citation.cfm?id=2429093"}