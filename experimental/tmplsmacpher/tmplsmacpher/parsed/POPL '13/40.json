{"article_publication_date": "01-23-2013", "fulltext": "\n Sub-Polyhedral Scheduling Using (Unit-)Two-Variable-Per-Inequality Polyhedra Ramakrishna Upadrasta \nAlbert Cohen INRIA and LRI, Universit \u00b4INRIA and erieure \u00b4 e Paris-Sud (11) Ecole Normale Sup \u00b4 Ramakrishna.Upadrasta@inria.fr \nAlb ert.Cohen@inria.fr Abstract Polyhedral compilation has been successful in the design and implementation \nof complex loop nest optimizers and paralleliz\u00ading compilers. The algorithmic complexity and scalability \nlim\u00aditations remain one important weakness. We address it using sub-polyhedral under-aproximations of \nthe systems of constraints resulting from af.ne scheduling problems. We propose a sub\u00adpolyhedral scheduling \ntechnique using (Unit-)Two-Variable-Per-Inequality or (U)TVPI Polyhedra. This technique relies on simple \npolynomial time algorithms to under-approximate a general poly\u00adhedron into (U)TVPI polyhedra. We modify \nthe state-of-the-art PLuTo compiler using our scheduling technique, and show that for a majority of the \nPolybench (2.0) kernels, the above under\u00adapproximations yield polyhedra that are non-empty. Solving the \nunder-approximated system leads to asymptotic gains in complex\u00adity, and shows practically signi.cant \nimprovements when com\u00adpared to a traditional LP solver. We also verify that code generated by our sub-polyhedral \nparallelization prototype matches the per\u00adformance of PLuTo-optimized code when the under-approximation \npreserves feasibility. Categories and Subject Descriptors D.3.4 [Programming Lan\u00adguages]: Processors \nCompilers, Optimization General Terms Approximations, Complexity, Scheduling, Opti\u00admization, Performance \nKeywords Approximation Algorithms, Complexity Theory, Com\u00adpiler Optimizations, Parallelism, Loop Transformations, \nAf.ne Scheduling, Optimization, Geometric Algorithms 1. Motivation Polyhedral compilation is well established \nas the most effective framework to reason with loop programs. At its heart lie the rep\u00adresentation of \nloop nests as polyhedra, and the search for their semantics-preserving loop transformations. In general, \nsemantics preservation amounts to the feasibility of rational polyhedra, while performance and pro.tability \nare related to optimization on ra\u00adtional polyhedra. This simplicity is both the strength as well as the \nweakness of polyhedral compilation. Strength because polyhe\u00addral abstractions are extremely powerful \nto analyze loops, encoding Permission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies are not made or distributed for \npro.t or commercial advantage and that copies bear this notice and the full citation on the .rst page. \nTo copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci.c \npermission and/or a fee. POPL 13, January 23 25, 2013, Rome, Italy. Copyright c &#38;#169;2013 ACM 978-1-4503-1832-7/13/01. \n. . $10.00 multitudes of useful transformations, with the ability to automati\u00adcally parallelize and \ntile them on newer architectures being one of the main practical applications. And weakness because the \nrichness of polyhedra itself becomes an obstacle due to the complexity of its core algorithms. This latter \nissue creates a practical challenge when programs of non-trivial size are being compiled. Our view is \nthat using approximations of polyhedra alleviates and provides a solu\u00adtion for this scalability challenge. \nIn a previous paper [53], we proposed different directions for sub-polyhedral compilation, where approximations \nof general con\u00advex polyhedra could be used so that the problems that are being solved by polyhedral compilers \ncan be made scalable with worst\u00adcase polynomial time guarantee. This was followed by the intro\u00adduction \nof sub-polyhedral scheduling in [54], where we proposed using (U)TVPI sub-polyhedral under-approximations \nto reduce the complexity of af.ne scheduling. In the current paper, we build from the above works and \nmake solid progress towards polyhedral schedulers that are intrinsically scalable in the program size. \nOur techniques rely on strongly poly\u00adnomial algorithms, i.e., whose time complexity is polynomial only \nin the number of integers in the input and not on the bit-size of the input encoding. For example, we \nare able to substitute linear pro\u00adgramming with min-cost .ow and shortest paths problems, which are solvable \nusing graph theoretic methods like the well known Bellman-Ford algorithm. Our method applies to latency \nand depth minimization ap\u00adproaches like Feautrier s algorithm [25], as well as methods cen\u00adtered on loop \ntiling like Bondhugula et al. s PLuTo [13, 29]. 1.1 Polyhedral compilation Af.ne scheduling [21] now \nis a part and parcel of many com\u00adpilers which aspire to compile for parallel architectures (GCC, LLVM, \nIBM XL, Reservoir Labs R-Stream). The seminal work of Feautrier [25] opened the avenue of constraint-based \naf.ne transformation methods, building on the af.ne form of the Farkas lemma. This approach has been \nre.ned, extended and applied in many directions. To cite only two recent achievements at the two extremes \nof the complexity spectrum: the tiling-centric PLuTo al\u00adgorithm of Bondhugula et al. [13] extending the \nForward Com\u00admunication Only (FCO) principle of Griebl et al. [29] for coarse\u00adgrain parallelization, and \nthe complete, convex characterization of Vasilache [55] and decoupled exploration heuristic of Pouchet \net al. [39]. Much progress has been made in the understanding of the theoretical and practical complexity \nof polyhedral compilation problems. Nevertheless, when considering multidimensional af.ne transformations, \nnone of these are strongly polynomial in the size of the program. The lowest complexity heuristics such \nas PLuTo are reducible to linear programming, which is only weakly polyno\u00admial, its traditional Simplex \nimplementation being associated with large memory requirements and having a worst-case exponential complexity. \n 1.2 Unscalability of a current scheduler Figure 1. Unscalability for Large Loop Programs In this section, \nwe show an example of unscalability of cur\u00adrent methods in PLuTo. We have arti.cially unrolled two typical \nkernels from PolyBench [40], matmul and seidel, by a variable number of times so as to increase the number \nof dependences in the loop nests. We have also enclosed the unrolled loops in two time loops , mimicking \nthe behavior of a scienti.c computing kernel. The above transformations induce thousands of dependences \nin the input program. The compilation times are shown in Figure 1. with matmul in blue (crosses) and \nseidel in red (circles). We checked that the compilation time (auto trans time of PLuTo) increases in \na roughly n 5 complexity in the number of statements in the system. The rest of the modules of PLuTo \nin particular, dependence analysis and code generation (CLooG) took signi.cantly less than the above. \nIt may seem that the above unrolling based method is an arti.\u00adcial way to induce unscalability, with \ninlining being a better can\u00addidate for the same in real world benchmarks. While unrolling is much simpler \nto simulate, limitations of current infrastructures do not provide a platform to study the asymptotic \ntime complexity as\u00adsociated with code size increases associated with inlining. Hence, the above examples \ncould only be taken as representatives of the unscalability problem. But it should also be remembered \nthat when discussing the solution time with respect to the input code size in\u00adcrease, the number of constraints \nin the overall LP problem is linear in the number of dependences in the input code. So, a method like \nthe above which gives a representative sample to increase the size of the LP program is not a limitation. \nFurther, in current benchmarks for loop nest optimization like the PolyBench the range of dependences \nis in tens, and it can ar\u00adguably be said that presently there exists no scalability problem like in the \nabove arti.cial examples. But, polyhedral compilers will soon face such large problems, arising from \naggressive interproce\u00addural optimization, domain-speci.c program generation, or simply as the applicability \nrestrictions continue to be lifted [10, 51]. In ad\u00addition, there could be a restriction of the time limit \nin just-in-time compilers that would further exacerbate the scalability problem, such as just-in-time \napplications of LLVM/Polly [30]. Note that IBM XL, LLVM, and R-Stream have schedulers similar to PLuTo. \nIn the following, we aim for lower complexity feasibility and optimization algorithms, with worst-case \nstrongly polynomial bounds, and closer to n 2 time complexity for large-scale and/or just-in-time compilation \napplications.  1.3 Contributions In this paper, we make the following contributions: We show that state-of-the-art \nparallelization and af.ne schedul\u00ading heuristics such as PLuTo can be adapted to (U)TVPI sub\u00adpolyhedra, \nthereby reducing their algorithmic complexity.  Using elementary polyhedral concepts, we present a \nsimple and powerful framework (an approximation scheme) which can be used for designing Under-Approximation \n(UA) algorithms of general convex polyhedra, linearizing the UA problem.  We evaluate these methods \nby integrating them into PLuTo. We show that for a signi.cant percentage of Farkas-polyhedra aris\u00ading \nfrom a wide range of test cases from af.ne scheduling, the (U)TVPI-UAs proposed above are precise enough \nto preserve feasibility. We show that our approximations when solved with a Bellman-Ford algorithm show \nconsiderable improvement in running time over a well established Simplex implementation. Further, we \nshow that preliminary integration of the above UA polyhedra into PLuTo yields code in most cases that \ndoes not suffer signi.cant increase in execution time.  We show how our framework is general enough \nto extend to various other problems in compiler scheduling, either within the af.ne transformations or \nbeyond.   The paper is structured as follows. Section 2 introduces TVPI and UTVPI sub-polyhedra. Section \n3 introduces polyhedral schedul\u00ading, our method to tackle with the scalability problem, and the mathematical \nframework for the linearization of the under-approximation problem and for establishing its correctness. \nSection 4 proposes simple algorithms that under-approximate a general convex poly\u00adhedron into a (U)TVPI \npolyhedron. Section 5 discusses the theo\u00adretical and practical implications of the above algorithms. \nIn Sec\u00adtion 6, we discuss the various methods for having better control of feasibility in the presence \nof multiple polyhedra. In Section 7 we discuss the results of implementing these algorithms in PLuTo. \nIn Section 8 we discuss extensions and related work, and we conclude in Section 9. 2. Sub-Polyhedra: \nTVPI and UTVPI In this section, we brie.y cover some basics of TVPI and UTVPI approximations of polyhedra. \nA more extensive discussion on these, including other .avors of sub-polyhedra as used by the static analysis \ncommunity can be found in our earlier work [53]. For a polyhedron described in constraint form, let m \nbe the number of inequalities, n be the number of variables and B the upper bound on the absolute value \nof the coef.cients describing the system. 2.1 TVPI sub-polyhedra In TVPI polyhedra, each constraint is \nof the form: axi + bxj = c a, b, c . Q. TVPI are obviously closed under projection, and hence many algorithms \non geometric operations that are developed for planar polyhedra (polygons) are directly applicable to \ngeneral TVPI, giving rise to simple algorithms with low complexity. Fur\u00adthermore, the dual of a TVPI \nprogram is a generalized min-cost .ow problem, which can be solved using graph algorithms. So, the linear \nprogramming community has been interested in TVPI poly\u00adhedra because it can be dealt with strongly polynomial \ntime algo\u00adrithms. Application of graph theory to linear programming using TVPI systems was pioneered \nby Shostak [46]. Aspvall and Shiloach [4] showed the polynomiality of the feasibility problem of TVPI-LP \nformulations by introducing a unique strongly polynomial time procedure that can be used to decide the \nrange of a particular variable with respect to a given constant. This latter procedure is a Bellman-Ford \nstyle propagation of values assigned to variables through inequalities in the system, and is the heart \nof all subse\u00adquent algorithms in the TVPI literature. The following result by Wayne [58] is the best \nto date for the TVPI optimization problem: Lemma 2.1.1 [LP optimization on TVPI] Linear programming \noptimization on TVPI systems can be solved in O(m 3 n 2 log mlog B) worst case time. It is well known \nthat for general polyhedra, the optimization and the feasibility problems have the same weakly-polynomial \ntime hardness. But it is interesting to note that till date, they have different complexities on TVPI \nsystems. The feasibility problem on TVPI systems has lower complexity than the above weakly polynomial \ntime result by Wayne on the optimization problem. Network .ow based ( combinatorial ) strongly polynomial \ntime algorithms for the feasibility problem were given by Cohen and Megiddo [16]. Hochbaum and Naor [32] \nshowed that feasibility of TVPI polyhedra can be determined in strongly polynomial time: Lemma 2.1.2 \n[Feasibility on TVPI] Feasibility of TVPI systems can be solved in O(mn 2 log m)worst case time. The \nabove nearly cubic time algorithm by Hochbaum-Naor is surprisingly simple. It embeds the mentioned decision \nprocedure of Aspvall-Shiloach into a binary search, along with a selected ap\u00adplication of Chernikova \non a planar polyhedron. It can be seen that the above result can as well be used to derive strongly polynomial \ntime cubic bounds for Fourier-Motzkin elimination, and for projec\u00adtion of variables from TVPI systems. \nTVPI systems have been used for various problems in abstract interpretation and veri.cation [47].  2.2 \nUTVPI sub-polyhedra (octagons) Octagons have constraints of the form axi + bxj = c;a, b . {0,\u00b11}, c . \nQ, and are called so because in 2-dimensions, their geometric shape is octagonal. They are also referred \nto as Unit Two Variables Per Inequality (UTVPI) because of the nature of their constraints. Since UTVPI \n. TVPI, the complexity bounds of TVPI polyhedra apply to UTVPI polyhedra as well. But, as the dual of \nthe LP formulation of shortest-paths problem has just difference constraints [1] with the form xi -xj \n= c, general UTVPI systems can be solved with same quadratic complexity as Bellman-Ford, giving the following \nlemma: Lemma 2.2 [Feasibility on UTVPI] Feasibility of UTVPI stys\u00adtems can be solved in O(mn)worst case \ntime and in O(m + n) space. The above decision algorithm can also return a feasibility cer\u00adti.cate of \nthe UTVPI polyhedron. UTVPI polyhedra have successfully been used for various problems in abstract interpretation \nand program veri.cation [38]. Also, they have well supported implementations, in Apron [34], the Astr \n\u00b4 ee analyzer [12, 17], and in the Parma Polyhedra Library (PPL) [5].  2.3 Sub-polyhedra vs. general \n(convex) polyhedra The use of sub-polyhedra by the static analysis community has dif\u00adferent requirements \nthan our proposed application to af.ne schedul\u00ading. In the former, they serve as abstract domains and \nprovide oper\u00adations like union, intersection, projection etc., which are ef.ciently solved by (U)TVPI \npolyhedra providing better worst-case com\u00adplexity. The objective functions in polyhedral scheduling are \nvery simple functionals with unit coef.cients, and lexicographic min\u00adima. Our (U)TVPI sub-polyhedral \napproximate solutions are based on the assumption that the objective function itself could be ap\u00adproximated \nand any valid feasibility certi.cate is usually enough. In this aspect, sub-polyhedra fare better than \ngeneral convex poly\u00adhedra for which ef.cient algorithms are usually based on the widely used Simplex \n[18] algorithm. The worst-case complexity of LP is known to be (weakly) polynomial in time [44], and \nthe Simplex algorithm, while taking exponential time on worst-case scenario squashed cube inputs (as \nshown by Klee-Minty), is known to run well in practice [11]. In practice, we are more interested in the \naverage-case com\u00adplexity of LP, to evaluate the merit of our approximation methods. Let Z(m, n)be the \ncomplexity of LP using the Simplex algorithm with m constraints and n variables. Determining typical \nvalue of Z is not an easy task, as it is well known to depend on many details of the algorithm, like \nrelative ratio of m and n, pivoting rule, method of exploiting sparsity, and even many implementation \ndetails. In this paper however, we will be using the following folklore result which does not count the \nbit-size complexity: on a typical input, Z = O((m + n)mn)on average. In the above empirical estimate, \nwe assume that the Gaussian elimination steps of the Simplex are very fast, exploiting sparsity, and \nassuming a linear number O(m + n)of pivoting steps. Also, the above estimate is practically very accurate \nfor the LP pro\u00adgrams we observe, despite the recent invalidation of Hirsch con\u00adjecture [43, 61]. Note \nthat advanced algorithmic analyses using novel perturbation-based probabilistic techniques have even \nbeen proposed by Spielman and Teng [48] to establish the average-case polynomial running time; but these \nresults go way beyond the em\u00adpirical estimate we need to evaluate our algorithms. 3. Polyhedral Scheduling \nand Approximations In this section, we show how the above classes of sub-polyhedra could be used to help \nin overcoming the scalability challenge in af.ne scheduling. 3.1 Polyhedral scheduling and Farkas lemma \nLet us .rst recall some essential notations and results about poly\u00adhedral compilation. The input to any \npolyhedral scheduling algorithm is a polyhe\u00addral dependence graph G, which is a result of a dependence \nanal\u00adysis, and is de.ned to be a multi-graph G = (V, E), where V is the set of statements, and each particular \ndependence edge e . E is annotated with a parametrized polyhedron De. Each of the con\u00adstraints of De \nis af.ne and involves (I, N)where vectors I and N are the iteration and parameter vectors, respectively. \nIn Feautrier s algorithm [25], these are converted into a per\u00addependence edge polyhedron Pe(\u00b5, .), with \n\u00b5-variables being the Farkas multipliers that come from domain constraints and .\u00advariables being the \nFarkas multipliers that come from dependence constraints. This conversion is done by application of the \naf.ne form of the Farkas lemma [44, Corollary 7.1h] given as: Lemma 3.1 [Af.ne Form of Farkas s Lemma] \nLet D be a nonempty polyhedron de.ned by p inequalities akx + bk = 0, for any k . {1,. . . , p}. An af.ne \nform F is non-negative over D if and only if it is a non-negative af.ne combination of the af.ne forms \nused to de.ne D, meaning: p F(x)= .0 + .k(akx + bk ); .k . [0, p].k = 0 k=1 The nonnegative values .k \nare called Farkas multipliers. In the per-edge Farkas polyhedron Pe(\u00b5, .), both of the newly created \n. and \u00b5 variables are called the Farkas multipliers. By putting together all the per-edge Farkas polyhedra, \none obtains an overall Farkas polyhedron P = ne.E Pe, which is amenable to Linear Programming. Any rational \npoint that satis.es P is consid\u00adered a valid schedule. The above application of the Farkas lemma results \nin all the con\u00adstraints in the Feautrier s scheduler, with some additional variables to model the strong/strict \nsatisfaction of dependences at a given di\u00admension of the af.ne schedule. In PLuTo, a different but conceptu\u00adally \nsimilar method results in a majority of dependence constraints of the same form as Feautrier s. It has \nbeen shown elsewhere [27, 53] that these methods re\u00adsult in an LP problem of size m \u00d7 n (d \u00b7 |E|)\u00d7 (d \n\u00b7 |V|), where dis the mean depth of the loop nests. Assuming a usual sim\u00adplex method whose complexity \nhas been alluded in the previous section for solving systems with bounded d leads to a close to O(|E|2|V|) \n= O(|V|5)asymptotic complexity (not counting the bit-size complexity, and assuming |E|= O(|V|2 )), closely \nmatch\u00ading the curves in Figure 1 and leading to unscalability problems.  3.2 Schedule space under-approximation \nIn this paper, we propose that P be Under-Approximated (UA) to improve the scalability of the scheduling \nalgorithm. This means that instead of searching for an optimal feasible point in P, we search in Pa = \nUA(P). The above approximation is legal and only leads to a conservative approximation of losing schedules, \nthough it is well proven [39] that P is highly redundant with respect to schedule points. The overall \nprocess has to ensure that the approx\u00adimation algorithm, as well as the solution .nding time be scalable \nalgorithms. We can restrict these requirements further and say that both of these algorithms should have \nworst-case strongly polyno\u00admial time running times matching the complexities of (U)TVPI polyhedra introduced \nin Section 2. The above approximation can also be done on a per-dependence\u00adedge basis. In this method, \nthe per-dependence-edge Farkas poly\u00adhedra Pe are under-approximated, and the solution is found from the \noverall polyhedron obtained by putting together all the per\u00addependence UAs. Namely, by doing Pa = UA(P)= \nne.E UA(Pe). If the above approximation leads to a non-empty polyhedron, then we can .nd schedule using \nUA(P)instead of P. In the above, we are exploiting the property that each of the individual Pe s of polyhedral \ncompilation are guaranteed to be non-empty [25] directly as a result of dependence analysis. It also \nhelps that each of the per-dependence-edge Farkas polyhedra Pe are comparably much smaller than the overall \npolyhedron. In the rest of this section, we de.ne a simple and sound math\u00adematical background to build \n(U)TVPI approximations of poly\u00adhedra, to linearize the problem of .nding approximations, and to prove \nthat the algorithms we construct in later sections return valid approximations. For giving these suf.ciency \nconditions, we take two approaches: an intuitive and geometric explanation using du\u00adality/polarity in \nnext section, followed by a more direct way using the equivalent Farkas solution.  3.3 Convexity and \napproximations Starting with a polyhedron given in constraint form as P = {x|Ax + b = 0}, we show that \nsimple approximations of P can be obtained by reasoning about over-approximations of the dual (polar), \nassociated with the transpose matrix [A|b]T . To accom\u00adplish the above, we introduce some basic lemmas \nabout convexity and then show how these geometric concepts help reasoning about under-approximations \nand over-approximations in a uni.ed man\u00adner. The reader may refer to standard books for a complete cover\u00adage \n[44, 61]. A more detailed presentation of this material can be found in [52, 54]. 3.3.1 Homogenization \nand conical polarity In projective geometry, homogenization can be done on polyhedra in H-form constraint \nform or in V-form vertex form or gen\u00aderator form. The following de.nition is when P is in H-form. De.nition \n3.3.1a [Homogenization] Let P = P(A, b) (P = {x|Ax + b = 0}) be a H-polyhedron, then its homogenization \nis a cone and is also a H-polyhedron: (( )( )) A b 0 homog(P)= , = C(P) (1) 0T 1 0 It can be noted that \nif Ais a m\u00d7n-matrix (m constraints and n variables), and b is a m\u00d71-vector, then the homogenized constraint \nsystem C (or homog(P)) is of size (m + 1) \u00d7 (n + 1). Note that the constants dimension has become an \nadditional dimension in the (n + 1)-dimensional space. We would be referring to this dimension as homogenizing \ndimension and the other dimensions as non-homogenizing dimensions. Though the homogenization is a rather \ntrivial process, involving special marking of the dimensions, it however needs to be mentioned because \nthis paper deals with (U)TVPI constraints having at most two non-zero coef.cients in the non-homogenizing \ndimensions. The following is the de.nition of polar of a polyhedron: De.nition 3.3.1b [Conical Polarity] \nFor C . Rn, the polar set is de.ned by ** * C = {c . (Rn) : cx = 0 for all x . C} . (Rn) From the above \nde.nition, K, the polar cone corresponding to P can directly be constructed from homog(P)in (1), and \nwhose generator vectors are columns of the following matrix: ( ) AT 0 K = cone (2) bT 1 In matricial \nform, K is of size (n + 1) \u00d7(m + 1), the transpose of constraint matrix of (1), and can also be written \nas the following: ( )()()( ) T T T a a a0 1 j m K = cone,. . . , ,. . . , , b1 bj bm 1 (3)  3.3.2 Polarity, \n(U)TVPI and (U)TCPV Though a polyhedron can be represented in either of H/V-forms there is certain naturality \nin describing the primal in H-form for problems that occur in polyhedral compilation. The polar can be \nbuilt in linear time in V-form as in (3). But converting the primal to V-form or the polar to H-form \nis very costly; it involves the Chernikova algorithm which takes exponential time. In the follow\u00ading \nlemma, we will implicitly be using the above dual interpreta\u00adtion, without making an actual call to Chernikova. \nLemma 3.3.2 [(U)TVPI and (U)TCPV] For a TVPI-polyhedron, the polar has vertices and rays (generators) \nwhich have not more than two non-zero components in the non-homogenizing dimen\u00adsions. For a UTVPI-polyhedron, \nthe polar has generators which have not more than two non-zero components each in the non\u00adhomogenizing \ndimensions, with them being from {-1,+1}. We de.ne the polar of a (U)TVPI constraint as (Unit-)Two-Components\u00adPer-Vector \nor (U)TCPV vector. Further, we de.ne the polar of a (U)TVPI polyhedron as a (U)TCPV polyhedron. 3.3.3 \nUnder-approximation and over-approximation The following lemma and its corollary are standard results: \nLemma 3.3.3a [Polarity and Inclusivity] For any two cones K1 and K2, K1 . K2 . K1 * . K2 * . Corollary \n3.3.3b [Toggling between OA/UA] (OA(K * )) * . K The above corollary means that by taking the polar of \na cone and taking the resultant s Over-Approximation (OA) one obtains a cone whose polar is an UA of \nthe original cone. By using it, the problem of .nding a UA of a polyhedron in H-form is reduced to the \nproblem of .nding a OA of its polar cone in V-form. More speci.cally, if we let Pa (respectively Ka) \nbe the approximation of P (respectively K), we have the following: Ka . K . Pa . P (4) So, the objective \nin the polar space of .nding the TCPV-OA of cone K given in V-form as in (3), is equivalent to .nding \na cone Ka such that: K . cone (Ka) (5) By this equation, if each column of K as in (3) can be written \nas a conical sum of the vectors in Ka, then the approximation remains a valid approximation. If each \nvector in Ka is a (U)TCPV vector, then the corresponding Pa would be a (U)TVPI approximation of P.  \n 3.4 Approximation scheme for TVPI-UA In this section, we formulate the suf.cient conditions to prove \nthat the UAs we will construct in Section 4 are valid approximations. In fact, with the Farkas lemma \nin Section 3.1 and the equivalent homogenization-polarity intuition in Section 3.3, we do have all the \nnecessary ammunition to construct an approximation scheme which allows us to built TVPI-UAs of general \npolyhedra. (UTVPI-UAs are simple extensions of the material in this section.) In the rest of this section, \nfor ease of exposition, we assume that the polyhedron P is 3-dimensional; these are also called 3VPI \npolyhedra. Generalization to n > 3is straightforward.1 Let the j-th column of K, with 1 = j = m, be Kj \n= ( )T (aj , bj )T = aj,1 aj,2 aj,3 bj . Then we have . . . . j,1 j,1 tt0 aj,1 12 j,2 j,2 . . j . aj,2 \n. t1 0 t3 j . . K= . . . cone j,3 j,3 = cone(T ) . aj,3 . 0 t2 t3 j j j bj ppp 1 2 3 (6) where the (t, \np)-variables constitute the elements of the unknown T-matrix and are to be found out. It can be noticed \nthat each column of the matrix Tj is a TCPV vector and hence is a TVPI constraint in the primal space. \nThe above is what we call an approximation scheme. In this scheme: De.nition 3.4 [Approximation scheme] \nA non-TVPI constraint aj,1x1 +aj,2x2 +aj,3x3 +bj = 0in the original system is replaced j,1 j,2 j by the \nset of constraints UA(x1, x2, x3)= {tx2 +p= 1 x1 +t1 1 j,1 j,3 j j,2 j,3 j 0; tx1 + tx3 + p= 0; tx2 + \ntx3 + p= 0}. 22 233 3 In the above scheme, every column vector of K which has more than two non-zero \ncomponents is replaced by a set of TCPV vectors such that the original vector remains in the conical \nsum of the replacements. The conical combination of the replacement vectors would hence be an OA of the \noriginal vector Kj satisfying (5). The above scheme remains valid as long as the (t, p)variables and \nthe (a, b)constants satisfy the convexity requirement. One way for ensuring the same is by making the \n(t, p)-variables satisfy the following additional constraints, which we would be referring to as 1It \nis also possible to reduce a general polyhedron into an equivalent 3VPI one. According to Shostak [46], \nthe transformation has been suggested by R. Tarjan. It is similar to the reduction of an arbitrary boolean \nsatis.ability ( SAT ) problem to a 3SAT problem. This transformation is not an approx\u00adimation, which \nis the subject of this paper. context constraints for reasons that will be exposed later: j,1 j,1 j,2 \nj,2 j,3 j,3 {t+ t= aj,1 ;t+ t= aj,2;t+ t= aj,3; 12 13 23 j j j p1 + p2 + p3 = bj } (7) If such a set \nof T matrices can be found for each non-TCPV vec\u00ad {} m tor of K, then we have Ka = T1, T 2 ,. . . , \nT and the resultant {} TCPV approximation would be K . cone T1 , T 2 ,. . . , T m . We have the following \ntheorem: Theorem 3.4 [TVPI and UA] Pa is a valid TVPI-UA of the original Polyhedron P. Proof We employ \nthe af.ne form of Farkas lemma, with the premise of non-emptiness of P guaranteed because of the per\u00adconstraint \nmethod. For the UA to be proper, the af.ne form cor\u00adresponding to the original constraint aj x + bj should \nbe express\u00adible as a positive sum of the replacement TVPI vectors: aj,1x1 + j j j,1 j,2 j j j,1 aj,2x2 \n+aj,3x3 +bj = .0 +.(tx1 +tx2 +p)+.(tx1 + 11 1 122 j,3 j j,2 j,3 j tx3 + p) + .j (tx2 + tx3 + p), where \nthe .-multipliers 2 2333 3 j j j j satisfy ., ., ., .= 0. Pairwise matching for each of the x\u00ad 0 1 2 \n3 variables yields: j j,1 j j,1 j j,2 j j,2 {aj,1 = .1t+ .2t;aj,2 = .1 t+ .3t; 12 13 j j,3 j j,3 j jjjjj \n= .+ .= .} (8) aj,3 2t2 3t3 ;bj 0 + .1p1 + .2p2 + .3 p3 j j j Setting .= .= .= 1 yields the context \nconstraints de.ned 1 2 3 in (7) proving the validity of the approximation, except for the additional \n0 in the homogenizing-dimension-matching. Setting .j .j 0 = 0 is safe, and we will see later that it \nactually contributes to the quality of the approximation. The problem remains to .nd such a set of (t, \np)variables satis\u00adfying the above context constraints (7), so that the approximation remains valid. It \ncan be seen that searching for the (t, p)variables directly could lead to a non-linear (quadratic) formulation. \nEven searching for t-variables that satisfy the above constraints turns out to be non-linear. But, as \nthey are existentially quanti.ed, the equations can be solved using advanced quanti.er elimination tech\u00adniques \nlike [49], which are well known to be unscalable beyond small inputs and certainly not polynomial. The \nnext section proposes some linearizations of the above men\u00adtioned approximation scheme, so that the approximation \nalgorithms remain scalable. This is done .rst as a heuristic where both (t, p)\u00advariables are arbitrarily \n.xed, then as a more careful method where only the t-variables are .xed, while the p-variables are found \nby an LP formulation. 4. TVPI and UTVPI UA Algorithms In this section, we use the framework developed \nin earlier section and develop worst-case polynomial time algorithms for obtaining (U)TVPI under-approximations \nof Polyhedra. 4.1 The median method for TVPI-UA In this section, we introduce a simple, per-constraint, \nstrongly poly\u00adnomial heuristic, using the framework developed in Section 3.4. The main idea of this approximation \nis (5), saying that the original vector can be approximated by any set of the replacement TCPV vectors, \nas long as the former remains in the cone of the latter. De.nition 4.1 [Median method: 3-d case] The \ninequality ax + by + cz + 1 = 0 can be approximated by the set of inequalities 2 22 {ax + by + = 0; \nax + cz + = 0; by + cz + = 0; }. 3 33  Geometric intuition for the above derives from the observation \nof the polar space, where the above approximation is the following: . . . . a 1 2 a 1 2 a 0 1 1 . . . \nb . b 0 2 b . 2 . . cone . . 1 1 c . c . 0 c 2 2 1 11 1 3 33 The intuition for the above approximation \ncomes from (6). The values of the t-variables and p-variables need to satisfy (7). As explained earlier, \nthe t-variables have to be instantiated a priori to avoid solving a non-linear problem. The method in \nthis section .xes the pvariables also in a heuristic manner by dividing the available budget in the homogenizing \ndimensions equally between the values in the homogenizing dimensions of the replacement TCPV vectors. \nWe call it the median method because the original vector is the median of the replacement vectors in \nthe polar space. General n-d case The above can be easily generalized to n-d polyhedra. Let s be the \nsparsity of a polyhedron, i.e., the number of non-zero variables (outside the homogenizing dimension) \nfor a () speci.c constraint, with 1 = s = n. Let q = 2s = s(s-1)/2and let r = s -1. The resultant set \nof TCPV vectors corresponding to the particular constraint are n + 1 dimensional, with cardinality q. \nThe coef.cients of the non-homogenizing dimensions are divided by r, while the homogenizing dimension \nis uniformly divided by q. In each of the cases, it can be seen that the approximation being proposed \nis a TCPV approximation, making its polar a TVPI ap\u00adproximation. It can also be veri.ed using the construction \ngiven in the earlier sections and equations (5) and (6) that the UA proposed in each case is a valid \napproximation. Example 1 Let the input system be the triangular pyramid (3d\u00adsimplex) {x, y, z = 0; x \n+ y + z = 1}. Only the inequality x + y + z = 1 is not TVPI and is approximated by the set of in\u00adequalities \n{x+y = 2 ;x+z = 2 ;y+z = 2 }. It can be seen that the 3 3 3 approximation is a non-empty TVPI system \n(it is a UTVPI system), 2 211 with vertices {(0,0,0),(2 ,0,0),(0, ,0),(0,0, ),(1 , , )}, 3 3 3333 each \nof which are inside the vertices of the original system {(0,0,0),(1,0,0),(0,1,0),(0,0,1)}. (The above \nexample is similar [21, 25] to the Farkas system induced in the compilation of the matrix-vector-product: \ns[i]+ = A * x[i].) D With reference to the choice of Farkas multiplier .0 in Theo\u00adrem 3.4, it can be \nseen that setting .0 = 0 in the above exam\u00adple has the advantage that the three replacement TVPI hyperplanes \nintersect exactly on the original non-TVPI hyperplane. Any other strictly positive choice for .0 would \nmean that the point of intersec\u00adtion would lie strictly inside the positive half-space of the original \nand thus giving rise to an UA which is in a way less effective. Example 2 Let the input system be the \nskewed triangular pyra\u00admid {x, y, z = 0; 1000x + 100y + 10z = 1}. Only the in\u00adequality 1000x + 100y + \n10z = 1 is not TVPI and is approxi\u00admated by the set of inequalities {1000x + 100y = 2 3 ; 1000x + 10z \n= 2 3 ; 100y + 10z = 2 3 }. It can be seen that the resul\u00adtant approximation is a non-empty TVPI system \nwith vertices: 1 1 10 100 {(0,0,0),( 1 ,0,0),(0, ,0),(0,0, ),( 1 , , )}, 1500 150 15 3000 3000 3000 each \nof which are inside the vertices of the original system, which are {(0,0,0),( 1 ,0,0),(0,0, 1 ),(0, 1 \n,0)}. D 1000 10 100 It can be seen that the median method is simple and easy to implement, but does not \nhave any guarantee of ensuring that the resultant approximation is non-empty. In the next section, we \nwill generalize this method to formulate a parametrized approximation and formulate an LP problem to \n.nd the approximation.  4.2 LP-based parametrized TVPI approximation To ease the exposition, we will \nprimarily deal with 3-dimensional polyhedra again; higher dimensional extensions are straightfor\u00adward. \nThe median method can easily be extended by de.ning the approximation as a parametrization on the values \nin the homoge\u00adnizing dimension entries: as UA2(aj x = bj )= {(x,pj )|aj,1x1 + aj,2x2 = 2p1 j ;aj,1x1 \n+ aj,3x3 = 2p2 j ;aj,2x2 + aj,3 x3 = jj j 2p3;p= bj ;} where the values of the 3-dimensional p\u00advector \nare unknown and have to be found out. In the above approximation, it can be observed that the coef\u00ad.cients \nin the non-homogenizing dimensions (t-variable values) have been .xed, much similar to their choice in \nthe median method. But, the coef.cients in the homogenized dimension (p-variable val\u00adues) are unknown \nand have to be found out. The context constraint j jjj p= p1 +p2 +p3 = bj is not arbitrary. It is determined \nby the choice of the multipliers for the t-variables so that the Kj vector is in the convex-sum of Tj \n, as given in (6). The resultant system is SH D (x,pj ) = {(x,pj )|a1x = b1;. . . ;aj-1x = bj-1; UA2 \n(aj x = bj );aj+1x = bj+1 ;. . . ;amx = bm;}. In the following discussion, we show that the higher di\u00admensional \nsystem SH D (x,pj )can be interpreted in two ways, a geometric and an algorithmic ways, each having its \nown merits. 4.2.1 A parametrized approximation SH D can geometrically be considered as a parametrized \napproxi\u00admation, with pj being the parametric vector and the context con\u00ad  straintpj = bj considered \nas the parametric context. When the values of the vector pj are known, then the system: S2(x)= SH D (x,pj \n)|pj (9) is a non-parametrized LP problem, which can be tested for feasibil\u00adity in the usual x-variables. \nSince the context constraint pj = bj is respected, it follows from the proofs of earlier sections that \nS2(x) is a proper approximation of S(x). If the value of the vector pj is not known, S2 (x)can be considered \nas a parametrized approx\u00adimation of S(x). Note that the context constraint is only on the p-variables, \nwhile the t-variables have been assigned a .xed value. 4.2.2 An LP formulation Algorithmically we can \nconsider SH D (x,pj )to be a non-parametric LP system with unknown variables (x,pj ). Such an interpretation \nis possible because there exist no non-linear terms in the de.ni\u00adtion of S2 (x,pj )and it is only the \nfeasibility of the approximation that is interesting. Supposing the system SH D (x,pj )is solved for \nfeasibility, with the unknown variables vector as (x,pj ), and a valid assignment for the values of both \nx-variables as well as the pj -variables is found, then the system S2 (x)as given by (9) can be considered \nan approximation of the system S(x), as long as the pj -variables satisfy the constraint pj = bj . On \nthe other hand, SH D (x,pj )is a higher dimensional system than S(x)and hence cannot be considered as \nan approximation of the latter. It is an intermediate form useful for algorithmic purposes. Example 3 \nHere is a reduced example from Banerjee s book [8]. S(x, y, z)= {(x, y, z)| -z + 3 = 0; x -z = 0; -y \n+ z -1 = 0; -x + y + z + 1 = 0}. It is clearly not a TVPI system as the fourth constraint is non-TVPI. \nThe median method applied to this constraint yields the system S1(x, y, z) = {(x, y, z)| - z + 3 = 0; \nx -z = 0; -y+ z -1 = 0; -x + y+ 2 3 = 0; -x + z + 2 3 = 0; y + z + 2 3 = 0}, which turns out to be an \nempty system. On the other hand, the method in this section would lead to the following higher dimensional \nsystem: SH D (x, y, z, p1, p2, p3) = {(x, y, z, p1 , p2, p3)|-z+3 = 0; x-z = 0; -y+z-1 = 0; -x+ y+2p1 \n= 0; -x+z+2p2 = 0; y+z+2p3 = 0; p1 +p2 +p3 = 1}, where the variables p1, p2, p3 are additional context \nvariables and the last constraint p1 + p2 + p3 = 1 is a context constraint. When system SH D is solved \nfor a feasible point, and the set of p-variables that are obtained as solution (1 2 ,0, 1 2 ) are substituted, \nwe obtain S2(x, y, z)= {(x, y, z)| - z + 3 = 0; x -z = 0; -y + z -1 = 0; -x + y+ 1 = 0; -x + z = 0; y+ \nz + 1 = 0}. The reader can verify the satis.ability of the two approximations S1 and S2. D The above \nmethod involves only one call to a standard LP solver. The disadvantage is that the system SH D (x,p) \nhas n + ) (1aj 1 2 = n + Jaj J(Jaj J - 1)/2 O(n + Jaj J2)dimensions, where Jaj Jis the number of non-zero \nelements in the vector aj . As the method involves a call to an LP solver, its theoretical cost is not \nstrongly polynomial time.  4.3 Multiple constraint LP formulations When there exist multiple non-TVPI \nconstraints in S(x), each one of them has to approximated to .nd a TVPI approximation of the polyhedron. \nLet mk be the number of non-TVPI constraints in S(x), with mk = m. Without loss of generality, we can \nas\u00adsume that the constraints have been ordered such that the non-TVPI constraints come .rst, followed \nby the TVPI constraints. This means that the constraints of S(x)are {1,. . . , mk ,. . . , m}, with the \nconstraints {1,. . . , mk } being non-TVPI constraints and the constraints {mk + 1,. . . , m}being TVPI \nconstraints. 4.3.1 One-shot method A straightforward way in which one can .nd a TVPI approxima\u00adtion of \nthe above non-TVPI system S(x) is to construct a sys\u00adtem SH D (x,p 1 ,. . . , p in which all the non-TVPI \nconstraints mk ) in S(x) are approximated using the scheme described in Sec\u00adtion 4.2.2. This system can \nbe solved using an LP formulation in variables as {x,p 1 ,. . . , , p mk }. An approximation of S(x)could \nbe found as SH D (x,p 1 ,. . . , p mk )|p 1 ,. . . , p mk . It can easily be shown that the latter system \nis a proper approximation as long as the context constraints p 1 = b1,. . . , p mk = bmk are respected. \nBut, .nding the approximations of all the non-TVPI constraints simultaneously in the above fashion would \nlead to a large LP sys\u00ad ) (1al1 tem. SH D (x,p 1 ,. . . , p mk )could have up to n + mk = l=1 2 n + mk \nJalJ(JalJ - 1)/2-dimensions which could be as large l=1 as O(n+JSmkJ3 JSmk S), with SJbeing the average \nnumber of non\u00adzero coef.cients in the non-TVPI constraints in S(x). 4.3.2 Iterative methods We can also \niterate the above process described in Section 4.2.2 for each of the mk non-TVPI constraints in S(x)on \nan iterative basis. Clearly, there could be choice in the methods in whether the original system is being \nupdated with the approximation constraints of each non-TVPI constraint or not. We refer to the case when \nthe original system is immediately updated as the incremental method. We refer to the case when the approximations \nof all non-TVPI constraints are found by constructing LP formulations on the same LP system as the independent \nmethod. It could be noticed that each of the above methods involves mul\u00adtiple LP calls: one for each \nnon-TVPI constraint S(x). This means making upto O(m)LP calls in total for building the approximation \nsystem. But, the dimension of each of the LP systems is in the or- S J2 der of O(n + JSmk), which is \nmuch more reasonable than the previous one-shot formulation.  4.4 Per constraint UTVPI-UA of TVPI Let \nus sketch a simple per-constraint algorithm that takes a TVPI constraint and returns its UTVPI under-approximation. \n From Lemma 3.3.2, a vector in the polar space needs to be TCPV, and should have equal magnitude components \nin the non\u00adhomogenizing dimensions for the original to be UTVPI. So, the intuition for this algorithm \nis similar to the TCPV-OA, namely that reasoning about the original TCPV vectors in the polar space and \ncomputing a set of UTCPV vectors which OA the original TCPV vector resulting in an OA. Suppose the TCPV \nvector has components as (a, b, p) in the (xi, xj , x0) dimension (with x0 being the homogenizing dimen\u00adsion), \nthen we can replace it with two UTCPV vectors. The re\u00adplacement has to just take care of the fact that \nthe new UTCPV vectors are such that the original vector is in the conical sum of the replacements. As \nthe case when a = b means that the vec\u00adtor is already a UTCPV vector, the other cases can be handled \nin a > b : cone{(b, b, 2 p )T ,(a -b, 0, p 2 )T } the following way:p p . )T )T a < b : cone{(a, a, \n2 ,(0, b -a, 2 } In either of the above cases, it can be noticed that the .rst vector is a UTCPV vector \nand the second is an interval vector. The .rst case is illustrated in Figure 2. Figure 2. TCPV to UTCPV \napproximation Lemma 4.4 [Validity of the above UTVPI approximation] Given a TVPI constraint, the above \nmethod returns a valid UTVPI-UA of the constraint. Proof The geometric proof derives from the observation \nthat the sum of the corresponding UTCPV replacement vectors is the orig\u00adinal vector. Hence every vector \nin the polar space that is reachable by the original vector is reachable by these replacement vectors, \nmeaning that they give an OA in the polar space. In the primal space, the replacements necessarily give \nan UA (Cor. 3.3.3b). Example 4 Let PV = conv{(1,1),(4,1),(1,2)}, with the H\u00adform of P being PH = {1 = \nx = 4; 1 = y = 2; x + 3y = 7}. It can be seen that P is a TVPI system, but not a UTVPI system, as the \nthird constraint is a non-UTVPI constraint. The UTVPI approximation by the above method is {1 = x = 4; \n1 = y = 2; x + y 2 ; 2y 2 }which can be seen to be non-empty. D = 7 = 7 The cost of .nding the UA is \nlinear and the method is simple to implement, just like the median method mentioned in Section 4.1. Just \nlike that method, this method does not have any guarantee that the approximated system is non-empty. \n 4.5 LP based parametrized UTVPI approximation This approximation is similar to the parametrized TVPI \napproxima\u00adtion of Section 4.2, in the sense of searching for pvariables instead of t s. We are not covering \nit for lack of space. 5. Metrics and Discussion Let us now study the size of the new system, the complexity \nof the conversion, the overall complexity of .nding a solution, and discuss some fundamental and methodological \nlimitations of our approach. 5.1 Sizes Let the original matrix be of size m \u00d7 n: m constraints and n \nvariables with mk and mt being the number of non-TVPI and TVPI constraints respectively. Also let the \noverall sparsity factor of the system be s , which means that for a constraint in the input system, on \naverage s variable elements are non-zero. It will be seen in Sections 7.1 that for practical purposes, \ns is a small constant (little more than 4) and relatively independent of n. TVPI-UA For a system described \nas above, each of the TVPI-UA methods (given in Sections 4.1 and 4.2) replace a non-TVPI () 1aj 1 constraint \naj with the same number of 2 = Jaj J(Jaj J-1)/2 TVPI constraints. Doing the above process for each of \nthe mk non-TVPI constraints creates an approximated TVPI system of size ma \u00d7 n, where ma = JSmk JSmk \nJ - 1)mk/2. It can be mt + SJ(Sassumed that the new system is approximately of the size s 2 m\u00d7n. In the \nrare case that s = n (which means that the constraint matrix does not have any zero entries) then the \nsize of the resultant TVPI n(n-1)m 2 constraint system is 2 \u00d7n which is of the order of n m\u00d7n. UTVPI-UA \nFor UTVPI approximation given in Section 4.4, we have the same order of complexity of additional constraints \nas in the TVPI-UA case, though double in number. Constraint graph The Bellman-Ford algorithm constructs \na con\u00adstraint graph, or incidence matrix, of nearly twice the size of the input UTVPI system before solving \nit: a m\u00d7n input system results in (2n + 1) nodes and (2m + 2n)edges, due to simple transfor\u00admations of \naddition of positive and negative forms of each variable [38, Section 2.2], and addition of a pseudo-source-vertex \nfor the shortest-paths problem [1]. The former is a relaxation of UTVPI into difference constraints and \nis exact for rational points [38], though some (odd) integer points in the original are lost.  5.2 Complexity \nof conversion Both the median method of TVPI approximation covered in Sec\u00adtion 4.1 and the UTVPI approximation \ncovered in Section 4.4 are strongly polynomial time algorithms as they do not use any LP call when constructing \nthe approximation. The parametric approxima\u00adtion covered in Section 4.2.2 is a weakly polynomial time \nalgorithm for it needs to solve an LP problem for .nding the homogenizing dimension values. For the case \nof multiple constraints, complexity of conversion is one LP call for the one-shot algorithm of Section \n4.3.1, and one LP call per non-TVPI constraint for both incremental and independent algorithms. Each \nof the above numbers are weakly polynomial time, but are worst-case times nonetheless. 5.3 Complexity \nof .nding a solution For the TVPI approximation, as the worst-case complexity of Hochbaum-Naor is O(mn \n2 log m) [32], applying it to the resul\u00adtant approximated system would lead to O( s 2 mn 2 log sm) time \nwith constraint sparsity s , and O(n 4 m log nm)in the unlikely case when s = O(n). For the UTVPI approximation, \nas the theoretical worst-case complexity of solving UTVPI systems is O(mn) (if we use the traditional \nBellman-Ford algorithm on the difference constraints), the corresponding complexities would be O( s 2 \nmn), and O(n 3 m) respectively. We will see in Section 7.1 empirical evidence suggest\u00ading that TVPI constraints \nare always UTVPI in practice, making this method very attractive.  5.4 Pre/post-processing It can be \nnoticed that our algorithms do not ask for removal of redundant inequalities, which is as hard as determining \nif the input system is feasible. Polyhedra in compilation have lot of duplicate constraints, which gets \nre.ected in the approximations as well. Compilers like PLuTo remove these by methods like textual matching \nwhich leads to a large decrease in the number of constraints. A more advanced scheme, .nely-tuned to \nUTVPI constraints, would use a hashing or radix-sort technique (like in [3]) packing the UTVPI constraint \nin a few integers: exploiting the fact that the non-homogenizing dimen\u00adsion values are from {0,\u00b11}, while \nthe homogenizing dimension values are always from a very small set [-5,5]. It would bring the simpli.cation \ncomplexity to linear time. We thus believe that the problem of duplicate elimination is asymptotically \ninsigni.cant. The following example shows an inherent limitation of the process of TVPI-UA itself. Example \n5 The polyhedron described using the constraints {x + y+z = -1; x +y+z = 1}is not TVPI. Though it is \nunbounded in both directions, the only TVPI approximations that are possible of the above polyhedron \nare bounded TVPI polyhedra, which can be considered a failure of the UA approach. D The above kinds of \npolyhedra can be considered as degener\u00adate cases and identi.ed by a pre-processing step that removes \nthe lineality space from the input polyhedron. Further, the Farkas poly\u00adhedra that arise in polyhedral \nscheduling are always pointed poly\u00adhedra and can never have non-trivial lineality space. 5.5 Integer \nscaling and TU polyhedra The parametric approximations of Sections 4.2 and 4.5 can ensure that the resulting \ndifference constraints are integer constraints of the type xi - xj = c, where c . Z. This would involve \nscal\u00ading up the pseudo-parametric variables, changing the context con\u00adstraints accordingly, and solving \nan ILP problem instead of the usual LP problem though on a per-Pe basis. Such a local trans\u00adformation \nwill not only induce integer schedules because incidence matrices are a sub-class of network matrices, \nbut also has the addi\u00adtional property that all vertices of the resultant overall polyhedron are integral \n(because such matrices are Totally Unimodular [44, Chapter 19]). We will show in Section 8 that this \nmethod would help in polynomial time approximations of NP-Complete problems and code-generation. 6. Feasibility \nControl With Many Polyhedra The previous sections discuss algorithms for the under approxima\u00adtion of \none single polyhedron, so that the UA polyhedron can be solved as a means of trading expressiveness (e.g., \nthe ability to .nd good af.ne transformations) for scalability. In af.ne scheduling however, the overall \nsystem P is an intersection of many polyhe\u00addra, each of them arising from a dependence-edge as in the \ncase of Feautrier s scheduler [25, 26], or the above along with some addi\u00adtional constraints as in the \ncase of Bondhugula s scheduler [13, 29]. More speci.cally: Feautrier s scheduler to minimize latency: \nPL = ne.E Pe, where Pe is the per-dependence-edge Farkas polyhedron. A given dependence edge induces \na (weak satisfaction) constraint in the system until it is strongly satis.ed by outer dimensions of the \nmultidimensional af.ne schedule (i.e., the sink of a depen\u00addence is scheduled at a strictly higher time \nstep than the source).  Bondhugula s FCO scheduler to expose loop tiling opportun\u00adties: PF C O = ne.E \nPe nv.V Hv nv.V Nv , where Pe is similar to Feautrier s scheduler s per-dependence polyhedron, enforc\u00ading \nweak dependence satisfaction only, but at all dimensions of the multidimensional af.ne schedule; while \nHv collects the per-statement linear independence constraints, and Nv collects the per-statement non-negativity \nor trivial solution avoidance constraints.  These two cases are different because, thanks to the condi\u00adtions \nin [26], the polyhedron built by Feautrier s multi-dimensional scheduler is known to be always feasible. \nThis condition however is not necessarily true for FCO-based systems: though the number of problematic, \nadditional constraints |HV |+ |NV |is small (around 2|V|or less than 10%) when compared to Farkas multiplier \ncon\u00adstraints |PE |, the overall polyhedron could be empty, complicating the UA procedure. The feasibility \npreservation problem is to .nd a non-empty UA(P)in each of the above cases, without making any queries \nlike an LP call on the overall system P. 2 In this section we discuss practical methods to increase the \ncontrol on feasibility for the overall approximated system. 6.1 Feautrier s scheduler Let us .rst discuss \na technique for Feautrier s scheduler. Let us start by approximating each of the Pe by calling the one-shot \nalgorithm discussed in Section 4.3.1, and construct the overall approximation by putting all the individual \napproximations together: UA(P) = ne.E UA(Pe). The overall approximation is the result of |E|calls to \nthe one-shot procedure. This will be affordable because each of the Pe-s are quite small. Feautrier s \nalgorithm always .nds a (rational) multi-dimensional af.ne schedule, which guarantees that P is a feasible \npolyhedron. The main question is whether feasibility will be preserved when intersecting the per-dependence \napproximations. It may sound as a surprise that the answer is yes. Notice that he original schedule of \nthe source program is always feasible. The key idea is to seed the approximation of the per-dependence \nPe polyhedra, forcing this original schedule into the UA. It simply amounts to substituting the values \nof the coef.cients of the original schedule in the Farkas constraints, and adding the resulting (in)equalities \nto the systems before applying the one-shot UA method. This result is important because of the large \nnumber of af.ne scheduling heuristics deriving from Feautrier s algorithm. Feautrier s .ne-grain parallelization \nmethod is already useful for loop vector\u00adization [26]. Our seeding approach is directly applicable to \ncoarse\u00adgrain parallelization heuristics as well [36]. We could stop there and declare success. But more \nadvanced methods combining tiling for locality enhancement and parallelism extraction require additional \neffort to preserve feasibility. The next section studies the most successful of these heuristics, pushing \nus to enhance the under-approximation method to tackle feasibility preservation in presence of more complex \naf.ne constraints.  6.2 FCO scheduling in PLuTo In this section, we discuss the approximation of P in \nthe context of Bondhugula s FCO scheduler, as implemented in PLuTo. Here, seeding with the original schedule \nis not possible since the loops of the source program may not be directly permutable/tilable. We study \na variety of Per-Dependence constraint clustering techniques. PD1: Fully-separate Each Pe is approximated \nalone, and all the additional constraints are approximated together as in: UA(P) = ne.E UA(Pe)nUA(nv.V \nHv nv.V Nv ). The overall approxima\u00adtion is the result of |E|+ 1 applications of the one-shot method. \nPD2: Total augmentation Each Pe is augmented with all the additional constraints, and this augmented \npolyhedron is approx\u00adimated, as in: UA(P)= ne.E UA(Pe nv.V Hv nv.V Nv ). The overall approximation is \nthe intersection of |E|one-shot approx\u00adiations, and each element in HV and NV could be approximated multiple-times. \nPD3: Selected augmentation Each Pe is augmented with its sec\u00adtion of the additional constraints Hu and \nNu (for statements u on 2If such a query could be made, then it could as well be used to solve for the \nobjective function resulting in the schedule... either side of dependence edge) and the augmented per-dependence \npolyhedron is approximated. If e : v1 . v2, UA(P)= neUA(Pen Hv1 nHv2 nNv1 nNv2 ). The overall approximation \nis the inter\u00adsection of |E|one-shot approximations. PD4: Just Farkas Each Pe is approximated independently, \njust like in Feautrier s scheduler, while the additional constraints are thrown away. The overall approximation \nis the intersection of |E| one-shot approximations. This method clearly does not give an UA. It is meant \nto bet\u00adter characterize the source of the infeasibility in the previous three methods. Indeed, considering \nthe intersection of UAs of per\u00addependence, weak-satisfaction constraints we know that the Pe s are homogeneous \ncones. This tells us that the origin can be used as a seed to build a non-empty approximation. 7. Experimental \nResults We conduct systematic experiments, tiling and parallelizing the PolyBench (2.0) [40] with the \nsource-to-source polyhedral opti\u00admizer PLuTo (PLuTo-0.6). Subsequent to our experiments, later versions \nof PolyBench( 3.x) have been released, and which have similar numbers. Farkas systems are extracted from \nPLuTo. We compare the default linear programming calls to PIP [24] with the TVPI-UA and then the UTVPI-UA \nalgorithms. The UTVPI-UA is further relaxed into a constraint graph (or incidence matrix) which is fed \nto a custom implementation of the Bellman-Ford algorithm. Note that we still use PIP for the much smaller \nlinear program\u00adming problems arising from the feasibility enhancing heuristics. Also, Pluto makes use \nof PIP in parts unrelated with the scalability of af.ne scheduling problems: the PIP calls from PLuTo \noriginate from the functions dep satisfaction test (DS), get dep direction (DIR) and find permutable \nhyperplanes (FPH). We focus on FPH calls which correspond to af.ne scheduling problems. There are a lot \nmore calls of the DS and DIR variety when compared to the FPH calls, optimization of the former pair \nof LP calls is entirely a different problem from the FPH variety in many ways. Primarily, the former \nneed to be over-approximated [21, 53] as otherwise, it results in incorrect transformations. The latter \nof course are the main topic of this paper and need to be under-approximated leading to a conservative \napproximation and perhaps loss of useful schedules. 7.1 Features of the polyhedra Here we are referring \nto Table 1.a. The initial three columns refer to the size of the loop nest: L is the number of loops, \nS the number of statements, and D the number of dependences. The next sets of columns indicate the polyhedral \ncharacteristics of the different varieties of calls from PLuTo. As the DS and DIR variety are similar \ntypes of calls, they have been summarized together. The remaining 8 columns (Table 1.b) will be discussed \nin the next section. The number of polyhedra of different types are indicated in the P columns (PDS, \nPDIR and PFPH ). As written earlier, there are lot more polyhedral calls of DS/DIR than when compared \nto FPH. But the former are smaller calls and are linearly dependent on the size of the loop nest. For \na particular benchmark and variety of polyhedra (DS/DIR or FPH), n and m columns indicate the number \nof variables (unknowns), and average number of constraints in the particular LP formulation respectively. \nm t column indicates the average number of TVPI constraints for that benchmark and variety of polyhedra. \nDS/DIR As it can be expected, most of the constraints in the DS/DIR polyhedra are TVPI constraints. In \nthese polyhedra, there  Table 1. (a) Problem size, Polyhedral and TVPI-ness characteristics, (b) UA \neffectiveness is never more than 1constraint per polyhedron which is non-TVPI. In all the cases, whenever \na constraint is TVPI, it is always a UTVPI constraint, having the same absolute magnitude for the two \ncoef.cients, when it has two entries. FPH In the FPH variety, it can be noticed that the sizes of some \nof the FPH polyhedra are small and comparable to the DS/DIR polyhedra. This is either the result of a \nsmall problem size or because PLuTo uses Fourier-Motzkin elimination, along with a syntactic heuristic \nto reduce the duplicate constraints. As it can be expected, m t, number of TVPI constraints in the original \nsystem, is highly benchmark dependent. But, just like in DS/DIR polyhedra, a TVPI constraint is always \na UTVPI constraint. S JSmk J is the average number of non-zero coef.cients in the non-TVPI constraints \nfor that benchmark. It can be seen that this number, though again being benchmark dependent, is a small \ncon\u00adstant when compared to the dimension size n. m a is the number of constraints in the new (approximated \nTVPI) system. As seen ear- SJ(S lier, m a = JSmk JSmk J - 1)(m -mt)/2 + mt. The relative growth of the \napproximated system with respect to the original one is de.ned as the ratio between the sum of entries \nin the m a and m columns. We found the average value of this to be 8, meaning that the overall sparsity \nfactor s is a little more than 4. Sometimes the growth of the approximated system is signi.cant, but \nit has to be remembered that m a is the number of constraints without any simpli.cation, while m is obtained \nafter systematic simpli.cation and elimination of duplicates in PLuTo. For comparison purposes with m \na, we have added the m u column, which is the average unsimpli.ed system size when the simpli.cation \ntechniques used in PLuTo are turned off. It can be observed that m u and m a are of comparable sizes. \nOur experience is that when the approximated system undergoes simpli.cation and duplication removal techniques, \nit leads to a much smaller system, comparable to the one in m columns. Also, asymptotically, the size \nincrease that only depends on sparsity does not matter much.  7.2 UA feasibility Here we are referring \nto Table 1.b. These numbers are for TVPI-UA. (The results for UTVPI-UA are similar and are not being \ngiven because of space constraint.) These columns refer to the median method discussed in Section 4.1, \nto the LP based independent method discussed in Section 4.3.2 and the per-dependence methods discussed \nin Section 6 respectively. All the columns except for the LP-indep method refer to the per-dependence \nmethods. Only the LP-indep method was implemented on the overall Farkas system, one that is obtained \nafter putting together all the individual systems and after simpli.cation by PLuTo. The PLuTo calls \nof the FPH variety are for LexMin. In the cur\u00adrent table, we discuss the feasibility results only. The \ncolumns YY (Yes-Yes), YN (Yes-No), denote the feasibility (Y) or infeasibility (N) of the original and \napproximated systems respectively. They have been highlighted accordingly. Since the FPH system is used \nto .nd an optimization point in the overall system, a YN entry would mean loss of parallelization. It \ncan be seen that the LP-indep method (29 YY and 16 YN cases or 10 out of 16 PolyBench problems) performs \nmuch bet\u00adter than the median method (19 YY and 26 YN cases or 6 out of 16 PolyBench problems). The latter \nperforms not as poorly as the simplicity of the approach would hint to. We expect the incremental method \nto have much better performance than the current indepen\u00addent method. The results of PD1 (not shown) \nfare marginally better than the median method. The results of PD2 are close to LP-indep (24 YY and 21 \nYN cases or 8 out of 16 PolyBench loop nests). Advanced clustering strategy PD3 has not been implemented. \nPD4 (Just Farkas) gives 100% feasibility preservation results (45 YY and 0 YN), making it quite attractive. \nThe next sections study the impact on compilation time and on the performance of the gener\u00adated code. \n 7.3 Scalability comparison: Simplex vs. Bellman-Ford  the latter is our own implementation of a standard \nBellman-Ford algorithm called for testing for presence of negative weight cycles. The input programs \nmatmul and seidel are the same as in Fig\u00adure 1, and each call of auto trans of PLuTo to .nd schedule \nhy\u00adperplanes has tens of calls of the above variety, which explains the difference in scales. Prior to \nsolving either of these, the duplicates were eliminated using a syntactic matching like currently in \nPLuTo. Even on the systems with duplicates, the improvements were simi\u00adlar, though being more prominent. \nIt can be seen from the graphs that for their respective inputs, BF (worst-case O( s 2 mn) O(|E||V|) \n O(|V|3)) is asymp\u00adtotically as well as empirically faster than PIP s Simplex (observed O((m + n)mn) \nO(|V|5 )), answering positively the scalability challenge. The regression coef.cients (linear R2) for \nthe above for\u00admulae are around 99.9%. The curves for BF appear linear because the x-axis counts the number \nof dependences, which is practically O(|V|2). Though the memory improvements are not shown in the above \ngraphs, the linear memory of BF is considerably lighter than the quadratic memory of Simplex. The relatively \nlesser magnitudes and hence lesser improvements of seidel when compared to matmul could be attributed \nto the fact that while the former has just uniform dependences which are easily amenable to the Gaussian \neliminations of PIP, the latter has more complex af.ne dependences and also induces many more Farkas \nmultiplier variables. 7.4 UA generated code performance Perf. Comparison (Seconds) Benchmark Orig Par \ncur Par new Til cur Til new gemver 0.31 0.15 (2x) 0.15 (2x) 0.15 (2x) 0.15 (2x) mvt 1.40 0.27 (5x) 0.28 \n(5x) 0.42 (5x) 0.43 (5x) seidel 11.8 3.6 (3x) 3.6 (3x) 11.5 (1x) 11.5 (1x) Table 2. UA Code Performance \nWe are referring here to Table 2, limiting ourselves to a subset of the YY cases in the previous table \nthat our current implemen\u00adtation could handle; we will later consider all YY and YN cases with a more \nrobust implementation. In each case, we replaced the original system(s) by the approximated TVPI ones \nobtained by the independent LP method. The cost function was unchanged and the solution was found using \nPIP. It can be seen that performance gains closely match the default polyhedral method in PLuTo, despite \nthe approximation taking place. The impact of YN approximations on PLuTo s effectiveness is yet to be \nstudied, but we express some hope on PLuTo s loop distribution heuristic to break infeasible sys\u00adtems. \n8. Extensions and Related Work In this section we will .rst see how our framework can be applied to other \n(scalability) problems, either in af.ne scheduling framework or beyond, and then cover some related work. \n 8.1 Applications to other loop transformation problems One important strength of our under-approximation \nframework is its ability to trade precision for scalability using (U)TVPI approx\u00adimations of polyhedra. \nScalability arises from the excellent worst\u00adcase complexities of sub-polyhedral operations, and the ability \nto largely implement the approximation as an independent optimiza\u00adtion problem for each dependence-edge. \nThe latter property is an\u00adother strength of our approach, which happens to be general enough to be applied \nto other compilation problems, related and unrelated to scheduling. Darte-Vivien s scheduling As the \nframework of Feautrier which we build from is the most powerful among the class of algorithms that .nd \naf.ne schedules [56], the LP formulation in Darte-Vivien s scheduling [22] can directly be approximated \nby our method. Shifting 1-d loops for pipelining and compaction To solve the cyclic scheduling (decomposed \nsoftware pipelining) problem, Calland et al. [14] give an LP formulation whose constraint ma\u00adtrix is \nbuilt from incidence matrices of the original and a retimed dependence graph, and prove it to be a Totally \nUnimodular (TU) matrix. As given, the formulation is not a (U)TVPI polyhedron having at most 3 non-zero \nelements per row. But, it can bene.t from our framework as the constraints are constructed on a per\u00addependence-edge \nbasis, and the constraint matrix elements belong to the set {0,\u00b11}. Darte-Huard s loop shifting for compaction \n[19] uses a very similar framework to the above paper, where an LP for\u00admulation which is a variation \nof a min-cost .ow problem is used, and hence could as well bene.t from a (U)TVPI approximation resulting \nin a strongly polynomial time algorithm. n-d loop alignment for fusion Darte-Huard show that external \nshifting in the multi-dimensional loop alignment problem is NP-Complete [20]. They provide an ILP formulation \nconstructed on a per-dependence-edge basis, with 4 variables per constraint and general coef.cients (not \njust {0,\u00b11}). A possible heuristic for their formulation that builds on our techniques would be to under\u00adapproximate \neach per-edge constraint and to solve the overall sys\u00adtem using the Bellman-Ford algorithm. This is correct \nbecause rational relaxation and subsequent UA may result in less integer points, but no new ones. The \noverall UA, if non-empty, can be solved in polynomial time. Further, it can be made to directly re\u00adsult \nin an integer multi-dimensional shift using the integer scaling discussed earlier in Section 5.5. In \nthis method, an ILP problem is solved on a per-Pe-basis, while solving a normal Bellman-Ford problem \non the overall UA polyhedron resulting in an integer multi\u00addimensional shift. In this aspect, UTVPI polyhedra \nare very special, and such a polynomial time shifting algorithm will not be possible even with a TVPI-UA, \nas solving integer-TVPI polyhedra is known to be NP-Complete [35]. Simpler code generated As seen earlier, \nby the TU property, a restriction of the homogenizing dimensions to integers can ensure that all the \nvertices of the resultant UTVPI-UA are integers. Using these UAs in scheduling has the additional advantage \nof simpler code being generated, even with af.ne schedules generated through (rational) linear programming \n(PLuTo uses integer linear program\u00adming by default). Rational schedule coef.cients may indeed induce \nmodulos in loop bounds and conditional expressions of the gener\u00adated code, a risk completely eliminated \nwith our approach. 8.2 Related work Feautrier s scalable modular scheduling Feautrier s approach [27] \nstarts with Gaussian elimination, and suggests a Minkowski decomposition (generator representation) of \nPe for projection. We consider this approach as complementary to ours in the sense of per-dependence \nmethods, but the use of generator representation makes it less likely to be scalable than ours. Approximations \nin loop optimization Previous approaches to ap\u00adproximations in loop optimization concerned themselves \nwith re\u00adstricting the kinds of input programs or the kinds of transforma\u00adtions being searched for [21, \n60]. The most notable examples of the above are the Dependence Levels of Allen and Kennedy [2] and Direction \nVectors of Wolf and Lam [59]. Each of the above are less powerful than the af.ne scheduling model of \nFeautrier [25]. Our approach tackles the scalability problem from within the af.ne scheduling model and \nis more powerful. (U)TVPI in loop optimization UTVPI polyhedra have previously been used by Balasundaram \nand Kennedy [7] in task level loop par\u00adallelization and pipelining applications, by data dependence anal\u00adysis, \ndomain simpli.cation and (limited form of) code generation. Their use is similar to the framework of \nclassical dependence over\u00adapproximations [21, 53, 60]. Our use of (U)TVPI is more pow\u00aderful than theirs \nbecause of our af.ne scheduling framework and UA algorithms. Further, Pugh [42] and Maydan et al. [37] \nalso hint that (U)TVPI polyhedra could be solved with better complexity and hence used in dependence \nanalysis citing some statistics from par\u00adallelization benchmarks. Our statistics in Table 1.a could be \nseen as extensions of the above. (U)TVPI in static analysis General polyhedra are known [31, 33, 38] \nto be considerably more expensive than (U)TVPI polyhedra for abstract domain operations. Hence, sub-polyhedra \nhave been widely employed in abstract interpretation problems by the static analysis community as a means \nof trading precision for scalabil\u00adity. A comparison of the use of sub-polyhedra, along with their applicability \nto polyhedral compilation can be found in [53]. Of the many classes of sub-polyhedra, (U)TVPI polyhedra \nhave ex\u00adtensively been used [6, 38, 47], with impressive results such as in Astr \u00b4ee [12, 17]. Approximations \nto sub-polyhedra We are not aware of any pre\u00advious algorithm for .nding approximations of general polyhedra \ninto sub-polyhedra other than .nding the interval ( box ) polyhe\u00addral OAs. The literature is scarce about \npolyhedral approximations and more so for UAs. (The na\u00a8ive method of projecting out the extra variables \non a per-constraint basis using Fourier-Motzkin method leads to an over-approximation with no bound on \nthe complexity.) Vivien-Wicker [57] propose an algorithm to .nd the parallelepiped-OA of a 3d-polyhedron \nin vertex representation. Min \u00b4e [38, Sec\u00adtion 4.3] adapts the vertex method for .nding the UTVPI-OA \nof a general polyhedron. Simon et al. [47, Section 3.2.6] propose an iterative algorithm for the TVPI-OA \nof a general polyhedron using LP. Our algorithm for TVPI-UA in Section 4.3 can be seen as com\u00adplementary \nto the latter, though our algorithm formulates the LP problem by searching only for the replacements \nin the homogeniz\u00ading dimension. Feasibility and optimization algorithms The state of the art in feasibility \ntesting for TVPI systems is by Hochbaum-Naor [32], and for optimization of TVPI systems by Wayne [58]. \nWe are not aware of existing implementations of the above algorithms. The state of the art for feasibility \nand optimization of UTVPI systems is by the well understood Bellman-Ford algorithm [41], which involves \nconstructing a nearly 2m \u00d7 2n size constraint graph encoding the difference constraints [23] and testing \nit for presence of negative weight cycles using the shortest-path problem. The latter has been extensively \nstudied, beginning from [9, 28] to most recently in [15]. Ef.cient, though closure based methods for \nthe same [6, 38] are available in Apron [34] and PPL [5]. More general forms than ours, like boolean \nformulae involving UTVPI constraints have been considered in constraint programming, like by Seshia et \nal. [45]. 9. Conclusions and Future Work We have presented sub-polyhedral scheduling using (U)TVPI poly\u00adhedra. \nWe have proposed worst-case polynomial time algorithms to compute (U)TVPI under-approximations from a \ngeneral convex polyhedron. We have shown initial results of the above approxima\u00adtions as well as their \nintegration in PLuTo. Our experiments clearly indicate an asymptotic improvement in scalability, answering \nposi\u00adtively the scalability challenge. To stay within polynomial time, care was taken to linearize the \nunder-approximation model, avoiding the exponential vertex and ray construction associated with the Chernikova \nalgorithm. The most dif.cult problem is the lack of a scalable way to reliably pre\u00adserve the non-emptiness \nof the underapproximation when the orig\u00adinal polyhedron is non-empty. The one-shot linear programming \nmethod offers this guarantee, but not in strongly polynomial time. We explore different heuristics relying \non bounded-size linear pro\u00adgrams to trade complexity for feasibility. We know this problem is dif.cult \nbecause a strongly polynomial method with this guaran\u00adtee could directly lead to a solution of .nding \nstrongly polynomial feasibility test for general polyhedra. The scalability experiments indicate an asymptotic \nimprove\u00adment of Bellman-Ford over linear programming. Future experi\u00adments should include a fast duplicate \nelimination, a complete eval\u00aduation of the performance impact of the approximation on all Poly-Bench \nusing an approximated objective function compatible with the Bellman-Ford algorithm, and a complementary \nstudy of the memory complexity (known to be overwhelmingly in our favor). An illustration of the power \nof our approximation scheme has been shown on important af.ne scheduling problems. Future work would \ninvolve applying the scheme to a wider range of compila\u00adtion problems, including ILP or mixed-ILP formulations, \nso that ei\u00adther heuristics with better complexity measures are obtained, or the resulting approximations \nprovide practical improvements on com\u00adplex systems. Acknowledgments We are grateful to Paul Feautrier \nfor the tremendously helpful suggestions and insights. We are also thank\u00adful to C\u00b4edric Bastoul and Armin \nGr \u00a8 o\u00dflinger for their feedback at different stages. Our work also bene.ted from Uday Bondhugula s PLuTo \nframework, and from the detailed comments from anony\u00admous reviewers. This work was partly supported by \nthe French Nano2012 project Mediacom, in collaboration with STMicroelec\u00adtronics, and by the European \nFP7 project CARP id. 287767. References [1] R. K. Ahuja, T. L. Magnanti, and J. B. Orlin. Network .ows: \ntheory, algorithms, and applications. Prentice-Hall, Inc., NJ, USA, 1993. [2] R. Allen and K. Kennedy. \nAutomatic translation of fortran programs to vector form. ACM Trans. Program. Lang. Syst., 9(4):491 542, \nOct. 1987. [3] A. Andersson and S. Nilsson. Implementing radixsort. J. Exp. Algo\u00adrithmics, 3, Sept. 1998. \n[4] B. Aspvall and Y. Shiloach. A polynomial time algorithm for solving systems of linear inequalities \nwith two variables per inequality. SIAM J. Comput., 9(4):827 845, 1980. [5] R. Bagnara, P. M. Hill, and \nE. Zaffanella. The Parma Polyhedra Li\u00adbrary: Toward a complete set of numerical abstractions for the \nanalysis and veri.cation of hardware and software systems. Science of Com\u00adputer Programming, 72(1 2):3 \n21, 2008. [6] R. Bagnara, P. M. Hill, and E. Zaffanella. Weakly-relational shapes for numeric abstractions: \nimproved algorithms and proofs of correctness. Formal Methods in System Design, 35(3):279 323, 2009. \n[7] V. Balasundaram and K. Kennedy. A technique for summarizing data access and its use in parallelism \nenhancing transformations. In PLDI, pages 41 53, 1989. [8] U. Banerjee. Loop Transformations for Restructuring \nCompilers: The Foundations. Kluwer Academic Publishers, Boston, 1992. [9] R. Bellman. On a routing problem. \nQuarterly of Applied Mathematics, 16:87 90, 1958. [10] M.-W. Benabderrahmane, L.-N. Pouchet, A. Cohen, \nand C. Bastoul. The polyhedral model is more widely applicable than you think. In Proceedings of the \nInternational Conference on Compiler Construc\u00adtion (ETAPS CC 10), number 6011 in LNCS, Paphos, Cyprus, \nMar. 2010. Springer-Verlag. [11] R. E. Bixby. Solving real-world linear programs: A decade and more \nof progress. Oper. Res., 50(1):3 15, Jan. 2002. [12] B. Blanchet, P. Cousot, R. Cousot, J. Feret, L. \nMauborgne, A. Min \u00b4e, D. Monniaux, and X. Rival. A static analyzer for large safety-critical software. \nIn PLDI, pages 196 207. ACM, 2003. [13] U. Bondhugula, A. Hartono, J. Ramanujam, and P. Sadayappan. A \npractical automatic polyhedral parallelizer and locality optimizer. In PLDI, pages 101 113, 2008. [14] \nP.-Y. Calland, A. Darte, and Y. Robert. Circuit retiming applied to decomposed software pipelining. IEEE \nTrans. Parallel Distrib. Syst., 9(1):24 35, Jan. 1998. [15] B. V. Cherkassky, L. Georgiadis, A. V. Goldberg, \nR. E. Tarjan, and R. F. Werneck. Shortest-path feasibility algorithms: An experimental evaluation. J. \nExp. Algorithmics, 14:7:2.7 7:2.37, Jan. 2010. [16] E. Cohen and N. Megiddo. Improved algorithms for \nlinear inequalities with two variables per inequality. SIAM J. Comput., 23:1313 1350, December 1994. \n[17] P. Cousot, R. Cousot, J. Feret, L. Mauborgne, A. Min \u00b4e, and X. Ri\u00adval. Why does Astr \u00b4ee scale \nup? Formal Methods in System Design, 35(3):229 264, Dec 2009. [18] G. B. Dantzig. Linear Programming \nand Extensions. Princeton University Press, Princeton, NJ, 1963. [19] A. Darte and G. Huard. Loop shifting \nfor loop compaction. Int. J. Parallel Program., 28(5):499 534, Oct. 2000. [20] A. Darte and G. Huard. \nComplexity of multi-dimensional loop align\u00adment. In Proceedings of the 19th Annual Symposium on Theoretical \nAspects of Computer Science, STACS 02, pages 179 191, London, UK, UK, 2002. Springer-Verlag. [21] A. \nDarte, Y. Robert, and F. Vivien. Scheduling and Automatic Paral\u00adlelization. Birkha \u00a8user, 2000. [22] \nA. Darte and F. Vivien. Optimal .ne and medium grain parallelism detection in polyhedral reduced dependence \ngraphs. Int. J. Parallel Program., 25:447 496, December 1997. [23] H. Edelsbrunner, G. Rote, and E. Welzl. \nTesting the necklace condition for shortest tours and optimal factors in the plane. Theor. Comput. Sci., \n66(2):157 180, 1989. [24] P. Feautrier. Parametric integer programming. RAIRO Recherche Op\u00b4erationnelle, \n22(3):243 268, 1988. http://www.piplib.org/. [25] P. Feautrier. Some ef.cient solutions to the af.ne \nscheduling problem: I. one-dimensional time. IJPP, 21:313 348, October 1992. [26] P. Feautrier. Some \nef.cient solutions to the af.ne scheduling problem: Part ii: Multidimensional time. Int. J. Parallel \nProgram., 21:389 420, December 1992. [27] P. Feautrier. Scalable and structured scheduling. International \nJournal of Parallel Programming, 34(5):459 487, 2006. [28] L. R. Ford, Jr. and D. R. Fulkerson. Flows \nin Networks. Princeton University Press, 1962. [29] M. Griebl, P. Feautrier, and A. Gr \u00a8Forward communication \no\u00dflinger. only placements and their use for parallel program construction. In W. Pugh and C.-W. Tseng, \neditors, LCPC, volume 2481 of Lecture Notes in Computer Science, pages 16 30. Springer, 2002. [30] T. \nGrosser, H. Zheng, A. Raghesh, A. Simb \u00a8o\u00dflinger, and urger, A. Gr \u00a8L.-N. Pouchet. Polly -Polyhedral \nOptimization in LLVM. In IMPACT 2011, in conjunction with CGO 2011, Chamonix, France, Apr 2011. [31] \nN. Halbwachs, D. Merchat, and L. Gonnord. Some ways to reduce the space dimension in polyhedra computations. \nForm. Methods Syst. Des., 29:79 95, July 2006. [32] D. S. Hochbaum and J. Naor. Simple and fast algorithms \nfor linear and integer programs with two variables per inequality. SIAM J. Comput., 23(6):1179 1192, \n1994. [33] J. Jaffar, M. J. Maher, P. J. Stuckey, and R. H. C. Yap. Beyond .nite domains. In A. Borning, \neditor, PPCP, volume 874 of Lecture Notes in Computer Science, pages 86 94. Springer, 1994. [34] B. Jeannet \nand A. Min \u00b4e. Apron: A library of numerical abstract domains for static analysis. In CAV, pages 661 \n667, 2009. [35] J. C. Lagarias. The computational complexity of simultaneous dio\u00adphantine approximation \nproblems. SIAM J. Comput., 14(1):196 209, 1985. [36] A. W. Lim and M. S. Lam. Communication-free parallelization \nvia af.ne transformations. In POPL, pages 201 214, Paris, France, jan 1997. [37] D. E. Maydan, J. L. \nHennessy, and M. S. Lam. Ef.cient and exact data dependence analysis. In Proceedings of the ACM SIGPLAN \n1991 conference on Programming language design and implementation, PLDI 91, pages 1 14, New York, NY, \nUSA, 1991. ACM. [38] A. Min \u00b4e. The octagon abstract domain. Higher-Order and Symbolic Computation, \n19(1):31 100, 2006. [39] L.-N. Pouchet, U. Bondhugula, C. Bastoul, A. Cohen, J. Ramanujam, P. Sadayappan, \nand N. Vasilache. Loop transformations: convexity, pruning and optimization. In POPL, pages 549 562, \n2011. [40] L.-N. Pouchet, U. Bondhugula, et al. The polybench benchmarks. http://www.cse.ohio-state.edu/~pouchet/software/ \npolybench. [41] V. R. Pratt. Two easy theories whose combination is hard. Techni\u00adcal report, Massachusetts \nInstitute of Technology, Cambridge, Mass, 1977. http://boole.stanford.edu/pub/sefnp.pdf. [42] W. Pugh. \nA practical algorithm for exact array dependence analysis. Commun. ACM, 35(8):102 114, Aug. 1992. [43] \nF. Santos. A counterexample to the hirsch conjecture. Annals of Mathematics, 176:383 412, 2012. [44] \nA. Schrijver. Theory of linear and integer programming. John Wiley &#38; Sons, Inc., New York, NY, USA, \n1986. [45] S. A. Seshia, K. Subramani, and R. E. Bryant. On solving boolean combinations of UTVPI constraints. \nJournal on Satis.ability, Boolean Modeling and Computation (JSAT), 3(1-2):67 90, 2007. [46] R. Shostak. \nDeciding linear inequalities by computing loop residues. J. ACM, 28:769 779, October 1981. [47] A. Simon \nand A. King. The two variable per inequality abstract domain. Higher Order Symbol. Comput., 23(1):87 \n143, Mar. 2010. [48] D. A. Spielman and S.-H. Teng. Smoothed analysis of algorithms: Why the simplex \nalgorithm usually takes polynomial time. J. ACM, 51:385 463, May 2004. [49] A. Tarski. A decision method \nfor elementary algebra and geometry. Univ. of California Press, Berkeley, 2nd edition, 1951. [50] M. \nJ. Todd. The many facets of linear programming. Mathematical Programming, 91:417 436, 2002. [51] K. Trifunovic, \nA. Cohen, D. Edelsohn, F. Li, T. Grosser, H. Jagasia, R. Ladelsky, S. Pop, J. Sj\u00a8odin, and R. Upadrasta. \nGraphite two years after: First lessons learned from real-world polyhedral compilation. In GCC Research \nOpportunities Workshop (GROW 10), Pisa, Italy, Jan. 2010. [52] R. Upadrasta. Scalability Challenges \nin the Polyhedral Model: An Algorithmic Approach using (Unit-)Two-variable Per Inequality Sub-Polyhedra. \nPhD thesis, Universit\u00b4e Paris-Sud (11), Orsay, France, January 2013. [53] R. Upadrasta and A. Cohen. \nPotential and Challenges of Two\u00adVariable-Per-Inequality Sub-Polyhedral Compilation. In First In\u00adternational \nWorkshop on Polyhedral Compilation Techniques (IM-PACT 11), in conjunction with CGO 11, Chamonix, France, \nApr. 2011. [54] R. Upadrasta and A. Cohen. A Case for Strongly Polynomial Time Sub-Polyhedral Scheduling \nUsing Two-Variable-Per-Inequality Poly\u00adhedra. In Second International Workshop on Polyhedral Compila\u00adtion \nTechniques (IMPACT 12), in conjunction with HiPEAC 12, Paris, France, Jan. 2012. [55] N. Vasilache. Scalable \nProgram Optimization Techniques In The Polyhedral Model. PhD thesis, Paris-Sud 11 University, Sept. 2007. \n[56] F. Vivien. On the optimality of feautrier s scheduling algorithm. Concurrency and Computation: Practice \nand Experience, 15(11\u00ad12):1047 1068, 2003. [57] F. Vivien and N. Wicker. Minimal enclosing parallelepiped \nin 3d. Comput. Geom. Theory Appl., 29:177 190, November 2004. [58] K. D. Wayne. A polynomial combinatorial \nalgorithm for generalized minimum cost .ow. In Proceedings of the thirty-.rst annual ACM symposium on \nTheory of computing, STOC 99, pages 11 18, New York, NY, USA, 1999. ACM. [59] M. E. Wolf and M. S. Lam. \nA loop transformation theory and an algorithm to maximize parallelism. IEEE Trans. Parallel Distrib. \nSyst., 2(4):452 471, Oct. 1991. [60] Y.-Q. Yang, C. Ancourt, and F. Irigoin. Minimal data dependence \nabstractions for loop transformations. In K. Pingali, U. Banerjee, D. Gelernter, A. Nicolau, and D. A. \nPadua, editors, LCPC, volume 892 of Lecture Notes in Computer Science, pages 201 216. Springer, 1994. \n [61] G. Ziegler. Lectures on polytopes. Graduate texts in mathematics. Springer Science, 2006.   \n  \n\t\t\t", "proc_id": "2429069", "abstract": "<p>Polyhedral compilation has been successful in the design and implementation of complex loop nest optimizers and parallelizing compilers. The algorithmic complexity and scalability limitations remain one important weakness. We address it using sub-polyhedral under-aproximations of the systems of constraints resulting from affine scheduling problems. We propose a sub-polyhedral scheduling technique using (Unit-)Two-Variable-Per-Inequality or (U)TVPI Polyhedra. This technique relies on simple polynomial time algorithms to under-approximate a general polyhedron into (U)TVPI polyhedra. We modify the state-of-the-art PLuTo compiler using our scheduling technique, and show that for a majority of the Polybench (2.0) kernels, the above under-approximations yield polyhedra that are non-empty. Solving the under-approximated system leads to asymptotic gains in complexity, and shows practically significant improvements when compared to a traditional LP solver. We also verify that code generated by our sub-polyhedral parallelization prototype matches the performance of PLuTo-optimized code when the under-approximation preserves feasibility.</p>", "authors": [{"name": "Ramakrishna Upadrasta", "author_profile_id": "81493658284", "affiliation": "INRIA and LRI, Universit&#233; Paris-Sud (11), Paris, France", "person_id": "P3978038", "email_address": "Ramakrishna.Upadrasta@inria.fr", "orcid_id": ""}, {"name": "Albert Cohen", "author_profile_id": "81100146104", "affiliation": "INRIA and &#201;cole Normale Sup&#233;rieure, Paris, France", "person_id": "P3978039", "email_address": "Albert.Cohen@inria.fr", "orcid_id": ""}], "doi_number": "10.1145/2429069.2429127", "year": "2013", "article_id": "2429127", "conference": "POPL", "title": "Sub-polyhedral scheduling using (unit-)two-variable-per-inequality polyhedra", "url": "http://dl.acm.org/citation.cfm?id=2429127"}