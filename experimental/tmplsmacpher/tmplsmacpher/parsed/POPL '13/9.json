{"article_publication_date": "01-23-2013", "fulltext": "\n Static and Dynamic Semantics of NoSQL Languages V\u00e9ronique Benzaken1 Giuseppe Castagna2 \u00ean1 Kim Nguy \nJ\u00e9r\u00f4me Sim\u00e9on3 1LRI, Universit\u00e9 Paris-Sud, Orsay, France, 2CNRS, PPS, Universit\u00e9 Paris Diderot, Sorbonne \nParis Cit\u00e9, Paris, France 3IBM Watson Research, Hawthorne, NY, USA Abstract We present a calculus for \nprocessing semistructured data that spans differences of application area among several novel query lan\u00adguages, \nbroadly categorized as NoSQL . This calculus lets users de.ne their own operators, capturing a wider \nrange of data process\u00ading capabilities, whilst providing a typing precision so far typical only of primitive \nhard-coded operators. The type inference algo\u00adrithm is based on semantic type checking, resulting in \ntype infor\u00admation that is both precise, and .exible enough to handle structured and semistructured data. \nWe illustrate the use of this calculus by encoding a large fragment of Jaql, including operations and \nitera\u00adtors over JSON, embedded SQL expressions, and co-grouping, and show how the encoding directly yields \na typing discipline for Jaql as it is, namely without the addition of any type de.nition or type annotation \nin the code. Categories and Subject Descriptors D.3.1 [Programming Lan\u00adguages]: Formal De.nitions and \nTheory; F.3.2 [Logics and Mean\u00adings of Programs]: Semantics of Programming Languages Ope\u00adrational semantics; \nF.3.3 [Logics and Meanings of Programs]: Studies of Program Constructs Type structure; H.3.3 [Informa\u00adtion \nStorage and Retrieval]: Information Search and Retrieval Query formulation Keywords NoSQL, BigData Analytics, \nJaql, Cloud Computing, Type Inference. 1. Introduction The emergence of Cloud computing, and the ever \ngrowing impor\u00adtance of data in applications, has given birth to a whirlwind of new data models [17, 22] \nand languages. Whether they are developed under the banner of NoSQL [28, 33], for BigData Analytics [6, \n16, 26], for Cloud computing [4], or as domain speci.c languages (DSL) embedded in a host language [19, \n25, 30], most of them share a common subset of SQL and the ability to handle semistruc\u00adtured data. While \nthere is no consensus yet on the precise bound\u00adaries of this class of languages, they all share two common \ntraits: (i) an emphasis on sequence operations (eg, through the popular MapReduce paradigm) and (ii) \na lack of types for both data and pro\u00adgrams (contrary to, say, XML programming or relational databases \nwhere data schemas are pervasive). In [19, 20], Meijer argues that such languages can greatly bene.t \nfrom formal foundations, and suggests comprehensions [8, 31, 32] as a unifying model. Although we agree \nwith Meijer for the need to provide uni.ed, formal foun- This work was partially supported by the ANR \nTYPEX project n. ANR-11-BS02-007 and by a visiting researcher grant of Fondation Digiteo . Permission \nto make digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior speci.c permission and/or a fee. POPL 13, January \n23 25, 2013, Rome, Italy. Copyright c &#38;#169; 2013 ACM 978-1-4503-1832-7/13/01. . . $15.00 dations \nto those new languages, we argue that such foundations should account for novel features critical to \nvarious application do\u00admains that are not captured by comprehensions. Also, most of those languages provide \nlimited type checking, or ignore it altogether. We believe type checking is essential for many applications, \nwith usage ranging from error detection to optimization. But we understand the designers and programmers \nof those languages who are averse to any kind of type de.nition or annotation. In this article, we propose \na calculus which is expressive enough to capture languages that go beyond SQL or comprehensions. We show \nhow the calculus adapts to various data models while retaining a precise type checking that can exploit \nin a .exible way limited type information, information that is deduced directly from the structure of \nthe program even in the absence of any explicit type declaration or annotation. Example. We use Jaql \n[6, 16], a language over JSON [17] devel\u00adoped for BigData analytics, to illustrate how our proposed calculus \nworks. Our reason for using Jaql is that it encompasses all the fea\u00adtures found in the previously cited \nquery languages and includes a number of original ones, as well. Like Pig [26] it supports sequence iteration, \n.ltering, and grouping operations on non-nested queries. Like AQL [4] and XQuery [7], it features nested \nqueries. Further\u00admore, Jaql uses a rich data model that allows arbitrary nesting of data (it works on \ngeneric sequences of JSON records whose .elds can contain other sequences or records) while other languages \nare limited to .at data models, such as AQL whose data-model is sim\u00adilar to the standard relational model \nused by SQL databases (tuples of scalars and of lists of scalars). Lastly, Jaql includes SQL as an embedded \nsub-language for relational data. For these reasons, al\u00adthough in the present work we focus almost exclusively \non Jaql, we believe that our work can be adapted without effort to a wide array of sequence processing \nlanguages. The following Jaql program illustrates some of those features. It performs co-grouping [26] \nbetween one JSON input, containing information about departments, and one relational input contain\u00ading \ninformation about employees. The query returns for each de\u00adpartment its name and id, from the .rst input, \nand the number of high-income employees from the second input. A SQL expression is used to select the \nemployees with income above a given value, while a Jaql .lter is used to access the set of departments \nand the elements of these two collections are processed by the group ex\u00adpression (in Jaql $ denotes the \ncurrent element). group (depts -> filter each x (x.size > 50)) by g = $.depid as ds, (SELECT * FROM employees \nWHERE income > 100) by g = $.dept as es into { dept: g, deptName: ds[0].name, numEmps: count(es) }; The \nquery blends Jaql expressions (eg, filter which selects, in the collection depts, departments with a \nsize of more than 50 employees, and the grouping itself) with a SQL statement (select\u00ading employees in \na relational table for which the salary is more than 100). Relations are naturally rendered in JSON as \ncollections of records. In our example, one of the key difference is that .eld access in SQL requires \nthe .eld to be present in the record, while the same operation in Jaql does not. Actually, .eld selection \nin Jaql is very expressive since it can be applied also to collections with the effect that the selection \nis recursively applied to the components of the collection and the collection of the results returned, \nand simi\u00adlarly for filter and other iterators. In other words, the expression filter each x (x.size > \n50) above will work as much when x is bound to a record (with or without a size .eld: in the latter case \nthe selection returns null), as when x is bound to a collection of records or of arbitrary nested collections \nthereof. This accounts for the semistructured nature of JSON compared to the relational model. Our calculus \ncan express both, in a way that illustrates the difference in both the dynamic semantics and static typing. \n In our calculus, the selection of all records whose mandatory .eld income is greater than 100 is de.ned \nas: let Sel = nil => nil | ({income: x, .. } as y , tail) => if x > 100 then (y,Sel(tail)) else Sel(tail) \n(collections are encoded as lists \u00e0 la Lisp) while the .ltering among records or arbitrary nested collections \nof records of those where the (optional) size .eld is present and larger than 50 is: let Fil = nil => \nnil | ({size: x, .. } as y,tail) => if x > 50 then (y,Fil(tail)) else Fil(tail) | ((x,xs),tail) => (Fil(x,xs),Fil(tail)) \n| (_,tail) => Fil(tail) The terms above show nearly all the basic building blocks of our calculus (only \ncomposition is missing), building blocks that we dub .lters. Filters can be de.ned recursively (eg, Sel(tail) \nis a recur\u00adsive call); they can perform pattern matching as found in functional languages (the .lter \np . f executes f in the environment resulting from the matching of pattern p); they can be composed in \nalterna\u00adtion (f1| f2 tries to apply f1 and if it fails it applies f2), they can spread over the structure \nof their argument (eg, ( f1,f2) of which (x,Sel(tail)) is an instance requires an argument of a prod\u00aduct \ntype and applies the corresponding fi component-wise). For instance, the .lter Fil scans collections \nencoded as lists \u00e0 la Lisp (ie, by right associative pairs with nil denoting the empty list). If its \nargument is the empty list, then it returns the empty list; if it is a list whose head is a record with \na size .eld (and possibly other .elds matched by . . ), then it captures the whole record in y, the content \nof the .eld in x, the tail of the list in tail, and keeps or discards y (ie, the record) according to \nwhether x (ie, the .eld) is larger than 50; if the head is also a list, then it recursively applies both \non the head and on the tail; if the head of the list is neither a list, nor a record with a size .eld, \nthen the head is discarded. The encoding of the whole grouping query is given in Section 5.1. Our aim \nis not to propose yet another NoSQL/cloud comput\u00ading/bigdata analytics query language, but rather to \nshow how to express and type such languages via an encoding into our core cal\u00adculus. Each such language \ncan in this way preserve its execution model but obtain for free a formal semantics, a type inference \nsys\u00adtem and, as it happens, a prototype implementation. The type infor\u00admation is deduced via the encoding \n(without the need of any type annotation) and can be used for early error detection and debugging purposes. \nThe encoding also yields an executable system that can be used for rapid prototyping. Both possibilities \nare critical in most typical usage scenarios of these languages, where deployment is very expensive both \nin time and in resources. As observed by Mei\u00adjer [19] the advent of big data makes it more important \nthan ever for programmers (and, we add, for language and system designers) to have a single abstraction \nthat allows them to process, transform, query, analyze, and compute across data presenting utter variability \nboth in volume and in structure, yielding a mind-blowing number of new data models, query languages, \nand execution fabrics [19] . The framework we present here, we claim, encompasses them all. A long-term \ngoal is that the compilers of these languages could use the type information inferred from the encoding \nand the encoding itself to devise further optimizations. Types. Pig [26], Jaql [16, 27], AQL [4] have \nall been conceived by considering just the map-reduce execution model. The type (or, schema) of the manipulated \ndata did not play any role in their de\u00adsign. As a consequence these languages are untyped and, when present, \ntypes are optional and clearly added as an afterthought. Differences in data model or type discipline \nare particularly im\u00adportant when embedded in a host language (since they yield the so-called impedance \nmismatch). The reason why types were/are disregarded in such languages may originate in an alleged tension \nbetween type inference and heterogeneous/semistructured data: on the one hand these languages are conceived \nto work with collec\u00adtions of data that are weakly or partially structured, on the other hand current \nlanguages with type inference (such as Haskell or ML) can work only on homogeneous collections (typically, \nlists of elements of the same type). In this work we show that the two visions can coexist: we type data \nby semantic subtyping [15], a type system conceived for semi\u00adstructured data, and describe computations \nby our .lters which are untyped combinators that, thanks to a technique of weak typing in\u00adtroduced in \n[10], can polymorphically type the results of data query and processing with a high degree of precision. \nThe conception of .lters is driven by the schema of the data rather than the execution model and we use \nthem (i) to capture and give a uniform semantics to a wide range of semi structured data processing capabilities, \n(ii) to give a type system that encompasses the types de.ned for such languages, if any, notably Pig, \nJaql and AQL (but also XML query and processing languages: see Section 5.1), (iii) to infer the pre\u00adcise \nresult types of queries written in these languages as they are (so without the addition of any explicit \ntype annotation/de.nition or new construct), and (iv) to show how minimal extensions/modi.\u00adcations of \nthe current syntax of these languages can bring dramatic improvements in the precision of the inferred \ntypes. The types we propose here are extensible record types and het\u00aderogeneous lists whose content is \ndescribed by regular expressions on types as de.ned by the following grammar: Types t ::= v (singleton) \n||||||| { e:t, . . . , e:t} { e:t, . . . , e:t , .. } [r] int | char any | empty | null t| t t\\ t (closed \nrecord) (open record) (sequences) (base) (special) (union) (difference) Regexp r ::= e | t | r* | r+ \n| r? | r r | r| r where e denotes the empty word. The semantics of types can be expressed in terms of \nsets of values (values are either constants such as 1, 2, true, false, null, 1 , the latter denoting \nthe character 1 , records of values, or lists of values). So the single\u00adton type v is the type that contains \njust the value v (in particular null is the singleton type containing the value null). The closed record \ntype { a:int, b:int} contains all record values with exactly two .elds a and b with integer values, while \nthe open record type { a:int, b:int , .. } contains all record values with at least two .elds a and b \nwith integer values. The sequence type [r] is the set of all sequences whose content is described by \nthe regular expres\u00adsion r; so, for example [char*] contains all sequences of charac\u00adters (we will use \nstring to denote this type and the standard double quote notation to denote its values) while [({ a:int} \n{ a:int} })+] denotes nonempty lists of even length containing record values of type { a:int} . The union \ntype s| t contains all the values of s and of t, while the difference type s\\ t contains all the values \nof s that are not in t. We shall use bool as an abbreviation of the union of the two singleton types \ncontaining true and false: true| false. any and empty respectively contain all and no values. Recursive \ntype de.nitions are also used (see Section 2.2 for formal details).  These types can express all the \ntypes of Pig, Jaql and AQL, all XML types, and much more. So for instance, AQL includes only homogeneous \nlists of type t, that can be expressed by our types as [ t* ]. In Jaql s documentation one can .nd the \ntype [ long(value=1), string(value=\"a\"), boolean* ] which is the type of arrays whose .rst element is \n1, the second is the string \"a\" and all the other are booleans. This can be easily expressed in our types \nas [1 \"a\" bool*]. But while Jaql only allows a lim\u00adited use of regular expressions (Kleene star can only \nappear in tail position) our types do not have such restrictions. So for exam\u00adple [char* @ char* . (( \nf r )|( i t ))] is the type of all strings (ie, sequences of chars) that denote email ad\u00addresses ending \nby either .fr or .it. We use some syntactic sugar to make terms as the previous one more readable (eg, \n[ .* @ .* ( .fr | .it )]). Likewise, henceforth we use { {a?:t} to de\u00adnote that the .eld a of type t \nis optional; this is just syntactic sugar for stating that either the .eld is unde.ned or it contains \na value of type t (for the formal details of this encoding see the full version of this work available \non line). Coming back to our initial example, the .lter Fil de.ned before expects as argument a collection \nof the following type: type Depts = [ ( {size?: int, ..} | Depts )* ] that is a, possibly empty, arbitrary \nnested list of records with an optional size .eld of type int: notice that it is important to specify \nthe optional .eld and its type since a size .eld of a different type would make the expression x > 50 \nraise a run-time error. This information is deduced just from the structure of the .lter (since Fil does \nnot contain any type de.nition or annotation). We de.ne a type inference system that rejects any argument \nof Fil that has not type Depts, and deduces for arguments of type [({size: int, addr: string}| {sec: \nint} | Depts)+] (which is a subtype of Depts) the result type [({size: int, addr: string}|Depts)*] (so \nit does not forget the .eld addr but discards the .eld sec, and by replacing * for + recognizes that \nthe test may fail). By encoding primitive Jaql operations into a formal core cal\u00adculus we shall provide \nthem a formal and clean semantics as well as precise typing. So for instance it will be clear that apply\u00ading \nthe following dot selection [ [{a:3}] {a:5, b:true} ].a the result will be [ [3] 5 ] and we shall be \nable to deduce that _.a applied to arbitrary nested lists of records with an op\u00adtional integer a .eld \n(ie, of type t = { {a?:int} | [ t * ] ) yields arbitrary nested lists of int or null values (ie, of type \nu = int | null | [ u * ]). Finally we shall show that if we accept to extend the current syntax of Jaql \n(or of some other language) by some minimal .lter syntax (eg, the pattern .lter) we can obtain a huge \nimprovement in the precision of type inference. Contributions. The main contribution of this work is \nthe de.ni\u00adtion of a calculus that encompasses structural operators scattered over NoSQL languages and \nthat possesses some characteristics that make it unique in the swarm of current semi-structured data \nprocessing languages. In particular it is parametric (though fully embeddable) in a host language; it \nuniformly handles both width and deep nested data recursion (while most languages offer just the former \nand limited forms of the latter); .nally, it includes .rst-class arbitrary deep composition (while most \nlanguages offer this opera\u00adtor only at top level), whose power is nevertheless restrained by the type \nsystem. An important contribution of this work is that it directly com\u00adpares a programming language approach \nwith the tree transducer one [13]. Our calculus implements transformations typical of top\u00addown tree transducers \nbut has several advantages over the trans\u00adducer approach: (1) the transformations are expressed in a \nformal\u00adism immediately intelligible to any functional programmer; (2) our calculus, in its untyped version, \nis Turing complete; (3) its trans\u00adformations can be statically typed (at the expenses of Turing com\u00adpleteness) \nwithout any annotation yielding precise result types (4) even if we restrict the calculus only to well-typed \nterms (thus losing Turing completeness), it still is strictly more expressive than well\u00adknown and widely \nstudied deterministic top-down tree transducer formalisms. The technical contributions are (i) the proof \nof Turing com\u00adpleteness for our formalism, (ii) the de.nition of a type system that copes with records \nwith computable labels (iii) the de.nition of a static type system for .lters and its correctness, (iv) \nthe de.ni\u00adtion of a static analysis that ensures the termination (and the proof thereof) of the type \ninference algorithm with complexity bounds expressed in the size of types and .lters and (iv) the proof \nthat the terms that pass the static analysis form a language strictly more expressive than top-down tree \ntransducers. Outline. In Section 2 we present the syntax of the three com\u00adponents of our system. Namely, \na minimal set of expressions, the calculus of .lters used to program user-de.ned operators or to en\u00adcode \nthe operators of other languages, and the core types in which the types we just presented are to be encoded. \nSection 3 de.nes the operational semantics of .lters and a declarative semantics for operators. The type \nsystem as well as the type inference algorithm are described in Section 4. In Section 5 we present how \nto han\u00addle a large subset of Jaql. Section 6 reports on some subtler design choices of our system and \ncompare with related work. For space constraints, proofs, secondary results, encodings, some formal def\u00adinitions \n(in particular the de.nition of the static analysis for ter\u00admination and the interpretation of record \nvalues as quasi-constant functions), and further extensions can be found only in the full ver\u00adsion available \nonline. 2. Syntax In this section we present the syntax of the three components of our system: a minimal \nset of expressions, the calculus of .lters used to program user-de.ned operators or to encode the operators \nof other languages, and the core types in which the types presented in the introduction are to be encoded. \nThe core of our work is the de.nition of .lters and types. The key property of our development is that \n.lters can be grafted to any host language that satis.es minimal requirements, by simply adding .lter \napplication to the expressions of the host language. The minimal requirements of the host language for \nthis to be possi\u00adble are quite simple: it must have constants (typically for types int, char, string, \nand bool), variables, and either pairs or record val\u00adues (not necessarily both). On the static side the \nhost language must have at least basic and products types and be able to assign a type to expressions \nin a given type environment (ie, under some typing as\u00adsumptions for variables). By the addition of .lter \napplications, the host language can acquire or increase the capability to de.ne poly\u00admorphic user-de.ned \niterators, query and processing expressions, and be enriched with a powerful and precise type system. \n 2.1 Expressions In this work we consider the following set of expressions De.nition 1 (expressions). \nExprs e ::= c (constants) | x (variables) | (e, e) (pairs) | {e:e, ..., e:e} (records) | e + e (record \nconcatenation) | e \\ e (.eld deletion) | op(e, ..., e) (built-in operators) | fe (.lter application) \nwhere f ranges over .lters (de.ned later on), c over generic con\u00adstants, and e over string constants. \nIntuitively, these expressions represent the syntax supplied by the host language though only the .rst \ntwo and one of the next two are really needed that we extend with (the missing expres\u00adsions and) the \nexpression of .lter application. Expressions are formed by constants, variables, pairs, records, and \noperation on records: record concatenation gives priority to the expression on the right. So if in r1 \n+ r2 both records contains a .eld with the same label, it is the one in r2 that will be taken, while \n.eld deletion does not require the record to contain a .eld with the given label (though this point is \nnot important). The metavariable op ranges over operators as well as functions and other constructions \nbelong\u00ading to or de.ned by the host language. Among expressions we sin\u00adgle out a set of values, intuitively \nthe results of computations, that are formally de.ned as follows: v ::= c | (v, v) | {e:v; . . . ; e:v} \nWe use \"foo\" for character string constants, c for characters, 1 2 3 4 5 and so on for integers, and \nbackquoted words, such as foo, for atoms (ie, user-de.ned constants). We use three distin\u00adguished atoms \nnil, true, and false. Double quotes can be omitted for strings that are labels of record .elds: thus \nwe write {name:\"John\"} rather than {\"name\":\"John\"}. Sequences (aka, heterogeneous lists, ordered collections, \narrays) are encoded \u00e0 la LISP, as nested pairs where the atom nil denotes the empty list. We use [e1 \n. . . en] as syntactic sugar for (e1, . . . , (en, nil)...). 2.2 Types De.nition 2 (types). Types t \n::= b (basic types) | v (singleton types) | ( t, ,t) (products) | { e:t, . . . , e:t} (closed records) \n| { e:t, . . . , e:t , .. } (open records) | t| t (union types) | t&#38; t (intersection types) | \u00ac t \n(negation type) | empty (empty type) | any (any type) | \u00b5T.t (recursive types) | T (recursion variable) \n| Op(t, ..., t) (foreign type calls) where every recursion is guarded, that is, every type variable is \nseparated from its binder by at least one application of a type constructor (ie, products, records, or \nOp). Most of these types were already explained in the introduction. We have basic types (int, bool, \n....) ranged over by b and sin\u00adgleton types v denoting the type that contains only the value v. Record \ntypes come in two .avors: closed record types whose val\u00adues are records with exactly the .elds speci.ed \nby the type, and open record types whose values are records with at least the .elds speci.ed by the type. \nProduct types are standard and we have a complete set of type connectives, that is, .nite unions, intersections \nand negations. We use empty, to denote the type that has no values and any for the type of all values \n(sometimes denoted by _ when used in patterns). We added a term for recursive types, which al\u00adlows us \nto encode both the regular expression types de.ned in the introduction and, more generally, the recursive \ntype de.nitions we used there. Finally, we use Op (capitalized to distinguish it from expression operators) \nto denote the host language s type operators (if any). Thus, when .lter applications return values whose \ntype belongs just to the foreign language (eg, a list of functions) we sup\u00adpose the typing of these functions \nbe given by some type operators. For instance, if succ is a user de.ned successor function, we will suppose \nto be given its type in the form Arrow(int,int) and, simi\u00adlarly, for its application, say apply(succ,3) \nwe will be given the type of this expression (presumably int). Here Arrow is a type operator and apply \nan expression operator. The denotational semantics of types as sets of values, that we informally described \nin the introduction, is at the basis of the de.\u00adnition of the subtyping relation for these types. We \nsay that a type t1 is a subtype of a type t2, noted t1 = t2, if and only if the set of values denoted \nby t1 is contained (in the set-theoretic sense) in the set of values denoted by t2. For the formal de.nition \nand the decision procedure of this subtyping relation the reader can refer to the work on semantic subtyping \n[15].  2.3 Patterns Filters are our core untyped operators. All they can do are three different things: \n(1) they can structurally decompose and transform the values they are applied to, or (2) they can be \nsequentially composed, or (3) they can do pattern matching. In order to de.ne .lters, thus, we .rst need \nto de.ne patterns. De.nition 3 (patterns). Patterns p ::= t (type) | x (variable) | ( p, ,p) (pair) | \n{ e:p, . . . , e:p} (closed rec) | { e:p, . . . , e:p , .. } (open rec) | p| p (or/union) | p&#38; p \n(and/intersection) where the subpatterns forming pairs, records, and intersections have distinct capture \nvariables, and those forming unions have the same capture variables. Patterns are essentially types in \nwhich capture variables (ranged over by x, y, . . . ) may occur in every position that is not under a \nnegation or a recursion. A pattern is used to match a value. The matching of a value v against a pattern \np, noted v/p, either fails (noted O) or it returns a substitution from the variables occurring in the \npattern, into values. The substitution is then used as an environment in which some expression is evaluated. \nIf the pattern is a type, then the matching fails if and only if the pattern is matched against a value \nthat does not have that type, otherwise it returns the empty substitution. If it is a variable, then \nthe matching always succeeds and returns the substitution that assigns the matched value to the variable. \nThe pair pattern ( p1, ,p2) succeeds if and only if it is matched against a pair of values and each sub-pattern \nsucceeds on the corresponding projection of the value (the union of the two substitutions is then returned). \nBoth record patterns are similar to the product pattern with the speci.city that in the open record pattern \n.. matches all the .elds that are not speci.ed in the pattern. An intersection pattern p1&#38; p2 succeeds \nif and only if both patterns succeed (the union of the two substitutions is then returned). The union \npattern p1| p2 .rst tries to match the pattern p1 and if it fails it tries the pattern p2.  For instance, \nthe pattern ( int&#38; x, ,y) succeeds only if the matched value is a pair of values (v1, v2) in which \nv1 is an in\u00adteger in which case it returns the substitution {x/v1, y/v2} and fails otherwise. Finally \nnotice that the notation p as x we used in the examples of the introduction, is syntactic sugar for p&#38; \nx. This informal semantics of matching (see [15] for the formal de.nition) explains the reasons of the \nrestrictions on capture vari\u00adables in De.nition 3: in intersections, pairs, and records all patterns \nmust be matched and, thus, they have to assign distinct variables, while in union patterns just one pattern \nwill be matched, hence the same set of variables must be assigned, whichever alternative is se\u00adlected. \nThe strength of patterns is their connections with types and the fact that the pattern matching operator \ncan be typed exactly. This is entailed by the following theorems (both proved in [15]): Theorem 4 (Accepted \ntype [15]). For every pattern p, the set of all values v such that v/p O is a type. We call this set \nthe accepted = type of p and note it by )p5. The fact that the exact set of values for which a matching \nsucceeds is a type is not obvious. It states that for every pattern p there exists a syntactic type produced \nby the grammar in De.nition 2 whose semantics is exactly the set of all and only values that are matched \nby p. The existence of this syntactic type, which we note )p5, is of utmost importance for a precise \ntyping of pattern matching. In particular, given a pattern p and a type t contained in (ie, subtype of) \n)p5, it allows us to compute the exact type of the capture variables of p when it is matched against \na value in t: Theorem 5 (Type environment [15]). There exists an algorithm that for every pattern p, \nand t = )p5 returns a type environment t/p . Vars(p) . Types such that (t/p)(x) = {(v/p)(x) | v : t}. \n 2.4 Filters De.nition 6 (.lters). A .lter is a term generated by: Filters f ::= e (expression) |||||||| \np . f (pattern) ( f ,f) (product) { e:f, . . . , e:f , ..} (record) f| f (union) \u00b5 \u00b5X. .f (recursion) \nXa (recursive call) f;f (composition) o (declarative operators) Operators o ::= groupby f (.lter grouping) \n| orderby f (.lter ordering) Arguments a ::= x (variables) ||| c ( a,a) { e:a, ..., e:a} (constants) \n(pairs) (record) such that for every subterm of the form f;g, no recursion variable is free inf. Filters \nare like transducers, that when applied to a value re\u00adturn another value. However, unlike transducers \nthey possess more programming-oriented constructs, like the ability to test an in\u00adput and capture subterms, \nrecompose an intermediary result from captured values and a composition operator. We .rst describe in\u00adformally \nthe semantics of each construct. The expression .lter e always returns the value corresponding to the \nevaluation of e (and discards its argument). The .lter p . f applies the .lter f to its argument in the \nenvironment obtained by matching the argument against p (provided that the matching does not fail). This \nrather powerful feature allows a .lter to perform two critical actions: (i) inspect an input with regular \npattern-matching before exploring it and (ii) capture part of the input that can be reused during the \nevaluation of the sub.lter f. If the argument ap\u00ad plication of fi to vi returns vi then the application \nof the product .lter ( f1,f2) to an argument (v1, v2) returns (v1, v2); otherwise, if any application \nfails or if the argument is not a pair, it fails. The record .lter is similar: it applies to each speci.ed \n.eld the corre\u00adsponding .lter and, as stressed by the . . , leaves the other .elds unchanged; it fails \nif any of the applications does, or if any of the speci.ed .elds is absent, or if the argument is not \na record. The .l\u00adter f1| f2 returns the application of f1 to its argument or, if this fails, the application \nof f2. The semantics of a recursive .lter is given by standard unfolding of its de.nition in recursive \ncalls. The only real restriction that we introduce for .lters is that recursive calls can be done only \non arguments of a given form (ie, on arguments that have the form of values where variables may occur). \nThis restriction in practice amounts to forbid recursive calls on the result of another recursively de.ned \n.lter (all other cases can be easily encoded). The reason of this restriction is technical, since it \ngreatly simpli\u00ad.es the analysis of Section 4.4 (which ensures the termination of type inference) without \nhampering expressiveness: .lters are Tur\u00ading complete even with this restriction (see Theorem 7). Filters \ncan be composed: the .lter f1;f2 applies f2 to the result of applying f1 to the argument and fails if \nany of the two does. The condition that in every subterm of the form f ;g, f does not contain free re\u00adcursion \nvariables is not strictly necessary. Indeed, we could allow such terms. The point is that the analysis \nfor the termination of the typing would then reject all such terms (apart from trivial ones in which \nthe result of the recursive call is not used in the composition). But since this restriction does not \nrestrict the expressiveness of the calculus (Theorem 7 proves Turing completeness with this restric\u00adtion), \nthen the addition of this restriction is just a design (rather than a technical) choice: we prefer to \nforbid the programmer to write recursive calls on the left-hand side of a composition, than systematically \nreject all the programs that use them in a non-trivial way. Finally, we singled out some speci.c .lters \n(speci.cally, we chose groupby and orderby ) whose semantics is generally speci.ed in a declarative rather \nthan operational way. These do not bring any expressive power to the calculus (the proof of Turing completeness, \nTheorem 7, does not use these declarative operators) and actually they can be encoded by the remaining \n.lters, but it is interesting to single them out because they yield either simpler encodings or more \nprecise typing. 3. Semantics The operational semantics of our calculus is given by the reduction semantics \nfor .lter application and for the record operations. Since the former is the only novelty of our work, \nwe save space and omit the latter, which are standard anyhow. We de.ne a big step operational semantics \nfor .lters. The de.nition is given by the inference rules in Figure 1 for judgments of the form d;. feval \nf (a) . r and describes how the evaluation of the application of .lter f to an argument a in an environment \n. yields an object r where r is either a value or O. The latter is a special value which represents a \nruntime error: it is raised by the rule (error) either because a .lter did not match the form of its \nargument (eg, the argument of a .lter product was not a pair) or because some pattern matching failed \n(ie, the side condition of (patt) did not hold). Notice that the argument a of a .lter is always a value \nv unless the .lter is the unfolding of a recursive  (expr) d;. feval e(v) r (prod) d;. feval f1(v1) \nr1 d;. feval f2(v2) d;. feval ( f1,f2) )(v1, v2) (r1, r2) r2 (patt) d;. , v/p feval f(v) d;. feval (p \n. f)(v) r r (comp) d;. feval f1(v) r1 d;. feval f2(r1) d;. feval (f1;f2)(v) r2 r2  r = eval(., e) if \nr1= O and r2 = O if v/p = O if r1 = O (union1) (union2) (rec) (rec-call) (error) d;. feval f1(v) r1 \n= O if r1 d;. feval (f1| f2)(v) r1 d;. feval f1(v) O d;. feval f2(v) r2 d;. feval (f1| f2)(v) r2 d, (X \n. f);. feval f(v) r d;. feval (\u00b5 \u00b5X. .f)(v) r d;. feval (d(X))(a) r d;. feval (Xa)(v) r if no other \nrule applies d;. feval f(a) O d;. feval f1(v1) r1 \u00b7 \u00b7 \u00b7 d;. feval fn(vn) rn (recd) if .i, ri= O d;. \nfeval { 1:f1, ..., n:fn , ..} ({ 1:v1, ..., n:vn, ..., n+k:vn+k} ) { 1:r1, ..., n:rn, ..., n+k:vn+k} \n Figure 1. Dynamic semantics of .lters call, in which case variables may occur in it (cf. rule rec-call). \nEnvironment d is used to store the body of recursive de.nitions. The semantics of .lters is quite straightforward \nand inspired by the semantics of patterns. The expression .lter discards its input and evaluates (rather, \nasks the host language to evaluate) the expression e in the current environment (expr). It can be thought \nof as the right-hand side of a branch in a match_with construct. The product .lter expects a pair as \ninput, applies its sub-.lters component-wise and returns the pair of the results (prod). This .lter is \nused in particular to express sequence mapping, as the .rst component f1 transforms the element of the \nlist and f2 is applied to the tail. In practice it is often the case that f2 is a recursive call that \niterates on arbitrary lists and stops when the input is nil. If the input is not a pair, then the .lter \nfails (rule (error) applies). The record .lter expects as input a record value with at least the same \n.elds as those speci.ed by the .lter. It applies each sub-.lter to the value in the corresponding .eld \nleaving the contents of other .elds unchanged (recd). If the argument is not a record value or it does \nnot contain all the .elds speci.ed by the record .lter, or if the application of any sub.lter fails, \nthen the whole application of the record .lter fails. The pattern .lter matches its input value v against \nthe pattern p. If the matching fails so the .lter does, otherwise it evaluates its sub\u00ad.lter in the environment \naugmented by the substitution v/p (patt). The alternative .lter follows a standard .rst-match policy: \nIf the .lter f1 succeeds, then its result is returned (union-1). If f1 fails, then f2 is evaluated against \nthe input value (union-2). This .lter is particularly useful to write the alternative of two (or more) \npattern .lters, making it possible to conditionally continue a computation based on the shape of the \ninput. The composition allows us to pass the result of f1 as input to f2. The composition .lter is of \nparamount importance. Indeed, without it, our only way to iterate (deconstruct) an input value is to \nuse a product .lter, which always rebuilds a pair as result. Finally, a recursive .lter is evaluated \nby recording its body in d and evaluating it (rec), while for a recursive call we replace the recursion \nvariable by its de.nition (rec-call). This concludes the presentation of the semantics of non\u00addeclarative \n.lters (ie, without groupby and orderby). These form a Turing complete formalism (complete proof in the \nfull version): Theorem 7 (Turing completeness). The language formed by constants, variables, pairs, equality, \nand applications of non\u00addeclarative .lters is Turing complete. Proof (sketch). We can encode untyped \ncall-by-value .-calculus by .rst applying continuation passing style (CPS) transformations and encoding \nCPS term reduction rules and substitutions via .lters. Thanks to CPS we eschew the restrictions on composition. \nTo conclude the presentation of the semantics we have to de.ne the semantics of groupby and orderby. \nWe prefer to give the semantics in a declarative form rather than operationally in order not to tie it \nto a particular order (of keys or of the execution): Groupby: groupby f applied to a sequence [v1 . . \n. vm] reduces to a sequence [ (k1, l1) . . . (kn, ln) ] such that: 1. .i, 1 = i = m, .j, 1 = j = n, s.t. \nkj = f(vi) 2. .j, 1 = j = n, .i, 1 = i = m, s.t. kj = f(vi) 3. .j, 1 = j = n, lj is a sequence: [ v1 \nj . . . vj ]  nj 4. .j, 1 = j = n, .k, 1 = k = nj , f(vk j ) = kj 5. ki = kj . i = j 6. l1, . . . \n, ln is a partition of [v1 . . . vm ]  Orderby: orderby f applied to [v1 . . . vn] reduces to [v1 . \n. . vn] such that: 1. [v1 . . . vn] is a permutation of [v1 . . . vn], 2. .i, j s.t. 1 = i = j = n, \nf(vi) = f (vj )  Since the semantics of both operators is deeply connected to a notion of equality and \norder on values of the host language, we give them as built-in operations. However we will illustrate \nhow our type algebra allows us to provide very precise typing rules, specialized for their particular \nsemantics. It is also possible (see full version) to encode co-grouping (or groupby on several input \nsequences) with a combination of groupby and .lters. Syntactic sugar. The reader may have noticed that \nthe produc\u00adtions for expressions (De.nition 1) do not de.ne any destructor (eg, projections, label selection, \n. . . ), just constructors. The reason is that destructors, as well as other common expressions, can \nbe encoded by .lter applications: def e.e = ({ e:x , .. } . x)e def fst(e) = (( x, any) . x)e snd(e) \ndef = (( any, ,x) . x)e let p = e1 in e2 def = (p . e2)e1 if e then e1 else e2 def = ( true . e1| false \n. e2)e match e with p1 . e1|...|pn . en def = (p1 . e1| . . . | pn . en)e These are just a possible \nchoice, but others are possible. For in\u00adstance in Jaql dot selection is overloaded: when _.e is applied \nto a record, Jaql returns the content of its e .eld; if the .eld is ab\u00adsent or the argument is null, \nthen Jaql returns null and fails if the argument is not a record; when applied to a list ( array in Jaql \nterminology) it recursively applies to all the elements of the list. So Jaql s _.e is precisely de.ned \nas \u00b5 \u00b5X. ({ e:x , .. } . x | ({ ..} | null) . null | ( h, ,t) . ( Xh,Xt ) ))  Besides the syntactic \nsugar above, in the next section we will use t1 + t2 to denote the record type formed by all .eld types \nin t2 and all the .eld types in t1 whose label is not already present in t2. Similarly t \\ e will denote \nthe record types formed by all .eld types in t apart from the one labeled by e, if present. Finally, \nwe will also use for expressions, types, and patterns the syntactic sugar for lists used in the introduction. \nSo, for instance, [p1 p2 ... pn] is matched by lists of n elements provided that their i-th element matches \npi. 4. Type inference The type inference system assign types to expressions. Variables, constants, and \npairs are typed by standard rules, while we suppose that the typing of foreign expressions is provided \nby the host lan\u00adguage.1 So we omit the corresponding rules (they can be found in the full version). The \ncore of our type system starts with records. 4.1 Typing of records The typing of records is novel and \nchallenging because record ex\u00adpressions may contain string expressions in label position, such as in \n{ e1:e2} , while in all type systems for record we are aware of, labels are never computed. It is dif.cult \nto give a type to { e1:e2} since, in general, we do not statically know the value that e1 will return, \nand which is required to form a record type. All we can (and must) ask is that this value will be a string. \nTo type a record expression { e1:e2} , thus, we distinguish two cases according to whether the type t1 \nof e1 is .nite (ie, it contains only .nitely many values, such as, say, Bool) or not. If a type is .nite, \n(.niteness of regular types seen as tree automata can be decided in polynomial time [11]), then it is \npossible to write it as a .nite union of values (actually, of singleton types). So consider again { e1:e2} \nand let t1 be the type of e1 and t2 the type of e2. First, t1 must be a sub\u00adtype of string (since record \nlabels are strings). So if t1 is .nite it can be expressed as e1| \u00b7 \u00b7 \u00b7 | en which means that e1 will \nreturn the string ei for some i . [1..n]. Therefore { e1:e2} will have type { ei : t2} for some i . [1..n] \nand, thus, the union of all these types, as expressed by the rule [RCD-FIN] below. If t1 is in.nite instead, \nthen all we can say is that it will be a record with some (unknown) labels, as expressed by rule [RCD-INF]. \n[RCD-FIN] G f e : e1| \u00b7 \u00b7 \u00b7| en G f e : t G f {e:e } : { e1:t} | \u00b7 \u00b7 \u00b7| { en:t} [RCD-INF] G f e : t \nG f e : t t = string t is in.nite G f {e:e } : { ..} [RCD-MUL] G f {e1:e1} : t1 \u00b7 \u00b7 \u00b7 G f {en:en} : \ntn G f {e1:e1, . . . , en:en} : t1 + \u00b7 \u00b7 \u00b7 + tn [RCD-CONC] [RCD-DEL] G f e1 : t1 G f e2 : t2 t1 = { ..} \nG f e : t t = { ..} G f e1 + e2 : t1 + t2 t2 = { ..} G f e \\ e : t \\ e Records with multiple .elds are \nhandled by the rule [RCD-MUL] which merges the result of typing single .elds by using the type operator \n+ as de.ned in CDuce [5, 14], which is a right-priority record concatenation de.ned to take into account \nunde.ned and unknown .elds: for instance, { a:int, b:int} + { a?:bool} = { a:int| bool, b:int} ; unknown \n.elds in the right-hand side may 1 Notice that our expressions, whereas they include .lter applications, \ndo not include applications of expressions to expressions. Therefore if the host language provides function \nde.nitions, then the applications of the host language must be dealt as foreign expressions, as well \n(cf. apply in \u00a72.2). override known .elds of the left-hand side, which is why, for in\u00adstance, we have \n{ a:int, b:bool} + { b:int , .. } = { b:int , .. } ; likewise, for every record type t (ie, for every \nt subtype of { ..} ) we have t + { ..} = { ..} . Finally, [RCD-CONC] and [RCD-DEL] deal with record concatenation \nand .eld deletion, respectively, in a straightforward way: the only constraint is that all expressions \nmust have a record type (ie, the constraints of the form ... = { ..} ). See the full version for formal \nde.nitions of all these type operators. Notice that these rules do not ensure that a record will not \nhave two .elds with the same label, which is a run-time error. Detect\u00ading such an error needs sophisticated \ntype systems (eg, dependent types) beyond the scope of this work. This is why in the rule [RCD-MUL] we \nused type operator + which, in case of multiple occur\u00adring labels, since records are unordered, corresponds \nto randomly choosing one of the types bound to these labels: if such a .eld is selected, it would yield \na run-time error, so its typing can be am\u00adbiguous. We can .ne tune the rule [RCD-MUL] so that when all \nthe ti are .nite unions of record types, then we require to have pairwise disjoint sets of labels; but \nsince the problem would still persist for in.nite types we prefer to retain the current, simpler formulation. \n 4.2 Typing of .lter application Filters are not .rst-class: they can be applied but not passed around \nor computed. Therefore we do not assign types to .lters but, as for any other expression, we assign types \nto .lter applications. The typing rule for .lter application [FILTER-APP] G f e : t G ; \u00d8; \u00d8 f.l f(t) \n: s G f fe : s relies on an auxiliary deduction system for judgments of the form G ; . ; M f.l f(t) : \ns that states that if in the environments G, ., M (explained later on) we apply the .lter f to a value \nof type t, then it will return a result of type s. To de.ne this auxiliary deduction system, which is \nthe core of our type analysis, we .rst need to de.ne )f5, the type accepted by a .lter f. Intuitively, \nthis type gives a necessary condition on the input for the .lter not to fail: De.nition 8 (Accepted type). \nGiven a .lter f, the accepted type of f, written )f5 is the set of values de.ned by: = any e5 Xa5 = any \n= p . f5 \u00b5X. .f5 = f1| f25 = ) p f1 55 &#38; | ))ff 2 55 ) \u00b5 )f5 groupby f5 = [any*] ( f1,f2) 5 = ( ) \nf1 5 , ) f2 5 ) ) orderby f5 = [any*] f1;f25 = ) { e1:f1,.., en:)ffn 1 , 5 ..} 5 = { e1: ) f1 5 ,.., \nen: ) f2 5 , .. } It is easy to show that an argument included in the accepted type is a necessary (but \nnot suf.cient, because of the cases for composition and recursion) condition for the evaluation of a \n.lter not to fail: Lemma 9. Let f be a .lter and v be a value such that v / . )f5. For every ., d, if \nd;. feval f(v) r, then r = O. The last two auxiliary de.nitions we need are related to product and record \ntypes. In the presence of unions, the most general form for a product type is a .nite union of products \n(since intersections distribute on products). For instance consider the type ( int, int) | ( string, \nstring) This type denotes the set of pairs for which either both projections are int or both projections \nare string. A type such as ( int| string, int| string) is less precise, since it also allows pairs whose \n.rst projection is an int and second projection is a string and vice versa. We see that it is necessary \nto manipulate .nite unions of products (and similarly for records), and therefore, we introduce the following \nnotations:  Lemma 10 (Product decomposition). Let t . Types such that t = ( any, any) . A product decomposition \nof t, denoted by p (t) is a set of types: 1 1 n n p (t) = {( t1, ,t2) , . . . , ( t1 , ,t2 ) } such \nthat t =ti. For a given product decomposition, we ti.p (t) say that n is the rank of t, noted rank(t), \nand use the notation p j j i (t) for the type ti . There exist several suitable decompositions whose \ndetails are out of the scope of this article. We refer the interested reader to [14] and [21] for practical \nalgorithms that compute such decompo\u00adsitions for any subtype of ( any, any) or of { .. } . These notions \nof decomposition, rank and projection can be generalized to records: Lemma 11 (Record decomposition). \nLet t . Types such that t = { ..} . A record decomposition of t, denoted by . (t) is a .nite set of types \n. (t)={r1, . . . , rn} where each ri is either of the form iiii iiii { e1:t1, . . . , eni :tni} or of \nthe form { e1:t1, . . . , eni :tni , .. } and such that t =ri. For a given record decomposition, we ri.. \n(t) say that n is the rank of t, noted rank(t), and use the notation . j H(t) for the type of label e \nin the jth component of . (t). In our calculus we have three different sets of variables. The set Vars \nof term variables, ranged over by x, y, ..., introduced in patterns and used in expressions and in arguments \nof calls of recursive .lters. The set RVars of term recursion variables, ranged over by X, Y, ... and \nthat are used to de.ne recursive .lters. The set TVars of type recursion variables, ranged over by T, \nU, ... used to de.ne recursive types. In order to use them we need to de.ne three different environments: \nG : Vars . Types denoting type environments that associate term variables with their types; . : RVars \n. Filters denoting de.nition environments that associate each .lter recursion variable with the body \nof its de.nition; M : RVars \u00d7 Types . TVars denoting memoization environments which record that the call \nof a given recursive .lter on a given type yielded the introduction of a fresh recursion type variable. \nOur typing rules, thus work on judgments of the form G ; . ; M f f(t) : t stating that applying f to \nan expression of type t in the environments G, ., M yields a result of type t . This judgment can be \nderived with the set of rules given in Figure 2. These rules are straightforward, when put side by side \nwith the dynamic semantics of .lters, given in Section 3. It is clear that this type system simulates \nat the level of types the computations that are carried out by .lters on values at runtime. For instance, \nrule [FIL-EXPR] calls the typing function of the host language to determine the type of an expression \ne. Rule [FIL-PROD] applies a product .lter recursively on the .rst and second projection for each member \nof the product decomposition of the input type and returns the union of all result types. Rule [FIL-REC] \nfor records is similar, recursively applying sub-.lters label-wise for each member of the record de\u00adcomposition \nand returning the union of the resulting record types. As for the pattern .lter (rule [FIL-PAT]), its \nsub.lter f is typed in the environment augmented by the mapping t/p of the input type against the pattern \n(cf. Theorem 5). The typing rule for the union .lter, [FIL-UNION] re.ects the .rst match policy: when \ntyping the second branch, we know that the .rst was not taken, hence that at runtime the .ltered value \nwill have a type that is in t but not in )f15. Notice that this is not ensured by the de.nition of accepted \ntype which is a rough approximation that discards grosser errors but, as we stressed right after its \nde.nition, is not suf.cient to ensure that evaluation of f1 will not fail but by the type system itself: \nthe premises check that f1(t1) is well-typed which, by induction, implies that f1 will never fail on \nvalues of type t1 and, ergo, that these values will never reach f2. Also, we discard from the output \ntype the contribution of the branches that cannot be taken, that is, branches whose accepted type have \nan empty intersection with the input type t. Composition (rule [FIL-COMP]) is straightforward. In this \nrule, the restriction that f1 is a .lter with no open recursion variable ensures that its output type \ns is also a type without free recursion variables and, therefore, that we can use it as input type for \nf2. The next three rules work together. The .rst, [FIL-FIX] intro\u00adduces for a recursive .lter a fresh \nrecursion variable for its output type. It also memoize in . that the recursive .lter X is associated \nwith a body f and in M that for an input .lter X and an input type t, the output type is the newly introduced \nrecursive type variable. When dealing with a recursive call X two situations may arise. One possibility \nis that it is the .rst time the .lter X is applied to the input type t. We therefore introduce a fresh \ntype variable T and recurse, replacing X by its de.nition f. Otherwise, if the input type has already \nbeen encountered while typing the .lter variable X, we can return its memoized type, a type variable \nT . Finally, Rule [FIL-ORDBY] and Rule [FIL-GRPBY] handle the special cases of groupby and orderby .lters. \nTheir typing is explained in the following section.  4.3 Typing of orderby and groupby While the structural \n.lters enjoy simple, compositional typing rules, the ad-hoc operations orderby and groupby need specially \ncrafted rules. Indeed it is well known that when transformation languages have the ability to compare \ndata values type-checking (and also type inference) becomes undecidable (eg, see [2, 3]). We therefore \nprovide two typing approximations that yield a good compromise between precision and decidability. First \nwe de.ne an auxiliary function over sequence types: De.nition 12 (Item set). Let t . Types such that \nt = [any*]. The item set of t denoted by item(t) is de.ned by: item(empty) = \u00d8 item(t) = item(t&#38; \n( any,any) )) if t = ( any,any) t 12 1 2 item( ( ti , ,ti ) )) = ({ti } . item(ti )) 1=i=rank(t) 1=i=rank(t) \nThe .rst and second line in the de.nition ensure that item() returns the empty set for sequence types \nthat are not products, namely for the empty sequence. The third line handles the case of non-empty sequence \ntype. In this case t is a .nite union of products, whose .rst components are the types of the head of \nthe sequence and second components are recursively the types of the tails. Note also that this de.nition \nis well-founded. Since types are regular trees the number of distinct types accumulated by item() is \n.nite. We can now de.ned typing rules for the orderby and groupby operators. orderby f: The orderby .lter \nuses its argument .lter f to compute a key from each element of the input sequence and then returns the \nsame sequence of elements, sorted with respect to their key. Therefore, while the types of the elements \nin the result are still known, their order is lost. We use item() to compute the output type of an orderby \napplication: t OrderBy(t) = [( ti) * ] ti.item(t) groupby f: The typing of orderby can be used to give \na rough approximation of the typing of groupby as stated by rule [FIL-GRPBY]. In words, we obtain a list \nof pairs where the key com\u00adponent is the result type of f applied to the items of the sequence, and use \nOrderBy to shuf.e the order of the list. A far more pre\u00adcise typing of groupby that keeps track of the \nrelation between list elements and their images via f is given in the full version.  4.4 Soundness, \ntermination, and complexity The soundness of the type inference system is given by the property of subject \nreduction for .lter application  [FIL-PROD][FIL-EXPR] [FIL-PAT] i=1..rank(t), j=1, 2 G; .; M f.l fj \n(p i (t)) : si j j G . t/p ; . ; M f.l f(t) : s i i t = p &#38; f G ; . ; M f.l ( f1,f2) )(t) :( s1, \n,s2) G ; . ; M f.l e(t) : type(G, e,) G ; . ; M f.l p . f(t) : s i=1..rank(t) [FIL-REC] [FIL-UNION] \ni i=1..rank(t), j=1..m G ; .; M f.l fj (. i Hj (t)) : sj i = 1, 2 G; .; M f.l fi(ti) : si t = f1 | f2 \nt1 = t&#38; f1 G ; . ; M f.l { 1:f1, . . . , m:fm , ..} (t) :{ 1:s1 i , . . . , m:s i , .. } G ; . ; \nM f.l f1| f2(t) :si t2 = t&#38; \u00ac f1 m i=1..rank(t){i|si =empty} [FIL-COMP] G ; . ; M f.l f1(t) : s \nG; . ; M f.l f2(s) : s G ; . ; M f.l f1;f2(t) : s [FIL-FIX] G ; ., (X . f) ; M, ((X, t) . T ) f.l f(t) \n: s G; . ; M f.l (\u00b5 \u00b5X. .f)(t) : \u00b5 \u00b5T. .s T fresh [FIL-CALL-NEW] G ; . ; M, ((X, t) . T ) f.l .(X)(t) \n: t G; .; M f.l (Xa)(s) : \u00b5 \u00b5T. .t t = type(G, a) (X, t) . dom(M) T fresh [FIL-CALL-MEM] G ; . ; M f.l \n(Xa)(s) : M(X, t) t = type(G, a) (X, t) . dom(M) [FIL-ORDBY] .ti . item(t) G; .; M f.l f(ti) : si G ; \n. ; M f.l (orderby f)(t) : OrderBy(t) t = [any*]t isi is ordered [FIL-GRPBY] .ti . item(t) G ; .; M f.l \nf(ti) : si G ; . ; M f.l (groupby f)(t) : [( (( : i si),OrderBy(t)) *] t = [any*] Figure 2. Type inference \nalgorithm for .lter application Theorem 13 (subject reduction). If \u00d8; \u00d8; \u00d8 f.l f (t) : s, then for all \nv : t, \u00d8;\u00d8 feval f(v) r implies r : s. whose proof is given in the full version. It is easy to write \na .l\u00adter for which the type inference algorithm, that is the deduction of f.l, does not terminate: \u00b5 \n\u00b5X. .x . X( x,x) . The deduction of G ; . ; M f.l f (t) : s simulates an (abstract) execution of the \n.lter f on the type t. Since .lters are Turing complete, then in general it is not possible to decide \nwhether the deduction of f.l for a given .lter f will terminate for every input type t. For this reason \nwe de.ne a static analysis Check(f) for .lters that ensures that if f passes the analysis, then for every \ninput type t the deduction of G ; . ; M f.l f(t) : s terminates. For space reasons the formal de.nition \nof Check(f) is available in only the full version, but its behavior can be easily explained. Imagine \nthat a recursive .lter f is applied to some input type t. The algorithm tracks all the recur\u00adsive calls \noccurring in f; next it performs one step of reduction of each recursive call by unfolding the body; \n.nally it checks in this unfolding that if a variable occurs in the argument of a recursive call, then \nit is bound to a type that is a subtree of the original type t. In other words, the analysis veri.es \nthat in the execution of the derivation for f(t) every call to s/p for some type s and pattern p always \nyields a type environment where variables used in re\u00adcursive calls are bound to subtrees of t. This implies \nthat the rule [FIL-CALL-NEW] will always memoize for a given X, types that are obtained from the arguments \nof the recursive calls of X by replac\u00ading their variables with a subtree of the original type t memoized \nby the rule [FIL-FIX]. Since t is regular, then it has .nitely many distinct subtrees, thus [FIL-CALL-NEW] \ncan memoize only .nitely many distinct types, and therefore the algorithm terminates. More precisely, \nthe analysis proceeds in two passes. In the .rst pass the algorithm tracks all recursive .lters and for \neach of them it (i) marks the variables that occur in the arguments of its recursive calls, (ii) assigns \nto each variable an abstract identi.er represent\u00ading the subtree of the input type to which the variable \nwill be bound at the initial call of the .lter, and (iii) it returns the set of all types obtained by \nreplacing variables by the associated abstract identi.er in each argument of a recursive call. The last \nset intuitively repre\u00adsents all the possible ways in which recursive calls can shuf.e and recompose the \nsubtrees forming the initial input type. The second phase of the analysis .rst abstractly reduces by \none step each re\u00adcursive .lter by applying it on the set of types collected in the .rst phase of the \nanalysis and then checks whether, after this reduction, all the variables marked in the .rst phase (ie, \nthose that occur in ar\u00adguments of recursive calls) are still bound to subtrees of the initial input type: \nif this checks fails, then the .lter is rejected. It is not dif.cult to see that the type inference algorithm \ncon\u00adverges if and only if for every input type there exists a integer n such that after n recursive calls \nthe marked variables are bound only to subtrees of the initial input type (or to something that does \nnot depend on it, of course). Since deciding whether such an n exists is not possible, our analysis checks \nwhether for all possible input types a .lter satis.es it for n=1, that is to say, that at every recursive \ncall its marked variables satisfy the property; otherwise it rejects the .lter. Theorem 14 (Termination). \nIf Check(f), then for every type t the deduction of G ; \u00d8; \u00d8 f.l f (t) : s is in 2-EXPTIME. Furthermore, \nif t is given as a non-deterministic tree automaton (NTA) then G ; \u00d8; \u00d8 f.l f(t) : s is in EXPTIME, where \nthe size of the problem is |f| \u00d7 |t|. This complexity result is in line with those of similar formalisms. \nFor instance in [18], it is shown that type-checking non determin\u00adistic top-down tree transducers is \nin EXPTIME when the input and output types are given by a NTA. All .lters de.ned in this article pass \nthe analysis. As an example consider the .lter rotate that applied to a list returns the same list with \nthe .rst element moved to the last position (and the empty list if applied to the empty list): \u00b5 \u00b5X. \n( ( x,( y,z) ) . ( y,X( x,z) ) | w . w ) The analysis succeeds on this .lter. If we denote by .x the \nabstract subtree bound to the variable x, then the recursive call will be ex\u00adecuted on the abstract argument \n( .x,.z) . So in the unfolding of the recursive call x is bound to .x, whereas y and z are bound to two \ndistinct subtrees of .z. The variables in the recursive call, x and z, are thus bound to subtrees of \nthe original tree (even though the ar\u00adgument of the recursive call is not a subtree of the original tree), \ntherefore the .lter is accepted . In order to appreciate the precision of the inference algorithm consider \nthe type [int+ bool+], that is, the type of lists formed by some integers (at least one) followed by \nsome booleans (at least one). For the application of rotate to an argument of this type our algorithm \nstatically infers the most pre\u00adcise type, that is, [int* bool+ int]. If we apply it once more the inferred \ntype is [int* bool+ int int]|[bool* int bool]. Generic .lters are Turing complete. However, requiring \nthat Check() holds meaning that the .lter is typeable by our system restricts the expressive power of \nour .lters by preventing them from recomposing a new value before doing a recursive call. For instance, \nit is not possible to typecheck a .lter which reverses the elements of a sequence. Determining the exact \nclass of transforma\u00adtions that typeable .lters can express is challenging. However it is possible to \nshow (see the full version for the proof) that typeable .lters are strictly more expressive than top-down \ntree transducers with regular look-ahead, a formalism for tree transformations in\u00adtroduced in [13]. For \nan intuition of this result consider the tree:  a(u1(. . . (un()))v1(. . . (vm()))) that is, a tree \nwhose root is labeled a with two children, each being a monadic tree of height n and m, respectively. \nIt is not possible to write a top-down tree transducer with regular look-ahead that creates the tree \na(u1(. . . (un(v1(. . . vm()))))) which is just the concatenation of the two children of the root, seen \nas sequences, a transformation that can be easily programmed by typeable .lters. The key difference in \nexpressive power comes from the fact that .lters are evaluated with an environment that binds capture \nvariables to sub-trees of the input. This feature is essential to encode sequence concatenation and sequence \n.attening two pervasive operations when dealing with sequences that cannot be expressed by top-down tree \ntransducers with regular look-ahead. 5. Jaql In this Section, we show how .lters can be used to capture \nsome popular languages for processing data on the Cloud. We consider Jaql [16], a query language for \nJSON developed by IBM. We give translation rules from a subset of Jaql into .lters. De.nition 15 (Jaql \nexpressions). We use the following simpli.ed grammar for Jaql (where we distinguish simple expressions, \nranged over by e, from core expressions ranged over by k). e ::= c (constants) | x (variables) | $ (current \nvalue) | [e,..., e] (arrays) | { e:e,..., e:e } (records) | e.l (.eld access) | op(e,...,e) (function \ncall) | e -> k (pipe) k ::= filter (each x )? e (.lter) | transform (each x)? e (transform) | expand \n((each x)? e)? (expand) | group ((each x)? by x = e (as x)?)? into e (grouping) In order to ease the \npresentation we extend our syntax by adding .lter de.nitions (already informally used in the introduction) \nto .lters and .lter calls to expressions: e ::= let filter F [F1, . . . , Fn] = f in e (.lter defn.) \nf ::= F [f, . . . , f] (call) where F ranges over .lter names. The mapping for most of the language we \nconsider rely on the following built-in .lters. let filter Filter [F ] =\u00b5 \u00b5X. nil . nil | ( ( x, xs) \n, ,tl) . ( X(x, xs),X(tl)) | ( x, ,tl) . F x ;( true . ( x, X(tl)) | false . X(tl)) let filter Transform \n[F ] =\u00b5 \u00b5X. nil . nil | ( ( x, xs) , ,tl) . ( X(x, xs),X(tl)) | ( x, ,tl) . ( F x, X(tl)) let filter \nExpand =\u00b5 \u00b5X. nil . nil | ( nil, ,tl) . X(tl) | ( ( x, xs) , ,tl) . ( x,X(xs, tl)) Jaql expressions \nare mapped to our expressions as follows (where $ is a distinguished expression variable interpreting \nJaql s $ ): = c [c] = x [x] = $ [$ ] [{e1:e1,...,en:en}] = { [e1] :[e1] , ...,[en] :[en] } [e.l] = [e] \n.l [op(e1, ..., en)] = op([e1] , ...,[en]) [[e1,...,en = ([e1] , ...([en] , nil)...) ]] [e -> k] = [e] \n;[k]F Jaql core expressions are mapped to .lters as follows: [filter e]F = [filter each $ e]F [filter \neach x e]F = Filter [x . [e] ] [transform e]F = [transform each $ e]F [transform each x e]F = Transform \n[x . [e] ] [expand each x e]F = [expand]F ;[transform each x e]F [expand]F = Expand [group into e]F = \n[group by y=true into e]F [group by y=e1 into e2]F = [group each $ by y=e1 into e2]F [group each x by \ny=e1 into e2]F = [group each x by y = e1 as $ into e2]F [group each x by y = e1 as g into e2]F = groupby \nx . [e1] ; Transform [( y,g) . [e2] ] This translation de.nes the (.rst, in our knowledge) formal seman\u00adtics \nof Jaql. Such a translation is all that is needed to de.ne the semantics of a NoSQL language and, as \na bonus, endow it with the type inference system we described without requiring any modi\u00ad.cation of the \noriginal language. No further action is demanded since the machinery to exploit it is all developed in \nthis work. As for typing, every Jaql expression is encoded into a .lter for which type-checking is ensured \nto terminate: Check() holds for Filter[], Transform[], and Expand (provided it holds also for their arguments) \nsince they only perform recursive calls on recom\u00adbinations of subtrees of their input; by its de.nition, \nthe encoding does not introduce any new recursion and, hence, it always yields a composition and application \nof .lters for which Check() holds. 5.1 Examples To show how we use the encoding, let us encode the example \nof the introduction. For the sake of the concision we will use .lter de.nitions (rather than expanding \nthem in details). We use Fil and Sel de.ned in the introduction, Expand and Transform[] de.ned at the \nbeginning of the section, the encoding of Jaql s .eld selection as de.ned in Section 3, and .nally Head \nthat returns the .rst element of a sequence and a family of recursive .lters Rgrpi with i . N+ both de.ned \nbelow: let filter Head = nil => null | (x,xs) => x let filter Rgrpi = nil => nil | ((i,x),tail) => (x \n, Rgrpi tail) | _ => Rgrpi tail Then, the query in the introduction is encoded as follows 1 [employees \ndepts]; 2 [Sel Fil]; 3 [Transform[x =>(1,x)] Transform[x =>(2,x)]]; 4 Expand; 5 groupby ( (1,$)=>$.dept \n| (2,$)=>$.depid ); 6 Transform[(g,l)=>( 7 [(l; Rgrp1) (l; Rgrp2)]; 8 [es ds] => 9 { dept: g, 10 deptName: \n(ds ; Head).name), 11 numEmps: count(es) } )] In words, we perform the selection on employees and .lter \nthe departments (lines 1-2); we tag each element by 1 if it comes from employees, and by 2 if it comes \nfrom departments (line 3); we merge the two collections (line 4); we group the heterogeneous list according \nto the corresponding key (line 5); then for each element of the result of grouping we capture in g the \nkey (line 6), split the group into employees and depts (line 7), capture each subgroup into the corresponding \nvariable (ie, es and ds) (line 8) and return the expression speci.ed in the query after the into (lines \n8-10). The general de.nition of the encoding for the co-grouping can be found in the full version. Let \nus now illustrate how the above composition of .lters is typed. Consider an instance where: employees \nhas type [ Remp* ], where Remp = { dept: int, income:int, ..}  depts has type [ (Rdep | Rbranch)* ], \nwhere Rdep = {depid:int, name: string, size: int} Rbranch = {brid:int, name: string} (this type is a \nsubtype of Dept as de.ned in the introduction)  The global input type is therefore (line 1) [ [ Remp* \n] [ (Rdep | Rbranch)* ] ] which becomes, after selection and .ltering (line 2) [ [ Remp* ] [ Rdep* ] \n] (note how all occurrences of Rbranch are ignored by Fil). Tagging with an integer (line 3) and .attening \n(line 4) yields [ (1,Remp)* (2,Rdep)* ] which illustrates the precise typing of products coupled with \nsin\u00adgleton types (ie, 1 instead of int). While the groupby (line 5) in\u00adtroduces an approximation the \ndependency between the tag and the corresponding type is kept [ (int, [ ((1,Remp) | (2,Rdep) )+ ]) * \n] Lastly the transform is typed exactly, yielding the .nal type [ {dept:int, deptName:string|null, numEmps:int \n}* ] Note how null is retained in the output type (since there may be employees without a department, \nthen Head may be applied to an empty list returning null, and the selection of name of null re\u00adturns \nnull). For instance suppose to pipe the Jaql grouping de.ned in the introduction into the following Jaql \nexpression, in order to produce a printable representation of the records of the result transform each \nx ( (x.deptName)@\":\"@(to_string x.dep)@\":\"@(x.numEmps)) where @ denotes string concatenation and to_string \nis a conver\u00adsion operator (from any type to string). The composition is ill-typed for three reasons: \nthe .eld dept is misspelled as dep, x.numEmps is of type int (so it must be applied to to_string before \ncon\u00adcatenation), and the programmer did not account for the fact that the value stored in the .eld deptName \nmay be null. The encoding produces the following lines to be appended to the previous code: 12 Transform[ \nx => 13 (x.deptName)@\":\"@(to_string x.dep)@\":\"@(x.numEmps)] in which all the three errors are detected \nby our type system. A subtler example of error is given by the following alternative code 12 Transform[ \n13 { dept : d, deptName: n&#38;String, numEmps: e } => 14 n @ \":\" @ (to_string d) @ \":\" @ (to_string \ne) 15 | { deptName: null, .. } => \"\" 16 | _ => \"Invalid department\" ] which corrects all the previous \nerrors but adds a new one since, as detected by our type system, the last branch can be never selected. \nAs we can see, our type-system ensures soundness, forcing the pro\u00adgrammer to handle exceptional situations \n(as in the null example above) but is also precise enough to detect that some code paths can never be \nreached. In order to focus on our contributions we kept the language of types and .lters simple. However \nthere already exists several con\u00adtributions on the types and expressions used here. Two in particular \nare worth mentioning in this context: recursive patterns and XML. De.nition 3 de.nes patterns inductively \nbut, alternatively, we can consider the (possibly in.nite) regular trees coinductively gen\u00aderated by \nthese productions and, on the lines of what is done in CDuce, use the recursive patterns so obtained \nto encode regular expressions patterns (see [5]). Although this does not enhance ex\u00adpressiveness, it \ngreatly improves the writing of programs since it makes it possible to capture distinct subsequences \nof a sequence by a single match. For instance, when a sequence is matched against a pattern such as [ \n(int as x | bool as y | _)* ], then x captures (the list of) all integer elements (capture variables \nin reg\u00adular expression patterns are bound to lists), y captures all Boolean elements, while the remaining \nelements are ignored. By such pat\u00adterns, co-grouping can be encoded without the Rgrp. For instance, the \ntransform in lines 6-11 can be more compactly rendered as: 6 Transform[(g,[ ((1,es)|(2,ds))* ]) => 7 \n{ dept: g, 8 deptName: (ds;Head).name, 9 numEmps: count(es) }] For what concerns XML, the types used \nhere were originally de\u00ad.ned for XML, so it comes as a no surprise that they can seam\u00adlessly express \nXML types and values. For example CDuce uses the very same types used here to encode both XML types and \nele\u00adments as triples, the .rst element being the tag, the second a record representing attributes, and \nthe third a heterogeneous sequence for the content of the element. Furthermore, we can adapt the results \nof [10] to encode forward XPath queries in .lters. Therefore, it requires little effort to use the .lters \npresented here to encode lan\u00adguages such as JSONiq [28] designed to integrate JSON and XML, or to precisely \ntype regular expressions, the import/export of XML data, or XPath queries embedded in Jaql programs. \nThe description of these encodings can be found in the long version of this article, where we also argue \nthat it is better to extend NoSQL languages with XML primitives directly derived from our system rather \nthan to use our system to encode languages such as JSONiq. As a mat\u00adter of fact, existing approaches \ntend to juxtapose XML and JSON operators thus yielding to strati.ed (ie, not tightly integrated) sys\u00adtems \nwhich have several drawbacks (eg, JSONiq does not allow XML nodes to contain JSON objects and arrays). \nSuch restrictions are absent from our approach since both XML and JSON operators are encoded in the same \nbasic building blocks and, as such, can be freely nested and combined.  5.2 Extensions Hitherto we used \n.lters only to encode primitive operators of some NoSQL languages, in particular Jaql. However, it is \npossible to add .lters to other languages, so as to have user-de.ned operators typed as precisely as \nprimitive ones. From a linguistic point of view this is a no-brainer: it suf.ces to add .lter application \nto the expressions of the host language. However, such an extension can be problematic from a computational \nviewpoint, since it may disrupt the execution model, especially for what concerns aspects of parallelism \nand dis\u00adtribution. A good compromise is to add only .lters that have local effects, which can already \nbring dramatic increases in expressive\u00adness and type precision without disrupting the distributed compila\u00adtion \nmodel. For instance, one can add just pattern and union .lters as in the following (extended) Jaql program: \ntransform ( { {a:x,..} as y => { {y.*, sum:x+x} | y => y ) (with the convention that a .lter occurring \nas an expression de\u00adnotes its application to the current argument $ ). With this syntax, our inference \nsystem is able to deduce that feeding this expression with an argument of type [{ {a?:int, c:bool} }*] \nreturns a result of type [({ {a:int, c:bool, sum:int} | { {c:bool} })*]. This precision comes from the \ncapacity of our inference system to dis\u00adcriminate between the two branches of the .lter and deduce that \na sum .eld will be added only if the a .eld is present. Similarly by using pattern matching in a Jaql \nfilter expression, we can deduce that filter ( int => true | _ => false ) fed with any sequence of elements \nalways returns a (possibly empty) list of in\u00adtegers. An even greater precision can be obtained for grouping \nex\u00adpressions when the generation of the key is performed by a .lter that discriminates on types: the \nresult type can keep a precise cor\u00adrespondence between keys and the corresponding groups.  6. Commentaries \nFinally, let us explain some subtler design choices for our system. Filter design: The reader may wonder \nwhether products and record .lters are really necessary since, at .rst sight, the .lter ( f1,f2) could \nbe encoded as (x, y) . (f1x, f2y) and similarly for records. The point is that f1x and f2y are expressions \nand thus their pair is a .lter only if the fi s are closed (ie, without free term recursion variables). \nWithout an explicit product .lter it would not be possible to program a .lter as simple as the identity \nmap, \u00b5 \u00b5X. nil . nil| (h, t) . ( h,Xt) since Xt is not an ex\u00adpression (X is a free term recursion variable). \nSimilarly, we need an explicit record .lter to process recursively de.ned record types such as \u00b5 \u00b5X. \n({ head:int, tail:X} | nil). Likewise, one can wonder why we put in .lters only the open record variant \nthat copy extra .elds and not the closed one. The reason is that if we want a .lter to be applied only \nto records with exactly the .elds speci.ed in the .lter, then this can be simply obtained by a pattern \nmatching. So the .lter { e1:f1, . . . , en:fn} (ie, without the trailing . . ) can be simply introduced \nas syntactic sugar for { e1:any, . . . , en:any} . { e1:f1, . . . , en:fn , ..} Constructors: The syntax \nfor constructing records and pairs is exactly the same in patterns, types, expressions, and .lters. The \nreader may wonder why we did not distinguish them by using, say, \u00d7 for product types or = instead of \n: in record values. This, com\u00adbined with the fact that values and singletons have the same syntax, is \na critical design choice that greatly reduces the confusion in these languages, since it makes it possible \nto have a unique representation for constructions that are semantically equivalent. Consider for in\u00adstance \nthe pattern ( x, ( (3, nil) ) . With our syntax ( (3, nil) denotes both the product type of two singletons \n3 and nil, or the value ( (3, nil) , or the singleton that contains this value. According to the interpretation \nwe choose, the pattern can then be interpreted as a pattern that matches a product or a pattern that \nmatches a value. If we had differentiated the syntax of singletons from that of values (eg, {v}) and \nthat of pairs from products, then the pattern above could have been written in .ve different ways. The \npoint is that they all would match exactly the same sets of values, which is why we chose to have the \nsame syntax for all of them. Record types: In order to type records with computed labels we distinguished \ntwo cases according to whether the type of a record label is .nite or not. Although such a distinction \nis simple, it is not unrealistic. Labels with singleton types cover the (most common) case of records \nwith statically .xed labels. The dynamic choice of a label from a statically known list of labels is \na usage pattern seen in JavaScript when building an object which must conform to some interface based \non a run-time value. Labels with in.nite types cover the fairly common usage scenario in which records \nare used as dictionaries: we deduce for the expression computing the label the type string, thus forcing \nthe programmer to insert some code that checks that the label is present before accessing it. The rationale \nbehind the typing of records was twofold. First and foremost, in this work we wanted to avoid type annotations \nat all costs (since there is not even a notion of schema for JSON records and collections only the notion \nof basic type is de.ned we cannot expect the Jaql programmer to put any kind of type information in the \ncode). More sophisticated type systems, such as dependent types, would probably preclude type reconstruction: \ndependent types need a lot of annotations and this does not .t our requirements. Second, we wanted the \ntype-system to be simple yet precise. Making the .nite/in.nite distinction increases typing precision \nat no cost (we do not need any extra machinery since we already have singleton types). Adding heuristics \nor complex analysis just to gain some precision on records would have blurred the main focus of our article, \nwhich is not on typing records but on typing transformations on records. We leave such additions for \nfuture work. Record polymorphism: The type-oriented reader will have no\u00adticed that we do not use row \nvariables to type records, and nev\u00adertheless we have a high degree of polymorphism. Row variables are \nuseful to type functions or transformations since they can keep track of record .elds that are not modi.ed \nby the transformation. In this setting we do not need them since we do not type transforma\u00adtions (ie, \n.lters) but just the application of transformations (.lters are not .rst-class terms). We have polymorphic \ntyping via .lters (see how the .rst example given in Section 5.2 keeps track of the c .eld) and therefore \nopen records suf.ce. Related work: In the (nested) relational (and SQL) context, many works have studied \nthe integration of (nested)-relational algebra or SQL into general purpose programming languages. Among \nthe .rst attempts was the integration of the relational model in Pascal [29] or in Smalltalk [12]. Also, \nmonads or comprehensions [8, 31, 32] have been successfully used to design and implement query languages \nincluding a way to embed queries within host languages. Signi.cant efforts have been done to equip those \nlanguages with type systems and type checking disciplines [1, 9, 23, 24] and more recently [25] for integration \nand typing aspects. However, these approaches only support homogeneous sequences of records in the context \nof speci.c classes of queries (practically equivalent to a nested relational algebra or calculus), they \ndo not account for records with computable labels, and therefore they are not easily transposable to \na setting where sequences are heterogeneous, data are semi-structured, and queries are much more expressive. \nWhile the present work is inspired and stems from previous works on the XML iterators, targeting NoSQL \nlanguages made the .lter calculus presented here substantially different from the one of [10, 21] (dubbed \nXML .lters in what follows), as well in syntax as in dynamic and static semantics. In [10] XML .lters \nbehave as some kind of top-down tree transducers, termination is enforced by heavy syntactic restrictions, \nand a less constrained use of the com\u00adposition makes type inference challenging and requires sometimes \ncumbersome type annotations. While XML .lters are allowed to operate by composition on the result of \na recursive call (and, thus, simulate bottom-up tree transformations), the absence of explicit arguments \nin recursive calls makes programs understandable only to well-trained programmers. In contrast, the main \nfocus of the current work was to make programs immediately intelligible to any functional programmer \nand make .lters effective for the typing of sequence transformations: sequence iteration, element .ltering, \none-level .attening. The last two are especially dif.cult to write with XML .lters (and require type \nannotations). Also, the integra\u00adtion of .lters with record types (absent in [10] and just sketched in \n[21]) is novel and much needed to encode JSON transformations.  7. Conclusion Our work addresses two \nvery practical problems, namely the typ\u00ading of NoSQL languages and a comprehensive de.nition of their \nsemantics. These languages add to list comprehension and SQL operators the ability to work on heterogeneous \ndata sets and are based on JSON (instead of tuples). Typing precisely each of these features using the \nbest techniques of the literature would probably yield quite a complex type-system (mixing row polymorphism \nfor records, parametric polymorphism, some form of dependent typ\u00ading,...) and we are skeptical that this \ncould be achieved without us\u00ading any explicit type annotation. Therefore we explored the formal\u00adization \nof these languages from scratch, by de.ning a calculus and a type system. The thesis we defended is that \nall operations typical of current NoSQL languages, as long as they operate structurally (ie, without \nresorting on term equality or relations), amount to a com\u00adbination of more basic bricks: our .lters. \nOn the structural side, the claim is that combining recursive records and pairs by unions, inter\u00adsections, \nand negations suf.ces to capture all possible structuring of data, covering a palette ranging from comprehensions, \nto heteroge\u00adneous lists mixing typed and untyped data, through regular expres\u00adsions types and XML schemas. \nTherefore, our calculus not only provides a simple way to give a formal semantics to, reciprocally compare, \nand combine operators of different NoSQL languages, but also offers a means to equip these languages, \nin they current de.nition (ie, without any type de.nition or annotation), with pre\u00adcise type inference. \nThis type inference yields and surpasses in pre\u00adcision systems using parametric polymorphism and row \nvariables. The price to pay is that transformations are not .rst class: we do not type .lters but just \ntheir applications. However, this seems an advantageous deal in the world of NoSQL languages where se\u00adlects \nare never passed around (at least, not explicitly), but early error detection is critical, especially \nin the view of the cost of code deployment.2 The result are .lters, a set of untyped terms that can be \neasily included in a host language to complement in a typeful framework existing operators with user-de.ned \nones. The requirements to in\u00adclude .lters into a host language are so minimal that every modern typed \nprogramming language satis.es them. The interest resides not in the fact that we can add .lter applications \nto any language, rather that .lters can be used to de.ne a smooth integration of calls to domain speci.c \nlanguages (eg, SQL, XPath, Pig, Regex) into general purpose ones (eg, Java, C#, Python, OCaml) so as \nboth can share the same set of values and the same typing discipline. Like\u00adwise, even though .lters can \nprovide a prototyping platform for queries, they cannot currently be used as a .nal compilation stage \nfor NoSQL languages: their operations rely on a Lisp-like encod\u00ading of sequences and this makes the correspondence \nwith optimized bulk operations on lists awkward. Whether we can derive an ef.\u00adcient compilation from \n.lters to map-reduce (recovering the bulk semantics of the high-level language) is a challenging open \nques\u00adtion. Future plans include practical experimentation of our tech\u00adnique: we intend to benchmark our \ntype analysis against existing collections of Jaql programs, gauge the amount of code that is ill typed \nand verify on this how frequently the programmer adopted defensive programming to cope with the potential \ntype errors. References [1] A. Albano, G. Ghelli, and R. Orsini. Fibonacci: A programming language for \nobject databases. The VLDB Journal, 4:403 444, 1995. [2] N. Alon, T. Milo, F. Neven, D. Suciu, and V. \nVianu. XML with data values: typechecking revisited. In PODS 01. ACM, 2001. 2 Only .lter-encoded operators \nare not .rst class: if the host language provides, say, higher-order functions, then they stay higher-order \nand are typed by embedding the host type system, if any, via foreign type calls . [3] N. Alon, T. Milo, \nF. Neven, D. Suciu, and V. Vianu. Typechecking XML views of relational databases. ACM Trans. Comput. \nLogic, 4:315 354, July 2003. [4] A. Behm et al. Asterix: towards a scalable, semistructured data platform \nfor evolving-world models. DAPD, 29(3):185 216, 2011. [5] V. Benzaken, G. Castagna, and A. Frisch. CDuce: \nan XML-friendly general purpose language. In ICFP 03. ACM, 2003. [6] K. Beyer et al. Jaql: A scripting \nlanguage for large scale semistruc\u00adtured data analysis. PVLDB, 4(12):1272 1283, 2011. [7] S. Boag, D. \nChamberlain, M. F. Fern\u00e1ndez, D. Florescu, J. Robie, and J. Sim\u00e9on. XQuery 1.0: An XML query language, \nW3C rec., 2007. [8] P. Buneman, L. Libkin, D. Suciu, V. Tannen, and L. Wong. Compre\u00adhension syntax. SIGMOD \nRecord, 23(1):87 96, 1994. [9] P. Buneman, R. Nikhil, and R. Frankel. A Practical Functional Pro\u00adgramming \nSystem for Databases. In Proc. Conference on Functional Programming and Architecture. ACM, 1981. [10] \nG. Castagna and K. Nguy \u00ean. Typed iterators for XML. In ICFP 08. ACM, 2008. [11] H. Comon, M. Dauchet, \nR. Gilleron, F. Jacquemard, C. L\u00f6ding, D. Lugiez, S. Tison, and M. Tommasi. Tree automata techniques \nand applications. http://www.grappa.univ-lille3.fr/tata, 2007. [12] G. Copeland and D. Maier. Making \nSmalltalk a database system. In ACM SIGMOD Conf., 1984. [13] J. Engelfriet. Top-down tree transducers \nwith regular look-ahead. Mathematical Systems Theory, 10(1):289 303, Dec. 1976. [14] A. Frisch. Th\u00e9orie, \nconception et r\u00e9alisation d un langage de pro\u00adgrammation adapt\u00e9 \u00e0 XML. PhD thesis, Universit\u00e9 Paris 7 \nDenis Diderot, 2004. [15] A. Frisch, G. Castagna, and V. Benzaken. Semantic subtyping: Deal\u00ading set-theoretically \nwith function, union, intersection, and negation types. Journal of the ACM, 55(4):1 64, 2008. [16] Jaql. \nhttp://code.google.com/p/jaql. [17] JavaScript Object Notation (JSON). http://json.org/. [18] W. Martens \nand F. Neven. Typechecking top-down uniform unranked tree transducers. In ICDT 03. Springer, 2002. [19] \nE. Meijer. The world according to LINQ. ACM Queue, 9(8):60, 2011. [20] E. Meijer and G. Bierman. A co-relational \nmodel of data for large shared data banks. Communications of the ACM, 54(4):49 58, 2011. [21] K. Nguy \n\u00ean. Language of Combinators for XML: Conception, Typing, Implementation. PhD thesis, Universit\u00e9 Paris-Sud \n11, 2008. [22] Odata. http://www.odata.org/. [23] A. Ohori and P. Buneman. Type Inference in a Database \nProgramming Language. In LISP and Functional Programming, 1988. [24] A. Ohori, P. Buneman, and V. Tannen. \nDatabase Programming in Machiavelli a Polymorphic Language with Static Type Inference. In ACM SIGMOD \nConf., 1989. [25] A. Ohori and K. Ueno. Making standard ML a practical database programming language. \nIn ICFP 11, 2011. [26] C. Olston, B. Reed, U. Srivastava, R. Kumar, and A. Tomkins. Pig latin: a not-so-foreign \nlanguage for data processing. In ACM SIGMOD Conf., 2008. [27] F. \u00d6zcan et al. Emerging trends in the \nenterprise data analytics: con\u00adnecting Hadoop and DB2 warehouse. In ACM SIGMOD Conf., 2011. [28] J. Robie \n(editor). JSONiq. http://jsoniq.org. [29] J. Schmidt and M. Mall. Pascal/R Report. Technical Report 66, \nFachbereich Informatik, universit\u00e9 de Hamburg, 1980. [30] Squeryl: A Scala ORM and DSL for talking with \nDatabases with min\u00adimum verbosity and maximum type safety. http://squeryl.org/. [31] V. Tannen, P. Buneman, \nand L. Wong. Naturally embedded query languages. In ICDT, pages 140 154, 1992. [32] P. Trinder and P. \nWadler. Improving list comprehension database queries. In 4th IEEE Region 10 Conference (TENCON), 1989. \n[33] Unql. http://www.unqlspec.org/.   \n\t\t\t", "proc_id": "2429069", "abstract": "<p>We present a calculus for processing semistructured data that spans differences of application area among several novel query languages, broadly categorized as \"NoSQL\". This calculus lets users define their own operators, capturing a wider range of data processing capabilities, whilst providing a typing precision so far typical only of primitive hard-coded operators. The type inference algorithm is based on semantic type checking, resulting in type information that is both precise, and flexible enough to handle structured and semistructured data. We illustrate the use of this calculus by encoding a large fragment of Jaql, including operations and iterators over JSON, embedded SQL expressions, and co-grouping, and show how the encoding directly yields a typing discipline for Jaql as it is, namely without the addition of any type definition or type annotation in the code.</p>", "authors": [{"name": "V&#233;ronique Benzaken", "author_profile_id": "81100297082", "affiliation": "Universit&#233; Paris-Sud, Orsay, France", "person_id": "P3977926", "email_address": "veronique.benzaken@u-psud.fr", "orcid_id": ""}, {"name": "Giuseppe Castagna", "author_profile_id": "81100388576", "affiliation": "CNRS - Universit&#233; Paris Diderot, Sorbonne Paris Cit&#233;, Paris, France", "person_id": "P3977927", "email_address": "Giuseppe.Castagna@univ-paris-diderot.fr", "orcid_id": ""}, {"name": "Kim Nguyen", "author_profile_id": "81318495209", "affiliation": "Universit&#233; Paris-Sud, Orsay, France", "person_id": "P3977928", "email_address": "Kim.Nguyen@lri.fr", "orcid_id": ""}, {"name": "J&#233;r&#244;me Sim&#233;on", "author_profile_id": "81351594728", "affiliation": "IBM Watson Research, Hawthorne, NY, USA", "person_id": "P3977929", "email_address": "simeon@us.ibm.com", "orcid_id": ""}], "doi_number": "10.1145/2429069.2429083", "year": "2013", "article_id": "2429083", "conference": "POPL", "title": "Static and dynamic semantics of NoSQL languages", "url": "http://dl.acm.org/citation.cfm?id=2429083"}