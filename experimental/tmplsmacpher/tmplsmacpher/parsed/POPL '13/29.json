{"article_publication_date": "01-23-2013", "fulltext": "\n Logical Relations for Fine-Grained Concurrency Aaron Turon Jacob Thamsborg Amal Ahmed Lars Birkedal \nDerek Dreyer Northeastern IT University of Northeastern IT University of MPI-SWS, University Copenhagen \nUniversity Copenhagen Germany turon@ccs.neu.edu thamsborg@itu.dk amal@ccs.neu.edu birkedal@itu.dk dreyer@mpi-sws.org \n Abstract Fine-grained concurrent data structures (or FCDs) reduce the gran\u00adularity of critical sections \nin both time and space, thus making it possible for clients to access different parts of a mutable data \nstruc\u00adture in parallel. However, the tradeoff is that the implementations of FCDs are very subtle and \ntricky to reason about directly. Con\u00adsequently, they are carefully designed to be contextual re.nements \nof their coarse-grained counterparts, meaning that their clients can reason about them as if all access \nto them were sequentialized. In this paper, we propose a new semantic model, based on Kripke logical \nrelations, that supports direct proofs of contextual re.nement in the setting of a type-safe high-level \nlanguage. The key idea behind our model is to provide a simple way of expressing the local life stories \nof individual pieces of an FCD s hidden state by means of protocols that the threads concurrently accessing \nthat state must follow. By endowing these protocols with a simple yet powerful transition structure, \nas well as the ability to assert invariants on both heap states and speci.cation code, we are able to \nsupport clean and intuitive re.nement proofs for the most sophisticated types of FCDs, such as conditional \ncompare-and-set (CCAS). Categories and Subject Descriptors D.3.1 [Programming Lan\u00adguages]: Formal De.nitions \nand Theory; F.3.1 [Logics and Mean\u00adings of Programs]: Specifying and Verifying and Reasoning about Programs \nKeywords Re.nement, .ne-grained concurrency, linearizability, separation logic, logical relations, data \nabstraction, local state 1. Introduction Suppose you want to take a sequential mutable data structure \nand adapt it to a concurrent setting, so that multiple threads can safely access it in parallel. The \nsimplest way to do it is to treat all of the operations on the data structure as critical sections governed \nby a common lock. This coarse-grained approach to concurrency is easy for clients to reason about, since \nit essentially sequentializes all access to the data structure, but by the same token it also thwarts \nany speedup one might hope to gain from parallelism. In contrast, .ne\u00adgrained concurrent data structures \n(or FCDs) reduce the granularity of critical sections in both time and space, often down to a single \nprimitive atomic instruction like compare-and-set (CAS), so that clients can exploit parallelism by having \ndifferent threads manipulate different parts of the data structure simultaneously. Permission to make \ndigital or hard copies of all or part of this work for personal or classroom use is granted without fee \nprovided that copies are not made or distributed for pro.t or commercial advantage and that copies bear \nthis notice and the full citation on the .rst page. To copy otherwise, to republish, to post on servers \nor to redistribute to lists, requires prior speci.c permission and/or a fee. POPL 13, January 23 25, \n2013, Rome, Italy. Copyright c &#38;#169; 2013 ACM 978-1-4503-1832-7/13/01. . . $10.00 As FCDs are very \ntricky to reason about directly, they are carefully designed to be contextual re.nements of their course\u00adgrained \ncounterparts. This means essentially that, performance gains aside, no client can tell they are working \nwith the .ne-grained version of a data structure instead of the coarse-grained version. Put another way, \nthe FCD is a faithful implementation of its coarse\u00adgrained speci.cation. Thus, clients can safely reason \nabout the FCD as if all access to it were sequentialized, while at the same time reaping the ef.ciency \nbene.ts of parallelism. Contextual re.nement is clearly an essential property that clients of an FCD \nexpect to hold [20]. The question is how to prove it. In this paper, we propose a new semantic model \nthat supports direct proofs of contextual re.nement in the setting of a type-safe high-level language. \nThe key idea behind our model is to provide a simple way of expressing the protocols that govern the \nhidden state of an FCD and that the threads concurrently accessing it must follow. By endowing these \nprotocols with a simple yet powerful transition structure, as well as the ability to assert invariants \non both heap states and speci.cation code, we are able to support intuitive re.nement proofs for the \nmost sophisticated types of FCDs. We now examine these selling points in more detail. Direct: Rather \nthan prove contextual re.nement directly, most prior approaches have focused on proving a related property \non traces called linearizability [21]. Linearizability is often viewed as being synonymous with contextual \nre.nement, but in fact this has only recently been shown by Filipovic et al. [14] to be the case (and \nonly for a particular class of languages). As Filipovic et al. argue, re.nement is the property that \nclients of an FCD actually want, and linearizability is one technique for proving it. We instead provide \na direct proof technique for contextual re.nement, which sidesteps any discussion of linearizability. \nHigh-level language: Most prior work has only considered FCDs coded in .rst-order C-like languages. However, \none of the most widely-used FCD libraries, java.util.concurrent, is written in a type-safe high-level \nlanguage (Java) and indeed depends on the ab\u00adstraction facilities of Java to ensure that the private \nstate of its FCDs is hidden from clients. Our approach is the .rst to prove contextual re.nement for \njava.util.concurrent-style FCDs in the setting of a higher-order language with abstract types, recursive \ntypes, and gen\u00aderal mutable references, thus establishing the correctness of these FCDs when linked with \nunknown well-typed client code. How? To achieve these .rst two aims, we employ a step-indexed Kripke \nlogical relations (SKLR) model [2, 3, 9, 4]. SKLRs have been actively developed in recent years as an \neffective tool for reasoning about representation independence for higher-order stateful ADTs, and thus \nprovide a solid foundation for reasoning about contextual re.nement of concurrent objects in a realistic, \nhigh-level setting. To adapt SKLRs to reasoning about FCDs, we follow the approach of recent work by \nBirkedal et al. [5] and employ a small-step-style model, which accounts properly for the possibility \nthat threads are preempted after every step of computation. Birkedal et al. s model, however, is limited \nin its ability to reason about interference between threads, which is ubiquitous in FCDs. We therefore \ngeneralize their model with support for protocols.  Protocols: To understand how an FCD works, it helps \nto think of each piece of the data structure (e.g., each node of a linked list) as being subject to a \nprotocol that tells its life story : how it came to be allocated, how its contents evolve over time, \nand how it eventually dies by being disconnected (or deleted) from the data structure. This protocol \ndescribes the rules by which all threads must play as they access the shared state of the FCD. A number \nof FCDs additionally require their protocols to support role-playing that is, a mechanism by which different \nthreads participating in the protocol can dynamically acquire certain roles . These roles may enable \nthem to make certain transitions that other threads cannot. A simple example of this is a locking protocol, \nunder which the thread that acquires the lock adopts the unique role of lock-holder and thus knows that \nno other thread has the ability to release the lock. How? Following recent work by Dreyer et al. in the \nsetting of SKLRs [9], our model supports a direct encoding of protocols as state transition systems (STSs) \nof a certain kind. In comparison to Dreyer et al. s work, we deploy STSs at a much .ner granularity and \nuse them to tell local life stories about individual nodes of a data structure. Of course, there are \nalso global constraints connecting up the life stories of the individual nodes, but to a large extent \nwe are able to reason about FCDs at the level of these local life stories and their local interactions \nwith one another. In order to account for role-playing, we enrich our STSs with a notion of tokens. Intuitively, \nthe idea is that, while the STS de.nes the basic roadmap for the possible changes to the state of the \nFCD, some of the roads on that map are toll roads that may only be traversed by threads owning certain \ntokens. This idea is highly reminiscent of recent work on concurrent abstract predicates [7], but we \nbelieve our approach is simpler and more direct. The most sophisticated types of FCDs: In proving re.nement \nfor each operation of an FCD, a key step is identifying its linearization point, the point during its \nexecution at which the operation can be considered to have committed , i.e., the point at which its coarse\u00adgrained \nspec can be viewed as having executed atomically. What sets the most sophisticated FCDs apart from the \npack, and makes them so challenging to verify, is that their linearization points are hard to identify \nin a thread-local and temporally-local way. For example, the elimination stack FCD [19] provides side \nchannels by which a push and pop operation can mutually decide to cancel each other out without touching \nthe stack itself. This works by having one operation (say, push) use the side channel to offer its argument \nto be pushed; if a thread running the pop operation sees this offer, it can commit both the push and \npop at once. Although it is very kind of the pop thread to cooperate with the push thread by helping \nit complete its operation in this way, it also means that the linearization point for push occurs during \nthe execution of pop, thus making thread-local veri.cation dif.cult. In other algorithms like CCAS [18, \n15], the nondeterminism in\u00adduced by shared-state concurrency has the effect that it is impossible to \ndetermine where the linearization point has occurred until after the .ne-grained operation has completed \nits execution. This in turn makes it challenging to reason about re.nement in a temporally\u00adlocal way, \ni.e., showing that each step of the algorithm, considered in isolation, obeys the FCD s shared-state \nprotocol. How? The whole point of SKLRs is to provide a way of describing local knowledge about the hidden \nresources of an abstract data type, but in prior work those hidden resources have been synonymous with \nlocal variables or a private piece of the heap . To support reasoning about thread cooperation and nondeterminism, \nwe make two orthogonal generalizations to the notion of resources. First, to model cooperation, we extend \nresources to also include speci.cation code. This extension makes it possible for the right to commit \nan operation (e.g., push, in the example above) to be treated as a shareable resource, which one thread \nmay pass to other helper threads to run on its behalf. Second, to model nondeterminism, we extend resources \nto include sets of speci.cation states. This extension makes it possible to speculate about all the possible \nspeci.cation states that our FCD implementation could be viewed as re.ning, so that we can wait until \nthe implementation has .nished executing to decide which one we want to choose. Both of these extensions \nare formally and conceptually simple especially in comparison to prior approaches relying on ghost and \nprophecy variables [1] and we will demonstrate their utility on both illustrative toy examples (Sections \n2.5 and 2.6) and a more realistic FCD example toward the end of the paper (Section 4). 2. The main ideas \n 2.1 The language We study FCDs within a variant of the polymorphic lambda cal\u00adculus, extended with tagged \nsums, general mutable references (higher-order state), equi-recursive types, CAS, and fork. These features \nsuf.ce for modeling the kinds of FCDs used within java.util.concurrent, which (a) are polymorphic, (b) \nuse recur\u00adsive, linked data structures, and (c) rely on the abstraction facilities of the language for \ndata hiding.1 Figure 1 presents the syntax of the language together with excerpts of the static and dynamic \nsemantics. In examples, we will employ a few other features that trivially extend the language de.ned \nhere, e.g., immutable pairs and records. While the language is essentially standard, there are a few \nunusual aspects that help keep our treatment of FCDs concise. Terms are not annotated with types, but \npolymorphism is nevertheless introduced and eliminated by explicit type abstraction (..e) and application \n(e ). Reference and tuple types are combined into the type ref(t), useful for constructing objects with \nmany mutable .elds. The term e[i] reads and projects the i-th component from a tuple reference e, while \ne[i] := e' assigns a new value to that component. When e is a single-cell reference, we will usually \nwrite e := e' instead of e[1] := e'. Finally, the type ref?(t) of option references provides an untagged \nunion of the unit and reference types, with explicit coercions null and some(e). Because reading and \nwriting operations work on references, and not option references (which must be separately eliminated \nby cases), there are no null\u00adpointer errors. The net effect of .attening these types is fewer layers \nof indirection, which simpli.es veri.cation. The type system imposes two important restrictions. First, \nrecur\u00adsive types \u00b5a.t are required to be productive, meaning that all free occurrences of a in t must \nappear under a non-\u00b5 type constructor. More subtle is the restriction on CAS, which can only be used \non components of comparable type s. This constraint is needed be\u00adcause CAS performs an equality comparison \nof word-sized values at the hardware level; it is only reasonable to apply it at types whose representation \nallows such a comparison, i.e., base types and loca\u00adtions. Tagged sums are allocated on the heap and \nhence represented using locations, making them comparable as well. Figure 1 presents the nonstandard \ntyping rules; the remaining rules are standard (see appendix [35]). Note that we use u to denote values \nthat can be stored in the heap. A threadpool T is a .nite map from thread identi.ers to expres\u00adsions. \nA program con.guration h; T pairs a heap with a thread pool. We de.ne a small-step, call-by-value operational \nsemantics as a re\u00ad 1 We will use closures as our primary means of hiding local state.  t ::= 1 | B | \nN | t + t | ref(t) | ref?(t) | \u00b5a.t | .a.t | a | t . t s ::= 1 | B | N | t + t | ref(t) | ref?(t) | \u00b5a.s \ne ::= () | true | false | if e then e else e | n | e + e | x | ..e | e | e e | rec f(x).e | new e | e[i] \n| e[i] := e | CAS(e[i], e, e) | J | fork e | null | some(e) | case(e, null . e, some(x) . e) | inji e \n| case(e, inj1 x . e, inj2 y . e) v ::= rec f(x).e | ..e | () | n | true | false | J | x u ::= (v) | \ninji v G ::= \u00b7 | G, x : t . ::= \u00b7 | ., a O ::= .; G K ::= [ ] | some(K) | case(K, null . e, some(x) . \ne) | K | \u00b7 \u00b7 \u00b7 T . ThreadPool l N .nl Exp h . Heap l Loc .nl HeapVal Type rules O f e : t O f e : ref(t) \nti = s O f eo : s O f en : s O f e : 1 O f CAS(e[i], eo, en) : B O f fork e : 1 O f e : ref(t) O, a \nf e : t O f e : .a.t O f some(e) : ref?(t) O f ..e : .a.t O f e : t[t ' /a] ' Primitive reductions h; \ne '. h ' ; e h; J[i] '. h; vi when h(J) = (v) h; CAS(J[i], vo, vn) '. h[J[i] = vn]; true when h(J)[i] \n= vo h; CAS(J[i], vo, vn) '. h; false when h(J)[i]= vo h; case((), null . e1, some(x) . e2) '. h; e1 \nh; case(J, null . e1, some(x) . e2) '. h; e2[J/x] h; new (v) '. h 1 [J . (v)]; J h; null '. h; () h; \ninji v '. h 1 [J . inji v]; J h; some(J) '. h; J h; ..e '. h; e ' h; T . h ' ; T Program reduction ' \nh; e '. h ' ; e h; T 1 [i . K[e]] . h ' ; T 1 [i . K[e ' ]] h; T 1 [i . K[fork e]] . h; T 1 [i . K[()]] \n1 [j . e] Figure 1. Our language: F \u00b5! with fork and CAS lation . between program con.gurations, allowing \nthe creation of new threads and the nondeterministic interleaving of existing ones. We use evaluation \ncontexts K to specify a left-to-right evaluation order within each thread.  2.2 A .ner look at FCDs \nThe granularity of a concurrent data structure is a measure of the locality of synchronization between \nthreads accessing it. Coarse\u00adgrained data structures provide exclusive, global access for the duration \nof a critical section: a thread holding the lock can access as much of the data structure as needed, \nsecure in the knowledge that it will encounter a consistent, frozen representation. By contrast, .ne-grained \ndata structures localize or eliminate synchronization, forcing threads to do their work on the basis \nof limited knowledge about its state sometimes as little as what the contents of a single word are at \na single moment. The local, highly-concurrent nature of FCDs is best understood by example. In Figure \n2, we give a variant of Michael and Scott s lock-free queue [29].2 The queue maintains a reference, head, \nto a nonempty linked list; the .rst node of the list is considered a sentinel whose data does not contribute \nto the queue. Nodes are dequeued from the front of the list, so we examine the deq code .rst. If the \nqueue is logically nonempty, it contains at least two nodes: the sentinel (physical head), and its successor \n(logical 2 We use the shorthand cons(e, e ' ) l some(new (e, e ' )). head). Intuitively, the deq operation \nshould atomically update the head reference from the sentinel to its successor; after doing so, the old \nlogical head becomes the new sentinel, and the next node, if any, becomes the new logical head. Because \nthere is no lock protecting head, however, a concurrent operation could update it at any time. Thus, \ndeq employs optimistic concurrency: after gaining access to the sentinel by dereferencing head, it does \nsome additional work .nding the logical head while optimistically assuming that head has not changed \nbehind its back. In the end, optimism meets reality through CAS, which performs an atomic update only \nwhen head is unchanged. If its optimism was misplaced, deq must start from scratch. After all, the queue \ns state may have entirely changed in the interim. The key thing to notice is just how little knowledge \ndeq has as it executes. Immediately after reading head, the most that can be said is that the resulting \nnode was once the physical head of the queue. The power of CAS is that it mixes instantaneous knowledge \nthe head is now n with instantaneous action the head becomes n ' . The weakness of CAS is that this potent \nmixture applies only to a single word of memory. For deq, this weakness is manifested in the lack of \nknowledge CAS has about the new value n ', which should still be the successor to the physical head n \nat the instant of the CAS. Because CAS cannot check this fact, it must be established pessimistically, \ni.e., guaranteed to be true on the basis of the queue s internal protocol. We will see in a moment how \nto formulate such a protocol, but .rst, we examine the more subtle enq. In a singly-linked queue implementation, \none would expect to have both head and tail pointers, and indeed the full Michael-Scott queue includes \na tail pointer. However, because CAS operates on only one word at a time, it is impossible to use a single \nCAS operation to both link in a new node and update a tail pointer. The classic algorithm allows the \ntail pointer to lag behind the true tail by at most one node, while the implementation in java.util.concurrent \nallows multi-node lagging. These choices affect performance, of course, but from a correctness standpoint \none needs to make essentially the same argument whether one has a lagging tail, or simply traverses from \nhead as we do. In all of these cases, it is necessary to .nd the actual tail of the list (whose successor \nis null) by doing some amount of traversal. Clearly, this requires at least that the actual tail be reachable \nfrom the starting point of the traversal; the loop invariant of the traversal is then that the tail is \nreachable from the current node. But in our highly-concurrent environment, we must account for the fact \nthat the data structure is changing under foot, even as we traverse it. The node that was the tail of \nthe list when we began the traversal might not even be in the data structure by the time we .nish.  \n2.3 The story of a node We want, ultimately, to prove that MSQ re.nes its coarse-grained speci.cation \nCGQ. The latter uses a lock to protect its internal state; sync(e) { e ' } is a simple derived form that \nwraps acquisition/re\u00ad ' lease of the non-reentrant spinlock e around the code e . 3 Ideally, the proof \nwould proceed in the same way one s intuitive reasoning does, i.e., by considering the execution of a \nsingle function by a single thread one line at a time, reasoning about what is known at each program \npoint. To achieve this goal, we must solve two closely\u00adrelated problems: we must characterize the possible \ninterference from concurrent threads, and we must characterize the knowledge that our thread can gain. \nWe solve both of these problems by introducing a notion of protocol, based on the abstract state transition \nsystems of Dreyer et al. [9], but with an important twist: we apply these transition systems at the level \nof individual nodes, rather than as a description 3 The de.nition of sync is given in the appendix [35]. \n MSQ: .a.1 . { enq : a . 1, deq : 1 . ref?(a) }MSQ .. .(). let head = new (new (null, null)) in {deq \n= rec try(). let n = head[1] in case n[2] of some(n ' ) . if CAS(head[1], n, n ' ) then n ' [1] else \ntry() | null . null, (* queue is empty *) enq = .x. let n = cons(some(new x), null) in let rec try(c) \n= case c[2] of some(c ' ) . try(c ' ) | null . if CAS(c[2], null, n) then () else try(c) in try(head[1]) \n} CGQ .. .(). let head = new null, lock = new false in {deq = .(). sync(lock) {case head[1] of some(n) \n. head[1] := n[2]; some(new n[1]) | null . null }, enq = .x. sync(lock) { . . . } (* elided *) } Per-node \nprotocol: Logically in queue Reachable Global interpretation: I(s) headI .I J0 * J0 .I (v0, vI) * .vS. \nheadS .S vS * lock .S false * link(vI, vS, sL) * (s(J ' ) = . . Ji .I (vi, J ' )) i i when s = [J0 . \nSentinel(v0, vI)] 1 sL 1 [Ji . Dead(vi, J ' )] i link(null, null, \u00d8) emp ' ' link(JI, JS, [JI . Live(vI, \nv I )] 1 s) .v, vS, v S. v .V vS : a * ' ''' vI .I v * JI .I (vI, v I ) * JS .I (vS, v S) * link(vI , \nv S, s) Figure 2. A variant of Michael and Scott s queue of the entire data structure. These transition \nsystems describe what we call the local life stories of each piece of an FCD. The diagram in Figure 2 \nis just such a story. Every heap location can be seen as a potential node in the queue, but all but .nitely \nmany are unborn (state .). After birth, nodes go through a progression of life changes. Some changes \nare manifested physically. The transition from Live(v, null) to Live(v, e), for example, occurs when \nthe successor .eld of the node is updated to link in a new node. Other changes re.ect evolving relationships. \nThe transition from Live to Sentinel, for example, does not represent an internal change to the node, \nbut rather a change in the node s position in the data structure. Finally, a node dies when it becomes \nunreachable. The bene.t of these life stories is that they account for knowledge and interference together, \nin a local and abstract way. Knowledge is expressed by asserting that a given node is at least at a certain \npoint in its life story. This kind of knowledge is inherently stable under interference, because all \ncode must conform to the protocol, and is therefore constrained to a forward march through the STS. The \nlife story gathers together in one place all the knowledge and interference that is relevant to a given \nnode, even knowledge like reachability which is ostensibly a global property. This allows us to draw \nglobal conclusions from local information, which is precisely what is needed when reasoning about FCDs. \nFor example, notice that no node can die with a null successor .eld. A successful CAS on the successor \n.eld from null to some location like the one performed in enq entails that the successor .eld was instantaneously \nnull (local information), which by the protocol means the node was instantaneously reachable (global \ninformation), which entails that the CAS makes a new node reachable. Similarly, the protocol makes it \nimmediately clear that the queue is free from any ABA problems [34], because nodes cannot be reincarnated \nand their .elds, once non-null, never change. To formalize this reasoning, we must connect the abstract \nac\u00adcount of knowledge and interference provided by the protocol to concrete constraints on the queue \ns representation. We do this by giving a state-dependent invariant I for the data structure, where state \nrefers to abstract STS states. For the queue, we have the following set of states for each node s local \nSTS: ' S0 {.} . {Live(v, v ' ) | v, v . Val} ' . {Sentinel(v, v ' ) | v, v . Val} . {Dead(v, J) | v . \nVal, J . Loc} along with the transition relation .0 given in the diagram, where the annotated edges denote \nbranches for choosing particular concrete v and e values. The data structure as a whole is governed by \na product STS with states S Loc .n. indicates that all but . S0, where .n.nitely many locations are in \nthe . (unborn) state in their local STS. The transition relation . for the product STS is just the pointwise \nlifting of the one for each node s STS: s . s ' iff .e. s(e) .0 s ' (e). Thus, at the abstract level, \nthe product STS is simply a collection of independent, local STSs. At the concrete level of the invariant \nI, however, we record the constraints that tie one node s life story to another s; see Figure 2. The \ninvariant is essentially a relational version of the recursive list predicate from separation logic [33]. \nThe relational nature is apparent in the fact that the invariant makes assertions about both the implementation \n( .I) and speci.cation ( .S) heap, carving out the portion of each corresponding to an instance of MSQ \nand CGQ, respectively. It is thus a kind of linking invariant (or re.nement map ) [22, 1], as one would \nexpect to .nd in any re.nement proof. At a high level, the invariant says: the state of the product STS \nmust decompose into exactly one sentinel node (at location e0), a collection of Live nodes sL, and a \ncollection of Dead nodes at locations ei (the overbar notation represents a list). The link assertion \nrecursively asserts that each Live node in the implementation corresponds to some node in the speci.cation, \nand that a node is Live iff it is reachable from the sentinel. Since the queue is parametric over the \ntype a of its data, the data stored in each live implementation node must re.ne the data stored in the \nspeci.cation node at type a (written v1 .V v2 : a; see Section 3). The invariant also accounts for two \nrepresentation differences between MSQ and CGQ. First, the node data in the implementation is stored \nin a ref?(a), while the speci.cation stores the data directly. Second, the speci.cation has a lock. The \ninvariant requires that the lock is always free (false) because, as we show that MSQ re.nes CGQ, we always \nrun entire critical sections of CGQ at once, going from unlocked state to unlocked state. These big steps \nof the CGQ correspond to the linearization points of the MSQ. Finally and crucially Dead nodes must have \nnon-null suc\u00adcessor pointers whose locations are in a non-. state. This property is the key for giving \na simple, local loop invariant for enq, namely, that the current node c is at least in a Live state. \nIt follows that if the successor pointer of c is not null, it must be another node at least in the Live \nstate. If, on the other hand, the successor node of c is null, we know that c is both not Dead, and at \nleast Live, which means that c must be (at that instant) reachable from the implementation s head pointer. \nThe proof outlines for MSQ and the other examples in this section are given in the appendix [35].  \n2.4 Role-playing and tokens Although Michael and Scott s queue is already tricky to verify, there is \na speci.c sense in which its protocol in Figure 2 is simple: it treats all threads equally. All threads \nsee a level playing .eld with a single notion of legal transition, and any thread is free to make any \nlegal transition according to the protocol. Many FCDs, however, require more re.ned protocols in which \ndifferent threads can play different roles granting them the rights to make different sets of transitions \nand in which threads can acquire and release these roles dynamically as they execute. In fact, one need \nnot look to complex FCDs for instances of this dynamic role-playing the simple lock used in the coarse-grained \nspec of the Michael-Scott queue is a perfect and canonical example. In a protocol governing a single \nlock (e.g., lock, in CGQ), there are two states: Unlocked and Locked. Starting from the Unlocked state, \nall threads should be able to acquire the lock and transition to the Locked state. But not vice versa: \nonce a thread has acquired the lock and moved to the Locked state, it has adopted the role of lock\u00adholder \nand should know that it is the only thread with the right to release the lock and return to Unlocked. \nTo support this kind of role-playing, we enrich STSs with a notion of tokens, which are used to grant \nauthority over certain types of actions in a protocol. Each STS may employ its own appropriately chosen \nset of tokens, and each thread may privately own some subset of these tokens. The idea, then, is that \ncertain transitions are only legal for the thread that privately owns certain tokens. Formally speaking, \nthis is achieved by associating with each state in the STS a set of tokens that are currently free, i.e., \nnot owned by any thread.4 We then stipulate the law of conservation of tokens: for a thread to legally \ntransition from state s to state s ', the (disjoint) union of its private tokens and the free tokens \nmust be the same in s and in s ' . For instance, in the locking protocol, there is just a single token \ncall it TheLock. In the Unlocked state, the STS asserts that TheLock must belong to the free tokens and \nthus that no thread owns it privately, whereas in the Locked state, the STS asserts that TheLock does \nnot belong to the free tokens and thus that some thread owns it privately. Pictorially, denotes that \nTheLock is in the free tokens, and . denotes that it is not: When a thread acquires the physical lock \nand transitions to the Locked state, it must add TheLock to its private tokens in order to satisfy conservation \nof tokens. Thereafter, no other thread may transition back to Unlocked because doing so requires putting \nTheLock back into the free tokens of the STS, which is something only the private owner of TheLock can \ndo. Usually the invariant for the Unlocked state owns all of the hidden state for the data structure, \nwhile the Locked invariant owns nothing. Thus, a thread taking the lock also acquires the resources it \nprotects, but must return these resources on lock release (in the style of CSL [31]). As this simple \nexample suggests, tokens induce very natural thread-relative notions of rely and guarantee relations \non states of an STS. For any thread i, the total tokens A of an STS must equal the disjoint union of \ni s private tokens Ai, the free tokens Afree in the current state s, and the frame tokens Aframe (i.e., \nthe combined private tokens of all other threads but i). The guarantee relation says which future states \nthread i may transition to, namely those that are accessible by a series of transitions that i can pay \nfor using its private tokens Ai. Dually, the rely relation says which future states other threads may \ntransition to, namely those that are accessible by 4 Another perspective is that the free tokens are \nowned by the STS itself, as opposed to the threads participating in the protocol; cf. CSL [31]. redFlag \n.(). let .ag = new true, chan = new 0 in { .ip = rec try(). if CAS(chan, 1, 2) then () else if CAS(.ag, \ntrue, false) then () else if CAS(.ag, false, true) then () else if CAS(chan, 0, 1) then if CAS(chan, \n1, 0) then try() else chan := 0 else try(), read = .(). .ag[1] } blueFlag .(). let .ag = new true, lock \n= new false in {.ip = .(). sync(lock) { .ag := not .ag[1] }, read = .(). sync(lock) { .ag[1] } } O.ered(j, \nK ); . j, K Empty; Q .x : B. .agI .I x * .agS .S x * lock .S false I(Empty) Q * chan .I 0 I(O.ered(j, \nK )) Q * chan .I 1 * j >S K[.ipS()] I(Accepted(j, K )) Q * chan .I 2 * j >S K[()] Figure 3. Red .ags \nversus blue .ags a series of transitions that can be paid for without using i s private tokens Ai (i.e., \nonly using the tokens in Aframe ). These two relations play a central role in our model (Section 3). \n 2.5 Cooperation and speci.cations-as-resources As explained in the introduction, some FCDs use side \nchannels, separate from the main data structure, to enable threads executing different operations to \ncooperate. To illustrate this, we use a toy example inspired speci.cally by elimination stacks [19] that \nisolates the essential challenge of reasoning about cooperation, minus the full-blown messiness of a \nreal data structure. Figure 3 shows the example, in which redFlag is a lock-free implementation of blueFlag. \nThe latter is a very simple data struc\u00adture, which maintains a hidden boolean .ag, and provides oper\u00adations \nto .ip it and read it. One obvious lock-free implementa\u00adtion of .ip would be to keep running CAS(.ag, \ntrue, false) and CAS(.ag, false, true) repeatedly until one of them succeeds. How\u00adever, to demonstrate \ncooperation, redFlag does something more clever : in addition to maintaining .ag, it also maintains a \nside channel chan, which it uses to enable two .ip operations to cancel each other out without ever modifying \n.ag at all! More speci.cally, chan adheres to the following protocol, which is visualized in Figure 3 \n(ignore the K s for now). If chan .I 0, it means the side channel is not currently being used (it is \nin the Empty state). If chan .I 1, it means that some thread j has offered to perform a .ip using the \nside channel and moved it into the O.ered(j, -) state. If chan .I 2, it means that another thread has \naccepted thread j s offer and transitioned to Accepted(j, -) thus silently performing both .ip s at once \n(since they cancel out) but that thread j has not yet acknowledged that its offer was accepted. Like \nthe locking example, this protocol uses a single token call it O.er which is free in state Empty but \nwhich thread j moves into its private tokens when it transitions to the O.ered(j, -) state. After that \ntransition, due to its ownership of O.er, thread j is the only thread that has the right to revoke that \noffer by setting chan back to 0 and returning to Empty. On the other hand, any thread may transition \nfrom O.ered(j, -) to Accepted(j, -), since the two states have identical free tokens, namely, none. Once \nin the Accepted(j, -) state, though, thread j is again the only thread able to Empty the channel.  The \nimplementation of .ip in redFlag then works as follows. First, we use CAS to check if another thread \nhas offered to .ip (i.e., if chan .I 1), and if so, we accept the offer by setting chan to 2. We then \nimmediately return, having implicitly committed both .ips right then and there, without ever accessing \n.ag. If that fails, we give up temporarily on the side-channel shenanigans and instead try to perform \na bona .de .ip by doing CAS(.ag, true, false) and CAS(.ag, false, true) as suggested above. If that fails \nas well, then we attempt to make an offer on the side channel by changing chan from 0 to 1. If our attempt \nsucceeds, then we (rather stupidly5) try to immediately revoke the offer and loop back to the beginning. \nIf perchance another thread has preempted us at this point and accepted our offer (i.e., if CAS(chan, \n1, 0) fails, implying that another thread has updated chan to 2), then that other thread must have already \ncommitted our .ip on our behalf, so we simply set chan back to 0, thus making the side channel free for \nother threads to use, and return. Finally, if all else fails, we loop again. As far as the re.nement \nproof is concerned, there are essentially two interesting points here. The .rst concerns the CAS(chan, \n1, 0) step. As we observed already, the failure of this CAS implies that chan must be 2. Why? Because \nof the way our protocol uses tokens. After the previous CAS(chan, 0, 1) succeeded, we knew that we had \nsuccessfully transitioned to the O.ered(j, -) state, and thus that our thread j now controls the O.er \ntoken. Our ownership of O.er tells us that other threads can only transition to a limited set of states \nvia the rely ordering (i.e., without owning O.er): they can either leave the state where it is, or they \ncan transition to Accepted(j, -). Thus, when we observe that chan is not 1, we know it must be 2. The \nsecond, more interesting point concerns the semantics of cooperation. If we make an offer on chan, which \nis accepted by another thread, it should imply that the other thread performed our .ip for us, so we \ndon t have to. At least that s the intuition, but how is that intuition enforced by the protocol? That \nis, when we observe that our offer has been accepted, we do so merely by inspecting the current value \nof chan. But how do we know that the other thread that updated chan from 1 to 2 actually performed our \n.ip for us? For example, as perverse as this sounds, what is to prevent redFlag from performing chan \n:= 2 as part of its implementation of read? Our key to enforcing that the semantics of cooperation is \nre\u00adspected is to treat speci.cation code as a kind of resource. We introduce a new assertion, j >S e, \nwhich describes the knowledge that thread j (on the spec side) is poised to run the term e. Ordinarily, \nthis knowledge is kept private to thread j itself, but in a cooperative protocol, the whole idea is that \nj should be able to pass control over its spec code e to other threads, so that they may execute some \nsteps of e on its behalf. Speci.cally, this assertion is used to give semantic meaning to the O.ered(j, \nK ) and Accepted(j, K ) states in our protocol (see the interpretation of F in Figure 3). In the former \nstate, we know that j >S K[.ipS()], which tells us that thread j has offered its spec code K[.ipS()] \nto be run by another thread, whereas in the latter state, we know that j >S K[()], which tells us that \nj s .ip has been executed. (The K is present here only because we do not want to place any restrictions \non the evaluation context of the .ip operation.) These interpretations demand that whatever thread accepts \nthe offer by transitioning from O.ered(j, K ) to Accepted(j, K ) must take the responsibility not only \nof updating chan to 2 but also of executing .ipS() and only .ipS() on j s behalf. When j subsequently \nmoves back to the Empty state, it 5 At this point in a real implementation, it would make sense to wait \na while for other threads to accept our offer, but we elide that detail since it is irrelevant for reasoning \nabout correctness. rand .(). let y = new false in (fork y := true); y[1] lateChoice .x. x := 0; rand() \nearlyChoice .x. let r = rand() in x := 0; r ( V xI xS : ref(N) . j >S K[earlyChoice(xS)] xI := 0 ( V \nxI xS : ref(N) . (j >S K[true] . j >S K[false]) rand() ( ret. (ret = true . ret = false) . (j >S K[true] \n. j >S K[false]) ( ret. j >S K[ret]) Figure 4. Late choice versus early choice regains private control \nover its speci.cation code, so that other threads may no longer execute it.  2.6 Nondeterminism and \nspeculation Another tricky aspect of reasoning about FCDs (like the conditional CAS example we consider \nin Section 4) is dealing with nondeter\u00adminism. The problem is that when proving that an FCD re.nes some \ncoarse-grained spec, we want to reason in a temporally-local fashion i.e., using something akin to a \nsimulation argument, by which the behavior of each step of FCD code is matched against zero or more steps \nof spec code but nondeterminism, it would seem, foils this plan. To see why, consider the late choice/early \nchoice example in Figure 4. This example is really simple: it does not maintain any hid\u00adden state (hence \nno protocol), and is not in fact an FCD at all, but it nevertheless illustrates the core dif.culty with \nnondeterminism. We want to show that lateChoice re.nes earlyChoice. Both functions .ip a coin (i.e., \nuse rand() to nondeterministically choose a boolean value) and set a given variable x to 0, but they \ndo so in opposite orders. Intuitively, though, the order shouldn t matter: there is no way to observe \nthe coin .ip until the functions return. However, if we try to reason about the re.nement using a simulation \nargument, we run into a problem. The .rst step of lateChoice is the setting of x to 0. To simulate this \nstep in earlyChoice, we need to match the assignment of x to 0 as well, since the update is an externally \nobservable effect. But to do that we must .rst .ip earlyChoice s coin. While we have the freedom to choose \nthe outcome of the .ip,6 the trouble is that we don t know what the outcome should be: lateChoice s coin \n.ip has yet to be executed. The solution is simple: speculate! That is, if you don t know which spec \nstates to step to in order to match an implementation step, then keep your options open and maintain \na speculative set of speci.cation states that are reachable from the initial spec state and consistent \nwith any observable effects of the implementation step. In the case of lateChoice/earlyChoice, this means \nthat we can simulate the .rst step of lateChoice (the setting of x to 0) by executing the entire earlyChoice \nfunction twice. In both speculative states x is set to 0, but in one the coin .ip returns true, and in \nthe other it returns false. This reasoning is captured in the Hoare-style proof outline given in Figure \n4. The precondition j >S K[earlyChoice(xS)] an in\u00adstance of the assertions on speci.cation code introduced \nin the previ\u00adous section denotes that initially the spec side is poised to execute earlyChoice. After \nwe execute x := 0 in lateChoice, we speculate that the coin .ip on the spec side could result in earlyChoice \neither returning true or returning false. This is represented by the spec\u00adulative assertion (j >S K[true] \n. j >S K[false]) appearing in the postcondition of this .rst step, in which the . operator provides a \nspeculative choice between two subassertions characterizing pos\u00adsible spec states. In the subsequent \nstep, lateChoice .ips its coin, 6 For every implementation execution, we must construct some speci.cation \nexecution.  yielding a return value ret of either true or false. We can then re.ne the speculative set \nof speci.cation states to whichever one (either j >S K[true] or j >S K[false]) matches ret, and simply \ndrop the other state from consideration. In the end, what matters is that we are left with at least one \nspec state that has been produced by a sequence of steps matching the observable behavior of the implementation \ns steps. The idea of speculation is not new: it is implicit in Lynch and Vaandrager s notion of forward-backward \nsimulation [28]. What is new here is that we capture speculation assertionally without using prophecy \nvariables, which allows us to compose speculations without fuss. For example, we can use speculative \nreasoning in a private, thread-local way (as in the choice example) while separately using it within \nthe protocol governing some shared state (as in the CCAS example, Section 4). We give a more detailed \ncomparison to related work in Section 5. 3. The formal model In this section, we develop the syntax and \nsemantics of a logic for concurrent programs in our higher-order, polymorphic language. We leave a proof \ntheory for future work, and instead work in the model (semantically) when verifying examples. 3.1 Overview: \nproving re.nement via hiding and protocols Our logic is ultimately used to prove contextual re.nements, \nso it is important to .rst make clear what that entails, both formally and practically. To de.ne contextual \nre.nement, we .rst introduce a standard typing judgment for contexts, C : (O, t ) (O ' , t ' ) so that \nwhenever O f e : t, we have O ' f C[e] : t '. Then if O f eI : t and O f eS : t, we say eI contextually \nre.nes eS, written O |= eI eS : t, if: for every i, j and C : (O, t ) (\u00d8, N) we have .n..TI. \u00d8; [i . \nC[eI]] . * h1; [i . n] l TI =. .TS. \u00d8; [j . C[eS]] . * h2; [j . n] l TS Re.nement formalizes observable \nbehavior as anything a client of an expression could test. A context C captures the notion of an unknown \n(but well-typed!) client, which can interact arbitrarily with the two expressions. If eI behaves in a \nway that its spec, eS, does not, it should be possible to .nd a context C whose main thread returns a \nnumber with eI that, with eS, it does not. The only practical way to prove re.nement is to .nd some other, \nmore structured way of characterizing observable behavior one that does not require detailed reasoning \nabout particular contexts. In a high-level language, FCDs hide their internal state within the functions \nthey export (their methods ), thereby greatly limiting the scope of interaction they have with their \nclients. In particular, a client context can neither observe nor alter the internal state directly; all \ninteractions are mediated by the methods. From the perspective of an FCD, then, the behavior of a client \ncan be reduced to a collection of possibly-concurrent method invocations. And from the perspective of \na client, the behavior of an FCD can be reduced to the answers it returns from those invocations. How \ncan we prove that an arbitrary collection of concurrent method calls to an FCD will yield the same answers \nas its spec? Protocols are the key: they capture the effect methods can have on the internal state and \nthereby the effect they have on each other abstractly, i.e., without reference to FCD code. Protocols \nenable us to reason locally, about one method invocation at a time. Instead of considering an arbitrary \nsequence of prior method invocations, we simply start from an arbitrary protocol state. And instead of \nconsidering arbitrary concurrent invocations, we simply force our local reasoning to withstand arbitrary \nrely moves in a protocol. P . . ::= | | ::= ::= v = v | emp | v .I u | v .S u | i >S e | P * P P . P \n| P . P | P . P | .x.P | .x.P P . P | . | . | t P | T @m (x. P ) (P ) e (x. Q) | v V v : t | O f e E \ne : t (., I , s, A) where v I . ..S . Assert, s . ..S, A . ..A, A#..F (s) . ::= (S, A, , F ) where S, \nA sets, . S \u00d7 S, F . S . P(A) m ::= i | none Figure 5. Syntax of assertions  3.2 Assertions In program \nlogics for .rst-order languages, there is a strict separation between assertions about data (e.g., heap \nassertions) and assertions about code (e.g., Hoare triples). But the distinction makes less sense for \nhigher-order languages, where code is data and hence claims about data must include claims about code. \nOur logic is therefore built around a single notion of assertion, P , shown in Figure 5, that plays several \ndisparate roles. Assertions are best understood one role at a time. The .rst role they play is similar \nto that of heap assertions in separation logic: they capture knowledge about a part of the (implementation \ns) heap, e.g., x .I 0, and support the composition of such knowledge, e.g., x .I 0*y .I 1. In this capacity, \nassertions make claims contingent on the current state, which may be invalidated in a later state. On \nthe other hand, some assertions are pure, meaning that if they hold in a given state, they will hold \nin any possible future state. The syntactic subclass of code assertions . all have this property, and \nthey include Hoare triples (P ) e (x. Q). The Hoare triple says: for any future state satisfying P , \nif the (implementation) expression e is executed until it terminates with a result, the .nal state will \nsatisfy Q (where x is the value e returned). So, for example, (emp) new 0 (x. x .I 0) is a valid assertion, \ni.e., it holds in any state. More generally, the usual rules of separation logic apply, including the \nframe rule, the rule of consequence and sequencing. The sequencing rule works, even in our concurrent \nsetting, because heap assertions describe a portion of heap that is privately owned by the expression \nin the Hoare triple. In particular, that portion of the heap is guaranteed to be neither observed nor \naltered by threads concurrent with the expression. The next role assertions play is expressing knowledge \nabout shared resources. All shared resources are governed by a protocol. For hidden state, the protocol \ncan be chosen freely, modulo proving that exported methods actually follow it. For visible state, however, \ne.g., a reference that is returned directly to the context, the protocol is forced to be a trivial one \nroughly, one that allows the state to take on any well-typed value at any time, accounting for the arbitrary \ninterference an unknown context could cause (see Section 3.4). Claims about shared resources are made \nthrough island asser\u00adtions . (inspired by LADR [10]), which .rst of all assert the existence of said \nresources. We call each shared collection of resources an island, because each collection is disjoint \nfrom the others and is governed by an independent protocol. An island assertion gives the protocol governing \nits resources: The component . = (S, A, , F ) formalizes the STS for the protocol, where S is its set \nof states, A is its set of possible tokens, is its transition relation, and F is a function telling which \ntokens are free at each state. (We will use dot notation like ..S to project named components from compound \nobjects.)  The component I tells how each state of the STS is interpreted as an assertion characterizing \nthe concrete, hidden resources that are actually owned by the island in that state.   Domains Island \nand world operations StateSet { S . Heap \u00d7 ThreadPool | S .nite, nonempty } |(., J, s, A)| (., J, s, \n\u00d8) |(k, .)| (k, .i.|.(i)|) { . . Heap \u00d7 StateSet } k < n, . . N  v{ Resource frame(., J, s, A) (., \nJ, s, ..A - ..F (s) - A) . . STS, s . ..S, A . ..A, A#..F (s), Islandn (., J, s, A) frame(k, .) (k, \n.i.frame(.(i))) mon J . ..S . UWorldn . P(Resource) ( .n l Islandk q(., J, s0, A)hk (., .s.I(s) r UWorldk, \ns0, A) Worldn W = (k, .)t(k + 1, .) (k, .i.q.(i)hk) UWorldn VReln { U . Worldn | U = |U| } mon V . UWorldn \n. P(Val \u00d7 Val) ( interp(., J, s, A) J(s) Composition State sets S1 . S2 { h1 1 h2; T1 1 T2 | hi; Ti \n. Si } when all compositions are de.ned Resources (h1, S1) . (h2, S2) (h1 1 h2, S1 . S2) ' ' Islands \n(., J, s, A) . (. ' , J , s ' , A ' ) (., J, s, A 1 A ' ) when . = . ' , s = s ' , J = J Worlds (k, .) \n. (k ' , . ' ) (k, .i..(i) . . ' (i)) when k = k ' , dom(.) = dom(. ' ) Protocol conformance guar guar \n . f (s, A) (s ' , A ' ) s . s ' , ..F (s) 1 A = ..F (s ' ) 1 A ' (k, .) r (k ' , . ' ) k = k ' , .i \n. dom(.). q.(i)hk/ r . ' (i) guar rely guar ' ' (., J, s, A) r (. ' , J , s ' , A ' ) . = . ' , J = \nJ ' , . f (s, A) * (s , A ' ) W r W ' frame(W ) r frame(W ' ) World satisfaction: . : W, . ' W.k > 0 \n=. . = . ' . .i, .i . dom(W..). .i . interp(W..(i))(t|W |) Figure 6. Semantic structures and operations \non them In addition, island assertions express knowledge about the state of the protocol (the component \ns) and any privately-owned tokens (the component A). This last bit of knowledge gives a lower bound on \nthe actual state of the protocol, which may in fact by in any rely\u00adfuture state of s, i.e., any state \nthat can be reached from s by the environment without using the privately-owned tokens A. Finally, assertions \nplay two re.nement-related roles. The .rst is to express re.nement itself, either between two closed \nvalues (vI V vS : t) or between open expressions (O f eI E eS : t ) the syntactic counterpart to semantic \nre.nement (Section 3.1). Until this point, we have avoided saying anything about spec terms, but in order \nto prove re.nement we need to show that the observable behavior of an implementation can be mimicked \nby its spec. This brings us to an essential idea: E eI eS : t (roughly!) o .j. (j >S eS) eI xI. .xS. \nxI V xS : t . j >S xS By treating spec code as a resource, we can reduce re.nement reasoning to Hoare-style \nreasoning. Thus, the .nal role assertions play is to express knowledge about and ownership of spec resources, \nwhich include portions both of the heap (e.g., x .S 0) and of the threadpool (e.g., j >S eS). These resources \ncan be shared and hence governed by protocols, just as implementation\u00adside resources can. When proving \nre.nement for an FCD, we will prove something like the above Hoare triple for an arbitrary application \nof each of its methods usually in the scope of an island assertion giving the protocol for its shared, \nhidden state. For each method invocation, we start from an arbitrary state of that protocol, and are \ngiven ownership of the spec code corresponding to the invocation, which we may choose to transfer to \nthe protocol to support cooperation (as explained in Section 2.5). But in the end, when the implementation \ns invocation has .nished and returned a value xI, we must have regained exclusive control over its spec, \nwhich must have mimicked it by producing a value xS that xI re.nes. The remaining forms of assertions \ninclude standard logical connectives, and two more technical forms of assertions CP and T @m (x. P ) \nwhich we explain in the Section 3.4.  3.3 Semantic structures The semantics of assertions is given using \ntwo judgments, one for general assertions (W, . |= . P ) and the other for code assertions (U |= . .), \nwhere P and . contain no free term variables but may contain free type variables bound by .. To explain \nthese judgments, we begin with the semantic structures of worlds W , resources . and environments ., \ntogether with operations on them needed to interpret assertions all de.ned in Figure 6. Resources The \nresources . = (h, S) that assertions claim knowl\u00adedge about and ownership of include both implementation \nheaps h, and speculative sets S of spec con.gurations. (Recall that a con.gu\u00adration . = h; T consists \nof a heap and a threadpool.) Resources can be combined at every level, which is necessary for interpreting \nthe * operator on assertions. For heaps and threadpools, composition is done via l, the usual disjoint \nunion. The composition of state sets is just the set of state compositions but it is only de.ned when \nall such state compositions are de.ned, so that speculative sets S have a single footprint consisting \nof all the locations/threads ex\u00adisting in any speculative state. To ensure that this footprint is .nite, \nwe require that speculation is itself .nite. Finally, composition of resources is the composition of \ntheir parts. Islands and possible worlds All assertions are interpreted in the context of some possible \nworld W , which contains a collection . of islands this is the Kripke in Kripke logical relations. Semantic \nislands look very much like syntactic island assertions: the only difference is that STS states are interpreted \nsemantically via J, rather than syntactically via I. Unfortunately, this creates a circularity: J is \nmeant to interpret its syntactic counterpart I, and since assertions are interpreted in the contexts \nof worlds, the interpretation must be relative to the current world but we are in the middle of de.ning \nworlds! The step index k in worlds is used to stratify away circularities in the de.nition of worlds \nand the logical relation; it and its attendant operators C and L-Jk are completely standard, and so for \nspace reasons we direct the interested reader to earlier work for a detailed explanation [9, 10]. The \nless interested reader should simply ignore step indices from here on.  There is one additional subtlety, \nhowever: it is crucial that all participants in a protocol agree on the protocol s interpretation of \na state, which must therefore be insensitive to which tokens a particular participant owns. We guarantee \nthis by giving the interpretation J access to only the unprivileged part of a participant s world, |W \n|, which has been stripped of any tokens; see the constraint on the type of J. To determine the meaning \nof assertions like . * . ', we must allow islands to be composed. Semantic island composition . is de.ned \nonly when the islands agree on all aspects of the protocol, including its state; their owned tokens are \nthen (disjointly) combined. Note, however, that because island assertions are rely-closed, an assertion \nlike . * . ' does not require . and . ' to assert the same state. It merely requires that there is some \nstate that is in both of their rely-futures. Worlds are composable only when they de.ne the same islands \nand those islands are composable. Environments Type variables are interpreted by an environment . that \nmaps them to relations V . VRel. This interpretation of types captures the usual relational parametricity \n[32], in which the interpretation of an abstract type may relate values of potentially different types \non the the implementation and speci.cation sides. Protocol conformance The judgment . f (s, A) (s ' , \nA ' ) cod\u00adi.es the law of conservation of tokens (Section 2.4) for a single step.7 We use this judgment \nin de.ning a guarantee relation govern\u00ading the changes an expression can make to an island s state, given \nthe tokens it owns. An expression can likewise rely on its environ\u00adment to only change the island . according \nto the tokens that the environment owns, i.e., the tokens owned by frame(.). The rely and guarantee views \nof a protocol give rise to two notions of future worlds. In both cases, the world may grow to include \nnew islands, but any existing islands are constrained by their rely and guarantee relations, respectively. \nThe guarantee relation on islands may include changes to both the state of the STS and the privately-owned \ntokens. The rely relation only allows the state to change, since only the (implicit) environmental participant \nmaking the move may gain or lose tokens. Island interpretations J are required to be monotone with respect \nto the rely relation on worlds (written mon .), which ensures that making a rely move in one island cannot \npossibly invalidate the interpretation of another. World satisfaction Worlds describe shared state abstractly, \nin terms of protocol states. Expressions, on the other hand, are executed against some concrete resources. \nThe world satisfaction relation . : W, . ' de.nes when a given collection of concrete resources . satis.es \na world, meaning that it breaks into a disjoint portion for each island, with each portion satisfying \nits island s current interpretation. The parameter . ' represents additional resources that are private, \nand therefore disjoint from those governed by the world, which is convenient for de.ning the semantics \nof assertions below.  3.4 Semantics The semantics of assertions given in Figure 7 satis.es a fundamental \nrely . ' ' . property: if W, . |= P and W . W then W , . |= P . All assertions are therefore stable under \narbitrary interference from other threads. This should not be a surprise: assertions are either statements \nabout private resources (for which interference is impossible) or about shared islands (for which interference \nis assumed, e.g., we are careful to only assert lower bounds on the state of an island). The only subtlety \nis in the semantics of implication, which must be explicitly rely-closed to ensure stability. The semantics \nof the basic assertions about private resources and 7 We use the more readable notation s . s ' in place \nof .. (s, s ' ). islands are entirely straightforward, as are those for the basic logical connectives \n(we omit =, . and .). The value re.nement assertion v1 V v2 : t requires that any observations a context \ncan make of v1 at type t can also be made of v2. Those readers familiar with Kripke logical relations \nwill recognize it as essentially the standard de.nition of logical approximation between values. Base \ntype values must be identical. For function types, we check that the bodies of the functions are related \nwhen given related arguments, which, due to the semantics of implication, might happen in a rely-future \nworld. For recursive types, we check that the values are related at the unfolded type, which is well-founded \ndue to the productivity requirement on recursive types (and our strategic uses of C in type constructors). \nValues that are exposed to the context at heap-allocated type ref and sum types are forced to be governed \nby a trivial island allowing all type-safe updates (in the case of ref s) and no updates (in the case \nof sums). Hidden state, on the other hand, is by de.nition state that does not escape directly to the \ncontext, and so we need say nothing about it for value re.nement. The fact that re.nement is a pure assertion \n(insensitive to the state of private resources or the ownership of private tokens) is essential for soundness, \nfor a simple reason: once a value has reached the context, it can be copied and used concurrently. We \ntherefore cannot claim that any one copy of the value privately owns some resources. Note that, if P \nis impure, we use U |= . P as shorthand for ... U, . |= . P . For expression re.nement O f e1 E e2 : \nt , we .rst close off any term or type variables bound by O with the appropriate universal quanti.cation. \nClosed expression re.nement is de.ned in terms of a Hoare triple, almost in the way we suggested in Section \n3.2. The main difference in the actual de.nition is that we additionally quantify over the unknown evaluation \ncontext K in which a speci.cation is running; this annoyance appears to be necessary for proving that \nre.nement is a precongruence. Hoare triples are de.ned via the threadpool simulation assertion T @m (x. \nP ), which is the engine that powers our model. Thread\u00adpool simulation accounts for the fact that an \nexpression can fork threads as it executes, but that we care about the return value only from the initial \nthread, m. To satisfy T @m (x. P ) at some W and ., the threads in T must .rst of all continuously obey \nthe protocols of W , given private ownership of .. That is, every atomic step taken by a thread must \ntransform its shared resources in a way that corre\u00adsponds to a guarantee move in the protocol, and it \nmust preserve as a frame any private resources of its environment, but it may change private resources \n. in any way it likes. In between each such atomic step, the context might get a chance to run, which \nwe model by quantifying over an arbitrary rely-future world. If at any point the main thread m terminates, \nit must do so in a state satisfying P , where x is bound to the value the main thread returned. Afterward, \nany lingering threads are still required to obey the protocol. That threadpool simulation is, in fact, \na simulation is due to its use of the speculative stepping relation S = S ', which requires any changes \nto the spec state to represent feasible execution steps: every new state must be reachable from some \nold state, but we are free to introduce multiple new states originating in the same old state, and we \nare free to drop irrelevant old states on the .oor. As a result of how simulation is de.ned, such changes \nto the spec state can only be made to those pieces that are under the threadpool s control, either as \npart of its private resources (allowing arbitrary feasible updates) or its shared ones (allowing only \nprotocol-permitted updates).  3.5 Soundness for re.nement Our key theorem is: Theorem 1 (Soundness). \nIf U |= \u00d8 O f e1 E e2 : t for all U then O |= e1 e2 : t .  Private and shared-state assertions: W, . \n|=. R iff Value re.nement: U |=. v1 V v2 : t0 iff R . emp v .I u v .S u i >S e P . Q P . Q .x.P tP P1 \n* P2 P1 . P2 (., I , s, A) Requirements t0 |W | |=. . tb W = |W |, . = (\u00d8, {\u00d8; \u00d8}) a ' . = ([v . u], \n{\u00d8; \u00d8}) t . t . = (\u00d8, {[v . u]; \u00d8}) .a.t . = (\u00d8, {\u00d8; [i . e]}) \u00b5a.t W, . |=. P and W, . |=. Q ref?(t) \nrely ' ref(t) .W ; W. W ' , . |=. P =. W ' , . |=. Q t1 + t2 .v. W, . |=. P [v/x] W.k > 0==. P Requirements \nv1 = v2, f vi : tb for tb . {1, B, N} (v1, v2) . .(a)(U) E ' vi = rec f(x).ei, U |=. t(x : t f e1[v1/f] \ne2[v2/f] : t ) . E vi = ..ei, U |=t(a f e1 e2 : t) . V U |=v1 v2 : t[\u00b5a.t /a] . V V U |=v1 v2 : 1 . v1 \nv2 : ref(t) V U |=. inv(.x, y.x y : t . v1 .I (x) * v2 .S (y)) V .i. U |=. .x, y. x y : ti . inv(v1 \n.I inji x * v2 .S inji y) . tW, . |E Expression re.nement: U |=. O f e1 e2 : t iff W = W1 . W2, . = \n.1 . .2, Wi, .i |=. Pi O ..S = S1 . S2, W, (..h, Si) |=. Pi \u00b7 rely ' .i. W ; (W.k, [i . (., [I], s, \nA)]) x:t , O ' where [I] .s..U.{. | U, . |=. I(s)} a, O ' Requirements ( V .K, j. U |=. (j >S K[e2]) \ne1 x. .y. x y : t . j >S K[y] . V' E .v1, v2. U |=v1 v2 : t . O ' f e1[v1/x] e2[v2/x] : t .V . U |=.[a \n.V ] O ' f e1 E e2 : t Hoare triples: Invariant protocols: inv(P ) (({1}, \u00d8, \u00d8, . .\u00d8), . .P, 1, \u00d8) ' \n' U |=. (P ) e (x. Q) .i. U |=. P . [i . e]@i (x. Q) Spec stepping: S = S ' .. . S ' . .. . S. . .* \n. Threadpool simulation: rely W0, . |=. T @m (x. Q) .W ; W0, .F #.. if W.k > 0 and h, S : W, . . .F then: \nguar '' ' if h; T . h ' ; T then .S ' , . ' , W ; W. S = S ' , h ' , S ' : W ' , . ' . .F , W ' .k = \nW.k - 1, W ' , . ' |=. T @m (x. Q) guar ' if T = T0 1 [m . v] then .S ' , . ' , W ; W. S = S ' , h, \nS ' : W ' , . ' . .F , W ' .k = W.k, W ' , . ' |=. Q[v/x] * T0@none (x. tt) Figure 7. The semantics of \nassertions The proof of this theorem, given in the appendix [35], is built on novel lemmas expressing \nkey framing properties for the threadpool simulation assertion, the most important being: o Lemma 1 (Framing). \nIf W1, .1 |= . T1@m1 x. Q1 and o W2, .2 |= . T2@m2 x. Q2 with m1 = none or m1 . dom(T1), o then W1 . \nW2, .1 . .2 |= . T1 l T2@m1 x. Q1 . These lemmas allow us to prove that re.nement assertions are con\u00adgruent \n(i.e., they compose), which is the dif.cult part of soundness. We can also derive the following inference \nrules for valid, pure assertions (true at every world): ((( ' ' ' '' (P ) e x. P .x.P e y. P (P ) e (x. \nQ) ( ' '' (P ) let x = e in e y. P (P * R) e (x. Q * R) (( ' ' P . P P e x. Q ' Q ' . Q tP . P (P ) e \n(x. Q) P The .rst three of these inference rules are the expected ones for a separation logic (our assertions \nalso model intuitionistic BI). The last rule is the L \u00a8 ob rule, which allows us to reason about recursive \nfunctions by assuming their speci.cation holds one step later [10]. In giving proof outlines in the next \nsection, we make implicit use of these rules, and also drop the variable binding in the postcondition \nwhen it is irrelevant. 4. Case study: conditional CAS With the details of our model in hand, we are now \nin a position to tackle, in detail, a rather complex FCD: Harris et al. s conditional CAS [18, 15], which \nperforms a compare-and-set on one word of memory, but only succeeds when some other word (the control \n.ag) is non-zero at the same instant. This data structure is the workhorse that enables Harris et al. \nto build their remarkable lock-free multi\u00adword CAS from single-word CAS. As with the Michael-Scott queue, \nwe have boiled down condi\u00adtional CAS to its essence, retaining its key veri.cation challenges while removing \nextraneous detail. Thus, we study lock-free condi\u00adtional increment on a counter, with a .xed control \n.ag per instance of the counter; see the speci.cation counterS in Figure 8. These sim\u00adpli.cations eliminate \nthe need to track administrative information about the operation we are trying to perform but do not \nchange the algorithm itself, so adapting our proof of conditional increment to full CCAS is a straightforward \nexercise. 4.1 The protocol To explain our implementation, counterI, we begin with its rep\u00adresentation \nand the protocol that governs it. The control .ag f is represented using a simple boolean reference; \nall of the action is in the counter c, which has type ref(N + N). A value inj1 n rep\u00adresents an inactive \ncounter with logical value n. A value inj2 n, in contrast, means that the counter is undergoing a conditional \nin\u00adcrement, and had the logical value n when the increment began. Because inj2 n records the original \nvalue, a concurrent thread at\u00adtempting another operation on the data structure can help .nish the in-progress \nincrement. This helping is actually not so sel.ess: really, one thread is just helping another thread \nget out of its way. The question is how to perform a conditional increment without using any locks. Remarkably, \nthe algorithm simply reads the .ag f, and then in a separate step updates the counter c with a CAS; see \nthe complete function. It is possible, therefore, for one thread performing a conditional increment to \nread f as true, at which point another thread sets f to false; the original thread then proceeds with \nincrementing the counter, even though the control .ag is false! Proving that counterI re.nes counterS \ndespite this blatant race  counterS let c = new 0, f = new false, lock = new false let setFlag(b) = \nsync(lock) { f := b } let get() = sync(lock) { c[1] } let cinc() = sync(lock) { c[1] := c[1] + if f[1] \nthen 1 else 0 } in (get, setFlag, cinc) counterI let c = new inj1 0, f = new false let setFlag(b) = \nf := b let complete(x, n) = if f[1] then CAS(c, x, inj1 (n + 1)) else CAS(c, x, inj1 n) let rec get() \n= let x = c[1] in case x of inj1 n . n | inj2 n . complete(o, n); get() let rec cinc() = let x = c[1] \nin case x of inj1 n . let y = inj2 n in if CAS(c, x, y) then complete(y, n); () else cinc() | inj2 n \n. complete(x, n); cinc() in (get, setFlag, cinc) Upd(d, \u00d8); . Gone; d Upd(d, {1}); . .;  n Dead; \nLoc .n d ::= n, j, K B . {0, 1} A Loc S l S0 S0 {., Upd(d, B), Done(d), Gone, Const(n), Dead} I(s) .b \n: B. fI .I b * fS .S b * lock .S false * .!Jc. s(J) . {Const(-), Upd(-, -)} v linkUpd(Jc, n, j, K, B) \ns(Jc) = Upd(n, j, K, B) * linkConst(Jc, n) s(Jc) = Const(n) * J .I inj2 n * j >S K[()] *s(\u00a3)=Done(n,j,K) \n* J .I inj2 - * J .I inj1 - *s(\u00a3)=Gone *s(\u00a3)=Dead linkConst(Jc, n) cI .I Jc * Jc .I inj1 n * cS .S n \nlinkUpd(Jc, n, j, K, B) cI .I Jc * Jc .I inj2 n . . cS .S n * j >S K[cinc()] . . * .. cS .S n * j >S \nK[()] if 0 . B. . cS .S (n + 1) * j >S K[()] if 1 . B Figure 8. Conditional increment, a simpli.cation \nof CCAS condition will require all the features of our model, working in concert. An initial idea is \nthat when the physical value of the counter is inj2 n, its logical value is ambiguous: it is either n \nor n + 1. This idea will only work if we can associate such logical values with feasible executions of \nthe spec s cinc code, since logical value really means the spec s value. The dif.culty is in choosing \nwhen to take spec steps. If we wait to execute the spec code until a successful CAS in complete, we may \nbe too late: as the interleaving above shows, the .ag may have changed by then. But we cannot execute \nthe spec when we read the .ag, either: the CAS that follows it may fail, in which case some other thread \nmust have executed the spec. The way out of this conundrum is for threads to interact via a speculative \nprotocol, shown in Figure 8. Recall that injections into sum types are heap-allocated, so every value \nc takes on has ( let complete(x, n) = x . Upd(n, j, K, \u00d8) ( if f[1] then x . Upd(n, j, K, {1}) ( CAS(c, \nx, inj1 (n + 1)) x . Done(n, j, K) ( else x . Upd(n, j, K, {0}) ( CAS(c, x, inj1 n) x . Done(n, j, K) \n( let rec cinc() = j >S K[cincS()] * (., I, \u00d8, \u00d8) ( >S K[cincS()] * j let x = c[1] in x . Const(-) . \nx . Upd(-, -) case x of ( inj1 n . j >S K[cincS()] * x . Const(n) ( j >S K[cincS()] * x . Const(n) * \nlet y = inj2 n in y . inj2 n ( if CAS(c, x, y) then x . Dead(n) . y . Upd(n, j, K, \u00d8) . ( y . Upd(n, \nj, K, \u00d8) complete(y, n); y . Done(n, j, K) () ret. ret = () . j >S K[()] . y . Gone ( . ret. ret = () \n. j >S K[()] ( else j >S K[cincS()] * (., I, \u00d8, \u00d8) ( cinc() ret. ret = () . j >S K[()] | inj2 n . j >S \nK[cincS()] * x . Upd(n, -, -, -) complete(x, n); j >S K[cincS()] * x . Done(n, -, -) ( . (j >S K[cincS()] \n* (., I, \u00d8, \u00d8) cinc() ret. ret = () . j >S K[()] Figure 9. Proof outline for conditional increment an \nidentity: its location. The protocol gives the life story for every possible location in the heap as \na potential value of c, with the usual constraint that all but .nitely many locations are in the unborn \n(.) state. The .rst step of the protocol re.ects the choice latent in the sum type: either this location \nis a quiescent inj1 n (represented initially by Const(n)) or an active increment operation inj2 n (represented \ninitially by Upd(d, \u00d8)). The logical descriptor d gives the old value n of the counter, together with \nthe thread id j and speci.cation evaluation context of the thread attempting the increment. The latter \ninformation is necessary because thread j temporarily donates its spec to the protocol, permitting helping \nthreads to execute the spec on its behalf. Following the pattern laid out in Section 2.5, in return for \ndonating its spec, thread j receives a token call it Attempt which will later permit it, and only it, \nto recover its spec. As usual, we depict the token with a bullet. The life story for a quiescent inj1 \nn is quite mundane: either it is the current value pointed to by c, or it is Dead. An active cell inj2 \nn leads a much more exciting life. In the .rst phase of life, Upd(d, B), the cell records which branches \nB . {0, 1} of the complete code have been entered by a thread. Initially, no thread has executed complete, \nso the set is empty. If a thread subsequently reads that f = true in the .rst step of executing complete, \nit moves to the set {1}, since it is now committed to the branch that adds 1 to the initial value n. \nCrucially, this step coincides with a speculative run of the speci.cation; the un-run spec is also retained, \nin case some other thread commits to the 0 branch. The branch-accumulation process continues until some \nthread (perhaps not the original instigator of the increment) actually succeeds in performing its CAS \nin complete. At that point, the increment is Done, and its inj2 n cell is effectively dead, but not yet \nGone: in the end, the thread that instigated the original increment reclaims its spec, whose execution \nis guaranteed to be .nished.  4.2 The proof We now formally justify that counterI re.nes counterS by \ngiving a concrete interpretation to the protocol and providing a Hoare-style proof outline for complete \nand cinc. The outline for get is then a straightforward exercise.  To formalize the protocol, we .rst \ngive the set of states S0 for an individual life story; see Figure 8. The states S for the data structure \nare then a product of individual STS states indexed by location, with all but .nitely many locations \nrequired to be in state .. The set of tokens A for the product STS is just the set of locations, i.e., \nthere is one token per location (and hence per individual life story). The transition relation on the \nproduct STS lifts the one for individual life stories: s s ' .e. s(e) = s ' (e) . s(e) s ' (e). If F0 \nis the free-token function for an individual STS, we can then de.ne the product STS as follows: . (S, \nA, , .s.{e | F0(s(e)) = {Attempt}}) The interpretation I for states of the product STS given in Figure \n8 is fairly straightforward. The implementation and speci.cation .ag values must always match. There \nmust exist a unique location ec ( .!ec ) in a live state of Const or Upd. This unique live location will \nbe the one currently pointed to by c. In the Upd state, it also owns speculative spec resources according \nto the branch set B. Finally, Done nodes retain a .nished spec, while Dead and Gone nodes are simply \ngarbage inj1 (-) and inj2 (-) nodes, respectively. To show the re.nement counterI E counterS : t, where \nt = (1 . N \u00d7 B . 1 \u00d7 1 . 1), it suf.ces to show the following Hoare triple for every j, K : V (j >S \nK[counterS]) counterIzI. .zS. zI zS : t . j >S K[zS] The execution of counterI is short and simple: it \nallocates the hidden state of the data structure, and then immediately returns three procedures for manipulating \nthat state. In the proof of the triple, after the hidden state is allocated, we construct an island to \ngovern it and add the island to the world (a guarantee extension). The new island is described by the \nassertion .e. (., I , [e . Const(0)], \u00d8), which says that it follows the conditional increment protocol \n(. and I), is in some rely-future state of [e . Const(0)] (in which every location other than e is unborn), \nand currently owns no tokens. Adding this island requires us to show that the initial values of the hidden \nstate in the implementation and speci.cation satisfy the invariant at this state, which they clearly \ndo. We must then show, in the context of this extended world, that each of the implementation procedures \nre.nes the corresponding speci.cation procedure; we give the detailed proof for cinc, i.e., (j >S K[cincS()] \n* (., I , \u00d8, \u00d8)) cincI() (ret. ret = () . j >S K[()]) In the precondition, we weaken our knowledge about \nthe island to simply saying that it is in a rely-future state of \u00d8 (where every location maps to .), \nsince this is all we need to know. The locality of the local life stories is manifested in our ability \nto make isolated, abstract assertions about a particular location governed by the data structure. Because \nevery location is in some rely-future state of ., we can focus on a location x of interest by asserting \nthat the product STS is in a rely-future state of [x . s0], where s0 . S0. For readability, we employ \nthe following shorthand for making such local assertions about the island: x . s0 (., I , [x . s0], \u00d8) \nx . s0 (., I , [x . s0], {x}) Thus empowered, we can glean some additional insight about the algorithm: \nthat the complete function satis.es the triple (x . Upd(n, j, K, \u00d8)) complete(x, n) (ret. x . Done(n, \nj, K )) In reading this triple, it is crucial to remember that assertions are closed under rely moves \nso x . Upd(n, j, K, \u00d8) means that the location x was once a live, in-progress update. The interesting \nthing about the triple is that, regardless of the exact initial state of x, on exit we know that x is \nat least Done and there s no going back. The proof outline for complete at the top of Figure 9 states \nthat, after reading the value of the .ag, the location x is in an appropriately speculative state. To \nprove that fact, we must consider the rely-future states of Upd(n, j, K, \u00d8), and show that for each such \nstate we can reach (via a guarantee move) a rely-future state of Upd(n, j, K, {1}) or Upd(n, j, K, {0}), \ndepending on the value read. For example, if the initial state is s0 and we read that the .ag is true, \nwe take a guarantee move to s0 ' as follows: ' ' If s0 is then s is If s0 is then s is 0 0 Upd(d, \u00d8) \nUpd(d, {1}) Upd(d, {1}) Upd(d, {1}) Upd(d, {0}) Upd(d, {0, 1}) Upd(d, {0, 1}) Upd(d, {0, 1}) Done(d) \nDone(d) Gone Gone If the initial state already included the needed speculation (or was Done or Gone), \nthere is nothing to show; otherwise, changing the state requires speculative execution of the spec. We \nperform a similar case analysis at the CAS step, but there we start with the knowledge that the appropriate \nspeculation has already been performed which is exactly what we need if the CAS succeeds. If, on the \nother hand, the CAS fails, it must be the case that x is at least Done: if it were still in an Upd state, \nthe CAS would have succeeded. With complete out of the way, the proof of cinc is relatively easy; see \nthe bottom of Figure 9.8 When entering the procedure, all that is known is that the island exists, and \nthat the speci.cation is owned. The thread .rst examines c to see if the counter is quiescent, which \nis the interesting case. If the subsequent CAS succeeds in installing an active descriptor inj2 n, that \ndescriptor is the new live node (in state Upd(n, j, K, \u00d8)) and the thread, being responsible for this \ntransition, gains ownership of the descriptor s token. The resulting assertion y . Upd(n, j, K, \u00d8) is \nequivalent to y . Upd(n, j, K, \u00d8) * y . Upd(n, j, K, \u00d8) which means that we can use y . Upd(n, j, K, \n\u00d8) as a frame in an application of the frame rule to the triple for complete(y, n). This gives us the \nframed postcondition y . Done(n, j, K ) * y . Upd(n, j, K, \u00d8) which is equivalent to y . Done(n, j, K \n). Since our thread still owns the token, we know the state is exactly Done(n, j, K ), and in the next \nstep (where we return the requisite unit value) we trade the token in return for our spec which some \nthread has executed. 5. Discussion and related work We have presented a model for a high-level language \nwith concur\u00adrency that enables direct re.nement proofs for sophisticated FCDs, via a notion of local \nprotocol that encompasses the fundamental phenomena of role-playing, cooperation, and nondeterminism. \nIn this section, we survey the most closely related work along each of these axes. High-level language \nBirkedal et al. [5] recently developed the .rst logical-relations model for a higher-order concurrent \nlanguage similar to the one we consider here. Their aim was to show the soundness of a sophisticated \nLucassen-and-Gifford-style [27] type\u00adand-effect system, and in particular to prove the soundness of a \nParallelization Theorem for disjoint concurrency expressed by the effect system (when the Bernstein conditions \nare satis.ed). The worlds used in the logical relation capture the all-or-nothing approach to interference \nimplied by the type-and-effect system. As a result, the model has rather limited support for reasoning \nabout FCDs: it can only prove correctness of algorithms that can withstand arbitrary interference. We \nare unaware of any other proof methods that handle higher\u00adorder languages, shared-state concurrency, \nand local state. Direct re.nement proofs Herlihy and Wing s seminal notion of linearizability [21] has \nlong been the gold standard of correctness for FCDs, but as Filipovic\u00b4et al. argue [14], what clients \nof an FCD really want is a contextual re.nement property. Filipovi c\u00b4et al. go on 8 The steps labeled \nwith . indicate uses of the rule of consequence.  to show that, under certain (strong) assumptions about \na program\u00adming language, linearizability implies contextual re.nement for that language. More recently, \nGotsman and Yang generalized both lin\u00adearizability and this result (the so-called abstraction theorem) \nto include potential ownership transfer of memory between FCDs and their clients [16]. While it is possible \nto compose this abstraction theorem with a proof of linearizability to prove re.nement, there are several \nadvantages to our approach of proving re.nement di\u00adrectly. First of all, it allows us to treat re.nement \nas an assertion in our logic, which means that we can compose proofs of re.nement when reasoning about \ncompound FCDs, and do so while working within a single logic. Second, it allows us to leverage recent \nwork for reasoning about re.nement and hidden state, e.g., Dreyer et al. s STS-based logical relations \n[9]. Third, it allows us to reason about FCDs that use higher-order features, e.g., the universal FCD \ncon\u00adstruction given in [20], which would otherwise require extending the de.nition of linearizability \nto the higher-order case. Finally, it allows us to seamlessly combine reasoning about .ne-grained concurrency \nwith other kinds of reasoning, e.g., relational parametricity [32]. Turon and Wand developed the .rst \nlogic for reasoning directly about contextual re.nement for FCDs [36]. Their model is based on ideas \nfrom rely-guarantee and separation logic and was developed for a simple .rst-order language, using an \nextension of Brookes s trace-based denotational semantics [6]. While it is capable of proving re.nement \nfor simple FCDs, such as Treiber s stack, it does not easily scale to more sophisticated algorithms. \nMore recently, Liang et al. proposed RGSim [25], a composi\u00adtional inter-language simulation relation \nbased on rely-guarantee for verifying program transformations in a concurrent setting. Liang et al. also \nuse their method to prove that some simple, but realis\u00adtic, FCDs are simulated by their spec. While the \noriginal paper on RGSim did not relate simulation to re.nement or linearizability, new (currently unpublished) \nwork has done so [24]. Local protocols O Hearn et al. s work on Linearizability with hindsight [30] clearly \narticulates the need for local protocols in reasoning about FCDs, and demonstrates how a certain mixture \nof local and global constraints leads to insightful proofs about lock\u00adfree traversals. At the heart of \nthe work is the remarkable Hindsight Lemma, which justi.es conclusions about reachability in the past \nbased on information in the present. Since O Hearn et al. are focused on providing proofs for a particular \nclass of algorithms, they do not formalize a general notion of protocol, but instead focus on a collection \nof invariants speci.c to the traversals they study. We have focused, in contrast, on giving a simple \nbut general account of local protocols that suf.ces for temporally-local reasoning about a range of FCDs. \nIt remains to be seen, however, whether our techniques yield a satisfying temporally-local correctness \nproof for the kinds of traversals O Hearn et al. study, or whether (as O Hearn et al. argue) these traversals \nare best understood non-locally. The notion of protocol most closely related to ours is Dinsdale-Young \net al. s Concurrent abstract predicates (CAP) [7]. CAP extends separation logic with shared, hidden regions \nsimilar to our islands. These regions are governed by a set of abstract predicates, which can be used \nto make localized assertions about the state of the region. In addition, CAP provides a notion of named \nactions which characterize the possible changes to the region. Crucially, actions are treated as a kind \nof resource which can be gained, lost, or split up (in a fractional permissions style), and executing \nan action can result in a change to the available actions. It is incumbent upon users of the logic to \nshow that their abstract predicates and actions cohere, by showing that every abstract predicate is self-stable \n(remains true after any available action is executed). While CAP s notion of protocol is very expressive, \nit is also somewhat low-level compared to our STS-based protocols, which would require a somewhat unwieldy \nencoding to express in CAP. In addition, our protocols make a clear separation between knowledge bounding \nthe state of the protocol (treated as a copyable assertion) and rights to change the state (treated as \na linear resource: tokens), which are mixed in CAP. Another major difference is that CAP exposes the \ninternal protocol of a data structure as part of the speci.cation seen by a client which means that the \nspec for a given FCD often depends on how the client is envisioned to use it. Additional specs (and additional \ncorrectness proofs) may be necessary for other clients. By contrast, we take a coarse-grained data structure \nas an all-purpose spec; if clients then want to use that data structure according to some sophisticated \ninternal protocol, they are free to do so. Finally, our protocols support speculation and spec code as \na resource, neither of which are supported in CAP. Role-playing The classic treatment of role-playing \nin shared-state concurrency is Jones s rely-guarantee reasoning [23], in which threads guarantee to make \nonly certain updates, so long as they can rely on their environment to make only certain (possibly differ\u00adent) \nupdates. More recent work has combined rely-guarantee and separation logic (SAGL [13] and RGSep [38]), \nin some cases even supporting a frame rule over the rely and guarantee constraints them\u00adsevles (LRG [12]). \nThis line of work culminated in Dodds et al. s deny-guarantee reasoning [8] the precursor to CAP which \nwas designed to facilitate a more dynamic form of rely-guarantee to ac\u00adcount for non-well-bracketed thread \nlifetimes. In the deny-guarantee framework, actions are classi.ed into those that both a thread and its \nenvironment can perform, those that neither can perform, and those that only one or the other can perform. \nThe classi.cation of an action is manifested in terms of two-dimensional fractional permissions (the \ndimensions being deny and guarantee ), which can be split and combined dynamically. Our STSs express \ndynamic evolution of roles in an arguably more direct and visual way, through tokens. Cooperation Vafeiadis \ns thesis [37] set a high-water mark in veri\u00ad.cation of the most sophisticated FCDs (such as CCAS [18, \n15]). Building on his RGSep logic, Vafeiadis established an informal methodology for proving linearizability \nby employing several kinds of ghost state (including prophecy variables and one-shot re\u00adsources, the \nlatter representing linearization points). By cleverly storing and communicating this ghost state to \nanother thread, one can perform thread-local veri.cation and yet account for coopera\u00adtion: the other \nthread .res the single shot of the one-shot ghost resource. While this account of cooperation seems intuitively \nrea\u00adsonable, it lacks any formal metatheory justifying its use in lineariz\u00adability or re.nement proofs. \nOur computational resources generalize Vafeiadis s one-shot ghost state, since they can (and do) run \ncom\u00adputations for an arbitrary number of steps, and we have justi.ed their use in re.nement proofs showing, \nin fact, that the technique of logical relations can be expressed in a unary (Hoare logic) style by using \nthese computational resources. Concurrently with our work, Liang and Feng have extended their RGSim framework \nto account for cooperation [24]. The new simulation method is parameterized by a pending thread inference \nmap T, which plays a role somewhat akin to our worlds. For us, worlds impose a relation between the current \nprotocol state, the current implementation heap, and the current, speculative spec resources. By contrast, \nT imposes a relation between the current implementation heap and the current spec thread pool. To recover \nsomething like our protocols, one instead introduces ghost state into the implementation heap, much as \nVafeiadis does; as a result, T can be used to do thread-local reasoning about cooperative FCDs. However, \nthere are several important differences from our approach. First, there is no notion of composition on \nthread inference maps, which take the perspective of the global implementation heap and global pool of \nspec threads. Thus thread inference maps do not work as resources that can be owned, split up, transferred \nand recombined. Second, the assertions that are used in pre-and post-conditions cannot talk directly \nabout the thread inference map; they must control it indirectly, via ghost state. Third, the simulation \napproach does not support speculation or high-level language features like higher-order functions or \npolymorphism. Finally, it requires encoding protocols via traditional ghost state and rely/guarantee, \nrather than through standalone, visual protocols.  Groves and Colvin propose [17] a radically different \napproach for dealing with cooperation, based on Lipton s method of reduc\u00adtion [26]. Reduction, in a sense, \nundoes the effects of concurrency by showing that interleaved actions commute with one another much like \nlinearizability. Groves and Colvin are able to derive an elimination stack from its spec by a series \nof transformations, justi\u00adfying each by considering possible interleavings and proving, very roughly, \nthat the relevant actions commute. Elmas et al. also de\u00adveloped a method for proving linearizability \nusing reduction and abstraction [11], and while they do not study cooperation explicitly, it is likely \nthat their method can cope with it too. Nondeterminism Forward simulation is well-known to be sensi\u00adtive \nto differences in the timing of nondeterminism (also known as the branching structure of a transition \nsystem) [39]. On the other hand, forward simulation is appealingly local, since it con\u00adsiders only one \nstep of a program at a time (as opposed to e.g., trace semantics). To retain temporally-local reasoning \nbut permit differences in nondeterminism (as in the late/early choice example), it suf.ces to use a combination \nof forward and backward simula\u00adtion or, equivalently, history and prophecy variables [28, 1]. Lynch and \nVaandrager showed that there are also hybrids of forward and backward simulations, which relate a single \nstate in one system to a set of states in the other much like our speculation. Our technique goes further, \nthough, in combining this temporally-local form of reasoning with thread-local reasoning: hybrid simulations \nwork at the level of complete systems, whereas our threadpool simulations can be composed into larger \nthreadpool simulations. Composability allows us to combine thread-private uses of speculation with shared \nuses of speculation in protocols; moreover, it is crucial in showing soundness for contextual re.nement. \nReferences [1] M. Abadi and L. Lamport. The existence of re.nement mappings. Theoretical Computer Science, \n82(2):253 284, 1991. [2] A. Ahmed. Step-indexed syntactic logical relations for recursive and quanti.ed \ntypes. In ESOP, 2006. [3] A. Ahmed, D. Dreyer, and A. Rossberg. State-dependent representation independence. \nIn POPL, 2009. [4] L. Birkedal, B. Reus, J. Schwinghammer, K. St\u00f8vring, J. Thamsborg, and H. Yang. Step-indexed \nKripke models over recursive worlds. In POPL, 2011. [5] L. Birkedal, F. Sieczkowski, and J. Thamsborg. \nA concurrent logical relation. In CSL, Sept. 2012. [6] S. Brookes. Full abstraction for a shared variable \nparallel language. Information and Computation, 127(2):145 163, 1996. [7] T. Dinsdale-Young, M. Dodds, \nP. Gardner, M. Parkinson, and V. Vafeiadis. Concurrent abstract predicates. In ECOOP, June 2010. [8] \nM. Dodds, X. Feng, M. Parkinson, and V. Vafeiadis. Deny-guarantee reasoning. In ESOP, 2009. [9] D. Dreyer, \nG. Neis, and L. Birkedal. The impact of higher-order state and control effects on local relational reasoning. \nIn ICFP, 2010. [10] D. Dreyer, G. Neis, A. Rossberg, and L. Birkedal. A relational modal logic for higher-order \nstateful ADTs. In POPL, 2010. [11] T. Elmas, S. Qadeer, A. Sezgin, O. Subasi, and S. Tasiran. Simplifying \nlinearizability proofs with reduction and abstraction. In TACAS, 2010. [12] X. Feng. Local rely-guarantee \nreasoning. In POPL, pages 315 327. ACM, 2009. [13] X. Feng, R. Ferreira, and Z. Shao. On the relationship \nbetween concurrent separation logic and assume-guarantee reasoning. In ESOP, 2007. [14] I. Filipovi\u00b4c, \nP. O Hearn, N. Rinetzky, and H. Yang. Abstraction for concurrent objects. Theoretical Computer Science, \n411, 2010. [15] K. Fraser and T. Harris. Concurrent programming without locks. ACM Trans. Comput. Syst., \n25(2), 2007. [16] A. Gotsman and H. Yang. Linearizability with ownership transfer. In CONCUR, 2012. [17] \nL. Groves and R. Colvin. Trace-based derivation of a scalable lock-free stack algorithm. Form. Asp. Comput., \n21(1-2):187 223, 2009. [18] T. Harris, K. Fraser, and I. Pratt. A practical multi-word compare-and\u00adswap \noperation. In DISC, 2002. [19] D. Hendler, N. Shavit, and L. Yerushalmi. A scalable lock-free stack algorithm. \nIn SPAA, 2004. [20] M. Herlihy and N. Shavit. The Art of Multiprocessor Programming. Morgan Kaufmann, \n2008. [21] M. P. Herlihy and J. M. Wing. Linearizability: a correctness condition for concurrent objects. \nTOPLAS, 12(3):463 492, 1990. [22] C. A. R. Hoare. Proof of correctness of data representations. Acta \nInformatica, 1(4):271 281, 1972. [23] C. B. Jones. Tentative steps toward a development method for interfering \nprograms. TOPLAS, 5(4):596 619, 1983. [24] H. Liang and X. Feng. Modular veri.cation of linearizability \nwith non-.xed linearization points. Manuscript, July 2012. [25] H. Liang, X. Feng, and M. Fu. A rely-guarantee-based \nsimulation for verifying concurrent program transformations. In POPL, 2012. [26] R. J. Lipton. Reduction: \na method of proving properties of parallel programs. Commun. ACM, 18(12):717 721, 1975. [27] J. Lucassen \nand D. Gifford. Polymorphic effect systems. In POPL, 1988. [28] N. Lynch and F. Vaandrager. Forward and \nbackward simulations I: untimed systems. Inf. Comput., 121(2):214 233, 1995. [29] M. M. Michael and M. \nL. Scott. Nonblocking algorithms and preemption-safe locking on multiprogrammed shared memory multi\u00adprocessors. \nJ. Parallel Distrib. Comput., 51(1):1 26, 1998. [30] P. O Hearn, N. Rinetzky, M. Vechev, E. Yahav, and \nG. Yorsh. Verifying linearizability with hindsight. In PODC, 2010. [31] P. W. O Hearn. Resources, concurrency, \nand local reasoning. Theor. Comput. Sci., 375(1-3):271 307, 2007. [32] J. C. Reynolds. Types, Abstraction \nand Parametric Polymorphism. In Information Processing, 1983. [33] J. C. Reynolds. Separation logic: \nA logic for shared mutable data structures. In LICS, 2002. [34] R. Treiber. Systems programming: coping \nwith parallelism. Technical report, Almaden Research Center, 1986. [35] A. Turon, J. Thamsborg, A. Ahmed, \nL. Birkedal, and D. Dreyer. Logical relations for .ne-grained concurrency (Technical appendix), 2012. \nURL: http://www.ccs.neu.edu/home/turon/relcon. [36] A. Turon and M. Wand. A separation logic for re.ning \nconcurrent objects. In POPL, 2011. [37] V. Vafeiadis. Modular .ne-grained concurrency veri.cation. PhD \nthesis, University of Cambridge, 2008. [38] V. Vafeiadis and M. Parkinson. A marriage of rely/guarantee \nand separation logic. In CONCUR, 2007. [39] R. J. van Glabbeek. The linear time -branching time spectrum. \nIn CONCUR, 1990.   \n\t\t\t", "proc_id": "2429069", "abstract": "<p>Fine-grained concurrent data structures (or FCDs) reduce the granularity of critical sections in both time and space, thus making it possible for clients to access different parts of a mutable data structure in parallel. However, the tradeoff is that the implementations of FCDs are very subtle and tricky to reason about directly. Consequently, they are carefully designed to be contextual refinements of their coarse-grained counterparts, meaning that their clients can reason about them as if all access to them were sequentialized.</p> <p>In this paper, we propose a new semantic model, based on Kripke logical relations, that supports direct proofs of contextual refinement in the setting of a type-safe high-level language. The key idea behind our model is to provide a simple way of expressing the \"local life stories\" of individual pieces of an FCD's hidden state by means of protocols that the threads concurrently accessing that state must follow. By endowing these protocols with a simple yet powerful transition structure, as well as the ability to assert invariants on both heap states and specification code, we are able to support clean and intuitive refinement proofs for the most sophisticated types of FCDs, such as conditional compare-and-set (CCAS).</p>", "authors": [{"name": "Aaron J. Turon", "author_profile_id": "81418594363", "affiliation": "Northeastern University, Boston, MA, USA", "person_id": "P3977988", "email_address": "turon@ccs.neu.edu", "orcid_id": ""}, {"name": "Jacob Thamsborg", "author_profile_id": "81418598508", "affiliation": "IT University of Copenhagen, Copenhagen, Denmark", "person_id": "P3977989", "email_address": "thamsborg@itu.dk", "orcid_id": ""}, {"name": "Amal Ahmed", "author_profile_id": "81548050164", "affiliation": "Northeastern University, Boston, MA, USA", "person_id": "P3977990", "email_address": "amal@ccs.neu.edu", "orcid_id": ""}, {"name": "Lars Birkedal", "author_profile_id": "81100622053", "affiliation": "IT University of Copenhagen, Copenhagen, Denmark", "person_id": "P3977991", "email_address": "birkedal@itu.dk", "orcid_id": ""}, {"name": "Derek Dreyer", "author_profile_id": "81548019178", "affiliation": "MPI-SWS, Saarbruecken, Germany", "person_id": "P3977992", "email_address": "dreyer@mpi-sws.org", "orcid_id": ""}], "doi_number": "10.1145/2429069.2429111", "year": "2013", "article_id": "2429111", "conference": "POPL", "title": "Logical relations for fine-grained concurrency", "url": "http://dl.acm.org/citation.cfm?id=2429111"}