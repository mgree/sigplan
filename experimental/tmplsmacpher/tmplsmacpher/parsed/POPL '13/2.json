{"article_publication_date": "01-23-2013", "fulltext": "\n The Sequential Semantics of Producer Effect Systems Ross Tate Cornell University ross@cs.cornell.edu \nAbstract Effects are fundamental to programming languages. Even the lambda calculus has effects, and \nconsequently the two famous evaluation strategies produce different semantics. As such, much research \nhas been done to improve our understanding of effects. Since Moggi introduced monads for his computational \nlambda cal\u00adculus, further generalizations have been designed to formalize in\u00adcreasingly complex computational \neffects, such as indexed monads followed by layered monads followed by parameterized monads. This succession \nprompted us to determine the most general formal\u00adization possible. In searching for this formalization \nwe came across many surprises, such as the insuf.ciencies of arrows, as well as many unexpected insights, \nsuch as the importance of considering an effect as a small component of a whole system rather than just \nan isolated feature. In this paper we present our semantic formal\u00adization for producer effect systems, \nwhich we call a productor, and prove its maximal generality by focusing on only sequential com\u00adposition \nof effectful computations, consequently guaranteeing that the existing monadic techniques are specializations \nof productors. Categories and Subject Descriptors D.3.1 [Programming Lan\u00adguages]: Formal De.nitions and \nTheory Semantics General Terms Languages, Theory Keywords Effects, Monads, Effectors, Productors, Thunks \n1. Introduction Effects have been around since the beginning of programming languages. After all, even \nprograms written in the lambda calculus have effects: some programs diverge and some programs disregard \ntheir inputs. How these effects interact determines the semantics of a program, leading to strict or \nlazy evaluation depending on what choices are made. Yet, despite their prevalence, effects and their \nbehavior remain fairly mysterious, especially when one considers how thoroughly data and types have been \nformalized. We view effects and types as complementary systems, so by improving our understanding of \neffects to match that of types we hope to broaden the perspective of programming languages as a whole. \nBefore we go any further, we should de.ne what we mean by an effect, since this term has come to have \ndifferent meanings in dif\u00adferent communities. For some an effect is a classi.cation of a com\u00adputation \ndetermined by some analysis [17, 19, 22, 23, 30 32, 34]. Permission to make digital or hard copies of \nall or part of this work for personal or classroom use is granted without fee provided that copies are \nnot made or distributed for pro.t or commercial advantage and that copies bear this notice and the full \ncitation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to \nlists, requires prior speci.c permission and/or a fee. POPL 13, January 23 25, 2013, Rome, Italy. Copyright \nc &#38;#169; 2013 ACM 978-1-4503-1832-7/13/01. . . $10.00 For others an effect is synonymous with a monad \n[3, 9, 12, 13, 38]. Indeed, Wadler and Thiemann illustrated the intimate connec\u00adtion between the former \ntype-and-effect perspective and the latter computational-effect perspective [38], which we will re.ne \nwithin this paper. What we mean by an effect is a classi.cation of a com\u00adputation independent of the \nvalues of its inputs and outputs. For ex\u00adample, writes to the heap is an effect but writes to the heap \nonly if the input integer is non-zero is not. Thus, whereas types classify the inputs and outputs of \ncomputations, effects classify the inter\u00adnals of computations, forming complementary classi.cation sys\u00adtems. \nMore speci.cally, we are avoiding imposing the complexity of dependent classi.cation systems in addition \nto the complexity of reasoning about effects in a language-independent manner. This informal de.nition \nis much broader than what many might consider an effect. For example, disregards its input satis.es our \nde.nition: though it mentions the input to the computation, the meaning of this classi.cation does not \ndepend on the actual value of that input. Similarly, uses variable x satis.es our de.nition. These effects \ncorrespond to the use of context. They are often disre\u00adgarded since many consider freely accessible persistent \ncontexts to be a basic part of programming languages, even though such con\u00adtexts are not necessarily \npresent in resource-constrained or stack\u00adbased languages. When we realized this assumption, we had to \nre\u00adconsider our own notion of effect. What we found is that effects like uses its input multiple times \ncan be formalized as consumer ef\u00adfects because they reason about how a computation consumes its inputs. \nIn fact, many fundamental of behaviors of programming languages re.ect these consumer effects. Unfortunately, \nin order to keep this paper focused, we must consider consumer effects to be outside of the scope at \nhand. Instead, we will focus on the much more familiar dual notion that we call producer effects. A producer \neffect reasons about how a computation produces its outputs. For example, may throw an exception instead \nof pro\u00adducing its output or may examine and alter the heap before pro\u00adducing its output are familiar \nproducer effects. Here is where most existing work on effects lies, and so likewise here is where we \nfo\u00adcus our attention. The reason that producer effects are so common is that they are intimately tied \nto the notion of thunked computa\u00adtions, meaning computations which have been delayed so that they can \nbe treated as values. Indeed, we will de.ne an effect as a pro\u00adducer effect if all computations with \nthat effect can be thunked as pure computations for a domain-speci.c notion of purity. From this basic \nproperty, we will prove that the sequential semantics of all producer effects can be formalized by our \nframework. As a re\u00adsult, Moggi s monads for his computational lambda calculus [20], Wadler s indexed \nmonads [38], Filinski s layered monads [5], and Atkey s parameterized monads [2] are subsumed by our \nframe\u00adwork, and we will illustrate how these various formalizations arise from simple assumptions on \nthe producer effects at hand, as well as how those assumptions restrict the kind of information that \nthe effect system can track. As an example, we present an effect system for ensuring shared memory is \naccessed only in critical regions, the semantics of which is guaranteed to be inexpressible by these \nex\u00adisting systems because it violates their assumptions. However, we show how the semantics can easily \nbe formalized with our frame\u00adwork to prove that the effect system prevents race conditions.  Just as \nwe restrict our scope to producer effects, we also re\u00adstrict it to sequential composition of computations. \nThere are many other forms of composition, such as multiplicative composition (for parallelism and for \nadjacent subexpressions), additive composition (for branching), and coinductive composition (for recursion \nand for loops). We choose sequential composition because that is where existing research has focused \nits efforts. We focus on just the one form of composition so that we may address its challenges in full. \nWe believe many of the insights and techniques in this paper can be adapted to other forms of composition, \nthough there is still plenty of interesting work to be done there. With our semantic framework, which \nwe call productors, we provide a powerful solution to the problem of formalizing the se\u00adquential semantics \nof producer effects. This solution is not tied to common assumptions such as freely accessible persistent \ncontexts or higher-order functions, enabling us to delve into more interest\u00ading semantic domains as well \nas narrow down the fundamental aspects of programming languages. Because it is so general, not only does \nour framework provide a means for specifying the se\u00admantics of programming languages or .rst steps towards \nimprov\u00ading the extensibility of programming languages, it also provides a meta-language for discussing \neffects and their properties. We will demonstrate how such a meta-language illuminates the constructs \nin existing languages, particularly in prior formalizations of effects. This paper begins by .rst providing \nan example of an effect system greatly simpli.ed for sake of exposition (Section 2). Then we present \nmonads and how they formalize the semantics of an effect, albeit in a slightly different light than prior \nsuch explana\u00adtions (Section 3). Afterwards, we adapt this approach to our ex\u00adample effect system, showing \nwhere and why monads need to be generalized (Section 4). That concludes our example-driven por\u00adtion of \nthe paper. We then move on to formalizing effect systems as effectors and effectoids (Section 5). We \nfollow that by formal\u00adizing their semantics as productors and productoids (Section 6). With these formalisms \nestablished, we illustrate how a variety of existing semantic frameworks are special cases of productors \naris\u00ading from extra assumptions on the structure of the effect system at hand (Section 7). We then show \nwhat basic properties of a lan\u00adguage with effects guarantee all its effects are producer effects and \nso formalizable using productors (Section 8). Finally, we remark on higher-level insights, opportunities \nto expand existing semantic techniques, and further directions to take our research in order to meet \nour goal of providing an abstract language for discussing and formalizing programming languages (Section \n9). 2. Effects for Locking Effects are a classi.cation of computations independent of the ex\u00adplicit inputs \nand outputs of those computations. Sometimes effects are superimposed on top of an existing language \nin order to identify optimization opportunities or potential bugs [19, 31, 32, 34]. Other times effects \nare integrated into the type system [1]. For example, Java has checked exceptions, and Haskell uses monads \nin order to combine laziness and side effects. In this paper we will demon\u00adstrate how patterns in applications \nof effects lead to patterns in the semantics of effects. To demonstrate that these are just patterns \nand not necessarily inherent to effects, we present a toy language that breaks from all of these patterns. \nOur toy language Crit, shown in Figure 1, has explicit synchro\u00adnization and shared memory, limited for \nsimplicity in that there is only one lock and one shared integer. Clearly this is just a subset of a \nmore realistic language with branches, functions, and actual par\u00ad x : Z, y : Z f z := x + y -z : Z | \ne \u00d8 f acquire -\u00d8 | locking \u00d8 f release -\u00d8 | unlocking \u00d8 f x := get() -x : Z | critical x : Z f set(x) \n-\u00d8 | critical G f p -G' | e G f p -G' | e e = e' Prop Sub \u00af G, G f p -G\u00af, G' | e G f p -G' | e' G f p \n-G' | e G' f p' -G'' | e' e ; e' . e'' Seq2 ; G f p; p' -G'' | e'' Figure 1. A toy language Crit with \ncritical-region effects EFF = {e, locking, unlocking, critical, entrant} e = critical, entrant . ; . \ne locking unlocking critical entrant e e locking unlocking critical entrant locking locking - entrant \nlocking - unlocking unlocking critical - - unlocking critical critical - unlocking critical - entrant \nentrant locking - - entrant Figure 2. The effect system for Crit allelism, as well as a much richer type \nsystem. However, our intent is just to address sequential composition, so we strip down the lan\u00adguage \nto sequential composition. Nonetheless, even the semantics of this very simple language cannot be formalized \nusing existing frameworks, at least not in a way that allows the semantics to prove the desired properties \nof the effect system. In particular, the effect system for this language is intended to prove that whenever \na lock is acquired it is subsequently released (without being acquired again beforehand) and every use \nof shared memory occurs within such a critical region. Before we get into the effect system, let us explain \nCrit at a high level. The judgement G f p -G' | e indicates that the program p consumes context G as \nits input, produces context G' as its output, and has overall effect e. Note that Crit is linear: every \nvalue created is used exactly once. There is one exception: an integer stored to the shared state via \nset can be read multiple times via multiple gets. We can do this because it is possible to implement \na duplicating function Z -!Z in linear type theory. One could make Crit non\u00adlinear by restricting its \ntypes to ones which have such a duplicating function. We chose not to because it is orthogonal to the \nconcerns of sequential composition and would make the denotational semantics of Crit unnecessarily complex. \nNext, consider the .ve primitive statements of Crit and their effect. We give addition the effect e, \nwhich we call the basic effect, and which more generally can be used for all effects not related to locking \n(such as non-determinism or memory allocation). The statement acquire has the effect locking because \noverall it changes the state of the lock from free to occupied. Dually, release has the effect unlocking \nbecause overall it changes the state of the lock from occupied to free. Lastly, get and set have the \neffect critical because they must occur inside a critical region. Finally, consider the three general-purpose \nrules. Prop allows unused context to propagate through a program without altering the effect. While this \nis not necessary, and indeed is not possible for all effect systems (e.g. low-level stack-based effect \nsystems), we will show how this relates to the notion of strength used in existing semantic frameworks. \nSub conveys the notion of subeffects analo\u00adgous to subtyping. Seq2 ; speci.es how effects sequence statically. \nThese rules can easily be used for other languages, so we give their speci.cs separately in Figure 2. \nNote that Figure 2 has an additional effect entrant used to indicate code that enters and leaves a critical \nsection (possibly multiple times), so though the code restores the state of the lock it .rst needs the \nlock to be free in order to execute.  This effect system has a number of unusual attributes. First, \nthe effects do not form a lattice. In particular, e is not a subeffect of every effect. Second, not all \neffects can be sequenced, not even necessarily with themselves. This means that effects can in.uence \nthe typability of a program. Third, the order of the effects matters. locking before unlocking is considered \nentrant but the reverse is not allowed at all. All these attributes have important consequences on the \ndenotational semantics of Crit and on the insuf.ciency of existing frameworks. But before we discuss \nthose we should present the basics of denotational techniques for effects. 3. Monads In 1958, Godement \ninvented standard constructions [7], which be\u00adcame known as (Kleisli) triples, which became known as \nmon\u00adads. In 1988, Moggi migrated the concept of monads from the category-theory community to the programming-languages \nseman\u00adtics community [20]. In 1990, Wadler carried this concept over to the functional-languages community \n[35, 36], and in 1993 these concepts were realized as monadic programming and added to Haskell to make \nI/O more convenient and to embrace imperative functional programming [12], blurring the line between \nimperative programming and pure programming. This progression has made monads the most well known technique \nfor formalizing the seman\u00adtics of effects. Here we show how a monad formalizes an effect, but in a way \nthat highlights opportunities for generalization. Propagating an Effect Consider the expression (64 \u00f7 \nx) + 1 (using integer division). Focus on the problem that + expects its .rst argument to be an integer \nbut the \u00f7 in the .rst argument may fail to produce one because \u00f7 is a partial operation. In other words, \n\u00f7 has the partial effect. We might represent this by saying that \u00f7 has partial type Z\u00d7 Z ---. Z. We want \nto formalize what it means to have the partial effect. Moggi observed that we can do so by modifying \nthe return type of \u00f7 [20]. In particular, we can view \u00f7 as a function that returns an integer or a failure \ncode. We can de.ne an algebraic data type Ppartial to represent these two cases: Ppartial (t ) = success(t) \n| failure Then we can formalize \u00f7 as a pure operation Z\u00d7 Z . Ppartial (Z). When a failure occurs, any \nsubsequent computations need to propagate this failure. In particular, if 64\u00f7x fails, then (64\u00f7x) + 1 \nshould fail as well. Thus, (64 \u00f7 x) + 1 also has the partial effect. We can do this by making the pure \ncomputation .v.v + 1 from Zto Zinstead take a Ppartial (Z) and return a Ppartial (Z). This is achieved \nwith a map operation: mappartial : .a, \u00df. (a . \u00df) .((Ppartial (a) . Ppartial (\u00df)) success(x) . success(f(x))mappartial \n(f) = .px. case px failure . failure Thus map turns a pure computation into one which takes an effect\u00adful \nargument and propagates the effect. In this case, mappartial indi\u00adcates that if a failure code is present \nthen all computation should be bypassed and the failure propagated. Using map we can formalize the semantics \nof (64\u00f7x) + 1 as mappartial (.v.v + 1)(64 \u00f7 x). We map the computation after the effectful operation \nso that it can take an effectful argument, then pass the effectful result to this mapped computation \nwhich propagates the effect. Thus if 64 \u00f7 x fails so does the entire expression. This pair of a type \nconstructor P : TY P E . TY P E and a function on computations map : (t . t ' ) . (P (t ) . P (t ' )) \nis called an (endo)functor (on the category of types) provided it satis.es a few additional equalities \nwhich we do not repeat here, and this is just one part of a monad. In the setting of effects, the type \nconstructor P indicates how a production of the effect can be described as data so that effectful computations \ncan be represented as pure computations with a modi.ed return value, and the function map de.nes how \nto propagate the effect through pure computations. Sequencing Effectful Computations Now consider a slightly \nmore complex example: (64 \u00f7 x) \u00f7 y. Here we are sequencing two effectful computations. We can use the \nfunctor representation of the partial effect in order to formalize this expression: mappartial (.v.v \n\u00f7 y)(64 \u00f7 x) The type of this formalization, though, is Ppartial (Ppartial (Z)), since the computation \nwe mapped, namely .v.v \u00f7 y, also has the partial effect. Although having a doubly partial value allows \nus to de\u00adtermine which \u00f7 failed, in this case we are only concerned with whether any \u00f7 failed. That is, \nwe want the effect of (64 \u00f7 x) \u00f7 y to be partial rather than (conceptually) partial2. We can accomplish \nthis by using a monadic join operation to turn doubly partial values into (singly) partial values: joinpartial \n: .a.Ppartial (Ppartial (.a)) . Ppartial (a) .success(success(x)) . success(x) success(failure) . failure \njoinpartial (ppx) = case ppx . failure . failure Note that in this example there is only one effect in \nthe whole system, and this is why we can use a single monad. Some semantic frameworks generalize from \none effect to multiple effects by using multiple monads, but our semantic framework provides a more general \nsystem in which these approaches are special cases. Making Pure Computations Effectful With the above \nstructures we can sequence a non-empty list of effectful computations in\u00adterspersed with pure computations \ntogether into a single effectful computation. Monads have one more component that enables an empty list \nof effectful computations (from a type to itself) to be turned into a single effectful computation. This \nstructure also en\u00adables pure computations to be turned into effectful computations. It is known as the \nunit of the monad, in this case turning pure values into partial values: unitpartial : .a. a . Ppartial \n(a) unitpartial (x) = success(x) While it is important to be able to treat pure computations as effectful \ncomputations in order to handle computations uniformly, a system with many effects need only have one \neffect with a unit in order to accomplish this. Recognizing this is important since it allows effects \nto express signi.cantly more concepts, as we will demonstrate later. Coherency The operations for monads \nhave a variety of equali\u00adties which must hold, known as the identity and associativity laws. We do not \nrepeat those laws here, rather we convey their signi.\u00adcance. Consider the expression (((64 \u00f7 x) \u00f7 y) \n\u00f7 z) + 5. This is essentially .ve computations sequenced together: 64, .v.v \u00f7 x, .v.v \u00f7 y, .v.v \u00f7 z, \nand .v.v + 5. Using the structures above we can sequence these computations (maintaining order) in a \nvariety of ways depending on how we combine consecutive pairs and whether we turn pure computations into \neffectful computations or simply use functorial structure to propagate effects through them, but the \nresult should be the same no matter which way is used. We call this concept semantic coherency, and the \nmonad laws are necessary and suf.cient to ensure this for sequential composition.  4. Semantics for \nLocking With monadic techniques in mind, let us consider how we might de.ne a denotational semantics \nfor Crit. First, we need to choose a semantic domain on which to de.ne all of our operations. Choos\u00ading \na good semantic domain can enable the semantics to automati\u00adcally ensure useful properties. In this case, \nwe want to ensure that acquires match up with releases one-to-one, and gets and sets hap\u00adpen only inside \ncritical regions. Selecting a Semantic Domain To this end, we opt for linear type theory as our semantic \ndomain, since linear type theory already has a notion of counting built into it. We use an abstract type \nL to denote a free lock and an abstract type C to denote the capability to access shared memory. An abstract \noperation acquire converts a free lock L to a capability C, and an abstract operation release does the \nreverse. An abstract operation get fetches the value of the shared state provided a capability C is present, \nand an abstract operation set replaces the value of the shared state provided a capability C is present. \nOf course, L and C actually stand for Z, but by hiding that information and ensuring every primitive \noperation consumes and produces one L or C we are guaranteed that every acquire matches up with a release \nand all gets and sets occur in between. These abstract constants and their types are summarized below. \nL : TY P E C : TY P E acquire : L -C release : C -L get : C -C . Z set : C . Z -C Representing Effectful \nValues Next, we must specify how to represent productions of each effect as data. While we could de\u00ad.ne \nPe (t ) as simply t , we want to emphasize that e represents all effects not related to locking. For \nexample, e could be non\u00addeterminism, in which case Pe (t ) would be \u00b5t. t . (t &#38; t). As such, we \nsimply assume Pe (t) is some monad with operations map , join , and unite , along with an operation strength \nof type ee e .a, \u00df. a . Pe (\u00df) -Pe (a . \u00df) which we will explain later. As for the impure effects, their \nsemantics all have the form: TS,S1 (t) = S -Pe (S ' . t) TS,S1 transforms the state from S to S '. In \nparticular, the impure effects transform between L and C: Plocking = TL,C Punlocking = TC,L Pcritical \n= TC,C Pentrant = TL,L Thus a computation with the locking effect must have access to a (necessarily \nunique) free lock L, do some computation with effect e, and complete with a (necessarily unique) capability \nto access shared memory C still available. Formalizing Primitive Operations Once we have determined how \nto represent an effect, we can give semantics to the primi\u00adtive effectful computations. The strategy \nformalizes the semantics of a judgement G f s -G ' | e as a term in linear type theory with r r the type \n( G) -Pe( G ' ). With this in mind, we de.ne the semantics of the primitive effectful computations in \nFigure 3. Note that the semantic domain guarantees that only one portion of code can hold the lock at \na time. Thus, just the fact that these primitives can be expressed in that semantic domain ensures that \nthey preserve that property. This helps a language designer deter\u00admine how they can safely extend the \nlanguage, since now they need only check that any potential new constructs are expressible in this domain. \nPropagating Effects Next, we have to de.ne how effects propa\u00adgate through pure computations. Since the \nimpure effects have the same shape, they also use approximately the same map operation: mapTS,S1 = .f..g..s.map \n(.(s ' , x).(s ' , f (x)))(g(s)) e [x : Z, y : Z f z := x + y -z : Z | e] = .(m, n).unite(m + n) [\u00d8 f \nacquire -\u00d8 | locking] = .()..l.unite ((acquire(l), ())) [\u00d8 f release -\u00d8 | unlocking] = .()..c.unite ((release(c), \n())) [\u00d8 f x := get() -x : Z | critical] = .()..c.unite (get(c)) [x : Z f set(x) -\u00d8 | critical] = .n..c.unite \n((set(c, n), ())) Figure 3. Semantics of the primitive effectful computations of Crit Sequencing Effectful \nComputations The most challenging se\u00admantic component is sequencing effectful components. Here is where \nwe have to stray from monads. After all, not all of these effects are monads. For example, there is no \njoin operation for locking. To see why, recall that a locking computation needs a free lock to be available \nand then removes that free lock, turning it into a capability. Thus, immediately after a locking computation \nthere is no free lock available, so there cannot be another locking computa\u00adtion that needs a free lock. \nFortunately, our effect system is designed to address precisely this problem. In Figure 2, locking ; \nlocking is intentionally unde.ned so that such programs are disallowed and we do not have to worry about \nsequencing two locking computations. This suggests how we might generalize monads for sequencing computation. \nSuppose we had functions p : A -Pe(B) and p ' : B -Pe1 (C) representing effectful computions with different \neffects. Furthermore, suppose e ; e ' were de.ned as e ''. Then we should be able to combine p and p \n' into a function of type A -Pe11 (C) representing a compu\u00adtation from A to C with effect e ''. So far, \nwe can use map to get the following function: .a.mape(p ' )(p(a)) : A -Pe(Pe1 (C)) This function essentially \nproduces a doubly effectful value like with monads, but this time with two different effects. So, like \nwith monads, we need some way to turn this doubly effectful value into a (singly) effectful value with \njust effect e ''. Thus, whenever e ; e ' is de.ned as e '' we need the following operation to sequence \nthe two effectful computations: joine,e1 : .a. Pe(Pe1 (a)) -Pe11 (a) For Crit there are four major categories \nof such sequencings: (1) basic followed by basic, (2) impure followed by basic, (3) basic followed by \nimpure, and (4) impure followed by impure. The .rst is simply assumed to be de.ned as discussed before: \njoin = join e,e e The second is fairly simple to de.ne: joinT= .f..s.join (map (strength )(f(s))) ee \ne S,S1 ,e Note that this is simply the result of sequencing the linear func\u00adtion f : S -Pe (S ' . Pe \n(a)), viewed as a computation with ef\u00adfect e, with strength e : S ' . Pe (a) -Pe (S ' . a), also viewed \nas a computation with effect e. What strength e is doing is pulling the context of the state S ' produced \nby f into the effectful value Pe (a) so that it can be accessed by subsequent computations. Unlike in \nHaskell, strength e does not exist for all monads in linear type the\u00adory. One notable exception is the \nerror monad: 1 . t . This is why languages with both errors and locks have to have special constructs \nfor handling the case when an error is thrown by a thread holding a lock. Similarly, this is why many \nlanguages have constructs for resource-sensitive data in the face of errors, such as C# s using construct \nfor IDisposable objects. Also, we were careful to de\u00ad.ne non-determinism using &#38; rather than . (which \ncorresponds to the (non-empty) list monad used by parsers) since the former has linear strength whereas \nthe latter does not.  Moving on, the third is only slightly more complex to de.ne: join = f ..s.join \n(.(s,f).f(s)))(strength (s, f ))) S,S1 . (map e,Teee Here we have an S-state s and f is an e-effectful \nvalue containing a function expecting an S, so once again we have to use strength e and mape to give \nthe contained function f access to the state s. This results in a doubly e-effectful value, so we use \njoine . The fourth case is the most interesting regarding this paper: joinT= .f..s.join (map (.(s ' , \ng).g(s ' ))(f(s))) e e S,S1 ,TS1,S11 Note that it is only de.ned when the intermediate state S ' agrees \nfor both effectful computations, in which case it results in a TS,S11 -effectful computation. In fact, \nwhen we consider just the impure effects we get an instance of Atkey s parameterized mon\u00adads [2], a connection \nwe will discuss in Section 7. With these components we can de.ne the semantics of Seq2 ; : ' -G '' ' \n[G f p -G ' | e] = f [G ' f p | e ' ] = f [e ; e '] = joine,e1 . e '' ' -G '' | e '' [G f p; p ] = .g.joine,e1 \n(mape(f ' )(f(g))) Coercing Effectful Computations Crit has another feature, name\u00adly subeffects, that \nwe could not encounter with monads since there was only one effect. Like subtypes, the intuition for \nsubeffects is that there is a way to coerce computations pertaining to the subeffect into ones pertaining \nto the supereffect. As such, whenever e = e ' holds, we need an operation with the following form: coerce \ne,e1 : .a. Pe(a) -Pe1 (a) With Crit, we can de.ne such coercions from the basic effect to an impure effect \nwhenever the input and output states match up, namely for critical and entrant: coerce e,TS,S = . (s, \nx..s.strength e x) Using these coercions we can de.ne the semantics of Sub : [G f p -G ' | e] = f [e \n= e ' ] = coerce e,e1 [G f p -G ' | e ' ] = .g.coerce e,e1 (f(g)) Propagating Context There is only one \nrule left to handle for Crit. Prop is in a sense orthogonal to the concerns of sequen\u00adtial composition, \nyet it is so common in languages that we in\u00adclude it for sake of discussion. Suppose we have some function \nr r f : G -Pe( G ' ) denoting the semantics of G -p f G ' | e. We want to extend f so that it propagates \nthe values of an addi\u00adtional context G through the computation. It is easy to de.ne the \u00af following function \nthat does at least part of the job: r r r r r \u00af\u00af\u00af G. f :G. ( G) -G. Pe ( G) r \u00af So the issue is that \nwe need to somehow pull the G inside the Pe r r \u00af so that we have the necessary Pe(( G) . ( G)). This \nissue should look familiar, since when sequencing the impure effects we needed some way to pull state \ninside of Pe , and so we use the same technique we used there. In particular, we need every effect to \nhave a strength operation: strength e : .a, \u00df. a . Pe(\u00df) -Pe(a . \u00df) We already assumed such an operation \nhas been de.ned for e, so we need only de.ne it for the impure effects. Again, the strength operations \nfor the impure effects all have the same form: strength T= .(x, f )..s.map (swap)(strength (x, f (s))) \ne e S,S1 swap = .(x, (s ' , y)).(s ' , (x, y)) Note that this strength need only be de.ned for types \na that can occur in a context G. So, if one were to extend Crit to be G0 f p1 -G1 | e1 . . . Gn-1 f pn \n-Gn | en [e1, . . . , en] -; . e ; SeqG0 f p1; . . . ; pn -Gn | e Figure 4. Assumed typing rule for effect \nsystems non-linear and ensure all Crit types t had an operation t -!t , then the effects could use a \nnon-linear strength. This way Crit s effect system could be extended with exceptions, so long as one \nwere to carefully track and de.ne the interaction of exceptions with locking. However, since we already \nneed the basic effect to have linear strength, we make no such restriction on the types occurring in \nG here. We mentioned that Prop is not necessary for sequential compo\u00adsition; it comes from the notion \nthat the context is freely accessible. In low-level stack-based languages, effectful operations may be \nre\u00adquired for even just accessing the context. Sometimes the context can be extended, but only if the \neffect is altered as well. For ex\u00adample, in a low-level stack-based language, it is occasionally the \nresponsibility/privilege of the exception handler to clean-up/work\u00adwith the stack that existed at the \npoint the exception was thrown. Such a language might have an effect of the form exc( G is G) where the \nstate of the stack at the point the exception was thrown. Deno\u00ad r tationally, the effect may be represented \nas PG)(t ) = t . G. exc( Should the stack be extended prior to throwing the exception, then this must \nbe encoded in the effect. As such, this language might have a rule like the following: G f p -G ' | exc( \nG) \u00af\u00af G, G f p -G, G ' | exc(G\u00af, G) Thus this effect has no need for strength. We emphasize this exam\u00adple \nboth to illustrate the origin of strength present in many existing formalizations and to illustrate the \nwide variety of languages our framework is capable of handling. 5. Effectors While so far we have been \nfocusing on our example language Crit, the intent of this work is to apply to nearly all effect systems. \nIn this section, we introduce effectors to formalize effect systems as they pertain to typing sequential \ncomposition of programs. In the next section, we then present our semantic framework for effectors. Following \nthat, we show how existing semantic frameworks .t within our own. Finally, we show what common properties \nof a language and its effect system guarantee that the semantics can be formalized with our framework. \nFor the sake of formalizing effect systems, we assume the un\u00adknown language at hand admits the rule Seq; \nin Figure 4. The lan\u00adguage may (and should) admit many other rules as well; we simply need to know that \nit admits at least Seq; . The symbol . is a rela\u00ad -; tionship from lists of effects to effects, so that \n[e1, . . . , en] . e -;indicates that e is the overall effect of sequencing computations with effects \ne1 through en in that order. Seq; informs us that the effects of computations are independent of their \nspeci.c inputs and outputs, granting the modularity one would expect from an effect system. Thus we do \nnot concern ourselves with some form of de\u00adpendent effects. Note that we intend Seq; to include when \nn is 0, corresponding to typing the empty program. The following short\u00adhands may further elucidate the \nmeaning of -;  .: e . e means [] . e -;e = e ' . e ' means [e] -; . e '' ; . e '' e ; e 'means [e, e \n' ] - Seq; may not seem like much to work with, but it actually pro\u00advides some very useful structure. \nIn particular, it is important to realize that given a sequence of computations, say [p1, p2, p3], the \nsyntax p1; p2; p3 could be parsed as a whole or as (p1; p2); p3 or as p1; (p2; p3) or even as (); p1; \n(); p2; (); p3; () with the empty pro\u00ad gram interspersed. As such, one can show that, should [e1, e2] \n-; . e ' and [e ' , e3] . e hold, then we can assert that [e1, e2, e3] . e also -; -;holds without changing \nthe effects of programs. In light of this, we formalize effect systems as follows: De.nition (Effector). \nA set EFF along with a relation . between -; List(EFF) and EFF satisfying the following two properties: \nIdentity Associativity e = e ' ;; .e1...en.(.i.Eei . ei) . [e1...en - -] . e .e, e ' . . .eE1...Een, \ne. . ;; [e] -E+ . . . +en . e . e ' e1 ++ E - The identity rule says that sequencing an effectful computation \nwith no other computations should have (at least) the same effect. The associativity rule says that, \nshould one partition a list of ef\u00adfectful computations and sequence each partition together and then \nsequence each of those results together, sequencing the original list all together should have (at least) \nthe same overall effect. This for\u00admalization may seem rather foreign with respect to existing work on \neffects. However, when an effector is semi-strict, meaning the associativity implication is actually \nan if-and-only-if, there is an equivalent monoid-like de.nition more akin to prior work: Theorem 1. A \nsemi-strict effector can equivalently be de.ned as an effectoid: a set EFF along with a unary relation \ne . -, a binary relation - = -, and a ternary relation - ; - . - satisfying: .e\u00a3. e . e\u00a3 . e\u00a3 ; e . e \n' effector is one where for every list of effects Ee such that E -; e . e holds for some effect e there \nexists a minimal such e with respect to =. Principalled effectors are common for the same reasons prin\u00adcipalled \ntype systems are common, such as simplifying type check\u00ading and type inference. Note that not all effectors \nare semi-strict, especially ones used in analyses. For example, an analysis may say that x := *p has \nthe read effect and that *p := x has the write effect but then give x := *p; *p := x the e effect even \nthough generally those ef\u00adfects would combine into the read-write effect. The analysis is using information \nnot contained in the descriptions read and write, namely that the same value and pointer is used in both \nheap uses, to rea\u00adson that the program s semantics factor through the operation co\u00adercing pure computions \ninto read-write computations and so can be treated as pure. In general, such effectors typically arise \nwhen an analysis uses more detailed reasoning intraprocedurally but coarser reasoning interprocedurally. \nThus, while effectoids are extremely common, the generality of effectors is necessary to capture exist\u00ading \neffect systems. 6. Productors At last, we present our semantic framework for sequential composi\u00adtion \nof effectful computations, which we call a productor. Our goal is to give a denotational semantics to \nSeq; in such a way that all the many possible parsings of p1; . . . ; pn are guaranteed to produce the \nsame semantics. Our fundamental assumption is that the semantics of the judgement G f p -G ' | e can \nbe represented as a morphism [G] . Pe([G ' ]) for some function-on-objects Pe. De.nition (Productor for \nan effector (EFF, -;A category Sem .)). with endofunctors {(Pe, mape)}e.EFF and natural transformations \n{join[e1,...,en],e : Pe1 . \u00b7 \u00b7 \u00b7 . Pen . Pe};such that [e1,...,en]- .e join[e],e is always the identity \ntransformation and the following diagram commutes whenever all terms are de.ned: Pe1 . \u00b7 \u00b7 \u00b7 . Pen joinEe1,e1 \n* . . . * joinEen,en . join[e1,...,en],e . . . Pe1 .\u00b7 \u00b7. P. Pe E\u00b7 enE joinE e1 ++... ++ E en,e Re.exive \nCongruenceAssociativityIdentity .e. e\u00af1 ; e2 . e\u00af. e\u00af; e3 . e .e1, e2, e3, e. . .e. e 2 ; e3 . e . e1 \n; e . e .e. e = e .e, e ' . e . e . e = e ' =. e . e ' .e1, e2, e, e ' . e1 ; e2 . e . e = e ' =. e1 \n; e2 . e ' Proof. The proof can be found in the technical report [33]. Semi-strict effectors are common \nbecause compositional type Notice the similarity between the commutativity requirements .e, e ' . e = \ne ' for productors and the implicational requirements for effectors. The idea is to ensure that the many \nways that a sequence of programs .er. e . er . e ; er . e ' may be typed all correspond to the same semantics. \nIn other words, systems are common. Conceptually, if Seq; is actually an if-and\u00ad the speci.c proof used \nto type check a program should be irrelevant to its semantics. In the technical report [33], we ensure \nprecisely that for the following denotational semantics of Seq; : [G0 f p1 -G1 | e1] = f1 . . .] [Gn-1 \nf pn -Gn | en] = fn ; [[e1, . . . , en] - . e= join[e1,...,en],e [G0 fp1; . . . ; pn -Gn |e] = f1; mape1(. \n. . (fn )); join[e1,...,en],e One might wonder why we worry about parsing ambiguity. Af\u00adter all, a language \ncould always parse sequential composition left\u00adassociatively, like Haskell does. The reason is that by \naddressing ambiguity concerns we also address the kind of program transfor\u00admations one would intuitively \nexpect from sequential composition. For example, should f() be de.ned as p1; p2; p3, g() as the empty \nonly-if, then the effector for the language is semi-strict. Note that e . e does not necessarily imply \ne is an identity in the usual sense for monoids. It simply means that pure computations (particularly \nprogram, and h() as p4, then one would expect f(); g(); h() to have the same semantics as p1; p2; p3; \np4. This is precisely what our commutative diagrams ensure. We have proven that when the effector is \nsemi-strict there is an the empty program) can be given the e effect. We call such effects centric effects. \nThe effect system for Crit is de.ned as a congruently preordered equivalent de.nition of productors that \nmay appear more familiar for those acquainted with existing semantic frameworks: partial monoid. This \nworks because congruently preordered partial Theorem 2. A productor for an effectoid (EFF, e, =, ;) can \nequiv\u00admonoids are equivalent to principalled effectoids. A principalled alently be de.ned as a productoid: \na category Sem with end\u00ad ofunctors {(Pe, mape)}e.EFF along with natural transformations {unite : Sem \n. Pe}e .e and {coercee,e1 : Pe . Pe1 }e=e1 and {joine,e1,e11 : Pe . Pe1 . Pe11 }e ; e1 .e11 such that \ncoercee,e is al\u00adways the identity morphism and the following diagrams commute whenever all terms are \nde.ned: Pe\u00a3 . Pe . unite\u00a3 . joine\u00a3,e,e1 . coercee,e1 . . Pe . Pe ' mape(uniter ) . . joine,er,e1 Pe \n. Per Pe\u00af. Pe3 joine1,e2,\u00af. join e \u00afe,e3,e . . Pe1 . Pe2 . Pe3 Pe . mape1 (joine2,e3, ) . joine1, \ne e,e Pe1 . Pe . unite Pe coercee,e1 . joine1,e2,e Pe coercee,e1 . . Id unite1 . Pe1 Pe1 . Pe2 Pe1. \njoine1,e2,e1 Proof. The proof can be found in the technical report [33]. The implicational requirements \nof an effectoid is essential to the above theorem. Using a productoid for a pre-effectoid, meaning an \neffectoid not necessarily satisfying the relevant implications, can indeed result in ambiguous semantics. \nFigure 5 provides a toy example of such. The intuition behind this example is that pure represents no \neffect, err represents a possible error, and hndl indicates an er\u00adror handler has been speci.ed (more \nprecisely the .a.a must be a natural transformation note that sometimes there can be many inhabitants \nof .a.a, such as differing error messages or even null in object-oriented domains). A primitive err-effectful \nopera\u00adtion error() throws an error, and a primitive hndl-effectful operation handle(h) sets the handler. \nThe de.nition of joinhndl,hndl,hndl indi\u00adcates that, if the handler is set twice in a row, the second \nhandler should be used in place of the .rst one. The interaction of handlers and exceptions is de.ned \nsolely by the joinhndl,err,pure operation. In particular, this de.nition indicates that, should an exception \nbe thrown, then the previously set handler should be used to handle the exception and proceed with normal \nexecution. The result is that a computation has effect pure only if a handler has been set every place \nan exception might be thrown. One can check that these operations do indeed satisfy the re\u00adquirements \nof our de.nition in Theorem 2. However, supposing h1 and h2 are handlers, the following pure-effectful \nprogram still has two possible semantics: handle(h1); handle(h2); error(); error() In particular, the \nprogram can result in h1 or h2. The program re\u00adsults in h1 if handle(h2) gets matched up with the .rst \nerror() (via joinhndl,err,pure), sequencing into a pure-effectful computation so that then handle(h1) \ncan be matched up with the second error() (via joinhndl,err,pure), sequencing into a pure-effectful computation \nresulting in h1. The program results in h2 if handle(h2) over\u00adrides handle(h1) (via joinhndl,hndl,hndl) \nand the two error()s are combined (via join ), so that then handle(h1); handle(h2) err,err,err can be \nmatched up with error(); error(), sequencing into a pure\u00adeffectful computation resulting in h2. Thus, \nthe speci.c proof used to show the program has effect pure actually in.uences the pro\u00adgram s semantics, \nviolating semantic coherency. EFF = {pure, err, hndl} .e. pure ; e . e e ; pure . e err ; err . err hndl \n; hndl . hndl hndl ; err . pure Ppure(t ) = t Perr(t ) = Ppartial(t) Phndl(t ) = t \u00d7 .a.a .e. joinpure,e,e \n= joine,pure,e = identity joinerr,err,err = joinpartial joinhndl,hndl,hndl = .((x, h ' ), h).(x, h ' \n) ( success(x) . x joinhndl,err,pure = .(px, h).case px failure . h Figure 5. A semantically incoherent \npre-effectoid and productoid This productoid was designed by intentionally devising a pre\u00adeffectoid that \nviolates our associativity requirement for effectoids. For example, hndl ; hndl . hndl and hndl ; err \n. pure hold, but there is no e such that hndl ; err . e and hndl ; e . pure hold. As such, one cannot \nprogressively transform either proof into the other using the required equivalences on the productoid, \nso they can produce different semantics. This illustrates the subtleties of semantic coherency for effect \nsystems addressed by our framework. Because the pre-effectoid does not satisfy the appropriate requirements, \nthe productoid does not extend to a productor. Although not immediately apparent, in general a productor \nactually has more equational requirements than a productoid in order to maintain semantic coherency; \nit just happens to be that the implicational requirements of an effectoid imposes enough structure on \nits productoids to ensure they satisfy those additional equational requirements. In general, additional \nstructure on the effector at hand imposes structure on its productors. This interplay will be a common \ntheme during our discussion of existing semantic frameworks for effects. 7. Existing Frameworks While \nthe components and requirements of a productor can be described concisely, the high-level nature of this \ndescription can be daunting. As such, it is helpful to see how existing semantic frameworks for effects \ncan be seen as special classes of productors. We start with the simplest such framework: monads. Monads \nA monad (M, map, unit, join) is a productor for the effector with one effect e and with Ee . e always \nholding. M is -;simply Pe; map is mape; unit is join[],e; and join is join[e,e],e. The equational requirements \nfor a monad are precisely those re\u00adquired by our de.nition of productor. More generally, for any effector \nwith an effect e such that e . e and e ; e . e hold (called a centric idempotent effect), any productor \ns representation of e is necessarily a monad. Monad Morphisms A monad morphism coerce from a monad ' \n'' (M, map, unit, join) to another monad (M , map , unit , join ' )is a natural transformation from M \nto M ' such that the following diagrams commute: M ' . M ' . join ' coerce * coerce . M . . unit coerce \n. M . M M ' Sem M ' . . ' . join coerce unit M These equational requirements ensure that implicit coercions \nfor effectful computations do not affect the semantics of the program, in the same spirit as Reynolds \nrequirements for implicit coercions for data types [28].  As we mentioned, monads correspond to centric \nidempotent ef\u00adfects. Furthermore, given two such effects e and e ' with the addi\u00adtional property that \ne is a subeffect of e ', then the equational re\u00adquirements for productors guarantee precisely that the \ncorrespond\u00ading natural transformation coerce e,e1 = join[e],e1 from Pe to Pe1 is a monad morphism. Thus \nboth the concepts of monads and monad morphisms are special cases of productors. Indexed Monads Program \nanalysis is one of the most common applications of effect systems. Such analyses typically use what is \nknown as a type-and-effect system [19, 22, 32], which taints code with a collection of atomic effects. \nFor example, Talpin and Jouvelot taint code with how it accesses various heap regions [31], and Abadi \nshowed that the dependency core calculus essentially takes this approach as well [1]. Abstractly, these \neffect systems form a join semi-lattice, and the combined effect of a sequence of computations is the \njoin (U) of the effects of the individual computations, with pure and empty computations being given \nthe . effect [3, 13]. Wadler and Thiemann famously showed that one of Talpin and Jouvelot s effect systems \ncan be formalized by what has become known as an indexed monad [38]. An indexed monad is a join semi\u00adlattice \nof monads connected by monad morphisms. The semantics of sequencing effectful computations was de.ned \nby coercing both computations to the least common supermonad and then using the join of that monad. They \ndid this for a single effect system, but with our framework it is easy to determine that join semi-lattice \neffectors are intimately connected to indexed monads, and in these cases the semantics of sequential \ncomposition are guaranteed to be de.ned with this coerce -then-join strategy. To see why, consider the \nfollowing properties of an effector where sequential composition of effects coincides with a lattice\u00adjoin \noperation. They are guaranteed to be centric effectors, mean\u00ading all their effects are centric effects. \nThey are guaranteed to be idempotent, meaning all their effects are idempotent. Lastly, they are guaranteed \nto be increasing, meaning whenever Ee . e holds -; then all elements of Ee are subeffects of e. The .rst \ntwo properties already guarantee that a productor rep\u00adresents all effects as monads and furthermore all \ncoercions be\u00adtween such effects must be monad morphisms. Thus a productor for any centric idempotent \neffector must be a network of monads and monad morphisms, like an indexed monad. All we have left to \ndo is show why sequential composition must take the coerce -then\u00adjoin strategy: Theorem 3. Any increasing \ncentric idempotent effector is semi\u00adstrict. For any productor of an increasing idempotent effector, joine1,e2,e \nis determined by coerce e1,e, coerce e2,e, and joine,e,e. Proof. First, suppose Ee1 ++ . . . ++ Een . \ne holds. Since the effec\u00adtor is increasing, all effects in each Eei must be subeffects of e. Since the \neffector is centric and idempotent, [e, . . . , e] . e holds for all arities. These and the de.nition \nof effector imply that Eei . e holds. Thus, since [e, . . . , e] . e also holds, the effector is semi-strict. \nSecond, suppose e1 ; e2 . e holds. Since the effector is in\u00adcreasing, ei = e holds so join[ei],e must \nexist. Since the effector is idempotent, [e, e] . e holds so join[e,e],e must exist. Then by def\u00adinition \nof productor since [e1] ++[e2] equals [e1, e2], join[e1,e2],e must equal (join[e1],e * join[e2],e) ; \njoin[e,e],e, which is the long\u00adhand form of (coerce e1,e * coerce e2,e) ; joine,e,e. In the other direction, \nlet us consider what kind of effectors in\u00addexed monads can handle. Suppose an effector is represented \nby an indexed monad. Then that effector can be faithfully extended so that it is a principalled, idempotent, \nincreasing, centric, and further\u00ad ;; more commutative, meaning if Ee . e holds then E - -t . e holds \nfor all permutations Et of Ee. This tells us three things: the effector cannot guarantee properties that \ndepend on an effectful computation oc\u00adcurring; the effector cannot guarantee properties that depend on \nthe order of effectful computations; and the effector cannot guarantee properties that depend on the \nfrequency of effectful computations. First, we know that an effector represented by an indexed monad \ncannot have effects that guarantee an effectful computation occurs, since all effects are centric. In \nCrit it is important to distinguish computations that must lock from those that might lock in order to \nensure shared-memory accesses occur in critical regions. In partic\u00adular, the effector must not confuse \na pure or empty computation for one that de.nitely locks, so that whatever effect e used to identify \nde.nite locking cannot satisfy e . e and so cannot be centric. As such, indexed monads cannot be used \nfor effectors that guarantee locking or guarantee responsiveness (e.g. a server always produc\u00ading a response \nfor each request). Second, we know that an effector represented by an indexed monad cannot guarantee \nproperties dependent on the order of oper\u00adations, since the effector is commutative. In Crit it is important \nto distinguish accessing shared memory after acquiring a lock from accessing shared memory before acquiring \na lock. Yet, if the effec\u00adtor were commutative, then locking ; critical would necessarily equal critical \n; locking so such computations would be indistinguishable in the effector. As such, indexed monads cannot \nbe used for effectors that need order sensitivity. Third, we know that an effector represented by an \nindexed monad cannot guarantee properties dependent on the frequency of operations, since each effect \nis idempotent. In Crit it is important to distinguish acquiring a lock once from acquiring a lock multiple \ntimes since Crit prevents reentrant code. However, if locking were idempotent, then locking ; locking \nwould have to equal locking so that multiple acquisitions would appear the same as a single acquisition. \nAs such, indexed monads cannot be used for effectors that want to track the quantity of effectful operations. \nThese limitations of indexed monads were part of why we en\u00addeavored to .nd a more general framework. \nThe discussion here illustrates that, not only are there advantages to having a more gen\u00aderal semantic \nframework, but our formalizations provide a useful language for discussing effectors, another reason \nwhy we began our investigations. It also bears some importance for effect analyses, since our .ndings \nindicate that lattice-based effectors have some severe limitations in what they can determine about a \nprogram. We hope these insights will encourage investigations into new analyti\u00adcal techniques and new \nsemantic techniques. Layered Monads Filinski introduced the semantic technique of ' ' ' layered monads \n[5]. A layering of a monad (M , map , unit , join ' )over another monad (M, map, unit, join) is a natural \ntransforma\u00adtion layer : M . M ' . M ' satisfying certain equational prop\u00aderties. layer looks very much \nlike our join operation for effects T and . should . ; T . T hold. Indeed, Filinski s equational requirements \nfor layerings are simply our equivalent requirements for productoids of an increasing centric idempotent \neffectoid, and Filinski uses the layer operation to sequence a . computation fol\u00adlowed by a T computation, \nas our framework would do with the corresponding join operation. In more recent work [6], Filinski allows \nusers to build a tree of monadic subeffects, with non-termination at the root of the tree, so that programmers \ncould build their own effect system. Any effect system designed in such a way is guaranteed to be an \nincreasing centric idempotent effector and so has many of the key limitations that we discussed for indexed \nmonads. However, there is one signi.cant difference: indexed monads necessarily correspond to total effectors \nwhereas Filinski s tree of subeffects may not be  total. An effector is total if for every list of effects \nEe there is some effect e such that Ee . e holds. This is not necessarily a limitation -;of Filinski \ns approach, though. It has the advantage that it can express user effects that are incompatible with \neach other. This is an important property for a modular language where users should be able to design \ntheir effects without worrying about what other effects may be present elsewhere in the system should \nthey never meet each other. Parameterized Monads Atkey made the observation that many effect systems \nhave to do with transitioning from one state to an\u00adother [2]. Indeed, most of the effects in Crit are \nused to indicate the required incoming state of the lock and the guaranteed outgoing state of the lock. \nAs such, Atkey designed a parameterized monad to address this common case, and showed how parameterized \nmon\u00adads can formalize a variety of concepts such as composable contin\u00aduations [4, 37]. A parameterized \nmonad is (equivalently) a category S of states and primitive transitioning operations  a functor (T, \nmap) : Sop \u00d7 S . (Sem . Sem)  an extranatural transformation unit : .s. Id . T (s, s)  an extranatural \ntransformation  ' '' ' ' '' '' join : .s, s , s . T (s, s ) . T (s , s ) . T (s, s ) satisfying equational \nrequirements we do not repeat here. This should look slightly familiar. In fact, the productor cor\u00adresponding \nto just the impure effects of Crit is a parameterized monad. The category S is just the discrete category \nwith objects free and occupied. The extranaturality conditions hold automati\u00adcally since the category \nis discrete. Now, it is important to note that a parameterized monad does two things simultaneously: \nit speci.es how to sequence computa\u00adtions and it speci.es the semantics of primitive operations. For \nex\u00adample, our semantics for acquire and release could be adapted into a parameterized monad for the free \ncategory generated by the graph with two objects and a morphism between them in each direction. However, \nthe intent of our framework is only to formalize sequen\u00adtial composition of computations, so we focus \non a restricted sub\u00adset of parameterizations. In particular, we focus on parameterized monads where S \nis thin, meaning there is at most one morphism between any two states. Such an S is actually a preorder, \nand so represents states with a substate relation. Given such a set of states S and a substate preorder \n=, de.ne an effectoid E(S,=) as follows: EFF is S \u00d7 S, so that the effect (s, s ' ) represents computations \nthat transition from state s to state s ' .  e . (s, s ' ) holds iff s = s ' holds.  '' '' (s1, s2) \n= (s1, s2) holds iff s1 = s1 and s2 = s2 hold. ''' '' (s1, s2) ;(s2, s3) . (s1, s3) holds iff s1 = s1, \ns2 = s2, and s3 = s3 ' hold. Due to their de.nition, all effectors formalizable by parameterized monads \nare of this form. Theorem 4. Given a set of states S and a substate preorder =, there is a one-to-one \ncorrespondence between parameterized monads for the category (S, =) and productoids for the effectoid \nE(S,=). Proof. Given a parameterized monad (T, map, unit, join) for (S, =), de.ne the productoid for \nE(S,=) as follows: ' (P (s,s1), map(s,s1)) = T (s, s ) units map(s=s,s=s 1) unit(s,s1) = Id ---. T (s, \ns) - --------. T (s, s ' ) coerce(s1 ,s2),(s1,s1 ) = map(s1 ' = s1, s2 = s2 ' ) 1 2 join(s11 1 = ,s2),(s,s3),(s1,s) \n12 3 T (s1 ' , s2) . T (s2 ' , s3) ' '' map(s1 = s1, s2 = s2) * map(s2 = s2, s3 = s3) . T (s1, s2) . \nT (s2, s ' 3) join 1 . s1,s2,s 3 T (s1, s3 ' ) The necessary equivalences hold due to the equational \nrequire\u00adments of parameterized monads. Given a productoid (P, map, unit, coerce, join) for E(S,=), de.ne \nthe parameterized monad for (S, =) as follows: T (s, s ' ) = (P(s,s1), map(s,s1)) map(s ' 1 = s1, s2 \n= s ' 2) = coerce(s1 ,s2),(s1,s1 ) 1 2 units = unit(s,s) joins,s1,s11 = join(s,s1),(s1,s11),(s,s11) The \nnecessary equivalences hold to due to the equational require\u00adments of productoids. These two processes \nare clearly inverses of each other. Now that we have seen how parameterized monads .t within our framework, \nlet us consider the expressiveness of parameterized monads. First, a parameterized monad is not a family \nof monads. While T (s, s) forms a monad since the input and output are the same, this is not the case \nfor T (s, s ' ) whenever s and s ' are not equivalent to each other. This is because e . (s, s ' ) holds \nonly when s is a substate of s ' and (s, s ' ) ;(s, s ' ) . (s, s ' ) holds only when s ' is a substate \nof s. So, the effectoid E(S,=) is neither centric, idempotent, nor increasing in general. Thus the very \nnotion of distinct states is inexpressible by indexed monads and layered monads, indicating how powerful \nparameterized monads are. Parameterized monads have their limitations, though. For ex\u00adample, information-.ow \neffect systems [29] and contextual ef\u00adfects [21] do not meet the requirements of Theorem 4 and so cannot \nbe formalized using parameterized monads. Interestingly, though, should we allow EFF for E(S,=) to be \njust a subset of S \u00d7 S rather than all pairs of states, then we can represent information-.ow ef\u00adfect \nsystems and contextual effects. For example, in [29] the states are levels of secrecy, so EFF contains \nonly those pairs (s, s ' ) where s is a lower level of secrecy than s ' , since inputs can be propagated \nto outputs. Thus, Theorem 4 suggests such effect systems must be formalizable by something very similar \nto a parameterized monad. Even such generalizations are still too restrictive for some ef\u00adfectoids. For \nexample, the full effectoid for Crit including the basic effect e cannot be formalized by a parameterized \nmonad. The issue is that, ignoring substates since they are orthogonal to the following concerns, a parameterized \nmonad can only have one effect between any two states. However, both e and critical computations are \nper\u00admitted when the lock is acquired and .nish with the lock acquired. If we wanted to prevent race conditions, \nthen we would want to distinguish e and critical computations since we should allow critical computations \nbe ran in parallel with e computations but not with other critical computations. Furthermore, if we wanted \nto track other effectful attributes that are state agnostic, such as non-determinism, then we would need \nmultiple effects between any two states. Even just an indexed monad can be viewed as having many effects \nbe\u00adtween a single state, showing why non-trivially indexed monads cannot be formalized by parameterized \nmonads. Thus, while pa\u00adrameterized monads are expressive, there are still useful effectoids inexpressible \neven by generalized parameterized monads but still expressible by our framework.  Arrows and Freyd Categories \nArrows [8], another generalization of monads, are a little dif.cult to discuss because they are simulta\u00adneously \ntoo general and too restrictive. First, they have three com\u00adponents, which we describe informally: a \nway to sequentially compose arrows  a way to turn a pure computation into an arrow  a way to propagate \nadditional context through an arrow  These three components must satisfy equalities which essentially \nindicate that the arrows form a category, the pure computations form a pure subcategory, and a pure computation \ncan be executed alongside an arrow without altering the overall effect of the arrow. In fact, the above \nintuition behind the equational properties has been formalized. Heunan and Jacobs proved that arrows \nare equiv\u00adalent to Freyd categories [10], an attempt to formalize arbitrary effectful computations. Informally, \na Freyd category [26] is a pre\u00admonoidal category [25] (a category with a notion of extending mor\u00adphisms \nto propagate context) with a wide cartesian monoidal sub\u00adcategory of pure morphisms that can be executed \nalongside other morphisms without altering their overall effect. Note that an important aspect of the \nabove descriptions is the notion of extending and propagating context. Herein lies our pri\u00admary criticism \nof arrows and Freyd categories. What we deter\u00admined is that nearly all of their structure has to do with \nextending and propagating context. If one removes the components that are present to serve those roles \n(i.e. anything using .), what remains is just a category of effectful computations with a wide subcategory \nof pure computations. That is, arrows and Freyd categories say noth\u00ading about sequencing effectful computations \nbesides the fact that sequencing pure computations results in a pure computation (not to mention they \nonly handle one effect). So, while arrows generalize strong monads, they do so at the cost of discarding \nmost of the use\u00adful structure to work with. It is for this reason that we focused on producer effects; \nour investigations suggest that being as general as possible means not providing any structure beyond \na category with a distinguished subcategory. 8. Generality The intent of this work is not to generalize \nprior semantic frame\u00adworks for effects, but to design the most general such framework possible. However, \nas we just mentioned while discussing arrows, the most general framework possible is not very useful. \nAs such, we focused on producer effects and de.ned productors for formal\u00adizing producer effect systems. \nHere we show how a few language properties guarantee that the semantics of sequential composition forms \na productor. Figure 6 extends the language assumptions made in Figure 4. Again, while we assume these \nrules can be imposed upon the language at hand, we expect the language will have a different syntax and \nmore rules in addition to those we assume. Figure 6 introduces two new judgements. The judgement G f \np -G ' indicates that p is a pure program with input G and output G ' for some notion of purity. In prior \nwork, this separa\u00adtion usually is termed as values versus computations. For example, OCaml restricts \ntype generalization to values rather than arbitrary computations [39]. However, not all notions of purity \nmay be re\u00adstricted to values. For example, Haskell s notion of purity includes exceptions and non-termination. \nHere we let the designer determine their own notion of purity. Then we will show how effectful com\u00adputations \ncan be expressed by a productor on pure computations for whatever notion of purity the designer decided \nupon. The other new judgement f p e G1 p ' indicates that p and p ' , G viewed as e-effectful computations \nfrom G to G ', are semantically equivalent. When e is absent, then they are semantically equivalent G0 \nf p1 -G1 G f p -G ' ' -G '' . . . G ' f p | e G '' '' -G ''' Gn-1 f pn -Gn f p Seq Seq e G0 f p1; . . \n. ; pn -Gn G f p; p ' ; p '' -G ''' | e G f p -G ' | e Thunk Exec G f LpJG ' wG.e f exece -G | e e -w.e \nG ' G f p -G ' | e G f p -w Eq\u00df Eq. .e f LpJ; exece e p f Lp; execeJe p e G G1 G .G1De Figure 6. Typing \nand semantic rules that guarantee productors when viewed as pure computations. While not explicitly stated \nin Figure 6, we assume semantic equivalence is congruent with re\u00adspect to sequential composition, implicit \ncoercion of pure compu\u00adtation into effectful computation, and thunking. Figure 6 also introduces three \nnew meta-operations. The op\u00aderation L-Je maps an e-effectful program to a pure thunked ver\u00adsion of that \nprogram, delaying its execution and treating it as a value. In OCaml, given an (e-effectful) expression \ne, LeJe would be fun () -> e, effectively delaying the evaluation of e by turn\u00ading it into a function \nwaiting for an input, namely (). The opera\u00adtion w-.e maps a context G to the context representing a thunked \ne-effectful computation that would produce G whenever it is exe\u00adcuted, i.e. the output of a thunked computation \nLpJe. In OCaml, would be unit -> t . Lastly, the operation exece is the pro\u00ad wt .e gram that .nally executes \na thunked e-effectful computation. In OCaml, exece would be - (), i.e. the program .nally passing () \nto the thunked computation of the form fun () -> e. Note that, while our OCaml example de.nes the operations \nby wrapping the relevant program or context, other examples may be done by a more in-depth translation. \nFor example, if a language has sum types, then thunking an exception-throwing program can be done by \nrecur\u00adsively translating the entire program to inject left or right instead of throwing an exception \nand to use pattern matching to explic\u00aditly propagate the exception. Thus a language does not have to \nbe higher order in order to satisfy the requirements of Figure 6. The rules in Figure 6 formalize a number \nof language properties, some of which are obvious, and some of which are fundamental to the notion of \nproducer effects. Seq indicates that pure computations are closed under sequential composition. Note \nin particular that the empty computation is pure. Seqe indicates that sequential composi\u00adtion of effectful \ncomputations with pure computation preserves the effect of a computation. This formalizes the idea that \nthe effect of a computation is not dependent on the values of its inputs or outputs. Note that these \nrules combined with using the empty computation for Seq; admit the following: G f p -G ' e . e G f p \n-G ' | e Thunk and Exec formally evidence the notion of producer ef\u00adfects. First, any effectful computation \ncan be thunked into a pure computation by modifying only the output in a uniform manner. Second, there \nis a similarly effectful computation that executes a thunked computation. Eq\u00df and Eq. essentially correspond \nto \u00df\u00adand .-equivalence for these constructs. Note that these are not the only rules of the language; \nthey are simply properties that may be superimposed upon a language. In particular, there can be (and \ntypically are) many rules that operate on contexts of form wG.e so that exece is not necesarilly the \nonly operation that can be performed on a thunked computation. Nonetheless, regardless of what additional \nrules the language at hand may have, so long as it admits at least the rules in Figures 4 and 6 then \nit is guaranteed to be an instance of our framework.  With these we can .nally present our fundamental \ntheorem: Theorem 5. If a language with an effector (EFF, .) admits -;at least the rules in Figures 4 \nand 6, then there is a productor (- , map, join) using pure computations modulo semantic equiv\u00ad w alence \nas Sem, such that the language admits the following: G0 f p1 -G1 | e1 . . . Gn-1 f pn -Gn | en [e1, . \n. . , en] -; . e Lp1; . . . ; pnJLp1J; mape1(. . . (LpnJ e G0 Ge1en)) ; join[e1,...,en],e nDe Proof. \nDe.ne the productor as follows: Pe(G) is already de.ned as G e w mape(p) = Lexece; pJe join[e1,...,en],e \n= Lexece1 ; . . . ; execen Je In the technical report [33], using Eq\u00df, Eq., and congruence. Here we prove \nonly the desired semantic property of sequential compo\u00adsition, using approximately the same proof strategy. \nFirst, there is an important lemma: given pure programs p and p ' from G to G ', if p; exece is semantically \nequivalent to p ' ; exece, then p and p ' are semantically equivalent. The lemma assumptions and congruence \nimply that Lp; execeJe is semantically equivalent to Lp ' ; execeJe. Eq. then tells us that the former \nis semantically equivalent to p and the latter to p '. Thus, by transitivity, p and p ' are semantically \nequivalent. Now, to prove that Lp1; . . . ; pnJe is semantically equivalent to Lp1Je1 ; mape1 (. . . \n(LpnJen )); join[e1,...,en],e, due to our lemma we can instead prove Lp1; . . . ; pnJe; exece is semantically \nequiv\u00adalent to Lp1Je1 ; mape1 (. . . (LpnJen )); join[e1,...,en],e; exece. So, Lp1; . . . ; pnJe; exece \nis semantically equivalent to p1; . . . ; pn by Eq\u00df. From Eq\u00df and the de.nition of join, join[e1,...,en],e; \nexece is semantically equivalent to exece1 ; . . . ; execen . From Eq\u00df and the de.nition of map, mapei \n(p); execei is semantically equivalent to execei ; p. Using that fact and Eq\u00df , we know that LpiJ; mapei \n(. . . (LpnJ)); execei ; . . . ; execen is equivalent ei en to pi;Lpi+1J; mapei+1(. . . (LpnJ)); execei+1 \n; . . . ; execen . ei+1 en Via induction, Lp1J; mape1(. . . (LpnJ)); exece1 ; . . . ; execen e1 en is \nequivalent to p1; . . . ; pn. Taking advantage of congruence, then Lp1Je1 ; mape1 (. . . (LpnJen )); \njoin[e1,...,en],e is also equivalent to p1; . . . ; pn. Therefore the desired semantic property holds. \nExample Our toy language Crit does not actually satisfy the re\u00adquirements of Figure 6. This illustrates \nthat Figure 6 is suf.cient but not necessary for our framework, demonstrating that our frame\u00adwork is \nuseful even for formalizing languages without a notion of thunks. Nonetheless, for sake of illustration, \nwe show how we might extend Crit so that it models Figure 6. A suf.cient extension of Crit is shown in \nFigure 7. Semantically we assume the usual \u00df-and .-equivalences. The .rst extension is a singleton type \nUnit. The second and more important extension is effectful function types. In particular, t -eG represents \nan . e-effectful function accepting a t as input and during execution assigning values of the appropriate \ntypes to the variables in G. Thus this extension is like the computational lambda calculus with multiple \neffects mixed with a register-based language. With this language we can de.ne the required judgements \nand operations in Figure 6 as follows: e G f p -G ' = G f p -G ' | e G e = t : Unit . G- w LpJ= t := \n(.u.() := u; p) exece = u := (); t u e That is, a pure computation is one with the basic effect e. Thunking \nworks by turning the computation into a function waiting for a G, x : t f p -G ' | e . G ' \u00d8 f u := () \n-u : Unit | e G f f := .x.p -f : (t -e) | e u : Unit f () := u -\u00d8 | e f : (t -e-f x f G | e . G), x : \nt Figure 7. Crit extended with effectful functions Unit value, eliminating that unit value, and then \n.nally running the computation. That function is stored into variable t so that thunking results in an \noutput context of the form t : Unit -e . G. Executing a thunked computation, then, simply involves storing \nthe unique value of Unit into a variable and then calling the thunk with that variable as the argument, \nthus causing the body of the function to .nally execute. These de.nitions clearly satisfy the requirements \nof Figure 6. Thus, our fundamental theorem implies that the semantics of ex\u00adtended Crit can be formalized \nwith a productor de.ned on just the extended Crit computations with the basic effect e. Furthermore, \nthe theorem shows how to construct this productor. Closed Freyd Categories Now we would like to revisit \nexist\u00ading frameworks to see what insights the assumptions in Figure 6 can offer. As we discussed, while \nFreyd categories offer a lot of useful structure for working with contexts, they provide little structure \nregarding sequential composition. However, Power and Thielecke recognized that strong monads arise for \nclosed Freyd categories [27]. This is no surprise given our theorem, since the structures that makes \na Freyd category closed are essentially the assumptions in Figure 6. Similarly, Atkey de.ned a notion \nof pa\u00adrameterized Freyd categories [2], and observed that closed parame\u00adterized Freyd categories gave \nrise to parameterized monads. Again, the additional structure for a closed parameterized Freyd category \ncorresponds to the assumptions in Figure 6, so this result is no sur\u00adprise given our theorem. 9. Conclusion \nWe have presented productors, a semantic framework for sequen\u00adtial composition of computations with producer \neffects, a concept we were able to formalize abstractly. In particular, we showed why the widespread \nnotion of thunking makes producer effects so com\u00admon. In our discussions of existing frameworks, we argued \nwhy it is important to restrict our attention to producer effects. We illus\u00adtrated how monadic frameworks \n.t within productors, and used our framework to illustrate how properties of the effect system at hand \ngive rise to various semantic structures. In all this discussion, we have elided the higher categorical \ncon\u00adnections. For example, productors can be viewed as functors from the effector to the 2-category of \ncategories. This higher perspec\u00adtive suggests some ways to adapt the framework to more special\u00adized forms \nof computation. For example, by changing the target 2-category to that of premonoidal categories (i.e. \ncategories with a notion of propagating context), one arrives at productors that can propagate context, \nsuch as a strong monad. Or, by using the 2-category of categories and partial functors, one can drop \nthe im\u00adplicit assumption of Theorem 5 that G e is de.ned for all con\u00ad w texts G even if there is no e-effectful \ncomputation with G as its out\u00adput. In another direction, while we chose to present effectors along the \nline of lax List-monad algebras in Rel, they can equivalently be described as thin multicategories [15]. \nIt would be interesting to in\u00advestigate how the concepts for multicategories translate to effectors and \nproductors. For example, a representable thin multicategory is equivalently a total principalled effectoid \n(equivalent to a congru\u00adently preordered monoid).  Our preliminary investigations into the dual concept \nof con\u00adsumer effects and consumptors have been very intriguing. The tra\u00additional notion of context seems \nto be well described as a consump\u00adtor. Strength appears to be formalizable as an interplay of this con\u00adsumer \neffect with the producer effects. Similarly, non-linear uses of inputs also seem best described as a \nconsumptor. After all, an intuitionistic implication P . Q translates to linear implication !P -Q with \na modi.cation on the input rather than the output. The ! modality is a comonad, which is a special class \nof consump\u00adtors just like monads are a special class of productor. Furthermore, it seems that strictness \nand laziness arise as two dual ways to make such a consumptor interact with producer effects. Finally, \nwhile we have discussed what the semantic framework for producer effects should look like, we have not \ninvestigated how to actually build productors. With monads, there have been a va\u00adriety of techniques \nfor composing monads [11] (though interest\u00adingly many of these, such as distributive laws [14], arise \nas spe\u00adcial cases of our framework), building monads from monad trans\u00adformers [16], or combining algebraic \nmonads [24] with tensors and sums [9]. We would like to see how these concepts extend to pro\u00adductors. \nFor example, the coproduct of two monads is relatively simple to de.ne [18], but at .rst thought it is \nnot clear whether the coproduct of two effectors and respective productors would be simpler or more complex. \nAlso, transformers such as for exceptions rely heavily on the presence of a unit operation, so one wonders \nif they are restricted to just centric effectors. By reexamining these techniques in this new light, \nwe expect to acquire a better under\u00adstanding of their fundamental structure. With this framework we aim \nto lay new grounds for the foun\u00addations of programming languages. Of course, there are still many more \nforms of composition to formalize the semantics of, but we have already begun transferring the strategy \ntaken here to those settings and found some fruitful results. In the end we expect to have composable \nframeworks for formalizing the many roles ef\u00adfects have in programming languages. We hope this work will \npro\u00advide a new means to abstractly formalize, expand, and communi\u00adcate programming languages. Acknowledgements \nIn addition to our anonymous reviewers, we thank John C. Baez, Thomas Ball, Daniel Brown, Jeffrey S. \nFoster, Michael Hicks, Ohad Kammar, Daan Leijen, Sorin Lerner, Daniel Marino, Andrew Myers, Patrick Rondon, \nMichael Shulman, and Zachary Tatlock for their valuable feedback on the research, its context, and our \nwriting. References [1] Mart\u00b4in Abadi. Access control in a core calculus of dependency. In ICFP, 2006. \n[2] Robert Atkey. Parameterised notions of computation. JFP, 19:335 376, July 2009. [3] Nick Benton, \nJohn Hughes, and Eugenio Moggi. Monads and effects. In International Summer School on Applied Semantics, \n2000. [4] Olivier Danvy and Andrzej Filinski. A functional abstraction of typed contexts. Technical report, \nUniversity of Copenhagen, 1989. [5] Andrzej Filinski. Representing layered monads. In POPL, 1999. [6] \nAndrzej Filinski. Monads in action. In POPL, 2010. [7] Roger Godement. Topologie Alg \u00b4ebrique et Th\u00b4eorie \ndes Faisceaux. Hermann, 1958. [8] John Hughes. Generalising monads to arrows. Science of Computer Programming, \n37(1-3):67 111, May 2000. [9] Martin Hyland, Gordon Plotkin, and John Power. Combining effects: Sum and \ntensor. Theoretical Computer Science, 357(1):70 99, 2006. [10] Bart Jacobs, Chris Heunen, and Ichiro \nHasuo. Categorical semantics for arrows. JFP, 19:403 438, 2009. [11] Mark P. Jones and Luc Duponcheel. \nComposing monads. Technical report, Yale University, New Haven, CT, USA, December 1993. [12] Simon Peyton \nJones and Philip Wadler. Imperative functional pro\u00adgramming. In POPL, 1993. [13] Richard B. Kieburtz. \nTaming effects with monadic typing. In ICFP, 1998. [14] David J. King and Philip Wadler. Combining monads. \nIn ETAPS, 1992. [15] Tom Leinster. Higher Operads, Higher Categories. Cambridge Uni\u00adversity Press, 2004. \n[16] Sheng Liang, Paul Hudak, and Mark Jones. Monad transformers and modular interpreters. In POPL, 1995. \n[17] John M. Lucassen and David K. Gifford. Polymorphic effect systems. In POPL, 1988. [18] Christoph \nL \u00a8uth and Neil Ghani. Composing monads using coproducts. ACM SIGPLAN Notices, 37(9):133 144, 2002. [19] \nDaniel Marino and Todd Millstein. A generic type-and-effect system. In TLDI, 2009. [20] Eugenio Moggi. \nComputational lambda-calculus and monads. In LICS, 1989. [21] Iulian Neamtiu, Michael Hicks, Jeffrey \nS. Foster, and Polyvios Pratikakis. Contextual effects for version-consistent dynamic software updating \nand safe concurrent programming. In POPL, 2008. [22] Flemming Nielson and Hanne Riis Nielson. Type and \neffect systems. In ACM Computing Surveys, 1999. [23] Hanne Riis Nielson, Flemming Nielson, and Torben \nAmtoft. Polymor\u00adphic subtyping for effect analysis: The static semantics. In LOMAPS, 1997. [24] Gordon \nPlotkin and John Power. Notions of computation determine monads. In FoSSaCS, 2002. [25] John Power and \nEdmund Robinson. Premonoidal categories and no\u00adtions of computation. Mathematical Structures in Computer \nScience, 7:453 468, October 1997. [26] John Power and Hayo Thielecke. Environments, continuation seman\u00adtics \nand indexed categories. In TACS, 1997. [27] John Power and Hayo Thielecke. Closed Freyd-and .-categories. \nIn ICAL, 1999. [28] John C. Reynolds. Using category theory to design implicit conver\u00adsions and generic \noperators. LNCS, 94:211 258, 1980. [29] Nikhil Swamy, Nataliya Guts, Daan Leijen, and Michael Hicks. \nLightweight monadic programming in ML. Technical report, Mi\u00adcrosoft Research, 2011. [30] Jean-Pierre \nTalpin. Theoretical and Practical Aspects of Type and Effect Inference. PhD thesis, Ecole des Mines de \nParis and University \u00b4 Paris VI, Paris, France, 1993. [31] Jean-Pierre Talpin and Pierre Jouvelot. Polymorphic \ntype, region and effect inference. JFP, 2:245 271, 1992. [32] Jean-Pierre Talpin and Pierre Jouvelot. \nThe type and effect discipline. Information and Computation, 111(2):245 296, 1994. [33] Ross Tate. The \nsequential semantics of producer effect systems. Technical report, Cornell University, 2012. [34] Andrew \nP. Tolmach. Optimizing ML using a hierarchy of monadic types. In Types in Compilation, 1998. [35] Philip \nWadler. Comprehending monads. In LISP and Functional Programming, 1990. [36] Philip Wadler. The essence \nof functional programming. In POPL, 1992. [37] Philip Wadler. Monads and composable continuations. LISP \nand Symbolic Computation, 7:39 56, January 1994. [38] Philip Wadler and Peter Thiemann. The marriage \nof effects and monads. Transactions on Computational Logic, 4(1):1 32, 2003. [39] Andrew K. Wright. Simple \nimperative polymorphism. LISP and Symbolic Computation, 8(4):343 355, 1995.  \n\t\t\t", "proc_id": "2429069", "abstract": "<p>Effects are fundamental to programming languages. Even the lambda calculus has effects, and consequently the two famous evaluation strategies produce different semantics. As such, much research has been done to improve our understanding of effects. Since Moggi introduced monads for his computational lambda calculus, further generalizations have been designed to formalize increasingly complex computational effects, such as indexed monads followed by layered monads followed by parameterized monads. This succession prompted us to determine the most general formalization possible. In searching for this formalization we came across many surprises, such as the insufficiencies of arrows, as well as many unexpected insights, such as the importance of considering an effect as a small component of a whole system rather than just an isolated feature. In this paper we present our semantic formalization for producer effect systems, which we call a productor, and prove its maximal generality by focusing on only sequential composition of effectful computations, consequently guaranteeing that the existing monadic techniques are specializations of productors.</p>", "authors": [{"name": "Ross Tate", "author_profile_id": "81392610098", "affiliation": "Cornell University, Ithaca, NY, USA", "person_id": "P3977907", "email_address": "ross@cs.cornell.edu", "orcid_id": ""}], "doi_number": "10.1145/2429069.2429074", "year": "2013", "article_id": "2429074", "conference": "POPL", "title": "The sequential semantics of producer effect systems", "url": "http://dl.acm.org/citation.cfm?id=2429074"}