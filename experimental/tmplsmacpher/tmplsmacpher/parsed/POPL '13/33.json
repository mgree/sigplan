{"article_publication_date": "01-23-2013", "fulltext": "\n The Principles and Practice of Probabilistic Programming Noah D. Goodman Stanford University ngoodman@stanford.edu \nCategories and Subject Descriptors D[3]: m Keywords probabilistic models, probabilistic programs Probabilities \ndescribe degrees of belief, and probabilistic infer\u00adence describes rational reasoning under uncertainty. \nIt is no won\u00adder, then, that probabilistic models have exploded onto the scene of modern arti.cial intelligence, \ncognitive science, and applied statis\u00adtics: these are all sciences of inference under uncertainty. But \nas probabilistic models have become more sophisticated, the tools to formally describe them and to perform \nprobabilistic inference have wrestled with new complexity. Just as programming beyond the simplest algorithms \nrequires tools for abstraction and composition, complex probabilistic modeling requires new progress \nin model representation probabilistic programming languages. These lan\u00adguages provide compositional means \nfor describing complex prob\u00adability distributions; implementations of these languages provide generic \ninference engines: tools for performing ef.cient probabilis\u00adtic inference over an arbitrary program. \nIn their simplest form, probabilistic programming languages extend a well-speci.ed deterministic programming \nlanguage with primitive constructs for random choice. This is a relatively old idea, with foundational \nwork by Giry, Kozen, Jones, Moggi, Saheb-Djahromi, Plotkin, and others [see e.g. 7]. Yet it has seen \na resur\u00adgence thanks to new tools for probabilistic inference and new com\u00adplexity of probabilistic modeling \napplications. There are a number of recent probabilistic programming languages [e.g. 8, 9, 11 17], embodying \ndifferent tradeoffs in expressivity, ef.ciency, and per\u00adspicuity. We will focus on the probabilistic \nprogramming language Church [6] for simplicity, but the design of probabilistic languages to best support \ncomplex model representation and ef.cient infer\u00adence is an active and important topic. Church extends \n(the purely functional subset of) Scheme with elementary random primitives, such as .ip (a bernoulli), \nmultino\u00admial, and gaussian. In addition, Church includes language con\u00adstructs that simplify modeling. \nFor instance, mem, a higher-order procedure that memoizes its input function, is useful for describing \npersistent random properties and lazy model construction. (Inter\u00adestingly, memoization has a semantic \neffect in probabilistic lan\u00adguages.) If we view the semantics of the underlying deterministic language \nas a map from programs to executions of the program, the semantics of the probabilistic language will \nbe a map from programs to distributions over executions. When the program halts Copyright is held by \nthe author/owner(s). POPL 13, January 23 25, 2013, Rome, Italy. ACM 978-1-4503-1832-7/13/01. 1 2 3 4 \n5  1 1 2 2 3 3 4 4 5 Figure 1. (Top) De.ning conditional inference in Church as a stochastic recursion: \nrejection sampling represents the conditional probability of the thunk conditioned on the condition predicate \nbeing true. We typically use special query syntax (Bottom, left), which can be desugared into a query \nthunk (Bottom, right). with probability one, this induces a proper distribution over return values. Indeed, \nany computable distribution can be represented as the distribution induced by a Church program in this \nway (see [3, \u00a76], [1, \u00a711], and citations therein). Probabilistic graphical models [10], aka Bayes nets, \nare one of the most important ideas of modern AI. Probabilistic programs extend probabilistic graphical \nmodels, leveraging concepts from programming language research. Indeed, graphical models can be seen \nas .ow diagrams for probabilistic programs and just as .ow diagrams for deterministic programs are useful \nbut not powerful enough to represent general computation, graphical models are a useful but incomplete \napproach to probabilistic modeling. For an example of this, we need look no further than the fundamental \noperation for inference, probabilistic conditioning, which forms a posterior distribution over executions \nfrom the prior distribution speci.ed by the program. Conditioning is typically viewed as a special operation \nthat happens to a probabilistic model (capturing observations or assumptions), not one that can be expressed \nas a model. However, because probabilistic programs allow stochastic recursion, conditioning can be de.ned \nas an ordinary probabilistic function (Fig. 1, Top). (However see [1] for complications in the case of \ncontinuous values.) A wide variety of probabilistic models are useful for diverse tasks, including unsupervised \nlearning, vision, planning, and statis\u00adtical model selection. Due to space limitations, we only mention \ntwo characteristic examples here. The .rst, Fig. 2, captures key concepts for commonsense reasoning about \nthe game tug-of-war. This conceptual library of probabilistic functions can be used to reason about many \npatterns of evidence, via different queries, with\u00adout needing to specify ahead of time the set of people, \nthe teams, or the matches. The program thus enables a very large numbers of different inferences to be \nmodeled, and the model predictions match human intuitions quite well (a correlation of 0.98 between Figure \n2. Modeling intuitive concepts in the tug-of-war domain. While this model is simple, probabilistic queries \ncan explain human reasoning from diverse evidence with high quantitative accuracy [4].  model and human \njudgements in the experiments of [4]). The abil\u00adity to model many inferences is inherited from the productivity \nof the underlying programming language, while the ability to capture complex, graded commonsense reasoning \nis inherited from proba\u00adbilistic inference. A more subtle model is shown in Fig. 3. Here we model prag\u00admatic \ninference in natural language following [2, 5, 18]. Critically, because query is an ordinary function \nthat may be nested in itself, we are able to model a listener reasoning about a speaker, who rea\u00adsons \nabout a naive listener. This model formalizes the idea that a listener is trying to infer what the speaker \nintended, while a speaker is trying to make the listener form a particular belief. Versions of this model \nagain predict human judgements with high quantitative accuracy [2, 5]. The critical obstacle to probabilistic \nprogramming as a practical tool is ef.cient implementation of the inference operator. It is clear that \nthe expected runtime for the rejection-query operator in Fig. 1 increases as the probability of satisfying \nthe condition decreases. Hence, while rejection is useful as a de.nition, it is impractical for real \nmodeling tasks, where the condition is typically very unlikely. An obvious alternative that does not \ntake longer when the condition is less likely is to enumerate all execution paths of the program, for \ninstance by using co-routines. Unfortunately, naive enumeration takes time proportional to the number \nof executions, which grows exponentially in the number of random choices. This can be ameliorated in \nsome cases by using dynamic programming. Indeed it is possible to do inference for the simple pragmatics \nmodel in Fig. 3 by using a sophisticated dynamic programming approach [18]. For other classes of program, \nsuch as the tug-of-war model, Fig. 2, it can be better to switch to an approximate inference technique \nbased on Markov chain Monte Carlo. One such algorithm [6] can be derived from the Metropolis Hastings \nrecipe: a random change to a single random choice is made on each step, leading to a random walk in the \nspace of program executions. This walk is guaranteed to visit executions in proportion to their probability. \nWe individuate random choices using their position in the execution this can be achieved by a code transformation, \nresulting in a very lightweight inference process [20]. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 \n19 20 Figure 3. A probabilistic model of natural language pragmatics using nested-query to model reasoning-about-reasoning. \nEach step of this MCMC process requires re-executing the pro\u00adgram, in order to compute updated probabilities. \nSeveral orders of magnitude in speed increases are seen by just-in-time com\u00adpiling this probability update \n[21]. To do so, we separate struc\u00adtural random choices (those that can in.uence control .ow) from non-structural. \nWhen we encounter a new setting of the structural choices we partially-evaluate the program to straight-line \ncode, which is aggressively optimized. Structural choices can sometimes be found automatically by .ow \nanalysis, but in other instances must be annotated by hand. The distinction between structural and non-structural \nchoices can also be used to construct an algorithm that more effectively explores the space of executions \nfor open universe models those that de.ne distributions over a variable number of entities [22]. This \nclass of programs is especially well-suited to complex procedural modeling in computer graphics. For \ninstance, Yeh et al. [22] describe models for layout of virtual worlds. These procedural models typically \nhave many real-valued random choices. In this setting Hamiltonian Monte Carlo, which exploits the gradient \nof the score to make more well-directed steps in execution space, can be a useful additional tool. The \nneeded gradient can be derived automatically by a non-standard interpretation of the program [19]. The \nsampling of inference algorithms described above suggests an important generalization: advanced statistical \ninference tech\u00adniques can be implemented for arbitrary programs by leveraging methods developed in programming \nlanguage research (program analysis, non-standard interpretation, partial evaluation, etc.). Ex\u00adisting \nresearch only scratches the surface of potential interactions between statistics, AI, and PL. Though \ntechnically demanding, this interaction may be the critical ingredient in bringing complex prob\u00adabilistic \nmodeling to the wider world.  Acknowledgments Thanks to Dan Roy, Cameron Freer, and Andreas Stuhlm \u00a8uller \nfor helpful comments. This work was supported by ONR grant N00014-09-0124.  References [1] N. Ackerman, \nC. Freer, and D. Roy. Noncomputable conditional distributions. In Logic in Computer Science (LICS), 2011 \n26th Annual IEEE Symposium on, pages 107 116. IEEE, 2011. [2] M. Frank and N. Goodman. Predicting pragmatic \nreasoning in language games. Science, 336(6084):998 998, 2012. [3] C. E. Freer and D. M. Roy. Computable \nde Finetti measures. Annals of Pure and Applied Logic, 163(5):530 546, 2012. doi: 10.1016/j.apal.2011.06.011. \n[4] T. Gerstenberg and N. D. Goodman. Ping pong in Church: Productive use of concepts in human probabilistic \ninference. In Proceedings of the 34th annual conference of the cognitive science society, 2012. [5] N. \nGoodman and A. Stuhlm \u00a8uller. Knowledge and implicature: Modeling language understanding as social cognition. \nTopics in Cognitive Science, 2013. [6] N. Goodman, V. Mansinghka, D. Roy, K. Bonawitz, and J. Tenenbaum. \nChurch: A language for generative models. In In UAI, 2008. [7] C. Jones and G. Plotkin. A probabilistic \npowerdomain of evaluations. In Logic in Computer Science (LICS), 1989 4th Annual IEEE Symposium on, pages \n186 195, Jun 1989. doi: 10.1109/LICS.1989.39173. [8] A. Kimmig, B. Demoen, L. De Raedt, V. S. Costa, \nand R. Rocha. On the implementation of the probabilistic logic programming language ProbLog. Theory and \nPractice of Logic Programming, 11(2-3):235 262, 2011. [9] O. Kiselyov and C. Shan. Embedded probabilistic \nprogramming. In Domain-Speci.c Languages, pages 360 384, 2009. [10] D. Koller and N. Friedman. Probabilistic \ngraphical models: principles and techniques. MIT press, 2009. [11] A. McCallum, K. Schultz, and S. Singh. \nFactorie: Probabilistic programming via imperatively de.ned factor graphs. In Neural Information Processing \nSystems Conference (NIPS), 2009. [12] B. Milch, B. Marthi, S. Russell, D. Sontag, D. L. Ong, and A. Kolobov. \nBLOG: Probabilistic models with unknown objects. In International Joint Conference on Arti.cial Intelligence \n(IJCAI), pages 1352 1359, 2005. [13] A. Pfeffer. IBAL: A probabilistic rational programming language. \nIn International Joint Conference on Arti.cial Intelligence (IJCAI),pages 733 740. Morgan Kaufmann Publ., \n2001. [14] A. Pfeffer. Figaro: An object-oriented probabilistic programming language. Charles River Analytics \nTechnical Report, 2009. [15] D. Poole. The independent choice logic and beyond. Probabilistic inductive \nlogic programming, pages 222 243, 2008. [16] M. Richardson and P. Domingos. Markov logic networks. Machine \nLearning, 62:107 136, 2006. [17] T. Sato and Y. Kameya. PRISM: A symbolic-statistical modeling language. \nIn International Joint Conference on Arti.cial Intelligence (IJCAI), 1997. [18] A. Stuhlm \u00a8uller and \nN. Goodman. A dynamic programming algorithm for inference in recursive probabilistic programs. arXiv \npreprint arXiv:1206.3555, 2012. [19] D. Wingate, N. Goodman, A. Stuhlmueller, and J. Siskind. Nonstan\u00addard \ninterpretations of probabilistic programs for ef.cient inference. In Advances in Neural Information Processing \nSystems 23, 2011. [20] D. Wingate, A. Stuhlmueller, and N. Goodman. Lightweight imple\u00admentations of probabilistic \nprogramming languages via transformational compilation. In Proceedings of the 14th international conference \non Arti.cial Intelligence and Statistics, page 131, 2011. [21] L. Yang, P. Hanrahan, and N. Goodman. \nIncrementalizing mcmc on probabilistic programs through tracing and slicing. Under review. [22] Y. Yeh, \nL. Yang, M. Watson, N. Goodman, and P. Hanrahan. Synthesizing open worlds with constraints using locally \nannealed reversible jump mcmc. ACM Transactions on Graphics (TOG),31 (4):56, 2012.  \n\t\t\t", "proc_id": "2429069", "abstract": "", "authors": [{"name": "Noah D. Goodman", "author_profile_id": "81461651663", "affiliation": "Stanford University, Stanford, CA, USA", "person_id": "P3978007", "email_address": "ngoodman@stanford.edu", "orcid_id": ""}], "doi_number": "10.1145/2429069.2429117", "year": "2013", "article_id": "2429117", "conference": "POPL", "title": "The principles and practice of probabilistic programming", "url": "http://dl.acm.org/citation.cfm?id=2429117"}