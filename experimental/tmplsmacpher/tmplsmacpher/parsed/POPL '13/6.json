{"article_publication_date": "01-23-2013", "fulltext": "\n Advanced Automata Minimization Lorenzo Clemente Richard Mayr LaBRI, University of Bordeaux I University \nof Edinburgh lorenzo.clemente@labri.fr homepages.inf.ed.ac.uk/rmayr Abstract We present an ef.cient \nalgorithm to reduce the size of nondeter\u00administic B\u00a8uchi word automata, while retaining their language. \nAd\u00additionally, we describe methods to solve PSPACE-complete au\u00adtomata problems like universality, equivalence \nand inclusion for much larger instances (1-3 orders of magnitude) than before. This can be used to scale \nup applications of automata in formal veri.ca\u00adtion tools and decision procedures for logical theories. \nThe algorithm is based on new transition pruning techniques. These use criteria based on combinations \nof backward and forward trace inclusions. Since these relations are themselves PSPACE\u00adcomplete, we describe \nmethods to compute good approximations of them in polynomial time. Extensive experiments show that the \naverage-case complexity of our algorithm scales quadratically. The size reduction of the au\u00adtomata depends \nvery much on the class of instances, but our algo\u00adrithm consistently outperforms all previous techniques \nby a wide margin. We tested our algorithm on B \u00a8 uchi automata derived from LTL-formulae, many classes \nof random automata and automata de\u00adrived from mutual exclusion protocols, and compared its perfor\u00admance \nto the well-known automata tool GOAL [34]. Categories and Subject Descriptors D.2.4 [Software Veri.ca\u00adtion]: \nModel checking; F.1.1 [Models of Computation]: Automata General Terms Automata minimization, inclusion \nchecking Keywords Buchi automata, simulation, minimization \u00a8 1. Introduction Nondeterministic B\u00a8uchi \nautomata are an effective way to represent and manipulate .-regular languages, since they are closed \nunder boolean operations. They appear in many automata-based formal software veri.cation methods, as \nwell as in decision procedures for logical theories. For example, in LTL software model check\u00ading [13, \n22], temporal logic speci.cations are converted into B \u00a8 uchi automata. In other cases, different versions \nof a program (obtained by abstraction or re.nement of the original) are translated into au\u00adtomata whose \nlanguages are then compared. Testing the confor\u00admance of an implementation with its requirements speci.cation \nthus reduces to a language inclusion or language equivalence prob\u00adlem. Another application of B \u00a8 uchi \nautomata in software engineer\u00ading is program termination analysis by the size-change termination Permission \nto make digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior speci.c permission and/or a fee. POPL 13, January \n23 25, 2012, Rome, Italy. Copyright c @2013 ACM 978-1-4503-1832-7/13/01. . . $15.00 method [15, 26]. \nVia an abstraction of the effect of program oper\u00adations on data, the termination problem can often be \nreduced to a language inclusion problem about two derived B\u00a8uchi automata. Our goal is to improve the \nef.ciency and scalability of automata\u00adbased formal software veri.cation methods. We consider ef.cient \nalgorithms for the minimization of automata, in the sense of obtain\u00ading a smaller automaton with the \nsame language, though not neces\u00adsarily with the absolute minimal possible number of states. (And, in \ngeneral, the minimal automaton for a language is not even unique.) The reason to perform minimization \nis that the smaller minimized automaton is more ef.cient to handle in a subsequent computation. Thus \nthere is an algorithmic tradeoff between the effort for min\u00adimization and the complexity of the problem \nlater considered for this automaton. If only computationally easy questions are asked (e.g., reachability/emptiness; \nsolvable in Logspace/PTIME) then extensive minimization usually does not pay off. Instead, the main applications \nare the following: 1. Computationally hard automata problems like universality, equivalence, and inclusion. \nThese are PSPACE-complete [25], but many practically ef.cient methods have been developed [3, 4, 6, 10, \n11, 15, 16, 29]. Still, these all have exponential time complexity and do not scale well. Typically they \nare ap\u00adplied to automata with 15 100 states (unless the automaton has a particularly simple structure). \nThus, one should .rst minimize the automata before applying these exponential-time methods. A good minimization \nalgorithm makes it possible to solve much larger instances. Even better, many instances of the PSPACE\u00adcomplete \nuniversality, equivalence, and inclusion problems can already be solved in the polynomial time minimization \nalgo\u00adrithm (e.g., by reducing the automaton to the trivial universal automaton), so that the complete \nexponential time methods only need to be invoked in a small minority of instances. 2. Cases where the \nsize of an automaton strongly affects the complexity of an algorithm. In LTL model checking [22] one \nsearches for loops in a graph that is the product of a large system speci.cation with an automaton derived \nfrom an LTL-formula. Smaller automata often make this easier, though in practice it also depends on the \ndegree of nondeterminism [30]. 3. Procedures that combine and modify automata repeatedly. Model checking \nalgorithms and automata-based decision pro\u00adcedures for logical theories compute automata products, unions, \ncomplements, projections, etc., and thus the sizes of automata grow rapidly. Thus, it is important to \nintermittently minimize the automata to keep their size manageable, e.g., [27].  In general, .nding \nan automaton with the minimal number of states for a given language is computationally hard; even decid\u00ading \nwhether a given automaton is minimal is already PSPACE\u00adcomplete [23]. Thus much effort has been devoted \nto .nding meth\u00adods for partial minimization [8, 13, 14, 24]. Simulation preorders played a central role \nin these efforts, because they provide PTIME\u00adcomputable under-approximations of trace inclusions. However, \nthe quality of the approximation is insuf.cient in many practical examples. Multipebble simulations [12] \nyield coarser relations by allowing the Duplicator player to hedge her bets in the simulation game, but \nthey are not easily computable in practice. 1. We present methods for transition pruning, i.e., removing \ntran\u00adsitions from automata without changing their language. The idea is that certain transitions can \nbe removed, because other better transitions remain. The better criterion relies on com\u00adbinations of \nforward and backward simulations and trace inclu\u00adsions. We provide a complete picture which combinations \nare correct to use for pruning. Moreover, the pruned transitions can be removed in parallel (i.e., without \nre-computing the simula\u00adtions and trace inclusions after every change), which makes the method ef.cient \nand practical. 2. We present an ef.cient practical method to compute good under-approximations of trace \ninclusions, by introducing looka\u00adhead simulations. While it is correct to use full trace inclusions and \nmaximal-pebble multipebble simulations in our minimiza\u00adtion methods, these are not easily computed (PSPACE-hard). \nHowever, lookahead simulations are PTIME-computable, and it is correct to use them instead of the more \nexpensive trace inclusions and multipebble simulations. Lookahead itself is a classic concept in parsing \nand many other areas, but it can be de.ned in many different variants. Our contribution is to iden\u00adtify \nand formally describe the lookahead-variant for simulation preorders that gives the optimal compromise \nbetween ef.cient computability and maximizing the sizes of the relations.1 Prac\u00adtical degrees of lookahead \nrange from 4 to 25, depending on the size and shape of the automata. Our experiments show that even moderate \nlookahead helps considerably in obtaining good ap\u00adproximations of trace-inclusions and multipebble simulations. \n 3. We show that variants of the polynomial time minimization algorithm can solve most instances of the \nPSPACE-complete language inclusion problem. Thus, the complete exponential time methods of [3, 4, 6, \n10, 11, 15, 16] need only be invoked in a minority of the cases. This allows to scale language inclusion \ntesting to much larger instances (e.g., automata with = 1000 states) which are beyond traditional methods. \n 4. We performed extensive tests of our algorithm on automata of up-to 20000 states. These included random \nautomata accord\u00ading to the Tabakov-Vardi model [33], automata obtained from LTL formulae, and real-world \nmutual exclusion protocols. The empirically determined average-case time complexity on ran\u00addom automata \nis quadratic, while the (never observed) worst\u00adcase complexity is O(n 4). The worst-case space complexity \nis quadratic. Our algorithm always minimizes better, on average, than all previously available practical \nmethods. However, the exact advantage varies, depending on the type of instances; cf. Section 7. For \nexample, consider random automata with 100 1000 states, binary alphabet and varying transition density \ntd. Random automata with td = 1.4cannot be minimized much by any method. The only effect is achieved \nby the trivial removal of dead states which, on average, yields automata of 78% of the original size. \nOn the other hand, for td = 1.8, . . . , 2.2, the best previous minimization methods yielded automata \nof 85% 90% of the original size on average, while our algorithm yielded au\u00adtomata of 3% 15% of the original \nsize on average.  While we present our methods in the framework of B \u00a8 uchi automata, they directly \ncarry over to the simpler case of .nite-word automata. 1A thorough literature search showed that this \nhas never been formally described so far. 2. Preliminaries Anon-deterministic B\u00a8uchi Automaton (BA) \nA is a tuple (S, Q, I, F, d) where S is a .nite alphabet, Q is a .nite set of states, I . Q is the set \nof initial states, F . Q is the set of accepting states, and s d . Q\u00d7S \u00d7Q is the transition relation. \nWe write p -. q for (p, s, q) .d. A transition is transient iff any path can contain it at most once. \nTo simplify the presentation, we assume that automata are forward and backward complete, i.e., for any \nstate p . Q and s s symbol s . S, there exist states q0, q1 . Q s.t. q0 -. p -. q1. Every automaton \ncan be converted into an equivalent complete one by adding at most two states and a linear number of \ntransitions.2 A state is dead iff either it is not reachable from an initial state, or it cannot reach \nan accepting loop. In our simpli.cation techniques, we always remove dead states. A B\u00a8automaton A describes \n uchi a set of in.nite words (its language), i.e., a subset of S.. An in.nite trace of A on a word w \n= s0s1\u00b7 \u00b7 \u00b7 . S. (or w-trace) starting in a state q0 . Q is an s0s1 in.nite sequence of transitions \np = q0 -. q1 -. \u00b7 \u00b7 \u00b7 . By p[0..i] s0si-1 we denote the .nite pre.x p = q0 -. \u00b7 \u00b7 \u00b7 -. qi, and by p[i..] \nthe sisi+1 in.nite suf.x qi -. qi+1 -. \u00b7 \u00b7 \u00b7 . Finite traces starting in q0 and ending in a state qm \n.Q are de.ned similarly. A .nite or in.nite trace is initial iff it starts in an initial state q0 . I; \nif it is in.nite, then it is fair iff qi .F for in.nitely many i. The language of A is L (A) = {w .S. \n|A has an in.nite, initial and fair trace on w}. Language inclusion. When automata are viewed as a .nite \nrep\u00adresentation for languages, it is natural to ask whether two different automata represent the same \nlanguage, or, more generally, to com\u00adpare these languages for inclusion. Formally, for two automata A \n= (S, QA , IA , FA , dA ) and B = (S, QB , IB , FB , dB ) we write A .B iff L (A) .L (B) and A B iff \nL(A) = L (B). The language inclu\u00adsion/equivalence problem consists in determining whether A . B or A \n B holds, respectively. For general non-deterministic au\u00adtomata, language inclusion and equivalence are \nPSPACE-complete [25] (which entails that, under standard theoretic-complexity as\u00adsumptions, they admit \nno ef.cient deterministic algorithm). There\u00adfore, one considers suitable under-approximations. De.nition \nA preorder . on QA \u00d7QB is good for inclusion (GFI) ' iff the following holds: If .q.IA .q .IB \u00b7 q.q \n' , then A . B . In other words, GFI preorders give a suf.cient condition for inclu\u00adsion, by matching \ninitial states of A with initial states of B. (They are not necessary for inclusion since there are several \ninitial states.) Moreover, if computing a GFI preorder is ef.cient, than also inclu\u00adsion can be established \nef.ciently. Finally, if a preorder is GFI, then all smaller preorders are GFI too, i.e., GFI is .-downward \nclosed. Quotienting. Another interesting problem is how to simplify an automaton while preserving its \nsemantics, i.e., its language. Gen\u00aderally, one tries to reduce the number of states/transitions. This \nis useful because the complexity of decision procedures usually de\u00adpends on the size of the input automata. \nA classical operation for reducing the number of states of an automaton is that of quotienting, where \nstates of the automaton are identi.ed according to a given equivalence, and transitions are pro\u00adjected \naccordingly. Since in practice we obtain quotienting equiva\u00adlences from suitable preorders, we directly \nde.ne quotienting w.r.t. a preorder. Formally, .x a BA A = (S, Q, I, F, d) and a preorder . on Q, with \ninduced equivalence ==. n .. Given a state q.Q, we denote by [q] its equivalence class w.r.t. =, and, \nfor a set of states P.Q, [P] is the set of equivalence classes [P] = {[ p] |p.P}. 2For ef.ciency reasons, \nour implementation works directly on incomplete automata. De.nition The quotient of A by . is A/ .= \n(S, [Q], [I], [F], d ' ), '' '' where d ' = {([q1], s, [q2]) |.q . [q1], q . [q2]. (q1, s, q ) . d}, \n12 2 i.e., transitions are induced element-wise. s0s1 Clearly, every trace q0 -. q1 -. \u00b7 \u00b7 \u00b7 in A immediately \ninduces s0s1 a corresponding trace [q0] -. [q1] -. \u00b7 \u00b7 \u00b7 in A / ., which is fair/initial if the former \nis fair/initial, respectively. Consequently, A . A/ .for any preorder .. If, additionally, A/ . . A, \nthen we say that the preorder .is good for quotienting (GFQ). De.nition A preorder .is good for quotienting \niff A/ . A. Like GFI preorders, also GFQ preorders are downward closed (since a smaller preorder is \nquotienting less ). Therefore, we are interested in ef.ciently computable GFI/GFQ preorders. A classi\u00adcal \nexample is given by simulation relations. Simulation relations. Basic forward simulation is a binary \nrela\u00adtion on the states of A; it relates states whose behaviors are step\u00adwise related, which allows one \nto reason about the internal structure of automaton A i.e., how a word is accepted, and not just whether \nit is accepted. Formally, simulation between two states p0 and q0 can be described in terms of a game \nbetween two players, Spoiler and Duplicator, where the latter wants to prove that q0 can step\u00adwise mimic \nany behavior of p0, and the former wants to disprove it. The game starts in the initial con.guration \n( p0, q0). Inductively, given a game con.guration (pi, qi) at the i-th round of the game, si Spoiler \nchooses a symbol si .S and a transition pi -. pi+1. Then, si Duplicator responds by choosing a matching \ntransition qi -. qi+1, and the next con.guration is (pi+1, qi+1). Since the automaton is assumed to be \ncomplete, the game goes on forever, and the s0s1 two players build two in.nite traces p0 = p0 -. p1 -. \n\u00b7 \u00b7 \u00b7 and s0s1 p1 = q0 -. q1 -. \u00b7 \u00b7 \u00b7 . The winning condition depends on the type of simulation, and \ndifferent types have been considered depend\u00ading on whether one is interested in GFQ or GFI relations. \nHere, we consider direct [10], delayed [14] and fair simulation [21]. Let x . {di, de, f}. Duplicator \nwins the play if C x(p0, p1) holds, where C di(p0, p1) .. .(i=0) \u00b7 pi .F =. qi .F (1) C de(p0, p1) .. \n.(i=0) \u00b7 pi .F =. .( j=i) \u00b7 qj .F (2) C f(p0, p1) .. if p0 is fair, then p1 is fair (3) Intuitively, \ndirect simulation requires that accepting states are matched immediately (the strongest condition), while \nin delayed simulation Duplicator is allowed to accept only after a .nite delay. In fair simulation (the \nweakest condition), Duplicator must visit ac\u00adcepting states only if Spoiler visits in.nitely many of \nthem. Thus, C di(p0, p1) implies C de(p0, p1), which, in turn, implies C f(p0, p1). We de.ne x-simulation \nrelation .x.Q\u00d7Qby stipulating that p0 .x q0 iff Duplicator has a winning strategy in the x-simulation \ngame, starting from con.guration (p0, q0); clearly, .di ..de ..f . Simulation between states in different \nautomata A and B can be computed as a simulation on their disjoint union. All these simula\u00adtion relations \nare GFI preorders which can be computed in polyno\u00admial time [10, 14, 20]; moreover, direct and delayed \nsimulation are GFQ [14], but fair simulation is not [21]. Lemma 2.1 ([10, 14, 20, 21]). For x . {di, \nde, f}, x-simulation .x is a PTIME, GFI preorder, and, for y . {di, de}, .y is also GFQ. Trace inclusions. \nWhile simulations are ef.ciently computable, their use is often limited by their size, which can be much \nsmaller than other GFI/GFQ preorders. One such example of coarser GFI/GFQ preorders is given by trace \ninclusions, which are ob\u00adtained through a modi.cation of the simulation game, as follows. In simulation \ngames, the players build two paths p0, p1 by choosing single transitions in an alternating fashion; Duplicator \nmoves by knowing only the next 1-step move of Spoiler. We can obtain coarser relations by allowing Duplicator \na certain amount of lookahead on Spoiler s moves. In the extremal case of .\u00adlookahead, i.e., where Spoiler \nhas to reveal her whole path in ad\u00advance, we obtain trace inclusions. Analogously to simulations, we \nde.ne direct, delayed, and fair trace inclusion, as binary relations on Q. For x . {di, de, f}, x\u00adtrace \ninclusion holds between p and q, written p .x q iff, for every word w = s0s1\u00b7 \u00b7 \u00b7 .S., and for every \nin.nite w-trace p0 = s0s1 p0 -. p1 -. \u00b7 \u00b7 \u00b7 starting at p0 = p, there exists an in.nite w-trace s0s1 \n p1 = q0 -. q1 -. \u00b7 \u00b7 \u00b7 starting at q0 = q, s.t. C x(p0, p1). All these trace inclusions are GFI preorders \nsubsuming the corresponding simulation, i.e., .x ..x (since Duplicator has more power in the trace inclusion \ngame); also, .di is a subset of .de , which, in turn, is a subset of .f . Regarding quotienting, .di \nis GFQ (like .di ; this follows from [12]), while .f is not, since it is coarser than fair simulation, \nwhich is not GFQ [21]. While delayed simulation .de is GFQ, delayed trace inclusion .de is not GFQ [8]. \nLemma 2.2. For x . {di, de, f}, x-trace inclusion .x is a GFI preorder. Moreover, .di is a GFQ preorder. \nFinally, though .de and .di are incomparable, there exists a com\u00admon generalization included in .de called \ndelayed .xed-word sim\u00adulation which is GFQ [8].3 Backward simulation and trace inclusion. Yet another \nway of obtaining GFQ/GFI preorders is to consider variants of simula\u00adtion/trace inclusion which go backwards \nin time. Backward simula\u00adtion .bw ([32], where it is called reverse simulation) is de.ned like ordinary \nsimulation, except that transitions are taken backwards: si From con.guration ( pi, qi), Spoiler selects \na transition pi+1 -. pi, si Duplicator replies with a transition qi+1 -. qi, and the next con\u00ad.guration \nis ( pi+1, qi+1). Let p0 and p1 be the two in.nite back\u00adward traces built in this way. The corresponding \nwinning condition considers both accepting and initial states: { C bw(p0, p1) pi .F =. qi .F, and .. \n.(i=0) \u00b7 (4) pi .I =. qi .I .bw is an ef.ciently computable GFQ preorder [32] incomparable with forward \nsimulations. It can be used to establish language inclusion by matching .nal states of A with .nal states \nof B (dually to forward simulations); in this sense, it is GFI. Lemma 2.3 ([32]). Backward sim. is a \nPTIME GFQ/GFI preorder. The corresponding notion of backward trace inclusion .bw is de\u00ad.ned as follows: \np.bw qiff, for every .nite word w= s0s1 \u00b7 \u00b7 \u00b7 sm-1 . s0s1 S* , and for every initial, .nite w-trace \np0 = p0 -. p1 -. sm-1 \u00b7 \u00b7 \u00b7 -. pm ending in pm = p, there exists an initial, .nite w-trace s0s1sm-1 \n p1 = q0 -. q1 -. \u00b7 \u00b7 \u00b7 -. qm ending in qm = q, s.t., for any i=0, if pi . F, then qi . F. Note that \nbackward trace inclusion deals with .nite traces (unlike forward trace inclusions), which is due to the \nasymmetry between past and future in .-automata. Clearly, .bw ..bw ;we observe that even .bw is GFQ/GFI. \nTheorem 2.4. Backward trace inclusion is a GFQ/GFI preorder. Proof. We .rst prove that .bw is GFQ. Let \n.:=.bw . Let w = s0s1 \u00b7 \u00b7 \u00b7 . L (A/ .), and we show w . L(A ). There exists an s0s1 initial, in.nite \nand fair w-trace p = [q0] -. [q1] -. \u00b7 \u00b7 \u00b7 . For i=0, let wi = s0s1 \u00b7 \u00b7 \u00b7 si (with w-1 = e), and let \np[0..i] be the wi-1-trace 3Delayed .xed-word simulation is de.ned as a variant of simulation where Duplicator \nhas .-lookahead only on the input word w, and not on Spoiler s actual w-trace p0; that it subsumes .di \nis non-trivial. pre.x of p. For any i = 0, we build by induction an initial, .nite wi-1-trace pi ending \nin qi (of length i) visiting at least as many accepting states as p[0..i] (and at the same time p[0..i] \ndoes). For i= 0, just take the empty e-trace p0 = q0. For i> 0, assume that an initial wi-2-trace pi-1 \nending in qi-1 has already been built. si-1 We have the transition [qi-1] -. [qi] in L (A/ .). There \nexist q. ' ' si-1 [qi-1] and q .[qi] s.t. we have a transition q -. q in A . W.l.o.g. ' we can assume \nthat q = qi, since [qi] = [q ' ]. By qi-1 .bw q , there exists an initial, .nite wi-2-trace p ' ending \nin q. By the de.nition of backward inclusion, p ' visits at least as many accepting states as pi-1, which, \nby inductive hypothesis, visits at least as many si-1 accepting states as p[0..i-1]. Therefore, pi := \np ' -. qi is an initial, .nite wi-1-trace ending in qi. Moreover, if [qi] . F ' , then, since backward \ninclusion respects accepting states, [qi] .F, hence qi .F, and, consequently, pi visits at least as many \naccepting states as p[0..i]. Since p is fair, the .nite, initial traces p0, p1, \u00b7 \u00b7 \u00b7 visit unboundedly \nmany accepting states. Since A is .nitely branching, by K\u00a8onig s Lemma there exists an initial, in.nite \nand fair w-trace p.. Therefore, w .L(A ). We now prove that .bw is GFI. Let A and B be two automata. \nFor backward notions, we require that every accepting state in A is in relation with an accepting state \nin B. Let w = s0s1 \u00b7 \u00b7 \u00b7 .L(A), s0s0 and let p0 = p0 -. p1 -. \u00b7 \u00b7 \u00b7 be an initial and fair w-path in \nA. Since p0 visits in.nitely many accepting states, and since each such state is .bw -related to an accepting \nstate in B, by using the de.nition of .bw it is possible to build in B longer and longer .nite initial \ntraces in B visiting unboundedly many accepting states. Since B is .nitely branching, by K\u00a8onig s Lemma \nthere exists an in.nite, initial and fair w-trace p. in B. Thus, w .L(B ). 3. Transition Pruning Minimization \nTechniques While quotienting-based minimization techniques reduce the num\u00adber of states by merging them, \nwe explore an alternative method which prunes (i.e., removes) transitions. The intuition is that certain \ntransitions can be removed from an automaton without changing its language when other better transitions \nremain. De.nition Let A = (S, Q, I, F, d) be a BA and let P a transi\u00adtive, asymmetric relation on d. \nThe pruned automaton is de\u00ad.ned as Prune(A, P) := (S, Q, I, F, d ' ), with d ' = {( p, s, r) . ' ' d \n|.( p , s ' , r ' ) .d \u00b7 ( p, s, r)P( p , s ' , r ' )}. By the assumptions on P, the pruned automaton \nPrune(A, P) is uniquely de.ned. Notice that transitions are removed in parallel . Though P might depend \non d, P is not re-computed even if the removal of a single transition changes d. This is important because \ncomputing P may be expensive. Since removing transitions cannot introduce new words in the language, \nPrune(A , P) .A. When also the converse inclusion holds (so the language is preserved), we say that Pis \ngood for pruning (GFP), i.e., Pis GFP iff Prune(A, P) A . Clearly, GFP is .-downward closed (like GFI \nand GFQ). We study GFP relations obtained by comparing the endpoints of transitions over the same input \nsymbol. Formally, given two binary relations Rb, Rf .Q\u00d7Q, we de.ne ' ' P(Rb, Rf) = {(( p, s, r), ( p \n, s, r ' )) |pRbp and rRfr ' } P(\u00b7, \u00b7) is monotone in both arguments. In the following, we explore which \nstate relations Rb, Rf induce GFP relations P(Rb, Rf). P(.bw It has long been known that P(id, .di ) \nand , id) are GFP (see [7] where the removed transitions are called little brothers ). Moreover, even \nthe relation Rt(.f ) := P(id, .di ) . {(( p, s, r), ( p, s, r ' )) | ( p, s, r ' ) is transient and r \n.f r ' } is GFP [32], i.e., strict fair trace inclusion suf.ces if the remaining tran\u00adsition can only \nbe used once. However, in general, P(id, .f ) is Rb\\Rf id .di .di .de .f id .bw .bw \u00d7 . . . . . . . \u00d7 \n\u00d7 \u00d7 \u00d7 \u00d7 \u00d7 \u00d7 Figure 1. GFP relations P(Rb, Rf) not GFP. Moreover, even if only transient transitions \nare com\u00adpared/pruned, P(.bw , .f ) is not GFP; cf. Fig. 2. Theorem 3.1. For every asymmetric and transitive \nrelation R ..di , P(id, R) is GFP. ' ' Proof. Let A = Prune(A, P(id, R)). We show A . A . If w = s0s1\u00b7 \n\u00b7 \u00b7 .L(A ) then there exists an in.nite fair initial trace p on w in A . We show w .L (A ' ). s0s1 We \ncall a trace p = q0 -. q1 -. \u00b7 \u00b7 \u00b7 on w in A i-good if it does s j not contain any transition qj-. qj+1 \nfor j< is.t. there exists an A s j ' transition qj -. q with qj+1Rq ' (i.e., no such transition is j+1 \nj+1 used within the .rst isteps). Since A is .nitely branching, for every state and symbol there exists \nat least one R-maximal successor that is still present in A ' , because R is asymmetric and transitive. \nThus, for every i-good trace p on w there exists an (i+ 1)-good trace p ' on w s.t. p and p ' are identical \non the .rst i steps and C di(p, p ' ), because R..di . Since p is an in.nite fair initial trace on w \n(which is trivially 0-good), there exists an in.nite initial trace p on w that is i-good for every i \nand C di(p , p ). Moreover, p is a trace in A ' . Since p is fair and C di(p , p ), p is an in.nite fair \ninitial trace on w that is ' i-good for every i. Therefore p is a fair initial trace on w in A and thus \nw .L(A ' ). Theorem 3.2. For every asymmetric and transitive relation R ..bw , P(R, id) is GFP. ' ' \nProof. Let A = Prune(A, P(R, id)). We show A . A . If w = s0s1 \u00b7 \u00b7 \u00b7 .L(A ) then there exists an in.nite \nfair initial trace p on w in A . We show w .L (A ' ). s0s1 We call a trace p = q0 -. q1 -. \u00b7 \u00b7 \u00b7 on w \nin A i-good if it does s j not contain any transition qj-. qj+1 for j< is.t. there exists an A s j ' \n transition q -. qj+1 with qjR q ' (i.e., no such transition is used j j within the .rst i steps). We \nshow, by induction on i, the following property (P): For every i and every initial trace p on w in A \nthere exists an initial i-good trace p ' on w in A s.t. p and p ' have identical suf.xes from step i \nonwards and C di(p, p ' ). The base case i= 0 is trivial with p ' = p. For the induction step there are \ntwo cases. If p is (i+ 1)-good then we can take p ' = p. si ' Otherwise there exists a transition q \n-. qi+1 with qiR q ' . Without i irestriction (since A is .nite and R is asymmetric and transitive) ' \n we assume that qi is R-maximal among the si-predecessors of si ' qi+1. In particular, the transition \nq -. qi+1 is present in A ' . Since i si ' R..bw , there exists an initial trace p '' on w that has \nsuf.x q -. i si+1 qi+1 -. qi+2. . . and C di(p, p '' ). Then, by induction hypothesis, ' si there exists \nan initial i-good trace p ' on w that has suf.x q -. i si+1 ' qi+1 -. qi+2. . . and C di(p '' , p ' \n). Since q is R-maximal among i the si-predecessors of qi+1 we obtain that p ' is also (i+ 1)-good. Moreover, \np ' and p have identical suf.xes from step i+ 1onwards. Finally, by C di(p, p '' ) and C di(p '' , p \n' ), we obtain C di(p, p ' ). Given the in.nite fair initial trace p on w in A, it follows from property \n(P) and K \u00a8 onig s Lemma that there exists an in.nite initial trace p on w that is i-good for every i \nand C di(p , p ). Therefore p is  ' an in.nite fair initial trace on w in A and thus w .L(A ' ). Theorem \n3.3. If A = A/ .bw then P(.bw , .di ) is GFP. ' ' Proof. Let A = Prune(A, P(.bw , .di )). We show A . \nA . Let w = s0s1\u00b7 \u00b7 \u00b7 .L (A). Then there exists an in.nite fair initial trace p on w in A. We show w \n.L(A ' ). s0s1 We call a trace p = q0 -. q1 -. \u00b7 \u00b7 \u00b7 on w start-maximal iff it is ' ' s0s1 initial and \nthere does not exist any trace p ' = q -. q -. \u00b7 \u00b7 \u00b7 on w 0 1 s0s1 ' s.t. C di(p, p ' ) and q0 .bw q \n. We call a trace p = q0 -. q1 -. \u00b7 \u00b7 \u00b7 0on w i-good iff it is start-maximal and p does not contain any \ns j transition qj -. qj+1 for j < i s.t. there exists an A transition s j ' ' qj -. q with qj+1 .bw q \nand there exists an in.nite trace j+1 j+1 ' p ' [ j+ 1..] from qj+1 with C di(p[ j+ 1..], p ' [ j+ 1..]). \nSince A is .nite, there are .bw -maximal elements among those .nitely many successors of every state \nqj from which there exists an in.nite trace p ' [ j+ 1..] with C di(p[ j+ 1..], p ' [ j+ 1..]). Thus, \nfor every in.nite i-good trace p on w there exists an (i+ 1)-good trace p ' on w s.t. p and p ' are identical \non the .rst i steps and C di(p, p ' ). Since there is an in.nite fair initial trace p on w, there also \nexists a start-maximal, and thus 0-good, fair initial trace on w, because .bw has maximal elements. Then \nit follows from the property above that there exists an in.nite initial trace p on w that is i-good for \nevery iand C di(p , p ). In particular, this implies that p is fair. So p is an in.nite fair initial \ntrace on w that is i-good for every i. s0s1 Let now p = q0 -. q1 -. \u00b7 \u00b7 \u00b7 . We show that p is also possible \n' in A by assuming the opposite and deriving a contradiction. Sup\u00ad s j pose that p contains a transition \nqj -. qj+1 that is not present s j ' ''' in A . Then there must exist a transition q -. q in A s.t. j \nj+1 qj .bw ' ' q and qj+1 .di q . We cannot have j= 0, because in j j+1this case p would not be start-maximal \nand thus not even 1-good. s j-1 ' So we get j=1. Since qj .bw qj and qj-1 -. qj there must ex\u00ad s j-1 \n''' ' ist a state q s.t. q -. q and qj-1 .bw q . In particular, j-1 j-1 j j-1' qx .F . q .F for x . { \nj-1, j}. By A = A / .bw we obtain x+1 ' ' that either qj-1 = qj-1 or qj-1 .bw q . The .rst case would \nim\u00ad j-1' ply that p ' is not j-good, because qj+1 .di qj+1, and thus yield a ' contradiction. Therefore, \nwe must have qj-1 .bw qj-1. We cannot have j-1= 0, because in this case p ' would not be start-maximal \nand thus not even 1-good. So we get j-1=1. The whole argument above repeats with j-1, j-2, j-3, . . . \nsubstituted for juntil we get a contradiction or 0 is reached. Reaching 0 also yields a con\u00adtradiction \nto start-maximality of p , as above. Therefore p is a fair ' initial trace on w in A and thus w .L (A \n' ). P(.bw Theorem 3.4. , .di ) is GFP. ' ' Proof. Let A = Prune(A, P(.bw , .di )). We show A . A . Let \nw = s0s1 \u00b7 \u00b7 \u00b7 .L (A). Then there exists an in.nite fair initial trace p on w in A. We show w .L(A ' \n). s0s1 Given some in.nite initial trace p = q0 -. q1 -. \u00b7 \u00b7 \u00b7 on w, we ' call it i-good iff its .rst \ni transitions are also possible in A . We now show, by induction on i, the following property (P): For \ns0s1 every in.nite initial trace p = q0 -. q1 -. \u00b7 \u00b7 \u00b7 on w and every ' ' s0s1 i=0, there exists an in.nite \ninitial trace p ' = q -. q -. \u00b7 \u00b7 \u00b7 on 0 1 ' w that is i-good and C di(p, p ' ) and .j=i. qj .di qj. \nThe base case i= 0 is trivially true with p ' = p. For the induction s0s1 step consider an in.nite initial \ntrace p = q0 -. q1 -. \u00b7 \u00b7 \u00b7 on w. By induction hypothesis, there exists an in.nite initial trace 1 1 \ns0s1 p1 = q -. q -. \u00b7 \u00b7 \u00b7 on w that is i-good and C di(p, p1) and 0 1 .j=i. qj .di 1 qj. If p1 is (i+ \n1)-good then we are done. Otherwise, the transition 1 1 ' si q -. q is not present in A ' . Since A \n= Prune(A, P(.bw , .di )), i i+1 2 si2 ' 1 .bw 2  there must exist a transition q -. q in A s.t. q q \nand i i+1 i i 1 .di 2  q q . It follows from the de.nitions of .bw and .di that i+1 i+1 2 2 s0s1 there \nexists an in.nite initial trace p2 = q -. q -. \u00b7 \u00b7 \u00b7 on w 0 1 1 .di 2 1 .di 2 s.t. C di(p1 , p2), q \nq and .j=i+ 1. q q . (This last i+1 i+1 j jproperty uses the fact that .di propagates forward. Direct \ntrace in\u00adclusion .di does not suf.ce.) By induction hypothesis, there exists 3 3 s0s1 an in.nite initial \ntrace p3 = q -. q -. \u00b7 \u00b7 \u00b7 on w that is i-good 0 1 2 .di 3 and C di(p2 , p3) and .j= i. q q By transitivity \nwe obtain j j. C di(p1 1 .di 3 1 .di 3 , p3), q q and .j=i+ 1. q qj. i+1 i+1 j If p3 is (i+ 1)-good \nthen we are done. Otherwise, the argument of the above paragraph repeats and we obtain an in.nite initial \n5 5 s0s1 trace p5 = q -. q -. \u00b7 \u00b7 \u00b7 on w that is i-good and C di(p3 , p5), 0 1 3 .di 5 3 .di 5 that \nq q and .j= i+ 1. q q . This process cannot i+1 i+1 j jrepeat in.nitely often, because this would imply \nan in.nite strictly 2x+1 increasing .di -chain q for x = 0, 1, 2, . . . , which is impossible i+1 in \n.nite automata. Therefore, for some .nite index x, we obtain an x x s0s1 in.nite initial trace px = \nq -. q -. \u00b7 \u00b7 \u00b7 on w that is (i+ 1)-good 0 1 x and, by transitivity, C di(p, px) and .j=i+ 1. qj .di \nqj. Thus px is the trace p ' that we were looking for. Given the in.nite fair initial trace p on w in \nA, it follows from property (P) and K \u00a8 onig s Lemma that there exists an in.nite initial trace p on \nw that is i-good for every i and C di(p , p ). Therefore p is ' an in.nite fair initial trace on w in \nA and thus w .L (A ' ). , .di ) Theorem 3.4 implies that P(.bw , .di ) is GFP, but P(.bw is not; see \nFigure 2. Moreover, P(id, .de ) is not GFP (even if A = A/ .de ); see Figure 2. The quotienting and transition \npruning techniques described above use intricate combinations of backward and forward simu\u00adlations (and \nmore general trace inclusions). In particular, they sub\u00adsume previous attempts to combine backward and \nforward simula\u00adtions for automata minimization by mediated preorder [5] (but not vice-versa). Mediated \npreorder is de.ned as the largest fragment M ..di .(.bw )-1 s.t. M. .di .M. In particular, M is a preorder \nthat is GFQ. However, an automaton A that has been minimized by the techniques described above cannot \nbe further reduced by me- A/ .bw A/ .di diated preorder. First we have A = = by repeated quotienting. \nSecond, there cannot exist any distinct states x, y in A y.x .bw s.t. (x .di y) by the pruning techniques \nabove (used with simulations as approximations for trace inclusions) and the removal of dead states. \nUnder these conditions, quotienting with mediated preorder has no effect, as the following theorem shows. \nA / .bw Theorem 3.5. Let A be an automaton s.t. (1) A = = y.x .bw A/ .di and (2) x .di y.x = y. Then \nA = A /M. Proof. We show that xMy .yMx .x = ywhich implies A = A/M. Let xMy and yMx. By de.nition of \nM there exist mediators z x .di x .bw s.t. z and y .bw z, and w s.t. w and y .di w. Since M. .di . M \nwe have xMw. Thus there exists a mediator k s.t. x .di k and w .bw k. By transitivity of .bw we also \nhave x .bw k. By (2) we get x = k. Thus x .bw w and w .bw x. By (1) we get = x .di x = w. Thus y.di \nw z and by transitivity y.di z. Moreover, y .bw x .di z as above. By (2) we get z = y. Thus z = y and \ny.di w = x. By (1) we get x = y.  aa (a) P(.bw , .di ) is not GFP: If the dashed transitions p0 -. \nq0 and r1 -. 5 = A / .bw s1 are removed, then a e . is no longer accepted. Note that A = A / .di . (This \nexample even holds for .k-bw , .k-di and k= 3; cf. Section 4). (b) GFP is not closed under (c) P(id, \n.de ) is not GFP: We union: Pruning automaton A with have q .de p, but removing P(id, .di ) . P(.bw a \n, id) would the dashed transition p -. a remove the transitions p -. r and q makes the language empty, \na= A / .de q -. s, and thus aac . would no even though A . longer be accepted. a, b a a a, b (d) P(.bw \n, .f ) is not GFP: In the automaton above, both transi\u00ad aa tions p -. q and q -. r are transient. Moreover, \nr .f q (even r .de q) and q .bw p. However, removing the smaller transition a. q -. r changes the language, \nsince a is no longer accepted. Thus, P(.bw , .f ) is not GFP even when one restricts to compar\u00ading/pruning \nonly transient transitions (unlike P(id, .f )). Figure 2. Pruning counterexamples. 4. Lookahead Simulations \nWhile trace inclusions are theoretically appealing as GFQ/GFI pre\u00adorders coarser than simulations, it \nis not feasible to use them in practice, because they are too hard to compute (even their member\u00adship \nproblem is PSPACE-complete). As a .rst attempt at achieving a better trade-off between complexity and \nsize we recall multipeb\u00adble simulations [12], which are obtained by providing Duplicator with several \npebbles, instead of one. However, computing multi\u00adpebble simulations is not feasible in practice either, \non automata of nontrivial size. Therefore, we explore yet another way of obtain\u00ading good under-approximations \nof trace inclusion: We introduce lookahead simulations, which are obtained by providing Duplicator with \na limited amount of information about Spoiler s future moves. While lookahead itself is a classic concept \n(e.g., in parsing) it can be de.ned in several different ways in the context of adversarial games like \nin simulation. We compare different variants for com\u00adputational ef.ciency and approximation quality. \n k-pebble simulation. Simulation preorder can be generalized by allowing Duplicator to control several \npebbles instead of just one. In k-pebble simulation, k > 0, Duplicator s position is a set of at most \nk states (while Spoiler still controls exactly 1 state), which allows Duplicator to hedge her bets in \nthe simulation game. The direct, delayed, fair and backward winning conditions can be gen\u00aderalized to \nthe multipebble framework [12]. For x . {di, de, f, bw} and k > 0, k-pebble x-simulation is coarser than \nx-simulation and it implies x-containment; by increasing k, one can control the qual\u00adity of the approximation \nto trace inclusion. Direct, delayed, fair and backward k-pebble simulations are not transitive in general, \nbut their transitive closures are GFI preorders; the direct, delayed and backward variants are also GFQ. \nHowever, computing k-pebble simulations is infeasible, even for modest values for k. In fact, for a BA \nwith n states, computing k-pebble simulation requires solving a game of size n\u00b7 n k. Even in the simplest \ncase of k= 2 this means at least cubic space, which is not practical for large n. For this reason, we \nconsider a different way to extend Duplicator s power, i.e., by using lookahead on the moves of Spoiler. \nk-step simulation. We generalize simulation by having the play\u00aders select sequences of transitions of \nlength k > 0 instead of sin\u00adgle transitions: This gives Duplicator more information, and thus yields \na larger simulation relation. In general, k-step simulation and k-pebble simulation are incomparable, \nbut k-step simulation is strictly contained in n-pebble simulation. However, the rigid use of lookahead \nin big-steps causes at least two issues: 1) For a BA with n states, we need to store only n 2 con.gurations \n( p, q) (which is much less than k-pebble simulation). However, in every round we have to explore up-to \ndk different moves for each player (where d is the maximal out-degree of the automaton). In practice \n(e.g., d = 4, k = 12) this is still too large. 2) Duplicator s lookahead varies be\u00adtween 1 and k, depending \nwhere she is in her response to Spoiler s long move. Thus, Duplicator might lack lookahead where it is \nmost needed, while having a large lookahead in other situations where it is not useful. In the next notion, \nwe attempt at ameliorating this. k-continuous simulation. Duplicator is continuously kept in\u00adformed about \nSpoiler s next k moves, i.e., she always has looka\u00adhead k. Formally, a con.guration of the simulation \ngame consists in a pair (.i, qi), where .i is the sequence of the next k-1 moves from pi that Spoiler \nhas already committed to. In every round of the game, Spoiler reveals another move k steps in the future, \nand then makes the .rst of her announced k moves, to which Dupli\u00adcator responds as usual. A pair of states \n( p, q) is in k-continuous simulation if Duplicator can win this game from every con.gura\u00adtion (., q), \nwhere . is a sequence of k-1 moves from p. (k = 1 is ordinary simulation.) k-continuous simulation is \nstrictly contained in n-pebble simulation (but incomparable with k-pebble simula\u00adtion), and larger than \nk-step simulation. While this is arguably the strongest way of giving lookahead to Duplicator, it requires \nstoring 2 n \u00b7 dk-1 con.gurations, which is infeasible for nontrivial n and k (e.g., n = 10000, d = 4, \nk = 12). k-lookahead simulation. We introduce k-lookahead simulation as an optimal compromise between \nk-step and k-continuous simu\u00adlation. Intuitively, we put the lookahead under Duplicator s control, who \ncan choose at each round how much lookahead she needs (up to k). Formally, con.gurations are pairs ( \npi, qi) of states. In every round of the game, Spoiler chooses a sequence of k consecutive sisi+1si+k-1 \n transitions pi -. pi+1 -. \u00b7 \u00b7 \u00b7 -. pi+k. Duplicator then chooses a number 1 = m = k and responds with \na matching sequence of sisi+1si+m-1 m transitions qi -. qi+1 -. \u00b7 \u00b7 \u00b7 -. qi+m. The remaining k-m moves \nof Spoiler are forgotten, and the next round of the game starts at ( pi+m, qi+m). In this way, the players \nbuild two in.nite traces p0 from p0 and p1 from q0. Backward simulation is de\u00ad.ned similarly with backward \ntransitions. For acceptance condition x . {di, de, f, bw}, Duplicator wins this play if C x(p0, p1) holds. \n De.nition Two states ( p0, q0) are in k-lookahead x-simulation, written p0 .k-x q0, iff Duplicator has \na winning strategy in the above game. Since .k-x is not transitive (unless k = 1) [2], we denote its \ntran\u00adsitive closure, which is a preorder, by jk-x, and its asymmetric re\u00adstriction by .k-x=jk-x \\(jk-x)-1 \n. Lookahead simulation offers the optimal trade-off between k\u00adstep and k-continuous simulation. Since \nthe lookahead is discarded at each round, k-lookahead simulation is (strictly) included in k\u00adcontinuous \nlookahead (where the lookahead is never discarded). However, this has the bene.t of only requiring to \nstore n 2 con\u00ad.gurations, which makes computing lookahead simulation space\u00adef.cient. On the other side, \nwhen Duplicator always chooses a max\u00adimal reply m = k we recover k-step simulation, which is thus in\u00adcluded \nin k-lookahead simulation. Moreover, thanks to the fact that Duplicator controls the lookahead, most \nrounds of the game can be solved without ever reaching the maximal lookahead k: 1) for a .xed attack \nby Spoiler, we only consider Duplicator s responses for small m = 1, 2, . . . , k until we .nd a winning \none, and 2) also Spoiler s attacks can be built incrementally since, if she loses for some lookahead \nh, then she also loses for h ' >h. In practice, this greatly speeds up the computation, and allows us \nto use lookaheads in the range 4-25, depending on the size and structure of the au\u00adtomata; see Section \n7 for the experimental evaluation and bench\u00admark against the GOAL tool [34]. k-lookahead simulation can \nalso be expressed as a restriction of n-pebble simulation, where Duplicator is allowed to split peb\u00adbles \nmaximally (thus n-pebbles), but after a number m =k rounds (where m is chosen dynamically by Duplicator) \nshe has to discard all but one pebble. Then, Duplicator is allowed to split pebbles maximally again, \netc. Thus, k-lookahead simulation is contained in n-pebble simulation, though it is generally incomparable \nwith k-pebble simulation. Direct, delayed, fair and backward k-lookahead simulation have a .xed-point \ncharacterization expressible in \u00b5-calculus [2], which can be useful for a symbolic implementation. However, \nour current algorithm computes them with an explicit-state representation. 5. Automata Minimization We \nminimize automata by transition pruning and quotienting. While trace inclusions would be an ideal basis \nfor such techniques, they (i.e., their membership problems) are PSPACE-complete. In\u00adstead, we use lookahead \nsimulations as ef.ciently computable under-approximations; in particular, we use jk-di in place of direct \ntrace inclusion .di (which is GFQ [12]).  jk-de in place of n-pebble delayed simulation (GFQ [12]). \n jk-f in place of fair trace inclusion .f (which is GFI).  jk-bw in place of backward trace inclusion \n.bw (which is GFQ by Theorem 2.4).  For pruning, we apply the results of Section 3 and the substitutions \nabove to obtain the following GFP relations: P(id, .k-di ), P(.k-bw , id), P(.bw , jk-di ), P(jk-bw , \n.di ), Rt(.k-f ) For quotienting, we employ delayed jk-de and backward jk-bw k\u00adlookahead simulations \n(which are GFQ). Below, we describe two possible ways to combine our simpli.cation techniques: Heavy-k \nand Light-k (which are parameterized by the lookahead value k). Heavy-k. We advocate the following minimization \nprocedure, which repeatedly applies all the techniques described in this paper until a .xpoint is reached: \n1) Remove dead states. 2) Prune transi\u00adtions w.r.t. the GFP relations above (using lookahead k). 3) Quo\u00adtient \nw.r.t. jk-de and jk-bw . The resulting simpli.ed automaton cannot be further reduced by any of these \ntechniques. In this sense, it is a local minimum in the space of automata. Applying the tech\u00adniques in \na different order might produce a different local mini\u00admum, and, in general, there does not exist an \noptimal order that works best in every instance. In practice, the order is determined by ef.ciency considerations \nand easily computable operations are used .rst [1, 2]. Remark While quotienting with ordinary simulation \nis idempo\u00adtent, in general this is not true for lookahead simulations, because these relations are not \npreserved under quotienting (unlike ordi\u00adnary simulation). Moreover, quotienting w.r.t. forward simulations \ndoes not preserve backward simulations, and vice-versa. Our exper\u00adiments showed that repeatedly and alternatingly \nquotienting w.r.t. jk-de and jk-bw (in addition to our pruning techniques) yields the best minimization \neffect. The Heavy-k procedure strictly subsumes all simulation-based automata minimization methods described \nin the literature (remov\u00ading dead states, quotienting, pruning of little brother transitions, mediated \npreorder), except for the following two: 1) The fair simu\u00adlation minimization of [19] works by tentatively \nmerging fair sim\u00adulation equivalent states and then checking if this operation pre\u00adserved the language. \n(In general, fair simulation is not GFQ.) It subsumes quotienting with .de (but not jk-de ) and is implemented \nin GOAL [34]. We benchmarked our methods against it and found Heavy-k to be much better in both effect \nand ef.ciency; cf Sec\u00adtion 7. 2) The GFQ jumping-safe preorders of [8, 9] are incompa\u00adrable to the techniques \ndescribed in this paper. If applied in addition to Heavy-k, they yield a very modest extra minimization \neffect. Light-k. This procedure is de.ned purely for comparison reasons. It demonstrates the effect of \nthe lookahead k in a single quotienting operation and works as follows: Remove all dead states and then \nquotient w.r.t. jk-de . Although Light-k achieves much less than Heavy-k, it is not necessarily faster. \nThis is because it uses the more expensive to compute relation jk-de directly, while Heavy-kapplies other \ncheaper (pruning) operations .rst and only then computes jk-de on the resulting smaller automaton. 6. \nLanguage Inclusion Checking The language inclusion problem A .B is PSPACE-complete [25]. It can be solved \nvia complementation of B [31, 34] and, more ef\u00ad.ciently, by rank-based ([17] and references therein) \nor Ramsey\u00adbased methods [3, 4, 15, 16], or variants of Piterman s construc\u00adtion [29, 34]. Since these \nall have exponential time complexity, it helps signi.cantly to .rst minimize the automata in a preprocessing \nstep. Better minimization techniques, as described in the previous sections, make it possible to solve \nsigni.cantly larger instances. However, our simulation-based techniques can not only be used in preprocessing, \nbut actually solve most instances of the inclu\u00adsion problem directly. This is signi.cant, because simulation \nscales polynomially (quadratic average-case complexity; cf. Section 7). 6.1 Inclusion-preserving minimization \nInclusion checking algorithms generally bene.t from language\u00adpreserving minimization preprocessing (cf. \nSec. 5). However, pre\u00adserving the languages of A and B in the preprocessing is not ac\u00adtually necessary. \nA preprocessing on A , B is said to be inclusion\u00ad '' '' preserving iff it produces automata A , B s.t. \nA .B .. A .B ' (regardless of whether A A or B B ' ). In the following, we consider two inclusion-preserving \npreprocessing steps. Simplify A. In theory, the problem A . B is only hard in B, but polynomial in the \nsize of A . However, this is only relevant if one actually constructs the exponential-size complement \nof B, which is of course to be avoided. For polynomial simulation-based algorithms it is crucial to also \nminimize A. The idea is to remove transitions in A which are covered by better transitions in B. De.nition \nGiven A = (S, QA , IA , FA , dA ), B = (S, QB , IB , FB , dB ), let P.dA \u00d7dB be a relation for comparing \ntransitions in A and B. The pruned version of A is Prune(A, B, P) := (S, QA , IA , FA , d ' ) ' ' with \nd ' = {( p, s, r) .dA |.( p , s ' , r ' ) .dB . ( p, s, r)P(p , s ' , r ' )}. A . B implies Prune(A, \nB, P) . B (since Prune(A , B, P) . A). When also the other direction holds (so pruning is inclusion\u00adpreserving), \nwe say that P is good for A , B-pruning, i.e., when A . B .. Prune(A, B, P) . B. Intuitively, pruning \nis cor\u00adrect when the removed edges do not allow A to accept any word which is not already accepted by \nB . In other words, if there is a counter example to inclusion in A, then it can even be found in Prune(A \n, B, P). As in Sec. 3, we compare transitions by look\u00ading at their endpoints: For state relations Rb, \nRf . QA \u00d7QB , let ' ' P(Rb, Rf) = {((p, s, r), (p , s, r ' )) |pRbp .rRfr ' }. Since inclusion-preserving \npruning does not have to respect the language, we can use much weaker (i.e., coarser) relations for comparing \nendpoints. Let .bw- be the variant of .bw where accepting states are not taken into consideration. P(.bw- \nTheorem 6.1. , .f ) is good for A , B-pruning. = P(.bw- Proof. Let P , .f ). One direction is trivial. \nFor the other direction, by contraposition, assume Prune(A, B, P) . B, but A . B . There exists a w .L \n(A) s.t. w ./L(B). There exists an initial s0s1 fair trace p = q0 -. q1 -. \u00b7 \u00b7 \u00b7 on w in A . There are \ntwo cases. si 1. p does not contain any transition qi -. qi+1 that is not present in Prune(A , B, P). \nThen p is also an initial fair trace on w in Prune(A, B , P), and thus we obtain w .L(Prune(A, B, P)) \nand w .L (B). Contradiction. si 2. p contains a transition qi -. qi+1 that is not present in ' ' si Prune(A, \nB , P). Therefore there exists a transition q -. q i i+1 ' ' in B s.t. qi .bw- q and qi+1 .f q . Thus \nthere exists an initial i i+1 fair trace on w in B and thus w .L(B ). Contradiction. We can approximate \n.bw- with (the transitive closure of) a cor\u00adresponding k-lookahead simulation .k-bw-, which is de.ned \nas .k-bw , except that only initial states are considered, i.e., the win\u00adning condition is C bw-(p0, \np1) .. .(i =0) \u00b7 pi .I =. qi .I. Let jk-bw- be the transitive closure of .k-bw-. Since GFP is .\u00addownward \nclosed and P(\u00b7, \u00b7) is monotone, we get this corollary. P(jk-bw- Corollary 6.2. , jk-f ) is good for A \n, B-pruning. Simplify B. Let A \u00d7B be the synchronized product of A and B. The idea is to remove states \nin B which cannot be reached in A \u00d7B. Let R be the set of states in A \u00d7B reachable from IA \u00d7IB , and \nlet ' X .QB be the projection of R to the B-component. We obtain B from B by removing all states ./X \nand their associated transitions. ' Although B B, this operation is clearly inclusion-preserving.  \n6.2 Jumping fair simulation as a better GFI relation We further generalize the GFI preorder jk-f by allowing \nDupli\u00adcator even more freedom. The idea is to allow Duplicator to take jumps during the simulation game \n(in the spirit of [9]). For a pre\u00adorder = on Q, in the game for =-jumping k-lookahead simulation Duplicator \nis allowed to jump to =-larger states before taking a ' si transition. Thus, a Duplicator s move is of \nthe form qi = qi -. si+1si+m-1 ' -. \u00b7 \u00b7 \u00b7 -. qi+m, and she eventually builds an in.\u00ad qi+1 =qi+1 nite \n=-jumping trace. We say that this trace is accepting at step iiff '' '' ' .q .F. qi =q =q , and fair \niff it is accepting in.nitely often. As i iiusual, =-jumping k-lookahead fair simulation holds iff Duplicator \nwins the corresponding game, with the fair winning condition lifted to jumping traces. Not all preorders \n= induce GFI jumping simulations. The pre\u00adorder =is called jumping-safe [9] if, for every word w, there \nexists a =-jumping initial fair trace on w iff there exists an initial fair non\u00adjumping one. Thus, jumping-safe \npreorders allows to convert jump\u00ading traces into non-jumping ones. Consequently, for a jumping-safe preorder \n=, =-jumping k-lookahead fair simulation is GFI. One can prove that .bw is jumping-safe, while .bw- is \nnot. We even improve .bw to a slightly more general jumping-safe relation .bw-c, by only requiring that \nDuplicator visits at least as many accepting states as Spoiler does, but not necessarily at the .bw-c \nsame time. Formally, pm qm iff, for every initial w-trace s0s1sm-1 p0 = p0 -. p1 -. \u00b7 \u00b7 \u00b7 -. pm, there \nexists an initial w-trace s0s1sm-1 p1 = q0 -. q1 -. \u00b7 \u00b7 \u00b7 -. qm, s.t. |{i|pi .F}| = |{i|qi .F}|. Theorem \n6.3. The preorder .bw-c is jumping-safe. Proof. Since .bw-c is re.exive, the existence of an initial \nfair trace on w directly implies the existence of a .bw-c-jumping initial fair trace on w. Now, we show \nthe reverse implication. Given two initial .bw-c\u00adp0 .bw-c ' s0' s1 jumping traces on w p0 = p0 -. p1 \n.bw-c p1 -. \u00b7 \u00b7 \u00b7 and q0 .bw-c ' -. q1 .bw-c ' s0s1 p1 = q q -. \u00b7 \u00b7 \u00b7 we de.ne C c(p0, p1) 0 1 j '' \n. F. pi .bw-c '' .bw-c ' '' iff |{i = j|.p p p }| = |{i = j|.q . i iii F. qi .bw-c '' .bw-c ' q q }|. \nWe say that an initial .bw-c-jumping i itrace on w is i-good iff it does not jump within the .rst i \nsteps. We show, by induction on i, the following property (P): For p0 .bw-c every i and every in.nite \n.bw-c-jumping initial trace p = s0s1 ' ' p -. p1 .bw-c p -. \u00b7 \u00b7 \u00b7 on w there exists an initial i-good \ntrace 0 1 s0s1si pi = q0 -. q1 -. \u00b7 \u00b7 \u00b7 -. qi \u00b7 \u00b7 \u00b7 on w s.t. C c(p, pi) and the suf.xes i of the traces \nare identical, i.e., qi = pi and p[i..] = pi[i..]. For the case base i = 0 we take p0 = p. Now we consider \nthe induction step. By induction hypothesis we get an initial i-good trace pi s.t. C c(p, pi) and qi \n= pi and p[i..] = pi[i..]. If pi is (i+ 1)\u00ad i good then we can take pi+1 = pi . Otherwise, pi contains \na step qi .bw-c ' si qi -. qi+1. First we consider the case where there exists '' qi .bw-c '' .bw-c ' \n a q . F s.t. q q . (Note that the i-th step in pi i ii '' ' can count as accepting in C c because q \n.F, even if qi and qi are i not accepting.) By def. of .bw-c there exists an initial trace p '' on a \n'' pre.x of w that ends in qi and visits accepting states at least as often as the non-jumping pre.x \nof pi that ends in qi. Again by de.nition of .bw-c there exists an initial trace p ' on a pre.x of w \nthat ends ' p '' in q and visits accepting states at least as often as . Thus p ' i visits accepting \nstates at least as often as the jumping pre.x of pi ' that ends in q (by the de.nition of C c). By composing \nthe traces i si ' we get pi+1 = p ' (q -. qi+1)pi[i+ 1..]. Thus pi+1 is an (i+ 1)\u00ad i good initial trace \non w and p[i+ 1..] = pi[i+ 1..] = pi+1[i+ 1..] and C c (pi , pi+1) and C c (p, pi+1). The other case \nwhere there is no i+1i+1 '' '' .bw-c '  q .F s.t. qi .bw-c q q is similar, but simpler. i ii Let p be \nan initial .bw-c-jumping fair trace on w. By property (P) and K \u00a8 onig s Lemma there exists an in.nite \ninitial non-jumping fair trace p ' on w. Thus .bw-c is jumping-safe. As a direct consequence, .bw-c-jumping \nk-lookahead fair simula\u00adtion is GFI. Since .bw-c is dif.cult to compute, we approximate it by a corresponding \nlookahead-simulation .k-bw-c which, in the same spirit, counts and compares the number of visits to accept\u00ading \nstates in every round of the k-lookahead backward simulation game. Let jk-bw-c be the transitive closure \nof .k-bw-c . Corollary 6.4. jk-bw-c -jumping k-lookahead fair sim. is GFI.  6.3 Advanced inclusion \nchecking algorithm Given these techniques, we propose the following algorithm for inclusion checking \nA . B. (1) Use the Heavy-k procedure to minimize A and B, and ad\u00additionally apply the inclusion-preserving \nminimization tech\u00adniques from Sec. 6. Lookahead simulations are computed not only on A and B, but also \nbetween them (i.e., on their disjoint union). Since they are GFI, we check whether they already wit\u00adness \ninclusion. Since many simulations are computed between partly minimized versions of A and B, this witnesses \ninclu\u00adsion much more often than checking fair simulation between the original versions. This step either \nstops showing inclusion, ' ' or produces smaller inclusion-equivalent automata A , B . (2) Check the \nGFI jk-bw-c-jumping k-lookahead fair simulation ' from Sec. 6.2 between A and B ' , and stop if the answer \nis yes. (3) If inclusion was not established in steps (1) or (2) then try to .nd a counterexample to \ninclusion. This is best done by a Ramsey-based method (optionally using simulation-based sub\u00adsumption \ntechniques), e.g., [1, 4]. Use a small timeout value, since in most non-included instances there exists \na very short counterexample. Stop if a counterexample is found. (4) If steps (1)-(3) failed (rare in \npractice), use any complete method, (e.g., Rank-based, Ramsey-based or Piterman s con\u00ad  ' struction) \nto check A . B ' . At least, it will bene.t from work\u00ad' ' ing on the smaller instance A , B produced \nby step (1). Note that steps (1)-(3) take polynomial time, while step (4) takes exponential time. (For \nthe latter, we recommend the improved Ramsey method of [1, 4] and the on-the-.y variant of Piterman s \nconstruction [29] implemented in GOAL [34].) This algorithm allows to solve much larger instances of \nthe inclusion problem than previous methods [3, 4, 15 17, 29, 31, 34], i.e., automata with 1000-20000 \nstates instead of 10-100 states; cf. Section 7. 7. Experiments We test the effectiveness of Heavy-k minimization \non Tabakov-Vardi random automata [33], on automata derived from LTL for\u00admulae, and on automata derived \nfrom mutual exclusion protocols, and compare it to the best previously available techniques imple\u00admented \nin GOAL [34]. A scalability test shows that Heavy-k has quadratic average-case complexity and it is vastly \nmore ef.cient than GOAL. Furthermore, we test our methods for language inclu\u00adsion on large instances \nand compare their performance to previous techniques. Due to space limitations, we only give a summary \nof the results, but all details and the runnable tools are available [2]. Unless otherwise stated, the \nexperiments were run with Java 6 on Intel Xeon X5550 2.67GHz and 14GB memory. Random automata. The Tabakov-Vardi \nmodel [33] generates random automata according to the following parameters: the num\u00adber of states n, \nthe size of the alphabet |S|, the transition density td (number of transitions, relative to n and |S|) \nand the acceptance density ad (percentage of accepting states). Apart from this, they do not have any \nspecial structure, and thus minimization and lan\u00adguage inclusion problem are harder for them than for \nautomata from other sources (see below). Random automata provide general reproducible test cases, on \naverage. Moreover, they are the only test cases that are guaranteed to be unbiased towards any particular \nmethod. Thus, it is a particular sign of quality if a method performs well even on these hard cases. \n The inherent dif.culty of the minimization problem, and thus also the effectiveness of minimization \nmethods, depends strongly on the class of random automata, i.e., on the parameters listed above. Thus, \none needs to compare the methods over the whole range, not just for one example. Variations in ad do \nnot affect Heavy-k much [2], but very small values make minimization harder for the other methods. By \nfar the most important parameter is td. The following .gure shows typical results. We take n = 100, |S|= \n2, ad = 0.5 and the range of td = 1.0, 1.1, . . . , 3.0. For each td we created 300 random automata, \nminimized them with different methods, and plotted the resulting average number of states after minimization. \nEach curve represents a different method: RD (just remove dead states), Light-1, Light-12, Heavy-1, and \nHeavy-12 and GOAL. The GOAL curve shows the best effort of all previous tech\u00adniques (as implemented in \nGOAL), which include RD, quotient\u00ading with backward and forward simulation, pruning of little brother \ntransitions and the fair simulation minimization of [19] (which sub\u00adsumes quotienting with delayed simulation). \n a F F0 e F y Sparse automata with low td have more dead states. For td =1.4no technique except RD \nhas any signi.cant effect. GOAL minimizes just slightly worse than Heavy-1 but it is no match for our \nbest tech\u00adniques. Heavy-12 vastly outperforms all others, particularly in the interesting range between \n1.4 and 2.5. Moreover, the minimization of GOAL (in particular the fair simulation minimization of [19]) \nis very slow. For GOAL, the average minimization time per automa\u00adton varies between 39s (at td = 1.0) \nand 612s (maximal at td = 2.9). In contrast, for Heavy-12, the average minimization time per au\u00adtomaton \nvaries between 0.012s (at td = 1.0) and 1.482s (max. at td = 1.7). So Heavy-12 minimizes not only much \nbetter, but also at least 400 times faster than GOAL (see also the scalability test). For td = 2.0, Heavy-12 \nyields very small automata. Many of these are even universal, i.e., with just one state and a univer\u00adsal \nloop. However, this frequent universality is not due to triv\u00adial reasons (otherwise simpler techniques \nlike Light-1 and GOAL would also recognize this). Consider the following question: Given Tabakov-Vardi \nrandom automata with parameters n, |S| and td, what is the probability U(n, |S|, td) that every state \nhas at least one outgoing transition for every symbol in S? (Such an automaton would be trivially universal \nif ad = 1.) Theorem 7.1. U(n, |S|, td) = (a(n, T)/\u00df(n, T))|S|, with T = n\u00b7 td, () ()() () m-n nm-in-1 \nn a(n, T) = .n 2 .n (-1)iand \u00df(n, T) = 2 m=n T-ni=0in-1 T Proof. For each symbol in S there are T = \nn \u00b7 td transitions and n 2 possible places for transitions, described as a grid. a(n, T) is the number \nof ways T items can be placed onto an n \u00d7n grid s.t. every row contains = 1 item, i.e., every state has \nan outgo\u00ading transition. \u00df(n, T) is the number of possibilities without this (2) n restriction, which \nis trivially T . Since the Tabakov-Vardi model chooses transitions for different symbols independently, \nwe have U(n, |S|, td) = (a(n, T)/\u00df(n, T))|S|. It remains to compute a(n, T). For the i-th row let xi \n. {1, . . . , n}be the maximal column contain\u00ading an item. The remaining T -n items can only be distributed \nto ((. xi)-n)lower columns. Thus a(n, T) = .x1,...,xn T-n . With m = . xi and a standard dice-sum problem \nfrom [28] the result follows. For n = 100, |S|= 2 we obtain the following values for U(n, |S|, td): 10-15 \nfor td = 2.0, 2.9\u00b7 10-5 for td = 3.0, 0.03 for td = 4.0, 0.3 for td = 5.0, 0.67 for td = 6.0, and 0.95 \nfor td = 8.0. So this transition saturation effect is negligible in our tested range with td =3.0. While \nHeavy-12 performs very well, an even smaller lookahead can already be suf.cient for a good minimization. \nHowever, this depends very much on the density td of the automata. The following chart shows the effect \nof the lookahead by comparing Heavy-k for varying k on different classes of random automata with different \ndensity td = 1.6, 1.7, 1.8, 1.9, 2.0. We have n = 100, |S|= 2 and ad = 0.5, and every point is the average \nof 1000 automata.  Fair/delayed simulation is not much larger than direct simulation for k = 1, but \nthey bene.t strongly from higher k. Backward sim\u00adulation increases only slightly (e.g., from 365 to 381 \npairs for ad = 0.9). Initially, it seems as if backward/direct simulation does not bene.t from higher \nk if ad is small (on random automata), but this is wrong. Even random automata get less random during \nthe Heavy-k minimization process, making lookahead more effec\u00adtive for backward/direct simulation. Consider \nthe case of n = 300, td = 1.8 and ad = 0.1. Initially, the average ratio | j12-di |/| j1-di | is 1.00036, \nbut after quotienting with j12-de this ratio is 1.103. LTL. For model checking [22], LTL-formulae are \nconverted into B\u00a8 uchi automata. This conversion has been extensively studied and there are many different \nalgorithms which try to construct the smallest possible automaton for a given formula (see references \nin [34]). It should be noted however, that LTL is designed for hu\u00adman readability and does not cover \nthe full class of .-regular lan\u00adguages. Moreover, B \u00a8 uchi Store [35] contains handcrafted automata for \nalmost every human-readable LTL-formula and none of these automata has more than 7 states. Still, since \nmany people are inter\u00adested in LTL to automata conversion, we tested how much our min\u00adimization algorithm \ncan improve upon the best effort of previous techniques. For LTL model checking, the size of the automata \nis not the only criterion [30], since more non-determinism also makes the problem harder. However, our \ntransition pruning techniques only make an automaton more deterministic . Using a function of GOAL, we \ncreated 300 random LTL\u00adformulae of nontrivial size: length 70, 4 predicates and probability weights 1 \nfor boolean and 2 for future operators. We then con\u00adverted these formulae to B\u00a8uchi automata and minimized \nthem with 1 2 3 4 5 6 7 8 9 1A 11 12 LookaCDad The big advantage of Heavy-12 over Light-12 is due to \nthe prun\u00ading techniques. However, these only reach their full potential at higher lookaheads (thus the \nsmaller difference between Heavy-1 and Light-1). Indeed, the simulation relations get much denser with \nhigher lookahead k. We consider random automata with n = 100, |S|= 2 and td = 1.8 (a nontrivial case; \nlarger td yield larger simu\u00adlations). We let ad = 0.1 (resp. ad = 0.9), and plot the size of fair, delayed, \ndirect, and backward simulation as k increases from 1 to 12. Every point is the average of 1000 automata. \nBC1AADEFdC1 8DEadCA 1 4AAA 35AA GOAL. Of the 14 different converters implemented in GOAL we chose LTL2BA \n[18] (which is also used by the SPIN model checker [22]), since it was the only one which could handle \nsuch large for\u00admulae. (The second best was COUVREUR which succeeded on 90% of the instances, but produced \nmuch larger automata than LTL2BA. The other converters ran out of time (4h) or memory (14GB) on most \ninstances.) We thus obtained 300 automata and minimized them with GOAL. The resulting automata vary signi.\u00adcantly \nin size from 1 state to 1722 states [2]. Then we tested how much further these automata could be re\u00adduced \nin size by our Heavy-12 method [2]. In summary, 82% of the automata could be further reduced in size. \nThe average number of states declined from 138 to 78, and the average number of transi\u00adtions from 3102 \nto 1270. Since larger automata have a dispropor\u00ad air Eo E FaFe EiBE i laFioBErelaFioB ai r Eo E FaFe \nEiBE i laFio BErel aFioB 3AAA 25AA 2AAA 15AA 1AAA 5AA A 4AAA 35AA 3AAA 25AA 2AAA 15AA 1AAA 5AA A Fair \nDelayed DirecF Backward  Lookahea d BC1AADEFdC1 8DEa dC A 9 Fair Delayed DirecF Bac kward Lookahead \n tionate effect on averages, we also computed the average reduction ratio per automaton, i.e., (1/300) \n.300 newsizei/oldsizei. (Note the i=1 difference between the average ratio and the ratio of averages.) \nThe average ratio was 0.76 for states and 0.68 for transitions. The com\u00adputation times for minimization \nvary a lot due to different automata sizes (average 122s), but were always less than the time used by \nthe LTL to automata translation. If one only considers the 150 automata above median size (30 states) \nthen the results are even stronger. 100% of these automata could be further reduced in size. The av\u00aderage \nnumber of states declined from 267 to 149, and the average number of transitions from 6068 to 2435. The \naverage reduction ratio was 0.65 for states and 0.54 for transitions. To conclude, our minimization can \nsigni.cantly improve the quality of LTL to au\u00adtomata translation with a moderate overhead. Mutual exclusion \nprotocols. We consider automata derived from mutual exclusion protocols. The protocols were described \nin a language of guarded commands and automatically translated into B\u00a8 uchi automata, whose size is given \nin the column Original . By row, the protocols are Bakery.1, Bakery.2, Fischer.3.1, Fischer.3.2, Fischer.2, \nPhils.1.1, Phils.2 and Mcs.1.2. We minimize these au\u00adtomata with GOAL and with our Heavy-12 method and \ndescribe the sizes of the resulting automata and the runtime in subsequent erage computation time of \nHeavy-k on automata of size 100 and columns (Java 6 on Intel i7-740, 1.73 GHz). In some instances varying \ntd and k. The most dif.cult cases are those where min-GOAL ran out of time (2h) or memory (14GB). imization \nis possible (and thus the alg. does not give up quickly), but does not massively reduce the size of the \ninstance. For Heavy-k, Original GOAL Time Heavy-12 Time Heavy-12 Trans. States Tr. St. GOAL Tr. St. 2597 \n1506 N/A N/A > 2h 696 477 6.17s 2085 1146 N/A N/A > 2h 927 643 9.04s 1401 638 14 10 15.38s 14 10 1.16s \n3856 1536 212 140 4529s 96 70 5.91s 67590 21733 N/A N/A oom(14GB) 316 192 325.76s 464 161 362 134 540.3s \n359 134 11.51s 2350 581 284 100 164.2s 225 97 4.04s 21509 7968 108 69 2606.7s 95 62 48.18s  this peak \nis around td = 1.6, 1.7 (like in the scalability test). Scalability. We test the scalability of Heavy-12 \nminimization by applying it to Tabakov-Vardi random automata of increas\u00ading size but .xed td, ad and \nS. We ran four separate tests with td = 1.4, 1.6, 1.8 and 2.0. In each test we .xed ad = 0.5, |S|= 2 \nand increased the number of states from n = 50 to n = 1000 in increments of 50. For each parameter point \nwe created 300 random automata and minimized them with Heavy-12. We analyze the av\u00aderage size of the \nminimized automata in percent of the original size n, and how the average computation time increases \nwith n. For td = 1.4 the average size of the minimized automata stays around 77% of the original size, \nregardless of n. For td = 1.6 it stays around 65%. For td = 1.8 it decreases from 28% at n = 50 to 2% \nat n = 1000. For td = 2.0 it decreases from 8% at n = 50 to < 1% at n = 1000 [2]. Note that the lookahead \nof 12 did not change with n. Surprisingly, larger automata do not require larger lookahead for a good \nminimization. We plot the average computation time (measured in ms) in n and then compute the optimal \n.t of the function time = a * n b to the data by the least-squares method, i.e., this computes the pa\u00adrameters \na and b of the function that most closely .ts the experi\u00admental data. The important parameter is the \nexponent b. For td = 2.05 1.4, 1.6, 1.8, 2.0 we obtain 0.018 * n 2.14, 0.32 * n 2.39, 0.087 * n and 0.055 \n* n 2.09, respectively. Thus, the average-case complexity of Heavy-12 scales (almost) quadratically. \nThis is especially sur\u00adprising given that Heavy-12 does not only compute one simula\u00adtion relation but \npotentially many simulations until the repeated minimization reaches a .xpoint. Quadratic complexity \nis the very best one can hope for in any method that explicitly compares states/transitions by simulation \nrelations, since the relations them\u00adselves are of quadratic size. Lower complexity is only possible with \npure partition re.nement techniques (e.g., bisimulation, which is O(nlogn)), but these achieve even less \nminimization than quotient\u00ading with direct simulation (i.e., next to nothing on hard instances). 500000 \n Language Inclusion Checking. We test the language inclusion checking algorithm of Section 6.3 (with \nlookahead up-to 15) on nontrivial instances and compare its performance to previous tech\u00adniques like \nordinary fair simulation checking and the best effort of GOAL (which uses simulation-based minimization \nfollowed by an on-the-.y variant of Piterman s construction [29, 34]). In this test we use only the polynomial \ntime steps (1)-(3) of our algorithm, thus it may fail in some instances. We consider pairs of Tabakov-Vardi \nrandom automata with 1000 states each, |S|= 2 and ad = 0.5. For each separate case of td = 1.6, 1.8 and \n2.0, we create 300 such au\u00adtomata pairs and check if language inclusion holds. (For td < 1.6 inclusion \nrarely holds, except trivially if one automaton has empty language. For td > 2 inclusion often holds \nbut is easier to prove.) For td = 1.6 our algorithm solved 294 of 300 instances (i.e., 98%): 45 included \n(16 in step (1) and 29 in step (2)), 249 non\u00adincluded (step (3)), and 6 failed. Average computation time \n1167s. Ordinary fair simulation solved only 13 included instances. GOAL (timeout 60min, 14GB memory) \nsolved only 13 included instances (the same 13 as fair simulation) and 155 non-included instances. For \ntd = 1.8 our algorithm solved 297 of 300 instances (i.e., 99%): 104 included (103 in step (1) and 1 in \nstep (2)) and 193 non\u00adincluded (step (3)) and 3 failed. Average computation time 452s. Ordinary fair \nsimulation solved only 5 included instances. GOAL (timeout 30min, 14GB memory) solved only 5 included \ninstances (the same 5 as fair simulation) and 115 non-included instances. For td = 2.0 our algorithm \nsolved every instance: 143 included (shown in step (1)) and 157 non-included (step (3)). Average com\u00ad \n 450000 putation time 258s. Ordinary fair simulation solved only 1 of the 400000 350000 143 included \ninstances. GOAL (timeout 30min, 14GB memory) solved only 1 of 143 included instances (the same one as \nfair sim\u00ad ulation) and 106 of 157 non-included instances. 8. Conclusion and Future Work Our automata \nminimization techniques perform signi.cantly better than previous methods. In particular, they can be \napplied to solve PSPACE-complete automata problems like language inclusion for much larger instances. \nWhile we presented our methods in the con\u00adtext of B uchi automata, \u00a8most of them trivially carry over \nto the simpler case of automata over .nite words. Future work includes more ef.cient algorithms for computing \nlookahead simulations, ei\u00ad time in ms 300000 250000 200000 150000 100000 50000 0 Average time for Heavy-12 \nminimization for td=1.4,1.6,1.8, 2.0 with y=a*xb fit. The computation time of Heavy-k depends on the \nclass of au-ther along the lines of [20] for normal simulation, or by using sym\u00adtomata, i.e., on the \ndensity td, as the scalability test above shows. bolic representations of the relations. Moreover, we \nare applying Moreover, it also depends on k. The following graph shows the av-similar techniques to minimize \ntree-automata. References [1] RABIT tool: www.languageinclusion.org/doku.php?id=tools. [2] L. Clemente \nand R. Mayr. Advanced Automata Minimization. Tech. Rep. EDI-INF-RR-1414, U. of Edinburgh (2012), arXiv:1210.6624 \nhttp://www.inf.ed.ac.uk/publications/report/1414.html. [3] P. Abdulla, Y.-F. Chen, L. Clemente, L. Holik, \nC.-D. Hong, R. Mayr, and T. Vojnar. Simulation Subsumption in Ramsey-Based B\u00a8uchi Automata Universality \nand Inclusion Testing. In T. Touili, B. Cook, and P. Jackson, editors, Computer Aided Ver\u00ad i.cation, \nvolume 6174 of LNCS, pages 132 147, 2010. ISBN 978-3-642-14294-9. doi: 10.1007/978-3-642-14295-6 14. \nURL http://dx.doi.org/10.1007/978-3-642-14295-6 14. [4] P. Abdulla, Y.-F. Chen, L. Clemente, L. Holik, \nC.-D. Hong, R. Mayr, and T. Vojnar. Advanced Ramsey-based B \u00a8 uchi Automata Inclusion Testing. In J.-P. \nKatoen and B. K\u00a8onig, editors, International Confer\u00adence on Concurrency Theory, volume 6901 of LNCS, \npages 187 202, Sept. 2011. [5] P. A. Abdulla, Y.-F. Chen, L. Hol\u00b4ik, and T. Vojnar. Mediating for reduction \n(on minimizing alternating B\u00a8uchi automata). In FSTTCS, volume 4 of LIPIcs, pages 1 12. Schloss Dagstuhl \n-Leibniz-Zentrum fuer Informatik, 2009. [6] P. A. Abdulla, Y.-F. Chen, L. Holik, R. Mayr, and T. Vojnar. \nWhen Simulation Meets Antichains. In Tools and Algorithms for the Con\u00adstruction and Analysis of Systems, \nvolume 6015 of LNCS, 2010. URL http://hal.inria.fr/inria-00460294/en/. [7] D. Bustan and O. Grumberg. \nSimulation-based minimization. ACM Trans. Comput. Logic, 4:181 206, April 2003. ISSN 1529-3785. doi: \nhttp://doi.acm.org/10.1145/635499.635502. URL http://doi.acm.org/10.1145/635499.635502. [8] L. Clemente. \nB\u00a8uchi Automata Can Have Smaller Quotients. In L. Aceto, M. Henzinger, and J. Sgall, editors, ICALP, \nvol\u00adume 6756 of LNCS, pages 258 270. 2011. ISBN 978\u00ad3-642-22011-1. doi: 10.1007/978-3-642-22012-8 20. \nURL http://arxiv.org/pdf/1102.3285. [9] L. Clemente. Generalized Simulation Relations with Applications \nin Automata Theory. PhD thesis, University of Edinburgh, 2012. [10] D. L. Dill, A. J. Hu, and H. Wont-Toi. \nChecking for Language Inclu\u00adsion Using Simulation Preorders. In Computer Aided Veri.cation, vol\u00adume 575 \nof LNCS. Springer-Verlag, 1991. doi: 10.1007/3-540-55179\u00ad 4 25. URL http://dx.doi.org/10.1007/3-540-55179-4 \n25. [11] L. Doyen and J.-F. Raskin. Antichains Algorithms for Finite Au\u00adtomata. In Tools and Algorithms \nfor the Construction and Analysis of Systems, volume 6015 of LNCS, pages 2 22. Springer-Verlag, 2010. \n[12] K. Etessami. A Hierarchy of Polynomial-Time Computable Simulations for Automata. In International \nConference on Concurrency Theory, volume 2421 of LNCS, pages 131 144. Springer-Verlag, 2002. doi: 10.1007/3-540-45694-5 \n10. URL http://dx.doi.org/10.1007/3-540-45694-5 10. [13] K. Etessami and G. Holzmann. Optimizing B\u00a8uchi \nAutomata. In International Conference on Concurrency Theory, volume 1877 of LNCS, pages 153 168. Springer-Verlag, \n2000. [14] K. Etessami, T. Wilke, and R. A. Schuller. Fair Simu\u00adlation Relations, Parity Games, and State \nSpace Reduc\u00adtion for uchi SIAM Comput., B\u00a8Automata. J. 34(5):1159 1175, 2005. doi: 10.1137/S0097539703420675. \nURL http://epubs.siam.org/sam-bin/dbq/article/42067. [15] S. Fogarty and M. Vardi. B\u00a8 uchi Complementation \nand Size-Change Termination. In S. Kowalewski and A. Philippou, editors, Tools and Algorithms for the \nConstruction and Analysis of Systems, volume 5505 of LNCS, pages 16 30. 2009. doi: 10.1007/978-3-642-00768-2 \n2. URL http://dx.doi.org/10.1007/978-3-642-00768-2 2. [16] S. Fogarty and M. Y. Vardi. Ef.cient B\u00a8uchi \nUniversality Checking. In Tools and Algorithms for the Construction and Analysis of Systems, pages 205 \n220, 2010. [17] S. Fogarty, O. Kupferman, M. Y. Vardi, and T. Wilke. Unify\u00ading B\u00a8uchi Complementation \nConstructions. In M. Bezem, edi\u00adtor, Computer Science Logic, volume 12 of LIPIcs, pages 248 263. Schloss \nDagstuhl Leibniz-Zentrum fuer Informatik, 2011. doi: http://dx.doi.org/10.4230/LIPIcs.CSL.2011.248. \n[18] P. Gastin and D. Oddoux. Fast LTL to B\u00a8uchi automata translation. In CAV, volume 2102 of LNCS, pages \n53 65. Springer, 2001. [19] S. Gurumurthy, R. Bloem, , and F. Somenzi. Fair simulation mini\u00admization. \nIn CAV, volume 2404 of LNCS, pages 610 624. Springer, 2002. [20] M. R. Henzinger, T. A. Henzinger, and \nP. W. Kopke. Com\u00adputing simulations on .nite and in.nite graphs. In Founda\u00adtions of Computer Science, \nFOCS 95, Washington, DC, USA, 1995. IEEE Computer Society. ISBN 0-8186-7183-1. URL http://portal.acm.org/citation.cfm?id=796255. \n[21] T. A. Henzinger, O. Kupferman, and S. K. Rajamani. Fair Simulation. Information and Computation, \n173: 64 81, 2002. doi: 10.1006/inco.2001.3085. URL http://dx.doi.org/10.1006/inco.2001.3085. [22] G. \nHolzmann. The SPIN Model Checker. Addison-Wesley, 2004. [23] T. Jiang and B. Ravikumar. Minimal NFA Problems \nare Hard. In J. Albert, B. Monien, and M. Artalejo, editors, ICALP, volume 510 of LNCS, pages 629 640. \n1991. doi: 10.1007/3-540-54233-7 169. [24] S. Juvekar and N. Piterman. Minimizing Generalized B\u00a8uchi \nAu\u00adtomata. In Computer Aided Veri.cation, volume 4414 of LNCS, pages 45 58. Springer-Verlag, 2006. doi: \n10.1007/11817963 7. URL http://dx.doi.org/10.1007/11817963 7. [25] O. Kupferman and M. Vardi. Veri.cation \nof Fair Tran\u00adsition Systems. In Computer Aided Veri.cation, volume 1102 of LNCS, pages 372 382. Springer-Verlag, \n1996. URL http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.29.9654. [26] C. S. Lee, N. D. Jones, \nand A. M. Ben-Amram. The size-change principle for program termination. POPL 01, pages 81 92, 2001. doi: \nhttp://doi.acm.org/10.1145/360204.360210. [27] J. Leroux and G. Point. TaPAS: The Talence Presburger \nArithmetic Suite. In Proceedings of the 15th International Conference on Tools and Algorithms for the \nConstruction and Analysis of Systems (TACAS), volume 5505 of LNCS. Springer, 2009. [28] I. Niven. Mathematics \nof Choice. The Mathematical Association of America, 1965. [29] N. Piterman. From nondeterministic B\u00a8uchi \nand Streett automata to deterministic parity automata. In LICS, pages 255 264. IEEE, 2006. [30] R. Sebastiani \nand S. Tonetta. More deterministic vs. smaller B\u00a8uchi automata for ef.cient LTL model checking. In Correct \nHardware Design and Veri.cation Methods, volume 2860 of LNCS, 2003. [31] A. P. Sistla, M. Y. Vardi, and \nP. Wolper. The complemen\u00adtation problem for B\u00a8uchi automata with applications to tem\u00adporal logic. Theor. \nComput. Sci., 49:217 237, Jan. 1987. ISSN 0304-3975. doi: 10.1016/0304-3975(87)90008-9. URL http://dx.doi.org/10.1016/0304-3975(87)90008-9. \n[32] F. Somenzi R. Bloem. uchi fromand Ef.cient B\u00a8Automata LTL Formulae. In Computer Aided Veri.cation, \nvolume 1855 of LNCS, pages 248 263. Springer-Verlag, 2000. doi: 10.1007/10722167 21. URL http://dx.doi.org/10.1007/10722167 \n21. [33] D. Tabakov and M. Vardi. Model Checking B\u00a8Speci.cations.uchi In LATA, volume Report 35/07. Research \nGroup on Mathematical Linguistics, Universitat Rovira i Virgili, Tarragona, 2007. [34] Y.-K. Tsay, Y.-F. \nChen, M.-H. Tsai, W.-C. Chan, and C.-J. Luo. GOAL extended: Towards a research tool for omega automata \nand temporal logic. In C. Ramakrishnan and J. Rehof, editors, Tools and Algo\u00adrithms for the Construction \nand Analysis of Systems, volume 4963 of LNCS, pages 346 350. 2008. ISBN 978-3-540-78799-0. URL http://dx.doi.org/10.1007/978-3-540-78800-3 \n26. [35] Y.-K. Tsay, M.-H. Tsai, J.-S. Chang, and Y.-W. Chang. B\u00a8uchi store: An open repository of B\u00a8automata. \nIn P. Abdulla uchi and K. Leino, editors, Tools and Algorithms for the Con\u00ad struction and Analysis of \nSystems, volume 6605 of LNCS, pages 262 266. 2011. ISBN 978-3-642-19834-2. URL http://dx.doi.org/10.1007/978-3-642-19835-9 \n23. 10.1007/978-3-642-19835-9 23.  \n\t\t\t", "proc_id": "2429069", "abstract": "<p>We present an efficient algorithm to reduce the size of nondeterministic Buchi word automata, while retaining their language. Additionally, we describe methods to solve PSPACE-complete automata problems like universality, equivalence and inclusion for much larger instances (1-3 orders of magnitude) than before. This can be used to scale up applications of automata in formal verification tools and decision procedures for logical theories.</p> <p>The algorithm is based on new transition pruning techniques. These use criteria based on combinations of backward and forward trace inclusions. Since these relations are themselves PSPACE-complete, we describe methods to compute good approximations of them in polynomial time.</p> <p>Extensive experiments show that the average-case complexity of our algorithm scales quadratically. The size reduction of the automata depends very much on the class of instances, but our algorithm consistently outperforms all previous techniques by a wide margin. We tested our algorithm on Buchi automata derived from LTL-formulae, many classes of random automata and automata derived from mutual exclusion protocols, and compared its performance to the well-known automata tool GOAL.</p>", "authors": [{"name": "Richard Mayr", "author_profile_id": "81100006540", "affiliation": "University of Edinburgh, Edinburgh, United Kingdom", "person_id": "P3977917", "email_address": "homepages.inf.ed.ac.uk/rmayr", "orcid_id": ""}, {"name": "Lorenzo Clemente", "author_profile_id": "81474697245", "affiliation": "LaBRI, University of Bordeaux I, Bordeaux, France", "person_id": "P3977918", "email_address": "lorenzo.clemente@labri.fr", "orcid_id": ""}], "doi_number": "10.1145/2429069.2429079", "year": "2013", "article_id": "2429079", "conference": "POPL", "title": "Advanced automata minimization", "url": "http://dl.acm.org/citation.cfm?id=2429079"}