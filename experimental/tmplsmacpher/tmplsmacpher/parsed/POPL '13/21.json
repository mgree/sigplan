{"article_publication_date": "01-23-2013", "fulltext": "\n Fault Tolerance via Idempotence G. Ramalingam and Kapil Vaswani Microsoft Research, India grama,kapilv@microsoft.com \nAbstract Building distributed services and applications is challenging due to the pitfalls of distribution \nsuch as process and communication failures. A natural solution to these problems is to detect potential \nfailures, and retry the failed computation and/or resend messages. Ensuring correctness in such an environment \nrequires distributed services and applications to be idempotent. In this paper, we study the inter-related \naspects of process fail\u00adures, duplicate messages, and idempotence. We .rst introduce a simple core language \n(based on .-calculus) inspired by modern dis\u00adtributed computing platforms. This language formalizes the \nnotions of a service, duplicate requests, process failures, data partitioning, and local atomic transactions \nthat are restricted to a single store. We then formalize a desired (generic) correctness criterion for \napplications written in this language, consisting of idempotence (which captures the desired safety properties) \nand failure-freedom (which captures the desired progress properties). We then propose language support \nin the form of a monad that automatically ensures failfree idempotence. A key characteristic of our implementation \nis that it is decentralized and does not require distributed coordination. We show that the language \nsupport can be enriched with other useful constructs, such as compensations, while retaining the coordination-free \ndecentralized nature of the implementation. We have implemented the idempotence monad (and its variants) \nin F# and C# and used our implementation to build realistic appli\u00adcations on Windows Azure. We .nd that \nthe monad has low runtime overheads and leads to more declarative applications. Categories and Subject \nDescriptions D.4.5 [Operating Systems]: Reliability Fault-tolerance; C.2.4 [Computer-Communication Net\u00adworks]: \nDistributed Systems Client/server, Distributed applica\u00adtions General Terms Reliability, Languages, Design \nKeywords fault tolerance, idempotence, work.ow, transaction, monad 1. Introduction Distributed computing \nis becoming mainstream. Several modern platforms offer virtualized distributed systems at low entry cost \nwith the promise of scaling out on demand. But distributed comput- Permission to make digital or hard \ncopies of all or part of this work for personal or classroom use is granted without fee provided that \ncopies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. POPL 13, January 23 25, 2013, Rome, Italy. \nCopyright c &#38;#169; 2013 ACM 978-1-4503-1832-7/13/01. . . $10.00 ing comes with its own pitfalls, \nsuch as process failures, imperfect messaging, asynchrony and concurrency. Consider the prototypical \nbank account transfer service in Fig. 1. The goal of the service is to transfer money between bank accounts, \npotentially in different banks. If the accounts belong to different banks, ensuring that the transfer \nexecutes as an atomic (distributed) transaction is usually not feasible, and the natural way of expressing \nthis computation is as a work.ow [10, 20] consisting of two steps, a debit followed by a credit. What \nif the process executing the work.ow fails in between the debit and credit steps? A natural solution \nis to detect this failure and ensure that a different process completes the remaining steps of the work.ow. \nA challenging1 aspect of realizing this solution is .guring out whether the original process failed before \nor after completing a particular step (either debit or credit). If not done carefully, the debit or credit \nstep may be executed multiple times, leading to further correctness concerns. Services often rely on \na central work.ow manager to manage process failures during the work.ow (using distributed transactions). \nNow consider a (seemingly) different problem. Messages sent between the client initiating the transfer \nand the service may be lost. The only option for a client, when it does not receive a response within \nsome reasonable time, is to resend its request. Yet the client does not want the transfer to occur twice! \nIn this paper, we study process and communication failures in the context of work.ows. The seemingly \ndifferent problems caused by process and communication failures are, in fact, inter-related. Idempotence, \na correctness criterion that requires the system to tol\u00aderate duplicate requests, is the key to handling \nboth communication and process failures ef.ciently. Idempotence, when combined with retry, gives us the \nessence of a work.ow, a fault tolerant composi\u00adtion of atomic actions, for free without the need for \ndistributed co\u00adordination. In the transfer example, a fault tolerant account trans\u00adfer can be implemented \nwithout a central work.ow manager if the debit and credit steps can be designed to be idempotent, Formalizing \nFailfree Idempotence. In this paper, we introduce a simple core language .FA I L , inspired by contemporary \ncloud plat\u00adforms. This language formalizes process failure, duplicate requests, partitioned data, and \nlocal transactions. A local transaction pro\u00advides ACID guarantees but is restricted to access data within \na sin\u00adgle partition (typically a single server). Computations in .FAIL are like work.ows, but without \nany fault-tolerance guarantees for the composition (i.e., the computation may fail between transactions). \nWe then formalize a generic correctness criterion for applica\u00adtions written in .FA I L . A simple, powerful \nand tempting criterion is that an application s behavior in the presence of duplicate requests and process \nfailures should be indistinguishable from its behav\u00adior in the absence of duplicate requests and failures. \nWe formal\u00ad 1 In general, detecting failures perfectly in an asynchronous, message pass\u00ading system is \nimpossible [8]. Conservative failure detection can also lead to the same problem of duplicated computation. \n let process (request) = match request with | ( getBalance , (branch, account)) . atomic branch {lookup \naccount} | ( transfer , (fromBranch, fromAccount, toBranch, toAccount, amt) . atomic fromBranch { update \nfromAccount ((lookup fromAccount) - amt) }; atomic toBranch { update toAccount ((lookup toAccount) + \namt) }; Transfer complete. Figure 1: A banking service example, in syntactically sugared .FAIL , that \nis neither idempotent nor fault-tolerant. ize a slightly weaker, but more appropriate, correctness criterion, \nnamely failure-freedom modulo message duplication. Informally, this criterion permits the system to send \nduplicate responses. This weakening is appropriate from the perspective of composition: if the recipient \nof the responses can also tolerate duplicate messages, then the sender is freed of the obligation to \nsend the response ex\u00adactly once. Automating Idempotence. Next, we address the problem of auto\u00admatically \nensuring idempotence for a service. We present our solu\u00adtion as a monad, the idempotence monad. We then \nshow that idem\u00adpotence, when coupled with a simple retry mechanism, provides a free solution to the problem \nof tolerating process failures, guaran\u00adteeing failure-freedom modulo message duplication. We then pro\u00adpose \ndedicated language support for idempotent computations. Decentralized Idempotence and Work.ow. The idea \nunderlying the idempotence monad is conceptually simple, but tedious to im\u00adplement manually (i.e., without \nthe monad). Given a unique iden\u00adti.er associated with a computation, the monad essentially adds logging \nand checking to each effectful step in the work.ow to en\u00adsure idempotance. An important characteristic \nof our implementa\u00adtion (of the monad) is that it is designed to work with contemporary distributed storage \nsystems such as key-value tables. Speci.cally, it does not assume the presence of dedicated storage for \nlogs that can be accessed atomically with each transaction. The monad reuses the underlying store (in \nthis case a key-value table) to simulate a distinct address space for logging. This leads to a decentralized \nimplementation of idempotence that does not require any centralized storage or any (distributed) coordination \nbetween different stores. Thus, the implementation of idempotance preserves the decentralized nature \nof the under\u00adlying computation. This, in turn, leads to a completely decentral\u00adized implementation of \na (fault-tolerant) work.ow, unlike tradi\u00adtional work.ow implementations, which use a centralized work\u00ad.ow \ncoordinator and/or a centralized repository for runtime status information. Extensions. We then extend \nthe idempotence monad with other useful constructs, while preserving the decentralized nature of the \nconstruct. One extension allows the application to associate each transaction in a work.ow with a compensating \naction. Another ex\u00adtension allows the application to generate intermediate responses to client requests \nand then asynchronously retry the requests on client s behalf. This idiom, especially useful in long \nrunning com\u00adputations, frees the client from having to track status of requests, and leads to more responsive \nclients. Implementation. We have implemented the idempotence work\u00ad.ow monad in F# targeting the Windows \nAzure platform. We have implemented several realistic applications using the idempotence work.ow monad \n[2, 4]. We .nd that the core business logic in these applications can be declaratively expressed using \nthe monad. Our evaluation shows that performance overheads of using the monad over hand-coded implementations \nare statistically insigni.cant. The rest of the paper is organized as follows. In Section 2, we introduce \na language .FA I L and formalize duplicate requests and process failures. We formalize what it means \nfor a .FA I L application to correctly tolerate duplicate requests and failures. In Section 3, we present \nthe idempotence monad and show how it can be used to tolerate duplicate requests as well as process failures. \nIn Section 4, we describe extensions of the idempotence construct. In Section 5, we evaluate the idempotence \nmonad and our implementation from the perspective of expressiveness, bene.ts and overheads. Section 6 \ndiscusses related work. 2. Failfree Idempotence In this section we present a language .FAI L that distils \nessential el\u00adements of distributed computing platforms such as Windows Azure and formalize the concept \nof failfree idempotence. 2.1 The Language .FAIL Informal Overview. A .FA I L program e represents a service \nthat receives input requests and produces output responses. An input request v is processed by creating \nan agent to evaluate e v. When ' the evaluation of e v terminates, producing a value v, v' is sent back \nas the response. Multiple input requests can be processed con\u00adcurrently, and their evaluation can be \ninterleaved. Shared, mutable, persistent data is stored in tables. Agents. An agent has its own internal \nstate (captured by local variables of the code). An agent may fail at any point in time. A failure models \nproblems such as hardware failure, software crashes and reboots . Data stored in tables is persistent \nand is unaffected by agent failures. Tables. Tables are persistent maps. They provide primitives to update \nthe value bound to a key and lookup up the value asso\u00adciated with a key. The language provides a limited \nform of an atomic transaction, which enables a set of operations on the same table to be performed transactionally. \nSpeci.cally, the construct atomic t e evaluates e in the context of table t (lookups and updates are \npermitted only on table t within e), guaranteeing iso\u00adlation (no other agent can access t in the middle \nof e s evaluation) and the all-or-nothing property (no process failure can happen in the middle of e \ns evaluation). Example. Fig. 1 presents a simple bank-account-transfer exam\u00adple in syntactically sugared \n.FA IL . This example is neither idempo\u00adtent (e.g., executing the same transfer request twice is not \nequiva\u00adlent to executing it once) nor fault-tolerant (e.g., if the agent pro\u00adcessing a transfer request \nfails in between the debit and credit steps). Syntax. Fig. 2 presents the syntax of .FA I L , which extends \n.\u00adcalculus with the primitive operations on tables explained above. In the rest of the paper, we will \nuse extensions such as natural numbers, arithmetic operators, and ordered pairs for brevity. These can \nbe encoded in the core language or added to it in the usual fashion. We also use syntactic sugar, such \nas lookup e, where e . (Val), as shorthand, e.g., for (.x.lookup x)e. Semantic Domains Fig. 2 de.nes \nthe semantic domains used in the semantics of .FA I L . Let (Val) denote the set of all basic values: \nthese basically consist of function abstractions. (As usual, natural numbers, string constants, ordered \npairs of values, etc. can all be encoded within (Val) or added to it.) Let (Val)opt represent the set \nof optional values (of the form NONE or SOME v). An element of SST (a map from (Val) to (a) Evaluation \nContexts E ::= [\u00b7] | E e | v E (b) The Set of Evaluation Rules A used to de.ne the standard semantics \nof .FAIL . [I N PUT] [OUTPUT ] v . (Val) vo . (Val) in(v) out(vi,vo) (e, \u00b5, a, I , O) . (e, \u00b5, a l {v \nC (e v)}, I . {v}, O) (e, \u00b5, a l {vi C vo}, I , O) . (e, \u00b5, a, I , O . {(vi, vo)}) [NORMAL] [FAIL] .' \n' (\u00b5, e) (\u00b5 ,e ) ' ' (p, \u00b5, a l {v C e}, I , O) . (p, \u00b5 ' , a l {v C e ' }, I , O) (p, \u00b5, a l {v C e}, \nI , O) . (p, \u00b5, a, I , O) [ATOMIC] [UPDATE] [LOOKUP] . * (\u00b5[tn], e()) (t, v), v . (Val) (t,k,v) '. (\u00b5, \natomic tn e) (\u00b5[tn . t], v) (t, update k v) (t[k . (SOME v)], 0) (t, lookup k) (t, t[k]) [CONTEXT] [ \nCONTEXT] [LIFT ] [LIF T] u [BETA] '' ' .'' ' (t, e) (t , e ) (\u00b5, e) (\u00b5 , e ) e . e e . e u' ' .' ' '' \n'' (t, E[e]) (t , E [e ]) (\u00b5, E [e]) (\u00b5 , E [e ]) (t, e) (t, e ) (\u00b5, e) (\u00b5, e ) (.x.e) v . e[v/x] [ RETRY] \n v . I ' (e, \u00b5, a, I , O) . (e, \u00b5, a l {v C (e v)}, I , O) (c) Additional Rules Used To De.ne the Ideal \nSemantics of .FA I L . [ UNI QUE-INPUT] [DUP LINPUT ] [DUPLSEN D ] v . (Val), v . I v . (Val), v . I \nv . O in(v) in(v) out(v) (e, \u00b5, a, I , O) . (e, \u00b5, a l {v C (e v)}, I . {v}, O) (e, \u00b5, a, I , O) . (e, \n\u00b5, a, I , O) (e, \u00b5, a, I , O) . (e, \u00b5, a, I , O) Figure 3: Operational semantics of .FAIL . The state \nof an executing program is represented by a system x . (Identi.er) con.guration (e, \u00b5, a, I , O) . S, \nwhere e is the program itself, v . (Val) ::= x | .x.e \u00b5 represents the values of all tables, a is the \nmulti-set of currently e . (Exp) ::= v | e e | atomic v1 v2 | executing agents, I represents the set \nof all input requests received update v1 v2 | lookup v so far, and O represents the set of all responses \nproduced so far. (A response to a request vi is a pair of the form (vi, vo) where vo is tn . (TableName) \n= (Val) the result.) Let S represent the set of all con.gurations. t . SST = (Val) . (Val)opt Let l \ndenote the union operator for multi-sets. \u00b5 . ST = (TableName) . SST (Req) = (Val)(Resp) = (Req) \u00d7 (Val) \nSemantics. Fig. 3 presents an operational semantics for .FAIL as r (v C e) . SA = (Req) \u00d7 (Exp) a labelled \ntransition relation . on the set of con.gurations S. The \u00d7 2(Req) \u00d7 2(Resp) S = (Exp) \u00d7 ST \u00d7 S+ A evaluation \nof a program p starts in the initial con.guration ((p)) . (p, \u00b5 I , \u00d8, \u00d8, \u00d8), where \u00b5 I = .t..k.NONE. \n(Initially, all tables map every key to a default value of NONE. We utilize a standard encoding Figure \n2: The Syntax of .FAIL and its Semantic Domains. of optional values consisting of either NONE or SOME \nv.) System Transitions. ( r .) As rule INPUT indicates, the arrival of an input request v spawns a \nnew agent to evaluate e v. As rule OUT\u00ad(Val)opt) represents the value of a single table. An element \u00b5 \n. ST PUT indicates, when an agent s evaluation completes, the resulting represents the values of all \ntables. value is sent back as a response. The labels on system transitions As explained earlier, an agent \nrepresents a thread of computa-represent requests and responses. Rule NORMAL describes a nor\u00adtion spawned \nto process a given input request. The state of an agent mal system transition caused by a potentially \neffectful execution is represented by a pair of the form v C e, where v represents the step performed \nby a single agent, described below. As the rule in\u00adinput request being processed and e represents a partially \nevaluated dicates, the execution steps of different agents can be interleaved in expression (and represents \nthe local state of the agent). Let SA rep-a non-deterministic order. Rule FAIL indicates that an agent \ncan fail resent the set of all agent-states. at any point in time. Agent Transitions. ( . ) Execution \nsteps in the evaluation of a single agent are described by the transition relation . on ST \u00d7 ' ' (Exp). \nA transition (\u00b5, e) . (\u00b5 , e ) indicates that an agent expres\u00adsion e is transformed to an agent expression \ne ', with the side-effect of transforming the table-state from \u00b5 to \u00b5 '. The label . represents a sequence \nof updates to a single table performed atomically in this step. (This label, however, identi.es an internal \ntransition not visi\u00adble externally, which is why the label is omitted in the correspond\u00ading system transition \nin rule NORMAL.) These transitions are of two types: pure (standard .-calculus evaluation, BETA), and \neffectful, which take the form of atomic table operations. Atomic Table Operations. The expression atomic \nt e iden\u00adti.es a set of operations to be performed on a single table t in an atomic and failfree fashion. \nIts semantics is de.ned by rule ATOMIC, which utilizes a transition relation u on atomic evalua\u00adtion \ncon.gurations of the form (t, e), which indicates the atomic evaluation of an expression e at a table \nt. The labels on such tran\u00adsitions are either E or represent a single update to a table. Rules UP-DATE/LOOKUP \nde.ne the semantics of an update/lookup operation on a table. The rule ATOMIC indicates that no other \nexecution step interleaves with the evaluation of an atomic expression. Note that the evaluation of an \natomic expression cannot fail in the middle. In other words, either all effects of the atomic expression \nevaluation happen or none does. Retry. The rule RETRY models one of the key ingredients used to tolerate \nprocess failures, namely a retry mechanism. The rule indicates that a request must be retried periodically. \nTypically, retrying logic is built into clients. A client will resend a request if it does not receive \na response within a pre-de.ned amount of time. This basic scheme can be optimized, as discussed in Section \n4: the system (application) can send an acknowledgement back to the client, after which the client can \nstop resending a request and the application takes on the responsibility of retrying the request (to \nensure progress in the presence of failures). The system can exploit various optimizations in implementing \nthe retry logic, but rule RETRY suf.ces for our purpose here. As we will soon see, the key reason for \nadding the RETRY rule to the semantics is to formalize a weakened progress guarantee ( progress modulo \nretries ) that is appropriate in the presence of failures.  2.2 Formalizing Failfree Idempotence We \nnow formalize a natural correctness goal of any .FA IL program: namely, that it correctly handles process \nfailures and duplicate messages. We will later see how we can automatically ensure this property for \nany program. We formalize this correctness criterion as follows. We de.ne an alternative semantics for \n.FAIL , which we refer to as the ideal semantics, representing an idealized execution platform. We then \nde.ne a program to be correct iff its behavior under the standard semantics is equivalent to its behavior \nunder the ideal semantics. Ideal Semantics. Let A denote the set of all rules de.ned in Fig. 3(b). We \nwill de.ne the ideal semantics as a different labelled transition relation on program con.gurations, \nby adding and re\u00admoving some rules to set A. Let .S denote the labelled transition relation on S induced \nby a given set of rules S. Thus, .A is the transition relation capturing the standard semantics of .FA \nI L . We .rst omit rule FAIL eliminating process failures from the ideal semantics. In the ideal semantics, \nwe assume that all input requests are distinct, by replacing the INPUT rule by the UNIQUE-INPUT and DUPLINPUT \nrules. We also drop rule RETRY. Finally, process failures make it hard to ensure that an application \nsends an output message exactly once. A common approach to this problem is to weaken the speci.cation \nand permit the application to send the same output message more than once. We do this by adding rule \nDUPLSEND. We de.ne IDEAL to be A \\ { FAIL, RETRY, INPUT} . { UNIQUE-INPUT, DUPLINPUT, DUPLSEND}. We refer \nto .IDEAL as the ideal semantics for .FA I L . Observational Idempotence (Safety). We now consider two \n(well-studied) notions of behavioral equivalence in formalizing our correctness criterion. Recall that \n((p)) denotes the initial con\u00ad.guration in an execution of program p (which is the same under both semantics). \nGiven a labelled transition relation . on con.g\u00adurations, an execution of a program p (with respect to \n.) is an r1 alternating sequence of states and labels, denoted s0 . s1 \u00b7 \u00b7 \u00b7 sn, representing a sequence \nof transitions starting from the initial pro\u00adgram state s0 = ((p)). We say that the observed behavior \nobs(.) of an execution . is the sequence of non-E labels in .. Note that obs(.) is a sequence of (input) \nrequests and (output) responses. Speci.cally, it does not include updates to tables, which are inter\u00adnal \ntransitions but not visible externally. DE FIN I T I ON 2.1. We say that a .FA I L program p is observationally \nidempotent if for every execution .1 of p under the standard seman\u00adtics there exists an execution .2 \nof p under the ideal semantics such that obs(.1) = obs(.2). We present a simpler, more abstract, formalization \nof idempo\u00adtence in the appendix. However, the preceding de.nition in terms of the ideal semantics will \nbe useful in formalizing progress prop\u00aderties, as below, and in proving correctness of our implementation. \nFailfree Idempotence (Progress). An observationally idempotent program, by de.nition, gives no progress \nguarantees. Consider a modi.ed version of the account-transfer example that checks the input request \nto determine if it is a duplicate request and processes it only if it is not a duplicate. This ensures \nthat the program is idempotent. However, if the agent fails in between the debit and credit steps, we \nwould still have a problem. This motivates the following stronger correctness condition, based on the \nnotion of weak bisimulation. A labelled transition system (S, .) consists of a relation .. . S \u00d7 S for \nevery e . (Label). DE FIN I T I ON 2.2. A weak bisimulation between two labelled tran\u00adsition systems \n(S1, .1) and (S2, .2) is a relation ~ . S1 \u00d7 S2 such that for any s1 ~ s2 we have: .' * 2 s '' . 1. s1 \n.1 s1 ' . e = E . .s2 ' , s 2 '' .s2 . 2 .2 s2 ' . s1 ' ~ s2 ' .' * 1 s '' . 2. s2 .2 s2 ' . e = E . \n.s1 ' , s 1 '' .s1 . 1 .1 s1 ' . s1 ' ~ s2 '  * ' ' 3. s1 .1 s1 ' . .s2 ' .s2 .2 s2 ' . s1 ' ~ s2 ' \n' ' 4. s2 .2 s2 ' . .s1 ' , s 1 ' .s1 .1 * s1 ' . s1 ' ~ s2 ' We will write (s1, S1, .1) . (s2, S2, \n.2) to indicate that there exists a weak bisimulation ~ between (Sr 1, .1) and (Sr 2, .2 ) under which \ns1 ~ s2, where Sr i represents the set of states in Si that are reachable from si via a sequence of .i \ntransitions. We will omit S1 and S2 in this notation if no confusion is likely. DE FIN I T I ON 2.3. \nA .FA I L program p is said to be failfree idempo\u00adtent iff (((p)), .A) . (((p)), .IDEAL). This de.nition \nrequires a failfree idempotent program to pro\u00advide progress guarantees: at any point in time, if the \nsystem can pro\u00adduce a response r under the ideal semantics, then the system should be capable of producing \nthe same response r under the standard semantics also. However, the inclusion of rule RETRY in the stan\u00addard \nsemantics means that this progress guarantee holds provided requests are retried. Absolute progress guarantees \nare not possible since an agent may fail before it executes even its .rst step. TH E O R E M 2.4. A failfree \nidempotent program is also observation\u00adally idempotent. let imreturn v = fun (guid, tc) . (tc, v) let \nimatomic T f = fun (guid, tc) . atomic T { let key = (0, (guid, tc)) in match lookup key with | Some(v) \n. (v,tc+1) | None . let v = f () in (update key v); (v,tc+1) } let imbind (idf, f) = fun (guid, tc) . \nlet (ntc, v) = idf (guid, tc) in f v (guid, ntc) let imrun idf x = let (j,v) = idf x (x, 0) in v let \nimupdate key val = update (1,key) val let imlookup key = lookup (1,key) Figure 4: The Idempotence Monad. \nFailfree Idempotent Realization. Failfree idempotence is a generic correctness property we expect of \na .FA IL program. More generally, the following de.nition combines this property with a speci.ca\u00adtion \n(provided as another .FA I L program q). DE FI NI TI O N 2.5. A .FA IL program p is said to be a failfree \nidem\u00adpotent realization of q iff (((p)), .A) (((q)), .IDEAL). 3. Realizing Failfree Idempotence We now \npresent a generic, application-independent, strategy that can be used to ensure failfree idempotence. \nInformally, a function is idempotent if multiple evaluations of the function on the same argument, potentially \nconcurrently, behave the same as a single evaluation of that function with the same argument. Consider \nthe example in Fig. 1. In this example, the parameter requestId serves to distinguish between different \ntransfer requests (and identify du\u00adplicate requests). This service can be made idempotent and failfree \nby (a) using this identi.er to log debit and credit operations, when\u00adever the operations are performed, \nand (b) modifying the debit and credit steps to check, using the log, if the steps have already been \nperformed. This strategy can ensure that multiple (potentially par\u00adtial and concurrent) invocations of \ntransfer with the same identi.er have the same effect as a single invocation. Manually ensuring idempotence \nis tedious and it introduces the possibility of various subtle bugs and in general makes implementa\u00adtion \nless comprehensible. We now describe a monad-based library that realizes idempotence and failure-freedom \nin a generic way. 3.1 The Idempotence Monad The Intuition. A computation performed by a single agent \ncon\u00adsists of a sequence of steps, pure as well as effectful ones (namely, atomic local transactions which \ncan read/update a single table.) We use the following strategy to make a computation idem\u00adpotent: 1. \nAssociate every computation instance (that we wish to make idempotent) with a unique identi.er. 2. Associate \nevery step in an idempotent computation with a unique number.  3. Whenever an effectful step is executed, \nwe simultaneously per\u00adsistently record the fact that this step has executed and save the value produced \nby this step. 4. Every effectful step is modi.ed to .rst check if this step has already been executed. \nIf it has, then the previously saved value (for this step) is used instead of executing the step again. \n  Note that .FAI L does not have any non-deterministic construct (in a single agent s evaluation). Non-deterministic \nconstructs can be supported by treating them as an effectful step so that once a non-deterministic choice \nis made, any re-execution of the same step makes the same choice. Details. We now describe in detail \nhow individual computation steps can be made idempotent and how these idempotent steps can be composed \ntogether into idempotent computation. Our solution is essentially a monad (Fig. 4). We represent an idempotent \ncomputation as a function that takes a tuple (guid, tc) as a parameter (where guid and tc represent an \nidenti.er and a step number used to uniquely identify steps in a computation) and returns a value along \nwith a new step number. We can execute an idempotent computation idf as shown by the function imrun, \nusing the function s argument itself as the guid value and an initial step number of 0. The function \nimreturn (the monadic return ) lifts primitive values to idempotent computations. This transformation \ncan be used for any pure (side-effect-free) expressions. Side-effects (table operations) are permitted \nonly inside the atomic construct. The function imatomic is used to make a local transaction idempotent. \nSpeci.cally, imatomic T fm is an idem\u00adpotent representation for atomic T f , where fm is the monadic \nform of f constructed as described later. As explained above, this is represented as a function that \ntakes a pair (guid, tc) as a param\u00adeter and realizes the memoization strategy described above. The pair \n(guid, tc) is used as a unique identi.er for this step. We check whether this step has already executed. \nIf so, we return the pre\u00adviously computed value. If not, we execute this computation step and memoize \nthe computed value. In either case, the step number is incremented in this process and returned. It is, \nhowever, critical to do all of the above steps atomically to ensure that even if two agents concurrently \nattempt to execute the same computation, only one of them will actually execute it. However, note that \nan atomic expression is allowed to access only a single table. Hence, the memoized information for this \ncomputation step must be stored in the same table that is accessed by the computation step. However, \nwe must keep our memoization and book-keeping information logically distinct from the program s own data \nstored in the same table. We achieve this by creating two distinct address spaces (of key values) for \nthe table. We convert a key value k used by our idempotence implementation to (0, k) and convert a key \nvalue k used by the program itself to (1, k). The functions imupdate and imlookup do this wrapping for \nthe user s code. Thus, the expression f to be evaluated in the atomic transaction is transformed to its \nmonadic representation fm by replacing lookup and update in f by imlookup and imupdate respectively. \nWe now consider how to compose individual computation steps in an idempotent computation (the monadic \nbind function). Con\u00adsider a computation of the form let x = step1 in step2 which is equivalent to (.x.step2)step1 \n. We transform step1 to its monadic form, say idf. We transform step2 to its idempotent form, say g. \nThe function bind, applied to idf and .x.g, produces the idempotent form of the whole computation. (A \nformal and more complete description of the transformation is presented later.) The result of imbind \nidf f is de.ned as follows: it is an idempotent function that takes a parameter (guid, tc), and invokes \nidf (the .rst step). It uses the value and the step number returned by the idempotent function and invoke \nthe second idempotent function f. Thus the monad effectively threads the step number through the computation, \nincrementing it in every atomic transaction. Note that a key characteristic of this implementation is \nthat it is completely decentralized. When a transaction completes, the cur\u00adrent agent simply attempts \nto execute the next (idempotent) transac\u00adtion in the work.ow; no coordination with a centralized transaction \nmanager is required. In general, distributed coordination requires the use of expensive blocking protocols \nsuch as 2 phase commit. In contrast, a work.ow implementation based on this idea of idempo\u00adtence coupled \nwith (client-side) retry logic are non-blocking. The implementation does not have a single point-of-failure \nor single performance bottleneck, which can lead to increased scalability. Also note that this implementation \ncreates one log entry per transaction. Once logging for idempotence is separated from the core business \nlogic, it can be optimized in several ways. We can avoid redundant logging if the underlying storage \nengine maintains its own log. Logging can be avoided if the transaction is (declared to be) semantically \nidempotent. Example. The monadic library lets us construct the monadic ver\u00adsion em of any .FAIL expression \ne, by using the monadic version of any primitive and the monadic bind in place of function appli\u00adcation. \nE.g., the monadic version of the following code fragment (from the money transfer example) atomic fb \n{update fa ((lookup fa) - amt)}; atomic tb {update ta ((lookup ta) + amt)}; is the following: imbind \n(imatomic fb {imupdate fa ((imlookup fa) - amt)}) {imatomic tb {imupdate ta ((imlookup ta) + amt)}} where \n{e} is shorthand for .x.e, where x is not free in e.  3.2 Idempotence Monad Programs Are Failfree In \nthis section, we show that programs written using the idempo\u00adtence monad are failfree. Idempotent Executions. \nConsider any execution of a .FA I L pro\u00adgram. We refer to any transition generated by rule NORMAL as \nan . execution-step and identify it by the triple (v C e) --+ (v C e ' ). . Thus, an execution-step a \n--+ b represents a transition caused by an agent a that transforms to an agent b, with a side-effect \n. on . the tables. An execution-step a --+ b, after an execution ., is said to be a repeated execution-step \nif the execution . contains some .1 execution-step of the form a --+ b '. (Note that this does not sig\u00adnify \na cycle in the execution since the states of the tables could be different in the two corresponding con.gurations.) \nDE FI NI TI O N 3.1. An execution is said to be operationally idempo\u00ad .1. ' tent iff for any two execution-steps \na --+ b and a --+ b ' in the execution (in that order), a = a ' implies that b = b ' and . ' is empty. \nNote that operational idempotence is a property that involves the side-effects on the tables, unlike \nobservational idempotence. As we show below, it is a stronger property that can be used to establish \nobservational idempotence. LEMM A 3.2. Any operationally idempotent execution . of a pro\u00adgram p (in the \nstandard semantics) is observationally equivalent to some ideal execution . ' of p (in the ideal semantics): \ni.e., obs(.) = obs(. ' ). PRO O F SK E T C H. Let . be an operationally idempotent execution of a program \np under the standard semantics .A. We show how to construct an execution . ' of p under the .IDEAL semantics \nsuch that obs(.) = obs(. ' ). We .rst omit any transitions in . due to the FAIL rule since their labels \nare empty. We next omit the transitions corresponding to repeated execution\u00adsteps: . The key point to \nnote here is that an execution-step a --+ b affects the (legality of) subsequent transitions in two ways: \nindi\u00adrectly through the side-effects . on the tables and directly through b (which may take part in subsequent \ntransitions). A repeated execution-step is redundant in any idempotent execution that has no failures \n(transitions due to the FAIL rule): it has no side-effects on the tables, and if the value b is subsequently \nused in a non-repeated execution-step, then we can show that another agent identical to b already exists \nin the con.guration. We then omit any transition due to the RETRY rule. We .nally replace INPUT transitions \ncorresponding to a duplicate and replace INPUT transitions corresponding to a non-duplicate input request \nby a UNIQUE-INPUT transition. This leaves us with duplicate responses that may be produced by the OUTPUT \nrule for the same input (due to multiple agents that process it). We replace these transitions by corresponding \nDUPLSEND transition. This transformation produces an execution . ' of p under the .IDEAL semantics such \nthat obs(.) = obs(. ' ). Our next goal is to show that all executions of programs written using the idempotence \nmonad are operationally idempotent. How\u00adever, this claim requires the programs to be well-typed, as de.ned \nbelow. Well-Typed IM Programs. The idempotence monad can be used to write monadic programs in the usual \nway. The monad may be thought of as de.ning a new language .IM , obtained from .FAIL by replacing the \nkeywords atomic, lookup, and update by the keywords imatomic, imlookup, imupdate, imbind, and imrun. \nA .I M program can also be thought as a .FAIL program, using the de.nition of the monad in Fig. 4. We \n.rst mention a few type restrictions used to simplify presen\u00adtation. We refer to .-calculus terms as \npure and their types as pure types. We assume that the types of keys and values of all tables are pure \ntypes. We assume that the types of the input request and the output response are also pure types. We \nuse (tk, tv) to denote the type of a table with keys of type tk and values of type tv. We assume that \ntable names come from some .xed set of identi.ers and that the typing environment maps these table names \nto their types. The non-pure terms can be classi.ed into two kinds. The .rst kind are expressions (such \nas imupdate k v), which are meant to be evaluated in the context of a single table (as part of a local \ntrans\u00adaction). Fig. 5 presents typing rules for intra-transaction expres\u00adsions. The type t !(tk, tv) \nsigni.es a term that can be evaluated in the context of a table of type (tk, tv), producing a value of \ntype t. The second kind of expressions are the ones that use the idem\u00adpotence monad, used to represent \nwork.ow computations which execute one or more local transactions. Fig. 6 presents typing rules for such \nexpressions (which are the standard monad typing rules). In the sequel, we will use the term well-typed \nIM program to refer to any expression e such that Gtn |= e : ti .I D to, where ti and to are, respectively, \nthe types of input and output messages, and Gtn provides the typings of tables. This is essentialy a \nprogram of the form imrun e ', where e ' is constructed from the other monadic constructs. [IMLOOKUP] \n[IMUPDATE] [AT-VAR] G |= e : tk!(tk, tv)G |= imlookup e : tv!(tk, tv) G |= e1 : G |= tk!(tk, tv) G |= \ne2 : tv!(tk, tv)imupdate e1 e2 : unit!(tk, tv) x : t |= x : t !(tk, tv) [AT-LAMBDA] [AT-APPLY] G, x \n: t1 |= e : t2!(tk, tv) (tk,tv)G |= e1 : (t1 . t2)!(tk, tv) G |= e2 : t1!(tk, tv) (tk,tv) G |= .x.e \n: (t1 . t2!(tk, tv)) G |= e1e2 : t2!(tk, tv) Figure 5: A type system for intra-transaction expressions. \n[ IMATOMIC] [IMRE TURN] (tk,tv) G |= e : t G |= t : (tk, tv) G |= e : (unit . t )!(tk, tv)G |= imreturn \ne : IM t G |= imatomic t e : IM t [IMBIND] [IMRUN] G |= e2 : t1 . IM t2 G |= e1 : IM t1 G |= e : t1 . \nIM t2 G |= imbind e1 e2 : IM t2 G |= imrun e : t1 .ID t2 [VA R] [LAMBDA] [APPLY] x : t |= x : t G, x \n: t1 |= e : t2 G |= .x.e : t1 . t2 G, x : t1 |= e : t2 G |= .x.e : t1 . t2 Figure 6: A type system for \nidempotence monad. LEMM A 3.3. All executions of a well-typed IM program are oper\u00adationally idempotent. \nPRO O F SK ETC H. Let us refer to a value of the form (0, k) (used as a key for a table lookup/update \noperation) as a system key. Consider any execution of a well-typed IM program. It is easy to verify that \nonce the value associated with a system key (0, k) in any table is set to be v, it is never subsequently \nmodi.ed. . .1 Consider any two execution-steps a --+ b and a ' --+ b ' in the execution (in that order) \nwhere a = a '. The idempotence property follows trivially whenever these steps are effect-free steps. \nThe only effectful steps are produced by the evaluation of an imatomic construct. Since a = a ', the \nkey value used in the evaluation of imatomic in both steps must be identical. The implementation of imatomic \nguarantees that whatever value is produced by the evaluation of imatomic in the .rst step will be memoized \nwith the given value. This guarantees that the second step will .nd this same value in the table (thanks \nto the property described earlier). Hence, it will evaluate to the same value and will have no side-effects \non the table. It follows that the execution is idempotent. TH EO R E M 3.4. Any well-typed IM program \nis observationally idempotent. PRO O F SK E TC H. Follows immediately from the previous two lem\u00admas. \nAs stated earlier, observational idempotence does not give us any progress guarantees in the presence \nof failures. We now estab\u00adlish progress guarantees in the form of a weak bisimulation. DE FI NI TI O \nN 3.5. Let p be any .FA I L program. Let Sp A denote the set of con.gurations { s . S | ((p)) . * A s \n} that can be produced by the execution of p under the standard semantics. Let Sp I denote the set of \ncon.gurations { s . S | ((p)) . * } that can IDEAL s be produced by the execution of p under the ideal \nsemantics. We de.ne the relation between Sp A and Sp by (p, \u00b51, a1, I1, O1) I (p, \u00b52, a2, I2, O2) iff \nI1 = I2 and \u00b51 = \u00b52. Note that in the above de.nition, the condition I1 = I2 im\u00adplies that both con.gurations \nhave received the same set of input requests. The condition \u00b51 = \u00b52 implies that the processing of each \ninput request v in both con.gurations have gone through iden\u00adtical sequences of effectful steps so far. \n(This follows since every effectful step is memoized and is part of the table state \u00b51 and \u00b52.) TH E \nO R E M 3.6. If p is a well-typed IM program, then is a weak bisimulation between (Sp A , .A) and (Sp \nI , .IDEAL). PRO O F SKET C H. Consider any s1 s2. The interesting transi\u00adtions are those that are effectful \n(involving an atomic operation) or produce an output. If either s1 or s2 can perform an interesting transition, \nwe must show that the other can perform an equivalent transition (possibly after a sequence of silent \ntransitions). Consider the case when an agent v C e in s1 can perform an effectful transition in the \nstandard semantics. Let v C e0 be the state of the same agent after its most recent imatomic evaluation. \nThen, we must have some side-effect-free evaluation e0 . e1 \u00b7 \u00b7 \u00b7 ek = e. Consider con.guration s2. By \nde.nition, we restrict our attention to reachable states. Hence, there exists some execution in the ideal \nsemantics that produces s2. This execution must have received in\u00adput request v and produced an agent \nv C (p v). Consider the evalu\u00adation of this agent. The effectful steps in this agent s evaluation in \nthe ideal semantics must have produced the same sequence of val\u00adues as in the evaluation in the standard \nsemantics (since the mem\u00adoized values for these steps are the same in both s1 and s2). Thus, s2 must \nhave some agent of the form v C ei for some 0 = i = k. This will produce, after zero or more silent transitions, \nthe agent v C e that can then perform the same effectful transition in the ideal semantics as in the \nstandard semantics. Consider the case when an agent v C e in s2 can perform an ef\u00adfectful transition \nin the ideal semantics. We can create a new agent v C (p v) using rule RETRY in the standard semantics. \nWe can then lation preserves types: translation of a well-typed .FA I L program [x]V = x (satisfying \nthe restrictions mentioned above) will produced a well\u00ad [.x.e]V = .x.[e]I typed monadic program. [x]I \n= imreturn x 3.4 Failfree Realization Via Idempotence [.x.e]I [n e]I [v n]I ' ]I [v v [atomic x e]I [x]A \n[.x.e]A [e1e2]A [update e1 e2]A [lookup e]A (e, {})(e, {x} l Y ) = imreturn (.x.[e]I) Given any .FAIL \nexpression e, we can construct its monadic version = imbind [n]I (.x.[x e]I) ,n . (Val) em as explained \nabove. The preceding results imply that em must = imbind [n]I (.x.[v x]I) ,v . (Val), n . (Val)be failfree. \nWe now establish a stronger result, namely that em is = [v]V [v ' ]V ,v, v ' a failfree realization of \ne. (Failure-freedom is a weak correctness . (Val) = (imatomic x [e]A , fv) where fv = {x} . freevars \n(e) = x = .x.[e]A = [e1]A[e2]A = imupdate [e1]A [e2]A = imlookup [e]A = e = (imbind x (.x.e), Y ) Figure \n7: Transforming .FA I L expressions into idempotent expres\u00adsions. duplicate the entire execution history \nof v C e (which is guaranteed to be the same in both semantics by the de.nition of ). Thanks to the idempotence \nproperty, this duplicate execution will have no extra side-effects and will eventually produce the same \neffectful transition as in the ideal semantics. TH EO R E M 3.7. A well-typed IM program is failfree. \n 3.3 Transforming .FA I L Programs To Idempotent Programs We have seen that the idempotence monad lets \nus construct failfree programs. Given any .FAIL -program e, we can construct its equiv\u00adalent monadic \nrepresentation using standard techniques (which are conceptually straightforward, though the details \nare intricate). The transformation is presented in Fig. 7 (based on the transformation in [17]). We make \na few simplifying assumptions (type restrictions) about the source .FAIL program e in this transformation \nalgorithm (similar to those mentioned in Section 3.2). We assume that the types of keys and values of \nall tables are pure types. We assume that intra-transaction expressions do not manipulate values of work.ow \ntype: e.g., atomic t {.x.(atomic s e)} is not allowed. The translation is de.ned using multiple translation \nfunctions. [v]V is a translation function that applies only to values (which must be of the form x or \n.x.e). The translation function [e]I is the heart of the monadic transformation and can be applied to \nmulti\u00adtransaction expressions. It uses the idempotence monad to sequence local transactions. The translation \nfunction [e]A is applied to intra\u00adtransaction expressions and it is used primarily to replace occur\u00adrences \nof update and lookup by imupdate and imlookup respec\u00adtively. Note that a single local transaction (i.e.intra-transaction \nex\u00adpression) evaluation is done using standard evaluation (without us\u00ading any monad). The auxiliary function \n(e, X ) is used to transform the monadic values used in the evaluation of multi-transactions to standard \nvalues required in the evaluation of a single local transac\u00adtion. Finally, given a top-level .FAIL program \ne (which is assumed to be a functional value of the form .x.e '), it s monadic form em is de.ned to be \nimrun [e]V. As usual, it can be shown that the trans\u00adcriterion. It indicates that a program s behavior \nunder the standard semantics is equivalent to it s behavior under the ideal semantics.) The notion of \nfailure-freedom does let us simplify verifying cor\u00adrectness of a program by considering only its behavior \nunder the ideal semantics. In particular, we have: TH E O R E M 3.8. If p and q are weakly bisimilar \nunder the ideal se\u00admantics (i.e., if (((p)), .IDEAL) (((q)), .IDEAL), and p is failfree, then p is a \nfailfree realization of q. PRO O F SK E T C H. Follows as we can compose the two weak bisim\u00adulations \ntogether. This theorem simpli.es proving that a program is a failfree realization of another. In particular, \nwe have already seen that a monadic program is failfree (Theorem 3.7). Hence, to prove that em is a failfree \nrealization of e, it suf.ces to show a weak bisimilarity between the ideal executions of a .FA I L program \ne and the ideal executions of its monadic representation em. TH E O R E M 3.9. Let em be the monadic \nrepresentation of e. Then, em is a failfree realization of e. PRO O F SK ETC H. As explained above, it \nis suf.cient to relate eval\u00aduations of e and em under the ideal semantics. In this setting, it is intuitively \nsimple to see why the monadic program em simulates the given program e. The key distinction is that the \nmonadic imple\u00admentation uses imatomic to perform any effectful step. This step will .rst check the memoized \ndata to see if this step has already executed. In an ideal execution, this check will always fail, and \nthe monadic implementation then performs the same effectful step as the original program (and memoizes \nit). The check is guaranteed to always fail because the keys (g, i) used in distinct executions of imatomic \nare distinct. The value of g will be different for exe\u00adcutions corresponding to different inputs x and \ny. The value i will be different for different steps corresponding to the same input x. 3.5 Idempotent \nFailfree Computations as a Language Feature We have now seen how idempotent failfree computations can \nbe automatically realized using the idempotence monad. We now pro\u00adpose a new language construct for idempotent \nfailfree computa\u00adtions. The construct idff v e indicates that the computation of e should be failfree \nand should be idempotent with respect to v: i.e., multiple (potentially concurrent) invocations of this \nconstruct with the same value v behaves as though only one of the invoca\u00adtions executed. The above construct \npermits users to specify the code fragment for which they desire automatic idempotence and failure-freedom \n(provided by the compiler). This enables users to rely on other methods, perhaps application-speci.c, \nto ensure these properties elsewhere. (For instance, some computation may be se\u00admantically idempotent \nalready.) It also lets the users specify what should be used as computation identi.er to detect duplicates, \nas illustrated below. We refer to this enhanced language as .ID F F . A formal seman\u00adtics of .I DFF appears \nin the appendix. We illustrate the meaning of the .rst parameter of idff using the incorrect .FA I L \nexample of Fig. 1. We can wrap the idff construct around this example in the following two ways, with \ndifferent semantics. Note that the input in this example is a pair (reqId, req) consisting of a request-id \nas well as the actual request. Consider: f1 = .(reqId , req ). idff reqId (process (reqId , req )) f2 \n= .(reqId , req ). idff (reqId , req ) (process (reqId , req )) The behavior of these two functions differ \nin the cases where multiple inputs arrive with the same request-id but differing in the second parameter \nreq. f1 treats such requests as the same and will process only one of them, while f2 will treat them \nas different requests and process them all. One of the subtle issues with the semantics and implementation \nof the idff construct is the treatment of invocations that have the same id (.rst parameter) but have \ndifferent expressions as the sec\u00adond parameter. We take a simple approach with our semantics (that the \neffect is as if only the .rst invocation occurred). This has some implications for the underlying implementation \n(in the presence of failures). One solution is to use a continuation-passing style com\u00adputation, and \nmemoize the entire continuation in the memoization step (rather than just the value computed for the \nstep). 4. Extensions We have seen how idempotence can serve as the basis for fail\u00adfree composition of \ncomputations: essentially, a simple form of fault-tolerant work.ow. In this section, we describe two \nextensions that enrich this construct, namely compensating actions and asyn\u00adchronous evaluation, which \nsimplify writing applications. These concepts are not new, but what is interesting is that they can be \nintegrated without affecting the light-weight, decentralized, nature of our idempotence implementation. \n 4.1 Compensating Actions The idff construct allows us to compose several transactions into an idempotent \nwork.ow that appears to execute exactly once with\u00adout process failures. However, the lack of isolation \nmeans that when a transaction in the work.ow is executed, its precondition may not be satis.ed and we \nmay need to abort the work.ow. For example, in the transfer example (Fig. 1), the debit step may succeed, \nbut we may be unable to complete the subsequent credit step because the account does not exist. One way \nof recovering from this failure is to compensate for the debit by crediting the amount back to the source \naccount. If compensating actions are correct, the work.ow can guarantee all-or-nothing semantics i.e. \neither all or none of the transactions in the work.ow appear to execute. We .rst formalize the desired \nsemantics of work.ows with com\u00adpensating actions. We present a language .IDW F which provides language \nconstructs to associate transactions with compensating actions, and to declare logical failures. Finally, \n.I DWF supports a construct idworkflow id e, which composes transactions with compensating actions into \na work.ow. We present semantics of this construct, and then show how this construct is realized using \na com\u00adpensation monad. Syntax. .IDW F modi.es and extends .FA IL in the following ways (see Fig. 8). \natomic t ea ec extends the atomic construct of .FA I L by specifying ec as the compensation for the atomic \ntransaction ea. abort indicates that a logical failure has occured and the work\u00ad.ow must be aborted. \nidworkflow id e represents an idempotent work.ow with identi.er id, where e is the work.ow consisting \nof a composition of atomic transactions with compensations. Expres\u00adsion of the form e 0 ec arise only \nduring evaluation and are not source language constructs. Semantic Domains. The semantic domains for \n.IDW F are the same as for .FAI L with minor extensions. The runtime expression e 0 ec is used to represent \na work.ow during its execution. Here, e represents the partially evaluated form of a work.ow and ec represents \nthe compensating action to be performed in case the work.ow needs to be aborted. Agents can also be of \nthe form id . ew 0 ec, indicating an agent evaluating a work.ow. Semantics. The semantics of .IDW F is \nde.ned using a set of rules consisting of all the rules in A used to de.ne the semantics of .FAIL except \nfor FAIL and ATOMIC, plus the new rules presented in Fig. 8. The initiation of a work.ow (rule IDWF-BEGIN) \ncreates a new agent of the form id . ew 0 ec, provided no agent has already been created for id. This \nagent evaluates the work.ow ew and tracks the compensating action ec to be performed in case of an abort. \nRule IDWF-END indicates how the computation proceeds once the work.ow evaluation is complete (or if a \nprevious work\u00ad.ow with the same id has already been initiated). The rules ATOMIC and ATOMIC-ABORT de.ne \nthe semantics of transactions in a work.ow. Informally, the expression atomic t ea ec is evaluated as \nfollows. First, ea is evaluated atomically to produce a value v. Then, as a side-effect, the compensating \naction is up\u00addated to indicate that ec v should be evaluated as the .rst step of the compensation before \nexecuting the original compensation. Finally, the whole expression evaluates to v. Thus, note that the \nvalue produced by the atomic action ea is available to the subse\u00adquent computation as well as the compensating \naction ec. When a work.ow is aborted (rules ATOMIC-ABORT and IDWF-ABORT), the compensation expression \nis evaluated. Rule ATOMIC of .FAIL is replaced by the pair of rules PURE-ATOMIC and PURE-ATOMIC-ABORT, \nwhich describe the behavior of a transaction that is not contained within a work.ow. PURE-ATOMIC describes \nthe successful completion of a transaction, while PURE\u00adATOMIC-ABORT describes the case where the transaction \nis aborted. (The auxiliary relation used here is de.ned by the rules for .FA I L .) Compensation monad \nWe now describe the compensation monad that can be used to realize work.ows with compensating actions. \nFor simplicity, we describe an implementation of the compensat\u00ading monad that focuses only on logical \nfailures and compensations. We assume there are no duplicate requests or process failures. We can realize \nidempotent work.ows with compensating actions by composing this monad and the idempotence monad [13]. \nThe compensation monad, shown in Fig. 9, is a combination of the exception monad and the continuation \npassing style monad. Transactions return a value of the form Value(v) (on successful completion) or a \nspecial value Abort to indicate that the transaction was aborted. Work.ows are represented as a function \nin continua\u00adtion passing style. The helper function compensateWith associates a transaction a with a \ncompensating action to construct a primi\u00adtive work.ow. compensateWith constructs a function in continua\u00adtion \npassing style. This function .rst evaluates a(). If a aborts, the whole transaction aborts and returns \nAbort. Otherwise, the contin\u00aduation is evaluated using the value returned by the transaction. If the \ncontinuation itself aborts (because one of the following trans\u00adactions aborts), we evaluate the compensating \naction and return the value Abort. The monad s return simply lifts a value (with no com\u00adpensation) into \na work.ow. The monadic bind is standard for con\u00adtinuation passing style computations. The function run \nshows how to execute a work.ow by passing it an empty continuation. 4.2 Asynchronous evaluation Work.ows \nare commonly used to perform computations involving several transactions. Consequently, work.ows are \noften long run\u00adning with highly variable latencies. Large latencies accentuate the (a) Syntax and Evaluation \nContext x . (Identi.er); v . (Val) ::= x | .x.e e . (Exp) ::= x | .x.e | e e | atomic vt va vc | idworkflow \nvi vw | abort | update v v | lookup v | e 0 ec SA = (Exp) \u00d7 (Exp) + (Exp) \u00d7 (Exp) E ::= [\u00b7] | E e | v \nE | E 0 e (b) Evaluation Rules (in addition to A \\ { ATOMIC }) [ IDWF-END] [IDWF -BEGIN] id . u . a \nu . (Val)v C E[idworkflow id w] . a .e ' .id . e ' . a ' (p, \u00b5, a l {v C E [idworkflow id w]}, I , O) \n. ' (p, \u00b5, a, I , O) . (p, \u00b5, a l {id . (w() 0 0)}, I , O) (p, \u00b5, a l {v C E[u]}, I , O) [ATOMIC] [ATOMIC-A \nBORT] [NORMAL] . * . * . ' ' (\u00b5, e) (\u00b5 , e ) (\u00b5[tn], ea()) (t, v), v . (Val) (\u00b5[tn], ea()) (t, E[abort]) \n(p, \u00b5, a l {v . e}, I , O) . (\u00b5, E[atomic tn ea ec] 0 e) . (\u00b5, E[atomic tn ea ec] 0 e) ' (p, \u00b5 ' , a \nl {v . e ' }, I , O) (\u00b5[tn . t], E[v] 0 ((ec v); e)) (\u00b5, E [abort] 0 e) [CONTEXT ] [IDWF -ABORT] e . \ne ' (\u00b5, (E[abort]) 0 ec) (\u00b5, ec) (\u00b5, E[e] 0 ec) . (\u00b5, E [e ' ] 0 ec) [PURE-ATOMIC] [PUR E-ATOMIC-ABORT] \n. * . * (\u00b5[tn], ea()) (t, v), v . (Val) (\u00b5[tn], ea()) (t, E[abort]) (\u00b5, atomic tn ea ec) . (\u00b5[tn . t], \nv) (\u00b5, atomic tn ea ec) ' (\u00b5, 0) Figure 8: Syntax and Semantics of .IDW F . let compensateWith a comp \n= fun f . match a() with | Abort . Abort | Value(b) . match (f b) with | Abort . let = comp b in Abort \n| Value(c) . Value(c) let bind (v, f) = fun g . v (fun a . f a g) let return a = compensateWith (fun \n() . a) (fun v . ()) let run a = a (fun x . Value(x)) Figure 9: The compensation monad problem of duplicate \nrequests because clients cannot easily distin\u00adguish between a long running work.ow and one that has failed \nto generate a response. While the idempotence monad guarantees cor\u00adrectness in such cases, idempotence \ndoes come at a performance cost (due to log lookups). A design pattern commonly used to re\u00adduce the number \nof duplicate requests is for the system to take over the task of retrying (a part of) the request on \nbehalf of the client asynchronously, and sending an intermediate response to the client, typically with \nthe transaction identi.er. The client can use transac\u00adtion identi.er to periodically poll for the status \nof the request. The idempotence monad can be extended to support asyn\u00adchronous evaluation as follows. \nAt a programmer de.ned point in the evaluation of the work.ow, we create a checkpoint. A check\u00adpoint \nis essentially a closure representing the rest of the work\u00ad.ow, along with relevant state variables. \nThe checkpoint is then persisted, typically in a distributed queue (essentially a worklist). Once the \ncheckpoint has been created, an intermediate response is sent to the client. A set of special agents \nperiodically query the queue, retrieve checkpoints, and continue evaluation, deleting the checkpoint \nonly when the work.ow has been fully evaluated. Supporting asynchronous evaluation requires some additional \nsupport from the platform and a minor change to the monad s bind function. We assume that the platform \nprovides a channel that can only be accessed by the idempotence monad (hence not exposed in .FA I L ). \nMessages can be sent to this channel using the function send and received using the function recv. We \nassume the channel supports the following handshake protocol for messages in order to guarantee at-least-once \nprocessing of messages. In this protocol, recv does not delete messages from the channel. Instead, an \nagent that receives a message must acknowledge the message (using ack) before the message is permanently \ndeleted. If an agent crashes after receiving a message and before acknowledging it, the message reappears \nin the channel and may be processed by other agents. (Windows Azure provides such a channel implementation, \nwhich our implementation uses.) The changes to the idempotence monad are illustrated in Fig\u00adure 10. Instead \nof invoking the remainder of the work.ow, the mod\u00adi.ed bind (Fig. 10) creates a closure for the rest \nof the work.ow and persists the closure in a special queue ( worklist ) (using the function send). Special \nagent processes (agent) retrieve the check\u00adpoints (using recv) and continue evaluating the rest of work.ow. \nIf an agent manages to evaluate the work.ow without failing, it deletes the work.ow from the queue (using \nack). In our implemen\u00adtation, the choice of using asynchronous evaluation (and the partic\u00adular step at \nwhich to create a checkpoint) is left to the programmer. We evaluate the bene.ts of these optimizations \nin Section 5. let agent () = while (true) let (msgid, msg) = recv worklist in let (f, params) = msg \nin let result = f params in let = ack worklist (msgid,msg) in result let bind (idf, f) = fun (guid, tc) \n. let (ntc, v) = idf (guid, tc) send worklist (f a (guid, ntc + 1)) Figure 10: The idempotence monad \nwith asynchronous evaluation let saveSurvey response = idwork.ow { do! addAtomic responses response.Id \nresponse return! atomic{ let! summary = readAtomic summaries response.SurveyName do summary.Merge(response) \nreturn! writeAtomic summaries response.SurveyName summary }} Figure 11: The save survey operation expressed \nas in idempotent work.ow. 5. Evaluation We have implemented the idempotence work.ow monad and its variants \nin C# and F#, targeting Windows Azure, a platform for hosting distributed applications. Azure provides \na key-value store with explicit support for data partitioning, where partitions are units of serializability. \nIn this section, we focus on evaluating the expres\u00adsiveness and performance overheads of the idempotence \nmonad. Sample applications We found several real world applications whose core logic can be declaratively \nexpressed as work.ows. We brie.y describe an expense management application [4] and a survey application \n[3]. Other applications we have implemented include applications for online auctions [2], blogging and \nbanking. Survey application. The Surveys application enables users to de\u00adsign a survey, publish the survey, \nand collect the results of surveys. The most performance critical scenario in this application is when \na user submits a survey. This operation involves two steps, recording the response and updating the survey \nsummary with the response. For scalability, survey responses and summaries are stored in dif\u00adferent data \npartitions. Figure 11 shows an implementation of this operation using work.ows. The syntax here is from \nthe actual F# implementation and differs from .FA IL in non-essential ways. The work.ow is composed of \ntwo transactions, a transaction that saves the response and a transaction that updates the summary. In \nthis implementation, survey responses do not re.ect in the summary immediately. In general, deferred \nwrites are often acceptable as long as the writes will eventually appear to occur exactly once, a guarantee \nwork.ows provide. The use of the idempotence monad guarantees that even if multiple requests are received \nwith the same survey, the work.ows appears to execute just once. Auction application. The auction application \nallows users to bid for items on auction and track the status of their bids. Sellers can register items \nfor auction with a time limit and a minimum bid price. Auction sites are often high volume sites, and \nboth latency and scalability is important. The most critical operation in this ap\u00adplication is the operation \nthat processes new bids. This operation can be expressed as a work.ow composed of several transaction. \n Hand\u00adcoded Molecules Banking 94 7296 Blogging 243 6484 Auction 107 7256 Surveys 530 9004 Expense 378 \n6504 Figure 12: Average message size (in bytes) of the hand-coded and work.ow implementations.  The \n.rst transaction checks if the bid is valid and then checks if the bid beats the current maximum bid, \nin which case the bid is recorded in a bids table. The second transaction marks the current winner and \nloosing bidders in a separate table. The web front-end is programmed to poll this table periodically \nfor the latest bid status. Subsequent transactions update other tables such as an aggregate table that \nmaintains the most frequently bid items, more frequent bidders etc. With the idempotent work.ow monad, \nit is easy to guarantee that each bid is processed exactly once even if clients retry with little change \nin complexity. Overheads. There are two sources of overheads associated with idempotent work.ows compared \nto hand-coded implementations. First, asynchronous evaluation requires serialization and deserial\u00adization \nclosures, which can be expensive. Compiler generated clo\u00adsures tend to capture a lot more state than \nhand-coded implementa\u00adtions. Idempotent work.ows also add unnecessary logging to trans\u00adactions that are \nalready idempotent. Fig. 12 shows the average size of messages sent in hand-coded and work.ow monad based \nimplementations. As expected, the work.ow based implementation generates signi.cantly larger mes\u00adsages. \nHowever, experiments on Windows Azure show that the size of the message does not signi.cantly in.uence \nthe read/write la\u00adtency or throughput of channels [1], as con.rmed by our experi\u00adments below. Next, we \nevaluate the overall performance overheads of using the idempotence monad relative to hand-coded implementations. \nFor each benchmark application, we evaluate three versions -the original hand-coded version, and two \nmonad based versions. The .rst version (synchronous) evaluates the work.ow synchronously in the context \nof the current agent and delegates the responsibility of retrying operations to the client. The second \nversion uses asyn\u00adchronous evaluation. We hosted each of the applications on Windows Azure and ensured \nthat each variant was assigned the same hardware re\u00adsources i.e. 5 virtual machines, each with a 4-core \nprocessor and 7 GB RAM. We assigned two virtual machines each for the front end agents that service browser \nrequests and agents that interact with the storage system and one virtual machine for a background agent \nuses for asynchronous/check-pointed evaluation. In the syn\u00adchronous evaluation variant, we assigned an \nadditional virtual ma\u00adchine to the storage interaction agent. For each benchmark, we cre\u00adated a workload \nthat simulates users exercising performance crit\u00adical scenarios in the application. For example, in the \nsurveys ap\u00adplication, the workload consists of a mix of surveys response re\u00adquests and survey analysis \nrequests. In each case, the workload is biased towards write requests (e.g. 75% responses and 25% anal\u00adysis \nrequests in the survey application) to ensure that molecular transactions are on the critical path. We \nran the workload for 3 min\u00adutes (with a 30 seconds warm-up period) and measured throughput (number of \nrequests serviced per second) and latency (average re\u00adsponse time) for varying number of simultaneous \nusers. Fig. 13 shows the measured throughput and latency for the surveys application. In the baseline \nversion, both throughput and latency increase almost linearly with the number of users. All monad based \nimplementations closely follow this trend, suggesting very little additional overhead due to our implementation. \nThe throughput (60 operations/second) and latencies are along expected lines. The expected throughput \nof Azure tables is 500 reads/writes per second, whereas our operation is a work.ow consisting of two \ntransactions.  The auction application (Fig. 14) has slightly different perfor\u00admance characteristics. \nThe throughput of this application is lower than the surveys application and it decreases with load. \nThis is expected since the work.ow is signi.cantly longer. Synchronous evaluation achieves better throughput \nthan the hand-coded imple\u00admented (which is asynchronous) at almost the same latency. As expected, asynchronous \nevaluation improves latency, especially at low loads, but at the cost of reduced throughput (up to 25% \nlower). At high loads, all monad based implementations perform better than the hand-coded implementation. \nThis is due to a few low level optimizations (such as batching of requests) performed by our im\u00adplementation. \nWe leave a more detailed performance analysis and optimization of the idempotence monad for future work. \n6. Related Work Our work builds on previous literature in the topics of building reliable distributed \nsystems and transactional work.ow. Idempo\u00adtence has been widely and informally recognized as an important \nproperty of distributed systems. Our key contributions include: (a) A formal speci.cation of the desired \ncorrectness properties, both safety (idempotence) as well as progress (failure freedom). (b) An automatic \ntechnique for guaranteeing failfree idempotence, pre\u00adsented as a monad. This implementation technique \nis decentralized and does not require distributed coordination. (c) Language con\u00adstructs for idempotent \nwork.ow. The need to move beyond atomic transactions to sequential compositions of atomic transactions \n(i.e., work.ows) motivated the early work on Sagas and long running transactions [10, 20]. These constructs \nare weaker than distributed transactions and are gener\u00adally used to orchestrate processes that run for \nextended periods. Kamath et al [12] discusses several issues relating to the implemen\u00adtation of transaction \nmanagers in Sagas and work.ows. A distin\u00adguishing aspect of our work is that it exploits the fact that \na service is required to be observationally idempotent (from its clients per\u00adspective) to simplify the \ninternal implementation of the work.ow. In particular, this lets us avoid the need for distributed coordination \nwith work.ow managers. Modern web applications often exploit horizontal partitioning for scalability, \nwhich involves storing data in a number of differ\u00adent databases or non-relational data-stores. This leads \nto a need for work.ow-style computations even within a single intra-enterprise application. Since scalability \nis key (and the motivation for data partitioning), conventional work.ow engines are rarely considered \nfor use in this context. Instead, the programmer usually realizes the work.ow manually, exposing themselves \nto all the subtleties of realizing such work.ow correctly. Pritchett [16] describes pro\u00adgramming methodologies \nfor use in these scenarios. Our goal is to provide lightweight language mechanisms that can be used to \nreal\u00adize idempotent work.ows correctly in such scenarios. Helland [11] explains in detail why idempotence \nis an essential property for reliable systems. Frolund et al. [9] de.ne a correctness criteria known \nas X-Ability for replicated services. A history is said to be x-able if it is equivalent to some history \nwhere every request is processed exactly once. Much like failfree idempotence, X-Ability is both a safety \nand liveness criteria. Our notion of failfree idempotence is general\u00adizes X-Ability beyond replication \nrequests to work.ows. There are also signi.cant differences in our implementation techniques. Our work \nis also related to open nested multi-level transac\u00adtions [15, 19]. These two constructs share the use \nof compensat\u00ading actions, but are semantically different. Open nested transactions provide a way for \ndealing with con.icts at a higher level of abstrac\u00adtion, which often leads to increased concurrency. \nOur basic setting is similar to Argus [14]. However, the con\u00adstruct that Argus provides programmers to \ndeal with process fail\u00adures is a conventional transaction. As with sagas, we show that many applications \ncan be expressed using work.ows (as we cover in Section 2) with compensating actions to compensate for \nthe lack of isolation. The transactor programming model [7] also provide primitives for dealing with \nprocess failures in a distributed system. However, there is no shared state in the transactor model. \nThe prim\u00aditives provided by the transactor model (stabilize, checkpoint, and rollback) are different \nfrom the primitives we study. Bruni et al [5] formalize compensating actions in an abstract language. \nTheir formalism, however, does not explicitly model state. Their paper, in fact, suggests study of compensation \nin the context of imperative features (state, variables, control-.ow con\u00adstructs) as future work. Our \nwork provides a compensation-based transactional construct as a library in a real language (F#) for a \nreal system (Azure), in addition to the theoretical treatment in the set\u00adting of lambda-calculus with \nmutable shared state. Luis et. al [6] propose an abstract model of compensating ac\u00adtions (based on process \ncalculus) for reasoning about correctness for work.ows that use compensating actions. Faulty lambda calculus \n[18] is a programming language and a type system for fault tolerance. However, .FAIL is addresses process \nfailures, while faulty lambda calculus addresses transient data corruption errors. Acknowledgments We \nwould like to acknowledge Siddharth Agarwal, Nirmesh Malviya, Khilan Gudka, and Dheeraj Singh for their \ncontributions. References [1] Azurescope: Benchmarking and Guidance for Windows Azure. http://azurescope.cloudapp.net/BenchmarkTestCases/ \n#4f2bdbcc-7c23-4c06-9c00-f2cc12d2d2a7, June 2011. [2] Bid Now Sample. http://bidnow.codeplex.com, June \n2011. [3] The Tailspin Scenario. http://msdn.microsoft.com/ en-us/library/ff966486.aspx, June 2011. \n[4] Windows Azure Patterns and Practices. http://wag.codeplex. com/, 2011. [5] Roberto Bruni, Hern \u00b4an \nMelgratti, and Ugo Montanari. Theoretical Foundations for Compensations in Flow Composition Languages. \nIn Proceedings of POPL, pages 209 220, 2005. [6] Luis Caires, Carla Ferreira, and Hugo Vieira. A Process \nCalculus Analysis of Compensations. In Trustworthy Global Computing, volume 5474 of Lecture Notes in \nComputer Science, pages 87 103. 2009. [7] John Field and Carlos A. Varela. Transactors: A Programming \nModel for Maintaining Globally Consistent Distributed State in Unreliable Environments. In Proceedings \nof POPL, pages 195 208, 2005. [8] M.J. Fischer, N.A. Lynch, and M.S. Paterson. Impossibility of Distributed \nConsensus with one Faulty Process. Journal of the ACM (JACM), 32(2):374 382, 1985. [9] Svend Frolund \nand Rachid Guerraoui. X-Ability: A Theory of Replication. Distributed Computing, 14, 2000. [10] Hector \nGarcia-Molina and Kenneth Salem. Sagas. In Proc. of ICMD, pages 249 259, 1987. [11] Pat Helland. Idempotence \nis not a medical condition. ACM Queue, 10(4):30 46, 2012. [12] Mohan Kamath and Krithi Ramamritham. Correctness \nIssues in Work.ow Management. Distributed Systems Engineering, 3(4):213, 1996. [13] Sheng Liang, Paul \nHudak, and Mark Jones. Monad Transformers and Modular Interpreters. In In Proc. of POPL, pages 333 343, \n1995. [14] Barbara Liskov. Distributed programming in argus. Communications of ACM, 31:300 312, March \n1988. [15] J. Eliot B. Moss. Nested Transactions: An Approach to Reliable Distributed Computing, 1981. \n[16] Dan Pritchett. Base: An acid alternative. Queue, 6(3):48 55, May 2008. [17] Philip Wadler and Peter \nThiemann. The Marriage of Effects and Monads. ACM Trans. Comput. Log., 4(1):1 32, 2003. [18] David Walker, \nLester Mackey, Jay Ligatti, George A. Reis, and David I. August. Static Typing for a Faulty Lambda Calculus. \nIn In ACM International Conference on Functional Programming, 2006. [19] Gerhard Weikum and Hans-J. Schek. \nConcepts and Applications of Multilevel Transactions and Open Nested Transactions. In Database Transaction \nModels for Advanced Applications, pages 515 553, 1992. [20] Gerhard Weikum and Gottfried Vossen. Transactional \nInformation Systems: Theory, Algorithms, and the Practice of Concurrency Control. 2001. A. An Abstract \nDe.nition of Idempotence We now present a more abstract de.nition of idempotence in terms of histories. \nBasics. The users of the system issue requests to the system. Let I denote a set of input messages (or \nrequests). The system responds to requests with a response. Let O denote a set of output values. An output \nmessage or response is a pair (i, o) . I \u00d7 O indicating that the output value o is produced in response \nto input request i. A history p is a sequence e1e2 \u00b7 \u00b7 \u00b7 ek of requests and responses (i.e., each ei \ncan be either a request or a response). We will restrict our attention to histories that satisfy the \nsimple property that every response corresponds to an earlier request in the history. We de.ne a speci.cation \nF to be a set of histories. In the sequel, we use a set of histories to specify desired safety properties \nof the system. In the sequel, let q . I range over requests and let r . I \u00d7 O range over responses. Let \na, \u00df, and . range over sequences of requests and responses. DE FIN I T I ON A.1. A speci.cation F is \nsaid to be asynchronous if it satis.es the following properties. 1. a\u00dfq. . F . aq\u00df. . F. 2. ar\u00df. . F \n. a\u00dfr. . F.  The above conditions are a natural restriction on speci.cations because of messaging delays \nthat cannot be controlled. The above property is also related to the notion of linearizability. Given \nany sequential speci.cation Fs, linearizable Fs can be de.ned as the smallest asynchronous speci.cation \nthat contains Fs. Every lin\u00adearizable speci.cation sati.es the asynchronous property de.ned above, but \nnot all asynchronous speci.cations are linearizable. Idempotence. Two requests q1 and q2 (in the same \nhistory) are said to be duplicates if q1 = q2. Two responses (q1, r1) and (q2, r2) (in the same history) \nare said to be duplicates if q1 = q2. DE FI NI TI O N A.2. A speci.cation F is said to idempotent iff: \n1. Duplicate requests have no effect: aq\u00dfq. . F iff aq\u00df. . F. 2. Duplicates responses have the same \nvalue:  a(q, o1)\u00df(q, o2). . F . o1 = o2. 3. Duplicate responses are allowed: ar\u00dfr. . F iff ar\u00df. . F. \n(The above de.nition is intended for asynchronous speci.ca\u00adtions. Hence, the asynchrony conditions have \nbeen used to simplify the de.nition.) Idempotence Closure. We de.ne a history to be repetitionless if \nit contains no duplicate requests or duplicate responses. We de.ne a speci.cation F to be repetitionless \nif all histories in F are repetitionless. Given a repetitionless speci.cation F, we de.ne its idempo\u00adtence \nclosure idem(F) to be the smallest speci.cation that contains F and is idempotent. We now summarize our \ngoal: given a program p that satis.es a repetitionless speci.cation F in the absence of process failures \nand duplicate requests, construct a program p ' that satis.es idem(F) even in the presence of process \nfailures and duplicate requests. Extension The above de.nitions can be generalized to permit requests \nto be of the form (k, v), where k is a unique-identi.er (key) for an input request, and allowing responses \nto be of the form (k, o), where k is the unique-identi.er of the input request for which the response \nis produced. Note that the above de.nition does not capture the progress conditions of failure-freedom. \n B. The Semantics of .IDF F We adapt the earlier operational semantics of .FAIL as shown in Fig. 15 to \nde.ne the semantics of .I DFF . We extend the de.nition SA, the set of agents: previously, an agent was \nof the form v C e. Now, an agent may now also be of the form v . e, indicating a idff evaluation of expression \ne with identi.er v. Syntax Extension: e . (Exp) ::= \u00b7 \u00b7 \u00b7 | idff v1 v2 Semantic Domain Changes: SA = \n(Exp)\u00d7 (Exp)+(Exp) \u00d7(Exp) Additional Evaluation Rules: [I DFF-NEW ] v C E[idff id e] . a .e ' .id . e \n' . a ' (p, \u00b5, a, I , O) . (p, \u00b5, a l {id . e()}, I , O) [I D FF-USE] id . w . a w . (Val) ' (p, \u00b5, \na l {v C E[idff id e]}, I , O) . (p, \u00b5, a l {v C E [w]}, I , O) [NORMAL] (\u00b5, e) (\u00b5 ' , e ' ) (p, \u00b5, a \nl {v . e}, I , O) . (p, \u00b5 ' , a l {v . e ' }, I , O) Figure 15: The language .I DFF , de.ned via extensions \nto language .FA I L . Rules IDFF-NEW and IDFF-USE must be duplicated for v . E[idff id e] as well. Rule \nIDFF-NEW handles the evaluation of the construct idff v e, when no preceding idff computation with the \nsame identi.er v has been initiated. This has the effect of creating a new agent v . e. Rule IDFF-USE \nallows the computation idff v e to proceed once the created agent completes evaluation (as indicated \nby the presence of an agent of the form v . w, where w is a value). The same rule also applies to duplicate \nevaluations with the same id.   \n\t\t\t", "proc_id": "2429069", "abstract": "<p>Building distributed services and applications is challenging due to the pitfalls of distribution such as process and communication failures. A natural solution to these problems is to detect potential failures, and retry the failed computation and/or resend messages. Ensuring correctness in such an environment requires distributed services and applications to be idempotent.</p> <p>In this paper, we study the inter-related aspects of process failures, duplicate messages, and idempotence. We first introduce a simple core language (based on lambda calculus inspired by modern distributed computing platforms. This language formalizes the notions of a service, duplicate requests, process failures, data partitioning, and local atomic transactions that are restricted to a single store.</p> <p>We then formalize a desired (generic) correctness criterion for applications written in this language, consisting of idempotence (which captures the desired safety properties) and failure-freedom (which captures the desired progress properties).</p> <p>We then propose language support in the form of a monad that automatically ensures failfree idempotence. A key characteristic of our implementation is that it is decentralized and does not require distributed coordination. We show that the language support can be enriched with other useful constructs, such as compensations, while retaining the coordination-free decentralized nature of the implementation.</p> <p>We have implemented the idempotence monad (and its variants) in F# and C# and used our implementation to build realistic applications on Windows Azure. We find that the monad has low runtime overheads and leads to more declarative applications.</p>", "authors": [{"name": "Ganesan Ramalingam", "author_profile_id": "81479640532", "affiliation": "Microsoft Research, Bangalore, India", "person_id": "P3977960", "email_address": "grama@microsoft.com", "orcid_id": ""}, {"name": "Kapil Vaswani", "author_profile_id": "81100057042", "affiliation": "Microsoft Research, Bangalore, India", "person_id": "P3977961", "email_address": "kapilv@microsoft.com", "orcid_id": ""}], "doi_number": "10.1145/2429069.2429100", "year": "2013", "article_id": "2429100", "conference": "POPL", "title": "Fault tolerance via idempotence", "url": "http://dl.acm.org/citation.cfm?id=2429100"}