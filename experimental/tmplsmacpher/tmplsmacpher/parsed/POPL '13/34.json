{"article_publication_date": "01-23-2013", "fulltext": "\n A Model-Learner Pattern for Bayesian Reasoning Andrew D. Gordon (Microsoft Research and University \nof Edinburgh) Mihhail Aizatulin (Open University) Johannes Borgstr \u00a8Guillaume Claret (Microsoft Research) \nom (Uppsala University) Thore Graepel (Microsoft Research) Aditya V. Nori (Microsoft Research) Sriram \nK. Rajamani (Microsoft Research) Claudio Russo (Microsoft Research) Abstract A Bayesian model is based \non a pair of probability distributions, known as the prior and sampling distributions. A wide range of \nfundamental machine learning tasks, including regression, classi\u00ad.cation, clustering, and many others, \ncan all be seen as Bayesian models. We propose a new probabilistic programming abstraction, a typed Bayesian \nmodel, based on a pair of probabilistic expressions for the prior and sampling distributions. A sampler \nfor a model is an algorithm to compute synthetic data from its sampling distribu\u00adtion, while a learner \nfor a model is an algorithm for probabilis\u00adtic inference on the model. Models, samplers, and learners \nform a generic programming pattern for model-based inference. They sup\u00adport the uniform expression of \ncommon tasks including model test\u00ading, and generic compositions such as mixture models, evidence\u00adbased \nmodel averaging, and mixtures of experts. A formal seman\u00adtics supports reasoning about model equivalence \nand implemen\u00adtation correctness. By developing a series of examples and three learner implementations \nbased on exact inference, factor graphs, and Markov chain Monte Carlo, we demonstrate the broad applica\u00adbility \nof this new programming pattern. Categories and Subject Descriptors D.3.3 [Programming Lan\u00adguages]: Language \nConstructs and Features Patterns Keywords Bayesian reasoning, machine learning, model-learner pattern, \nprobabilistic programming 1. Introduction Background: Bayesian Models and Inference We give the sim\u00adple, \ngeneral structure of a Bayesian model (see MacKay (2003) for instance); many examples follow later in \nthe paper. Let y be the output of the model, such as the object to be predicted or observed, and let \nx be any input information on which to condition, such as the feature vector in classi.cation or regression. \nLet w be the pa\u00adrameters of the model and let h be the hyperparameters. The key ingredients of a Bayesian \nmodel are the two conditional probabil\u00adity distributions: the prior distribution p(w|h) over the parameters; \n the sampling distribution p(y|x,w) over the output.  Permission to make digital or hard copies of \nall or part of this work for personal or classroom use is granted without fee provided that copies are \nnot made or distributed for pro.t or commercial advantage and that copies bear this notice and the full \ncitation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to \nlists, requires prior speci.c permission and/or a fee. POPL 13, January 23 25, 2013, Rome, Italy. Copyright \nc &#38;#169; 2013 ACM 978-1-4503-1832-7/13/01. . . $10.00 The parameters w parameterize the sampling \ndistribution over the output, while the hyperparameters h parameterize the prior distri\u00adbution over the \nparameters. Given training data d = (x,y), we obtain by Bayes rule expres\u00adsions for computing the following \ndistributions: the posterior distribution p(w|d,h); ' the posterior predictive distribution p(y'|x,d,h), \nassuming ' that (x,y') are independent from and identically distributed as (x,y). This fundamental Bayesian \nmodel represents a wide variety of machine learning tasks. There are also a great many machine learning \nalgorithms for probabilistic inference, that is, for comput\u00ading exactly or approximately the posterior \np(w|d,h), and for using ' p(y'|x,d, h) to make predictions. Background: Probabilistic Programming for \nInference Prob\u00adabilistic programming language systems allow automatic gener\u00adation of bespoke machine-learning \nalgorithms. Such systems in\u00adclude AutoBayes (Schumann et al. 2008), Alchemy (Domingos et al. 2008), BUGS \n(Gilks et al. 1994), Church (Goodman et al. 2008), Csoft (Winn and Minka 2009), FACTORIE (McCallum et \nal. 2009), Figaro (Pfeffer 2010), Fun (Borgstr \u00a8om et al. 2011), HANSEI (Kiselyov and Shan 2009), HBC \n(Daum\u00b4e III 2008), IBAL (Pfeffer 2001), Probabilistic cc (Gupta et al. 1999), PFP (Er\u00adwig and Kollmansberger \n2006), and Probabilistic Scheme (Radul 2007), amongst others. The user writes a short probabilistic program, \noften embedded within a larger conventional program, and the system produces an algorithm for learning \ndistributions given by the probabilistic pro\u00adgram. Hence, probabilistic programming saves development \ncosts compared to the alternative of writing the inference algorithm from scratch. Probabilistic programming \nis more .exible than the alter\u00adnative of relying on a .xed algorithm for a particular task, as one can \neasily compose, refactor, and write variations of models. On the other hand, for some tasks a .xed algorithm \nmay be more ef\u00ad.cient, but much progress is being made on inference engines for probabilistic programs. \nStill, the current practice of probabilistic programming is low\u00adlevel, irregular, and unstructured. Probabilistic \nprograms represent Bayesian models, but simply intermingle the code for de.ning pa\u00adrameters, predicting \noutputs, observing data, and so on. The ab\u00adsence of such structure is a missed opportunity. For example, \nin our own experience with Fun, we have over a dozen samples, but very little code re-use even though \neach sample performs similar tasks such as training, parameter learning, and prediction. We also dupli\u00adcate \ncode for the task of testing a model by sampling parameters, generating synthetic data from the predictive \ndistribution, and then comparing with the outcome of learning the parameters from the synthetic data. \nMoreover, there is repetition of common probabilis\u00adtic patterns such as constructing mixture models. \nOther probabilis\u00adtic programming systems share these problems.  Our Proposal: The Model-Learner Pattern \nThe central idea of the paper is to add structure and code re-use to probabilistic pro\u00adgramming by representing \na Bayesian model by a generic type Model<TH,TW,TX,TY>. A value of this type is a record contain\u00ading a \nhyperparameter together with probabilistic expressions for the prior and sampling distributions. The \ntype parameters correspond to the constituents of a Bayesian model: hyperparameters h : TH, parameters \nw : TW, inputs x : TX, and outputs y : TY. The aim of the model-learner pattern is to make construction, \nuse, and re-use of models easier (rather than to make inference more ef.cient). We are aware of no probabilistic \nsystem that encourages writing Bayesian models in a generic format. Common patterns of constructing Bayesian \nmodels can be writ\u00adten as functions on these typed models. Given any model, we can derive a sampler object, \nwhich has general methods to draw sam\u00adples from the prior and sampling distributions, for test purposes. \nGiven any model and a suitable algorithm, we can derive a learner object, which has general methods to \ntrain on data, and to compute the posterior distribution and posterior predictive distributions. Fun \nWe evaluate the model-learner pattern by developing the idea in detail using Fun (Borgstr \u00a8 om et al. \n2011), a probabilistic lan\u00adguage embedded within F#, a dialect of ML for .NET. (The model\u00adlearner pattern \ncan be developed in other probabilistic languages too, with or without types, but Fun has both a precise \nformal seman\u00adtics and an ef.cient implementation.) As background, Section 2 re\u00adcalls the syntax, type \nsystem, and informal semantics of Fun. Model-Learner Pattern, and a Reference Learner Section 3 de\u00adscribes \nour notions of models, samplers, and learners using Fun, independently of any particular implementation \nof probabilistic in\u00adference. Theorem 1 asserts that the reference learner does indeed compute the posterior \nand posterior predictive distributions. Learners based on Exact Inference and Factor Graphs We present \nin Section 4 a learner for discrete models that uses Al\u00adgebraic Decision Diagrams (ADDs) as a compact \nrepresentation of joint distributions. Theorem 2 shows correctness of the ADD back\u00adend. We show by example \nthat Bayesian networks can be presented as models, and solved using ADDs, with performance comparable \nto other systems. Section 5 describes inference using probabilistic graphical models in Infer.NET (Minka \net al. 2009). We describe our typed API for inferring distributions using Infer.NET. Theory We enhance \nthe previous semantics of Fun. The original version of Fun and its semantics (Borgstr \u00a8om et al. 2011) \ndid not include sums or observations on composite types, did not describe a semantics of Fun in the probability \nmonad, and did not consider how to compute a density function for a probabilistic expression. In Section \n6, we recall the measure-transformer semantics of Fun from previous work (Borgstr \u00a8 om et al. 2011). \nWe extend Fun and the measure-transformer semantics with sum types, and give an in\u00adductive de.nition \nfor observations on composite types. We identify a normal form with a single outermost observation, Theorem \n3, en\u00adabling a semantics for many Fun expressions using the standard probability monad. These results \nprovide a .rm foundation for our semantics of the model-learner pattern. Generic Mixtures Section 7 shows \nhow generic compositions such as mixture models, evidence-based model averaging, and mix\u00adtures of experts \ncan be seen as model combinators, that is, F# func\u00adtions for producing models from models. Fun itself \nis .rst-order in the sense that functions are not values, but our combinators are higher-order functions \nin the host language F#. Theorem 4 shows that we can compute the evidence for a model (a measure of how \nwell the model .ts the observed data) using a simple if-expression, enabling a simple de.nition of these \ncombinators and setting us apart from many other probabilistic languages. Third Learner: via Markov chain \nMonte Carlo In Section 8, we develop the theory and implementation of a learner based on Markov chain \nMonte Carlo (MCMC) techniques. This learner is built on Filzbach (Purves and Lyutsarev 2012), a generic \nMCMC system. Given training data d = (x, y), Bayesian MCMC compu\u00adtations explore different values for \nparameter w, and for each w require us to compute the density of the posterior function, that is, p(w|d, \nh). Hence, to implement a learner, we need to compute the density of a probabilistic expression. We present \na algorithm based on the direct symbolic evaluation of a probabilistic expression with only deterministic \nlet bindings, verify its correctness as Theorem 5, and report our implementation. With our method, the \nuser provides only a model, and our system automatically computes the density of the posterior; hence, \nwe save the user effort compared to existing practice with Filzbach (and other MCMC systems) where the \nuser provides code for both the sampling distribution and the density calculation. Table of Model Types \nTable 1 at the end of the paper summa\u00adrizes our collection of typed models. Our examples demonstrate \nthat a wide range of tasks, including regression, classi.cation, topic modelling, all .t the model-learner \npattern. All our example models have been tested via at least one of our three learner backends. Our \nInfer.NET learner generates code of the same form as one would execute directly, so there is no performance \npenality. Our initial Filzbach learner used an interpretive log-posterior function, which was much less \nef.cient than typical log-posterior functions written in C. Our current implementation now uses run-time \ncode gener\u00adation to compile the log-posterior function, yielding performance competitive with the C-code; \nfurther gains may be achieved by run\u00adtime specialization of this function to the data at hand. Our practical \nexamples are evidence that a wide range of machine learning tasks are executable using the model-learning \npattern, with no loss in performance in principle. but with the advantage of automatic syn\u00adthesis of \ntest data from the model, and in the case of MCMC-based systems such as Filzbach, the automatic construction \nof the density function. Section 9 discusses related work and Section 10 concludes. Contributions of \nthe Paper The new conceptual insight is that code-based machine learning can be structured around typed \nBayesian models, which are records containing a hyperparame\u00adter together with probabilistic expressions \nfor prior and sampling distributions. Our speci.c technical contributions: De.nition of a type of Bayesian \nmodels, with combinators for compositionally constructing models, and operations to derive samplers and \nlearners from an arbitrary model.  Many Bayesian examples expressed as such executable models.  A formal \nsemantics for models, learning, and prediction in terms of Fun, and its semantics using measure transformers \nand the probability monad.  Learners based on Algebraic Decision Diagrams, message\u00adpassing on factor \ngraphs, and MCMC.  Our generic format for models, and generic learner and sampler interfaces have several \nadvantages over conventional probabilistic programming. An end-user may assemble new models from pre\u00adexisting \nmodels and combinators, without writing models from scratch. Our API is accessible from other languages \nsuch as C#. It is easy to add a new inference algorithm as a new learner. Finally, we enable generic \nprogramming for code re-use.  Most of the listings in the paper are directly imported from our F# code, \nand are a mixture of quoted probabilistic Fun code and deterministic functions in full F#. A full version \nwith additional details and proofs is available (Gordon et al. 2013). 2. Fun: Probabilistic Expressions \n(Review) We recall the core calculus Fun (Borgstr \u00a8om et al. 2011), enriched here with sum types to support \nthe normal form of Section 6. Fun is a .rst-order functional language, without recursion. Our implementation \nef.ciently supports a richer language with arrays and array comprehensions, and Vector and Matrix types, \nwhose semantics can be seen as shorthands for constructs in this core. We let c range over constant data \nof base and unit type, n over integers and r over real numbers. We write ty(c) = t to mean that constant \nc has type t. Values and Types of Fun: U,V ::= x | c | (V,V ) | inl V | inr V value a, b ::= int | real \nbase type t,u ::= unit | (t1 *t2) | (t1 +t2) compound type Let bool . unit + unit. We let Vt be the set \nof closed values of type t (real numbers, integers, and so on). Semantically we consider real to be the \nreals, but our implementation uses double-precision .oats. We assume a collection of deterministic functions \non these types, including arithmetic and logical operators, and fst and snd projections on pairs. Each \nfunction f of arity n has a signature of the form val f : t1 * \u00b7\u00b7\u00b7 * tn .tn+1. We assume standard families \nof primitive probability distributions of type PDist(t), such as the following. Distributions: Dist : \n(x1 : t1 * \u00b7\u00b7\u00b7 * xn : tn) . PDist(t) Bernoulli : (bias : real) . PDist(bool) Beta : (a : real * b : real) \n. PDist(real) Gaussian : (mean : real * prec : real) . PDist(real) Gamma : (shape : real * scale : real) \n. PDist(real) A Bernoulli distribution is a biased coin .ip; the bias lies in the unit interval [0,1] \nand is the probability of true. A Beta distribution, often used as a prior distribution on the bias of \na Bernoulli distri\u00adbution, is a distribution on the unit interval; when a = 1 and b = 1, it is the uniform \ndistribution on the unit interval. A Gaussian dis\u00adtribution is parameterized by its mean and precision; \nthe standard deviation s follows from the identity s2 = 1/prec. A Gamma dis\u00adtribution, often used as \na prior distribution on the precision of a Gaussian distribution, is a distribution on the positive reals. \nThe expressions of Fun are in a syntax akin to administrative normal form, with let-expressions for sequential \ncomposition. Expressions of Fun: M,N ::= expression V value f (V1, . . . ,Vn) deterministic application \nlet x = M in N let (scope of x is N) match V with inl x : M matching (scope of x is M, | inr y : N scope \nof y is N) random(Dist(V )) primitive distribution observe f (V1, . . . ,Vn) observation fail failure \nWe rely on several standard syntactic conventions. We write if V then M else N for match V with inl : \nM | inr : N. We write M;N for let x = M in N when x is not free in N. Although formally Fun uses administrative \nnormal form, we often allow arbitrary expressions M in places where values V are expected, assuming the \ninsertion of suitable let-expressions. We allow arbitrary length tuples, formed by multiple pairings. \nWe make use of records, and arrays and array comprehensions, where the size of each array is known statically; \nwe consider operations on records and on statically-sized arrays to be reducible to operations on tuples. \nObservation and failure expressions represent conditioning. We usually write observations in the form \nobserve f (V ), where V is short for V1, . . . ,Vn. Such an observation expresses (unnormalized) conditioning \non the event that the indicator function f (V ) yields a zero, de.ned as a closed value where every instance \nof c is 0. The primitive fail represents an impossible event. It has the same meaning as an observation \nsuch as observe(1 + 1) whose indicator function cannot yield zero. Note that observe is a no-op on bool, \nsince any Boolean is a zero according to the above de.nition. We write observe (x = V ) for observe (x \n-V ) when V = (c1, . . . , cn) and x -V is the component-wise difference. We write G f M : t to mean \nthat in type environment G = x1 : t1, . . . , xn : tn (xi distinct) the expression M has type t. We write \n< for the empty type environment. Apart from the following, the typing rules are standard. Selected Typing \nRules: G f M : t (FU N RA N D O M) (FU N OB SE RVE) Dist : (x1 : t1 * \u00b7\u00b7\u00b7 * xn : tn) . PDist(t) G f f \n(V ) : t G f V : (t1 * \u00b7\u00b7\u00b7 *tn) G f observe f (V ) : unit G f random(Dist(V )) : t Fun is designed as \na subset of F#, so we represent Fun expressions using F# s features for re.ection (Syme 2006): quotation, \nevalua\u00adtion, and antiquotation. We represent a closed term M of type t by the F# quotation <@ M @> of \nF# type Expr<t>. More generally, if <,x1 : t1, . . . , xn : tn f M : t we represent M by the F# quotation \n<@ fun (x1, . . . , xn) . M @>. Suppose E is a quotation: E.Eval evaluates E to its value, and inside \na quotation, the % symbol de\u00adnotes antiquotation the expression (%E) embeds the expression E within the \nquotation. The semantics of Fun is detailed in Section 6; as usual, for pre\u00adcision concerning mixes of \ndiscrete and continuous probabilities, we turn to measure theory. Formally, the semantics of a closed \nFun expression M is a .nite measure M [[M]] over its return type. A sub\u00adtlety is that the semantics may \nor may not be a probability distribu\u00adtion, that is, one whose total measure is 1. Still, until Section \n6, we think simply of all Fun expressions as de.ning probability distri\u00adbutions (implicitly normalising \nthe measure), and use conventional mathematical notations for probability (as in the start of Section \n1). We assume some familiarity with basic concepts such as joint dis\u00adtributions and marginalization. \nLet an inference engine be an algorithm that given a quotation of a closed Fun expression M of type t \nreturns a possibly approximate representation of the distribution M [[M]]. We can represent an inference \nengine as a function Expr<t> .DistT where DistT is the type of the representation. 3. Models, Samplers, \nand Learners This section explains the central idea of the paper: that the prior and sampling distributions \nconstituting a Bayesian model may be represented by a pair of typed Fun expressions. We introduce the \ndistributions using conventional mathematical notations, alongside our formal notation using Fun expressions. \nWe begin by explaining the idea for a speci.c example, and then generalize.  3.1 A Speci.c Bayesian \nModel: Linear Regression in Fun As outlined in the introduction, a Bayesian model consists of a prior \ndistribution p(w|h) over parameter w, given hyperparameter h, together with a sampling distribution p(y|x,w) \nover output y, given input information x and parameter w. Viewed as a function of the parameters w, with \ny .xed, the sampling distribution is also known as the likelihood function. The prior distribution represents \nour uncertain beliefs about the parameters, given hyperparameter h. The sampling distribution represents \nour view of how output y is produced from input x given parameter w. We consider the problem of linear \nregression, that is, of .nding the best .tting line given a set of points. Our input data are d = (x,y) \nwhere x = [|x1;. . . ; xn|] and y = [|y1;. . . ; yn|] are arrays of coordinates. Intuitively, we .t a \nnoisy line yi = Axi + B + e to the data, where the noise e is drawn from a Gaussian distribution with \nmean 0 and precision Prec. The expression prior h below expresses our initial uncertainty about the parameters \nA, B, and Prec of the noisy line, where hyper\u00adparameter h provides parameters for these distributions. \n(A stan\u00addard alternative to a constant hyperparameter is to consider a prior over the hyperparameter, \nknown as the hyperprior; we discuss hy\u00adperpriors in the full version.) The probabilistic expression gen(w,x) \nde.nes how to generate each yi from xi and parameters w. (In the Fun code below, we use record and array \nnotations which are even\u00adtually treated as operations on tuples.) The Prior and Sampling Distributions: \nlet prior h = { A = random (Gaussian(h.MeanA, h.PrecA)) B = random (Gaussian(h.MeanB, h.PrecB)) Prec \n= random (Gamma(h.Shape, h.Scale)) } let gen (w,x) = [| for xi in x . (w.A * xi) + w.B + random(Gaussian(0.0, \nw.Prec)) |] These expressions formalize the prior and sampling distributions. The prior distribution \np(w|h) is the density of M [[prior h]] at w, while the sampling distribution p(y|x,w) is the density \nof M [[gen(w,x)]] at y. We assemble the two components of our model in the function predictive below, \nto obtain the prior predictive dis\u00adtribution p(y|x,h), the density of M [[predictive(h,x)]] at y. The \nPrior Predictive Distribution: let predictive (h,x) = let w = prior h in gen(w,x) The semantics of the \nlet-expression is obtained by forming the joint distribution of M [[prior h]] and M [[gen(w,x)]], and \nmarginalizing with respect to M [[gen(w,x)]]. Formally, the semantics is obtained as the following integral. \n p(y|x,h) =p(y|x, w)p(w|h)dw (1) We may sample from the prior predictive distribution by choosing h and \ninput x as shown by the following F# output. val h = {MeanA=0.0; PrecA=1.0; MeanB=5.0; PrecB=0.3; Shape=1.0; \nScale=1.0}val x = [|0.0; 1.0; 2.0; 3.0; 4.0; 5.0|] We .rst sample w from the prior distribution w = prior \nh, and then the output y = gen(w,x). val w = {A = 0.70; B = 4.33; Prec = 0.58;} // sampled prior(h) val \ny = [|4.85; 5.19; 7.36; 4.49; 8.10; 8.06|] // sampled gen(w,x) We express the posterior and posterior \npredictive distributions as Fun expressions, using observe to condition on the data (x,y). The Posterior \nand Posterior Predictive Distributions: let posterior (h,x,y) = let w = prior h in observe (y=gen(w,x)); \nw let posterior predictive(h,x,y,x ) = let w = posterior (h,x,y) in gen(w,x ) Given observed data d \n= (x,y), via Bayes Rule we obtain a poste\u00adrior distribution, that is, the prior in light of the data. \np(y|x,w)p(w|h) p(w|d, h) = (2) p(d|h) The normalization constant p(d|h) =p(y|x,w)p(w|h)dw is known as \nthe evidence or marginal likelihood. We also obtain the (poste\u00adrior) predictive distribution: ' ' p(y \n' |x ,d,h) =p(y ' |x , w)p(w|d,h)dw (3) Using a particular inference engine for Fun we may obtain concrete \nrepresentations of the normalized distributions. In the case of our running example, we try to infer \nthe parameters used to generate our sample data y. By running our Infer.NET implementation of Fun to \ncompute the distribution posterior(h, x,y) we obtain the following approximations. { A = Gaussian(0.5576, \n0.05089); // actual A=0.70 B = Gaussian(4.936, 0.404); // actual B=4.33 Prec = Gamma(1.695, 0.46)[mean=0.78];} \n// actual Prec=0.58 ' Moreover, suppose we have new input x = [|6.0; 7.0; 8.0; 9.0|]. By running our \nInfer.NET implementation of Fun to compute the distribution posterior predictive(h,x,y, x ' ) we obtain \nthe following. [| Gaussian(8.282, 0.1858); Gaussian(8.839, 0.1654); Gaussian(9.397, 0.1469); Gaussian(9.954, \n0.1303); |] To summarize, we modelled a noisy line by distributions written as the Fun expressions prior \nh and gen(w, x). We ran these expressions to draw samples from the predictive distribution, so as to \ncreate a synthetic dataset d = (x,y). We wrote Fun expressions for the pos\u00adterior and posterior predictive \ndistributions, and ran an inference engine to learn the posterior and to make predictions given fresh \ndata. These tasks are the essence of Bayesian reasoning. The re\u00admainder of this section proposes generic \ntypes and interfaces for these tasks.  3.2 Typed Bayesian Models in General In general, let a typed \nBayesian model be a value of the record type Model<TH,TW,TX,TY>, where the type parameters correspond \nto the different data of a Bayesian model: hyperparameters h : TH, parameters w : TW, inputs x : TX, \nand outputs y : TY. Typed Bayesian Model: type Model< TH, TW, TX, TY> = { HyperParameter: TH Prior: Expr< \nTH . TW> Gen: Expr< TW * TX . TY> } Given a model m, the Fun expression h = m.HyperParameter is the hyperparameter, \nthe Fun expression (%m.Prior) h is the prior distribution p(w|h), and the Fun expression (%m.Gen)(w, \nx) is the sampling distribution p(y|x,w). (We treat F# expressions such as (%m.Prior) h as being closed \nFun expressions, although strictly speaking they are F# expressions that evaluate via antiquotation and \nfunction application to closed Fun expressions.) The following module packages our linear regression \ncode as a typed model M1. We use F# quotations <@. . . @> to treat the bodies of prior and gen above \nas Fun expressions.  Linear Regression Model: module LinearRegression = type TH = {MeanA: real; PrecA: \nreal; ... } type TW< TA, TB, TN> = {A: TA; B: TB; Prec: TN} type TX = real[] type TY = real[] let M1 \n= { HyperParameter = h Prior = <@ fun h .prior h @> Gen = <@ fun (w,x) .gen(w,x) @> } : Model<TH,TW<real,real,real>,TX,TY> \n 3.3 Sampling Parameters and Data Given any model m, with h = m.HyperParameter, we construct a new sampler \nS by sampling w from p(w|h), and providing the methods: S.Parameters : TW is w sampled from the prior \np(w|h);  S.Sample(x) : TY samples the sampling distribution p(y|x,w).  The Sampler Interface: type \nISampler< TW, TX, TY> = interface abstract Parameters: TW abstract Sample: x: TX . TY end Hence, a sampler \ndraws from the prior predictive distribution. We omit our sampler code, which uses Eval to evaluate quotations. \n 3.4 Learning Parameters and Making Predictions Given any model m, with h = m.HyperParameter, and an \ninference engine, we may construct a new learner L with the following interface. The Learner Interface: \ntype ILearner< TDistW, TX, TY, TDistY> = interface abstract Train: x: TX * y: TY .unit abstract Posterior: \nunit . TDistW abstract Predict: x: TX . TDistY end The type TDistW represents distributions over parameter \nTW, while the type TDistY represents distributions over output TY. Different learners may use different \nrepresentations. Our ADD learner exactly represents distributions over Booleans using an ADD data structure, \nwhile our Infer.NET learner yields approxi\u00admate parameters of the marginal distributions of each dimension \nof the distribution and our MCMC learner represents a distribution as an ensemble of samples. We can \nthink of a Fun quotation as an exact representation of a conditional distribution on its return type, \nindependent of any inference engine. Using this idea, we present below our reference learner, which captures \nthe intended exact semantics of our API, by assembling suitable quotations. The mutable variable d takes \nas values Fun expressions that represent the current parameter distribution, initially the posterior. \nEach call to Train updates d by conditioning with the training data. Calls to Posterior and Predict return \nsuitable quotations for the posterior and posterior predictive distributions. (Compare with the posterior \nand posterior predictive functions in Section 3.1.) Reference Learner L for Model m: let h = m.HyperParameter \nlet d = ref <@ (%m.Prior) h @> { new ILearner<Expr< TW>, TX, TY,Expr< TY>> with member l.Train(x: TX,y: \nTY) = d := <@ let w = (% !d) in observe(y = ((%m.Gen) (w,x))) w @> member l.Posterior():Expr< TW> = (!d) \nmember l.Predict(x: TX):Expr< TY> = <@ let w = (% !d) in (%m.Gen) (w,x) @> } Theorem 1. After n calls \nto Train with arguments d = {(xi,yi)}n i=1, L.Posterior represents the posterior distribution p(w|d,h); \n' L.Predict(x ' ) represents the predictive distribution p(y ' |x ,d,h). Our reference learner code is \nin fact the basis of our exact learner based on ADDs. Our other learners use numeric represen\u00adtations \nof intermediate distributions, rather than a quotation. For example, a generic usage of our samplers \nand learners is to test whether an inference algorithm can recover known param\u00adeters from synthetic data. \nConsider a learner L constructed from a model m, hyperparameter h, and some inference engine. Given some \ninput x, we may test the effectiveness of L by constructing a new sampler S for m and h, and running \nthe following. let w: TW = S.Parameters // .xed parameters L.Train(x,S.Sample(x)) // train on synthetic \ndata let wD: TDistW = L.Posterior() // inferred distribution on w ... // test how probable w is according \nto wD We abstract this pattern (as a function) in Section 3.6.  3.5 Generic Combinator for IID Models \nIf we assume that the data is a collection d = {(xi,yi)}n of i=1 identically and independently distributed \n(IID) observations, then the sampling distribution factorizes according to: n p({yi}n i=1|{xi}in =1,w) \n= . p(yi|xi,w) (4) i=1 Hence, we arrive at our .rst model combinator, IIDArray. Given a model that sends \nTX to TY, IIDArray builds a new model with the same prior, but which sends TX[] to TY[]. Learning with \nany model built from this combinator is an instance of batch learning, where multiple data items are \nprocessed simultaneously. (In the code below, we use an F# quotation of a fun-abstraction to represent \na Fun expression with two free variables w and xs, as described in Section 2.) Combinator to Lift Model \nto Act on Arrays: module IIDArray = let M(m:Model< TH, TW, TX, TY>) = {HyperParameter = m.HyperParameter \nPrior = m.Prior Gen = <@ fun (w,xs) .[|for x in xs .(%m.Gen)(w,x)|] @> } : Model< TH, TW, TX[], TY[]> \nFor example, M2 below is linear regression on a single (x,y) point, and M3, obtained by our combinator, \nis equivalent to our original model M1. Models in the style of M2 are useful because we may assemble \nthem with other combinators before applying IIDArray, as shown in Section 7.  Linear Regression, Again \nand Again: let M2 = { HyperParameter = h Prior = <@ fun h .prior h @> // as before Gen = <@ fun(w,x) \n. (w.A * x) + w.B + random(Gaussian(0.0,w.Prec)) @> }: Model<TH,TW<real,real,real>,real,real> let M3 \n= IIDArray.M(M2)  3.6 Generic Loopback Testing A common method to test the effectiveness of a learner \non a par\u00adticular model is to generate IID sample data for different xi for a .xed w, and then evaluate \nthe posterior distribution for w obtained by training on that data. We here give a generic procedure \nfor such a loopback test. Functions for loopback testing: let test (toLearner: Model< TH, TW, TX, TY> \n. ILearner< DistW, TX, TY, DistY>) (m:Model< TH, TW, TX, TY>) (x: TX) : TW * DistW = let L = toLearner(m) \nlet S = Sampler.FromModel(m) let y = S.Sample(x) do L.Train(x,y) (S.Parameters,L.Posterior()) let testMany \nl m xs = test l (IIDArray.M(m)) xs Since the details of evaluating the inferred posterior depend on its \nlearner-speci.c representation, test simply returns the parame\u00adters and posterior distribution to the \ncaller. Notice how testMany is constructed by a simple application of the model combinator IIDArray.M. \nHere is an application of testMany to our running example, linear regression, using the Infer.NET learner \nwe describe below: Loopback testing of linear regression: let toL m = InferNetLearner.LearnerFromModel(m,Marginalize) \nlet (w,dW) = testMany toL M2 [| -100.0 .. 1.0 .. 100.0 |] On one test run, we obtained: w= { A = 0.3477837603; \nB = -27.13593566; Prec = 2.103660906;}dW={ A = Gaussian(0.3479, 1.467e06); B = Gaussian(-27.09, 435.0); \nPrec = Gamma(99.74, 0.02203)[mean=2.197];} Here the learner has inferred close approximations to A, B \nand Prec, with very high precision especially for A. 4. A Learner using Exact Inference The exact learner \nuses symbolic evaluation of Boolean functions representing discrete distributions in order to give an \nanswer that is either an exact marginal of each variable or the exact joint distribution. Let Bernoulli \nFun be the .nite fragment of Fun where the only sum type is bool = unit + unit, the only matches are \nconditionals, there are no instances of observe, and every random expression takes the form random (Bernoulli(c)) \nfor some real c . (0,1). The semantics of a closed Bernoulli Fun program is a sub-probability distribution \nover its discrete return type, that is, a measure that may sum to less than 1. 4.1 Inference by Symbolic \nEvaluation of Programs The procedure PO S T symbolically evaluates a Bernoulli Fun ex\u00adpression M, and \ncomputes the posterior sub-distribution over the result of M given a valuation of its free variables. \nThe algorithm relies on the following notations. We write let x : t = N in M for the corresponding untyped \nlet-expression when G f N : t. We write ite(v = U,VT ,VF ) for the conditional statement that returns \nVT when v = U and VF otherwise. A valuation s is a .nite map from variables to closed values; A G-valuation \ns is a .nite map x1 .V1, . . . , xn .Vn where each Vi . Vti . We let < denote the empty valuation, and \nwrite S(G) for the set of G-valuations. If G f V : t and s is a G-valuation, V s is the closed value \nresulting from substitut\u00ading s(x) for x in V for all x . dom(G). Given an open expression M, where G \nf M : t, POST is a function from valuations s . S(G) to discrete sub-probability distributions over Vt \n. By uncurrying, the semantics of M can also be expressed as a function from S(G) \u00d7 Vt to sub-probabilities \n[0,1]. The algorithm PO S T (M) computes this function by structural recursion on M. The PO S T function \nis a for\u00adward analysis that computes the posterior distribution, and is named by analogy with the standard \npost-condition computation in pro\u00adgram analysis. Post Calculation PO S T (M) : S(G) \u00d7 Vt . real when \nG f M : t PO S T(V ) = . s ,v.ite(v = V s,1,0) PO S T( f (V1, . . . , Vn)) = . s ,v.ite(v = f (V1s, . \n. . ,Vns),1,0) PO S T(Bernoulli(r)) = . s , v.ite(v = true,r,1 - r) PO S T(if V then N1 else N2) = . \ns ,v.POST(V )[s,true] \u00b7 PO S T(N1)[s,v] + POST(V )[s,false] \u00b7 POST(N2)[s , v] PO S T(let x : t = N1 \nin N2) = . s ,v..u.Vt PO S T(N1)[s,u] \u00b7 POST(N2)[(s, x . u),v] PO S T(fail) = . (s , ()).0 The POST \ncomputation uses four different types of operations on functions: (1) pointwise product, (2) pointwise \nsum, (3) if-then\u00adelse, (4) summation over a variable, or existential quanti.cation. The operations (1) \nand (2) are used, for example, in the case for if, operation (3) is used in the cases for values, deterministic \nfunctions, and Bernoulli, and operation (4) is used in the let case. Each of these operations is directly \nsupported by ADD packages such as CUDD (Somenzi 2012) and takes time proportional to the product of the \nsizes of the arguments in the worst case. Theorem 2. Let M be a Bernoulli Fun program with < f M : t \n. For all values V . Vt , PO S T(M)[<,V ] = M [[M]] {V }. The ADD learner builds on the reference learner, \ncalling the PO S T algorithm on the Fun expression returned from its Posterior and Predict methods. An \nADD learner has the following type, where ADD< T> is the type of a decision diagram that maps values \nof type T to probabilities. : ILearner<ADD< TW>, TX, TY, ADD< TY>> In our experiments, the performance \nof the ADD learner is com\u00adpetitive with algorithms implemented in SamIam (Darwiche 2009). Claret et al. \n(2012) report performance in detail for symbolic eval\u00aduation of a related imperative probabilistic language. \n 4.2 Example: Bayes Net for Sprinkler The sprinkler model is a classical example of a Bayes net (Pearl \nand Shafer 1995). In our variant, the model describes the conditional probabilities of rain having fallen \nand the sprinkler having been on, given that the grass is observed to be wet. In detail, given prior \ndistributions for rain and sprinkling, and the view that rain has a 90% chance of wetting the grass, \nsprinkling an 80% chance, and some other cause a 10% chance, what is the posterior distribution for rain \nand sprinkling, given that the grass is wet. We perform inference using the ADD learner described above, \nyielding an exact posterior.  Sprinkler model module Sprinkler = type TH = {RainH: real; SprinklerH: \nreal} type TW< b> = {Rain: b; Sprinkler: b} type TX = IsGrassWet // a unit type type TY = bool let M:Model<TH,TW<bool>,TX,TY> \n= {HyperParameter = {RainH=0.3; SprinklerH=0.5} Prior = <@ fun h . {Rain = random(Bernoulli(h.RainH)) \nSprinkler = random(Bernoulli(h.SprinklerH))} @> Gen = <@ fun (w,x) . (random (Bernoulli(0.9)) &#38;&#38; \nw.Rain) || (random (Bernoulli(0.8)) &#38;&#38; w.Sprinkler) || random (Bernoulli(0.1)) @>} Given an ADD \nlearner L for this model, here is the outcome of L.Train(IsGrassWet,true);L.Posterior. [({Rain = false; \nSprinkler = false;}, 0.05777484318); ({Rain = true; Sprinkler = false;}, 0.2253218884); ({Rain = false; \nSprinkler = true;}, 0.4737537141); ({Rain = true; Sprinkler = true;}, 0.2431495543)]; 5. A Learner based \non Factor Graph Inference Infer.NET is a probabilistic programming system which generates ef.cient, scalable \ninference algorithms based on message-passing on factor graphs. We compile a Fun expression to the input \nformat of Infer.NET, Csoft, and perform inference (Borgstr \u00a8 om et al. 2011). Since the original description \nof Fun, we have made a series of enhancements which make the present paper possible. These include the \nAPI described next, support for arrays, and a compiler based on multiple transformations of the Fun expression. \n5.1 Learner for Message-Passing Algorithms in Infer.NET We describe our interface for creating learners \nbased on Infer.NET, but omit the details of the implementation, which rely on the exist\u00ading translation \nfrom Fun to Infer.NET. Infer.NET computes marginal posterior distributions from joint distributions involving \nobserved and unobserved variables. For ex\u00adample, given a probabilistic program de.ning a joint distribution \non pairs of type bool \u00d7 real, Infer.NET computes the marginal of the .rst projection as a value d1 say \nof the distribution type Bernoulli, and computes the approximate marginal of the second projection as \na value d2 say of type Gaussian. (Bernoulli and Gaussian are In-fer.NET types representing the distribution \nas a record of its pa\u00adrameters.) Infer.NET uses the distribution families of the priors on these projections \nas (approximate) distribution types for the marginals. Let a marginal type G be a nesting of tuples, \nrecords and arrays, over distribution types such as Bernoulli and Gaussian. Let joint(G) be G with each \noccurrence of an Infer.NET type re\u00adplaced with its range; for instance, joint(Bernoulli) = bool and joint(Gaussian) \n= real. We perform inference using Infer.NET using the following function, where CompoundDistribution \nis a dynamically typed value representing the composite of the marginal distributions. Core Inference \nfor Infer.NET Fun: val inferDynamic: Expr< TA . TB> . TA .CompoundDistribution Next, we describe use \nof automatic coercions to achieve better static typing than with inferDynamic. A compound distribution \nfor G is a value of CompoundDistribution representing a value of type G. A G-coercion is a function CompoundDistribution \n. G that coerces a compound distribution for G to the corresponding value. To create a learner for m \n: Model<TH,TW,TX,TY> we ask the user to supply F# marginal types GW and GY such that joint(GW) = TW and \njoint(GY) = TY, plus a MarginalizeModel<GW,GY>, which is a pair of a GW-coercion and a GY-coercion. (We \nhave helper functions to construct these coercions, but we omit the de\u00adtails.) The resulting learner \ndoes approximate inference on the model m. After each call to Train, we run inference and record the \nresult at marginal type GW. Calling Posterior returns the current distribution. Calling Predict runs \ninference, and we return the result at marginal type GY. Constructing an Infer.NET Learner: type MarginalizeModel< \nGW, GY> = { MarginalizePrior: CompoundDistribution . GW MarginalizeGen: CompoundDistribution . GY } \nval Learner: (Model< TH, TW, TX, TY> * TH * MarginalizeModel< GW, GY> .ILearner< GW, TX, TY, GY>  5.2 \nExample: Gaussian Here is the model for the ubiquitous Gaussian. The parameters are its Mean and Precision. \nTheir priors are a Gaussian and a Gamma. In turn, the hyperparameter consists of the parameters for the \npriors. The sampling distribution simply draws from the Gaussian, ignoring the input of type TX = unit. \n(A model that ignores its input is said to be unsupervised.) Gaussian Model: module GaussianModel = type \nGammaW< TA, TB> = {Shape: TA; Scale: TB} type TW< TA, TB> = {Mean: TA; Precision: TB} type TH = { Gaussian: \nTW<real,real> Gamma: GammaW<real,real> } let M: Model<TH,TW<real,real>,unit,real> = {HyperParameter \n= {Gaussian={Mean=0.0;Precision=1.0} Gamma={Shape=1.1;Scale=2.0}} Prior= <@ fun h .{Mean= let m = h.Gaussian.Mean \nlet p = h.Gaussian.Precision random(Gaussian(m,p)) Precision=let sh = h.Gamma.Shape let sc = h.Gamma.Scale \nrandom(Gamma(sh,sc))}@> Gen= <@ fun (w,x) .let m,p = w.Mean,w.Precision random(Gaussian(m,p)) @> } We \nobtain a learner of the following type. The subject of the marginal type for y, Gaussian, is real, while \nthe subject of the marginal type for w, TW<Gaussian,Gamma>, is TW<real,real>. (We make TW generic so \nit can express both these types). : ILearner<TW<Gaussian,Gamma>,unit,real,Gaussian> After inference, \nwe can inspect the parameters of the inferred Gaussian and Gamma distributions to learn the inferred \nmean and variance of the data, and the remaining uncertainty thereof.  Probit is a standard binary classi.er \nbuilt from a Gaussian. Its model shares the same prior, and its output is a probabilistic Boolean dependent \non its input. We use this model in Section 7. Probit Model: let Probit : Model<TH,TW<real,real>,real,bool> \n= {HyperParameter = M.HyperParameter Prior = M.Prior Gen = <@ fun (w,x) . x < random(Gaussian(w.Mean,w.Precision)) \n@> } We have a range of other models including multivariate linear regression, the Bayes Point Machine \nclassi.er (Minka 2001), the LDA (Latent Dirichlet Allocation) topic modelling (Blei et al. 2003), and \nTrueSkill (Herbrich et al. 2006), but we omit the details. 6. Semantics of Models: Measures and Monads \nIn this section, we review the measure-transformer semantics of Fun (Borgstr \u00a8 om et al. 2011), and extend \nit with sum types and ob\u00adservations on composite types. Based on our experience with Fun models, their \nstructure often includes a single outermost observe statement. Here, we show that given a certain compatibility \ncon\u00adstraint on its semantics, any Fun expression can be transformed to one with a single outermost observe, \npreserving the seman\u00adtics. As part of the proof, we show that the measure-transformer semantics of observe-free \nFun models is closely related to their semantics when considered as programs of the stochastic lambda\u00adcalculus \nof Ramsey and Pfeffer (2002). This clari.es the relation\u00adship between our measure-transformer semantics \nand the probabil\u00adity monad, in particular for programs with none or only a single outermost observation. \n 6.1 Measure Transformer Semantics for Fun Expressions In this section, we recall the measure-transformers \nused in Borgstr \u00a8 om et al. (2011), and augment these with operations on sum types and failure. We also \ngive an inductive de.nition of the semantics of observations on composite types, and recall the compositional \nde\u00adnotational semantics of a Fun expression as a measure transformer, augmented with sum types. For more \nexplanations and intuitions, see (Borgstr \u00a8 om et al. 2011). We conclude by introducing the no\u00adtion of \ncompatible measure, and show that it is suf.cient (but not necessary) for all observations to be well-de.ned. \nWe de.ne the measurable sets of type t, written Mt , as the Lebesgue-measurable subsets of Vt , which \nin particular contains all closed sets. We write f : t . u to mean that f is a measurable function from \ntype t to type u, that is, that f -1(A) . Mt for all A . Mu. Let M t be the set of .nite measures on \nt, that is, additive functions from Mt to the non-negative real numbers. Let the sub\u00adprobability distributions \nS t be the .nite measures whose range is contained in [0,1]. Let t . u be the set of measure transformers \nfrom t to u, de.ned as the partial functions M t -M u. If G = x1 : t1, . . . , xn : tn we let range(G) \nt1 * \u00b7\u00b7\u00b7 *tn. We make use of the following constructions on measures. The Dirac d measure is dV (A) \n1 if V . A, 0 otherwise.  Given a function f : t . u and a measure \u00b5 . M t, there is a measure \u00b5 f -1 \n. M u given by (\u00b5 f -1)(B) \u00b5( f -1(B)). We can add two measures on the same set as (\u00b51 + \u00b52)(A) \u00b51(A) \n+ \u00b52(A). The disjoint sum (\u00b51 \u00b52) of two measures is de.ned as (\u00b51 \u00b52)(A B) = \u00b51(A) + \u00b52(B).  Given \na measure \u00b5 on t, a measurable set A . Mt and a function  f : t . real, we write A f d \u00b5 or equivalently \nA f (x)d\u00b5(x) for standard (Lebesgue) integration. This integration is always well-de.ned if \u00b5 is .nite \nand f is bounded. Given a measure \u00b5 on t, a function D \u00b5 : t . real is a density for \u00b5 iff \u00b5(A) = A(D \n\u00b5) d. for all A, where . is the completion of the standard measure on t (which is built by taking products \nand disjoint sums of the counting measure on int and the interval (Borel) measure on real). The semantics \nof a Fun program is given in terms of the follow\u00ading measure transformers, which encapsulate standard \ntheorems in .nite measure theory. Measure Transformer Combinators: pure . (t . u) . (t . u) >>> . (t1 \n. t2) . (t2 . t3) . (t1 . t3) extend . (t . S u) . (t . (t * u)) observe . (t . real) . (t . t) ||| . \n(t1 . u) . (t2 . u) . ((t1 +t2) . u) fail . (t . t) To lift a pure measurable function to a measure \ntransformer, we use the combinator pure. Given f : t . u, we let pure f \u00b5 A \u00b5 f -1(A), where \u00b5 . M t \nand A is a measurable set from u. To sequentially compose two measure transformers we use stan\u00addard function \ncomposition, de.ning T >>> U U . T . The combinator extend extends the domain of a measure using a function \nyielding sub-probability distributions. We let extend m \u00b5 AB m(x)({y | (x, y) . AB}) d\u00b5(x). Vt The combinator \nobserve computes the conditional density of a measure \u00b5 over t on the event that an indicator function \np of type t . real is zero. Note that this conditioning is unnormalized. We consider the family of events \np(x) = r where r ranges over R. We let Br e be the closed ball of radius e around r (that is, [x -e,r \n+ e]), and de.ne \u00b5.[A|| p = r] . R(the \u00b5-density at p = r of A) by: Conditional Density: \u00b5.[A|| p = r] \n\u00b5.[A|| p = r] lime.0 \u00b5(A n p-1(Br e ))/ Br1d. if the limit exists e We de.ne observe p \u00b5 A \u00b5.[A|| p = \n0]. As an example, if t = u * real, p = . (x,y).(y - c) and \u00b5 has continuous density D \u00b5 then observe \np \u00b5 A = D \u00b5(x,c) d. (x) (5) {x|(x,c).A} and R\u00b5.[A|| p = x] d. (x) = \u00b5(A). Notice that observe p \u00b5 A is \ngreater than \u00b5(A) if the density at p = 0 is greater than 1, so we cannot consider only transforming \n(sub-)probability distributions. Support for sum types (new) To add support for sum types, we in\u00adtroduce \na new measure transformer. We let (T1 ||| T2) \u00b5 A T1(X . \u00b5(X \u00d8))(A) + T2(X . \u00b5(\u00d8X))(A). We also let fail \n\u00b5 A 0. Observations on composite types (new) The Fun language of Borgstr \u00a8 om et al. (2011) permits observations \nonly on atomic types. To achieve a normal form (Theorem 3) we here extend observations to composite types. \nWe write obs f @u if f : t . u; then the measure transformer obs f has type t . t. Below we let 1b 1, \n(), 1t*u (1t ,1u) and 1t+u 1t . We write split p = 1unit pure . s.if p(s) then inl s else inr s. Observations \non Composite Types: obs f @u obs f @real observe f obs f @int split (. s. f (s) = 0) >>> (pure id ||| \nfail) obs f @unit pure id obs f @(u1 * u2) obs (snd . f )@u2 >>> obs (fst . f )@u1 obs f @(u1 + u2) split \nf >>> (obs (either id . .1u1 ) . f @u1 ||| obs (either . .1u2 id) . f @u2)  Observation at real type \nis primitive; it is trivial at unit type. At int we fail unless f returns 0. For products, we observe \neach component in turn. For sum types, we split the set of states depending on the branch of the sum \npicked by f , and run the observation in each branch. Note that observations on discrete types yield \na series of splits where some branches fail, and that obs . .1@int = fail. Measure transformer semantics \nWe can then give a composi\u00adtional semantics of a Fun program as a measure transformer, tak\u00ading a measure \nover assignments to its free variables and return\u00ading a joint measure over variable assignments and return \nvalue. To bind the values of a valuation to the corresponding variables we use pat(<) = () and pat(xs, \nx) := (pat(xs),x) as patterns and in return values. Below we de.ne the measure transformer semantics \nof a program as A [[M]]xs where xs are the free variables in the program. We use an auxiliary de.nition \nA [[M]]xs y for the semantics of the program M in the scope of y. For closed terms M we obtain M [[M]] \nby transforming the trivial probability measure. Each signature val f : t1 * \u00b7\u00b7\u00b7 * tn . tn+1 means that \nf is a total function with f : Vt1*\u00b7\u00b7\u00b7*tn . Vtn+1 . If Dist : t . PDist(u) and V . Vt , there is a corresponding \nmeasure \u00b5Dist(V ) on Vu. This measure is a probability measure for legal values of V , and otherwise, \nsuch as in Bernoulli(3.0), it is the zero measure. To apply ||| below we need to lift a sum to the top \nlevel of a type. We do this with lift1 : t *(u1 + u2) . (t *u1) + (t *u2) de.ned as . x, y.match y with \ninl z : inl(x,z) | inr z : inr(x,z). Measure Transformer Semantics of Fun: M [[M]], A [[M]]xs \u00b5 M [[M]] \nA A [[M]]< d() {()} \u00d7 A A [[M]]y >> pure . (pat(xs,y), z).(pat(xs),z) xs A [[M]]xs,y > A [[V ]]xs pure \n. pat(xs).(pat(xs),V ) A [[ f (V1, . . . , Vn)]]xs pure . pat(xs).(pat(xs), f (V )) A [[match V with \ninl x : M | inr y : N]]xs y xs A [[V ]]xs >>> pure lift1 >>> (A [[M]]x ||| A [[N]]xs ) A [[let x = M \nin N]]xs A [[M]]xs >>> A [[N]]x xs A [[random(Dist(V ))]]xs extend . pat(xs).\u00b5Dist(V ) A [[observe f \n(V )]]xs obs . pat(xs). f (V ) >>> A [[()]]xs A [[fail]]xs fail >>> A [[()]]xs Note that A [[observe \n1 + 1]]xs = A [[fail]]xs . Indeed, fail suf.ces to implement all observations on discrete types (by inlining \nobs, implementing split using if). Proposition 1 (Static Adequacy). If G f M : t then A [[M]]dom(G) . \nrange(G) (range(G) \u00d7 t). As seen above, (observe p) \u00b5 might be unde.ned, if its de.ning limit does not \nexist. We here give a suf.cient condition for the limit to exist. We inductively de.ne compatibility \nof a measure with respect to all the observations in a composition of measure transformer combinators \nas follows. We write \u00b5 |= T for \u00b5 is compatible with respect to T . Compatibility: \u00b5 |= T (T . t u and \n\u00b5 . M t) \u00b5 |= pure f \u00b5 |= extend m \u00b5 |= fail \u00b51 \u00b52 |= T1 ||| T2 iff \u00b51 |= T1 and \u00b52 |= T2 \u00b5 |= T >>> \nU iff \u00b5 |= T and (T \u00b5) |= U \u00b5 |= observe p iff p is an af.ne function, and there is e such that D \u00b5 is \ncontinuous on p-1(Be 0) and p is not constant on any open subset of p-1(Be 0). Proposition 2. If \u00b5 |= \nT , then T \u00b5 is de.ned. Compatibility is not necessary for T \u00b5 to be de.ned: Hybrid Measure: let hybrid1 \n= if random(Bernoulli(0.5)) then 1.0 else random(Gaussian(0.0,1.0)) let hybrid2 = (random(Gaussian(0.0,r)),hybrid1) \n The measure \u00b5 = M [[hybrid2]] is the average of a two-dimensional Gaussian distribution and a line mass \nat y = 1.0. Here \u00b5 |= observe . (x, y).y, but we do not have \u00b5 |= observe . (x,y).x because the line \nx = 0.0 crosses the line mass at y = 1.0, where the density fails to exist. However, by the de.nition \nof observe we get v (observe . (x,y).x) M [[hybrid2]] = r \u00b7f(0.0) \u00b7M [[(0.0,hybrid1)]] where f(0.0) \n0.3989 is the probability density of the standard normal distribution at 0.0. Note that as the precision \nr above grows, the weight of the resulting distribution grows, with no upper bound.  6.2 Monadic Semantics \nfor Fun with fail If M does not contain any occurrence of observe or fail then M is a term in the language \nof (Ramsey and Pfeffer 2002) which has a semantics using the probability monad (Giry 1982). To treat \nfail we work in the sub-probability monad (Panangaden 1999), where the set of admissible distributions \n\u00b5 also admits |\u00b5| < 1 (cf. the semantics of Barthe et al. (2012)). The valuation s maps the free variables \nof M to closed values. Monadic Semantics of Fun with fail: P[[M]] s (\u00b5 >>= f ) A f (x)(A) d\u00b5(x) Monadic \nbind (return v) A 1 if v . A, else 0 Monadic return zero A 0 Monadic zero P[[V ]] s return (V s ) P[[ \nf (V1, . . . ,Vn)]] s return f (V1s, . . . , Vns) P[[match V with inl x : M | inr y : N]] s P[[V ]] s \n>>= either (. v.P[[M]] (s,x . v)) (. v.P[[N]] (s,y . v)) P[[let x = M in N]] s P[[M]] s >>= . v.P[[N]] \n(s,x . v) P[[random(Dist(V ))]] s \u00b5Dist(V s) P[[fail]] s zero Proposition 3. If x1 : t1, . . . , xn \n: tn f M : t and xs = x1, . . . , xn and all observations in M are fail, then A [[M]]xs = extend . (v1, \n. . . , vn).P[[M]] (x1 . v1, . . . , xn . vn). In particular, if < f M : t we have M [[M]] = P[[M]] <. \n 6.3 A Normal Form for Fun Expressions Observation Translation: O[[M]] (o does not appear in N) O[[observe \nf (V )]] (), f (V ) O[[let x = M in N]] ' ' let x,o = O[[M]] in let y,o = O[[N]] in y,(o , o) O[[match \nV with inl x : M1 | inr y : M2]] match V with | inl x . let r,o = O[[M1]] in r,inl o | inr y . let r,o \n= O[[M2]] in r,inr o O[[M]] M, () otherwise Proposition 4. If G f M : t then there is u such that G \nf O[[M]] : t *u. Theorem 3. If G f M : t and N = let r,o = O[[M]] in observe o;r and \u00b5 |= A [[N]]dom(G) \nthen A [[M]]dom(G) \u00b5 = A [[N]]dom(G) \u00b5. Corollary (Normal form for closed terms). If < f M : t and N \nis as above and d() |= A [[N]]< then M [[M]] = M [[N]].  The program M = observe (fst (hybrid2)) does \nnot satisfy the compatibility precondition of the normal form theorem. This can be checked using static \nanalyses (Bhat et al. 2012a) that can de\u00adtermine if M [[O[[M]]]] is absolutely continuous and thus admits \na density. A more sensitive analysis could additionally determine the discontinuities of the density \nfunction in many cases, allowing to check compatibility statically. The results of this section imply \nthat observe-free models (that is, generative ones), and models with discrete observations only, have \nmeasure-transformer semantics that correspond to their monadic semantics, so we can use the latter for \nits simplicity. 7. Generic Probabilistic Conditionals Perhaps surprisingly, the lowly if expression gives \nrise to three use\u00adful and interesting ways of composing Bayesian models: mixture models, model averaging \nand a mixture of experts. We also show how to use the if expression as a general means for computing \nmodel evidence and evidence ratios. 7.1 Bayesian Mixture Models Given a number of models mk with k = \n1 . . . K with the same types of inputs xi and IID data yi, we can create a mixture of these models by \nintroducing an indicator variable zi (conditionally independent given w) for each sample yi that indicates \nwhich mixture compo\u00adnent mk it was generated by. This composition is helpful when the data can be generated \nby one of several known models. Below, we write wk and hk for the parameters and the hyperparameter associ\u00adated \nwith model k, and w for w1, . . . , wK . The sampling distribution is then given by: n K p({yi}n i=1|{xi}in \n=1,w,w) = . . p(zi = k|w)p(yi|xi, mk, wk) (6) i=1 k=1 We here give a generic combinator for creating \na mixture of two models, given a prior over the probability of choosing the .rst distribution. Mixture \nModel Combinator let M (m1:Model< TH1, TW1, TX, TY>, m2:Model< TH2, TW2, TX, TY>) : Model<(BetaW * TH1* \nTH2), (real * TW1* TW2), TX, TY> = {HyperParameter = (uniformBeta, m1.HyperParameter,m2.HyperParameter) \nPrior = <@ fun (bw,h1,h2) . (random(Beta(bw.trueCount,bw.falseCount)), (%m1.Prior) h1, (%m2.Prior) h2) \n@> Gen= <@ fun ((bias,w1,w2),x) . if breakSymmetry(random(Bernoulli(bias))) then (%m1.Gen) (w1,x) else \n(%m2.Gen) (w2,x) @>} (The call to breakSymmetry is a pragma for the Infer.NET back\u00adend it avoids the \npitfall of learning symmetric solutions.) We can create a mixture of two Gaussian models, where we have \nno prior knowledge of the probability of each model being used. Mixture of two Gaussian models let m \n= M(GaussianModel.M,GaussianModel.M) Such a model is useful for .tting a scalar property of two popula\u00adtions \nof unknown sizes. 7.2 Model Evidence by a Conditional An important concept in Bayesian machine learning \nis the notion of model evidence, which intuitively is the likelihood of the model given a particular \nset of observations. It is for instance used to choose between different models of the same data (model \nselec\u00adtion (MacKay 2003, ch.22)), and as an objective function to maxi\u00admize in certain inference techniques. \nIf M is a closed term we de.ne E (M) |M [[M]]|, that is, the total measure of the semantics M [[M]]. \nNote that E (M) may be different from 1 if M contains instances of observe or fail, as dis\u00adcussed in \nSection 6. More generally, if G f M : t we let E (M, \u00b5) |A [[M]]dom(G) \u00b5|. When choosing between two \n(or more) different models M and N (for the same data), we typically want to choose the model that has \nthe highest likelihood given the observations (of the data). The presence of arbitrary if-expressions \nin the language allows to compute the evidence of a model in a uniform way. More gen\u00aderally, the ratio \nof the evidence of two models (also known as the Bayes factor) can be computed as the ratio between the \nprobabili\u00adties of the two possible outcomes of a Boolean variable (compare Minka and Winn (2008)), by \ninferring the posterior probability of an a priori unbiased selector variable being true. Evidence ratio \nby if-expression: E [[M, N]] E [[M,N]] let x = random(Bernoulli(0.5)) in if x then M; () else N; () \nx Lemma 5. If G f M : t and G f N : u and \u00b5 . M (range(G)) then E (M, \u00b5) A [[E [[M,N]]]]dom(G) \u00b5 {(s,true) \n| s . Vrange(G)} = . E (N, \u00b5) A [[E [[M, N]]]]dom(G) \u00b5 {(s,false) | s . Vrange(G)} Proof: By expanding \nA [[E [[M,N]]]]dom(G). Given Lemma 5, we can compute the evidence of a model M from its evidence ratio \nto the trivial model (), which has evidence 1. Theorem 4. If < f M : t we have E (M) = 2\u00b7M [[E [[M, ()]]]] \n{true}. For open terms, if G f M : t and \u00b5 . M (range(G)) then E (M, \u00b5) = 2 \u00b7 (A [[E [[M,()]]]]dom(G) \n\u00b5 {(s,true) | s . Vrange(G)}). For this way of computing model evidence to work, it is critical that \nobserve denotes unnormalized conditioning. It does not gen\u00aderalize to languages such as Church (Goodman \net al. 2008), where observations (queries) correspond to a renormalized distribution: in such a language \nM [[E [[M,N]]]] {true} = 0.5 for all admissi\u00adble M, N. The technique also does not work in languages \nwhere the probability distribution of a program is the limit of its succes\u00adsive approximations, such \nas Probabilistic cc (Gupta et al. 1999). In such a language M [[E [[M,()]]]]{true} = 0.5 for all M. Frame\u00adworks \nbased on Gibbs sampling such as BUGS also fail, since if\u00adexpressions containing [random] variables require \na facility for computing the evidence of a submodel, which Gibbs sampling does not provide (Minka and \nWinn 2008).  7.3 Model averaging In Section 7.1 we were combining models mk by choosing a model for \neach data point. Another standard notion of composition is model averaging (Hoeting et al. 1999), where \nwe have some prior belief p(mk|h) about how likely each model mk is to have generated all of the data, \nthat is then updated based on the evidence of each model given the data, that we here assume to be IID. \nK n p({yi}n i=1|{xi}in =1,w,w) = . p(z = k|w) . p(yi|xi,mk,wk) (7) k=1 i=1  We show below a combinator \nfor the case K = 2. The parameter of the combined model includes a random bool switch, which is inspected \nto generate each output yi from the model mswitch. Combinator for Model Averaging: let M (m1:Model< TH1, \nTW1, TX, TY>, m2:Model< TH2, TW2, TX, TY>) : Model<(real* TH1* TH2), (bool* TW1* TW2), TX, TY> = {HyperParameter \n= (0.5, m1.HyperParameter,m2.HyperParameter) Prior = <@ fun (bias,h1,h2) . (random(Bernoulli(bias)), \n(%m1.Prior) h1, (%m2.Prior) h2) @> Gen = <@ fun ((switch,w1,w2),x) . if switch then (%m1.Gen) (w1,x) \nelse (%m2.Gen) (w2,x) @>} The posterior distribution over the bias gives us the evidence ratio between \nthe two models m1 and m2 (cf. Lemma 5), multiplied with the prior odds. p(z = 1|d, h) p(d|m1,h1) p(z \n= 1|h) = (8) p(z = 2|d, h) p(d|m2,h2) p(z = 2|h)  7.4 Mixture of Experts A powerful supervised counterpart \nto the unsupervised mixture models discussed in 7.1 is the Mixture of Experts model (Jacobs et al. 1991), \nand its hierarchical variants (Bishop and Svens\u00b4 en 2003; Jordan and Jacobs 1994). The idea is to use \na so-called gating model p(zi|xi,w) to decide for each input xi which model to use for generating the \ncorresponding output yi. We consider a mixture of K models with conditional sampling distributions p(yi|xi,mk,wk), \npriors p(wk|hk) and a gating model p(zi|xi,w) with prior p(w|h) resulting in the combined sampling distribution \nSome Examples of Hierarchical Mixtures of Experts let m = ExpertMixture.M(Probit,M2,M2) let n = IIDArray.M(ExpertMixture.M(Probit,m,m)) \n The model M2 .ts a line to the data. The model m above attempts to .t two different lines l1 and l2, \nand to .nd a point x0 where the data gradually shifts from being .tted to l1 (for x \u00ab x0) and being .tted \nto l2 (for x \u00bb x0). The model n attempts to .t four lines with three separating points, and operates \non arrays of IID data. 8. A Learner based on Monte Carlo Inference Markov chain Monte Carlo (MCMC) methods \nare an important class of inference algorithms for Bayesian models because they make it possible to obtain \nsamples from the posterior for a wide variety of models, even if that posterior is not in a simple fam\u00adily \nof parametric probability distributions or densities. The idea of MCMC is to construct a Markov chain \nin the parameter space of the model whose equilibrium distribution is the posterior distribu\u00adtion over \nmodel parameters. Neal (1993) provides an excellent re\u00adview of MCMC methods. As an example implementation, \nwe con\u00adsider Filzbach (Purves and Lyutsarev 2012), an implementation of an adaptive MCMC sampler based \non the Metropolis-Hastings al\u00adgorithm (Hastings 1970; Metropolis et al. 1953). All that is required to \napply an MCMC algorithm to a particular model is the ability to calculate posterior probabilities for \na given set of parameters. The algorithm then generates an ensemble of samples from the poste\u00adrior. This \nensemble serves as a representation of the posterior, or to calculate desired marginal distributions \nof individual parameters or other integrals under the posterior distribution. 8.1 Direct Calculation \nof Log-Posterior Bhat et al. (2012a) report an algorithm (but not an implementation), for computing the \ndensities of the distribution computed by a prob\u00adabilistic program, and a type system that guarantees \ntheir existence. Their algorithm uses integration at every let-expression; we here implement a simpli.ed \nversion without integrals but instead limited K to deterministic let-bound variables only. This pattern \nis common n . in the real-world models we have studied, for greater ef.ciency and . p({yi}n i=1|{xi}n \ni=1, w,w) = p(z = k|xi,w)p(yi|xi,mk,wk) simplicity. (In unpublished work, Bhat et al. (2012b) implement \na i=1 k=1 (9) As before, we here implement the binary case with two data models mk with k . {T, F}, with \nindependent priors given their parameters: p(w,wF , wT |h,hT , hF ) = p(w|h)p(wT |hT )p(wF |hF ). Combinator \nfor Mixture of Experts: let M(mc:Model< TH, TW, TX,bool>, m1:Model< TH1, TW1, TX, TY>, m2:Model< TH2, \nTW2, TX, TY>) : Model< TH* TH1* TH2, TW* TW1* TW2, TX, TY> = {HyperParameter = (mc.HyperParameter, m1.HyperParameter,m2.HyperParameter) \nPrior = <@ fun (hc,h1,h2) . (%mc.Prior) hc, (%m1.Prior) h1, (%m2.Prior) h2 @> Gen = <@ fun ((wc,w1,w2),x) \n. if (%mc.Gen) (wc,x) then (%m1.Gen) (w1,x) else (%m2.Gen) (w2,x) @>} The hierarchical Mixture of Experts \nmodel (Bishop and Svens\u00b4 en 2003; Jordan and Jacobs 1994) can easily be obtained by a tree of calls to \nthe above combinator. It is also straightforward to build an n-ary version of this combinator from the \nabove construction, or a variation that operates on arrays of models with identical type parameters. \nmore general algorithm for calculating densities of Fun programs.) We describe a recursive function PR \nE, which is an effective partial algorithm for calculating the log-density for the parameters of a generative \nmodel written in Fun. (It is standard to compute logs to avoid under.ow when dealing with very small \nprobabilities.) The PR E function is a backward analysis that computes the density function, and is named \nby analogy with the standard pre-condition computation in program analysis. Log-density calculation PRE(V,s \n, M) . R. {-8,.} PR E(V,s,U) = log(1.0) if Us = V else log(0.0) PR E(V,s, f (V1, . . . ,Vn)) = log(1.0) \nif f (V1s, . . . ,Vns ) = V log(0.0) otherwise PR E(V,s,random(Dist(U))) = log(D (\u00b5Dist(Us)(V ))) PR \nE(V,s,match U with inl x : M | inr y : N) = ' PRE(V,(s , x . U' ),M) if Us = inl U ' PRE(V,(s , y . U' \n),N) if Us = inr U PR E(V,s,let x = M in N) = PRE(V,(s,x . U),N) if M is deterministic and Ms evaluates \nto U . otherwise PR E(V,s,fail) = log(0.0) PR E(V,s,observe f (V )) = .  When de.ned (that is, not \n.), the PR E(V,s,M) function computes the log-posterior density of the parameters (in s) given the data \nV . Theorem 5. If G f M : t and s is a G-valuation and V . Vt and PR E(V,s,M) = . then PRE(V,s,M) = log((D \n(P[[M]] s ))(V )). The log-posterior of the parameters Vw is then obtained by adding the log-density \nof the prior at Vw, which is obtained by calling PR E(Vw,h . Vh,Prior), where Vh is the hyperparameter, \nto the log\u00adposterior density PR E(Vy,w . Vw,M). Here is the actual log-posterior function we pass to \nFilzbach from our MCMC learner (space precludes more detail): let (Lambda(vh,p)) = m.Prior let (Lambda(vw,e)) \n= <@ fun w .[|for x in xs .(%m.Gen) (w,x)|] @> let likelihood : TW .real = (* fun w .PRE(ys,[vw,w],e) \n+ PRE(w,[vh,h],p) *) Compile(fun w .<@ (%PRE(<@ ys @>,[vw,w],e)) + (%PRE(w,[vh,<@ h @>],p)) @>) Here, \n(xs,ys) is the observed data (from training) and m is the orig\u00adinal model. Though not shown here, the \nMCMC learner represents distributions simply as an ensemble (an array) of samples.  8.2 Example: Purves \nPlantGrowth As a typical example, we present a simpli.ed model of plant growth, adapted from a Filzbach \nexample. Given an array of (de\u00adterministic) temperature data, temps, the model generates the mass of \na plant on harvest day x, assuming that 0 = x < |temps|. The parameter of the model is a record of numbers: \nthe plant s initial mass (imass), daily growth rate (alpha), optimum temperature for growth (topt), temperature \nsensitivity (trho) and the deviation of some noise (sigma). Each .eld is given a Gaussian prior. The \nharvest mass is calculated by iterating a daily growth func\u00adtion (the argument of Array.Fold below) on \nthe initial mass. This de\u00adterministic computation is .nally perturbed by some noise, drawn from a Gaussian \nof unknown precision (prec obtained from sigma). PlantGrowth Model (Prior omitted due to lack of space) \n: <@ fun (w,x) .// .rst grow plant deterministically... let prec = 1.0/(w.sigma * w.sigma) let mass = \n// compute the mass after x days Array.fold // compute next mass in a time series (fun mass i airtemp \ni . let r = (airtemp i-w.topt)/w.trho mass i + (w.alpha * mass i) * exp (-1.0*(r*r))) w.imass (Array.sub \ntemps 0 x) random(Gaussian(mass,prec)) @> // ... .nally add noise To perform inference on this model, \na Filzbach user must also pro\u00advide a function computing the log-posterior of the model s param\u00adeters \ngiven some observed data (here, pairs of harvest days and masses). It is this function that Filzbach \ns MCMC algorithm at\u00adtempts to maximize over the parameter space. Moreover, the user typically also needs \nto write an explicit generative model for pro\u00adducing predictions, based either on the mean of the obtained \nposte\u00adrior parameter distribution or, better yet, by error propagation using an ensemble of parameters \ndrawn from this posterior. Instead, we use the PR E algorithm described above to automatically compute \nthe log-posterior function from the generative model. (Adaptation of our learner to related methods (such \nas the com\u00adputation of either the maximum likelihood (MLE) or the maximum a posteriori probability (MAP) \nestimate) appears straightforward but is beyond the scope of the paper.) 9. Related Work Formal Semantics \nof Probabilistic Languages There is a long history of formal semantics for probabilistic languages with \nsam\u00adpling primitives, often with recursive computation. One of the .rst semantics is for Probabilistic \nLCF (Saheb-Djahromi 1978), which augments the core functional language LCF with weighted binary choice, \nfor discrete distributions. Kozen (1981) develops a proba\u00adbilistic semantics for while-programs with \nrandom assignment. He develops two equivalent semantics; one more operational, and the other denotational. \nMcIver and Morgan (2005) develop a theory of abstraction and re.nement for probabilistic while programs \nwith real-valued variables, based on weakest preconditions. Jones and Plotkin (1989) investigate the \nprobability monad, and apply it to languages with discrete probabilistic choice. Ramsey and Pfeffer (2002) \ngive a stochastic . -calculus with a measure\u00adtheoretic semantics in the probability monad, and provide \nan em\u00adbedding within Haskell; they also do not consider observations. Several languages (such as PFP \n(Erwig and Kollmansberger 2006)), IBAL (Pfeffer 2007), HANSEI (Kiselyov and Shan 2009), CertiPriv (Barthe \net al. 2012) and the informal sampling semantics of Csoft (Minka et al. 2009)) have Boolean assertions \nor fail state\u00adments that give rise to sub-probabilities. Such languages support the mixture operations \nand evidence calculations of Section 7, for discrete observations only. The concurrent constraint programming \nlanguage PC C of Gupta et al. (1999) allows describing continuous probability distributions using independent \nsampling and constraints. However, their seman\u00adtics of constraints is different than that of Fun observations. \nThis makes PC C less suitable for compositional Bayesian modelling, since computation of model evidence \nusing an if statement (Theo\u00adrem 4) does not work: the program choose X from {0,1} in if X = 1 then [b,P] \nyields probability at most 0.5 of b being true. In contrast, the measure-transformer semantics of Fun \nallows computing E (M) > 1 for well-.tted models with continuous ob\u00adservations using the one-sided if \nstatement of Theorem 4, as do the direct factor graph semantics of Csoft (Minka et al. 2009) using gates \n(Minka and Winn 2008). Probabilistic Languages for Machine Learning Previous pro\u00adgramming models for \nmachine learning mainly represent particular implementation strategies, rather than general Bayesian \nmodels. An exception is Church (Goodman et al. 2008), which repre\u00adsents probability distributions over \nLisp terms as Lisp programs, and uses various inference techniques. However, since every ad\u00admissible \nterm denotes a probability distribution, E [[M,()]] always yields true with probability 0.5. Early work \nincludes that of Koller et al. (1997) where a prob\u00adabilistic model is represented as a functional program \nwith prob\u00adabilistic choice. FACTORIE (McCallum et al. 2009) is a library for imperatively constructing \nfactor graphs. BUGS (Gilks et al. 1994) uses Gibbs sampling for Bayesian inference on Bayesian net\u00adworks, \nrepresented as imperative probabilistic programs. IBAL (Pf\u00adeffer 2007) integrates Bayesian parameter \nestimation and decision\u00adtheoretic utility maximisation, but only works with discrete data\u00adtypes. Alchemy \n(Domingos et al. 2008) represents Markov logic networks, which are inherently discrete. Park et al. (2005) \nalso de\u00ad.ne a probabilistic functional language, where as in Church terms always denote probability distributions. \nPMML (Guazzelli et al. 2009) is an XML format for represent\u00ading trained instances of a range of models, \nincluding association rules, neural networks, decision trees, support vector machines, etc. PMML does \nnot represent arbitrary graphical or generative mod\u00adels. In our terms, a PMML model corresponds roughly \nto a trained learner. We have not developed any format for persisting trained learners; a format based \non PMML may be suitable.  Example / Learner TH TW TDistW TX TY TDistY Sprinkler / A SP.TH SP.TW<bool> \nADD<SP.TW<bool>> SP.TX bool ADD<bool> TwoCoins / A TC.TH TC.TW<bool> ADD<TC.TW<bool>> TC.TX bool ADD<bool> \nTwo Coins / IN TC.TH TC.TW<bool> TC.TW<Bernoulli> TC.TX bool Bernoulli Friends / A bool[][] bool list \nlist ADD<bool list list> int * int * int bool ADD<bool> Students / A int * int bool list list ADD<bool \nlist list> int * int * int bool ADD<bool> Gaussian / IN GM.TH GM.TW<R,R> GM.TW<N ,G> unit real N Gaussian \nMix/ IN MX1.TH R* GaussW*GaussW \u00df* GM.TW<N ,G>*GM.TW<N ,G> unit real N Gaussian Mix / F MX2.TH (GaussW*GaussW) \n(GaussW*GaussW)[] unit real R[] PlantGrowth / F unit PG.TW PG.TW[] int real R[] TrueSkill / IN TS.TH \nTS.TW<R> TS.TW<N > TrueSkill.TX bool Bernoulli Lin. Reg. / IN LR.TH LR.TW<R,R,R> LR.TW<N ,N ,G> real \nreal N MV Gaussian / IN MVG.TH MVG.TW< .R,M > MVG.TW< .N ,W > unit .R .N R= real N = Gaussian \u00df= Beta \nG= Gamma . R= Vector M = PositiveDe.niteMatrix W = Wishart // generalizes G to multiple dimensions . \nN = VectorGaussian // multivariate Gaussian distribution GaussW= {Mean:R; Precision:R} BetaW = {trueCount: \nR; falseCount: R} SP.TH = {RainH: R; SprinklerH: R} SP.TW< TB> = {Rain: TB; Sprinkler: TB} SP.TX = IsGrassWet \n// a unit type TC.TH = {Bias1: R; Bias2: R} TC.TW< TB> = {Heads1: TB; Heads2: TB} TC.TX = AreEitherHeads \n// a unit type GM.TW< TM, TP> = {Mean: TM; Precision: TP} GM.TH ={ Gaussian: GaussW, Gamma: GammaW } \n MX1.TH = BetaW * GM.TH * GM.TH MX2.TH = GM.TH * GM.TH PG.TW = {alpha:R;topt:R;trho:R;imass:R;sigma:R} \nTS.TH = { Players: int; G: GaussW; P: GammaW } TS.TW< TA> = { Skills: TA[]} TS.TX = { P1:int; P2: int \n} LR.TH = {MeanA: R; PrecA: R; MeanB: R; PrecB: R; Shape: R; Scale: R}LR.TW< TA, TB, TN> = {A: TA; B: \nTB; Prec: TN}MVG.TH = {NCols:int; MeanVectorPrecisionCount:R; WishartShapeConstant:R; WishartScaleConstant:R}MVG.TW< \nTM, TC> = {Mean: TM; Covariance: TC} Table 1. Rows show types for L : ILearner(TDistW,TX,TY,TDistY) \nfor m : Model<TH,TW,TX,TY> (A=ADD, IN=Infer.NET, F=Filzbach) Inference using ADDs The idea of using ADDs \nfor probabilis\u00adtic inference has been explored before. Sanner and McAllester (2005) de.ne Af.ne Algebraic \nDecision Diagrams to perform in\u00adference over Bayesian networks and Markov Decision Processes. Kwiatkowska \net al. (2006) have used a variants of ADDs to perform probabilistic model checking in the PRISM project. \nBozga and Maler (1999) have used ADDs to symbolically simulate Markov chains. Chavira and Darwiche (2007) \nuse ADDs to compactly rep\u00adresent factors in a Bayesian network and thereby perform ef.cient inference \nvia variable elimination. In contrast, the ADD backend described in this paper avoids factor graphs altogether \nand uses ADDs to represent symbolic program states (which are distribu\u00adtions) at every program point, \nmuch like a data-.ow analysis or an abstract interpreter (Cousot and Cousot 1977). Mardziel et al. (2011) \ndevelop an approximate representation for discrete distributions based on abstract interpretations, with \napplication to knowledge-based security policies. 10. Conclusions We proposed typed programming abstractions, \nmodels and learn\u00aders, for packaging, composing, and training and predicting with Bayesian models. We \nare aware of no prior generic abstractions for Bayesian models. Future work includes adding functions \nover recursive types, and general recursion to the Fun language. We will also investigate the possibility \nof relaxing the condition of compatibility of a measure with respect to a model. Acknowledgements Conversations \nabout this work with Chris Bishop, John Bronskill, Tom Minka, Drew Purves, Robert Sim\u00admons, Matthew Smith, \nand John Winn were invaluable. Sooraj Bhat, Jael Kriener, and Sameer Singh commented on a draft. References \nG. Barthe, B. K \u00a8opf, F. Olmedo, and S. Zanella B \u00b4eguelin. Probabilistic relational reasoning for differential \nprivacy. In J. Field and M. Hicks, editors, POPL, pages 97 110. ACM, 2012. S. Bhat, A. Agarwal, R. W. \nVuduc, and A. G. Gray. A type theory for probability density functions. In J. Field and M. Hicks, editors, \nPOPL, pages 545 556. ACM, 2012a. S. Bhat, J. Borgstr \u00a8om, A. D. Gordon, and C. Russo. Deriving probability \ndensity functions from probabilistic functional programs. Draft paper, 2012b. C. M. Bishop and M. Svens\u00b4en. \nBayesian hierarchical mixtures of experts. In C. Meek and U. Kj\u00a8arulff, editors, Uncertainty in Arti.cial \nIntelligence (UAI 03), pages 57 64. Morgan Kaufmann, 2003. D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent \nDirichlet allocation. Journal of Machine Learning Research, 3:993 1022, 2003. J. Borgstr \u00a8om, A. D. Gordon, \nM. Greenberg, J. Margetson, and J. Van Gael. Measure transformer semantics for Bayesian machine learning. \nIn European Symposium on Programming (ESOP 11), volume 6602 of LNCS, pages 77 96. Springer, 2011. Download \navailable at http: //research.microsoft.com/fun. M. Bozga and O. Maler. On the representation of probabilities \nover struc\u00adtured domains. In Computer Aided Veri.cation (CAV 99), pages 261 273, 1999. M. Chavira and \nA. Darwiche. Compiling Bayesian networks using variable elimination. In International Joint Conference \non on Arti.cial Intelli\u00adgence (IJCAI 07), pages 2443 2449, 2007. G. Claret, S. K. Rajamani, A. V. Nori, \nA. D. Gordon, and J. Borgstr \u00a8om. Bayesian inference for probabilistic programs via symbolic execution. \nTechnical Report MSR TR 2012 86, Microsoft Research, 2012. P. Cousot and R. Cousot. Abstract interpretation: \na uni.ed lattice model for the static analysis of programs by construction or approximation of .xpoints. \nIn POPL, pages 238 252, 1977. A. Darwiche. Modeling and Reasoning with Bayesian Networks. CUP, 2009. \n H. Daum \u00b4e III. HBC: Hierarchical Bayes Compiler, 2008. Available at http://www.cs.utah.edu/~hal/HBC/. \nP. Domingos, S. Kok, D. Lowd, H. Poon, M. Richardson, and P. Singla. Markov logic. In L. De Raedt, P. \nFrasconi, K. Kersting, and S. Muggle\u00adton, editors, Probabilistic inductive logic programming, pages 92 \n117. Springer-Verlag, Berlin, Heidelberg, 2008. M. Erwig and S. Kollmansberger. Functional pearls: Probabilistic \nfunc\u00adtional programming in Haskell. J. Funct. Program., 16(1):21 34, 2006. W. R. Gilks, A. Thomas, and \nD. J. Spiegelhalter. A language and program for complex Bayesian modelling. The Statistician, 43:169 \n178, 1994. M. Giry. A categorical approach to probability theory. In B. Banaschewski, editor, Categorical \nAspects of Topology and Analysis, volume 915 of Lecture Notes in Mathematics, pages 68 85. Springer Berlin \n/ Heidel\u00adberg, 1982. N. Goodman, V. K. Mansinghka, D. M. Roy, K. Bonawitz, and J. B. Tenenbaum. Church: \na language for generative models. In Uncertainty in Arti.cial Intelligence (UAI 08), pages 220 229. AUAI \nPress, 2008. A. D. Gordon, M. Aizatulin, J. Borgstr \u00a8om, G. Claret, T. Graepel, A. Nori, S. Rajamani, \nand C. Russo. A model-learner pattern for Bayesian reasoning. Technical Report MSR-TR-2013-1, Microsoft \nResearch, 2013. A. Guazzelli, M. Zeller, W. Chen, and G. Williams. PMML: An open standard for sharing \nmodels. The R Journal, 1(1), May 2009. V. Gupta, R. Jagadeesan, and P. Panangaden. Stochastic processes \nas concurrent constraint programs. In POPL, pages 189 202, 1999. W. K. Hastings. Monte Carlo sampling \nmethods using Markov chains and their applications. Biometrika, 57(1):97 109, 1970. R. Herbrich, T. Minka, \nand T. Graepel. Trueskilltm: A Bayesian skill rating system. In Advances in Neural Information Processing \nSystems (NIPS 06), pages 569 576, 2006. J. A. Hoeting, D. Madigan, A. E. Raftery, and C. T. Volinsky. \nBayesian model averaging: A tutorial. Statistical Science, 14(4):382 401, 1999. R. A. Jacobs, M. I. Jordan, \nS. J. Nowlan, and G. E. Hinton. Adaptive mixtures of local experts. Neural Computation, 3:79 87, 1991. \nC. Jones and G. D. Plotkin. A probabilistic powerdomain of evaluations. In Logic in Computer Science \n(LICS 89), pages 186 195. IEEE Computer Society, 1989. M. I. Jordan and R. A. Jacobs. Hierarchical mixtures \nof experts and the EM algorithm. Neural Computation, 6(2):181 214, 1994. O. Kiselyov and C. Shan. Monolingual \nprobabilistic programming us\u00ading generalized coroutines. In Uncertainty in Arti.cial Intelligence (UAI \n09), 2009. D. Koller, D. A. McAllester, and A. Pfeffer. Effective Bayesian inference for stochastic programs. \nIn AAAI/IAAI, pages 740 747, 1997. D. Kozen. Semantics of probabilistic programs. Journal of Computer \nand System Sciences, 22(3):328 350, 1981. M. Z. Kwiatkowska, G. Norman, and D. Parker. Quantitative analysis \nwith the probabilistic model checker PRISM. In Quantitative Aspects of Programming Languages (QAPL 2005), \nvolume 153(2) of ENTCS, pages 5 31, 2006. D. J. C. MacKay. Information Theory, Inference, and Learning \nAlgorithms. CUP, 2003. P. Mardziel, S. Magill, M. Hicks, and M. Srivatsa. Dynamic enforcement of knowledge-based \nsecurity policies. In Computer Security Foundations Symposium (CSF 11), pages 114 128, 2011. A. McCallum, \nK. Schultz, and S. Singh. Factorie: Probabilistic program\u00adming via imperatively de.ned factor graphs. \nIn Advances in Neural In\u00adformation Processing Systems (NIPS 09), pages 1249 1257, 2009. A. McIver and \nC. Morgan. Abstraction, re.nement and proof for proba\u00adbilistic systems. Monographs in computer science. \nSpringer, 2005. N. Metropolis, A. W. Rosenbluth, M. N. Rosenbluth, A. H. Teller, and E. Teller. Equation \nof state calculations by fast computing machines. Journal of Chemical Physics, 21:1087 1092, 1953. T. \nMinka. A family of algorithms for approximate Bayesian inference. PhD thesis, MIT, 2001. T. Minka and \nJ. M. Winn. Gates. In Advances in Neural Information Processing Systems (NIPS 08), pages 1073 1080. MIT \nPress, 2008. T. Minka, J. Winn, J. Guiver, and A. Kannan. Infer.NET 2.3, Nov. 2009. Software available \nfrom http://research.microsoft.com/ infernet. R. M. Neal. Probabilistic inference using Markov chain \nMonte Carlo methods. Technical Report CRG-TR-93-1, Dept. of Computer Science, University of Toronto, \nSeptember 1993. S. Park, F. Pfenning, and S. Thrun. A probabilistic language based upon sampling functions. \nIn POPL, pages 171 182. ACM, 2005. J. Pearl and G. Shafer. Probabilistic reasoning in intelligent systems: \nNetworks of plausible inference. Synthese-Dordrecht, 104(1):161, 1995. P. Panangaden. The category of \nMarkov kernels. Electronic Notes in Theoretical Computer Science, 22:171 187, 1999. A. Pfeffer. IBAL: \nA probabilistic rational programming language. In B. Nebel, editor, International Joint Conference on \nArti.cial Intelligence (IJCAI 01), pages 733 740. Morgan Kaufmann, 2001. A. Pfeffer. The design and implementation \nof IBAL: A general-purpose probabilistic language. In L. Getoor and B. Taskar, editors, Introduction \nto Statistical Relational Learning. MIT Press, 2007. A. Pfeffer. Practical probabilistic programming. \nIn P. Frasconi and F. A. Lisi, editors, Inductive Logic Programming (ILP 2010), volume 6489 of Lecture \nNotes in Computer Science, pages 2 3. Springer, 2010. D. Purves and V. Lyutsarev. Filzbach User Guide, \n2012. Available at http://research.microsoft.com/en-us/um/cambridge/ groups/science/tools/filzbach/filzbach.htm. \nA. Radul. Report on the probabilistic language scheme. In Proceedings of the 2007 symposium on Dynamic \nlanguages (DLS 07), pages 2 10. ACM, 2007. N. Ramsey and A. Pfeffer. Stochastic lambda calculus and monads \nof probability distributions. In POPL, pages 154 165, 2002. N. Saheb-Djahromi. Probabilistic LCF. In \nMathematical Foundations of Computer Science (MFCS), volume 64 of LNCS, pages 442 451. Springer, 1978. \nS. Sanner and D. A. McAllester. Af.ne Algebraic Decision Diagrams (AADDs) and their application to structured \nprobabilistic inference. In International Joint Conference on on Arti.cial Intelligence (IJCAI 05), pages \n1384 1390, 2005. J. Schumann, T. Pressburger, E. Denney, W. Buntine, and B. Fischer. Au\u00adtoBayes program \nsynthesis system users manual. Technical Report NASA/TM 2008 215366, NASA Ames Research Center, 2008. \nF. Somenzi. CUDD: CU decision diagram package, release 2.5.0, 2012. Software available from http://vlsi.colorado.edu. \nD. Syme. Leveraging .NET meta-programming components from F#: integrated queries and interoperable heterogeneous \nexecution. In A. Kennedy and F. Pottier, editors, ML Workshop, pages 43 54. ACM, 2006. J. Winn and T. \nMinka. Probabilistic programming with Infer.NET. Ma\u00adchine Learning Summer School lecture notes, available \nat http:// research.microsoft.com/~minka/papers/mlss2009/, 2009.     \n\t\t\t", "proc_id": "2429069", "abstract": "<p>A Bayesian model is based on a pair of probability distributions, known as the prior and sampling distributions. A wide range of fundamental machine learning tasks, including regression, classification, clustering, and many others, can all be seen as Bayesian models. We propose a new probabilistic programming abstraction, a typed Bayesian model, which is based on a pair of probabilistic expressions for the prior and sampling distributions. A sampler for a model is an algorithm to compute synthetic data from its sampling distribution, while a learner for a model is an algorithm for probabilistic inference on the model. Models, samplers, and learners form a generic programming pattern for model-based inference. They support the uniform expression of common tasks including model testing, and generic compositions such as mixture models, evidence-based model averaging, and mixtures of experts. A formal semantics supports reasoning about model equivalence and implementation correctness. By developing a series of examples and three learner implementations based on exact inference, factor graphs, and Markov chain Monte Carlo, we demonstrate the broad applicability of this new programming pattern.</p>", "authors": [{"name": "Andrew D. Gordon", "author_profile_id": "81100037731", "affiliation": "Microsoft Research, Cambridge, United Kingdom", "person_id": "P3978009", "email_address": "adg@microsoft.com", "orcid_id": ""}, {"name": "Mihhail Aizatulin", "author_profile_id": "81464671169", "affiliation": "Open University, Milton Keynes, United Kingdom", "person_id": "P3978010", "email_address": "avatar@hot.ee", "orcid_id": ""}, {"name": "Johannes Borgstrom", "author_profile_id": "81100636571", "affiliation": "Uppsala University, Uppsala, Sweden", "person_id": "P3978011", "email_address": "Johannes.borgstrom@it.uu.se", "orcid_id": ""}, {"name": "Guillaume Claret", "author_profile_id": "81501685513", "affiliation": "Microsoft Research, Bangalore, India", "person_id": "P3978012", "email_address": "guillaume@claret.me", "orcid_id": ""}, {"name": "Thore Graepel", "author_profile_id": "81100450764", "affiliation": "Microsoft Research, Cambridge, United Kingdom", "person_id": "P3978013", "email_address": "thore.graepel@microsoft.com", "orcid_id": ""}, {"name": "Aditya V. Nori", "author_profile_id": "81320493380", "affiliation": "Microsoft Research, Bangalore, India", "person_id": "P3978014", "email_address": "adityan@microsoft.com", "orcid_id": ""}, {"name": "Sriram K. Rajamani", "author_profile_id": "81100468626", "affiliation": "Microsoft Research, Bangalore, India", "person_id": "P3978015", "email_address": "sriram@microsoft.com", "orcid_id": ""}, {"name": "Claudio Russo", "author_profile_id": "81100638789", "affiliation": "Microsoft Research, Cambridge, United Kingdom", "person_id": "P3978016", "email_address": "crusso@microsoft.com", "orcid_id": ""}], "doi_number": "10.1145/2429069.2429119", "year": "2013", "article_id": "2429119", "conference": "POPL", "title": "A model-learner pattern for bayesian reasoning", "url": "http://dl.acm.org/citation.cfm?id=2429119"}