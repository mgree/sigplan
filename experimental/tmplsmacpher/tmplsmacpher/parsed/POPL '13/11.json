{"article_publication_date": "01-23-2013", "fulltext": "\n Inductive Data Flow Graphs Azadeh Farzan Zachary Kincaid University of Toronto Abstract The correctness \nof a sequential program can be shown by the anno\u00adtation of its control .ow graph with inductive assertions. \nWe pro\u00adpose inductive data .ow graphs, data .ow graphs with incorpo\u00adrated inductive assertions, as the \nbasis of an approach to verifying concurrent programs. An inductive data .ow graph accounts for a set \nof dependencies between program actions in interleaved thread executions, and therefore stands as a representation \nfor the set of concurrent program traces which give rise to these dependencies. The approach .rst constructs \nan inductive data .ow graph and then checks whether all program traces are represented. The size of the \ninductive data .ow graph is polynomial in the number of data de\u00adpendencies (in a sense that can be made \nformal); it does not grow exponentially in the number of threads unless the data dependen\u00adcies do. The \napproach shifts the burden of the exponential explosion towards the check whether all program traces \nare represented, i.e., to a combinatorial problem (over .nite graphs). Categories and Subject Descriptors \nD.2.4 [Software/Program Veri.cation]: Correctness Proofs; F.3.1 [Logics and Meanings of Programs]: Specifying, \nVerifying and Reasoning about Programs General Terms Languages, Veri.cation Keywords Concurrency, Veri.cation, \nStatic Analysis 1. Introduction The success of the two main approaches to algorithmic veri.cation is \nwell established for their intended target domains: static analysis for the veri.cation of sequential \nprograms [9],  model checking for the veri.cation of .nite-state concurrent protocols [8].  This paper \naddresses the algorithmic veri.cation of concurrent programs. Considerable progress has been made with \napproaches that, depending on terminology, extend static analysis to concurrent control or extend model \nchecking to general data domains [3, 11 13, 17, 22, 27]. Each of these approaches provides a different \nangle of attack to circumvent the same fundamental issue: the space required to prove the correctness \nof a concurrent program grows exponentially in the number of its threads. We propose inductive data .ow \ngraphs (iDFGs), data .ow graphs with incorporated inductive assertions, as the basis of an Permission \nto make digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior speci.c permission and/or a fee. POPL 13, January \n23 25, 2013, Rome, Italy. Copyright c &#38;#169; 2013 ACM 978-1-4503-1832-7/13/01. . . $10.00 Andreas \nPodelski University of Freiburg approach to verifying concurrent programs. An iDFG accounts for a set \nof dependencies between data operations in interleaved thread executions, and therefore stands as a representation \nfor the set of concurrent program traces which give rise to these dependencies. The approach .rst constructs \nan iDFG and then checks whether all program traces are represented. The size of an iDFG is polynomial \nin the number of data dependencies (in a sense that can be made for\u00admal); it does not grow exponentially \nin the number of threads unless the data dependencies do. Intuitively, this succinctness is possible \nbecause iDFGs represent only the data .ow of the program, and abstract away control features that are \nirrelevant to the proof. The approach shifts the burden of the exponential explosion towards the check \nwhether all program traces are represented, which is a com\u00adbinatorial problem (over .nite graphs). There \nare several directions in which one can explore the practi\u00adcal potential of the approach. This is not \nthe focus of this paper. The focus in this paper is to introduce the approach and to investigate its \nformal foundation. We will next explain in what sense our approach relies on the respective strength \nof both static analysis and model checking. In static analysis, techniques have been developed that are \nsuc\u00adcessful in .nding solutions to .xpoint equations, namely by solv\u00ading them over abstract domains. \nIn the settings where the ele\u00adments of the abstract domain denote state predicates, static anal\u00adysis \namounts to establishing a Floyd-Hoare annotation of the con\u00adtrol .ow graph of the program. The validity \nof the .xpoint equa\u00adtion translates to the inductiveness of the annotation, i.e., the valid\u00adity of the \nHoare triple at each node of the control .ow graph. To establish the validity (of the solution for the \n.xpoint equation/of the Hoare triples), the Floyd-Hoare annotation of the control .ow graph has to be \nstored. This becomes an obstacle for the naive ap\u00adproach to apply static analysis to a concurrent program. \nThe naive approach is to .atten the concurrent program, i.e., transform it to a non-deterministic sequential \nprogram whose executions include all interleaved executions of the concurrent program. The size of the \ncontrol .ow graph of the .attened program grows exponentially with the number of threads. One motivation \nbehind the work in this paper is the question whether static analysis with the techniques that are well \nestab\u00adlished for sequential programs [4] can be put into work for con\u00ad current programs. Our approach \ndoes not aim at improving those techniques (for constructing abstract domains, widening, strength\u00adening \nloop invariants, etc.) but, instead, relies on them. The ques\u00adtion is whether the output of the static \nanalysis applied to inter\u00adleaved executions of the threads of a concurrent program can be assembled (and \nused) in a space-ef.cient way. The answer we pro\u00adpose lies in iDFGs. In model checking, techniques have \nbeen developed that are successful for the exhaustive exploration of the .nite, though expo\u00adnentially \nlarge state space of concurrent protocols. Model checking is a combinatorial problem that is easy in \nthe sense that it can be reduced to search, and therefore can be solved in PS PAC E. Our approach relies \non the ef.ciency of space exploration techniques as used in model checking for checking whether all program \ntraces are represented by a given iDFG. The check solves a problem which is directly related to model \nchecking, and inherits its theoretical com\u00adplexity bound; i.e., the check can be implemented in polynomial \nspace (polynomial in the number of threads of the given concurrent program).  We will next explain how \nthe concept of data dependencies in connection with Hoare triples leads to iDFGs. Assume trace t is composed \nof three actions a1: x++, a2: y++, and a3: z:=x+y (possibly from different threads of a concurrent program). \nThe two edges in the graph to the right express the data dependencies in t . Conceptually, the graph \nrepresents the set of all traces that preserve the data depen\u00addencies expressed by its edges. Such traces \ncontain the three ac\u00adtions in an order that complies with the two edges (i.e., a1 and a2 must occur before \na3); they can contain additional actions as long as these actions do not modify the value of x, y, or \nz. All such traces have the same behavior, which formally means that they satisfy the same set of pre/postcondition \npairs. Now, in the set\u00adting of veri.cation where correctness is de.ned by one particular pre/postcondition \npair, we would like to have a more targeted sense of data dependencies (and thus a means to de.ne a more \ntargeted set of traces). As an example, we take the pre/postcondition pair that de.nes the correctness \nof the trace t in the Hoare triple below: {x = 0 . y = 0} x++ ; y++ ; z:=x+y {z > 0}. The essence of \na possible correctness proof for t consists of the three Hoare triples below. {x > 0 . y > 0} z:=x+y \n{z > 0}{y = 0} y++ {y > 0} (1) {x = 0} x++ {x > 0} In addition to the three essential Hoare triples, \nthe correctness proof contains the two stability Hoare triples below, which state the invariance ( stability \n) of an assertion under an action. {y = 0} x++ {y = 0}{x > 0} y++ {x > 0} We obtain the two Hoare triples \nbelow by taking the sequential composition of Hoare triples that appear above. {x = 0} x++ ; y++ {x > \n0}{y = 0} x++ ; y++ {y > 0} To obtain a correctness proof for t, we apply the conjunction rule to these \ntwo Hoare triples and then sequentially compose with the Hoare triple for z:=x+y in (1). The iDFG to \nthe right is constructed from the correctness proof for the trace t . The annotation of edges with assertions \ncorresponds to the three Hoare triples in (1) which form the essence of the cor\u00ad rectness proof. We consider \nthe iDFG to be a repre\u00adsentation the set of all traces such that the three Hoare triples in (1) form \nthe essence of a possible correctness proof (all other axioms used in the cor\u00adrectness proof are stability \nHoare triples). By de.nition, all traces in this set are correct. The set contains the trace t . We next \ngive more examples of traces in the set. If we permute the actions a1 and a2 in the trace t , the resulting \ntrace t1 lies in the set: the essence of the correctness proof for t1 is the three Hoare triples in (1); \nthe other Hoare triples in the correctness proof for t 1 state the stability of x = 0 under the action \ny++ and the stability of y > 0 under the action x++. If we add actions to the trace t (or to the trace \nt 1) and the assertion at each position with an added action is stable under the added action, then the \nnew trace lies in the set: the essence of its correctness proof are the Hoare triples in (1); the new \nHoare triples in the correctness proof are stability Hoare triples. The edges in an iDFG explicitly formulate \nordering constraints on the actions in a trace (as in the example above). We will next illustrate that \nthe assertion labels of edges implicitly de.ne addi\u00adtional ordering constraints. The iDFG shown to the \nright can be constructed from the correctness proof for the trace composed of the actions b1: x:=y+1, \nb2: y:=x+1, and b3: z:=x*(y-x) (correctness is de.ned as above, through the same pre/postcondition pair). \nThe assertions labeling the two edges are x > 0 and y > x. The assertion x > 0 is stable under the action \nb2 (the Hoare triple {x > 0} y:=x+1 {x > 0} holds). However, the assertion y > x is not stable under \nthe action b1 (the Hoare triple {y > x} x:=y+1 {y > x} does not hold). This means that the action b2 \nmay come after the action b1, but not vice versa. That is, the trace b1.b2.b3 lies in the set of traces \nrepresented by the iDFG, but the trace b2.b1.b3 does not. Note that the trace b1.b2.b3 is correct but \nthe trace b2.b1.b3 is not. We use iDFGs as the basis of an algorithm to verify concurrent programs (see \nalso Figure 3). The algorithm iteratively picks a program trace, applies a static analysis to the program \ntrace, uses the output of the static analysis to construct an iDFG, merges the new iDFG with the one \nconstructed so far, and checks whether the set of traces that is represented by the resulting iDFG includes \nall program traces. If this is the case, the iteration stops; the program is proven correct. In Section \n2 we will see examples in the context of concurrent programs. Sections 3 to 7 then present the individual \nconceptual and technical contributions of our work: the concept of iDFGs as a representation of a set \nof interleav\u00adings of thread executions, and the characterization of program correctness by an iDFG (Section \n3),  the reduction of the check of program correctness, given a program and an iDFG, to a combinatorial \nproblem in PS PAC E (Section 4),  the formalization of a measure of data complexity and the proof that \niDFGs can be polynomial in this measure (Section 5),  a veri.cation algorithm (Section 6), and  the \nproof that the algorithm constructs a polynomial-size iDFG under the assumption that the underlying static \nanalysis pro\u00advides the required inductive assertions (Section 7).  2. Examples In this section, we use \na few simple examples of concurrent pro\u00adgrams to illustrate iDFGs. Lamport s Bakery Algorithm The code \nbelow implements a version of Lamport s mutual exclu\u00adsion protocol for two threads [24]. The integer \nvariables n1 and n2 (the tickets) are initially set to 0, the boolean variables e1 and e2 to false. We \nuse the notation [exp] for assume statements ( if(exp) Figure 1. Four traces of the Bakery algorithm \nand their corresponding iDFGs. The four iDFGs constitute a proof of the correctness of the mutual exclusion \nproperty. Correctness here means: every trace that violates mutual exclusion is infeasible, i.e., has \npostcondition false. The precondition PreC is n1 = n2 = 0 . e1 = e2 = false. The action init is a dummy \nlabel of the initial node in an iDFG.  then skip else block ). Thread B cannot enter its critical sec\u00adtion \nunless either n1 = 0 (Thread A has not asked to enter the critical section) or n1 < n2 (Thread A has \nasked but it has done so after Thread B). Symmetrically for Thread A, except if there is a tie (n1 = \nn2), Thread A has priority over Thread B to enter its critical section .rst. We break the ticket acquisition \nstatement (n1 := n2 + 1) into two statements tmp1 := n2 and n1 := tmp1 + 1 to re.ect the possibility \nof the update happening non-atomically (we treat individual program statements as atomic). The .ag e1 \n(initially false) is used to communicate that Thread A is entering (that is, it is in the process of \nacquiring its ticket). The .ag e2 is used similarly for Thread B. We wish to verify that this protocol \nguarantees mutual exclu\u00adsion: only one thread at a time can be in its critical section. Each trace leading \nto a state where both threads are in their critical sec\u00adtions (after executing a6 respectively b6) must \nbe infeasible (i.e., there exists no initial state that has an execution along the trace) or, equivalently, \nmust have the postcondition false. Trace 1 in Figure 1 is a run of Bakery where .rst Thread A runs until \nit enters its critical section and then Thread B runs until it enters its critical section. The iDFG \nG1 in Figure 1 expresses the essence of the proof that Trace 1 is infeasible. This graph should be read \nfrom bottom to top, and should be thought of as backwards proof argument, along the lines of the following \ndialogue between Student and Teacher: Why is Trace 1 infeasible? Thread B can not execute the statement \nb6. Why not? When Thread B tries to execute b6, n1 > 0 (Thread A has a ticket) and n1 < n2 (Thread \nA s ticket is smaller than Thread B s ticket). The dialogue then bifurcates: Why is n1 > 0 at b6? Thread \nA executes n1:=tmp1+1 at a3 when tmp1 = 0. But what about everything that happens between a3 and b6? \n Irrelevant. Nothing between a3 and b6 can change the fact that n1 > 0. Why is tmp1 = 0 at a3? ... \n Why is n1 < n2 at b6? . . .  The iDFG G1 in Figure 1 keeps account of the relevant details of the \ntrace (e.g., the edge labeled n1 > 0 represents that a3 must be executed before b6, and nothing executed \nbetween a3 and b6 may change the fact that n1 > 0). It abstracts away irrelevant details of the trace \n(e.g., the ordering between a2 and b2). The iDFG represents the set of traces which are infeasible for \nessentially the same reason as Trace 1 (i.e., which have essentially the same Hoare triples in the proof \nthat the postcondition is false). The traces in the set can be obtained from Trace 1 by reordering actions \nor by inserting actions that are irrelevant for the proof (they leave the corresponding assertion stable). \nInformally, the set contains all traces where Thread B enters its critical section while Thread A has \na smaller (nonzero) ticket. Every trace that violates mutual exclusion falls into one of four scenarios, \neach scenario being exempli.ed by a trace in Figure 1. The scenarios are: (Trace 1) Thread B attempts \nto enter its critical section when Thread A has a smaller ticket, (Trace 2) the symmet\u00adric scenario where \nThread A attempts to enter its critical section when Thread B has a smaller ticket, (Trace 3) Thread \nB attempts to enter its critical section when Thread A and B have the same ticket (recalling that Thread \nA has priority in this situation), (Trace 4) Thread B enters its critical section without waiting for \nThread A to receive its ticket. The iDFGs G1,. . . , G4 express the essence of the proof of the infeasibility \nof each trace that falls into the corre\u00adsponding scenario. Together, they form a proof of the correctness \nof Bakery. This is because each trace of Bakery falls into one of the four scenarios; formally, it lies \nin one of the four sets repre\u00adsented by G1,. . . , G4 (which one can check with the algorithm of Section \n4). Note that there is no need for a new iDFG for the trace symmetric to Trace 4. The corresponding scenario \nis accounted for by the scenario of Trace 3. This asymmetry is due to the asymmetry of the protocol (Thread \nA has priority over Thread B if they have the same ticket).  Increment Example We use a simple program \nwith a parametrized number of threads to illustrate the exponential succinctness of data .ow graphs. \nThe program has one global variable x (initially 0) and is composed of a thread that asserts x = 0 and \na parametrized number N of threads that each increment x once. The program is safe (the assert never \nfails) if x = N is an invariant. For\u00admally, the correctness of the program means that each trace containing \nthe assume statement [x>N] is infeasible, i.e., has postcondition false. The iDFG shown on the right \nis a proof of the cor\u00adrectness of the program. Its size linear in N. As one can check algorithmically \n(Section 4), the set of traces it represents contains all program traces. Thus, it is a proof of the \ncorrectness of the program. The program is based on an ex\u00adample that was used in [29] to illus\u00ad trate \nthe need for auxiliary variables in compositional proof rules. The pro\u00adgram is a challenge for many exist\u00ading \napproaches. We ran experiments, for example, with the tool TH R E A D E R [17], which generates Owicki-Gries \ntype proofs and rely-guarantee type proofs, and with the tool SLAB [12], which uses abstraction-re.nement \nusing Craig inter\u00ad polation with slicing. In both tools, the space used for the proof grows exponentially \nin N. If we consider the increment actions of each thread as pairwise distinct (i.e., the program has \nN actions instead of just one), then we can give a correctness proof in the form of an iDFG of size quadratic \nin N. The graph can be depicted as an (N+1)\u00d7N matrix of vertices. Each column i contains N vertices labeled \nwith the increment action of the i-th thread (plus the vertex labeled [x>N]). Each vertex in row j has \nan edge to every vertex in row j + 1. The set represented by this iDFG thus contains traces with more \nthan one occurrences of the increment action of the same thread (for example, the traces corresponding \nto a column). These traces do not correspond to a path in the control .ow graph but they are correct. \nThis illustrates the point that a set of correct traces can represented more compactly if it is larger. \nTicket Algorithm In the parametrized version of the ticket algorithm [1] depicted below, the statement \nat line e1,i (for i . [1, N ]) indicates an atomic operation that increments t after assigning its current \nvalue to mi . Initially s = t = 0. The algorithm is a challenge for many existing approaches to concurrent \nprogram veri.cation in that, as with the increment example, the space used for the proof will grow exponentially \nin N. We can give a correctness proof in the form of an iDFG of quadratic size in N. It is related to \nthe one for the increment example. We present its construction (for N = 3) as an example in Section 6. \n3. Inductive Data Flow Graphs (iDFGs) We will .rst introduce the notation needed to de.ne the correctness \nof programs. We then introduce the notion of inductive data .ow graphs and use it to characterize program \ncorrectness. To abstract away from the speci.cs of a programming language and from the speci.cs of a \nprogram analysis and veri.cation en\u00advironment, we use a very general setup. A program is given as an \nedge-labeled graph P = (Loc, d) (the control .ow graph) where the edge labels are taken from the set \nActions (a given set of ac\u00adtions); i.e., d . Loc \u00d7Actions\u00d7Loc. We say that the edge between the two vertices \n(locations) e and e1 is labeled by the action a if (e, a, e1) . d. The program P = (Loc, d) can be de.ned \nas the parallel composition of a number N of programs P1 = (Loc1, d1), . . . , PN = (LocN , dN ) (we \nwill say that P1, . . . , PN are the threads of the program P). Its set of locations is the Cartesian \nproduct of the sets of thread locations, i.e., Loc = Loc1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 LocN . Each edge (ei, ai, e1 i) in \nthe i-th thread Pi gives rise to an edge (e, ai, e1) in the program P; the edge is labeled by the same \naction ai and goes from the location e = (e1, . . . , ei, . . . , eN ) to the location e1 = (e1, . . \n. , ei 1 , . . . , eN ) (i.e., only the i-th component can change). In algorithms that take P1, . . . \n, PN as the input, the control .ow graph for the program P is generally not constructed explicitly. We \nassume a set F of assertions. Each assertion . is a .rst\u00adorder logic formula over a given vocabulary \nthat includes a set Var of variables (the program variables). We assume that the set of assertions comes \nwith a binary relation, the entailment relation. Each action a comes with a binary relation between assertions \n(the set of its precondition/postcondition pairs). We say that the Hoare triple {.pre } a {.post } is \nvalid if the binary relation for the action a holds between the assertions .pre and .post . It is useful \nto suppose that we have actions that correspond to assume statements. That is, for every assertion . \nwe have an action [.] such that the Hoare triple {.pre } [.] {.post } is valid if the assertion .post \nis entailed by the conjunction . . .pre . A trace t is a sequence of actions, say t = a1 . . . an. We \nex\u00adtend the validity of Hoare triples to traces in the canonical way. The Hoare triple {.pre } t {.post \n} is valid for the empty trace e if .pre entails .post . It is valid for the trace t = a1 . . . an if \neach of the Hoare triples {.pre } a1 {.1}, . . . , {.n-1} a {.post }is valid. Later, when we formulate \nalgorithms, we will abstract away from the speci.c procedure (static analysis, interpolant gener\u00adation, \n. . . ) used to construct the sequence of intermediate assertions .1, . . . , .n-1.  To de.ne the correctness \nof the program P, we need to de.ne its set of program traces. We assume that the control .ow graph of \nthe program P comes with an initial location e0 and a set F of .nal locations. We say that the trace \nt is a program trace of P if t labels a path between the initial and a .nal location. Not every such \npath corresponds to a possible execution of the program. The set of program traces is generally not pre.x-closed. \nWe say that the program P is correct w.r.t. to the pre/postcondition pair (.pre , .post ) if the Hoare \ntriple {.pre } t {.post } is valid for every program trace t of P. We will later characterize program \ncorrectness in terms of the notion that we introduce next. De.nition 3.1 (Inductive Data Flow Graph, \niDFG) An inductive data .ow graph (iDFG) G = (V , E , .pre , .post , v0, V.nal) consists of a vertex-labeled \nedge-labeled graph (V , E ), an assertion .pre (the precondition), an assertion .post (the postcondition), \na vertex v0 . V (the initial vertex), and the subset of vertices V.nal . V (the set of .nal vertices). \n Vertices are labeled by actions. We use act(v) to denote the action that labels v. The initial vertex \nv0 has a dummy label: the special action init which has the same Hoare triples as skip, i.e., {.} init \n{.1} holds if . entails .1 .  Edges are labeled by assertions, i.e., E . V \u00d7 F \u00d7 V . We require that \nthe initial vertex v0 has no incoming edges. We  . will use the notation v -. v 1 to denote an edge \nfrom v to v 1 labeled by the assertion .. The labeling of edges with assertions is inductive, i.e., for \nevery vertex v labeled with, say, the action a, the Hoare triple {.} a {.1} holds for assertions . and \n.1 chosen as follows. If v is the initial vertex (i.e., v = v0), then . is the precon\u00addition .pre ; otherwise, \n. is the conjunction of the assertions labeling the incoming edges to v. If v is a .nal vertex (i.e., \nv . V.nal), then .1 is the conjunc\u00adtion of the postcondition .post and the assertions labeling the outgoing \nedges of v; otherwise .1 is just the conjunc\u00adtion of the assertions labeling the outgoing edges of v. \n. Remark 3.2 If the initial vertex of the iDFG G is a .nal vertex, then the precondition of G entails \nits postcondition (formally, if v0 . V.nal then .pre |= .post ). . We will use an iDFG G as a representation \nof a set of traces [G](the denotation of G). The set [G] consists of those traces t for which G can be \nused in a prescribed way to justify the correctness of t (in this paper, whenever the context is given \nby an iDFG G, the term correctness refers to the pre/postcondition pair of G). We will next use an example \nto explain how one uses an iDFG G to justify the correctness of a trace t , i.e., how one derives that \nt . [G]. Consider the iDFGs G0, . . . , G4 in Figure 2 (which all have the same precondition, .pre = \nx = 0 . y = 0, but different post\u00adconditions), the trace t0 = x++.y++.z:=x+y, and its pre.x traces t1 \n= x++.y++, t2 = x++, and t3 = e. We will derive t0 . [G0]via t3 . [G3], t2 . [G1], and t1 . [G1] in one \nderivation branch and via t3 . [G4], t2 . [G4], and t1 . [G2] in the other. By Remark 3.2, the precondition \nof G3 entails its postcondition, so we can use G3 to justify the correctness of the empty trace w.r.t. \nthe speci.cation of G3 (i.e., the validity of {.pre } e {x = 0}). Thus, t3 . [G3]. The iDFG G1 has the \npostcondition x > 0. The trace t2 is of the form t2 = t3.x++. We have already used G3 to justify that \nt3 has the postcondition x = 0. Since, by the inductiveness of G1, the Hoare triple {x = 0} x++ {x > \n0} is valid, we can use G1 to justify the correctness of t2 (w.r.t. the postcondition x > 0). Thus, t2 \n. [G1]. The trace t1 is of the form t1 = t2.y++. We have already used G1 to justify that t2 has the postcondition \nx > 0. The iDFG G1 has the postcondition x > 0 which is stable under the action y++, i.e., {x > 0} y++ \n{x > 0} is valid. Taken together, this means that we can use G1 also to justify the correctness of t1 \n(w.r.t. the postcondition x > 0). Thus, t1 . [G1]. We use G2 to justify the correctness of t1 (now w.r.t. \nthe post\u00adcondition y > 0). In three steps that are similar but not symmet\u00adric to the previous one, we \nderive (1) e . [G4] (by Remark 3.2), (2) t2 . [G4] (because t2 = e.x++ and the postcondition of G4 is \nstable under the action x++, i.e., {y = 0} y++ {y = 0}), and (3) t1 . [G2] (t1 = t2.y++ and, by the \ninductiveness of G1, {y = 0} x++ {y > 0} is valid).  The trace t0 is of the form t0 = t1.z:=x+y. We \ncan use G1 and G2 together to justify that t1 has the postcon\u00addition x > 0 . y > 0 which is the conjunction \nof the postcon\u00additions of G1 resp. G2. Since, by the inductiveness of G0, {x > 0 . y > 0} z:=x+y {z > \n0} is valid, we can use G0 to jus\u00adtify the correctness of t0 (w.r.t. the postcondition z > 0). Thus, \nt0 . [G0]. In the derivation above, the iDFGs G1 and G2 corre\u00adsponding to the two incom\u00ading edges to \nthe .nal node of G0 are treated in conjunction: one derives that the pre.x t1 of t0 lies in [G1] and \nin [G2]. We next illustrate that the notion of iDFGs also pro\u00advides a concept of disjunc\u00adtion. In the \niDFG G1 0 to the right, two of the three incom\u00ad ing edges to the .nal node are labeled with the same \nassertion, namely x > 0. The two iDFGs corresponding to the two edges are G1 and G1 1, where G1 is as \nbefore and G1 1 is the iDFG whose .nal node is the new node (labeled with the action x:=2). Both, G1 \nand G1 1, have the postcondition x > 0. They are treated in disjunction: in order to derive t0 . [G1 \n0], one can derive that t1 lies in [G1] or in [G1 1]. As a consequence, we can derive not only t0 . [G1 \n0] (via t1 . [G1]) but also t0 1 . [G1 0] where t0 1 = x:=2.y++.z:=x+y (via t1 1 . [G1 1] where t1 1 \n= x:=2.y++). The examples above illustrate how one can use an iDFG G to justify the correctness of a \ntrace t and derive t . [G]. The de.nition below generalizes the examples. De.nition 3.3 (Denotation of \nan iDFG, [G]) We de.ne when a trace t lies in the denotation of an iDFG G, formally t . [G] by induction \nover the construction of the trace t . The empty trace t = e lies in [G] iff v0 . V.nal. The trace t \n= t 1 .a obtained by attaching the action a at the end of the trace t 1 lies in the denotation of an \niDFG G of the form G = (V, E, .pre , .post , v0, V.nal) if either: the postcondition of G is stable under \nthe action a and the trace t 1 lies in the denotation of G, i.e., 1 {.post } a {.post } and t . [G] or \nFigure 2. Example iDFGs used to illustrate De.nition 3.3. The .ve iDFGs differ in the postcondition .post \nand the set of .nal vertices V.nal. In each iDFG, V.nal consists of the vertex with a dangling outgoing \nedge (the edge has no target or its target lies in the gray part of the graph), and .post is the assertion \nlabeling that edge. For example, the .nal vertex of G1 is the vertex labeled with the action x++ and \n.post is x > 0; the .nal node of G3 is the initial vertex and .post is x = 0.  one of the .nal vertices \nvf . V.nal is labeled by the action a, and: the assertions .1, . . . , .n are the labels of the incoming \nedges of the .nal vertex vf , i.e., . {.1, . . . , .n} = {. | v -. vf }, for each i = 1, . . . , n, the \ntrace t 1 lies in the denotation of the iDFG Gi which we obtain from G by taking the assertion .i for \nthe postcondition and the set of vertices Vi (de.ned below) for the set of .nal vertices, i.e., Gi = \n(V, E , .pre , .i, v0, Vi) where Vi is the set of vertices that have an outgoing edge labeled with the \nassertion .i to the .nal vertex vf , i.e., .i Vi = {v | v -. vf }. . Remark 3.4 An alternative, equivalent \nde.nition of the denotation of an iDFG is based on .xpoints. Given the iDFG G as above, we de.ne a monotone \nfunction Actions * Actions * F : (V . 2) . (V . 2) over the complete lattice V . 2Actions * as follows: \nF (L)(v0) = {e} and for all v = v0,  F (L)(v) =L(u).Stable(.) * .act(v) . . {.|.u.V.u -.v} {u|u-.v} \nwhere Stable(.) = {a . Actions | {.} a {.}}We use L0 to denote the least .xpoint of F . The denotation \nof G can be equivalently de.ned as below.  [G] = * L0(v).Stable(.post ) . v.V.nal The following observation, \na direct consequence of De.ni\u00ad tion 3.3, relates the correctness of a trace with the denotation of an \niDFG. Remark 3.5 Let G = (V , E , .pre , .post , v0, V.nal) be an iDFG. Then for any t . [G], the Hoare \ntriple {.pre } t {.post } holds. . Since we de.ned that the program P is correct w.r.t. the pre/postcondition \npair (.pre , .post ) if the Hoare triple {.pre } t {.post } is valid for every program trace, the above \nob\u00adservation gives immediately rise to a characterization of program correctness. Theorem 3.6 (Program \ncorrectness via iDFG) A program P is correct w.r.t. the precondition .pre and the postcondition .post \nif there exists an iDFG G with precondition .pre and postcondition .post such that every program trace \nof the program P lies in the denotation of the iDFG G: {program traces of P } . [G]. . We say that G \nis a proof for P if the above inclusion holds. In the next sections, we will investigate how one can \nalgorithmically check the inclusion for a given program and a given iDFG, and how one can construct an \niDFG. 4. Checking iDFGs In order to show that a given iDFG G is a proof for a given program P, we need \nto check the condition that every program trace of P lies in the denotation of G. We will reduce this \ncheck to an inclusion problem between two automata. The set of program traces of the program P is the \nset of label\u00adings of paths between the initial location e0 and a .nal location e . F in the control .ow \ngraph of the program P (as mentioned previously, not every such path corresponds to a possible execution \nof the program). Thus, the set of program traces is a regular lan\u00adguage over the alphabet Actions, the \nset of actions. It is recognized by the program P viewed as an automaton. Its set of states is Loc, a \nthe set of program locations. Its transition are the edges e1 -. e2 (state e1 goes to state e2 upon reading \nletter a). The initial state is the initial location e0 and the set of .nal states is the set F of .nal \nlocations. We use L(A) to denote the language recognized by the automaton A. We thus have {program traces \nof P } = L(P). We will next transform an iDFG G into an alternating .nite automaton. Alternating .nite \nautomata [6, 7] or AFA are a gener\u00ad alization of NFA. This generalization is easily understood if one \nviews the generalization from DFA to NFA as follows. The value of the deterministic successor function \nis generalized from a single successor state to a disjunction of states, and the initial state is gen\u00aderalized \nto be a disjunction of states. The NFA associates with any word a Boolean expression (in fact, a disjunction \nof states) obtained by repeatedly rewriting the initial formula using the successor func\u00adtion, and accepts \nif that Boolean expression evaluates to true under the assignment that sends .nal states to true and \nnon-.nal states to false. The generalization to AFA is then very natural: one allows a general Boolean \nexpression over states for the image of the succes\u00adsor function and for the initial condition.  From \ndata .ow graph G to alternating .nite automaton AG. Given the data .ow graph G = (V, E, .pre , .post \n, v0, V.nal), we de.ne the AFA AG = (S, Q, d, q0, Q.nal) where S = Actions . Q = {(., v) | .v 1 . V. \nv -. v 1} . {(.post , vf ) | vf . V.nal}  d((., v), a) = skip((., v), a) . step((., v), a) where  \n(., v) if {.} a {.} skip((., v), a) = false otherwise step((., v) , a)) = l . l . (., v1) if act(v) \n= a {.|v-.v}{vl|v-.v} false otherwise q0 =(.post , vf ) vf .V.nal Q.nal = {(., v) . Q | v = v0}. The \nobservation below is immediate by the fact that the construc\u00adtion of AG mimics the de.nition of the denotation \nof G. We write Lrev for the reversal of the language L. Remark 4.1 The set of traces denoted by G is \nthe reverse of the language recognized by AG, i.e., [G] = L(AG)rev . . We can now reformulate Theorem \n3.6 from the previous section. The check whether a given language is included in the reversal of the \nlanguage of a given AFA can bypass the construction of the reversal of the AFA (because we can reverse \nthe program traces instead). Theorem 4.2 (Checking program correctness via an iDFG) A program P is correct \nw.r.t. the precondition .pre and the postcon\u00addition .post if there exists an iDFG G with precondition \n.pre and postcondition .post such that the language inclusion L(P) . L(AG)rev holds for the program automaton \nof P and the alternating .nite automaton AG derived from the iDFG G. . The theorem above expresses that \nwe can shift the burden of having to deal with the exponential size of the control .ow graph of a concurrent \nprogram P de.ned by the parallel composition of N threads to a combinatorial problem over .nite graphs \n(and thus to a place where it hurts less, in comparison to a setting where one has to deal with data). \nWe can view the problem as a .nite-state model checking problem since it is equivalent to the satisfaction \nof a linear time property (a regular safety property de.ned by the AFA) by a .nite-state model (the control \n.ow graph of P). It is a classical result that this problem can be solved in polynomial space. Theorem \n4.3 (PS PAC E) The problem of checking whether an iDFG G is a proof for a program P given as the parallel \ncomposi\u00adtion of N threads, (i.e., checking whether all program traces of P are included in the denotation \nof G) is PSPAC E (in the number N of threads). . 5. Succinctness of iDFGs In this section, we justify \nour claim that iDFGs are succinct proof objects. We introduce localized proofs in order to de.ne a measure \nof the dif.culty of proving that given program P satis.es a spec\u00adi.cation (.pre , .post ); we call this \nmeasure the data complexity of P (with respect to (.pre , .post )). We prove that if there exists a small \nproof of correctness for a program (i.e., if the program has low data complexity), then there exists \na small iDFG proof. It is easy to construct an iDFG proof from a control .ow graph with a Floyd annotation \nby replacing edges with vertices, and vertices with edges. This construction proves the completeness \nof our proof rule (relative to the completeness of Floyd/Hoare proofs), but it leaves something to be \ndesired: for a concurrent program P, the iDFG resulting from this construction is of the same size as \nthe control .ow graph for P, which is exponential in the number of threads in P. In the rest of this \nsection, we develop a characterization of when it is possible to prove that small iDFG proofs exist. \nConsider the increment example from Section 2. There is an intuitive sense in which it is easy to prove \nthat this program is correct because, although the size of the control .ow graph for increment is exponential \nin the number of threads, the number of distinct assertions in the labeling of the control .ow graph \nas required by a Floyd/Hoare proof is linear (the assertions appearing in the Floyd annotation are the \nones of the form x = i, for i . [1, N ]). Following this example, a .rst attempt at de.ning a measure \nof the inherent dif.culty of proving a program correct w.r.t. a speci.cation may be the minimal number \nof assertions in a Floyd/Hoare proof of the property. This de.nition fails a natural requirement for \nsuch a measure, which is that if the threads have no shared data (disjoint parallelism), then their parallel \ncomposition should have a small proof. It is for this reason that we introduce the concept of localized \nproofs. We .rst explain the intuition behind localized proofs, and then give their formal de.nition. \nLocalized proofs are a way of exposing how compositional a Floyd/Hoare proof is, while avoiding syntactic \nissues common to practical compositional proof methods such as auxiliary variables in Owicki-Gries or \nRely/Guarantee. A localized proof essentially splits each assertion in the Floyd annotation of the control \n.ow graph of P into a global assertion and a set of local assertions (local for a thread). The total \nnumber of distinct assertions (both global and local) can then be seen as a measure of the inherent complexity \nof the proof. The intuition behind this view comes from the idea of transforming a succinct proof (in \nsome other framework, e.g., Owicki-Gries) to a Floyd/Hoare proof for the product program. In performing \nthis transformation, assertions from the succinct proof will be replicated several times: an assertion \nattached to the control location e of some thread Pi will appear as a conjunct in each assertion attached \nto a location of the control .ow graph of P where thread Pi is at e, i.e., a location of the form (e1, \n. . . , ei, . . . , eN ) where ei = e. De.nition 5.1 (Localized proof) Given a concurrent program P de.ned \nby the parallel composition of N threads P1, . . . , PN and a pre/postcondition pair (.pre , .post ), \na localized proof . = (.0, .1, . . . , .N ) is as a tuple of annotations (i.e., mappings from locations \nof the control .ow graph of P to assertions) where each assertion .0(e) ( the global assertion at e ) \nmay refer to global and local variables but each assertion .i(e) may refer only to the local variables \nof thread Pi. 1. If the edge (e, a, e 1) labeled by the action a from the location e to the location \ne 1 in the control .ow graph of P is induced by the edge (ek, a, e 1 k) of the thread Pk (i.e., if e \n= (e1, ..., eN ) 111 1 and e = (e1, ..., e N ) then ei = ei for i = k), then the Hoare triple {.0(e) \n. .k(e)} a {.0(e 1) . .k(e 1)} is valid and for all i > 0 different from k, the local assertion is the \nsame for location e and location e 1; i.e., .i(e) = .i(e 1) for i = k. 2. The assertions at the initial \nlocation are entailed by the precon\u00addition; i.e., .pre |= .i(e0) for i = 0, 1, . . . , N . 3. Every \n.nal location is annotated with the same tuple of asser\u00adtions and their conjunction entails the postcondition; \ni.e., there   exist .0, .1, . . . , .N such that .(e) = (.0, .1, . . . , .N ) for all .nal locations \ne and .0 . .1 . \u00b7 \u00b7 \u00b7 . .N |= .post . . A localized proof can be viewed as a particular presentation \nof the Floyd/Hoare proof that labels each location e with the conjunction of the global assertion and \nall local assertions for location e. In a degenerate case of a localized proof, the local annotations \n.1,\u00b7 \u00b7 \u00b7 , .N are all trivial; i.e., .i(e) = true for each location e of P and for i = 1, . . . , N . \nIn this case, the whole assertion for location e in a Floyd/Hoare proof is put into the global assertion \n.0(e) of the localized proof. The size of a localized proof . = (.0, .1, . . . , .N ) is the number of \ndistinct assertions appearing in .. Formally, size(.) =|Actionsi| \u00b7 |rng(.0)| \u00b7 |rng(.i)| i.[1,N] where \nrng(.) denotes the range of the annotation ., and Actionsi is the set of actions in the i-th thread Pi. \nExample 5.2 Consider the following simple example program, consisting of N threads, which each increment \na local variable ti before storing that value in a global variable x: Thread 1: t1 ++; x:=t1 . . . \nThread N: tN ++; x:=tN In the Hoare proof that this program satis.es the speci.cation .pre : x = t1 \n= ... = tN = 0/.post : x = 1, a distinct assertion is required for each combination of values for the \nti and x variables (except the combination where x = 1 and each ti = 0). The total number of such assertions \nis 2N+1-1. In the localized Hoare proof, .i assigns ti = 1 to every location after thread i executes \nti ++ and ti = 0 to every other location. The global annotation .0 assigns x = 1 to every location where \nsome thread has executed x:=ti and x = 0 to every other location. The size of this localized proof is \nx 2 \u00b7 2 \u00b7 2 = 8N. . i.[1,N] De.nition 5.3 (Data complexity) Given a pre/postcondition pair (.pre , .post \n), the data complexity of a program P is the mini\u00admum size of a localized proof that P satis.es the speci.cation \n(.pre , .post ). . We can now state the main result of this section. Theorem 5.4 (Size of iDFG proofs) \nGiven the pre/postcondition pair (.pre , .post ), if a proof for the program P exists, then there ex\u00adists \na proof for P in the form of an iDFG whose size is polynomial in the data complexity of P. . Proof. Let \n.pre , .post be a speci.cation, P = (Loc, d) be a program obtained as the parallel composition of N threads \nP1, . . . , PN , and . = (.0, .1, . . . , .N ) be a localized proof of min\u00adimum size that P satis.es \n.pre /.post . We construct an iDFG G = (V , E , .pre , .post , v0, V.nal) that proves that P satis.es \nthe speci.cation .pre /.post . The set of vertices of G consists of the initial vertex v0, regular vertices, \nand .nal vertices. V = {v0} . Vregular . V.nal The regular vertices are de.ned as: Vregular = {(a, .0(e), \n.i(e)) | i . [1, N ], .e 1 .(e, a, e 1) . di}, where for any i . [1, N ], di denotes the set of edges \ndue to thread Pi. The .nal vertices are de.ned as: V.nal = {(a, .0(e), .1(e), . . . , .N (e)) | (e, a, \ne 1) . d, e1 . F } where F denotes the set of .nal locations of P. The labeling act of a vertex v with \nan action is as one expects. The construction of the set of edges of G uses two auxiliary functions: \nthe function in maps each vertex v to a set of assertions that will label the incoming edges to v, and \nthe function pre maps each vertex to an assertion that can be thought of as its precondition. in(v0) \n= \u00d8 in((a, ., .i)) = {., .i} in((a, .0, .1, . . . , .N )) = {.0, .1, . . . , .N } pre(v0) = .pre pre((a, \n., .i)) = . . .i pre((a, .0, .1, . . . , .N )) = .0 . .1 . \u00b7 \u00b7 \u00b7 . .N The set of edges is de.ned as . \nE = {u -. v | u, v . V, u . V.nal, . . in(v), and {.} a {.} for . = pre(u) and a = act(u)} This completes \nour de.nition of G. It is easy to check that the labeling of the edges of G with assertions . is inductive. \nWe now argue that the number of vertices of G is linear in the size of the localized proof . (and therefore \nthe size of G is polynomial). A regular vertex is a triple of an action, a global assertion, and a local \nassertion. Hence the number of regular vertices is at most size(.). A .nal vertex corresponds to a location \ne that has an edge to a .nal location e 1 in P. According to the de.nition of a localized proof, .(e) \ncan vary from .(e 1) only in two components (the global assertion and one of the local assertions), and \nevery .nal location is annotated with the same tuple of assertions, i.e., .(e 1). Hence, the number of \n.nal vertices is at most size(.). In order to prove that every program trace t of P belongs to [G], we \nprove the lemma below; here, the notation G/(v, .) refers to the iDFG obtained from G by making v the \n(unique) .nal vertex and . the postcondition, for a vertex v of G and an assertion .. G/(v, .) = (V, \nE, .pre , ., v0, {v}) Lemma 5.5 For any path e0a0 \u00b7 \u00b7 \u00b7 an-1en in P of length n = 0 (starting at the \ninitial location e0 and leading to the location en), there exists a tuple of non-.nal vertices (u0, u1, \n. . . , uN ) such that a0\u00b7 \u00b7 \u00b7 an-1 . [G/(ui, .i(en))] for i = 0, 1, . . . , N . . Before we prove this \nlemma, we show why it implies that every trace of P belongs to [G]. Suppose t is a trace of P. Then there \nexists a path e0a0\u00b7 \u00b7 \u00b7 an-1enanen+1 such that a0\u00b7 \u00b7 \u00b7 an = t and en+1 is a .nal location. By the lemma, \nthere exists some u0, u1, . . . , uN such that a0 \u00b7 \u00b7 \u00b7 an-1 . [G/(ui, .i(en))] for all i = 0, 1, . . \n. , N . Let uf = (an, .0(en), .1(en), . . . , .N (en)). Then .i(n) it follows from the construction of \nE that ui -. uf . E for all i = 0, 1, ..., N , and thus that t = a0 \u00b7 \u00b7 \u00b7 an-1an . [G]. Now we prove \nthe lemma by induction on the length n of paths in P. The base case n = 0 is trivial: we may set each \nof u0, u1, . . . , uN to be the initial vertex v0. For the induction step, suppose that e0a0 \u00b7 \u00b7 \u00b7 an-1enanen+1 \nis a 11 1 path in P and that there exists u0, u 1,\u00b7 \u00b7 \u00b7 , u N such that a0 \u00b7 \u00b7 \u00b7 an-1 belongs to [G/(ui \n1 , .i(en))] for all i . [0, N ]. Let k . [1, N ] be the index of the thread that executes the last action \nan. We distinguish two cases: For i = k, we may take ui = ui 1 . By the induction hypothesis, a0\u00b7 \u00b7 \u00b7 \nan-1 . [G/(ui 1 , .i(en))]. By condition 1 of De.nition 5.1, .i(en) = .i(en+1), so a0\u00b7 \u00b7 \u00b7 an-1 . [G/(ui \n1 , .i(en+1))]. Since .i(en+1) is stable under the action an (since an is executed by thread k = i), \nit follows that a0\u00b7 \u00b7 \u00b7 an-1an . [G/(ui 1 , .i(en+1))].  For i = k, we make take ui = (an, .0(en), .k(en)). \nBy the de.nition of E, we have: .0( ).k( ) 1 n 1 n u0 -. ui . E and uk -. ui . E. Since (by the induction \nhypothesis), a0\u00b7 \u00b7 \u00b7 an-1 is in both [G/(u0 1 , .0(en))] and [G/(ui 1 , .i(en))], we have the desired \nre\u00adsult: a0\u00b7 \u00b7 \u00b7 an-1an . [G/(ui, .k(en+1))]. We still need to choose a vertex of G that we take for \nu0. We take the same vertex that we took for ui. The same argument as above shows that a0\u00b7 \u00b7 \u00b7 an-1an \n. [G/(ui, .0(en+1))]. The proof shows that the number of nodes of G is linear in the data complexity. \nIn the extreme case of disjoint parallelism (a concurrent program composed of N threads with no shared \ndata) there exists a localized proof without global assertion (formally, each global assertion is true.) \nIn this case, the iDFG constructed in the proof of Theorem 5.4 is essentially the collection of local \nproofs as in an Owicki-Gries style proof (the local proofs are connected via edges from the initial vertex \nv0). Its vertices correspond to pairs consisting of an action and a local assertion. Hence, the number \nof its vertices is linear in N. A special case of this theorem is a concurrent program in a parametrized \nnumber N of threads that has a localized proof where the number of global assertions (i.e., the size \nof rng(.0)) grows linearly in N, and the number of actions and local assertions (i.e., the size of rng(.i)) \nis constant. The increment example from Sec\u00adtion 2 falls into this case. The iDFG constructed in the \nproof of Theorem 5.4 has O(N) vertices (as does the one we constructed manually in Section 2). In Section \n2, we also hinted at the iDFG (with O(N2) vertices) for the case where we rename apart the ac\u00adtion x++ \nin each thread. This case illustrates the motivation to use the number of actions as a parameter for \nthe data complexity (which becomes quadratic in N in this case). In the case of the Ticket algorithm, \nnot only the number of global assertions, but also the number of local assertions grows linearly in N. \nTherefore, the data complexity is quadratic in N. Indeed, the iDFG we will construct in Section 6 has \nO(N2) ver\u00adtices. 6. Veri.cation Algorithm In this section, we develop an iDFG-based algorithm for verifying \nconcurrent programs. Given a program P and a pre/postcondition pair (.pre , .post ), the goal of this \nalgorithm is to either: construct a proof for P in the form of an iDFG G, or  return a counterexample, \ni.e., a program trace t such that the Hoare triple {.pre } t {.post } does not hold.  We use Figure \n3 to explain the basic idea behind the algorithm. The algorithm starts by picking a program trace t . \nIf the trace does not satisfy the Hoare triple {.pre } t {.post }, then the program is not correct with \nrespect to the given speci.cation. If it does, then t is abstracted into an iDFG Gt . The algorithm maintains \nan iDFG G that represents a set of traces proven correct (initially, this set is empty). The iDFG G is \nupdated by merging it with Gt . If the resulting iDFG contains every program trace, then G is a proof \nfor the program. If not, then the algorithm keeps generating traces and updating G until either a proof \nor a counterexample is found. In component (d), we check whether an iDFG G is a proof for P, which means \nchecking the inclusion L(P) . [G]; we discussed how to accomplish this in Section 4. The failure of the \ninclusion check means that there is a trace t in the difference between the two sets, L(P) \\ [G]. By \nchoosing such a trace in component (a), Figure 3. Veri.cation algorithm based on inductive data .ow graphs \n(iDFGs). Initially G is empty. we ensure progress of the algorithm (i.e., after each iteration, G denotes \na larger set of correct traces). In what follows, we discuss the remaining components (b) and (c). We \nwill explain how an iDFG Gt is constructed from a trace t (Section 6.1) and how iDFG G is updated by \nmerging it with Gt (Section 6.2). Finally, we present the full algorithm in Section 6.3. 6.1 Constructing \nan iDFG from a trace We will present an algorithm that, given a trace t and a pre/postcondition pair \n(.pre , .post ) such that the Hoare triple {.pre } t {.post } holds, constructs an iDFG Gt (with the \nsame pre/postcondition pair) whose denotation contains t . By Re\u00admark 3.5, Gt is a proof for the correctness \nof t , i.e., for the validity of the Hoare triple {.pre } t {.post }. This algorithm uses an auxiliary \nprocedure Interpolate. Given a trace t which ends in the action a, i.e., t = t1 .a and which is correct \nfor a given pair of assertions (.pre , .post ), i.e., 1 {.pre } t .a {.post }, the call of the auxiliary \nprocedure Interpolate(.pre , t , a, .post ) returns an intermediate assertion . such that {.pre } t 1 \n{.} and {.} a {.post }. The auxiliary procedure Interpolate is always applied to a .nite program trace, \nwhich is effectively a very simple sequential pro\u00adgram. This means that it can be implemented by one \nform or an\u00adother of static analysis applied backwards to this sequential pro\u00adgram (that consists just \nof the trace t). One can leverage the power of existing static analysis methods such as apply a backwards \ntrans\u00adformer in some abstract domain, Craig interpolation, weakest pre\u00adcondition computation, among others \nto construct iDFGs [9]. Intuitively, the procedure construct-idfg(t, ., .1) takes the trace t and detects \nwhat actions in t and what ordering constraints embodied in t are irrelevant (for attaining the postcondition \n.1). This is accomplished by leveraging the Interpolate procedure as follows: If Interpolate(., t 1, \na, .1) returns the postcondition .1 (and thus, the postcondition .1 is stable w.r.t. the action a (i.e., \n{.1} a {.1}), and therefore a is irrelevant.  Otherwise, if Interpolate(., t 1, a, .1) returns a proper \ncon\u00adjunction .1 .\u00b7 \u00b7 \u00b7..k, then each assertion .1, . . . , or .k can be attained as postcondition for \nthe pre.x trace t1 in parallel (as opposed to in sequential order).  One possibility to maximize parallelism \nis to have Interpolate return formulae in conjunctive normal form. The particular strategy to break the \nintermediate assertions into conjuncts does not matter for the correctness of the overall algorithm. \nNote that construct-idfg produces acyclic iDFGs without dis\u00adjunctions (no vertex has multiple incoming \nedges labeled with the same assertion). Disjunctions and cycles will be produced by the merge procedure \nthat we will discuss in Section 6.2.  Algorithm construct-idfg(t, ., .1) Input: trace t and a pair of \nassertions (., .1) such that {.} t {.1}Output: proof for t in the form of an iDFG Gt (i.e., Gt has the \npre/postcondition pair (., .1) and t . [Gt ]) if t = e then return Gt = ({v0}, \u00d8, ., .1 , v0, {v0})else \nt = t 1 .a for some trace t 1 and action a . . Interpolate(., t 1, a, .1) if . = .1 then return construct-idfg(t \n1, ., .1) else . = .1 .\u00b7 \u00b7 \u00b7 . .k v . fresh vertex with label a (i.e., act(v) = a) parallel for i = \n1, . . . , k do (Vi, Ei, ., .i, v0, V i , ., .i) .nal) . construct-idfg(t 1 end parallel for V . {v} \n. Vi i=1,...,k .i E . (Ei . {u -. v | u . V i i=1,...,k .nal}) return Gt = (V, E, ., .1 , v0, {v}) end \nif end if Example 6.1 We will demonstrate the algorithm by applying it to Trace 1 of Bakery, pictured \nin Figure 1(a). The precondi\u00ad tion is PreC = n1 = n2 = 0 . \u00ace1 . \u00ace2 and the postcondition is false. \nWe use t to denote Trace 1 and t1 to denote the pre.x of t obtained by removing last action b6. For this \nexample, we use the Craig interpolation feature of MathSAT5 [16] to implement Interpolate. The .rst call \nto Interpolate in construct-idfg yields the following interpolant: Interpolate(PreC, t1, b6, false) = \nn1 > 0 . n1 < n2 Intuitively, the order in which the two assertions n1 > 0 and n1 < n2 are enforced is \nirrelevant, as long as both are enforced before b6 is executed. This is re.ected by how the execution \nof construct-idfg proceeds, namely by calling construct-idfg(t1, PreC, n1 > 0) and construct-idfg(t1, \nPreC, n1 < n2) in parallel. The two calls correspond, respectively, to the left and the right branch \nof the iDFG (c) in Figure 1. In order to illustrate how construct-idfg suppresses irrelevant actions, \nwe will continue and follow the execution of the call construct-idfg(t1, PreC, n1 > 0). We use t2 to \ndenote the pre.x of t1 obtained by removing last action, which is [\u00ace1]. The action [\u00ace1] is irrelevant \nin the sense that it is not needed to enforce the postcondition n1 > 0. This is established by construct-idfg \nwhen the next interpolant along this branch is computed: Interpolate(PreC, t2, [\u00ace1], n1 > 0) = n1 > \n0 Since the computed interpolant is the same as the postcondition, construct-idfg enters the then branch \nof the conditional and pro\u00adceeds to the call construct-idfg(t2, PreC, n1 > 0). The algorithm continues \nto suppress irrelevant actions in this way, going backwards along t2 until it comes to the .rst action \nunder which n1 > 0 is not stable, which is a3 : n1 := tmp1 + 1. This action becomes the next vertex along \nthe n1>0 branch of the iDFG in Figure 1(a). . The essential property of construct-idfg is that it constructs \nan iDFG proof of the correctness of a trace with respect to a given pre/postcondition. This is expressed \nin the following lemma. Lemma 6.2 Let t be a trace that is correct with respect to the pre/postconditions \n./.1, and let Gt = construct-idfg(t , ., .1). Then t . [Gt ]. .  6.2 Merging iDFGs Our merge operator \nG1 I G2 can be thought of as a three step process: in the .rst step, we construct the disjoint union \nof G1 and G2; in the second step, completion, we saturate this iDFG by adding edges that do not violate \nthe inductiveness property for iDFGs; in the third step, reduction, we collapse equivalent vertices. \nWe begin with a declarative de.nition for what it means for an iDFG to be complete (i.e., edge-saturated). \nDe.nition 6.3 (Complete) An iDFG G = (V, E , ., .1 , v0, V.nal)is complete if: For any v . V with such \nthat {pre(v)} act(v) {.1} holds, v . V.nal  For any u, v . V , and any . such that v has an incoming \nedge  . labeled . and {pre(u)} act(u) {.} holds, u -. v . E where . if v = v0 pre(v) = ) . . otherwise \nu-.v.E . Completion plays an essential role in the construction of iDFG proofs for programs. It is essential \nfor producing proofs of pro\u00adgrams that contain loops. Note that, as we mentioned in Section 6.1, the \nconstruct-idfg procedure does not introduce any disjunc\u00adtions. Disjunctions are essential for capturing \nprogram behaviour produced by loops. The completion process can introduce these necessary disjunctions. \nFor example consider the following sim\u00adple program and its speci.cation: {x = y} while(*): x++; y++ {x \n= y}The iDFG pictured to the right proves that this program satis.es the speci.cation. The subgraph consisting \nonly of the solid arrows can be generated by construct-idfg from a sample trace of the program x++.y++. \nThe remaining dotted edge is added to the iDFG by the completion procedure (since x++ has an incoming \nedge labeled x = y and the Hoare triple {pre(y++)} y++ {x = y} holds). This generalizes the iDFG so \nthat it proves the correctness of any number of iterations of the loop rather than a single iteration. \nThe following is a declarative de.nition for what it means for an iDFG to be reduced (i.e., vertex-minimal) \nDe.nition 6.4 (Reduced) An iDFG G = (V, E , ., .1 , v0, V.nal)is reduced if there exist no distinct u, \nv . V such that act(u) = act(v) and the set of assertions labeling the incoming edges to u . and v are \nthe same (i.e., {. | .w.w -. v . E} = {. | . .w.w -. u . E}). . There is an algorithm that, given an \narbitrary iDFG G, con\u00adstructs a reduced complete iDFG rc(G) such that [rc(G)] contains [G]. We call rc(G) \nthe reduced completion of G. This is stated for\u00admally in the following proposition:  Proposition 6.5 \nFor every iDFG G = (V, E, .pre , .post , v0, V.nal), there is a reduced, complete iDFG rc(G) that can \nbe computed from G in O(|V | \u00b7 |E|) time and such that the precondition of rc(G) is .pre , the postcondition \nof rc(G) is .post , and [G] . [rc(G)]. . Proof. For any vertex v, we de.ne . in(v) = {. | .u.u -. v . \nG} We de.ne rc(G) = (V 1, E1 .nal), where , .pre , .post , v0, V 1 V 1 = {v0} . {(act(v), in(v)) | v \n. V \\ {v0}} . 1 111 E = {(a, .) -. (a , .) | . . .. {.} a {.}} . .{v0 -. (a 1 , .1) | . . .1 . {.pre \n} a {.}} and 1 {v0} . {(act(v), in(v)) | v . V.nal} if .pre . .post V = .nal {(act(v), in(v)) | v . V.nal} \notherwise It is easy to check that rc(G) is well-de.ned. The machinery required to prove that [G] . [rc(G)] \nis presented in Section 7. Finally, we describe our merge operation. The merge operator G1 I G2 functions \nby forming the disjoint union of G1 and G2, and then taking the reduced completion of the resulting iDFG. \nFormally, we de.ne merge as follows: De.nition 6.6 Given two iDFGs G1 = (V1, E1, ., .1 .nal) , v0, V \n1 and G2 = (V2, E2, ., .1 , v0, V 2 I G2 is .nal), their merge, G1 de.ned to be the iDFG = rc((V I, EI, \n., .1 , v0, V I G1 I G2 .nal)) where V I = V1 . V2, EI .nal .nal . V 2 = E1 . E2, V I= V 1 .nal, and \nact(v) is de.ned in the obvious way. For simplicity, this de.nition assumes that the initial vertex of \nG is also the initial vertex of G1 and that otherwise their vertices are disjoint. . Applying the rc \noperator in the merge plays an essential role in reducing the size of iDFG proofs. For example, the iDFG \nproof for the Ticket example from Section 2 would require O(N!) vertices without applying the rc operator \n(see Example 6.9). The progress of our algorithm depends on the following lemma concerning the merge \noperator: Lemma 6.7 Let G1 and G2 be two iDFGs. We have [G1] . [G2] . [G1 I G2]. . An example of the \nmerge operator appears in Figure 4: part (c) is the merge of parts (a) and (b). Notice that (c) proves \nthat the trace m1=t++.m3=t++.[m3 <= s] is infeasible (starting from the precondition s = t = 0), even \nthough this fact is not proved by (a) or (b).  6.3 Putting it all together We are now ready to present \na formal description of our veri.cation algorithm. The last component of this algorithm that has not \nalready been described is the initialization of G. If .pre implies .post (or equivalently, the Hoare \ntriple {.pre } e {.post } holds), then G is initialized to an iDFG with a single vertex that is both \ninitial and Figure 4. Intermediate iDFGs for the Ticket algorithm .nal (and therefore, [G] = {e}). Otherwise, \nG is initialized to an iDFG that has no .nal vertices (and therefore, [G] = \u00d8). Algorithm Veri.cation \nalgorithm Input: Program P, precondition .pre , postcondition .post Output: Yes if P is correct w.r.t. \nthe speci.cation; No otherwise. if .pre entails .post then G := (v0, \u00d8, .pre , .post , v0, {v0}) else \nG := (v0, \u00d8, .pre , .post , v0, \u00d8) end if while L(P) . [G] do Let t . L(P) \\ [G] if the Hoare triple \n{.pre } t {.post } holds then Gt := construct-idfg(t, .pre , .post ) G := G I Gt else return No (with \ncounter-example t ) end if end while return Yes As a direct consequence of Lemmas 6.2 and 6.7, we can \nstate the following progress property of the veri.cation algorithm: Proposition 6.8 (Progress) If Gi \nand Gi+1 are the iDFGs con\u00adstructed by the veri.cation algorithm in (respectively) the round i and round \n(i + 1), then we have [Gi] . [Gi+1] (the inclusion is strict). . We conclude this section with an example \nrun of the veri.cation algorithm on the Ticket algorithm mentioned in Section 2. Example 6.9 We consider \nthe 3-thread instance of the Ticket mutual exclusion algorithm, which runs three copies of the thread \nbelow in parallel (where i is substituted for the thread id). The .rst two rounds of the veri.\u00adcation \nalgorithm are depicted in Fig\u00adure 4. Since the property of interest is mutual exclusion, we take the \ntraces of this program to be the ones that end with (at least) two threads inside their critical section, \nand prove that the program meets the speci.\u00adcation with precondition s = 0 . t = 0 and postcondition \nfalse (i.e., every trace violating mutual exclusion is infeasible). The algorithm begins with the empty \niDFG, G\u00d8. The .rst pro\u00adgram trace t1 that we select in L(P) \\ [G\u00d8] is depicted in Fig\u00adure 4(a) along \nwith an iDFG proof for t1, as constructed by construct-idfg. We call this iDFG G1.  Figure 5. Correctness \nproof of the Ticket algorithm. Note that the post-conditions (all false) have been removed from the .gure \nto keep it clean. In a complete version of this .gure all 6 nodes with labels [mi = s] have a dangling \narrow with a label false. On the next iteration of the loop, we select the trace t2 depicted in part \nFigure 4(b), and merge its corresponding iDFG proof (called Gt2 ) with G1 to form G2 pictured in Figure \n4(c). Since [G2] does not cover every program trace (i.e., L(P) . L(G2)), the algo\u00adrithm continues the \nloop. After four more iterations, the algorithm will terminate after all traces that violate mutual exclusion \nhave been proved to be infeasible. The iDFG that is constructed by this algorithm is depicted in Figure \n5. The intuition for the iDFG in Figure 5 can be used to construct a proof for the Ticket mutual exclusion \nalgorithm for any number of threads. For any such iDFG proof, the number of vertices is quadratic in \nthe number of threads. . 7. Properties of the Veri.cation Algorithm We will now investigate the properties \nof the veri.cation algorithm. In particular, we prove that the algorithm is sound, that it is com\u00adplete \n(under the assumption that the auxiliary procedure Interpo\u00adlate returns the right assertions), and that \nits time and space com\u00adplexity is polynomial in the data complexity of the input program and speci.cation \n(again, under the assumption about Interpolate). We will also introduce some technical machinery required \nto prove these results. We begin with soundness (if the algorithm returns a result, then the result is \ncorrect), which is a direct consequence of Theorem 3.6. Theorem 7.1 (Soundness) If the veri.cation algorithm \nreturns Safe, P satis.es the given speci.cation. If it returns Unsafe, the program does not satisfy the \ngiven speci.cation. . We now move on to our completeness result. No algorithm ex\u00adists that is complete \nin the strong sense (due to the halting prob\u00adlem). Instead, we show that the veri.cation algorithm is \ncomplete in the sense that, if a there exists a safety proof for a program, then the algorithm will .nd \none under the assumption that Interpolate produces the right assertions. We formalize this by assuming \nthat Interpolate is a nondeterministic procedure that may return any valid interpolant, and showing that \nthe following holds: Theorem 7.2 (Completeness) If a program has a Hoare safety proof, then there exists \na run of the veri.cation algorithm that terminates with a Safe result. . In fact, we can strengthen this \ncompleteness theorem and give bounds on the time and space required by the veri.cation algo\u00adrithm: Theorem \n7.3 (Complexity) If a program P has a Hoare proof that it satis.es a speci.cation .pre /.post , then \nthere exists a run of the veri.cation algorithm that terminates with an iDFG of size polyno\u00admial in the \ndata complexity of P (w.r.t. .pre /.post ). Moreover, the number of iterations of the main loop of the \nalgorithm required to produce this proof is polynomial in the data complexity. . The proof of this theorem \nrequires some technical machinery, which will be presented in the following. However, we present some \nearly intuition on the proof for readers that wish to skip the technical details in the remainder of \nthis section. We assume that we are given a proof presented as an iDFG G0 (the size of which we may assume \nis polynomial in the data complexity, by Theorem 5.4). Given any trace t, there is a substructure of \nG0 that corresponds to t (roughly speaking, this structure corresponds to the accepting run of t through \nthe AFA corresponding to G0). Under the assumption that Interpolate produces the assertions corresponding \nto this struc\u00adture, construct-idfg(t, .pre , .post ) produces this structure exactly. Intuitively, this \nis a way of reasoning about why the appropriate assertions exist for Interpolate to produce. We can show \nthat G0 is covered by .nitely many such substructures, so that the algo\u00adrithm will terminate in .nite \ntime with a proof that is essentially the same as G0. Proofs We now present the technical machinery required \nto prove Theo\u00adrem 7.3. The main concept we introduce here is iDFG embeddings, which is a structural relationship \nbetween iDFGs. We say that an iDFG G embeds into G1 if G can be mapped onto a subgraph of G1 in a way \nthat is, in some sense, tight . Formally, we de.ne an iDFG embedding as follows: De.nition 7.4 (Embedding) \nGiven iDFGs G = (V, E, .pre , .post , v0, V.nal) and G1 = (V 1, E1 0 1 , V 1 , .pre , .post , v .nal) \nsharing the same precondition and postcondition, we say that a map h : V . V 1 is an embedding if the \nfollowing hold: .v . V , act(v) = act(h(v)) and . . {. | .u.u -. v . E} = {. | .u.u -. h(v) . E1} h(v0) \n= v0 1  .v . V.nal, h(v) . V 1  .nal . . .u -. v . E, h(u) -. h(v) . E1 If such an embedding exists, \nwe say that G embeds into G1 . . The main property of interest concerning embeddings is the following \nlemma: Lemma 7.5 If G and G1 are iDFGs such that G embeds into G1 , then [G] . [G1]. .  The key idea \nof our completeness theorem is that we can use a given iDFG proof G0 as an oracle to guide the interpolation \nprocedure, so that construct-idfg(t, .pre , .post ) will yield an iDFG representing some substructure \nof the target proof G0 (i.e., an iDFG that embeds into G0). Formally, Lemma 7.6 For any iDFG G = (V, \nE, .pre , .post , v0, V.nal) and any trace t . [G], there is a run of construct-idfg(t , .pre , .post \n) that produces an iDFG Gt such that Gt embeds into G. . Our proof of Theorem 7.2 is based on being able \nto maintain a loop invariant that G embeds into some target proof G0. In order to prove this invariant, \nwe must show that if G and G1 embed into some target proof G0, then G I G1 also embeds into G0. Formally, \nwe have the following: Lemma 7.7 For any iDFGs G and G1 sharing the same precon\u00addition and postcondition, \nboth G and G1 embed into G I G1. For any complete iDFG G0 such that G and G1 both embed into G0, 1 G \nI G1 embeds into G0 . . We are now ready to provide a sketch of the proof of Theo\u00adrem 7.3. In fact, we \nwill prove a stronger (by Theorem 5.4) result: for any reduced, complete iDFG G0 such that L(P) . [G0], \nthere exists a run of the veri.cation algorithm that terminates with an iDFG G such that L(P) . [G] and \nsuch that G embeds into G0 (since G and G0 are reduced and complete, the fact that G embeds into G0 implies \nthat G is no larger than G0). Moreover, regardless of how traces are chosen from L(P)\\[G], the algorithm \nwill termi\u00adnate in at most |V0| iterations, where |V0| is the number of vertices in G0. The proof is \nby induction: we assume that at the start of the iteration of the loop, G embeds into G0, and prove that \nthere is an execution of the loop body such that the number of vertices in G increases and such that \nG still embeds into G0 at the end of the loop. Let t . L(P) \\ [G]. Since G0 is a safety proof, we must \nhave t . L(G0). By Lemma 7.6, there exists a run of construct-idfg(t, .pre , .post ) that produces an \niDFG Gt that em\u00adbeds into G0. We let G1 = G I Gt be the iDFG at the end of the loop. The invariant that \nG1 embeds into G0 is ensured by Lemma 7.7. The condition that G1 has more vertices than G is en\u00adsured \nby the fact that Gt embeds into G1, but not G (since that would contradict t /. [G]). 8. Related Work \nConcurrent Program Veri.cation. As mentioned above, exist\u00ading approaches to the algorithmic veri.cation \nof concurrent pro\u00adgrams, e.g. [11, 12, 17, 20], provide a different angle of attack at the same fundamental \nissue: the exponential space complexity (exponential in the number of threads). None of these approaches \nshifts the burden of the exponential growth of space towards a com\u00adbinatorial problem (over .nite graphs). \nThe practical potential of these approaches is demonstrated on a selection of practical ex\u00adamples. None \nof the approaches investigates the question whether there are assumptions under which the space complexity \nis poly\u00adnomial. The exponential cost of space is not an issue that occurs just in theory. We observe \nthe exponential curve, e.g., when we run TH R E A DE R [17] and SL A B [12] on the ticket algorithm and \nthe program Increment (see Section 2). These are two concurrent programs (parametrized in the number \nn of threads) where our ap\u00adproach comes with a formal guarantee for polynomial space con\u00adsumption. 1 \nThis proposition establishes that G ! G1 is a coproduct in the category of reduced complete iDFGs, where \nthe morphisms are embeddings. All the approaches mentioned above are based on abstraction. The construction \nof an iDFG from a trace can be viewed as an abstraction of the trace. It is interesting to compare the \ntwo concepts of abstraction. In the setting of the above-mentioned approaches, if an abstraction is too \ncoarse, it introduces spurious errors. In our setting, it is desirable to abstract a trace aggressively \n(in order to obtain a succinct iDFG that denotes a large set of traces). This is because iDFG s are used \nto represent correct behaviours rather than program behaviours. More over-approximation leads to a larger \nset of correct traces. Compositional Proof Rules. Our notion of inductive data .ow graph bears similarities \nwith Owicki-Gries style proof rules and other compositional proof rules for concurrent programs (e.g., \nthe use of local assertions, of conjunction, of stability) [2]. Composi\u00ad tionality is often thought of \nas the only way to go for polynomial\u00adsized proofs (in the number of threads). Our approach to algo\u00adrithmic \nveri.cation can be viewed as the automation of a non\u00adcompositional proof rule (the proof rule is obtained \nby reformu\u00adlating the characterization of program correctness in Theorem 3.6). However, one can view \nsets of traces as semantics-based modules (as opposed to modules based on the syntactic construction \nof pro\u00adgrams). These modules capture the intuitive notion of scenarios. The composition of modules is \nset union. Thread-Modular Veri.cation. The thread-modular approach to veri.cation [15] achieves modularity \nat the expense of giving up completeness. In fact, the approach is complete exactly for the class of \nprograms whose correctness can be proven in Owicki-Gries style proofs without auxiliary variables [10, \n25]. Our approach, in contrast, is motivated by combining the goal of full (relative) completeness with \nspace ef.ciency. Data Flow Graphs. Variations of data .ow graphs have a long history within the compilers \ncommunity, both as a means for ex\u00adposing parallelism in sequential code for parallelizing compilers [14, \n23], and as a data structure for use in sparse data.ow analy\u00ad sis [21, 31]. Our use of DFGs is closer \nto this .rst line of work: construct-idfg can be seen as a procedure that exposes the paral\u00adlelism in \na single example trace. For a parallelizing compiler to be correct, parallelization must preserve the \nbehaviour of the sequen\u00adtial code; the correctness of construct-idfg depends on the much weaker condition \nthat a proof argument is preserved. More recently, DFGs have been used for invariant generation in both \nconcurrent [13] and sequential [28] settings. The work of [13] is particularly relevant: it uses data \n.ow graphs to generate numerical invariants for parameterized concurrent programs. These invariants can \nbe used to prove safety properties, but if the invari\u00adants are too weak to prove a property of interest, \na false alarm is reported. Our veri.cation algorithm produces no false alarms, and can provide a counter-example \nfor properties that fail. Moreover, the inductive DFGs presented in this paper are capable of express\u00ading \nproofs that cannot be represented using the (variable-labeled, rather than assertion-labeled) DFGs used \nin [13] (i.e., every DFG proof in [13] corresponds to an iDFG, but some iDFG s do not cor\u00ad respond to \nDFG proofs). Trace Abstraction. The work in [18, 19] presents an automata\u00ad theoretic approach to the \nanalysis and veri.cation of sequential and recursive programs. The present paper continues this line \nof work and extends it to concurrent programs. Another form of trace ab\u00adstraction appears in the work \non trace covers and trace partitioning [5, 26, 30]. There, the general goal is to obtain to obtain more \npre\u00ad cision by targeting the static analysis to speci.c subsets of program traces. In contrast, in our \nwork, the subsets of program traces are de.ned by the iDFGs (which are constructed via the static analysis \nof a single trace).  9. Conclusion and Future Work In this paper, we have introduced a new approach \nfor the veri.\u00adcation of concurrent programs. The approach succeeds in putting the well-established static \nanalysis techniques for sequential pro\u00adgrams to work, namely by assembling the output of the static anal\u00adysis \napplied to interleaved executions in a space-ef.cient way. We formalize under what assumptions the space \nef.ciency can be guar\u00adanteed. For fundamental reasons, we cannot avoid the exponential explosion in the \nnumber of threads but we can shift its burden to a combinatorial problem over .nite graphs ( to a place \nwhere it hurts less ). The approach has an interesting practical potential. The focus in this paper was \nto introduce the approach and to investigate its formal foundation. There are several directions in which \none can explore the practical potential of the approach. This should be the focus of future work. The \nmost critical operation is perhaps the construction of an iDFG from a given trace t of the program (construct-idfg). \nIn Section 6.1, we already hinted at several directions for practical optimizations. We need to develop \nand evaluate these optimizations for practical examples. The operation construct-idfg depends on the \nstatic analysis that is applied to the trace (as a special case of a sequential program). The abstract \nvalues generated by the static analysis are used to ex\u00adtract single conjuncts for the labeling of an \niDFG with inductive assertions. One possible research direction for practical implemeta\u00adtion of construct-idfg \nis to design a procedure that heuristically splits formulae into independent clauses that can be achieved \nin parallel (for use in the current Interpolate procedure). Another di\u00adrection is to develop a Craig \ninterpolation procedure that can gen\u00aderate the branching structure of an iDFG directly from a resolution \nproof. References [1] G. R. Andrews. Concurrent programming -principles and practice. Benjamin/Cummings, \n1991. ISBN 978-0-8053-0086-4. [2] K. R. Apt, F. S. de Boer, and E. R. Olderog. Veri.cation of Sequential \nand Concurrent Programs. Springer-Verlag, 2009. ISBN 978-1\u00ad84882-744-8. [3] J. Berdine, T. Lev-Ami, R. \nManevich, G. Ramalingam, and M. Sagiv. Thread quanti.cation for concurrent shape analysis. In CAV, volume \n5123 of LNCS, pages 399 413. Springer Berlin / Heidelberg, 2008. [4] B. Blanchet, P. Cousot, R. Cousot, \nJ. Feret, L. Mauborgne, A. Min \u00b4e, D. Monniaux, and X. Rival. A static analyzer for large safety-critical \nsoftware. In PLDI, pages 196 207, 2003. [5] F. Bourdoncle. Abstract interpretation by dynamic partitioning. \nJour\u00adnal of Functional Programming, 2(04):407 435, 1992. [6] J. Brzozowski and E. Leiss. On equations \nfor regular languages, .nite automata, and sequential networks. Theoretical Computer Science, 10 (1):19 \n 35, 1980. [7] A. K. Chandra, D. C. Kozen, and L. J. Stockmeyer. Alternation. J. ACM, 28(1):114 133, \nJan. 1981. [8] E. M. Clarke and E. A. Emerson. Design and synthesis of synchro\u00adnization skeletons using \nbranching-time temporal logic. In Logic of Programs, pages 52 71, 1981. [9] P. Cousot and R. Cousot. \nAbstract interpretation: A uni.ed lattice model for static analysis of programs by construction or approxima\u00adtion \nof .xpoints. In POPL, pages 238 252, 1977. [10] R. Cousot. Fondements des m \u00b4ethodes de preuve d invariance \net de fatalit\u00b4eles. les-Nancy, 1985. e de programmes parall` [11] A. F. Donaldson, A. Kaiser, D. Kroening, \nand T. Wahl. Symmetry\u00adaware predicate abstraction for shared-variable concurrent programs. In CAV, pages \n356 371, 2011. [12] K. Dr\u00a8ager, A. Kupriyanov, B. Finkbeiner, and H. Wehrheim. Slab: a certifying model \nchecker for in.nite-state concurrent systems. In TACAS, pages 271 274, 2010. [13] A. Farzan and Z. Kincaid. \nVeri.cation of parameterized concurrent programs by modular reasoning about data and control. In POPL, \npages 297 308, 2012. [14] J. Ferrante, K. J. Ottenstein, and J. D. Warren. The program depen\u00addence graph \nand its use in optimization. ACM Trans. Program. Lang. Syst., 9(3):319 349, 1987. [15] C. Flanagan and \nS. Qadeer. Thread-modular model checking. In SPIN, pages 213 224, 2003. [16] A. Griggio. A Practical \nApproach to Satis.ability Modulo Linear Integer Arithmetic. JSAT, 8:1 27, January 2012. [17] A. Gupta, \nC. Popeea, and A. Rybalchenko. Predicate abstraction and re.nement for verifying multi-threaded programs. \nIn POPL, pages 331 344, 2011. [18] M. Heizmann, J. Hoenicke, and A. Podelski. Re.nement of trace abstraction. \nIn SAS, pages 69 85, 2009. [19] M. Heizmann, J. Hoenicke, and A. Podelski. Nested interpolants. In POPL, \npages 471 482, 2010. [20] T. A. Henzinger, R. Jhala, R. Majumdar, and S. Qadeer. Thread\u00admodular abstraction \nre.nement. In CAV, pages 262 274, 2003. [21] R. Johnson and K. Pingali. Dependence-based program analysis. \nIn PLDI, pages 78 89, 1993. [22] V. Kahlon, S. Sankaranarayanan, and A. Gupta. Semantic reduction of \nthread interleavings in concurrent programs. In TACAS, pages 124 138, 2009. [23] D. J. Kuck, R. H. Kuhn, \nD. A. Padua, B. Leasure, and M. Wolfe. Dependence graphs and compiler optimizations. In POPL, pages 207 \n218, 1981. [24] L. Lamport. A new solution of Dijkstra s concurrent programming problem. Commun. ACM, \n17(8):453 455, 1974. [25] A. Malkis. Cartesian abstraction and veri.cation of multithreaded programs. \nPhD thesis, University of Freiburg, 2010. [26] L. Mauborgne and X. Rival. Trace partitioning in abstract \ninterpreta\u00adtion based static analyzers. In ESOP, page 520, 2005. [27] A. Min\u00b4e. Static analysis of run-time \nerrors in embedded critical parallel C programs. In ESOP, pages 398 418, 2011. [28] H. Oh, K. Heo, W. \nLee, W. Lee, and K. Yi. Design and implementation of sparse global analyses for C-like languages. In \nPLDI, pages 229 238, 2012. [29] S. Owicki and D. Gries. Verifying properties of parallel programs: an \naxiomatic approach. Commun. ACM, 19:279 285, May 1976. [30] A. Venet. Abstract co.bered domains: application \nto the alias analysis of untyped programs. In SAS, pages 366 382, 1996. [31] D. Weise, R. F. Crew, M. \nErnst, and B. Steensgaard. Value dependence graphs: representation without taxation. In POPL, pages 297 \n310, 1994.  \n\t\t\t", "proc_id": "2429069", "abstract": "<p>The correctness of a sequential program can be shown by the annotation of its control flow graph with inductive assertions. We propose inductive data flow graphs, data flow graphs with incorporated inductive assertions, as the basis of an approach to verifying concurrent programs. An inductive data flow graph accounts for a set of dependencies between program actions in interleaved thread executions, and therefore stands as a representation for the set of concurrent program traces which give rise to these dependencies. The approach first constructs an inductive data flow graph and then checks whether all program traces are represented. The size of the inductive data flow graph is polynomial in the number of data dependencies (in a sense that can be made formal); it does not grow exponentially in the number of threads unless the data dependencies do. The approach shifts the burden of the exponential explosion towards the check whether all program traces are represented, i.e., to a combinatorial problem (over finite graphs).</p>", "authors": [{"name": "Azadeh Farzan", "author_profile_id": "81350568899", "affiliation": "University of Toronto, Toronto, Canada", "person_id": "P3977934", "email_address": "azadeh@cs.toronto.edu", "orcid_id": ""}, {"name": "Zachary Kincaid", "author_profile_id": "81472649374", "affiliation": "University of Toronto, Toronto, Canada", "person_id": "P3977935", "email_address": "zkincaid@cs.toronto.edu", "orcid_id": ""}, {"name": "Andreas Podelski", "author_profile_id": "81100130920", "affiliation": "University of Freiburg, Freiburg, Germany", "person_id": "P3977936", "email_address": "podelski@informatik.uni-freiburg.de", "orcid_id": ""}], "doi_number": "10.1145/2429069.2429086", "year": "2013", "article_id": "2429086", "conference": "POPL", "title": "Inductive data flow graphs", "url": "http://dl.acm.org/citation.cfm?id=2429086"}