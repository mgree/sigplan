{"article_publication_date": "01-23-2013", "fulltext": "\n Linear Dependent Types for Differential Privacy Marco Gaboardi* Andreas Haeberlen* Justin Hsu* Arjun \nNarayan* Benjamin C. Pierce* *University of Pennsylvania Universit `a di Bologna INRIA EPI Focus Abstract \nDifferential privacy offers a way to answer queries about sensitive information while providing strong, \nprovable privacy guarantees, ensuring that the presence or absence of a single individual in the database \nhas a negligible statistical effect on the query s result. Proving that a given query has this property \ninvolves establishing a bound on the query s sensitivity how much its result can change when a single \nrecord is added or removed. A variety of tools have been developed for certifying that a given query \nis differentially private. In one approach, Reed and Pierce [34] proposed a functional programming language, \nFuzz, for writing differentially private queries. Fuzz uses linear types to track sensitivity and a probability \nmonad to express randomized computation; it guarantees that any program with a certain type is differentially \nprivate. Fuzz can successfully verify many useful queries. However, it fails when the sensitivity analysis \ndepends on values that are not known statically. We present DFuzz, an extension of Fuzz with a combination \nof linear indexed types and lightweight dependent types. This com\u00adbination allows a richer sensitivity \nanalysis that is able to certify a larger class of queries as differentially private, including ones \nwhose sensitivity depends on runtime information. As in Fuzz, the differential privacy guarantee follows \ndirectly from the soundness theorem of the type system. We demonstrate the enhanced expres\u00adsivity of \nDFuzz by certifying differential privacy for a broad class of iterative algorithms that could not be \ntyped previously. Categories and Subject Descriptors D.3.2 [Programming Lan\u00adguages]: Language Classi.cations \nSpecialized application lan\u00adguages; F.3.3 [Theory of computation]: Studies of Program Const\u00adructs Type \nstructure General Terms Design, Languages, Theory Keywords differential privacy, type systems, linear \nlogic, depen\u00addent types 1. Introduction An enormous amount of data accumulates in databases every day \ntravel reservations, hospital records, location data, etc. This infor\u00admation could potentially be very \nvaluable, e.g., for scienti.c and Permission to make digital or hard copies of all or part of this work \nfor personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior \nspeci.c permission and/or a fee. POPL 13, January 23 25, 2013, Rome, Italy. Copyright c &#38;#169; 2013 \nACM 978-1-4503-1832-7/13/01. . . $15.00 medical research, but much of it cannot be safely released due \nto privacy concerns. Moreover, aggregation and anonymization are not suf.cient to safeguard privacy: \nrecent experience with the Net\u00ad.ix prize [30], for example, has shown how easy it is to leak sensi\u00adtive \ninformation accidentally, even when the data is carefully sani\u00adtized before it is released. Differential \nprivacy [4, 12, 13] is a promising approach to this problem: it offers strong statistical privacy guarantees \nfor certain types of queries, even in worst-case scenarios. Intuitively, this is accomplished by a) admitting \nonly queries whose result does not depend too much on the data of any single individual, and b) adding \nsome random noise to the result of each query. Thus, if we pick any individual I and remove all of I \ns data from the database before answering a query, the probability that the result is any given value \nv remains almost the same. This limits the amount of information that can be learned about a single individual \nby observing the result of the query. Many speci.c queries have been shown to be differentially private, \nincluding machine learning algorithms such as empirical risk minimization [5] and k-means, combinatorial \noptimization algorithms such as vertex cover and k-medians [18], and many others. But checking that a \ngiven query is differentially private can be both tedious and rather subtle. The key challenge is to \nprove an upper bound on the query s sensitivity, i.e., the maximum change in the query s output that \ncan result from changing the data of a single individual. (Brie.y, higher-sensitivity queries require \nmore noise to maintain privacy.) Since most data analysts are not experts in differential privacy, they \ncannot bene.t from its strong guarantees unless they have access to suitable tools. The approach we focus \non is to provide analysts with a pro\u00adgramming language for differentially private queries: the analyst \ncan formulate queries in this language and then submit them to a special compiler, which determines their \nprivacy cost and rejects them if this cost exceeds a budget that has been speci.ed by the analyst. This \napproach is attractive because differential privacy is compositional; for instance, the privacy cost \nof a sequence of dif\u00adferentially private computations is simply the sum of the individual costs. Thus, \nwe can reason about large, complex queries by manu\u00adally inspecting a few simple primitives and then suitably \ncomposing the analysis results of larger and larger subqueries. This is the basis of previous systems \nlike PINQ [26], which provides a SQL-like lan\u00adguage, Airavat [36], which implements a MapReduce framework, \nand Fuzz [20, 33, 34], a higher-order functional language. The analysis in Fuzz is based on a type system \n[33, 34] that cer\u00adti.es queries as differentially private via two components: numeric annotations at \nthe type level to describe the sensitivity of functions and a probability monad to represent probabilistic \ncomputations. Fuzz can certify many useful queries, but it fails for other impor\u00adtant kinds of queries, \nincluding some fairly simple ones. For in\u00adstance, iterative algorithms such as k-means can only be analyzed \nwhen the number of iterations is a constant, and there are other instances (such as a program that computes \na cumulative distribu\u00adtion function for an arbitrary list of cutoffs) where even the type of the program \ncannot be expressed in Fuzz! Fuzz fails for these programs because their overall sensitivities are not \nsimply a static composition of the sensitivities of subprograms; rather, they depend on some input data, \nsuch as the number of iterations or the list of numeric cutoffs. Numeric sensitivity annotations are \nnot suf.cient to express such dependencies between input data and sensitivity; however, as we show in \nthis paper, these dependencies can be ex\u00adpressed with dependent types. Dependent types enable static \nreasoning about information that will be available to a program only at runtime. But we must make a choice \nat the outset regarding the complexity of the dependencies we wish to support: richer dependencies would \nexpand the range of queries that can be certi.ed as differentially private, but systems with rich dependent \ntypes tend to require extensive program an\u00adnotations, which would make DFuzz more dif.cult to use by \nnon\u00adexperts. At the extreme end of this spectrum would be a system like CertiPriv [3], which can certify \na broad range of differentially private queries but requires the programmer to supply most of the proof. \nTo preserve our goal of usability by non-experts, we instead work near the other end, choosing a lightweight \nform of dependent types that requires few annotations but still yields a powerful anal\u00adysis. We introduce \na language called DFuzz Dependent Fuzz that combines a rich form of type-level sensitivity annotations \nwith lightweight dependent types. The sensitivity annotations we con\u00adsider are arithmetic expressions \nover real numbers. Our dependent types are a simple form of indexed types, where indices describe the \nsize of data types and provide the programmer with dependent pattern matching in the style of Dependent \nML [38]. Sensitivity annotations and dependent types combine well, and the resulting language possesses \nstrong metatheoretic properties. Most impor\u00adtantly, our type system (like that of Fuzz) natively captures \nthe con\u00adcept of differential privacy, since DFuzz enjoys an extension of the metric preservation property \nof [33, 34]. To demonstrate the capa\u00adbilities of DFuzz, we show that it can certify several examples \nfrom the differential privacy literature quite naturally, including Iterative Database Construction [19], \nk-means [4], k-medians [18], and the Cumulative Distribution Function [27]. In summary, we offer the \nfollowing contributions: DFuzz, a core calculus for differential privacy that combines the earlier sensitivity \nanalysis of Reed and Pierce [34] with lightweight dependent types inspired by Dependent ML [38] (Section \n3);  the fundamental metatheory for DFuzz, including an adaptation of the usual basic metatheory substitution, \npreservation and progress as well as a generalization of the metric preservation theorem from [33, 34] \n(Sections 4 and 5); and  four example programs that express algorithms from the dif\u00adferential privacy \nliterature and can be successfully certi.ed by DFuzz (Section 6).  We discuss related work in Section \n7 and future research directions in Section 8. 2. Background and Overview Differential privacy We begin \nby reviewing some basic de.ni\u00adtions. Suppose the private data is collected in a database that con\u00adtains \nrows of the same type, and each individual s data is in a single row. Let db be the type of such databases, \nand assume for the mo\u00adment that we are interested in real-valued queries. Then the key de.nition of differential \nprivacy (based on [13]) is: 2.1 De.nition: A randomized function f : db . R is E\u00addifferentially private \nif, for all possible sets of outputs S . R and for any two databases b, b' that differ in only one row, \nE ' Pr[f(b) . S] = e\u00b7 Pr[f(b) . S]. Intuitively, this means that the presence or absence of one individ\u00adual \ns data has only a small effect on the distribution of f s (ran\u00addomized) outputs. Here E is a privacy \nparameter; the larger E is, the more information about individuals is potentially revealed by the result. \nFor small E, the factor eE can be thought of as 1 + E. We will often refer to E as the privacy cost of \nrunning f. We can extend the de.nition to data types other than db and R if we assign to each data type \na metric that measures the distance ' between values. We write f v ~r v: t to indicate that two values \nv and v' of type t are at most r apart. Thus we obtain: 2.2 De.nition: A randomized function q : t . \ns is E-differentially private if, for all sets S of closed values of type s and for all ' ' v, v: t \nsuch that f v ~r v: t , we have: Er ' P r[q(v) . S] = ePr[q(v) . S] To determine whether a given query \nhas this property, the following de.nition is useful: 2.3 De.nition: A function f : t . s is c-sensitive \nfor c . R=0 if, ' ' for all v, v: t with f v ~r v: t , we have f f(v) ~c\u00b7r f(v') : s. In other words, \na c-sensitive function magni.es changes in its inputs by at most a factor of c. When a function q is \nnot c-sensitive for any c . R=0 in the sense of the above de.nition, we will refer to it as 8-sensitive. \nReal-valued functions with limited sensitivity can be converted into E-differentially private queries \nusing the Laplace mechanism (introduced in [13]). Here, we write L\u00df to denote the Laplace distribution \nwith scaling parameter \u00df. 2.4 Proposition: Let f : db . R be a c-sensitive function, and let q : db . \nR be the randomized function q = .b. f (b) + N, where N is a random variable distributed according to \nLc/E. Then q is E-differentially private. In other words, the Laplace mechanism converts f into a ran\u00addomized \nfunction q by adding noise from the Laplace distribution. Note that the parameter of the distribution \nthe magnitude of the noise depends on both c and E: the stronger the privacy require\u00adment (smaller E) \nand the higher the sensitivity of f (larger c), the more noise must be added to f s result to preserve \nprivacy. Linear types for differential privacy The key idea behind static type systems for differential \nprivacy is to provide a compositional analysis that tracks both the privacy cost E and the sensitivity \nc of a function. For context, we sketch the analysis in Fuzz [33, 34], on which our work is based. Fuzz \nhas a linear type system: functions can have types of the form !rs -t , where the modality !r is annotated \nwith a numeric upper bound r on the function s sensitivity. For instance, the term .x.(2 \u00b7 x) can be \ngiven the type !2R -R to express the fact that f(x) = 2 \u00b7 x is 2-sensitive in its argument x. 1 Fuzz \nchecks these types with standard typing judgments of the form G f e : s, where the type environment G \nadditionally contains hypotheses about sensitivity. For instance, a typing judgment x : !cs f e : t means \nthat e can be given type t if e is at most c-sensitive in the free variable x. 1 In [33, 34] the symbol \n! appears both in types (as here) and in terms (to help guide typechecking algorithms). Here, for simplicity, \nwe elide uses in terms. As an illustration, consider the slightly more complex program (.z.3 \u00b7 z + 8)(5 \n\u00b7 x) It is easy to see that x : !5R f 5 \u00b7 x : R and f .z.3 \u00b7 z + 8 : !3R -R, and Fuzz thus uses the typing \nrule G f e : !rt -s . f g : t G + r. f e g : s to infer x : !15R f (.z.3 \u00b7 z + 8)(5 \u00b7 x) : R. This re.ects \nthe fact that 3 \u00b7 (5 \u00b7 x) + 8 = 15 \u00b7 x + 8 is 15-sensitive in x. To turn this (deterministic) program \ninto a (randomized) differentially private one, Fuzz introduces a probability monad o and assigns monadic \ntypes to randomized computations. Accordingly, the concept of sensitivity is generalized so that the \nprivacy cost of a differentially private function is interpreted as its sensitivity. Speci.cally, the \ntype system guarantees that any program that can be given a type !Edb -oRis differentially private, with \nprivacy cost E. 2 The full Fuzz type system is described in [33, 34], which also show how several practical \nprograms can be successfully type\u00adchecked; one of these is k-means clustering, a machine learning algorithm \nthat takes a list of points and computes centers around which the points cluster. However, as mentioned \nabove, a key limi\u00adtation of Fuzz is that its sensitivity annotations are purely numeric. In the case \nof k-means clustering, this is a major inconvenience: the sensitivity depends on the number of iterations \nof the algorithm, but Fuzz types cannot express the fact that a function s sensitivity in one parameter \nmay depend on the value of another the only way out is to hard-code the number of iterations. For instance, \nin Fuzz a k-means program performing two iterations can be typed as: 2iter-k-means : !8L(R.R) -!6EBag(R.R) \n-O(L(R.R)) This type says that 2iter-k-means is a program that, when provided with an initial list of \ncenters (of type L(R . R)), produces a 6E\u00addifferentially private algorithm that maps a dataset to a new \nlist of centers. A k-means program that performs a different number of iterations would have a different \ntype. Worse, in Section 6, we will see several examples of practical algorithms whose types cannot be \nexpressed in Fuzz, e.g., the IDC algorithm [19], in which the desired sensitivity is itself a parameter. \nThis means that we cannot even add such operations to Fuzz as primitives. In DFuzz, we overcome this \nlimitation by adding dependent types. In particular, we add a lightweight form of dependent types that \nallow us to give the following type to a general algorithm for k-means: k-means : .i, k.( !8N[i]-!8L(R. \nR)[k] -!3iEBag(R. R)-O(L(R. R)[k])) This type gives more precise information about the behavior of k-means: \nit says that k-means is a program that, when provided with a natural number i specifying the number of \niterations (the sensitivity annotation is 8 since this parameter does not need to be kept private) and \na list of k centers, produces a 3iE-differentially private algorithm that maps the dataset to a list \nof k centers. In the next section, we describe DFuzz s dependent types in more detail. 3. DFuzz The main \nnovelty of DFuzz is the way it combines a lightweight form of dependent types, ` a la Dependent ML [38], \nwith a sensi\u00adtivity analysis inspired by the one in [33, 34]. This combination requires several ingredients, \nwhich are formally de.ned in Fig\u00adure 1 (type grammar), Figure 2 (program syntax), Figure 3 (kinding 2 \nIn [34], a .xed value of E was baked into the metric on distributions. In contrast, we use the convention \nfrom [33], where the privacy parameter E is not .xed, and is made explicit in the type annotation. . \n::= . | . (kinds) Z ::= N[S] | L(t )[S] | R[R] | P [S] (precise types) a ::= R| db (basic types) a ::= \n.i : ..t | .i : ..t (quanti.ed types) t ::= a | Z | a | A -t | Ot (types) A ::= !Rt (modal types) S ::= \ni | 0 | S + 1 (size terms) R ::= k | r | S | R + R | R \u00b7 R | 8 (sensitivity terms) F ::= \u00d8 | S = 0 | \nS = i + 1 | F . F (assumptions) C ::= S = S | R = R (constraints) f ::= \u00d8 | f, i : . (kind environments) \nG ::= \u00d8 | G, x : A (type environments) Figure 1. DFuzz Types p ::= (r1, . . . , rn) (probabilities) e, \ng ::= x | .x.e | e e | f | .x x.e (expressions) | 0 | s e | nil | cons[S](e, e) | caseN e of 0 . e | \nx[i] + 1 . e | caseL e of nil . e | cons[i](y, x) . e | .i.e | e[I] | pack(e, I ) as .i : ..s | unpack \ne as (x, i) in e | p | return e | {e, (e1, . . . , en)}| let Ox = e in e v ::= .x.e | f | .x x.e | () \n(values) | 0 | s v| nil | cons[S](v, v) | .i.e | pack(v, I ) as .i : ..s | p | return v | {v, (e1, . \n. . , en)} s ::= do e | {p, (s1, . . . , sn)} (states) f ::= do return v | {p, (f1, . . . , fn)} (.nal \nstates) Figure 2. DFuzz Syntax rules), Figure 4 (subtyping rules), Figures 5, 6, 7 (typing rules), and \nFigure 8 (selected operational semantics rules). We describe each of these components below, omitting \nsome secondary details for brevity. (In particular, we elide some linear data types like ., &#38;, and \n.. A full description is available in [14].) Sizes and sensitivities The special feature of DFuzz types \n(Fig\u00adure 1) is that they can contain size or sensitivity information. This information is described by \nmeans of terms in a small language at the type level. A size term, written S, is a term that conforms \nto the grammar S ::= i | 0 | S + 1 where i is a size variable. Size terms are simple expressions over \nnatural numbers; they are used to describe the size of data types. A sensitivity term, denoted R, is \na term that conforms to the grammar R ::= k | r | S | R + R | R \u00b7 R | 8 where k is a sensitivity variable \nand r is a non-negative real con\u00adstant, i.e., r . R=0. Sensitivity terms are simple expressions that \nconsist of positive real numbers and the symbol 8; they are used to describe program sensitivity. An \nannotation of 8 means that f(i) = . r . R=0 f f S : . (k.0) (k.8) (k.Ax) (k.R) (k. + 1) f f 0 : . f \nf 8 : . f f i : . f f r : . f f S + 1 : . f f R1 : . f f R2 : . f f R1 : . f f R2 : . f f S : . (k.+) \n(k.\u00b7) (k.<:) f f R1 + R2 : . f f R1 \u00b7 R2 : . f f S : . Figure 3. Kinding rules f; F |= s1 s2 f; F |= \ns2 s3 f; F |= s3 s1 f; F |= s2 s4 (st.R) (st.T ) (st.-) f; F |= s s f; F |= s1 s3 f; F |= s1 -s2 s3 -s4 \nf, i : .; F |= s1 s2 i /. FSV(F) f; F |= R2 = R1 f; F |= s1 s2 (st..) (st.!) f; F |= .i : ..s1 .i : \n..s2 f; F |= !R1 s1 !R2 s2 f, i : .; F |= s1 s2 i /. FSV(F) f; F |= S = S ' f; F |= S = S ' f; F |= s \nt (st..) (st.N) (st.L(s)) f; F |= .i : ..s1 .i : ..s2 f; F |= N[S] N[S ' ] f; F |= L(s)[S] L(t)[S ' ] \nFigure 4. Subtyping rules f; F; G, x : !1s f x : s (Ax) f; F; G f e : s f; F |= s t f; F; G f e : t ( \n.R) f; F; G f e : s f; F |= . G f; F; . f e : s ( .L) f; F; G, x : !8s f e : s f; F; 8 \u00b7 G f .x x.e : \ns (.x) f; F; G, x : !Rs f e : t f; F; G f .x.e : !Rs -t (-I) f; F; G f e : !Rs -t f; F; . f g : s f; \nF; G + R \u00b7 . f e g : t (-E) Figure 5. Core Typing Rules f; F; G f e : N[S] f; F; G f e : s f; F; . f \ng : L(s)[S] (s) (n) (c) f; F; G f s e : N[S + 1] f; F; G f nil : L(s)[0] f; F; G + . f cons[S](e, g) \n: L(s)[S + 1] f; F; G f e : N[S] f; F . S = 0; . f g1 : s f, i : .; F . S = i + 1; ., x : !RN[i] f g2 \n: s (0) (case)N f; F; G f 0 : N[0] f; F; . + R \u00b7 G f caseN e of 0 . g1 | x[i] + 1 . g2 : s f; F; G f \ne : L(t )[S] f; F . S = 0; . f g1 : s f, i : .; F . S = i + 1; ., x : !RL(t)[i], y : !Rt f g2 : s (case)L(t) \n f; F; . + R \u00b7 G f caseL e of nil . g1 | cons(y, x[i]) . g2 : s f, i : .; F; G f e : t i /. FSV(F; G) \nf; F; G f e : t{I /i} f f I : . (.I) (.I) f; F; G f .i.e : .i : ..t f; F; G f pack(e, I ) as .i : ..t \n: .i : ..t f; F; G f e : .i : ..t f f I : . f; F; G f e : .i : ..t f, i : .; F; ., x : !Rt f g : s i \n/. FSV(F; .; s; R) (.E) (.E) f; F; G f e[I] : t{I /i} f; F; . + R \u00b7 G f unpack e as (x, i) in g : s \nf; F imply f R-sensitive s . t (Ext) f; F; G f f : !Rs -t Figure 6. Data Types and Polymorphism Typing \nRules f; F; G f e : s f; F; G f e : Os f; F; ., x : !8s f g : Ot (OI) (OE) (P) f; F; 8G f return e : \nOs f; F; G + . f let Ox = e in g : Ot f; F; G f (r1, . . . , rn) : P[n] f; F; G f e : P[n] f; F; . f \nei : Os (.i) f; F; G f p : P[n] f; F; . f si : s (.i) f; F; G f e : Os ({e}) ({s}) (do) f; F; G + . f \n{e, (e1, . . . , en)} : Os f; F; G + . f {p, (s1, . . . , sn)} : s f; F; G f do e : s Figure 7. Probability \nLayer Typing Rules there is no guaranteed bound on the sensitivity. The binary opera\u00adtors + and \u00b7 are \nthe symbolic counterparts on sensitivities of sum and product on values. Their precise meaning is described \nbelow. We will use the metavariable I to range over sizes and sensitivities. Any size term is also a \nsensitivity term. This is important for expressing dependencies between sizes (for example, number of \niterations) and sensitivities (for example, privacy cost). To ensure the correct behavior of size and \nsensitivity terms, and to prevent undesired substitutions, we consider size and sensitivity terms to \nbe typed as well; to avoid confusion, we refer to the types for size and sensitivity terms as kinds. \nDFuzz uses two kinds: . for size annotations and . for sensitivity annotations. Kinds are assigned to \nterms via kind judgments of the form f f I : . where f is a kind environment, i.e., a set of kind assignments \nto size and sensitivity variables. The rules for deriving the judgment f f I : . are presented in Figure \n3. Notice that we have a subkind relation on terms that is induced by the rule (k. <:). This relation \nallows us to consider size terms as sensitivity terms. For notational convenience, we will sometime write \nsizes and sensitivities terms without making explicit their kind. We can interpret size terms over the \ndomain N of natural num\u00adbers and sensitivity terms over the domain R=0 . {8} of non\u00adnegative extended \nreal numbers. To interpret expressions with free variables we need, as usual, a notion of assignment \nthat is, a map\u00adping . from size variables to elements of N and from sensitivity variables to elements \nof R=0 . {8}. We write dom(.) for the do\u00admain of .; this is a set of variable-kind mappings of the shape \ni : . where each variable i appears at most once. Given a term f f I : . and an assignment . such that \nf . dom(.), we inductively de.ne the interpretation [I].: [0]. = 0  [i]. = .(i)  [S + 1]. = [S]. + \n1  = r [r].  [R1 + R2]. = [R1]. + [R2].  [R1 \u00b7 R2]. = [R1]. \u00b7 [R2].  = 8 [8]. where + and \u00b7 are the \nusual sum and product extended to 8: 8 + r = r + 8 = 8 for every r . R=0 . {8}, 8 \u00b7 r = r \u00b7 8 = 8 for \nevery r = 0, and 8 \u00b7 0 = 0 \u00b7 8 = 0 (Note that this de.nition implicitly coerces size terms natural numbers \ninto real numbers as needed.) The well-de.nedness of the interpretation is ensured by the following lemma: \n3.1 Lemma: Let f f I : . and f . dom(.). Then: If . = ., then [I]. . N.  If . = ., then [I]. . R=0 \n. {8}.  Proof: By induction on the derivation proving f f S : .. D Data types and dependent pattern \nmatching Our syntax of types offers just two representative algebraic data types: natural numbers N[S] \nand lists L(s)[S] with elements of type s (see Figure 1). We could have gone further and included general \nalgebraic datatype declarations and pattern matching, but this would add to the com\u00adplexity of the notation \nwithout (we believe) raising new conceptual issues; we leave this generalization to future work. Intuitively, \nthe size of a natural number corresponds to its value (we consider a unary encoding of the natural numbers, \nand we assume the natural number 0 to be of size 0), and the size of a list corresponds to the number \nof elements in it (we assume nil to be of size 0). Types like N[S] and L(s)[S] are inhabited only by \nterms whose evaluation produces a value of size S (if it terminates). The approach is reminiscent of \nHayashi s singleton types [21]. To illustrate why size terms are helpful for expressiveness, let us consider \nthe list data type in more detail. Values of the list type are built through the constructors nil and \ncons[S](e, g) (see Figure 2), where cons carries the size S of the expression g as additional information. \nThis information is used to support a simple form of dependent pattern matching: the caseL destructor \ncan, in addition to the usual term-level pattern matching, also perform type-level pattern matching. \nThis is described by the following reduction rule (see Figure 8) caseL cons[S](e1, e2) of nil . g1 | \ncons[i](y, x) . g2 . g2{e1/y}{e2/x}{S/i} in which the size S is propagated to the term g2. In a similar \nway, in the reduction rule (Op-Cases) in Figure 8 for dependent pattern matching on N[S], the information \nn is propagated to the term g2. The data type elimination rules in Figure 6 are crucial to making this \ntechnique work. For instance, the elimination rule for the list data type f; F; G f e : L(t)[S] f; F \n. S = 0; . f g1 : s f, i : .; F . S = i + 1; ., x : !RL(t )[i], y : !Rt f g2 : s f; F; . + R \u00b7 G f caseL \ne of nil . g1 | cons[i](y, x) . g2 : s adds assumptions (explained in detail below) to the contexts used \nto typecheck the two branches of the case construct, re.ecting information gleaned by the case about \nthe size S of the test value e. In the .rst branch, it is assumed that S = 0; this extra piece of information \nenables us to remember, at the typing judgment level, that e has been matched to nil. Conversely, in \nthe second branch, it is assumed that S = i + 1, giving us a name i for the size of the tail of the list \ne, which appears in the type assumed for the pattern variable x. A similar mechanism is used in the elimination \nrule for the N[S] data type. Assumptions and constraints More formally, a judgment f; F; G f e : s contains, \nbesides the usual environment G for term variables and the kind environment f, an extra parameter F that \nrecords the assumptions under which the typing is obtained. Intuitively, the expression e can be assigned \nthe type s for any value of its free size variables satisfying F. Since assumptions F record the size \nconstraints generated by the pattern matching rules, we consider assumptions that conform to the following \ngrammar: F ::= \u00d8 | S = 0 | S = i + 1 | F . F An assumption F is well de.ned when it is associated with \na kind environment f that determines the kinds of the free size variables in F. We write f f F in this \ncase. Given an assumption f f F and an assignment . such that f . dom(.), the interpretation [F]. is \nde.ned inductively as follows: = true [\u00d8].  [S = 0]. = ([S].  = 0) [S = i + 1]. = ([S]. = [i]. + \n1)  [F1 . F2]. = [F1]. . [F2].  where = is equality on natural numbers. The use of assumptions is \ncrucial for the subtyping relation (Figure 4). Indeed, subtyping judgments involve judgments of the shape \n f; F |= C where C is a constraint of the form R = R ' or S = S ' . The mean\u00ading of a constraint C also \nfollows directly from the interpretation of the size and sensitivity terms it contains. Given an assignment \n. for the size and sensitivity variables appearing in C, the interpretation [C]. is de.ned inductively \nas R = R ' ]. = ([R]. = [R ' ].) [ S = S ' ]. = ([S]. = [S ' ].)  where = is the expected extension \nof = to cover 8. The judgment f; F |= C asserts that the constraint C is a logical consequences of the \nassumption F, i.e. for every . such that [F]. = true we have = true. Analogously, we will also use the \nnotation f; F |= [C]. . to denote the fact that the assumption . is a logical consequence of the assumption \nF. We will see how these judgments are used later when we present the subtyping rules in detail. Generalized \nsensitivities and the scaling modality Sensitivity terms are the key ingredients in the sensitivity analysis. \nThey ap\u00adpear as decorations of the scaling modality !R. The scaling modal\u00adity is used to de.ne types \nwith the shape !Rs, which are used in turn in function types and in term environments. Suppose that a \ngiven expression e can be assigned the function type !Rs -t : f; F; G f e : !Rs -t Then we know that \ne is a function that maps elements of type s to elements of type t as long as F is satis.ed, and that \nin this case R is an upper bound on the function s sensitivity. Two points merit further explanation. \nFirst, a static analysis cannot always describe the exact sensitivity of a program; we are always dealing \nwith conservative upper bounds. Second, we say that R describes this upper bound because it is not necessarily \na number: it can depend on size or sensitivity variables from f, which can themselves be involved in \nassumptions from F. This dependency is central to the expressivity of DFuzz. Modal types also appear \nin type environments. A typing judg\u00adment of the form f; F; x1 : !R1 s1, . . . , xn : !Rn sn f e : s says \nthat the expression e is a function whose sensitivity in the ith argument is described by the sensitivity \nterm Ri. Notice that this means that every function type comes with a sensitivity term associated to \nits input type. Indeed, here unlike [33, 34] we are implicitly using the standard linear logic decomposition \nof the intuitionistic implication s . t = !s -t and the observation that it suf.ces to have modal types \nappearing in negative position. For convenience, we will similarly use the notation s . t as a shorthand \nfor non-sensitive functions, i.e., functions of a type !Rs -t where [R] = 8. Modal types can be combined \nby arithmetic operations !Rs+ !T s = !R+T s and T \u00b7 !Rs = !T \u00b7Rs and these operations are lifted to type \nenvironments. The sum of two contexts is de.ned inductively as: \u00d8 + \u00d8 = \u00d8  (G, x : A) + (., x : B) \n= (G + .), x : A + B  (G, x : A) + . = G + (., x : A) = (G + .), x : A, if x . dom(G, .)  The product \nof a type environment with a sensitivity term T is: T \u00b7 \u00d8 = \u00d8  T \u00b7 (G, x : A) = T \u00b7 G, x : T \u00b7 A  \n It is worth emphasizing that the sum G + . of two type en\u00advironments G and . is de.ned only in the case \nthat for each x . dom(G)ndom(.) the type A+B is de.ned, where x : A . G and x : B . .. This in turn requires \nthat there is a type s such that A = !R1 s and B = !R2 s. In all the binary rules in Figure 5 we implicitly \nassume this condition. A sensitivity term can also appear as the annotation of a pre\u00adcise type R[R]. \nThis class of types is close in spirit to the data types N[S] and L(s)[S], since it classi.es terms whose \nvalue is described by R. However, we have no destructor operation for this kind of type. We will see \nin Section 5.4 that these types can be used to dynamically specify the sensitivity of certain operations. \nSubtyping and quanti.ers The assumptions introduced by pat\u00adtern matching play a crucial role in the subtyping \nrelation , which is de.ned by the rules in Figure 4. A subtyping judgment has the shape f; F f s t where \nf is a kind environment and F is an assumption. Intuitively, the subtyping relation f; F f s t says that \n(1) s and t are equal up to their decorations with size and sensitivity terms, and (2) the decoration \nof t is more permissive than the decoration of s under the assumption F. One main purpose of the subtyping \nrelation is to capture the fact that an R-sensitive function is also R '-sensitive for R = R ' . This \nis ensured by the rule for the scaling modality: f; F |= R2 = R1 f; F |= s1 s2 (st.!) f; F |= !R1 s1 \n!R2 s2 Indeed, the combination of this rule and the rule for function types f; F |= s3 s1 f; F |= s2 \ns4 (st.-) f; F |= s1 -s2 s3 -s4 (which, as usual, is contravariant in its .rst argument) ensures that \nf; F f !Rs -t !RI s -t iff f; F |= R = R ' . In other words, if we can prove that R = R ' under assumption \nF, then every expression that can be given the type !Rs -t can also be given the type !RI s -t. Subtyping \nis also needed for putting the dependent pattern matching mechanism to work, to unify the contexts and \nthe types in the two branches of a case rule. This is obtained by using sub\u00adtyping judgments on data \ntypes. For instance, for natural numbers we have a rule f; F |= S = S ' (st.N) f; F |= N[S] N[S ' ] \nthat allows us to obtain the type N[S ' ] from the type N[S] if we have f; F |= S = S ' . Subtyping can \nbe applied by using the rule ( .R) in Figure 5. Moreover, it can be extended to type environments: the \njudgment f; F |= G . holds if for every x : A . . we have x : B . G and f; F |= B A. Note that the type \nenvironment G can contain variable assignments for variables that do not appear in .. Thus the rule ( \n.L) in Figure 5 can also be seen as a kind of weakening. In order to be able to express the size and \nsensitivity dependen\u00adcies in a uniform way, , we additionally enrich the type language with a universal \nquanti.er . and an existential quanti.er . over size and sensitivity variables (Figure 6). For instance, \nthe type .i : ..(N[i] -!iR -R) can be assigned to a program that, given a value of type N[S], returns \nan S-sensitive function of type !SR -R, for any size term f S : .. Similarly, the type .i : ..(N[i] -.k \n: ..( !j R -R))  abstracts the sensitivity of the function that is returned when a term of this type \nis provided with a value of type N[S]. Finally, the type .i : ..N[i] is the type of the ordinary natural \nnumbers. The probability layer To ensure differential privacy, we need to be able to write probabilistic \ncomputations. This is achieved in DFuzz by considering a grammar of programs with two layers (Fig\u00adure \n2) . We have an expression layer that contains the constructors and destructors for each data type, and \nan additional probability state layer that contains the program constructions for describing probabilities \nover expressions and values. To mediate between the two layers, we use a monadic type Ot, similar to \nthe one found in Ramsey and Pfeffer s stochastic lambda calculus [32]. In order to describe probabilistic \nstates, we need to have prob\u00adability vectors p, i.e., lists of real numbers from the interval [0, 1] \nthat sum up to 1. Probability vectors can be typed by means of a precise type of the shape P[S], where \nthe size term S describes the length of the vector. Though S can range over variables, the typing rules \nfor vectors only use types P[n] indexed by a constant natural number n. A probabilistic state s then \nis either an object of the shape {p, (s1, . . . , sn)} where p is a probabilistic vector, or an object \nof the shape do e. Intuitively, the former associates a discrete probability distribution, described \nby p, to a list of prob\u00adabilistic states (s1, . . . , sn), whereas the latter turns an expression into \na probabilistic state. Probabilistic states represent probabilistic computations in the sense that their \nevaluation results in .nal states that assign probabilities to values. For an example, the state {(0.2, \n0.8), (do return 1, {(0.5, 0.5), (do return 2, do return 3)})} is a .nal state in which the value 1 is \nreturned with probability 0.2 and the values 2 and 3 are each returned with probability 0.8 \u00b7 0.5 = \n0.4. We use three monadic expression forms to ensure that only expressions representing probabilistic \ncomputations can be turned into probabilistic states. The expression return e can be seen as representing \nthe distribution that deterministically yields e; the monadic sequencing let Ox = e in e ' can be seen \nas a program that draws a sample x from the probabilistic computation e and then continues with the computation \ne '; and the explicit probabil\u00adity construction {e, (e1, . . . , en)} associates an expression e rep\u00adresenting \na probability vector with a list of random computations (e1, . . . , en). The typing rules (Figure 7) \nensure that we consider only well-formed expressions. In particular, the rule f; F; G f e : P[n] f; F; \n. f ei : Os (.i) ({e}) f; F; G + . f {e, (e1, . . . , en)} : Os ensures that (1) e represents a probability \nvector, and (2) every ei is a probabilistic computation. Notice also that this rule is responsible for \nthe well-formedness of the probabilistic states. It ensures that we associate lists of expressions of \nlength n only with probability vectors of the same length. This is the reason for introducing the precise \ntype P[n]. As a last remark, notice that the different components of a state represent independent computations, \nso, even though evaluation of expressions is de.ned sequentially, we de.ne evaluation on states in parallel. \nSensitivity and primitive operations One last component that is necessary to make the framework practical \nis a way to add trusted primitive operations, i.e., operations that are known to preserve the properties \nof the type system. This is intuitively the meaning of the following typing rule from Figure 6: f; F \nimply f R-sensitive s . t (Ext) f; F; G f f : !Rs -t which says that we can add to DFuzz any primitive \noperation f of type !Rs -t as long as we know that this operation represents a mathematical function \nf that maps values in the type s to values in the type t and that is R-sensitive under the assumption \nF. At the operational level, the evaluation rules for the primitive operation f must respect the behavior \nof the function f. This is obtained by means of the two following rules (Figure 8): e . e ' (Op-Ext-Tr) \n(Op-Ext) f e . f e ' f v . f(v) We can also extend DFuzz with additional primitive types, as long as \nthey are equipped with a metric that respects the properties we describe in Section 5. 4. Basic Metatheory \nIn this section, we develop fundamental properties of DFuzz. In order to show the usual properties one \nwould expect from our programming language type preservation and progress we ad\u00additionally need to prove \nsome properties that are particular to our use of size and sensitivity annotations. These will also be \nuseful to prove the Metric Preservation Theorem in the next section. We give just proof sketches; full \nproofs are in [14]. 4.1 Properties of Sizes and Sensitivities As we saw earlier, typing judgments in \nDFuzz have the form f; F; G f e : s i.e., they are indexed by a set of kind assignments to index variables \nf and an assumption F. Intuitively, the fact that the subtyping can prove statements using the assumptions \nin F is what makes the dependent pattern matching work: it enables us to recover the same type from the \ndifferent branches. Here, we prove some properties of the typing with respect to the assumption F. The \n.rst says that strengthening the assumption preserves the typing. 4.1.1 Lemma [Assumption Strengthening]: \nSuppose that f; . |= F. Then, we have: 1. If f; F |= C, then f; . |= C. 2. If f; F |= s t , then f; \n. |= s t.  3. If f; F; G f e : s, then f; .; G f e : s. 4. If f; F; G f s : s, then f; .; G f s : s. \n  Proof: (1) follows directly from the transitivity of the logical implication; (2) follows by induction \non the derivation proving f; F |= s t , using Point 1 when needed. (3) follows by induction on the derivation \nproving f; F; G f e : s, using Point 2 when needed. (4) follows by induction on the derivation proving \nf; F; G f s : s, using Point 3 when needed. D The environment f can contain free size and sensitivity \nvariables; these can be thought of as placeholders for any size or sensitivity index term, and they can \nbe instantiated with a concrete index term when necessary. This is captured by the next lemma: 4.1.2 \nLemma [Instantiation]: 1. If f, i : .; F |= C, then, for every f f I : ., we have f; F{I /i} |= C {I \n/i}. 2. If f, i : .; F |= s t, then, for every f f I : ., we have f; F{I /i} |= s{I /i} t {I /i}.  \n (Op-Case0) (Op-Case) (caseN 0 of 0 . e1 | x[i] + 1 . e2) . e1 (caseN s n of 0 . e1 | x[i] . e2) . e2{n/x}{n/i} \ns e . e ' (Op-Case-List-Tr) (caseL e of nil . e1 | cons[i](x, y) . e2) . (caseL e ' of nil . e1 | cons[i](x, \ny) . e2) e . e ' (Op-Ext-Tr) (Op-Casenil ) (Op-Ext) f e . f e ' (caseL nil of nil . e1 | cons(x, y[i]) \n. e2) . e1 f v . f(v) (Op-Casec) (Op-.x.) (caseL cons[S](v, w) of e1 | cons(x, y[i]).e2) . e2{v/x}{w/y}{S/i} \n(.x x.e)v . e{.x x.e/x}v e . e ' (Op-.x.) (Op-.-Tr-1) (.x x.e)[I] . e{.x x.e/x}[I] unpack e as (x, i) \nin g . unpack e ' as (x, i) in g e . e ' (Op-.-Tr-2) pack(e, I ) as .i : ..s . pack(e ' , I ) as .i \n: ..s (Op-.) unpack (pack(v, I ) as .i : ..s) as (x, i) in e . e{I /i}{v/x} e . e ' e . e ' (Op-Return-Tr) \n(Op-P-Exp-Tr) return e . return e ' {e, (e1, . . . , en)} . {e ' , (e1, . . . , en)} e . e ' (Op-O-Tr) \n(Op-O-Return-Tr) let Ox = e in e0 . let Ox = e ' in e0 let Ox = return v in e ' . e ' {v/x} (Op-O) (Op-Do) \n'' let Ox = {p, (ei)i.1...n} in e . {p, (let Ox = ei in e )i.1...n} do {p, (ei)i.1...n} . {p, (do ei)i.1...n}' \n\u00d8 = I . 1 . . . n si . si sj = sj = fj (.i . I , j . I) e . e ' ' (Op-Do-Tr) (Op-P-State-Tr) ' ' do e \n. do e {p, (si)i.1...n} . {p, (si)i.1...n} Figure 8. Selected Evaluation Rules 3. If f, i : .; F; G f \ne : s, then, for every f f I : ., we have f; F{I /i}; G{I /i} f e{I /i} : s{I /i}. 4. If f, i : .; F; \nG f s : s, then, for every f f I : ., we have f; F{I /i}; G{I /i} f s{I /i} : s{I /i}.  Proof: (1) follows \ndirectly from the de.nition of constraint satis\u00ad.ability; (2) follows by induction on the derivation \nproving f, i : .; F |= s t , using Point 1 when needed. (3) follows by induc\u00adtion on the derivation proving \nf, i : .; F; G f e : s, using Point 2 when needed. (4) follows by induction on the derivation proving \nf, i : .; F; G f s : s, using Point 3 when needed. D  4.2 Type Soundness and Type Preservation Using \nthe properties on size and sensitivities annotations detailed in the previous section, we are now ready \nto prove the usual prop\u00aderties we want a programming language to enjoy: substitution, type preservation, \nand progress. 4.2.1 Theorem [Substitution]: If f; F; G, x : !Rt f e : s and f; F; . f g : t , then f; \nF; G + R \u00b7 . f e{g/x} : s. Proof: By induction on the derivation proving f; F; G, x : !Rt f e : s. D \nThe proof of the Substitution Lemma is straightforward, except for the management of the term variable \nenvironments. The proof of Type Preservation, however, is not as straightforward because it requires \nmanaging the constraints and the size and sensitivity annotations in various places. 4.2.2 Theorem [Type \nPreservation]: 1. If f e : s and e . e ', then f e ' : s. 2. If f s : s and s . s ', then f s ' : s. \n Proof: Part (1) proceeds by induction on the derivation proving f e : s and case analysis on the possible \nderivations for e . e ' , using the Substitution Lemma (4.2.1). Lemmas (4.1.1) and (4.1.2) are needed \nwhen the step taken comes from a dependent pattern matching rule. Part (2) now follows by induction on \nthe derivation proving f s : s and case analysis on the possible derivations for s . s ', using Point \n1 when needed. D We can also prove Progress as usual: 4.2.3 Theorem [Progress]: 1. If f e : s, then \neither e . e ' , or e is a value. 2. If f s : s, then either s . s ' , or s is .nal.  Proof: Both \nparts proceed by induction on the given derivation, using part (1) as needed in part (2). D 5. Metric \nPreservation and E-Differential Privacy The design of the DFuzz type system is intimately related to \nthe metric relation we present in this section. This connection is cap\u00adtured by the Metric Preservation \nTheorem (5.2.7), which states that the evaluations of two well typed expressions at a given distance \nresult in two values at the same distance or less. 5.1 Metric Relations To formalize the notion of sensitivity, \nwe need a metric relation on programs and states that captures an appropriate notion of infor\u00admation \ndistance for each type. For this purpose, we .rst introduce a metric relation ~r on values and .nal states, \nand then extend it to a metric relation r on expressions and states through substitutions of related \nvalues. Concretely, we begin with metric judgments on values and .nal states f v ~r v ' : s f f ~r f \n' : s asserting, respectively, that values v, v ' and .nal states f, f ' are related at type s and that \nthey are no more than r apart, where r . R=0 . Using these metric judgments, we can also relate substitutions \nof values for variables in an environment G. First, we need some notation. Let G. be the environment \nobtained from G as follows: G. = {xi : si | xi : !Ri si . G} Suppose that . is a vector of positive reals \nindexed by variables in dom(G), i.e. . = (x1 := r1, . . . , xn := rn). Then, we also de.ne a metric judgment \nwith shape: f d ~. d ' : G. This asserts that the substitutions d and d ' of values for the variables \nin dom(G) are related at the types described by G., and that they are no more than . apart. That is, \nfor every value vi = d(xi) and v ' = d ' (xi) we have f vi ~.(xi) v ' i i. Finally, we have judgments \nfor expressions and states: G f e r e ' : s G f s r s ' : s These assert that the expressions e, e ' \nand states s, s ' are related at the type s, and that they are no more then r apart, in the type environment \nG. Given an environment G = (x1 : !R1 s1, . . . , xn : !Rn sn) and a variable-indexed vector of positive \nreals . = (x1 := i r1, . . . , xn := rn), we de.ne .[G] as n ri \u00b7 [Ri]. By de.\u00ad i=1 nition, .[G] can \nassume values in R=0 . {8}. In what follows, we will be particularly interested in the cases where .[G] \nis .nite. All these metric judgments are de.ned inductively by the rules in Figure 9 where |b1Lb2| is \nthe size of the symmetric difference between the two databases b1 and b2. It is worth noting that the \nmetric on expressions considers only expressions that are typable with no constraints and no free size \nvariables. This ensures that the index r in the relations ~r and r is actually a value r . R=0 . The \nmetric presented here, like the one used in [33], differs signi.cantly from the one presented in [34] \nin its treatment of non\u00advalue expressions and states. In particular, we do not require the relation on \nexpressions to be closed under reduction. This makes the proof of metric preservation easier, and, as \nwe will see, it is suf.cient to ensure differential privacy.  5.2 Metric Preservation The Metric Preservation \nTheorem (5.2.7), which we present at the end of this section, can be seen as an extension of the Type \nPreservation Theorem (4.2.2). We can read it as asserting that the evaluation of expressions and states \npreserves not only their types but also the distances between related input values, up to a constant \nfactor given by the metric relation. The proof of the metric preservation theorem involves .ve ma\u00adjor \nsteps. The goal of these steps is to ensure that the different met\u00adric relations respect the properties \nof the type system. The .rst step is to show that the metric on expressions and states internalizes a \nsort of weakening: 5.2.1 Lemma: 1. If . f e r e ' : s and r = p, then . f e p e ' : s. 2. If . f s r \ns ' : s and r = p, then . f s p s ' : s.  Proof: The proof of (1) is by inversion on the rule proving \n. f ' '' e r e : s, using the fact that, if f v ~rI v : t and r = p ', then f v ~pI v ' : t . The base \ncase of (2) follows from (1); the inductive case follows directly by induction hypothesis. D The second \nstep is to show that the metric relation is well behaved with respect to the subtyping relation. This \nis formalized by the following lemma.  5.2.2 Lemma [Subtyping on metrics]: 1. If f e r e ' : s, and \n\u00d8; \u00d8 f s t , then f e r e ' : t . 2. If f s r s ' : s, and \u00d8; \u00d8 f s t , then f s r s ' : t . Proof: \n(1) by induction on the derivation proving f e r e ' : s; (2) by induction on the derivation proving \nf s r s ' : s. D The third technical lemma is an intermediate step to show that the two metric relations \n~r and r coincide on expressions and states that happen to be values and .nal states, respectively. \n5.2.3 Lemma: 1. Suppose \u00d8; \u00d8; G f e : t . If f d1 ~. d2 : G. and d1e is a value, then d2e must also be \na value, and f d1e ~.[G] d2e : t . 2. Suppose \u00d8; \u00d8; G f s : t. If f d1 ~. d2 : G. and d1s is .nal, then \nd2s must also be .nal, and f d1s ~.[G] d2s : t .  5.2.4 Corollary: 1. f v r v ' : t iff f v ~r v ' \n: t . 2. f f r f ' : t iff f f ~r f ' : t.  The last important property that the metric inherits from \nthe DFuzz type system is a substitution property on the judgments involving the relation r. 5.2.5 Lemma \n[Substitution into ]: The following rule is admis\u00adsible: '' ' .1 f e1 r1 e1 : t1 .2, x : !Rt1 f e2 r2 \ne2 : t2 R \u00b7 .1 + .2 f e2{e1/x} r1[R]+r2 e2 ' {e1 ' /x} : t2 Combining these four results, we can prove \nthe main lemma:  5.2.6 Lemma [Metric Compatibility]: Suppose f d ~. d ' : G. such that .[G] . R=0. Then: \n' d ' ' 1. If \u00d8; \u00d8; G f e : s and de . g, then .g . e . g and f g .[G] g ' : s. 2. If \u00d8; \u00d8; G f s : s \nand ds . sf , then .sf ' . d ' s . s ' f and f sf .[G] sf ' : s. Proof: By induction on the typing derivation \nproving \u00d8; \u00d8; G f e : s and \u00d8; \u00d8; G f f : s, respectively, with further case analysis on the evaluation \nstep taken and using Corollary (5.2.4) and the Substitution Lemma (5.2.5). D The Metric Compatibility \nlemma is the main ingredient we need to prove that well-typed programs map related input values to related \noutput values: 5.2.7 Theorem [Metric Preservation]: ' ''' 1. If f e r e : s and e . ef , then .ef . \ne . ef and f ef r e ' f : s. ' ''' 2. If f s r s : s and s . sf , then .sf . s . sf and f sf r sf ' : \ns. Proof: (1) By inversion on the rule proving the judgment f e r e ' : s, using the Metric Compatibility \nLemma (5.2.6); (2) by induction on the derivation proving the judgment f s r s ' : s, again using Lemma \n(5.2.6). D We can then make a corresponding statement about the com\u00adplete evaluation .. is . of two expressions \nor two states, where .the re.exive, transitive closure of the step relation .: 5.2.8 Theorem [Big-Step \nMetric Preservation]: 1. If f e r e ' : s and e . v, then there exists v ' such that . '' ' e .and f \nv ~r v : s. . v 2. If f s r s ' : s and s Y ' such that . f, then there exists f '' ' e .and f f ~r f \n: s. . f  \u00d8; \u00d8; \u00d8 f v : s \u00d8; \u00d8; \u00d8 f f : s \u00d8; \u00d8; \u00d8 f n : N[S] \u00d8; \u00d8; \u00d8 f r : R[R] \u00d8; \u00d8; \u00d8 f nil : L(s)[0] \nf v ~0 v : s f f ~0 f : s f n ~0 n : N[S] f r ~0 r : R[R] f nil ~0 nil : L(s)[0] f v1 ~r1 v ' 1 : s f \nv2 ~r2 v ' 2 : L(s)[S] ' ' f v ~r v ' : s ' r = r ' |r1 - r2| = r |b1Lb2| = r f cons[S](v1, v2) ~r1+r2 \ncons[S](v1, v 2) : L(s)[S + 1] f v ~rI v : s f r1 ~r r2 : R f b1 ~r b2 : db x : !Rs f e r e ' : t x : \n!8s f e r e ' : s f v ~r v ' : s{I /i} f .x.e ~r .x.e ' : !Rs -t f .x x.e ~r .x x.e ' : s f pack(v, I \n) as .i : ..s ~r pack(v ' , I ) as .i : ..s : .i : ..s . f R : . f v{R/i} ~r v ' {R/i} : s{R/i} | ln(ri/ri \n' )| = s (.i) ' ' f .i.v ~r .i.v ' : .i : ..s f (r1, . . . , rn) ~s (r1, . . . , r n) : P[n] '' '' f \np ~r p : P[n] f fi ~s fi : t (.i) f p ~r p : P[n] f ei s ei : Ot (.i) ''' ''' f {p, (f1, . . . , fn)} \n~r+s {p , (f1, . . . , f n)} : t f {p, (e1, . . . , en)} ~r+s {p , (e1, . . . , e n)} : Ot ' f p ~r \np ' : P[n] G f si s si ' : t (.i) G f e r e : Ot f d1 ~. d2 : G. \u00d8; \u00d8; G, . f e : s ''' ' G f {p, (s1, \n. . . , sn)} r+s {p , (s1, . . . , s n)} : t G f do e r do e : t . f d1e .[G] d2e : s f vi ~ri vi ' : \nsi (.i) {v1/x1, \u00b7 \u00b7 1 ' /x1, \u00b7 \u00b7 \u00b7 , v ' /xn} : (x1 : s1, . . . , xn : sn) \u00b7 , vn/xn} ~(x1:=r1,\u00b7\u00b7\u00b7 ,xn:=rn) \n{v n Figure 9. Metric Rules  5.3 Well-Typed Programs are E-Differentially Private The Big-Step Metric \nPreservation Theorem (5.2.8) ensures that programs map related inputs to related outputs. Combined with \nthe properties of the probability layer, this allow us to show that well\u00adtyped programs are differentially \nprivate. To formalize this, we need to de.ne the probability Prf [v] that a .nal state f yields a value \nv. We recursively de.ne: 1 if v = v ' Prdo return vI [v] = 0 otherwise n Pr{(p1,...,pn),(f1,...,fn)}[v] \n= pi Prfi [v] i=1 Note that, by the typing rule for probabilistic states ({s}), the tuples (p1, . . . \n, pn), (f1, . . . , fn) must have the same length. The metric on probability distributions is carefully \nchosen so that the metric on .nal states corresponds to the relation on probability distributions needed \nin the de.nition of differential privacy. 5.3.1 Lemma: Let f f : s and f f ' : s be two closed .nal states \nsuch that f ~r f ' : s for some r . R=0. Then, for every value f v : s, Prf [v] = e rPrfI [v]. Thus, \nwe can show that the type system can verify that a program is E-differentially private. 5.3.2 Theorem \n[E-Differential Privacy]: The execution of any closed program e such that f e : !Es -Ot is an E-differentially \nprivate function from s to t . That is, for all closed values v, v ' : s such that f v ~r v ' : s, and \nall closed values w : t, if do(e v) . f and do(e v ' ) . f ', then Prf [w] = e rE PrfI [w]. Proof: By \nusing the fact that f {v/x} ~(x:=r) {v ' /x} : (x : s), we have that f do(e x){v/x} rE do(e x){v ' /x} \n: t . So, by the Big-step Metric Preservation Theorem (5.2.8), we obtain f rE f ' : t. We conclude by \nCorollary (5.2.4) and Lemma (5.3.1). D The above theorem shows that in order to ensure that the execution \nof a program e corresponds to an E-differentially private random\u00adized function from values in s to values \nin t, it is suf.cient to check that the program e has a type of this form: f e : !Es -Ot  5.4 Primitive \nOperations for Privacy As outlined in Section 3, one important property of DFuzz is that it can be extended \nby means of primitive operations. In particular, we are interested in adding two basic building blocks, \nallowing us to build more involved differentially private examples. The .rst operation we add is the \nLaplace mechanism (Proposition (2.4)), with the following signature: add noise : .E : ..R[E] . !ER -OR \n Note that, unlike the version presented in Fuzz, this primitive allows the level of noise (and thus, \nthe level of privacy) to be speci.ed by the user. Another primitive operation that .ts well in our framework \nis the exponential mechanism [28] exp noise :.s : ., E : ..R[s] . Bag(s) . (s . !sdb -R) . R[E] . !Edb \n-Os (1) where Bag(s) is a primitive type representing a multiset of ob\u00adjects of type s. The exponential \nmechanism takes a set of possible outputs, a quality score that assigns to each element in the range \na number (depending on the database), and the database itself. The quality score is at most s-sensitive \nin the database, and here we al\u00adlow this sensitivity to be passed in as a parameter. The algorithm privately \noutputs an element of the range that approximately maxi\u00admizes the quality score. 6. Case Studies To illustrate \nhow DFuzz s dependent types expand the range of programs that can be certi.ed as differentially private, \nwe now present four examples of practical algorithms that can be imple\u00admented in DFuzz, but not in Fuzz. \nEach algorithm is taken from a different paper from the differential privacy literature (speci.cally, \n[4, 18, 19, 27]). The .rst three algorithms rely on the following new feature that is enabled by DFuzz \ns dependent types: Iterative privacy: The ability to express a dependency between the total privacy \ncost of a function and a parameter that is chosen at runtime, such as a number of iterations. The last \nexample illustrates how allowing slightly more complex sensitivity annotations can increase the expressivity, \nand it also shows another important use for dependent types: Privacy-utility tradeoff: The ability of \na function to control its own privacy cost, e.g., by scaling the precision of an iterated operation to \nmake the total cost independent of the number of iterations. We are experimenting with a prototype implementation \nof DFuzz, and we present the examples in the actual syntax used by the proto\u00adtype, an extension of Fuzz \nthat closely follows the concrete syntax from Figure 2 (for instance, we write sample x=e; e to denote \nlet Ox = e in e '); we only omit instantiations of size/sensitivity terms for brevity. The examples also \nuse some additional con\u00adstructs, such as (a,b) for tensor products of types a and b, with as\u00adsociated \nprimitive operations. Details of these extensions are avail\u00adable in [14]. 6.1 Iterative Privacy k-means \n[4] Our .rst example (Figure 10) is k-means clustering, an algorithm from data mining that groups a set \nof data points into k clusters. Given a set of points and an initial guess for the k cluster centers, \nthe algorithm iteratively re.nes the clusters by .rst associating each point with the closest center, \nand then moving each center to the middle of its associated points; the differentially private version \nensures privacy by adding some noise to the re.ned centers. The k-means algorithm can be implemented \nin DFuzz as follows: Here, iterate is a caller-supplied procedure performing function kmeans (iters : \nNat[i]) (eps : num[e]) (db :[3 * i * e] (num,num) bag) (centers : list(num,num)[j]) (iterate : num[e] \n-> (num,num) bag -o[3*e] list(num,num)[j] -> Circle list(num,num)[j]) : Circle list(num,num)[j] { case \niters of 0 => return centers | n + 1 => sample next_centers = kmeans n eps db centers iterate; iterate \neps db next_centers } Figure 10. k-means in DFuzz an update of the centers (details omitted for brevity). \nNote that the sensitivity of kmeans in the database db depends on two other parameters: the number of \niterations [i] and the privacy cost per iteration [e] that the analyst is willing to tolerate. This is \nenabled by the dependent types in DFuzz; in contrast, Fuzz is only able to typecheck a simpli.ed version \nof k-means in which both parameters are hard-coded. Iterative Database Construction [19] Our second example \n(Fig\u00adure 11) is an algorithm that can ef.ciently answer exponentially many queries with good accuracy. \nThis is done by .rst construct\u00ading a public approximation of the private database, which can then be \nused to answer queries without further privacy cost. IDC builds the approximation iteratively: given \nan initial guess, it uses a pri\u00advate distinguisher (PA) to .nd a query that would be inaccurate on the \ncurrent approximation and then applies a database update algo\u00adrithm (DUA) to re.ne the approximation. \nPA and DUA are parame\u00adters of the algorithm, and the sensitivity of an IDC instance depends on the sensitivities \nof its PA. This can be expressed in DFuzz as fol\u00adlows: Several choices for PA and DUA have been proposed \nin the function IDC (iter : Nat[i]) (eps : num[e]) (db :[2 * i * e] db_type) (qs : query bag) (PA : \n(query bag) -> approx_db -> db_type -o[e] Circle query) (DUA : approx_db -> query -> num -> approx_db) \n(eval_q : query -> db_type -o[1] num) : Circle approx_db { case iter of 0 => return init_approx | n + \n1 => sample approx = (IDC n eps db qs PA DUA); sample q = PA qs approx db; sample actual = add_noise \neps (eval_q q db); return (DUA approx q actual) } Figure 11. Iterative Database Function in DFuzz literature; \nsome can be written directly in DFuzz (e.g., the expo\u00adnential distinguisher from [22]), and others can \nbe added as trusted primitives (e.g., the sparse distinguisher from [35]). By contrast, a parametric \nIDC cannot be written in plain Fuzz because there is no way to express the dependency between the sensitivity \nof the PA/DUA and the sensitivity of the overall algorithm. Cumulative Distribution Function [27] Our \nthird example (Fig\u00adure 12) is an algorithm that computes the CDF function. Given a database of numeric \nrecords and a list of buckets de.ned by cutoff values, it computes the number of records in each bucket. \n[27] presents three variants of this algorithm with different pri\u00advacy/utility tradeoffs, only one of \nwhich was previously supported in Fuzz. To understand why, consider the following version, imple\u00admented \nin DFuzz: Note that the sensitivity in this version depends function cdf (eps : num[e]) (buckets : list(num)[i]) \n(db :[i * e] num bag) : Circle list(num)[i] { case buckets of [] => return [] | (x :: y) => let (lt,gt) \n= bag_split (fun n => n<x) db; sample count = add_noise eps (bag_size lt); sample bigger = cdf eps y \ngt; return (count :: bigger) } Figure 12. Cumulative Distribution Function in DFuzz on the number of \ncutoff values or buckets. Since Fuzz cannot cap\u00adture such a dependency, this CDF variant is not just \nimpossible to write in Fuzz it cannot even be added as a trusted primitive, since there is no way to \nexpress its type signature. In contrast, DFuzz can directly support this program, and it could also support \nthe last version in [27] with a small extension that we discuss in Section 8.  6.2 Privacy-Utility Tradeoff \nOur fourth and most complex example shows how DFuzz enables programmers to write functions that control \ntheir own privacy cost, and it also illustrates how small extensions to the language for sen\u00adsitivity \nannotations can further increase expressivity. The extension we use here introduces a new operation R1 \n0 R2 on sensitivity terms R1 and R2 that provides a limited form of division. The in\u00adterpretation is \nextended accordingly as follows: r [R1 0 R2]. = if [R1]. = r . [R2]. =s . r, s /. {0, 8}s [R1 0 R2]. \n= 0 if [R1]. = 0 . [R2]. = 8 8 otherwise [R1 0 R2]. = The behavior of the 0 operator is different from \nthat of the ordinary division operator: 0 does not enjoy the usual properties of division with respect \nto multiplication and addition, i.e., it is not the inverse of multiplication, and it does not enjoy \nthe usual distributivity laws. However, it does have two key properties. First, Lemma (3.1) still holds \nfor the system that includes 0 . If the usual division \u00f7 were added instead, the interpretation would \nno longer be total because r \u00f70 and 8 \u00f7 8 are in general unde.ned when \u00f7 is the inverse of \u00b7 and enjoys \nthe usual distributivity laws. The choice of 0 ensures the preservation of the metatheoretic results \nof Sections 4 and 5. Second, the interpretation of 0 satis.es the following inequality (the need for \nwhich will become apparent shortly) for every .: [(R1 0 (R2 + 1)) \u00b7 R2]. = [R1]. (2) In terms of 0 , \nwe can add a safe division primitive to the language for terms: div : .i : ...j : ..R[i] . R[j + 1] . \nR[i 0 (j + 1)] Note that the div operation is simply a restriction of the usual division to positive \nreal numbers that we make total by restricting the domain of the denominator to reals greater than or \nequal to 1. k-medians [18] With this extension, we can implement our fourth example (Figure 13): an algorithm \nfor k-medians, a classic prob\u00adlem in combinatorial optimization. Given a database V of locations, with \ndistances between locations, a desired number k of (say) fac\u00adtories to build, and a private demand set \nD . V , the goal is to select a set F . V of k locations to minimize the cost, de.ned to be the sum of \nthe distances from each demand point to the closest factory. In the program, the distances are given \nimplicitly via the cost function, which maps sets of factory locations to costs. The al\u00adgorithm starts \nwith an initial random choice F0 of k locations and runs several iterations, each time using the exponential \nmechanism (Equation (1) in Section 5.4) to .nd the best location to swap for a location in the candidate \nset of factories. After building up a collec\u00adtion of candidate factory sets, the mechanism uses the exponential \nmechanism once more to choose the best con.guration. The helper function kmedians_aux runs the main loop, \nwhich repeatedly tries to improve the cost of a set of locations by replac\u00ading a location from the set \nwith one that is not in the set. The outer kmedians function simply chooses an initial (random) set of \nfac\u00adtory locations and sets up the privacy and iteration constants. The program uses several primitive \nfunctions for manipulating bags: bagcontains checks the membership in bags, bagproduct builds the Cartesian \nproduct of two bags, bagadd adds an element to a bag, bagswap swaps two element in a bag, and bagselect \nchooses subset of a given size from a bag uniformly at random. The helper function score measures how \nmuch a swap can improve the cost of a set of locations. It can also be written in DFuzz (see [14]). The \nkey challenge in implementing k-medians is that it scales the privacy level of the exponential mechanism \nto achieve a con\u00ad function kmedians_aux (iter : Nat[i]) (F0 : (loc bag)) (delta : num[s]) (cost : (loc \nbag) -> (loc bag) -o[s] num) (eps : num[e]) (V : loc bag) (D :[e * (2 * s) * i] loc bag) : Circle (loc \nbag, (loc bag) bag) { case iter of | 0 => return (F0, (bag F0)) | x + 1 => sample pair = kmedians_aux \nx F0 delta cost eps V D; let (Fi, Fs) = pair; let (_, F_comp) = bagsplit (fun l => bagcontains V l) Fi; \nswaps = bagproduct Fi F_comp; sample best = exp_noise delta swaps (score cost Fi) (eps * (2 * delta)) \nD; let (a, b) = best; Fnew = bagswap a b Fi; return (Fnew, bagadd Fs Fnew) } function kmedians (T : \nNat[i]) (delta : num[s]) (V : loc bag) (cost : (loc bag) -> (loc bag) -o[s] num) (D :[2 * e] loc bag) \n(eps : num[e]) (k : Nat[n]) : Circle (loc bag) { sample F1 = bagselect k V; eps = div(div(eps,(T + 1)),(2 \n* delta)+1); sample result = kmedians_aux T F1 delta cost eps V D; let (_, Fs) = result; exp_noise delta \nFs cost eps D } Figure 13. k-medians in DFuzz stant overall privacy cost, independent of the number \nof iterations. To derive the correct type for kmedians, DFuzz must prove that, in the call to the auxiliary \nfunction kmedians_aux, the sensitivity of D is at most eps. This involves checking a subtyping application \nthat requires the following inequality between sensitivity terms: ((e 0 (i + 1)) 0 (2 \u00b7 s + 1)) \u00b7 (2 \n\u00b7 s) \u00b7 i = e This inequality follows from two applications of Equation (2). Thus, with the additional \n0 operator, algorithms that scale their pri\u00advacy cost depending on the number of iterations can be expressed \nand veri.ed in DFuzz, using dependent types. 7. Related Work Differential privacy Our system provides \ndifferential privacy [12], one of the strongest privacy guarantees that has been proposed to date. Alternatives \ninclude randomization, l-diversity, and k\u00adanonymity, which are generally less restrictive but can be \nvulnera\u00adble to certain attacks on privacy [24]. Differential privacy offers a provable bound on the amount \nof information an attacker can learn about any individual, even with access to auxiliary information. \nPINQ [26] is an SQL-like differentially private query language embedded in C#; Airavat [36] is a MapReduce-based \nsolution using a modi.ed Java VM. Both PINQ and Airavat check privacy at runtime, while DFuzz uses a \nstatic check. The other previous work in this area is Fuzz [20, 33, 34], on which DFuzz is based. DFuzz \nhas a richer type system: while Fuzz uses linear types to track sensitivity, DFuzz uses a combination \nof linear indexed types and lightweight dependent types, which substantially expands the set of differentially \nprivate queries that can be certi.ed successfully. Another recent language-based solution is CertiPriv \n[3]. This is a machine-assisted framework built on top of the Coq proof assistant for reasoning about \ndifferential privacy from .rst prin\u00adciples. CertiPriv can certify even more queries than DFuzz, includ\u00ading \nqueries that do not rely on standard building blocks such as the Laplace mechanism indeed, it can be \nused to prove the cor\u00adrectness of the Laplace mechanism itself. However, this comes at the cost of much \nhigher complexity and less automation, making CertiPriv more suitable for experts who are expanding the \nbound\u00adaries of differential privacy. In contrast, DFuzz s certi.cation is au\u00adtomatic, allowing it to \ntarget a broader class of potential users. Other work in this area include Xu [39], who considered differ\u00adential \nprivacy in a distributed setting, using a probabilisitc process calculus, and Mohan et al. [29], who \nintroduce a platform for pri\u00advate data analysis that optimizes error for certain queries. Linear dependent \ntypes Linear types, inspired by Girard s linear logic [15], have emerged as key tools to support .ne \ngrained rea\u00adsoning about resource management [37]. In this context, the idea of using indexed modalities \n!p for counting multiple uses of the same resource, as we do, emerged early on. Bounded Linear Logic \n[16] introduced modalities indexed by polynomial expressions. Those are similar to our sensitivity annotations \nwith the essential differ\u00adence that they are polynomials over natural numbers, while we con\u00adsider polynomials \nover the reals augmented with 8. This approach has been extended in [9] by constrained universal and \nexistential quanti.ers, providing mechanisms for polymorphism and abstrac\u00adtion over polynomial expressions \nsimilar to the ones made available by our quanti.ers over size and sensitivity variables. Different forms \nof lightweight dependent types form the basis for programming languages such as Dependent ML, ATS, and \nEpigram. Moreover, they have also started to be incorporated in more standard programming languages, \nsuch as Haskell [40]. In all these approaches the types can express richer properties of the program \nthat can then be ensured automatically by type checking. ATS [7] is a language that combines linear and \ndependent types. Its type system is enriched with a notion of resources that is a type-level representation \nof memory locations. The management of location resources uses a linear discipline. This aspect makes \nATS useful for reasoning about properties of memory and pointers. However, linear types as used in ATS \nare not enough to reason about the sensitivity of programs. Linear indexed types and lightweight dependent \ntypes have also been combined in [8] with the aim of providing a general framework for implicit complexity. \nThe approach in [8] is similar in spirit to the one developed in the current paper; however, it considers \nonly type-level terms that represent natural numbers, and its typing judgments are parametric in an equational \nprogram providing for the semantics of the type-level language. Moreover, [8] allows a further dependency \nbetween different modal operators that is not needed for our analysis. More recently, Krishnaswami et \nal. [23] proposed a language that combines linear (non-indexed) types with dependent types in order to \nprovide program modules with a re.ned control of their state. Related notions of privacy and sensitivity \nThe study of database privacy and statistical databases has a long history. Recent work in\u00adcludes Dalvi, \nR\u00b4 e, and Suciu s study of probabilistic database man\u00adagement systems [10], and Machanavajjhala et al. \ns comparison of different notions of privacy with respect to real-world census data [25]. Quantitative \nInformation Flow is concerned with how much one piece of a program can affect another, but measures this \nin terms of how many bits of entropy leak during one execution. While Differential Privacy and Quantitative \nInformation Flow are clearly related concepts, the question of establishing formal relations be\u00adtween \nthem has been approached only recently [1, 2]. Provenance analysis in databases tracks the input data \nactually used to compute a query s output, and is also capable of detecting that the same piece of data \nwas used multiple times to produce a given answer [17]. Palamidessi and Stronati [31] recently proposed \na constraint\u00adbased approach to compute the sensitivity of relational algebra queries. Their analysis \nis able in particular to compute the exact sensitivity for a wide range of queries. In contrast, the \ngoal of our approach is to provide an upper bound on the sensitivity not only of relational queries but \nalso for higher order functional programs. Chaudhuri et al. in [6] study automatic program analyses that \nestablish sensitivity (which they call robustness) of numerical pro\u00adgrams. Their approach can also used \nto ensure differential privacy, although they do not study this application in depth. Our approach differs \nin that we ensure differential privacy through a logically mo\u00adtivated type system, rather than a program \nanalysis. 8. Conclusions and Future Work We have presented a new core language, DFuzz, for differentially \nprivate queries. Like its predecessor, Fuzz, DFuzz has a type sys\u00adtem with strong metatheoretic properties, \nwhich guarantee that all queries of a certain type are differentially private. The key nov\u00adelty in DFuzz \nis a lightweight form of linear dependent types that track size and sensitivity information; this considerably \nexpands the range of programs that can be certi.ed as differentially private. We have demonstrated the \nexpressivity of DFuzz using four exam\u00adple algorithms from the differential privacy literature that can \nbe implemented in Fuzz only in greatly simpli.ed form, or not at all. Prototype We are currently working \non an algorithmic version of DFuzz and a prototype implementation. The key implementa\u00adtion challenge \nis related to checking the subtype relation. A nat\u00adural approach to this problem is to generate numeric \nand logical constraints that would have to be satis.ed in order to type the pro\u00adgram, and to then pass \nthese constraints to an external solver. In the case of DFuzz, some of the constraints are over real \nnumbers, but they are of a form that is supported by the Z3 solver [11]. Since DFuzz s additional constructs \nand annotations are erasable at run\u00adtime, a prototype can share a backend with Fuzz, and is thus able \nto take advantage of Fuzz s defenses against side channels [20]. Possible extensions The languages for \nsize and sensitivity anno\u00adtations we have used in this paper are fairly restrictive, but this is merely \na design choice, and not an inherent limitation of the ap\u00adproach. As we have shown in the k-medians example, \nallowing more complex size and sensitivity annotations can increase the ex\u00adpressivity, though it may \nalso make the generated constraints harder to solve. There are other simple increments to the annotation \nlanguages that would increase DFuzz s expressivity. For example, [27] pro\u00advides a higher-utility algorithm \nfor the cumulative distribution function that uses a divide-and-conquer approach. The resulting sensitivity \nis proportional to the logarithm of the number of buck\u00adets. If logarithms of sizes were added to the \nannotation language, it would become possible to implement this algorithm. (Of course, this addition \nwould again come at the cost of increasing the dif.\u00adculty of constraint solving.) Another possible direction \nwould be to include more general data types, as in [38], in addition to the pre\u00adcise types L(s)[S] and \nN[S] we have focused on. For instance, the private Facility Location algorithm, as presented in [18], \nis similar to k-medians, but, instead of a .xed number of factories to build, we are given a .xed cost \nper factory, and the goal is to minimize the total cost. This algorithm requires recursion over generalized \ntrees, which we speculate could be implemented in DFuzz if support for sized versions of these data types \nwere added. Limitations and future work We designed DFuzz to certify algo\u00adrithms for which the differential \nprivacy property can essentially be proven with a rich compositional sensitivity analysis to show that \nthe noise is drawn from an appropriate distribution. However, in the differential privacy literature \nthere are algorithms whose analysis requires more involved reasoning. One example is the private ver\u00adsion \nof the Unweighted Vertex Cover algorithm that was presented in [18]. One way to handle the analysis for \nthis algorithm would be to use the probabilistic relational reasoning that is the basis of the CertiPriv \nframework [3]. The ability to reason about relations between different programs and data, i.e., about \nclosely neighbor\u00ading databases, seems crucial to this approach. However, by using the CertiPriv framework, \none loses the ability to reason about dif\u00adferential privacy in an automated way. One natural way to preserve \nthe automatic approach and to nevertheless expand the scope of the analysis would be to combine the approach \nof DFuzz with a limited form of relational reasoning. Acknowledgments We thank Emilio Jes \u00b4Gallego Arias \nfor us many valuable comments on the .nal version of this work. We thank M\u00b4 ario S. Alvim, Loris D Antoni, \nSanjeev Khanna, Aaron Roth, and the anonymous reviewers for their helpful comments. This research was \nsupported by ONR grants N00014-09-1-0770 and N00014-12-1-0757, and NSF grant #1065060. Marco Gaboardi \nwas supported by the European Community s Seventh Framework Programme (FP7/2007-2013) under grant agreement \nn. 272487. References [1] M. S. Alvim, M. E. Andres, K. Chatzikokolakis, and C. Palamidessi. On the relation \nbetween Differential Privacy and Quantitative Infor\u00admation Flow. In Proc. ICALP, 2011. [2] G. Barthe \nand B. K \u00a8opf. Information-theoretic Bounds for Differen\u00adtially Private Mechanisms. In Proc. CSF, 2011. \n[3] G. Barthe, B. K \u00a8 eguelin. Probabilistic opf, F. Olmedo, and S. Zanella B \u00b4 relational reasoning \nfor differential privacy. In Proc. POPL, 2012. [4] A. Blum, C. Dwork, F. McSherry, and K. Nissim. Practical \nprivacy: the SuLQ framework. In Proc. PODS, 2005. [5] K. Chaudhuri, C. Monteleoni, and A. D. Sarwate. \nDifferentially private empirical risk minimization. J. Mach. Learn. Res., 12:1069 1109, July 2011. [6] \nS. Chaudhuri, S. Gulwani, R. Lublinerman, and S. NavidPour. Proving programs robust. In Proc. FSE, 2011. \n[7] C. Chen and H. Xi. Combining programming with theorem proving. In Proc. ICFP, pages 66 77, 2005. \n[8] U. Dal Lago and M. Gaboardi. Linear dependent types and relative completeness. In IEEE LICS 11, pages \n133 142, 2011. [9] U. Dal Lago and M. Hofmann. Bounded linear logic, revisited. In TLCA, volume 5608 \nof LNCS, pages 80 94. Springer, 2009. [10] N. Dalvi, C. R \u00b4e, and D. Suciu. Probabilistic databases: \ndiamonds in the dirt. Commun. ACM, 52(7):86 94, 2009. [11] L. de Moura and N. Bj\u00f8rner. Z3: An ef.cient \nSMT solver. In Proc. TACAS, 2008. [12] C. Dwork. Differential privacy. In Proc. ICALP, 2006. [13] C. \nDwork, F. McSherry, K. Nissim, and A. Smith. Calibrating noise to sensitivity in private data analysis. \nIn Proc. TCC, 2006. [14] M. Gaboardi, A. Haeberlen, J. Hsu, A. Narayan, and B. C. Pierce. Linear dependent \ntypes for differential privacy (extended version). http://privacy.cis.upenn.edu/papers/dfuzz-long.pdf. \n[15] J. Girard. Linear logic. Theor. Comp. Sci., 50(1):1 102, 1987. [16] J. Girard, A. Scedrov, and P. \nScott. Bounded linear logic. Theoretical Computer Science, 97(1):1 66, 1992. [17] T. J. Green, G. Karvounarakis, \nand V. Tannen. Provenance semirings. In Proc. PODS, 2007. [18] A. Gupta, K. Ligett, F. McSherry, A. Roth, \nand K. Talwar. Differen\u00adtially private combinatorial optimization. In Proc. SODA, 2010. [19] A. Gupta, \nA. Roth, and J. Ullman. Iterative constructions and private data release. In Proc. TCC, 2012. [20] A. \nHaeberlen, B. C. Pierce, and A. Narayan. Differential privacy under .re. In Proc. USENIX Security, 2011. \n[21] S. Hayashi. Singleton, union and intersection types for program extraction. In Proc. TACS, volume \n526 of LNCS, pages 701 730. Springer Berlin / Heidelberg, 1991. [22] S. P. Kasiviswanathan, H. K. Lee, \nK. Nissim, S. Raskhodnikova, and A. Smith. What can we learn privately? In Proc. FOCS, Oct. 2008. [23] \nN. R. Krishnaswami, A. Turon, D. Dreyer, and D. Garg. Super.cially substructural types. In Proc. ICFP, \n2012. [24] N. Li, T. Li, and S. Venkatasubramanian. t-closeness: Privacy beyond k-anonymity and l-diversity. \nIn Proc. ICDE, 2007. [25] A. Machanavajjhala, D. Kifer, J. Abowd, J. Gehrke, and L. Vilhuber. Privacy: \nTheory meets practice on the map. In Proc. ICDE, 2008. [26] F. McSherry. Privacy integrated queries: \nan extensible platform for privacy-preserving data analysis. In Proc. SIGMOD, 2009. [27] F. McSherry \nand R. Mahajan. Differentially-private network trace analysis. In Proc. SIGCOMM, 2010. [28] F. McSherry \nand K. Talwar. Mechanism design via differential privacy. In Proc. FOCS, 2007. [29] P. Mohan, A. Thakurta, \nE. Shi, D. Song, and D. Culler. GUPT: privacy preserving data analysis made easy. In Proc. SIGMOD, 2012. \n[30] A. Narayanan and V. Shmatikov. Robust de-anonymization of large sparse datasets. In Proc. IEEE S&#38;P, \n2008. [31] C. Palamidessi and M. Stronati. Differential privacy for relational algebra: Improving the \nsensitivity bounds via constraint systems. In Proc. QAPLS, volume 85 of EPTCS, pages 92 105, 2012. [32] \nN. Ramsey and A. Pfeffer. Stochastic lambda calculus and monads of probability distributions. In Proc. \nPOPL, 2002. [33] J. Reed, M. Gaboardi, and B. C. Pierce. Distance makes the types grow stronger: A calculus \nfor differential privacy (extended version). http://privacy.cis.upenn.edu/papers/fuzz-long.pdf. [34] \nJ. Reed and B. C. Pierce. Distance makes the types grow stronger: A calculus for differential privacy. \nIn Proc. ICFP, Sept. 2010. [35] A. Roth and T. Roughgarden. The median mechanism: Interactive and ef.cient \nprivacy with multiple queries. In Proc. STOC, 2010. [36] I. Roy, S. T. V. Setty, A. Kilzer, V. Shmatikov, \nand E. Witchel. Airavat: security and privacy for MapReduce. In Proc. NSDI, 2010. [37] D. Walker. Advanced \nTopics in Types and Programming Languages, chapter Substructural Type Systems. The MIT Press, 2005. [38] \nH. Xi and F. Pfenning. Dependent types in practical programming. In Proc. POPL, 1999. [39] L. Xu. Modular \nreasoning about differential privacy in a probabilistic process calculus. Manuscript, 2012. [40] B. A. \nYorgey, S. Weirich, J. Cretin, S. Peyton Jones, D. Vytiniotis, and J. P. M. es. Giving Haskell a promotion. \nIn Proc. TLDI, 2012.     \n\t\t\t", "proc_id": "2429069", "abstract": "<p>Differential privacy offers a way to answer queries about sensitive information while providing strong, provable privacy guarantees, ensuring that the presence or absence of a single individual in the database has a negligible statistical effect on the query's result. Proving that a given query has this property involves establishing a bound on the query's sensitivity---how much its result can change when a single record is added or removed.</p> <p>A variety of tools have been developed for certifying that a given query differentially private. In one approach, Reed and Pierce [34] proposed a functional programming language, Fuzz, for writing differentially private queries. Fuzz uses linear types to track sensitivity and a probability monad to express randomized computation; it guarantees that any program with a certain type is differentially private. Fuzz can successfully verify many useful queries. However, it fails when the sensitivity analysis depends on values that are not known statically.</p> <p>We present DFuzz, an extension of Fuzz with a combination of linear indexed types and lightweight dependent types. This combination allows a richer sensitivity analysis that is able to certify a larger class of queries as differentially private, including ones whose sensitivity depends on runtime information. As in Fuzz, the differential privacy guarantee follows directly from the soundness theorem of the type system. We demonstrate the enhanced expressivity of DFuzz by certifying differential privacy for a broad class of iterative algorithms that could not be typed previously.</p>", "authors": [{"name": "Marco Gaboardi", "author_profile_id": "81367593990", "affiliation": "Universit&#224; di Bologna - University of Pennsylvania, Bologna - Philadelphia, Italy", "person_id": "P3977994", "email_address": "gaboardi@cs.unibo.it", "orcid_id": ""}, {"name": "Andreas Haeberlen", "author_profile_id": "81548028158", "affiliation": "University of Pennsylvania, Philadelphia, PA, USA", "person_id": "P3977995", "email_address": "ahae@cis.upenn.edu", "orcid_id": ""}, {"name": "Justin Hsu", "author_profile_id": "81553211056", "affiliation": "University of Pennsylvania, Philadelphia, PA, USA", "person_id": "P3977996", "email_address": "justhsu@seas.upenn.edu", "orcid_id": ""}, {"name": "Arjun Narayan", "author_profile_id": "81488653290", "affiliation": "University of Pennsylvania, Philadelphia, PA, USA", "person_id": "P3977997", "email_address": "narayana@cis.upenn.edu", "orcid_id": ""}, {"name": "Benjamin C. Pierce", "author_profile_id": "81100303310", "affiliation": "University of Pennsylvania, Philadelphia, PA, USA", "person_id": "P3977998", "email_address": "bcpierce@cis.upenn.edu", "orcid_id": ""}], "doi_number": "10.1145/2429069.2429113", "year": "2013", "article_id": "2429113", "conference": "POPL", "title": "Linear dependent types for differential privacy", "url": "http://dl.acm.org/citation.cfm?id=2429113"}