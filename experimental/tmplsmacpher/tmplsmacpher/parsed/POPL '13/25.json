{"article_publication_date": "01-23-2013", "fulltext": "\n High-Level Separation Logic for Low-Level Code Jonas B. Jensen IT University of Copenhagen jobr@itu.dk \nAbstract Separation logic is a powerful tool for reasoning about structured, imperative programs that \nmanipulate pointers. However, its appli\u00adcation to unstructured, lower-level languages such as assembly \nlan\u00adguage or machine code remains challenging. In this paper we de\u00adscribe a separation logic tailored \nfor this purpose that we have ap\u00adplied to x86 machine-code programs. The logic is built from an assertion \nlogic on machine states over which we construct a speci.cation logic that encapsulates uses of frames \nand step indexing. The traditional notion of Hoare triple is not applicable directly to unstructured \nmachine code, where code and data are mixed together and programs do not in general run to completion, \nso instead we adopt a continuation-passing style of speci.cation with preconditions alone. Nevertheless, \nthe range of primitives provided by the speci.cation logic, which include a higher-order frame connective, \na novel read-only frame connec\u00adtive, and a later modality, support the de.nition of derived forms to \nsupport structured-programming-style reasoning for common cases, in which standard rules for Hoare triples \nare derived as lem\u00admas. Furthermore, our encoding of scoped assembly-language la\u00adbels lets us give de.nitions \nand proof rules for powerful assembly\u00adlanguage macros such as while loops, conditionals and proce\u00addures. \nWe have applied the framework to a model of sequential x86 machine code built entirely within the Coq \nproof assistant, includ\u00ading tactic support based on computational re.ection. Categories and Subject Descriptors \nF.3.1 [Logics and mean\u00adings of programs]: Specifying and Verifying and Reasoning about Programs Assertions, \nInvariants, Logics of programs, Mechani\u00adcal veri.cation, Pre-and post-conditions, Speci.cation techniques; \nD.3.2 [Programming Languages]: Language Classi.cations Macro and assembly languages; D.2.4 [Software \nEngineering]: Software / Program Veri.cation Correctness proofs, formal meth\u00adods General Terms Languages, \ntheory, veri.cation Keywords Separation logic, machine code, proof assistants 1. Introduction Formal \nveri.cation is one of the most important techniques for building reliable computer systems. Research \nin software veri.ca- Permission to make digital or hard copies of all or part of this work for personal \nor classroom use is granted without fee provided that copies are not made or distributed for pro.t or \ncommercial advantage and that copies bear this notice and the full citation on the .rst page. To copy \notherwise, to republish, to post on servers or to redistribute to lists, requires prior speci.c permission \nand/or a fee. POPL 13, January 23 25, 2013, Rome, Italy. Copyright c &#38;#169; 2013 ACM 978-1-4503-1832-7/13/01. \n. . $15.00 Nick Benton Andrew Kennedy Microsoft Research Cambridge {nick,akenn}@microsoft.com tion typically, \nand quite reasonably, concerns reasoning about the high-level programming languages with which most programmers \nwork. But to build genuinely trustworthy systems, one really needs to verify the machine code that actually \nruns, whether it be hand\u00adcrafted or the output of a compiler. This is particularly important for establishing \nsecurity properties, since failures of abstraction be\u00adtween the high and low-level models often lead \nto vulnerabilities (and because hand-crafted machine code is often found in security\u00adcritical places, \nsuch as kernels). A further motivation for verifying low-level code is that real systems are composed \nof components written in many different languages; machine code is the only truly universal lingua franca \nby which we can reason about properties of such compositions. Finally, experience shows that hand-written \nlow-level programs are simply much harder to get right than higher\u00adlevel ones, increasing the credibility \ngap between formal and infor\u00admal veri.cations. Verifying low-level, unstructured programs [18, 39], and \ncom\u00adpilers that produce them [24], both have a long history. And since such veri.cations are, like low-level \nprograms themselves, ex\u00adtremely lengthy and error-prone, some form of mechanical assis\u00adtance is absolutely \ncrucial. This assistance often takes the form of automated decision procedures for .rst-order logic and \nvari\u00adous more specialized theories, combined by an SMT solver [21]. Here, however, our focus is on deductive \nHoare-style veri.cation of machine-code programs using an interactive proof assistant, in our case Coq. \nThis requires more manual effort on the part of the user, but allows one to work with much richer mathematical \nmod\u00adels and speci.cations, which are particularly important for mod\u00adularity. Both approaches to mechanization \nalso have considerable history, with much pioneering work applying interactive provers to low-level code \nhaving been done with the Boyer-Moore prover in the 1980s [25, 41]. Recently, however, an exciting con.uence \nof advances in foundational theory, program logics (most notably separation logic [35]) and the technology \nof proof assistants, to\u00adgether with increased interest in formal certi.cation, have led to an explosion \nof work on mechanized veri.cation of real (or at least, realistic) software, including compilers and \noperating systems. Al\u00adthough many of these formalizations do involve reasoning about machine code or \nassembly language programs, program logics for low-level code are generally much less satisfactory, and \nmore ad hoc, than those for high-level languages. The design of a high-level program logic tends to follow \nclosely the structure and abstractions provided by the language. Commands in a while-language, for example, \nmay be modelled as partial func\u00adtions from stores to stores, which are only combined in certain very \nrestricted ways. The classical Hoare triple, relating a predicate on inputs to a predicate on outputs, \nis a natural (indeed, inevitable) and generally satisfactory form of speci.cation for such functions. \nFurthermore, the structured form of programs leads to particularly elegant, syntax-directed program logic \nrules for composing veri.\u00adcations. Machine code, by contrast, has almost nothing in the way of inherent \nstructure or abstractions to guide one, supports chal\u00adlenging patterns of programming and also involves \na host of messy complexities.  The messy complexities include large instruction sets with variable-length \nencodings, the need to work with bit-level oper\u00adations and arithmetic mod 232, alignment, a plethora \nof .ags, reg\u00adisters, addressing modes, and so on. These inevitably cause some pain, but are just the \nsort of thing proof assistants are good at check\u00ading precisely and, with a well-engineered formalization, \nremoving some of the drudgery from.1 There is, of course, complexity of a quite different order associated \n(at both high-and low-level) with concurrency especially relaxed memory models on multiproces\u00adsors \nwhich we do not address at all in this paper. Even in the sequential case, however, the lack of inherent \nstructure in low-level code is a fundamental problem. Machine code features unstructured control .ow. \nA contiguous block of instructions potentially has many entry points and many exits, with the added complication \nthat the same bytes may decode differently according to the entry point. Machine code is almost entirely \nuntyped and higher-order, with no runtime tagging: any word in memory or a register may be treated as \na scalar value, a pointer or a code pointer, and common coding patterns do make use of this .exibility: \nstealing bits in pointers, storing metadata at offsets from code pointers, computing branches, and so \non. Finally, code and data live in the same heap, allowing code generation, self\u00admodifying code and code \nexamination, for example for interpreting instructions. The most basic abstractions, such as memory alloca\u00adtion \nor function calling, are not built in, but are conventions that must be speci.ed, followed and veri.ed \nat appropriate points. Fur\u00adthermore, code that implements even the simplest of these abstrac\u00adtions, such \nas .rst-order function calls, uses features of machine code whose high-level analogues (higher-order, \ndynamically allo\u00adcated local state) are challenging to reason about and a subject of active research \n even in very high-level languages such as ML. Some logics, type systems and analyses for machine code \ndeal with these complexities by imposing structure and restrictions on the code they deal with. For example, \none can enforce a tradi\u00adtional basic block structure, hard-code memory allocation as a spe\u00adcial pseudo-instruction \nor treat calling and a call-stack specially [5, 27, 31, 40]. Such techniques can work well for verifying \ncode that looks like it came from a C compiler, but we would like some\u00adthing more generally applicable, \nable to verify smoothly higher\u00adorder code, systems code such as schedulers and allocators, and code that \nuses clever bit-level representation tricks. In previous work, for example on compiling a functional \nlanguage to a rather idealized assembly language [7], one of us has proved useful re\u00adsults in Coq using \na shallow embedding of step-indexed, separated predicates and relations, a notion of biorthogonality \n( perping ) for code pointers, explicit second-order quanti.cation for framing, and a more-or-less ad \nhoc collection of lemmas for instructions, quan\u00adti.er manipulation and entailment. Given suf.cient effort, \nsuch an approach can undeniably be pushed through, but the proofs and speci.cations are very clumsy; \nalthough some of the connectives have respectable properties, there is certainly no sense that one is \nworking in a well-structured program logic, with a well-behaved proof theory. Applying such a naive approach \nin the context of real machine code, with the above-mentioned messy complexities and in which we would \nclearly need to build numerous higher-level proof abstractions, seemed unlikely to work well. Separation \nlogics for higher-level languages, by contrast, do have a good proof theory. In particular, work on higher-order \nframe rules allows local reasoning about higher-order programs, allowing invariants to be framed onto \ncommands in context by distributing 1 Logics for high-level languages often ignore .xed-length arithmetic, \neven when that is what is provided by real implementations. them through the speci.cations of parameters \n[9]. A major goal of the work described here is to bring the power and concision of higher-order frame \nrules to reasoning about machine-code pro\u00adgrams. At .rst sight, it may seem unclear how to incorporate \neven the .rst-order frame rule {P } C {Q} {P * R} C {Q * R} into a system for reasoning about machine \ncode. Firstly, the frame rule is typically justi.ed using a global property of commands with respect \nto a semantics de.ned over partial heaps: if a command executes without faulting in some heap, then it \ndoes so in any extension of that heap and moreover, if it terminates it preserves the extension. Partial \nheaps in the semantics model a built-in allocator, but, as in our previous work in low-level code [6, \n7], we do not wish to de.ne the ground semantics (with respect to which we interpret speci.cations) using \npartial heaps: whatever memory is in the machine is there all the time, and the allocator is just another \npiece of code to be speci.ed and veri.ed in our framework. Secondly, the postcondition of a triple corresponds \nto the single exit point of a .rst-order command. Machine code fragments do not have single exits, or \neven a natural, local notion of terminating exe\u00adcution. We are ultimately concerned with the observable \nbehaviour of whole programs, and do not wish to restrict ourselves to a form of speci.cation that relies \non the non-observable, intensional prop\u00aderty of reaching a particular intermediate program counter value. \nWe thus take our basic form of safety speci.cations to be one that only involves a precondition: execution \nfrom a given address in a state satisfying the precondition is safe. As Chlipala [16] observes, it is \nnot obvious how to attach a frame soundly to such a speci.\u00adcation. We address these problems by going \nbeyond a shallow em\u00adbedding of speci.cations of individual program points, to an em\u00adbedding of a fully-.edged \nspeci.cation logic [23, 34], making the context within which code fragments are proved explicit, and \nwith a semantics that captures (but a surface notation which hides) the way in which frames are preserved. \nThe speci.cation logic allows one to work with subtle patterns of invariant preservation, but does not \nimpose particular forms of speci.cation. Rather, it provides building blocks from which more complex \npatterns, including Hoare triples, may be built. The rich, well-behaved theory of the core logic allows \nderived rules for new forms of speci.cation to be expressed and proved concisely. We have formalized \nour speci.cation logic in the Coq proof assistant, and instantiated it for the particular case of a model \nfor sequential x86 machine code. Our formalization also includes a range of re.ective tactics for solving \nseparation entailments and performing speci.cation logic proofs at a high level of abstraction. This \npaper mainly discusses the logic in a machine-independent way, but we use the x86 instantiation for examples \nand motivation. In summary, the contributions of this work include: 1. A separation logic for unstructured \nmachine code that supports both .rst-and higher-order frame rules. 2. Accounts of speci.cation-level \nconnectives including framing, a read-only frame, a later modality and a full range of intu\u00aditionistic \nconnectives, all with good logical properties. 3. Examples of higher-level patterns, such as Hoare triples, \nand associated proof rules being de.ned smoothly within the logic. 4. An certi.ed assembler, supporting \nconvenient macro de.nitions with internal label generation and natural derived proof rules, with examples \nincluding while-program constructs and proce\u00addure calling.   5. A semantics involving no instrumention \nor other modi.cations to the underlying machine model. Memory can be total, and step-counting and auxiliary \nvariables happen only in the logic. 6. All this is formalized in Coq, with an instantiation for x86 \nmachine code and tactic support for high-level proving. The formalization is available via the authors \nweb pages.  2. Machine model Our separation logic is not tied to any particular machine architec\u00adture, \nbut in order to illustrate its application we will be presenting examples from 32-bit x86, the architecture \nfor which we have built a model in Coq.2 In this section we present enough concrete detail of this model \nto support subsequent sections. We have modelled a subset of the 32-bit x86 instruction set, considering \nsequential execution only, but treating memory, regis\u00adters and .ags in suf.cient detail to obtain accurate \nspeci.cation of its behaviour. Machine words and arithmetic We model n-bit machine words simply as n-nary \ntuples of boolean values, deploying an indexed type in Coq for the purpose. The x86 architecture makes \nvarious use of 8-bit (BYTE), 16-bit (WORD), 32-bit (DWORD) and 64-bit (QWORD) values, and so nat-dependent \ntypes in Coq are a boon to speci.cation. Logical and arithmetic operations are de.ned directly in terms \nof bits, although to prove useful properties of arithmetic it proved handy to map words into arithmetic \nmodulo 2n, making use of the ssreflect library for algebraic identities [20]. Machine state The state \nof the machine is described by a triple of registers, .ags and memory state: S = (reg . DWORD) \u00d7 (.ag \n. {true, false, undef}) \u00d7 (DWORD . (BYTE l {unmapped})) Register state is a straightforward mapping from \nthe x86 s nine core registers (EAX, EBX, ECX, EDX, ESI, EDI, EBP, ESP and EIP) to 32-bit values. The \nEIP instruction pointer register is the x86 s program counter and points to the next instruction to be \nde\u00adcoded by the processor. Rather than model the special EFLAGS as a monolithic register, we split it \nup into boolean-valued .ags. The undef value represents the unde.ned state in which many instruc\u00adtions \nleave .ags. Any dependence of execution on an unde.ned state, such as in a conditional branch instruction, \nis then treated as unspeci.ed behaviour. Memory is modelled straightforwardly as 32-bit-addressable bytes, \nwith the possibility that any byte might be missing or inaccessible. For now, we are not interested in \n.ner distinctions such as read-only or no-execute, though it would be a simple matter to incorporate \nthese notions. It is however important to note that the partiality of memory has nothing to do with the \npartial states of separation logic; indeed we could choose to model and fully specify the x86 support \nfor fault handling, using the very same logic. Instructions Any machine model must of course include \na datatype of instructions. The x86 instruction set is notoriously large and baroque but by careful subsetting \nand factoring our datatype is made reasonably concise. Figure 1 presents the instruc\u00adtion datatype, with \nonly some names changed for the purposes of this paper. Instruction decoding The x86 instruction format \nis also complex, being variable in length and not canonical: a single instruction can sometimes be encoded \nin many ways. We have implemented an 2 There is no particular reason for choosing x86 over x64 or ARM. \nTypeof d = if d then DWORD else BYTE Scale = S1 | S2 | S4 | S8 MemSpec = Reg \u00d7 option (NonSPReg \u00d7 Scale) \n\u00d7 DWORD RegMem = RegMemR (r:Reg) | RegMemM(ms:MemSpec) RegImm d = RegImmI (c:Typeof d) | RegImmR (r:Reg) \nSrc = SrcI (c:DWORD) | SrcM (ms:MemSpec) | SrcR(r:Reg) DstSrc d = | RR (dst:Reg) (src:Reg) | RM (dst:Reg) \n(src:MemSpec) | MR (dst:MemSpec) (src:Reg) | RI (dst:Reg) (src:Typeof d) | MI (dst:MemSpec) (src:Typeof \nd) BinOp = ADC | ADD | AND | CMP | OR | SBB | SUB | XOR UnaryOp = INC | DEC | NOT | NEG | POP BitOp \n= BT | BTC | BTR | BTS ShiftOp = ROL | ROR | RCL | RCR | SHL | SHR | SAL | SAR Count = ShiftCL | ShiftImm \n(b:BYTE) Condition = O | B | Z | BE | S | P | L | LE Instr = | UOP (d:bool) (op:UnaryOp) (dst:RegMem) \n| BOP d (op:BinOp) (ds:DstSrc d) | BITOP (op:BitOp) (dst:RegMem false) | TEST d (dst:RegMem) (src:RegImm \nd) | MOV d (ds:DstSrc d) | SHIFTOP (d:bool) (op:ShiftOp) (dst:RegMem) (c:Count) | MUL (src:RegMem) | \nLEA (reg :Reg) (src:RegMem) | JCC (cc:Condition) (dir:bool) (tgt:DWORD) | PUSH (src:Src) | POP (dst:RegMem) \n| CALL (src:Src) | JMP (src:Src) | RET (size:WORD) | CLC | STC | CMC | HLT Figure 1. Instruction datatype \ninstruction decoder as a partial function decode : DWORD \u00d7 (DWORD . (BYTE l {unmapped})) -DWORD \u00d7 Instr \nsuch that if decode(i, m) = (j, .) then memory m from address i up to but not including address j is \nde.ned and decodes to in\u00adstruction .. The decoder reads memory incrementally, returning an unde.ned value \nif memory is inaccessible or out of range, or if the contents do not describe an instruction in our chosen \nsubset. (There is no need to specify explicitly a maximum instruction length.) Lift\u00ading decode to machine \nstates, and threading the updating of the EIP register through, yields a partial function in S -S\u00d7 Instr. \nInstruction execution Instruction execution is given by a partial function on states S \u00d7 Instr -S, which \nwhen composed with instruction decoding gives rise to a small-step transition function on machine states \nstep : S -S. When this function is unde.ned, it means that either a fault occurred (such as the decoding \nof an illegal instruction or an access to unmapped memory), or behaviour is simply unspeci.ed (such as \nbranching on an unde.ned .ag).  3. Assertion logic 3.1 Partial states Assertions in separation logic \ndescribe a subset, or footprint , of the machine state. For high-level imperative programs with dy\u00adnamic \nallocation this footprint consists of a subset of the heap. In\u00addeed a common idiom is to prove that some \ncode starts or .nishes with an empty heap. Here, there is no such thing: we have the whole machine at \nour disposal, and we must carve out our own abstractions such as heaps or stacks, so the footprint is \nsimply that part of the state that we care about right now. We also .nd it useful to use separation in \ndescribing the manipulation of registers and .ags, and so de.ne partial states as follows, noting the \nresemblance to the de.nition of total states in Section 2. S = (reg -DWORD) \u00d7 (.ag -{true, false, undef}) \n\u00d7 (DWORD -(BYTE l {unmapped})) There is a partial binary operation l on elements of S, de.ned when its \noperands have disjoint domains on all three tuple compo\u00adnents and yielding a tuple with the union of \neach of the maps. This makes (S, l) a separation algebra [15]; i.e., a partial commutative monoid.  \n3.2 Assertion logic An assertion is a predicate on partial states: asn . P(S) Since (S, l) is a separation \nalgebra, its powerset asn forms a complete boolean BI algebra, i.e., a model of the assertion language \nof classical separation logic, where the connectives are de.ned in the standard way [15]: .x: T . P (x) \n. P (x) .x: T . P (x) . P (x) x:T x:T P . Q . {s | s . P . s . Q} emp . {([], [], [])} P * Q . {s | \n.s1, s2. s = s1 l s2 . s1 . P . s2 . Q} P -* Q . {s2 | .s1. .s = s1 l s2. s1 . P . s . Q} The propositional \nconnectives (., T) and (., .) are just binary and nullary special cases of . and . respectively. As usual, \nentail\u00adment is de.ned as P f Q . P . Q, and we write f P for T f P and P = Q whenever P f Q and Q f P \n. There is a notion of points-to [35] for registers and for .ags: r ff b . {([], [f . b], [])} f v . \n{([r . v], [], [])} The meaning of the points-to assertion for memory, i..j . v, depends on the type \nof v; this is done using a type class [37] in our Coq implementation. For BYTE and DWORD types, points-to \nmeans that memory from address i to j contains that value. In these cases, j is uniquely determined to \nbe i + 1 or i + 4 respectively. For syntactic assembly instructions ., it means that the memory at i..j \ndecodes to .. In other words, decode(i, m) = (j, .) where m is the memory component of the state. Instruction \nencoding is not unique, so more than one byte sequence in memory may decode to the same .. We write i \n. v to mean .j. i..j . v. Discussion. Another option would have been to let the registers and .ags behave \nlike the stack in traditional separation logic [35] and not split them over *. This is the approach taken \nby Shao et al. [14, 31] and Chlipala [16], but it leads to the side condition on the frame rules that \nthe program may not modify any registers mentioned by the frame. In a setting where programs have multiple \nentry and exit points and may be self-modifying, it is not even clear what that side condition means \nor how to check it, so we instead make registers and .ags split across *, following Myreen et al. [29]. \n4. Speci.cation logic 4.1 Safety We extend the single-step partial function step : S -S to a function \nrun : N\u00d7 S -S, where run(k, s) is the state that results from successful execution of k instructions \nstarting from state s. Unlike the high-level languages typically modelled with Hoare logics, a CPU has \nno natural notion of .nishing a computation. It will run forever until it either faults or loses power3. \nThis means that we cannot apply the standard Hoare-logic approach of describ\u00ading a computation by a precondition \nand a postcondition since there is no meaningful time to check the postcondition. Instead, speci.cations \nrevolve around safety. We characterise the safe machine con.gurations as the set of pairs (k, P ) : N\u00d7 \nasn such that the machine will run for at least k steps without faulting if started from a state in P \n: safe . {(k, P ) | .s . P. .s ; s. .s' : S. run(k, s) = s'} The relation s ; s states that all the mappings \nin s are also found in s. This is how we connect the partial states found in assertions to the total \nstates executed by the machine. Example 1. It is safe to sit in a tight loop forever. That is, .k, i. \n(k, (EIP f i * i . jmp i)) . safe The EIP register is the instruction pointer, and jmp i is an uncon\u00additional \njump to address i. The proof goes by induction on k. o The number k plays the role of a step index [2]. \nWe are ulti\u00admately always interested in proving computations safe for an ar\u00adbitrary number of steps, \nbut exposing an intermediate step index gives us a value on which we can do induction. As a running example, \nwe will attempt to specify the uncondi\u00adtional jump instruction. We can show that for all i, a, k, R, \n(k, Q * R) . safe . (k + 1, P * R) . safe where P = (EIP and f i * i . jmp a) Q = (EIP f a * i . jmp \na) In words, if you need to show that P * R is a safe con.guration for k + 1 steps, it suf.ces to show \nthat Q * R is safe for k steps. When a speci.cation follows this pattern, we can think of P as a precondition, \nQ as a postcondition, and R as a frame. The speci.cation does not say that Q * R will ever hold. Rather, \nit requires that if Q * R does hold, then we are in a safe con.gu\u00adration. This can be seen as a CPS version \nof Hoare logic, which is appropriate for machine code since nothing ever returns or .nishes at this level \n[5, 31, 38]. We will re.ne this speci.cation in later examples as we develop constructions at higher \nlevels of abstraction.  4.2 Speci.cation logic Reasoning directly about membership of safe is awkward \nsince the step index and frame are explicit and visible even though their use always follows the same \npattern. The solution is to instead consider safe as a formula in a speci.cation logic. We de.ne a speci.cation \nto be a set of of (k, P )-pairs that is closed under decreasing k and under starring arbitrary assertions \nonto P : spec . {S . N\u00d7asn | .(k, P ) . S, k' = k, R. (k', P *R) . S} 3 Even when it faults, it will \ntypically reboot and so keep running, but this behaviour is outside of our model.  Intuitively, a speci.cation \nS : spec describes how many steps the machine has to execute before it no longer holds and what frames \nthe execution will preserve. This idea comes from the work of Birkedal, Torp-Smith and Yang on higher-order \nframe rules [8, 10], and spec is essentially a step-indexed version of Krishnaswami s speci.cation logic \nmodel [23]. The de.nition of spec is such that safe . spec. Furthermore, spec is a complete Heyting algebra \nand thus a model of intuition\u00adistic logic. This gives us a notion of entailment (f .) and the logical \nconnectives (., ., ., ., T, ., .) with the expected rules. The de.nitions of the connectives follow a \nstandard Kripke model: .x: T . S(x) S(x) .x: T . S(x) S(x) x:T x:T S . S ' {(k, P ) | .k ' = k. .R. \n(k ' , P * R) . S . (k ' , P * R) . S ' } Again, the propositional connectives (., T) and (., .) are \njust binary and nullary special cases of . and . respectively. Notice how the semantics of . requires \narbitrary frames to be preserved across the implication. This was not a choice we made it is the only \nde.nition that makes . be the right adjoint of ., and it falls out of giving standard Kripke semantics. \nWe also get two new connectives: the later connective C and the frame connective .. We will de.ne and \ndiscuss these in the next two subsections. They will enable us to state the speci.cation of the jmp instruction \nfrom Section 4.1 more succinctly: C safe.(EIPfa*i.jmp a) f safe.(EIPfi*i.jmp a) (1) We can even factor \nout the duplicated part of the assertion and just write f (C safe . (EIP f a) . safe . (EIP f i)) . i \n. jmp a or, informally, reading from right to left: given that i points to instruction jmp a, it is safe \nto execute with the instruction pointer set to i if it is later safe to execute with the instruction \npointer set to a .  4.3 Frame connective Following the literature on higher-order frame rules [8, 10, \n23], we de.ne the frame connective . : spec \u00d7 asn . spec as S . R {(k, P ) | (k, P * R) . S} This is \nalso known as the invariant extension connective [8] because an intuitive reading of S .R is that the \ncomputation described by S is allowed to additionally depend on and maintain the invariant R. Note that, \nby unpacking the de.nitions, f safe . P iff .k, R. (k, P * R) . safe relating judgements in the speci.cation \nlogic with the safety of executions. Since we de.ned spec such that any S can be extended by any invariant, \nwe can immediately prove the higher-order frame rule: FRAM E S f S . R The frame connective distributes \nover all other connectives, including C and itself. That means, for example, that .-. (S . S ' ) . R \n= S . R . S ' . R It also interacts with emp and * as follows. .-E M P S . emp = S .-* S . R1 . R2 = \nS . (R1 * R2) Example 2. We can now start to see why FR AME should be thought of as a frame rule. Assume \nwe have proved for some P and Q that f safe . Q . safe . P. Then by FR A M E, .-. and .-*, we can derive \nf safe . (Q * R) . safe . (P * R). Visually this looks like the standard frame rule, and it performs \nthe same function: to extend both pre-and post-condition by an invariant. o The formula S . R is covariant \nin S with respect to entailment, meaning that S f S ' .-f S . R f S ' . R The variance in R is more complicated \nand will be discussed in Section 7.1. Example 3. To illustrate informally how FRAM E generalises the \nstandard .rst-order frame rule, consider a program in a high-level functional programming language f1 \n: (unit . unit) . unit, whose speci.cation is, for some particular P , Q and R, .g. {P * R} g () {Q * \nR} . {P * R} f1(g) {Q * R} That is, f1 forwards the speci.cation of g. Most likely, f1 simply applies \nits argument to the unit value, but assume that it has been veri.ed separately and we should not see \nits implementation. If we have g1 with speci.cation {P } g1 () {Q}, we cannot immediately apply f1(g1) \nsince the speci.cation does not match what f1 requires. However, we can apply the ordinary frame rule \nto deduce that g1 also has the speci.cation {P * R} g1 () {Q * R}, and then we can call f1(g1) if we \nare in a state satisfying P * R. Now instead consider an f2 with the speci.cation .g. {P } g () {Q} . \n{P } f2(g) {Q} and a g2 with speci.cation {P * R} g2 () {Q * R}. It is impossible with just the standard \nframe rule to call f2(g2) since the speci.ca\u00adtion of g2 cannot be re.ned to match what is assumed by \nf2. But with the higher-order frame rule, we can instead re.ne the speci.\u00adcation of f2 to be .g. ({P \n} g () {Q} . {P } f2(g) {Q}) . R = .g. {P } g () {Q} . R . {P } f2(g) {Q} . R = .g. {P * R} g () {Q * \nR} . {P * R} f2(g) {Q * R} It is now compatible with our g2. Without the higher-order frame rule, we \nwould have had to either re-verify the implementation of f2 or generalise the original speci.cation of \nf2 to explicitly quantify over all possible frames that may be threaded through. The latter option is \nessentially what the de.nition of spec does, but this is invisible and implicit. o Using . to give concise \nand modular speci.cations to higher\u00adorder functions is as important here as in any other separation logic, \nbut that is not our main reason for including .. We do it because it allows our logic to have a frame \nrule despite the program being un\u00adstructured and low-level. Chlipala [16] uses explicit second-order \nquanti.cation in place of a frame rule, whilst Shao et al. [14, 31] have a frame rule that only applies \nto judgements in a very restric\u00adtive speci.cation logic; in particular, it does not apply directly to \nspeci.cations of function pointers.  4.4 Later connective Just as we hide the explicit frames using \n., we hide the step indexes using the later connective, C. This is a trick pioneered by Nakano [30] that \nexploits the fact that we are never interested in the absolute number of steps but only that they are \nthe same or differ by exactly one between two speci.cation formulas. We de.ne  CS {(k, P ) | .k ' < \nk. (k ' , P ) . S} Because any S : spec is closed under decreasing steps, an equivalent de.nition is \nthat (0, P ) . CS for all P , and (k+1, P ) . CS iff (k, P ) . S. The closure under decreasing steps \nis expressed logically as the rule C-W EA K E N S f CS As mentioned in Section 4.1, the purpose of step \nindexes is to serve as a handle for induction. We can phrase the induction principle on natural numbers \nusing the following rule [3, 30], which is named for its similarity to a corresponding rule in G \u00a8ob \nodel-L\u00a8logic [3]. CS f S L \u00a8 OB f S The L \u00a8ob rule is a reformulation of the strong induction principle \nfor natural numbers: if (.k ' < k. P (k ' )) . P (k) for all k, then P (n) holds for any n. It is a powerful \nrule in that it almost allows assuming the formula one wants to prove, except that the assumption may \nonly be used after taking one step of computation. Example 4. Recall the speci.cation of a tight loop \nfrom Exam\u00adple 1. We can now express and prove that inside the speci.cation logic in just two steps: (1) \nC safe . (EIP f i * i . jmp i) f safe . (EIP f i * i . jmp i) L \u00a8 OB f safe . (EIP f i * i . jmp i) o \nThe C connective distributes over every other connective we have mentioned except for . and existential \nquanti.cation over empty types. Discussion. Like we saw in the rule for jmp (Equation (1)), every step \nof computation allows us to relax our remaining proof obligation by adding a C. For example, we could \nprove that f (CCsafe.(EIPfj).safe.(EIPfi)).i..j.(nop; nop) (2) where nop is the no-operation instruction. \nThere are two C s on the postcondition of (2) because it takes two steps of computation to get there. \nIt turns out, however, that it is never useful to have more than one C applied to a speci.cation since \nthe purpose of step indexes is to do induction, and induction will always give us the necessary assumptions \non the immediate predecessor of the number of interest. Furthermore, we have found that C is not necessary \nin code that only moves forward. L \u00a8ob induction only makes sense when veri\u00adfying loops, and a loop requires \nsome form of backward jump un\u00adless we consider highly-contrived self-modifying code. Therefore, in practice, \nwe would state (2) without any C-connective at all.  4.5 Read-only frame The instruction rules we have \ndiscussed so far are too weak for some purposes. Recall the rule for jmp: f (C safe . (EIP f a) . safe \n. (EIP f i)) . i . jmp a Because the meaning of i . jmp a is only that the memory starting at i decodes \nto jmp a, the rule would be satis.ed in a semantics where the jmp instruction not only performed the \njump but also replaced its own machine code in memory with a different byte sequence that also decoded \nto jmp a. This would be a problem for programs whose code needs to stay unmodi.ed; e.g., to verify that \nthe checksum of the code remains the same. Our solution is to make this speci.cation more precise by \nem\u00adploying the read-only frame connective, de.ned as S 0 R .s . R. S . {s}. A more precise speci.cation \nfor jmp is then f (C safe . (EIP f a) . safe . (EIP f i)) 0 i . jmp a Intuitively, S 0 R requires S not \nonly to preserve the truth of R but to leave unmodi.ed the underlying state fragment that made R true. \nThe state may be changed temporarily, just as R might be broken, as long as it is restored at the end \nof the computation described by S. Like for ., there is a frame rule: FR A M E -RO S f S 0 R The 0 connective \ndoes not distribute over every other connec\u00adtive like . does, but it does distribute over ., ., T, ., \n0, C. It only distributes in one direction over . and .: (.a. S(a) 0 R) f (.a. S(a)) 0 R (S . S') 0 R \nf S 0 R . S' 0 R The formula S 0R is covariant in S and contravariant in R with respect to entailment, \nmeaning that S f S ' R ' f R 0-f S 0 R f S ' 0 R ' Another convenient property is that existential quanti.ers \ncan be moved in and out of the frame: S 0 (.x. R(x)) = .x. S 0 R(x) These last two properties of 0 about \nvariance and commuting with existentials do not generally hold for .. The cases where they hold are discussed \nin Sections 7.1 and 7.2. We explore further properties of 0 in Section 7.3. Discussion. This connective \nis reminiscent of fractional permis\u00adsions [12] but more coarse-grained and light-weight. We mention connections \nto other notions of weak ownership [22] in Section 9. Our de.nition of 0 may not be the only good one, \nbut we have examined three other candidate de.nitions and found that the one given above had the most \nconvenient properties for our purposes. The candidates relate to each other as follows. .R ' f R * T. \nS . R ' f .R ' f R. S . R ' f .s . (R * T). S . {s} = .s . R. S . {s} 5. High-level assembly code 5.1 \nBasic blocks Using safe and the connectives discussed so far, we can specify code with multiple entry \npoints and exit points, jumps to code pointers, self-modi.cation, and so on. In practice, though, most \ncode is much simpler. For code that behaves like a basic block, with control .ow always coming in at \nthe top and going out at the bottom, we can describe its behaviour with a Hoare triple, de.ned in the \nspeci.cation logic as: {P } c {Q} .i, j. (safe . (EIP f j * Q). safe . (EIP f i * P )) 0 i..j . c Example \n5. The instruction mov r, v (move literal v to register r) can be speci.ed as f {r?} mov r, v {r f v}, \nwhere r? is shorthand for .v. r f v. This is much more compact and readable than writing the speci.cation \nin terms of safe. o  This triple satis.es the structural rules we expect from a Hoare triple in separation \nlogic: ' ' Q ' P f P S f {P } c {Q ' } f Q S f {P } c {Q} S f .x. {P (x)} c {Q} S f {P } c {Q} S f {.x. \nP (x)} c {Q} S f {P * R} c {Q * R} The frame rule for the triple follows from FR A ME and the fact that \n. distributes into the triple: .-TRI P L E {P } c {Q} . R = {P * R} c {Q * R} There is no rule of conjunction \nfor the triple since this would be unsound in the presence of FRAM E [8]. Discussion. This kind of triple \nis certainly not the only useful one. One could also adapt the position-indexed triples of Myreen and \nGordon [29] to this setting, allowing use of the triple metaphor in specifying code with multiple entry \nand exit points. It would be a matter of taste whether this seemed more convenient to work with than \nreasoning directly in terms of safe. The triple de.ned here can be thought of as encoding a very simple \ncalling convention: inlining; i.e., concatenation of code. We envision de.ning triples for other conventions \nas needed and proving similar properties about them. See Section 5.5 for another example. It is a valid \nquestion to ask why there is no C on the postcon\u00addition part of the triple so it would read C safe . \n(EIP f j * Q). It would give stronger speci.cations for single instructions like in Example 5, but as \ndiscussed in Section 4.4, it would also be unnec\u00adessary since control .ow always moves forward in a triple. \nWe will also see in Section 5.3 that there are useful values of c that take no computation steps.  5.2 \nRules for x86 instructions With a variety of logical building blocks in place, we can give ap\u00adpealingly \nsimple rules for x86 instructions. These split into instruc\u00adtions that do not touch the instruction pointer, \nfor which we can use the Hoare-triple form, and control .ow instructions, for which we describe their \neffect on the instruction pointer explicitly. Example 6. The following rule for add register indirect \nwith off\u00adset is a typical instance. Here d is a literal DWORD offset, and addition of two 32-bit values \nproduces a pair (c, v) where v is the 32-bit (truncated) result, and c is the carry into bit 32. f {r1 \nf v1 * OF? * SF? * ZF? * CF? * PF?} add r1, [r2 + d] {r1 f v * OF f \u00ac(msb v1 . msb v2) . msb v * SF \nf msb v * ZF f (v = 0) * CF f c * PF f lsb v} . (r2 f w * w + d . v2) where v1 + v2 = (c, v) The \u00ac and \n. operators are boolean negation and xor respectively. The instruction affects .ags OF, SF, ZF, CF and \nPF whose values initially are arbitrary (F ? is shorthand for .f. F ff, where f may be undef). Notice \nthe framing of invariant registers and memory. o Example 7. For the jump-if-zero instruction, we specify \ntwo post\u00adconditions , the .rst for when the branch is taken, and the second for when it isn t. f (C safe \n. (b . EIP f a * ZF f b) . safe . (\u00acb . EIP f j * ZF f b) . safe . (EIP f i * ZF f b)) 0 i..j . jz a \nNote the use of the later connective when the (possibly backwards) branch is taken. o Our approach is \nto give a very general speci.cation to each instruction and then on top of that provide convenience de.nitions \nfor common cases. In a sense, our rules are therefore just a logical reformulation of the operational \nsemantics, which may seem a bit unimpressive but turns out to be a strong platform on which to build \nhigher-level layers of abstraction.  5.3 Instruction encoding and assembly language We have implemented \nan encoder for syntactic instructions, and it has the property that i..j . encode(i, .) f i..j . . That \nis, if the memory at i..j contains the sequence of bytes encode(i, .), then that memory will decode to \nthe instruction .. The instruction decoder referred to here is the same one that is part of the operational \nsemantics for the machine. The encode function takes i as parameter because the encoding of x86 instructions \nis not generally position-independent. This encoder is the main ingredient in our assembler: a certi.ed \nand executable Coq function that takes a program as input and produces a list of bytes as output. A program \nis a value in the following inductive de.nition. p ::= (.) | skip | p; p | l: | LOCAL l; p That is, a \nprogram is essentially a list of instructions with la\u00adbel markers l: interspersed. A label l may be declared \nlocal to program p with the LOCAL l; p construction. A label is simply a memory address; i.e., a 32-bit \nword, and it can therefore be used as an argument to jump instructions. The following is a closed pro\u00adgram \nthat loops forever. LOCAL l; l: jmp l The LOCAL constructor in our Coq implementation has type (DWORD \n. program) . program, so writing LOCAL l; p is just syntactic sugar for LOCAL(.l. p(l)). The bene.t of \nmodelling label scopes with function spaces is that Coq handles all aspects of label naming transparently, \nincluding the necessary capture\u00adavoidance and a-conversion. The downside is that it is not viable to \nstatically rule out ill-formed programs, such as programs that place the same label more than once. The \nassembler function, assemble, is partial and maps an ad\u00address and a program to a sequence of bytes. It \nis unde.ned if the program is ill-formed. Where de.ned, it has the correctness prop\u00aderty that i..j . \nassemble(i, p) f i..j . p Here, i..j . p is de.ned recursively as follows. i..j . (.) i..j . . i..j . \nskip i = j . emp i..j . p1; p2 .i ' . i..i ' . p1 * i ' ..j . p2 i..j . LOCAL l; p .l. i..j . p(l) i..j \n. l: i = j = l . emp Recall that the de.nition of triples {P } c {Q} in Section 5.1 did not require c \nto have a particular type; the de.nition and its rules are valid for any c that can occur on the right \nof a points-to. Thus, we can put a program p in a triple, and it turns out that the following rules hold. \nS f {P } p1 {Q} S f {Q} p2 {R}f {P } skip {P } S f {P } p1; p2 {R} S f {P } . {Q} S f .l. {P } p(l) {Q} \nS f {P } (.) {Q} S f {P } LOCAL l; p {Q}  There is no useful rule for the case of l: in a triple. Example \n8. We cannot specify jmp with a triple in any useful way, but we can specify the special case of a tight \nloop shown above: f {emp} LOCAL l; l: jmp l {.} The proof is by .rst applying the triple rule for LOCAL, \nthen unfolding the de.nition of the triple and applying the result of Example 4. o  5.4 Assembly macros \nA useful assembly language has not only labels but also macros; i.e., parameterised de.nitions that expand \nto instruction sequences when invoked. We get macros almost for free since assembly pro\u00adgrams are written \nand parsed inside Coq and can be intermixed with all the features of its term language. This includes \nlet-bindings, .xpoint computations, custom syntax, coercions, overloading and other features of a modern \ndependently-typed programming lan\u00adguage. An example of a very useful macro is the following de.nition \nof while(p1, t, b, p2), where p1 is a loop test, p2 is a loop body, t encodes the combination of processor \n.ags to be branched on, and b is a boolean that indicates whether the test should be inverted. while(p1, \nt, b, p2) LOCAL l1, l2; jmp l1; l2: p2; l1: p1; jcc t, b, l2 The jcc instruction is the general conditional \njump on x86. We see here how LOCAL lets us declare labels that will be fresh for every invocation of \nthe while macro. Real-world macro assemblers also have that functionality, although the scope is usually \ntied to the nearest named macro or global label. Our Coq notations for assembly syntax, including LOCAL, \nare chosen to be compatible with MASM, the Microsoft assembler. Macros such as while give us the usual \nconvenience of not having to write similar code many times. But even better, it lets us avoid writing \nsimilar proofs many times. If the body and test can be speci.ed in terms of a triple, then the loop as \na whole also has a triple speci.cation: S f {P } p1 {.b ' . I (b ' ) * cond(t, b ' )} S f {I(b) * cond(t, \nb)} p2 {P } WHI LE S f {P } while(p1, t, b, p2) {I(\u00acb) * cond(t, \u00acb)} Here, cond(t, b) translates t, \nof type Condition from Figure 1, to an assertion that tests the relevant .ags. For example, cond(Z, b) \n= ZF f b, where ZF is the zero .ag. There are two loop invariants, P and I, representing the state before \nand after executing the test p1 since this may have side effects. The proof of the WH I L E rule involves \nC-operators and the L \u00a8 O B rule, but these technicalities do not leak out into the rule statement. With \nif and while macros and the sequence operator on programs, we have the building blocks to easily write \nand verify programs with structured control .ow. These constructs also facilitate using our assembly \nlanguage as the target of a veri.ed compiler from a structured language, which is something we hope to \ninvestigate more in future work.  5.5 Procedure calls The triple {P } c {Q} encodes and abstracts the \noften-occurring programming pattern of structured control .ow. Another crucial pattern to capture is \nprocedure calls. We will here show the theory of a very simple calling convention [29]: store the return \naddress in register EDX and jump to the procedure entry point. The following macro calls the procedure \nwhose code is at address f. call f LOCAL iret; mov EDX, iret; jmp f; iret: The calling convention does \nnot specify how to pass arguments or return values; this is instead part of individual procedure speci.\u00adcations. \nA more realistic calling convention would maintain a stack of arguments and return addresses to allow \ndeep call hierarchies and reentrancy, but this would clutter our examples with arithmetic side conditions \nbecause the stack has to be .nite [29]. The following de.nition describes the behaviour of a procedure \nstarting at f with precondition P and postcondition Q. f.{P }{Q} .iret. safe . (EIP f iret * EDX? * Q) \n. safe . (EIP f f * EDX f iret * P ) Recall that EDX? is shorthand for .v. EDX f v. This de.nition satis.es \nthe usual rules for a triple-like formula, including .-PROC f.{P }{Q} . R = f.{P * R}{Q * R} In contrast \nwith the triple de.ned in Section 5.1, this de.nition of a procedure speci.cation does not mention the \ncode stored at f. The code should be mentioned separately from its behaviour such that the footprint \nof the code covers both the caller and the callee. The rule for calling a procedure looks fairly standard: \nCA L L Cf.{P }{Q} f {P } call f {Q} . EDX? It reveals that EDX is overwritten as part of the calling \nconvention. The C modality on the premise, together with L \u00a8 O B, permits recur\u00adsion [3]. Example 9. \nThis is the .rst of three examples to illustrate inde\u00adpendent veri.cation of caller and callee. Consider \nthe following de.nition of a program that calls some procedure at f twice: pcaller(f) call f; call f \nIf the intention with this program is to compose it with a proce\u00addure that satis.es Scallee(f) .a. f.{EAX \nf a}{EAX f a + 2}, then we can specify the caller as Scallee(f) f {EAX f a} pcaller(f) {EAX f a + 4} \n. EDX? We can prove this speci.cation directly from the program sequenc\u00ading rule and CA LL. No C connective \nis put on the assumption since no recursion is intended. o If a procedure body p is structured and returns \nat its very end, we can prove its speci.cation through the following rule. S f {P } p {Q} 0 EDX? BO DY \nS f f.{P }{Q} 0 f . (p; jmp EDX) In words, this means that calling f behaves as (P, Q) when in memory \nwhere the program (p; jmp EDX) is at address f, as\u00adsuming we can prove the given triple, which is allowed \nto access EDX as long as it restores its value in the end. Example 10. The following program almost satis.es \nScallee as de\u00ad.ned in Example 9. pcallee inc EAX; inc EAX; jmp EDX We say almost because the inc instruction \naffects the status .ags of the CPU as a side effect. The caller is not interested in the .ags, but they \nhave to be in the speci.cation of pcallee since they do get affected. Let .ags be the assertion that \nall .ags are of some (existential) value. Then we can prove  f Scallee(f) . .ags 0 f . pcallee. The \nproof is by applying BO DY, whose conclusion matches the above speci.cation after rewriting by .-PROC. \no The next example demonstrates how to compose a caller and a callee, even if the callee has a larger \nfootprint than what the caller assumes. This shows how to execute the informal reasoning from Example \n3 in our logic. Example 11. We can now compose the implementations of the caller from Example 9 and the \ncallee from Example 10 to obtain the following closed program. We arbitrarily choose to place the callee \nin memory before the caller. pmain(entry) LOCAL f; f: pcallee; entry: pcaller(f) We can give the following \nspeci.cation to this program, which says that the code between entry and j will increment EAX by 4 and \nstep on EDX and the .ags. f (safe . (EIP f j * EAX f a + 4) . safe . (EIP f entry * EAX f a) ) . (EDX? \n* .ags) 0 (i..j . pmain(entry)) The crucial step in proving this speci.cation is to satisfy the caller \ns assumption, Scallee, with the callee speci.cation, which is essentially Scallee . .ags. The former \nentails the latter, but here we would need the entailment to go the other way. Instead, we exploit that \nFRAM E is a higher-order frame rule [9] and lets us frame an assertion on to the left and right side \nof an entailment simultaneously. This is allowed by the rules .-f and 0-f from Sections 4.3 and 4.5. \nAbbreviating Scaller(f ) .a. {EAX f a} pcaller(f) {EAX f a + 4} . EDX?, we can derive Ex. 9 Scallee(f) \nf Scaller(f) .-f Scallee(f) . .ags f Scaller(f) . .ags 0-f Scallee(f) . .ags 0 f . pcallee f Scaller(f) \n. .ags 0 f . pcallee We know from Example 10 that f Scallee(f) . .ags 0 f . pcallee, so by transitivity \nof f we conclude f Scaller(f)..ags0f .pcallee. From this, it is straightforward to derive our desired \nspeci.cation for pmain. o The preceding example showed how to use FR A M E as a second\u00adorder [9], or, \nhypothetical [32] frame rule. The procedure involved was .rst-order at run-time, though. The following \nexample in\u00advolves a proper higher-order procedure; i.e., a procedure that takes a pointer to another \nprocedure as argument. Example 12. The simplest example of a higher-order procedure is apply , which \nin a functional programming language would be de.ned as apply(g, x) = g(x). In our set-up, an apply procedure \nthat takes its g argument in register EBX is implemented simply as papply jmp EBX. Its speci.cation re.ects \nhow it forwards the behaviour (P, Q) of g: fg.{P * EBX?}{Q} . f.{P * EBX f g}{Q}0 f . papply. o 6. \nPractical veri.cation We have used our Coq development not only to build a machine model and to validate \nthe logic developed in this paper; it is also an environment for building and verifying actual machine-code \nprograms. In this section we describe the Coq tactic support that we have developed for making machine \ncode veri.cation manageable, and present a slightly larger example of assembly language (seven in\u00adstructions!) \nin order to give a .avour of the Coq proof of its cor\u00adrectness. 6.1 Example: memory allocation We illustrate \nthe use of the logic, rules, and Coq tactics with a slightly more challenging example: the speci.cation \nof a memory allocator and its simplest possible realisation, the bumping of a pointer and checking it \nagainst a limit. Its speci.cation is as follows, parameterized by the number of bytes n to be allocated \nand an address fail to jump to on failure. allocSpec(n, fail, inv, code ) .i, j. (safe . (EIP f fail \n* EDI?) . safe . (EIP f j * .a. (EDI f a+n) * (a .. a+n . )) . safe . (EIP f i * EDI?)) . (ESI? * .ags \n* inv) 0 (i..j . code ) The speci.cation is framed by an assertion that register ESI is used as scratch \nstorage, .ags are updated arbitrarily, and an internal in\u00advariant inv is maintained. The latter might \nbe the well-formedness of some representation of free lists, or in our trivial allocator, sim\u00adply a pair \nof pointers. The calling convention is inline , in other words, the allocator is just a macro consisting \nof assembly in code . In Section 6.5, we will wrap a slightly less trivial calling convention around \nit. Control either drops through, if successful, or branches to ad\u00address fail, if memory cannot be allocated. \nOn success, the allocator leaves an address a in EDI that is just beyond the n bytes of mem\u00adory that \nwere allocated; on failure, EDI is trashed. Perhaps surprisingly, even a bump-and-check implementation \nconsists of seven instructions: allocImp(info, n, fail) mov ESI, info; mov EDI, [ESI]; add EDI, n; jc \nfail; cmp [ESI + 4], EDI; jc fail; mov [ESI], EDI. The implementation invariant inv is the following: \ninv(info) .base , limit. info . base * (info + 4 . limit) * (base .. limit . ). In other words, at address \ninfo there is a pair of pointers base and limit that bound a piece of mapped memory.  6.2 Applying instruction \nrules During a proof, we typically keep the goal in the form Sctx f (S . safe . P ) 0 R. The speci.cations \ndiscussed in this paper are easy to put into that form by applying distributivity rules for . and decurrying \nnested implications, and we have implemented a tactic to do this automatically. Typically, R describes \nthe code to be executed, and P describes the instruction pointer and the remaining state that will go \ninto proving the precondition of the next instruction. We may use the full range of speci.cation-logic \nrules on this goal, but eventually we will want to apply the lemma appropriate for the code that EIP \nis pointing to in P . We assume that the lemma has the same form as the goal and apply a lemma through \nthe following rule.  S ' ' ) 0 R ' ctx f (S ' . safe . P Sctx f S ' ctx P f P ' * RP R f R ' * T Sctx \nf (S . S ' . RP) 0 R SP EC APPLY Sctx f (S . safe . P ) 0 R The top premise is the lemma to be applied, \nand the bottom premise is the remaining proof obligation that describes the symbolic state after having \napplied the lemma. If the lemma is an instruction rule, the three middle premises correspond to satisfying \nits precondi\u00adtions at the level of speci.cations, data memory and code mem\u00adory respectively. The latter \ntwo can be dealt with by our entailment checker, described in the next subsection.  6.3 Assertion entailment \nsolving Much of the activity in a formal separation logic proof is proving entailment between assertions. \nThis happens every time a precon\u00addition needs to be discharged, and if it is not automated, the proofs \nwill drown in the details of fragile manual context manipulation and rewriting modulo associativity and \ncommutativity. Typically, we are given a description of the current state P and a precondition P ', and \nwe must show P f P ' * R for our own choice of frame R, which represents all the left-over state that \nwas not consumed by the precondition and can therefore be framed out. Our approach to this automation \nis similar to other separation\u00adlogic tools [4, 16]: if P and P ' consist only of *, emp and atomic assertions, \nwe iterate through the conjuncts of P ', attempting to unify each with a conjunct found in P and let \nthe two cancel out. Typically, P ' is full of holes corresponding to universally\u00adquanti.ed variables \nthat have yet to be instantiated. The holes are represented in Coq as uni.cation variables, which are \nidenti.ers that will receive a value upon being uni.ed with a subformula from P . Several subformulas \nof P may unify, but typically only one choice will permit the entailment as a whole to be solved. For \nexample, we may be proving EAX f i * j . 2 * i . 1 f EAX f U1 * U1 . U2 * T, where U1 and U2 are uni.cation \nvariables of type DWORD. If our algorithm should attempt to unify the atom U1 . U2 with the atom j . \n2, it will succeed, but the remaining proof obligation will be EAX f i * i . 1 f EAX f j * T The algorithm \nsucceeds even if it did not solve the goal entirely, leaving the rest to be proved interactively, but \nin this case there is no solution for the remaining part of the goal. Rather than try to support backtracking, \nwhich does not com\u00adbine well with interactive proof, we make the algorithm greedy but predictable: subformulas \nof P ' are uni.ed from left to right. In our current example, this would .rst .x the choice of U1 to \nbe i, and the second conjunct of P ' would therefore become i . U2, which rules out the bad uni.cation \nchoice from before. There is of course no guarantee that this always works, but we have found that it \nvirtually always works in practice as long as preconditions are written with this left-to-right order \nin mind. This happens naturally since it is also more readable for humans who read from left to right. \nIf the algorithm should still fail, it remains possible to manually instantiate the uni.cation variables. \nThe entailment solving algorithm is implemented with a hybrid approach, where the uni.cation is done \nby Coq s built-in higher\u00adorder uni.cation engine, while the cancellation of identical terms is done with \nproof by re.ection [17], which has good performance. If an entailment has existential quanti.ers on the \nleft-hand side, we can apply the rule .x. (C[P (x)] f Q) C[.x. P (x)] f Q where C is formula with a hole \nthat contains only *-connectives in the path from the root to the hole. This lets us effectively move \nthe quanti.ed variable into the Coq variable context. If an entailment has existential quanti.ers on \nthe right-hand side, we will eventually need to instantiate them with witnesses. This can be done with \nthe rule .x. (P f C[Q(x)]) P f C[.x. Q(x)] We immediately apply the rule, but we instantiate x with a \nuni.cation variable, which in practice defers instantiation until a uni.cation forces it to happen as \ndescribed above. We have extended the tactic for moving quanti.ers into the context so it also works \non speci.cation-logic entailments. For example, given the goal S ' f .x1. S . safe . (.x3. P (x1, x3)) \n0 (.x2. R(x2)), the extended tactic will introduce x1, x2 and x3 into the Coq variable context and leave \nthe new goal S ' f S . safe . P (x1, x3) 0 R(x2). The rules allowing x2 and x3 to be pulled out are given \nin Sections 4.5 and 7.2 respectively. 6.4 Proving the allocator Correctness consists of proving the \nfollowing, for any info, n, fail. f allocSpec(n, fail, inv(info), allocImp(info, n, fail)). Here is a \nfragment of the Coq proof script that deals with the second instruction in the implementation. (We make \nuse of ssreflect extensions to standard Coq tactic notation [20].) (* mov EDI, [ESI] *) rewrite {2}/inv. \nspecintros => base limit. specapply MOV_RM0_rule. -by ssimpl. For this instruction, almost everything \nis handled automatically. The initial rewrite simply unfolds the invariant inv to expose the existential \nquanti.ers. The custom tactic specintros pulls the existentially-quanti.ed variables from deep within \nthe goal to intro\u00adduce them into the Coq context. The tactic specapply l will .rst normalise both the \ngoal and lemma l to have the form required by the SP E C APP LY rule from Section 6.2. It will then invoke \nSPEC AP-P LY with l as the .rst premise. In this case, l is MOV_RM0_rule, the rule for instructions of \nthe form mov r1, [r2]. The second and fourth premises are trivial, leaving only the precondition of the \nmov rule as a subgoal. This can be discharged by our ssimpl tactic, which implements entailment checking \nas described in Section 6.3. In fact none of the instructions needs more than four lines of proof, and \nwe hope to reduce this further through the use of additional lemma and tactic support once we have more \nexperience with proving.  6.5 Wrapping the allocator Having veri.ed a component, such as the allocator, \nit is reasonably straightforward to use to the logic to verify higher-level abstrac\u00adtions in a modular \nway. As an example, we show the wrapping of the allocator in a procedure for consing onto a list.  We \nstart with the inductive list segment assertion of separation logic (originally due to Burstall [13]): \nlistSeg(p, e, vs) l .q. (p f v) * (p+4 f q) * listSeg(q, e, vs ' ) if vs = v :: vs ' p = e . emp otherwise \nHere, vs is a list of DWORDs and the assertion says that memory contains a linked list starting at p \nand ending at e with elements given by vs. A possible speci.cation for our cons function is consSpec(r1, \nr2, info, i, j, code ) .h, t, e, vs. (i . {r1 f h * r2 f t * listSeg(t, e, vs) * EDI?}{r1? * r2? * ((EDI \nf 0 * listSeg(t, e, vs)) . (.q. EDI f q * listSeg(q, e, h :: vs))}) . (ESI? * .ags * inv(info)) 0 (i..j \n. code ) specifying a procedure that is passed a value h in r1 and a pointer to a list starting at t \nin r2. On return, EDI is either zero, and the original linked list is preserved, or EDI points to a linked \nlist segment ending at e with h added as the new head element. An implementation is given by cons(r1, \nr2, info) LOCAL fail; LOCAL succeed ; allocImp(info, 8, fail); sub EDI, 8; mov [EDI], r1; mov [EDI + \n4], r2; jmp succeed; fail: mov EDI, 0; succeed : jmp EDX The proof that for any r1, r2, info, i and j, \nf consSpec(r1, r2, info, i, j, cons(r1, r2, info)) is entirely modular, relying on the BO DY rule and \nthe previous result that allocImp meets allocSpec. 7. Properties of the frame connectives We now return \nto the frame connective, ., de.ned in Section 4.3. In previous literature on higher-order frame rules \n[8 10, 33], the R in S .R tends to be inert and does not interact with its environment until it has distributed \ninwards across all connectives and has been merged into the pre-and post-conditions of a triple. Only \nat that point will the rule of consequence and the existential rule for triples be used to interact with \nR. Since we only see triples in certain special cases, as described in Section 5.1, we are interested \nin speci.cation-level generalisa\u00adtions of the consequence and existential rules, just as FRAM E is a \nspeci.cation-level generalisation of the frame rule for triples. The use of these generalised rules in \npractice is similar to their counter\u00adparts in ordinary Hoare logic. 7.1 Speci.cation-level rule of consequence \nThe standard Hoare rule of consequence states that {P } c {Q} is contravariant in P and covariant in \nQ with respect to entailment. Analogously, the generalisation we present here describes the vari\u00adance \nof S . R in R. It turns out that S . R is not always covariant nor always contravariant in R; it can \nbe either, depending on S. We encode this as two predicates on S: frame+(S) .P, Q. (P f Q) . (S . P f \nS . Q) frame-(S) .P, Q. (P f Q) . (S . Q f S . P ) These de.nitions directly give rise to our two speci.cation-level \nrules of consequence: frame+(S) P f Q frame-(S) P f Q S . P f S . Q S . Q f S . P All we did so far was \nto switch the problem to proving frame+(S) or frame-(S) for particular S, but it turns out that there \nis a very schematic set of rules for this. Writing f : (V1, . . . , Vn) -. V to mean .S1, . . . , Sn. \nframeV1 (S1) . . . . . frameVn (Sn) . frameV (f(S1, . . . , Sn)), we can tabulate the rules for various \nconnectives concisely: safe : - T, . : + and - C, ., 0, ., . : + -. + and - -. - ., . : (+, +) -. + and \n(-, -) -. - . : (-, +) -. + and (+, -) -. - Notice that all the logical connectives preserve either covariance \nor contravariance of their operands (modulo the .ip that happens for implication), but there is no way \nto combine the variances. Example 13. For all P1 and P2, frame-(Csafe.P1 .safe.P2).o Example 14. For \nall P , frame+(safe . P . .). o There are no de.nitions analogous to frame+(S) and frame-(S) for the \nread-only frame connective since S 0 R is always con\u00adtravariant in R. But as we will see in Section 7.3, \nthe frame family of predicates plays an important role for 0 too. It is no coincidence that these rules \nfor variance have not been studied in the previous literature. There is no rule for frame+ or frame- \non Hoare triples, and in a logic where the only atomic speci.cations are the triple and T, ., then any \nS that satis.es frame+(S) or frame-(S) is equivalent to either T or ..  7.2 Speci.cation-level existential \nrule The existential rule in Hoare logic allows moving an existential quanti.er from the precondition \nof a Hoare triple out into the logical variable context. Just as we have generalised the frame and consequence \nrules, we can generalise the existential rule to work with other speci.cations than triples. Using the \nsame approach as in Section 7.1, we de.ne frame.(S) .P. (.x. S . P (x)) f S . (.x. P (x)) We can then \nstate the speci.cation-level existential rule as frame.(S) S ' f .x. S . P (x) S ' f S . (.x. P (x)) \nThe following rules, using the notation introduced in Sec\u00adtion 7.1, let us schematically prove frame.. \nsafe : . C, ., 0, . : . -. . . : (., .) -. . . : (-, .) -. . The natural converse of frame.(S), with \nthe entailment in the other direction, is equivalent to frame-(S). This gives an intuitive justi.cation \nof the rule for implication above. Example 15. For all P, c, Q, we have frame.({P } c {Q}). This is seen \nby unfolding the de.nition of the triple and applying the above rules. o  7.3 Further properties of \nread-only frame connective The read-only frame and the frame connectives are interchangeable in certain \ncases: 1. For singleton frames: S 0 {s} = S . {s}. 2. If frame.(S) then S 0 R f S . R. 3. If frame-(S) \nthen S . R f S 0 R.  Whereas two adjacent frame connectives can always be merged and split by the .-* \nrule, this is not always possible for the read\u00adonly frame connective: 1. If frame.(S) then S 0 (R * R \n' ) f S 0 R 0 R ' . 2. Unconditionally, S 0 R 0 R ' f S 0 (R * R ' ).  8. Related work This paper builds \non previous work on higher-order frame rules, assembly-language veri.cation and guarded recursion. Separation \nlogic for assembly code. Our work shares many goals with the work of Myreen et al. [28, 29]. They have \nbuilt a separa\u00adtion logic for subsets of ARM and x86 in the HOL4 proof assis\u00adtant. Their logic emphasises \ntotal correctness, but since assembly\u00adlanguage programs do not terminate, total correctness does not \nmean guaranteed termination as it usually would. Instead, a post\u00adcondition Q means that execution will \neventually reach a machine state in Q. This makes speci.cations much more intensional than in our case, \npreventing, for example, relocating and patching (or inter\u00adpreting) code in memory in an externally-unobservable \nway unless this has been somehow explicitly allowed by the speci.cation. The logic of Myreen et al. lacks \nlabels in assembly programs, relying instead on explicit instruction address arithmetic. Their en\u00adtire \nspeci.cation logic takes place in a generalised Hoare triple with multiple pre-and postconditions and \noffset transformer functions. This is general enough to support jumps, function calls and self\u00admodifying \ncode, but it remains a triple and is thus restricted to what can be expressed with preconditions, postconditions \nand code blocks. The CAP family of logics from Shao et al. are also Hoare logics for low-level code, \nall veri.ed in Coq. The family includes XCAP [31], GCAP [14], SCAP and ISCAP [40]. Unfortunately, neither \nof them is a generalisation of any of the others, so each has its strengths and weaknesses. All except \nGCAP and SCAP have high\u00adlevel heap manipulation commands such as allocation or function calls built into \nthe machine semantics. All except GCAP have the program residing in a map from labels to instruction \nsequences, which is a high level of abstraction and cannot support treating code as data. As Myreen [28] \nand hopefully this paper have shown, it is not dif.cult to treat code as data and support function pointers \nif the logic is fundamentally set up for it. In contrast, GCAP supports it with some awkwardness by attempting \nto impose the map-from-labels abstraction on top of what is actually happening in the machine. Affeldt \net al [1] have formalized in Coq a separation logic for .rst-order MIPS assembly code, extending a simpler \nlogic due to Saabas and Uustalu [36], and applied it to verifying provable security of implementations \nof cryptographic primitives. Chlipala s Bedrock project [16] is also a Coq framework for ver\u00adifying low-level \ncode with separation logic. Like our framework, Bedrock has while and if macros and associated proof \nprinci\u00adples for common patterns of structured code. Chlipala emphasises automated veri.cation, and the \nprogram logic is therefore not very expressive. There is no frame rule, so frames are instead passed \naround explicitly in procedure speci.cations. Chlipala explains the problem with de.ning a frame rule \nfor programs with unstructured jumps; here we have demonstrated how this may be solved. None of the logics \ndiscussed above feature a higher-order frame rule. Higher-order frame rules. The frame rule was extended \nby O Hearn et al. [32] to the hypothetical frame rule, which allowed framing invariants onto a context \nof procedure speci.cations in addition to the triple under consideration. This allowed greater modularity \nin separate veri.cation of caller and callee, but it still required programs to have structured control \n.ow. The higher-order frame rule was proposed by Birkedal et al. and used in a separation-logic type \nsystem for a programming language with higher-order functions and ground store [8 10]. It has later been \nextended to languages with higher-order store and used by Krishnaswami [23] and by Pottier [33]. In all \ncases, it has been for high-level functional programming languages, whereas we have applied it to machine \ncode. We believe we are the .rst to complement the higher-order frame rule with a higher-order rule of \nconsequence and a higher-order existential rule (Sections 7.1 and 7.2). Typed assembly language. The \nwork of Appel et al. on typed as\u00adsembly language and foundational proof-carrying code has demon\u00adstrated \nthat step indexing [2] is a viable technique for describing the behaviour of low-level programs. The \nVery Modal Model paper [3] popularised Nakano s later operator, which we also use here, and demonstrated \nits applicability to assembly code. The work on typed assembly language focuses on safety of reference \ntypes coming from high-level languages and does not attempt to verify code for full functional correctness \nas we do. 9. Future work The logic described in this paper will form the foundation for broader research \non language-based security in veri.ed systems software. Although the focus of this paper is on the general \ndesign of a separation logic for machine code, and is thus largely parametric in the underlying machine \nmodel, one s con.dence in the real world validity of veri.cations in the logic is undeniably limited \nby one s con.dence in the accuracy of that model. Our x86 model was hand-constructed from reading the \nIntel manuals and, although small programs extracted from Coq seem to run as expected on real hardware, \nhas not been subject to any systematic testing or veri.cation. Indeed, there is one aspect of the Intel \nspeci.cation that we knowingly do not currently model, namely the presence of a code cache: instructions \nwritten to memory are, on post-Pentium processors, not guaranteed to be picked up by subsequent execution \nuntil a jump or other synchronizing instruction has occurred. We plan to treat the code cache following \nthe approach of Myreen [28], which we expect to be unproblematic. More generally, however, we would like \nto work with a more trustworthy machine model; these have previously been obtained by extensive testing \n[19, 26] and semi-automated extraction from the text of reference manuals [11]. An important feature \ncurrently missing from our machine model is I/O. By adding this we would incorporate observations beyond \nthe simple notion of safe execution, but we believe that our frame\u00adwork is generic enough that the safe \nspeci.cation can be gener\u00adalized to safety properties involving observable input and output transitions. \nWe have not so far given any serious thought to how one might also prove liveness properties in a comparably \nexten\u00adsional way, though that is clearly an interesting subject for future work. It would also be useful \nto extend our logic to deal with binary relations, rather than unary predicates, on machine states. Such \nan extension would allow us to verify information .ow and abstraction properties [7].  We have already \nbegun to experiment with veri.ed compilation, building a tiny imperative language, its compiler, program \nlogic and proof of correctness, all within Coq. It is straightforward to mix machine code with higher-level \nlanguages, as our logic provides a common framework for specifying their interaction at a suitably high \nlevel. We plan to develop a number of domain-speci.c little languages within the same framework. Low-level \ncode often makes sophisticated use of low-level data structures whose ownership properties cannot easily \nbe captured by the default model of separation described here. We might instead employ .ctional separation \nlogic [22]; it is interesting to note that even our use of partial states S to describe the machine state \nSin a more .ne-grained way is reminiscent of .ctional separation. Acknowledgements. We would like to \nthank Lars Birkedal and Kasper Svendsen for many discussions on higher-order frame rules and their applications. \nReferences [1] R. Affeldt, D. Nowak, and K. Yamada. Certifying assembly with formal security proofs: \nthe case of BBS. Sci. Comput. Prog., 77(10\u00ad11), 2012. [2] A. W. Appel and D. McAllester. An indexed model \nof recursive types for foundational proof-carrying code. ACM Trans. Program. Lang. Syst., 2001. [3] A. \nW. Appel, P.-A. Melli ` es, C. D. Richards, and J. Vouillon. A very modal model of a modern, major, general \ntype system. In Proceedings of POPL, 2007. [4] J. Bengtson, J. B. Jensen, and L. Birkedal. Charge! a \nframework for higher-order separation logic in Coq. In Proc. of ITP, 2012. [5] N. Benton. A typed, compositional \nlogic for a stack-based abstract machine. In APLAS, volume 3780 of LNCS, 2005. [6] N. Benton. Abstracting \nallocation: The new new thing. In Computer Science Logic (CSL 2006), volume 4207 of LNCS, 2006. [7] N. \nBenton and N. Tabareau. Compiling functional types to relational speci.cations for low level imperative \ncode. In TLDI, 2009. [8] L. Birkedal, N. Torp-Smith, and H. Yang. Semantics of separation\u00adlogic typing \nand higher-order frame rules. In Proc. of LICS, 2005. [9] L. Birkedal, N. Torp-Smith, and H. Yang. Semantics \nof separation\u00adlogic typing and higher-order frame rules for Algol-like languages. Logical Methods in \nComputer Science, 2006. [10] L. Birkedal and H. Yang. Relational parametricity and separation logic. \nLogical Methods in Computer Science, 2008. [11] F. Blanqui, C. Helmstetter, V. Joloboff, J.-F. Monin, \nand X. Shi. De\u00adsigning a CPU model: from a pseudo-formal document to fast code. In 3rd Workshop on Rapid \nSimulation and Performance Evaluation: Methods and Tools (RAPIDO 2011), 2011. [12] R. Bornat, C. Calcagno, \nP. W. O Hearn, and M. J. Parkinson. Permis\u00adsion accounting in separation logic. In Proceedings of POPL, \n2005. [13] R. M. Burstall. Some techniques for proving correctness of programs which alter data structures. \nMachine Intelligence, 7, 1972. [14] H. Cai, Z. Shao, and A. Vaynberg. Certi.ed self-modifying code. In \nProc. of PLDI, 2007. [15] C. Calcagno, P. W. O Hearn, and H. Yang. Local action and abstract separation \nlogic. In Proc. of LICS, 2007. [16] A. Chlipala. Mostly-automated veri.cation of low-level programs in \ncomputational separation logic. In Proc. of PLDI, 2011. [17] A. Chlipala. Certi.ed Programming with Dependent \nTypes. MIT Press, to appear. [18] R. W. Floyd. Assigning meanings to programs. In J. T. Schwartz, editor, \nMathematical Aspects of Computer Science, volume 19 of Proc. of Symposia in Applied Mathematics, Providence, \nRhode Island, 1967. AMS. [19] A. C. J. Fox and M. O. Myreen. A trustworthy monadic formalization of the \nARMv7 instruction set architecture. In 1st International Con\u00adference on Interactive Theorem Proving (ITP \n2010), volume 6172 of LNCS, 2010. [20] G. Gonthier, A. Mahboubi, and E. Tassi. A small scale re.ection \nextension for the Coq system. Technical Report 6455, INRIA, 2011. [21] C. Hawblitzel and E. Petrank. \nAutomated veri.cation of practical garbage collectors. In POPL, 2009. [22] J. B. Jensen and L. Birkedal. \nFictional separation logic. In Proc. of ESOP, volume 7211 of LNCS. Springer, 2012. [23] N. R. Krishnaswami. \nVerifying Higher-Order Imperative Programs with Higher-Order Separation Logic. PhD thesis, Carnegie Mellon \nUniversity, 2012. [24] J. McCarthy and J. Painter. Correctness of a compiler for arith\u00admetic expressions. \nIn Mathematical Aspects of Computer Science, volume 19 of Proc. of Symposia in Applied Mathematics. AMS, \n1967. [25] J. Strother Moore. A mechanically veri.ed language implementation. Journal of Automated Reasoning, \n5, 1989. [26] G. Morrisett, G. Tan, J. Tassarotti, J.B. Tristan, and E. Gan. Rocksalt: Better, faster, \nstronger SFI for the x86. In 33rd ACM SIGPLAN Conference on Programming Language Design and Implementation \n(PLDI 2012). ACM, 2012. [27] G. Morrisett, D. Walker, K. Crary, and N. Glew. From System F to typed assembly \nlanguage. ACM Transactions on Programming Languages and Systems, 21(3), 1999. [28] M. O. Myreen. Veri.ed \njust-in-time compiler on x86. In Proc. of POPL, 2010. [29] M. O. Myreen and M. J. C. Gordon. Hoare logic \nfor realistically modelled machine code. In Proc. of TACAS, 2007. [30] H. Nakano. A modality for recursion. \nIn Proc. of LICS, 2000. [31] Z. Ni and Z. Shao. Certi.ed assembly programming with embedded code pointers. \nIn Proc. of POPL, 2006. [32] P. W. O Hearn, H. Yang, and J. C. Reynolds. Separation and informa\u00adtion \nhiding. In Proc. of POPL, 2004. [33] F. Pottier. Hiding local state in direct style: a higher-order anti-frame \nrule. In Proc. of LICS, 2008. [34] J. C. Reynolds. An introduction to speci.cation logic. In Logics of \nPrograms, 1983. [35] J. C. Reynolds. Separation logic: A logic for shared mutable data structures. In \nProc. of LICS, 2002. [36] A. Saabas and T. Uustalu. A compositional natural semantics and Hoare logic \nfor low-level languages. Theor. Comput. Sci., 373(3), 2007. [37] M. Sozeau and N. Oury. First-class type \nclasses. In Proc. of TPHOLs, 2008. [38] G. Tan and A. W. Appel. A compositional logic for control .ow. \nIn 7th International Conference on Veri.cation, Model Checking, and Abstract Interpretation (VMCAI 2006), \n2006. [39] A. M. Turing. Checking a large routine. In Report of a Conference on High Speed Automatic \nCalculating Machines, 1949. [40] X. Jiang W. Wang, Z. Shao and Y. Guo. A simple model for certifying \nassembly programs with .rst-class function pointers. In Proc. of TASE, 2011. [41] W. D. Young. A mechanically \nveri.ed code generator. Journal of Automated Reasoning, 5, 1989.    \n\t\t\t", "proc_id": "2429069", "abstract": "<p>Separation logic is a powerful tool for reasoning about structured, imperative programs that manipulate pointers. However, its application to unstructured, lower-level languages such as assembly language or machine code remains challenging. In this paper we describe a separation logic tailored for this purpose that we have applied to x86 machine-code programs.</p> <p>The logic is built from an assertion logic on machine states over which we construct a specification logic that encapsulates uses of frames and step indexing. The traditional notion of Hoare triple is not applicable directly to unstructured machine code, where code and data are mixed together and programs do not in general run to completion, so instead we adopt a continuation-passing style of specification with preconditions alone. Nevertheless, the range of primitives provided by the specification logic, which include a higher-order frame connective, a novel read-only frame connective, and a 'later' modality, support the definition of derived forms to support structured-programming-style reasoning for common cases, in which standard rules for Hoare triples are derived as lemmas. Furthermore, our encoding of scoped assembly-language labels lets us give definitions and proof rules for powerful assembly-language 'macros' such as while loops, conditionals and procedures.</p> <p>We have applied the framework to a model of sequential x86 machine code built entirely within the Coq proof assistant, including tactic support based on computational reflection.</p>", "authors": [{"name": "Jonas B. Jensen", "author_profile_id": "81479645640", "affiliation": "IT University, Copenhagen, Denmark", "person_id": "P3977972", "email_address": "jobr@itu.dk", "orcid_id": ""}, {"name": "Nick Benton", "author_profile_id": "81100165244", "affiliation": "Microsoft Research, Cambridge, United Kingdom", "person_id": "P3977973", "email_address": "nick@microsoft.com", "orcid_id": ""}, {"name": "Andrew Kennedy", "author_profile_id": "81100450709", "affiliation": "Microsoft Research, Cambridge, United Kingdom", "person_id": "P3977974", "email_address": "akenn@microsoft.com", "orcid_id": ""}], "doi_number": "10.1145/2429069.2429105", "year": "2013", "article_id": "2429105", "conference": "POPL", "title": "High-level separation logic for low-level code", "url": "http://dl.acm.org/citation.cfm?id=2429105"}