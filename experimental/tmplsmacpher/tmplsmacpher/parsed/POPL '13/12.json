{"article_publication_date": "01-23-2013", "fulltext": "\n Abstract Con.ict Driven Learning Vijay D Silva Leopold Haller Daniel Kroening Department of Computer \nScience Department of Computer Science Department of Computer Science University of California, Berkeley \nOxford University Oxford University vijayd@eecs.berkeley.edu leopold.haller@cs.ox.ac.uk daniel.kroening@cs.ox.ac.uk \n Abstract Modern satis.ability solvers implement an algorithm, called Con\u00ad.ict Driven Clause Learning, \nwhich combines search for a model with analysis of con.icts. We show that this algorithm can be gen\u00aderalised \nto solve the lattice-theoretic problem of determining if an additive transformer on a Boolean lattice \nis always bottom. Our generalised procedure combines overapproximations of greatest .xed points with \nunderapproximations of least .xed points to obtain more precise results than computing .xed points in \nisolation. We generalise implication graphs used in satis.ability solvers to derive underapproximate \ntransformers from overapproximate ones. Our generalisation provides a new method for static analyzers \nthat op\u00aderate over non-distributive lattices to reason about properties that require disjunction. Categories \nand Subject Descriptors F.3 [Logics and Meanings of Programs]: Specifying and Verifying and Reasoning \nabout Programs Keywords Satis.ability, Con.ict Driven Clause Learning, Lattices 1. The Algebraic Essence \nof Satis.ability Solvers The performance of solvers for the Boolean satis.ability problem (S AT) has \nimproved at an exponential rate in the last decade. Several factors contribute to these improvements \nincluding and elegant algorithm, ef.cient, architecture-aware implementations of data structures, and \nheuristics that exploit the non-adversarial nature of practical problem instances. A recent survey of \nthese developments by Malik and Zhang [21] concludes with the following question: Given its theoretical \nhardness, the practical success of S AT has come as a surprise to many in the computer science community. \n[...] Can we take these lessons to other problems and domains? This paper presents an approach to instantiating \nthe Con.ict Driven Clause Learning algorithm (C D C L) in SAT solvers on new problems. We introduce a \nlattice-theoretic generalisation of Boolean satis.ab\u00adility. The bottom-everywhere problem is to determine \nif a function on a lattice maps all elements of the lattice to bottom. Instances of the bottom-everywhere \nproblem include satis.ability for formulae in propositional logic and quanti.er-free .rst order theories, \nsource to target reachability in transition systems, language emptiness for automata, and assertion violation \nin programs. We show that CD CL Permission to make digital or hard copies of all or part of this work \nfor personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior \nspeci.c permission and/or a fee. POPL 13, January 23 25, 2013, Rome, Italy. Copyright c &#38;#169; 2013 \nACM 978-1-4503-1832-7/13/01. . . $10.00 solves an instance of the bottom-everywhere problem for certain \nfunctions on .nite, powerset lattices. The contribution of this paper is Abstract Con.ict Driven Clause \nLearning (ACD C L), a strict, mathematical generalisation of C D CL to lattice-based abstractions. We \nshow that C DC L is a speci.c technique to combine overapproximations of a greatest .xed point with underapproximations \nof a least .xed point. We provide a correctness argument for AC DC L under general, lattice-theoretic \nconditions. These conditions are satis.ed by abstract domains used in practice and lead to a new family \nof analysis procedures. Our work also enables a new understanding of C D CL, discussed next. Abstract \nInterpretation Perspectives of CDCL Many existing lattices used in static analysis lack negation, have \nmeet operations that precisely model conjunction, and join opera\u00adtions that overapproximate disjunction. \nPrecision loss due to joins is often eliminated by enriching a domain or analysis with disjunction. Such \nenrichment may suffer from case explosion, meaning that the number of disjunctive cases to be considered \ngrows infeasibly large as the analysis progresses. We show that the main data structure in a SAT solver, \ncalled a partial assignment, represents elements of a well known abstract domain, and that constraint \npropagation in SAT solvers is .xed point iteration in this domain. S AT solvers compute .xed points in \nnon-Boolean abstract domains. The conceptual insight of this paper is that learning techniques used by \nS AT solvers can be viewed as synthesising an abstract transformer for negation. The combination of precise \nconjunction in the partial assignments domain with imprecise negation provided by learning allow a solver \nto reason indirectly about disjunction without enumerating cases. We show that abstract domains used \nin practice satisfy the conditions required to support learning. A second insight of this paper is that \nthe implication graph construction in S AT solvers is a technique for constructing an underapproximate \ntransformer starting from an overapproximation. An abstract transformer can be viewed as a directed graph \nin which edges represent transformer application. The inverted graph represents a dual transformer and \nsets of vertices approximate applications of this dual transformer. In a S AT solver, the graph represents \na deduction transformer and its inverse represents an abduction transformer. When lifted to programs, \nwe can start with an overapproximate postcondition transformer and derive an underapproximate precondition \ntransformer. The insights above have practical rami.cations, which we have demonstrated with two practical \ninstantiations of ACDC L. We have instantiated AC D C L with the interval abstract domain to analyse \nC programs that manipulate .oating point variables [14]. Our analyser is more precise that a standard \nstatic analyser and more ef.cient than an S M T solver, if either is run in isolation. We have also instantiated \nAC D CL in the MathSAT framework to derive an S M T solver for .oating point logic [17]. In both cases, \nour instantiations avoid case explosion experienced by several competing tools.  Contribution and Content \nThis paper presents Abstract Con.ict Driven Learning (ACD L), a procedure for reasoning about functions \non Boolean lattices by operating on non-Boolean lattices. Our goal is to present an account of AC D CL \nthat reveals its generality and to present correctness arguments that apply to all instantiations of \nAC DC L. Towards this end, we make the contributions below. 1. The bottom-everywhere problem, a lattice-theoretic \nproblem which encompasses satis.ability of logical formulae, reachabil\u00adity in transition systems, and \nassertion violation in programs. 2. A view of C D CL as a procedure for solving a speci.c instance of \nthe bottom-everywhere problem. 3. A generalisation of C DC L to solve the bottom-everywhere problem \nby combining greatest .xed point and least .xed point computation. 4. A novel technique for deriving \nunderapproximate transformers from overapproximate transformers, based on a generalisation of implication \ngraphs to abstract domains. 5. Lattice-theoretic soundness and completeness arguments that  apply to \nall instances of ACD C L. The paper is organised as follows: We introduce the bottom\u00adeverywhere problem \nin Section 2 and discuss several instances of this problem. We apply abstract interpretation to derive \nover-and underapproximate solutions to the bottom-everywhere problem in Section 3, and combine these \napproximations in Sections 4 and 5. The generalisation of implication graphs to abstract domains is discussed \nand illustrated in Section 6. 2. The Bottom-Everywhere Problem We introduce the bottom-everywhere problem \nand show that satis\u00ad.ability of formulae and error reachability in transition systems are instances of \nthis problem. Subsequent sections lift CD CL to solve this problem. Lattice-Theoretic Terminology A lattice \nis called bounded if it has a greatest element, called top and denoted T, and has a least element called \nbottom and denoted .. A function f on a complete lattice L is additive if f(x U y) = f(x) U f(y) and \nis completely . . additive if f( X) = f(X). A completely additive function maps . to .. The dual notions \nare called multiplicative and completely multiplicative. The function f is reductive if f(x) . x for \nall x and is extensive if f(x) ; x for all x. A function is idempotent if f(f(x)) = f(x) for all x. A \ntransformer is a monotone function on a lattice. An upper closure is an idempotent and extensive transformer, \nand a lower closure is an idempotent and reductive transformer. The pointwise order f . g between functions \nfrom a set to a poset holds if f(x) . g(x) holds for all x. The pointwise meet of f and g, denoted f \nn g, where both functions map into a lattice is de.ned as .x. f (x) n g(x). The pointwise join is similarly \nde.ned. The set of transformers on a complete lattice form a complete lattice under the pointwise order. \nThe De Morgan dual of a function f on a Boolean lattice is f = \u00ac . f . \u00ac. We require the following property \nof De Morgan duals. Proposition 1. If f is a completely additive, reductive function on a powerset lattice, \nf is a completely multiplicative, extensive function. The least .xed point of a transformer f on a complete \nlattice is denoted lfp(f) and the greatest .xed point is denoted gfp(f). 2.1 Bottom and Top Everywhere \nThe material below is new. De.nition 2. A function f on a bounded lattice is bottom-everywhere if f(x) \n= . for all x. The bottom-everywhere problem is to determ\u00adine if a function on a lattice is bottom-everywhere. \nA non-bottom witness is an element a such that f(a) is not bot\u00adtom. A non-bottom witness a is minimal \nif no b c a is a non-bottom witness. The top-everywhere property and top-everywhere problem are similarly \nde.ned. The dual notions for the top-everywhere prob\u00adlem are a non-top witness and a maximal non-top \nwitness. In this paper, we consider the bottom-everywhere problem for completely additive, reductive \nfunctions on powerset lattices. Lemma 3. A completely additive, reductive function on a powerset lattice \nis a lower closure. Proof. Consider a function f : P(S) . P(S). For each x in S, f({x}) is either \u00d8 or \nis {x}, because f is reductive. Since f is completely additive, f(X) equals f(X) for every subset X \nof S. In particular, f(X) equals {x . X | f({x}) = \u00d8}. It follows that f(f(X)) equals f(X). Due to Lemma \n3 we abbreviate completely additive, reductive function to additive closure for the rest of the paper. \nTheorem 4 below is a consequence of Lemma 3. It is straightforward to prove but the .xed point view is \nvaluable because an abstraction of a lower closure may not be idempotent, in which case iterating a transformer \nin the abstract yields strictly more precision than applying it once. Theorem 4. The following statements \nare equivalent for a com\u00adpletely additive, reductive function f on a powerset lattice. 1. f is bottom-everywhere. \n 2. gfp(f ) is bottom. 3. f is top-everywhere. 4. lfp(f ) is top.  The implication from 1 to 2 is \nstraightforward, while the implic\u00adation 2 to 1 holds because f is a lower closure. The equivalence of \n1 and 3 follows by negation and of 3 and 4 due to the closure property and complementation of .xed points. \nNext, we show that two well-known problems, satis.ability and error reachability, can be reduced to the \nbottom-everywhere problem. Neither reduction is mathematically surprising but allows us to think of decision \nproblems rather than function problems.  2.2 Unsatis.ability as Bottom Everywhere Let Struct be a set \nof structures and Form be a set of formulae, and |= . Struct \u00d7 Form be a satisfaction relation between \nstructures and formulae. If (s, .) is in |=, we write s |= ., and say s satis.es ., or that s is a model \nof .. The details of the structures and the formulae are not relevant for the formalisation. A formula \n. is satis.able over Struct if some s in Struct satis.es ., and is unsatis.able over Struct otherwise. \nWe drop the quali.er over Struct . The satis.ability problem is to determine if a given formula . is \nsatis.able. We formulate satis.ability in terms of a transformer. The do\u00admain of structures is (P(Struct), \n., ., n). The model transformer mod . : P(Struct) . P(Struct) maps a set of structures to the subset \ncontaining only models of .. mod .(S) = {s . S | s |= .} Properties of . can be expressed as properties \nof mod .. The set of models of . is mod .(Struct) and . is unsatis.able exactly if mod .(Struct) is empty. \nObserve that mod . is completely additive and reductive, hence it is in the scope of problems we consider. \nWe denote the function (\u00ac . mod . . \u00ac) as ucmod . and call ucmod . the con.ict transformer. The con.ict \ntransformer ucmod . adds to any set of structures all countermodels of .. The set of countermodels of \n. is ucmod .(\u00d8) and . is unsatis.able exactly if ucmod .(\u00d8) contains all structures. Observe that ucmod \n. is completely multiplicative and extensive.  Theorem 5. A formula . is unsatis.able exactly if mod \n. is bottom\u00adeverywhere. A non-bottom witness for mod . is a set of structures that contains a model of \n.. A minimal non-bottom witness is a singleton set containing a model of .. A non-top witness for ucmod \n. is a set of structures strictly contained in Struct that excludes some models of .. A maximal non-top \nwitness for ucmod . is a set that contains all structures except one model of .. We de.ne two logics \nfor use in examples. Propositional Logic Let Prop be a .nite set of propositional vari\u00adables. The set \nof literals is Lit = {p, \u00acp | p . Prop}, containing a variable or its negation. A clause is a disjunction \nof literals and a C N F formula is a conjunction of clauses. As is common in the SAT literature, we view \nclauses as sets of literals and formulae as sets of clauses. The set of truth values is B = {t, f}. Literals, \nformulae and clauses are interpreted over functions Asg = Prop . B, from variables to truth values, and \nare called propositional assignments. The entailment relation |= is de.ned as follows. For each assign\u00adment \ns and literal l, s |= l exactly if s(l) = t if l is a variable, and s(l) = f if l is the negation of \na variable. For each clause C, s |= C exactly if s |= l for some literal l in C. For a C N F formula \n., we say s |= . if s |= C for every clause C in .. Inequality Logic Inequality logic can express order \nbetween vari\u00adables. Let Vars be a set of .rst-order variables. The set of dif\u00adference predicates is {x \n< y | x, y . Vars }. The set of inequality literals contains all difference predicates and their negations. \nWe interpret difference formulae over integers. The set of structures Struct = Vars . Z consists of functions \nfrom variables to in\u00adtegers. A structure s satis.es a predicate x < y if the inequality s(x) < s(y) holds. \nFor formulae, the entailment relation |= is de.ned as expected.  2.3 Feasible Traces via Bottom Everywhere \nLet M = (S, T ) be a transition system where S is a set of states and T . S \u00d7 S is a transition relation. \nLet S+ be the set of non\u00adempty sequences of states. A trace is a sequence t = t0, . . . , tn-1 satisfying \nthat every (ti, ti+1) is a valid transition. A state t is reachable from s if there exists a trace starting \nin s and ending in t. Given sets of states P and Q, the feasible trace problem is to determine if there \nexists a trace from a state in P to a state in Q. We formulate the feasible trace problem as a bottom-everywhere \nproblem. The domain of sequences is P(S+). The feasible trace transformer trace P,Q : P(S+) . P(S+) maps \na set of non\u00adempty sequences of states to the subset containing only traces that start from a state in \nP and reach a state in Q. We assume tn is the last state of t below. trace P,Q (X) = {t . X | t is a \ntrace, t0 . P, tn . Q} The set of traces from P to Q has several .xed point characterisa\u00adtions, consolidated \nby Cousot [6]. Once again, the function trace P,Q is completely additive and reductive. We denote the \nfunction (\u00ac . trace P,Q .\u00ac) as uctrace P,Q , and call it the countertrace transformer. The set uctrace \nP,Q (X) con\u00adtains the set X, as well as all sequences that are not traces, all sequences that start in \nP but do not lead to Q and all sequences that end in Q but do not start in P . Theorem 6. Given a transition \nsystem, and sets of states P and Q, there is no trace from P to Q exactly if trace P,Q is bottom\u00adeverywhere. \nA non-bottom witness for trace P,Q is a set of sequences con\u00adtaining a trace from P to Q. A minimal non-bottom \nwitness is a singleton set containing a feasible trace from P to Q. A non-top witness for uctrace P,Q \nis a set of sequences that excludes a trace from P to Q. A maximal non-top witness contains all sequences \nexcept one trace from P to Q. The view of feasible traces as bottom-everywhere lifts to reach\u00adability \nand assertion checking in programs. The de.nition of the set of traces of a program is well-known and \nis not recalled here. We directly apply the procedures developed in this paper to reason about programs. \n 2.4 Reachable States via Bottom Everywhere We consider reachability problems de.ned in terms of states \nrather than over traces. We make such a distinction because the details of lifting CD C L are different \nfor trace-based abstractions and for state-based abstractions. The material recalled here is also required \nto distinguish C D C L from Cousot s forward-backward iteration [7]. Let M = (S, T ) be a transition \nsystem where S is a set of states and T . S \u00d7 S is a transition relation. The concrete lattice of states \nis P(S). Recall that a transition system de.nes the two transformers below. post (X) = {t . S | (s, t) \nis in T and s is in X} pre (X) = {s . S | (s, t) is in T and t is in X} The sets of forward and backward \nreachable states have standard .xed point characterisations, recalled below. Forward-backward reachable \nstates consist of pairs (X, Y ) such that every state in X reaches some state in Y and vice-versa. When \napplied in an abstract domain, this kind of iteration yields strictly more information than forward analysis \nor backward analysis in isolation. freach P,Q (X) = X n [lfp x. (P . post (x))] n Q breach P,Q (X) = \nX n [lfp x. (Q . pre (x))] n P fbreach P,Q (X, Y ) = [lfp x. (P n Y ) . post (x), lfp x. (Q n X) . pre \n(x)] The functions freach P,Q and breach P,Q above are completely additive and reductive. The function \nfbreach P,Q is completely additive (see Chapter 22 of Cousot s notes [7] for a proof). To see that fbreach \nP,Q is not reductive, consider fbreach P,Q (P, Q), which will contain the states reachable from P and \nQ respectively. Component-wise intersection can be used to make the function fbreach P,Q reductive. Theorem \n7. The following are equivalent, given a transition system and sets of states P and Q. 1. No state in \nQ is reachable from a state in P . 2. freach P,Q is bottom-everywhere. 3. breach P,Q is bottom-everywhere. \n 4. fbreach P,Q is bottom-everywhere.  The proof of the .rst three statements is straightforward. The \nproof of equivalence to 4 is due to Cousot [7]. A minimal non\u00adbottom witness for freach P,Q is a state \nin Q that is reachable from P . A minimal non-bottom witness for breach P,Q is a state in P from which \na state in Q is reachable. The function (\u00ac . freach P,Q .\u00ac) maps a set of states X to a set containing \nX and states that are not reachable from P and states not in Q. Observe that such a function can directly \nbe computed by computing a .xed point using the function \u00ac . post .\u00ac. The same applies for breach P,Q \nand the function \u00ac . pre .\u00ac. These dual functions have been implemented in model checkers and used to \ncompute .xed points. Henzinger et al. [19] discuss temporal properties that can be checked with these \n.xed points, and Cousot and Cousot [11] combine these .xed points with abstraction.  Note that the functions \nfbreach P,Q and (\u00ac . fbreach P,Q .\u00ac) are different. Intuitively, forward-backward iteration exploits \na temporal duality between forwards and backwards analysis. In contrast, AC D C L will exploit a different \nduality between functions and their De Morgan duals. 3. Abstract Procedures for Bottom-Everywhere In \nthis section, we apply abstraction to the bottom-everywhere prob\u00adlem. If an overapproximation of a function \nf on P(S) is bottom\u00adeverywhere, the function f is also bottom-everywhere. If an under\u00adapproximation of \nthe dual function f is top-everywhere, the function f is also top-everywhere. We now highlight the overapproximate \nand underapproximate analysis present in S AT solvers and model checkers. We begin by recalling abstract \ninterpretation. Abstract Interpretation The key idea of abstract interpretation is to characterise solutions \nto a problem by a .xed point and derive approximate solutions by .xed point approximation. For conveni\u00adence, \nwe work in the Galois connection framework. Cousot and Cousot [10] extensively discuss generalisations. \nThese generalisa\u00adtions are required to analyse theories such as linear arithmetic be\u00adcause the lattice \nof polyhedra is not complete, and for analysis of automata, and trace-based abstractions. A Galois connection \nbetween posets (C, .) and (A, ), written . .-- C --.A, is a pair of monotone functions a : C . A and \na . : A . C satisfying that a(x) y holds exactly if x . .(y) does. The lattice C is called the concrete \ndomain and A is called the abstract domain. Monotone functions on C are called concrete transformers \nand those on A are called abstract transformers. An abstract transformer fo : A . A soundly approximates \nf : C . C if the pointwise order f . . . . . fo holds. If (C, .) is a powerset lattice, an abstract domain \n(A, ) is called an overapproximation because it satis.es x . .(a(x)) for all x. If (A, ;) is an abstract \ndomain of (C, .) it is called an underapproximation of (C, .) because it satis.es x . .(a(x)). A sound \nabstract transformer on an overapproximation is called a sound overapproximation and one on an underapproximation \nis called a sound underapproximation. The next two notions formalise precision of an approxima\u00adtion. \nA Galois connection implies there is a maximally precise approximation a . f . ., called the best abstract \ntransformer. An abstract transformer fo approximating f is .-complete at a if f(.(a)) = .(fo(a)), and \nis .-complete if it is .-complete at every a. If an abstract transformer fo is .-complete at a, no preci\u00adsion \nis lost. For example, if post is a concrete successor transformer and apost is a sound abstraction that \nis .-complete at an abstract state a, every abstract transition from a also exists in the concrete. In \nother words, there are no spurious transitions. Giacobazzi and Quintarelli [16] discuss .-completeness \nin detail. Parametric Fixed Points We require the notion of a .xed point above (or below) an element. \nLet f : L . L be a monotone function on a complete lattice and a be an element of L. The greatest .xed \npoint below a denoted gfpa(f) is the greatest .xed point of the function .x.f(x n a). The least .xed \npoint above a denoted lfpa(f) is the least .xed point of the function .x.f(x U a). The parametric .xed \npoint functions below map an element x of L to the least .xed point above x and greatest .xed point below \nx, respectively. plfp(f) = .x.lfpx(f) pgfp(f) = .x.gfpx(f) Extrapolation and Interpolation We use the \nterms extrapolation and interpolation for techniques used to accelerate .xed point computation. Figuratively, \nextrapolation operators move upwards or downwards from an element in a lattice, as illustrated in Figure \n1. Interpolation operators move between elements. T T  lfp gfp  r  . . Figure 1. Downwards extrapolation \nto underapproximate a greatest .xed point, and upwards interpolation to underapproximate a least .xed \npoint. The two operations are not dual. Let L be a lattice. An upwards extrapolation vr : L . L is a \nunary extensive function. We write vra for the application vr(a). The dual notion of a downwards extrapolation \nv : L . L is a unary reductive function. The de.nitions above are based on (but not identical to) those \nfor widening operators without a well\u00adfoundedness requirement [10]. A downwards interpolation is a binary \nfunction L : L\u00d7L . L satisfying that for all x and y in L, the inequality x ; y implies the inequality \nx ; L (x, y) ; y. We write interpolation application as x L y. The standard de.nition of a narrowing \noperator extends downwards interpolation with a well-foundedness condition. The notions of upwards interpolation \nand dual narrowing, both denoted Lr, are dually de.ned. Note that extrapolation and interpolation are \nnot dual operations, just as widening and narrowing are not dual. Though interpolation is self-dual, \nwe distinguish between upwards and downwards interpolation based on whether the operator is used in a \nleast or greatest .xed point computation. Downset Completion Downset completion is an operation that \nenriches an abstraction with disjunction [9]. Consider an abstraction (A, , U, n) of a powerset lattice \nP(S) with concretisation .. The abstraction A is disjunctive if .(a U b) = .(a) . .(b). A subset Q of \nA is downwards closed if for every x in Q and y in A, y x implies that y is in Q. A downwards-closed \nset is called a downset. The smallest downset containing Q is denoted Ql, and the downset of a singleton \nset {x} is denoted xl. In examples, we denote a downset as the set of its maximal elements. We omit a \ngeneral discussion of when such a representation is possible. The downset lattice over A, written (D(A), \n., n, .), is the set of downsets of A ordered by inclusion. Downsets strictly generalise powersets because \nthe downset lattice with respect to the identity relation is isomorphic to P(S). The downset completion \nof A is the lattice D(A) with the abstrac\u00adtion and concretisation functions below. In contrast to the \nstandard treatment, we use downsets as underapproximating abstractions.  .D(A) : D(A) . P(S) .D(A)(Q) \n= {.(x) | x . Q} aD(A) : P(S) . D(A) aD(A)(P ) = {x | .(x) . P } Consult [9] for proofs that the pairs \nof functions above form Galois connections and that the domains are disjunctive. 3.1 Abstract Bottom-Everywhere \nWe apply abstract interpretation to determine if a function is bottom\u00adeverywhere. Let (O, , U, n) be \nan abstract domain in a Galois connection with (P(S), ., ., n). We denote the abstraction and concretisation \nfunctions as aO and .O. We assume that .O(.) is the empty set. Let fo : O . O be a sound abstraction \nof a completely additive, reductive function f. If gfp(fo) concretises to ., the function f is bottom-everywhere. \nIf gfp(fo) does not concretise to ., we do not know if f is bottom-everywhere, due to imprecision in \nthe transformer. This intuition is stated below and the proof follows from the basic soundness results \nof abstract interpretation.  Theorem 8. If f a completely additive, reductive function on a powerset \nlattice, fo : O . O is a sound overapproximation of f , and gfp(fo) is ., the function f is bottom-everywhere. \nIf we know that fo is .-complete at some element a, we can determine, despite working in an abstraction, \nthat f is not bottom\u00adeverywhere. Since fo only has to be .-complete at a single element, the lattice \nand transformer may still be imprecise. and p = . to the set de.ned below. .(p) = {s | for all x . Vars, \np(x) = T implies p(x) = s(x)} Propositional solvers deduce properties about a formula using the unit \nrule. The unit rule asserts that if a partial assignment is de.ned on all but one literals in a clause \nand does not satisfy those literals, it must satisfy the remaining literal to satisfy the formula. For \na propositional literal l, we write p |= l if l = x and p(x) = t or if l = \u00acx and p(x) = f. Lattice-theoretically, \nthe unit rule for a clause is a decreasing transformer on the lattice of partial assignments. . .. . \n.. . . . . Proposition 9. If fo is .-complete at a and .(fo(a)) is not ., f is not bottom-everywhere. \n. for all l . C. p |= \u00acl C p n {x . t} The procedure Abstract-non-. below takes as input an overap- UnitC \n(p) = = C' . {x}, p(x) = T and for all l . C'. p |= \u00acl = C' . {\u00acx}, p(x) = T and proximate transformer \nfo, an abstract element o and a downwards p n {x . f} C extrapolation operator v . In addition, a procedure \nto check empti\u00ad ness .(o) = \u00d8 is required together with a suf.cient criterion for for all l . C'. p |= \n\u00acl p otherwise .-completeness of fo. The procedure attempts to determine if fo is bottom on all elements \nbelow o in the lattice. The output is a pair with the .rst element being either ., not . or unknown, \nand the second element representing the last lattice element obtained. If pgfp(fo)(o) concretises to \n., the function f is bottom on elements below .O(o). If fo is not bottom on the .xed point o' and is \n.-complete at o', we know f is not bottom-everywhere. Neither condition above may hold due to imprecision \nin the transformer or domain. In this case, downwards extrapolation is used to check if fo is also bottom \non elements below the .xed point. Unlike standard applications of widening, downwards extrapolation is \napplied here to improve precision. Algorithm 1: Abstract Search for a Non-. Witness v Abstract-non-.(fo \n: O . O, : O . O, o : O) repeat o' . o o . o n fo(o) ' until o = oor .(o) = \u00d8 if .(o) = \u00d8 then return \n(., o') if fo satis.es .-completeness criterion at o then return (not ., o) d . v o if d = o then return \n(unknown,d) return Abstract-non-.(fo, v , d) Model Search in SAT Solvers We show that the procedure in \nAlgorithm 1 generalises model search in SAT solvers. The abstract lattice contains partial assignments, \nthe abstract transformer is called the unit rule, the greatest .xed point computation pgfp(fo) is Boolean \nConstraint Propagation (BCP) and downwards extrapolation is implemented by decisions. We brie.y elaborate \non these points. Further details are in [13]. A partial assignment maps each variable to true, false, \nor unknown. The set of partial assignments PAsg = (Vars . {true, false, T}).{.} consists of partial assignments \nand a unique element .. In implementations of SAT solvers, the state . is indic\u00adated by a special con.ict \n.ag. Partial assignments form a lattice with respect to the natural pointwise order and are equivalent \nto the constants lattice or the Cartesian abstraction for Boolean valued variables. The element T represents \na partial assignment in which all variables have unde.ned values, and . represents the empty set. The \nconcretisation function . : PAsg . P(Asg) maps . to \u00d8, Moreover, UnitC can be characterised as the best \nabstract trans\u00adformer for the model transformer modC . The unit rule extends to formulae in CNF by taking \nthe pointwise meet of unit rules for each clause in the formula. n Unit. = UnitC C.. Boolean Constraint \nPropagation (BCP) repeatedly applies the unit rule and computes the .xed point gfp(Unit.). A solver makes \na decision by assuming that some variable, which is unknown in a partial assignment, has a de.nite value. \nMathematically, a decision maps a partial assignment p to p[x . v], where x is unknown in p and v is \na truth value. Observe that decisions are applications of a downward extrapolation operator v. : PAsg \n. PAsg. If all variables are assigned to t or f in p, then v.p = p. Otherwise, v.p is p[x . t] or p[x \n. f] for some x such that p(x) = T. Consider the conditionals in Abstract-non-.. If BCP starts from T \nand leads to ., the formula is unsatis.able, as returned by the .rst conditional. A SAT solver terminates \nif it .nds a satisfying assignment. Since assignments are partial assignments, the unit rule applies \nto them. Observe that Unit.(p) for an assignment is . if p does not satisfy . and is p otherwise. More \ngenerally, .(Unit.(p)) equals mod.(.(p)), so .nding a satisfying assignment is a suf.\u00adcient condition \nfor .-completeness. Another suf.cient condition is that at least one literal in each clause is satis.ed \nby the current partial assignment. To summarise, the algorithmic content of model search in a SAT solver \nis an instance of Abstract-non-. where O = PAsg, v. is propositional decision making, fo is given by \nUnit. and the .-completeness check tests whether all variables have been assigned to t or f. Several \nheuristics and carefully implemented data structures are required to achieve high performance with BCP, \nbut these aspects correspond to optimising the .xed point iteration and abstract domain implementation. \nModel Search for Inequalities We instantiate Abstract-non-. for reasoning about inequality formulae. \nLet Lit be the set of inequality literals. We introduce an inequality abstract domain, which contains \nonly conjunctions of inequalities. Speci.cally, each element of (Ineq, ., ., n), where Ineq = P(Lit) \nrepresents a conjunction of inequalities. We use the superset order because a larger set of constraints \nrepresents more constraints and has fewer models. The join is intersection of sets of constraints and \nmeet is union. The empty set represents true and the set of all inequalities is one representation of \nfalse. Observe that false has multiple representations because every set containing a predicate and its \nnegation is equivalent to false.  Recall that the concrete domain of structures was P(Vars . Z). The \nconcretisation function .Ineq : Ineq . P(Vars . Z) maps a set of constraints to their models. .Ineq (p) \n= {s | s |= P, for every P in p} We use an example due to McMillan et al. [23] to illustrate Abstract-non-. \non an inequality formula. . = (a < b) . (a < c) . (b < d . c < d) . (d < a) As with propositional logic, \nwe will use the best abstract trans\u00adtop-everywhere problem is not dual to Abstract-non-.. Let (U, ?, \n., .) be an abstract domain that underapproximates (P(S), ., ., n) with functions aU and .U . We assume \nthat .U (T) is S. Let fu : U . U be a sound abstraction of a completely mul\u00adtiplicative, extensive function \nf , where f is completely additive and reductive. If lfp( fu ) concretises to T, then the function f \nis bottom-everywhere. The procedure Abstract-non-T computes an underapproxim\u00adation of a least .xed point \nusing the upwards interpolation oper\u00ad . Correctness follows directly from known theorems in ab\u00ad stract \ninterpretation and it suf.ces for us to discuss instantiations of the abstract. this procedure. amod \n. = n aIneq . mod C . .Ineq C.. Algorithm 2: Abstract non-T Latorr former for a clause, but construct \nthe transformer for a formula in We apply Abstract-non-. to check satis.ability of .. The sets Abstract-non-T( \nfu : U . U, L : U \u00d7 U . U, u : U) r below represent the evaluation of each clause in the abstract domain. \nL.c ur (u U fu (u)) amod .(\u00d8) = {a < b} . {a < c} . \u00d8 . {d < a} = {a < b, a < c, d < a} = p amod .(p) \n= p . {d < b, d < c}. (amod b<d(p) n amod c<d(p)) if .(c) = T then return (T, c) if fu satis.es .-completeness \ncriterion at c then return (not T, c) if c = u then return (unknown, c) Lreturn TAbstract-non-(fu ,r \n, c) = Lit The second step above requires explanation. The best abstract transformer for the singleton \nclauses when applied to p has the effect of computing the transitive closure of constraints in p. Due \nCon.ict Analysis in SAT Solvers We now show that con.ict analysis in S AT solvers is an instance of to \n(a < b) and (d < a), the inequality (d < b) is added to p. Abstract-non-T. Consider the downset completion \n(D(PAsg ), .)The conjunction of p with b < d, represented by amod b<d(p), of partial assignments treated \nas an underapproximation. That is, is unsatis.able, as is the conjunction of p with c < d. The best every \nset of truth assignments is underapproximated by a set of representation of an unsatis.able conjunction \nis the set of all partial assignments. Con.ict minimisation [26] is a technique used constraints. Compare \nthe calculation above to solving the same formula with D PLL(T). In D PLL(T), a propositional variable \nwould have been introduced for each predicate, and two solvers, one for propositional logic and one for \ninequalities would have been required to solve the formula. By instantiating Abstract-non-., we can solve \nthe formula using only the abstract domain, which plays the role of a theory solver, and without introducing \nextra variables. This difference becomes important for large formulae and is the motivation for techniques \nlike natural domain S MT [5]. The generalised D P LL technique of McMillan et al. [23], referred to as \nG D P L L, also solves this formula without introducing proposi\u00adtional variables. Unlike AC D C L, G \nD PL L makes decisions and uses the shadow rule for con.ict analysis to solve the formula. An instantiation \nof Abstract-non-. over Ineq may use amod as de.ned above for fo, a decision operator v. : Ineq . Ineq \nsuch that v.Ineq = Ineq and otherwise v.p = p . {x < y} such that x < y is not in p. The .-completeness \ncheck for an element p can be performed by determining whether for every clause C . ., the intersection \np n C is non-empty. The check for .(o) being empty amounts to detecting a conjunction of unsatis.able \nconstraints. If constraints are represented as directed graphs, the emptiness check is implemented by \ncycle detection. Feasible Traces The automata-theoretic approach to the feasible traces problem is to \nconstruct an automaton representing the traces of a system and the negation of a correctness property, \nand check if the language of this automaton is empty. This approach can be viewed as indirectly computing \nthe greatest .xed point of trace P,Q , where P represents the set of initial states of a system, and \nQ represents states that violate a property.  3.2 Abstract Top-Everywhere CD C L is distinguished from \nseveral procedures that combine dual reasoning because the procedure used to reason about the by SAT \nsolvers to generalise the reason behind a partial assignment leading to a con.ict. Minimisation techniques \nreplace a partial assignment p with partial assignments from which p can be derived by the unit rule. \nThere may not be a unique partial assignment from which p is derived, so downsets of partial assignments \nhave to be considered. We de.ne a transformer for con.ict minimisation below. minimise. : D(PAsg ) . \nD(PAsg ) minimise.(Q) = Q . {p | UnitC (p) . Q for some C . .} Con.ict minimisation can dually be viewed \nas restricted application of resolution. Example 1. Consider a formula . = . . (x . \u00acy) . (\u00acx . y) and \na partial assignment p = {x . true, y . true} such that pgfp(Unit.)(p) is .. Since y can be derived by \nthe unit rule if x is true and x can be derived if y is true, we have minimise.(p) = {{x . true} , {y \n. true}}. < There may be many ways to minimise a con.ict but generating all minimisations exhausts solver \nmemory. To avoid representing sets of con.icts, a S AT solver usually chooses one con.ict. In Example \n1, each set in . = {{x . true}, {y . true}} can be used. We model this step with a choice function choice \n: D(PAsg ) \u00d7 D(PAsg ) . D(PAsg ). The choice function ensures that the effort of generalising a con.ict \nis not lost, and satis.es . choice (., . ' ) . ', where . and . ' are downsets. Observe that choice is \nan upwards interpolation. We emphasise that the soundness of the procedures in this section follows from \nstandard results in abstract interpretation. Over programs, instances of Abstract-non-T are procedures \nthat underapproximate least .xed point computations. For example, the set of all counterexamples leading \nto an error can be de.ned by a least .xed point. Counterexample analysis techniques usually underapproximate \nthis set by heuristically choosing one, or some subset of counterexamples.  Abstract-non-. Abstract-non-T \nTheorem 10. Let fo be a sound overapproximation of a completely Figure 2. Abstract Con.ict Driven Learning \n4. Abstract Con.ict Driven Learning The procedures in the previous section were treated separately. The \nC DC L algorithm in modern solvers combines Abstract-non-. and Abstract-non-T as summarised in Figure \n2. Rather than return unknown from Abstract-non-., information from the .xed point computation drives \nAbstract-non-T. If Abstract-non-T produces inconclusive results, Abstract-non-. can still learn information \nabout the con.ict for the greatest .xed point computation. This section makes this combination precise. \nThe Abstract Con.ict Driven Learning procedure (AC D L) is shown in Algorithm 3. The procedures alternates \nruns of Abstract\u00adnon-. and Abstract-non-T. Communication between the two pro\u00adcedures is achieved using \ntwo functions, fou and learn . Con.icting elements are transferred from Abstract-non-. to Abstract-non-T \nusing a function fou . We require that this function soundly underap\u00adproximates f , i.e., that f . .O \n. .U . fou . A natural choice for this transformer is to compute the composition aU . .O, which maps \nan abstract element in O to its best underappproximation in U. In the other direction, a transformer \nis learnt, as discussed below. Algorithm 3: Abstract Con.ict Driven Learning v ACDL(fo : O . O, : O . \nO, additive, reductive transformer f and let fu be a sound underapprox\u00adimation of the De Morgan dual \nf of f. If pgfp(fo)(o) represents the empty set, then the transformer fo n learn u, where u is plfp( \nfu )(aU (.O(o))) is a sound overapproximation of f. We give an example of a simple, sound learning procedure \nsupported by all lattices. An element that leads to . is tabu, in the sense of tabu search. Tabu learning \nde.nes a sound learning transformer Tabu : U \u00d7 O . O. . if .(o) . .(u) Tabu u(o) = o otherwise The tabu \nrule checks whether search has entered a region that is known to map to .. Theorem 11 (Soundness). If \nACDL returns not ., then f is not bottom everywhere. If it returns ., then f is bottom everywhere. Proof. \nAssume the algorithm returns not .. Then Abstract-non-. returned (not ., o). Then fo is .-complete at \no, .O(o) = . and fo(o) = o. By .-completeness, we then conclude that f is not bottom at .O(o). Assume \nthe algorithm returns .. Then Abstract-non-T re\u00adturned T. We denote by u the initial value of u in Abstract\u00adnon-T, \nand by u ' the return value with .U (u ' ) = T. It is an invariant of the algorithm that fo is a sound \noverapproximation of f. Hence, whenever Abstract-non-. returns (., o) it holds that f(.O(o)) = .. It \nholds that u = aU . .O(o) for such an o. By duality, we have that f (.) . .O(o) . .U (u). Abstract-non-T \nreturns u ' ; u such that .U (u ' ) = T. By soundness of abstract interpretation it holds that .U (u \n' ) = f (.U (u)) = T. Since f is an upper closure operator with f (.) . .U (u) and f (.U (u)) = T, it \nfollows by idempotence that f (.) = T. Dually, it follows that f(T) = .. Therefore, f is bottom everywhere. \n fou : O . U, learn : U . (O . O)) loop L. .fu U U U U U\u00d7: :, ,r (s, o) . Abstract-non-.(fo, T, v ) \nif s = . then return (s, o) u . fou (o) L. TAbstract-non-(() fu s, uu, ,r ) if s = T then return (., \nu) 5. Abstract Con.ict Driven Clause Learning The previous section showed that the notion of learning \nis very general. Clause learning is a speci.c form of learning in which abstract transformers are implicitly \nrepresented by clauses. Since model search in S AT solvers is driven by the unit rule, clause learn\u00ading \ncan be viewed as learning unit rule transformers. We present a generalised unit rule that lifts clause \nlearning to richer lattices. fo . fo n learn (u) Best Learning Transformer If Abstract-non-. derives \n. start\u00ading from o, to determine if f is bottom at a concrete element c, it suf.ces to check if f is \nbottom at cn\u00ac.O(o), because f is reductive. If Abstract-non-. is invoked on an abstract element a, it \nsuf.ces to check if fo is bottom on elements below aO(\u00ac.O(o) n .O(a)). The procedure Abstract-non-T is \nused to generalise an element that leads to a con.ict but it operates in an underapproximating domain. \nThus, we need to learn information about overapproximate elements after generalising within an underapproximation. \nThe best learning transformer is a function that takes elements u and o and subtracts u from o. learn \n: U \u00d7 O . O learn u(o) = aO(.O(o) n \u00ac.U (u)) Abstract Con.ict Driven Clause Learning (ACD C L) is a strict \ngener\u00adalisation of the C DC L algorithm in S AT solvers. In \u00a7 3, we demon\u00adstrate that propositional solvers \noperates over the overapproximate partial assignments domain PAsg and its underapproximate down\u00adset completion \nD(PAsg ). We present AC D CL as a variant of the AC D L procedure presented in the previous section operating \nover an overapproximation O and its downset completion D(O). 5.1 Generalising Clause Learning To understand \nthe lattice-theoretic essence of clause learning, it is useful to compare the unit rule with tabu learning. \nConsider a clause C = p . \u00acq and the partial assignment p = {p . f, q . t}, which contains no satisfying \nassignment to C. Consider p ' strictly greater than p. We have that Tabu p(p ' ) = p '. However, if p \n' is the partial assignment {p . false}, by unit rule application we have UnitC (p ' ) = {p . false, \nq . false}. The tabu rule only drives search away from elements where f is bottom. In contrast, if f \nis  y y A . A. In contrast to the propositional unit rule, which is de.ned since the negation of a \nclause is a partial assignment on which 4 6 6 c with respect to a clause, we de.ne the generalisation \nwith respect to elements that lead to .. This is only a difference of presentation, gunitc(q) Unit. \nis bottom. 2 . .. . .. . . for all m . mdc(c).a m x x a n n mdc(c) = M . {n}and for all m . M. a 246 \n246 gunit (a) = c Figure 3. Generalised unit rule gunit for intervals c m a otherwise almost bottom \non p ' , the unit rule drives the search away from the part of p ' that leads to bottom. Example 2. We \nillustrate a generalised unit rule for the abstract domain of intervals over integers. Assume an abstract \ntransformer fo is . on the interval c in Figure 3. We design a generalised unit rule that maps the shaded \ninterval q in the .gure to the interval gunitc(q), which drives the search to the portion of q not known \nto lead to .. In what follows, we write an interval constraint such as 5 = x = 7 . -8 = y = 8 as {x . \n[5, 7]}. The complement of the interval c = {x . [1, 4], y . [1, 6]}is not an interval. However, c is \nthe intersection of the one-way in.nite intervals {x . [1, 8]}, {x . [-8, 4]}, {y . [1, 8]}and {y . [-8, \n6]}. The complement of each of these intervals is an interval, and the set of complements can be viewed \nas a clause containing {x . [-8, 0]}, {x . [5, 8]}, {y . [-8, 0]} and {y . [7, 8]}. The meet of q = {x \n. [2, 6], y . [2, 4]} with each element of this generalised clause is bottom for all elements except \n{x . [5, 8]}, so we only consider {x . [5, 6]}. In this way, we generalise the propositional unit rule \nto new domains. < We point out the similarities to the propositional unit rule. Every interval is the \nintersection of one-way in.nite intervals, just as a partial assignment is the conjunction of literals. \nOne-way in.nite intervals, like propositional literals, have complements. The complement of a partial \nassignment is a clause, and the complement of an interval can be represented as the disjunction of one-way \nin.nite intervals. The rest of this section lifts the unit rule to new domains. Complementable Decompositions \nThis section deals with decom\u00adpositions of lattice elements into elements which cannot be further decomposed, \ncalled irreducibles. An element m is meet irreducible if x n y = m implies that x = m or y = m. The set \nof meet irreducibles of a lattice A is MA. A function mdc : A . P(MA) is a meet decomposition if, for \nall a, the set mdc(a) is .nite, andn mdc(a) = a. A meet decomposition is irredundant, if for any a . \nA and b . mdc(a) it holds that n mdc(a) \\ {a} = a. Most abstract domains used in practice have an obvious \nunique irredund\u00adant meet decomposition (e.g., partial assignments decompose into partial assignments \nwith only one variable taking a Boolean value, intervals and octagons decompose into sets of half-spaces). \nA num\u00adber of examples of possible abstract domains with complementable decompositions are given in Figure \n4. Complementable decompos\u00aditions are not limited to numeric abstractions. For example, the depicted \ncontrol-.ow abstraction abstractly represents a set of traces in terms of control-.ow branches. An element \na of an abstract domain A has a precise complement if there exists an element a such that .(a) = \u00ac.(a). \nA has comple\u00admentable meet irreducibles if every element m . MA has a precise complement m . MA. A domain \nA admits complementable meet decompositions if A has complementable meet irreducibles and a meet decomposition. \nGeneralised Unit Rule An abstraction A with complementable meet decompositions admits a generalised unit \nrule gunit : A \u00d7 If gunitc(a) returns a n n as above, we call n a unit irreducible. The transformer gunit \ncan be lifted to a learning transformer gunit : O \u00d7 U . O, where U = D(O). For a downwards\u00adclosed set \nS, we de.ne gunitS = nc.S gunitc. We skip the proof that gunit is a sound learning transformer. We instantiate \nabstract con.ict driven learning with gunit to derive Abstract Con.ict Driven Clause Learning.  5.2 \nAn Abstract Backjumping Algorithm Abstract con.ict driven learning is sound. To achieve complete\u00adness, \nwe need to ensure that learning drives the Abstract-non-. procedure to explore new regions of the abstract \nlattice. The ab\u00adstract backjumping algorithm appearing next generalises the back\u00adjumping search of propositional \nCDCL. The abstract procedure in Algorithm 4 operates over an overapproximate domain O with com\u00adplementable \ndecompositions, and its underapproximate downset completion D(O). Learning re.nes the transformer fo \nwith the generalised unit rule gunit. Algorithm 4: Backjumping and Clause Learning ACDCL(fo: O . O, v \n: O . O, fu: D(O) . D(O), L r: D(O)\u00d7 D(O) . D(O)) Initialize . . O * to singleton sequence TO repeat \n(s, .) . Abstract-non-. * (fo, ., v ) if s = . then return (s, n .) u . {n .} (s, {c}) . Abstract-non-T( \nfu, u, L ) r l . gunitc . . backjump(l, fo, .) fo . fo n l until . is empty return not . v Abstract-non-. \n* (fo : O . O, : O . O, . : O * ) o . n . repeat o ' . o; o . fo(o) n o until o = o ' or .(o) = \u00d8 if \n.(o) = \u00d8 then return (., .) . . . \u00b7 o if fo satis.es .-completeness criterion at o then return (not \n., .) d . v o if d = o then return (unknown,o) return Abstract-non-. * (fo, . \u00b7 d, v ) The algorithm \nuses a modi.cation of Abstract-non-. to record transformer application on a stack .. Concatenation of \ntwo stacks  IN T E RVA LS OCTAGON S EQUALI TY Concrete Domain P(V . Z) P(V . Z) P(V . Z) Abstract Elements \n(l = x = u, . . .) (\u00b1x \u00b1 y < c, . . .) (x = y, w = z, . . .) Meet Irreducibles (x = 4) (x + y < 1) (x \n= y) Complemented Irreducible (5 = x) (-x -y < 0) (x = y) ARR AY AB S T RAC TI O N SET ABSTR ACTI O N \nCO NT ROL-F LOW AB ST RAC T ION Concrete P(V . (N . Z)) P((V . D) \u00d7 (SV . P(D)) P(Traces ) Abstract Elements \n(x : .i.cx, y : .i.cy, . . .) (x . S, y /. R, . . .) (li . else, . . .) Meet Irreducibles x : .i.4 x \n. Q (l1 . else) Complemented Irreducibles x : .i.4 x . Q (l1 . if) Figure 4. Complementable Decompositions. \nThe set V represents program variables. . and . ' is denoted by . \u00b7 . '. The difference to Abstract-non-. \nis that after learning backjumping resets the procedure to a state before the element that is negated \nby the learning transformer. A call to backjump(l, fo, .) returns a non-empty pre.x . ' of . such that \n.(fo ' (n . ' )) = . where fo ' = l n fo. If no such pre.x exists, the empty sequence is returned, indicating \nthat fo is bottom-everywhere. Progress and Precision If we disregard extensions such as restarts and \nforgets, propositional C D C L never generates the same clause twice. The number of clauses over a .xed \nset of literals is .nite, so a CDC L solver makes constant progress and necessarily terminates. The procedure \nACDCL makes progress if each iteration of the outer loop decreases some well-founded order. C D C L solvers \ndo not return unknown. Decisions re.ne partial assignments until either . or a satisfying assignment \nis obtained. In propositional solvers, decisions are made by computing a greatest lower bound between \nthe current element and a meet irreducible, that is, a partial assignment in which only one variable \nis not T. Decisions never directly cause a con.ict. We generalise these conditions below. A meet irreducible \nextrapolation is a downwards extrapolation operator v : O . O satisfying two conditions. 1. v o equals \no n m for some meet-irreducible m. 2. If .O(o) is not ., .O(v o) is not .. The precision requirement \ngiven next ensures that a sequence of decisions eventually leads to an abstract element that can be analysed \nwithout loss of precision. A meet-irreducible extrapolation is precise  if whenever o is equal to v \no, the function fo is .-complete at o with respect to f. The progress condition in a propositional solver \nis that learn\u00ading causes model search to deduce new information. The general\u00adised notion we use is that \nof asserting backjumps, which ensures that the procedure navigates away from the element that most re\u00adcently \nled to .. A backjump function is asserting if whenever backjump(l, fo, .) returns . ', then . ' is the \nempty sequence or l(n . ' ) is strictly smaller than n . '. This condition ensures that backjumping drives \nthe search into a new part of the search space. Lemma 12. In some non-.nal iteration of the ACDCL main \nloop, let . and . ' be, respectively, the stack before and after backjumping. If the backjumping function \nis asserting, then after learning, it holds that fo(n . ' ) and n . are incomparable. Proof. Let o and \no ' be, respectively, n . and n . ', and let {c} be the element returned by Abstract-non-T. It holds \nthat c ; o. Let gunitc(o ' ) = o ' n m where m is the unit irreducible and therefore after learning fo(o \n' ) m. It holds that m ; c and therefore also m ; o. We now show that fo(o ' ) and o are not ordered. \nAssume for a contradiction that fo(o ' ) o. Then it holds that fo(o ' ) m. Since we also know that fo(o \n' ) m it must hold that .(fo(o ' )) = \u00d8. This violates the condition that the backjumping function returns \na non-con.icting pre.x . '. Therefore fo(o ' ) o. Now assume for a contradiction that o fo(o ' ). Then \no m, and from o c we can derive o m and consequently .(o) = .. This is not possible because Abstract-non-. \n* never returns a trail representing the empty set. We have shown that o and fo(o ' ) are incomparable. \nAnalogous to the case of propositional SAT, asserting backjumps can be implemented using a meet-irreducible \nextrapolation operator. We informally sketch the reason: Let . be the stack .1 . . . .k(.knm) during \na run of the procedure such that the last element (.k n m) is derived by applying a meet-irreducible \nextrapolation operator. If we can determine that f(.O(n .)) = \u00d8, we may re.ne fo by learning the transformer \ngunitn .. Backjumping to .1 . . . .k and then applying gunitn . yields the element .k n m, which drives \nthe search to a new region. Theorem 13 (Relative Completeness). If the downwards extrapol\u00ad v L ation \nv is precise, and ACDCL(fo, , fu , r) terminates, the result is . or not .. Proof. We prove by contradiction. \nIf unknown is returned, Abstract\u00adnon-. returned (unknown, o), which is only possible if v o equals o. \nSince v is precise, we know that fo is .-complete at o. This is impossible because not . would have been \nreturned. Theorem 14 (Termination). If O is .nite and backjumps are assert\u00ading, then ACDCL makes progress. \nProof. The procedures Abstract-non-. and Abstract-non-T ter\u00adminate over .nite lattices. To prove the \ntheorem, we assume that the outer loop of ACDCL is non-terminating and derive a contradiction. Let r \nbe the last element of the sequence . at the beginning of an arbitrary iteration of the main loop of \nAC D CL. Since the lattice is .nite, we can reason using well-founded induction. We show that backjumping \neventually resets the stack to a pre.x whose last element is b : r. Base step: Assume r is an atom, meaning \nevery r ' that satis.es r ' c r also satis.es r ' = .. Then the Abstract-non-T step will be initialised \nwith {r}, and will return an element c ; r. Therefore, it holds that after learning, fo(r) = .. Backjumping \ntherefore necessarily returns to an element b : r in order to satisfy the condition that after a backjump \nfo(b) = .. Induction step: Assume that for all r ' c r, the induction hypothesis holds. Assume for a \ncontradiction that backjumping never jumps to an element greater than r. Then r is the target of a backjump \nin.nitely often. Then it must be the case that in two subsequent iterations i1 and i2, Abstract-non-. \n* returns a stack with the same .nal con.icting element c such that r is the target of the corresponding \nbackjump in both cases. After the .rst such backjump, we have by Lemma 12 that fo immediately infers \nan element a c r such that a and c are unordered. Speci.cally, a is not greater than c. Therefore, any \nsubsequent call to Abstract-non\u00ad. * returns a stack with a last element that is smaller than a, and therefore \ndifferent from c. This contradicts the above, which states that i2 backtracks from c.  This completes \nthe proof. It therefore holds that, in an in.nite run, backtracking will eventually return an element \nstrictly greater than T. This is impossible, hence all runs of AC D C L terminate. Though we only show \ntermination for .nite lattices, the termina\u00adtion of C D C L is usually non-trivial to argue. When applied \nto decid\u00adable logics that involve in.nite lattices (such as linear arithmetic), speci.c details of the \ntheory, the transformers and acceleration tech\u00adniques must be used to prove termination. It is not clear \nthat there are general termination arguments for in.nite lattices. Ascending and descending chain conditions \nare not enough because of the al\u00adternation of the two procedures and the use of decisions and choice, \nwhich also operate on in.nite sets. 6. Abstract Implication Graphs The design of underapproximate transformers, \nsuch as fu , is a challenging problem that has received less attention than the design of overapproximate \ntransformers. S AT solvers use fo applications to construct an implication graph, and use this graph \nto derive fu . We now generalise implication graphs to other domains. Implication Graphs as Transformer \nAbstractions Consider a proof rule that when applied to antecedents a1, . . . , an yields the consequent \nc. If we view antecedents and consequents as elements of an abstract domain, we can formalise a proof \nrule as a transformer fo that satis.es fo(n {a1, . . . , an}) c. Implication graphs provide a means to \nderive antecedents from consequents thereby implementing an abduction transformer. Recall that MO is \nthe set of meet irreducibles of O. An im\u00adplication edge is a directed hyperedge in EI = P(MO) \u00d7MO. We \norder implication edges so that E1 . E2 means E1 requires weaker antecedents than E2 to derive stronger \nconsequents. De.ne (M1, m1) . (M2, m2) to hold if m1 m2 and for every m1 ' ' '' in M1 there exists m2 \nin M2 satisfying m1 ; m2. An implica\u00adtion graph G is an upwards-closed set of implication edges and the \ndomain of implication graphs I = (U (EI), ., ., n) contains implication graphs with the supserset order. \nIn practice, an upwards\u00adclosed set is represented by its minimal elements. Figure 5 contains examples \nof implication graphs. All incoming edges to a node represent one hyperedge and each hyperedge represents \nits upwards-closure. A hyperedge of the form ({}, m) is depicted by m. An implication graph represents \nan abstraction of a transformer, as shown below. Assume below that fo is a reductive transformer. Let \n(Red O, ) be the lattice of reductive transformers on O with the pointwise order. De.ne two functions \naI : Red O . I and .I : I . Red O below. aI(f) = {(M, m) . EI | f(n M ) m} .I(I) = .o. n{m | .(M, m) \n. I . n M ; o} Proposition 15. The functions aI and .I form a Galois connection. Example 3. We illustrate \nthe application graph construction for reachability analysis of a program with the interval abstract \ndomain. The problem is to determine if the location i is reachable in the .ow graph in Figure 6. An implication \ngraph derived by applying abstract successor transformers is shown alongside. The label DL0, denotes \ndecision level 0, which contains meet irreducibles that are deduced without making assumptions. These \nrepresent facts obtained by a run of a static analyser. Due to imprecision, the error is reachable in \nthe abstract. Suppose we apply downwards extrapolation to force the con\u00adstraint a = -42 at the location \nn1. This constraint is shaded in the .gure. If we continue static analysis with this assumption, we conclude \nthat i is unreachable. The facts required to arrive at this conclusion are shown in the implication graph \nin the .gure. < Extracting Dual Transformers We now show how con.ict ana\u00adlysis can be viewed as construction \nof a dual transformer from an implication graph. The dual transformer maps a set of consequences C to \nthe set of antecedents A from which we could have derived C. The dual implication transformer de.ned \nby an implication graph I is fu I : D(O) . D(O) below. fu I (C) = C . {q n (n M) | q n m . C . (M, m) \n. I} Theorem 16. If the implication graph I represents an overapprox\u00adimation of fo, the transformer fu \nI underapproximates f . Proof. Consider an implication graph I that overapproximates the transformer \nfo and a downwards closed set C . O. We prove the inequality .D(O) . fu I (C) . f . .D(O)(C). We show \nthat fo(c) is in C for all c in fu I (C). Consider c in fu I (C). If c is in C, we have that fo(c) is \nin C because fo is reductive and C is downwards-closed. If c is in fu I (C) \\ C, c is, by de.nition of \nthe form c = q n (n M), where q n m is in C for some (M, m) in I. Since I soundly approximates fo, we \nhave that fo(n M) m, and as a consequence also that fo(c) = fo(q n (n M)) q n m. Since C is downwards \nclosed and q n m is in C it holds that c . C. We have shown that {fo(c) | c . fu I (C)} is contained \nin C. The soundness of fo implies that the downwards-closure of T } f(.O(c)) | c . fu I (C) is contained \nin the downwards clos- T } ure of .O(c) | c . fu I (C) . We can rewrite the above condition as f . .D(O) \n. fu I . .D(O), where . is the pointwise lifting of the subset order. The functions (f, f ) form a Galois \nconnection, due to which, the inequality f . .D(O) . fu I . .D(O) is equivalent to .D(O) . fu I . f . \n.D(O). Thus, fu I underapproximates f . Example 4. Consider the interval implication graph I shown in \nFigure 5, consisting of the following hyperedges. {(\u00d8, y = 5), (\u00d8, x = 2), ({y = 5}, x = 6), ({x = 6, \nx = 2}, .)} We compute a least .xed point over fu I , and represent downwards\u00adclosed sets by their maximal \nelements. C0 = fu I (.) = {(x = 6, x = 2)} C1 = fu I (C0) = C0 . {{(y = 5, x = 2), (x = 6)}} C2 = \nfu I (C1) = C1 . {T} = T It follows that the formula is unsatis.able. This .xed point compu\u00adtation, in \nwhich meet irreducibles are replaced by their explanations, is similar to con.ict analysis in SAT solvers. \n< Generalising Implication Graphs We brie.y discuss implication graph generalisation. See [17] for a \nmore algorithmic discussion. In propositional C D CL, meet irreducibles are of the form {x . v} where \nv is t or f. Meet irreducible partial assignments are incomparable. In more general lattices, such as \nintervals, there is an order on meet irreducibles. The order on meet irreducibles represents implications \nthat follow from theory axioms. This order  IN TERVA LS OCTAGO NS EQUA L I TY y = 5  x - y < 0 x \n- z < 1 x = y x = z x = 6  . . x = 2 y - z < 1 -x + z < -8 y = z x = z . AR R AY AB STR. SET \nABST R. CON TROL-F LOW AB S T R . x : .i.0 y : .i.1  x /. Q . l1 . else l2 . else y : .i.9 . \nx . S x . Q l2 . if (to reach error) . Figure 5. Abstract Implication Graphs DL0 [a = -2] [a = 1] \n[a = -1] [a = 0]  DL1 n1 : a = -2 c1 : n2 : b = 1  Figure 6. Implication Graph for Reachability of \nthe Program Location Marked with i can be used to generalise implication graphs to derive more useful \ndual transformers. Example 5. We illustrate implication graph generalisation on the example in Figure \n6. Computing fu I for the graph I depicted leads to a sound transformer. Using this transformer one can \n.nd, for example, that n2 : b = 2 is suf.cient to deduce a con.ict. There is a more general graph that \nstill allows deduction of unreachability of i: The constraint on the node n2 : b = 2 can be weakened \nto obtain n2 : b = 1. Likewise, c1 : a = -42 and n1 : a = -42 can be generalised, respectively, to c1 \n: T and n1 : a = -2. Computing fu I1 over the resulting graph I ' allows one to .nd that n2 : b = 1 is \nsuf.cient to deduce a con.ict. By generalising the graph, we can derive more general conditions under \nwhich a location is unreachable. Learning allows us to negate this condition and discover that the .ow \ngraph always satis.es n2 : b = 0 because the variable b is in the interval [-1, -2] at the location n2. \nWe can now prove that i is unreachable without any assumptions. We note that AC DCL instantiated in this \nway over programs is a method to dynamically discover a trace partitioning [25]. < 7. Related Work A \nsurvey of the literature on combining logical solvers and abstract interpreters is beyond the scope of \nthis paper. We limit ourselves to discussing extensions of CDC L solvers to richer problems. The DP L \nL (T) architecture, also called lazy SM T combines a S AT solver with a solver for the conjunctive fragment \nof a theory, to determine satis.ability of quanti.er-free formulae in a theory [15]. In the original \nlazy S M T architecture, decisions and learning take place in the propositional solver, which may cause \ntheory facts to be enumerated. Splitting on demand allows new theory facts to be encoded by propositional \nvariables and added to a formula [2]. AC DC L is an alternative to lazy SMT in which all reasoning happens \ndirectly in a fragment of the theory, and no propositional solver is required. See [3] for a theoretical \nand empirical comparison of ACD C L and D P L L(T). Other approaches that jettison a propositional solver \nin favour of making decisions and learning directly in a theory include natural-domain SM T [5], generalised \nDP L L [23] and theory speci.c solutions for equality [1], and integer linear arithmetic [20] using the \ncutting-planes proof system. AC DC L is one recipe for generalising CD CL to new domains. The details \nof the decision and learning operations have to be designed anew for each theory, but the algorithm used \nby the solver, and its soundness and relative\u00adcompleteness follow from our work. In separate work, we \nhave instantiated AC DC L to derive an SM T solver for .oating-point logic and shown that this solver \nperforms better than using a propositional encoding [17], and better than D P L L (T) [17]. S M T solvers \nuse the Nelson-Oppen and Delayed Theory Com\u00adbination methods to combine solvers for different theories. \nAbstract interpretation is a modular framework, which allows abstract do\u00admains to be combined in several \ndifferent ways [8]. The Nelson-Oppen method and D P L L (T) have recently been shown to be in\u00adstances \nof more general product constructions in abstract interpret\u00adation [3, 12]. These product constructions \nlift to instantiations of AC D C L: if AC DC L can be instantiated over domains O1 and O2, it can be \ninstantiated over the cardinal product O1 \u00d7 O2. Our work allows C D C L to be lifted to veri.cation problems. \nWe have instantiated AC D C L with the interval abstract domain to analyse the bounds of variables in \nprograms with .oating-point variables [14]. Lazy annotation with interpolants [22] lifts C D CL to programs, \nand uses an SMT solver to implement constraint propagation, and Craig interpolation for learning. We \nwork with abstract domains, and do not assume the domain is closed under Boolean operations, or that \nit supports interpolation. Satis.ability Modulo Path Programs [18] lifts DPL L(T) to programs by combining \na S AT solver and an abstract interpreter, and St \u00b0 almarck s method has been lifted to programs in [28]. \nOur work provides an abstract interpretation view of C D CL, which we extended to other procedures in \n[13], and to D P L L (T)  in [3]. An account of St\u00b0 almarck s method is given in [27], and [24] contains \na transition system view of CD C L. A natural question is whether AC D CL is a form of Counter-Example \nGuided Abstraction Re.nement (CE G A R) [4]. C E G A R uses non-. witnesses to construct a new domain \nand new transformers. AC D CL implements proof-guided transformer re.nement and makes progress when no \nnon-. witness is found. AC D C L never changes the domain, and this immutability is crucial for ef.ciency, \nbecause the implementations of the abstract domain and transformers can be highly optimised. 8. Conclusion \nIn this paper, we applied abstract interpretation to study the Con.ict Driven Clause Learning algorithm \n(C DCL) implemented by contem\u00adporary S AT solvers. We showed that CD C L can be understood as a lattice-theoretic \napproximation algorithm that combines overap\u00adproximations of greatest .xed points and underapproximations \nof least .xed points to determine properties of a function on a Boolean lattice. Our generalised Abstract \nCon.ict Driven Clause Learning (AC DC L) procedure and its correctness proofs rely on properties enjoyed \nby the data structures in S AT solvers as well as by abstract domains used in practice. In separate work, \nwe have instantiated AC DC L to derive an S M T solver and a program analyser and obtained positive results. \nProblems to investigate in future instantiations include satis.ability in theories of weak arithmetic, \nassertion checking with relational abstract domains, and nullness of pointers. A second family of problems \nis to design non-Boolean abstractions of states and traces so that AC D C L can be applied to model checking \nproblems, and to determine emptiness of non-deterministic automata over .nite and in.nite words. We opened \nthe paper by recalling a question posed by Malik and Zhang about whether the lessons from the success \nof SAT solvers lift to other domains. We believe that our work contributes a mathematical answer to this \nquestion that applies to algorithmic issues. Our work does not explain or provide a means to lift the \nheuristics used by SAT solvers to new problems. We conjecture that heuristics to boost ef.ciency of constraint \npropagation are closely related to those for exploiting sparsity in program analysis. It is our hope \nthat future work will provide a framework for understanding these connections and for lifting engineering \ntechniques in SAT solvers to new problem domains. Acknowledgments This work was supported by the Toyota \nMotor Corporation, ERC project 280053, EPSRC project EP/H017585/1, and the FP7 STREP PINCETTE. Vijay \nD Silva was supported by a Microsoft Research PhD Scholarship. We thank the abstract interpretation community, \nin particular Patrick and Radhia Cousot, for their support and encouragement of this work. Vijay D Silva \nthanks Heejong Lee and Wonchan Lee for their diligence in an independent project, which allowed him time \nfor this one. References [1] B. Badban, J. van de Pol, O. Tveretina, and H. Zantema. Generalizing DPLL \nand satis.ability for equalities. Information and Computation, 205(8):1188 1211, 2007. [2] C. Barrett, \nR. Nieuwenhuis, A. Oliveras, and C. Tinelli. Splitting on demand in SAT modulo theories. In Proc. of \nLogic for Programming, Arti.cial Intelligence, and Reasoning, pages 512 526, 2006. [3] M. Brain, V. D \nSilva, L. Haller, A. Griggio, and D. Kroening. An abstract interpretation of DPLL(T). In Proc. of Veri.cation, \nModel Checking and Abstract Interpretation, 2013. To appear. [4] E. Clarke, O. Grumberg, S. Jha, Y. Lu, \nand H. Veith. Counterexample\u00adguided abstraction re.nement for symbolic model checking. J. of the ACM, \n50:752 794, 2003. [5] S. Cotton. Natural domain SMT: A preliminary assessment. In Proc. of Formal Modeling \nand Analysis of Timed Systems, pages 77 91, 2010. [6] P. Cousot. Constructive design of a hierarchy of \nsemantics of a transition system by abstract interpretation. Theoretical Computer Science, 277(1-2):47 \n103, Apr. 2002. [7] P. Cousot. Abstract interpretation. MIT course 16.399, Feb. May 2005. [8] P. Cousot \nand R. Cousot. Systematic design of program analysis frameworks. In Proc. of Principles of Programming \nLanguages, pages 269 282, 1979. [9] P. Cousot and R. Cousot. Abstract interpretation and application \nto logic programs. Journal of Logic Programming, 13:103 179, 1992. [10] P. Cousot and R. Cousot. Abstract \ninterpretation frameworks. Journal of Logic and Computation, 2(4):511 547, Aug. 1992. [11] P. Cousot \nand R. Cousot. Re.ning model checking by abstract interpretation. Automated Software Engineering, 6(1):69 \n95, 1999. [12] P. Cousot, R. Cousot, and L. Mauborgne. The reduced product of abstract domains and the \ncombination of decision procedures. In Proc. of Foundations of Software Science and Computational Structures, \npages 456 472, 2011. [13] V. D Silva, L. Haller, and D. Kroening. Satis.ability solvers are static analysers. \nIn Proc. of Static Analysis Symposium, pages 317 333, 2012. [14] V. D Silva, L. Haller, D. Kroening, \nand M. Tautschnig. Numeric bounds analysis with con.ict-driven learning. In Proc. of Tools and Algorithms \nfor the Construction and Analysis of Systems, pages 48 63, 2012. [15] H. Ganzinger, G. Hagen, R. Nieuwenhuis, \nA. Oliveras, and C. Tinelli. DPLL(T): Fast decision procedures. In Proc. of Computer Aided Veri.cation, \npages 175 188, 2004. [16] R. Giacobazzi and E. Quintarelli. Incompleteness, counterexamples, and re.nements \nin abstract model-checking. In Proc. of Static Analysis Symposium, pages 356 373, 2001. [17] L. Haller, \nA. Griggio, M. Brain, and D. Kroening. Deciding .oating\u00adpoint logic with systematic abstraction. In Proc. \nof Formal Methods in Computer-Aided Design, pages 131 140, 2012. [18] W. R. Harris, S. Sankaranarayanan, \nF. Ivanci.\u00b4c, and A. Gupta. Program analysis via satis.ability modulo path programs. In Proc. of Principles \nof Programming Languages, pages 71 82, 2010. [19] T. A. Henzinger, O. Kupferman, and S. Qadeer. From \npre-historic to post-modern symbolic model checking. Formal Methods in Systems Design, 23(3):303 327, \nNov. 2003. [20] D. Jovanovic and L. M. de Moura. Cutting to the chase -solving linear integer arithmetic. \nIn Proc. of Automated Deduction, pages 338 353, 2011. [21] S. Malik and L. Zhang. Boolean satis.ability: \nFrom theoretical hardness to practical success. Communications of the ACM, 52:76 82, Aug. 2009. [22] \nK. L. McMillan. Lazy annotation for program testing and veri.cation. In Proc. of Computer Aided Veri.cation, \npages 104 118, 2010. [23] K. L. McMillan, A. Kuehlmann, and M. Sagiv. Generalizing DPLL to richer logics. \nIn Proc. of Computer Aided Veri.cation, pages 462 476, 2009. [24] R. Nieuwenhuis, A. Oliveras, and C. \nTinelli. Solving SAT and SAT modulo theories: From an abstract Davis Putnam Logemann Loveland procedure \nto DPLL(T). JACM, 53:937 977, 2006. [25] X. Rival and L. Mauborgne. The trace partitioning abstract domain. \nACM Transactions on Programming Languages and Systems, 29, 2007. [26] N. S \u00a8orensson and A. Biere. Minimizing \nlearned clauses. In Proc. of Theory and Applications of Satis.ability Testing, pages 237 243, 2009. [27] \nA. Thakur and T. Reps. A Generalization of St\u00b0almarck s Method. In Proc. of Static Analysis Symposium, \npages 334 351, 2012. [28] A. Thakur and T. Reps. A method for symbolic computation of abstract operations. \nIn Proc. of Computer Aided Veri.cation. Springer, 2012.   \n\t\t\t", "proc_id": "2429069", "abstract": "<p>Modern satisfiability solvers implement an algorithm, called Conflict Driven Clause Learning, which combines search for a model with analysis of conflicts. We show that this algorithm can be generalised to solve the lattice-theoretic problem of determining if an additive transformer on a Boolean lattice is always bottom. Our generalised procedure combines overapproximations of greatest fixed points with underapproximation of least fixed points to obtain more precise results than computing fixed points in isolation. We generalise implication graphs used in satisfiability solvers to derive underapproximate transformers from overapproximate ones. Our generalisation provides a new method for static analysers that operate over non-distributive lattices to reason about properties that require disjunction.</p>", "authors": [{"name": "Vijay D'Silva", "author_profile_id": "81100215907", "affiliation": "University of California, Berkeley, Berkeley, CA, USA", "person_id": "P3977937", "email_address": "vijayd@eecs.berkeley.edu", "orcid_id": ""}, {"name": "Leopold Haller", "author_profile_id": "81438596327", "affiliation": "University of Oxford, Oxford, United Kingdom", "person_id": "P3977938", "email_address": "leopold.haller@cs.ox.ac.uk", "orcid_id": ""}, {"name": "Daniel Kroening", "author_profile_id": "81100210974", "affiliation": "University of Oxford, Oxford, United Kingdom", "person_id": "P3977939", "email_address": "daniel.kroening@cs.ox.ac.uk", "orcid_id": ""}], "doi_number": "10.1145/2429069.2429087", "year": "2013", "article_id": "2429087", "conference": "POPL", "title": "Abstract conflict driven learning", "url": "http://dl.acm.org/citation.cfm?id=2429087"}