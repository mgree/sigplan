{"article_publication_date": "01-23-2013", "fulltext": "\n Principled Parsing for Indentation-Sensitive Languages Revisiting Landin s Offside Rule Michael D. Adams \nPortland State University http://michaeldadams.org/ Abstract Several popular languages, such as Haskell, \nPython, and F#, use the indentation and layout of code as part of their syntax. Be\u00adcause context-free \ngrammars cannot express the rules of indenta\u00adtion, parsers for these languages currently use ad hoc techniques \nto handle layout. These techniques tend to be low-level and opera\u00adtional in nature and forgo the advantages \nof more declarative spec\u00adi.cations like context-free grammars. For example, they are often coded by hand \ninstead of being generated by a parser generator. This paper presents a simple extension to context-free \ngrammars that can express these layout rules, and derives GLR and LR(k) algorithms for parsing these \ngrammars. These grammars are easy to write and can be parsed ef.ciently. Examples for several languages \nare presented, as are benchmarks showing the practical ef.ciency of these algorithms. Categories and \nSubject Descriptors D.3.1 [Programming Lan\u00adguages]: Formal De.nitions and Theory Syntax; D.3.4 [Pro\u00adgramming \nLanguages]: Processors Parsing; F.4.2 [Mathemati\u00adcal Logic and Formal Languages]: Grammars and Other \nRewriting Systems Parsing General Terms Algorithms, Languages Keywords Parsing, Indentation, Offside \nrule 1. Introduction Languages such as Haskell [Marlow (ed.) 2010] and Python [Python] use the indentation \nof code to delimit various grammat\u00adical forms. In Haskell, the contents of a let, where, do, or case \nexpression can be indented relative to the surrounding code instead of being explicitly delimited by \ncurly braces. In Python, the body of a multi-line function or compound statement must be indented relative \nto the surrounding code; there is no alternative, explicitly\u00addelimited syntax. For example, in Haskell \none may write: mapAccumR f = loop where loop acc (x:xs) = (acc , x : xs ) where (acc , x ) = f acc x \n(acc , xs ) = loop acc xs loop acc [] = (acc, []) Permission to make digital or hard copies of all or \npart of this work for personal or classroom use is granted without fee provided that copies are not made \nor distributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. POPL 13, January 23 25, 2013, Rome, Italy. Copyright c &#38;#169; \n2013 ACM 978-1-4503-1832-7/13/01. . . $15.00 The indentation of the bindings after each where keyword \ndeter\u00admines the parse structure of this code. For example, the indentation of the last line determines \nthat it is part of the bindings introduced by the .rst where instead of the second where. Likewise, in \nPython one may write: def factorial(x): result = 1 for i in range(1, x + 1): result = result * i return \nresult print factorial(5) Here the indentation determines that the for loop ends before the return and \nthe factorial function ends after the return. While Haskell and Python are well known for being indentation\u00adsensitive \nlanguages, quite a few other languages also use indenta\u00adtion. Landin s ISWIM [Landin 1966] introduced \nthe concept of the offside rule for indentation, which requires that all tokens in an expression be indented \nat least as far as the .rst token of the expression. Variations on this rule are used by Haskell, Miranda \n[Turner 1989], occam [INMOS Limited 1984], Orwell [Wadler 1985], Curry [Hanus (ed.) 2006], and Habit \n[HASP Project 2010]. F# is indentation sensitive when its lightweight syntax is enabled [Syme et al. \n2010, \u00a715.1]. The block styles in the YAML [Ben-Kiki et al. 2009] data serialization language are indentation \nsen\u00adsitive, as are many forms in the Markdown [Gruber] and reStruc\u00adturedText [Goodger 2012] markup languages. \nEven Scheme has an indentation-sensitive syntax in the form of SRFI-49 [M\u00f6ller 2005], though it is not \noften used. Whitespace sensitivity may be controversial, but regardless of whether it is a good idea \nfrom a language design perspective, it is important that the grammars of layout-sensitive languages be \nprecisely speci.ed. Unfortunately, many language speci.cations are informal in their description of layout \nor use formalisms that are not amenable to practical implementation. The task of parsing layout is thus \noften left to ad hoc, handwritten code. The lack of a standard formalism for expressing these layout \nrules and of parser generators for such a formalism increases the complexity of writing parsers for these \nlanguages. Often, practi\u00adcal parsers for these languages have signi.cant structural differ\u00adences from \nthe language speci.cation. For example, the layout rule for Haskell is speci.ed in terms of an extra \npass between the lexer and the parser that inserts explicit delimiters. This extra pass uses information \nabout whether the parsing that occurs later in the pipeline will succeed or fail on particular inputs. \nDue to the re\u00adsulting cyclic dependency, Haskell implementations do not actu\u00adally structure their parsers \nthis way. As a result, the structural dif\u00adferences between the implementation and the speci.cation make \nit dif.cult to determine if one accurately re.ects the other.   This paper aims to resolve this situation \nby proposing a gram\u00admar formalism for expressing layout rules. This formalism is both theoretically sound \nand practical to implement. Indentation\u00adsensitive grammars are easy and convenient to write, and fast \nand ef.cient parsers can be implemented for them. The primary contri\u00adbutions of this paper are: a grammar \nformalism for expressing indentation-sensitive lan\u00adguages, which is informally described in Section 2 \nand formally de.ned later in Section 4;  a demonstration in Section 3 of the expressivity of these gram\u00admars \nby showing how to express the layout rules of ISWIM, Miranda, Haskell, and Python in terms of these grammars; \n a development in Section 5 of GLR and LR(k) parsing algo\u00adrithms for these grammars, which is possible \nby a careful fac\u00adtoring of item sets into state and indentation sets; and  a demonstration in Section \n6 of the practical performance of these parsing techniques relative to existing ad hoc techniques.  \nSection 7 of this paper reviews related work. Section 8 concludes. Note that Section 5 of this paper \nassumes a fair amount of familiarity with standard parsing techniques and in particular the work by Knuth \n[1965] on LR(k) parsing. Other than Section 5, however, this paper assumes only a basic knowledge of \ncontext\u00adfree grammars. 2. The Basic Idea In order to support indentation-sensitive parsing, we use a \nmodi\u00ad.cation of traditional, context-free grammars. We parse over a se\u00adquence of terminals where every \nterminal is annotated with the col\u00adumn at which it occurs in the source code. We call this its inden\u00adtation. \nDuring parsing, we also annotate each non-terminal with an indentation. The grammar speci.es a numerical \nrelation that the indentation of each non-terminal must have with the indentation of its immediate children. \nThese relations are usually chosen so the indentation of a non-terminal is the minimum column at which \nany token in the non-terminal is allowed to occur. Thus, the indentation of a non-terminal usually coincides \nwith the intuitive notion of how far a block of code is indented. Formally, however, the indentation \nof a non-terminal has no meaning other than that it must appropri\u00adately relate to the indentation of \nthe non-terminal s children. We call these grammars indentation-sensitive context-free gram\u00admars (IS-CFG) \nto contrast them with traditional indentation\u00adinsensitive context-free grammars (II-CFG). Section 3 gives \nexam\u00adples of IS-CFGs for real world languages, and Section 4 formally de.nes this class of grammars. \nAs a simple example, we may write A . ( = A> ) = to mean that ( and ) must be at the same indentation \nas the A on the left of the production arrow but the A on the right must be at a greater indentation. \nWe may also write A . [ =A> ] = to mean the same except that [ and ] must be at a greater or equal in\u00addentation \nthan the Aon the left of the production arrow. In addition, we may write A . A = A = to mean that the \nindentation of both occurrences of Aon the right of the production must be at indenta\u00adtions equal to \nthat of the Aon the left of the production. Combined with the production A . e, these form a grammar \nof nested paren\u00adtheses and square brackets. In that grammar, matching parentheses must align vertically, \nand things enclosed in parentheses must be in\u00addented more than the parentheses are indented. Things enclosed \nin square brackets merely must be indented more than the surrounding code. Figure 1 shows examples of \nparse trees for this grammar on the words (1 [4(5)5]7)1 and (1[8(6 )6[8]9]4(3)3)1 where we write Xi to \nmean that iis the indentation of X. In these parse trees, take particular note of how the indentations \nof the non-terminals A1 A1 (1 A2 )1 (1 A3 )1 [4 A5 ]7 A3 A3 (5 A6 )5 [8 A6 ]4 (3 A4 )3 e A6 A6 e (6 \nA7 )6 [8 A7 ]9 e e Figure 1. Example IS-CFG parse trees for (1 [4(5)5]7)1 and (1 [8(6)6[8]9]4 (3)3)1 \nrespectively. and terminals relate according to the indentation relations speci.ed in the grammar. .1 \n.2 In general, we write a production as A . X1 X2 \u00b7 \u00b7 \u00b7 Xn .n , where .1, .2, \u00b7 \u00b7 \u00b7 , .n are relations \nbetween indentations, to mean that the indentation of A relates to the indentations of each X1, X2, \u00b7 \n\u00b7 \u00b7 , Xn according to .1 , .2, \u00b7 \u00b7 \u00b7 , .n. That is to say, this production requires j1 .1 i, j2 .2 i, \n\u00b7 \u00b7 \u00b7 , jn .n i if i is the in\u00addentation of Aand j1, j2, \u00b7 \u00b7 \u00b7 , jn are the respective indentations of \nX1, X2, \u00b7 \u00b7 \u00b7 , Xn. While in principle any set of indentation relations can be used, we restrict ourselves \nto the relations =, >, =, and .. The =, >, and = relations have their usual meanings. The . relation \nis {(i, j)|i, j . N}and effectively disassociates the indentation of a child from that of its parent. \nIndentation-sensitive languages typically have forms that re\u00adquire the .rst token of a subexpression \nto be at the same inden\u00adtation as the subexpression itself even though the non-terminal for that subexpression \ndoes not normally require this. Thus for every non-terminal or terminal, X, we introduce the non-terminal \n|X| that is identical to X except that its indentation is always equal to the indentation of its .rst \ntoken. This is merely syntactic sugar, as we can introduce the production |a|. a = for each terminal \na and the production |A| . |Xm| = X .m+1 \u00b7 \u00b7 \u00b7 X.n for each production m+1 n .1 .2 A . XX\u00b7 \u00b7 \u00b7 X.n where \nn = 1 and each m = n such that 1 2 n X1, X2, \u00b7 \u00b7 \u00b7 , Xm-1 are all nullable. For example, with the above \ngrammar, |A|would have the productions |A|. | ( | = A> ) = = A> and |A|. | [ | ] = from the .rst two \nproductions for A. It would also have |A| . |A = |A = and |A| . |A = | from the third production for \nA. By replacing .m with =, the .rst symbol of each production is forced to have the same indentation \nas the non\u00adterminal on the left of the production. By transitivity, this is the in\u00addentation of the .rst \ntoken in the non-terminal. This is a straightfor\u00adward, mechanical transformation requiring no input from \nthe user. 3. Indentation-Sensitive Languages Despite this system s simplicity, it can express a wide \narray of layout rules. This section demonstrates this by presenting the layout rules of several languages \nin terms of IS-CFGs. Where possible, we use the non-terminal names from the original grammar of each \nlanguage. Though not shown here, sketches for other indentation\u00adsensitive languages have been constructed \nfor occam, 1 Orwell, Curry, Habit, and SRFI-49. 1The additional indentation relation {(i + 2, i) | i \n. N} is required by occam as it has forms that require increasing indentation by exactly 2.  3.1 ISWIM \nand Miranda With ISWIM, Landin [1966] introduced several innovations still used in programming languages \ntoday. Among these is the use of indentation to indicate program structure. Landin de.nes an offside \nrule 2 that speci.es that: The southeast quadrant that just contains the phrase s .rst symbol must contain \nthe entire phrase, except possibly for [parenthesized] subsegments. [Landin 1966] This rule requires \nthat all the tokens in a particular non-terminal be at a column that is at least as far right (i.e., \nin the southeast quadrant) as the column of the .rst token. The IS-CFG for ISWIM is similar to the II-CFG \nfor ISWIM that ignores the offside rule except that we annotate each production of the IS-CFG with appropriate \nindentation relations. As an example, consider the productions in Figure 2 where we have applied the \noffside rule to only the right-hand side of where clauses.3 Note that the productions for |expr|are created \nautomatically from the productions for expr as speci.ed in Section 2. We do likewise for the terminals.4 \nUsing this grammar to parse the expression x + v where x = -( y + z) + w results in the parse tree in \nFigure 3. For productions that do not involve the offside rule, such as the productions for addition \nand negation, we annotate the non\u00adterminals with = and the terminals with =. This means that these productions \ndo not change the current indentation and terminals are allowed at any column greater than or equal to \nthe current indentation. For example, the expressions for x + v, (y + z), and all the variable references \nare all at the same indentation as their respective parents. For those expressions, no special indentation \nrule is in effect and we simply use = to propagate the indentation from parent to child. The terminals \nuse = so they can be at that or any greater indentation. If a non-terminal is too far left (e.g., if \nw was at column 5), then the = constraint is violated and the code would be rejected. This is a common \npattern that we will see in other grammars as well. Next, consider forms that involve the offside rule. \nWherever a non-terminal Ashould trigger the offside rule, we simply use |A|= instead of A = . An example \nis the right-hand side of a where clause = where we use |expr|= instead of expr . Because we use =instead \nof =, the right-hand side is allowed to be at a greater indentation than its parent. We see this in the \nexample parse tree where the right-hand side of the where clause has an indentation of 6 while its parent \nhas an indentation of 1. If the top-level expr were at indentation 7, however, such a parse with a right-hand \nside at indentation 6 would be rejected. We also use |expr| instead of expr so that the .rst token of \nthe right-hand side of the where clause has the same indentation as the entire right-hand side. As a \nconsequence, in the example parse tree, the indentations on the path from the right-hand side of the \nwhere to the - are all exactly 6. Finally, since parenthesized expressions are exempt from the indentation \nconstraints of the surrounding code, the production for parenthesized expressions uses expr . instead \nof expr = . This frees 2Landin spells it as both offside and off-side . We adopt the spelling consistent \nwith the rule from sports. 3Landin is not clear about whether his offside rule applies to all syntactic \nforms. His examples imply that it does not. For the sake of example, we also simplify where and omit \nthe multiple bindings case. That case uses the same techniques as Haskell s statement blocks. 4Of course, \nthe productions generated for the terminals = , + , ) and where are unreachable from the rest of the \ngrammar and can be omitted. Productions written by the user: expr . expr = where = ID= = = |expr|= = \n= expr . expr + = expr = expr . - = expr expr . ( = expr . ) = expr . ID= Productions added by desugaring: \n|expr| . |expr| = where = ID= = = |expr|= = |expr| . |expr| = + = expr = |expr| . | - | = expr |expr| \n. | ( | = expr . ) = |expr| . |ID| = | = | . = = | + | . + = | - | . - = | ( | . ( = | ) | . ) = |ID| \n. ID = | where | . where = Figure 2. IS-CFG productions for ISWIM. 1 expr expr 1 where 7 ID2 x = 4 |expr|6 \n11 6   expr + 3 expr |expr|6 + 8 expr 6 ID10ID1 x ID5 v | - |6 expr w  - 6 ( 7 expr 0 ) 6  expr \n0 + 3 expr 0 ID1 y ID5 z Figure 3. IS-CFG parse tree for ISWIM. the indentation of the expression inside \nthe parentheses from any indentation constraints coming from the context of the expression. This is used \nin the example parse tree where the y + z expression has an indentation of 0 even though the (y + z) \nexpression has an indentation of 6. Uses of the offside rule inside the parentheses still have effect, \nof course. The Miranda language uses the same offside rule as ISWIM ex\u00adcept for two differences. The \n.rst is that expressions inside paren\u00adtheses are subject to indentation constraints imposed by the context \noutside the parentheses. The production for parenthesized expres\u00adsions thus uses = instead of . and is \nsimply: = expr . ( = expr ) = The second difference is that the language speci.cation adopts the notational \nconvention that for any non-terminal x, x(;) means that x is followed by an optional semicolon and is \nsubject to the offside rule..., so that every token of x must lie below or to the right of the .rst. \nProvided the layout makes it clear where x terminates, the trailing semicolon may be omitted. [Turner \n1989, \u00a725] This notation is easily handled by introducing, for each non\u00adterminal A, the non-terminal \nA(;) and the two productions A(;). |A|= and A(;). |A|= ; .  L (<n>:ts) (m:ms) = ; : (L ts (m:ms)) if \nm = n = } : (L (<n>:ts) ms) if n < m L (<n>:ts) ms = L ts ms L ({n}:ts) (m:ms) = { : (L ts (n:m:ms)) \nif n > m L ({n}:ts) [] = { : (L ts [n]) if n > 0 L ({n}:ts) ms = { : } : (L (<n>:ts) ms) L ( } :ts) (0:ms) \n= } : (L ts ms) L ( } :ts) ms = parse-error L ( { :ts) ms = { : (L ts (0:ms)) L ( t :ts) (m:ms) = } : \n(L (t:ts) ms) if m .= 0 and parse-error(t) L ( t :ts) ms = t : (L ts ms) L [] [] = [] L [] (m:ms) = } \n: L [] ms if m .0 = Figure 4. Haskell s L function [Marlow (ed.) 2010, \u00a710.3].  3.2 Haskell 3.2.1 Language \nHaskell uses a more sophisticated offside rule than does ISWIM. Indentation-sensitive blocks (e.g. the \nbodies of do, case, or where expressions) are made up of one or more statements or clauses that not only \nare indented relative to the surrounding code but also are indented to the same column as each other. \nThus, lines that are more indented than the block continue the current clause, lines that are at the \nsame indentation as the block start a new clause, and lines that are less indented than the block are \nnot part of the block. In addition, semicolons (;)and curly braces ({ and })can explicitly separate clauses \nand delimit blocks, respectively. Explicitly delim\u00adited blocks are exempt from indentation restrictions \narising from the surrounding code. While the indentation rules of Haskell are intuitive to use in practice, \nthe way that they are formally expressed in the Haskell language speci.cation [Marlow (ed.) 2010, \u00a710.3] \nis not nearly so intuitive. The indentation rules are speci.ed in terms of both the lexer and an extra \npass between the lexer and the parser. Roughly speaking, the lexer inserts special {n} tokens where a \nnew block might start and special <n> tokens where a new clause within a block might start. The extra \npass then translates these tokens into explicit semicolons and curly braces. The special tokens are inserted \naccording to the following rules: If a let, where, do, or of keyword is not followed by the lexeme {, \nthe token {n} is inserted after the keyword, where n is the indentation of the next lexeme if there is \none, or 0 if the end of .le has been reached.  If the .rst lexeme of a module is not { or module, then \nit is preceded by {n} where n is the indentation of the lexeme.  Where the start of a lexeme is preceded \nonly by white space on the same line, this lexeme is preceded by <n>, where n is the indentation of the \nlexeme, provided that it is not, as a consequence of the .rst two rules, preceded by {n}. [Marlow (ed.) \n2010, \u00a710.3]  Between the lexer and the parser, an indentation resolution pass converts the lexeme stream \ninto a stream that uses explicit semi\u00adcolons and curly braces to delimit clauses and blocks. The stream \nof tokens from this pass is de.ned to be L tokens [] where tokens is the stream of tokens from the lexer \nand L is the function in Fig\u00adure 4. Thus the context-free grammar has to deal with only semi\u00adcolons and \ncurly braces. It does not deal with layout. This L function is fairly intricate, but the key clauses \nare the ones dealing with <n> and {n}. After a let, where, do, or of keyword, the lexer inserts a {n} \ntoken. If n is a greater indentation than the current indentation, then the .rst clause for {n} executes, \n= case . case > exp of > altBlock = --Explicitly delimited blocks altBlock . { > alts. } . --Layout-delimited \nblocks altBlock . altLayout> altLayout . altLayout = |alts| = altLayout . |alts| = --Clause sequences \nalts . alt = alts . alts = ; > alt = Figure 5. Grammatical productions for case. an open brace ({)is \ninserted, and the indentation n is pushed on the second argument to L (i.e., the stack of indentations). \nIf a line starts at the same indentation as the top of the stack, then the .rst clause for <n> executes \nand a semicolon (;)is inserted to start a new clause. If it starts at a smaller indentation, then the \nsecond clause for <n> executes and a close brace (})is inserted to close the block started by the inserted \nopen brace. Finally, if the line is at a greater indentation, then the third clause executes, no extra \ntoken is inserted, and the line is a continuation of the current clause. The effect of all this is that \n{, ;, and } tokens are inserted wherever layout indicates that blocks start, new clauses begin, or blocks \nend, respectively. The other clauses in L handle a variety of other edge cases and scenarios. Note that \nL uses parse-error to signal a parse error, but uses parse-error(t) as an oracle that predicts the future \nbehavior of the parser that runs after L. Speci.cally, if the tokens generated so far by L together with \nthe next token t represent an invalid pre.x of the Haskell grammar, and the tokens generated so far by \nL followed by the token } represent a valid pre.x of the Haskell grammar, then parse-error(t) is true. \n[Marlow (ed.) 2010, \u00a710.3] This handles code such as let x = do f; g in x where the block starting after \nthe do needs to be terminated before the in. This requires knowledge about the parse structure in order \nto be handled properly, and thus parse-error(t) is used to query the parser for this information. In \naddition to the operational nature of this de.nition, the use of the parse-error(t) predicate means that \nL cannot run as an independent pass; its execution must interact with the parser. In fact, the Haskell \nimplementations GHC [GHC 2011] and Hugs [Jones 1994] do not use a separate pass for L. Instead, the lexer \nand parser share state consisting of a stack of indentations. The parser accounts for the behavior of \nparse-error(t) by making close braces optional in the grammar and appropriately adjusting the indentation \nstack when braces are omitted. The protocol relies on some mildly complicated interactions between the \nlexer and parser [Jones 1994] and is tricky to use. While preparing the parser in Section 6, we found \nthat even minor changes to the error propagation of the parser affected whether syntactically correct \nprograms were accepted by this style of parser. While we may believe the correctness of these parsers \nbased on their many years of use and testing, the signi.cant and funda\u00admental structural differences \nbetween their implementation and the language speci.cation are troubling.  3.2.2 Grammar Haskell s \nlayout rule is more complicated than those of ISWIM and Miranda, but is also easily speci.ed as an IS-CFG. \nBy using an IS-CFG there is no need for an intermediate L function, and the lexer and parser can be cleanly \nseparated into self-contained passes. The functionality of parse-error(t) is simply implicit in the structure \nof the grammar. Figure 5 shows example productions for case expressions. For productions that do not \nchange the indentation, we annotate non\u00adterminals with a default indentation relation of = and terminals \nwith a default indentation relation of >. We use > instead of = because Haskell distinguishes tokens \nthat are at an indentation equal to the current indentation from tokens that are at a strictly greater \nindentation. The former start a new clause while the latter continue the current clause. In Haskell, \na block can be delimited by either explicit curly braces or use of the layout rule. In Figure 5, this \nis re.ected by the two different productions for altBlock. If altBlock expands to { > alts. }., then \nthe . relation allows alts to not re\u00adspect the indentation constraints from the surrounding code.5 Since \nHaskell s layout rule allows closing braces to occur at any col\u00adumn, we use . instead of the usual > \non } . On the other hand, if altBlock expands to altLayout>, then the > relation in\u00adcreases the indentation. \nIn the productions for altLayout, the use of |alts|instead of alts ensures that the .rst tokens of the \nalts all align to the same column. Note that within an alts, each alt must be separated by a semicolon \n(;). Thus, because altLayout refers to alts instead of alt, each instance of alt can be sepa\u00adrated using \neither layout or a semicolon. When using curly braces to explicitly delimit a block, semicolons must \nalways be used. Other grammatical forms that use the layout rule follow the same general pattern as case \nwith only minor variation to account for differing base cases (e.g., let uses decl in place of alt)and \nstructures (e.g., a do block is a sequence of stmt ending in an exp). A subtlety of Haskell s layout \nrule is that tokens on the same line as, but after, a closing brace may not have to respect the current \nin\u00addentation. This is because the L function considers the indentation of only the .rst token of a line \n(i.e., where <n> is inserted) and tokens after a let, where, do or of keyword (i.e., where {n} is inserted). \nOne might view this as an artifact of how the language speci.cation uses L to de.ne layout, but this \naspect of Haskell s layout rule is still expressible by having the lexer annotate tokens whose indentation \nis to be ignored with an indentation of in.n\u00adity.6 Since terminals have an indentation relation of >, \nthe in.nite indentation of these tokens will always match. We have the lexer handle this instead of the \nparser because it is the linear order of to\u00adkens instead of the grammatical structure of the syntax that \ncontrols what tokens are indentation sensitive. For example, the token after the do keyword is indentation \nsensitive regardless of the structure of the expression following the do. This requires the lexer to \nmain\u00adtain a bit of extra state indicating whether we are at the start of a line or after a let, where, \ndo or of keyword, but this is a fairly light requirement as the lexer is presumably already tracking \nstate to determine the column of each token. Finally, GHC also supports an alternative indentation rule \nthat is enabled by the RelaxedLayout extension. It allows opening braces to be at any column regardless \nof the current indentation [GHC 2011, \u00a71.5.2]. This is easily implemented by changing the .rst production \nfor altBlock to be: altBlock . { . alts. } . 5This assumes no tokens are at column 0, which we reserve \nfor this purpose. 6Of course, column information for error reporting should still use the actual position \nof the token.  3.3 Python 3.3.1 Language Python represents a different approach to specifying indentation \nsensitivity. It is explicitly line oriented and features NEWLINE in its grammar as a terminal that separates \nstatements. The grammar uses INDENT and DEDENT tokens to delimit indentation-sensitive forms. An INDENT \ntoken is emitted by the lexer whenever the start of a line is at a strictly greater indentation than \nthe previous line. Matching DEDENT tokens are emitted when a line starts at a lesser indentation. In \nPython, indentation is used only to delimit statements, and there are no indentation-sensitive forms \nfor expressions. This, com\u00adbined with the simple layout rules, would seem to make parsing Python much \nsimpler than for Haskell, but Python has line joining rules that complicate matters. Normally, each new \nline of Python code starts a new statement. If, however, the preceding line ends in a backslash (\\), \nthen the current line is joined with the preceding line and is a continuation of the preceding line. \nIn addition, tokens on this line are treated as if they had the same indentation as the backslash itself. \nPython s explicit line joining rule is simple enough to imple\u00adment directly in the lexer, but Python \nalso has an implicit line join\u00ading rule. Speci.cally, expressions in parentheses, square brackets or \ncurly braces can be split over more than one physical line without using backslashes. ... The indentation \nof the continuation lines is not important. [Python, \u00a72.1.6] This means that INDENT and DEDENT tokens \nmust not be emitted by the lexer between paired delimiters. For example, the second line of the following \ncode should not emit an INDENT and the indentation of the third line should be compared to the indentation \nof the .rst line instead of the second line. x = [ y ] z = 3 Thus, while the simplicity of Python s \nindentation rules is attrac\u00adtive, they contain hidden complexity that requires interleaving the execution \nof the lexer and parser.  3.3.2 Grammar Though Python s speci.cation presents its indentation rules \nquite differently from Haskell s speci.cation, once we translate it to an IS-CFG, it shares many similarities \nwith that of Haskell. The lexer still needs to produce NEWLINE tokens, but it does not produce INDENT \nor DEDENT tokens. As with Haskell, we start with a gram\u00admar where the non-terminals and terminals are \nannotated with in\u00addentation relations of = and >, respectively. In Python, the only form that changes \nindentation is the suite non-terminal, which represents a block of statements contained inside a compound \nstatement. For example, one of the productions for while is: = while_stmt . while > test : > suite = \nA suite has two forms. The .rst is for a single-line statement and is the same as with the standard Python \ngrammar. The second is for multi-line statements. The following productions handle both of these two \ncases. suite . stmt_list = NEWLINE> suite . NEWLINE> block> block . block = |statement| = block . |statement| \n=  When a suite is of the multi-line form (i.e., using the second pro\u00adduction), the initial NEWLINE \ntoken ensures that the suite is on a separate line from the preceding header. The block inside a suite \nmust then be at some indentation greater than the current inden\u00adtation. Such a block is a sequence of \nstatement forms that all start with their .rst token at the same column. In Python s gram\u00admar, the productions \nfor statement already include a terminating NEWLINE, so NEWLINE is not needed in the productions for \nblock. For implicit line joining, we employ the same trick as for paren\u00adthesized expressions in ISWIM \nand braces in Haskell. For any pro\u00adduction that contains parentheses, square brackets or curly braces, \nwe annotate the part contained in the delimiters with the . inden\u00adtation relation. Since the .nal delimiter \nis also allowed to appear at any column, we annotate it with .. For example, one of the pro\u00adductions \nfor list construction becomes: atom . [ > listmaker. ]. There remain a few subtleties with Python s line \njoining rules that we must address. First, as with Haskell, tokens after a closing delimiter can appear \nat any column. For example, the following code is validly indented according to Python s rules: while \nTrue: x = 1 + ( 2) + 3 To handle this we use the same trick as for Haskell and annotate tokens that \nare not at the start of a line with an in.nite indentation. Second, while a lexer based on regular expressions \ncan detect the start of a line and thus produce .nite indentations for the .rst to\u00adken of a line but \nin.nite indentations for other tokens, it cannot de\u00adtect matching parentheses to determine that NEWLINE \ntokens should be omitted inside delimited forms. Thus non-terminals that occur inside delimited forms \nneed to allow the insertion of NEWLINE to\u00adkens at arbitrary locations. This may mean there have to be \ntwo forms of a non-terminal (i.e., for expressions inside versus outside a delimited form), but this \nis a fairly mechanical transformation that can be automated by the use of syntactic sugar similar to \nthe syntactic sugar for |A|. Alternatively, it may be possible to use a grammar that does not use NEWLINE \ntokens at all and instead, like for Haskell, uses vertical alignment to delimit statements. Finally, \nas with the standard Python parser, the lexer still han\u00addles the explicit line joining that is triggered \nby a line ending in a backslash (\\). It gives the tokens of an explicitly joined line the same indentation \nas the backslash itself, and the backslash is not emitted as a token.  3.4 Conventions and syntactic \nsugar In an IS-CFG, every symbol in every production must be anno\u00adtated with an indentation relation. \nIn many indentation-sensitive languages, however, productions often allow terminals to appear at any \nindentation greater than the current indentation but do not themselves change the current indentation. \nThus we can simplify the job of writing an IS-CFG by adopting the convention that if a symbol on the \nright-hand side of a production is not explicitly an\u00adnotated with an indentation relation, then it implicitly \ndefaults to = if it is a non-terminal and > if it is a terminal. For example, with this convention, the \nonly productions in Figure 5 that need explicit annotations are those for altBlock. All other productions \nsimply use the defaults. Using this convention most productions in a gram\u00admar do not have to be annotated \nwith indentation relations. They thus look like ordinary II-CFG productions, and only the forms that \nexplicitly deal with indentation must be explicitly annotated. In addition, just as II-CFGs often allow \nthe use of alternation bars (|)or Kleene stars (*) to simplify writing grammars, it is often convenient \nto allow symbols on the right-hand side of a production to be annotated with a composition of indentation \nrelations. Thus we might write A instead of the more verbose . C>. A . B. B . C> These conventions are \nmerely notational conveniences and do not affect the fundamental theory. 4. Indentation-Sensitive Grammars \nThe formalism for IS-CFGs that this paper proposes is an extension of II-CFGs. Thus to review the standard \nde.nition of II-CFGs, recall that a grammar is a four-tuple G = (N, S, d, S)where N is a .nite set of \nnon-terminal symbols, S is a .nite set of terminal symbols, d is a .nite production relation, and S . \nN is the start symbol. The relation d is a subset of N \u00d7 (N .S) * , and we write A . X1X2 \u00b7 \u00b7 \u00b7 Xn for \na tuple (A, X1X2 \u00b7 \u00b7 \u00b7 Xn) that is an element of d. As a notational convention let A, B, C be elements \nof N, let a, b, c be elements of S, and let X, Y, Z be elements of N .S. Let U, V, W be elements of (N \n.S) * , and u, v, w be elements of S * . We de.ne a rewrite relation (.). (N .S) * \u00d7(N .S) * such that \nUAV . UX1X2 \u00b7 \u00b7 \u00b7 XnV iff A . X1X2 \u00b7 \u00b7 \u00b7 Xn. We de.ne (. * )as the re.exive, transitive closure of (.). \nThe language recognized by a grammar is then de.ned as L(G) = {w . S * |S . * w}and is the set of words \nreachable by the rewrite relation from the start symbol. An IS-CFG is also a four-tuple, G = (N, S, d, \nS), except that d and S account for indentations. S is an element of N \u00d7 N and records the indentation \nof the initial non-terminal. The production relation, d, is an element of N \u00d7 ((N .S) \u00d7 I) * where I \nis the domain of indentation relations and each indentation relation is a subset of N \u00d7N. In principle, \nthese indentation relations can be any subset of N \u00d7 N, but for our purposes we restrict I to the relations \n=, >, =and .. Here and in the remainder of this paper, we restrict ourselves to .nite indentations, but \neverything generalizes straightforwardly to languages with in.nite indentations. As a notational convention, \nlet i, j and l be indentations and . be an indentation relation. For the sake of compactness, we adopt \nthe notations Xi and X., respectively, for a pair of X and either an indentation i or an indentation \nrelation .. Thus we write A . .1 .2 XX\u00b7 \u00b7 \u00b7 X.n for a tuple (A, (X1, .1) (X2 , .2)\u00b7 \u00b7 \u00b7 (Xn, .n)) 1 2 \nn that is an element of d. As with II-CFGs, we de.ne a rewrite relation (.). ((N .S) \u00d7 N) * \u00d7 ((N .S) \n\u00d7 N) * j1 j2 .1 .2 where UAiV . UXX\u00b7 \u00b7 \u00b7 Xjn V iff A . XX\u00b7 \u00b7 \u00b7 X.n 1 2 n 1 2 n and j1 .1 i, j2 .2 i, \n\u00b7 \u00b7 \u00b7 , jn .n i. The (. * )relation, the language L(G), derivations, and parse trees are all de.ned as \nwith II-CFGs except that they are in terms of this new rewrite relation. Note that every II-CFG is encodable \nas an IS-CFG by translat\u00ading every production A . X1X2 \u00b7 \u00b7 \u00b7 Xn to A . X.X. \u00b7 \u00b7 \u00b7 X. 1 2 n i1 i2 im and \nevery word a1 a2 \u00b7 \u00b7 \u00b7 am to a a \u00b7 \u00b7 \u00b7 a with arbitrary 1 2 m i1 , i2 , \u00b7 \u00b7 \u00b7 , im . N. Conversely, erasing \nthe indentations and in\u00addentation relations in an IS-CFG results in an II-CFG. Note that translating \nfrom an II-CFG to an IS-CFG will not introduce ambi\u00adguities, but translating from an IS-CFG to an II-CFG \nmight. 5. Parsing Of course, a grammar is not practically useful if we cannot effec\u00adtively parse with \nit. In this section, we show how to modify tradi\u00adtional parsing techniques for II-CFGs to handle IS-CFGs. \nWe show this for both GLR and LR(k) parsing. This can also be done for CYK, SLR, LALR, GLL, and LL(k), \nbut we do not present those here as they are straightforward once the techniques for LR(k) pars\u00ading are \nunderstood.  In order to derive GLR and LR(k) parsing algorithms, we .rst prove a number of basic properties \nabout indentation relations and IS-CFGs (Section 5.1). Then, we model IS-CFGs by using in.nite II-CFGs \n(Section 5.2). Next, we consider traditional rewrite systems (Section 5.3) and parsing algorithms (Section \n5.4) applied to this in.nite II-CFG. Finally, we factor out the parts of these constructions representing \nindentations so that these algorithms can be expressed .nitely (Sections 5.5, 5.6 and 5.7) and discuss \nsome practical ef.ciency considerations (Section 5.8). The key insights here are, .rst, expressing the \nsemantics of an IS-CFG in terms of an in.nite II-CFG and, second, factoring the representation of item \nsets into a .nite representation. 5.1 Basic properties There are a few technical de.nitions and properties \nthat we will use in our parsing algorithms. We present these without discussion. . * De.nition 1. A non-terminal \nAis nullable if Ai efor all i. Lemma 2 (Composition of indentation relations). Every .nite se\u00adquence \nof compositions of elements from I is one of =, =, ., =., >n , or >n. for some n = 1. Proof. = is a left \nand right identity under composition. . is a left annihilator under composition. The compositions of \n> with = and =with > are both >. Lemma 3 (Closure of indentation relations). The closure of I under .nite \nsequences of composition and either .nite or in.nite sets of unions is the set of unions of one or more \nof =, =, ., =., >n, and >m. for some n, m = 1 where each of these is in the union only once. We call \nthis closure \u00afI and as a notational convention let \u00af. be an element of \u00afI. Proof. By Lemma 2, every .nite \ncomposition is of the form =, =, ' ' nm nn ' m m ., =., >, or >.. Since >.>if n = n and >..>. if m ' \n= m, at most one occurrence of >n and one occurrence of >m. needs to be in the union. Lemma 4 (Indentations \nof unique parses). If Ai . * W, then the set of all indentations i ' such that Ai ' . * W using the same \nsequence of productions is either the set N, a singleton or the upper bounded set {i |i = n}for some \nn . N. Furthermore, if it is a singleton or upper bounded set, then the maximum indentation is limited \nto be at most the maximum indentation in W. Proof. Consider the derivation as a parse tree. Each edge \ncan be annotated with the indentation relation between parent and child nodes. The relation between the \nindentation of the root Ai and each leaf aj is then a composition of the edges in the path from Ai to \naj . The possible values of i are the values compatible with every leaf indentation and the root s relation \nto them. By Lemma 2 every such relation is one of =, =, ., =., >p or >q . and for a particular leaf the \ncompatible root indentations are thus are either N, a singleton, or {i |i = m}for some m . N. The intersection \nof these over all the leaves in the parse tree is thus either the set N, a singleton, or {i |i = n}for \nsome n . N. Lemma 5 (Indentations of ambiguous parses). If Ai . * W, Ai ' . * then the set of all indentations \ni ' such that W using any sequence of productions is either the set N or a .nite subset of N. Moreover, \nthe .nite subsets of N are bounded by the maximum indentation in W. Proof. By Lemma 4 and the union over \nall parses. 5.2 Translating IS-CFGs to in.nite II-CFGs The .rst step of our approach is to model the \nIS-CFG by an in.nite II-CFG. Of course we do not actually compute with this in.nite grammar, but it provides \na mathematical model from which we derive a computable parsing algorithm. Given an IS-CFG G = (N, S, \nd, S), this II-CFG is G ' = (N ' , S ' , d ' , S ' )where: N ' = N \u00d7 N S ' = S\u00d7 N { ' i j1 j2 jn d = \nA. XX\u00b7 \u00b7 \u00b7 X 1 2 n .1 .2 .n |A . X1 X2 \u00b7 \u00b7 \u00b7 Xn . d, i, j1, j2, \u00b7 \u00b7 \u00b7 , jn . N, j1 .1 i, j2 .2 i, \u00b7 \u00b7 \n\u00b7 , jn .n i} S ' = S This grammar has an in.nite number of non-terminals, termi\u00adnals and productions \nper non-terminal, but we still limit derivations to .nite lengths. Note that traditional parsing algorithms \non this grammar may not terminate due to the in.nite size of G ' , so we formally model G ' as the limit \nof successive approximations where each approxi\u00admation bounds the maximum indentation in any non-terminal, \nter\u00adminal or production to successively greater values. We gloss over this detail in the remainder of \nthis paper. Lemma 6 (Equivalence). S . * W for G iff S ' . * W for G ' . Proof. By induction on the number \nof reductions and the fact that '' ' for all W and W , W . W in G iff W . W in G ' .  5.3 Rewrite system \nIn this subsection, we consider LR(k) parsing in terms of a rewrite system that concisely speci.es what \nit means for a parser to be LR(k). In later subsections, we derive more conventional stack\u00adbased parsing \nalgorithms. In this development, we closely follow the original presentation of LR(k) parsing by Knuth \n[1965] with only minor changes to use current notational conventions. Recall that an LR(k) parser is \none that always produces a right\u00admost derivation, and a rightmost derivation is one in which the rightmost \nnon-terminal is always expanded before any other non\u00adterminals. The symbols resulting from such an expansion \nstep are called the handle. For example, if * j l1 lp * S . U XV . UY 1 \u00b7 \u00b7 \u00b7 Yp V . W l1 lp is a rightmost \nderivation, then a handle of UY 1 \u00b7 \u00b7 \u00b7 Yp V is l1 lp Y1 \u00b7 \u00b7 \u00b7 Yp . Note that in order for this to be \na rightmost deriva\u00adtion, V necessarily contains only elements of S, though U and Y1 l1 \u00b7 \u00b7 \u00b7 Yp lp may \ncontain elements of both Sand N. Since an LR(k) parser works from the result of a rightmost derivation \nback to the start symbol, LR(k) parsing can be ac\u00adcomplished by iteratively searching for the handle \nof a string and performing the appropriate reduction. Again following Knuth [1965], given the in.nite \nII-CFG G ' = (N ' , S ' , d ' , S ' ) we G '' construct the right-linear (and thus regular) grammar = \n'' ' '' (N , N .S ' , d '' , S )for recognizing pre.xes that end in a han\u00addle. Here the non-terminals \nare {[] } '' i j1 j2 jk i ' j1 j2 jk ' N = A, aa\u00b7 \u00b7 \u00b7 a A. N , a, a, \u00b7 \u00b7 \u00b7 , a . S 1 2 k 1 2 k and represent \nthe part of the string that contains the handle. The j1 j2 jk a1 a2 \u00b7 \u00b7 \u00b7 ak track the k terminals expected \nafter the handle and  j1 m+1 are the lookahead. For each Ai . X\u00b7 \u00b7 \u00b7 Xjm Xj\u00b7 \u00b7 \u00b7 Xjn in 1 m m+1 n l1 \nl2 lk d ' and each u = a a \u00b7 \u00b7 \u00b7 a , we include the following in d '' : 1 2 k [] i j1 jn A, u. X1 \u00b7 \u00b7 \n\u00b7 Xn u [][] i j1 jm jm+1 A, u. X\u00b7 \u00b7 \u00b7 Xm X, v 1 m+1 () jm jn for each v . Hk Xm \u00b7 \u00b7 \u00b7 Xn u Here, u and \nv are the lookaheads expected by the parser. Hk (W) computes such lookaheads by computing the k-length \npre.xes of L(W)and is de.ned as {} l1 l2 lk l1 l2 lk Hk (W)= a a \u00b7 \u00b7 \u00b7 a W . * a a \u00b7 \u00b7 \u00b7 a U 1 2 k 1 \n2 k where the reduction relation . * is for G ' . The intuition here is that the .rst production expands \nto the handle along with a lookahead string and the second production expands to an intermediate non-terminal \nthat in turn eventually expands to the handle. Since this grammar is regular, it can be implemented by \na state machine [Brzozowski 1964], which leads to the following rewrite based algorithm for parsing. \nAlgorithm 7. Given input string W, if S = W then stop and accept the string. Otherwise, .nd all pre.xes \nof W that match the regular language L(G '' ). If there are no such matches, then reject the string. \nOtherwise, non-deterministically choose one of the matches, and let the last production of the match \nbe [] i j1 j2 jn 1 2 n A, u. XX\u00b7 \u00b7 \u00b7 Xu j1 j2 Replace this occurrence of XX\u00b7 \u00b7 \u00b7 Xjn in W with Ai as \nit is 1 2 n a handle of W, and repeat this algorithm with the new value of W. Note that this algorithm \nis non-deterministic and accepts the word if any path through the algorithm accepts the word. We allow \nthis because productions in G (e.g., A . B. and A . C>) may produce multiple productions when translated \nto G ' (e.g., all Ai . Bj and Ai . Ck such that k > i). These may introduce ambiguities in G ' even when \nthere are no ambiguities in G. Once we convert to a .nite version of the parsing algorithm, we will eliminate \nthis non-determinism.  5.4 Parsing with stacks Of course, rewriting the entire string and restarting \nthe automaton from the start as done in Algorithm 7 is inef.cient. Instead, we can save a trace of the \nstates visited. When a handle is reduced, we rewind to the state just before the .rst symbol of the handle \nand proceed from there. This is the essential idea behind the traditional LR(k) parser development by \nKnuth [1965]. We apply this idea to our in.nite II-CFG to obtain the following construction. We begin \nwith the notion of an item. We denote an item by [] i j1 jm jm+1 jn A. X\u00b7 \u00b7 \u00b7 X X\u00b7 \u00b7 \u00b7 X;u 1 m m+1 n \nwhere Ai . X1 j1 \u00b7 \u00b7 \u00b7 Xn jn is a production in G ' and u . (S ' )k is the lookahead. The algorithm maintains \na stack of sets of items S0S1 \u00b7 \u00b7 \u00b7 Sn where Sn is the top element of the stack. We use the notation \nS0 S1 \u00b7 \u00b7 \u00b7 Sn |a1a2 \u00b7 \u00b7 \u00b7 akw to denote that S0S1 \u00b7 \u00b7 \u00b7 Sn is the current stack and a1a2 \u00b7 \u00b7 \u00b7 akw is \nthe input remaining to be consumed by the parser. To parse a word w, we start with the con.guration i0 \ni0 i0 S0 |w ..\u00b7 \u00b7 \u00b7 . 1 2 k {[]} where S0 = S . S ' ;.0 1.2 0 \u00b7 \u00b7 \u00b7 .k 0 . We let S be a fresh non\u00adterminal \nand .1, .2, \u00b7 \u00b7 \u00b7 , .k be fresh terminals that pad the string to have at least k tokens of lookahead. \nWe then run the following parsing algorithm. i1 i2 ik Algorithm 8. Given con.guration S0S1 \u00b7 \u00b7 \u00b7 Sn |a \n, a \u00b7 \u00b7 \u00b7 a w, []1 2 k i1 i2 ik =.i0 .i0 .i0 if S . S ;u. Sn and a , a \u00b7 \u00b7 \u00b7 a w \u00b7 \u00b7 \u00b7 , then 1 2 k 1 \n2 k accept. Otherwise: 1. Compute the closure, S ' , of Sn where S ' is the least set of items satisfying \nthe recurrence {[] ' jm+1 l1 lp S = Sn. X. Y \u00b7 \u00b7 \u00b7 Yp ;v m+1 1 [] i j1 jm jm+1 jn A. X\u00b7 \u00b7 \u00b7 Xm X\u00b7 \u00b7 \n\u00b7 Xn ;u. Sn, 1 m+1 jm+1 l1 lp ' X. Y \u00b7 \u00b7 \u00b7 Yp . d m+1 1 ()} jm+1 jn v . Hk Xm+1 \u00b7 \u00b7 \u00b7 Xn u 2. Compute \nthe acceptable lookahead set Kwhere {[] i j1 jm jm+1 jn ' K = v A. X\u00b7 \u00b7 \u00b7 X X\u00b7 \u00b7 \u00b7 X;u. S , 1 m m+1 n \n()} jm+1 jn v . Hk X\u00b7 \u00b7 \u00b7 Xu m+1 n 3. For each production Ai . X1 j1 \u00b7 \u00b7 \u00b7 Xn jn in d ' , compute the \n(j1 ) acceptable lookahead set K Ai . X1 \u00b7 \u00b7 \u00b7 Xn jn where () i j1 jn K A. X1 \u00b7 \u00b7 \u00b7 Xn = {[]} i j1 jn \n' u A. X1 \u00b7 \u00b7 \u00b7 Xn ;u. S () 4. Let GOT O S, Zl = {[] i j1 jm jm+1 jm+2 jn A. X\u00b7 \u00b7 \u00b7 Xm X X\u00b7 \u00b7 \u00b7 Xn ;v \n1 m+1 m+2 [] i j1 jm jm+1 jm+2 jn A. X\u00b7 \u00b7 \u00b7 X XX\u00b7 \u00b7 \u00b7 X;u. S, 1 m m+1 m+2 n } jl Xm+1 = Z m+1 and non-deterministically \nchoose one of the following. i1 i2 ik (a) If a a \u00b7 \u00b7 \u00b7 a . K, then do a shift action by looping back \n1 2 k to the start of the algorithm with the new con.guration () i1 i2 ik S0S1 \u00b7 \u00b7 \u00b7 SnGOT O Sn, a 1 \n|a2 \u00b7 \u00b7 \u00b7 ak w () i1 i2 ik Ai j1 (b) If a a \u00b7 \u00b7 \u00b7 a . K . X\u00b7 \u00b7 \u00b7 Xjn for some pro\u00ad 1 2 k 1 n duction \nAi . X1 j1 \u00b7 \u00b7 \u00b7 Xn jn , then do a reduce action by looping back to the start of the algorithm with the \nnew con\u00ad.guration () i i1 i2 ik S0S1 \u00b7 \u00b7 \u00b7 Sn-mGOT O , A|a a \u00b7 \u00b7 \u00b7 a w Sn-m 1 2 k  5.5 Finite representations \nof stacks Algorithm 8 contains both non-determinism and in.nite sets. Here we depart from Knuth [1965] \nin order to eliminate these. Up to this point, our parser is simply a standard LR(k)parser, albeit on \nan in.nite II-CFG, and we rely on the correctness of the standard LR(k) parsing algorithm for our correctness. \nFrom here forward, we ensure correctness by ensuring that our modi.ed version of the algorithm models \nthe same item sets as Algorithm 8, albeit using a more ef.cient representation. As a .rst step, consider \nthe sets of items that form the stack. Each item is of the form [] i j1 jm jm+1 jn A. X\u00b7 \u00b7 \u00b7 Xm X\u00b7 \u00b7 \n\u00b7 Xn ;u 1 m+1 where Ai . N \u00d7 N, X1 j1 , \u00b7 \u00b7 \u00b7 , Xn jn . (N .S) \u00d7 N, and u . (S \u00d7 N)k. Observe that the \nindentations to the left of the bullet,  j1, \u00b7 \u00b7 \u00b7 , jm, do not effect the parsing process, and multiple \nitems that differ only in the values of j1, \u00b7 \u00b7 \u00b7 , jm can therefore be repre\u00adsented by a single item. \nThis reduces the state space slightly, but we can go further and also factor out jm+1 ,\u00b7 \u00b7 \u00b7 , jn by \nobserving that the algorithm preserves the following completeness property for item sets. De.nition 9 \n(Item-set completeness). We say that an item set, S, is complete if for every A . X1 .1 \u00b7 \u00b7 \u00b7 Xn .n . \nd, [] i j1 jm jm+1 jn A. X\u00b7 \u00b7 \u00b7 X X\u00b7 \u00b7 \u00b7 X;u. S 1 m m+1 n implies that {[j ' j ' ]i j1 jm m+1 n S . A. \nX\u00b7 \u00b7 \u00b7 X X\u00b7 \u00b7 \u00b7 X;u 1 m m+1 n } ''' ' jm+1 , \u00b7 \u00b7 \u00b7 , j n . N, j m+1 .m+1 i, \u00b7 \u00b7 \u00b7 , j n .n i This property \nis preserved by each loop through Algorithm 8, as stated in the following lemma. Lemma 10. If every item \nset on the stack is complete at the start of a loop through Algorithm 8, then every item set on the new \nstack at the start of the next loop is also complete. .1 .p Proof. For every production Xm+1 . Y \u00b7 \u00b7 \n\u00b7 Yp , if the clo\u00ad [1 ] jl1 lp sure step adds the item Xm+1 . Y \u00b7 \u00b7 \u00b7 Yp ;v, then by con\u00ad m+1 1 [] l \n' l ' m+1 p j1 struction it adds all items of the form X. Y \u00b7 \u00b7 \u00b7 Yp ;v m+1 1 where l1 ' .1 jm+1 , \u00b7 \n\u00b7 \u00b7 , l p ' .1 jm+1 . Thus this step of the algorithm preserves completeness. The GOT O operation .lters \nthe set of items by requiring that jm+1 Zl jm+1 X= , but afterwards Xis moved to the left of the m+1 \nm+1 bullet and is therefore no longer relevant to the completeness of the item set. Since jm+1 is related \nto jm+2 , \u00b7 \u00b7 \u00b7 , jn only indirectly through i, the remaining jm+2 , \u00b7 \u00b7 \u00b7 , jn are complete given the \ni that remain in the item set. Since the item sets in the stack are all complete, we no longer need to \nrepresent the individual j1, \u00b7 \u00b7 \u00b7 , jn. The only indentations we need to record are i, the indentation \nof the non-terminal on the left-hand side of the production, and the indentations in u, the expected \nlookahead. Thus, we can represent items that differ only in the values of j1, \u00b7 \u00b7 \u00b7 , jn with [] .1 .m \n.m+1 .n A . X1 \u00b7 \u00b7 \u00b7 Xm X m+1 \u00b7 \u00b7 \u00b7 Xn ;L where A . X1 .1 , \u00b7 \u00b7 \u00b7 , Xn .n . d and L . N \u00d7 (S \u00d7 N)k. We \ncall L the indentation-and-lookahead set. Each element of L is a pair of i, the indentation for A, and \nu, the lookahead word. With this factoring we can implement a GLR parser [Tomita 1985] by letting k = \n0and translating Algorithm 8 to use this rep\u00adresentation of item sets. Since k = 0, the lookahead is \nalways the empty string and L is simply a set of indentations. By examining the algorithm we can further \ndetermine that this set will always be either the set N, a .nite subset of N, or the union of a .nite \nsubset of N with the set of all elements of N greater than jfor some j . N. These sets are all .nitely \nrepresentable and thus computable. Fi\u00adnally, the standard technique of elaborating the possible item \nsets and the transitions between them can be used to construct a state machine for a push-down automaton \nthat recognizes G. During this elaboration, we keep the set of indentations, L, abstract. This en\u00adsures \nthat there are only .nitely many item sets to be elaborated. At runtime, both the current state and the \nstack elements then simply store a reference to one of these pre-elaborated item sets along with a concrete \nindentation set. Any non-determinism remaining in the algorithm is handled using the standard stack representation \nused by GLR parsers.  5.6 Parsing LR(k) grammars After having factored the representation of the item \nsets this far, we eliminate the non-determinism in the algorithm by restricting the algorithm to only \nLR(k) grammars. Recall that an LR(k) grammar is de.ned to have a unique rightmost derivation for any \ngiven word. Thus there is always a unique handle at each parsing step, and thus in the non-deterministic \nchoice at the end of Algorithm 8 there is never more than one allowed choice. Otherwise, we have a shift/reduce \nor a reduce/reduce con.ict, and the grammar is not an LR(k) grammar. With the original representation \nof item sets, we did not apply this criterion because there might be multiple reductions that differ \nonly in their indentations. For example, if we have A . in G, B. then both A1 . B1 and A2 . B1 are in \nG ' . These could lead to spurious reduce/reduce con.icts. However, now that items are identi.ed in terms \nof productions from d (e.g., A . B. )instead of productions from d ' (e.g., A1 . B1), these con.icts \nno longer occur. Any remaining shift/reduce or reduce/reduce con.icts re.ect a con.ict in the original \ngrammar, G, and are not artifacts of the translation to G ' . The last remaining source of ambiguity \nis the indentation-and\u00adlookahead set, L, which can grow in.nitely. To resolve this we change the representation \nof Lfrom being a subset of N\u00d7(S \u00d7 N)k ()k \u00af\u00af\u00af to being a subset of N along with an element of I \u00d7 I \u00d7 \nS\u00d7 I. We write such an item as [ .1 .m .m+1 .n A . X1 \u00b7 \u00b7 \u00b7 Xm X m+1 \u00b7 \u00b7 \u00b7 Xn ; I; (.\u00af1,1, \u00af.1,2, a1 \n)\u00b7 \u00b7 \u00b7 (.\u00afk,1,.\u00afk,2, ak).\u00afk+1 ] This represents the item [] .1 .m .m+1 .n A . X1 \u00b7 \u00b7 \u00b7 Xm X m+1 \u00b7 \u00b7 \u00b7 \nXn ;L Bi0 where S = and {() j1 jk L = i, a1 \u00b7 \u00b7 \u00b7 ak |i . I, i .\u00af1,1 l1, l1 .\u00af2,1 l2, \u00b7 \u00b7 \u00b7 , lk-1 .\u00afk,1 \nlk , j1 \u00af.1,2 l1 , j2 \u00af.2,2 l2 , \u00b7 \u00b7 \u00b7 , jk \u00af.k,2 lk , lk \u00af.k+1 i0} The intuition behind this representation \nis best understood in terms of the sort of parse tree that could lead to the item that is trying to parse \nan A with lookaheads of a1 \u00b7 \u00b7 \u00b7 ak. This situation is depicted in Figure 6. Note that there are no terminals \nbetween any of A, or a1, a2 , \u00b7 \u00b7 \u00b7 , ak, and in the general case, we consider l1 l2 lk a node to be \nan ancestor of itself so some of C, C, \u00b7 \u00b7 \u00b7 , Cor 1 2 k Bi0 may actually be the same node. The lookahead \ntoken a1 j1 is in the lookahead only because A shares with it the ancestor C1 at indentation l1. The \n.\u00af1,1 and .\u00af1,2 relations record the possible indentation relations between C1 and, j1 j2 respectively, \nA and a1 . The second lookahead token a2 is in the lookahead only because C1, the common ancestor of \nthe item and the .rst lookahead token, also shares with a2 the common ancestor C2 at indentation l2. \nThe \u00af.2,1 and \u00af.2,2 relations record the possible indentation relations between C2 and, respectively, \nC1 and a2. And so on, until we reach the .nal ancestor, Ck , at indentation lk. This ancestor has a minimum \nindentation at which it can occur so we use \u00af.k+1 to record the indentation relation between Ck and B, \nthe start terminal. The lookahead computation, Hk, that is de.ned in Section 5.3 must of course be modi.ed \nto account for this representation. We omit this because, while conceptually simple, its formal de.nition \nis fairly intricate.  Bi0 . . . Clk k . . . . . . Cl2 2 ajk k . . . . . . Cl1 1 aj2 2 . . . . . . Ai \naj1 1 Figure 6. The structure of lookahead tokens in a parse tree. With this representation, the lookahead \nset is .nitely repre\u00adsented so the only potentially in.nite part remaining is the indenta\u00adtion set. However, \nas before, we can show that these sets are always either the set N, a .nite subset of N, or the union \nof a .nite subset of N with the set of all elements of N greater than jfor some j . N. Indentation sets \nare thus .nitely representable, and we have com\u00adpletely reduced the parsing algorithm to a .nite representation. \nWe can now construct an LR(k)parser just as we constructed the GLR parser at the end of Section 5.5. \nAs before, to do this we translate Algorithm 8 to use the new item set representation, keep the indentation \nsets abstract while we elaborate the possible item sets to form the states of the push-down automaton, \nand at runtime augment the automaton states and stack entries with concrete indentation sets.  5.7 Correctness \nof item set representation We must consider carefully the correctness of the parsing algorithm based \non the item-set representation given in Section 5.6. At .rst glance, the representation looks like it \ncould lead to lookaheads matching when they should not. Indeed, the following grammar of variable references \n(i.e., ID)and do blocks with vertically aligned statements shows how this can arise: expr . ID> expr \n. do > |stmts|> stmts . |expr| = = stmts . stmts |expr| = For example, consider the parse of the word \ndo 1 do 4 ID7 ID2 which may come from the code: do do x y Note that y does not align with either the \nsecond do or the x, and thus this code should be rejected. Put another way, the valid lookaheads when \nlooking ahead at the ID for y but before re\u00adducing x to expr are ID4 , ID7 , do 4, and do 7 . The string \nshould thus be rejected since ID2 , the token for y, is not in that set. But the LR(1) lookahead set \nusing the new representation is {((=, =, ID)>), ((=, =, do )>)}. Since the current indenta\u00adtion is 7 \nand there exist l such that 7 = l, 2 = l, and l > 0, the string will not be immediately rejected. This \nrepresentation thus appears to over approximate the set of indentations for a particular lookahead. This \nmeans that in the non\u00addeterministic choice at the end of Algorithm 8 there could be more reductions possible \nthan there should be. However, as we restrict ourselves to LR(k)grammars, this turns out to not be a \nproblem. This is because there are only four cases where these spurious reductions occur: Case 1. There \nshould be no reductions or shifts possible, but the approximation makes one extra reduction possible. \nIn this case, the parser should reject the program, but will in\u00adstead reduce and continue parsing. However, \nthis case happens only when some other item set further up the stack also checks the lookahead tokens. \nThough the string is not rejected imme\u00addiately, it will be rejected once we reach that point in the stack. \nIn our example, once x reduces to expr and then do x reduces to an expr that .nally reduces to stmts, \nthe lookahead set will be {((=, =,ID)>), ((=, =, do )>)}. Since the inden\u00adtation at that point is 4 but \nthe next token is ID2, the parser will reject the program. The program might not be rejected as soon \nas we expect, but it is eventually rejected. This case may arise even when the grammar is LR(k). Case \n2. There should be no shifts or reductions possible, but the approximation makes two or more extra reductions \npossible. The representation for lookaheads in Section 5.6 is designed so that every lookahead word that \nit represents comes from a valid parse. Thus if this representation generates multiple possible reductions, \nthen there is some string that would also generate those multiple possible reductions with the original \nrepresentation in Algorithm 8. In that case, the grammar is not LR(k), and the grammar should be rejected \nby the parser generator. Case 3. There should be one reduction or shift possible, but the approximation \nmakes one or more extra reductions possible. The same reasoning applies as in the preceding case. Case \n4. There should be two or more reductions or shifts possible. Then the original grammar is not LR(k), \nand the grammar should be rejected by the parser generator. This means that the representation in Section \n5.6 is valid for any grammar that is LR(k), and furthermore we can detect when a grammar is not LR(k) \nby using this representation.  5.8 An ef.ciency consideration Note that each item in an item set may \nhave a different set of indentations. For example, we may have an item set containing both of the following \ntwo items: [> ] b = A . a ;I;u = b = [A . a ;I;u] Even if they start with the same indentation set, I, \nafter reading an a, the indentation sets for these items will be different from each other. For the .rst \nitem, the indentation set will be restricted to in\u00addentations strictly greater than the indentation for \na. For the second item, the indentations will be restricted to indentations equal to that of a. Thus, \ndifferent items can have different indentation sets, and a naive factoring (e.g., sharing I between items \nin an item set) is insuf.cient. This does not preclude the possibility of a clever refac\u00adtoring like \nthe one done with the lookahead sets, but we have been unable to .nd such a factoring that works in all \ncases. Nevertheless, as a practical matter the following techniques seem to work well. Observe that we \nneed to keep only the indentation sets for items before the closure is taken. Items generated by the \nclosure operation can be annotated with the \u00af.that can lead to them and this value can be incorporated \ninto the lookahead check. In addition, when we can determine that some set of items will always have \nthe same indentation set, we can represent them using a common  IS-CFG parsing time relativeto the unmodi.ed \nparser 5 4 3 2 1 0 Number of nodes in the AST Figure 7. Benchmark results. indentation set. These techniques \nreduce the number of indentation the unmodi.ed haskell-src parser. There is a slight upward trend in \nthe factor by which the IS-CFG based parser is slower than the unmodi.ed parser. This is primarily due \nto the fact that we are graphing the ratio between the performance of the modi.ed and unmodi.ed parsers \nand the unmodi.ed parser has low-order perfor\u00admance overheads that are more signi.cant on small inputs. \nGiven that this is a prototype implementation with little optimization, the fact that the IS-CFG version \nis only one to three times slower than the standard haskell-src parser is promising. This overhead is \nlikely due to the manipulation of the indentation sets as the repre\u00adsentation of indentation sets is \nnaive. Since in practice only certain sorts of sets are common (e.g., singletons and the set N), an im\u00adproved \nversion could optimize for these sorts of sets. In addition, we could take advantage of the tokens with \nan indentation of in.n\u00adity by adding a fast path through the parser that short circuits the indentation \ncomputations. 7. Related Work The uulib parser library [Swierstra 2011] and the indents [An\u00adklesaria \n2012] and indentparser [Kurur 2012] extensions to the sets that must be passed from one state to another, \nand based on the experience implementing the Haskell parser in Section 6, the resulting number of such \nsets is usually one. 6. Implementation In order to verify the real-world practicality of this parsing \ntech\u00adnique, we modi.ed the Happy parser generator [Marlow and Gill 2009] to support the parsing techniques \npresented in this paper. The parser is LALR instead of LR(k), but the techniques shown in Section 5 generalize \nstraightforwardly to LALR. Note that this implementation is only a prototype and is only intended for \ntesting the practical feasibility of parsing with IS-CFGs. In particular, no signi.cant effort was put \ninto optimizing the performance of the generated parser. The Haskell parser from the haskell-src package \n[Marlow et al. 2011] uses techniques for implementing layout similar to those used by GHC. However, it \nis packaged as a standalone parser, and this makes it easy to isolate for benchmarking purposes. This \nparser was modi.ed to use an IS-CFG instead of using shared state between the lexer and parser. Both \nthe modi.ed and un\u00admodi.ed versions were then run on the preprocessed source .les from the base package \n[bas 2012]. Two modules (GHC.Float and GHC.Constants) could not be preprocessed due to missing header \n.les. Of the 178 remaining Haskell modules, 85 cannot be parsed by the unmodi.ed haskell-src parser due \nto syn\u00adtactic extensions, such as rank-2 types, that are not supported by haskell-src. This left 93 source \n.les that are parsable by the un\u00admodi.ed parser. All of these .les parsed and produced the same parse \ntree when parsed using the IS-CFG based parser. Figure 7 shows the parsing times of the modi.ed parser \nrel\u00adative to the unmodi.ed parser when run on these .les. These benchmarks were compiled using GHC version \n7.0.3 with the -O2, -optl-static and -optl-pthread .ags and were run on a 64\u00adbit, 3.2GHz Xeon running \nUbuntu 12.04 with 4GB of RAM. Tim\u00ading measurements were collected using Criterion version 0.6.0.1 with \n1000 samples per benchmark. Note that the haskell-src parser is designed to run both the lexer and the \nparser simultaneously so they can share information about indentations. It is dif.cult to separate the \nexecution of the parser from the execution of the lexer, so the lexing time is included in the times \nfor both that parser and the IS-CFG based one. As expected, the IS-CFG based parser runs in approximately \nlinear time. It is geometrically on average 1.73 times slower than Parsec [Leijen and Martini 2012] parser \nlibrary provide support for indentation-sensitive parsing. To the best of our knowledge there is no published, \nformal theory for the sort of indentation that these parsers implement. They are all combinator-based, \ntop-down parsers and use some variation of threading state through a parser monad to track the current \nindentation. Hutton [1992] describes an approach to parsing indentation\u00adsensitive languages that is based \non .ltering the token stream. This idea is further developed by Hutton and Meijer [1996]. In both cases, \nthe layout combinator searches the token stream for appro\u00adpriately indented tokens and passes only those \ntokens to the com\u00adbinator for the expression to which the layout rule applies. As each use of layout \nscans the remaining tokens in the input, this can lead to quadratic running time. Given that the layout \ncombinator .lters tokens before parsing occurs, this technique also cannot support subexpressions, such \nas parenthesized expressions in Python, that are exempt from layout constraints. Thus, this approach \nis inca\u00adpable of expressing many real-world languages including ISWIM, Habit, Haskell, and Python. Erdweg \net al. [2012] propose a method of parsing indentation\u00adsensitive languages by effectively .ltering the \nparse trees generated by a GLR parser. The GLR parser generates all possible parse trees irrespective \nof layout. Indentation constraints on each parse node then remove the trees that violate the layout rules. \nFor performance reasons, this .ltering is interleaved with the execution of the GLR parser when possible. \nAside from the fact that they require a GLR parser and thus generate parse trees that might not be used, \na crit\u00adical difference between their system and the one presented in this paper is that their indentation \nconstraints are in terms of the set of tokens under a non-terminal whereas the system in this paper uses \nconstraints between non-terminals and their immediate chil\u00addren. Thus, the two approaches look at the \nproblem from different perspectives. Erdweg et al. [2012] do not consider the question of an LR(k) parser. \nBrunauer and M\u00fchlbacher [2006] take a unique approach to specifying the indentation-sensitive aspects \nof a language. They use a scannerless grammar that uses individual characters as tokens and has non-terminals \nthat take an integer counter as parameter. This integer is threaded through the grammar and eventually \nspeci.es the number of spaces that must occur within certain productions. The grammar encodes the indentation \nrules of the language by carefully arranging how this parameter is threaded through the grammar and thus \nhow many whitespace characters should occur at each point in the grammar.  While encoding indentation \nsensitivity this way is formally pre\u00adcise, it comes at a cost. The YAML speci.cation [Ben-Kiki et al. \n2009] uses the approach proposed by Brunauer and M\u00fchlbacher [2006] and as a result has about a dozen \nand a half different non\u00adterminals for various sorts of whitespace and comments. With this encoding, \nthe grammar cannot use a separate tokenizer and must be scannerless, each possible occurrence of whitespace \nmust be ex\u00adplicit in the grammar, and the grammar must carefully track which non-terminals produce or \nexpect what sorts of whitespace. The au\u00adthors of the YAML grammar establish naming conventions for non\u00adterminals \nthat help manage this, but the result is still a grammar that is dif.cult to comprehend and even more \ndif.cult to modify. While this approach bears some similarity to the technique pro\u00adposed in this paper, \na key difference is that their method uses the parameters of non-terminals to generate explicit whitespace \ncharac\u00adters and thus incurs a signi.cant accounting overhead in the design of the grammar. On the other \nhand, the system presented in this pa\u00adper operates at a higher level, using the parameter to indicate \nthe column or indentation at which non-terminals and terminals should occur. This is a subtle distinction, \nbut it has a profound impact. As shown in Section 3, layout rules are comparatively simple to encode \nthis way, and as shown in Section 5, this formalism is amenable to traditional parsing techniques such \nas LR(k) parsing. Note that none of the systems reviewed above present an LR(k) parsing algorithm. They \nuse either top-down parsers or, in the case of Erdweg et al. [2012], a GLR parser. 8. Conclusion This \npaper presents a grammatical formalism for indentation\u00adsensitive languages. It is both expressive and \neasy to use. We derive provably correct GLR and LR(k) parsers for this formal\u00adism. Though not shown here, \nCYK, SLR, LALR, GLL and LL(k) parsers can also be constructed by appropriately using the key tech\u00adnique \nof factoring item sets. Experiments on a Haskell parser using this formalism show that the parser runs \nbetween one and three times slower than a parser using traditional ad hoc techniques for handling indentation \nsensitivity. Improvements in the handling of indentation sets may reduce this overhead. Using these techniques, \nthe layout rules of a wide variety of languages can be expressed easily and parsed effectively. Acknowledgments \nFeedback from Andrew Tolmach, R. Kent Dybvig, Tim Sheard, Mark P. Jones, Nathan Collins, Steffen L\u00f6sch \nand the anonymous referees helped improve the presentation of this paper. References base version 4.5.1.0, \nJune 2012. URL http://hackage.haskell.org/ package/base/. Sam Anklesaria. indents version 0.3.3, May \n2012. URL http:// hackage.haskell.org/package/indents/. Oren Ben-Kiki, Clark Evans, and Ingy d\u00f6t Net. \nYAML Ain t Markup Language (YAML) Version 1.2, 3rd edition, October 2009. URL http: //www.yaml.org/spec/1.2/spec.html. \nLeonhard Brunauer and Bernhard M\u00fchlbacher. Indentation sen\u00adsitive languages. Unpublished manuscript, \nJuly 2006. URL http://www.cs.uni-salzburg.at/~ck/wiki/uploads/ TCS-Summer-2006.IndentationSensitiveLanguages/. \nJanusz A. Brzozowski. Derivatives of regular expressions. Journal of the ACM (JACM), 11(4):481 494, October \n1964. ISSN 0004-5411. doi: 10.1145/321239.321249. Sebastian Erdweg, Tillmann Rendel, Christian K\u00e4stner, \nand Klaus Oster\u00ad mann. Layout-sensitive generalized parsing. In Software Language En\u00ad gineering, Lecture \nNotes in Computer Science. Springer Berlin / Heidel\u00ad berg, 2012. URL http://sugarj.org/layout-parsing.pdf. \nTo appear. The Glorious Glasgow Haskell Compilation System User s Guide, Version 7.2.1. The GHC Team, \nAugust 2011. URL http://www.haskell. org/ghc/docs/7.2.1/html/users_guide/. David Goodger. reStructuredText \nMarkup Speci.cation, January 2012. URL http://docutils.sourceforge.net/docs/ref/rst/ restructuredtext.html. \nRevision 7302. John Gruber. Markdown: Syntax. URL http://daringfireball.net/ projects/markdown/syntax. \nRetrieved on June 24, 2012. Michael Hanus (ed.). Curry: An integrated functional logic language (version \n0.8.2). Technical report, March 2006. URL http://www. informatik.uni-kiel.de/~curry/report.html. HASP \nProject. The Habit programming language: The revised prelim\u00adinary report, November 2010. URL http://hasp.cs.pdx.edu/ \nhabit-report-Nov2010.pdf. Graham Hutton. Higher-order functions for parsing. Journal of Functional Programming, \n2(03):323 343, July 1992. doi: 10.1017/ S0956796800000411. Graham Hutton and Erik Meijer. Monadic parser \ncombinators. Technical Report NOTTCS-TR-96-4, Department of Computer Science, Univer\u00adsity of Nottingham, \n1996. INMOS Limited. occam programming manual. Prentice-Hall international series in computer science. \nPrentice-Hall International, 1984. ISBN 978\u00ad0-13-629296-8. Mark P. Jones. The implementation of the Gofer \nfunctional programming system. Research Report YALEU/DCS/RR-1030, Yale University, New Haven, Connecticut, \nUSA, May 1994. Donald E. Knuth. On the translation of languages from left to right. Information and Control, \n8(6):607 639, December 1965. ISSN 0019\u00ad9958. doi: 10.1016/S0019-9958(65)90426-2. Piyush P. Kurur. indentparser \nversion 0.1, January 2012. URL http: //hackage.haskell.org/package/indentparser/. P. J. Landin. The next \n700 programming languages. Communications of the ACM, 9(3):157 166, March 1966. ISSN 0001-0782. doi: \n10.1145/ 365230.365257. Daan Leijen and Paolo Martini. parsec version 3.1.3, June 2012. URL http://hackage.haskell.org/package/parsec/. \nSimon Marlow and Andy Gill. Happy User Guide, 2009. URL http: //www.haskell.org/happy/doc/html/. For \nHappy version 1.18. Simon Marlow, Sven Panne, and Noel Winstanley. haskell-src version 1.0.1.5, November \n2011. URL http://hackage.haskell.org/ package/haskell-src. Simon Marlow (ed.). Haskell 2010 Language \nReport, April 2010. URL http://www.haskell.org/onlinereport/haskell2010/. Egil M\u00f6ller. SRFI-49: Indentation-sensitive \nsyntax, May 2005. URL http://srfi.schemers.org/srfi-49/srfi-49.html. Python. The Python Language Reference. \nURL http://docs.python. org/reference/. Retrieved on June 26, 2012. S. Doaitse Swierstra. uulib version \n0.9.14, August 2011. URL http: //hackage.haskell.org/package/uulib/. Don Syme et al. The F# 2.0 Language \nSpeci.cation. Microsoft Corporation, April 2010. URL https://research.microsoft.com/en-us/um/ cambridge/projects/fsharp/manual/spec.html. \nUpdated April 2012. Masaru Tomita. Ef.cient Parsing for Natural Language: A Fast Algorithm for Practical \nSystems. Kluwer International Series in Engineering and Computer Science. Kluwer Academic Publishers, \n1985. ISBN 978-0\u00ad89838-202-0. D. A. Turner. Miranda System Manual. Research Software Lim\u00adited, 1989. \nURL http://www.cs.kent.ac.uk/people/staff/ dat/miranda/manual/. Philip Wadler. An introduction to Orwell. \nTechnical report, Programming Research Group at Oxford University, 1985.    \n\t\t\t", "proc_id": "2429069", "abstract": "<p>Several popular languages, such as Haskell, Python, and F#, use the indentation and layout of code as part of their syntax. Because context-free grammars cannot express the rules of indentation, parsers for these languages currently use ad hoc techniques to handle layout. These techniques tend to be low-level and operational in nature and forgo the advantages of more declarative specifications like context-free grammars. For example, they are often coded by hand instead of being generated by a parser generator.</p> <p>This paper presents a simple extension to context-free grammars that can express these layout rules, and derives GLR and LR(k) algorithms for parsing these grammars. These grammars are easy to write and can be parsed efficiently. Examples for several languages are presented, as are benchmarks showing the practical efficiency of these algorithms.</p>", "authors": [{"name": "Michael D. Adams", "author_profile_id": "81552909856", "affiliation": "Portland State University, Portland, OR, USA", "person_id": "P3978049", "email_address": "michaeladams@pdx.edu", "orcid_id": ""}], "doi_number": "10.1145/2429069.2429129", "year": "2013", "article_id": "2429129", "conference": "POPL", "title": "Principled parsing for indentation-sensitive languages: revisiting landin's offside rule", "url": "http://dl.acm.org/citation.cfm?id=2429129"}