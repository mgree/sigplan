{"article_publication_date": "01-23-2013", "fulltext": "\n Quantitative Relaxation of Concurrent Data Structures Thomas A. Henzinger\u00b0 Christoph M. Kirsch` Hannes \nPayer` Ali Sezgin\u00b0 Ana Sokolova` \u00b0IST Austria ` University of Salzburg {tah,asezgin}@ist.ac.at .rstname.lastname@cs.uni-salzburg.at \nAbstract There is a trade-off between performance and correctness in im\u00adplementing concurrent data structures. \nBetter performance may be achieved at the expense of relaxing correctness, by rede.ning the semantics \nof data structures. We address such a rede.nition of data structure semantics and present a systematic \nand formal frame\u00adwork for obtaining new data structures by quantitatively relaxing existing ones. We \nview a data structure as a sequential speci.ca\u00adtion containing all legal sequences over an alphabet of \nmethod calls. Relaxing the data structure corresponds to de.ning a distance from any sequence over the \nalphabet to the sequential speci.ca\u00adtion: the k-relaxed sequential speci.cation contains all sequences \nover the alphabet within distance k from the original speci.ca\u00adtion. In contrast to other existing work, \nour relaxations are seman\u00adtic (distance in terms of data structure states). As an instantiation of our \nframework, we present two simple yet generic relaxation schemes, called out-of-order and stuttering relaxation, \nalong with several ways of computing distances. We show that the out-of-order relaxation, when further \ninstantiated to stacks, queues, and prior\u00adity queues, amounts to tolerating bounded out-of-order behavior, \nwhich cannot be captured by a purely syntactic relaxation (distance in terms of sequence manipulation, \ne.g. edit distance). We give con\u00adcurrent implementations of relaxed data structures and demonstrate that \nbounded relaxations provide the means for trading correctness for performance in a controlled way. The \nrelaxations are mono\u00adtonic, which further highlights the trade-off: increasing k increases the number \nof permitted sequences, which as we demonstrate can lead to better performance. Finally, since a relaxed \nstack or queue also implements a pool, we obtain new concurrent pool implemen\u00adtations that outperform \nthe state-of-the-art ones. Categories and Subject Descriptors D.3.1 [Programming lan\u00adguages]: Formal \nde.nitions and theory semantics; E.1 [Data Structures]: Lists, stacks, and queues; D.1.3 [Programming \nlan\u00adguages]: Programming techniques concurrent programing General Terms Theory, Algorithms, Design, Performance \nKeywords (concurrent) data structures, relaxed semantics, quan\u00adtitative models, costs Permission to make \ndigital or hard copies of all or part of this work for personal or classroom use is granted without fee \nprovided that copies are not made or distributed for pro.t or commercial advantage and that copies bear \nthis notice and the full citation on the .rst page. To copy otherwise, to republish, to post on servers \nor to redistribute to lists, requires prior speci.c permission and/or a fee. POPL 13, January 23 25, \n2013, Rome, Italy. Copyright c . 2013 ACM 978-1-4503-1832-7/13/01. . . $15.00 1. Introduction Concurrent \ndata structures may be a performance and scalabil\u00adity bottleneck and thus prevent effective use of increasingly \npar\u00adallel hardware [18]. There is a trade-off between scalability (per\u00adformance) and correctness in implementing \nconcurrent data struc\u00adtures. A remedy to the scalability problem is to relax the semantics of concurrent \ndata structures. The semantics is given by some no\u00adtion of equivalence with sequential behavior. The \nequivalence is determined by a consistency condition, most commonly lineariza\u00adbility [7], and the sequential \nbehavior is inherited from the sequen\u00adtial version of the data structure (e.g., the sequential behavior \nof a concurrent stack is a regular stack). Therefore, relaxing the seman\u00adtics of a concurrent data structure \namounts to either weakening the consistency condition (linearizability being replaced with sequen\u00adtial \nconsistency or quiescent consistency) or rede.ning (relaxing) its sequential speci.cation. In this paper, \nwe present a framework for relaxing sequential speci.cations in a quantitative manner. For an example \nof a relaxation, imagine a k-stack in which each pop removes one of the most recent k elements and an \noperation size which returns a value that is at most k away from the correct size. It is intuitively \nclear that such a k-stack relaxes a regular stack, but current theory does not provide means to quantify \nthe relaxation. Our framework does, it provides a way to formally describe and quantitatively assess \nsuch relaxations. We view a data structure as a sequential speci.cation S consist\u00ading of all semantically \ncorrect sequences of method calls. We iden\u00adtify the sequential speci.cation with a particular labeled \ntransition system (LTS) whose states are sets of sequences in S with indis\u00adtinguishable future behavior \nand transitions are labeled by method calls. A sequence is in the sequential speci.cation if and only \nif it is a .nite trace of this LTS. Our framework for quantitative relaxation of concurrent data structures \namounts to specifying costs of transitions and paths. In the LTS, only correct transitions are allowed, \ne.g., a transition labeled by poppaq is only possible in a state of a stack with a as top element. In \na relaxation, we are exactly interested in allowing the wrong transitions, but they will have to incur \ncost. Our framework makes this possible in a controlled quantitative way. The framework is instantiated \nthrough specifying two cost func\u00adtions: A local function, transition cost, that assigns a penalty to \neach wrong transition, and a global function, path cost, that accu\u00admulates the local costs (using, e.g., \nmaximum, sum, or average) to obtain the overall distance of a sequence. Via this local-global dichotomy, \nwe are able to achieve a separation of concerns, mod\u00adularity and .exibility: Different transition costs \ncan be used with the same path cost, or vice versa, leading to different relaxations. Once the distance \nof a sequence from the original sequential speci\u00ad.cation S is de.ned in this way, a k-relaxation of the \ndata structure becomes the set of all sequences within distance k from S. Returning to the stack example \nabove, we can set the transition cost of a pop transition at a state to be the number of elements that \nare between the popped element and the top of the stack. We can de.ne the path cost to be the maximum \ntransition cost that occurs along a sequence. Then, the corresponding k-relaxation precisely captures \nwhat we intuitively described. We instantiate the framework on two levels. On the abstract level, we \npresent two generic relaxations called out-of-order and stuttering relaxation, which provide a way to \nassign transition costs, together with several different path cost functions for any data structure. \nOn the concrete level, we instantiate the out-of-order relaxation to stacks, queues, and priority queues. \nWe spell out the effects of the relaxation in these concrete cases and prove that they indeed correspond \nto the intuitive idea of bounded relaxed out-of\u00adorder behavior. We also instantiate the stuttering relaxation \nto a Compare-And-Swap (CAS) object and a shared counter and prove correspondence results as well. We \nshow that the relaxation framework is indeed of practi\u00adcal value: we give an ef.cient new implementation \nof an out-of\u00adorder stack and .t an existing ef.cient implementation of an out\u00adof-order queue [11] in \nour framework as well. The experimental results demonstrate ideal behavior: linear scalability and perfor\u00admance. \nIn particular, the relaxed stack implementation we present outperforms and outscales state-of-the-art \nstack, queue, and pool algorithms on various workloads. We also present implementations for a stuttering \nCAS, a stuttering shared counter using this stut\u00adtering CAS and a different stuttering shared counter, \nall of which further demonstrate increased scalability and performance. The main contributions of this \npaper are: (1) the framework for quantitative relaxation of data structures, and (2) ef.cient concur\u00adrent \nimplementations. The way to the framework is paved by for\u00admally capturing the semantics of a data structure. \nOther contribu\u00adtions made possible by the framework are: the generic out-of-order and stuttering relaxations \nof data structures; characterizations of the out-of-order relaxation in concrete terms for stacks, queues, \nand priority queues; characterization of the stuttering relaxation in con\u00adcrete terms for CAS and shared \ncounters. The structure of the paper is as follows. In the remainder of this section, we provide motivation \nfor the main features of our work. We present the formal view on data structures in Section 2, fol\u00adlowed \nby the framework for quantitative relaxation in Section 3. Throughout the formal part we use a stack \nas running example. We present the two generic instances, out-of-order and stuttering relax\u00adations, in \nSection 4 and instantiate them further to concrete data structures in Section 5 and Section 6, respectively. \nWe discuss re\u00adlated work in Section 7. In Section 8 we present implementation details and in Section \n9 experimental results con.rming our origi\u00adnal scalability and performance goal. We wrap up with concluding \nremarks in Section 10. In the related work survey, we put special emphasis on quasi\u00adlinearizability [2], \nthe only other work we are aware of that also tackled the problem of quantitatively relaxing sequential \ndata struc\u00adtures for better performance in the concurrent setting. As opposed to our semantic (state-based) \napproach in assigning distances to se\u00adquences, the relaxation of [2] is syntactic (permutation-based). \nWe argue that (1) the semantic approach is more expressive than the syntactic one, and (2) it allows \nthe designer of a data structure to formally capture the intent of a speci.c relaxation more easily and \nnaturally. Highlights Relaxation improves performance. A relaxation of the sequen\u00adtial speci.cation of \na data structure can lead to a distribution of contention points, diminishing the need for, and thus, \nthe cost of synchronization. For instance, instead of requiring that each pop operation updates the top \npointer of a concurrent stack, allowing a relaxation which sets the size of the window from which a re\u00admoval \nis deemed acceptable to some k > 1 (most recent elements) effectively reduces contention for the top \npointer. In Section 9, we show that even such a simple relaxation for stacks with k 80 on a 40-core \n(2 hyperthreads per core) server machine can lead to an eight-fold increase in performance compared to \nthe existing state\u00adof-the-art implementations of strict stacks. Note that a larger sequential speci.cation \nincreases the poten\u00adtial for better performance. Since our relaxations are monotonic, increasing k increases \nthe performance potential. However, the ex\u00adtent to which this potential can be utilized in practice depends \non many factors among which is the choice of hardware. Generality. Consider three different sequences \nbelonging to three different data structures, stack, queue, and priority queue, respec\u00adtively: pushpaqpushpbqpushpcqpushpdq \nenqpaqenqpcqenqpbqenqpdq inspaqinspbqinspdqinspcq where for the priority queue b has top priority, followed \nby a and c that have the same medium priotity, and d has low priority. If these sequences are extended \nwith a removal operation (pop, deq, rem, respectively), the expected return values are d (element at \nthe top of the stack), a (element at the head of the queue), and b (element with the highest priority). \nImagine instead, that the removal operation returns c for all of these three sequences. At .rst sight, \nthat c is returned seems to be arbitrary. However, a careful examination reveals a common pat\u00adtern: In \neach sequence, c is not the current, but next (possible) value to be removed. That is, in the stack it \nis the element immediately below the top element; in the queue it is the element immediately after the \nhead element; in the priority queue it is an element with the second highest priority. It then seems \nnatural to view all these relaxations as an instantiation of a common relaxation scheme. Our framework \nallows one to precisely express this and other common types of relaxations. For instance, our generic \nout-of\u00adorder relaxation provides exactly this for data structures in which information is retrieved according \nto some order, temporal in the case of queue and stack, logical in the case of a priority queue. The \ngeneric relaxation removes the need of relaxing each data structure separately. Modularity. Let us now \nconsider the situation immediately fol\u00adlowing the removal of c from the stack as was depicted above. \nWe have the following sequence: pushpaqpushpbqpushpcqpushpdqpoppcq The stack now contains the elements \na, b, and d, the last of which is on top. One might desire a particular relaxation where two consecutive \nout-of-order removals are not allowed, and hence, the next removal has to return d. Yet another might \n.nd it acceptable that at all times one of the top two elements are removed; it does not matter how long \nthe top element remains on the stack. Our framework allows one to express both. Each transition incurs \na transition cost. Observe that in both relaxations the same cost (out\u00adof-order removal cost) is assigned \nto each transition. Each sequence of transition costs incurs a path cost, and this is what distinguishes \nthe two relaxations. The .rst will require that there are no two consecutive transitions with non-zero \ncost; the second will require that the maximum of any transition cost is at most 1. We thus obtain a \nmodular framework in which existing relaxations can be tailored by modifying transition costs, path costs, \nor both. Measurability. The code given in Figure 1(a) represents a CAS\u00adbased strict shared counter. The \nshared variable c is a counter, and each thread tries to increment the value of the counter. Represen\u00adtative \nof many concurrent implementations, this code leads to poor scalability as all threads trying to increment \nthe counter will com\u00adpete for access to c. 1 wh ile (t r u e ) : 1 wh ile (t r u e ) : 2 x=c ; 2 x, \nv = g e t M a x A n d V a l u e A t ( c, f ( t ) ) ; 3 if (C A S ( &#38; c , x , x + 1 ) ) : 3 if (C \nA S ( &#38; c [ f ( t ) ] , v , x + 1 ) ) : 4 re tu rn x+ 1 ; 4 re tu rn x+ 1 ; (a) Single counter. (b) \nDistributed counter. Figure 1. Shared counters Next, consider a modi.ed version of this shared counter, \ngiven in Figure 1(b). Unlike the strict implementation, here we use an array c of k counters and the \nlogical value of the shared counter is taken to be the maximum value among all the counters in c. Each \nthread t can write only to the slot with index f(t). Each attempt of t incrementing the counter starts \nby reading the value contained in f(t) (stored in v) and the maximum value of all the counters in c (stored \nin x). Then, it tries to update its counter to x ` 1 provided that f(t) is not updated by a concurrent \nthread. The behavior of this code depends crucially on the value chosen for the size k of the array. \nFor instance, if k 1, then this implemen\u00adtation will be behaviorally equivalent to the single counter \ncode. For other values of k, it is evident that there will be a discrepancy between the behaviors of \nthe two codes. We go beyond this qualitative notion (existence vs. absence of relaxation) and provide \na measure for any relaxation de.ned in our framework. The distributed counter given in Figure 1(b) is \nin fact a k-stuttering relaxation of the shared counter of Figure 1(a). This way an application developer \nusing a quantitatively relaxed data structure can evaluate the gain in performance for k-relaxation vs. \nthe effort of modifying an application that uses it, and try to op\u00adtimize k. For instance, if the relaxed \nshared counter is used as a performance counter counting the occurrence of a given event, e.g. context \nswitches in a multi-processor scheduler, not all occurrences of events will be registered. Knowing that \nthe number of unregis\u00adtered event occurrences within one counter increment can not ex\u00adceed k (the size \nof c) is a crucial information for the application designer. Transparency. Now consider the code given \nin Figure 2. This code is very similar to the strict shared counter code of Figure 1(a) except for the \ncall to the method kCAS instead of CAS. The kCAS is a relaxed version of CAS such that up to at most \nk many concurrent threads trying to update the value of the CAS object can complete with false positive \n(see Section 6 and Section 8 for details). Although the kCAS and the dis\u00ad1 while(true): tributed counter \nof Figure 1(b) take 2 x = c; fundamentally different approaches 3 if (kCAS(&#38;c,x,x+1)):in relaxing \nthe strict semantics of a 4 return x+1; counter, they both implement a k\u00adstuttering relaxation. This \nillustrates Figure 2. kCAS counter. another use of our framework: It can be used as a simpler way to \nestablish abstract equivalence, thus providing transparency for a higher-level application. If one shows \nthat two implementations implement the same relaxation, then a client application using either implementation \nwill observe the same behavior, regardless of the differences in actual implemen\u00adtation details. 2. Data \nstructures, speci.cations, states Let S be a set of methods including input and output values. We will \nrefer to S as the sequential alphabet. A sequential history s is an element of S\u00b0, i.e., a sequence over \nS. As usual, by e we denote the empty sequence in S\u00b0.A data structure is a sequential speci.cation S \nwhich is a pre.x-closed set of sequential histories, S D S\u00b0 . EXA M PLE 2.1. The set of methods of a \nstack, with data in a set D, is SS tpushpdq| d P DuY tpoppdq| d P D Ytnulluu. The sequential speci.cation \nSS consists of all stack-valid se\u00adquences, i.e., sequences in which each pop pops the top of the stack \nand each push pushes an element at the top. For instance, the sequence sS pushpaqpoppaqpushpbq is in \nthe sequential speci\u00ad.cation SS, whereas the sequence tS pushpaqpushpbqpoppaq is not. The following \nde.nition is the core of our way of capturing semantics. Let S be a sequential speci.cation. DE FIN I \nT I ON 2.2. Two sequential histories s,t P S are S-equivalent, written s S t, if for any sequence u \nP S\u00b0 , su P S if and only if tu P S. It is clear that S is an equivalence relation. By rssS we de\u00adnote \nthe S-equivalence class of s. Intuitively, two sequences in the sequential speci.cation are S-equivalent \nif they lead to the same state . The following simple property follows directly from the de.nition of \nS-equivalence. LEM M A 2.3. If s S t and su P S, then su S tu. The intuition about states is made explicit \nin the next de.nition. In addition, we point out particular minimal representatives of a state. DE FIN \nI T I ON 2.4. A state of a data structure with sequential spec\u00adi.cation S is an equivalence class rssS \nwith respect to S. For a state q rssS, the kernel of q is the set kerpqq t t PrssS | t has minimal length \nu. A sequence s P S is a kernel sequence if s P kerprssSq. EXA M PLE 2.5. One can easily show that kernel \nsequences of a stack are all sequences in tpushpdq| d P Du\u00b0. Moreover, for any state q rssSS of a stack, \nthere is a unique sequence in kerpqq, i.e., |kerpqq| 1. This implies that different sequences in s Ptpushpdq|d \nP Du\u00b0 represent different states. Having identi.ed states, a data structure corresponds to a la\u00adbeled \ntransition system (LTS) that we de.ne next. DE FIN I T I ON 2.6. Let S be a (sequential speci.cation \nof a) data structure. Its corresponding LTS is LTSpSq pQ, S,\u00d1,q0q with set of states Q S{ S t rssS \n| s P Su,  set of labels S,  transition relation \u00d1D Q S Q given by m  rssS \u00d1rsmsS if and only if \nsm P S, and  initial state q0 resS.  Note that the transition relation is well de.ned (independent \nof the choice of a representative) due to Lemma 2.3. Also q0 is m well de.ned since S is pre.x closed. \nWe write q \u00d1 if there is m an m-labeled transition from q to some state; q \u00db if there is no u m-labeled \ntransition from q. We also write q \u00d1 if there is a u\u00ad u labeled path of transitions starting from q, \nand q \u00db if it is not the u case that q \u00d1. The following immediate observation provides the exact correspondence \nbetween the sequential speci.cation of a data structure and its LTS: S is the set of .nite traces of \nthe initial state of LT SpSq. LEM M A 2.7. Let S be a sequential speci.cation with LTSpSq pQ, S,\u00d1, q0q. \nThen for any u P S\u00b0 we have u P S if and only if u q0 \u00d1. EXA M P LE 2.8. Since different stack-kernel \nsequences represent different states, cf. Example 2.5, the transitions of LT SpSSq are fully described \nby pushpaq rssSS \u00d1rs \u00a8 pushpaqsSS poppaq rssSS \u00d1rs1sSS if s s1 \u00a8 pushpaq, and poppnullq rssSS \u00d1 resSS \nif s e where s is a kernel sequence in tpushpdq| d P Du\u00b0. Note that if s s1 \u00a8 pushpaq, then rs \u00a8 poppaqsSS \nrs1sSS . 3. Framework for quantitative relaxations We are now ready to present the framework for quantitatively \nrelax\u00ading data structures. Let S D S\u00b0 be a data structure with LTSpSq pQ,S,\u00d1,q0q. Our goal is to relax \nS to a so-called k-relaxed speci\u00ad.cation Sk D S\u00b0 in a bounded way, with k providing the bound. Giving \na relaxation for a data structure S amounts to the follow\u00ading three steps: 1. Completion. From LTSpSq \npQ, S, \u00d1, q0q we construct the completed labeled transition system LTScpSq pQ,S,Q S Q,q0q with transitions \nfrom any state to any other state by any method. 2. Transition costs. From LTScpSq a quantitative labeled \ntransi\u00adtion system QLTSpSq pQ,S, Q S Q,q0,C, costq is con\u00adstructed. Here C is a well-ordered cost domain, \nhence it has a minimum that we denote by 0, and cost : Q S Q \u00d1 C is the transition cost function satisfying \nm costpq,m,q1q 0 if and only if q \u00d1 q1 in LTSpSq. 1 We write q m\u00d1,kqfor the quantitative transition with \ncostpq, m,q1q k. A quantitative path of QLTSpSq is a sequence . q1 m\u00d11,k1 q2 m\u00d12,k2 q3 .. . qn m\u00d1n,kn \nqn`1. The sequence t pm1,k1qpm2, k2q.. . pmn,knqP pS Cq\u00b0 is the quantitative trace of ., notation qtrp.q, \nand the sequence u m1 .. . mn is the trace of the quantitative path . and of the quantitative trace \nqtrp.q, notation trp.q trpqtrp.qq u. By qtrpuq we denote the set of all quantitative traces of quantitative \npaths starting in the initial state with trace u and by qtrpSq the set of all quantitative traces of \nquantitative paths starting in the initial state. 3. Path cost function. We choose a monotone path cost \nfunction pcost : qtrpSq\u00d1 C. Monotonicity here is with respect to pre.x order: if a quantitative trace \nt is a pre.x of a quantitative trace t1, then pcostptq d pcostpt1q. Having performed these three steps, \nwe can de.ne the k-relaxed speci.cation. DE FI NI TI O N 3.1. The k-relaxed speci.cation Sk for k P C \ncontains all sequences that have a distance at most k from S, Sk tu P S\u00b0 | dSpuq d ku where dSpuq is \nthe distance of u to the sequential speci.cation S given by dSpuq mintpcostptq|t P qtrpuqu. REM AR K \n3.2. Both the distance dS and the relaxed speci.cation Sk are actually parametric in the transition cost \nfunction as well as in the path cost function. For simplicity, we prefer a light, over\u00adloaded notation \nthat does not explicitly mention these parameters. Also, for some applications one may wish for two \ndifferent cost domains, one for the transition, one for the path cost, of which only the second one needs \nto be well ordered. Again for simplicity, we restrict the presentation to a single cost domain. Some \nobvious properties of the quanti.ed relaxations resulting from our framework are: S0 S, ensured by \nthe condition on the transition cost function.  Every relaxation Sk is pre.x closed, ensured by the \nmonotonic\u00adity of the path cost function.  The relaxations are monotone, i.e., if k d m, then Sk D Sm. \n  To conclude, in order to relax a data structure all that one needs is a cost domain C, a transition \ncost for each transition in the completed LTS (item 2. above), and a path cost function (item 3. above). \nREMAR K 3.3. The current framework does not allow for relax\u00adations that leave the original state space \nof LT SpSq. An example of such is a prophetic relaxation of e.g. stack, where sequences with poppaq preceding \npushpaq need to be assigned .nite distance. This can be done by slightly changing the de.nition of LT \nScpSq: Instead of keeping the original states S{ S, one can take as set of states the quotient S\u00b0{ where \n is an equivalence that coincides with S when restricted to S. For simplicity and since we do not use \nsuch relaxations in this paper, our current de.nition of LT ScpSq keeps the states unchanged. 4. Generic \nrelaxations In this section we illustrate the relaxation framework on two generic examples. The value \nand generality of these particular examples become evident in Section 5 and Section 6 when we instantiate \nthem to concrete data structures. Let S D S\u00b0 be a data structure with LTSpSq pQ,S,\u00d1,q0q. We .rst .x the \ncost domain to C NYt8u. 4.1 Out-of-order relaxation For the out-of-order generic relaxation we de.ne \na transition cost function scost : Q S Q \u00d1 C, called segment cost, and mention two other related transition \ncost functions. DE FIN I T I ON 4.1. Let t pq,m,q1q be a transition in LTScpSq. Let v be a sequence with \nminimal length satisfying one of the following two conditions: (1) There exist sequences u, w such that \nuvw P kerpqq and uw is a kernel sequence and either m  (i) ruwsS \u00d1ru1wsS and q1 ru1vwsS, or m  (ii) \nruwsS \u00d1ruw1sS and q1 ruvw1sS.  (2) There exist sequences u, w such that uw P kerpqq and uvw is a  kernel \nsequence and either m (i) ruvwsS \u00d1ru1vwsS and q1 ru1wsS, or m  (ii) ruvwsS \u00d1ruvw1sS and q1 ruw1sS. \n  Then the segment cost is given by the length of v, scostptq |v|. If such a sequence v does not exist \nfor t , then scostptq 8. Intuitively, segment cost of a relaxed transition is the length of the shortest \nsubword (v) whose removal (1) or insertion (2) into the kernel sequence enables a transition. Observe \nthat the transition can be taken in LTSpSq if and only if its segment cost is 0, obtained by setting \nv e. We will see in the next section that this cost quanti.es out-of-order updates or observations, \nsuch as returning an element other than the top element in a stack or removing an element other than \nthe head of a queue. We note that segment cost just as any transition cost can also be used per method, \ni.e., some methods may be relaxed, some not.  4.2 Stuttering relaxation For the stuttering generic relaxation, \nwe de.ne the so-called stut\u00ad  tering cost. Figure 3. The ranges of elements which may be returned by \na pop DE FI NI TI O N 4.2. Let t pq, m,q1q be a transition in LTScpSq. operation of a k-stack with restricted \nout-of-order, out-of-order, and Then, the stuttering cost, st cost is de.ned as lateness relaxation \nwith k 3. The element a2 is already removed. st cost pq, m, q 1q $ &#38; % m1 0 if q \u00d1 q 5. Out-of-order \nstacks, queues, and priority mm \u00db q\u00d1 1 if q q 1 ^ q 1 ^ q queues 8 otherwise In this section we apply \nthe relaxation of Section 4.1 to stacks, FIFO queues, and priority queues. Due to lack of space, here \nwe leave out some common methods, e.g., top (for stack), head (for queue), size (for all). Inclusion \nof these methods does not change the results, in particular Propositions 5.1-5.3, presented in this section. \nwhere \u00d1 is the transition relation of LT SpSq. Intuitively, the stuttering relaxation allows for (already \nenabled) transitions to have no effect on the state. If, in the speci.cation S, q goes to q with method \nm and q q , then the stuttering cost of applying m at q and staying at q after the transition is 1. \nAll other transitions which are not part of the original speci.cation are set to have in.nite cost. An \nexample of an unbounded (except in the maximal size of the queue) stuttering relaxation is presented \nin [15], where workers are allowed to work on the same task by a relaxed queue semantics and an element \nin the queue can be dequeued a number of times (up to the maximal size of the queue). In favor of bounded \nstuttering we note that, typically, implementations can bene.t from retiring (completing rather than \nretrying) mutator method calls when there is too much contention and have the client handle false positives. \n 4.3 Path cost functions Let S D S\u00b0 be a data structure and t pm1,k1qpm2, k2q.. . pmn,knqa quantitative \ntrace in qtrpSq. We de.ne the following generic path cost functions (to be used with any transition cost): \n The maximal cost, pcostmax : qtrpSq\u00d1 NYt8u, maps t to the maximal transition cost along it. Formally, \n pcostmaxptq maxtki | 1 d i d nu.  The .-interval cost, pcostr.s : qtrpSq\u00d1 NYt8u, for . a binary predicate \n(.rst order formula with two free variables making statements about positions in the quantitative trace), \nmaps t to the length of a maximal consecutive quantitative subtrace that satis.es .. Hence, we have \npcostr.sptq maxt j \u00b4 i ` 1 | .pi, jq and 1 d i d j d nu.  The .-interval restricted maximal cost, pcostmax|r.s \n: qtrpSq\u00d1NYt8u, for . as in the .-interval cost, is given by  pcostmax|r.sptq maxtli, j | .pi, jq and \n1 d i d j d nu, where li, j maxtkr `pr \u00b4 i ` 1q| i d r d ju. We instantiate the out-of-order relaxation \nalong with the max\u00adimal, .-interval, and .-interval restricted maximal cost on stacks, queues, and priority \nqueues in Section 5. The .-interval restricted maximal cost is more complex and less intuitive than the \nother path cost functions, but when instantiated it provides valuable relaxation examples that are ef.ciently \nimplementable. In Section 6 we apply the stuttering relaxation along with the .-interval cost on a CAS \nobject and on a shared counter. Note that the other two cost func\u00adtions do not make much sense together \nwith the stuttering cost (the maximal cost is two-valued and the .-interval restricted maximal cost amounts \nto the .-interval cost plus one). Stack. We have already given the set of methods of a stack, its states, \nand its LTS in Example 2.1, Example 2.5, and Example 2.8. Let us recall that the sequential speci.cation \nSS consists of all stack-valid sequences, i.e., sequences in which each pop pops the top of the stack, \nand each push pushes an element at the top. Let s be a kernel sequence. A kernel sequence s1 is pushpaq-out-of-order-k \nfrom s if s1 u \u00a8 pushpaq\u00a8 v where s uv, v is minimal, and |v| k;  poppaq-out-of-order-k from s if \ns1 uv where s u \u00a8 pushpaq\u00a8v, v is minimal, and |v| k;  poppnullq-out-of-order-k from s if s s1 and \n|s| k;  By inspecting all cases, we can show the following proposition. PROPO S ITION 5.1. Let s and \ns1 be two kernel sequences of a stack. m,kThen rssSS \u00d1rs1sSS in the out-of-order relaxation with segment \ncost if and only if s1 is m-out-of-order-k from s. As mentioned in Section 4.1, the relaxations can be \napplied method-wise. We implemented k-relaxed stacks with only push and pop methods, of which only pop \nis relaxed according to the segment cost.The interpretation of the path cost functions from Section 4.3 \nand the corresponding relaxations are as follows: The maximal cost represents the maximal distance from \nthe top of a popped element, leading to an out-of-order k-stack. Hence, in an out-of-order k-stack, each \npop pops an element that is at most k away from the top. Let .pi, jq be the following .rst order formula \nwith free variables i and j: @r Pri, js.kr 0 The .-interval cost represents lateness, i.e., the maximal \nnum\u00adber of consecutive pops needed to pop the top, leading to a lateness k-stack. Hence, in a lateness \nk-stack at most the k-th consecutive pop pops the top.  The .-interval restricted maximal cost represents \nthe maximal size of a shrinking window starting from the top from which elements can be popped, leading \nto a restricted out-of-order k\u00adstack. In a restricted out-of-order k-stack, each pop removes an element \nat most k \u00b4 l away from the top, where l is the current lateness of the top.  Figure 3 presents a snapshot \nof a relaxed stack in each of the three out-of-order relaxations. It shows a state of a stack in which \nthe element a2, marked in grey, has been removed after the last removal of the top or the last push had \nhappened. The ranges show which elements may be returned by a pop operation applied to this state in \neach out-of-order relaxed version for k 3. FIFO queue. We now brie.y describe the out-of-order relaxation \nof a queue.The set of methods for a FIFO queue, with data set D, is SQ tenqpdq| d P DuY tdeqpdq| d P \nD Ytnulluu. The sequential speci.cation SQ consists of all queue-valid se\u00adquences, i.e., sequences in \nwhich each deq deques the head of the queue and each enq enqueues at the tail of the queue. For instance, \nthe following sequence sQ enqpaqenqpbqdeqpaq is in the sequential speci.cation SQ, whereas the sequence \ntQ enqpaqenqpbqdeqpbq is not. One can easily show that kernel sequences of a FIFO queue are all sequences \nin tenqpdq| d P Du\u00b0. Moreover, also here, for any state q rssSQ of the FIFO queue, there is a unique \nse\u00adquence in kerpqq, i.e., |kerpqq| 1. Hence different sequences in s Ptenqpdq| d P Du\u00b0 represent different \nstates. As a consequence, the transition relation of LTSpSQq can be described in a concise way. Let s \nbe a kernel sequence of a queue. We have, enqpaq rssSQ \u00d1rs \u00a8 enqpaqsSQ , deqpaq rssSQ \u00d1rs1sSQ if s enqpaq\u00a8 \ns1 , and deqpnullq rssSQ \u00d1 resSQ if s e. In a similar way as for stack, we can de.ne when a queue ker\u00adnel \nsequence is m-out-of-order-k from another kernel sequence, for m being a queue method. Furthermore, the \nanalogue of Proposi\u00adtion 5.1 (obtained by replacing stack by FIFO queue ) holds for queues as well which \nwe state below. PROPOS IT ION 5.2. Let s and s1 be two kernel sequences of a m,k 1 queue. Then rssSQ \n\u00d1rssSQ in the out-of-order relaxation with segment cost if and only if s1 is m-out-of-order-k from s. \nThe maximal path cost function leads to analogous out-of-order k-queue. For lateness and restricted out-of-order \nk-queues we need to employ slightly different path cost functions. Priority queue. The data set of a \npriority queue needs to be well\u00adordered, since data items carry priority as well. We take the data set \nto be N. The smaller the number, the higher the priority. The set of methods is SP tinspnq| n P NuY trempnq| \nn P NYtnulluu. The sequential speci.cation SP consists of all priority-queue\u00advalid sequences, i.e., sequences \nin which each rem removes an element with highest available priority. Kernel sequences of a priority \nqueue are all sequences in \u00b0 tinspnq| n P Nu. Unlike for stack and queue, there may be more than one \nsequence representing a state of a priority queue. For a state q, if s P kerpqq, then also any permutation \nof s is in kerpqq. Nevertheless, the order provides a canonical representative of a state: the unique \nkernel sequence ordered in non-increasing priority1. Let s be a canonical kernel sequence. The transitions \nof LT SpSPq are fully described by inspnq rssSP \u00d1rs \u00a8 inspnqsSP , rempnq rssSP \u00d1rs1sSP if s inspnq\u00a8 \ns1 , and 1 The canonical representative is a matter of choice. Equally justi.ed is using the unique kernel \nsequence ordered in non-decreasing priority, in which case the transitions of a priority queue resemble \nmore the transitions of a stack, highlighting the duality between FIFO queues and stacks. rempnullq rssSP \n\u00d1 resSP if s e. Again, we de.ne when a canonical kernel sequence is m-out-of\u00adorder-k from another canonical \nkernel sequence, where m is a priority queue method. We have the following result. PROPO S ITION 5.3. \nLet s and s1 be two kernel sequences of a prior\u00ad m,kity queue. Then rssSP \u00d1rs1sSP in the out-of-order \nrelaxation with segment cost if and only if s1 is m-out-of-order-k from s. Analogous relaxations are \nagain possible, only the path cost func\u00adtions are more complex since they need to capture when an element \nwith higher priority than all existing elements in the priority queue is inserted. 6. Stuttering relaxed \nCAS and shared counter In this section, we instantiate the relaxation from Section 4.2 to two concrete \nexamples. CAS. The set of methods for a Compare-And-Swap (CAS) object with a data set D and an initial \ndata value init P D can, for our purposes, be modeled as 1 SCAS tcaspd,d,bq| d,d1 P D, b PtT,Fuu. The \nsequential speci.cation SCAS is de.ned inductively as follows: The empty sequence e is in SCAS and any \nsequence of length one and shape caspinit,d,Tq is in SCAS for d P D. If s P SCAS , let t be the maximal \npre.x of s such that s t \u00a8 m \u00a8 u for m caspd,d1 , Tq. Then s \u00a8 m1 P SCAS if either (1) m1 caspd1 ,d2 \n,Tq, or (2) m1 caspd2 ,d3 , Fq and d2 d1 . Let s P SCAS and let as before t be the maximal pre.x of \ns such that s t \u00a8 m \u00a8 u for m caspd, d1 , Tq. Then, it is not dif.cult to show that, s caspinit,d1 \n,Tq. Hence, there is a unique kernel sequence in each equivalence class and it has length one. The transitions \nof LT SpSCAS q are given by caspinit,d,Tq resSCAS \u00d1 rcaspinit,d,TqsSCAS , caspd,d1 ,Tq rcaspinit, d,TqsSCAS \n\u00d1 rcaspinit,d1 ,TqsSCAS , and caspd1 ,d2 ,Fq rcaspinit, d,TqsSCAS \u00d1 rcaspinit,d, TqsSCAS if d d1 . \nIntuitively, the state of the CAS object is given by one data value d, initially set to init. In such \na state, a transition by method caspd, d1 ,Tq is enabled since the comparison of the .rst argument and \nthe current value succeeds (returns T, true) leading to the new state value d1, that is, a successful \ncomparison results in a swapped value. A transition by method caspd1 ,d2 , Fq in which the comparison \nfails (returns F, false) is enabled if indeed d d1 after which no swap happens and the state value remains \nd. The state with data value d is formally represented by the equivalence class of a sequence with a \nsingle method caspinit,d,Tq. Now let us formalize the notion of allowing invisible failures for CAS object \nupdates. For this purpose we de.ne another object called failCAS over the same set of methods, with a \nsomewhat different set of legal sequences. We call a sequence 1 2 caspd, d,Tq\u00a8 y \u00a8 caspd, d,Tq over \nSCAS a false positive sequence if caspd, d1 ,Tq\u00a8 y P SCAS , y is a sequence of symbols of the form caspd3 \n,\u00b4,Fq with d3 d, and d d1. Then, s s0 .. . sn P S f ailCAS if there exists a set of positions 0 i0 \n< i1 <. . .< ir n for s such that each sequence sij\u00b41 .. . sij , for 1 d j d r, is either a false positive \nsequence or is in SCAS . Let the failure count of x P Sf ailCAS be the maximum number of consecutive \nfalse positive sequences x contains. The corresponding relaxation in our framework is obtained by using \nstuttering cost on the CAS object and .-interval cost for the predicate .pi, jq given by 1 @r Pri, js.pkr \n 0 _Dd, d1 P D.pmr caspd,d,Fqqq, leads to a k-CAS in which up to k methods may stutter at the same state \n(fail to perform a swap even though the data values match). It is then easy to show the following correspondence \nresult. PROPOS IT ION 6.1. A sequence x P Sf ailCAS has failure count k if and only if x is in the speci.cation \nof k-CAS. Shared counter. The set of methods of a shared counter is SSC tget&#38;Incpnq| n P Nu. The \nsequential speci.cation SSC of a shared counter contains the empty sequence e and a sequence s of length \nn > 0 is in SSC if and only if spiq get&#38;Incpiq, for all 1 d i d n. One can easily show that each \nstate of a shared counter is a singleton, i.e., for s, t P SSC we have s t if and only if s t. The \nunique sequence representing a state is automatically a kernel sequence. The transitions of LT SpSSC \nq are obviously given by get&#38;Incp1q resSSC \u00d1 rget&#38;Incp1qsSSC , and get&#38;Incpn`1q \u00d1 rs \u00a8 get&#38;Incpn \n` 1qsSSC , rssSSC if spiq get&#38;Incpiq, for all 1 d i d n. We de.ne a failing shared counter analogously \nto a failing CAS object. A sequence s is a behavior of failSC if either s P te,get&#38;Incp1qu, or t \n u \u00a8 get&#38;Incpnq is a behavior of failSC and s t \u00a8 get&#38;Incpn ` aq, for a Pt0,1u. The failure \ncount of s P Sf ailSC is one less than the length of a maximal subsequence of identical symbols in s. \nThe corresponding relaxation of the shared counter is obtained by the stuttering cost and the .-interval \ncost, for .pi, jq @r P ri, js.kr 0. Thus, we get the k-stuttering shared counter, k-SC, in which a method \ncan stutter (fail to produce a new, incremented by one, value) at most k times. For instance, the sequence \nget&#38;Incp1qget&#38;Incp2qget&#38;Incp2qget&#38;Incp2qget&#38;Incp3q is in the sequential speci.cation \nof the 2-stuttering shared counter, 2-SC. We again have the following correspondence result. PROPOS IT \nION 6.2. A sequence x P S f ailSC has failure count k if and only if x is in the speci.cation of k-SC. \n7. Related work The general topic of this paper is part of a recent trend towards scalable but semantically \nweaker concurrent data structures [18]. We .rst discuss work related to our framework and then focus \non work related to our implementations. Framework. The relaxation framework generalizes previous work \non so-called semantical deviation and k-FIFO queues [10, 12] which correspond to restricted out-of-order \nk-FIFO queues here. Our work is closely related to relaxing the semantics of con\u00adcurrent data structures \nthrough quasi-linearizability [2]. Just like quasi-linearizability, we provide quantitative relaxations \nof concur\u00adrent data structures. Unlike quasi-linearizability which uses syn\u00adtactic distances, our relaxations \nare based on semantical distances from a sequence to the sequential speci.cation. We brie.y present the \nquasi-linearizability approach, identify two main issues, and how our method overcomes these. We call \ntwo sequences x, x1, both of length n, permutation equivalent, written x x1, if there exists a permutation \np on 1 t1,. .., nu such that for all 1 d i d n, xpiq xpppiqq. We write x p x1 to emphasize the permutation \nwitnessing x x1. In such a case, the permutation distance between x and x1 is given as maxt|i \u00b4 ppiq||1 \nd i d nu. Let S be a sequential speci.cation over S. In [2], the distance of a sequence x P S\u00b0 to S is \nde.ned via a collection D of subsets of S. Let y P S\u00b0 be a sequence such that z xy has a permutation \nequivalent z1 P S. Then, for A P D, the A-cost of obtaining z1 from z is the permutation distance between \nz|A and z1|A, where | denotes restriction. Let kA denote the minimal A-cost over all y. Then, x is quasi-linearizable \nwith quasi-linearization factor Q: D \u00d1 N, if for all A P D, kA d QpAq. Observe that the distance for \nx is obtained by quantifying over all possible extensions of x whose permutations are in S. We now show \nthat this de.nition fails to capture desired relaxation distances. 1. Not precise. Consider the following \nsequence: enqp1qenqp2qenqp3qdeqp1qdeqp2qenqp4qdeqp4q In order to assign a relaxation cost of 1 to this \nsequence belonging to an out-of-order queue, quasi-linearizability employs a scheme where only enqueue \noperations are allowed to commute. Formally, quasi-linearizability uses D tE nq, Dequ with QpE nqq k \nand QpDeqq 0, where E nq (resp., Deq) contains all enq (resp., deq) symbols. However, with this scheme \nthe sequence deqpiqn which removes elements from an empty queue will always be in any k\u00adrelaxation of \nthe queue because setting nn nn z deqpiqenqpiq, z 1 enqpiqdeqpiq will give kE nq kDeq 0, independent \nof the value n. This means that the following implementation is a 0-relaxation of queue. enq(x): { while(true);} \ndeq(): { return random();} This implementation is clearly not implementing a queue, nor any intended \nbounded relaxation of a queue, but all the sequences it generates will have zero distance relative to \nD as given above. Thus, quasi-linearizability cannot exactly capture intended relax\u00adations and might \nallow wrong behaviors. Observe that we have already shown in Proposition 5.2 that out-of-order k-queues \ncan never generate such erroneous behavior. 2. Not general. For a stack, consider the sequence n x pushpaqrpushpiqpoppiqs \nm pushpbqrpushpjqpoppjqs r poppaqrpushplqpopplqs where all symbols, a,b,. .. have distinct values. Prior \nto the re\u00admoval of a, the stack contains a and b, with the latter at the top po\u00adsition. The distance \nin the out-of-order relaxation induced by max\u00adimal path cost and segment cost in this case is 1 since \nthe element popped is immediately after the top entry. However, with quasi\u00adlinearization factor Q, it \nis impossible to precisely capture out-of\u00adorder penalty for data structures like stacks. First, consider \nthe case where we pick z x, which we can do since x has a permutation equivalent valid stack sequence. \nIn order to get a permutation x1 of x such that x1 is a valid sequence of a stack, either one of poppaq \nor pushpbq has to move over m copies of push and pop operations, or one of pushpaq or pushpbq has to \nmove over n copies of push and pop operations. So either D is empty which allows for any sequence to \nbe in the relaxation or it is always possible to pick the values for n and m such that the penalty is \narbitrarily large. Second, consider the case where we extend x with y such that xy has a permutation \nequivalent valid stack behavior. But because of the 2r-long suf.x of x, a similar reasoning as in the \nprevious case applies to this case as well. Similarly, a stuttering relaxation will not have a .nite \nquasi\u00adlinearizability distance, since no permutation of (an extension of) a stuttering sequence is in \nthe original sequential speci.cation. Consistency conditions. As opposed to relaxing the sequential speci.cation \nof a concurrent data structure, one may also relax the consistency condition, e.g., quiescent consistency \n[4] instead of linearizability. We note that linearizable out-of-order relaxation of a stack is incomparable \nto a quiescently consistent stack. To see this, .rst, consider a concurrent history c with two threads \nt1 and t2. The history c starts with the invocation of pushpaq by t1, n followed by a sequence poppiqpushpiqn \nall executed by t2. This history is quiescently consistent for stack because the reordering of methods \n(even those that do not overlap in time) is allowed as long as they are not separated by a quiescent \nstate. On the other hand, any linearization of c will have to observe out-of-order pop operations since \nthe operations of t2 do not overlap. So, for each k, there exists a history which is quiescently consistent \nfor stack but is not in the speci.cation of an out-of-order k-stack. Second, consider the sequential \nhistory pushpaqpushpbqpoppaq which has an out\u00adof-order relaxation distance of 1. Since the history is \nsequential, quiescent consistency will not allow any reordering. Thus, for any k, there exists a history \nwhich is in the speci.cation of an out\u00adof-order k-stack but not quiescently consistent. A comprehensive \noverview of variants of weaker and stronger consistency conditions than linearizability can be found \nin [6]. Implementations. Work related to our implementations and ex\u00adperiments is discussed in more detail \nin Section 8 and Section 9. Here we brie.y refer to all the work considered. Our relaxed stack implementation \nis closely related to the very recent ef.cient lock\u00adfree implementation of a relaxed k-FIFO queue [11] \n(by some of us and a third coauthor), but the change from queue to stack seman\u00adtics imposes a signi.cant \ndifference as well. The k-FIFO queue [11] is in turn related to implementations of relaxed FIFO queues \nsuch as the Random Dequeue and Segment Queue [2] as well as Scal queues [10, 12]. Both the Random Dequeue \nQueue and the Seg\u00adment Queue implement the restricted out-of-order relaxation. The Segment Queue [2] \nand the k-FIFO queue [11] implement a queue of segments. However, the implementations are quite different \nwith signi.cant impact on performance, see Section 9. Our relaxed stack implementation implements a stack \nof segments. Scal queues are relaxed queues with, in general, unbounded relaxation. Since any relaxed \nstack or queue implementation also implements a pool, we compare our work also to state-of-the-art pool \nimplementa\u00adtions [1, 3, 19]. In [5], the authors show that implementing deterministic data structure \nsemantics requires expensive synchronization mecha\u00adnisms which may prohibit scalability in high contention \nscenar\u00adios. We agree with that and show in our implementations and ex\u00adperiments that the non-determinism \nintroduced in the sequential speci.cation provides scalability and performance bene.ts. In [15], the \nauthors present a work-stealing queue with relaxed semantics where queue elements may be returned any \nnumber of times instead of just once. In comparison to other state-of-the-art work-stealing queues with \nnon-relaxed semantics this may provide better perfor\u00admance and scalability. Again the introduced non-determinism \npays off. Overview of different relaxations on hardware and software level is presented in [9, 18]. \n8. Implementations of relaxed data structures In this section we present the new implementation of a \nrestricted out-of-order stack (k-stack, for short) and present the two new implementations of a stuttering \nshared counter. It is interesting to note that the restricted relaxation seems to be crucial for obtaining \nperformance2, which is why we focus on it. 8.1 k-Stack The top pointer of a concurrent stack may become \na scalability bottleneck under high contention [20]. The main idea behind our k-stack implementation \nis to reduce contention on the top pointer by maintaining a stack of so-called k-segments3. We implemented \nthe stack that holds the k-segments similarly to the lock-free stack of [20] with the difference that \nthere is always at least one k\u00adsegment, even if it is empty, on the stack. This avoids unnecessary removal \nand adding of a k-segment, e.g. in the empty state. A k\u00adsegment (or just segment, when no confusion arises) \ncontains k atomic values (see next paragraph) which may either point to null indicating an empty slot \nor may hold a so-called item. Both push and pop operations are served by the top segment. Hence, up to \nk stack operations may be performed in parallel. A push operation tries to insert an element in the top \nsegment. It adds a new segment to the stack if the top segment is full. A pop operation tries to remove \nan element from the top segment. It removes the top segment from the stack if it is empty and is not \nthe only segment on the stack. Additionally, each segment contains an atomic counter remove that counts \nhow many threads are trying to remove it from the stack. The counter is initially set to zero. The pseudo \ncode of the lock-free k-stack algorithm with k >0 is depicted in Figure 4. The occurrence of the ABA \nproblem is made unlikely through version numbers. We refer to values enhanced with version numbers as \natomic values. Hence an atomic value has two .elds, the actual value val and its version number ver. \nThe methods init, try add new ksegment, and try remove ksegment implement the stack of segments. In the \nlatter, the atomic counter remove is updated and the method empty that performs an empty check is called. \nWe discuss the method empty within the pop method, as it is also called there. Let item represent an \nelement to be pushed on the k-stack. The push method .rst tries to .nd an empty slot for the item using \nthe find empty slot method (line 45). The find empty slot method randomly selects an index in the top \nk-segment and then linearly searches for an empty slot starting with the selected index and wrapping \naround at index k. Then the push method checks if the k-stack state has been consistently observed by \ntesting whether top changed in the meantime (line 46) which would trigger a retry. If an empty slot is \nfound (line 47) the method tries to insert the item at the location of the empty slot using a compare-and-swap \n(CAS) operation (line 49). If the insertion is successful the method veri.es whether the insertion is \nalso valid by calling the committed method (line 50), as discussed below. If any of these steps fails, \na retry is performed. If no empty slot is found in the current top segment, the push method tries to \nadd a new segment to the stack of segments (line 53) and then retries. The committed method (line 24) \nvalidates an insertion, it en\u00adsures that the inserted element is really inserted on the stack. This method \nis the core and the main novelty of the algorithm. It re\u00adturns true when the insertion is valid and false \nwhen it is not valid. An insertion is invalidated if a concurrent thread removes the segment to which \nthe element was inserted before the effect of the 2 For an ef.cient implementation one needs a sequential \nspeci.cation that .ts the properties of the hardware that it will run on. 3 The same high-level idea \nis used in the Segment Queue [2] and the k-FIFO queue [11] discussed in the next subsection.  1 glo \nbal to p ; 2 3 voi d init ( ) : 4 new_ks egment = c a l l o c ( s i z e o f ( k s e g m e n t ) ) ; 5 \nto p = a t o m i c _ v a l u e ( n e w _ k s e g m e n t , 0 ) ; 6 7 voi d tr y_ad d_n ew_k segm ent \n( t o p _ o l d ) : 8 if top_old == top: 9 new_kse gment = c a l l o c ( s i z e o f ( k s e g m e n \nt ) ) ; new_ ksegmen t -> n e x t = t o p _ o l d ; 11 to p_new = a t o m i c _ v a l u e ( n e w _ \nk s e g m e n t , top_ old . v e r + 1 ) ; 12 CA S ( &#38; t o p , t o p _ o l d , t o p _ n e w ) ; \n13 14 voi d tr y_ re mo ve _k se gm en t ( t o p _ o l d ) : 15 if top_old == top: 16 if top_old -> n \ne x t ! = n u l l : 17 at omic _in crem ent ( &#38; t o p _ o l d -> r e m o v e ) ; 18 if empty ( t \no p _ o l d ) : 19 top _new = a t o m i c _ v a l u e ( t o p _ o l d -> n e x t , top_old .ver +1); \nif CA S ( &#38; t o p , t o p _ o l d , t o p _ n e w ) : 21 re tu rn ; 22 at omic _de crem ent ( &#38; \nt o p _ o l d -> r e m o v e ) ; 23 24 boo l committed ( t o p _ o l d , i t e m _ n e w , i n d e x \n) : 25 if top_old -> s [ i n d e x ] ! = i t e m _ n e w : 26 re tu rn true ; 27 else i f top_old -> \nr e m o v e = = 0 : 28 re tu rn true ; 29 else : //top_old->remove >= 1 ite m_empt y = a t o m i c _ \nv a l u e ( E M P T Y , i t e m _ n e w . v e r + 1 ) ; 31 if top_old != top: 32 if !C A S ( &#38; t \no p _ o l d -> s [ i n d e x ] , i t e m _ n e w , i t e m _ e m p t y ) : 33 re tu rn true ; 34 else \n: 35 top _new = a t o m i c _ v a l u e ( top_ old . v a l , top_ old . v e r + 1 ) ; 36 if CA S ( &#38; \nt o p , t o p _ o l d , t o p _ n e w ) : 37 re tu rn true ; 38 if !C A S ( &#38; t o p _ o l d -> s \n[ i n d e x ] , i t e m _ n e w , i t e m _ e m p t y ) : 39 re tu rn true ; re tu rn false ; 41 42 voi \nd push ( item ) : 43 wh ile true : 44 top_old = top; 45 item_old , i n d e x = f i n d _ e m p t y _ \ns l o t ( t o p _ o l d ) ; 46 if top_old == top: 47 if item_old . v a l = = E M P T Y : 48 item_new \n= a t o m i c _ v a l u e ( i t e m , item_old . v e r + 1 ) ; 49 if CA S ( &#38; t o p _ o l d -> s \n[ i n d e x ] , i t e m _ o l d , i t e m _ n e w ) : if committed ( t o p _ o l d , i t e m _ n e w \n, i n d e x ) : 51 re tu rn true ; 52 else : 53 try _ad d_ne w_k segm ent ( t o p _ o l d ) ; 54 55 item \np o p ( ) : 56 wh ile true : 57 top_old = top; 58 item_old , i n d e x = f i n d _ i t e m ( t o p _ \no l d ) ; 59 if top_old == top: if item_old . v a l ! = E M P T Y : 61 item _empt y = a t o m i c _ v \na l u e ( E M P T Y , i t e m _ o l d . v e r + 1 ) ; 62 if CA S ( &#38; t o p _ o l d -> s [ i n d e \nx ] , i t e m _ o l d , i t e m _ e m p t y ) : 63 re tu rn item_old . v a l ; 64 else : 65 if on ly \n_k seg me nt ( t o p _ o l d ) : 66 if empty ( t o p _ o l d ) : 67 if top_old == top: 68 re tu rn null \n; 69 else : tr y_ r em ov e_ ks eg me nt ( t o p _ o l d ) ; Figure 4. Lock-free k-stack algorithm insertion \ntook place. Therefore, an insertion is valid if the inserted item already got popped at validation time \nby a concurrent thread (line 25, 32, 38) or the segment where the item was inserted was not removed by \na concurrent thread (line 27). A remove counter larger than zero indicates that the segment has been \nremoved or concurrent threads are trying to remove the segment from the stack (line 29). If the current \ntop segment is not equal to the segment where the item was inserted we have to conservatively assume \nthat the segment was removed from the stack (line 31) and undo the insertion (line 32). If the current \ntop segment is equal to the seg\u00adment where the item was inserted, a race with concurrent popping threads \nmay occur which may not have observed the insertion of the item and may try to remove the k-segment from \nthe stack in the meantime. This would result in loss of the inserted item. To prevent that, the method \ntries to increment the version number in the top atomic value using CAS (line 36) forcing threads that \nconcurrently try to remove that k-segment to retry. If this fails, a concurrent pop operation may have \nchanged top (line 20) which would make the insertion potentially invalid. Hence, in case of losing the \nrace, the method tries to undo the insertion using CAS (line 38). The pop method returns an item if \nthe k-stack is not empty. Otherwise it returns null. Similar to the push method, the pop method .rst \ntries to .nd an item in the top segment using the find item method (line 58). The find item method randomly \nselects an index in the top k-segment and then linearly searches for an item starting with the selected \nindex and wrapping around at index k. Then, the pop method checks if the k-stack state has been consistently \nobserved by checking whether top changed in the meantime (line 59) which would trigger a retry. If an \nitem was found (line 60) the method tries to remove it using CAS (line 62) and returns it if the removal \nwas successful (line 63). Otherwise a retry is performed. If no item is found and the current segment \nis the only segment on the stack (line 65) an empty check is performed using the method empty (line 66). \nThis method stores the values of the k slots of the segment in a local array (if they are empty) and \nsubsequently checks in another pass over the segment slots whether the values in the slots changed in \nthe meantime. If a non-empty slot was found, the empty method immediately returns false. If the empty \ncheck succeeded and the top did not change in the meantime (line 67), null is returned (line 68). Otherwise, \nif no item is found in the current segment and there is more than one segment in the stack, the method \ntries to remove the segment (line 70) and performs a retry. Correctness: k-Stack We now prove that the \nk-stack implementation is correct for the relaxed stack semantics. PROPO S ITION 8.1. The k-stack algorithm \nis linearizable with re\u00adspect to restricted out-of-order k-stack. Proof. Without loss of generality, \nwe assume that each item pushed on the stack is unique. A segment s is reachable from a segment s if \neither s =s or s is reachable from s->next. An item i is on the stack, if push(i) has already committed \nand there exists a segment reachable from the top segment containing a slot whose value is i. Note that \nreachability is important, i.e., only having a slot containing the item is not enough to guarantee that \nthe item is logically on the stack, because the slot could be in a segment (to be) removed by a concurrent \npop operation. We begin by identifying a linearization point of each method call. The goal is to show \nthat the sequential history obtained from a concurrent history by ordering methods according to their \nlin\u00adearization points is in the speci.cation of a restricted out-of-order k-stack. The linearization \npoint of push is the reading of the empty slot (line 45) in the last iteration (successful insertion) \nof the main loop. The linearization point of pop that does not return null is the reading of a non-empty \nslot (line 58) in the last iteration (success\u00adful removal) of the main loop. The linearization point \nof a null\u00adreturning pop is the point after the .rst pass of the segment in the call to empty method (line \n66) which returns true. The correctness argument is based on the following facts. 1 st ruct blk_ori \nginal { 1 st ruct blk_mod ified { 1. An item is pushed on the stack exactly once. This is a con-2 pi \ndtype X ; 2 pi dtype X [ k ] ; 3 bool Y; 3 bool Y [ k ] ; sequence of our unique-items assumption and \nthe control .ow of 4 val_t V; 4 val_t V; push(i), the only method that can modify a slot to contain \ni. 5 bool C ; 5 bool C [ k ] ; 2. An item is popped at most once. If an item i is on the stack, 6 val_t \nD ; 6 val_t D ; then it can only be removed once, because of 1. and the existence of a unique statement \nwhich replaces i with empty. If i is in some slot but not on the stack, then push(i) will erase i and \nretry insertion before committing. We have to show that while i is in some slot but not on the stack, \nno pop operation can return i. Clearly, the call to method committed by push(i) must return false. This \nimplies that until committed completes, the slot where i resides is not modi.ed by any other thread. \nOtherwise, either after the .rst if statement (line 25) or following failed CAS attempts (lines 32 and \n38) of replacing i with empty will lead to returning true. Furthermore, when control reaches the only \nexit point for returning false, it is guaranteed that there is no slot containing i. Thus, if i is not \non the stack, no pop operation could have replaced it with empty. 3. If a pop operation returns null, \nthen during its execution, there must be a state at which there are no items on the stack. Since returning \nnull is without any side-effect, it suf.ces to prove the existence of a state which corresponds to a \nlogically empty stack. The call to empty is only done when the top segment is the only segment in the \nstack. In the empty method, the value of top is checked at the beginning and after the .rst pass to ensure \nthat the pointer is not updated by concurrent operations. Hence, the stack is indeed logically empty \nat the linearization point since the second pass succeeds. 4. An item j cannot be popped before an item \ni, if they are both on the stack, and i, j are in segments s, s , respectively, with s reachable from \ns and s s. The segment s can become a top segment only after the segment s has been removed by some \npop operation. Moreover, if a segment becomes unreachable from the top segment, it remains unreachable. \nThese two observations imply that the linearization point of pop(i) must precede the linearization point \nof pop(j) which can only happen when s is a top segment. 5. An item i on the stack is popped only if \nit is one of the k \u00b4 l youngest items on the stack, where l is the current lateness of the youngest item. \nBy youngest we mean most recently pushed. Recall that lateness is the number of pops that were performed \nafter the pop of the previous youngest item or the push of the current youngest one. Assume i is popped \nfrom the stack at the current moment in time and at that point the youngest item is t with current lateness \nl. This means that ever since t is the youngest item on the stack, no push operation was performed and \nthere have been l pop operations performed none of which removed t. Let j be any of these l popped items. \nSince t is the last item pushed and it is still on the stack, t is in the top segment. Since j is removed \nbefore t, by 4. it must have also resided in the top segment. For the same reason, also i is in the top \nsegment prior to its removal. Hence, at the moment in which pop(i) happens, there are at most k \u00b4l items \nin the top segment.  Now, 5. shows that the sequential behavior obtained by order\u00ading methods according \nto their linearization points satis.es the restricted-out-of-order k-stack. Moreover, 3. shows an even \nstricter behavior, a linearizable empty check.4 Thus, any concurrent execu\u00adtion generated by the given \nalgorithm is linearizable for restricted out-of-order k-stack. Observe that already 1. and 2. show that \nthe k-stack has pool semantics. l 4 We could easily relax the linearizable empty check to .t the restricted \nout-of-order speci.cation, by removing line 66 in the code. However, a lin\u00adearizable empty check is a \nvaluable feature of a concurrent implementation. 7 }; 7 }; (a) CAS. (b) k-CAS. Figure 5. Wait-free CAS \nand k-CAS state structures. Without any particular dif.culty, but with a somewhat lengthy argument, \none can show that the k-stack algorithm is lock-free by showing that whenever a thread retries an operation, \nanother thread completes its operation ensuring progress of at least one thread. 8.2 k-Stuttering shared \ncounters We implemented the two versions of a stuttering k-shared counter, as discussed in the introduction. \nThe .rst version is based on a k-relaxed stuttering version of a wait-free software CAS opera\u00adtion [13] \n(k-CAS for short). It uses a structure blk original, shown in Figure 5(a), to keep track of the state \nof concurrent CAS operations. The atomic value is located in .eld V and the CAS oper\u00adation uses the decision \n.elds X, Y, and C to determine which thread gets permission to change V. We modi.ed the blk original \nstruc\u00adture so that the .elds X, Y, and C are arrays of size k depicted in structure blk modified in Figure \n5(b). We keep the main CAS op\u00aderation unmodi.ed but use a balancing function that maps threads to array \nindices i smaller than k (the thread ID modulo k). A thread determines the state of its CAS operation \nby just accessing posi\u00adtion i in the arrays X, Y, and C. On success, a thread writes the new value into \nV. Hence, up to k concurrent threads may perform the k-CAS operation in parallel and change V resulting \nin a loss of at most k \u00b4 1 state changes, which further results in at most k \u00b4 1 lost shared counter \nupdates. The second version of the k-shared counter is the k-distributed counter depicted in Figure 1(b). \nIt is not dif.cult to show that both implementations are lineariz\u00adable with respect to the k-stuttering \nshared counter and they are lock-free. 9. Experiments We evaluate the performance and scalability of \nthe k-stack, several existing quantitatively relaxed FIFO queues, and the k-stuttering shared counter \nimplementations. All experiments ran on an Intel\u00adbased server machine with four 10-core 2.0GHz Intel \nXeon proces\u00adsors (40 cores, 2 hyperthreads per core), 24MB shared L3-cache, and 128GB of memory running \nLinux 2.6.39. We implemented a benchmarking framework to analyze our k-stack and k-shared counter implementations, \nas well as the implementations of relaxed queues. The benchmarking framework can be con.gured for a dif\u00adferent \nnumber of threads (n), number of operations each thread per\u00adforms (o), and the computational load performed \nbetween each op\u00aderation (c). The computational load between two consecutive op\u00aderations is created by \niteratively calculating p and c is the number of iterations performed. We use c 2000 which takes a total \nof 4600ns on average in our experiments. The framework uses static preallocation for memory used at runtime \nwith touching each page before running the benchmark to avoid paging issues. 9.1 k-Stack We compare our \nk-stack implementation with a standard lock\u00adbased stack (LS), which acquires a global lock for each stack \nop\u00aderation, and a non-blocking stack (NS) [20], which uses a CAS operation to manipulate the top pointer \nof the stack. Moreover, we compare our k-stack with different pools. The lock-free (BAG) [19] 9000 \n7000  5000 4500 8000 6000 4000 7000 operations/ms (more is better) 5000 3500 3000 4000 operations/ms \n(more is better) operations/ms (more is better) 2500 3000 2000 15002000 1000 2000 1000 500 1000 0 0 2 \n10 20 30 40 50 60 70 80 threads  (a) Stack(b) FIFO queue(c) Shared counter Figure 6. Benchmarks on a \n40-core (2 hyperthreads per core) server with an increasing number of threads 9000 7000 4500 8000 4000 \n6000 7000 3500   5000 4000 3000 2000 3000 2500 2000 1500 2000 1000 1000 1000 500 0 0 0k (logscale) \nk (logscale) k (a) Stack(b) FIFO queue(c) Shared counter Figure 7. Benchmarks on a 40-core (2 hyperthreads \nper core) server with increasing k pool is based on thread-local lists of elements. Threads put ele\u00adments \non their local list and take elements from their local list if it is not empty. If it is empty they take \nelements from the lists of other threads. The lock-free elimination-diffraction pool (ED) [1] uses a \nset of FIFO queues to store elements. Access to these queues is balanced using elimination arrays and \na diffraction tree. The syn\u00adchronous rendezvousing pool (RP) [3] implements a single elimina\u00adtion array \nusing a ring buffer. A get operation marks a slot identi.ed by its thread id and waits for a take operation \nto insert an element. Take operations scan the array for pending get operations. Figure 6(a) depicts \nthe performance analysis of our k-stack. We con.gure k 80, which is a good k (on average) for a broad \nrange of thread combinations and workloads on our server machine. This is no surprise since the server \nmachine has (logically) 80 cores. The analysis is done on a producer-consumer workload where half of \nthe threads are producers and half are consumers. The k-stack outscales and outperforms all considered \nstack and pool implemen\u00adtations. Figure 7(a) shows the effect of k on performance and scala\u00adbility. There \nexists an optimal k with respect to performance, which is also robust in the sense that there exists \nonly a single range of close-to-optimal k. For large k performance decreases due to higher sequential \noverhead, e.g. scanning for elements in almost empty k\u00adsegments. Note that an increase in performance \nabove k 80 is not to be expected on the given architecture.  9.2 Quantitatively relaxed FIFO queues \nWe also evaluate the existing implementations of a k-FIFO queue [11] and different FIFO queues, quasi-linearizable \nFIFO queues, and the pools introduced in the previous section. The k-FIFO queue [11] implements a restricted \nout-of-order k-queue as a lock-free linked list of k-segments. An enqueue operation is served by the \ntail k-segment and a dequeue operation is served by the head k-segment. Hence, up to k enqueue and k \ndequeue operations may be performed in parallel. The k-FIFO queue is empty if head and tail point to \nthe same k-segment which does not contain any el\u00adements. The lock-based (strict) FIFO queue (LB) locks \na global lock for each queue operation. The lock-free Michael-Scott (strict) FIFO queue (MS) [14] uses \nCAS operations to change head, tail, and next pointer in a linked list of elements. The .at-combining \n(strict) FIFO queue (FC) [8] is based on the approach that a single thread performs the queue operations \nof multiple threads by locking the whole queue, collecting pending queue operations, and applying them \nto the queue. The Random Dequeue Queue (RD) [2] imple\u00adments a quasi-linearizable FIFO with quasi-factor \nr where r de.nes the range r0, r \u00b4 1s of a random number. It actually implements a restricted out-of-order \nr-FIFO queue. RD is based on MS where the dequeue operation was modi.ed in a way that the random number \ndetermines which element is returned starting from the oldest el\u00adement. The Segment Queue (SQ) [2] is \na quasi-linearizable FIFO queue with quasi-factor s. It is logically similar (both implement a queue \nof segments and hence a restricted out-of-order queue) to the k-FIFO queue but does not provide a linearizable \nempty check, i.e., it may return null in the not-empty state. Also, SQ comes with a different segment \nmanagement strategy than the k-FIFO queue, which results on average in signi.cantly more CAS operations. \n Figure 6(b) depicts the performance analysis of the queues. We con.gure k r s 40, which turns out \nbe a good k (on average) for a broad range of thread combinations and workloads on our server machine. \nThis is no surprise since then the level of possible parallelism is 2k 80, the number of (logical) cores. \nWe use a producer-consumer workload where half of the threads are producers and half consumers. The k-FIFO \nqueue outscales and outperforms all considered FIFO queue, quasi-linearizable FIFO queue, and pool implementations. \nFigure 7(b) shows the effect of k on performance and scalability. Again, there exists a robust and optimal \nk with respect to performance. Also here, for large k performance decreases due to larger sequential \noverhead.  9.3 k-Shared counter We compare our k-CAS-based shared counter and distributed shared counter \nimplementations with a shared counter implemen\u00adtation based on a regular CAS operation depicted in Figure \n6(c). The threads perform in total one million counter increment opera\u00adtions in each benchmark run. The \nCAS version performs best until 30 threads. After that the k-CAS-based shared counter and the dis\u00adtributed \nshared counter version outperform it. Figure 7(c) shows the effect of k on performance and scalability. \nIn the k-CAS ver\u00adsion performance monotonically increases with larger k, whereas in the distributed shared \ncounter version performances decreases until k 10, but monotonically increases after that. Our educated \nguess is that this is caused by the trade-off between two possible sources of contention: (1) CAS on \nthe same memory location, and (2) bad caching, i.e., accessing many different locations in mem\u00adory. The \ndistributed shared counter decreases (1) but increases (2). However, except for small values of k, we \nobserve that the gain is larger than the loss. 10. Final remarks We have presented a framework for quantitative \nrelaxation of con\u00adcurrent data structures together with generic as well as further con\u00adcrete instances \nof it. Our main motivation is the belief that relaxed data structures may decrease contention and thus \nprovide the po\u00adtential for scalable and well-performing implementations. Indeed, the potential advantage \nwhich we demonstrate utilizable is striking. The lessons learned can be summarized as follows: The way \nfrom a sequential implementation to ef.cient concurrent implementation is always hard. Just because a \nsequential speci.cation is relaxed, it does not necessarily mean that an ef.cient implementation immedi\u00adately \nfollows. However, ef.cient implementations that bene.t from quantitative relaxations are possible, as \nwe demonstrate in this pa\u00adper. In our opinion, the framework provides a .rm formal ground for quantitative \nrelaxation of concurrent data structures and paves the road to designing ef.cient concurrent implementations. \nOur current results open up several directions for future work. One important issue is the applicability \nof relaxed data structures. Demonstrating applicability can either be achieved by exploring applications \nthat tolerate a relaxation, e.g. provide less accurate but nevertheless acceptable results, or showing \nthat end-to-end quality may remain the same despite the actual relaxation of semantics. In the latter \ncase, relaxations do not in.uence correctness in the sense of [16, 17]. Another evident but dif.cult \ngoal would be to synthesize well-performing implementations from relaxations. As a .rst step we believe \nit is important to study the main principles that lead to good performance. This is another line of future \nwork that we plan to undertake in small steps. Acknowledgements This work has been supported by the European \nResearch Council advanced grant QUAREM, the National Research Network RiSE on Rigorous Systems Engineering \n(Austrian Science Fund S11404-N23), and an Elise Richter Fellowship (Austrian Science Fund V00125). We \nthank the anonymous referees for their constructive and inspiring comments and suggestions. Ana Sokolova \nwishes to thank Dexter Kozen and in particular Joel Ouaknine: had they not saved her life, she would \nhave missed a lot of the fun involved in working on this paper and seeing it .nished. References [1] \nY. Afek, G. Korland, M. Natanzon, and N. Shavit. Scalable producer\u00adconsumer pools based on elimination-diffraction \ntrees. In Proc. Euro\u00ad pean Conference on Parallel Processing (Euro-Par), pages 151 162. Springer, 2010. \n[2] Y. Afek, G. Korland, and E. Yanovsky. Quasi-linearizability: Relaxed consistency for improved concurrency. \nIn Proc. Conference on Prin\u00adciples of Distributed Systems (OPODIS), pages 395 410. Springer, 2010. [3] \nY. Afek, M. Hakimi, and A. Morrison. Fast and scalable rendezvous\u00ading. In Proc. International Conference \non Distributed Computing (DISC), pages 16 31, Berlin, Heidelberg, 2011. Springer-Verlag. [4] J. Aspnes, \nM. Herlihy, and N. Shavit. Counting networks. Journal of the ACM, 41:1020 1048, 1994. [5] H. Attiya, \nR. Guerraoui, D. Hendler, P. Kuznetsov, M. Michael, and M. Vechev. Laws of order: expensive synchronization \nin concurrent algorithms cannot be eliminated. In Proc. of Principles of Program\u00adming Languages (POPL), \npages 487 498. ACM, 2011.  [6] M. Herlihy and N. Shavit. The Art of Multiprocessor Programming. Morgan \nKaufmann Publishers Inc., 2008. [7] M. Herlihy and J. Wing. Linearizability: a correctness condition \nfor concurrent objects. ACM Transactions on Programming Languages and Systems (TOPLAS), 12(3):463 492, \n1990. [8] D. H. I. Incze, N. Shavit, and M. Tzafrir. Flat combining and the synchronization-parallelism \ntradeoff. In Proc. Symposium on Paral\u00adlelism in Algorithms and Architectures (SPAA), pages 355 364. ACM, \n2010. [9] C. Kirsch and H. Payer. Incorrect systems: It s not the problem, it s the solution. In Proc. \nDesign Automation Conference (DAC). ACM, 2012. [10] C. Kirsch, H. Payer, H. R \u00a8ock, and A. Sokolova. \nBrief announcement: Scalability versus semantics of concurrent FIFO queues. In Proc. Symposium on Principles \nof Distributed Computing (PODC). ACM, 2011. [11] C. Kirsch, M. Lippautz, and H. Payer. Fast and scalable \nk-.fo queues. Technical Report 2012-04, Department of Computer Sciences, Uni\u00adversity of Salzburg, June \n2012. [12] C. Kirsch, H. Payer, H. R \u00a8ock, and A. Sokolova. Performance, scalabil\u00adity, and semantics \nof concurrent FIFO queues. In Proc. International Conference on Algorithms and Architectures for Parallel \nProcessing (ICA3PP), pages 273 287. LNCS 7439, 2012. [13] V. Luchangco, M. M., and N. Shavit. On the \nuncontended complex\u00adity of consensus. In Proc. International Symposium on Distributed Computing (DISC), \npages 45 59. Springer-Verlag, 2003. [14] M. Michael and M. Scott. Simple, fast, and practical non-blocking \nand blocking concurrent queue algorithms. In Proc. Symposium on Principles of Distributed Computing (PODC), \npages 267 275. ACM, 1996. [15] M. Michael, M. Vechev, and V. Saraswat. Idempotent work stealing. In Proc. \nPrinciples and Practice of Parallel Programming (PPoPP), pages 45 54. ACM, 2009. [16] S. Misailovic, \nS. Sidiroglou, H. Hoffmann, and M. C. Rinard. Quality of service pro.ling. In Proc. 32nd ACM/IEEE International \nConfer\u00adence on Software Engineering (ICSE) -Volume 1, pages 25 34. ACM, 2010. [17] A. Sampson, W. Dietl, \nE. Fortuna, D. Gnanapragasam, L. Ceze, and D. Grossman. Enerj: approximate data types for safe and general \nlow\u00adpower computation. In Proc. 32nd ACM SIGPLAN Conference on Programming Language Design and Implementation \n(PLDI), pages 164 174. ACM, 2011.  [18] N. Shavit. Data structures in the multicore age. Communications \nACM, 54:76 84, March 2011. [19] H. Sundell, A. Gidenstam, M. Papatrianta.lou, and P. Tsigas. A lock\u00adfree \nalgorithm for concurrent bags. In Proc. Symposium on Parallelism in Algorithms and Architectures (SPAA), \npages 335 344, New York, NY, USA, 2011. ACM. [20] R. Treiber. Systems programming: Coping with parallelism. \nTechnical Report RJ5118, IBM Almaden Research Center, April 1986.   \n\t\t\t", "proc_id": "2429069", "abstract": "<p>There is a trade-off between performance and correctness in implementing concurrent data structures. Better performance may be achieved at the expense of relaxing correctness, by redefining the semantics of data structures. We address such a redefinition of data structure semantics and present a systematic and formal framework for obtaining new data structures by quantitatively relaxing existing ones. We view a data structure as a sequential specification S containing all \"legal\" sequences over an alphabet of method calls. Relaxing the data structure corresponds to defining a distance from any sequence over the alphabet to the sequential specification: the k-relaxed sequential specification contains all sequences over the alphabet within distance k from the original specification. In contrast to other existing work, our relaxations are semantic (distance in terms of data structure states). As an instantiation of our framework, we present two simple yet generic relaxation schemes, called out-of-order and stuttering relaxation, along with several ways of computing distances. We show that the out-of-order relaxation, when further instantiated to stacks, queues, and priority queues, amounts to tolerating bounded out-of-order behavior, which cannot be captured by a purely syntactic relaxation (distance in terms of sequence manipulation, e.g. edit distance). We give concurrent implementations of relaxed data structures and demonstrate that bounded relaxations provide the means for trading correctness for performance in a controlled way. The relaxations are monotonic which further highlights the trade-off: increasing k increases the number of permitted sequences, which as we demonstrate can lead to better performance. Finally, since a relaxed stack or queue also implements a pool, we actually have new concurrent pool implementations that outperform the state-of-the-art ones.</p>", "authors": [{"name": "Thomas A. Henzinger", "author_profile_id": "81100034124", "affiliation": "IST Austria, Klosterneuburg, Austria", "person_id": "P3977977", "email_address": "tah@ist.ac.at", "orcid_id": ""}, {"name": "Christoph M. Kirsch", "author_profile_id": "81100151701", "affiliation": "University of Salzburg, Salzburg, Austria", "person_id": "P3977978", "email_address": "christoph.kirsch@cs.uni-salzburg.at", "orcid_id": ""}, {"name": "Hannes Payer", "author_profile_id": "81329491327", "affiliation": "University of Salzburg, Salzburg, Austria", "person_id": "P3977979", "email_address": "hannes.payer@cs.uni-salzburg.at", "orcid_id": ""}, {"name": "Ali Sezgin", "author_profile_id": "81508697355", "affiliation": "IST Austria, Klosterneuburg, Austria", "person_id": "P3977980", "email_address": "asezgin@ist.ac.at", "orcid_id": ""}, {"name": "Ana Sokolova", "author_profile_id": "81100005085", "affiliation": "University of Salzburg, Salzburg, Austria", "person_id": "P3977981", "email_address": "anas@cs.uni-salzburg.at", "orcid_id": ""}], "doi_number": "10.1145/2429069.2429109", "year": "2013", "article_id": "2429109", "conference": "POPL", "title": "Quantitative relaxation of concurrent data structures", "url": "http://dl.acm.org/citation.cfm?id=2429109"}