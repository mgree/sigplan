{"article_publication_date": "01-23-2013", "fulltext": "\n Optimizing Data Structures in High-Level Programs New Directions for Extensible Compilers based on Staging \nTiark Rompf * Arvind K. Sujeeth Nada Amin* Kevin J. Brown Vojin Jovanovic* HyoukJoong Lee Manohar Jonnalagedda* \nKunle Olukotun Martin Odersky* *EPFL: {.rst.last}@ep..ch Oracle Labs Stanford University: {asujeeth, \nkjbrown, hyouklee, kunle}@stanford.edu Abstract High level data structures are a cornerstone of modern \nprogram\u00adming and at the same time stand in the way of compiler optimiza\u00adtions. In order to reason about \nuser or library-de.ned data struc\u00adtures, compilers need to be extensible. Common mechanisms to extend \ncompilers fall into two categories. Frontend macros, stag\u00ading or partial evaluation systems can be used \nto programmatically remove abstraction and specialize programs before they enter the compiler. Alternatively, \nsome compilers allow extending the in\u00adternal workings by adding new transformation passes at different \npoints in the compile chain or adding new intermediate represen\u00adtation (IR) types. None of these mechanisms \nalone is suf.cient to handle the challenges posed by high level data structures. This pa\u00adper shows a \nnovel way to combine them to yield bene.ts that are greater than the sum of the parts. Instead of using \nstaging merely as a front end, we implement in\u00adternal compiler passes using staging as well. These internal \npasses delegate back to program execution to construct the transformed IR. Staging is known to simplify \nprogram generation, and in the same way it can simplify program transformation. De.ning a trans\u00adformation \nas a staged IR interpreter is simpler than implementing a low-level IR to IR transformer. With custom \nIR nodes, many opti\u00admizations that are expressed as rewritings from IR nodes to staged program fragments \ncan be combined into a single pass, mitigating phase ordering problems. Speculative rewriting can preserve \nopti\u00admistic assumptions around loops. We demonstrate several powerful program optimizations using this \narchitecture that are particularly geared towards data structures: a novel loop fusion and deforestation \nalgorithm, array of struct to struct of array conversion, object .attening and code generation for heterogeneous \nparallel devices. We validate our approach using several non trivial case studies that exhibit order \nof magnitude speedups in experiments. Categories and Subject Descriptors D.3.4 [Programming Lan\u00adguages]: \nProcessors Code generation, Optimization, Run-time environments; D.1.3 [Programming Techniques]: Concurrent \nPro\u00adgramming Parallel programming General Terms Design, Languages, Performance Keywords Staging, Code \nGeneration, Data Structures, Extensible Compilers Permission to make digital or hard copies of all or \npart of this work for personal or classroom use is granted without fee provided that copies are not made \nor distributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. POPL 13, January 23 25, 2013, Rome, Italy. Copyright c &#38;#169; \n2013 ACM 978-1-4503-1832-7/13/01. . . $10.00 // Vectors object Vector { def fromArray[T:Numeric](a: Array[T]) \n= new Vector { val data = a } def zeros[T:Numeric](n: Int) = Vector.fromArray(Array.fill(n)(i => zero[T])) \n} abstract class Vector[T:Numeric] { val data: Array[T] def +(that: Vector[T]) = Vector.fromArray(data.zipWith(that.data)(_ \n+ _)) ... } // Matrices abstract class Matrix[T:Numeric] { ... } // Complex Numbers case class Complex(re: \nDouble, im: Double) { def +(that: Complex) = Complex(re + that.re, im + that.im) def *(that: Complex) \n= ... } // (elided Numeric[Complex] type class implementation) Figure 1. Skeleton of a high-level Scala \nlinear algebra package 1. Introduction Compiling high-level programs to ef.cient low-level code is hard, \nparticularly because such programs use and de.ne high-level ab\u00adstractions. The compiler cannot see through \nabstractions ( abstrac\u00adtion penalty ), and it cannot reason about domain-speci.c prop\u00aderties ( general \npurpose bottleneck ). Among the most important abstractions are data structure and collection operations, \nand those also commonly present the most dif.culties to an optimizing com\u00adpiler. Let us consider an example \nof high level programming in Scala. We would like to implement a dense linear algebra package. Fig\u00adure \n1 shows a skeleton implementation of Vectors and Matrices as a thin layer over Arrays, using high-level \ncollection operations (.ll, zipWith) internally. Vectors and Matrices contain numeric values (type class \nNumeric). We also de.ne Complex numbers as a new numeric type. With these de.nitions at hand, we can \nwrite programs like the following: def diag(k:Int) = k Matrix.identity(n) * val m1 = (v1+v2).transpose \n(v1+v2) * val m2 = diag(l) if (scale) println(m1*m2) // == m1*(k*id) == k*m1*id == k*m1 else println(m1) \n// no need to compute m2 This code is elegant and high level, demonstrating how Scala s fo\u00adcus on abstraction \nand generalization increases development pro\u00adductivity. But unfortunately the code will run very slowly, \none or two orders of magnitude slower than a tuned low-level implemen\u00adtation using just arrays and while \nloops (see Section 5). What exactly is going wrong? Some of the reasons are:  1. Neither the Scala compiler \nnor the just-in-time compiler inside the JVM can apply generic optimizations, like common subex\u00adpression \nor dead code elimination (CSE, DCE), to non-trivial matrix or vector computations. 2. The involved compilers \nhave no notion of domain-speci.c laws like m*id=m (where m is a matrix and id the identity matrix), which \ncould be used for optimization. 3. Programming in a functional programming style creates lots of intermediate \nobjects. 4. The uniform heap-allocated object representation on the JVM is inef.cient for complex numbers. \n In order to enable a compiler to reason about programs with high-level abstractions, we need mechanisms \nto extend the com\u00adpiler so that it is able to resolve those abstractions. There are two common approaches. \nThe .rst is to translate the abstractions away before the program reaches the compiler proper, so that \nthe com\u00adpiler does not need to be aware of them. This is the idea behind macro systems and staging. Partial \nevaluation (PE) can also be used to specialize programs before they reach the actual compiler. The second \noption is to teach the compiler new domain-speci.c rules. Usually, compiler extensibility is understood \nas a means to add new phases. Some extensible compilers also allow adding new IR types but often it is \nnot clear how new nodes interact with existing generic optimizations. However, neither of these approaches \nalone is suf.cient to han\u00addle the challenges posed by high-level data structures and abstrac\u00adtions. Going \nback to our example, if all we have is staging or macros, then the expression m * id, which is equivalent \nto m, will be expanded into while loops before it even reaches the compiler, so no simpli.cation to m \ncan take place. In general, limited forms of simpli.cation can be added (see C++ expression templates \n[51]) but to be fully effective, the full range of generic compiler optimiza\u00adtions (CSE, constant propagation, \netc) would need to be duplicated, too. If on the other hand all we have is a facility to add new compiler \npasses, then we can add an optimization pass that simpli.es m*id to m, but we need another pass that \nexpands matrix multiplications into loops. These passes need to be implemented as low-level IR to IR \ntransformations, which is much more dif.cult than the macro or staging approach that can use regular \n(multi-stage) computation to express the desired target code. Implementing optimizations as separate \npasses also leads to phase ordering problems if multiple optimizations are added independently. We argue \nthat what we really want is the best of both worlds: On the one hand, we want to treat operations symbolically \nso that they can be matched by transformation rules. But on the other hand we also want the power of \nstaging to programmatically de.ne the result of a transformation. In addition to that, we need a way \nto de.ne transformations independently but avoid phase ordering problems when optimizations are applied. \nContributions In this paper, we present an extensible compiler architecture that solves this challenge, \nwhile still keeping the pro\u00adgramming effort required to express optimizations and transforma\u00adtions low. \nOur approach achieves large speedups on high-level pro\u00adgrams by fusing collection operations, changing \ndata layout and applying further optimizations on high-level objects, enabled by intermediate languages \nwith staging and a facility to combine inde\u00adpendently speci.ed optimizations without phase ordering issues. \nTo illustrate at a high level how our system works, consider again the linear algebra case. We use staging \nto obtain an inter\u00admediate representation from the initial program. In order to avoid the problems of \nthe pure staging or macro approach, we apply op\u00adtimizations at different levels, and we combine as many \noptimiza\u00adtions as possible together in a single pass to avoid many of the traditional phase-ordering \nproblems. Linear algebra optimizations are implemented by the library author as rewrite rules that rewrite \nsymbolic expressions into staged program fragments. The system applies rewriting speculatively using \noptimistic assumptions and rolls back intermediate transformations when they later turn out to be unsound. \nThis strategy eliminates phase ordering problems that are due to one optimization module having to make \npessimistic as\u00adsumptions about the outcome of another one. Once no further sim\u00adpli.cation is possible \non the linear algebra level, we want to switch to a lower level representation which consists of arrays \nand loops. We implement this kind of lowering transform as a staged inter\u00adpreter over the intermediate \nprogram. Since the interpreter again uses staging, it constructs a new program and thus acts as a pro\u00adgram \ntransformer. By default, this interpreter maps each expres\u00adsion to a structurally equivalent one. The \nlibrary author extends it to map linear algebra operations to their lower level representation. On this \nlower level, the system applies another set of optimizations (e.g. loop fusion) in a combined pass, re-applying \nglobal optimiza\u00adtions to take advantage of new opportunities exposed by changing the representation. \nThis process can be repeated for any desired number of lowering steps. In particular, we make the following \ncontributions: We use staging to build extensible multi-pass compilers that can also pro.tably combine \nmodular optimizations into single passes: Staged IR interpreters act as IR transformers and spec\u00adulative \nrewriting allows combining independently speci.ed op\u00adtimization while keeping optimistic assumptions. \n We use a graph-based intermediate representation that may con\u00adtain structured, compound expressions. \nSplitting and merging compound expressions allows reusing existing optimization on their pieces (e.g. \nDCE to remove unused parts of a data struc\u00adture). This approach extends to powerful data format conver\u00adsions \n(e.g. array-of-struct to struct-of-array).  We present a novel data parallel loop fusion algorithm that \nuni\u00adformly handles horizontal and vertical fusion and also includes asymmetric traversals (flatMap, groupBy). \n We demonstrate how this compiler architecture can solve tough optimization problems related to data \nstructures in a number of nontrivial case studies.  We build on our previous work on Lightweight Modular \nStaging (LMS) [39] and highlight similarities and differences to previous work as we go along. The speculative \nrewriting approach is based on earlier work by Lerner, Grove and Chambers [29]. Organization We start \nout by reviewing partial evaluation and staging (\u00a72). Insights from this section will help understand \nhow we use staging for program transformation (\u00a73), where we .rst present symbolic optimizations using \nspeculative rewriting (\u00a73.2) before delving into lowering passes as staged IR interpreters (\u00a73.3). A \nthird transformation we explore is the splitting and merging of compound expressions to reuse existing \noptimizations (\u00a73.4). We then present how these techniques can be used to perform data structure optimizations \n(\u00a74): our generic staged struct abstraction (\u00a74.1), which extends to unions and inheritance (\u00a74.2), an \narray of struct to struct of array transform (\u00a74.3), and loop fusion (\u00a74.4). We then present a set of \ncase studies where our transformations are particularly appealing (\u00a75): linear algebra (\u00a75.1), regular \nexpres\u00adsion matching (\u00a75.2), collection and query operations (\u00a75.3), and string templating (\u00a75.4). Finally \nwe discuss our results (\u00a76), review related work (\u00a76.1) and conclude (\u00a76.2). 2. Background Many computations \ncan naturally be separated into stages distin\u00adguished by frequency of execution or availability of information. \nMulti-stage programming (MSP, staging for short) as established by Taha and Sheard [46] make the stages \nexplicit and allows pro\u00adgrammers to delay evaluation of a program expression to a later stage (thus, \nstaging an expression). The present stage effectively acts as a code generator that, when run, produces \nthe program of the next stage.  Staging is closely related to partial evaluation [20], which spe\u00adcializes \nprograms to statically known parts of their input. For the purpose of this paper, we can treat partial \nevaluation (and in partic\u00adular binding-time analysis (BTA)) as automatic staging and staging as programmer-controlled \npartial evaluation. A key difference is that partial evaluation strictly specializes programs and usually \ncomes with soundness guarantees whereas adding staging annotations to a program in an MSP language such \nas MetaOCaml [7] provides more freedom for composing staged fragments but requires some care so as not \nto change the computed result. Much of the research on staging and partial evaluation was driven by the \ndesire to simplify compiler development. For exam\u00adple, specializing an interpreter to a particular program \nyields a com\u00adpiled program with the interpreter overhead removed (the .rst Fu\u00adtamura projection [15]). \nSelf applicable partial evaluators can gen\u00aderate compilers from interpreters and compiler generators. \nThe exposition in this paper uses Scala and Lightweight Modu\u00adlar Staging (LMS) [39], a library-only staging \napproach. Contrary to dedicated MSP languages based on quasi quotation, LMS uses only types to distinguish \nthe computational stages. Expressions be\u00adlonging to the second stage have type Rep[T] in the .rst stage \nwhen yielding a computation of type T in the second stage. Expressions of a plain type T will be evaluated \nin the .rst stage and become constants in stage two. The plain Scala type system propagates in\u00adformation \nabout which expressions are staged and thus performs a semi-automatic local BTA. Thus, LMS shares some \nof the bene.ts of automatic PE and manual staging. Example: Zero-Overhead Traversal Abstractions Arrays \nin Scala are bare JVM arrays which need to be traversed using while loops and indices. The Scala standard \nlibrary provides an enrich\u00adment that adds a foreach method: array foreach { i => println(i) } Adding \nforeach is achieved using an implicit conversion: implicit def enrichArray[T](a: Array[T]) = new { def \nforeach(f: T => Unit): Unit = { var i = 0; while (i < a.length) { f(a(i)); i += 1 } } } This implementation \nhas non-negligible abstraction overhead (closure allocation, interference with JVM inlining, etc). We \nwould like to tell the compiler, whenever it sees a foreach invocation, to just put the while loop there \ninstead. This is a simple case where macros or staging can help. Using LMS, we just change the method \nargument types. Fig\u00adure 2 shows a set of staged array and vector operations. The LMS framework provides \noverloaded variants of many operations that lift those operations to work on Rep types, i.e. staged expressions \nrather than actual data. It is important to note the difference between types Rep[A=>B] (a staged function \nobject) and Rep[A]=>Rep[B] (a function on staged values). By using the latter, foreach ensures that the \nfunction pa\u00adrameter is always evaluated and unfolded at staging time. Macro systems that only allow lifting \nexpression trees support only types Rep[A=>B]. This limits expressiveness and there are no guarantees \nthat higher order functions are evaluated at staging time. In addition to the LMS framework, we use the \nScala-Virtualized compiler [32] which rede.nes several core language features as method calls and thus \nmakes them overloadable as well. For ex\u00adample, the code // Array implicit def enrichArray[T](a: Rep[Array[T]]) \n= new { def foreach(f: Rep[T] => Rep[Unit]): Rep[Unit] = { var i = 0; while (i < a.length) { f(a(i)); \ni += 1 } } def zipWith(b: Rep[Array[T]])(f: (Rep[T],Rep[T]) => Rep[T]) = Array.fill(a.length min b.length) \n{ i => f(a(i), b(i)) } } // Vector trait Vector[T] extends Struct { val data: Array[T] } implicit def \nenrichVector[T:Numeric](a: Rep[Vector[T]]) = new { def +(b: Rep[Vector[T]]) = Vector.fromArray(a.data.zipWith(b.data)(_ \n+ _)) ... } // (companion objects define Array.fill and Vector.fromArray) Figure 2. Staged Array and \nVector Ops. var i = 0; while (i < n) { i = i + 1 } will be desugared as follows: val i = __newVar(0); \n__while(i < n, { __assign(i, i + 1) }) Methods __newVar, __assign, __while are overloaded to work on \nRep types. These methods need to be suitably de.ned and made available in scope. Scala-Virtualized also \nprovides overloaded .eld access and object construction methods. In the declaration of vec\u00adtors or complex \nnumbers, extending Struct serves as a marker to automatically lift object construction and .eld accesses \nto the do\u00admain of Rep types. This means that staged .eld accesses such as v.data are transparently available \non Rep[Vector[T]] values and (in this case) would return Rep[Array[T]] values. Similarly, new Vector \n{ val data = /* type Rep[Array[T]] */ } will return a Rep[Vector[T]] object. Generic Programming with \nType Classes Figure 2 uses the type class Numeric to abstract over particular numeric types. The type \nclass pattern [55], which decouples data objects from generic dis\u00adpatch .ts naturally with a staged programming \nmodel. We can de\u00ad.ne a staged variant of the standard Numeric type class and, with ad\u00addition on numeric \nvectors de.ned in Figure 2, make vectors them\u00adselves instance of Numeric: class Numeric[T] { def num_plus[T](a:Rep[T],b:Rep[T]): \nRep[T] } implicit def vecIsNumeric[T:Numeric] = new Numeric[Vector[T]] { def num_plus(a: Rep[Vector[T], \nb: Rep[Vector[T]) = a + b } This allows us to pass, say, a staged Vector[Int] to any function that works \nover generic types T:Numeric, such as vector addition itself. The same holds for Vector[Vector[Int]]. \nWithout staging, type classes are implemented by passing an implicit dictionary, the type class instance, \nto generic functions. Here, type classes are a purely stage-time concept. All generic code is specialized \nto the concrete types and no type class instances exist (and hence no virtual dispatch occurs) when the \nstaged program is run. Maintaining Evaluation Order Compared to staging or macro systems based on quasi \nquotation, LMS preserves program seman\u00adtics in more cases. In particular, adding Rep types does not change \nthe relative evaluation order of statements within a stage. In compute() foreach { i => println(i) } \nthe staged foreach implementation from Figure 2 will evaluate compute() only once, whereas purely syntactic \nexpansion would produce this target code: while (i < compute().length) { println(compute()(i)); i += \n1 } LMS performs on-the .y ANF conversion, similar to earlier work on PE with effects [49]. Systems based \non quasi quotation could pro.tably use the same method to maintain evaluation order but we are unaware \nof any sustem that does. Instead, most other systems leave it up to the programmer to insert let-bindings \nin the right places, which can easily lead to subtle errors.  Limitations of Front-End Staging and Macros \nDespite the given bene.ts, for top performance it is often not suf.cient to use staging (or macros, or \npartial evaluation) as a front end. Let us consider a simple yet non-trivial example: val v1 = ... v1 \n+ Vector.zeros(n) Staging will replace the zero vector creation and the subsequent ad\u00addition with arrays \nand loops. What we would like instead, however, is to apply a symbolic simpli.cation rule v+zero->v to \nremove the zero addition before expansion takes place. Let us imagine that our system would allow us \nto implement the vector plus operation in such a way that it can inspect its arguments to look for invocations \nof Vector.zeros. This would cover the simple case above but we would still run into problems if we complicate \nthe use case slightly: val v1 = ... val v2 = Vector.zeros(n) v1 + v2 To handle programs like this, it \nis not suf.cient to just inspect the (syntactic) arguments. We need to integrate the staging expan\u00adsion \nwith some form of forward data .ow propagation, otherwise the argument to plus is just an identi.er. \nThe deeper problem is that we are forced to commit to a single data representation. Even if we combine \nstaging with an extensible compiler we need to make a decision: Should we treat vectors as symbolic entities \nwith algebraic laws, implemented as IR nodes amenable to optimization? Or should we stage them so that \nthe compiler sees just loops and arrays without abstraction overhead? The following sections will discuss \nmechanisms to integrate these approaches and for treating data structures in a more abstract way. 3. \nProgram Transformation via Staging Staging usually is a method for generating programs: A multi\u00adstage \nprogram builds an object program which is then compiled normally again. We show that staging is also \nuseful as a tool for transforming programs. Any transformation can be broken down into a traversal and \na generation part. Not surprisingly, staging helps with the generation part. In our case, internal compiler \npasses are IR interpreters that happen to be staged so that they return a new program as the result. \nThis way they can delegate back to program execution to build the result of a program transformation. \nStaging for program transformation has a number of bene.ts. First, building a (staged) IR interpreter \nis far simpler than building a non-trivial IR to IR transformer. Second, optimizations can be added gradually \nto a staged program, starting, e.g., with the code from Figure 2. Third, the program (or library) itself \nis in control of the translation and can in.uence what kind of code is generated. One of the key aspects \nof our approach is to distinguish two kinds of transforms: Optimizations and Lowerings. Lowerings translate \nprograms into a lower-level representation (e.g. linear algebra operations into arrays and loops). Lowerings \nhave a natu\u00adral ordering so they can be pro.tably arranged in separate passes. Optimizations, by contrast, \nhave no clear ordering and are prone to phase ordering problems. Thus, they need to be combined for max\u00adimum \neffectiveness. Also, most lowerings are mandatory whereas optimizations are usually optional. Of course \nthe distinction is not always clear cut but many transforms fall into only one of the cate\u00adgories. In \nany case, it is important that all applicable optimizations are applied exhaustively before lowering \ntakes place. Otherwise, high-level optimization opportunities may be missed. After a low\u00adering step, \nthere may be new opportunities for optimization. Thus, our system performs a sequence of optimization, \nlowering, opti\u00admization steps, until the lowest-level representation is reached. The .nal representation \nis unparsed to target code. We recap the LMS extensible IR (\u00a73.1) and .rst consider opti\u00admizations (\u00a73.2), \nthen lowerings (\u00a73.3). Afterwards we descibe the treatment of compound expressions (\u00a73.4). 3.1 The LMS \nExtensible Graph IR We now turn to the level of primitive staged operations. Using LMS we do not directly \nproduce the second stage program in source form but instead as an extensible intermediate representation \n(IR). We refer the reader to our previous work for details on the IR [37 39, 41] but give a short recap \nhere. The overall structure is that of a sea of nodes dependency graph [9]. In Figure 3, which will be \nthe running example for this sec\u00adtion, we recast the vector implementation from Figure 2 in terms of \ncustom IR nodes. The user-facing interface is de.ned in trait Vectors, with abstract methods (vec_zeros, \nvec_plus) that are im\u00adplemented in trait VectorsExp to create IR nodes of type VectorZeros and VectorPlus, \nrespectively. The framework provides IR base classes via trait BaseExp, mixed into VectorsExp (but not \nVectors, to keep the IR hidden from user code). Expressions are atomic: abstract class Exp[T] case class \nConst[T](x: T) extends Exp[T] case class Sym[T](n: Int) extends Exp[T] Trait BaseExp de.nes Rep[T]=Exp[T], \nwhereas Rep[T] is left as an abstract type in trait Base. Custom (composite) IR nodes extend Def[T]. \nThey refer to other IR nodes only via symbols. There is also a type Block[T] to de\u00ad.ne nested blocks \n(not used in Figure 3). Taking a closer look at vec_zero reveals that its expected return type is Exp[T] \nbut the re\u00adsult value is of type Def[T]. This conversion is achieved implicitly by toAtom: implicit def \ntoAtom[T](d: Def[T]): Exp[T] = reflectPure(d) Method reflectPure maintains the correct evaluation order \nby bind\u00ading the argument d to a fresh symbol (on the .y ANF conversion). def reflectPure[T](d: Def[T]): \nSym[T] def reifyBlock[T](b: =>Exp[T]): Block[T] The counterpart reifyBlock (note the by-name argument) \ncaptures performed statements into a block object. Additional re.ect meth\u00adods exist to mark IR nodes \nwith various kinds of side effects (see [41] for details).  3.2 Combining Optimizations: Speculative \nRewriting Many optimizations that are traditionally implemented using an it\u00aderative data.ow analysis \nfollowed by a transformation pass can also be expressed using various .avors of (possibly context dependent) \nrewriting. Whenever possible we tend to prefer a rewriting variant because rewrite rules are easy to \nspecify separately and do not re\u00adquire programmers to de.ne abstract interpretation lattices. Rewrites \nand Smart Constructors LMS performs forward op\u00adtimizations eagerly while constructing the IR. Hash-consing \nim\u00adplements CSE and smart constructors apply pattern rewriting, in\u00adcluding various kinds of constant \npropagation. This can be seen as adding online PE to the existing of.ine stage distinction de\u00ad.ned by \nRep types [42, 45]. In Figure 3, trait VectorsExpOpt can be mixed in with VectorsExp and overrides vec_plus \nto simplify zero additions. Rewriting is integrated with other forward optimizations: Pattern matches \nof the form case Def(VectorZeros(n)) => ... will look up the available de.nition of a symbol. Here is \na simple ex\u00adample program, before (left) and after constant folding (middle): val x = 3 y val x = 3 y \nprintln(6 y) * * * println(2 x) println(6 y) * * DCE will later remove the multiplication and the binding \nfor x (right).  // Vector interface trait Vectors extends Base { // elided implicit enrichment boilerplate: \n// Vector.zeros(n) = vec_zeros(n), v1 + v2 = vec_plus(a,b) def vec_zeros[T:Numeric](n: Rep[Int]): Rep[Vector[T]] \ndef vec_plus[T:Numeric](a: Rep[Vector[T]], b: Rep[Vector[T]]): Rep[Vector[T]] } // low level translation \ntarget trait VectorsLowLevel extends Vectors { def vec_zeros_ll[T:Numeric](n: Rep[Int]): Rep[Vector[T]] \n= Vector.fromArray(Array.fill(n) { i => zero[T] }) def vec_plus_ll[T:Numeric](a: Rep[Vector[T]], b: Rep[Vector[T]]) \n= Vector.fromArray(a.data.zipWith(b.data)(_ + _)) } // IR level implementation trait VectorsExp extends \nBaseExp with Vectors { // IR node definitions and constructors case class VectorZeros(n: Exp[Int]) extends \nDef[Vector[T]] case class VectorPlus(a: Exp[Vector[T]],b: Exp[Vector[T]]) extends Def[Vector[T]] def \nvec_zeros[T:Numeric](n: Rep[Int]): Rep[Vector[T]] = VectorZeros(n) def vec_plus[T:Numeric](a: Rep[Vector[T]], \nb: Rep[Vector[T]]) = VectorPlus(a,b) // mirror: transformation default case def mirror[T](d: Def[T])(t: \nTransformer) = d match { case VectorZeros(n) => Vector.zeros(t.transformExp(n)) case VectorPlus(a,b) \n=> t.transformExp(a) + t.transformExp(b) case _ => super.mirror(d) } } // optimizing rewrites (can be \nspecified separately) trait VectorsExpOpt extends VectorsExp { override def vec_plus[T:Numeric](a:Rep[Vector[T]],b:Rep[Vector[T]])=(a,b)match{ \ncase (a, Def(VectorZeros(n))) => a case (Def(VectorZeros(n)), b) => b case => super.vecplus(a,b) _ _ \n} } // transformer: IR -> low level impl trait LowerVectors extends ForwardTransformer { val IR: VectorsExp \nwith VectorsLowLevel; import IR._ def transformDef[T](d: Def[T]): Exp[T] = d match { case VectorZeros(n) \n=> vec_zeros_ll(transformExp(n)) case VectorPlus(a,b) => vec_plus_ll(transformExp(a), transformExp(b)) \ncase _ => super.transformDef(d) } } Figure 3. Vector Implementation with IR and Lowering. Applying Rewrites \nSpeculatively Many optimizations are mutu\u00adally bene.cial. In the presence of loops, optimizations need \nto make optimistic assumptions for the supporting analysis to obtain best results. If multiple analyses \nare run separately, each of them ef\u00adfectively makes pessimistic assumptions about the outcome of all \nothers. Combined analyses avoid the phase ordering problem by solving everything at the same time. The \nchallenge, of course, is to automatically combine analyses and transformations that are im\u00adplemented \nindependently of one another. Since the forward optimizations described above are applied at IR construction \ntime where loop information is incomplete, LMS previously allowed rewrites and CSE only for purely functional \nop\u00aderations and not in the presence of imperative loops. These pes\u00adsimistic assumptions, together with \nmonolithic compound expres\u00adsions (see Section 3.4) prevented effective combinations of separate rewrites. \nLerner, Grove, and Chambers showed a method of compos\u00ading separately speci.ed optimizations by interleaving \nanalyses and transformations [29]. We use a modi.ed version of their algorithm that works on structured \nloops instead of CFGs and using depen\u00addency information and rewriting instead of explicit data .ow lat\u00adtices. \nTo the best of our knowledge, we are the .rst to extend this approach to extensible compilers and a purely \nrewriting based environment. Usually, rewriting is semantics preserving, i.e. pes\u00adsimistic. The idea \nis to drop that assumption. As a corollary, we need to rewrite speculatively and be able to rollback \nto a previous state to get optimistic optimization. The algorithm proceeds as fol\u00adlows: for each encountered \nloop, apply all possible transforms to the loop body, given empty initial assumptions. Analyze the result \nof the transformation: if any new information is discovered throw away the transformed loop body and \nretransform the original with updated assumptions. Repeat until the analysis result has reached a .xpoint \nand keep the last transformation as result. This process can be costly for deeply nested loops, but compares \nfavorably to the al\u00adternative of running independent transformations one after another until a global \n.xpoint is reached [29]. Here is an example of speculative rewriting, showing the un\u00adtransformed program \n(left), the initial optimistic iteration (middle), and the .xpoint (right) reached after the second iteration: \nvar x = 7 var x = 7 var x = 7 //dead var c = 0 var c = 0 var c = 0 while (c < 10) { while (true) { while \n(c < 10) { if (x < 10) print(\"!\") print(\"!\") print(\"!\") else x = c print(7) print(7) print(x) print(0) \nprint(c) print(c) c = 1 c += 1 c += 1 } } } This algorithm allows us to do all forward data .ow analyses \nand transforms in one uniform, combined pass driven by rewriting. In the example above, during the initial \niteration (middle), separately speci.ed rewrites for variables and conditionals work together to determine \nthat x=c is never executed. At the end of the loop body we discover the write to c, which invalidates \nour initial optimistic assumption c=0. We rewrite the original body again with the new information (right). \nThis time there is no additional knowledge discovered so the last speculative rewrite becomes the .nal \none. Speculative rewriting as used here is still a fully static transform not to be confused with speculative \ndynamic optimizations per\u00adformed inside virtual machines that may incur multiple pro.ling, compilation \nand deoptimization cycles.  3.3 Separate Lowering Passes: Transformers Previously LMS had a single code \ngeneration pass that scheduled the IR graph, performing DCE and code motion. We generalize this facility \nto allow arbitrary traversals of the program tree which re\u00adsults from scheduling the IR graph. Similar \nto other optimizations, DCE and code motion work on high-level (possibly composite) IR nodes. Generic \nIR Traversal and Transformation Once we have a traversal abstraction, transformation falls out naturally \nby building a traversal that constructs a new staged program. The implemen\u00adtation is shown in Figure \n4. Transformation needs a default case, which we call mirroring (see trait VectorsExp in Figure 3). Mir\u00adroring \nan IR node will call back to the corresponding smart con\u00adstructor, after applying a transformer to its \narguments. Mirroring loops is slightly more complicated than what is shown in Figure 3 because of the \nbound variable: case ArrayFill(n,i,y) => Array.fill(transformExp(n), { j => t.withSubst(i -> j) { transformBlock(y) \n} } Implementing Custom Transformers In our running example, we would like to treat linear algebra operations \nsymbolically .rst, with individual IR nodes like VectorZeros and VectorPlus. In Fig\u00adure 3, the smart \nconstructor vec_plus implements a rewrite that sim\u00adpli.es V+Z to V. CSE, DCE, etc. will all be performed \non these high level nodes. After all those optimizations are applied, we want to transform our operations \nto the low-level array implementation from Figure 2 in a separate lowering pass. Trait LowerVectors in \nFigure 3 imple\u00adments this transformation by delegating back to user-space code,  // traversal trait \nForwardTraversal { val IR: Expressions; import IR._ def traverseBlock[T](b: Block[T]): Unit = focusExactScope(block) \n{ stms => stms foreach traverseStm } def traverseStm[T](s: Stm[T]): Unit = blocks(stm.rhs) foreach traverseBlock \n} // transform trait ForwardTransformer extends ForwardTraversal { val IR: Expressions; import IR._ var \nsubst: Map[Exp[_],Exp[_]] def transformExp[T](s: Exp[T]): Exp[T] = // lookup s in subst def transformBlock[T](b: \nBlock[T]): Exp[T] = scopeSubst { traverseBlock(b); transformExp(b.res) } def transformDef[T](d: Def[T]): \nExp[T] = mirror(d, this) override def traverseStm(s: Stm[T]) = { val e = transformDef(s.rhs); subst \n+= (s.sym -> e); e } } Figure 4. Traversal and Transformer Interface. namely method vec_plus_ll in trait \nVectorsLowLevel. The result of the transform is a staged program fragment just like in Figure 2. This \nsetup greatly simpli.es the de.nition of the lowering trans\u00adform, which would otherwise need to assemble \nthe fill or zipWith code using low level IR manipulations. Instead we bene.t directly from the staged \nzipWith de.nition from Figure 2. Also, further rewrites will take place automatically. Essentially all \nsimpli.ca\u00adtions are performed eagerly, after each transform phase. Thus we guarantee that CSE, DCE, etc. \nhave been applied on high-level operations before they are translated into lower-level equivalents, on \nwhich optimizations would be much harder to apply. To give a quick example, the initial program val v1 \n= ... val v2 = Vector.zeros(n) val v3 = v1 + v2 v1 + v3 will become val v1 = ... Vector.fromArray(v1.data.zipWith(v1.data)(_ \n+ _)) after lowering (modulo unfolding of staged zipWith). Worklist Transformers and Delayed Rewriting \nTransformers can be extended with a worklist, which is useful if the result may contain terms that need \nfurther transformation. With a work\u00adlist transformer, we can register individual rewrites for particular \nnodes. Running the transformer applies the rewrites, which may register new replacements for the next \niteration. The process stops if no further work is required (empty work list). Delayed rewriting is a \nsimpli.ed interface that allows specify\u00ading lowerings together with the operations and the regular imme\u00addiate \nrewrites. This helps to reduce the boilerplate needed to de\u00ad.ne transformations. For example, here is \nthe VectorZeros lowering from Figure 3 recast as delayed rewrite: def vec_zeros[T:Numeric](n: Rep[Int]) \n= VectorZeros(n) atPhase(lowering) { Vector.fromArray(Array.fill(n) { i => zero[T] }) } The atPhase block \nis registered with worklist transformer lowering. Before lowering, the IR node remains a VectorZeros \nnode, which allows other smart constructor rewrites to kick in that expect this pattern.  3.4 Compound \nExpressions: Split and Merge Since our IR contains structured expressions like loops and condi\u00adtionals, \noptimizations need to reason about compound statements. This is not easy: for example, a simple DCE algorithm \nwill not be able to remove only pieces of a compound expression. Our solution is simple yet effective: \nWe eagerly split many kinds of compound statements, assuming optimistically that only parts will be needed. \nSplitting is implemented just like any other rewrite, and thus inte\u00adgrates well with other unrelated \noptimizations (see Section 3.2). We .nd out which parts are needed through the regular DCE algorithm \nand afterwards, we reassemble the remaining pieces. Effectful Statements A good example of statement \nsplitting is effectful conditionals: var a, b, c = ... var a, b, c = ... var a, c = ... if (c) { if (c) \na = 9 if (c) a = 9 a = 9; b = 1 if (c) b = 1 else c = 3 } else if (!c) c = 3 println(a+c) c = 3 println(a+c) \nprintln(a+c) From the conditional in the initial program (left), splitting creates three separate expressions, \none for each referenced variable (mid\u00addle). DCE removes the middle one because variable b is not used, \nand the remaining conditionals are merged back together (right). Of course successful merging requires \nto keep track of how expres\u00adsions have been split. An extension of this simple merge facility which attempts \nto merge expressions that may not have been split before is loop fusion (Section 4.4). Data Structures \nSplitting is also very effective for data struc\u00adtures, as often only parts of a data structure are used \nor modi.ed. This will be discussed in more detail in Section 4 below but here is already a quick example. \nAssume c1 and c2 are complex numbers: val c3 = if (test) c1 else c2 println(c3.re) The conditional will \nbe split for each .eld of a struct. Internally the above will be represented as: val c3re = if (test) \nc1re else c2re val c3im = if (test) c1im else c2im // dead val c3 = Complex(c3re, c3im) // dead println(c3re) \nThe computation of the imaginary component as well as the struct creation for the result of the conditional \nare never used and thus they will be removed by DCE. 4. Data Structure Optimizations High level data \nstructures are a cornerstone of modern program\u00adming and at the same time stand in the way of compiler \noptimiza\u00adtions. We illuminate the main issues compilers have with data struc\u00adtures as used by the two \ndominant programming paradigms. OOP Object oriented programming treats every data value as an object. \nThis is a powerful pattern that makes it easy to extend lan\u00adguages with new functionality. In Scala, \ne.g., it is easy to add a complex number class. But there is a price to be paid: allocating each complex \nnumber as a separate object will not perform well. Furthermore, if we are working with arrays of complex \nnumbers we get much better performance if we use a struct of array rep\u00adresentation. Staging and embedded \ncompilers allow us to abstract away the object abstraction and reason about individual pieces of data \nthat objects are composed of and possibly rearrange that data in more ef.cient ways. FP Functional programs \ncreate lots of intermediate results. This is particularly bad for collection operations. Theoretically, \ncom\u00adpilers can leverage referential transparency. But impure functional languages need sophisticated \neffects analysis, which is hard if there is a lot of abstraction. Our staged programs are much simpler \nbe\u00adcause abstraction is stripped away. We can do better and simpler effects analysis. A simple liveness \nanalysis can turn copying into in-place modi.cation. A novel loop fusion algorithm (data paral\u00adlel and \nasymmetric, includes flatMap and groupBy) removes many intermediate results (Section 4.4).  // generic \nstruct interface trait StructExp extends BaseExp { abstract class StructTag case class Struct[T](tag: \nStructTag, elems: Map[String,Rep[Any]]) extends Def[T] case class Field[T](struct: Rep[Any], key: String) \nextends Def[T] def struct[T](tag: StructTag, elems: Map[String,Rep[Any]]) = Struct(tag, elems) def field[T](struct: \nRep[Any], key: String): Rep[T] = struct match { case Def(Struct(tag, elems)) => elems(key).asInstanceOf[Rep[T]] \ncase _ => Field[T](struct, key) } } // splitting array construction case class ArraySoaTag(base: StructTag, \nlen: Exp[Int]) extends StructTag override def arrayFill[T](size: Exp[Int], v: Sym[Int], body: Def[T]) \n= body match { case Block(Def(Struct(tag, elems))) => struct[T](ArraySoaTag(tag,size), elems.map(p => \n(p._1, arrayFill(size, v, Block(p._2))))) case _ => super.arrayFill(size, v, body) } // splitting array \naccess override def infix_apply[T](a: Rep[Array[T]], i: Rep[Int]) = a match { case Def(Struct(ArraySoaTag(tag,len),elems)) \n=> struct[T](tag, elems.map(p => (p._1, infix_apply(p._2, i)))) case _ => super.infix_apply(a,i) } override \ndef infix_length[T](a: Rep[Array[T]]): Rep[Int] = a match { case Def(Struct(ArraySoaTag(tag, len), elems)) \n=> len case _ => super.infix_length(a) } Figure 5. Generic Struct interface and SoA Transform. 4.1 A \nGeneric Struct Interface Product types, i.e. records or structs are one of the core data struc\u00adture building \nblocks. It pays off to have a generic implementation that comes with common optimizations (Figure 5). \nTrait StructExp de.nes two IR node types for struct creation and .eld access. The struct creation node \ntakes a hash map that relates (static) .eld iden\u00adti.ers with (dynamic) values and a tag that can hold \nfurther in\u00adformation about the data representation. The method field tries to look up the desired value \ndirectly if the argument is a Struct node. With this struct abstraction we can implement the data structure \nsplitting and merging example from Section 3.4 by overriding the if-then-else smart constructor to create \na conditional for each .eld in a struct. 4.2 Unions and Inheritance The struct abstraction can be extended \nto sum types and inheritance using a tagged union approach [21, 34]. We add a clzz .eld to each struct \nthat refers to an expression that de.nes the object s class. Being a regular struct .eld, it is subject \nto all common optimizations. We extend the complex number example with two subtraits: trait Complex trait \nCartesian extends Complex with Struct { val re: Double; val im: Double } trait Polar extends Complex \nwith Struct { val r: Double; val phi: Double } Splitting transforms work as before: e.g. conditional \nexpressions are forwarded to the .elds of the struct. But now the result struct will contain the union \nof the .elds found in the two branches, inserting null values as appropriate. A conditional is created \nfor the clzz .eld only if the exact class is not known at staging time. Given straightforward factory \nmethods Cartesian and Polar, the expression val a = Cartesian(1.0, 2.0); val b = Polar(3.0, 4.0) if (x \n> 0) a else b produces this generated code: val (re, im, r, phi, clzz) = if (x > 0) (1.0, 2.0, null, \nnull, classOf[Cartesian]) else (null, null, 3.0, 4.0, classOf[Polar]) struct(\"re\"->re, \"im\"->im, \"r\"->r, \n\"phi\"->phi, \"clzz\"->clzz) The clzz .elds allows virtual dispatch via type tests and type casting, e.g. \nto convert any complex number to its cartesian repre\u00adsentation: def infix_toCartesian(c: Rep[Complex]) \n: Rep[Cartesian] = if (c.isInstanceOf[Cartesian]) c.asInstanceOf[Cartesian] else { val p = c.asInstanceOf[Polar] \nCartesian(p.r cos(p.phi), p.r sin(p.phi)) } * * Appropriate rewrites ensure that if the argument is known \nto be a Cartesian, the conversion is a no-op. The type test that inspects the clzz .eld is only generated \nif the type cannot be determined statically. If the clzz .eld is never used it will be removed by DCE. \n 4.3 Struct of Array and Other Data Format Conversions A natural extension of the splitting mechanism \nis a generic array\u00adof-struct to struct-of-array transform (AoS to SoA). The mecha\u00adnism is analogous to \nthat for conditionals. We override the ar\u00adray constructor arrayFill that represents expressions of the \nform Array.fill(n) { i => body } to create a struct with an array for each component of the body if the \nbody itself is a Struct (Figure 5). Note that we tag the result struct with an ArraySoaTag to keep track \nof the transformation. We also override the methods that are used to access array elements and return \nthe length of an array to do the right thing for transformed arrays. The SoA data layout is bene.cial \nin many cases. Consider for example calculating complex conjugates (i.e. swapping the sign of the imaginary \ncomponents) over a vector of complex numbers. def conj(c: Rep[Complex]) = if (c.isCartesian) { val c2 \n= c.toCartesian; Cartesian(c.re,-c.im) } else { val c2 = c.toPolar; Polar(c.r, -c.phi) } To make the \ntest case more interesting we perform the calculation only in one branch of a conditional: val vector1 \n= Array.fill(100) { i => Cartesian(...) } if (test) vector1.map(conj) else vector1 All the real parts \nremain unchanged so the array holding them need not be touched at all. Only the imaginary parts have \nto be transformed, cutting the total required memory bandwidth in half. Uniform array operations like \nthis are also a much better .t for SIMD execution. The generated intermediate code is: val vector1re,vector1im \n= ... val vector1clzz = // array holding classOf[Cartesian] values val vector2im = if (test) Array.fill(vector1size) \n{ i => -vector1im(i) } else vector1im struct(ArraySoaTag(Complex, vector1size), Map(\"re\"->vector1re,\"im\"->vector2im,\"clzz\"->vector1clzz)) \nNote how the conditionals for the re and clzz .elds have been eliminated since the .elds do not change \n(the initial array contains cartesian numbers only). If the struct expression will not be refer\u00adenced \nin the .nal code DCE removes the clzz array. In the presence of conditionals that produce array elements \nof different types, a possible optimization would be to use a sparse representation for the arrays that \nmake up the result SoA (similar to DPH [21]). However, all the usual sparse vs dense tradeoffs apply. \nOne concern with data representation conversions is what hap\u00adpens if an array is returned from a staged \ncode fragment to the en\u00adclosing program. In this case, the compiler will generate conversion code to \nreturn a plain array-of-struct copy of the data.  4.4 Loop Fusion and Deforestation The use of independent \nand freely composable traversal operations such as v.map(..).sum is preferable to explicitly coded loops. \nHow\u00adever, naive implementations of these operations would be expensive and entail lots of intermediate \ndata structures. We present a novel loop fusion algorithm for data parallel loops and traversals. The \ncore loop abstraction is loop(s) x =G { i => E[x . f(i)] }  Generator kinds: G ::= Collect | Reduce(.) \n| Bucket(G) Yield statement: xs . x Contexts: E[.] ::= loops and conditionals Horizontal case (for all \ntypes of generators): loop(s) x1=G1 { i1 => E1[ x1 . f1(i1) ] } loop(s) y1=G2 { i2 => E2[ x2 . f2(i2) \n] } loop(s) x1=G1, x2=G2 { i => E1[ x1 . f1(i) ]; E2[ x2 . f2(i) ] } Vertical case (consume collect): \nloop(s) x1=Collect { i1 => E1[ x1 . f1(i1) ] } loop(x1.size) x2=G { i2 => E2[ x2 . f2(x1(i2)) ] } loop(s) \nx1=Collect, x2=G { i => E1[ x1 . f1(i); E2[ x2 . f2(f1(i)) ]] } Vertical case (consume bucket collect): \nloop(s) x1=Bucket(Collect) { i1 => E1[ x1 . (k1(i1), f1(i1)) ] } loop(x1.size) x2=Collect { i2 => loop(x1(i2).size) \ny=G { j => E2[ y . f2(x1(i2)(j)) ] }; x2 . y } loop(s) x1=Bucket(Collect), x2=Bucket(G) { i => E1[ x1 \n. (k1(i), f1(i)); E2[ x2 . (k1(i), f2(f1(i))) ]] } Figure 6. Loop fusion. where s is the size of the \nloop and i the loop variable rang\u00ading over [0, s). A loop can compute multiple results x, each of which \nis associated with a generator G. There are three kinds of generators: Collect, which creates a .at array-like \ndata structure, Reduce(.), which reduces values with the associative operation ., or Bucket(G), which \ncreates a nested data structure, grouping gen\u00aderated values by key and applying G to those with matching \nkey. Loop bodies consist of yield statements x.f(i) that pass values to generators (of this loop or an \nouter loop), embedded in some outer context E[.] that might consist of other loops or conditionals. Note \nthat a yield statement x... does not introduce a binding for x, but passes a value to the generator identi.ed \nby x. For Bucket genera\u00adtors, yield takes (key,value) pairs. This model is expressive enough to represent \nmany common collection operations: x=v.map(f) loop(v.size) x=Collect { i => x . f(v(i)) } x=v.sum loop(v.size) \nx=Reduce(+) { i => x . v(i) } x=v.filter(p) loop(v.size) x=Collect { i => if (p(v(i))) x . v(i) } x=v.flatMap(f) \nloop(v.size) x=Collect { i => val w = f(v(i)) loop(w.size) { j => x . w(j) }} x=v.distinct loop(v.size) \nx=Bucket(Reduce(rhs)) { i => x . (v(i), v(i)) } Operation distinct uses a bucket reduction with function \nrhs, which returns the right-hand side of a tuple element, to return a .at sequence that contains only \nthe rightmost occurence of a duplicate element. Other operations are accommodated by generalizing slightly. \nInstead of implementing a groupBy operation that returns a sequence of (Key, Seq[Value]) pairs we can \nreturn the keys and values in sep\u00adarate data structures. For a given selector function f that computes \na key from a value, the equivalent of (ks,vs)=v.groupBy(f).unzip is: loop(v.size) ks=Bucket(Reduce(rhs)),vs=Bucket(Collect) \n{ i => ks . (f(v(i)), v(i)); vs . (f(v(i)), v(i)) } This loop ranges over the size of v and produces \ntwo result col\u00adlections ks, a .at sequence of keys, and vs, a nested collections of values that belong \nto the key in ks at the same index. The fusion rules are summarized in Figure 6. Fusion of two loops \nis only permitted if there are no other dependencies between def preferences(ratings: Rep[Matrix[Int]], \nsims: Rep[Matrix[Double]]) = { sims.mapRowsToVector { testProfile => val num = sum(0, ratings.numRows) \n{ i => testProfile(ratings(i,1))*ratings(i,2) } val den = sum(0, ratings.numRows) { i => abs(testProfile(ratings(i,1))) \n} num/(den+1) }} Figure 7. Snippet from collaborative .ltering to be optimized the loops, for example \ncaused by side effects. Since we are working with a graph-based IR, dependency information is readily \navailable. Vertical fusion, which absorbs a consumer loop into the producer loop, is only permitted if \nthe consumer loop does not have any de\u00adpendencies on its loop variable other than acessing the consumed \ncollection at this index. In a data parallel setting, where loops are chunked, the producer may not be \nable to compute the exact in\u00addex of an element in the collection it is building. Multiple instances of \nf1(i) are subject to CSE and not evaluated twice. Substituting x1(i2) with f1(i) will remove a reference \nto x1. If x1 is not used anywhere else, it will also be subject to DCE. Within fused loop bodies, unifying \nindex variable i and substituting references will trigger the usual forward rewriting and simpli.cation. \nThus, fu\u00adsion not only removes intermediate data structures but also provides additional optimization \nopportunities inside fused loop bodies (in\u00adcluding fusion of nested loops). Fixed size array construction \nArray(a,b,c) can be expressed as loop(3) x=Collect { case 0 => x . a case 1 => x . b case 2 => x . c \n} and concatenation xs ++ ys as Array(xs,ys).flatMap(i=>i): loop(2) x=Collect { case 0 => loop(xs.size) \n{ i => x . xs(i) } case 1 => loop(ys.size) { i => x . ys(i) }} Fusing these patterns with a consumer \nwill duplicate the consumer code into each match case. Therefore, implementations should im\u00adpose cutoff \nmechanisms to prevent code explosion. Code genera\u00adtion does not need to emit actual loops for .xed array \nconstructions but can just produce the right sequencing of yield operations. 5. Case Studies We present \nseveral case studies to illuminate how our compiler architecture and in particular the staging aspect \nenable advanced optimizations related to data structures. All experiments were performed on a Dell Precision \nT7500n with two quad-core Xeon 2.67GHz processors and 96GB of RAM. Scala code was run on the Oracle Java \nSE Runtime Environment 1.7.0 and the Hotspot 64-bit server VM with default options. We ran each application \nten times (to warm up the JIT) and report the average of the last 5 runs. For each run we timed the computational \nportion of the application. 5.1 Linear Algebra We .rst consider two examples using an extended version \nof the staged linear algebra library presented in Section 1. In the .rst example, we use staged transformation, \npartial evaluation, rewrite rules, and fusion together to transparently optimize a logical op\u00aderation \non a sparse matrix into a much more ef.cient version that only operates on its non-zero values. In the \nsecond example, we show how staged transformations can elegantly specialize loops to different computing \ndevices by selecting a dimension to unroll and expressing the unrolled loop as a simple while loop in \nthe source language. Sparse User-De.ned Operators Figure 7 shows an excerpt from a collaborative .ltering \napplication. This snippet computes a user s  trait SparseMatrix[A] extends Matrix[A] with Struct { val \nnumRows: Rep[Double], val numCols: Rep[Double], val _data: Rep[Array[A]], val _nnz: Rep[Int], val _rowPtr: \nRep[Array[Int]],val _colIndices:Rep[Array[Int]] } trait SparseVector[A] extends Vector[A] with Struct \n{ val length: Rep[Int], val _data: Rep[Array[A]], val _indices: Rep[Array[Int]],val _nnz: Rep[Int] } \nFigure 8. Data structure de.nition for sparse matrix and vector preference for other users given the \nuser s previous ratings and a pre-computed NxN similarity matrix. The code is written in a representation-agnostic \nway it works whether sims is dense or sparse using generic operations. Our goal is to transform the \nfunc\u00adtion mapRowsToVector into an ef.cient sparse operation if sims is sparse and the user-provided function \nargument to mapRowsToVector returns 0 when the input is a vector (row) containing only 0s. By ex\u00adpressing \nthis as a transformation (rather than during construction), the IR node for mapRowsToVector can take \npart in other analyses and rewritings prior to being lowered to operate on the underlying ar\u00adrays. Figure \n8 de.nes new data structures for sparse matrices and vec\u00adtors in the manner described in Section 4. Using \nthese de.nitions, we de.ne the transformed version of mapRowsToVector as follows: def infix_mapRowsToVector[A,B](x: \nRep[Matrix[A]], f: Rep[Vector[A]] => Rep[B]) = { def isZeroFunc(f: Rep[Vector[A]] => Rep[B], len: Rep[Int]) \n= // use symbolic evaluation to test the user-defined // function f against a zero-valued argument f(Vector.zeros[A](len)) \nmatch { case Const(x) if (x == defaultValue[B]) => true case _ => false } if (x.isInstanceOf[SparseMatrix] \n&#38;&#38; isZeroFunc(f,x.numRows)) MatrixMapRowsToVec(x,f) atPhase(lowering) { // transform to operate \nonly on non-zero values new SparseVector[B] { val length = x.numRows val _indices = asSparseMat(x).nzRowIndices \n// elided val _data = _indices.map(i=>f(x(i))) val _nnz = _indices.length } } else MatrixMapRowsToVec(x,f) \n// default } Staging is the critical ingredient that allows this transformation to be written expressively \nand correctly. We .rst use the program\u00admatic struct support with inheritance from Section 4.1 to operate \nuniformly on dense and sparse matrices and to obtain access to the underlying representation in order \nto perform the transformation. If the argument is a sparse matrix, mapRowsToVector calls isZeroFunc, \nwhich symbolically evaluates the user-de.ned function f at staging time to discover if it actually has \nno effect on empty rows in the ar\u00adgument matrix. This discovery is in turn enabled by further rewrite \nrules that implement symbolic execution by optimizing operations for constant values. For example, we \nde.ne sum as: override def sum[A:Numeric](start: Exp[Int], end: Exp[Int], block: Exp[Int] => Exp[A]) \n= block(fresh[Int]) match { case Const(x) if x == 0 => unit(0.asInstanceOf[A]) case _ => super.sum(start,end,block) \n} We are able to successfully perform this transformation of mapRowsToVector because LMS re-applies all \nrewrite rules as the 1 0.8 0.6 0.4 0.2 0 Plain Scala Staged w/ Fusion w/ Fusion + Sparse xform Parallel \n(8 threads) 75 85 95 Figure 9. LinAlg Benchmark. The y-axis shows the normalized execution time of the \ncollaborative .ltering snippet on a 10kx10k matrix with various optimizations. The x-axis represents \nthe per\u00adcentage of sparse rows in the test data. Speedup numbers are re\u00adported at the top of each bar. \n// x = input matrix, k = num clusters // c = distance from each sample to nearest centroid val newLocations \n= Matrix.fromArray { // returns an Array[Vector[Double]] Array.range(0,k) { j => val (points,weightedpoints) \n= sum(0, x.numRows) { i => if (c(i) == j) (1,x(i)) } val d = if (points == 0) 1 else points weightedpoints \n/ d }} Figure 10. Snippet from k-means clustering to be optimized transformed IR is constructed. After \nthe transformation, the code is further optimized by fusing together the two sum statements in Figure \n7 (but the fused summation is still only run on non-zero rows). Figure 9 shows the results of running \nthe collaborative .l\u00adtering snippet with input data of varying sparsity. As expected, the transformation \nprovides greater bene.t with increasing sparsity because we switched from a representation-agnostic implementa\u00adtion \nto a representation-sensitive one. We also implicitly parallelize the transformed map function, resulting \nin good speedup over the sequential baseline automatically. Loop Parallelization for Heterogeneous Processors \nThe .nal linear algebra example we will consider is the parallelization of a loop in the well-known k-means \nclustering algorithm. Figure 10 shows the snippet, which updates the cluster locations in the current \niteration to the mean of all the samples currently assigned to that cluster. K-means clustering is a \nstatistical algorithm that is amenable to both multicore parallelization and GPU execution [28]. However, \nthe Array.range statement in Figure 10 poses a challenge: each of the k cluster locations can be computed \nin parallel, or we can compute the inner sum statement in parallel. Since k \u00ab x.numRows usually, on multicore \nmachines with a smaller number of hardware threads (< 16) it is better to parallelize the outer (fromArray) \ndimen\u00adsion, but on massively parallel machines such as GPUs it is much better to parallelize the inner \n(sum) dimension. If we internally represent parallel operators such as Array.range and sum as loops, \na parallelization framework (such as Delite [5]) can automatically generate parallel and/or GPU code \nfor them. Using the notation from Section 3.3, we can then de.ne a device\u00adspeci.c transformation as follows: \ntrait LoopTransformerExp extends LinAlgExp { self => val t = new LoopTransformer { val IR: self.type \n= self } def hasNestedLoop(l: Loop[_]): Boolean // elided def transformCollectToWhile(l: Collect[_]) \n= {  // construct transformed representation var i = 0 val size = t.transformExp(l.size) val res = alloc(size) \nwhile (i < size) { t.transformBlock(l.update) (res,i,t.transformBlock(l.func)(i)) i += 1 } res }} trait \nLoopTransformer extends ForwardTransformer { val IR: LoopTransformerExp import IR._ override def transformStm(stm: \nStm): Exp[Any] = stm match { case TP(s,l:Loop[_]) if (generateGPU &#38;&#38; hasNestedLoop(l)) => l.body \nmatch { case c:Collect[_] => transformCollectToWhile(c) case _ => super.transformStm(stm) } case _ => \nsuper.transformStm(stm) }} This transformation unwraps the Array.range function, which is internally \nrepresented by a loop, into an explicit while loop to expose any nested parallel operators. Although \nwe elide it here, it is easy to add a heuristic to decide (statically if known or dynamically if not) \nwhether or not to unwrap a loop based on its size. Note that due to staging, the transformed representation \ncan be written as simple source code, which is easier to understand than direct IR manipulations. The \ntransformation is only triggered if we are attempting to generate GPU code; if we are generating CPU \ncode, we parallelize the outer loop to maximize our hardware utilization.  5.2 Regular Expression Matchers \nSpecializing string matchers and parsers is a popular benchmark in the partial evaluation literature \n[2, 10, 42]. Usually, specialization produces a set of mutually recursive functions, which is also our \nstarting point. However this code does not achieve top performance compared to fast automata libraries. \nWe show that treating gener\u00adated functions as data objects and transforming them into a more ef.cient \nform yields performance that exceeds that of an optimized library. We consider multi-threaded regular \nexpression matchers, that spawn a new conceptual thread to process alternatives in parallel. Of course, \nthese matchers do not actually spawn OS-level threads, but rather need to be advanced manually by client \ncode. Thus, they are similar to coroutines. Here is an example for the .xed regular expression .*AAB: \ndef findAAB(): NIO = { guard(C( A )) { guard(C( A )) { guard(C( B ), true) { stop() }}} ++ guard(W) { \nfindAAB() } // in parallel ... } The .rst argument to guard is a character classs, C(.) de\u00ad.nes a singleton \ncharacter class and W denotes the wildcard. We added combinators on top of the core abstractions that \ncan produce matchers from more conventional regular expressions: many(seq)(star(W), C( A ), C( A ), C( \nB )) for the example above. We can easily add a parser for textual regular expressions on top of these \ncombinators. NFA to DFA Conversion Internally, the given matcher uses an API that models nondeterministic \n.nite automata (NFA): def exploreNFA[A:Manifest](xs: NIO, cin: Rep[Char])( k: (Boolean, NIO) => Rep[A]): \nRep[A] = xs match { case Nil => k(false, Nil) case NTrans(W, e, s)::rest => val (xs1, xs2) = xs.partition(_.c \n!= W) exploreNFA(xs1,cin)(continueWith(k, x2)) case NTrans(cset, e, s)::rest => if (cset contains cin) \n{ val xs1 = // restrict rest: knowing cset exploreNFA(xs1,cin)((flag,acc) => k( flag || e(), acc ++ s())) \n} else { val xs1 = // restrict rest: knowing_not cset exploreNFA(xs1, cin)(k) } } Figure 11. NFA Exploration \ntype NIO = List[NTrans] case class NTrans(c: CharSet, x: () => Boolean, s: () => NIO) def guard(c: CharSet, \nx: => Boolean = false)(s: => NIO): NIO = { List(NTrans(c, () => x, () => s)) } def stop(): NIO = Nil \nAn NFA state consists of a list of possible transitions. Each transition may be guarded by a set and \nit may have a .ag to be signaled if the transition is taken. It also knows how to compute the following \nstate. For simplicity, we use set of characters for the guard and a boolean for the .ag, but of course, \nwe could use generic types as well. Note that the API does not mention where input is obtained from (.les, \nstreams, etc.). We will translate NFAs to DFAs using staging. The Automaton class is part of the unstaged \nDFA API. The staged API is just a thin wrapper. case class Automaton[I, O](out: O, next: I => Automaton[I,O]) \ntype DfaState = Automaton[Char,Boolean] type DIO = Rep[DfaState] def dfa_trans(e: Boolean)(f: Rep[Char] \n=> DIO): DIO Translating an NFA to a DFA is accomplished by creating a DFA state for each encountered \nNFA con.guration: def convertNFAtoDFA(flag: Boolean, state: NIO): DIO = { val cstate = canonicalize(state) \ndfa_trans(flag) { c: Rep[Char] => exploreNFA(cstate, c) { convertNFAtoDFA } }} convertNFAtoDFA(false, \nfindAAB()) The LMS framework memoizes functions which, with the state canonicalization (e.g. we need \nto remove duplicates), ensures ter\u00admination if the NFA is in fact .nite. Indeed, our approach is con\u00adceptually \nsimilar to generating a DFA using derivatives of regu\u00adlar expressions [6, 35]: both approaches rely on \nidentifying (ap\u00adproximately) equivalent automaton states; furthermore, computing a symbolic derivative \nis similar to advancing the NFA by a sym\u00adbolic character, as explained next. We use a separate function \nto explore the NFA space (see Fig\u00adure 11), advancing the automaton by a symbolic character cin to invoke \nits continuation k with a new automaton, i.e. the possible set of states after consuming cin. The given \nimplementation needs only to treat the wildcard character set (denoted by W) specially. The other character \nsets (single character or range) are treated generi\u00adcally, and it would be straightforward to add more \ncases without changing this function. The algorithm tries to remove as many re\u00addundant checks and impossible \nbranches as possible (it relies on  def naiveFindAAB(): def optFindAAB(input: String): Automaton[Char, \nBoolean] = { Boolean = { val x1 = {x2: (Char) => val n = input.length /*matched nothing*/ if (n==0) \nreturn false val x3 = x2 == A var id = 0 val x18 = if (x3) { var i = 0 x17 val n_dec = n-1 } else { \n while (i < n_dec) { x13 val x18 = input.charAt(i) } // first case analysis // for next state id x18 \nid = id match { } case 0 => /*matched nothing*/ val x12 = Automaton(true,x1) val x26 = x18 == A val \nx7 = {x8: (Char) => val x27 = if (x26) 2 /*matched AA*/ else 0 /* ... */ x27 } case 1 => val x10 = \nAutomaton(false,x7) /*matched AA ... */ val x4 = {x5: (Char) => case 2 => /*matched A*/ /*matched A \n...*/ /* ... */ } } i += 1 val x17 = Automaton(false,x4) } val x13 = Automaton(false,x1) val x18 = \ninput.charAt(i) x13 // second case analysis } // for final boolean flag id == 1 &#38;&#38; x18 == B } \nFigure 12. Generated matcher code for regular expression .*AAB. Optimized code on the right. the knowing \nand knowing_not binary functions on character sets). This only works because the guards are staging-time \nvalues. The generated code is shown in Figure 12 on the left. Each function corresponds to one DFA state. \nOne straightforward optimization is to signal early stopping if the boolean .ag remains the same in all \npossible next states. To do so, we change the DfaState so that the output can encode two boolean values: \nthe original .ag and whether we can stop early. This does not help for matching .*AAB, but it does for \n.*AAB.*, as we can stop as soon as we .nd an occurrence of AAB. Transforming Closures Our .nal implementation \ngenerates a complete matcher from a String input to a Boolean output. The generated code does not have \nfunctions nor Automaton states. In\u00adstead, we generate a case statement for each function. We do this \ntwice: in the middle of matching the input, the value of the case analysis is the next functional state, \nwhile for the last character matching, its value is the boolean .ag output. Because all the cases are \ninlined, we achieve early stopping by simply returning from the function with the boolean .ag output \nin the midst of the .rst case analysis. The optimized code is shown in Figure 12 on the right. Figure \n13 shows benchmarking results. We compare our naive implementation (Staged 1), which includes the early \nstopping op\u00adtimization, to the JDK and dk.brics.automaton [31] libraries. On average, our naive implementation \nis more than 2.5x faster than JDK s and about 3x slower than dk.brics.automaton. In or\u00adder to speed up \nour implementation, we experimented with various staged transformations. Our .nal implementation (Staged \n2) beats dk.brics.automaton by nearly a factor of 2 and JDK by a factor of 15. For comparison, we also \ntried a completely unstaged plain Scala version, where we directly evaluate the code instead of stag\u00ading \nit, and it is 140x slower than JDK. 1 103 0.8 0.6 102 0.4 101 0.2 0 (\u00b710) (\u00b7100) (\u00b7107) Summary  Staged \n1 Staged 2 brics JDK Plain Scala // lineItems: Array[LineItem] val q = lineItems filter (_.l_shipdate \n<= Date( 1998-12-01 )). groupBy (_.l_linestatus) map { case (key,g) => new { val lineStatus = key val \nsumQty = g.map(_.l_quantity).sum val sumDiscountedPrice = g.map(l => l.l_extendedprice*(1.0-l.l_discount)).sum \nval avgPrice = g.map(_.l_extendedprice).sum / g.size val countOrder = g.size }} sortBy(_.lineStatus) \nFigure 14. Sample query (similar to Q1 of the TPC-H benchmark)  5.3 Collection and Query Operations \nNext we consider the case of executing queries on in-memory col\u00adlections. Figure 14 shows a simpli.ed \nversion (for illustration pur\u00adposes) of TPC-H Query 1 written using the standard Scala collec\u00adtions API. \nA straightforward execution of this query as written will be substantially slower than what could be \nachieved with an opti\u00admized low-level imperative implementation. In particular executing this code in \nScala will perform a new array allocation for the out\u00adput of each collection operation. Furthermore, \nevery element of the arrays will be a heap allocated object since both LineItem and the anonymous class \nfor the result are implemented as JVM classes. These classes however contain only .elds, no methods, \nand as such we can apply struct optimizations to them during staging. Since the .elds of these structs \ncan all be represented as JVM primitive types, we can eliminate all of the heap allocated objects at \nruntime by performing array-of-struct (AoS) to struct-of-array (SoA) trans\u00adformations for each array. \nFurthermore since LineItem as de.ned by TPC-H contains 16 .elds, while Query 1 only accesses 7 of those \n.elds, the DCE engine eliminates the arrays of the other 9 .elds all together once the SoA form is created. \nFusion Optimization In addition, we eliminate the intermedi\u00adate array and map data structures produced \nby each of the collec\u00adtion operations by fusing operations (Section 4.4) at two levels. At the inner \nlevel, creating the output struct requires multiple reduc\u00adtion operations. Horizontal fusion combines \nthe reductions for each .eld into a single loop. On the outer level, the query groups the collection \naccording to a key function and then maps each group (sub-collection) down to a single element. We fuse \nthe groupBy and map together by using the rule for consuming a bucket-collect oper\u00adation. Essentially \nthe fusing algorithm observes that the output ele\u00adment of each group can be computed directly by reducing \nthe entire input collection into the buckets de.ned by the key function of the groupBy. Furthermore, \nthe vertical consume-collect rule fuses the filter with the groupBy by predicating the addition of each \nelement   1 CPU 8 CPU to the appropriate group (and after groupBy-map fusion, predicating the reduction \nof each element into the appropriate group). Finally, horizontal fusion combines the operations for each \n.eld (array in the SoA form) into a single loop. Therefore, the program produces the output collection \ndirectly from the input with a single loop and no intermediate arrays. Sorting the output yields the \n.nal result. We perform one .nal optimization during code generation of the fused loop. In order to implement \nBucket(Reduce) operations we internally allocate a generic HashMap instance to manage the buckets. However, \nwhen n Bucket(Reduce) operations with the same key function appear in the same loop, we do not allocate \na HashMap for each operation. Instead we use a single HashMap to determine an array index for each key, \nand then use that index to access the n arrays allocated to hold the result of each operation, eliminating \nthe overhead of computing the same bucket location multiple times. Performance Results We evaluate these \noptimizations in Figure 15. The execution times are normalized to the plain Scala library implementation \nof the query (leftmost bar). The middle bar shows the bene.ts of staging but without applying SoA transformations \nor groupBy fusion. Here staging is only applying basic optimiza\u00adtions such as common subexpression elimination \nand code mo\u00adtion. In particular the staged version contains the following key improvements. Higher-order \nabstractions such as anonymous func\u00adtions have been eliminated, construction of the Date object within \nthe .lter has been lifted out of the loop and converted into an Int, pairs of primitive types have been \npacked into a single primitive type with twice the bits (e.g., a Pair[Char,Char] is encoded as an Int), \nand the map-reduce chains in the inner loop have been fused. Since we do not perform SoA transformations \nin this version, the LineItem struct is generated as a custom JVM class and the unused .elds remain in \nthe generated code. These optimizations provide approximately 6x speedup over the library implementation. \nThe rightmost bar shows the results of adding AoS to SoA trans\u00adformations and all of the fusion optimizations \ndescribed above, namely fusing the .lter, groupBy, and map operations into a sin\u00adgle loop. These optimizations \nprovide approximately 5.5x speedup over the basic staging optimizations and 34x speedup over the li\u00adbrary \nimplementation. In addition, this version scales better when parallelized across multiple cores as it \nparallelizes over the size of the input collection rather than over the number of groups. Ul\u00adtimately, \nwith staging optimizations and parallel code generation, we were able to attain 163x speedup with 8 cores \nover a single\u00adthreaded library implementation. Heterogeneous Targets AoS to SoA conversion greatly improves \nthe ability to generate code for GPUs. In this example, however, the data transfer cost overwhelms any \nspeedup for end-to-end runs. Another interesting target besides multi-core and GPU devices are Big Data \ncluster frameworks such as Hadoop or Spark. We can use the same collections API to target several different \nframeworks by adding appropriate framework-speci.c lowerings. A recent pa\u00adper [1] contains more details \nand demonstrates good speedups.  5.4 String Templates String template libraries are in wide use today \nto transform struc\u00adtured data into a .at string representation (e.g. XML, HTML, or JSON). While the semantics \nare simple and easy to express in a functional way, template engine implementations are often intricate \nbecause data copying and intermediate data structures need to be avoided to obtain good performance. \nWe present the core of a tem\u00adplate engine based on purely functional collection operations (list construction, \nconcatenation and comprehensions), with loop fusion (Section 4.4) taking care of removing intermediate \ndata structures. A simple HTML menu can be rendered like this: def link(uri: Rep[String], name: Rep[String]) \n= List(\"<a href= \", uri, \" >\", name, \"</a\") def renderItem(i: Rep[Item]) = List(\"<li><ul>\") ++ i.subitems.flatMap(link(i.name, \ni.link)) ++ List(\"</ul></li>\") def usersTable(items: Rep[List[Item]]) = List(\"<ul>\") ++ items.flatMap(x \n=> renderItem(x)) ++ List(\"</ul>\") When executing this code without fusion optimization, each flatMap \noperation traverses the lists produced by the nested tem\u00adplate one more time. Each concat (++) operation \nincurs an addi\u00adtional traversal. This results in performance that, for .xed output size, decreases linearly \nwith the level of template nesting. The asymptotic worst case complexity is thus quadratic in the output \nsize, whereas an imperative implementation would be linear. Since all function calls are unfolded at \nstaging time, our fu\u00adsion transformation (Section 4.4) is able to remove the unnecessary traversals and \ndata structures, provided that only core collection operations are used to express templates. The generated \ncode tra\u00adverses the input data structure just once, emitting strings directly into a single output buffer. \nBenchmark results are reported in Figure 16. We demonstrate the ability of obtaining asymptotic speedups \non a synthetic bench\u00admark that generates HTML representations of a .le system struc\u00adture with growing \nlevels of nesting: with fusion enabled, speedups grow with the depth of nesting. Furthermore, we implemented \nmock templates to resemble the structure of three public web sites, yielding speedups of up to 8.6x over \nthe plain, unstaged Scala im\u00adplementation. 6. Discussion As shown in Section 5, our system is able to \nachieve order of magnitude (and in one case even asymptotic) speedups on a varied range of non-trivial \nhigh level programs. While we are not aware of any other system that demonstrably achieves comparable \nresults on a similar range of programs, these results bear the question whether they could be achieved \nwith similar effort using other means.  As suggested by one of the reviewers, a simpler design would \nbe to perform optimizations dynamically. For example, one could implement vectors and matrices as a hierarchy \nof classes, with a special class for zero vectors, instead of distinguishing zero vectors statically \nin the IR. This is certainly a valid design, but it comes with a number of drawbacks. First, runtime \ndistinctions introduce interpretive overhead, indirection and virtual calls, which also make it harder \nfor the compiler to generate ef.cient code. Second, while dynamic information may be more precise than \nstatic information at some times, the possibilities for dynamic optimizations are very limited in other \nkey aspects: For example, it is not clear how to integrate DCE or any other optimization that requires \nnon-local knowledge (liveness, horizontal fusion). Another question posed by a reviewer was whether staging \nre\u00adally simpli.es transformation compared to state-of-the art rewrit\u00ading systems such as those included \nin language workbenches like Spoofax [24] or JetBrains MPS [19]. While these systems are im\u00adpressive, \nwe believe they serve a different purpose and that the ca\u00adpability to programmatically remove abstraction \nat all intermediate levels, with strong guarantees about the residual code, is hard to overstate in the \ncontext of program optimization. For example, the code in Figure 3 transforms symbolic linear algebra \noperations to the same zero-overhead traversals shown in Figure 2, with higher order functions and type \nclass instances removed by staging, with\u00adout dedicated compiler analysis. Staging enables us to express \na program (or model) transformer as an interpreter; this is an ad\u00advantage because writing an interpreter \nin an expressive language is arguably simpler than implementing a transformer, even with good toolkit \nsupport. A key aspect is linguistic reuse of abstractions of the meta language. For example, our recent \nwork on JavaScript generation using LMS [27] re-uses Scala s CPS transform [40] at staging-time to generate \nasynchronous code that is in continuation passing style (CPS). By contrast, WebDSL [17], which is imple\u00admented \nin Spoofax, has to implement the CPS transform afresh, for each language construct of the object language. \nWorking with a graph-based IR also makes a qualitative differ\u00adence compared to expression trees. Most \nimportantly, it becomes much easier to implement transforms that require dependency in\u00adformation. Our \nfusion algorithm (Section 4.4), for example, does not require loops to be syntactically close or adjacent \nto be fused: def calcSum() = array.sum def calcCount() = array.filter(_ > 0).count println(\"sum: \" + \ncalcSum()) println(\"avg: \" + (calcSum() / calcCount())) Staging will inline the calcSum and calcCount \ncalls and the fusion algorithm will combine all traversals into a single loop. Any algo\u00adrithm that only \nconsiders dependent traversal expressions will miss the horizontal fusion opportunities across the two \nprintln state\u00adments. This is commonly the case for pure front-end approaches based e.g. on C++ expression \ntemplates. Another key aspect of our design is the different treatment of lowering transforms (handled \nin separate passes one after the other) and optimizations (combined into joint simpli.cation passes using \nspeculative rewriting). We believe this distinction is essential to make optimizations reliable by avoiding \nphase ordering problems and making sure that high-level optimizations are applied exhaus\u00adtively before \nlowerings take place. Finally, there is the question of programmer effort required to achieve similar \nresults. From our perspective, the most important aspects are that the optimizations we present can be \napplied grad\u00adually, and that they are reusable for all clients of an optimized li\u00adbrary. We expect that \nend user programmers will not usually write any transformations on their own, but they can add rewrites \nif they need. Library developers who wish to add optimizations need to implement functionality corresponding \nto the code we show (e.g. in Figure 3) but they can do so in a gradual way: Start with code similar to \nFigure 1, add staging (Figure 2), and, if that is not suf.\u00adcient, add further optimizations (Figure 3). \nDelayed rewriting (Sec\u00adtion 3.3) can further reduce boilerplate. The guarantees provided by the type \nsystem (non-Rep[T] expressions evaluated at staging time) and smart constructors (no IR node constructed \nif rewrites match) help ensure that optimizations take place in the intended way. As a debugging aid, \ntransformed code can be emitted at any time. 6.1 Related Work Extensible compilers have been studied \nfor a long time, recent examples in the Java world are Polyglot [33] and JastAdd [13]. The Glasgow Haskell \nCompiler also allows custom rewrite rules [22]. There are also elaborate approaches to library speci.c \nop\u00adtimizations inspired by formal methods. ROSE [36] is a frame\u00adwork for building source-to-source translators \nthat derives auto\u00admated optimizations from semantics annotations on library abstrac\u00adtions. COBALT [30] \nis a domain-speci.c language for implement\u00ading optimizations that are amenable to automated correctness \nrea\u00adsoning. Tate et al. [48] present a method of generating compiler optimizations automatically from \nprogram examples before and af\u00adter a transformation. Delite [5, 28, 41] is a parallelization framework \nfor DSLs devel\u00adoped on top of LMS. Previously, Delite had a notion of a multi-level IR: some IR nodes \ncould be viewed from several different angles at the same time (say, as a parallel loop and as a matrix \noperation). This caused problems because earlier , more high-level views had to be carried along and \nobsolete or fully exploited information was never discarded, limiting the choices of the compiler. We \nhave used the techniques presented in this work to extend Delite and to im\u00adplement domain-speci.c optimizations \n(similar to those presented in the case studies) for existing Delite DSLs such as OptiML [44]. Delite \nwas also used as the parallel execution engine for the case studies in this paper. Compared to Delite, \nthis paper presents a gen\u00aderal extensible compiler architecture, not limited to domain speci.c languages. \nPrevious publications on LMS [38, 39] presented only the pure front-end, single pass instantiation. The \nuse of Rep[T] types to de.ne binding times in LMS is inspired by earlier work on .nally tagless or polymorphic \nlanguage embedding [8, 18]. Tobin-Hochstadt et al. [50] propose languages as libraries in Racket and \nmake pervasive use of macros for translation. Our approach of using staging is similar in spirit. Earlier \nwork on realistic compilation by program transformation in the context of Scheme was presented by Kelsey \nand Hudak [25]. The ability to compile high-level languages to lower-level pro\u00adgramming models has been \ninvestigated in several contexts. Elliot et al. [14] pioneered embedded compilation and used a simple \nim\u00adage synthesis DSL as an example. Telescoping languages [26] is a strategy to automatically generate \noptimized domain-speci.c li\u00adbraries. In C++, expression templates [51] are popular to imple\u00adment limited \nforms of domain speci.c optimizations [23, 52]. The granularity is restricted to parts of a larger template \nexpression. Nystrom et al. [34] shows a library based approach to translating Scala programs to OpenCL \ncode. This is largely achieved through Java bytecode translation. A similar approach is used by Lime \n[3] to compile high-level Java code to a hardware description language such as Verilog. Our compiler \ninfrastructure implements a set of advanced op\u00adtimizations in a reusable fashion. Related work includes \nprogram transformations using advanced rewrite rules [4], combining anal\u00adyses and optimizations [9, 29, \n53] as well as techniques for elim\u00adinating intermediate results [12, 54] and loop fusion [16]. An in\u00adteresting \nalternative to speculative rewriting is equality saturation [47], which encodes many ways to express \nan operation in the IR. As an example of using partial evaluation in program trans\u00adformation, Sperber \nand Thiemann [43] perform closure conver\u00adsion and tail call introduction by applying of.ine partial evaluating \nto a suitable interpreter. More recently, Cook et al. [11] perform model transformation by partial evaluation \nof model interpreters. Partial evaluation corresponds to constant folding and specializa\u00adtion whereas \nour approach allows arbitrary compiler optimizations, arbitrary computation at staging/specialization \ntime to remove ab\u00adstraction overhead and provides strong guarantees about what be\u00adcomes part of the residual \ncode (type Rep[T] residual, vs T static).  6.2 Conclusion We have demonstrated a compiler architecture \nthat achieves order of magnitude speedups on high-level programs by fusing collection operations, changing \ndata layout and applying further optimiza\u00adtions on high-level objects, enabled by intermediate languages \nwith staging and a facility to combine optimizations without phase or\u00addering problems. Our system combines \nseveral novel pieces with existing techniques, which together provide optimization power that is greater \nthan the sum of the parts. Acknowledgments We thank the POPL reviewers for their valuable feedback and \nsuggestions. This research was sponsored by the European Re\u00adserach Council (ERC) under grant 587327 DOPPLER \n, DARPA through Oracle order US103282, DARPA contract HR0011-11-C\u00ad0007 SEEC , NSF grant SHF CCF-1111943, \nthe Stanford Per\u00advasive Parallelism Lab af.liates program (supported by Oracle, AMD, Intel, NVIDIA) and \nadditional support from Oracle. References [1] S. Ackermann, V. Jovanovic, T. Rompf, and M. Odersky. \nJet: An em\u00adbedded dsl for high performance big data processing. BigData, 2012. http://infoscience.epfl.ch/record/181673/files/paper.pdf. \n[2] M. S. Ager, O. Danvy, and H. K. Rohde. Fast partial evaluation of pattern matching in strings. ACM \nTrans. Program. Lang. Syst., 28(4): 696 714, 2006. [3] J. Auerbach, D. F. Bacon, P. Cheng, and R. Rabbah. \nLime: a java\u00adcompatible and synthesizable language for heterogeneous architec\u00adtures. OOPSLA, 2010. [4] \nM. Bravenboer, A. van Dam, K. Olmos, and E. Visser. Programtransformation with scoped dynamic rewrite \nrules. Fundam. Inf., 69: 123 178, July 2005. [5] K. J. Brown, A. K. Sujeeth, H. Lee, T. Rompf, H. Cha., \nM. Odersky,and K. Olukotun. A heterogeneous parallel framework for domain\u00adspeci.c languages. PACT, 2011. \n[6] J. A. Brzozowski. Derivatives of regular expressions. J. ACM, 11(4): 481 494, 1964. [7] C. Calcagno, \nW. Taha, L. Huang, and X. Leroy. Implementing multi\u00adstage languages using asts, gensym, and re.ection. \nIn GPCE, 2003. [8] J. Carette, O. Kiselyov, and C. chieh Shan. Finally tagless, partiallyevaluated: Tagless \nstaged interpreters for simpler typed languages. J. Funct. Program., 19(5):509 543, 2009. [9] C. Click \nand K. D. Cooper. Combining analyses, combining optimiza\u00adtions. ACM Trans. Program. Lang. Syst., 17:181 \n196, March 1995. [10] C. Consel and O. Danvy. Partial evaluation of pattern matching in strings. Inf. \nProcess. Lett., 30(2):79 86, 1989. [11] W. R. Cook, B. Delaware, T. Finsterbusch, A. Ibrahim, and B. \nWie\u00addermann. Model transformation by partial evaluation of model inter\u00adpreters. Technical Report TR-09-09, \nUT Austin Department of Com\u00adputer Science, 2008. [12] D. Coutts, R. Leshchinskiy, and D. Stewart. Stream \nfusion: from liststo streams to nothing at all. In ICFP, 2007. [13] T. Ekman and G. Hedin. The jastadd \nsystem -modular extensible compiler construction. Sci. Comput. Program., 69(1-3):14 26, 2007. [14] C. \nElliott, S. Finne, and O. de Moor. Compiling embedded languages.In W. Taha, editor, Semantics, Applications, \nand Implementation ofProgram Generation, volume 1924 of Lecture Notes in Computer Science, pages 9 26. \nSpringer Berlin / Heidelberg, 2000. [15] Y. Futamura. Partial evaluation of computation process -an approachto \na compiler-compiler. Higher-Order and Symbolic Computation, 12 (4):381 391, 1999. [16] C. Grelck, K. \nHinckfu\u00df, and S.-B. Scholz. With-loop fusion for datalocality and parallelism. IFL, 2006. [17] D. M. \nGroenewegen, Z. Hemel, L. C. L. Kats, and E. Visser. WebDSL:a domain-speci.c language for dynamic web \napplications. In OOP-SLA Companion, 2008. [18] C. Hofer, K. Ostermann, T. Rendel, and A. Moors. Polymorphicembedding \nof DSLs. GPCE, 2008. [19] JetBrains. Meta Programming System, 2009. URL http://www. jetbrains.com/mps/. \n[20] N. D. Jones, C. K. Gomard, and P. Sestoft. Partial evaluation and automatic program generation. \nPrentice-Hall, Inc., Upper Saddle River, NJ, USA, 1993. [21] S. L. P. Jones, R. Leshchinskiy, G. Keller, \nand M. M. T. Chakravarty.Harnessing the Multicores: Nested Data Parallelism in Haskell. In FSTTCS, 2008. \n[22] S. P. Jones, A. Tolmach, and T. Hoare. Playing by the rules: rewritingas a practical optimisation \ntechnique in ghc. Haskell, 2001. [23] S. Karmesin, J. Crotinger, J. Cummings, S. Haney, W. Humphrey, \nJ. Reynders, S. Smith, and T. J. Williams. Array design and expressionevaluation in pooma ii. In ISCOPE, \n1998. [24] L. C. L. Kats and E. Visser. The Spoofax language workbench. rulesfor declarative speci.cation \nof languages and IDEs. In SPLASH/OOP-SLA Companion, 2010. [25] R. Kelsey and P. Hudak. Realistic compilation \nby program transfor\u00admation. In POPL, 1989. [26] K. Kennedy, B. Broom, A. Chauhan, R. Fowler, J. Garvin, \nC. Koelbel, C. McCosh, and J. Mellor-Crummey. Telescoping languages: A sys\u00adtem for automatic generation \nof domain languages. Proceedings of the IEEE, 93(3):387 408, 2005. [27] G. Kossakowski, N. Amin, T. Rompf, \nand M. Odersky. Javascript as an embedded dsl. In ECOOP, 2012. [28] H. Lee, K. J. Brown, A. K. Sujeeth, \nH. Cha., T. Rompf, M. Oder\u00adsky, and K. Olukotun. Implementing domain-speci.c languages forheterogeneous \nparallel computing. IEEE Micro, 31(5):42 53, 2011. [29] S. Lerner, D. Grove, and C. Chambers. Composing \ndata.ow analysesand transformations. SIGPLAN Not., 37:270 282, January 2002. [30] S. Lerner, T. D. Millstein, \nand C. Chambers. Automatically provingthe correctness of compiler optimizations. In PLDI, 2003. [31] \nA. M\u00f8ller. dk.brics.automaton .nite-state automata and regular expressions for Java, 2010. http://www.brics.dk/automaton/. \n[32] A. Moors, T. Rompf, P. Haller, and M. Odersky. Scala-virtualized. PEPM, 2012. [33] N. Nystrom, M. \nR. Clarkson, and A. C. Myers. Polyglot: An extensiblecompiler framework for java. In CC, 2003. [34] N. \nNystrom, D. White, and K. Das. Firepile: run-time compilation forgpus in scala. GPCE, 2011. [35] S. Owens, \nJ. Reppy, and A. Turon. Regular-expression derivatives re\u00adexamined. J. Funct. Program., 19(2):173 190, \nMar. 2009. [36] D. J. Quinlan, M. Schordan, Q. Yi, and A. S\u00e6bj\u00f8rnsen. Classi.cationand utilization of \nabstractions for optimization. In ISoLA (Preliminary proceedings), 2004. [37] T. Rompf. Lightweight Modular \nStaging and Embedded Compilers:Abstraction Without Regret for High-Level High-Performance Pro\u00adgramming. \nPhD thesis, EPFL, 2012. [38] T. Rompf and M. Odersky. Lightweight modular staging: a pragmaticapproach \nto runtime code generation and compiled dsls. GPCE, 2010. [39] T. Rompf and M. Odersky. Lightweight modular \nstaging: a pragmaticapproach to runtime code generation and compiled dsls. Commun. ACM, 55(6):121 130, \n2012. [40] T. Rompf, I. Maier, and M. Odersky. Implementing .rst-class poly\u00admorphic delimited continuations \nby a type-directed selective cps\u00adtransform. In ICFP, 2009. [41] T. Rompf, A. K. Sujeeth, H. Lee, K. J. \nBrown, H. Cha., M. Odersky,and K. Olukotun. Building-blocks for performance oriented dsls. DSL, 2011. \n[42] A. Shali and W. R. Cook. Hybrid partial evaluation. OOPSLA, 2011. [43] M. Sperber and P. Thiemann. \nRealistic compilation by partial evalua\u00adtion. In PLDI, 1996. [44] A. K. Sujeeth, H. Lee, K. J. Brown, \nT. Rompf, M. Wu, A. R. Atreya, M. Odersky, and K. Olukotun. OptiML: an implicitly parallel domain\u00adspeci.c \nlanguage for machine learning. ICML, 2011. [45] E. Sumii and N. Kobayashi. A hybrid approach to online \nand of.inepartial evaluation. Higher-Order and Symbolic Computation, 14(2-3): 101 142, 2001. [46] W. \nTaha and T. Sheard. Metaml and multi-stage programming with explicit annotations. Theor. Comput. Sci., \n248(1-2):211 242, 2000. [47] R. Tate, M. Stepp, Z. Tatlock, and S. Lerner. Equality saturation: a new \napproach to optimization. In POPL, 2009. [48] R. Tate, M. Stepp, and S. Lerner. Generating compiler optimizations \nfrom proofs. In POPL, 2010. [49] P. Thiemann and D. Dussart. Partial evaluation for higher-order languages \nwith state. Technical report, 1999. URL http://www. informatik.uni-freiburg.de/~thiemann/papers/mlpe.ps.gz. \n[50] S. Tobin-Hochstadt, V. St-Amour, R. Culpepper, M. Flatt, and M. Felleisen. Languages as libraries. \nPLDI 11, 2011. [51] T. L. Veldhuizen. Expression templates, C++ gems. SIGS Publica\u00adtions, Inc., New York, \nNY, 1996. [52] T. L. Veldhuizen. Arrays in blitz++. In ISCOPE, 1998. [53] T. L. Veldhuizen and J. G. \nSiek. Combining optimizations, combiningtheories. Technical report, Indiana University, 2008. [54] P. \nWadler. Deforestation: Transforming programs to eliminate trees.Theor. Comput. Sci., 73(2):231 248, 1990. \n[55] P. Wadler and S. Blott. How to make ad-hoc polymorphism less ad\u00adhoc. In POPL, 1989.    \n\t\t\t", "proc_id": "2429069", "abstract": "<p>High level data structures are a cornerstone of modern programming and at the same time stand in the way of compiler optimizations. In order to reason about user- or library-defined data structures compilers need to be extensible. Common mechanisms to extend compilers fall into two categories. Frontend macros, staging or partial evaluation systems can be used to programmatically remove abstraction and specialize programs before they enter the compiler. Alternatively, some compilers allow extending the internal workings by adding new transformation passes at different points in the compile chain or adding new intermediate representation (IR) types. None of these mechanisms alone is sufficient to handle the challenges posed by high level data structures. This paper shows a novel way to combine them to yield benefits that are greater than the sum of the parts.</p> <p>Instead of using staging merely as a front end, we implement internal compiler passes using staging as well. These internal passes delegate back to program execution to construct the transformed IR. Staging is known to simplify program generation, and in the same way it can simplify program transformation. Defining a transformation as a staged IR interpreter is simpler than implementing a low-level IR to IR transformer. With custom IR nodes, many optimizations that are expressed as rewritings from IR nodes to staged program fragments can be combined into a single pass, mitigating phase ordering problems. Speculative rewriting can preserve optimistic assumptions around loops.</p> <p>We demonstrate several powerful program optimizations using this architecture that are particularly geared towards data structures: a novel loop fusion and deforestation algorithm, array of struct to struct of array conversion, object flattening and code generation for heterogeneous parallel devices. We validate our approach using several non trivial case studies that exhibit order of magnitude speedups in experiments.</p>", "authors": [{"name": "Tiark Rompf", "author_profile_id": "81442614474", "affiliation": "EPFL, Lausanne, Switzerland", "person_id": "P3978040", "email_address": "tiark.rompf@epfl.ch", "orcid_id": ""}, {"name": "Arvind K. Sujeeth", "author_profile_id": "81456635415", "affiliation": "Stanford University, Palo Alto, CA, USA", "person_id": "P3978041", "email_address": "asujeeth@stanford.edu", "orcid_id": ""}, {"name": "Nada Amin", "author_profile_id": "81546131156", "affiliation": "EPFL, Lausanne, Switzerland", "person_id": "P3978042", "email_address": "nada.amin@epfl.ch", "orcid_id": ""}, {"name": "Kevin J. Brown", "author_profile_id": "81481647088", "affiliation": "Stanford University, Palo Alto, CA, USA", "person_id": "P3978043", "email_address": "kjbrown@stanford.edu", "orcid_id": ""}, {"name": "Vojin Jovanovic", "author_profile_id": "81470646838", "affiliation": "EPFL, Lausanne, Switzerland", "person_id": "P3978044", "email_address": "vojin.jovanovic@epfl.ch", "orcid_id": ""}, {"name": "HyoukJoong Lee", "author_profile_id": "81481647522", "affiliation": "Stanford University, Palo Alto, CA, USA", "person_id": "P3978045", "email_address": "hyouklee@stanford.edu", "orcid_id": ""}, {"name": "Manohar Jonnalagedda", "author_profile_id": "81553085856", "affiliation": "EPFL, Lausanne, Switzerland", "person_id": "P3978046", "email_address": "manohar.jonnalagedda@epfl.ch", "orcid_id": ""}, {"name": "Kunle Olukotun", "author_profile_id": "81474681074", "affiliation": "Stanford University, Palo Alto, CA, USA", "person_id": "P3978047", "email_address": "kunle@stanford.edu", "orcid_id": ""}, {"name": "Martin Odersky", "author_profile_id": "81100056476", "affiliation": "EPFL, Lausanne, Switzerland", "person_id": "P3978048", "email_address": "martin.odersky@epfl.ch", "orcid_id": ""}], "doi_number": "10.1145/2429069.2429128", "year": "2013", "article_id": "2429128", "conference": "POPL", "title": "Optimizing data structures in high-level programs: new directions for extensible compilers based on staging", "url": "http://dl.acm.org/citation.cfm?id=2429128"}