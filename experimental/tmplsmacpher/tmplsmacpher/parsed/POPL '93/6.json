{"article_publication_date": "03-01-1993", "fulltext": "\n Imperative functional programming Simon L Peyton Jones Dept of Computing Science, Email: -(simonpj, \nwadler}(ldcs Abstract We present a new model, based on monads, for perform\u00ading input/output in a non-strict, \npurely func~,ional lan\u00adguage. It is composable, extensible, efficient, requires no extensions to the \ntype system, and extends smoothly to incorporate mixed-language working and in-place array updates. 1 \nIntroduction Input/output has always appeared to be one of the less satisfactory features of purely functional \nlanguages: fit\u00adting action into the functional paradigm feels like fitting a square block into a round \nhole. Closely related difficul\u00adties are associated with performing in-place update oper\u00adations on arrays, \nand calling arbitrary procedures written in some other (possibly side-effecting) language. Some mostly-functional \nlimguages, such as Lisp or SML, deal successfully with input/output by using side effects. We focus on \npurely-functional solutions, which rule out side effects, for two reasons, Firstly, the absence of side \neffects permits unrestricted use of equational reasoning and program transformat ~on. Secondly, we are \ninterested in non-strict languages, in which the order of evaluation (and hence the order of any side \neffects) is deliberately unspecified; laziness and Eide effect are fundam ent ally in\u00adimical. There is \nno shortage of pr,)posals for input/output in lazy functional languages, some of which we survey later, \nbut no one solution has become accepted as the consensus. This paper outlines a new approach based on \nmonads (Moggi [1989]; Wadler [1992]; Wadler [1990]), with a num\u00adber of noteworthy features. It is composable. \nLarge programs which engage in 1/0 are constructed by gluing together smaller pro- Permission to copy \nwithout fee all or part of this material is granted provided that the copies are not made or distributed \nfor direct commercial advantage, the ACM copyright notice and the title of the publication and its date \nappear, and notice is given that copying is by permission of the Association for Computing Machinery. \nTo copy otherwise, or to republish, requires a fee and/or specific permission. ACM-20th PoPL-I /93-S. \nC., USA @ 1993 ACM 0-89791-561-5/93/0001/0071 . ..$l .50 Philip Wadler University of Glasgow .glagsow. \nac. uk grams that do so (Section 2). Combined with higher. order functions and lazy evaluation, this \ngives a highly expressive medium in which to express 1/0. performing computations (Sectior, 2.2) ---quite \nthe reverse of the sentiment with which we begim this section. We compare the monadic approach to 1/0 \nwith other stand:u d approaches: dialogues and continuations (Section 3), and effect systems ancl linear \ntypes (Sec\u00adtion 7). It is easily extensible. The key to our implementation is to extend Haskell with \na single form that allc~ws one to call an any procedure written in the programming language C (Kernighan \n&#38; Ritchie [1978]), without losing referential transparency (S,sction 2.3). Using it programmers can \nreadily extend the power of t hc 1/0 system, by writing Haskell functions which call operating system \nprocedures.  It is eficient. Our Haskell compiler has C as its target code. Given a Haskell prog:am \npe rforming arl 1/0 loop, the compiler can produce C code which is very similar to that which one would \nwrite by hand (Section 4).  Its ejjiciency is achieved by applying simple pro\u00adgram t ransformata ons. \nWe use unboxecl data type:~ (Peyton Jones &#38; Launchbury [1!3!11]) to expose rep\u00adresentation and order-of-evaluation \ndetail t,c, code\u00adimproving transformations, rather than relying on ad hoc optimisations in the code generator \n(Sectiml 4.1),  It extmads uniformly to-provtde interlea~ed I/O and reference types (Section 5).  It \nextends uniformly to support ~ncremental array~ with m-place update (Section 6). Our implement a\u00adtion \nis eflicient enough that we can define monolithic Haskell array operations in terms of incremental ar\u00adrays. \nHudak have proposed a similar method based on cent inuations. Our method is more general than his in \nthe following sense: monads can implement continuations, but not the converse.  It is based (only,) \non the Ilimiley-Milner type system. Some other proposals require linear types or existen. tial tj pes; \nours does not.  We have implemented all that we describe in the con\u00ad how can we write a program to \nprint two exclamation text of a compiler for Haskell (Hudak et al. [1992]), with marks? To do so, we \nintroduce two glue combinators: the exception of the extension to arrays and reference types. The entire \n1/0 system provided by our compiler doneIO :: IO () is written in Haskell, using the non-standard extensions \nseqIO :: IOa->IOb->IOb we describe below. The language s standard Dialogue interface for 1/0 is supported \nby providing a function to convert a Dialogue into our IO monad. The system is freely available by FTP. \nWe do not claim any fundamental expressiveness or effi\u00adciency which is not obtainable through existing \nsystems, except where arrays are concerned. Nevertheless we feel that the entire system works particularly \nsmoothly as a whole, from the standpoint of both programmer and im\u00adplementor. ! 2 Overview We need a \nway to reconcile being with dotng: an expres\u00adsion in a functional language denotes a value, while an \n1/0 command should perform an, action. We integrate these worlds by providing a type IO a denoting actions \nthat, when performed, may do some 1/0 and then return a value of type a. The following provide simple \nUnix\u00adflavoured 1/0 operations. getcIO :: IO Char putcIO :: Char -> IO () Here get cIO is an action which, \nwhen performed, reads a character from the standard input, and returns that char\u00adacter; and put cIO a \nis an action which, when performed, writes the character a to the standard output. Actions which have \nnothing interesting to return, such as putcIO, return the empty tuple (), whose type is also written \n(). Notice the distinction between an action and its perfor\u00admance. Think of an action as a script , which \nis per\u00adformed by executing it. Actions themselves are first-class citizens. Howj then, are actions performed? \nIn our sys\u00adtem, the value of the entire program is a single (perhaps large) action, called mainIO, and \nthe program is executed by performing this action. For example, the following is a legal Haskell program. \nmainIO :: 100 mainIO = putcIO ! This is the point at whi<h being is converted to doing: when executed, \nthe put c] O action will be performed, and write an exclamation mark to the standard output. 2.1 Composing \n1/0 operations The functions defined above allow one to define a single action, but how can acticms be \ncombined? Fo uexample, The compound action m seqIO n is performed, by first performing m and then performing \nn, returning whatever n returns as the result of the compound action. (Back\u00adquotes are Haskell s syntax \nfor an infix operator. ) The action doneIO does no 1/0 and returns the unit value, (). To illustrate, \nhere is an action puts IO, which puts a string to the standard output: putsIO :: [Char] -> 10 () putsIO \n[1 = doneIO putsIO (a:as) = putcIO a seqIO{ putsIO as We can now use puts 10 to define a prcgram which \nprints hello twice: mainIO = hello seqIO ( hello where hello = putsIO hello This example illustrates \nthe distinction between an iictiori and its performance: hello is an action which happens tc) be performed \ntwice. The program is precisely equivalent to one in which puts IO hello is substituted for either or \nboth of the occurrences of hello. In short, programs remain referentially transparent. In general, an \naction may also return a value. Again, there are two combinators. The first is again trivial: unit IO \n:: a->IOa If x is of type a, then unit IO x denotes the action that, when performed, does nothing save \nret urn x. The second combines two act ions. bindIO :: IOa->(a->IOb)->IOb If m:: IOaand k:: a -> IO b \nthen m bindIO k denotes the action that, when performed, behaves as fol \u00ad lows: first perform action \nm, yielding a value x of type a, then perform action k x, yielding a value y of type b, and then return \nvalue y. To illustrate, here is an ,action that echoes the standard input to the standard output, (In \nHaskell, \\x -> e stands for a lambda abstraction; the body of the abstraction extends as far as possible.) \necho :: 10 () echo = getcIO bindIO \\a -> if (a == eof) then doneIO else putc IO a seqIO echo seqsIO \n(a:as) = a seqIO seqsIO as The combinators bindIO and unit IO are generalisations of seqIO and done IO. \nHere are definitions for the latter in terms of the former: doneIO = unitIO () m seqIO n = n bindIO \n\\a -> n The combinators have a useful algebra: doneIO and seqIO form a monoid, while bindIO and unit \nIO form a monad (Moggi [1989]; Wadler [1992]; Wadler [1990]).  2.2 Imperative programming It will not \nhave escaped the reader s notice that programs written in the monadic style look rather similar to imper\u00adative \nprograms. For example, the echo program in C might look something like this: echoo { loop: a = getchar(a) \n; if (a == eof) return; else { putchar(a); goto loop; 1 3 (Indeed, as we discuss later, our compiler \ntranslates the echo function into essentially this C code. ) Does the monadic style force one, in effect, \nto write a functional facsimile of an imperative program, thereby losing any advantages of writing in \nx functional language? We be\u00adlieve not. Firstly, the style in which one writes the functional pro\u00adgram \ns internal computation is unaffected. For instance, the argument to put sIO can be computed using the \nusual list-processing operations provided by a functional lan\u00adguage (list comprehensions, map, append, \nand the like). Secondly, the power of higher-order functions and non\u00adstrict semantics can be used to \nmake 1/0 programming easier, by defining new action-manipulating combinators. For example, the definition \nof put sIO given above uses explicit recursion. Here is an alternative way to write putsIO which does \nnot do so: putsIO as = seqs IO (map putcIO as) The map applies putcIO to each character in the list as \nto produce a list of actions. The combinator seqs IO takes a list of actions and perfclrms them in sequence; \nthat is, it encapsulates the recursion. It is easy to define seqs IO thus: seqsIO :: [IO al -> IO () \nseqsIO [] = doneIO or even, using the standard list-processing function foldr, thus: seqsIO = foldr seqIO \ndoneIO To take another example, here is a function which writes a given number of spaces to the standard \noutput: spaceIO :: Int -> IO () spaceIO n = seqs IO (take n (repeat (putc IO  ) ) ) The functions \ntake and repeat are standard list\u00adprocessing functions (with nothing to do with 1/0) from Haskell s standard \nprelude. The function repeat takes a value and returns an infinite list each of whose elements is the \ngiven value. The function take takes a prefix of given length from a list. These necessarily small examples \ncould easily be pro\u00adgrammed with explicit recursion without significant loss of clarity (or even a gain!). \nThe poinl we are making is that it is easy for the programmer to define new glue tc] combine actions \nin just the way which is suitable for the program being written. It s a bit like being able to define \nyour own control structures in an imperative language. 2,3 Calling C directly Since the primitive functions \nput cIO, get cIO, and SC, on must ultimately be implemented b:y a call to the un derlying operating system, \nit is natural to provide the ability to call any operating system function directly. To achieve this, \nwe provide a new form of expressicm, the ccall, whose general form is: ccall proc el . . . e,, Here, \nproc is the name of a C procedure, and ef , . . . . e. are the parameters to be passed to i!. This expression \nis an action, with type IO Int; when performed, it calls the named procedure, and delivers its result \niis the value of the action. Here, for example, are the definitions of getcIO and putcIO: putcIO a = \nccall putchar a getcIO = ccall getchar These ccalls directly invoke the system-provided func\u00adtions; no \nfurther runtime support is necessary. ~Jsing this single primitive allows us to implement our entire \n1/0 system in Haskell. We define ccall to be a language construct rather than simply a function because: \nThe first argument must, be the literal name of the C procedures to be called, and not (say) an expres\u00adsion \nwhich evaluates to a string which is the name of the function. Type information alone cannot express \nthis. e Different C procedures take different numbers of ar\u00adguments, and some take a variable number \nof ar\u00adguments. (It would be possible to check the type\u00adcorrectness of the C call by reading the signature \nof the C procedure, but we dlo not at present do so.) o Different C procedures take arguments of different \ntypes and sizes. (At present, ~e only permit the arguments to be of base types, such as Char, Int, Float, \nDouble and so on, though we are working on extensions which allow structured arguments to be built.) \nTreating ccall as a construct allows these variations to be accommodated without difficulty. 3 Comparison \nwith other 1/0 styles In this section we briefly t:ompare our approach with two other popular ones, dialogues \nand continuations. 3.1 13ialogues The 1/0 system specified for the Haskell language (Hudak et al, [1992]) \nis based on dzalogues, also called iazy streams (Dwelly [1989]; O Donnell [1985]; Thompson [1989]). In \nHaskell, the value of the program has type Dialogue, a synonym for a function between. a list of 1/0 \nresponses to a list of 1/0 requests: type Dialogue = [Response] -> [Request] main :: Dialogue Request. \nand Response are algebraic data types which embody all the possible 1/0 operations and their results, \nrespectively: data Request = Put c Char I Get c data Response = OK I OKCh char (For the purposes of exposition \nwe have grossly simpli\u00adfied these data types compared with those in standard Haskell.) A system wrapper \nprogram repeatedly gets the next request from the list of requests returned by main, interprets and performs \nit, and attaches the re\u00adsponse to the end of the response list to which main is applied. Here, for example, \nis the echo program written using a Dialogue. (In Haskell xs ! ! n extracts the n th element from the \nlist xs.) echo :: Dialogue echo resps = Getc : if (a == eof) then [] else Putt a : echo (drop 2 resps \n) where OKCh a = resps ! ! 1 The difficulties with this programming style are all too obvious, and have \nbeen well rehearsed elsewhere (Perry [1991]): e It is easy to extract the wrong element of the re\u00adsponses, \na synchronasation en-or. This may show up in a variety of ways. If the 2 in dle above prograi-rl was \nerroneously written as <I the program woulci fail with a pattern-mathing error in getCharIO; if it were \nwritten 3 it would deadlock. e The Response data type has to contain a constructor for every possible \nresponse to every request. Even though Put c may only ever return a response OKChar, the pattern-matching \nperformed hy get has to take account of all these other responses. e Even more seriously, the style is \nnot composable: there is no direct way to take two va(ues of typ<, Dialogue and combine them to make \na [arger value of type Dialogue (try it!). Dialogues and the IO monad have equal expressive power, as \nFigure 1 demonstrates, by using Dialogues to emu-. late the IO monad, and vice versa. The function dToIO, \nwhich emulates Dialogues in terms c,f IO is rather cu\u00adrious, because it involves applying the single \nddogu{> d to both bottom (1) and (later) to the real list of responses (Hudak &#38; Sundaresh [1~89]; \nPcyton Jone~{ [1988]). This causes both duplicated work and a space leak, but no more efficient purely-func-fiional \nemulation is known. The reverse function, ioToD does not suffer from these problems, and this asymmetry \nis the main reason that Dialogues are specified as primitive in Haskell. We return to this this matter \nin Section 5 3. 3.2 Continuations The continuation-style 1/0 model (Gordon [1989]; Hudak &#38; Sundaresh \n[1989]; Karlsson [1982]; Perry [1991]) pro\u00advides primitive 1/0 operations which take as one of their \narguments a continuation which says what to do after the 1/0 operation k performed: main :: Result putcc \n:: Char -> Result > Result getcC :: (Char -> Result) -> Result doneC :: Result Dialogue to ID dToIO \n:: Dialogue -> IO () dToIO d = case (d bottom) of c1 -> doneIO (q:qs) -> doReqq bindIO \\r -> dToIO (\\rs \n-> tail (d (Y:rs))) bottorrr :: a bottom = error Should never be evaluated doReq :: Request -> IO Response \ndoReq (GetChar f) = getCharIO f bindIO (\\c -> unitIO (OKChar c)) doReq (PutChar f c) = putCharIO f c \nseqIO unitIO OK I IO to Dialogue I type 10 a = [Response] -> (a, [Request], [Responsel) ioToD :: IO \n() -> Dialogue ioToD action = \\rs -> case (io rs) of (-, qs, -) ->qs unitIO v = \\rs -> (v, [1, rs) blndIO \nop fop = \\rs -> let (vi, qsl, rsl) = op rs (v2, qs2, rs2) = fop VI rsl in (v2, qsl++qs2, rs2) Figurel: \nConverting between Dialogue and IO Using these primitives, the echo program can rewritten as follows: \necho :: Result -> Result echo c =getcC (\\a-> if (a == eof) then then c else putcC a (echo c)) Since \nwe might want to do some more 1/0 after the echo\u00ading is completed, we musl, provide echo with a continuii\u00adtion, \nc, to express what todowhen echo is finished. This extra argument is required for every I/O-performing \nfunction if it is to be composable, a pervasive and tire\u00adsome feature. The above presentation of continuation-style \n1~0 is a lit\u00ad tle different from those cited above. In allthosedescrip\u00adtions, Resultis an algebraic data \ntype, with aconstruc\u00adtorforeachprimitiveI/Ooperation. As with Dialogues, I Continuations to IO I type \nResult = IO () cToIO :: Result -> IO () cToIO r = r putCharC :: File -> Char -> Result -> Result putCharC \nf c k = putCharIO f c seqIO k getCharC :: File -> Char -> (Char -> Result) -> Result getCharC f k = \ngetCharIO f thenIO k lIOtocontinuationsl type IO a = (a -> Result) -> Result ioToC :: IO () -> Result \nioToC action = action (\\ () -> nopC) unitIO v =\\k->kv bindIO op fop =\\k -> op (\\a -> fop ak) putCharIO \nf c = \\k -> putCharC f c (k ()) getCharIO f =\\k -> getCharCf (\\c -> k c) Figure2: Converting between \ncontinuation a~dIO execution is drivenby a wrapper program, which eval\u00aduates main, performs the operation \ninc~icated by the con\u00adstructor, and applies the continuat,ion inside theconstruc\u00adtor to the result. This \napproach has the disadvantage that it requires existential types if polymorphic opera\u00adtions, such as \nthose we introduce later in Section .5.3, are to be supported. An obvious improvement, which wehavenotseen \nprevi\u00adously suggested, is to implement the primitive continu\u00adation operations (such as putcC, getcC and \ndoneC) di\u00adrectly, making the Result type an abstract data type with no operations defined on it other \nthan the primi\u00adtives themselves. This solves theproblern. Fur Continuations are easily emulatedby theIOmonad,ancl \nvice versa, as Figure 2 shows. The comparison between the monadic and continuation approach is further \nex\u00adploredin Section 6. 4 ImplementingmonadicI/O So farwe have shown that an entirel/Osystemcanbe expressed \ni.rlterms ofccall, bindIO, a,ndunitIO, andof course the IO type itself. How are these combinators tc~ \nbe implemented? One possibility is to build them in as primitives, butit turns out tobe both simple randmore \nefficient to implement all except ccall in Haskell. The idea is that an action of type IO a is implemented \nas a function, which takes as its input a value representing the entire current state of the world, and \nreturns a pair, consisting of (a value representing) the new state of the world, and the result of type \na: type IO a = World -> IORes a data IORes a = MkIORes a World The type declaration introduces a type \nsynomym for IO, and the auxiliary algebraic dat at yp,e I ORes simply pairs the result with the new world. \nRecall that the value of the entire program is of type IO (). The type World is abstract, with only one \noperation defined on it, namely ccall. Conceptually, the program is executed by apply\u00ading main to a value \nof type World representing current state of the world, extracting the resulting World value from the \nMkIORes constructor, and applying any changes embodied therein to the real world. If implemented literally, \nsuch a system would be unwork\u00adably expensive. The key to making it cheap is to ensure that the world \nstate is used in a single-threaded way, so that 1/0 operations can bt applied immediately to the real \nworld. One way to ensure this would be to do a global analysis of the program. A much simpler way LSto \nmake IO into an abstract data type which encapsulates the data types IO and IORes, and the combinators \nbmdIO and unit IO. Here are suit able definitions for the latter: uni.tIOaw =MkIORes aw bindIO mk w= \ncase (mw) of MkIORes a w ,-> k a w Notice that blndIO and unitIO carefully avoid duplicat\u00ading the world. \nProvided that the primitive ccall ac\u00adtions are combined only with these combinators, we can guarantee \nthat the ccalls will be linked tn a single, lm\u00adear chain, connect ed by data dependencies in which each \nccall consumes the world state produced by the previous one. In turn this means that the ccall operations \ncan update the real world in place . 4.1 Implementing ccall So much for the combinators. All: that remains \nis the implement at ion of c call. The only complication here is that we must arrange to evaluate the \narguments to the ccall before passing theln to C. This is very similar to the argument evaluation required \nfor built-in functions such as addition, for which we have earlier developed the idea of unbozed data \ntiipes (Pey\u00adton Jones &#38; Launchbury [1991]). These allow represent a\u00adtion and order-of-evaluation \ninformation to be exposed t o code-improving transformations. For example, consider the expression x+x \nwhere x is oft ype Int. The improve\u00adment we want to express is that x need only be evaluated once The \nkey idea is to define the type Int (which is usually primitive) as a structured algebraic data type with \na sin\u00adgle constructor, MkInt, like this: data Int = MkInt Int# A value of type Int is represented by \na pointer to a heap\u00adallocated object, which may either be an unevaluated sus\u00adpension, or a MkInt constructor \ncent aining the machine bit-pattern for the integer. This bit-pattern is of type Int#. Now that Int is \ngiven structure, we can make explicit the evaluation performed by +, by giving the following definition, \nwhich expresses + in terms of the primitive machine operation +#: a+ b=case aof MkInt a# -> case b of \nMkInt b# -> MkInt (a# +# b#) Inlining this definition of + in the expression x+x, and performing simple, \nroutine simplifications, gives the fol-. lowing, in which x is evaluated only once: case x of MkInt x# \n-> MkInt (x# +# x#) (Unboxed types and ccall are not part of standard Haskell. They are mainly used internally \nin our compiler, though we do also make them available to programmers as a non-standard extension.) We \napply exactly the same ideas to ccall. In particular, instead of implementing ccall directly, we unfold \nevery use of c call to make the argument evaluation explicit before using the truly primitive operation \nc call#. For example, the uses of ccall in the definitions of putcIO and get cIO given above (Section \n2.3), are unfolded thus: putcIO a = \\w -> case a of MkChar a# -> case (ccall# putchar a# w) of MkIORes# \nn# w -> MkIORes () w getcIO = \\w -> case (ccall# getchar w) of MkIORes# n# w -> MkIORes (MkChar n#) w \n Like Int, the type Char is implemented as an algebraic data type thus: data Char = MkChar Int# The outer \ncase expression of putcIO, therefore, evalu\u00adates a and extracts the bit-pattern a#, which is passed to \nccall#. The inner case expression evaluates the ex\u00adpression ( ccall# put char a# w), which returns a \npair, constructed by MkIORes#, consisting of the value n# re\u00adturned by the C procedure put char (which \nis ignored), and a new world w (which is returned). In the case of getcIO, the (primitive, unboxed) value \nn# returned by getchar is !Iot ignored as it is in putcIO; rather it is wrapped in a MkChar constructor, \nand re\u00adturned as part of the result. The differences between ccall and ccall# are as follows. Firstly, \nccall# takes only unboxed arguments, ready to call C directly. Secondly, it returns a pair built with \nMkIORes#, contain\u00ading an unboxed integer result direct from the ~ call. The IORes# type is very similar \nto I ORes: data IORes# = MkIORes# Int# World (IORes and IORes# are distinct types, because while our \nextended type system recognises unboxed types, it does not permit polymorphic type constructors, such \nas 10Res, to be instantiated at an unboxed type, such as Int#.) Thirdly, the ccall# primitive is recognised \nby the code generator and expanded to an actual call to ~. SPecifi\u00adtally, the expression: case (ccall# \nproc a# b# c# w) of MkIORes# n# w -> . . . generates the C statement n# = proc(a#, b#, c#); ... This \nsimple translation is all that the code generator is required to do. The rest is done by generic program \ntrans\u00adformations; that is, transformations which are riot specific to 1/0 or even to unboxing (Peyton \nJones &#38; Launchbury [1991]). 4.2 Where has the world gone? But what has become of the world values \nin the final C code? The world value manipulated by the program represents the current stat e of the \nreal world, but since the real world is updated (in ~Jace the world value carries no useful information. \nHence we simply arrange that no code is ever generated to move values of type World. This is easy to \ndo, as type information is preserved throughout the compiler. In particular, the world is never loaded \ninto a register, stored in ZL data structure, or passed to C procedure calls. 1s it possible, then, to \ndispense with the world in the func\u00adtional part of the implementation as well? For example, can we define \nthe I ORes type and b indI O combinat ors like this? ~~~~1~~ ~ = kloRes a = case (m w) of MkIORes a-> \nkaw No, we cannot! To see this, suppose that bindIO was ap plied to a function k which discarded it \n~ argument. Then, if bind I O was unfolded, and the expression (k r w) was simplified, there would be \nno remainmg data dependency to force the call of k to occur afler that of m. A compiler Would be free \nto call them in either order, which destroys the 1/0 sequencing. To reiterate, the world is there to \nfcrm a linear chain of data dependencies between successive ccalls. It is quite safe to expose the representation \nof the IO type to code-improving transformations, because the chain of data dependencies will prevent \nany transformations which reorder the ccalls. Once the code generator is reached, though, the work of \nthe world values is done, so it is safe to generate no code for them. 4-3 Cho eVisited The implementation \nwe have outlined ]s certainly simPle, but is it efficient? Perhaps surprisingly, the answer is an emphatic \nyes. The reason for this is that because the combinators are written in Haskell, the compder can zm\u00adfold \nthem at all their cali sites; that is, perform procedure inlining. Very little special-purpose code \nis required in the cc,mpiler to achieve this effect essentially all that is required is that the Haskell \ndefinitions of bindIO, unitIO, putcIfl and so on, be unfolded by the compi] er. In cent rast, if bindIO \nwere primitive, then every call to blndIO will re\u00adquire the ( obstruction of two heap-allocated closures \nfor its two arguments. Even if bindIO itself took no time at all, this would be a heavy cost. To illustrate \nthe effectiveness of the approach we have outlined, we return to the echo program of Section 2.1. If \nwe take the code there, unfold the calls of seqIO, doneIO, eof, putcIO and getcIO, and do some simplification, \nwe get the following: echo =\\w -> case (ccall# getchar w) of MkIORes# a# WI -> case (a# ==# eof#) of \nT# -> MkIORes () WI F# -> case (ccall# putchar a# wI) of MkIORes# n# W2 -> echo W2 When this is compiled \nusing the simple code-generator described, the following ~ is produced: echoo { int a; a = getcharo; \nif (a==eof) { retVal = unitTuple; RETURN; 1-else { put char(a) ; JUMP( echo ); }3 (JUMP and RETURN are \nartefacts of our use of C M a target machinecode (PeytonJones [1992]). Theyexpandonly to a machine instruction \nor two. ) This is very close to the C one would write by hand! We know ofno other implementation of 1/0 \nwith better efficiency.  4.4 A continuation-passing implementation Like most abstract datatypes, there, \nismorethan one way toimplement IO. Inparticular, itispossible toimplernent the IO abstract type using \na continuation-passing style. The type IO a is represented by a function which takes a continuation expecting \na value of type a, and returns a value of the opaque type Result. type IO a = (a -> Result) -> Result \nIt is easy to implement bmdIO and unitIO: blndIO mk cent = m (\\a -> k a cent) unit IO r cent = cent r \nWhat is there to choose between these this representation of the IO type and the one we described initially \n(Sec\u00adtion 4)? The major tradeoff seems to be this with the continuation-passing representation, every \nuse of bindIO (even if unfolded) requires the construction of one heap\u00adallocated continuation. In contrast, \nthe implementation we described earlier keeps the continuation implicitly on the stack, which is slightly \ncheaper in our system. There is a cost to pay for the e,arlier representation, namely that a heavily \nleft-skewed c~mposit ion of b ind 10s can cause the stack to grow rather large. In contrast, the continuation-passing \nimplementation may use a lot of heap for such a composition, but its stack usage is con\u00adstant. The main \npoint is that the implementor is free to choose the represent ation for IO based only on considerations \nof efficiency and resource usage; the choice makes no differ\u00adence to the interface seen bv the rxosmammer. \n5 Extensions to the IO monad 5.1 Delayed 1/0 So far all 1/0 operations have been strictly sequenced \nalong a single trunk . Sometimes, though, such strict sequencing is unwanted. For example, almost all \nlazy functional-language 1/0 systems provide a readFile primitive, which returns the entire contents \nof a speci\u00adfied file aa a list of characters. It is often vital that this primitive should have lazy \nsemantics; that is, the file is opened, but only actually read when the resulting list is evaluated. \nThe relative ordering of other 1/0 operatiomi and the reading of the file is immaterial (provided the \nfile is not simultaneously written). This lazy reacl is USU. ally implemented by some ad hoc magic) in \nthe runtimc system, but within the monadic framework it is easy tc~ generalise the idea. What is required \nis a new combinator for the IO monad, delay IO, which forks off a new branch from the main trunk : delayIO \n:: IOa->IOa When performed, (delayIO action) immediately re\u00adturns a suspension which when it M subsequently \nforced will perform the 1/0 specified by act ion. The relative interleaving of the 1/0 operations on \nthe trunk and the branch is therefore dependent on the evaluation order of the program. The delayIO combinator \nis dangerous (albeit useful), be\u00ad cause the correctness of the program now requires tha~, arbitrary interleaving \nof 1/0 operations on the trunk and (branch cannot affect the result. Thts conddzon cannot be guaranteed \nby the compiler; it is a proof obli\u00ad gation for the programmer. In practice, we expect, that, delayIO \nwill be used mainly by system programmt;rs. With the aid of delayIO (and a few new primitives suck as \nfOpenIO), it is easy to write a lazy readFile: readFile :: [Char] -> IO [Char] readFile s = f OpenIO \ns bindIO \\f -> delayIO (lazyRd f) lazyRd :: File -> IO [Char] lazyRd f = readChar f cbindIO \\a -> if \n(a == eof) then fCloseIO f seqIO unitIO [1 else delayIO (lazyRd f) bi.ndIO \\as -> unit IO (a:as) The \ndelavIO combinator movides essentially the Dower of Gordon s suspend operator (Gordon [1989]). Implementation. \nA nice feature of the implementation technique outlined in Section 4 is that delayIO is very easy to \ndefine: delayIO m = \\W -> MkIORes res w where res = case (mw) of MkIORes r w -> r In contrast to bindIO, \nnotice how delayIO duplicates the world w, and then discards the final world w of the de\u00adlayed branch; \nit is this which allows the unsynchronised interleaving of 1/0 operations on the branch with those on \nthe (trunk . 5.2 Asynchronous 1/0 An even more dangerous but still useful combinator is perf ormIO, \nwhose type is as follows: performIO :: IO a -> a It allows potentially side-effecting operations to take \nplace which are not attached t{) the main trunk at all! The proof obligation here is tl~at any such side \neffects do not affect the behaviour of the rest of the program. An obvi\u00adous application is when one wishes \nto call a C procedure which really is a pure function; procedures from a numer\u00adical analysis library \nare one example. Implementation. The implementation is quite simple: performIO m = case (m newWorld) \nof MkIORes r w -> r Here, newWorld is a value of type World conjured up out of thin air, and discarded \nwhen the action m has been performed. 5.3 Assignment and reference variables Earlier, in Section 3.1, \nwe discussed the apparently in\u00adsoluble inefficiency of dTo I O, the function which emu\u00adlates Dialogues \nusing the IO monad. We can solve this problem by providing an extra general-purpose mecha\u00adnism, that \nof assignable Teference types and operations over them (Ireland [1989] ): newVar ::a -> IO (Ref a) assignVar \n:: Ref a -> a -> IO () deRefVar :: Refa->IOa The call newVar x allocates a fresh variable :ontaining \nthe value x; the call ass ignvar v x assigns value x to variable v; and the call d eRef Var v fetches \ntl.e value in variable v. By making these side-effecting operations part dTo IO :: Dialogue -> IO () \ndToIO dialogue = newVar (error Synch ) bi.ndIO \\rsV -> delayIO (deRefVar rsV) bindIO \\rs -> run (dialogue \nrs) rsV run :: [Request] -> Ref [Response] -> IO () run [] v = doneIO run (req: reqs ) v = doReq req \nbindIO \\r -> newVar (error Synch ) bindIO \\rsV -> \\rs -> Figure3: Efficient conversion from Dialogue \nto IO of the IO monad, we make sure that their order of evalu\u00adation, and hence semantics, is readily \nexplicable. With the aid of these primitives it is possible to write an efficent emulation of Dialogues \nusing IO (Figure 3). The idea is to mimic a system which directly implements Dialogues, which follows \nthe processing of each requesi, with a destructive update to add a new response to the end of the list \nof responses, Notice the uses of del ayI O, which reflects the fact that there is no guarantee thal, \ndialogue will not evaluate a response before it has emit\u00adted a request. If this occurs, the un-assigned \nvariable is evaluated, which elicits a suitable error message. References in languages such as ML require \na weakened form of polymorphism in order to maintain type safety (Tofte [1990]). For instance, in ML \na fresh reference to an empty list has type _a list ref, where the type variable )_a is weak, and so \nmay be instantiated only once. In contrast, here a fresh reference to an empty list has type IO (Ref \na), and the type variable a is normal. But no lack of safety arises, because an expression of this type \nallocates a new reference each time it is evaluated, The only way to change a value of type IO (Ref a) \nto one of type Ref a is via bindIO, but now the variable of type Ref a is not let-bound, and so can only \nbe instantiated once anyway. Hence the extra complication of weak type variables, required in languages \nwith side effects, seems unnecessary here. (We re indebted to Martin Odersky for this observation.) 6 \nArrays The approach we take to 1/0 smoothly extends to ar\u00adrays wit h in-place update. Hudak has recently \nproposecl a similar method based on continuations. For 1/(), the monad and continuation approaches are \ninterdefinable. For arrays, it turns out that monads can implement con\u00adtinuations, but not the converse. \nLet Arr be the type of arrays taking indexes of type Ind and yielding values of type Val. There are three \nopera\u00adtions on this type. new :: Val -> Arr lookup :: Ind -> Arr -> Val update :: Ind -> Val -> Arr -> \nArr The call new vreturns anarray with all entries set to v; the call lookup i x returns the value at \nindex i in ar\u00adray x; and the call update i v x returns an array where index i has value v and the remainder \nis identical to x. The behaviour of these operations is specified by the usual laws. lookup i (new v) \n= v lookup i (update i v x) = v lookup i (update j v x) = lookup 1 x where i # j in the last equation. \nIn practice, these oper\u00adationswould bemorecomplex; one needs away to specify the array bounds, for instance. \nBut the above suffices to explicate the main points. The efficient way to implement the update operation \nis to overwrite the specified entry of the array, but in a pure functional language this is only safe \nif there are no other pointers to the array extant when the update operation is performed. An array satisfying \nthis property is called single threaded, following Schmidt (Schmidt [1985]). As an example, consider \nthe following problem. An oc\u00adcurrence is either a dejilJitzon pairing an index with a value, or a use \nof an index. data Occ = Def Ind Val I Use Ind For illustration take Ind = Int and Val = Char. Given a \nlist os of occurrences, t] Lecall uses os returns for each use the most recently defined value (or - \nif there is no previous definition). If . 0s = [Def 1 a , Def 2 b , Use 1> Def 1 c , Use 2, Use I] then \nuses os = [ a , b , c ]. Here is the code. uses :: [Occ] -> [Val] uses 0s = loop os (new - ) loop :: \nCOCCI -> Arr -> [Vail loop [1 x= [1 loop (Def iv:0s) x=loop 0s(update ivx) loop (Use i :0s) x = lookup \ni x :loop OSX The update in this program can be performed by over\u00adwriting, but some care is required \nwit l-. the order of eval\u00aduation. In the last line, the lookup must occur be~oi-e the recursive call \nwhich may update the array. Some work has been done on analysing when update can be pm-formed in\u00adplace, \nbut it is rather tricky (Bless [1989]; Hudak [1986]). 6.1 Monadic arrays We believe that single threading \nis too important to leave to the vagaries of an analyser. Instead, we use monads tc, guarantee single \nthreading, in much the same way as was done with 1/0. Analogous to the type 10 a (the monad of 1/0 actions), \nwe provide an abstract type A a (the monad of array transformers). newA :: Val->Aa->a lookupA :: Ind \n-> A Val updateA :: Ind -> Val -> A () unitA ::a-> Aa bindA :: Aa->(a->Ab)->Ab For purposes of specification, \nwe can define these in terms of the proceeding operations as followti. type Aa = Arr -> (a, Arr) newAvm \n=fst (m(new v)) lookupA i = \\x -> (lookup i x, x) updateA i v = \\x -> ((), update i v x) unitA a = \\x \n-> (a,x) m bindA k = \\x -> let (a, y) = m x inkay A little thought shows that these operations are \nindeed single threaded. The only operation t,l at could duplicate the array is lookupA, but this may \nhe implemen~,ed as follows: first fetch the entry at the given index in the array, and then return the \npair consisting of this value and the pointer to the array. To enforce the necessary sequencing, we augment \nthe above specification with the requirement that lookupA and updat eA are strict in the index and array \narguments (but need not be strict in tht: value). The above is given for purposes of specification only \n the actual implementation is along the lines of Section 4. For convenience, define s eqA in terms of \nb indA in the usual way, m seqA n = m bindA \\a -> n Here is the definition-use problem, recoded in \nmonadi{; style. uses :: [Occl -> [Vail  uses os = neraA 2-> (1OOPA 0S) uses os = newC )-3 (loopC os \nunitC) loopA :: COCCI -> A [Vail loopA [1 = unitA [1 loopA (Def i v : OS) = updateA i v seqA loopA OS \n loopA (Use i. : OS) = lookupA i bindA( \\v -> loopA OS bindA \\vs -> unitA (V:VS) This is somewhat lengthier \nthan the previous example, but it is guaranteed safe to implement update by over\u00adwriting. 6.2 Continuation \narrays An alternative method of guaranteeing single threading for arrays has been proposed byHudak [1992]. \nLike the previous work of Swarup, Reddy &#38; Ireland [1991], it is based on continuations, but unlike \nthat work it requires no change to the type system. As with the array monad, one defines an abstract \ntype supporting various operations. The type is C z, and the operations are as follows. newC :: Val->Cz->z \nlookupC :: Ind -> (Val -> C z) -> C z updateC :: Ind->Val->Cz->Cz unitC ::2 ->CZ Here acontinuation, \noftype C z,represents the remaining series of actions to be performed on the array, eventual] y returning \n(via unit C) a value of type z. For purposes of specification, we can define these in terms of the array \noperations as follows. type Cz = Arr ->z newCvc =c(new v) lookupC i d =\\x -> d (lookup i x) x updateC \ni vc=\\x -> c(update i vx) unitC z =\\x->z Again, these operations are single threaded if lookupC and \nupdat eC are strict in the index and array arguments. For convenience, define m$c=mc This lets us \nomit some parentheses, since m ( \\x -> n) becomes m $ \\x -> n. Here is the definition-use problem, recoded \nin continua\u00adtion style. uses :: [Occ] -> [Val] loopc :: COCCI -> ([Vail -> C z) -> C z loopc [1 c =c[] \nloopC (Def i v :OS)c=updateC i v $ loopc 0s c loopC (Use i : 0s) c = lookupC i $ \\.v -:* loopc 0s $ \n\\vs -> c (V:vs) This is remarkably similar to the monadic style, where $ takes the place of bindA and \nseqA, and the current continuation c takes the place of uni.tA. (If c plays the role of unit A, why do \nwe need unit C? Because it acts as the top level continuation. ) However, there are two things to note \nabout the ccmtin uation style, First, the types are rather more complex compare the types of loopA and \nlc>opC. !$econd, the monadic style abstracts away from the notion of contin\u00aduation so there are no occurrences \nof c cluttering the defintion of loopA. 6.3 Monads vs. continuations We can formally compare the power \nof the two approaches by attempting to implement each in terms of the other. Despite their similarities, \nthe two approaches are not, equivalent. Monads are powerful enough to implement continuations, but not \n(quite) vice versa. To implement continuations in terms of monads is sim \u00adplicity itself. type Cz=Az \n= lookupC i d = lookupA i bindA d updateC i v c = updateA i v cseqA ( c unitC = unitA newCvc newAvc It \nis an easy exercise in equational reasoning to to prove that this implementation is correct in terms \nc)f the speci-. fications in Sections 6.1 and 6.2. The reverse implementation is not possible. The trouble \nis the annoying extra type variable, z, appearing m the types of lookupC and updateC. This forces the \nintroduc\u00adtion of a spurious type variable into any attempt to define monads in terms of continuations. \ninstead of a type A a, the best one can do is to define a type B a z. Here arc the types of the new operations. \nnewB :: Val->Baa->a lookupB :: Ind -> B Val z updateB :: Ind -> Val -> B () z unitB ::a ->Baz bindB :: \nBaz->(a->Bbz)->B bz And here are the implementations in terms of continua\u00adtions, type Baz= (a-> cz)->cz \n newB v m = newC v (munit C) lookupll i = \\d -> lookupC i d updateB i v = \\d -> updateC i v (d ()) unitB \na =\\d->da m binclB k = \\d -> m(\\a -> k ad) Again, it is easy to prove this implementation satisfies \nthe given specifications. So monads are more powerful than continuations, but only because of the types! \nIt is not clear whether this is simply an artifact of the 13indley-Milner type system, or whether the \ntypes are revealing a difference of funda\u00admental importance. (Our own intuition is the latter but it \ns only an intuition. ) 6,4 Conclusion The 1/0 approach outlined earlier manipulates a global state, \nnamely the entire state of the machine accessible via a (2 program. What has been shown in this section \nis that this approach extends smoothly to manipulating local state, s~~h as a single array. Further, \nalthough the monad and continuation approaches are interconvertible for 1/0, they are not for arrays: \nmonads are powerflll enough to define continuations, but not the reverse. For actual use with Haskell, \nwe require a slightly more SO\u00ad phisticated set of operations. The type A must take extra parameters \ncorresponding to the index and value types, the operation newA should take the array bounds, an! so \n on. By using a variant of newA that creates an unml\u00ad tialised array, and returns the array after all \nupdates are finished, it is possible to implement Haskell primitives for creating arrays in terms of \nthe simpler monad opera\u00ad tions. Thus the same strategy that works for implement\u00ad in.g 1/0 should work \nfor implementing arrays: use a small set of primitives based on monads, and depend on pro\u00ad gram transformation \nto n~ake this adequately efficient. One question that remains is how well this approach ex\u00adtends to \nsituations where one wishes to manipldate more than one state at a time, as when combining 1/0 with array \noperations, or operating on two arrays. In this re\u00adspect effect systems or linear t:ypes may be superior; \nsee below. 7 Related work 7.1 Effect systems Gifford and Lucassen introduced effect systems which use \ntypes to record the side-effects performed by a pro. gram, and to determine which components ot aprogram \ncan run in parallel without interference (Giiforcl &#38; Lu\u00adcassen [1986]). The original notion of effect \nwas fairl~ crude, there being only four possible effects: pure (no ef, feet), allocate (may ailocate \nstorage), function (may read storage),procedure (may write storage]. New systems arc more refined, allowing \neffects to be expressed separately for different regions of store (Jouvelot &#38; Gifford [ 1991]). A \ntheoretical precursor of the effects work is that of Reynolds, which also used types to record where \neffects could occur anti where parallelism was allowed (l%eynolds [1981]; Fteynoldk [1989]). Our work \nis si]milar to the above in its commitment to use types to indicate effects. But effect systems are de\u00adsigned \nfor impure, strict functional languaes, where the order of sequencing is inzplwxt. Our wmk is designed \nfor pure, lazy functional languages, and I,he purpose of the bind operation is to make sequencin$ explficit \nwhere it is required. With effect systems, one may use l;he usllal laws of equational reasoning on any \nprogralm segment wi thoul a write side effect. Our work differs in that the law); of equational reasoning \napply even where side e~rcts arc allowed. This is essential, because the optimisation phas~: of our compiler \nis based on equational reasoning. On the other hand, effect sYstems make it very eas~ to combine programswith \ndifferent effects. In our ap preach, each different effect would correspond to a differ\u00ad ent monad type \n(one for IO, one for each array manip-\u00adulated, and so on), and it is not so clear how one goes about \ncombining effects. 7.2 Linear lYPe~ The implementation of the 10 monad given in Section 4 is safe because \n(and only because) the code that manipu-. lates the world never duplicates or destroys it. We guar\u00adantee \nsafety by making the IO type abstract, so that user has no direct access to the world. An alternative \nis to allow the user access to the world, but introduce a type system that guarantees that the world \ncan never be duplicated or destroyed. .4 number of type systems have been proposed along such lines. \nSome have been Ibased on Girard s linear logic (Girard [1987]), and this remains an area of active exploration \n(Amam. sky [1990]; Gutiman &#38; Hudak [1990]; Wadler [1990]). An. other is the type system proposed \nby the Nijmegen Clean group, which is more ad-hoc but has been tested in prac\u00adtical applications similar \nto our own (Achten, Groningen &#38; Plasmeijer [1992]). For example, here is the echo program again, \nwritten in the style suggested by the Clean 1/0 system: echo :: File -> File -> World -> World echo fi \nfo w=if a==eof then WI else echo (putChar fo a wI) where (wl,a) = getChar fi w Comparedto the monad \napproach, this suffers from a number of drawbacks: programs become more cluttered; the linear type system \nhas to be explained to the pro\u00adgrammer and implemented in the compiler; and code\u00adimproving transformations \nneed to be re-examined teen\u00adsure they preserve linearity. The latter problem may be important; Wakeling \nfound that some standard transfor\u00admations could not be performed in the presence of linear\u00adity (Wakeling \n[1990]). The big advantage of a linear type system is that it en\u00adables us to write programs which manipulate \nmore than one piece of updatable state at a time. The monadic and continuation-passing presentations \nof arrays given above pass the array around implicitly, and hence can only eas\u00adily handle one at a time. \nThis is an important area for future work. On the practical side, the Clean work is impressive. They \nhave written a library of high-level routines 10 call the Macintosh window system, and demonstrated that \nit is possible to build pure functional programs with sophisti\u00adcated user interfaces. The same approach \nshould work for monads, and another area for future work is to confirm that this is the case. 8 Conclusions \nand further work We have been pleasantly surprised by both the expres\u00adsiveness and the eficiency of the \napproach we have de\u00adscribed. For example, we have found that while it is pos\u00adsible to write composable \n1/0 programs in other styles, it is almost impossible not to do so in using the monadic approach. Plenty \nremains to be done, We are working on our im\u00ad plementation of arrays; this in turn feeds into the ability \nto pass structured values in ccalls; we have not yet im\u00ad plemented assignable reference types. More \nimportantly, the model we have desribed concerns only the 1/0 in~rastructure. Much more work needs to \nbe done to design libraries of functions, built on Lop of this infrastructure, which present a higher-level \ninterface to the programmer (Achten, Groningen &#38; Plasmeijer [1992]; Hammond, Wadler &#38; Brady [1991]). \nAcknowledgements This work took place in the context of the team building the Glasgow Haskell compiler: \nCordy Hall, Kevin Ham\u00ad mond and Will Part sin. David Watt, Joe Morris, John Launchbury also made very \nhelpful suggestions about our presentation. We gratefully acknowledge their help. References S Abramsky \n[1990], Computational interpretations of linear logic, DOC 90/20, Dept of Computing, Imperial College. \nPM Achten, JHG van Groningen &#38; MJ Plasmeijer [1992], High-level specification of 1/0 in func\u00adtional \nlanguages, in Proc Glasgow Workshop on Functional Programming, Launchbury et al, ed., Springer Verlag. \nA Bless [Sept 1989], Update analysis and the efficient im\u00adplementation of functional aggregates, in l?unc\u00adtional \nProgramming Languages and Computer Architecture, London, ACM. A Dwelly [Sept 1989], (Dialogue combinators \nand dy namic user interfaces , in Functional Program\u00adming Languages and Computer Architecture London, \nACM. DK Gifford &#38; JM Lucassen [Aug 1986], Integrating func tional and imperative programming, in \n.4GW Conference on Lisp and Functional Proqram \u00adming, MIT, ACM, 28 38. J-Y Girard [1987], Linear Logic, \nTheoretical Computer Science 50, 1-102. A Gordon [Feb 1989], PFL+: a kernel scheme for func\u00adtional 1/0) \nTR 160, Computer Lab, University of Cambridge. JC Guzman &#38; P Hudak [1990], Single-threadecl poly\u00admorphic \nlambda idcuius , in Proc 5th Annual IEEE Symposium on Logic in Computer Science. K Hammond, PL Wadler \n&#38; D Brady [1991], Imperate: be imperative, Department of Computer Science, Univ of Glasgow. P Huclak \n[July 1992], Confirmation-based mut able ab-SL Peyton Jones &#38; J Larmchbury [Sept 199 1], Unboxed. \nstract datatypes, or how to have your state and values as first class citizens, in Functional F ro\u00admunge \nit too, YALEU/DCS/EtR-914, Depart-gramming Languages and Computer Architec\u00adment of Computer Science, \nYale University. ture, Boston, Hughes, ed., LNCS 523, Springer Verlag. P Hudak, SL Peyton Jones, PL Wadler, \nArvind, B Boutel, J Fairbairn, J Fasel, IVl Guzman, K Hammond, J J Reynolds [1981], The essence of Algol, \nin .41gorithmic Hughes, T Johnsson, R Kieburtz, RS Nikhil, W Languages, de Bakker &#38; van Vliet, eds., \nNorth Partain &#38; J Peterson [May 1992], (Report on the Holland, 345-372. functional programming language \nHaskell, Ver\u00adsion 1.2 , SIGPLAIV Notices 27. J Reynolds [1989], Syntactic control of interference, part, \nII, in International Colloquium on Automata, P Hudak &#38; RS Sundaresh [March 1989], On the ex-Languages, \nand Programming. pressiveness of purely-functional 1/0 systems, YALEU/DCS/RIL-665, Department of Com- \nDA Schmidt [Apr 1985], (Detecting global variables in de\u00adputer Science, Yale University. not ational \nspecifications, > TOPLAS 7, 299 3 10. Paul H.rrdak [Aug 1986], A semantic model of reference V Swarrrp, \nUS Reddy &#38; E Ireland [Sept 1991], Assign-. counting and its abstract ion , ) Proc ACM Con-ments for \napplicative languages, irl Functional ference on Lisp and Functional Programming. Programming Languages \nand Computer Archi. tecture, Boston, Hughes, ed., LNCS 523, Springer E Ireland [March 1989], Writing \ninteractive and file-verlag, 192-214. processing functional programs, MSC thesis, Victoria University \nof Wellington. SJ Thompson [1989], Interactive functional programs -a method and a formal semantics , \nin Declarative P Jouvelot &#38; D Gifford [Jan 1991], Algebraic reconstruc-Programming, DA Turner, ed., \nAddison }Vesley. tion oft ypes and effects, in 18 th ACM Sympo\u00adsium on Principles of Programming Languages \nM Tofte [Nov 1990], Type inference for polymorphic ref. (POP-L), Orlandc), ACM. erences, Information \nand Computation 89. PL Wadler [1990], Linear types can change i,he world! , system, Chalmers Inst, Goteborg. \nKent Karlsson [1982], (Nebula, -a functional operating in Prc)gramming concepts and methods, M Bro~ \n&#38; C Jones, eds., North Holland. B W Kernighan &#38; DM Ritchie [1978], The C programming language, \nPrentice Hall. PL Wadler [Jan 1992], The essence of functional pro\u00adgramming, in Proc Principles of Programming \nE Moggi [June 1989], Computational lambda calculus Languages, ACM. and monads , in Logic in Computer \nSciencej Cal\u00adifornia, IEEE. PL Wadler [June 1990], Comprehending monads, in Proc ACM Conference on .Lisp \nand Functional JT O Donnell [1985], Di alogu.es: a basis for construct-Progr,smming, Nice, ACM. ing programming \nenvironments , in Proc ACM Symposium on Language Issues in Programming D Wakeling [Nov 1990], Linearity \nand laziness, PhD Environments, Seattle, ACM, 19 27. thesis, Department of Computer Science, Univer\u00adsity \nof York. N Perry [1991], The implementation of prac:ical fun,:\u00adtional progr~mming languages, PhD ~hesis, \nInl\u00adperial College, Lcmdon. SL Peyton Jones [1992], (Implementing lazy functional languages on stock \nhardware: the Spi~eless Tag\u00adless G-machine, Journal of Functional Prograrr~\u00adming (to appear). SL Peyton \nJones [Ott 1988], Converting streams to corl\u00adtinuations and vice versa, Electronic mail on Haskell mailing \nlist.    \n\t\t\t", "proc_id": "158511", "abstract": "<p>We present a new model, based on monads, for performing input/output in a non-strict, purely functional language. It is composable, extensible, efficient, requires no extensions to the type system, and extends smoothly to incorporate mixed-language working and in-place array updates.</p>", "authors": [{"name": "Simon L. Peyton Jones", "author_profile_id": "81100271851", "affiliation": "", "person_id": "PP14102537", "email_address": "", "orcid_id": ""}, {"name": "Philip Wadler", "author_profile_id": "81100173596", "affiliation": "", "person_id": "PP39085016", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/158511.158524", "year": "1993", "article_id": "158524", "conference": "POPL", "title": "Imperative functional programming", "url": "http://dl.acm.org/citation.cfm?id=158524"}