{"article_publication_date": "03-01-1993", "fulltext": "\n Specifying the Correctness of Binding-Time Analysis Mitchell Wand* College of Computer Science Northeastern \n360 Huntington Boston, MA wandQflora.ccs. Abstract Mogensen has exhibited a very compact partiaf evaluator \nfor the pure lambda calculus, using binding-time analysis followed by specialization. We give a correctness \ncriterion for this partial evaluator and prove its correctness relative to this specification. We show \nthat the conventional properties of partial evaluators, such as the Futamura projections, are consequences \nof this specification. By considering both a flow analysis and the transformation it justifies together, \nthis proof suggests a framework for the incorporating flow analyses into verified compilers. Introduction \n Mogensen [1 I] has exhibited a very compact self-applicable partial evaluator for the pure lambda-calculus, \nusing binding\u00adtime analysis followed by specialization. He suggests that one virtue of such a minimalist \napproach is that, it might allow mathematical analysis of the resulting structure. He~e we do just that: \nwe state a correctness criterion for such an off-line partial evaluator and show that Mogensen s partial \nevaluator is correct with respect to this specification. The specification is similar to, but simpler \nand strougel than, the specification given by Gomard [5]. We then show that, the typical properties of \npartial evaluators, such as the Fn\u00adtamnra projections, follow from the specification. Although there \nare some technical details in the ptoofs, the major contribution of this paper is the formulation of \nthe specification. The traditional specification of partial evalu\u00adators is insufficiently specific to \ngenerate a correctness proof for partial evaluation of higher-order terms containing free variables. \nFor example, Mogensen writes that his represen\u00adtation strategy [has] the effect that the partial evaluator \nwill only work on closed A-terms. Yet in the course of par\u00adtially evaluating a closed term, his algorithm \nmanipulates open terms as well. So the correctness criterion nlust be strong enough to specify the behavior \nof the algoritll m on * Work supported by the National Science Foundation uncle! grant numbers CCR-9(102253 \nand CCR-9014603 Permission to copy without fee all or part of this material is granted provided that \nthe copies are not made or distributed for direct commercial advantage, the ACM copyright notice and \nthe title of the publication and its date appear, and notice is given that copying is by permission of \nthe Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific \npermission. ACM-20th PoPL-l/93-S.C., USA @ 1993 ACM 0.89791 .561 -5/93 /0001 /0137 .,.$1 .50 University \nAvenue, 161CN 02115, USA northeastern.edu open terms. Our specification simplifies and clarifies the \nspecification given by Gomard [5] for his self-applicable par\u00adtial evaluator. Our criterion is strong \nenough to specify the behavior of the partial This paper is part ture of flow analysis analysis is to \nannotate about the behavior of optimizations to the propositions. However, evaluator when it is applied \nto itself. of a larger investigation into the na\u00adand optimization. The goal of flow a program with certain \nproposition< that program. One can then apply program that are justified by those it has proven remarkably \ndifficult to specify the semantics of these propositions in a way that justifies the resulting optimization. \nFollowing Gomard [4], .. we formulate bmdmg-tlrne ana,l-vsls as a solution of a, set of constraints \non the annotation of a program (that is, as an annotated type-inference tree for a certain type system). \nOur correctness result shows that any solution to this set of constraints justifies the resulting staging \ntransformation [7]. Thus our result holds for any algorithm that solves such constraint systems. This \nwork suggests that the proposition associated wit h a flow analysis can simply be that the optimization \nworks.) This avoids a morass of model-theoretic details, at the ex\u00adpense of needing to know what it meaus \nfor the proposed optimization to work for each possible resnlt of the flow anal\u00adysis. Thus the ffow analysis \nand its enabled optiml zation should be proved colrect together This paper is olganized as follows: Ill \nSection 2 we ShOW how A-terms can be coded as normal forms insicle the A\u00adcalculus. In Sections 3 and \n4 we present, Mogensen s self\u00adevaluator and partial evaluatol, respectively. In Section 5 we state and \npro~w our rna,in theorem relatir~g binding-time analysis and pal tial evaluation, and show how the expected \nspecialization behavior of the partial eval uatol follows from the main theorem. In Section 6 we consider \nself-application of the paltial evaluator, and show how the Futamura projec\u00ad tions can be derived from \nthe main theorem. Section 7 com\u00ad pares our work to other approaches to binding-time analysis, and Section \n8 gi ves some conclusions. 2 Representirrg A-Terms in Pure A-calculus When consideli l~g the expressive \npower of pul e ~-cd lcnltis, one lnust show how the cluarrtities to be manipulated are explessed in the \n~-calculus. Thus, when considering comp\u00ad utations on the Integers, one first {lefines a set of numerals, \nthat i.~, a mappl[i~ associating each iute~er n with a closed normal form [nl. Then we say a function \n~ on the integers is representable if there exists a A-term F such that for all n, F[nl = [f(n)l . To \ncarry out this program for J-terms themselves, Mo\u00adgensen uses the following coding: [Zl z Aabc.ax ~MNl \n-Aabc.fr[Ml [iv] [k.Ml = Jabc.c(kz. [Ml) This coding uses two tricks: First, it uses the coding of the \nsum A+B as(VC)((A + C) + (B + C) + C), sothat each element of a (sum domain ) is modelled as its own \ncase\u00adfunction. Thus a case expression case m of Var(z)%. ..z... App(M, N)~. .. N... N... Abs(M)~... M... \nwould be represented as m(k... .x. ..)rrzn . . ..rrz. ..n. ..)(Jm . . .. m...) Second, it uses so-called \nhigher-order abstract syntax [13], in which the binding operators in the represented lan\u00adguage are modelled \nby binding in the lambda-calculus. Any free occurrences of z in AZ .M will be bound by the z in Jx. [M \n]. The usual terminology of partial evaluation discusses a language L and the assertion L(pgm, da ta) \n= output, For the J-calculus, where the inputs and oniputs are J-terms themselves, we can interpret this \nassertion using the coding [-1, as Definition 1 L(M> N) + Ru M(N1 =,, [R1 S zm?/arly, we write L(IV. \n[;V,, ,Nk]) E R +=+ MIN,l . [N~~ =@ [Rl Hele we write M R N to signify that M and N are identical up \nto conversion. ~ ben the inputs or outputs come from other sets. then we will use whatever corfings are \nappropri\u00adate. 3 The Self-Interpreter A self-interpreter is a ~-term E such that E[J41 =@ M [1]. E needs \nto have the properties that E(V(/r~) =Z E(App MN) = (EM)(EN) E(Abs 14} = Au. E(Mv) Here we are using \ninformal notation for abstract syntax of the J-term input, to E. It is easy to display a term that satisfies \ntflese propert ]es: 1? = Y(,+ern.m(ku. x) (Arnn.(c)n)(e?r)) (Arn.Au.e(mu))) where l IS the standald fmpotnt \noper,~tol It is also easy to sho,v, by structu~al induction on J1, that EIM1 = JI for any )-terni M [10]. \nThns L(JP/, N) ~ R iff EIM1 [iYl = R. Furthermore, L(E, [M. N]) ~ R iff EIE1 [Ml (NI = [Rl iff E(M1 [~} \n1 = [Rl iff L(!f, A ) s h . so F is an Illterp]eter under the usual definition ( e.g. [.5]) 4 The Partial \nEvaluator The partial evaluator is divided into two phases: binding\u00adtime analysis and specialization. \nThe binding-time analysis produces an annotated version of the input program by as\u00adsigning each phrase \nof the program a type in a system due to Gomard [4]. The types are as follows: t::=d[vlt+tlflw.t where \nin ~v.t, t must be of the form (tl+ ...-+tn+ d), and all occurrences of v in t must be positive. The \nintention is that the type d denotes the type of dynamic quantities: i.e., representations of A-terms. \nThe other types are built from d in the usual way, and correspond to the types one might find in an ordinary \ncompiler. Annotated terms are given by the following grammar: w ,:= u I Ww I Av.w 1w_w 1Av.w The first \nthree productions correspond to static (compile\u00adtime) calculations, and the last two correspond to dynamic \n(residual, run-time) operations, which correspond at compile\u00adtime to emitting code. The annotated terms \nare produced by decorating a type-inference tree. Thus, instead of consid\u00adering typing judgments of the \nform A F M : t, we consider judgments of the form A } Ii4 : t[W], meaning that under hypotheses A, term \nM can be assigned type twith anno\u00adtated term W. When this jndgernent is true, we say that W is an annotation \nof Jf with type t. The type system can be written as follows: A ~X:A(z) [z] A[z H t,] kM :t> [W] .4t-(Az \nM) :f, + t, [k. w] AEM:d[W,l A1-N:dlW,l A 1-MN : d [Wl_Wz] We can use a representation scheme for annotated \nterms similar to that for ordinary ~-terms: To specialize a program, oue must interpret the anno\u00adtated \nterm, evaluating all the st,atlc (compile-time) parts (just like E) and emitting code for all the dynamic \n(resid\u00adual) parts. This can be expressed in terms of abstract syntax by: P(Var z) =x P(App WW ) = (PW)(PW \n) P(Abs ~) = AV.P(WV) P(Dapp WW ) = App(PW)(PW ) P(Dabs W) = Abs(Av.P(W(Va~ .))) This can be realized \nby a A-term P operating on the l j represent ation as follows: P = Y(ApAm.??l(Az.z) (huw .(pw)(pzu )) \n(Awilv.p(wv)) (Aww .klbc.b(pw) (pw )) ow./labc.c(Jv.p( m(Aabc.av))))) 5 The Correctness Criterion All \nthis is in [11]. Now we address something new: the ques\u00adtion of the correctness of P. The correctness \ncondition must somehow involve the binding-time analysis as well: other\u00adwise one could change the binding-time \nanalysis arbitrarily without invalidating P, which is clearly impossible. Further\u00admore, the usnai notion \nof partial evaluation only involves the programs that can be assigned relatively simple types like d \nor d -d, while the algorithm clearly involves arbitrary types. Hence our correctness criterion must be \ndefined at all types. We do this by using the technique of logical relations. For eve~y closed type t, \nwe define a binary relation R, on ~-terms as follows: R~(h[, M ) u EM = M R,-,(M, M ) u (VN, N )(R, (N, \nN ) =+ R,(MN, M N )) We extend this to recursive types (pv.t) by defining R(PV ~, as the evident least \nfixpoint; this is defined since all the occurrences of v in f. are positive. The details are shown in \nAppendix A. For substitutions p and p we say (p, p ) + A iff for all x G dom A, ll~(z)(p(z), p (z)). \nNow we cm state-the main theorem: Theorem 1 (Main Theorem) if A } hf : t [W], and (P, P ) &#38;A, the?t \nR,(P[Wjp, Mp ). The theorem says roughly that if M can be assigned type t with annotated term W, then \nP( LVVJ ) and M are related by relation Rt. More precisely, it says that if p and p are substitutions \nrelated by symbol table (type hypotheses) A, then the co] responding substitution instances of P( lW] \n) and M are ~elatecl. This proposition is the generaiization of the simple first-order case (E(P[W]) \n= M) to handle higher-orcler terms. The proof is by induction on the construction of the proof of .1 \nk M : / [\\v]. There are 5 cases, one for each proof rule. The proof is shown in Appendix A. Most of the \ntime we will be collcelned with closed terms: This specification for a partial evaluator looks rather \ndif\u00adferent from the usual one, so it is useful to check to see that the usual properties of partial evaluators \nhold. The following corollary encapsulates most of our reasoning about partial evaluators: Corollary \n2 (Specialization Theorem) Let M, N, N be closed terms, and let Ma be an annotation of M with type t \ne d. If R,(N, N ), then E(P[h4a]N) = MN Proofi R,+j(PLM J , M) Main Theorem R,(N, N ) Assumption Rd(P[M \nj N, MN ) Definition of R,+d E(PIM JN) = MN Definition of Rd We can complete the connection the usual \nnotion of par\u00adtial evaluation by: Corollary 3 (Partial Evaluation Theorem) Let M be a program with an \nannotation Ma of type d -+ d. Then ~or any program N, E(PIM J [Nl) = MN Hence PLM ] [Nl is a program \nequivalent under E to MN: that is, it is a program for M speczalised to N. Proof: We can always annotate \na closed program N by making it entirely dynamic. Let Nd be the annotation that does this. Now E(N] = \nN, so we have &#38;( [N1, N). There\u00adfore, by the Specialization Theorem we have In some sense, this corollary \nis the result that we were aiming for: it justifies the use of binding-time analysis to\u00adgether with the \nalgorithm P to produce a specialized version of the input program. 6 Correctness of the Self-Applications \nWe next check to see that the properties of the Futarnura projections follow from our specification. \nBefore checking the Futamura projections, we need a bit of notation and a lemma, For any type /3, let \nT6 denote the type (pt).((p + /3) (t-t+ /3) -+ ((/3+ t)-/3) + /3) This is the type of the representation \nof J-terms interpreted as case functions with target /3. Lemma I If M M cl closed A-tern?, oncl /3 w \nany type, ihe)l RT6 ( [Ml , [Ml). Proof: .4s usual we need to cousicier open terms as well The induction \nhypothesis is: Let p, p be substitutions such that for all z 6 dom(p) = dorn(p ), R~p(p(r), p (z)). Then \nl?~fi ( (Ml p, [Ml p ) The ploof is hy i l,dnct ion on M. 0 6.1 First Projection The first Futamura projection \nstates that partially inter\u00adpreting an interpreter with respect to a program gives the same result as \ncompiling the program (into the language of the partial evaluator). To get this projection, Mogensen \nexhilbits the following annotation E of E with type Td -+ d: E = YAe.Am.m (Ax.x) (Amn.(e m)_(e n)) (Jm.~v.e \n(m v)) where Y = Ah.(Ax.h (zz))(Az.h(z z)) Hence we have &#38;~_~(P[Ea],E) Main Theorem RT, ( [Ml , [Ml) \nLemma 1 E(P lE J [J4] ) = E [Ml = M Specialization Theorem So for any N, we have E(P[E J [Ml )N = MN. \nHence P[Eaj ~Ml = (Rl such that for all N, L(R, IV) 0 L(Jf, N) as desired. In general, we can specify \na programming language by giving a, coding ( ) from programs and data in the language into the A-calculus, \nand a A-term S which is the semantics or interpreter for the language. We can then define the input\u00adoutput \nbehavior of the language by L.(P, D) = D u S(P)(D) =, (D ) A translation of a program m is a A-term T \nsuch that for all S-data D, L(T, D) E Ls(T, D), that is, T(D) = S(X)(D). Here we have extended the definition \nof L to include the coding( ), ascontemplated in the original definition. If the coding ( ) uses either \nordinary or higher-order ab\u00adstract syntax, then the same reasoning as Lemma 1 will hold, so that we get \nRt ((M), (M)) for some type t that represents the grammar of the language. If S is compositional, then \nwe can always annotate S with type t +d.This effectively views S as translating from the programs of \nthe source language into ~-terms (rather than into their meanings). We can do this by marking all of \nS as dynamic, except for those parts that actually assemble the pieces into a ~-term. For example, a \nfragment of the semantics might be S(e, + ez) = &#38;2.(+ (s(f31)p))(s(fa)P) where we have explicitly \ncurried the +. We would annotate this as: S(el + e2) = M.(+_((s(fa ))_fl))-((s(e2) )_P) In the homelier \nlanguage of backquotes and commas, the right-hand side would be something like (lambc~a (r) ((+ (,(S \nel) r))) (,(S e2) r)) Now we can state the first projection: Corollary 4 (First Projection) Jf T Q L(P, \n[S, T]), then T is 0 trclnslntion of ~. Proof Calculating as before, we get &#38;.td(PISaj , S ) Main \nTheorem R,((r), (T)) for any m in the language E(P[S J (m)) = S(r) Specialization Theorem So let P lS=J \n(T) = [T 1. Then T(D) = EIT1(D) = E(PISaj(@)(D) = S(T)(D) so L(T, D) ~ S(7, D) as desired. 0 6.2 Second \nprojection The second projection states that partially interpreting the partial interpreter with respect \nto the interpreter gives a compiler. A compiler for S is a A-term C such that for any S\u00adprogram ~ and \nS-data D, L(C, ~) a T where T is a transla\u00adtion of r, that is, C(z) = [Tl, where T(D) = S(T)(D). So :(;~ow \nC is a compiler, it suffices to show that E(C(r)) = We can proceed as before for the syntax 1 J of annota\u00adtions, \ngetting a type up = (},t).((p /?) -+(t-t-/l) +((P+t)+P) -+(t-t-p) -+$; -f)-P) for codings of annotated \nterms, and a lemma Lemma 2 If W is a closed annotuted A-termt ond /3 is uny tgpe, then RUP ( [W], LW \nj ). proof: AS for Lemma 1. 0 Corollary 5 (Second Projection) If C E .L(P, [F a, S ]). then C is u compiler \nfor S. Proof: Mogensen gives the following annotation P of P with type U~ d: Pa z YAp.~m.m(Ax.z) (Amn.(p \n7n)_(p )!)) (Am.~v.p (m 21)) (~mn.~csbc.b_(p m)_(p ~~)) (hn.~abc.c_(~t.p (m (Aobc.u_ [ )))) where Y = \n~h.(~Z.h(ZX))(/b. h (T J:)) Hence we have &#38;,-d(PIPa~, P) Main Theorem Rv, ( [SO] , lSaJ ) Lemma 2 \nE(PLPaj 1S ] ) = PLSQJ Speclalizatioil Theol-ern So, let C S L(P, [P , Sa]), that is P[P ] [S ] = [Cl. \nT1 e can now check to see that, C is a compiler E(C (r)) = I?(E[cl(7i)) = E(E( P[PaJ [S ])(r)) ddillition \n0( [( 1 = E(PLs( j(7r)) preceding calculation = s(m) first projection 0  6.3 Third Projection Futamura \ns third projection states that L(P, [P, P]) produces a compiler generator, that is, a term G such that \nfor any in\u00adterpreter S, L(G, S) z C where C is a compiler for S. That is, GIS J = [(71, where C is a \ncompiler for S. Corollary 6 (Third Projection) Let G -L(F , [P , Pa]). Then G is a compiler-generator. \nProof Let G s L(P, [F , P ]) ,that is, PIPaj lF ] = [G]. We first calculate, as before: RrJd+d(f lPa] \n, P) Main Theorem Rud ( [Paj , [P ] ) Lemma 2 E(.P lP J lP J ) = P LP J Specialization Theorem To check \nthat G is a compiler-generator, let define C by G [S ] = [C]. We need to show that C is a compiler. Now \n[Cl = G[SaJ = EIG1 [S j = E(PIPaj [Paj) lsaj = PIP J [Saj by the preceding = calculation But the second \nprojection showed that if we have [Cl = PIP j lS j, then C is a compiler. So G is a compiler gen\u00aderator. \n0 Note that these projections were derived solely from the definition of Rt and the fact that P satisfied \nthe Main The\u00adorem. They did not depend on the text of P. So they would continue to work for any other \npartial evaluator that satis\u00adfied the Main Theorem. 7 Comparison with Related Work Mogensen [11] is the \nprimary source for our development. He exhibits the various terms above. He also includes the binding-time \nannotations that allows the partial evaluator to be self-applied. He proves the correctness of the self\u00adinterpreter \n[10], but does not give either a specification or proof of the partial evaluator. Gomard [5] discusses \na larger self-applicable partial eval\u00aduator in a two-level ~-calculus with constants. He gives a correctness \ncriterion similar in intent to ours. Our specifica\u00adtion is simpler because it avoids the model-theoretic \ndetails and assumptions used in [5]. Both Mogensen and Goma,rd, as do we, use a two-level type system \nlike that of Nielson and Nielson [12], except that the run-time code is untypecl. Launchbury [9] discusses \nthe encoding issues for a strongly-typed self-applicable partial evaluator with a full multi-level type \nsystem. However, he remarks in Section 6 of [9] that the same encocling issues arise in the untyped case. \nThis paper is in part an attempt to sort out these issues. Launchbury [8] gives a characterization of \nbinding-time anal ysis in terms of congruence rel at ions: a qu anti ty may be deferred to run-time if \nits \\,alue does not alter the re\u00adsult of the compile-time computation. This idea was neatly extended \nto higher-order computations and partially-static structures by Hunt and Sands [6] using partial equivalence \nrelations. Unfortunately, this characterization does not seem to be strong enough to justify emitting \ncode on the basis of the binding-time analysis, which is our primary goal. Consel [2] presents an algorithm \nfor binding time anal~\u00adsis for a higher-order untyped language with partially static structures, based \non an abstract interpretation. However, no correctness criterion is stated. It would be interest\u00ading \nto see how our approach might handle partially static structures. More recently, Consel and Khoo have \nshown the correctness of on-line and off-line partial evaluators for a first-order language using an \ninstrumented semantics [3]; in this framework binding-time analysis appears as an ab\u00adstract interpretation \nof the on-line partial evaluation. They then derive the off-line specialize (corresponding to our P) \nfrom the on-line partial evaluator by coarsening the tests to depend only on the data available from \nthe binding-time anal ysis. Their approach also shows the correctness of the binding-time analysis and \nspecialization together, but the technical details seem quite different from ours; their aP\u00adproach is \nheavily dependent on model-theoretic details that we have been able to avoid. It would be interesting \nto see if our results could be adapted to give a simpler account in the first-order case, 8 Conclusions \nWe have given a correctness criterion for a partial evalu\u00ad ator based on binding-time analysis. Our criterion \nis sim\u00ad pler than others proposed in the literature and is sufficiently powerful to specify the behavior \nof the partial evaluator in the case of self-application. We consider this proof as being about, binding-time \nanal\u00adysis: the point of binding-time analysis is to justify the re\u00adplacement of potentially expensive \nrun-time computations by presumably cheaper compile-time computations when\u00adever possible. Our correctness \ncriterion is strong enough to justify this transformation. By considering both a,flow a,nal\u00adysis and \nthe transformation it justifies together: this proof suggests a framework for the incorporating flow \nanalyses into verified compilers. Acknowledgements Thanks to Bob Muller, whose questions in our semi~~ar \ngot me started on this topic, and to Neil Jones, Torben Mo\u00adgensen, and Carsten Gomard for their extensive \nII-mail dis\u00adcussions and comments. Appendix A: Proof of the Main Theorem We first state some lemmas. \nWe will use the symbol = to de\u00adnote @convertibility, and ~, when necessary, to distinguish congruence \nof A-terms. Lemma 3 P( lW] [AT/z]) = (.PIWj)[N/z] Proof: By induction on the size of W. We will do two \ncases; the others are similar. For the base case, we nave P(lxj [N/z]) = P(kzbcrle.aN) = N and (Pl:rj \n)[<Y/x] = z[N/z] = N. For static abstractions over a vari,~ble y dis\u00adtinct from z, we have P( [Ay.MJ \n[fv/x]) = ~((~abcde.c(~y. lJ4J))[N/z]) definition of lAY.M] = P(),abcde.c(,ly . lMJ [y /y][N/z])) substitution \n(y fresh) = Av. P((Ay . lMJ[?J /y][N/z])v) definition of P = Ay .P((Ay . Lkf][y /y][N/z] )y ) conversion \n= Ay .P([M][y /y] [z])]) ~-conversion = Ay .F ([Mj[N/z] [y /y]) ~# -Y = Ay .((P[Mj )[N/x])[y /y] induction \nhypothesis = (Ay.PIMj)[N/z] substitution = F (~Ay.M])[N/z] definition of P Corollary 7 (Substitution \nLemma) if p ZS a substitu\u00adtion, then (PIWj)p = P(lWjp). Proof: By repeated application of the preceding \nlemma for each zEdom p. 0 We will often write P lWJ p to mean, interchangeably, either of the terms above. \nNote that it would be incor\u00adrect to bring the p inside the 1 ]: P(lWjp) # P([Wp]). The resemblance of \nP LWJ p to a conventional environment semantics is, of course, not coincidental, since by choosing equalit,y \nto be interconvertibility, we are in effect working in the open term model, in which evaluation in an \nenvironment is the same as substitution. Our results hold a fortion in any A-model. We now define the \nrelations Rt for each type t.If Rand S are binary relations on A-terms, we define the relation R -S in \nthe standard way by  (R ---+ S )(M,M ) + (VN, N )(R(JV, N ) ==+ S(MN, M N )) Let ??range over maps from \ntype variables to binary re\u00adlations Then we can define R~,q by: R.d,q = {(M, M ) I EM =l; M } = ?/(,1) \nR 1.I,7J R(. +t),q = Rs,,t -St,v R(,t. t),q = n{s Is= %wv]} Since all of the occur~ences of v in t must \nbe positive, the least fixed point in the last line must exist and must, be a fixed point. In the remainder \nwe will consider only closed types, for which we clefine Rt = Rt,O. Had our language in\u00adcluded othel \nbase types o, Ro would be the equality relation on the other base types. Lemma 4 (Admissibility) If M \n= M and N = N , then R,(M, N) u Rt(M , N ). Proof: By induction on the length Ifl of f, clefil)ed l>Y \nIdl= 0,IsA t\\= \\tl+ 1, and \\\\~v.tl = Itl -t1.If the type is cl, then RJ(M, A ) ~ EM =N w EM = N w R, \nL(M , N ) If the type iss t,then Itl < Is tl, sowelmve Rs_, (M, N) ~ [R.(P. Q) * R,(MP. NQ)I * [17. (P, \nQ] ==+ R,(M P, N Q)] * It.. -,(il, fvf) )  The last case is that the type is vv.t. Now, since t is \nof the form tl -. . . --+ in +-+ d, t[pv.t/v] = tl[pv. t/v] -+ . . . t~[~m.t/v] + d. So lt[pv.t/v]] = \nItl < Ipv.tl. So RP.., (M, N) # &#38;[p..t/.](~, W ++ R[w.t/.](Jf , N ) e+ RPv.t( M , N ) u For substitutions \np and p we say (p, p ) ~ A iff for all z c dom A, l?~(z)(p(z), p (z)). For the proof, it is easiest to \nreformulate the main theo\u00adrem using using one more bit of notation. We say A + M : t [W] iff (p, p ) \n1= A =+ fi!t(PIWjp, Mp ). Now the main theorem becomes: Theorem 2 (Nfain Theorem, restated) lj A t-M \n: t [Wl, then A \\ M :t [W]. Proof: The proof is by induction on the structure of the proof of A > M : \nt [TV]. Thus the proof has five cases, one for each rule of inference for E. For each case, we show that \nthe rule preserves ~. Case 1 (Variable). Assume z E dmn A. Then A + z : .4(z) [z]. \\Ve need to show \nA + z : A(x) [z]. So assume (P. P ) 1= A. we need to prove that RA(~)(f lzj P, Z~ ) holds. We calculate: \nwhich holds because (p, p ) ~ A Case 2 (Static Abstraction). Assume A[x F s] + M : t [IV]. L\\-e need \nto show that A ~ (Ar. M) : s f [~z.;[ ]. So assume (p, p ) 1= A. Vile need to show R, , (Plk.W]p, (kc. \nM)p ) By the definition of Rs_t, it suffices to assume R.(FV1, Ml) and theu show t,hat Now, (we w,ite \np[i4 /.z] for the substitution mapping x to .V and any other variable .Z to p(z); thus ((kr. ilf)p) =6 \nI14(p[N/z]).) ?k~thermore, ( (Jx.flf)p ).1[1 = Mp [kfl/z], so it will suffice t,o show R,((P [W] )P[W, \n/r], Mp [MI /z]) But (P[ll~l/r], p IMl/. r] ) 1A[.r + s], so the desiled con\u00adclusion follows from .4[Z \n+-s] + M : t [W]. Case 3 ( Dynanlic Abstraction). Assume .4[r * d] \\ M d [H ]. tl e need to SIIOW that \n.4 ~ J.z.M : d [~r W]. So assnme (p, p ) &#38; A. IYe need to show that induction hypothesis, we know \nthat Rd(~lWJf21, Mp ), that is, .E(P[wJp) = Mp (1) Now, to establish Rd(~ @z.W~ p, (As. J4)p ), we calcu\u00adlate: \nE(P@.Wjp) = E(.P(Jabcde.e(Xc.lWj) )p) def n of l~z.W] = E(hbc.c(Jz.P((kc. lWj)(Mm.az)))p) definition \nof P = E(,labc. c(Jz.P(((.Xz. [Wj)p)(Azbc.az)))) Lemma 7 = E(k2bc.c(k.P(lWjp[( Jabc.az)/s]))) = E(Jabc.c(k.P(lWJpl))) \ndefinition of p] = Ax. E(P(lVVJp, )) definition of E = Xz.Jfp [$/z] Equation I = (Az.M)p Case 4 (Static \nApplication). Assume A \\ M : s \u00adt [w] and A ~ N :s[W ]. We must show A + MN : t [tVW ]. So assume (p, \np ) &#38; A. We want to show R,(PIWW JP, (MN)p ) By the induction hypotheses, we have R.+, (P[Wj p, Mp \n) and .R, (P IW Jp, Np ). So we have R~((PIWJp)(PLW J p), (Mp )(Np )) definition of Rs , Rt(((PlwJ)(PIW \nj ))p, (MN)p ) substitution Rt(PLww Jp, (A4N)p ) definition of P Case 5 (Dynamic Application). Assume \nA > M : d[W] and A 1=N :d[W ]. We must show A ~ MAr : d[W_W ]. So assume (p, p ) + A. We want to show \nRd(Plw_W jp, (~N)P ) We calculate: E(Plw_w jp = E(P(kbcde.d([WJp) (lW Jp))) = E(Jubc.b(PIW]p) (PIW Jp)) \n= (E(PlwJp))(E(Plw jp)) = (Mp )(Np ) induction hypotheses = (MN)p This completes the cases for the proof. \n0 References [1] Hen!i Barendregt. Self-Interpretation in Lambda Calcw lUS. Journal of Functional Programming, \n1(2):229-234, April 1991. [2] Charles Consel. Binding Time Analysis for Higher Order Untyped Functional \nLanguages. In PI-OC. 1990 .4 CM Symposium on Lisp and Functionctl Program\u00adming, pages 264 272, 1990. \n[3] Charles Consel and Siau Cheng Khoo, On-1ine /... Off-line Partial Evaluation: Semantic Specifica\u00adtions \nand Correctness Proofs. Technical Report Y-AI, EU/DCS/RR-912, Yale Univewity Department of C~omputer \nScience, June 1992. [4] Carsten K. Gomard. Partial Type Inference for Un\u00adtyped Functional Programs. In \nProc. 1990 ACM Sym\u00adposium on Lisp and Functional Programming, pages 282-287, 1990. [5] Carsten K. Gornard. \nA Self-Applicable Partial Evalu\u00adator for the Lambda Calculus: Correctness and Prag\u00admatic. ACM Transactions \non Programming Languages and Systems, 14(2):147 172, 1992. [6] Sebastian Hunt and David Sands. Binding \nTime Anal\u00adysis: A New Perspective. In Proceedings of the Sympo\u00adsium on Partial Evaluation and Semantics-Based \nPro\u00adgram Manipulation, pages 154-165, 1991. SIGPLAN Notices 26(9), September, 1991. [7] U. Jorring and \nWilliam L. Scherlis. Compilers and Stag\u00ading Transformations. In Conf. Rec. 13th ACM Sympo\u00adsium on Principles \nof Programming Languages, pages 86-96, 1986. [8] John Launchbury. Projection Factorization in Partial \nEvakatton. PhD thesis, University of Glasgow, Novem\u00adber 1989. [9] John Launchbury. A Strongly-Typed Self-Applicable \nPartial Evaluator. In John Hughes, editor, Functional Programming Languages and Computer Architecture, \nvolume 523, Berlin, Heidelberg, and New York, 1991. Springer-Verlag. [10] Torben W. Mogensen. Efficient \nSelf-Interpretation in Lambda Calculus. to appear, June 1992. [11] Torben .E. Mogensen. Self-applicable \nPartial Evalu\u00adation for Pnre Lambda Calculus. In Charles Consel, editor, .4 CM SIGPLA N Workshop on ParttaJ \nEvnl\u00aduatton and Setnantics-Based Program kfanipulatton, pages 116 1?1, 1992. [12] Flemrning Nielson and \nHanne Riis hTielson. Two-Level Semantics and Code C4eneration. Theoretical Computer $czence, .56:59 -133, \n1988. [13] Frank Pfenning and Conal Elliott. Higher-Order Al>\u00adstract Syntax. IN Proceedings SIGPLAN 88 \nConference on Progrommmg Langaage Design and In~plet?~er~tatl(>n, pages 199-208, June 1988. \n\t\t\t", "proc_id": "158511", "abstract": "<p>Mogensen has exhibited a very compact partial evaluator for the pure lambda calculus, using binding-time analysis followed by specialization. We give a correctness criterion for this partial evaluator and prove its correctness relative to this specification. We show that the conventional properties of partial evaluators, such as the Futamura projections, are consequences of this specification. By considering both a flow analysis and the transformation it justifies together, this proof suggests a framework for the incorporating flow analyses into verified compilers.</p>", "authors": [{"name": "Mitchell Wand", "author_profile_id": "81332534256", "affiliation": "", "person_id": "PP39085118", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/158511.158614", "year": "1993", "article_id": "158614", "conference": "POPL", "title": "Specifying the correctness of binding-time analysis", "url": "http://dl.acm.org/citation.cfm?id=158614"}