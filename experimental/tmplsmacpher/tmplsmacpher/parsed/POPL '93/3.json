{"article_publication_date": "03-01-1993", "fulltext": "\n A Novel Framework of Register Allocation for Software Pipelining Qi Ning Guang R. Gao School of Computer \nScience McGill University Montreal, Quebec Canada H3A 2A7 email: ning@cs.mcgill.ca gao@cs.mcgill.ca Abstract \nAlthough software pipelining has been proposed as one of the most important loop scheduling methods, \nsimultaneous scheduling and register allocation is less understood and re\u00admains an open problem [28]. \nThe objective of this paper is to develop a unified algorithmic framework for concurrent scheduling and \nregister allocation to support time-optimal software pipelining. A key intuition leading to this surpris\u00adingly \nsimple formulation and its efficient solution is the as\u00adsociation of maximum computation rate of a program \ngraph with its criticai cgcles due to Reiter)s pioneering work on Karp-Miller computation graphs [29]. \nIn particular, our method generalizes the work by Callahan, Carr and Kennedy on scalar expansion [6], \nthe work by Lam on modular variable expansion for soft ware pipelined loops [20], and the work by Rau \net al on register allocation for modulo scheduled loops [28]. \u00ad 1 Introduction Software pipelining has \nbeen proposed as one of the most im\u00adportant fine-grain loop scheduling methods. It determines a parallel \nschedule with a periodic pattern which may over\u00adlap instructions of a loo~ bodv from different iterations. \nS~ftware pipelining can be applied to high-performance pipelined processor architectures, super-scalar \nand VLIW architectures [2, 3, 4, 12, 13, 20, 27, 31]. Although much progress has been made in finding \ntime\u00adoptimal schedules for software pipelining, simultaneous scheduling and register allocation is less \nunderstood and re\u00admains an open problem. In terms of instruction scheduling for RISC processor architectures, \nit is well recognized that Permission to copy without fee all or part of this material is granted provided \nthat the copies are not made or distributed for direct commercial advantage, the ACM copyright notice \nand the title of the publication and its date appear, and notice is given that copying is by permission \nof the Association for Computing Machinery. To oopy otherwise, or to republish, requires a fee and/or \nspecific permission. ACM-20th PoPL-1/93-S.C., USA e 1993 ACM 0.89791 .561 -5/93 /0001 /0029 . ..$1 .50 \nperforming register allocation before instruction scheduling (postpass scheduling) may introduce new \nconstraints due to the reuse of registers, which may limit possible reordering of the instructions, as \nreported in [15, 17]. On the other hands, if instruction scheduling is done before (and independent of) \nregister allocation (prepass scheduling) [5], more registere than necessary may be needed, which may \ncause unneces\u00adsary register spills and severely degrade the performance of the resulting code. Warren \nhas described a technique which applies instruction scheduling twice: once before and once after the \nregister allocation [32]. The objective of this paper is to develop a unified scheduling-allocation framework \nfor concurrent scheduling and register allocation. We propose a framework in which register allocation \nfor software pipelining is solved in two stepa. Step 1: Time-Optimal Scheduling and Minimal Buffer Allocation: \nThe first step determines the time-optimal schedule for a software pipelined loop and allocates symbolic \nregisters organized as FIFO buffers, one buffer for each variable defined in the loop. Intuitively, such \na buffer is used to extend the lifetimes of the corresponding loop variable gen\u00aderated in successive \niteration, permitting multiple it\u00aderations to be overlapped in concurrent executions. We show that the \nminimum buffer allocation and the time\u00adoptimal scheduling problem can be formulated together as an integer \nprogramming problem called the Optimal Scheduling and Buffer Allocation (OSBA) problem. An efficient \npolynomial time solution is presented baaed on a transformation of the OSBA problem into rnin-cost jlow \nproblem. A key intuition leading to this surpris\u00adingly simple formulation and its efficient solution \nis the association of maximum computation rate of a program graph with its critical cycles due to Reiter \ns pioneering work on Karp-MiUer computation graphs [29]. Step 2: Nlapping buffers to physical registers: \nThe second step is to map the symbolic registers of the FIFO buffers into physical registers. Since a \nschedule is de\u00adrived from the solution of the OSBA problem, a color\u00ading algorithm can be applied to minimize \nthe number of physical registers required to implement the buffers. In particular, we can apply a recently \nproposed method based on coloring of cyclic interwd graphs [16]. Code generation schemes with or without \nspecial hardware support are discussed. We have implemented the OSBA algorithm. In experi\u00ad ments using \nour method on the working example used in Rau et. al. [28], a reduction of register usage from 28 to \n21 is achieved a 25% improvement in register use without compromising the speed of the schedule, The \ngeneral problem of allocating minimum number of registers to support a class of loop schedules of certain \nrate, possibly optimal rate, is proven to be NP-complete in [25]. However the two step approach proposed \nhere is a practi\u00ad cal way of attacking this hard problem, and it has a time complexity 0(n3 log n). Since \nthe register coloring methods are described in other works, our discussion centers on the buffer allocation \nstep and the code generation schemes. We organize the subse\u00adquent sections as follows. In Section 2 we \npresent a simple example loop to motivate the concept of buffers and the two step approach for concurrent \nscheduling and register allocw tion. In Section 3 we formulate the Optimal Scheduling and Buffer Allocation \n(OSBA) problem. A polynomial time so\u00adlution is presented in Section 4. The proof of the theorem in this \nsection can be found in [25, 24]. In Section 5 we address the code generation problem. In Section 6, \nwe explain how to reduce the register requirement further through coloring algorithms for backend code \ngeneration. This is the second step in our register allocation scheme. In Section 7 we com\u00adpare our approach \nwith related works. We also include an appendix which describes the example in Rau et al s paper [28], \nand our solution. 2 Motivation In this section, we motivate our work by studying register allocation \nfor a simple loop L, shown in Table 1, under soft\u00adware pipelining. Although a high leyel language represen\u00adtation \nof the loop is chosen here, it is intended only to give a simple description of our technical framework. \nThere is no difficulty in applying our framework to the lower level representation of the code. As shown \nin Table 1, the loop L cent ains three instructions in its b,ody. It also contains m Table 1: An example \nloop L. a loop carried dependence of distance 2 from S3 to S1. For instance, the value c[i] generated \nby S3 in iteration i is only used two iterations later by statement S1 in iteration i + 2. The other \ndata dependence in the loop are all within the same iteration. The Data Dependence Graph (DDG) of L is \nshown in Figure 1, where the positive number beside (s3, SI ) indicates its dependence distance. Figure \n1: Data dependence graph of the example loop L. Under software pipelining, the iterations are permitted \nto overlap so that the subsequent iterations may start before the previous iterations finish. Since there \nexist loop carried dependence, there should be a proper initiation delay inter\u00ad val P between successive \niterations so that when the next iteration starts P cycles after the previous iteration started, no loop \ncarried dependence are violated. In our example, assuming that the delay for Add is 1 cycle and the delay \nfor Multiply is 2 cycles, then a delay of P = 2 cycles between the starting times of two consecutive \niterations is optimal in the sense that the scheduled loop achieves the maximum speedup. A possible maximum \nspeedup schedule is shown in Table 2, in which sz,1 indicates the instruction sz in iteration 1. Note \nin Table 2 that iteration 3 starts after iteration 1 pro\u00adduces c[l], and iteration 4 starts after iteration \n2 produces c[2], etc. So the loop carried dependence is not violated. The schedule exploits parallelism \nsince there are two instructions scheduled in parallel at cycle 3 (5, 7 etc.). Under the above time-optimal \nschedule, S1 is executed twice before its suc\u00adcessor S3 is scheduled for the first time in cycle 3. Hence, \nin order to support the schedule, it is natural to provide a stor\u00adage buffer of size more than one (if \nnecessary) between the generator S1 and its successors sz, ss. To enforce the correct order of the values \nproduced, the buffer should behave like a first-in-first-out (FIFO) queue. Let us examine this in some \ndetail. Suppose that a FIFO buffer of two symbolic registers {ao, al } is allocated to a, such that a. \nis the tail and al is the head of the queue, Each of the other variables is allocated a buffer of size \n1, which is a single register. For convenience we will use the variable name as its register allocated \nif the buffer size is one. When a[l] is produced by S1 in cycle O, it is put into the tail a. of the \nbuffer. At cycle 2, a new iteration starts and a[2] is produced before a[l] is used at cycle 3. At this \nmoment, the new value can not be written into ao, otherwise it would overwrite a value (a[l]) which is \nstill needed in cycle 30 iteration 1 iteration 2 iteration 3 iteration 4 0 SI,I: a[l]=X+c[-1] 1 $2,1: \nb[l]=a[l]*F 2 s1,z: a[2]=X+c[O] 3 s3,1: c[l]=a[l]+b[l] SZ,Z: b[2]=a[2]*F 4 SI,3: a[3]=X+c[l] 5 s3,2: \nc[2]=a[2]+b[2] s2,3: b[3]=a[3]*F 6 SI,4: a[4]=X+c[2] 7 S3,3: c[3]=a[3]+b[3] s2,4: b[4]=a[4]*F !3 s..: \ncr41=flr41+br41 , , ,., , -., ,=--..L J III I{I I Table2: Apossible maximum speedup schedule. 3. Now \nsince deallocated two registers and organized them as a FIFO buffer, we can continue to write to the \nbuffer at a. al the tail at cycle 2. We assume that a mechanism is provided such that a buffer can shift \nits contents towards its head so (3+-+7-P(T)that the tail (ao ) is ready to get a new value. In our case, \nwe can assume that the old value in ao is shifted to al at beginning of cycle 2. So at cycle 2, a[l] \nis in al and a[2] is in ao. S* Conceptually, the pseudo code buffer-shift in Table 3 plays the role \nof such shifting. In practice, the buffer mech\u00ad 3 {ao, al} is the buffer for anism can be implemented \neither by explicit shifting using register moves, or by implicit shiftings based on a modulo S1. S2 reads \nfrom the tail addressing scheme. These schemes will be discussed in Sec\u00ad a. and S3 reads from tion 5. \nthe head al. Let us see how the successors sz, .93 of sl access the buffer of a in Table 3. Since S2,1 \nis scheduled at cycle 1, when it reads the value of a[l], we can assume it is still in register Figure \n2: A multiple-head buffer. ao. Therefore, the actual code for SZ,I in iteration 1 could be b = ao * F. \nOn the other hand, if we assume that the buffer shifts its contents at cycle 2, then at cycle 3 when \nS3,1 is executed, the value of a[l] is shifted to al in the buffer, hence .93,] should read from al. \nTherefore the code for S3,1 loop, they are not shown in Figure 3. should be c = al + b. Hence S2,, always \nreads from the tail a. As shown in Figure 3, the live range of c does not overlap and S3,* always reads \nfrom the head al. This phenomenon is with the live range of al, so they can share the same physical caused \nby the different scheduled timings of the successors register. Thus, the mapping of buffers to physical \nregisters of a producer. We call such a buffer muitiple-head FIFO (step 2 of our method) is somewhat \nsimilar to the traditional queue because the successors read the contents of the buffer register allocation \nproblem. In Section 6, we describe how to at different places. The multiple-head buffer is illustrated \nin apply the register allocation method based on cyclic interval Figure 2. graphs for this step. After \nthis procedure the code does Here the concept of FIFO buffer plays an important role, not need the extra \nregister c, which is replaced by a 1, as as it captures the notion of lifetime of a loop variable ex-shown \nin Table 4 where the symbolic register names e.g. al tended into successive iterations. The repeating \npattern now represent physical registers, and where the sign II means between cycles 2 and 3 in Table \n3 is, in fact, the time-optimal execute in parallel with . schedule for the software pipelining we expect \nto derive. We wiU choose this repeating pattern as our new loop body, which is the first step of our \nmethod. 3 Formulation of the Optimal Scheduling and When the new loop body and the buffer allocation \nhave Buffer Allocation (OSBA) Problem: Step 1 been decided, we notice that the buffers allocated to indi\u00advidual \ninstructions can share same physical registers if their In this section we give a mathematical formulation \nof the live ranges in the schedule are non-overlapping. In our ex-scheduling and buffer allocation problem. \nSection 3.1 intro\u00adample loop L and its given schedule, the live ranges of the duces the concept of periodic \nschedules and optimcd com\u00advariables in the repeating pattern are indicated in Figure 3. put ation rate. \nSection 3.2 presents an integer programming Since the live ranges of F and X are the whole range of the \nformulation of the Optimal Scheduling and Buffer Allocation iteration 1 iteration 2 Iteration 3 iteration \n4 0 buffer-shift: al = ao sl,l:aO=X+c 1 sz,l:b=aO*F 2 buffer-shift: al = ao  sl,z:aO=X-Fc 3 ss,l:c=al+b \ns2,2:b=ao*F 4 buffer-shift: al = ao sl,J:aO =X-kc 5 s3,2:c=al+b sz,S:b-=ao*F 6 buffer-shift: al = ao \nsl,*:aO=X+c 7 sa,s:c=al+b sz,b:b=aO*F 8 9 s3,4:c=a1+b Table 3: Code generated for loop L. prologue code \np <---\u00ad :----\u00ad ;.. 2 al p -------+-__; ----;-epilogue code ! ~:1 !E Table 4: Code from repeating pattern. \n3---- UUU\u00ad ___________-------------. ! (a) Life ranges drawn from ! , -----------------, \\ tie repeatin-pattern. \n,;; , Legend: (OSBA) problem. 1 ----------------------I I : H Definition point. \\ : x Last use point. \n] 3.1 Time-Optimal SchedulesI--------------------I We assume that a data dependence graph (DDG) supple\u00admented \nwith delay information is used to represent a loop. In a DDG, nodes represent the instructions in a program. \nAn arc from node i to node j indicates that there is a data dependence between them. Loop carried dependence \nare e H indicated by arcs with positive numbers beside them indi\u00adcycle 3 cating their dependence defined \nbelow. If node i produces a result at the current iteration and the result will be used k iterations \nlater by node j, then we say that the arc (i,j)  +&#38;i@ has a dependence distance k, and we use m,~ \nto indicate it. So for a data dependence not crossing iterations, the m,j is~(b) Life ranges drawn as \ncircular: O. d, (called delay) is used to indicate the number of clock ,arcs on a cycle. I1 ------------------------------4 \ncycles node i needs to finish its execution. Definition 3.1 A Timed DDG is a tupie (N, E, m, d) where \nFigure 3: Live ranges of the variables for code generated by N is the set of nodes, E is the set of arcs, \nm == {m,J, d(i,,i) E ASC scheme. Ej is the dependence distance vector on arc set E, and d = {d,, Vi EN) \nis the delay function on node set N. Later on in the paper, we WN simply US~~DDG to mean Timed DDG whenever \nno confusion can be caused. We will consider a class of software pipelining schemes called periodic 3.2 \nOptimal Scheduling and Buffer Allocation To\u00ad schedules. Definition 3.2 Let P be a positive integer. A \nschedule is called periodic with period P if for-any node i, the tames i is scheduled for its first instance, \nsecond instance, third instance,... are t,, t, + P,t, + 2P, . . .. Time t, is called the initial execution \ntime for node i. It is obvious that the computation rate of a periodic sched\u00adule with period P is ~. \nReiter [29] has shown that it is always possible to have an periodic schedule with optimal period P which \nequals the reciprocal of the maximum com\u00adputation rate of the dependence graph. When a periodic schedule \nachieves this maximum computation rate, we call it time-optimal. Themain objective ofthispaper isto finda \ntime-optimal periodic schedule which uses aminimumnum\u00adber of registers. The next lemma characterizes \nthe feasible periodic schedules. Lemma 3.1 (Reiter [29]) The initial execution times t, are feasible \nforaperiodic schedzde with period P if and only if they satisfy the following ineqwdities: t~ >t, +d% \nPm,J, V(i, j) GE (1) where d, is the delay of node i, P the period, and m;l the dependence distance \nfor arc (i,j). The maximum computation rate of a DDG (loop) is the maximum average number of executions \nof its nodes per clock cycle, restricted only by the data dependence in the graph. For example, the loop \nrepresented in Figure 1 has a maximum computation rate of ~, meaning that every node can be executed \nonce in every two time cycles. The follow\u00ading theorem characterizes the maximum rate one can obtain. \nProofs can be found in [29, 26]. Theorem 3.1 (Reiter [29]) Given a DDG=(N, E, m, d) which represents \nu loop. The maximum computation rate of every node in the graph is the critical ratio, min p,mc J , for \nall cycles C in the DDG, J EEC d The cycles that reach the min are called critical cycles. The critical \nratio can be computed efficiently by the al\u00adgorithm of solving the minimum cost-to-time ratio problem \n[21]. In this paper, we assume P (the reciprocal of criti\u00adcal ratio) to be an integer. In the case when \na fraction is derived for the period, we can always unrolled the loop a proper number of times and the \nresulting loop will have an integer period P, as illustrated in [25] or [24]. For loops with\u00adbout loop \ncarried dependence or its DDG does not contain dependence cycles, the criticaJ ratio is undefined, and \ncan be considered as positive infinity. In this case we choose any positive number as period P which \nis always feasible. How\u00adever a better choice of P should be obtained by analyzing the size of the loop \nbody and the available resources. In general if the loop body is large and the resource is limited, the \nperiod should be larger. gether In general, there are many time-optimal schedules. One of our goals \nin this paper is to find the best schedule t, such that it will need the least number of registers. In \nthis section we look at the problem of providing the minimum number of buffers so that successive iterations \ncan be initi\u00ad ated at the desired optimal rate. We do not assume that a fixed schedule is given here. \nInstead we will find one that can achieve both time-optimality and space-optimality. The time-optimal \nproperty is enforced by using an optimal period while not fixing the timings of the individual nodes. \nA schedule may produce different numbers of results at different times during the execution. So our first \nobjective is to minimize the maximum of the number of buffers re\u00adquired at different times during the \nexecution. Let K be a given time instance. We want to know what should be the size of the buffer for \nnode i at time K. This size depends on the timings of i s successors and also on the reservation scheme \nfor the registers. Here we take the most conserva\u00adtive assumption that a register is reserved at the \nissue time of the instruction. However our analysis can be applied to other reservation schemes with \nonly minor modification. For instance, if we assume that a register is reserved only at the output stage \nof the pipeline, then a modified formulation can be obtained. See [25, 24] for details. Let us consider \none node j of s~ch successors, so that (i, j) is an arc from node i to node j. Suppose node i has started \nits IcI -t h inst ante execution and the ( kl + 1)-t h inst ante has not been started. Hence we have \nthe following relation: t*+ P(kl l)<K<t, +Pkl. (2) Suppose that at time K node j has started executing \nits kz-th instance, and the (k-z + 1)-th instance has not been start ed. Hence we have the following \nrelation: tj +P(k2 1) ~ h < tj +PkQ. (3) Therefore at time K, the number of results produced by node \ni which have not yet been consumed by node j is ICI + m,~ kQ. Therefore we should allocated a buffer \nof size at least kl k2 + m,~ to node i. Next we calculate the bounds on kl and kZ in terms of t,and \nt~. From inequalities (2) and (3), we can transform them into: h t, <,l<K=L+,, (4) P P and K tj <k2<=+l. \n(5) P P Let us consider (4) first. There is only one integer value in the half-open-half-closed interval: \nK t, A t, (~, ~ +1]. (6) Hence kl equals ~ + 1 if the latter is an integer, oth\u00aderwise ICI equals integer \nceiling of the lower bound, i.e. /cl = [=1. But in both cases, it is easy to verify that the following \nis always true: (7) Similarly for /cz, it is always true that 4 Solution of the OSBA Problem K tj+l \nlm=( ~ 1. (8) At time instance K, the number of results produced by node i which has not yet been consumed \nby node j is: kl+rn,j-k2=[ -:+ll+m,, -F:+ll. (9) Let b, be the buffer size for node i Then we should \ncon\u00adsider all the immediate successors of node i for the bounds in (9). When we consider all the outcoming \narcs from node i, we have: b,~( -:+ll+?W,-(K-;+ll, V(i, j) e E,VK e z+. (lo) where Z+ is the set of all \npositive integers. The constraints in (10) are too complicated to handle in two aspects. First it must \nbe true for all the positive values of K. This condition can be reduced to consider K for P con\u00adsecutive \nvalues only, because of the periodicity of our sched\u00adule. Secondly, the right hand sides of the inequalities \ninvolve integral ceiling function which is not a linear function. In order to formulate the problem into \nan integer linear pro\u00adgramming formulation, some approximation must be taken to deal with the integral \nc&#38;ling function. Our approach here is to approximate k] in (7) by (11) and to approximate kz in \n(8) by (12) With these approximations in (11) and (12) we can get a simplified constraints on b, as the \nfollowing: bi >k~+m,j k~ = ~ +1+ m,J li-~+l, V(i,j) cE,VK cZ+ .~+m,~+l, Y(i, j) c E. (13) Let us notice \nthat in (13) the time instance parameter K haa disappeared. That is due to the result of the approxima\u00adtion. \nNow we are ready to formulate our optimal schedule and buffer allocation problem into an integer linear \nprogram problem by combining the constraints for feasible schedules and the constraints for buffer sizes, \nas follows: Optimal Scheduling and Buffer Allocation (OSBA) Problem: subject to Pb,+t, tj~P(m,j +1) 1, \nV(i,j) ~E t3 t, >d, Pm,J, V(i, j) e E t,, b, integers, Vi c N. (14) In last section we obtained an integer \nprogramming formu\u00adlation (14) of the OSBA problem. In this section we inves\u00adtigate its solution. Here \nwe do a variable substitution: b; = Pb, (15) and transform the formulation (14) into the following form: \nOSBA Problem with Variable Substitution (15): min ~,e~ b: subject to b:+tt tj >P(mtj+l) l, v(t,j)e E \nt] ti >d, Pm,J, V(i, j) c E b;, t, integers, Vi c N. (16) We state the following theorem which is essential \nto solve (16). The proof can be found in [25, 24], Theorem 4.1 The constraint matrix in (16) is totally \nuni\u00adrnoduiar, that is to gay, each of its square submatrices has a determinant equal to either O or 1 \nor 1. The right hand sides of (16) are all integral numbers. From linear programming theory [30] that \nif the constraint matrix is totally unimodular and the right hand sides are in\u00adtegral, then the integer \nprogramming problem can be solved as a linear programming problem. The optimal linear pro\u00adgramming solution \nis guaranteed to be integral. Hence we obtain the following corollary: Corollary 4.1 When (16) is solved \nas a linear program\u00adming problem (dropping the integer requirement), the sohJ\u00adtion obtained is always \nintegral, i.e. it is the integer pro\u00adgramming solution of (16). To solve (16), we can use general linear \nprogramming al\u00adgorithms like simplex methods [9], or ellipsoid method [19] or interior point methods \n[18]. Here we present an more efficient algorithm in next subsection. 4.1 More Efficient Algorithm for \nSolving OSBA In this subsection, we show a more efficient algorithm to solve OSBA problem as expressed \nin (16). The algorithm is a number of transformations of the problem to the minimum cost flow problem. \nSince the minimum cost flow problem can be solved more efficiently by the so called combinatorial algorithms, \nthis will imply that our original (16) can also be solved more efficiently. Let us first write down the \nlinear programming dual of (16): x subject to A,J=l, ViEN (17) 3CJ+(,) max -~ (PmiJ di)~ij Aij >0, ~tj \n>0, V(i, j) E E. (i,j)r3E where 6+ (i), 6-(i) are the sets of immediate successors and subject to immediate \npredecessors of i, respectively. If we reorganize the variables in the objective function, then the objective \nfunction can be written as: jC6i_(i) je6-(i) (i,j)GE (t,J)6E gi~ unrestricted, m,~ >0, V(i, j) c E. \n Formulation (22) is not yet a minimum cost flow problem. Next we show that (22) can be further reduced \nto a minimum cost flow problem. But let us first consider the following problem: subject to (23) jG6+(i) \njC6+(i) jC6-(i) Note that the [P l)n in (18) is a constant. The variables .fij ~ l, V(i, j) c E. in \nthe constraints can also be rearranged. After these rear\u00adrangement, the dual problem can be written in \nthe following The first set of constraints in (23) gives a lower limit on form: the sum of output flow \nfor each node. The second set of con\u00adstraints is the conservation restriction for the flow meaning that \nflow coming into a node must equal to the flow com\u00ading out of that node. If the first set of constraints \nhave not appeared in (23), then it is the ordhmry minimum cost net\u00ad subject to work flow problem. We \nwill show that how we can split the Alj=l,VieN (19) x nodes in the graph to make the current formulation \nfitting ~G6+(i) into the ordinary minimum cost flow problem. Actuslly, we can replace each node i in \nthe original graph by two nodes i and i . The origiual input arcs to node i jG6+(i) jC6-(i) are now \ndirected to node i . The original output arcs from Atj >0, ~lj ~ 0, V(i, j) 6 E. node i are now going \nout from node i . We also add a new arc from node ii to node i . Now consider the ordinary Now we do \na variable substitution: minimum cost flow problem on this split graph. Let N be g,j = r,j A, ,V(i, \nj) c E. (20) the set of i nodes and N be the set of i nodes. We use E to denote the set of arcs in the \nsplit graph. With thw variable substitution, the objective function in It is easy to see that the following \nminimum cost flow (18) becomes: problem (24) is equivalent to (23). subject to (i,j)~E iEN jG6+(i) tJe6+(u) \nVG6-(U) f; 2 -l, V(%W) G E . where we define the cost coefficients in the objective function (i,j)eE \niCN by Note that the last term in (21) is also a constant. Then ifu=i 6N andv=eN e, , the formulation \n(19) becomes the following with the variable d;v = fm j - ifu=i G N andv=i e N #. substitution 20: { \nLemma 4.1 Formulation (24) and formulation (23) are equivalent, that is, 9aVetI an Optimal sdut~on }(U,V)EEI \n{fth Of (~4) then the {ftj }(1.?)cE defined @ the fo~l@w formula is an optimai solution of (.23): andv=jl~N \nandi #j, fl~ = f;., if u= i EN Similarly, given an optimal solution {fij }(t,j)eE of (~~), the the following \ndefined {f&#38; }(ti,v)6E/ iS an oPtimal solution of (24): The proof of the lemma is straightforward. \nIt is well known that the minimum cost flow can always obtain an optimal integer flow if all the capacity \nconstraints on the arcs are integral [30]. The capacity constraints on the arcs in (23) and (24) are \nintegral, therefore they have optimal integral solutions. Actually the efficient out-of-kilter algorithm \n(see [21]) and its variants will give such an optimal integer solution when it is applied to (24). Now \nwe are ready to proof the equivalence between (17) and (23). Lemma 4.2 Given an optimal integra! solution \nof (23 ) we can transform it to obtain an optimal feasible solution for (17) and vice versa. Proofi Since \n(17) is equivalent to (22), we only need to show the equivalence of (22) and (23). Let {f,, }(,,,)e~ \nbe an optimal integral solution of (23). First let g,~ = f,j, V(i, j) c E. Because of the first set of \nconstraints in (23), and the fact that ~,~ S are integers, we have either ~ f,, = -1, (25) jc6+(i) or \n~ f,, > o. (26) 3G6+(I) If (25) happens, we let m,j = O,Vj c 6+ (i). If (26) happens, we let ~jj = 1 \n+ gjj, for a particularly given j, c 6+ (i) and ~i~ = O for all other j c f+(i) {ji}. It is ewy to verify \nthat such defined {gij, ~tj } is a fe~ible solution of (22). Furthermore, since X,j does not appeu in \nthe objective function in (22), this solution wiU have the same objective value as that of (23) defined \nby the given fij. Next we prove the reverse. Suppose that {g,j, Xtj } is an optimal solution of (22). \nSimply let fil = gi~, V(2, j) c E will give a feasible solution of (23). Again because ~,j does not appear \nin the objective fnnction in (22), snch defined ~.j will give the same objective value of (23) as gij, \n~ij gives for (22). Since the feasible solutions for both problems have the same objective value, one \nof them is optimal implies that other must be optimal. a Theorem 4.2 The problem (16) can be solved in \nO(n3 log n) time, where n is the number of nodes in the graph representing the loop. Proofi Recall that \n(17) is the dual of (16). Therefore it is equivalent to solve either of them. Lemma 4.2 proved the equivalence \nbet ween (17) and (22). Lemma 4.1 proved the equivalence between (22) and (23). Therefore (17) is equiv\u00adalent \nto (23) which is a minimum cost flow problem. Using the Out-of-Kilter method in [21] to solve the minimum \ncost flow problem, the algorithm for our case has a complexity 0(w3 log n). Hence our transformation \nprocedures never use more than 0(n3 log n) time, we can conclude here that (17) can be solved in 0(n3 \nlog n) time.  4.2 Back Substitution Our method reduces the problem to a minimum cost flow problem on \na network, which has a 0(n3 log n) algorithm. The t, variables give the optimal schedule of the nodes. \nThe b: variables have to be substituted back by the formula (27) However such b, s may not always be \nintegral since we have done a divide operation in (27). If we simply round the bi s to their integral \nceilings, a suboptimal solution may result. We can resolve this problem by noticing that by this time \nthe schedule is already produced. By fixing the schedule to be the t, s produced by the solution of OSBA, \nwe can recal\u00adculate the kl and kz by (7) and (8) only for P consecutive time inst antes of K, starting \nfrom the time it enters a steady state. It turns out that the starting time of the steady state can be \nthe cycle time when the first iteration finishes its lat\u00adest instructions. We use Q to denote the cycle \ntime when the latest instruction finishes. Considering all the successors of node i, we should allocate \na buffer of size R, determined by the following formula (28): Vje6+(i), for fC=Q, Q+l,..., Q+(P -1), \n} (28) where 6+ (i) is the set of immediate successors of i. For our example loop in Section 2, its OSBA \nproblem is: min bl+bz+ bs subject to 2bl+tl tz~l 2bl + tl -t3~l 2b2+iz-ts>l (29) 2bs+ts tl>5 tz tl~l \nt3 $1>1 t3 t2~2 tl t3~ 3 Solving (29) for loop L, we obtain the following scheduling and guidelines for \nthe buffer sises: tl=o, t2=l, t3 =3, (30) b1=2jb2=;, ba=l. (31) If we round up the solution for the \nhi s in (31), we would end up with 5 registers. However if we use (28) to calculate the real need for \nthe registers that support the schedule, then we can have the following allocation: R1=2, R2=I, R3=I. \n(32] which uses only 4 registers. The actusl schedule for the loop L is shown in (2) on page 3. In (2), \nnotice that by the time node 53 is first scheduled, its predecessor node S1 hss been executed 2 times. \nThat is why we allocated a FIFO buffer of size 2 for node sl. Code Generation In this section we discuss \nthe code generation problem baaed on our solution of the schedule and register allocation prob\u00adlem in \nSection 4. The unique sspect of our code generation method is the buffer registers allocated to each \nnode as a FIFO (First-In\u00adFirst-Out) queue which haa multiple heads. If we allocated more than one register \nto a node, arranging them as a FIFO queue can make sure that the results are consumed in the same order \nas they are produced. ConceptusJly, the new result produced by a node should always be written to the \ntail of the corresponding FIFO buffer, while the successors of the node should read the re\u00adsults at the \nproper places of the buffer. However, we must note that it is not always true that the successors should \nread the results from the same head of the FIFO buffer. In some cases it is possible a successor should \nread the result from a place in the FIFO buffer other than the head. In other word, the FIFO buffer should \nhave multiple heads. The intuition is that these successor nodes are, in general, executing at dif\u00adferent \ntime points in the software pipelined schedule. And the buffer shifts its contents each time a new value \nis pro\u00adduced by the associated node. Therefore the successors need to read from dfierent places of the \nbuffer. Hence, a FIFO with multiple heads is required. Such a multiple-head buffer was illustrated in \nFigure 2 in Section 2. In the rest of this section, we illustrate two schemes to generate code which \nimplements a FIFO buffer using regis\u00adters. The tradeoff of dedicated hardware architecture sup ports \nwill also be discussed. Scheme I: Access Stationary Coding (ASC). In this scheme, the FIFO buffer between \na producer node and its successor nodes is directly accessed using fixed reg\u00adister assignment for the \ntail and the heads. This sasign\u00adment is stationary , and will remain the same during the entire execution. \nOn the other hand, the data in the FIFOS are explicitly shifted each time the producer node is writing \na new value to its tail. The shift\u00ading can be realized by issuing multiple register move instructions, \nor by special architecture support for reg\u00adist er shifts. Scheme II: Data Stationary Coding (DSC). In \nthis scheme, instead of letting the registers in a buffer to shift their contents, we simply let the \nnext iteration write to the next position in the corresponding FIFO buffer. Thus, data are kept stationary, \nwhile accesses to the registers of a FIFO buffer are performed with the modulo addressing method. 5.1 \nScheme I: Access Stationary Coding Under Access Stationary Coding (ASC) scheme, the code generated with \nregister shifting for our example loop L is given in Table 5. In the table, at cycle time O (or cycles \n2, 4, 6, etc) the FIFO buffer of two registers allocated to node S1 (for variable a) shifts its contents, \nand a new value is written into its tail a.. We assume that at the beginning of the cycle, all the old \ncontent are read off from the registers, and at the end of the same cycle new contents are written back \ninto the registers. Therefore al = aO, = X + c have a. the effect of shifting the old contents in ao \nto the register al and the new result is written into the tail at the same aO clock cycle. It is safe \nto overwrite al at this moment because the schedule and the supporting buffer allocation guarantee that \nthe old content of al is no longer used. We always align the shifting operation at the point when the \ncorresponding instruction is issued. To ensure that the successors also read the correct results from \nthe right places, we have to calculate the positions for them to read in the FIFO buffer. We have seen \nthe use of multiple-head buffer in Section 2. Here we give a lemma to calculate the positions for the \nsuccessors to access the data from the buffers: Lemma 5.1 Let (i,j) be an dependence arc in the DDG, \nthat is to say, that node i is the producer and node j is a consumer (successor). The formula to calculate \nthe position from which node j should read in the FIFO buffer of node i is: (33) Proof of Lemma 5.1: \nNode i writes the result to the tail of its FIFO buffer at time ti+ (K -1)* P in iteration K. This result \nwill be read by node j in iteration K + rn,j. Node j in iteration K+ m,j is scheduled at cycle tj+ (K-1 \n+ WZ,J) * P. Therefore the time difference between the production and the consumption is: [tJ+(K-l+7?l:J) \n*P]-[ti+(K-l)*P] tj-ti+?71ij *P. During this time interval, there are p -t,j?nt,*P 1-1= [+1+%-1 many \nregister shiftings for the butfer allocated to node i. Also note that the above formula is independent \nof iteration K. Hence node j (in any iterations) should read from the buffer position indexed by the \nabove formula which is (33). Cl As an example, we use formula (33) to calculate the posi\u00adtions node 32 \nand node S3 read from the FIFO buffer of size 3 allocated for node S1. For node S2, we have tz tl indezl,z \n= ( p l+7nl,2-1   = [L# 1+o.1 =o, so node .SZshould read from ao iteration 1 iteration 2 iteration \n3 iteration 4 J o buffer-shift: al =ao sl,l:aO=X+c 1 sz,l:b=aO*F 2 buffer-shift: al = a. sl,z:aO=X+c \n3 ss,l:c=al+b sz,z; b=aO*F 4 buffer-shift: al = a. sl,3:a0=X+c 5 sa,z:c=al+b sz,a:b=aO*F 6 buffer-shift: \nal = ao sl,l:aO=X-+c 7 ss,a:c=al+b sz,4:b=ao*F 8 9 sa,d:c=al+b Table 5: Code generated by ASC scheme. \n For node S3, we have prologue code [ indezl,s = ( ~3; q+rn,,3-1 p] \u00ad epilogue code = 1, J so node \nS3 should read from al. If we examine line by line horizontally at the code in Table Table 7: Code from \nthe repeating pattern of the ASC code. 5, we can discover a repeating pattern of the code from cycles \n2 to 3, as shown in Table 6. So far, we assume that the register shifting operationiteration i iteration \ni+ 1 I can be implemented using register moves (copying) in con\u00adventional architectures. However, it \nis also possible that a processor architecture supports register shifting directly in ~ hardware. Such \nsupport allows the ALUs be devoted to other computation functions, thus improving performance. Table \n6: The repeating pattern from the ASC code. 5.2 Scheme II: Data Stationary Coding The Data Stationary \nCoding (DSC) scheme proposed here is intended to avoid explicit register shiftings in the previousWe \nwill use this repeating pattern as our new loop body. ASC scheme. Instead of letting the registers to \nshift theirThe original loop is now transformed into an new parallel contents, we simply let the next \niteration write to the next loop body plus a prologue (lines O to 1) and an epilogue. The position in \nthe corresponding FIFO buffer. For the successorimportant fact is that the new loop body is only of P(=2) \nnodes, we can not simply use formula (33) to calculate thecycles, which means that in every 2 cycles \na new iteration positions to read in the FIFOS. Instead we lmust use modulo will start. That is the optimal \nrate we can obtain. The new addressing according to the following lemma. parallel loop is shown is Table \n7, in which the II sign means execute in parallel with . Lemma 5.2 For a dependence arc (i,j), if in \nthe current Generally, let iteration node i is writing to Q, (where Q, is the index) of its buffer, then \nnode j in current iteration should read from position: index~j = (Qi -~iJ) mod Rij (34) then the pattern \nis formed from time instances T P + 1 to T. where R, is the bufler size. Proof of Lemma 5.2: Suppose \nthat the current iteration 6 Reduce Register Requirement Further by is K. Then node i writes to the position \n(K 1) mod R, since the data are not to be moved. In current iteration A , node j should read the result \nproduced by node i in iteration K m,~. Therefore node j in current iteration should read from position \n(K m,j 1) mod R,. Substitute K 1 with Q:, we obtain the formula (34), The code generated using modulo \naddressing scheme is shown in Table 8. For example, at cycle P(i 1), we have the instruction a(,-l) \nmod z = X + c for iteration i. iteration i I m Table 8: Code for one iteration generated by DSC scheme. \nIts expanded version is shown in Table 9 on the next page. In Table 9 the repeating pattern is from \ncycle 2 to cycle 3, which is shown in Table 10. iteration i iteration i+l SI,,+I : a =X+c dz S3,, : c=a \n 1-1 )modztb 2, +1 : b = at mod 2 * F Table 10: Repeating pattern from the I)SC code. We can see from \nTable 10 that the pattern derived by using the DSC scheme contains less instructions than that of the \nASC scheme, which is due to the elimination of the register shifting operations. The new parallel loop \nbody is shown in Table 11. prologue code II for i= 1to n-P do I a,mod, =x+c c=af,_l~modz+bllb=atmod \n2*F enddo erilome code Table 11: New loop body from the DSC code repeating pat\u00adtern. Coloring: Step \n2 In Section 5 we showed how to generate code from a re\u00adpeating pat tern. That finished the first step \nof our register allocation scheme. At this point, the FIFO buffer sizes and the schedule are all determined. \nHowever we still have the chance to share the buffer elements if their live ranges do not overlap with \neach other for this fixed schedule. Hence second step of our register allocation scheme is to apply the \nconventional coloring algorithm(s) to further reduce the reg\u00adister requirement. For each of the instructions \nthat we allocated a buffer of size 1, the register may be t bought as a symbolic register, Each such \nsymbolic registers may be reused. For an instruc\u00adtion we allocated a buffer of size more than 1, the \nchance to reuse the registers in the buffer is only for the head register in the FIFO because all the \nother registers are live in the entire range of the loop cycle. In our example loop L, suppose that we \nuse the ASC scheme to generate code, then we choose the repeating pat\u00adtern in (6), and draw the live-range \ndiagram of the variables in Figure 3 in Section 2. We can draw the interference graph according to the \ncir\u00adcular arcs in Figure 3, and color the interference graph with 3 colors. For instance, the following \nis a legal coloring with 3 colors: color-1 = {al, c}, color_2= {b}, color.3 = {aO}. Therefore the actual \nnumber of registers required for the re\u00adpeating pattern is 3 for these 3 colors, plus 2 extra for loop \ninvariants X and F, which totals 5 registers. In general we can use the coloring aigorit hms [8, 7, 16] \nto obt tin the mini\u00admum number of registers used in the new loop body. In this paper, we apply a recent \nmethod of cyclic interval graph coloring [16]. After the coloring algorithm the final code for the repeat\u00ading \npattern is shown in Table 4 in Section 2, in which c is replaced by al since they have the same color. \nThe coloring algorithm can also be applied to the code produced by DSC scheme. However the live ranges \nof the registers in a buffer of size more than one may last for sev\u00aderal repeating patterns (new iterations) \nbecause there are no explicit register-shiftings now. For instance, the live ranges of the variables \nin Table 11 generated by the DSC scheme is shown in Figure 4. In the picture no two variables can be \ncolored the same color. Hence the code already uses mini\u00admum number of registers and we do not need to \nchange the code again for this example. 7 Related Work The early work by Aiken and Nicolau [2, 3, 4] \ndid not con\u00adsider register allocation problem. In a recent paper, Nicolau et al [23] considered the register \nallocation problem by re\u00adnaming for the compact ion-by-percolation based algorithms. Ebcioglu et al have \nproposed the technique of enhanced soft\u00ad ware pipelining with resource constraints [12, 13, 11, 22]. \nHowever, they did not consider the minimum register allo\u00ad cation problem as discussed in this paper. \nIn Lam s work on software pipelining [20], an interesting scheme called raodrdo variable expansion is \nproposed to al\u00ad iteration 1 iteration 2 iteration 3 iteration 4 0 sl,l:aO=X+c 1 sz,l:b=aO*F 2 sl,z:al=X+c \n3 ss,l:c=ao+b sz,z:b=a~*F 4 sl,3:a0=X+c 5 ss,z:c=al+b sz,s:b=aO*F 6 sl,4:al=X+c 7 sa,s:c=ao+b s2,4:b=a1*F \n8 9 s3,4:c=a1+b Table 9: Expanded code from DSC scheme. at a fixed initiation interval. The register \nallocation prob\u00adlem is formulated as a bin-packing problem of vector lifetimes on a cylinder surface. \nA heuristic aJgorithm has been pro\u00ad ----:\u00ad ----.\u00ad posed for the register allocation and has been demonstrated \nc~ %mod2~ to be quite effective by experimental results. However, the paper did not attempt to describe \na complete concurrent b ----------------b---\u00ad:: scheduling-allocation strategy for software pipelining. \nIn fact we have calculated the number of registers needed by our method for the example in Rau et al \n[28]. Our method --A ----o---- . _ --\u00aduses 21 registers, while Rau et al claimed that 28 was needed b \nin their case, a 25~o improvement in register usage without  ----W - b compromising the speed. Other \nrelated work can be found in [14, 10]. Figure 4: Life range intervals for code generated by DSC scheme. \n8 Conclusions and Future Work Many conventional register allocation algorithms are based on the coloring \nof interference graphs representing overlap\u00adlow a scalar loop variables be expanded to use more than \nping relations of the life ranges of program variables given one location so that the unnecessary precedence \nconstraints a sequential execution schedule [8, 7, 1]. In this paper,due to scalar variable in different \niterations can be removed. we formulate the register allocation problem as part of the However modulo \nvariable expansion is only performed after joint schedule-allocation problem. Intuitively, we are usingthe \nschedule has been fixed. The work described in in this register allocation as a constraint to the software \npipelin\u00adpaper can be considered as an extension to modulo variable ing scheduling process to derive, \namong all time-optimalexpansion in the sense that it is incorporated in a unified schedules, the ones \nwhich have the potential to use the min\u00adframework of time-optimaJ scheduling, and minimizes the imum \nnumber of registers, Once such a schedule is fixed, we amount of storage for scalar variable expansion \nand array can use conventional methods to further minimize the regis\u00advariable shrinkage. ter requirements. \nThis paper has provided a new framework Callahan, Carr and Kennedy have studied re~ister allo\u00ad for simultaneous \nscheduling and register allocation for soft\u00adcation for subscripted variables. In their method, array \n ware pipelining. We plan to extend this framework to real references which are live across several iterations \nare rec\u00ad applications in our future work. One scenario is that the ognized and a source-to-source transformation \ncalled scalar number of available registers is given. Another scenario is replacement is performed such \nthat they can be handled by that the loop body may contain conditionals. coloring-based register allocators. \nHowever, their work is aimed at sequential loop execution and does not consider loop scheduling such \nas software pipelining. We have shown References in [25, 24] that our OSBA formulation (14) includes \nCalla\u00adhan et al s result as a special case. [I] A. V. Aho, R. Sethi, and J. D. Unman. Compilers Principles, \nTechniques, and Tools. Addison. WesleYIn a recent paper by Rau et al [28], a method of regis\u00ad ter allocation \nfor software pipelining was presented. In t-his Publishing Co., 1986. met hod, register allocation is \nperformed after the so-called [2] A. Aiken. Compaction-based parallelization. (PhD the\u00admodular scheduling \nphase. Successive iterations are initiated sis), Technical Report 88 922, Cornell University, 1988. \n[3] A. Aiken and A. Nicolau. Optimal loop parallelization. In Proceedings of the 1988A CM SIGPLAN Conference \non Programming Languages Design and Imp!ementa\u00adtiorz, June 1988. [4] A. Aiken and A. Nicolau. A realistic \nresource\u00adconstrained software pipelining algorithm. In Proceed\u00ading of the Third Workshop on Programming \nLanguages and Compilers for Parallel Computing, Irvine, CA, Au\u00adgust 1990. [5] D. G. Bradlee, S. J. Eggers, \nand R. R. Henry. Integrat\u00ading register allocation and instruction scheduling for RISCS. International \nConference on Architectural Sup\u00adport for Programming Languages and Operating .Sys\u00adtenas (ASPLOS IV), \npages 122-131, April 1991. [6] David Callahan, Steve Carr, and Ken Kennedy. Improv\u00ading register allocation \nfor subscripted variables. Pro\u00adceedings of the SIGPLA N 90 Conference on Program\u00adming Language Design \nand Implementation, June 1990. White Plains, NY. [7] G. J. Chaitin. Register allocation &#38; spilling \nvia graph coloring. ACM SIGPLAN Syrnp. on Compiler Con\u00adstruction, pages 98 105, 1982. [8] G. J. Chaitin, \nM. Auslander, A. Chandra, J. Cocke, M. Hopkins, and P. Markstein. Register allocation via coloring. \nComputer Languages 6, pages 47-57, January 1981.  [9] V. ChvataL Linear Porgranwning, W. Ii, Freeman \nand Company., 1983. [10] E. Duesterwald, R. Gupta, and M.L. Sofia. Register pipelining: An integrated \napproacn to register alloca\u00adtion for scalar and subscripted variables. Technical re\u00adport, Department \nof Computer Science, University of Pittsburgh, 1991. [11] K. Ebcioglu and T. Nakatani. A new compilation \ntechnique for parallelization loops with unpredictable branches on a VLIW architecture. Technical report, \nIBM, 1990. [12] K. Ebcio~lu. A compilation technique for software pipelining of loops with conditional \njumps. In Proceed\u00adings oj the 20th Annual Workshop on Microprograrn\u00adrning, December 1987. [13] K. Ebcio~lu \nand A. Nicolau. A global resource\u00adconstrained parallelization technique. In Proceedings of the ACM SIGARCH \nInternational Conference on Su\u00adpercomputing, June 1989. [14] Christine Eisenbeis, WiUiam Jalby, Daniel \nWindheiser, and Francois Bodin. A strategy for array management in local memory. In Third Workshop on \nProgramming Languages and Compiters for Parallel Computing. Uni\u00adversit y of California, Irvine, 1990. \nTo be published by Pitman/MIT Press. [15] P. B. Gibbons and S. S. Muchnick. Efficient instruc\u00adtion scheduling \nfor a pipelined architecture. In Proceed\u00adings of the ACM Symposium on Compiler Construction, pages 11 \n16, PaJo Alto, CA, June 1986. [16] L. Hendren, G.R. Gao, E. Altman, and C. Mnkerji. A register allocation \nframework based on hierarchical cyclic interval graphs. Lecture Notes in Computer Sci\u00adence 641, pages \n176 191, October 1992. [17] J. Hennessy and T. Gross. Postpass code optimization of pipelined constraints. \nACM Transactions on Pro\u00adgramming Languages and Systems, 5(3):422-448, July 1983. [18] N. Karmarkar. A \nnew polynomiaJ-time algorithm for linear programming. Combinatorics, 4:373-395, 1984. [19] L. G. Khachian. \nA polynomial algorithm in linear pro\u00adgramming. Scwiet Math. Doldadg, 20:191 194, 1979. [20] Monica Lam. \nSoftware pipelining: An effective schedul\u00ading technique for VLIW machines. In Proceedings of the 1988 \nACM SIGPLAN Conference on Programming Languages Design and Implementation, pages 318 328, Atlanta, GA, \nJune 1988. [21] Eugene L. Lawler. Combinatorial Optimization: Net\u00adworks and .kfatroids. Saunders College \nPublishing, Ft Worth, TX, 1976. [22] T. Nakatani and K. Ebcioglu. Using a lookahaed win\u00addow in a compaction \nbased parallelizing compiler. In Proceedings of the 23rd A nrmal Workshop on Micropro\u00adgramming and Microarchitectures, \npages 57 68, 1990. [23] A. Nicolau, R. Potasman, and H. Wang. Register al\u00adlocation, renaming and their \nimpact on fine-grained parallelism. In U. Banerjee et al., editor, Languages and Compilers for Parallel \nComputing, Lecture Notes in Computer Science 589, pages 359 373, Santa Clara, California, 1992. Springer-Verlag. \n[24] Q. Ning and G.R. Gao. A novel framework of reg\u00adister allocation for soft ware pipelining. Technical \nRe\u00adport ACAPS Technique Memo 42, School of Computer Science, McGill University, Montreal, Quebec, Canada, \n1992. [25] Qi Ning. Optimal Register Allocation to Support Time-Optimal Scheduling for Loops. PhD thesis, \nin prepa\u00adration, School of Computer Science, McGill University, 1992. [26] C. V. Ramamoorthy and G. S. \nHo. Performance evac\u00aduation of asynchronous concurrent systems using Petri Nets. IEEE Transactions on \nComputers, pages 440\u00ad448, September 1980. [27] B. R. Rau and C. D. Glaeser. Some scheduling tech\u00adniques \nand an easily schedulable horizontal architecture for high performance scientific computing. In Proceed\u00adings \nof the L/th Annual Workshop on Microprogram\u00adming, pages 183 198, 1981. [28] B.R. Rau, M. Lee, P.P. Tirumalai, \nand M.S. Schlansker. Register allocation for modulo scheduled loops: Strate\u00adgies, algorithms and heuristics. \nIn Proceedings oj SIG-PLAN 92 Conf. on Programming Language Design and Implementation, San Francisco, \nCA, 1992. [29] R. Reiter. Scheduling parallel computations. .70urnal oj ACM, 15:590 599, October 1968. \n[30] A. Schrijver. Theorg of Linear and Integer Program\u00adming. John Wiley and Sons, 1986. [31] R. F. Touzeau. \nA FORTRAN compiler for the FPS\u00ad164 scientific computer. In Proceedings of the ACM SIGPLAN 84 Symposium \non Compiler Construction, pages 48-57, June 1984. [32] H.S. Warren. Instruction scheduling for the IBM \nRISC System/6000 processor. IBM J. Res. Develop., 34(l), January 1990. Appendix A Example from Rau Et \nAl s Paper In this appendix, we look at the example loop given in Rau Et Al s paper [28]. The loop is \nshown in Table 12. fori=l tondo s=s+a[i] a[i] = .9* s* a[i] enddo n Table 12: Example from Rau et al \ns paper. Its low level representation, like 3-address code, is shown in Table 13. ur33 = VT33 + VT32 \n% vr33 is address of a[i] % zm34 = loadrn(or33) % vr34 = a[i]% V?-35 = ur35 + v r34 %vr35=s% vr36 = wr35 \n+ vr35 vr37 = VT36 + rw34 % vr37 = new a~] % store(rw37, m(vr33)) branch ifisn Table 13: Low level intermediate \nrepresent ation. We will focus on the low level representation and generate code for it. The data dependence \ngr,aph of the low level presentation is shown in Figure 5. The delay for Add and Store is 1, the delay \nfor Multiply is 2 and delay for Load is 13. There is only one directed cycle in the dependence graph, \nwhich is a self-loop from node a to a. The B-ratio of it is 1. Therefore we can generate a schedule with \nperiod of 1. However since Rau et al used 2 as their period, we will also use 2 as our period for generating \nthe schedule and the register allocation. The OSBA formulation of the low level representation is in \n35. Figure 5: Data dependence graph of the intermediate repre\u00adsent ation. minba+bb+bc+bd+be subject to \na ba+ta tb>l ba+ta tf~4 ba+ta ta>4 bb+tb tc>2 bt, +tb-te>2 bc+tc td>2 bd+td te>2 (35) be+t. tf>2 tb ta>13 \ntf ta> l ta t. > l t. tb~ls t. tb>ls td t. >l t. td> 2 tf te>2 Solving (s5) we obtain the following \nschedule and buffer allocation: t.=l, tb=o, t.=13, td=14, t.= 16, tf =18; (36) ba=; ,bb=9, bC=; ,bd=2, \nb.= 2. (37) With the technique (28) in Section 4, we obtain the fol\u00adlowing buffer allocation: R.=lO>Rt, \n=8, Rc=l, Rd=l, Re= 1. (38) Since the life ranges of the variables in the repeating pat\u00adtern all overlap, \nour second step of coloring algorithm is not necessary now. So there is a total of 21 register allocated, \nwhile Rau et al used 28 registers in [28].  \n\t\t\t", "proc_id": "158511", "abstract": "<p>Although software pipelining has been proposed as one of the most important loop scheduling methods, simultaneous scheduling and register allocation is less understood and remains an open problem [28]. The objective of this paper is to develop a unified algorithmic framework for concurrent scheduling and register allocation to support time-optimal software pipelining. A key intuition leading to this surprisingly simple formulation and its efficient solution is the association of maximum computation rate of a program graph with its <italic>critical cycles</italic> due to Reiter's pioneering work on Karp-Miller computation graphs [29]. In particular, our method generalizes the work by Callahan, Carr and Kennedy on <italic>scalar expansion</italic>[6], the work by Lam on <italic>modular variable expansion</italic> for software pipelined loops [20], and the work by Rau et al. on register allocation for modulo scheduled loops[28].</p>", "authors": [{"name": "Qi Ning", "author_profile_id": "81332518612", "affiliation": "", "person_id": "PP39081212", "email_address": "", "orcid_id": ""}, {"name": "Guang R. Gao", "author_profile_id": "81100134147", "affiliation": "", "person_id": "P100128", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/158511.158519", "year": "1993", "article_id": "158519", "conference": "POPL", "title": "A novel framework of register allocation for software pipelining", "url": "http://dl.acm.org/citation.cfm?id=158519"}