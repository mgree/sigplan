{"article_publication_date": "03-01-1993", "fulltext": "\n Static Single Assignment for Explicitly Parallel Programs Harini Oregon Srinivasan, James Graduate Institute \n19600 NWvon Beaverton, Hook and of Science Neumann OR 97006 Michael Wolfe* and Technology Dr. Abstract \nWe describe which use and prove the Parallel algorithms Computing to convert Forum programs Parallel \nSect ions construct (SSA) form. This into process Static allows Single Assignment compilers to apply \nclassical scalar optimization algorithms to explicitly parallel programs. To do so, we must define what \nthe concept of dominator and dominance frontier mean in parallel programs. We also describe how we extend \nSSA form to handle parallel updates and still preserve the SSA properties. k=() j=() loop Parallel Section \nj=j+l Sections A Section B 1 Introduction End k=s Parallel Sections While parallelizing sequential programs \nhas been a A(j) endloop = B(k) popular research topic, little work has been done on ap\u00ad plying grams. \nclassical optimizations to For sequential programs, explicitly parallel the Static Single pro-As\u00ad k=t) \nj=(l signment (SSA) form has proven to be an efficient and loop practical such as induction basis for \nmany code constant propagation, variable analysis, optimization redundancy and so on algorithms, elimination, \n[2, 9, 12, 13]; if (condition) j=j+l else k=5 then it has already found its way into modern commercial \nendif compilers. [4] that In this convert a paper we program extend the algorithms in into SSA form to \nhandle A(j) endloop = B(k) explicitly parallel programs. In order to translate a parallel program into \nSSA form, we have designed ab\u00ad stractions that represent both control flow and paral- Figure 1: Similar \nParallel and Sequential Programs, *Supported in part by NSF Grant CCR-9113885. but not Identical Permission \nto copy without fee all or part of ttis material is granted provided that the copies are not made or \ndistributed for direct commercial advantage, the ACM copyright notice and the title of the publication \nand its date appear, and notice is given that copying is by permission of the Association for Computing \nMachinery. To copy otherwise, or to republish, requires a fee and/or specific permission. ACM-20th PoPL-1193-S.C., \nUSA @ 1993 ACM 0-89791 -561 -5/93 /0001 /0260 . ..$1 .50 260 lelism. Other issues typical of parallel \nprograms, such as multiple parallel updates, are also considered. We chose to start with the Parallel \nComputing Fo\u00adrum Parallel Fortran extensions, since this is the basis of the ANSI committee X3H5 standardization \neffort [7]; because of the extensive industrial participation and commitment, these particular parallel \nlanguage exten\u00adsions will likely have widespread impact. While mul\u00adtiprocessor workstations are now available \nand parallel language extensions will allow users to write programs that use them, the actual performance \nof these pro\u00adgrams will depend to a large extent on the ability of the compiler to perform aggressive \noptimizations (par\u00adticularly scalar optimizations) within and across par\u00adallel constructs. For example, \nin the programs in Fig\u00adure 1, variable j is an induction variable in the parallel program, but not in \nthe sequential program. Recog\u00adnition of j as an induction variable is important for data dependence analysis \nand strength reduction. Fur\u00adthermore, variable k has constant value in the paral\u00adlel program since both \nbranches of the &#38;-k, i.e., the parallel sect ions statement, will be executed. This is also useful \ninformation since the compiler can deter\u00admine that the value assigned to A (j ) k loop invariant. This \nis not necessarily true for the sequential program since only one branch of the conditional statement \nexe\u00adcutes; even though the conditional branching behavior of the sequential program resembles the parallel \nfork structure of the parallel programl these two programs are different. Our work takes an important \nstep at the analysis necessary to support optimizations for parallel programs. Parallel Section Semantics \n The parallel sect ions construct [7] is similar to the cobegin/coend of Brinch Hansen [3] or the Parallel \ncases statement introduced by Allen et al [1]. It is a block structured construct used to specify parallel \nexecution of identified sections of code. The parallel sections may also be nested. The sections of code \nmust be data independent, except where an appropriate syn\u00ad chronization mechanism is used. Here we consider \nonly structured synchronization expressed as Wait clauses, i, e., DAG parallelism. Transfer of control \ninto or out of a=z (p) b=3 C=4 (p) (p) (p) if (Q) then Parallel Sections Section ii (s) if (P) then \n(t) b ==a*5 else (u) b=a+7 (u) f=b*a (v) end if Section B (w) c =C +15 (w) f =c *16 Section C, Wait(A) \n(x) d =b*a Section D, Walt(A, B) (y) c z=a*b+c*f (p) End Parallel Sections (n) d=d+f else (q) d=23 (r) \nend if (r) e=a+b*c*d Figure 2: Example Parallel Program a parallel section is not supported. An example \nparal\u00adlel program with Wait clauses is given in Figure 2. The Wait clause in section D specifies that \nthis section can start executing only after both sections A and B have completed execution. Note that \nWait clauses do not affect control dependence relations; all unconditional code in the parallel Sections \nconstruct is identically control dependent on the same predicate that controls execution of the parallel \nSections statement itself. Some definition must be made when two sections of code that can execute in \nparallel both modify the same variable, or when one section modifies a variable that is used by the other. \nWe advocate using copy-in/copy\u00adout semantics in the compiler [11]. This gives a well\u00addefined program \nwithout volatile variables, and allows optimization within a parallel section independent of code in \nother sections. Strict observance of the model has several potential problems, such as the overhead of \nmaking local copies of variables, and atomic merg\u00ading of updated variables. In the PCF extensions, the \nrequired data-independence between parallel sections allows the compiler to ignore this issue. The interplay \nbetween the language model, the compiler model and the architectural model is a subject for another paper. \n3 The Static Single Assignment Form This section reviews some concepts from [4]. The algo\u00adrithm to convert \na sequential program into SSA form uses the Control Flow Graph (CFG) of the program. A CFG is a graph \nG = (V, E, Entry, Ed), where V is a set of nodes representing basic blocks in the program, E is a set \nof edges representing sequential control. flow in the program and Entry and Exit are nodes repre\u00adsenting \nthe unique entry point into the program and the unique exit from the program respectively. When a program \nis converted into SSA representa\u00adtion, the resulting program has two key properties: 1. Every use of \nany variable in the program has ex\u00adactly one reaching definition, and 2. at appropriate confluence points \nin the CFG, merge functions called @functions are introduced. A q$-function for a variable merges the \nvalues of the variable from distinct incoming control flow paths.  One key algorithm to convert a program \nto SSA form is finding where to place @functions. Cytron et al de\u00adscribe the SSA algorithm using the \ndomtnance relatton and dommance frontiers. A node X domtnates a node Y (written X z Y) in a CFG if X \nappears on all paths from Entry to Y; a node X strtctly dominaies a nodeY(X>>Y)ifX~ YbutX#Y. The domi\u00adnance \nfrontier of a node X, DF(X), is defined as the set of nodes Z such that X dominates a predecessor of \nZ and X ~ Z. The dominance frontier of a set of nodes, DF(S), is defined as the union of the dominance \nfrontiers of the nodes in the set S. The iterated dom\u00adinance frontier, DF+ (S), is the limit of the increasing \nsequence of sets of nodes, DFl(S) = DF(S), DFi+l(S) = DF(S U DFi(S)) Cytron et al prove that ~-functions \nmust be placed at the iterated dominance frontier of the set of nodes which have assignments to a variable. \nDF+(S) is calculated using a simple and efficient worklist algo\u00adrithm. [Actually, the @functions are \nplaced at conflu\u00adence points in the CFG, defined as the Join set of two nodes in the CFG; Cytron et al \nprove that the iterated Join set is the same as the iterated dominance frontier.]  4 Flow Graphs for \nParallel Constructs As illustrated in the programs in Figure 1, traditional analysis methods on Control \nFlow Graphs will not triv\u00adially apply to parallel programs. It is not possible to analyze parallel merge \npoints, i. e., parallel joins using the techniques for sequential merge points, like endif statements. \nIn this section, we present an abstraction that handles both control flow and parallelism in par\u00adallel \nprograms. This abstraction, which is an extension to Control Flow Graphs, will be used to translate an \nex\u00adplicitly parallel program to its SSA representation just as the CFG was used to translate a sequential \nprogram to its SSA representation in Section 3. 4.1 Extended Flow Graph Set We define a Parallel Control \nFlow Graph (PCFG) as a CFG which may have a special type of node called a supernode. A supernode essentially \nrepresents an en\u00adtire parallel sections construct (or parallel block), described earlier. Formally, a \nPCFG is defined as a graph G = (VG, EG, EntryG, Exit G) where VG is a set of vertices (each representing \na basic block or an entire parallel block), EG is a set of edges representing con\u00adtrol flow, EntryG is \nthe unique start vertex and EzitG is the unique exit vertex. Parallel execution of the sections within \na parallel block is represented by a Parallel Precedence Graph (PPG). Nodes in the PPG represent the \nsections in the parallel block with two additional nodes, cobegin and coend. A wait clause in a parallel \nblock imposes watt-dependences between the waiting section and the sections specified in the wait clause. \nThe edges in the PPG (also called wait-dependence arcs) represent those wait dependence. Formally, a \nPPG is defined as a di\u00adrected graph P = (Vp, EP, Entryp, ExitP ) where VP is P~l Section A: %n Entry \n be EntryA P s AB 11 u PI t cD n v r eoend ExitA @@$ Figure3: EFGforthe a set of vertices, each \nrepresenting a section in a par\u00ad allel block, EP is the set of edges or wait-dependence arcs, Entryp \nis the cobegin node and Exztp is the co\u00ad end node. By definition of the language, the PPG must be acyclic. \nEach section in a parallel block is again represented by a PCFG S = (Vs, Es, EntryS, Exits) where Entry5 \nmarks the entry into that section and Exits marks the exit from that section. The Extended Flow Graph \nSet (EFG) is the set of PCFG s and PPG s representing control flow and par\u00adallelism for a single program \nunit. The distinguished PCFG corresponding to the program unit is called GmOin. We will talk about the \nset of nodes in an EFG, which is the union of all the nodes in all the PCFG S and PPG s in the EFG. These \nnodes fall into one of several categories: 1. program Entry or Exit nodes, 2. basic block nodes, 3. \nsupernodes corresponding to a parallel block, 4. cobegin or coend nodes in a PPG, 5. section nodes, \nand 6. section Entry or Exit nodes in a section PCFG.  The PCFG containing any basic block node X is \ndes\u00adignated GZ, and the section node correspond to G= is example parallel program designated S., and \nP. means the supernode represent\u00ading the PPG containing S=. The EFG for the parallel program in Figure \n2 is shown in Figure 3. The parallel block is represented by the node pi in Gmain, which is in turn represented \nby PPG pl. Each section of the parallel construct is represented by a separate PCFG, as shown in the \nfig\u00adure. For example, GV is the PCFG for Section A, SU is the node A in PPGP1, and P. is P1.  5 Factoring \nControl Flow Graphs capture information about the relative ordering of events. In particular, they allow \nus to determine what events may happen before a specific event and what events must happen before that \nevent. In the sequential case, this is determined by analyzing all paths through the graph. To relate \nproperties of the parallel programs to well understood properties of sequential programs, we exploit \nthe algebraic structure of parallelism to factor parallel programs, expressed as EFGs, into a set of \nCFGS representing all possible flow paths through the program. For example, the EFG in Figure 3 can be \nfactored into the CFGS in Figure 4. Questions of precedence can then be answered by do\u00ading conventional \nanalysis of the sequential factors and asking if there is a sequential factor in which one event always \nprecedes another. While factoring is a useful tool for reasoning about the application of sequential \ntechniques to parallel pro\u00adgrams, it is computationally intractable; the number of factors increases \nexponentially in the number of par\u00adallel segments in the program. In the sequel, factoring is used to \nargue that the efficient algorithms we develop for parallel program analysis are correct generalizations \nof sequential program analysis techniques. For example, we define the set of dominators of a basic block \nnode in the EFG M the union of the domi\u00adnators of this node in all the sequential CFG s derived by factoring \nthe EFG. Similarly, we define the domi\u00adnance frontier of any basic block node in the parallel program \nas the union of the sequential dominance fron\u00adtiers of the node in all the sequential CFG s derived by \nfactoring the EFG. We refer to this as the parallel dominance frontier (denoted PDF). The EFG in Figure \n3 has three factors, correspond\u00ading to the three paths through the PPG for supernode PI, as shown in \nFigure 4. In the A-C factor, p > s, and in the B-D factor, p > w; by our definition, p must dominate \nboth s and w in the original EFG. By our definition, since s and w do not appear in any com\u00admon factors, \nneither dominates the other. In general, a basic block node in an EFG may have more than one immediate \ndominator, meaning the parallel dominance relation can no longer be represented by a dominator tree. \nAnother benefit of using the hierarchical EFG ab\u00adstraction is that the dominator relation of each PCFG \nand PPG zs a tree; this allows us to use proven tech\u00adniques based on the dominator tree traversal, such \nas building dominance frontiers [4]. 5.1 Precedence relations and parallel precedence frontiers Computing \nparallel dominance frontiers as defined above is not efficient, due to the cost of factoring. This section \ndefines the precedence relation and the parallel precedence frontiers; the theorems that follow will \nthen relate the parallel dominance frontiers to the parallel precedence frontiers. Definition: For any \ntwo nodes X, Y, where X and Y are either nodes in the same PCFG, nodes in distinct PCFG S or nodes in \na PPG, we say X precedes Y if the execution of X must precede the execution of Y. Definition: The parallel \nprecedence frontier of a basic block node or supernode X, PPF(X), is defined as follows: If X ~ ExitG= \n(the exit node in G.) then PPF(X) = p~F/oca/(x) IfX ~ ExatG= then PPF(X) = PPFloca~X) U PPF(PC) where \nPPFloca~X) is the sequential domi\u00adnance frontier of X, defined within G.; PPFioca~X) is defined between \nnodes and supernodes in GZ and does not consider nodes within supernodes, and P. is the supernode containing \nX, as defined earlier. For example, in Figure 3, the sequential dominance frontier (SDF) of node v in \nthe PCFG for section A is the empty set. But the parallel precedence frontier of node v is PPF(v) = PPFzoca~v) \nU PPF(P1) = {r}. Similarly, the parallel precedence frontier of nodes w, x and y is the parallel precedence \nfrontier of the en\u00adclosing parallel block, PI, i. e., {r}. This is be\u00adcause P 1 represents the outermost \nparallel block and PPF(P1) = PPFioca{P1) = {r}. The algorithm to compute parallel precedence fron\u00adtiers \nof all nodes, given the sequential dominance fron\u00adtiers of each node in the PCFG S in the program is \ngiven in Figure 5. If P is the number of sequential sections of code in the parallel program, N and &#38; \nare the maximum of the total number of nodes and edges respectively in the PCFG s corresponding to each \nof these sections, then, the algorithm takes O(F x (fi2 + @ + P x N) time. The algorithm is called recur\u00adsively \nfor each section; the outer for loop is executed at most N times for each section, and the worst case \ntime to compute the sequential dominance frontiers is O(N2+ 2). 5.2 Equivalence between ~~F ancl l?~l? \nThe theorems presented in this section establish the equivalence of parallel precedence frontiers and \npar\u00adallel dominance frontiers defined above. Lemmas 5.1 264 A-C factor: A-D factor: B-D factor: Entry \nEtmy o Q Figure 4: Factors for the example parallel program PFRONT ( Gentrg ) Proc PFRONT( G ) /* Compute \nDF using the algorithm in [41*/ DF(*) + DFRONT(G) for each node X in G do PPF(X) + DF(X) if X dotninates \nExitGm then PPF(X) + PPF(X) U PPF(PT) endif if X is a parallel block, PX, then for each section G, in \nPz PFRONT( G, ) end endif end end PFRONT  Figure 5: Algorithm PFRONT: Computing parallel precedence \nfrontiers through 5.6 are used in the proof of the theorems. These lemmas establish the properties of \nPCFG s and factoring. For purposes ofproving the equivalence between the parallel dominance frontier \nand the parallel precedence frontier, the following extensions tothe constructionof the Parallel Control \nFlow Graph are made. These ex\u00adtensions do not change the semantics of the parallel program: For every \nsupernode, Pr, representing apar\u00adallelblock, two additional basic block nodes, called the head and tail \nnodes, are introduced. The head node captures all the incoming control flow edges to Pc and the control \nflow successors of the tail node are those of PC in the original PCFG. Clearly, the tail node has exactly \none control flow predecessor, namely P. and Pz is the only control flow successor of the cor\u00adresponding \nhead node. We refer to the head and tail nodes ofpc asxhead and xtaz~ respectively. Thefol\u00adlowing straightforward \nobservation can be made about the head and tail nodes: Clearly, X is reachable from is reachable from \nX in any sequen\u00ad head and tail tial CFG derived by factoring the PCFG that includes X. Since Xiazl is \nthe only successor of Pz, PPF(PZ) = PPF(Xtazi). Ler.mna 5.1 In any factored CFG G~ such that X Proofi \nBy definition of DF, we know that Xtail ~ P for some P c Pred(Z). Therefore by construction, X ~ P. Also, \nsince Xiazl ~ Z and any path from X to Z must include Xtaii) X ~ Z. Hence, Z E DF(X). 0 Lemma 5.2 1. \nX z Xta2/ and Y C F DF (Xiai/) then Y c PDF(X). Proof By construction, X c Gf (a factored CFG) ~ Xtaai \nE Gf. Hence, by lemma 5.1 and definition of PDF, PDF(Xtaij) ~ PDF(X). 0 Lemma 5.3 If Y c PPF1ocal(X) \nthen dZ e Y U Pred(Y), Z is not a supernode. Proof Clearly Y is not a supernode, since (by con\u00adstruction) \na supernode has exactly one predecessor, the corresponding head node. If a predecessor of Y is a su\u00adpernode \nthen, by construction, its only successor is the tail node (i. e. Y), which has only one predecessor; \nthis contradicts the assumption that Y c PPFloca~X). 0 Lemma 5.4 For any node X representing a baszc \nblock, PPFloca~X) ~ PDF(X). Proof Consider Y E PPFioca~X). By lemma 5.3, Y represents a basic block (i.e. \nY is not a supernode) and by definition of PPFioca~X), X s Z where Z is solme predecessor of Y and X \n~ Y. By lemma 5.3, Z is not a supernode. By construction and the definition of factoring, we know that \nif P and Q are two nodes that represent basic blocks in a PCFG then P ~ Q in the PCFG if and only if \nP ~ Q in some factored CFG where P and Q are nodes. Therefore, X ~ Z in some factored CFG where X and \nZ are nodes and X > Y (and X ~ Y) in all the factored CFG S where X ~d Y are nodes. Therefore, by definition \nof parallel dominance frontiers, Y E PDF(X). 0 Lemma 5.5 If X > ExitG= then PDF~Xj = pFlocalP). Proof \nLet RI be the set of basic block nodes that are reachable from Xtail, including Xtail. Let R2 be the \nset of basic block nodes that occur in a section S. that is preceded in execution order by S= (the section \ncorresponding to X) where SZ and S$ are sections in the same parallel block, i. e., S, wait depends on \nS.. Finally, let R3 be the set of basic blocks nodes in a parallel block, Pz, nested within PC. Clearly, \nPz can be a supernode in G. or not. In either case, the head node corresponding to Pa, Zhead, is the \nunique pre\u00ad decessor of P,. Therefore, by construction of factored graphs, any node that represents a \nbasic block within the parallel block corresponding to Pz @ PDF(X), i. e., VZ E R3, Z @ PDF(X) (1) Also, \nby construction, X ~ ExitG= +X > Z,VZ c (R1u R2). (2)  Claim 5.1 If X > ExitG= then VY such that Y c \nPDF(X), Y c G., i.e., Y IS local. Proof: Suppose Y is not local. Then, by construction, VP c Pred(Y), \neither P is local or P is not local. Since Y is not local, P is local iff P is ExitGZ and we know that \nX ~ Ex?tG=. If P is not local then we know that P c (Rl U R2 U R3). Consider the case when P E (Rl U \nR2). By equation 2, X ~ P. Clearly, X ~ P is a contradiction to Y c PDF(X). If P is not local and P E \nR3, then by construction, Y is also a node in R3. Therefore, by equation 1, we have a contradiction to \nY E PDF(X). Therefore, Y E G C. 0 Consider Y E PDF(X). Therefore, X ~ P for some P E Pred(Y) and X ~ \nY. From the above claim, we know that Y and its predecessors are local. From lemma 5.3 we know that Y \nand its predecessors are not supernodes. By definition of PPF~oca~X), Y E PPF1oca~X). Therefore if X \n} ExitG=, PDF(X) G PPFioca~X) By lemma 5.4,   l l F/oca/(x) G PDF(X) Thus, PPFloca(X) = PDF(X) if \nX ~ EZdG=. l  266 Lemma 5.6 IfX > Exit~= then PPF(X) ~ PDF(X). @ pDF(Xtazi). Therefore, either VP E \nPred(Z), Xtazl ~ P or Xiaz~ > Z, Proofi Proof proceeds by induction on the nest-level of Gc, the closest \nsupernode containing X. The outer\u00admost parallel block is at nest-level O. Let X occur at nest-level n. \nBasis: When the nest-level n = O, EdGm corre\u00adsponds to the exit node of G~ain by construction. Therefore, \nPPF(X) = PPFloca~X) which is a subset of PDF(X) by lemma 5.4. Inductive Hypothesis: Assume PPF(Y) ~ PDF(Y), \nVY at nest-level < n. Inductive Step: When nest-level = n. Define pDFunioca@) = pDF(~) pDFiOcaZ(~) . \nWe know that PDFioca~X) = PPFloca~X) and ppF(X) = PPF~oca~X) U PPF(Xta2i). Therefore, to prove that PPF(X) \n~ PDF(X), it is sufficient to show that l pF(Xtaz~) C l DFun~oca@). Since Xtall occurs at nest-level \n< n, by inductive hypothesis, PPF(Xtatl) G PDF(Xtazi). Therefore, to establish the inductive step, we \nshow that PDFunlocai(X) = PDF(Xtazl). The proof of this statement follows from the proof of the following \ntwo claims: Claim 5.2 pDF(X~azl) G DFunloca~x). Claim 5.3 PDFuniocap(X) ~ PDF(Xtazl). Proof of claim \n5.2 Since X > ExttG=, by con\u00adstruction, x 2 Xfaz/. Therefore, from lemma 5.2, PDF(Xtati) ~ PDF(X). By \ndefinition, VZ c PDF1oca~X), Z G Gz and by construction, VZ E PDF(Xtaz/), Z @ G. (since J%tTyG= and EZttG= \nare unique entry and exit nodes in Gc). Therefore, PDF(Xtazl) n PDFioca~X) = 0. Hence, PDF(Xiaz~) ~ DF~n/oca/(x). \nProof of claim 5.3 We know that VZ c PDFunloca~X), Z is not local. Therefore, consider\u00ading the sets, \nRI, R2 and R3 introduced in lemma 5.5, Z c RI U R2 (since Z @ R3 by equation 1). Since, X a EzitG=, by \nconstruction, X ~ W, VW c R2 and X 2 Xtail. Since Z c l DFun~oca~X), ~ P G pred(z) suchthatXz PandX~Z,i.e.,X~ \nZ(Z#X). Therefore, Z @ Rz and Z E RI {Xiazi}. Suppose, Z case (i): VP E Pred(Z), Xtai/ > P Since X > \nXtai/j we know by construction that X 2 Q # xtai/ > Q, where Q is not local. This is because, there are \nno jumps into or out of a parallel block and Entr~G= and Exit G= are unique entry and exit nodes. Therefore, \nusing this property in case (i) (since Z is not local), X * P; a contradiction to Z E PDFunlocal(X). \n case (ii): If Xtail >> Z, then by transitivity of the dominance relation, X ~ Z; a contradiction to \nZ ~ pDFun/oca(x). Contradictions to cases (i) and (ii) prove claim 5.3. From claims 5.2 and 5.3, PDFunloca~X) \n= pDF(Xtaii), completing the induction. 0 Theorem 5.1 PPF(X) ~ PDF(X) Proof: From lemma 5.4 we know that \nPPFloca~X) ~ PDF(X). Therefore, by definition of PPF(X), when X ~ ExitGz, PPF(X) ~ PDF(X). By lemma \n5.6, PPF(X) ~ PDF(X) when X > ExitG=. Therefore, by definition of PPF(X), PPF(X) ~ PDF(X). 0 Theorem \n5.2 PDF(X) ~ PPF (X) Proof: From lemma 5.5, PDF(X) = PPFioca{X) if X @ EzitG=. Therefore, it is enough \nto prove that PD~X) ~ PPF(X) when X ~ Ex2tG=. The proof proceeds by induction on the nest-level of the \nclosest supernode enclosing X. Basis: When nest-level, n = O, Ex2tG= and EntryG= nodes correspond to \nthe exit and entry nodes of G~ain. Therefore, PDF(X) = PPFloca~X). Inductive Hypothesis: Assume, VQ such \nthat Q is at nest-level < n, PDF(Q) ~ PPF(Q). Inductive Step: When nest-level = n (i. e., for X). We \nknow, by definition, PPF(X) = PPF1oca~X) U PPF(XtalJ, and PDF(X) = PDFloca[X) U l DFun~oca/(x). But! \npDF/ocafx) = ppl?~oca{x) and pDFun/ocafx) n ppF/ocaflx) = 0. Therefore, to 267 prove the inductive step, \nit suffices to show that for any two nodes X and Y that represent basic blocks in the PCFG is defined \nas : pDFun/ocaiX) ~ pPF(%~) By inductive hypothesis, PDF(Xtai~) C ppF(Xtai~) (3) From claims 5.2 and \n5.3 (lemma 5.6), we know that pDFuniOca@) = pDF(Xtazi). Substituting this in equation 3, we have the \nrequired proof. 0 6 Placement of @functions In the case of sequential programs, qLfunctions for a variable \nare located at all nodes that represent merge points in the program. These nodes are precisely the nodes \nin the set J+(S) (iterated join set), where S is the set of nodes containing assignments to the variable \n[4]. Cytron et al [4] have proved the equivalence be\u00ad tween ~+ (S) and DF+ (S) for the sequential case. \nWe define join sets for parallel programs in two ways; Ju is defined by taking the union of the join \nsets of the factors, while the parallel join set Jp is defined from basic principles. This section proves \nthese two defini\u00adtions are identical, establishes the equivalence between the iterated parallel join \nset (J; ) and the iterated par\u00adallel dominance frontier (PDF+); with Theorems 5.1 and 5.2, this is equivalent \nto PPF+. 6.1 Join sets for factored CFG S @functions for a variable should be placed at points in the \nparallel program where two or more definitions of the variable reach along control flow paths in the \nprogram that do not execute in parallel. Supernodes have to be considered in order to manage the paths \nthat can execute in parallel. One way of defining the join set is by considering all possible sequential \npaths of execution, z .e., by considering the sequential CFG s derived by factoring the PCFG. The join \nset J({P, Q}) for any two nodes P and Q in any factor of a PCFG is defined as in the sequential case \n(since the factor here is a sequential CFG). Two nodes X and Y which have assignments to the same variable \nin a PCFG can both appear in more than one factor; thus we are interested in the unzon of the join sets \nof the factors containing X and Y. The set J.({X, Y}) J.({x, y})= J({X, Y}) u jactoredCFG1s The union \nis considered over all the factored CFG S, ~, where X, Y E~.  6.2 Parallel join set for PCFG S It is \nalso possible to recursively define the parallel join set for nodes in a PCFG. For any two nodes, X and \nY, such that Gs = GY, the set Jloca~{X, Y}) is defined as the join set of nodes X and Y within G.. !ocai \ns defined just as in the sequential case, i.e., J({X, Y}). This is the base case for the recursive definition. \nIf G. is not the same as GY then the closest supern\u00adodes enclosing X and Y, Pc and Pv (if they exist) \nmay be the same or different. We will first consider the case where P.. and Pg are different. PY represents \na parallel block; any downwards ex\u00ad posed definition within this parallel block may reach points in the \nprogram outside the parallel block. If we assume PY to be at nest-level n and Gy to be nested within \nG=, then clearly X is at nest-level less than n, say m. In order to compute the join nodes, it is important \nto consider those points in the parallel pro\u00ad gram where the definitions at X and Y will converge, which \nis clearly at nodes at nest level m. Therefore, the join of X and Y is equal to the join of X and Pv \ni.e., JP({X, PV}). If the two sub-PCFG s are not nested, we must look at the supernodes, PZ and PY. If \nboth X and Y are at the same nest-level, the join set for X and Y is the join set of PX and Pg, JP({PZ, \nPv}). If the nest-level of X is less than that of Y, then the join set for X and Y is JP({X, PY}). The \ncase when P. and PY are the same is analyzed on the basis of whether SO and Sy (that represent sections \nin the parallel block) are related by the wait-closure relation (W+ ). Wait-closure is the transitive \nclosure of the wait-dependence relation between section nodes. If there is no wait-closure relation between \nS. and SY, the join set JP({X, Y}) is the empty set. This is the case when S z and Sy are defined along \npaths in the parallel program that execute concurrently. Consider the case when S= W+ Sy. Since the \nPar\u00adallel Precedence Graph is a DAG, i.e., has no cy\u00adcles, the join of X and Y is the same as the join \nof the EntryGY node and Y. Since EntryGY and Y are nodes in the same PCFG, (namely GY), JP({X, Y}) = \n3hCai{EntryG,, Y}). Now we prove the equivalence of the two definitions of the Join sets in our parallel \nprograms. Theorem 6.1 For every pair of nodes X,Y rep\u00adresenting basic blocks in a para[lel program, Z \nE Jp({X, Y}) zti Z E Ju({x> y}) Proo~ The proof of this theorem proceeds by con\u00adsidering the different \ncases in the definition of JP and showing the equivalence between the two sets for each case. Details \nof the proof can be found in [10], D We know for the sequential case (from [4]) that the iterated join \nset J+(S) is equal to the iterated domi\u00adnance frontier DF+ (S). Since this equivalence is un\u00adaffected \nby the union operator, the result obtained by iterating the union of the join sets for each factored \nCFG (JU) is equal to the iterated parallel dominance frontier set. The iterated parallel join set J:(S) \nfor a variable v, where S is the set of nodes that have assignments to v, is precisely the set of nodes \nwhere @functions for v need to be placed. However, computing JP+ can be very inefficient. We claim that \nthe iterated parallel domi\u00adnance frontier is equal to the iterated join set, JP+. The proof of this claim \nuses the definition of parallel domi\u00adnance frontier and the proof that shows the equivalence between \nJP and JU. Theorem 6.2 For a set S of assignment nodes, J:(S) = PPF+(S) Proofi From Theorem 6.1, we know \nthat Jp({~, y}) = J~({X, Y}). Therefore,  Ju({x, y}) VX,YESAX$Y ~ Y~ AX~y ({x })= U This equivalence \ncan be extended to the iterated joins i.e., J;(S) = J$ (S) (4) From [4], we know that J+(S) = DF+(S). \nThis equivalence again is unaffected by taking the union over all factored CFG s i.e., J/(S) = PDF+(S) \n(5) From equations 4 and 5, J;(S) = PDF+(S). The proof of the theorem follows from Theorems 5.1 and \n5.2. i.e., J;(S) = PPF+(S). 0 6.3 Algorithm to place #-functions  The algorithm to place @functions \nis a worklist al\u00ad gorithm, a trivial modification to the algorithm for placing +-functions in sequential \nprograms. The ~\u00ad functions are placed in the iterated parallel precedence frontier set. The algorithm \ntakes time proportional to the number of ordinary assignments and ~-functions plus the total number of \nrelevant parallel precedence frontiers to process a single variable. 7 Multiple Parallel Updates In a \nparallel sections construct, when only one sec\u00adtion updates a shared variable, the value of the variable \nafter the parallel block is well-defined. However, when two parallel sections may update the same variable, \nmore than one SSA name of the variable may reach assignments after the parallel block. Sim\u00adilarly, when \ntwo different array elements are updated in different parallel sections, the two %ames for the updated \narray must be merged to preserve the SSA properties. Since this merge does not arise from con\u00adtrol flow \nbranching, we use a different class of merge functions to distinguish them from @functions. Note that \na parallel merge does not mean that the program is incorrect, nondeterministic or contains an anomaly. \nInstead, a parallel merge means that a variable might be updated in more than one parallel section, and \nthe updated value must be used in code following the par\u00adallel block, regardless of which section did \nthe update. The updates may be controlled by mutually exclusive conditions, or may be to different elements \nof an array. We use the Parallel Precedence Graph to detect mul\u00adtiple updates. We introduce a new class \nof functions, called ~-functions, to merge multiple parallel updates. 269 Cobsgin x Cobegin A B c D coend \nWDF(X) .. D, coend D coend mend -- Figure 6: Wait dominator tree and wait-dominance frontier Placement \nof @functions must occur along with c3\u00adplacement, before the SSA renaming phase. Since the Parallel Precedence \nGraph (PPG) is a di\u00adrected graph with a single entry point, it is possible to find the dominance relation \nbetween the nodes in a PPG. We call this wait-dominance, and write X ~W Y if X and Y are nodes in a PPG \nand X dominates Y; the notion of strict wait-dominance (X >>W Y) and the wait-dominance frontier (WDF(X)) \nare defined analo\u00adgously. The wait domtnator tree and wait-dominance frontiers for the PPG in Figure \n3 are shown in Figure 6. It is tempting to place +-functions at the wait\u00addominance frontier in the PPG. \nUnfortunately, this is not correct; unlike qLplacement in flow graphs, the placement of @-functions should \nbe invariant in the presence of extra transitive edges in the PPG. Thus, @\u00adfunctions are placed as follows, \n[14]: let WDF+(S) be the iterated wait-dominance frontier of the set of nodes in a PPG that contain assignments \nto a variable. Com\u00adpute the pairwise intersection of the wait-dominance frontiers of every two distinct \nvertices X and Y such that neither X W+ Y nor Y W+ X (where W+ is the wait-closure relation defined in \nthe previous section) in S U WDF+ (S), and place @functions at these points; that is, place ~-functions \nat W(S), defined as: w(s) = WDF+(X) rl WDF+(Y) u (X,YC WDF (S])AQ where WDF*(S) = S U WDF+(S), and C \nis the condition, ((X W+ Y) V (Y W+ X)) (p) a1=2 (p) b1=3 (p) cl=4 (p) if (Q) then Parallel Sections \nSection A (s) if (P) then (t) b2=a1*5 else  (u) b3=a1+7 (u) f1=b3*a1 (v) endif (v) bq = @(bz, b~) \n (v) f2 = @(.fo, fl)  Section B (w) C2=Cl+i5 (w) j3=c2 *16 Section C, Wait(A) (x) d1=b4 *al  Section \nD, Wait(A, B) (y) f4 = ?J(f2, f3) (y) c3=a1*b4+c2*f4 (p) End Parallel Sections (p) f5 = @(f2, f4) \n (n) dz=dl+fq else  (q) ds =23 (r) endi.f (r) b5 = q$(b4, bl) (r) C4 = 4(C3, cl) (r) dl = #(da, \nd~) (r) f6 = ~(f5, jO) (r) e1=a1+b5*c4*d4  Figure 7: SSA form of Parallel Program [This is a correction \nof the definition in [14].] For example, in Figure 2, the variable f is defined in sections A and B, \nwhich can execute concurrently. Therefore, the definitions of f in these two sections are potentially \nanomalous. A ~-function for f is intro\u00ad duced in section D and at the coend using the above definition \nof W. The computation of W(S) is again potentially expensive. Therefore, the algorithm we use actually \ndoes place ~-functions at the wait-dominance frontier; spurious @-functions are removed during the renaming \nphase. For each variable, the compiler finds the set of nodes in each PPG where the variable is assigned. \nA sec\u00ad tion node in the PPG has an assignment to the vari\u00ad able if the section assigns the variable. \nThe algorithm to place #-functions given in [4] is used to place @ functions. This algorithm could introduce \nmany spuri\u00adous @-functions. These are easily identified since they will have only a single reaching modified \nSSA name at the parallel merge. S Renaming of variables The renaming algorithm in Figure 5 in [4] performs \na depth-first traversal of the dominator tree of the CFG. We modify this algorithm for parallel constructs \nas fol\u00adlows: 1 Begin the traversal with the Entry node for G~a~~. 2, When visiting a basic block node \nor Entry node, the algorithm works the same as in [4]. 3 When visiting a supernode, the procedure should \nrecurse to perform a depth-first traversal of the wait-dominator tree of the corresponding PPG. The order \nof traversal of the wait-dominator tree is important; the depth-first traversal of the section nodes \nmust preserve topological order that is, it must visit every predecessor of a section node before visiting \nthe section node itself. Since the PPG is acyclic, this order can be found trivially. 4. When visiting \na section node, spurious @functions can be removed. First, for each @function at this section node, remove \nany +-arguments whose def\u00adinitions are outside the parallel block. If the re\u00admaining @arguments all have \nthe same SSA name, then the @function can be removed, and this SSA name should be used in this section. \nIf there is more than one distinct SSA name among the remaining ~-arguments, the @-function is neces\u00adsary. \nThe procedure should then recurse to visit the dominator tree of the corresponding PCFG. Renaming of \n@arguments is done to the wait\u00addependence successors the same as renaming of arguments of @-functions. \n 5. When visiting an Eztt node for a section, the SSA name for every variable modified in that section \nmust be propagated back to the section node, as though there were an assignment in that section node. \n 6. Similarly, when visiting a coend node, each SSA name modified in the parallel block must be prop\u00adagated \nto the corresponding supernode. The SSA form of the parallel program in Figure 2 is shown in Figure 7. \nRecall the copy-in/copy-out se\u00admantics of the parallel language. 9 Conclusion Static Single Assignment \nform is a powerful intermedi\u00adate representation for optimizing sequential programs. We have extended \nthe algorithms to translate a sequen\u00adtial program to its SSA representation to handle par\u00adallel constructs \nwith synchronization. The resulting SSA representation of parallel programs can be used to optimize explicitly \nparallel programs. We believe that the ability to perform classical code optimizations on parallel programs \nis critical to the performance of such programs on existing and forthcoming high perfor\u00admance parallel \narchitectures. Previous work on apply\u00ading scalar optimizations to parallel programs focused on stricter \nlanguage semantics and the problems this caused for the compiler [5, 6]. Our language model is more appropriate \nfor user-level code, and allows more aggressive optimizations. References [1] Frances Allen, Michael \nBurke, Philippe Charles, Ron Cytron, and Jeanne Ferrante. An overview of the PTRAN analysis system for \nmultiprocessing. J. Parallel and Distributed Computing, 5(5):617\u00ad640, October 1988. [2] B. Alpern, M.N. \nWegman, and F.K. Zadeck. De\u00adtecting equality of variables in programs. In Con~ Record 15th Annual ACM \nSymp. Principles of Programming Languages [8], pages 1-11. [3] Per Brinch Hansen. Operating Systems Principles. \nAutomatic Computation. Prentice-Hall, 1973. [4] Ron Cytron, Jeanne Ferrante, Barry K. Rosen, Mark N. \nWegman, and F. Kenneth Zadeck. Efficiently computing static single assignment form and the control dependence \ngraph. ACM Trans. on Programming Languages and Systems, 13(4):451-490, October 1991. [5] %rnuel P. Midkiff \nand David A. Padua. Issues in the optimization of parallel programs. In Proc. 1990 International Conf. \non Parallel Processingj volume II, pages 105 1 13, St, Charles, IL, August 1990. Penn State Press. [6] \nSamuel P. Midkiff, David A. Padua, and Ron Cytron. Compiling programs with user paral\u00adlelism. In David \nGelernter, Alexandru Nicolau, and David A. Padua, editors, Languages and Com\u00adpilers for Parallel Computing, \nResearch Mono\u00adgraphs in Parallel and Distributed Computing, pages 402-422. MIT Press, Boston, 1990. [7] \nParallel Computing Forum. PCF Parallel For\u00adtran extensions. Fortran Forum, 10(3), September 1991. (special \nissue). [8] Conf. Record 15th Annual A CM Symp. Principles of Programming Languages, San Diego, CA, Jan\u00aduary \n1988. [9] B.K. Rosen, M.N. Wegman, and F.K. Zadeck. Global value numbers and redundant computa\u00adtions. \nIn Conf. Record 15th Annual ACM Symp. Principles of Programmmg Languages [8], pages 12-27. [10] Harini \nSrinivasan. Analyzing programs with ex\u00adplicit parallelism. M.S. thesis 91-TH-006, Ore\u00adgon Graduate Institute, \nDept. of Computer Sci\u00adence and Engineering, July 1991. [11] Harini Srinivasan and Michael Wolfe. Analyz\u00ading \nprograms with explicit parallelism. In Ut\u00adpal Banerjee, David Gelernter, Alexandru Nico\u00adlau, and David \nA. Padua, editors, Languages and Compders for Parallel Computmg, number 589 in Lecture Notes in Computer \nScience, pages 403\u00ad 419. Springer-Verlag, 1992. [12] Mark N. Wegman and F. Kenneth Zadeck. Constant \npropagation with conditional branches, ACM Trans. on Programming Languages and Sys\u00adtems, 13(2):181-210, \nApril 1991. [13] Michael Wolfe. Beyond induction variables. In Proc. ACM SIGPLAN 92 Conference on Pro\u00adgramming \nLanguage Design and Implementation, pages 162 174, San Francisco, June 1992. [14] Michael Wolfe and Harini \nSrinivasan. Data struc\u00adtures for optimizing programs with explicit par\u00adallelism. In Proc. First International \nConference of the Austrian Center for Parallel Computation, Salzburg, September 1991. \n\t\t\t", "proc_id": "158511", "abstract": "<p>We describe and prove algorithms to convert programs which use the Parallel Computing Forum Parallel Sections construct into Static Single Assignment (SSA) form. This proces allows compilers to apply classical scalar optimization algorithms to explicitly parallel programs. To do so, we must define what the concept of <italic>dominator</italic> and <italic>dominance frontier</italic> mean in parallel programs. We also describe how we extend SSA form to handle parallel updates and still preserve the SSA properties.</p>", "authors": [{"name": "Harini Srinivasan", "author_profile_id": "81100406633", "affiliation": "", "person_id": "PP31095510", "email_address": "", "orcid_id": ""}, {"name": "James Hook", "author_profile_id": "81341491447", "affiliation": "", "person_id": "PP14134226", "email_address": "", "orcid_id": ""}, {"name": "Michael Wolfe", "author_profile_id": "81100031703", "affiliation": "", "person_id": "PP39081236", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/158511.158644", "year": "1993", "article_id": "158644", "conference": "POPL", "title": "Static single assignment for explicitly parallel programs", "url": "http://dl.acm.org/citation.cfm?id=158644"}