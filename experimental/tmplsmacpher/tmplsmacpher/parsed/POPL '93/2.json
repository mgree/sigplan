{"article_publication_date": "03-01-1993", "fulltext": "\n Automatic Array Alignment in Data-l?arallel Programs Siddhartha Chatterjee * John R. Gilbert t Robert \nSchreiber* Shang-Hua Teng $ Abstract Data-parallel languages like Fortran 90 express parallelism in \nthe form of operations on data aggregates such es arrays. Mierdignment of the operands of an array operation \ncan re\u00adduce program performance on a distributed-memory parallel machine by requiring nonlocal data accesses. \nDetermining array alignments that reduce communication is therefore a key issue in compiling such languages. \nWe present a framework for the automatic determina\u00adtion of array alignments in data-parallel languages \nsuch as Fortran 90. Our language model handles array sectioning, reductions, spreads, transpositions, \nand masked operations. We decompose alignment functions into three constituents: axis, stride, and offset. \nFor each of these subproblems, we show how to solve the alignment problem for a basic block of code, \npossibly cent aining common subexpressions. Align\u00adments are generated for all array objects in the code, \nboth named program variables and intermediate results. The alignments obtained by our algorithms are \nmore general than those provided by the owner-computes rule. Finally, we present some ideaa for dealing \nwith control flow, replica\u00adtion, and dynamic alignments that depend on loop induction variables. 1 Introduction \nConsider the Fortran 90 [2] code fragment shown in Figure 1. Parallelism is expressed in this code in \nthe form of opera\u00adtions on whole arrays and array sections. Compiling such a program for a distributed-memory \nparallel machine requires Research institute for Advanced Computer Science, Mail Stop T045-1, NASA Ames \nResearch Center, Moffett Field, CA 94033-1000 (sc&#38;iacs.ed., schreibr&#38;iac..edu). The work of these \nauthors wss supported by the NAS Systems Division via Cooperative Agreement NCC 2-387 between NASA and \ntbe Universities Space Research Or\u00adganization (USRA). txerox pa]o Alto Research Center, 3333 Coyote Hill \nFhad, palo Alto, CA 943041314 (gilbert@parc.xerox.tom). Copyright @ 1992 by Xerox Cm.poration. All rights \nreserwe&#38; *Department of Mathematics, Massachusetts Institute of Technol\u00ad ogy, Cambridge, MA 02139 \n(sten@?theory.Ics .mit .edu). This work was performed when author was a postdoctoral scientist at Xerox \nPARC. Permission to copy without fee all or part of this material is granted provided that the copies \nare not made or distributed for direct commercial advantage, the ACM copyright notice and the title of \nthe publication and its date appear, and notice is given that copying is by permission of the Association \nfor Computing Machinery. To copy otherwise, or to republish, raquires a fee and/or spacific permission. \nACM-20th PoPL-1 /93-S.C.r USA 01993 ACM 0-89791 -561 -5/93 /0001 /0016 . ..$1.50 1 real a(50,100), b(100), \nc(1OO,2OO) 2 real d(100,50), e(50), f (50) , g(50) 3 4 d = c(:, I:200:4) 5 e = suis(a + transpose(d), \ndti=2) 6 f = srm(spread(b, dim=2, ncopies=50) + d; dirs=l) \u00ad7 g = b(l:100:2) + e + f Figure 1: A ??ortran \n90 program fragment involving arrays, regular sections of arrays, and array intrin\u00ad sic. ahgning the \nvarioua array objects (named variables as well as compiler-generated temporaries for intermediate results) \nwith each other, partitioning the data, and distributing the work among the processors. The goal of compilation \nis to produce an alignment and distribution that reduces com\u00adpletion time. One such alignment is shown \nin Figure 2, where we have ah made the array temporaries explicit. In thw paper, we present algorithms \nto automatically trans\u00adform unannotated code to annotated code. Completion time haa two components: computation \nand communication. Communication can again be separated into intrinsic and residual communication. Intrinsic \ncom\u00admunication arises from computational operations such as reductions and vector-valued subscripts that \nrequire data motion as an integral part of the operation. Residual com\u00admunication arieeefrom nonlocal \ndata references required ina computation whose operands are not aligned with reapect to each other. Not \nall communication patterns are equally ex\u00adpensive in distributed-memory machines. Typically, thereie \nathree-level hierarchy of patterns: nearest-neighbor (shifts), tree (reductions, parallel prefix, spreads), \nand unstructured (transpose, change ofstride, vector-valued subscripts). Tree patterns are used for intrinsic \ncommunication and are not considered in this paper. Thegord of alignment ia to provide acorreapondence \nbetween array objects that minimizes the residual communication in the program. We view the compilation \nprocess as two phases: align. ment and distribution. In the alignment phase, all array objects in the \nprogram are positioned with respect to each other so as to reduce residual communication. We achieve \nthis by aligning all array objects within an auxiliary Carte\u00adsian grid called a template. In the distribution \nphase that follows, the template is distributed onto the parallel ma\u00ad 16 1 real a(50,100), b(100), c(1OO,2OO) \n2 real d(100,50), e(50), f(50), g(50) 3 real tl(50,100), t2(50,100), t3(100,50), t4(100,50), t5(50), \nt6(50) 4 5 template T(IOOOO,IOOOO) 6 align a(i,j), ti(i,j), t2(i,j) with T(4*i-3,4*j-3) 7 align b(i) \nwith T(*,4*j-3) 8 align c(i,j) with T(4*j-3,i) 9 align d(i,j), t3(i,j), t4(i,j) uith T(4*j-3,4*i-3) 10 \nalign e(i), f(i), t5(i), t6(0, g(i) with T(4*i-3,*) 11 12 d = c(:,I:200:4) 13 tl = transpose(d) 14 t2=a+tl \n15 e = s~(t2, d~.2) 16 t3 = spread(b, dim=2, ncopies=50) 17 t4rnt3+d 18 f = s~(t4, dfi=l) 19 t5 =b(l:100:2) \n20 t6=tS+e 21 g=t6+f Figure2: The program ofFigurel with temporaries made explicit and alignment annotations \nadded. chine; the distributions of the arrays are determined by their alignment with respect to the template \nand the distribu\u00adtion of the template. This two-phase approach separates the language issues form the \nmachhe issues, and has been adopted by Fortran D [9], High Performance Fortran [15], CM-Fortran [24], \nand Vienna Fortran [4]. Ourworkisdis\u00adtinguished by performing these phaaesautomaticrdly rather than requiring \nthem to be done by the user. In this paper, we deal only with the alignment phaee, Alignment anddiztribution \nare theprimary determinants of the performance of data-parallel languages on massively parallel machines. \nThis is particularly critical for distributed\u00admemory or NUMA machines, where the cost of accessing nonlocsl \nmemory is high. This is a difficult problem: in cur\u00adrent compilers, data alignments and diet ributions \nare either supplied by the user, or canonical defaults are used; task mappings are derived from data \nmappings using the owner\u00adcomputes rule [16]. Requiring the programmer to specify alignment and d~tribution \nis onerous to the programmer and is insufficient for several reasons: The best mapping for a problem \nchanges with the in\u00adterconnection structure of the machine, thesize of the machine, and thesize of the \nproblem [25]. This means that the user must either write new mappings for each case, or be satisfied \nwith poor performance.  Even if the programmer maps named variables, the compiler must still map temporaries. \nMapping accord\u00ading to the owner-computes rule can cause unnecessary communication [18].  The best data \nand task mappings may be different in different portions of a program, Having to deal with  this explicitly \nwould complicate theprograrnmer s job still further. The owner-computes rule provides no guidance about \nplacement for array transformations such as reductions and spreads, We believe that the compiler should \ndeal with data align\u00ad ment and distribution. Some early partial results in this direction are the following. \n Knobe, Lucas, and Steele [19] developed a heuristic for data distribution based on alignment preferences \nfor arrays. Their work addresses automatic distribu\u00adtion, butisrestricted bytheowner-computes rule. We \nremove this restriction.  Wholey [25] demonstrated that optimal mappings are often non-obvious even \nfor a simple array-based pro\u00adgramming model. He further showed that a compiler can derive good mappings \nwithout programmer inter\u00advention. His algorithm used a combination of clus\u00adtering and hill-climbing techniques \nto achieve these results. He dealt only with axis alignment. We use different techniques to handle axis, \nstride, and offset alignment.  Li and Chen [21] presented a heuristic solution for axis alignment by \nreducing it to weighted bipartite graph mat thing.  Gupta [13] developed techniques along the lines \nof Li and Chen for axis and stride alignment of Fortran 77 programs. He uses a more detailed cost model \nfor edge weights but is restricted by the owner-computes rule. He also does not deal with offset rdignment. \n Gilbert and Schreiber [11] developed a fast optimal algorithm for aligning temporaries in expression \ntrees (single program statements) involving array sections, by characterizing interconnection networka \nas metric  spaces. Their algorithm applies to a class of so-called robust metrics, which includes multidimensional \ngrids, hypercubes, and the d~crete metric. They showed that the presence of common subexpressions renders \nthe problem NP-complete, making heuristic solutions necessary for basic blocks. Chatterjee et al. [6] \nex\u00adtended Gilbert and Schreiber s work in two different di\u00adrections. First, they reformulated the alignment \nprob\u00adlem in terms of dynamic programming and developed a family of compact dynamic programming (CDP) \nalgo\u00adrithms to handle both robust and non-robust metrics such as rings and fat-trees [20]. Second, they \nrelaxed the restriction that all arrays be conformable, mod\u00adeled this by allowing different weights on \nthe edges of the expression tree, and extended the CDP algorithms to edge-weighted expression trees. \nBoth those papers concentrated on offset alignment. Our paper extends this previous work in the following \nways: it relates changes in array size to the semantics of t ransformw tional array operators, it provides \nsolutions for axis and stride alignments, it generalizes the solution to basic blocks represented as \nedge-weighted DAGs, and it handles alignments for both variables and tempo\u00ad raries. This paper is the \nfirst systematic and unified treatment of the alignment problem. We develop a detailed and real\u00adistic \nmodel of communication cost, and formulate the align\u00adment problem as a constrained optimization of the \nresid\u00adual communication cost function. We identify three compo\u00adnents of alignment, present efficient \nalgorithms for finding optimal and near-optimal alignments of both temporaries and program variables, \nand show how to deal with reduc\u00adtion, spread, transposition, array sections, and masked op\u00aderations. \nFinally, we present some ideaa on how to deal with control flow, replication, and dynamic alignments \nthat depend on loop induction variables. The paper is organized as follows. Section 2 introduces the \nlanguage model. Section 3 defines the alignment prob\u00adlem, and reviews our previous work [6]. Section \n4 treats the axis and stride alignment problem, and Section 5 treats the offset alignment problem for \ngrids. Section 6 extends the al\u00adgorithms of the previous sections to handle situations where named variables \nmust be aligned in addition to temporaries. Section 7 presents extensions to thk work to handle controI \nflow, dynamic alignments, and replication. Finally, Section 8 presents conclusions. 2 The language model \nWe consider an imperative language with array-based data parallelism, incorporating the array features \nof Fortran 90 and APL [17]. There are three types of entities in the lan\u00adguage: array objects, section \nspecifiers, and permutations. Array objects are multidimensional rectangular collections of data. A section \nspecifier is a list of section expressions. Section expressions have three forms. The form t: h:s rep \nresents the sequence of indices from 1 to h in steps ofs. The form 1 is equivalent to t: t: 1, and the \nform : is equivalent to lower-bound: upper-bound: 1. A section specifier may be applied to an array to \ncreate a new array (this is called array sectioning). Assignment to a section of an array conceptu\u00adally \nproduces a new array that is identical to the original array everywhere except on the section. Let Abeanelx... \nx er array (i. e., an r-dimensional array with extents el through e~), let i and n be integers, let op \nbe a binary associative operator, and let P = ~1,..., p.] be a permutation vector. The following three \nunary operators can be applied to arrays to produce new arrays. 1. REDUCE(op, A, DIH = i) Produces an \nelx.. .xe,-lx ei+lX... x er array. 2. SPREAD (A, DIM = i, NCOPIES = n) Produces an el x . .. Xei_l X7ZXe1X. \n..Xer array. 3. [P] A Produces an en, x . . . x e.. array. This is a generalization of the Fortran 90 \nTRANSPCISE intrinsic function and can be done with the RESHAPE intrinsic of Fortran 90.  Binary elementwise \noperations such as array addition are permitted. In fact, any n-ary operation whose out\u00adput size is determinable \nfrom its input sizes is permissible. We assume the usual control flow mechanisms: conditionals (IF-ELSE-ENDIF) \nand iteration (DD loops). The UHERE construct of Fortran 90 can be readily incor\u00adporated in our model. \nThe language standard [2] specifies a set of rules to convert a WERE-ELSEUHERE-ENDUHERE block into a \nsequence of WHERE statements, and also specifies how the mask argument is propagated into the right-hand \nside expression oft he UHERE statement. For each elemental oper\u00adation, we define a corresponding masked \nversion that takes an additional mask argument. The algorithms presented in the remainder of the paper \nassume unary or binary oper\u00adations, but can be extended in the obvious way to handle ternary masked operations. \nWe use a directed acyclic graph (DAG) representation of the basic block of code we are aligning. The \nstandard al\u00adgorithm [1] is used to create the DAG representation of the basic block, special attention \nbeing paid to UHERE statements and assignment to array sections. A node in the graph rep resents an array \noperation, and the input and output ports of a node represent its inputs and outputs. Edges connect ports, \nrepresenting data flow. (We do not use control flow edges for alignment. ) Our algorithms provide an \nalignment at each port; any edge whose endpoints have different align\u00adments causes residual communication. \n3 Alignment Alignment is a relation (written H) that specifies a 1-1 cor\u00adrespondence between elements \nof a pair of array objects. Rather than specifying this correspondence directly, we in\u00adtroduce an auxiliary \nframe of reference called a template and specify all correspondences with respect to it. A template is \na Cartesian grid of sufficiently high dimension and (con\u00adceptually) infinite extent into which all array \nobjects can be mapped. Let A be an m-dimensional array and T be the n-dimensional template. The general \nform of the alignment relation is A(il,... ,i~)EEI T(~l(il, . . ..im). ~n(il, (ii, . . ..i~)). We restrict \nour attention to alignments in which every func\u00adtion fk k an affine function of a single distinct index \nvari\u00adable, fk = skia~ + ok. The paRtItIeterS ak, Sk, and ok correspond to the three components of the \nalignment func\u00adtion: axis, stride, and offset. This restriction does not per\u00admit replication or skew \nalignments (alignments of the form A(I) E T(I, 1)). Misalignment in the axis or stride com\u00adponents causes \nunstructured communication, and misalign\u00adment in the offset component causes nearest-neighbor com\u00admunication. \nWe now give examples of the various kinds of alignment. Example 1 (Axis alignment) Consider the statement \nB = B + [2,1]C, where B and C are two-dimensional arrays. Let T be a two\u00addimensional template. If l?(l, \nJ) E T(1, .7) and C(1, .7) H T(I, .7), execution of the statement will require general com\u00admunication \nto transpose C. If, however, 13(1, J)@T(l, J) and C(1, J) EBT(.7, 1), the operands of the array addition \nand as\u00adsignment are aligned, and no communication is necessary. Example 2 (Stride alignment) Consider \nthe statement A(l:N) = A(l:N) + B(2:2*N:2), where A and B are one-dimensional arrays. Let T be a one\u00addimensional \ntemplate. If A(I) E! T(I) and l?(l) E T(I), the operands of the addition will be misaligned, and general \ncommunication will result. The alignments A(l) E T(21) and B(1) R T(I) avoid residual communication. \nExample 3 (Offset alignment) Consider the statement A(l:N-1) =A(l:N-1) +B(2:N). Let T be a one-dimensiomd \ntemplate. If we have A(I) E n+ l (l) and I?(1) ET(1), nearest-neighbor communication will result. However, \nthe statement can be executed without communication if A(I) H T(I) and B(l) H T (1 1). Example 4 Consider \nthe statement td 50% A(O:2*N:2,1) =A(O:2*N:2,1) +B(I,3:N+3). Let T be a two-dimensional template. Residual \ncommunica\u00adtion can be avoided if A(l, J) E T(I + 6, .7) and II(I, J) R T (2J, 1). While we were able \nto completely eliminate residual com\u00admunication in the examples above, this is usually not possi\u00adble. \nThe only case guaranteed to have a zero-communication solution is where the graph of the code block is \na tree, and source positions are free. In general, our aim is to find an alignment that minimizes residual \ncommunication. There areseveral variauts of the&#38;gnment problem. Ta\u00adble llists some of these variants. \nThe code fragment we are aligning maybe a tree (single statement) ora DAG (basic block). Avariety ofdwtance \nfunctions may beused tomodel communication costs. Alignment for the source nodes may besuppliedaa input, \norwemay beasked to find ~gnments for some or all of them as well ss for internal nodes. The free-source \nvariant occurs when reconsider the sJignment of a basic block in isolation. The jized-source variant \nmod\u00adels the dependence between basic blocks, where inputs to a block can be constrained by the actions \nof preceding code blocks. 3.1 Distance functions and residual communication Let G = (N, E, W) be a weighted \nDAG representing a ba\u00adsic block of code, Here, N is the set of nodes representing computations, E is \nthe set of data flow edges, and W(X, y) is a nonnegative weight on edge (z, y) reflecting the number \nof data items being communicated along that edge. Since fanout corresponds to multiple uses of the same \narray ob\u00adject, the weight of an edge in fact depends only on its source node. We will therefore abbreviate \nW(z, y) to w=. The DAG for the code in Figure 1 in shown in Figure 3. (Con\u00adtrol flow edges may also be \npresent, but these will not enter into the discussion of alignment. ) G may be connected or consist of \nseveral connected components. Let P be a set of abstract positions (called the position spoce) encoding \npossible alignments. For instance, in Example 1, the po\u00adsition space consists of two positions, corresponding \nto the two alignments A(1, J) E T(I, J) and A(I, J) E T(.T,1) of an arbitrary array A. (Put another way, \nthe positions cor\u00adrespond to the row-wise and column-wisen orientations of two-dimensional arrays.) Let \nd be a dist ante function on P. The distance d(p, q) between two positions p and q in P is a nonnegative \nnumber corresponding to the cost per unit data of changing position from p to g. Given a DAG edge with \nweight w and endpoints at positions p and g, the contribution of the edge to the residual communication \nis therefore w . d(p, g). The distance function must be a metric, i.e., satisfy the following three conditions \nfor all positions p, g, and r in P: (Symmetry) d(p, g) = d(g, p). o (Nonnegativity) d(p, g) > O; d(p, \ng) = O if and only ifp=q. 50 t5 \\/ JN/ 51XW d 5(ZM a b I I I I Figure 3: Weighted computation DAG for \nprogram in Figure 1. o (Triangle inequality) d(p, q) + rf(q, r) ~ d(p, r). Some examples of distance \nfunctions are given in Table 2. For multidimensional grids, hypercubes, and rings, the dis\u00ad tance between \ntwo positions can be written as the sum of the dist antes in the multiple dimensions. This allows the \nalignment problem for such metrics to be separated into in\u00ad dependent subproblems for each dimension \n[6]. Let m : N ~ P be an alignment function specifying the locations of computation nodes in the position \nspace. Then C(G, d,r) = -WZ . d(~(z), ?@)) (1) L (.r,y)cE is the residual communication required for \nthe given code block under the alignment x. The alignment problem is to determine a x that minimizes \nthis residual communication. Note that the problem could be phrased equally well on the undirected graph, \nsince the distance function d is symmetric. Although our algorithms will proceed in the direction of \ndata flow, that is not inherent to the optimization problem. Strictly speaking, this formulation of residual \ncommu\u00adnication cost is correct only for trees. Consider the node shown in Figure 4. Let w be the weight \nof the edges leav\u00ading the node. Given positions for all the nodes as shown, Table 1: Taxonomy of alignment \nproblems. G-S = Gilbert-Schreiber algorithm for robust metrics, CDP = compact dynamic programming, CSP \n= cyclic source placement, LP = linear programming. Name Comments 4P> 9) G d 1P-91 Also for hypercube. \nUsed for shift alignment. I&#38;g min(lp ql, P -1P-ql) Not robust. Used for shift alignment. o p=q Models \ngeneral communication. Used for axis and Discrete 1 p#q stride alignment. { Fat-tree 2. HEIGHT(LCA(P, \nq)) Not robust. Table 2: Examples of distance functions. P is the size of the ring. w expression as \nfollows: (2) 123 Finding Steiner trees is NP-complete for most metrics [10] (the discrete metric is \nan important exception). In practice, the Steiner effectn is usually minor, heuristics such as the use \nof a minimum spanning tree perform quite well, and 0 bounds on the performance of the heuristics are \navailabk for many metrics of interest [26]. 3.2 Edge weights Edge weights model different amounts of \ncommunication Figure 4: The effect of the Steiner tree. The num\u00ad along different edges. Operations such \nas sectioning, re\u00adbers within the nodes are their positions. Assume duction, and spread change the sizes \nof array objects and the grid metric. The residual communication cost motivate the use of weights. Consider \nthe rank-one update of the graph fragment on the left is 6. However, if A = A + azyT, where A is a square \nmatrix, x and y are col\u00adthe Steiner tree on the right is used for the multi\u00ad umn vectors, and a is a \nscalar. The multiplication of x by cast, the residual communication cost is 3. For a yT takes two m-element \nvectors as input, and produces an one-dimensional grid, a Steiner tree is a minimum m2-element matrix \nas output. Thus the edges from z and y~ spanning tree of the positions, but that is not so in to the \nouter product node would reflect the cost of moving more than one dimension. m elements each, and the \nedge from the multiplication to the addition would reflect the cost of moving m2 elements. An alignment \nmight then move the 2m elements of z and y~ the above formulation would estimate the residual commu\u00adto \nalign with the columns and rows of A, spread them, and nication cost as the sum of the indhidual distances, \n:. e., perform both the multiplication and the addition in place,w(d(O, 1) + d(O, 2) + d(O, 3)). A better \nsolution is to find in preference to multiplying x by y~ and then aligning thethe minimum spanning tree \nof the positions O, 1, 2, 3 in mz elements of the product with A. the underlying metric space and use \nthk tree to distribute We assume that the sizes of all named variables are knownthe values. The optimal \nsolution uses a Steiner tree [23], and that the size of the output of an operation depends onlywhich \nis a tree of lowest possible total edge length con\u00ad on the sizes of the inputs, not their values. With \nthis re\u00adnecting the fanout vertex to its successors. Let S(z, d) is st riction, we can easily propagate \nsizes of array objects usingthe cost of the Steiner tree on the positions of node z and Chatterjee s \nsize inference algorithm [5] to obtain the sizes its successors in the DAG. If z has one successor y, \nthen of all array objects in the program. We can then Iabel eachS(Z, d) = d(m(z), m(y)). Then we rewrite \nthe residual cost edge with the size of the array object it represents. These edge weights are shown \nin Figure 3 for the program of Fig\u00adure 1, where the unit of edge weight is 50 array elements. 20  3.3 \nCompact dynamic programming for trees We now review the dynamic programming formulation of the alignment \nproblem for weighted trees with fixed source node (i. e., leaf) alignments [6]. Consider an expression \ntree T and a position space P. For each internal node z, define the .wbtree cost function C= such that \nC=(p) is the minimum cost of evaluating the subtree T= rooted at node z, subject to the constraint that \nthe root z of the subtree be placed at position p. Thus, C.(p) = m${C(T=, d, z) : T(Z) = p}. We can \nexpress the subtree cost function at a node x in terms of the subtree cost functions at its children \ny and z. It is just a minimum over all possible placements of y and z. C=(p) = ~ (CV(r)+wy.fi(p, r)) \n+m~ (CZ(r)+wz.d(p, r)). The key observation is that C.(p) is the sum of two inde\u00adpendent terms, one for \neach subtree below z. To emphasize th~, we rewrite the recurrence in terms of the contribution function \ngv, where g~(p) is the cost of computing the value of the subtree T% and delivering it to position p. \nFunction g~ is just the (rein,+) convolution of Cv with the distance function. This gives us the following \nmaster recurrence. c.(p) = 9U(P) + 9~ (P) (3) 9Y(P) = yj;(cy(r) + ~IJ o4pj r)) (4) 92(P) = ~#CZ(r) + \nwz d(p, r)). Finally, we define a position for y that realizes the mini\u00admum contribution gv(p): r;(p) \n= some r that minimizes CV(r) + WY . d(p, r). (s) These equations lead to the following dynamic program\u00adming \nalgorithm for solving the alignme~t problem .-- Algorithm 1 (Optimal alignment of an expres\u00ad sion tree \nusing dynamic programming.) Input: An edge-weighted rooted expression tree T, alignments for all source \nnodes of the tree, and a position space P. Output: An alignment function m that minimizes resid\u00adual communication. \nMethod: 1 for z 6 NODES T) in postorder do 2 for all p 6 L do 3 evaluate C=(p) and r;(p) using equations \nendd~) and (5), where z = PARENT(V) 4 5 enddo 6 m(RooT(T)) ~ some p at which Coot is minimized 7 for \nv c NODES(T) in preorder do 8 en~$~ i\u00ad rj(7r(PARENT(y))) 9 10 return ~ Metric Unwelghted tree Weighted \ntree Dwcrete O(Nh) O(Nh) Grid O(kN) O(kN log N) Ring O(kNh) 0(kN3) Fat-tree O(N h) 0(N3) Table 3: Summary \nof asymptotic running times of CDP algorithms for various distance functions. N is the number of nodes \nin the expression tree, and h is its height. For the grid and ring metrics, k is the dimensionality of \nthe grid or ring. Algorithm 1 exploits the structure of the tree but not of the dktance function. It \nworks for arbitrary distance func\u00adtions and haa time complexity 0(NP2 ) and space complex\u00adity O(NP), \nwhere N = INODES(T)I and P = IPI. Mace [22] originally used this algorithm to choose the best memory \nlayout for arrays, from a small set of possibilities. In our application we typically have N < P; hence \nAlgorithm 1 is not practical. However, we can exploit the structure of some specific dist ante functions \nto represent the cost func\u00adtions compactly, and substantially reduce the cost of dy\u00adnamic programming. \nWe call this family of algorit hms com\u00adpact dynamic programming (CDP). These various CDP al\u00adgorithms \nare described in detail by Chatterjee et al. [6], and the asymptotic running times for the various metrics \nare presented in Table 3. 4 Axis and stride alignment with fixed source positions We first solve the \nfollowing problem: Given a weighted di\u00ad rected acyclic graph G = (N, E, W) representing a basic block, \nand the sizes and positions of the source nodee of G (the named variables), find an axis and stride alignment \nm for all nodes in N that minimizes the residual communica\u00adtion cost of the DAG. Changing axis or stride \nrequires general communication using the interconnection network. It is generally difficult to predict \nthe cost of such communication, since it depends on factors such as contention and average transit distance. \nHowever, a reasonable model of this kind of communication is to consider it as having some fixed, high \ncost. Therefore, we use the dwcrete metric to model general communication throughout this section. Having \ndecided on the discrete metric as the distance function, we must now specify the position space. Let \ntbe the highest rank of any array object. This is also the rank of the template. We specify the position \nof an r-dimensional array object A by an ordered pair T(A) = A(A) IS(A), with A(A) being the axis specifier \nand S(A) the stride specifier. Each specifier is au r-tuple with au entry for each dimen\u00adsion of the \narray object. For instance, suppose that t= 3, and the template axes are labeled i, j, and k. Then a \n2\u00addlmensionrd array could have six possible axis orientations: (i, j), (~, i), (i, k), (k, i), (j, k), \n(k, j). In general, the array A has t!/(t -r)! possible axis orientations. Scalars are consid\u00adered free, \nwit h the universe as the possible set of positions. Note that this does not give all the information \nwe nqd about placing the object. If A(:,:) is oriented (i, j)l(l, 1), then every row object A(.t,:) is \noriented (j) I(1) irrespective of the row number L The additional information we need is 21 precisely \nthe offset alignment. There are only finitely many possible axis alignments, since the templdte has finite \nrank. However, there is a poten\u00adtially infinite space of stride alignments. We use Q, the set of rationals, \nas the space of strides. We choose Q rather than Z because Q is closed under multiplication and dMsion \nby nonzero integers, which is necessary for dealing with strided sections. Our algorithm can generate \nfractional strides, but it multiplies through at the end to make them sJI integers. Consider the array \nalignment A(1, .7) BT(3.J-2, 10 61). The axislstride position corresponding to this alignment is (j, \ni)l(3, 6). The negative stride indikates that the second array dimension is reversed with respect to \nthe template arie. 4.1 Unary operations Given a set of positions for an array object, we can derive sets \nof positions for the array objects created by unary array operations such as projection, strided section, \ngeneralized transpose, reduction, and spread. Let the template T have rank t. Let A be an array object \nof rank n < t, and B an array object of rank m < t.Let the templateax-be labeled I = [71, . . . . 7$]. \nLet i range over integers and p over rationals. Let Ri = [m? . . . ,Pi] be a permutation of [1,... ,;]. \nLet Sim [al,..., ai] be a section expression, where each ~i is a section specifier. We define the function \nAPPLY shown in Figure 5 that applies a section or a permut ation to a position for a given object to \nproduce the corresponding position(s) of the derived object. This leads to the set of rules for positions \nof derived array objects shown in Figure 5. Note that the output of a SPREAD operation can have a set \nof possible positions rather than a single position if there are multiple degrees of freedom between \nthe ranks of the template and the input of the SPREAD. Consider SPREAD (V, DIBf=2, NCOPIES-n), where \nthe vector V is aligned (i)l(l) in a 3-dimensional template. The possible orientations of the result \nare (i, j)l(l, Q) and (i, k)[(l, Q). The notation (i, ~)1(1, Q) is really an abbreviation of the set \nof positions {(i, j)l(l, p) : p G Q}. The stride in the spread direction is set to Q since there is no \na priori reason to constrain it further. 4.2 The alignment algorithm We will use a CDP algorithm M a \nheuristic to determine approximate coets at nodes. Note that this does not guar\u00ad antee an optimal solution \nany longer, since the dynamic pro\u00ad gramming formulation does not accurately model the effect of fanout. \nFirst, the costs of subtrees are no longer inde\u00ad pendent: equation (3) no longer holds. However, we will \ncontinue take equations (3) (5) as definitions of C=, gy, and r; in the DAG case, realizing that Cm(p) \nis only an approx\u00ad imation to the minimum cost of the subgraph below node z. Second, even supposing that \nwe had exact cost functions for all nodes, the presence of multiple sink vertices poses an\u00adother problem. \nAn optimal solution for the DAG does not necessarily place each sink node in a minimum-cost position. \nConsider the following piece of code and its program graph shown in Figure 6. real A(IOO, 100), B(1OO, \n100), C(1OO, 100) , T(IOO, 100), X(IOO) T = A + transpose(A) B = spread(X, dim=l, ncopies=100) + f C \n= spread (x, dirn=2, ncopies=100) + T 1000O 10000 1ooOO A 3 I x I  Figure 6: A DAG with multiple sink \nnodes. The opti\u00admal solution for the DAG does not necessarily place the sink nodes at their minimum-cost \npositions. The minimum-cost solution would orient both arrays B and C either in orientation (i, j) or \nin orientation (j, i), with a residual cost of 100. However, th~ does not place the two sink nodes in \ntheir minimum-cost positions. Insisting that the sink nodes be placed optimally would actually result \nin a residual cost of 10000. After we have an approximate solution, we will use the Steiner tree optimization \nto further reduce communication costs. Steiner trees can be found trivially for the discrete metric: \ndetermine the set of distinct positions of the desti\u00adnation nodes, and find any spanning tree of this \nset in the discrete metric space. The contribution function of a node has a particularly simple form \nin the discrete metric. Lemma 1 Let pv = minr Cv (r). Then, in the discrete metr\u00adic space, 9v(~) = min(GJp)J \nAV + wv). Thus, a node is placed either at its parent s location or at a location where its cost function \nis minimized. Lemma 1 alSO shows that the convolution defining g, (equation (4)) can be computed in constant \ntime for the dwrete metric. The cost function C. for a node z takes on only a small number of different \nvalues. Therefore, we represent it com\u00ad pactly by tabulating its lewd gets. A level set of node z is \na set L= of positions p where the cost value C=(p) is constant. (We do not require that level sets be \nmaximal.) Lemma 2 Let x be an internal node with children y and z. Let LY be a level set of w with C%(P) \n= P, and L. a level set of Z, with C=(p) = y. Then LY n LZ is a level set Of x, with C=(p) = min(/3, \np. + W9) + min(-f, p= + w,). Proof: Let p be a position in Lv n L.. By two applications of Lemma 1, Cm(p) \n= min(CV(p), p~ + w~) +min(Cz(p), V. + (Transpose) APPLY(&#38;, Al~) = APPLYl(&#38;, A)lAPPLYl(&#38;, \n~) APPLYl((fM, . . .. Pn). (al, cZn))Zn)) = (cYpl, . . ..rlpm) (Array sectioning) APPLY(S, Al~) = APPLY2(S, \nA) IAPPLY3(S, S) APPLY2((c71 , . an)) = REMOVE-ZEROS(~l, . . . . ~.) )~n),(o l!.., fYi B! = () :ye:w;e= \n1 : 0 { APPLY3((U1 , , cn), (Pi, . . . )%)) = REMOvE-zERos(91, . . . ,9n) Pi0 St where G,=II:hi :si \nor : 98=0 otherwise { Q where p= Q pos = p . s otherwise { Let x(A) = (al,..., a~)l(pl,. ... p~) and \nT(B) = (al, . . ..a~)l(pl. p~), p~) T(A(S)) APPLY(S, x(A)) (6) ~([&#38;JA) APPLY(&#38;, T(A)) (7) T(REDUCE(A, \ni)) (al,..., @,-1, O ,+l, . . ..an)l(j91 .Pi. l,Pi+l,i+l, . ...%) (8) r(SPREAD(&#38; i)) {(a,,..., ~i-l, \ny,~i, . . ..~m)l(pl . . . ..pt-l. Q.pil. ..>pm):Y~r_ A(B)} (9) Figure 5: Definition of alignments for \narray objects created by unary operations. w.) = min(~, pv + WV) + min(y, p, + w%). Thus, C.(P) is Algorithm \n2 (Axis and stride alignment for a independent of the position p, and Ly ~ L, is therefore a computation \nDAG.) level set of x. 1 Input: A weighted computation DAG G = (N, E, W). Output; An access and stride \nalignment function m that On the downward pass of the CDP algorithm, we choose reduces residual communication \ncost. the position of a node bssed on the position of its successors. Method: In the case of a node with \nfanout, placement is complicated by the Steiner problem. We implicitly construct the Steiner i for z \nc N in topological order do tree in the discrete metric as follows. The cost of a Steiner 2 if x is a \nunary operation node then tree on the set of positions {m,... , r~ } in the discrete met\u00ad 3 Use equations \n(6)-(9) to propagate the ric is n 1, which depends on the cardinality of the set but is cos @fo.rmati.on. \nindependent of its elements. Thus, we minimize the contri-4 else z M a binary operation node / ) bution \nof the node fanning out to these positions by choosing 5 LEVEL-SETS($) + @ the position in this set that \nminimizes the cost function of 6 g + LCHILD(%); Z t RCHILD(Z) the node. 7 for all &#38; E LEVEL-SETS(V) \nand L. The final cost of the alignment is not necessarily given LEVEL-SETS(Z) do by the sum of the minimum \ncosts of the sink nodes. Rather, 8 L= + &#38; L= once we have placed the nodes, we need to make a post-9 \nIf L, # # , add L= to LEVEL-SETS(Z), processing pass over the DAG and compute the cost of the using Lemma \n2 to compute the cost. alignment using equation (2). 10 enddo The above lemmas lead to the following \nCDP algorithm 11 endif for the axis and stride alignment problem. The algorithm 12 enddo finds a minimum-cost \nsolution if the DAG is in fact a forest, 13 Place sink nodes in minimum-cost positions. but in the general \ncase it is only a heuristic. 14 for x c N in reverse topological order do 15 u(z) + {r:(T(#)) : (Z, y) \nE E} 16 T(Z) + any p ~ a(z) that minimizes C. 17 enddo 18 return ~ The only concern in the upward sweep \nof the algorithm is the possibly exponential growth of the number of level sets. However, for the two \nmost important cases of fixed source positions and unconstrained source positions, we can choose a level \nset partition of the universe that haa a compact rep resentation and linear growth rate. Lemma 3 Let \nG = (N, E, W) be a computation DA G with s source nodes. If the alignment at each source node is either \nfree or jixed, then the maximum num$w of level sets at any node ofG iss. Proofi We proceed by induction \non the height of the node. The base case is a node x of height O, i.e., a source node. If the node is \nfree, the level set partition is L= = {P}. If the node is fixed at position x., the level set partition \nis L.= {{ T.}, P-{r=}}. For the induction case, assume that an internal node z has chddren g and z, and \nthe level set partition of y is LY = {BO, B,,..., 13~_l}and thatofz is Lz={CO, Cl,..., Cn_l}. The sets \nl?l ,..., Bl,C1 ,1,..., Cn-l are singletons, Ilo = P -U~~l Bit and Co = P ~~~ C,. (This is the na\u00adture \nof level sets moduced bv the base case.) Now. bv Lemma 2, the level-sets of z ar~ intersections of level \nsets ~f v and z. Hence the level set partition of z is L= = {Bo n CO, Bl,... ,Bl,Cl,l,... , C~_l }, which \nproves the lemma. E Theorem 1 Algorithm 2 runs in O(INIS2) time on a DA G with node set N ands source \nnodes. We illustrate this algorithm on the DAG of Figure 3. Let T be the two-dimensional template with \naxes named i and j to which all array objects will be aligned. Let the position of array a be fixed at \n(i, j) I(1, 1), and the positions of b and c be free. Let Q1 = Q -{1}. Then the level sets of the cost \nfunction for nodes a, b, and c are as shown in the second column of Table 4. We indicate a level set \nand its cost as a colon-separated pair. In the first phase of Algorithm 2, we traverse the DAG from sources \nto sinks computing cost functions at each node. We obtain the coat functions shown in the second column \nof Table 4. The final choices of positions made in the second phase of Algorithm 2 for the nodes are \nshown in the third column of Table 4. Nodes b and d have fanout and therefore require the Steiner optimization \nin the second phase. In the caae of node b, there are multiple minimum-cost positions. Nodes t3 and t4 \nare examples where strides are uncon\u00adstrained. In such cases, we arbitrarily set the strides to 1. Nodes \nb and c show that strides can be fractional. In this case, we multiply all strides by the least common \nmultiple of the denominators of the rationrd strides. After multiplying all the strides by 4 to make \nthem in\u00adtegral, we obtain the final alignments shown in Figure 2. With this alignment the only residual \ncommunication oc\u00adcurs along the edge from node b to node t5: here general communication is used to simultaneously \ntranspose the ar\u00adray b and change its stride. This is one of two minimum-cost solutions for this example. \n5 Offset alignment Axis and stride alignment require general communication, which is more expensive than \nthe nearest-neighbor com\u00admunication used in offset alignment. We therefore perform axis and stride alignment \nbefore offset alignment. If a DAG edge carries communication from axis and stride alignment, this subsumes \ncommunication due to offset alignment; we can perform both the axis/stride change and the s~lft in one \ngeneral communication step. We model thw during off\u00adset alignment by setting to zero the weights of those \nedges that already carry general communication from axis or stride alimment. For the offset alignment \nproblem, we use the metric (grid, ring, etc.) appropriate to the machine architecture. In thw section, \nwe concentrate on the grid (and hypercube) metric, dG(p, g) = lp ql. A k-dimensional grid is the Cartesian \nproduct of k one-flmensional grids, and the distance be\u00adtween two positions in the k-dimensional grid \nis the sum of the distances in the k one-dimensional grids between the appropriate projections of those \npositions. Thm allows the multidimensional problem to be separated into k indepen\u00ad dent one-dimensional \nproblems. For thk reason, it suffices to present the solution for a ondimensional grid. The offset alignment \nproblem for grids can be reduced to integer programming (except for the Steiner correction). Let a. be \nthe position of node x in some alignment r. Then the residual communication cost (which is the function \nwe want to minimize) is (x,u)cE Program operations such as sectioning and shifts introduce a set of linear \nequality constraints. For instance, the state\u00adment B = A(5: ~) introduces the constraint ~B = ~A + 5. \nTo reduce the offset alignment problem to integer programm\u00ading, introduce the variable 0=9 for every \nedge (z, y) of the DAG. For each such @=&#38;, add the following two inequality constraints: These constraints \nguarantee that O.v > [x. -mti 1. Also, change the ~bjective function to be G(O) = x WZ19=U. (x,u)-The \ntransformed problem is equivrdent to the original one, because the minimum of G(@) will always have Omv \n= Ir= \u00ad%Y1. Integer programming is NP-complete. However, we can relax the requirement that the solutions \nbe integers, and solve the corresponding linear programming problem to ob\u00adtain a solution for ~. Since \nthe solution is not necessarily integral, we must examine the integer-valued positions in the neighborhood \nof the solution and choose a position with minimum function value. 6 The free-source alignment problem \nOnce we have an algorithm for the fixed-source alignment problem, we can conceptually create an algorithm \nfor the free-source alignment problem by searching through all pos\u00adsible placements oft he source nodes. \nThii exhaustive search is not practical. We therefore resort to a relaxation tech\u00adnique we call cyclic \nsource placement that is related to Al\u00adternating Direction Search used in multivariate optimizat\u00adion \n[12]. In this method, we freeze the locations of all but I I Node n II c. I rr(n) ,,. /1, , , 1,,,, \n, ,  -~,(i, Ol(ck Q): 0} I (i!i)l(l,l/4 -, -, ---J.-J /1. ~ , , \\-, .,, \\ r , ,Q) :0, (i,.i)l(Q, Q) \n: 0} (j, ~)l(l,fi?) ;,Q) :0, (j,i)l(Q, Q) : o} (iw,Q) o)(i)l(Q) 0} -uwL--- Table 4: Cost functions \nand final positions of nodes in the computation DAG of Figure 3. one of the source nodes, and compute \nan alignment and cost. This also provides au fllgnment for the free source Algorithm 3 (Heuristic solution \nof free-source node. If this results in a lower cost, we use this as the new alignment problem with cyclic \nsource place\u00adalignment of the free source node, and move on to another ment.) source node. We cycle through \nthe source nodes until the Input: A computation DAG G = (N, E, W), and a po\u00adcost does not improve any \nfurther. The order of cycling sition space P. is from source nodes of heavier weight to those of lighter \nOutput: A low-cost alignment function ~. weight, and, within the same weight, from nodes with larger \nMethod: outdegree to those of smaller outdegree. We have found that in practice the algorithm converges \nin 2 or 3 cycles 1 Sort the source nodes of the DAG by rank and, and produces very good solutions. The \ndetails of the algo-within rank, by outdegree into array L. rithm are given below. We only use cyclic \nsource placement 2 Place the variable LIo] in an arbitrary orienta\u00adfor the axis and stride components; \nlinear programming di-tion, and let all other variables be free. 3 Apply Algorithm 2 to obtain an initial \nestimate rectly solves the free-source variant of the axis alignment problem in the grid metric. for \nTand ~= C(G, d,x). 4 for iter = 1 to maxiter do 5 for var = O to maxvar do 6 x(IJ + ~ with ~(L[var]) \nset to free 7 Apply Algorithm 2 with source node place\u00ad ments from ~(l) to obtain rf2J and N = C(G, d, \nr(z)). 8 if ~ < ~then 9 m+ J ); K+ K 10 endif 11 enddo 12 enddo 13 return r 7 Extensions and future \nwork This section deals with extensions the basic framework that make the analysis more powerful and \nallow the modeling of features such aa replication and dynamic alignments. The ideas in this section \nare topics of current and future research and are subject to change based on future experience. 7.1 Control \nflow Previous work by Chen and Wu [7] focused on finding align\u00ad ments for ~-blocks of code in isolation \nand then perform\u00ad 25 ing the necessary realignments at ~-block boundaries. How\u00adever, it is usually the \ncase that a code block gives a set of constraints that must be satisfied rather than a unique so\u00adlution, \nand there may be several alignments satisfying the constraints. Arbitrarily choosing one solution without \nrefer\u00adence to the uses of the variables in later blocks could cause more communication than necessary. \nThis suggests the need for a more global approach. Interlock alignment is a consideration only for variables \nthat are live at the beginning or end of a block. Dead vari\u00adables can be aligned optimally within their \nactive block. This requires solving a standard data flow problem to iden\u00adtify live and dead variables \n[14]. Our approach to interlock ahgnment is based on the no\u00adtion of traces [8] of the flow graph of the \nprogram. Our task is considerably simpler than that of classical trace schedul\u00ading for two reasons: our \ntraces are more coarse-grained, and we are not performing code motion. We use traces to gen\u00aderate a prioritized \nlist of larger basic blocks on which to apply our algorithms. We perform alignment across the basic blocks \nin the current trace subject to constraints imposed by previous (higher-priority) traces. To handle off-trace \nconstraints, we introduce split and merge nodes, whose purpose is to handle the necessary reahgnment \nin case of control flowing back and forth between a trace and a different, higher-priority trace. If \na variable can arrive at a point on the current trace either from within the trace or from another trace. \nwe rermesent. it with a merge node. Suppose the weight of the data on each edge is D, and the edge counts \nfor the on-and off-trace edges are N and F. Then the modified weights are ND/(N + F) for the on-trace \nedge and FD/(N + F) for the off-trace edge. The extension to multiway merge nodes is obvious. We do a \nsimilar thing, but in reverse, for split nodes. The addition of these nodes means that all constrained \nnodes of a trace are either source or sink nodes. Our previous algorithms can be applied to this modified \ngraph. During code generation, the merge and split nodes generate any communication resulting from realignment \nacross basic blocks.  7.2 Dynamic alignment Consider the following loop, taken from Knobe et al. [19]. \ndoi=l, n t = a(i, :) a(i, :) = b(i., :) b(i, :) =t enddo It is possible to execute this loop without \ncommunica\u00adtion, but not with a fixed aEgnment for the vector t. The zero-communication solution is achieved \nby aligning the vec\u00adtor t with the ith row of arrays a and b at the ith iteration of the loop. This shows \nthe need for dynamic alignment. We propose dynamic alignments that have some struc\u00adtured dependence \non loop induction variables, rather than those that are dependent on arbitrary runtime values of vari\u00adables. \nKnobe et al. [18] deal with the latter kind of dynamic alignments. In particular, we consider making \nthe stride and offset alignments of arrays affine functions of loop induction variables. The example \nabove was particularly simple since it had no loop-carried dependence. Each instance of the vector t \nwas live only within a single loop iteration and could be op\u00adtimized independently. However, consider \na loop with loop\u00adcarried dependence on some array variables, and think of two iterations i and J spanned \nby such a dependence edge e on an array A. Given that the alignment of A is an affine function of the \nloop induction variable, we know the align\u00adments of A at entry to iterations z and j, and we can compute \nthe possible alignments and their associated costs for A at exit from iteration i. If the ahgnment of \nA at its exit from iteration i does not match its desired alignment on entry to iteration j, the edge \ne will carry residual communication, reflected in the dynamic realignment of A. Clearly, we cannot actually \nunroll loops to perform this analysis; we must therefore find a representation that com\u00adpactly encapsulates \nsuch interactions between loop itera\u00adtions while maintaining the acyclic nature of the program graph \nif possible. Restricting alignments to affine functions of loop induction variables and assuming constant \ndistance vectors gives a uniform pattern to the interaction between loop iterations. We generate a closed-form \nrepresentation by introducing transformer and comparator nodes that en\u00adcapsulate the interaction between \nloop iterations. A trans\u00adformer node takes the alignment function of a variable on entry to the current \niteration i and produces its desired alignment function on entry to the iteration j affected by the loop-carried \ndependence. A comparator node compares the alignments of a variable at exit from iteration i and on entry \nto iteration j (the latter being derived from a trans\u00adformer node), and resolves the cost of realignment \nwhen the two are different. This captures inter-iteration dependence while allowing the optimization \nto be formulated as a DAG problem. However, some symbolic manipulation is required to solve this problem. \nWe are currently investigating the symbolic capabilities required for such a system.  7.3 Replication \n Replication can be modeled by allowing offsets to be sets of positions rat her than a single position. \nThese sets can be compactly represented as section specifiers. Thus, an align\u00adment of the form A(I) E \nT(I, O : 100 : 2) would cause the one-dimensional vector A to be replicated across a subset of the even-numbered \ncolumns of template T. We have identi\u00adfied two distinct kinds of replication: ualue replication and name \nreplication. In the former, the values of a smaller ar\u00adray are replicated across a larger array; this \nis the case with SPREAD. In the second case, the locus of ahgnments of a smaller array span a larger \narray; this is the case with dy\u00adnamic alignment functions, as in the example in Section 7.2, and is related \nto notions of ownership transfer proposed by Bala and Ferrante [3]. While regular sections are a reasonable \nsyntactic mecha\u00adnism for specifying replication, it is unclear how to use them for analysis. Replication \nis a vexing problem that has not been dealt with in current research, but is clearly important enough \nto merit serious attention. 7.4 Automatic distribution This paper deals only with alignment, and has \nomitted a discussion of automatic distribution. The issues there are determining the number of processors \nto be assigned to each dimension of the template, and the block size of the distri\u00adbution for each template \ndimension. Distribution offers the 26 possibility of further reducing communication by serializ\u00ading \ncertain template dimensions in memory; however, such communication reduction must be balanced against \nthe pos\u00adsible loss in parallelism. Automatic distribution can also be formulated as a constrained optimization \nproblem. We have made some progress in modeling this problem. Gupta [13] has developed a constraint-based \napproach to automatic dis\u00adtribution. Conclusions We have presented aframework for the automatic determi\u00adnation \nof array alignments in data-parallel languages such as Fortran 90. We have decomposed alignment functions \ninto three constituents: axis, stride, and offset. For each of these subproblems, we have shown how to \nsolve the align\u00adment problem for a basic block of code, possibly containing common subexpressions. The \nlanguage model handles sec\u00adtioning operations, reductions, spreads, transpositions, and masked operations. \nWe make few assumptions and are not limited bytheowner-computes rule. Wehave also presented some ideae \non extending the analysis to handle control flow, dynamic alignments, and replication. While fllgnment \nand distribution directives are being proposed in High Performance Fortran [15], most of the proponents \nagree that such user specification is a stopgap solution, and the ultimate solution should involve compiler \nanalysis. In many ways, the situation is similar to that in the early days of vectorizing compilers. \nVectorizers have grown in sophistication in the last decade, and are much less de\u00adpendent on user directives. \nOur work has put the alignment problem on a firm theoretical foundation and provided solu\u00adtions to some \nof the problems. However, many open prob\u00adlems remain to be solved before these techniques become part \nof the standard optimization toolkit in compilers for distributed-memory machines. Acknowledgments We \ngratefully acknowledge a helpful conversation with Mike Overton, who pointed out the transformation to \nreduce the offset alignment problem to integer programming. We thank Jay Sipelstein for his comments \non an early draft of this paper. References [1] AHO, A. V., SETHI, R., AND ULLMAN, J. D. CornpiL ers: \nPrinciples, Techniques, and Tools. Addison-Wesley Publishing Company, Reading, MA, 1986. [2] AMERICAN \nNATIONAL STANDARDS INSTITUTE. For\u00adtran 90: X3J3 internal document S8. 118 Submit\u00adted as Text for ANSI \nX3.198-1991, and ISO/IEC JTC1/SC.22/WG5 internal document N69.2 Submitted as Text for ISO/IEC 1539:1991, \nMay 1991. [3] BALA, V., AND FERRANTE, J. Explicit data place\u00adment (XDP): A methodology for explicit compile-time \nrepresentation and optimization of data movement. In Proceedings of the Second Workshop on Languages, \nCompilers, and Runtime Environments for Distributed Memoru Mtdtiprocessors (Boulder, CO, Oct. 1992). \n[4] CHAPMAN, B., MEHROTRA, P., AND ZIMA, H. Pro\u00adgramming in Vienna Fortran. Scientific Programming 1, \n1 (Fall 1992), 31-50. [5] CHATTERJEE, S. Compiling Data-Parallel Programs for Eflicient Execution on \nShared-Memory Multiprocessors. PhD thesis, School of Computer Science, Carnegie Mel\u00adlon University, Pittsburgh, \nPA, Ott. 1991. Available as Technical Report CMU-CS-91-189. [6] CHATTERJEE, S., GILBERT, J. R., SCHREIBER, \nR., AND TENG, S.-H. Optimal evaluation of array expres\u00adsions on massively parallel machines. In Proceedings \nof the Second Workshop on Languages, Compilers, and Runtime Environments for Distributed Memory Multi\u00adprocessors \n(Boulder, CO, Oct. 1992). Also available as RIACS Technical Report TR 92.17. [7] CHEN, M. C., AND WU, \nJ.-J. Optimizing FORTRAN\u00ad90 programs for data motion on massively paraJlel sys\u00adtems. Tech. Rep. YALEU/DCS/TR-882, \nDepartment of Computer Science, Yale University, New Haven, CT, Jan. 1992. [8] ELLIS, J. R. Bulldog: \nA Compiler for VLIW Architec\u00adtures. ACM Doctoral Dissertation Awards. The MIT Press, Cambridge, MA, 1986. \n[9] Fox, G. C., HIRANANDANI, S., KENNEDY, K., KOEL-BEL, C., KREMER, U., TSENG, C.-W., AND Wu, M.- Y. \nFortran D language specification. Tech. Rep. Rice COMP TR90-141, Department of Computer Science, Rice \nUniversity, Houston, TX, Dec. 1990.  [10] GAREY, M. R., AN?D JOHNSON, D. S. Computers and Intractability: \nA Guide to the Theory of NP-Completeness. W. H. Freeman and Company, San Fran\u00adcisco, CA, 1979. [11] GILBERT, \nJ. R., AND SCHREIBER, R. Optimal expres\u00adsion evaluation for data parallel architectures. Jorn-\u00adnal of \nParallel and Distributed Computing 13, 1 (Sept. 1991), 58-64. [12] GILL, P. E., MURRAY, W., AND WRIGHT, \nM. H. Prac\u00adtical Optimization. Academic Press, Orlando, FL, 1981. [13] GUPTA, M. Automatic Data Partitioning \non Dis\u00adtributed Memory Mrdticornputers. PhD thesis, Univer\u00adsity of Illinois at Urbana-Champaign, Urbana, \nIL, Sept. 1992. Available as technical reports UILU-ENG-92\u00ad2237 and CRHC-92-19. [14] HECHT, M. S. Flow \nAnalysis of Computer Programs. North-Holland, New York, NY, 1977. [15] HIGH PERFORMANCE FORTRAN FORUM. \nHigh Perfor\u00admance Fort ran language specification version 1.0. Draft, Sept. 1992. [16] HIRANANDANI, S., \nKENNEDY, K., AND TSENG, C.- W. Compiler support for machine-independent paral\u00adlel programming in Fortran \nD. Tech. Rep. Rice COMP TR90-149, Department of Computer Science, Rice Uni\u00adversity, Houston, TX, Feb. \n1991.  [17] IVERSON, K. E. A Programming Language. Wiley, New York, NY, 1962. 27 [18] KNOBE, K., LUKAS, \nJ. D., AND DALLY, W. J. Dy\u00adnamic fllgnment on distributed memory systems. In Proceedings of the Third \nWorkshop on Compilers for Parallel Computers (Vienna, Austria, July 1992), Aus\u00adtrian Center for Parallel \nComputation, pp. 394 404. [19] KNOBE, K., LIJKAS, J. D., AND STEELE JR, G. L. Data optimization: Allocation \nof arrays to reduce com\u00admunication on SIMD machines. Journal of Parallel and Distributed Computing 8, \n2 (Feb. 1990), 102-118. [20] LEISERSON, C. E. Fat-Trees: Universal networks for hardware-efficient supercomputing. \nIEEE Trans. Com\u00adput. C-34, 10 (Oct. 1985), 892-901. [21] LI, J., AND CHEN, M. The data ahgnment phase \nin compiling programs for distributed-memory machines. Journal of Parallel and Distributed Computing \n13, 2 (Oct. 1991), 213-221. [22] MACE, M. E. Memory Storage Patterns in Parallel Processing. Kluwer international \nseries in engineering and computer science. Kluwer Academic Press, Nor\u00adwell, MA, 1987. [23] MELZAK, Z. \nOn the problem of Steiner. Canadian Mathematical Bulletin 4 (1961), 143-148. [24] THINKING MACHINES CORPORATION. \nCM Fortran Ref\u00aderence Manual Versions 1.0 and 1.1. Cambridge, MA, July 1991. [25] WHOLEY, S. Automatic \nData Mapping for Distributed-Memory Parallel Computers. PhD thesis, School of Computer Science, Carnegie \nMellon University, Pitts\u00adburgh, PA, May 1991. Available as Technical Report CMU-CS-91-121. [26] WINTER, \nP. Steiner problem in networks: A survey. Networks 17 (1987), 129-167.  \n\t\t\t", "proc_id": "158511", "abstract": "<p>Data-parallel languages like Fortran 90 express parallelism in the form of operations on data aggregates such as arrays. Misalignment of the operands of an array operation can reduce program performance on a distributed-memory parallel machine by requiring nonlocal data accesses. Determining array alignments that reduce communication is therefore a key issue in compiling such languages.</p><p>We present a framework for the automatic determination of array alignments in data-parallel languages such as Fortran 90. Our language model handles array sectioning, reductions, spreads, transpositions, and masked operations. We decompose alignment functions into three constituents: axis, stride, and offset. For each of these subproblems, we show how to solve the alignment problem for a   basic block of code, possibly containing common subexpressions. Alignments are generated for all array objects in the code, both named program variables and intermediate results. The alignments obtained by our algorithms are more general than those provided by the &#8220;owner-computes&#8221; rule. Finally, we present some ideas for dealing with control flow, replication, and dynamic alignments that depend on loop induction variables.</p>", "authors": [{"name": "Siddhartha Chatterjee", "author_profile_id": "81100051284", "affiliation": "", "person_id": "PP14029125", "email_address": "", "orcid_id": ""}, {"name": "John R. Gilbert", "author_profile_id": "81339501109", "affiliation": "", "person_id": "PP42052579", "email_address": "", "orcid_id": ""}, {"name": "Robert Schreiber", "author_profile_id": "81406595126", "affiliation": "", "person_id": "PP40036541", "email_address": "", "orcid_id": ""}, {"name": "Shang-Hua Teng", "author_profile_id": "81100401847", "affiliation": "", "person_id": "PP39080018", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/158511.158517", "year": "1993", "article_id": "158517", "conference": "POPL", "title": "Automatic array alignment in data-parallel programs", "url": "http://dl.acm.org/citation.cfm?id=158517"}