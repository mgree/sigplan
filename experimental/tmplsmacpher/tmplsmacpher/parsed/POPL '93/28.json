{"article_publication_date": "03-01-1993", "fulltext": "\n The 3 R s of Optimizing Constraint Logic Programs: Refinement, Removal and Reordering Kim Marriottl \nIBM -T.J. Watson Research Center Yorktown Heights, NY 10598, U.S.A. Abstract Central to constraint logic \nprogramming (CLP) languages is the notion of a global constraint solver which is queried to direct execution \nand to which constraints are monotonically added. We present a methodology for use in the compilation \nof CLP languages which is designed to reduce the over\u00adhead of the global constraint solver. This method\u00adology \nis based on three optimizations. The first, refinement, involves adding new constraints, which in effect \nmake information available earlier in the computation, guiding subsequent execution away from unprofitable \nchoices. The second, removal, involves eliminating constraints from the solver when they are redundant. \nThe last, reordering, involves moving constraint addition later and con\u00adstraint remowd earlier in the \ncomputation. Deter\u00admining the applicability of each optimization re\u00adquires sophisticated global analysis. \nThese analy\u00adses are based on abstract interpretation and pro\u00advide information about potential and definite \nin\u00adteraction between constraints. Introduction Constraints provide a powerful and declarative pro\u00adgramming \nparadigm, in which the objects of compu\u00adtation are not explicitly constructed but rather they are implicitly \ndefined using constraints. Though the impor\u00adtance of constraints has been widely recognized in com\u00adputer \nscience [19], only recently have general purpose programming languages in which constraints are primi\u00adtive \nelements been developed. These include concurrent constraint languages [16], constraint query languages \nfor databases [12], functional constraint languages [4] and, in particular, the paradigm we shall concentrate \n1Affiliation from February 1992: Dept. of Computer Science! Monaah University, Clayton 3168, Australia. \nPermission to copy without fee all or part of this material is granted provided that the copies are not \nmade or distributed for direct commercial advantage, the ACM copyright notice and th~-_ title of the \npublication and its date appear, and notice is givafi that copying is by permission of the Association \nfor Computing Machinary. To copy otherwisa, or to republish, requires a fee and/or specific permission. \nACM-20th PoPL-1/93-S.C., USA @ 1993 ACM 0-89791 -561 -5/93 /0001 /0334 . ..$1 .50 Peter J. Stuckey Department \nof Computer Science University of Melbourne Parkville 3052, Australia on in this paper, constraint logic \nprogramming (CLP) languages [8]. Central to all of these languages is the notion of a global constraint \nsolver which is queried to direct execution and to which constraints are monoton\u00adically added. Because \nof monotonicit y, these languages are semantically simple. They are also important prac\u00adtically and have \nbeen used in such diverse fields as finan\u00adcial analysis [13], circuit design [18] and protocol testing \n[7]. In part this is because constraint programming al\u00adlows simple and concise programs which may be \nused in many different ways. However, this flexibility comes at a price, as general constraint solving \nis expensive. Thus, in CLP, as in the other constraint paradigms, the main overhead is constraint solving \nand so a main goal of op\u00adtimization in the compilation of constraint languages is to reduce this overhead. \nThis is made difficult by the need to determine, at compile time, non-trivial proper\u00adties of constraint \ninteraction. We develop a methodology for use in the compila\u00adtion of CLP languages. The key idea is to \ntransform a (monotonic) program into a non-monotonic program in which constraints are added to the constraint \nsolver only when they are needed and are subsequently removed when they are no longer needed. The methodology \nhas three steps in the optimization: clause refinement, con\u00adst raint removal and, finally, constraint \nreordering. . REFINEMENT adds new constraints to the clauses. These constraints will eventually become \nredundant, so the declarative meaning of the program remains the same. The advantage is that the new \nconstraints in effect make information available earlier in the compu\u00adtation, and so can improve the \noperational behavior by guiding subsequent execution away from unprofitable choices. REMOVAL involves \nadding removal instructions to each clause indicating that a constraint previously added to the constraint \nsolver can now be optionally removed aa the constraint has become redundant in the sense that its information \nis duplicated in the solver. Intuitively, removal is advantageous as it reduces the number of constraints \nin the constraint solver without significantly changing the operational behavior. This optimization is \nimportant if programs written in mono\u00adtonic languages are to approach the efficiency of pro\u00adgrams written \nin non-monotonic languages. o REORDERING moves constraint addition later and con\u00adstraint removal earlier \nin the clause (and hence com\u00adputation) when the constraint involv~d cannot affect the control flow in \nthe intervening computation. Like removal, reordering, is advantageous as it reduces the number of constraints \nin the constraint solver without significantly changing the operational behavior. Each optimization is \nuseful in its own right, but it is their complementary behavior which reveals their full power. In particular, \nrefinement together with reorder\u00ading allows constraint information to be moved through\u00adout the computation, \nwhile removal alleviates the po\u00adtential overhead created by the new constraints added in refinement as \nall such constraints will become redun\u00addant and so can be removed. Finally, reordering com\u00adbines with \nremoval to allow constraints to be removed even before they become redundant. Correctness of each optimization \ndepends on simple algebraic properties of the constraints and monotonicity of the original program. However, \ndetermining the ap\u00adplicability of each optimization requires sophisticated global analysis. These analyses \nare based on abstract interpretation and provide information about potential and definite interaction \nbetween constraints. We give generic analyses and specific analyses for the case of constraint solving \nover linear arithmetic. The optimizations we introduce have significant practical importance as preliminary \ntest results show that in conjunction with low-level optimizations they can give order of magnitude improvements \nin execution speed. Though the specific details of the optimizations and analyses are for languages based \non the CLP frame\u00adwork, we feel that the underlying ideas are applicable to the compilation of other constraint \nbased programming languages. Constraint based programming and database lan\u00adguages are a relatively new \ndevelopment. Accordingly there has been little research into global optimization and compilation techniques \nfor these languages. Low level optimization for the language CLP(%3) [9], where constraint solving is \npartially compiled into imperative statements is discussed in Jorgensen et al. [11], Jaffar et al. [10], \nand Marriott and !S@ndergaard [15]. While application of these techniques requires global informa\u00adtion, \nthe optimizations are essentially local. The op\u00adtimization discussed here, however, are global in na\u00adture \nand hence offer potentially greater performance im\u00adprovements. In fact, as we shall demonstrate, the \ntwo types of optimization are complementary. The refine\u00adment optimization generalizes the optimization \nsug\u00adgested by Sato and Tamaki [17] and Marriott et al. [14] in the context of logic programming, and \nis related to constraint propagation transformations by Kemp et al. [6] for deductive databases. The \nremoval and reordering optimizations are novel, although the future redundancy optimization of Jorgensen \net al. [11] can be seen as a particular special combination of the two in the case information about \ncalls is ignored. The remainder of this paper is organized aa follows. In the next section we informally \nintroduce the opti\u00admization using a simple example. In Section 3 we give preliminary notation and an \noperational semantics for CLP programs with removal instructions. In Section 4 we detail each optimization, \nand in Section 5 we give a generic analysis to support each optimization. Section 6 contains a more realistic \nexample of the optimizations use and Section 7 concludes. 2 An Example In this section we informally \nintroduce the optimiza\u00adtion through a worked example. Consider the follow\u00ading CLP( R) program defining \nthe relation sum where Sum(n, s)es= l+... + n. It can be viewed as a straightforward translation of the \nrecursive mathemati\u00adcal definition. SUIZ(N,S)+N=OAS=O. (SUM) sum(N, S)+-N>=l AS=N +SIA NI = N-1 A sum(Nl, \nS1). CLP(7?) programs may be thought of as Prolog pro\u00adgrams extended to allow linear arithmetic constraints \nas well as syntactic equality constraints. Upper case symbols in the clause denote variables. The first \nstep in optimizing the program is to deter\u00admine which constraints are implied by the program for the \ncalls that we are interested in, In this example we consider all possible calls. For any call to SUZI(N, \nS) every answer satisfies the constraints N >= O A S >= N. We refine the program by adding the redundant \ncon\u00adstraints to every clause body where the calls are made. Hence the refined program is sum(N, S)+-N= \nOAS=O. (REF) sum(N, S)-N>=l AS=N +SIA N1=N-l AN1>=OA S1 >= N1 A sum(Nl, S1) . The chief advantage of \nthe refined program is that it will work with a wider class of calls than the original. For instance, \nit terminates (with answer no) for the query ?-sum (N, 5) whereas the original will run forever. As refinement \nmay change the operational behavior of the program, albeit for the better, it is different in nature \nto the other steps in the optimization. Thus refine\u00adment might be performed in an optional pre-compilation \nstage, in which the programmer can intervene. The next step in the optimization is to determine which \nconstraints are made redundant in further exe\u00adcution and hence where they can be removed. Clearly in \nthe refined program, for any call, each of the con\u00adstraints N1 z= O and S1 >= N1 is redundant after the \ncall sum (N1, S1), this being why they were added, and sot hey can be removed. Thus, in general, all \nconstraints added by refinement will be subsequently removed. In this case we can do better, N1 z= O \nis redundant be\u00adfore it is reached because of the constraints N >= 1 A NI =N-1, hence we can remove it \nfrom the program altogether. The constraint N z= 1 is redundant after the call sum(Nl, S1) just because \nN1 z= O was. The removal optimized program therefore is Query SUM REF REO SPE DET LOW Speed Up N <= 100 \n86.48 143.88 53.95  1.60 s <= 1000 34.00 14.55  N = 100 2.; 19.50 5.63 1.85 0.16 0.80 5:0 Table 1: \nTimings for the Optimized Sum Program sum(N, S)-N= OAS=O. (REM) sum(N, S)+-N>=l AS= N+SIA Nl=N -lAS1>=NIA \nsum(Nl, S1) A rem(N >= 1) A rem(Sl >= Nl) . Finally we reorder the constraints and their re\u00admovals. \nFirst consider the case of reordering in the con\u00adtext of all possible calls. Consider the constraint \nN >= 1. It is not always satisfiable when reached for every call and hence cannot be moved later in the \nclause. But during the recursive call to sum (N1, S1) the constraint does not affect the computation \nbefore it is made re\u00addundant, (by N = O or N >= 1) and hence its removal can be moved before thecall \nto sum (N1, S1). Similar reasoning applies to S1 >= N1. The resulting reordering optimized program is \nsum(N, S)+ N= OAS=O. (REo) sum(N, S)+-N>= iAS= N+SIA NI=N-IASI>=NIA rem(N >= 1) A rem(Sl >= Nl) A sum(Nl, \nS1) . Note that the resulting program, REO, not only adds one less inequality per level of recursion \nthan the original program, SUM, but also terminates for a wider class of calls. Given more information \nabout the intended calls to a program even better code is produced. Consider calls where N is given a \nfixed value and S is unconstrained. In this case the constraint S1 >= NI is always satisfi\u00adable when \nreached. Hence it can be moved past its removal, and thus removed entirely from the program. similarly \nthe constraint S = N + S1 does not affect ex\u00adecution during the recursive call and can be moved af\u00adterwards. \nThe resulting program is sum(N, S)+ N= OAS=O. (SPE) sum(N, S) +-N>= 1 A rem(N >= 1) A NI =N-1A sum(Nl, \nS1) A S= N+SI. If we combine this with low level optimizations that re\u00adplace constraints with tests \nand assignments whenever possible, we arrive at a deterministic program which does not make use of the \nconstraint solver! sum(N) if N == O then S := O (DET) else if N>= 1then NI := N-l sum(Nl, S) S:= S-EN. \n This example illustrates the synergy between the transformations given here and previous low level opti\u00admization \nsuggested for CLP languages. In particular, refinement can make non-deterministic programs deter\u00administic, \nand reordering can allow replacement of calls to the constraint solver by simple Boolean tests and as\u00adsignments. \nEven with this simple program the optimizations offer significant speedup. In Section 6 we give another \nexample, in which the results are even more encourag\u00ading. The effect of the optimizations on three different \ntypes of call are shown in Table 1. The bottom line is given by the speed up column which is the ratio \nbetween the time for the original program and the optimized pro\u00adgram, with low level optimizations applied \nto both wher\u00adever possible. Actual times shown are CPU seconds for 100 executions of the given queries \non a SparcStation 2 using CLP(7?) v1. 1. The program LOWnot shown is the result of applying low level \noptimizations to the original program SUM. 3 Operational Semantics of CLP In this section we give some \npreliminary notation and an operational semantics for constraint logic programs in which constraints \ncan be removed. A constraint logic program, or program, is a finite set of clauses. A clause is of the \nform H i--B, where H, the head is an atom, and B the body is a sequence of the form BI A... A Bn where \neach Bi is a literal. A literal is an atom, a primitive constraint, or a removal instruction of the form \nrem(Bj ) where the primitive constraint Bj occurs earlier in the clause. We let Bl,j represent the sequence \nB~ A . . . A Bj when i < j and nil otherwise. We associate a program point i E {O, 1,2,..., n} with the \npoint immediately after B1. Note that program point O is the point just before BI. An atom has the form \nP(xI, .... zn) where p is a predicate symbol and the x, are distinct variables. A primitive constraint \nis essentially a pre-defined predicate over some computation domain. For example the primitive constraints \nin CLP(%?) are linear arith\u00admetic equalities and inequalities over the real numbers and synt act ic equalities \nover terms. A constraint is a multiset of primitive constraints. We let true denote the empty multiset \nof constraints. The reason we in\u00adtroduce the multiset representation is to enable correct removal of \nconstraints, but often we will treat a con\u00adst raint simply as the conjunction of its primitive con\u00adstraints. \nThus constraints are pre-ordered by logical implication, that is O < 0 iff O # 0 . We let 3 ~ 0 be a \nnon-deterministic function which returns a con\u00adstraint logically equivalent to 3 VI 3 Vz . . . Vn8 where \nvariable set W = { V1, ..., V.}. We let ~~d be con\u00adstraint 0 restricted to the variables in S. That is \n~50 is function wars takes a syntactic 3(7WS o)\\(uar8 s) O where object and returns the set of (free) \nvariables occurring in it. Finally, we let % denote the existential closure of and which are local, allows \nstronger optimization as the O and frequently write % in place of where O is satis\u00adfiable. Var is the \nset of variables, Atom the set of atoms, Prim the set of primitive constraints, Rem the set of removal \ninstructions, Cons the set of constraints, Clause the set of clauses, and Prog the set of CLP programs. \nPrograms without removal instructions are said to be monotonic. A renaming is a bijective mapping from \nVar to Var. We let Ren be the set of renamings, and naturally ex\u00adtend renamings to mappings between atoms, \nclauses, and constraints. Syntactic objects s and s are said to be variants if there is a p E Ren such \nthat p s = s . The definition of an atom A in program P with respect to variables W, (defnp A W), is \nthe set of variants of clauses in P such that each variant has A as a head and has variables disjoint \nfrom ( W vars A). The operational semantics of a program is in terms of its (derivations which are reduction \nsequences of states where a state consists of the current literal se\u00adquence, or goal , and the current \nconstraint. More formally, Goal = (Atom+ Prim+ Rem)* State = Goal x Cons.  A derivation of state s for \nprogram P is a sequence of states so ~ ... --i Sn where s = so and there is a reduction from s%to si+l \nwhere state (L : G, O) can be reduced as follows: 1. If L 6 Prim and >(L A 0), it can be reduced to (G, \n{L}wO); 2. If L E Rem and L = Tern(L ), it can be reduced to (G, O -{L }) or to (G,19); 3. If L c Atom, \nit can be reduced to (1? :: G,(3) where 3(L+ 1?) E (defip L (vars G U vars 0)).  Note that M denotes \nmultiset union and :: concatenation of sequences. The length of a derivation is the number of atom reductions \noccurring in the derivation, that is the num\u00adber of reductions using reduction rule (3). A derivation \nis successful if the last state in the derivation has the empty goal. Such a state is called a success \nstate. The constraint ~$e is a partial answer to state s if there is a derivation from s to some state \nwith constraint 0. An answer to state s is a partial answer correspond\u00ading to a successful derivation. \nWe denote the set of an\u00adswers to s for P by ( answersp s), the partial answers by (panswersp s) and the \nderivations by (derivp s). When restricted to monotonic programs the above definition is equivalent to \nthe usual operational semantics [8]. A derivation is quasi-monotonic iff in every appli\u00adcation of reduction \nrule (2), ZG O is logically equivalent to ZG (8 {L }). A program P is quasi-monotonic for states S if \nevery derivation from a state s ~ S for P is quasi-monotonic. Information about the intended use of the \nprogram, such as which predicates are exportable from a module optimization need only preserve behavior \nfor those uses. Thus we will optimize the program for a given set of ini\u00adtial states and optimize the \ndefinitions of an atom only for the particular calls made to it in the derivations from those states. \nMore precisely, given a program P and a set of states S , a call to atom A is a constraint ~ (p-l 0) \nsuch that for some renaming p there is state (~ A) : G, 0) in a derivation from some state in S. We let \n(call P A S) denote the set of such calls. 4 Optimizations 4.1 Refinement In refinement, constraints \nwhich will become redundant in the future are added to the start of the clause. As the constraints are \nredundant they do not affect the answers and so the declarative semantics is preserved. The advantage \nis that constraint information is moved earlier in the execution and so unsuccessful derivations will \nbe pruned earlier. In particular this may transform a non-deterministic atom definition into a deterministic \ndefinition and allow the program to be used for wider variety of calls as these calls will now finitely \nfail rather than lead to an infinite derivation. Definition. Constraint 13 is redundant for set of con\u00adstraints \n@ iff V@ 6 @ .0 s-0. 0 Definition. Let S be a set of initial states and P a monotonic program. We obtain \na refinement of P for S by taking each clause H t B in P and rewriting it to: ~+d A(B-d) where 0 is \nredundant for ( answerp (B, O )) for all d C3 ~;~~~dH #. In the case 0 is false the clause is simply \nTheorem 4.1 (Correctness of refinement) Let P be a refinement of monotonic program P for states S. For \nall s E S, answerp s = answerpf s. Furthermore, for each D c ( derivp, s) there is a corre\u00ad sponding \nD c ( derivp s) of the same length. Z 4.2 Removal In the removal optimization clause bodies are aug\u00ad \nmented with declarations of the form rem(6) indicating that at this point constraint 6, previously introduced \nin the clause, may be optionally removed without affect\u00ad ing the computation aa it has become redundant \nwith regard to the current constraint. This is a very impor\u00ad tant optimization as it alleviates the overhead \nassoci\u00ad ated with languages in which constraints can only be monotonically added. A strength of this \noptimization is that it can always be used to remove the new constraints added by refinement. In fact \nwe can weaken the con\u00ad ditions for this optimization as we are not concerned about local variables appearing \nin the constraints, but really only require that the constraint is redundant with respect to the variables \nappearing in the original goal and the subsequent computation. Definition. Primitive constraint 0 is \nW-redundant for set of constraints @ iff V@ ~ @ .~w(8 A 6) a~w#. Let H + B be a clause with n body literals. \nPrimitive constraint BJ is redundant at program point k, k ~ j, for constraints @ and program P iff Bj \nis W-redundant for (answer~ (( Bl,j-1 A Bj+l,~), O)) for each 6 c @ where W = (vars H U vars Bk+l,n). \n0 The only difficulty in the optimization is ensuring that the removals do not interfere as removing \none constraint mav invalidate the removal of another. For example, cons;der optimizing the following \nprogram P for the iniial state s = (p(X~, true). p(x) -X=l Aq(X). q(x) + x=1. Now call P p(X) {s} .= \n{true} call P q(X) {s} = {x= 1}.  However, if p is optimized with respect to the call true and q is \noptimized with respect to the call X = 1 we obtain p(X) e X=1Aq(X) Arem(X =1). q(x) . which is not equivalent \nto the original program. The point is that we must guarantee that the removals are correct for the calls \nwhich will be encountered when executing the program P which results from the opti\u00admization. A sufficient \ncondition for this to hold is that the removals are correct with respect to the the calls en\u00adcountered \nwhen executing the skeleton of the original program, where the skeleton of a program P is the pro\u00adgram \nobtained by removing all constraints from P. We note that this definition of skeleton can be relaxed \nso as to leave all constraints which are not affected by the removal or reordering optimization. Definition. \nLet S be a set of states and P a program with skeleton Pske. We obtain a removal optimization of P for \nS by taking each clause H + B in P and re\u00adpeatedly rewriting it as follows. Assume that B has n literals. \nLet B7 be some primitive constraint in B. If BJ is redundant at program point j for (call Pk. H S) and \nP there is no need to add B], and so we rewrite the clause to: H * Bl,l-l A Bj+~,n. Otherwise if Bj \nis redundant at program point k > j for (call Pske H S) and P we rewrite to: H + Bl,~ A rem(BY) A Bk+l,n.n \n Execution of the optimized program mimics that of the original program, except that constraints which \nhave become redundant with respect to the other cur\u00adrent constraints may be removed, and so: Theorem \n4.2 (Correctness of removal optimization). Let P be a removal optimization of monotonic pro\u00adgram P for \nS. Then for all s &#38; S, answerp s = answerp! s. Furthermore, there is an isomorphism be\u00adtween (derivp, \ns) and ( derivp s) in which length is pre\u00adserved. 0 The intuitive advantage of the removal optimiza\u00adtion \nis clear: at each point in the corresponding deriva\u00adtion there are fewer constraints in the solver, leading \nto faster tests for satisfiability. However we note that the actual benefits obtained depend on the particular \nconstraint domain and solver, and must take into ac\u00adcount the cost of the removal itself. For example, \nin the case of term equations there is little gain as unifica\u00adtion automatically removes redundant equalities, \nwhile in the case of more complex constraint domains such as linear inequalities, the benefits are large \nas redundant constraints are a major source of overhead in the solver. For this reason, removal instructions \nare optional advice to the compiler and run-time system.  4.3 Reordering For different calls the order \nin which constraints are added during execution should be different so as to re\u00adflect their actual use \nin the computation. This moti\u00advates the final optimization, called reordering, in which a primitive constraint \nis moved later in the clause body until it will be actually used to prune a derivation, and a removal \ninstruction rem(@) is moved earlier in the body to where % is last used to prune a derivation. The ad\u00advantage \nof reordering is clear: adding constraints later and removing constraints earlier leads to smaller cur\u00adrent \nconstraints and consequent reduced cost of testing satisfiability. Correctness of the reordering optimization \nis baaed on the observation that a constraint does not prune a derivation from a state iff it is consistent \nwith all oft he state s partial answers. Definition. A primitive constraint 0 is consistent with set \nof constraints @ iff V@ c @ . % =$=~(# A 0). Let H ~ B be a clause with n body Iiterals. Primitive con\u00adstraint \nBj is consistent between program points ~ and k, where j ~ i < k, for constraints @ and program P iff \nBj is consistent with panswersp (Ba+l,k, 0 ) for each 0 E answersp (B1,j_I A Bj+l,ild) and 0 E ~. We \nsay Bj is weakly consistent between program points i and k, where y < i < k, for constraints Cl and program \nP if Bj is consistent with panswersp (Bi+l,k, 0 ) for each $ c @ and 9 ~ answersp (Bl,j_l A Bj+l,i, 0) \nsuch that ~(0 A Bj). 0 Analogously to the optimization for removal, we must be careful to ensure that \nthe reordering do not interfere. Again we make use of the skeleton. However this time we require that \nthe reordering is correct for the convex closure of the calls encountered using the original program \nand its skeleton. We define the convex closure, (convex G), of a set of constraints El to be {ol~e , \ne ce. e <e <} }. Definition. Let S be a set of states and P a program with skeleton p~h~. We obtain \na reordering optimization of P for S by taking each clause H + B from P and repeatedly rewriting it as \nfollows. If primitive constraint Bj is consistent between pro\u00adgram points j and k for convex (call P.k, \nH S) U (call P H S) and P then we rewrite the clause to: H + Bl,j l A Bj+l,k A Bj A B~+-~,n. If literal \nB1 = rem(Bj ) and Bj is weakly consistent be\u00adtween program points i and 1 1 for convex (call P.ke H \nS) U (call P H S) and P then we rewrite the clause to H + Bl,i A ~em(Bj) A Bi+l,l_l A Bt+l,n.o Theorem \n4.3 (Correctness of reordering optimization). Let P be a reordering optimization of program P for states \nS. For all s c S, answerp s = answerpt s. Fur\u00adthermore, there is an isomorphism between (derivp~ s) and \n( derivp s) in which Iengt h is preserved.  5 Analyses In this section we present analyses to support \nthe opti\u00admization given in Section 4. These analyses are formal\u00adized in terms of abstract interpretation \nand are generic in the descriptions used for the constraints. The exact descriptions chosen depend on \nthe underlying constraint domain. Here we give descriptions and analyses for the case that the underlying \nconstraint domain is, Lin, the linear arithmetic equalities and inequalities. This do\u00admain is simple, \nyet still non-trivial to analyze as the programs over it (CLP(Lin) programs) are Turing com\u00adplete. Furthermore \nLin is a significant subset of the constraints used in all general-purpose CLP languages and so any analysis \nof programs in these languages must handle such constraints. 5.1 Abstract Interpretation In abstract \ninterpretation [1] an analysis is formal\u00adized as a non-standard interpretation of the data types and \nfunctions over those types. Correctness of the analysis with respect to the standard interpretation is \nargued by providing an approximation relation which holds whenever an element in a non-standard domain \ndescribes an element in the corresponding standard do\u00admain. We define the approximation relation in terms \nof an abstraction function which maps elements in in the standard domain to their best description. Definition. \n~ description (D, Q, E) consists of a de\u00adscription domain (a complete lattice) D, a data domain (a complete \nlattice) E, and a strict ind continuous ab\u00adstraction function a :E *D. We say that d a-approximates e, \nwritten d cxa e, iff a e < d, The approximation relation is lifted to func\u00adtions as follows. Let (Dl, \nq, El) and (Dz, a2, ~) be descriptions, and F : D1 + D2 and F : El + &#38; be functions. Then F cc F \niff VdED1. Vec E1. dual ea(Fd)cca2(F e). We lift to predicates by taking the convention that Bool is \nordered by true < false and that the description as\u00adsociated with Bool ~s (Bool, Id, Bool) where Id = \nA b.b. This means that approximation on predicates is conser\u00advative in the sense that it gives information \nabout things which are definitely true. When clear from the context we say that d approximates e and \nwrite d N e and we will sometimes let D denote both the description and the description domain. In our \nanalyses we will be concerned with describ\u00ading sets of constraints. Such a description is called a constraint \ndescription. We now give three example con\u00adstraint descriptions for the powerset of Lin constraints, \np Lin. These can be used with the generic analyses developed in the next section to analyze CLP(Lin) \npro\u00adgrams. The first description is the convex hull descrip\u00ad tion, which is based on descriptions used \nby Cousot and Halbwachs [3] for bounds analysis in conventional pro\u00ad gramming languages. Definition. \nThe convex hull description (CHU1l, QC~ull, pLin) is defined as follows. The description domain CHUU \nC Lin consists of all sets of linear constraints of the form s<s ands= s , It is ordered by logical implication. \nThe abstraction function ~CHUll : p Lin + CHull is defined by CYCHuUH = convex-hull {loosen 6 / 66 e} \n where (convex-hull G) is a set of constraints represent\u00ading the convex hull of the polytopes 6 c El \nand (loosen d) returns a set of constraints in which strict inequalities are relaxed to non-strict inequalities. \n0 Sign descriptions are a novel constraint description in which the actual coefficients in the linear \nconstraints are abstracted by their sign , Definition. The sign description (LSign, CYLSign, p Lin) \nis defined as follows. The description domain LSign consists of sets of linear equality and inequality \ncon\u00adstraints in which the coefficients and constants are ele\u00adments of Sign = {@, O, e, T}. The domain \nis finite for a fixed finite set of variables. The abstraction function &#38;LS:~n : p Lin + LSign is \ndefined by @Ls@ @= {(6%fJTZ 8) I 8 ~ ~} where, if (sign c) returns the sign of the coefficient c and \nOis Let P be a program and A Cons a constraint description. The answer semantics for P and AC ons has \nsemantic functions ans.lit P : (Atom+ Prim+ Rem) + ACons -+ AC ons ans.clap : Clause ~ A Cons -+ A Cons \nans.goalp : Goal + A Cons -+ A Cons. The semantic equations are ans_litp A II = U{ans.clap C H) \\ C \n, (~efw A {})}when A c Atom ans_lit p 6 II = (Aadd %II) when 8 E Prim ans_litP rem(f3) II = (Aremove \n6 II) when rem(0) E Rem ans_clap (A +--B) II ~ ~meet II (Arestrict (wars A) (ans_goalp B (Arestrict (w.ms \nA) II)))) ans_goalP nil II ans_goalP A : B II = ans.goalp B (ans_litp A II). where the functions Aadd, \nAmeet, Arestrict and Arernove are required to approximate add, meet, restn ct and remove respectively. \nFigure 1: The answer semantics for P and A Cons < f-%= C1. zl+... +cxnxn {} <  then ( Csign 8) is < \n(sign c-o) = (sign cl). q + ... + (sign cn) . zn.O {} < Elements of LSign can be used to determine when \nsets of constraints are definitely satisfiable. Thus LSign acts as a kind of definite degrees of freedom \nanal\u00adysis. Manipulation of LSign is performed with an abstract Fourier algorithm which mimics the manip\u00adulation \nof Fourier s algorithm on concrete linear con\u00adst raints. Fourier s algorithm works by repeatedly elim\u00adinat \ning variables, much like Gaussian elimination, until no variables are left. The original system is satisfiable \niff the final system is. As an example of the abstract Fourier algorithm s execution we show how it can \nbe used to prove that the constraints {z ~ O,y + z = 32, z > 4, y ~ 3} are sat isfiable. The algorithm \nstarts with the description It first eliminates z to obtain {@220, 0y+@z>0, Then z is eliminated, giving \n{eY 2 e, e3Y2 e}. Finally v is eliminated to give {O z e} which is always satisfiable, indicating that-any \nsystem described by the original description is satisfiable. The final example is a description which \ncombines CHUU and LSign. Definition. The combination description is (LComb, ~~~.~b, pLin) where LComb \n= CHull x LSign and the abstraction function ~Lc!Omb : p Lin -+ LComb is defined by ~LComb @ = (( QCHU1l \n~), (~LSign @)).a  5.2 Refinement Refinement requires us to find primitive constraints which are redundant \nwith respect to the answers of a state. This may be done by choosing a suitable subset DCons of the constraints \nCons as the description do\u00admain with an approximation relation ~DCo~# defined by n UDc.W @ iff m is redundant \nfor ~. For instance CHU1l is such a description in the case of linear constraints. These descriptions \nare then used in an analysis which finds an approximation to the answers of the state. By construction \nof the description and correctness of the analysis, any primitive constraint in the approximation must \nbe redundant with respect to the state s answers. The generic problem of finding a description for the \nanswers of a state has been addressed by Marriott and S@ndergaard [15] in the case of monotonic programs. \nThe semantic equations given in Figure 1 area straight\u00adforward modification of those given in [15] and \ncan be used as the basis for such analyses. The semantic equa\u00adtions make use of the functions add which \nadds a prim\u00aditive constraint to a set of const raints, meet which adds a set of eauat ions, restrict \nwhich restricts a set of con\u00adstraints to a variable set and remove which removes a primitive constraint \nfrom a set of constraints. They are defined by: The answer semantics is best understood by consid\u00adering \nthe equations obtained by replacing the abstract Let P be a monotonic program and A Cons a constraint \ndescription. The definitely redundant semantics for P and A Cons has semantic functions red-lit p : Prim \n+ p Var ~ (Atom + Prim + Rem) ~ ACons 4 Bool red.clap : Prim -+ p Var ~ Clause -+ ACons ~ Bool red-goalp \n: Prim * p Var ~ Goal 4 A Cons * Bool. The semantic equations are red-lit p 9 W A II = A{(red-clap 8 \nW CII) I C 6 (defip A (vars 6U W))},A E Atom red_litp 6 W 19 II = (Ared O W(Aadd0 II))when0 6 Prim con.litp \ne W rem(#) II = false when rem(d ) c Rem red_claP O W (A t B) II = (red.goalp O W B (Arestrict (vars \nA U vars O U W) II)) red-goalp 6 W nil II = false red.goalP 8 W A : B II = (red-litP O W A II) V (red_goalP \n13W B (ans.litp A II)). where A add, A restrict and A red are required to approximate add, restrict and \nred respectively. Figure 2: The definitely redundant semantics for P and A Cons functions Aadd, Ameet, \nArestrict and Aremove by add, !5.3 Removal meet, restrict and remove respectively and A Cons by To perform \nthe removal optimization we must findp Cons. In this case we can show that when a primitive constraint \nis definitely W-redundant V9 c @ . (ans-goalP G@) ~ answersP(G, 6). for a set of goals. The analysis \ngiven here is only for monotonic p;ograms. In our-cont;xt this is not a limi- Using results from abstract \ninterpretation theory we tation as it will only be used to analyze programs whichtherefore have that: \nare monotonic. This reason for our restriction is that Theorem 5.1 If II m Cl, it allows us to make the \nanalysis more efficient, be\u00adcause monotonicity ensures that once the constraint isV19 c (3. (ans_goalP \nG II) u answersp(G, 0). W-redundant on a derivation it will remain so. Thus Note that in the semantic \nequations when finding when the analysis finds a constraint is definitely W\u00ad the answers to an atom, \nthe current set of abstract con\u00adredundant, it terminates with true. The semantic st raints is restricted \nto the variables in the atom. On equations for the analysis are shown in Figure 2. They return the lost \ninformation is regained by meeting make use of the function the answer constraints with the call constraints. \nThis is important, as it means that if these equations are inter\u00adred 0 W 0 + 0 is W-redundant for ~. \npreted using a memorization approach [5], then they give a terminating analysis whenever the constraint \ndescrip\u00adtion domain has finite height. This is because for any Theorem 5.2 Let P be a monotonic program. \nIf II u goal G and description II, (ans_goalP G H) depends Cl and (red-goalP O W G II) holds, then 6 \nis W\u00adon only a finite number of calls to ans_goal, modulo redundant for answerp (G, 0 ) for any 0 6 @. \n_ variable renaming. If the description domain does not Consider the program for sum, given in Section \n2. have finite height, then widening fnarrowing techniques Using CHUU as the description domain, we \ncan use this[2] may be used to ensure termination. Similar com\u00adanalysis to show that the removal optimizations \nin thements will apply to the other analyses we introduce. example are allowed. Consider P the program \nfor sum, given in Section 2. Using CHU1l as the description domain, we can show that 5.4 Reordering \nans_goalp sum(lVl, S1) true = AU z 0 A S1 ~ IV1. To reorder addition and removal of constraints we must \ndetermine if a primitive constraint is definitely consis- This information allows the refinement optimization \ntent for a goal and set of calls. The analysis is onlygiven in the example. for quasi-monotonic programs, \nas similarly to the re-It is straightforward to modify the answer seman\u00addundancy analysis, the analysis \nuse redundancy infor\u00adtics so that it returns descriptions of the calls made to mation to terminate the \nanalysis with success because each atom for a particular set of goals. Intuitively we redundancy implies \ndefinite consistency. The new se\u00adneed only to change the first semantic equation in Figure mantic function \nof importance is the consistency test 1 so it collects the abstract call (Arestrict (vars A) II) for \nA. cons 8 @ # O is consistent with ~. Let P be a program and A Cons a constraint description. The definitely \nconsistent semantics for P and A Cons haa semantic functions: con_litp : Prim ~ (Atom+ Prim + Rem) ~ \nACons ~ Bool con.cla~ : Prim ~ Clause b A Cons d Bool con goalp : Prim ~ Goal ~ A Cons d Bool. The semantic \nequations are con.litp O A II = ~{(con-cla~ O C H) I C c (defw A (vain 0))} when A ~ Atom con_litp 0 \n@ II = (Aeons O (Aadd 0 II)) when 0 ~ Prim con-lit p 0 rem(#) II = true when rem(6 ) E Rem con-clap 6 \n(A t B) II = (con_goalP O B (~estrict (wars A U vars 0) II)) con-goalp O nil II = true con_goalP 8 A \n: 1? II = (Ared O II) V ((con_litp 6 A II) A (con_goalp 0 B (ans-litp A II))). where A add, A restrict, \nA red and Aeons are required to approximate add, restrict, red and cons respectively. Figure 3: The definitely \nconsistent semantics for P and A Cons The semantic eauations for the analvsis are shown in Consider the \nfollowiruz momam MG that relates ~a\u00ad . . Figure 3. Corre&#38;ness of the analysis requires that the \nrameters in a mortgage program is quasi-monotonic for all derivations encoun\u00ad mg(P, T, I, R, B) +- (MG)tered \nin the analysis. In our context this is not a prob\u00adT=l Alem as the analysis will only be applied to programs \nB=P* (1 + 1/1200) -R. resulting from the removal optimization, and they have mg(P, T, I, R,B) +the necessary \nproperty. T>=2A Theorem 5.3 Let El be a set of constraints and G a T1=T-l A goal. Let P be a program \nwhich is quasi-monotonic Pl=P*(l+I/1200)-RA for the states S = {(G,O )I0 c ~}. If II a @and P1>=OA (cons_goalP \n0 G II) holds, then O is consistent with mg(Pl, Tl, I, R, B). panswersp s for any state s c S. The first \nclause of the program states that for a loan Now, of length T = 1 month, the balance owing, B, is given \nby the principal, P, plus the addition of the interest cons 0 @ + (satis O@) V (red O~) (calculated from \nthe annual interest rate I) minus the where repayment R. The second clause states for loans of length \ngreater than or equal to 2 months, the new principal is the old principal plus interest minus repayment, \nthe new principal must be non-negative, and then what remains This observation leads to an analysis in \nwhich the set of is a loan with new principal and one less month. constraint descriptions is LComb and \nAeons is defined by We consider the optimization of this program with respect to calls in which interest, \nrepayment and bal- Aeons 8 (7TCHU11, rLSzgn ) * ance are non-negative. That is we are interested in calls \n(Asatis O T.L.Sign) V (Ared 6 TCHW) described by n, the convex hull description where Asatis and Ared \napproximate satis and red re-I> OAR> OAB>O. spectively. The definition of Asatis for the domain LSign \nis defined in terms of the abstract Fourier al-Clearly this is the most common use of the program. gorithm. \nUsing CHull we can determine that ans.goalP mg(P, 2 ,1, R, B) T =   Another Example T21AP20A 120 AR20AB \n20. The example program used in Section 2 is deliberately simple to illustrate the central ideas without \ndifficulty. This allows us to refine the program by adding con-Correspondingly the advantages of the \nanalysis are not straints before the recursive call to mg. Each of the new enormous in this case because \nthe interaction between constraints added is immediately redundant either with the constraints of the \nprogram is limited. In this section respect to the calling pattern or to the constraints in we examine \na more complex program that illustrates the clause. Hence refinement in this case produces no the effect \nof the optimizations where there is significant new constraints. Using the above knowledge about an\u00adinteraction \nbetween constraints. swers it is easy to determine that T >2 and P 1>0 are Query qn MG SQn Speed Up SQ* \nSQ* Speed Up ql 0.129 0.129 1.00 0.129 1.00 q2 5.434 0.176 30.88 0.324 16.77 q3 6.279 0.188 33.40 0.362 \n17.34 q4 !3.166 0.252 36.37 2.882 3.18 Table 2: Timings for the Optimized MG Program both redundant after \nthe recursive call, and hence can be removed. After removal optimization the resulting program is mg(~, \nT, I, R, B) ~ (RBM) : : ; : (1 +1/1200) -R. mg(P, T, I, R,B)t T>=2A TI=T-l A PI =P* (1 +1/1200) -RA \nP1>=OA mg(Pl, Tl, I, R, B) A rem(Pl >= O) A rem(T >= 2). We now consider four different types of calls, \neach implyingl ~OAR >OAB ~ O,andreorder the program separately for each (ql) ?-B >= O A mg(100000,360,12,1025, \nB). (q2) ?-mg(P,360,12,1025,12625.9). (q3) ?-R> OAB>=O Amg(P,360,12,R,B). (q4) ?-B>=OA Bt= 1030 A mg(10000O, \nT, 12, 1030, B). We shall concentration q4 since it is where the most reordering is possible. Because \nin every call to mg T is either unconstrained or bounded from below, the analysis determines that the \nconstraint T ~ 2 is always satisfiable when reached, hence the constraint can be moved laterin the computation. \nIt doesnot af\u00adfectsatisfiability ofanyof the constraints Tl= T-1, PI == P*(l+J/1200) R, and P1 20 and \nis consistent with therecursive call because it is made redundant im\u00admediately. Thus it can be moved \npast the rem(T 22) literal and both such literals can berernoved. Similarly the constraint T1 = T-1 is \nconsistent when reached for all calls. Again itisconsistent across the goal P1 = P*(l+l/1200) R,Pl ~ \nO,mg(Pl, Tl,l, R,B) and so it can be moved after the recursive call. The constraint.pl ~Oisnot always \nsatisfiable when reached but it is weakly consistent across the recursive call, so its removal can be \nmoved before the recursive call. The resulting program is mg(P, T, I, R, B) -(SQ4) B= P* (1 + 1/1200) \n-RA T=l. mg(P, T, I, R,B)+ P1=P* (1 +1/1200) -RA PI>=OA rem(Pl >= O) A mg(Pl, Tl, I, R, B) A T1 =T -1. \n For ql the analysis determines that the literals rem(T z 2) and rem(Pl z O) can be moved just after \ntheir respective constraints. Inthis case because Tand P1 are ground when the constraints are reached \nthere is no benefit in removing them as they will be executed as Boolean tests. In the case of q2 and \nq3 the analysis determines that P1 Z O is always consistent and so can be removed altogether. The literal \nrem( T z 2) can be moved just after the constraint T z 2 but again there is no benefit since T is ground. \nTable 2 compares the execution time of the original program MG, with each of the specialized programs \nSQ1, SQ2, SQ3, SQ4, and with the program SQ* resulting from specializing with respect to all four queries \nsimul\u00adtaneously. In fact SQ* is just SQ1. Timings are CPU seconds for execution using CLP(7?) V1. 1 on \na Sparc-Station 2. In this case we ignore the low-level optimiza\u00adtion since their effect is swamped by \nthe effects of the high-level optimization. The table shows order of mag\u00adnitude improvements in the execution \nof all but the first query. However this is the query which would have been most helped by low-level \noptimization. It is interesting to note that the program SQ* resulting from optimiz\u00ading with respect \nto all calls is obviously not aa efficient as the individual optimized programs but it still shows order \nof magnitude improvements on the original. This indicates that, in this example, most oft he speedup \nhaa resulted from synergy between removal and reordering which has allowed the removal of a constraint \nbefore an iterative call. 7 Conclusion The combined optimization and analyses give a pow\u00aderful methodology \nfor optimizing constraint logic pro\u00adgrams based on reordering and removal of useless con\u00adstraints. The \noptimization have significant practical importance as they are automatizable and test results show they \ncan give order of magnitude improvements in execution speed. Furthermore, though the specific details \nof the optimization and analyses are for CLP languages, we feel that the underlying ideas of removing \nconstraints when they become redundant and reorder\u00ading of constraint addition and removal when this does \nnot effect control flow, are applicable to the compilation of other constraint based programming languages. \n[1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14] P. Cousot and R. Cousot. Abstract Interpretation: \na Unified Lattice Model for Static Analysis of Pro\u00adgrams by Construction or Approximation of Fix\u00adpoints. \nIn Proc. Fourth Ann. ACM Symp. Prin\u00adciples of Programming Languages, pages 238 252. Los Angeles, California, \n1977. P. Cousot and R. Cousot. Systematic Design of Program Analysis Frameworks. In Proc. Sixth Ann. \nACM Symp. Principles of Programming Lan\u00adguages, pages 269-282. San Antonio, Texas, 1979. P. Cousot and \nN. Halbwachs. Automatic Discovery of Linear Restraints among Variables of a Program. In Fifth ACM Symp. \non Principles of Programming Languages, 84-96, 1978. J.D. Darlington, Y.G. Guo, and H.P. Pull. A New \nPerspective on Integrating Functional and Logic Languages. In Procs. Fifth Generation Computer Systems \n1992, Tokyo, Vol. 2, 682-693. June 1992. S. K. Debray and D. S. Warren. Functional Com\u00adputations in \nLogic Programs. ACM Transactions on Programming Languages and Systems 11 (3): 451-481, 1989.  D.B. Kemp, \nK. Ramamohanarao, I. Balbin, and K. Meenakshi. Propagating constraints in recursive deductive databases. \nIn E. Lusk and R. Overbeek, editors, Proc. First North American Conj. on Logic Programming, 981-998, \nCleveland, October 1989.  M.M. Gorlick, C.F. Kesselman, D.A. Marotta, and D.S. Parker. Mockingbird: \na Logical Methodology for Testing. Journal of Logic Programming 8:95\u00ad119, 1990. J. Jaffar and J.-L. \nLassez. Constraint Logic Programming. In Proc. Fourteenth Ann. ACM Symp. Principles of Programming Languages, \n111\u00ad  119. San Francisco, California, 1987. J. Jaffar, S. Michaylov, P. Stuckey, and R. Yap. The CLP(73) \nLanguage and System. ACM Trans\u00adactions on Programming Languages and Systems, 14(3), 339-395, July 1992. \nJ. Jaffar, S. Michaylov, P. Stuckey, and R. Yap. An Abstract Machine for CLP(7?). In Proc ACM SIGPLAN \nConf. on Programming Language De\u00adsign and Implementation, 128 139, San Francisco, June 1992. N. Jorgensen, \nK. Marriott, and S. Michaylov. Some Global Compile-Time Optimization for CLP(Z). In V. Saraswat and K. \nUeda, editors, Proc. 1991 Int. Symp on Logic Programming 420 434, San Diego, October 1991. P.C. Kanellakis, \nG.M. Kuper, and P. Revesz. Con\u00adstraint Query Languages. In Proc. ACM Symp. on Principles of Database \nSystems, 299 313, Nashville, April 1990. C. Lassez, K. McAloon, and R. Yap. Constraint Logic Programming \nand Options Trading. IEEE Expert 2:42-50, 1987. K. Marriott, L. Naish and J.-L. Lassez. Most Spe\u00adcific \nLogic Programs. In R. Kowalski and K. Bowen,  editors, Logic Programming: Proc. Fifth Int. Conf. Symp. \nMIT Press, 1988. [15] K. Marriott and H. S@ndergaard. Analysis of Con\u00adstraint Logic Programs. In Proc. \nof the North American Conf. on Logic Programming, 521-540, Austin, October 1990. [16] V.J, Saraswat. \nConcurrent Constraint Program\u00adming Languages. Ph.D. Thesis, CMU. 1989. Also in ACM Distinguished Dissertation \nSeries. [17] T. Sato and H. Tamaki. Enumeration of Success Patterns in Logic Programs. Theoretical Computer \nScience, 34:227-240, 1984. [18] H. Simonis and M. Dincbas. Using an Extended Prolog for Digital Circuit \nDesign. In IEEE Interna\u00adtional Workshop on AI Applications to CAD Sys\u00adtems for Electronics, 165 188, \nMunich, October 1987. [19] G.L. Steele and G.J. Sussman. Constraints. In Procs APL79, ACM SIG-PLAN STAPL \nAPL Quote Quad, 9:208-225, June 1979.  \n\t\t\t", "proc_id": "158511", "abstract": "<p>Central to constraint logic programming (CLP) languages is the notion of a global constraint solver which is queried to direct execution and to which constraints are monotonically added. We present a methodology for use in the compilation of CLP languages which is designed to reduce the overhead of the global constraint solver. This methododology is based on three optimizations. The first, refinement, involves adding new constraints, which in effect make information available earlier in the computation, guiding subsequent execution away from unprofitable choices. The second, removal, involves eliminating constraints from the solver when they are redundant. The last, reordering, involves moving constraint addition later and constraint removal earlier in the computation. Determining the applicability of each optimization requires sophisticated global analysis. These analyses are based on abstract interpretation and provide information about potential and definite interaction between constraints.</p>", "authors": [{"name": "Kimball G. Marriott", "author_profile_id": "81452613078", "affiliation": "", "person_id": "P161705", "email_address": "", "orcid_id": ""}, {"name": "Peter J. Stuckey", "author_profile_id": "81100133272", "affiliation": "", "person_id": "PP14057424", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/158511.158685", "year": "1993", "article_id": "158685", "conference": "POPL", "title": "The 3 R's of optimizing constraint logic programs: refinement, removal and reordering", "url": "http://dl.acm.org/citation.cfm?id=158685"}