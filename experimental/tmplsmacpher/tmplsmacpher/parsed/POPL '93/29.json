{"article_publication_date": "03-01-1993", "fulltext": "\n Layer Sharing : an improved Structure Sharing Framework E. Villemonte de la Clergerie * INRIA Rocquencourt \n-BP 105 78153 LE CHESNAY France Eric. Clergerie@inria.fr Abstract We present in this paper a structure \nsharing framework originally developed for a Dynamic Programming inter\u00adpreter of Logic Programs called \nDyALog. This mech\u00adanism should be of interest for alternative execution models of PROLOG which maintain \nmultiple compu\u00adtation branches and reuse sub computations in vari\u00adous contexts (computation sharing). \nThis category in\u00adcludes, besides our Dynamic Programming model, the tabular models (OLDT, SLDAL, XWAM), \nthe magic\u00adset models, and the independent AND and OR par\u00adallelism with solution sharing models. These \nmodels raise the problem of storing vast amount of data, mo\u00adtivating us to discard copying mechanisms \nin favor of structure sharkg mechanisms. Unfortunately, compu\u00adtation sharing requires joining computation \nbranches and possibly renaming some variables, which generally leads to complex structure sharing mechanisms. \nThe proposed layer-sharing framework succeeds however in remaining understandable and easy to implement. \n Introduction In the last few years, some alternative models have been proposed for extending or replacing \nthe standard WAM based implementation of PROLOG. For instance, one may cite the parallel models (independent \nAND parallelism[DeG84, HG90], OR parallelism[War87]), the tabular models (OLDT[TS86], SLDAL[Vie87], XWAM[War89]) \nor the magic-set models[BMSU86, Sek89, UOKT84]. These models dif\u00ad This work has been partially supported \nby the Eureka Soft\u00adware Factory (ES F) project. Permission to copy without fee all or part of this material \nis granted provided that the copies are not made or distributed for direct commercial advantage, the \nACM copyright notice and the title of the publication and its dste appear, and notice is given that copying \nis by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires \na fee and/or specific permission. ACM-20th PoPL-1193-S.C., USA @ 1993 ACM 0-89791 -561 -5/93 /0001 /0345 \n. ..$1 .50 fer from the original one essentially in that they si\u00admultaneously maintain several computation \nbranches instead of only one. This implies in particular that a variable may have multiple bindings in \nthe different branches, leading to a more complex management of environments[GJ90]. Another problem raised \nby these alternative models is the potentially vast number of terms one has to track between the different \ncompu\u00adtations. Copy mechanisms may become inefficient in space, and also in time due to the size of the \ncopied terms. Lastly, some of what we call non input models (tabular models, magic-set models and independent \nAND and OR parallel [iAO] models[SH91, GSCYH91]) try to reuse computed answers in different contexts \nin order to gain computation sharing. This adds to the aforementioned problems the task of joining two \ncomputation paths (the current one and the reused one), which essentially implies merging two environ\u00adments \nand perhaps renaming to avoid variable name clashes. Although the necessity of a structure sharing framework \nappears inevitable, no satisfactory imple\u00admentation that copes with the join and renaming op\u00aderation \nhas been proposed. For instance, Boyer and Moore s framework [BM72] suffers from large time over\u00adhead \nand Pereira s framework [Per85] does not address the renaming problem. We have encountered these various \nproblems during the development of our Dynamic Programming evalua\u00adtor of Logic Programs : DyALog [VdlC90]. \nTo solve them, we have developed a structure sharing frame\u00adwork called layer sharing. This framework \nis conceptu\u00adally simple, allows multiple environment management, and cleanly performs joining with renaming. \nWe access and bind variables in constant time. Task switching depends on environment sizes, but these \nsizes can be, in most of cases, kept reasonable by removing unnec\u00adessary bindings in the environments \nwith the collapse operation. Some preliminary results, done with a prototype im\u00adplementation, show that \ngains in space can be im\u00adport ant for programs which manipulate sizeable non ground data. Time overhead \nis minimal and largely A non input model has been proposed for inde\u00adcompensated for by the gains achieved \nby avoiding term pendent AND-parallelism and OR-parallelism (iAO) copying and by using structure sharing \nto induce com-models[GSCYH91] which try to explore in parallel al\u00adputation sharing. ternative clauses \nand independent goals (goal with no In Section 2, we explain our design choices in light common variables). \nThe answers to a sequence G1, G2 of execution models and structure sharing frameworks. of independent \ngoals are obtained by joining (by cross We also informally introduce the Layer Sharing frame-product) \nthe answers to G1 and Gz. work. A succinct formalization of the framework is However, non input models \nmainly include mod\u00adgiven in Section 3. An implementation is described in els developed to handle the \nincompleteness and ter-Section 4. Some results are provided and analyzed in mination problems of PROLOG. \nWe may cite the Appendix A. tabular models[TS86, Vie87], the magic-set like models[BMSU86] and our Dynamic \nProgramming (DP) model[Lan88, BVdlC92]. 2 Motivations Tabular models use a goal-indexed look up table \nto store computed answers. When a goal is created which 2.1 Non input models matches an existing entry, \nthe present, aa well as the future answers attached to the entry will be reused in-The last few years \nhave seen the emergence of what stead of starting a new computation. The match test we call the non input \nexecution models~ for Logic Pro\u00ad may be an identity test (for ground computations), a gramming. Figure \n1 will help us to introduce this no\u00ad variant test, or a subsumption test (the new goal is tion. + an \ninstance of the tabulated goal). In the last two cases, the tabulated answers must be adapted (by re\u00adnaming \nor instantiation) to the new goal. Magic set . %+. models achieve a bott em up evaluation of transformed \nprograms where prediction (i.e., top down) information ,..%,.%. ... .. . :. has been added to restrict \nfact propagation. , ... ..+.. .., Our DP interpreter DyALog is relatively close to ... %. ... ... %,.. \nmagic set models2. We use Logical Push Down M $ Automata[Lan88] to describe non-deterministic stack sg:mlt$ll \nParallel No##:t Models based computations on Herbrand domains. We are able to simulate SLD, Bottom Up, \nor Magic Set resolution mechanisms, but also more exotic ones. The DP in\u00ad Active Computation ~.. Waiting \nComputation terpretation of these LPDA breaks stack computations -Computation Jhing into sub computations \ncompactly represented by first order terms called items. The items may be combined to retrieve all general \ncomputations and are reusable Figure 1: Some execution principles in different contexts. The control \nalgorithm is similar to a fix-point computation with a subsumption mech\u00adanism : items are combined until \nno new items can be In the standard sequential model of PROLOG, a added. An item can be added if it is \nnot an instance of stack is used to store alternative waiting computations a previously computed item. \n(choice points). The use of a stack is p&#38;sibl~ because waiting computations form a branch and each \nvariable is at most bound once in this computation branch. OR 2.2 Copying vs Sharing ? parallel models, \non the other hand, explore several com\u00adputation branches at the same time ; a variable may WAM based \nPROLOG compilers mix structure copy\u00adtherefore be bound in several branches. Non input ing with more or \nless sophisticated forms of structure models are parallel models which enable the merging sharing. All \nof them copy clauses before each use to of several computation branches into one. This Kind rename them. \nOn the other hand, the bindings cre\u00adof model naturally arises in the context of computation ated by unification \nare not applied and term values are sharing when one wishes to reuse a previously computed obtained by \ndereferencing the variables through these result in a new context. The first example of non input bindings. \nIn sequential models, the value of a variable models is the bottom up model where computed facts is generally \ndirectly accessible through the variable. In are stored in a database and used to generate new facts. \nparallel models, a variable can have several bindings at zbut ~ little ~ore POwerful, less magic , 1by \nanalogy to the notion of non input resolutions used in and more theorem proving. understandable. the \nsame time and direct access to a single value is no longer possible. The most common solution is to store \nthe bindings in environments. Access becomes indirect because an entry for a variable must be first retrieved \nin the environment to find the associated value. The most popular structures for encoding environment \nare Binding Arrays[War84, War87]. This scheme incorrect as long as each variable is bounded at most once \nin each environment. Unfortunately, this is no longer the case for non input models where the same variable \nidentifier may denote two distinct logical variables in branches to be joined. After joining, we have \na unique variable iden\u00adtifier for two distinct variables. This problem has been called the renaming problem \nby[Per85]. A first solution consists in renaming one of the two variables (by introducing a new variable \nidentifier) which implies to copy the terms which contain the vari\u00adable to be renamed [Per85, War89]. \nThis solution is not really satisfactory, because of the potentially large amount of joining (this is \nthe case for DyALog) and the size of the data3. Furthermore, computations can\u00adnot be as easily discarded \nas in parallel models, because they may have to wait (eventually an infinite amount of time) for other \ncomputations to be joined. Thus, the number of waiting computations can be quite large, which implies \na great deal of terms to store. The second solution to the renaming problem is to add context information \nto the variable identifiers in the form of keys to differentiate them. Binding access now depends on \nkeys : we call this contextual. This solution is used in the foundation paper on Structure Sharing for \nLogic Programming[BM72]. The authors use inte\u00adgers as keys and environments are just lists of bindings \nof the form Xok w s&#38;l where k and k) are keys. Such a binding will only work for a variable X with \nkey k and will ret urn term s with key k 4. Unfortunate ely, variable access is too slow to make Boyer \nand Moore s formal\u00adism tractable. They use open environments linked in a graph structure which reflects \nthe different computation merges. Several environments have to been examined to dereference a variable, \nand the number of examined environments grows with the number of computation merges. Pereira has proposed \nunlinked closed environments which can be implemented to provide very rapid access to bindings. However, \njoining two environments with renaming to form a new closed environment generally implies rebuilding \nsome (significant ) parts oft he parent environments (because some keys in the bindings have to be modified \nto do the renaming). This rebuilding is 3For instance, we would like to use DyALog to execute natu\u00adral \nsemantics of programming languages [KahS71 where data are program fragments which can be quite large. \nIt k not realistic to copy them for each computation. 4This key k will then be propagated to the variables \nof s. 41 -A p(X,Y,Z)_O= p(f(r(a)),y_@n(x-2)) Figure 2: An example of environment costly in both time \nand space due to the large number of bindings created, The solution we propose is to generalize Boyer \nand Moore s notion of bindings : we interpret keys as the states of a deterministic automaton A and we \nlabel the bindings by actions of A. The dereferencing of a vari\u00adable .Xok through the binding X &#38; \ntreturns the term toht iff k --% k is a valid transition of A5. This way, the same binding can then be \nused for different keys. The Layer Sharing framework we introduce in this pa\u00adper is based on these remarks. \n 2.3 Layer Sharing Our Layer-Sharing framework was designed to handle simply the task of joining environments \nwith renam\u00ading. Access to bindings is contextual, and to accelerate access, we use closed environments. \nOur keys are just integers. ,. Environments are finite and ordered, bundles of Zay\u00ad ers. The kth layer \nis a table used to collect all bindings associated to key ,%. A binding has the form X L t where 6 is \na positive or negative integer which repre\u00adsents the relative shift of the access key : if the bind\u00ad \ning X &#38; t is present on layer k, then the value of variable Xoh is the term tok+&#38; Figure 2 shows \nan environment A : the value obtained by dereferencing . . term P(X, Y, 2)00 with the bindings of A is \nthe term P(~(r(a)), YOO,m(Xo2)). The use of relative shifts in bindings allows us to reuse bindings in \ndMferent environments for different access keys. Indeed, what is important is not the abso\u00adlute position \nof a layer in an environment but its posi\u00adtion relative to other layers. In particular, joining two environments \nA and B with renaming is immediately achieved by concatenating the two environments into 5This approach \nof structur~sharing is partially inspired by[Kat90, CHL92] where substitutions are represented by opera\u00adtors \nwhich forms a calculus. B> t _k -->cAxB> t _(k +lAl) 1 1 Figure 3: Environment join an environment A \n@B, as shown in Fig. 3. Terms of A remain unchanged in ABB, while terms of B are shifted by the number \nof layers of A, which corresponds to a renaming. Thus, at least for read access, we may im\u00admediately \njoin several environments by reusing layers (and their bindings). We only need to build a new set of \nslots to store the layers in the resulting environment. Now, when a binding or a set of bindings must \nbe added to (or merged with) an environment (during uni\u00adfication, for instance), we must be careful not \nto mod\u00adify other environments. A naive solution would be to copy the layers where the bindings are inserted. \nIt is more economical to implement layers so as to preserve as much of the original layer. Unfortunately, \nthe concatenation of environments tends to create very big environments (their size can be exponential \nin the number of joins). However, in prac\u00adtice, environment sizes may be kept quite reasonable by removing \nbindings and empty layers : indeed, some bindings or variables useful during computation may subsequently \nbecome useless and may be (dispensed with . That is the role of the collapsing algorithm. The following \ntwo sections develop-these different ideas.  3 Formalization 3.1 Preliminaries We assume the reader \nis familiar with the notion of a free algebra T(2.1, V) over a graded set of functors Z and a set of \nvariables V. Variables are denoted by uppercase letters. The set of all variables present in a term t \nis denoted Var(t). Substitutions are T(X, V) morphisms induced by finite mappings from V to T(Z, V) and \nthe order s over Z (E, V) is defined by t s r iff there exists a substitution u such that t~ = r. Term \nunification is an important issue we will have to consider, but we carefully distinguish two (close) \nnotions of unification. Two terms tand r are unifiable iff there exists a substi\u00adtution v such that t~= \nrv and r unifiable iff they own a common ~ instance (3.s ~ T(Z, V), t < s A r < s). Clearly, that t and \nr are unifiable implies that t and r are r unifiable but not conversely. However, t and T r unifiable \nimplies that t and rp are unifiable iff p is a renaming substitution such that rp = r and Var(rp) U Var(t) \n=0. 3.2 Terms Throughout this paper, we will deal with a finite sub\u00adset S of an algebra 2 (S, V). Elements \nof 5 are called skeletons. We can suppose that V is finite by taking v = Var(S). We require S to be closed \nby subterm (.f($l,... ,Sn)cs+ s,,..., Sn c S). All future terms will be (pseudo) built by binding together \nskeletons through their variables. We use positive integers as con\u00adtextual keys and introduce the set \nof contextual terms S.N. Definition 3.1 SON = { s~k Isc S, k c ~ } We distinguish, in SON, the subset \nVON of contextual variables VON= { Xok IX 6 V, k EN } and useit to build the free algebra T(Z, V.N) of \nextended terms. Contextual terms are naturally embedded in T(Il, V,N) by pushing the keys at the variable \nlevel as shown by the following example b. Example 1 The contextual term ~(a, X, g(y), g(b))ok gives \nthe extended term ~(a, Xok, g(y~k), g(b)). Symbols r,s, t (resp. z, y, z) will be used to denote skeletons \n(resp. extended terms). To sum up, skeletons are the bricks, contextual terms are the manipulated terms, \nand extended terms are vir\u00adtual terms we get by dereferencing contextual terms through sharing environments. \n 3.3 Environments We now formalize our two level structured sharing en\u00advironments. The lower level, called \nthe layer, is used to collect bindings associated to a given key k and the upper level is just a finite, \nordered collection of layers. Definition 3.2 A layer L is a (jinite) mapping from V into Z x S. An element \n(X, (6, s)) of the graph oj L where (6,s) # (O, X) is called a binding and is noted X&#38;s. Layers are \nrepresented by their set of bindings. 6From a theoretical point of view, there is no need to intro\u00adduce \ncontextual terms. From an operational point of view, con\u00adtextual terms have a more compact representation \n(by a pair (skeleton, integer) )than extended terms. Definition 3.3 An environment A is a couple Definition \n3.6 Joining two environment A and B (de\u00ad (\\ A/, MA) where 1A] is a positive integer and MA a mapping \nfrom [1, /A/] to the set of layers. MA is oflen represented by a set {kl : L1 . . . kn : Ln} where we \nsup\u00adpose that all missing layers are empty. The layer asso\u00adciated to key k in A is denoted A: k. We call \nBound(A) the set { x~h I 3X&#38;s ~ A: k } of bound variables. Uppercase symbols A, B, C will denote \nenvironments. Environments have a dereferencing operator which maps contextual terms into extended terms \nand, more generally, extended terms into themselves. Definition 3.4 The dereferencing operator TA associ\u00adated \nwith an environment A is a partial function of T(Z, VO[l,\\Al])7 into T(Z, VqI,/Al]) inductive defined \n@ A few remarks on this definition : Bindings are relative : 6 means a relative shift of the context \nkey k. We have implicitly used the embedding of contex\u00adtual terms into extended terms to apply TA to \ns~k+6. Furthermore, TA naturally applies to con\u00adtextual terms via this embedding. TA is a partial function. \nSome bindings may fall outside the range of A (k -I-6 $? [1, /Al]) or there may exist a cycle. We note \nDom(TA) the domain of TA, Practically, an environment is used to access the value of a finite set of \ncontextual terms (and very often a single term). We then associate a set ET of entry terms with an environment \nA and we introduce the notion of well-founded environment. Definition 3.5 An environment A is well-founded \nwrt a set of entry terms ET ifl ET C Dom(TA). In the rest of this paper, we suppose that environ\u00adments \nare well-founded and we note Env the set of all well founded environments on X and V. 3.4 Joining We \nintroduce the environment join (8) operation : is a short notation for the subset { XOJ$ ] X G ~, k C \n7~o[l,lAl] [1, lAI] } of VON. noted A @ B) is defined by /Ac3B/ = 1A] + II?] A@ B:k=A:k k c [1, /Al] \nA@ B:[Al+k=B:k k~[l,l.B/] This join operation preserves the semantics of joined environments and ensures \nrenaming (by a key translation) in the second environment (B) to avoid variable clashes. Proposition \n3.1 Given two environments A and B resp. well founded wrt ETA and ETB, then A @ B is a well founded environment \nwrt ETA@B = ETA U { so/A/+k / sok E ETB } and there exists a renaming substitution p such that, for all \nterms s@ c ETA and rol E ETB, we have T@BS~k = TASOk T/@BrolA,+, = (T~r.z)p ~ TBrot Var(T~@~s&#38;) n \nVar(T&#38;@~lAl+Z) = @ It must be noted that joining is a very easy operation which leaves the layers \nuntouched. 3.5 Merging To add bindings to layers, we introduce the environment merge (@) operation : \nDefinition 3.7 Given two environments A and B such that Bound(A) n Bound(B) = 0 and ]A/ = ]B/, the merging \nof A and B returns the environment A @ B defined by : /A@B1 = /A1 = /B/ VkG[l, /A@ B/], A@ B:k=A:k UB:k \nUnfortunately, merging does not preserve environ\u00adment well foundedness because cycles can appear. Fur\u00adthermore, \nit is a more complex operation to implement that the join operation because it is more dependent of the \nlayer implementation. Merging will be discussed in Section 4. 3.6 Deletion and Replacement In practice, \nwe need to implement two other environ\u00adment operations which can be formalized relative to the merge \noperation. These operations are introduced here, but are not be needed for the formalization. The delete \n(e) operation is used to remove one or more bindings from an environment. Given two envi\u00adronments A and \nB, the environment AOB is the unique environment (when it exists) which verifies Eq. 1. (A@ B)@B=B (1) \nIt follows from this equation that we should have IAI = IBI and that there exists an environment C such \nthat A== C@B. The replacement (@) operation is used to replace ex\u00adisting bindings by new ones. Given \nan environment of the form A @ B and an environment C such that lA/ = lB\\ = IcI and Bound(B) = Bound(C), \nthe envi\u00adronment (A@ B)@ C is the unique object which verifies Eq. 2. (A@ B)@ C=(A@C) (2) Deletion preserves \nenvironment well foundedness. Replacement suffers from the same problems that the merge operation. 3.7 \nUnification In this section, we show how unification is performed between contextual terms. The unification \nalgorithm is given by the set of in\u00adference rules of Table. 1. The rules act on pairs (E, A) where &#38; \nis a finite set of equations r~~ = sol between contextual terms and A is an environment. A unifica\u00adtion \n({vo~ = sol}, Ain;t) succeeds if a pair (0, Af;n.z) is derivable. ~1 ({ f(n,.. ,r~)ok = f(sI,... ,S~)ol} \nut, A) ({ Tiok= S;ol Ii E[l, n] } u &#38;,A) ~2 ({s., = Xc,t} u &#38;,A) ,$~k .$? VON ({X.( = so,} u \n8,A) ~3 ({xok = -&#38;k} U:, A) (&#38;,A) ~4 ({x~k = SOI} U .5, A) X~rEA:k ({rok+~ = sol} u S,A) ~5 ({X,$k \n= So,} U ~, A) Xok ~ Bound(A) (&#38;, A @ {k: XzGks}) { Ok ~ ar(TAsO~) ({sok = Se~} U E,A) 6 ({ X.k = \nXoi IX gVar(s) }u&#38;,A) Table 1: Unification rules Rules U1 ,U2, and U3 are standard for unification. \nRule U4 implements dereferencing and Rule U5 im\u00adplements variable binding (with an occur-check test). \nRule U6 is redundant with the other rules but is useful in a structure sharing framework because the \nunifica\u00adtion of two contextual terms based on the same skeleton often occurs. It is obvious to see that \nat each step (E, A) of a derivation, we have A = Ai.it 6 AA. The well\u00adfoundedness (wrt a set of entry \nterms) of all interme\u00addiate environments is easily shown by noting that the occur check test avoids the \ncreation of cycles. Proposition 3.2 Given an environment A and contex\u00adtual terms s~k, to~ in Dom(TA), \nthen TArok and TASol are unifiable ifl the unification ({r~k = sol}, A) suc\u00adceeds and returns a pair \n(0, Af ~~al), We have then T/i f,n., rok = TAj,m~, sol = (TArok)a = (TASol)a where o is a mgu of TArok \nand TASol. To perform an r unification between terms in dis\u00adtinct environments, we only need to join \nthe initial en\u00advironments (which automatically renames the terms) and then apply the unification algorithm. \nProposition 3.3 Given two environments A and B, a term Tok G Dom(TA) and a term sol E Dom(TB), then TArOk \nand TBSO1 are r-unifiable iff the terms TA~Brok and TA@Bso~A~+-1 are unifiable. 3.8 Collapsing Environments \ngenerally contain bindings or layers which are not used to build values for their entry terms. These \nuseless bindings or layers are only scoria of old computations. We would like to dispose of them, thus \ncompacting environments (to save space when storing). That is the aim of the environment collapsing opera\u00adtion. \nThis operation relies on the notions of useful/useless variables or bindings, and enteredlun-ent ered \nlayers de\u00adfined at the bottom of Table 2. These notions are de\u00adfined wrt an environment A and a set of \nentry terms. @A useful variable is either an entry variable or in\u00adductively a variable which belongs \nto the value of a bound useful variable. A useful binding is a binding which binds a useful variable. \nA layer k is said entered if there exists at least one useful variable Xok. Table 2 also presents a set \nof collapsing rules. Some of the rules can be weakened to be operationally tractable and other rules \nstill have to be imagined. Thus, we do not claim that our system is optimal. All presented rules act \non pairs (ET, A) where A is an en\u00advironment and ET a set of entry contextual terms. The closure rule \n[Cl] reduces the binding chains, accelerating variable dereferencing. Furthermore, some bindings (e.g. \nk + 6: Y A s) may possibly become useless. ET, A@{k:X&#38;Y}EB{k+&#38;Y&#38;} [cl] ET, A@{k:X6&#38;}@{k+ \n&#38;Y&#38;s} ET, A@ {k:x~ti~s} 6J&#38;l,, n {k :&#38; &#38;~} Var(s) = [XI,... , Xm] [co] Vi g[I,n], \n8;==k k V Var(s~) =0ET, A@ {k: X~&#38;[X1/sl,... ,Xn/s.]} G&#38;l,,.n {k : X#$s~} { ET, A ako,,.n {ki:XikDkiY} \nYo~ # UV(A, ET) [Re] Vi c [0, n], Xiobi c UV(A, ET) ET, A fkl..,n {~i:&#38;k @xo} { ET, AEBB :bD] UV(A \n@ l?, ET) n Bound(B) = 0 ET, A Lin = Lin(A @B 8 C, ET) [1,/Al] n Lin = 0 [IAI+Il?l+1,IAI +IBI+IcI] n \nLin =@ { a< b<c<d b a=d c AA=c a ET, A ki-A a<k<b Sw] { Sop(k) I S&#38;C ET }, { y(k) :xw(k+~-p(k)s \n/ k:x~s G A } p(k) = k A c<k<d {{ k otherwise entry variables EV(ET) n { ?&#38; \\ .$Qk ~ ET, X 6 Var(s) \n} useful variables UV(A, ET) = EV(ET) U UV(A, { SO~+&#38; I Xo~ e EV(ET) A X ~ S E A: k }) entered layers \nLin(A, ET) = { k c [1, lA/] 1 X.k e UV(A, ET) } Table 2: Collapsing rules The copying rule [Co] is a \nparadoxical rule in the one binding (ko : XO Gko Y), which will possibly context of a structure sharing \nframework, because free a layer. it aims to copy some terms8. This rule is not mandatory, but experiments \nhave shown its power The binding delete rule [bD] identifies useless bind\u00ad to reduce environment size. \nCopying a skeleton ings of an environment (here those present in the s in context k occurs when all variables \nof s~kl part 1?) and removes them. Practically, we only re\u00adare bound either to ground terms or to terms \nof move recent 11 useless bindings or more correctly, the same layer kft. Practically, we weaken the \nrule we do not install (recent useless bindings (see Sec\u00ad and only copy little terms whose complexity \nis tion 4). smaller than a given threshold 9. Skeletons are im-The layer delete rule [ID] identifies \nuseless leftmost plemented as DAGs10 and the copying algorithm and rightmost layers (those not entered \nby a useful shares the subterms of the skeleton to be copied binding) and suppresses them. As the range \nof the as much as possible. Furthermore, if several bind\u00adenvironment changes, the entry terms have to \nbe ings access term s~kl then the same copied term updated. The proposed updating scheme covers in s[xl/sl, \n. . . , Xn/sn] is used in all cases. fact two cases : a normal case to reduce by IAI o The renaming rule \n[Re] rebinds a family of vari-the entry term keys greater than IAI, and a special case to set to 1 those \nground entry term keys 12 ables Xio~i to a variable of the family when they which are less than or equal \nto [Al. The application are all originally bound to a variable Y&#38; only ac\u00adcessible through the variables \nx~~k i. Thus, we save conditions of the rule ensure that the special case only arises for ground terms. \n8We have omitted in Table 2 a variant of the rule which copies entry terms. The last switch rule [Sw] \nis a very general rule Various measures of this complexity are possible. We use the following one : CZ(X) \n= 1, Cz(a) = O and Cc(.f(sI,. . . ,sn)) = 11those created after the join of the parent environments. \nif~C x(si) =0 then O eken+~Cz(si)7. I zIn fact, ground terms do not need any key because their value \n1 Directed Acyclic Graphs is the same for all keys. which exchanges layers a to b with layers c to d \nin an environment. The bindings which enter or exit the switched layers have to been updated, as shown \nin Fig. 4. This updating is a complex opera\u00adtion which may build a lot of new bindings. Thus, switching \nis practically used in very special cases : one of the switched parts is a (hole where no (use\u00adful) bindings \nenter while the other is an end part from whence no (useful) bindings exit and where only recent bindings \nenter. This allows us to fill the internal holes (which cannot be freed by rule [lD]) and to compact \nenvironments. IAI 1 a bcd I 1~ . : : . : : . : Figure 4: An intuition for the Switch rule The values \nof entry terms are kept unchanged (mod-UIO a renaming) by the application of the collapsing rules : Proposition \n3.4 If a pair (ETA, A) derives into a pair (ET~, B) by applying collapsing rules and A is well\u00adfounded \nwrt ETA, then B is well-founded WTi ETB and there exists a renaming substitution p over T(Z, V.N) such \nthat e ETB = { (Sok)p Isok GETA } e dso~ ~ ETA, (TAsOk)p = TB(sQkp) Proof: It is sufficient to prove \nthe existence of a re\u00adnaming substitution p for the application of each collapsing rule. The well-foundness \nof the resulting environment fol\u00adlows from the properties of p. e For the [Cl], [Co] and [bD] rules, \nwe can take p = Id. o For the [Re] rule, we can take P = {&#38;/ XOOko }. o For the [lDJ rule, we take \np = { XO~/XO~_lAl I X*k G   vo[lA]+,,+m[}. For the [Sw] rule, we take p = { XOk/-XoPtkJ I Xok C VON \n}. m Now, from an operational point of view, it must be noted that these rules are implemented with the \nDelete and Replacement operations. We need replacement for all rules (except the [bD] and [lD] rules) \nand deletion for the [bD] and [Re] rules. 4 Implementation 4.1 Design principles Three principles have \nguided the development of our implementation of layer sharing. 1. We have a lot of environments to save. \nThus, we want a compact representation of environments, layers, and bindings. 2. We need very fast read/write \naccesses13 to vari\u00adables in environments, because studies have shown that it is a crucial point in a \nstructure sharing framework. It is generally believed that accesses should be done in constant time. \n 3. Each stored object owns its own environment  which is in first approximation of the form (A @ B)@ \nA where A and B are environments associated to other objects (parent objects) and A is a small set of \nnew bindings added by unification. Thus, a lot of bindings and parts of layers are common to several \nobjects. We want our merge mechanism to preserve common layer parts by sub layer sharing as much as possible. \nPoints 1 and 2 are slightly contradictory. Indeed, it is difficult to marry environment compactness and \nfast variable access. Our solution to handle this conflict is to use two environment representations. \nEnvironments for inactive objects are stored in a compact form. When an object is activated (to take \npart in a computation), an active representation of the stored environment is built which provides more \nefficient accesses. At the end of a successful computation whose result\u00ading object must persist, a stored \nrepresentation is built from the active one (including collapsing). The time overhead due to the building \nmust be compensated for by the speed up on variable access. These different stages are illustrated by \nFigure 5. To handle Point 3, we have chosen to represent layers by Virtual Copy Arrays (VCAS) [Per85]. \n 4.2 A compact representation Stored environments are represented by lists consti\u00adtuted of layers and \nintegers. Integers denote the length of sequences of empty layers called gaps. 4.3 A VCA implementation \nfor layers Virtual Copy Arrays are partial binary trees well designed to represent sparse vectors. To \nrepresent a vector of size 2h, a tree of height h is used and the 13A read ~cces~ means the search of \nthe binding associated to a variable x~k, while a write access means the addition of a binding relative \nto Xek. Stored Env d> Active EnvUnification Collapse Active Env Stored Env ( JOIN <c> ~ ::,: Stored \nEnv Read/Write Accesses<B> > Figure 5: Environment v o12 3 4 VoVI V4 o VoVI vi Figure 6: Virtual Copy \nArrays values are stored in the leaves. Parts of the tree which normally lead to empty values are not \nbuilt (see Fig. 6). We find this point interesting because, in most cases, only a fraction of the variables \nof a layer are bound and thus, we will use empty leaves to represent unbound variables, Furthermore, \nthe merging of two VCAS VI and V2 can be done with optimal sharing by only copying paths leading to a \nmodified leaf of one of the two VCAS, as shown in Fig. 7. Original VCAS are kept unchanged. Asymmetry \nbetween VI and V2 can be introduced in the merging algorithm to allow replacement or deletion of an old \nvalue in V1. Finally, note that a Read/Write access requires O(h) time. Now, we index variables by integers \nand provide a di\u00adrect access for a variable to its index. Layer can then be represented by VCAS using \nvariable indexes. The num\u00adber v of variables is a constant which usually depends on the number of variables \nin the logic program (the copying rule does not introduce new variables). Con\u00adsequently, read/write accesses \nin a layer can be done in a constant time, where the constant is in O(log(v)). Practically, v is around \na few dozen which makes access quite acceptable. 4.4 An active representation The essential structure \nof our active representation is a global vector called the layer table where the needed layers for a \ntask are loaded in order at task switching (see Fig. 8). In this way, we achieve two main results : management \ncycle , Vovi V4 V2vi Figure 7: Merging of VCAS The two VCAS A and B are merged to form the VCA A + B. \nOriginal sub trees of A and B are reused as much as possible and only two new nodes are needted to build \nA+ B. The parent VCAS A and B are left unchanged by the merging operation. 1. The access to a layer is \nnow (nearly) instantaneous and hence we have a constant variable access time. More precisely, the cost \nto access a variable is in O(log(v)) where v is, as previously mentioned, a relatively small constant. \n 2. Joining is easily done by loading the environments one after the other. The loading cost is propor\u00adtional \nto the size of the loaded environments.  Because a lot of computations fail during the unifica\u00adtions, \nwe try to be lazy and postpone as much as possi\u00adble space costly operations until the end of the task. \nIn particular, the new bindings are not immediately added (by merging) to the loaded layers, but rather \nkept in as\u00adsociative local binding lists attached to each variable of V. An entry in a local list has \nthe form (k, X &#38; s). When building the stored representation of the active environment at the end \nof the task, the (useful) local bindings will be added to the loaded layers 14. Variable access (to say \nXok) becomes a two step op\u00aderation : We first look in the local binding list of X lANote that ~~local>>bindings \ncorrespond in fact to the noticm of recent> bindings informally introduced in Section 3.8. I I Iv Local \nBinding List Local Binding List Figure 8: An active representation for Layer Sharing At task switching, \nthe layers are loaded in the layer table. New bindings are stored in the Local Binding list of the variables. \nfor a binding relative to key k and, if this look up fails, we look in the layer table for a binding. \nTheoretically, we no longer have a constant access time but, at worst, an O(s) cost where s denotes the \nsize of the current environment. The worst case occurs when a variable X is bound during a task for all \nthe layers of the en\u00advironment and in practice, that case never occurs : at most one or two local bindings \nper variable are created in a task, The access cost is still virtually constant and proportional to log(v), \n 4.5 Collapsing We add here a few remarks on the implementation of the collapsing algorithm. 1. We perform \na first dereferencing pass, starting from the entry variables. (a) to apply the closure rule. (b) to \napply the copying rule. (c) to identify potential applications of the re\u00adnaming rule (although we have \nto wait until the end of the pass to treat them). (d) to collect useful variables and mark useful  \nbindings. At this step, a variable Yokl which is only accessible through a binding k ; X Lk Y is not \nconsidered useful. (e) to collect entered layers and non terminal lay\u00ad ers. A non terminal layer k is \njust an entered layer which contains a useful binding X &#38; s with 6 # O and s non ground. 2. We analyze \nand carry out the potential cases of re\u00adnaming. We mark bindings which must be deleted. 3. We then apply \na filling algorithm to try to fill holes using the switch rule : our algorithm is not optima115 and we \nonly use filling blocks whose en\u00adtering useful bindings are all local (to avoid modi\u00adfying installed \nbindings in the loaded layers). 4. We then determine the exterior layers to be deleted by applying the \nlayer delete rule. 5. The result of the last two steps is a mapping from the range of the current environment \ninto the range of the future environment. We reorder the lay\u00aders following this mapping, add updated \nuseful bindings, and delete the bindings which have been marked during step 2. Useful local bindings \nare merged with the others. 6. We compress the resulting environment by replac\u00ading sequences of empty \nlayers by their size, as shown in Section 4.2.  Let us consider step 5 in more detail. The following \ncases may arise : We have to modify a local binding. We can do it physically because this binding is \nnot present in any other stored environment. We encourage this modification case. e We have to modify \na non local binding: we cannot do it physically. Thus, we create a local binding which will replace the \noriginal one at merging time. e We have to delete a useless local binding. We only need not to install \nit at merging time. We encour\u00adage this deletion case. e We have to delete a non local binding. We replace \nit by an empty binding at merging time. 4.6 Skeleton implementation Skeletons are implemented as DAGs. \nAs the number of skeletons remains quite reasonable (even when using the collapsing copying rule), we \nhave decided to include a direct access for skeletons to their tuple of variables and to their complexity, \nThis option helps to speed up occur check, application of Rule U6 during unification, and application \nof the collapsing rules. For inst ante, a direct access to the variables of a skeleton saves a great \ndeal of de-structuring during occur check to retrieve these variables. More generally, saving information \nin skeletons consumes little space (because of the limited number of skeletons) and helps to reduce the \nstructure sharing time overhead. 150ptimality would be too time consuming. 4.7 Towards a parallel implementation \n6 Acknowledgments We have presented an implementation destined for a sequential machine. We would like \nto point out that this implementation can be easily extended to exploit multiprocessing. We only need \nto associate a Layer Table per processor and to add an additional indirection to the Local Binding lists \nof the variables, as shown in Fig. 9. H22 m Figure 9: Layer Sharing and Multiprocessing We do not have \nany overhead due to the multipro\u00adcessing (except for the extra indirection to access local bindi;gs), \nIndeed, we only have read access on shared objects (bindings, layers, or environments) and write actions \nonly occur on local bindings or on local bind\u00ading lists. We do not need any synchronization between processors \nto manage structure sharing.  Conclusion We have shown in this paper that some Logic Pro\u00adgramming models \nare evolving towards non input mod\u00adels where multiple computation branches have to be maintained and \njoined together with possible renam\u00ading. These models are mostly used to achieve compu\u00adtation sharing. \nFor such models, copying mechanisms are inefficient because of the vast amount of data to store. Therefore, \nwe have presented a structure sharing framework called layer sharing originally designed for a sequential \nDynamic Programming Logic Program inter\u00adpreter called DyALog. The results of our first implementation \nare promis\u00ading and beckon for further extensions such as parallel execution. Lastly, preliminary results \nhave shown that, contrary to popular belief, a structure sharing imple\u00admentation can be faster (in some \ncases) than a copying based one. I would like to thank P. Codognet for his deep knowl\u00ad edge of parallel \nPROLOG models and Ian Jacobs for helping me with writing style. References [BM72] R. S. Boyer and J. \nS. Moore. The sharing of structure in theorem proving programs. In Machine Intelligence, chapter 7, pages \n101 1 16. John Wiley and Sons, New York, 1972. [BMSU86] F. Bancilhon, D. Maier, Y. Sagiv, and J. Unman. \nMagic-set and other strange ways to implement logic programs. In Proc. of the 5th ACM symp. on Principle \nof Database Systems, 1986. [BVdlC92] F. P. Barth61emy and E. Villemonte de la Clergerie. Subsumption-oriented \npush\u00addown automata. In Springer-Verlag, ed\u00aditor, Proc. of PLILP 92, pages 100 114, 1992. [CHL92] P.L. \nCurien, T. Hardin, and J.-J. L6vy. Confluence properties of weak and strong calculi of explicit substitutions. \nrap\u00adport 1617, INRIA, 1992. summitted to J. A.C.M. [DeG84] D. DeGroot. Restricted AND parallism. In Proc. \nof the Int. Conf. on Fith Com\u00adputer Systems, 1984. [GJ90] Gopal Gupta and Bharat Jayaraman. On criteria \nfor Or parallel execution models of logic programs. In Proc. of the 1990 NACLP, pages 737 756, 1990. \n[GSCYH91] Gopal Gupta, Vitor Santos Costa, Rong Yang, and Manuel V. Hermenegildo. ID-IOM: Integrating \ndependent And-, and Or parallelism. In Logic Programming, Proc. of the 1991 Int. Symposium, pages 152-166, \n1991. [HG90] M. V. Hermenegildo and K. J. Greene. &#38;-Prolog and its performance: Exploiting in\u00addependent \nAnd-Parallelism. In Proceed\u00adings of the Seventh International Confer\u00adence on Logic Programming [ICL90], \npages 253-268. [ICL90] Proceedings of the Seventh International Conference on Logic Programming. MIT \nPress, 1990. [Kah87] [Kat90] [Lax188] [Per85] [Sek89] [SH91] [TS86] [UOKT84] [VdlC90] [Vie87] Gilles \nKahn. Natural semantics. In Proc. of STA CS 1987, Lecture Notes in Computer Science, chapter 247. Springer \nVerlag, March 1987. Vinod Kumar Kathail. Optimal Inter\u00ad preters for Lambda calculus Based Func\u00ad tional \nLanguages. PhD thesis, M.I.T., 1990. Bernard Lang. Complete evaluation of Horn clauses: an automata theoretic \nap\u00adproach. Technical Report 913, INRIA, Rocquencourt, France, nov 1988. to ap\u00adpear in Int. Journal of \nFoundations of Computer Science. Fernando C. N. Pereira. A structure sharing representation for unification \nbased grammar formalism. In Proc. of the 23rd Annual Meeting of the Associa\u00adtion for computational Linguistic, \npages 137-144, 1985. H. Seki. On the power of alexander tem\u00adplates. In PTOC. of the 8th ACM symps. on \nprinciples of Databases Systems, 1989. Kish Shen and Manuel V. Hermenegildo. A simulation study of Or \nand indepen\u00addent And parallelism. In Logic Program\u00ad ming, PTOC. of the 1991 Int. Symposium, pages 135-151, \n1991. H. Tamakl and T. Sate. OLD resolu\u00adtion with tabulation. In E Shapiro, ed\u00adit or, Proc. of Third \nInt. Conf. on Logic Programming, pages 84-98, London, 1986. Springer-Verlag. Kuniakl Uehara, Ryo Ochitani, \nOsamu Kakusho, and Junichi Toyoda. A bottom\u00adup parser based on predicate logic: A sur\u00advey of the formalism \nand its implementa\u00adtion technique. In Proc. of the 1984 Int. Symps. on Logic Programming, 1984. Eric \nVillemonte de la Clergerie. DyALog: une implantation des clauses de horn en programmation dynamique. \nIn Proc, of the 9th Sdminaire de Programmation en Logique, pages 207 228. CNET, May 90. Laurent Vieille. \nDatabase-complete proof procedures based on SLD resolution. In PTOC. of the J int. Conf. on Logic Program\u00adming, \nMay 1987. [War84] David Scott Warren. Efficient prolog memory management for flexible control strategies. \nIn Proc. of the 1984 Int. Symps. on Logic Programming, pages 198 202, 1984. [War87] H. D. Warren. The \nSRI model for OR parallel execution of prolog. abstract de\u00adsign and implement ation issues. In dth Symp. \non Logic Programming, pages 46 53, San Francisco, Sept. 1987. [War89] David Scott Warren. The XWAM : \nA ma\u00adchine that integrate prolog and deductive query evaluation, October 1989. A Preliminary results \nWe cannot really provide comparison with other structure sharing frameworks because our Logic Pro\u00adgramming \nevaluator is relatively exotic, still a proto\u00adtype, and written in LeLisp 16. However, we can display \nsome results we hope to be convincing. We have selected a set of 10 programs to run our tests. The first \nfive programs (G programs) mainly manipulate ground data, while the five others (nG programs) mainly \nmanipulate non ground data. All examples are pure logic metic predicates). Most explanatory. The ending \ndication of the input data forth4%10 runs a little programs (no control or arith\u00adof the programs name \nare self numbers generally give an in\u00ad size (e.g., the length of a list). program FORTH with a mini forth \ninterpreter logic program. list of variables to bind them elements. boum%5 is a little signed for structure \nsharing by an exponential repetition runs j ib( 14) with an optimized transV%7 traverses a with new (non-ground) \nprogram specially de\u00ad where a term is created of a pattern. fibO%14 iterative program which acts on \nnon ground lists (to achieve computation shar\u00ading). loopVY050 runs an assembly loop 50 times with a miniature \nassembly emulator. Three versions of DyALog are compared. The No-Sharing version is a copying based \nversion. The Sharing No Copying version implements layer sharing without any copying. Lastly, the \n(Sharing ver\u00adsion adds to the previous version the copy of small terms. Table 3 displays the time and \nspace performances of the different versions. For G programs, we see that no space saving is achieved \nand that there ex\u00adists a slight time overhead for these programs (except for hanoi%7 and fort h4%10 which \nmanipulate large data). We can explain that by the fact that the copying algorithm of the No-Sharing \nversion does not copy lb LeLi~p is a product of ILOG Table 4: Analysis of environment size P.gm append%50 \nhanoi%7 queens%5 asort%10 ---\u00adtransV%7 boum%5 .,. -. .- . forth4%10 revV%51_l fibO%14 100DV%5O I I I \nI objects 160 368 -\u00ad123 733 1861 830 61 43 221 1499 I No Sharing IISharing No time I space time I. I( \n1.7 I 2 II 3.6 I II 16.3 I 12 I 202.5 I ,, -,\u00ad, -,, --\u00ad1 II 11.6 I x4 II 5.9 32.5 21 62.3 5.8 22 14.9 \n56.0 15 37.7 2.7 8 3.7 0.3 2 13 [ 11 1 II 11.6 / ;; II :\u00ad794/ II 171.2 80 II 71.7 I Copying space 3 42 \n2 29 38 24 2 1 43 33 II Sharing II time II 2.3 1 II 8.3 I II --\u00adi II 4.4 1 39.1 9.0 17.6 3.7 n~ II 5.8 \nII 52.6 I With Copying space 2 17 2 0 27 40 19 3 n IL 43 Table 3: Performances [objects] number of environments \n[time] Execution times in seconds [space] Needed space in Kcells (&#38;lK) Sharing No Copying Sharing \nWith Copying Pgm maxS maxRS avS avRS maxS maxRS avS avRS appendYo50 51 51 9.0 8.9 1 1 1.0 0.6 hanoi%7 \n305 305 96.4 90.1 6 6 1.8 1.7 queens%5 50 23 22.2 12.4 23 12 8.5 3.8 qsort%10 54 20 2.8 2.1 2 2 0.6 0.5 \nforth4%10 35 26 10.5 9.6 1 1 1.0 0.9 revVYo50 52 52 22.8 22.6 .52 51 22.8 21.8 transV%7 159 127 11.8 \n9.0 4 3 1.6 0.7 boum%5 94 48 8.3 4.2 34 15 3.7 1.5 fibO%14 984 611 242,7 161.3 8 7 3.4 2.8 loopv%50 4 \n4 3.2 2.0 3 3 1.8 0.8 [maxS] maximum number of slots in an environment [maxRS] maximum number of real \nslots in an environment [Ws] average number of slots in an environment [avRS] average number of real \nslots in an environment ground terms (or subterms) and that structure sharing ferent kinds of layers. \nThe field S indicates the total has a cost both in space and in time. On the other number of slots when \nenvironments are not compacted hand, structure sharing becomes interesting for nG to remove internal \nempty layers. A gap in an envi\u00adprograms : space gains can be very spectacular and ronment can be seen \nas denoting virtual slots. RS indi\u00adprograms run faster with sharing. cates the number of slots which \nare occupied by real non empty layers. The results show that we can have aWe also see that in all cases, \ncopying small terms is very large number of virtual slots, but generally a smallinteresting, both in \nspace and time : indeed, the size of number of real layers. As a same layer can be reusedenvironment \nis drastically cut, which saves space and without modification in several environments, we alsospeeds \nup environment loading. The effect on environ\u00adindicate in L the total of existing distinct layers. ment \nsize is shown in Table 4 where we display the average and maximum environment size. The real size Table \n6 carefully examines of what is gained by shar\u00addenotes the number of non empty layers in an environ-ing. \nThe field Terms indicates how many terms have ment. Environment sizes are generally reasonable (at been \nbuilt during the executions. The field %Lreuse most a few dozen and very often less than 10) when the \ndisplays the percentage of the space which has been Copying rule is used, but the reader must be conscious \nsaved by the partial sharing of the layers (by using the that no collapsing algorithm can avoid exponential \nen-VCA merge algorithm), This percentage is defined as vironment growth (for pathological programs). \nthe ratio h~~o;~c p~ where Sshare (resp. Scopy) rep- Table 5 precisely analyses the distribution of the \ndif-resents the space used by the layers with partial layer Table 5: Analysis of the different kind of \nlayers [S] Number of slots (virtual+ real) [RS] Number of real slots [L] Number of different layers hanoi%7 \n4464 13 95.97% 1270 I 11.16% queens%5 8035 118 64.43% II 1153 I 39.37% qsort %10 3428 61 32.94% 11 1652 \n1 1.84% forth4%10 2594 50 70.44% II 237 1 3.88% revVYo50 4110 6 67.71% II 71 67.00~o transV%7 794 11 \n69.92%. . ..\u00ad,. 11 99-. 1 t 5.80?4-. , boum%5 42613. 22 . . . . . . 48.58% 11 96 1 . . 36.77~o ! fibO \n?%14 IIII 36296. II 623 I .96.62 % II 1468 I 25.02% I loopv%50 II 38022. 103 1 23.16% II 1396 I 1.62% \n Table 6: Analysis of reuse [Terms] number of created terms [%reuse] saving due to the partial sharing \nof layers sharing (resp. by the layers without sharing). Collaps\u00ading rules which alter non local existing \nbindings (such as the Closure, Renaming, or Copying rules) tend to reduce this ratio, because they oblige \nto rebuild some parts of existing layers rather than reuse them. \n\t\t\t", "proc_id": "158511", "abstract": "<p>We present in this paper a structure&#8211;sharing framework originally developed for a Dynamic Programming interpreter of Logic programs called DyALog. This mechanism should be of interest for alternative execution models of PROLOG which maintain multiple computation branches and reuse sub-computations in various contexts (computation sharing). This category includes, besides our Dynamic Programming model, the tabular models (OLDT, SLDAL, XWAM), the &#8220;magic-set&#8221; models, and the independent AND and OR parallelism with solution sharing models. These models raise the problem of storing vast amount of data, motivating us to discard copying mechanisms in favor of structure-sharing mechanisms. Unfortunately, computation sharing requires joining computation branches and possibly renaming some variables, which generally leads to complex structure-sharing mechanisms. The proposed &#8220;layer-sharing&#8221; framework succeeds however in remaining understandable and easy to implement.</p>", "authors": [{"name": "E. Villemonte de la Clergerie", "author_profile_id": "81332495311", "affiliation": "", "person_id": "P74260", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/158511.158687", "year": "1993", "article_id": "158687", "conference": "POPL", "title": "Layer sharing: an improved structure-sharing framework", "url": "http://dl.acm.org/citation.cfm?id=158687"}