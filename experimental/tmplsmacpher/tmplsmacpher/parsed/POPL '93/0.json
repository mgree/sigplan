{"article_publication_date": "03-01-1993", "fulltext": "\n Computer Architectures and Programming Models for Scalable Parallel Computing Marc Snir IBM T. J. Watson \nResearch Center Yorktown Heights, NY 10598, USA Abstract Parallel computing technology offers the opportunity \nfor one basic computer architecture that achieves a wide performance range, at cost nearly linear in \nperformance: A family of products, ranging from a workstation to a massively parallel processor with \nthousands of nodes, built from the same basic hardware components; the same languages, the same user \ninterfaces used over all the range; the same application codes running efficiently on all these machines. \nIlk is the vision of scalable par\u00adallel computing. Is this dream, marketing hyperbole, or can it be reality? \nWe examine, in our presentation, some of the underpinnings of this technology, We shall focus on parallel \nmachines built of standard RISC mi\u00ad croprocessor nodes, and consider the likely evolution of such systems \nin coming 3-5 years. The effective performance of a parallel machine is a function of many parameters: \nnumber of nodes, node compute rate, memory, internode communication band\u00adwidth and latency, external \n1/0 bandwidth and latency, and more. An intuitive requirement for scalable perfor\u00admance is that memory \nper node, internal and external bandwidth per node, and internal and external latency, stay nearly constant, \nas the number of nodes increase. Available technology can achieve these goals, over the range of interest. \nAn examination of algorithmic perfor\u00admance show that these requirements for scalable perfor\u00admance have \nno better theoretical justification than the usual rules of thumb for uniprocessors (one byte of mem\u00adory \nand one bit of 1/0 per flop/s). However, like these rules of thumb, they can provide a convention on \nwhat is a balanced parallel architecture. Such convention, even if arbitrary, simplifies the design of \nscalable soft\u00adware, since it leaves us with only two free performance parameters: number of nodes, and \nnode performance. Internode communication latency is the performance parameter where current parallel \narchitectures show the widest spread. Currently, shared memory machines achieve a much lower latency \nthan distributed mem\u00adory machines. An examination of the likely evolution of both architectures indicates \nthat the gap will nar\u00adrow, with increasingly similar communication mecha\u00adnisms used on both. We shall \nexamine the pros and cons of communication baaed on memory to cache trans\u00adfers as compared to communication \nbased on memory to memory tranafers. Parallel codes are written with various levels of ex\u00adplicit parallelism. \nAt the most basic level, user explicitly control both data and control partition this is the case for \nmessage passing. The compiler can still play an im\u00adportant role in optimizing communication by hiding \nla. tency. At the next level, languages are designed either to express control partition (control parallelism) \nor to ex\u00adpress data partition (data parallelism). In the first case, the compiler derives an efficient \ndata layout and com\u00admunication pattern, from the partition of control; in the second case, it derives \nan efhcient assignment of compu\u00adtation to processors and an efficient communication pat\u00adtern, from the \npartition of data. Both approaches give larger scope for compiler optimization. Finally there is the \nHoly Grail of implicit parallelism, all dealt by the compiler. Both distribution of computation and minimization \nof communication are, oftentimes, essential parts of par\u00adallel algorithm design. Therefore, I expect \nlanguages with explicit parallelism to be essential for the success of scalable parallel computing. Languages, \nlike High Per\u00adformance Fortran, that offer a choice of levels of control (explicit data and control parallelism, \nexplicit data par\u00adallelism, no explicit parallelism), may be the right ap\u00adproach: the user specify data \nor control partition only when the compiler fails to derive an efficient partition on its own. Such strategy \ncan work only if the com\u00ad piler ceases to be a black box, and provides to the user a clear model of the \ncontrol and data partition derived from the source code, \n\t\t\t", "proc_id": "158511", "abstract": "", "authors": [{"name": "Marc Snir", "author_profile_id": "81490651996", "affiliation": "", "person_id": "PP39079437", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/158511.158513", "year": "1993", "article_id": "158513", "conference": "POPL", "title": "Computer architectures and programming models for scalable parallel computing (abstract)", "url": "http://dl.acm.org/citation.cfm?id=158513"}