{"article_publication_date": "03-01-1993", "fulltext": "\n Separating Stages in the Continuation-Passing Style Transformation Julia L. Lawall Olivier Danvy Department \nof Computer Indiana University jll@cs.indiana.edu Science * Department of Computing and Information Kansas \nState University t danvy@cis.ksu,edu Sciences Abstract The continuation-passing style (CPS) transformation \nis powerful but complex. Our thesis is that this transforma\u00adtion is in fact compound, and we set out \nto stage it. We factor the CPS transformation into several steps, separating aspects in each step: 1. \nIntermediate values are named. 2. Continuations are introduced. 3. Sequencing order is decided and \nadministra\u00adtive reductions are performed.  Step 1 determines the evaluation order (e.g., call-by-name \nor call-by-value). Step 2 isolates the introduction of con\u00adtinuations and is expressed with local, structure-preserving \nrewrite rules a novel aspect standing in sharp contrast with the usual CPS transformations. Step 3 determines \nthe ordering of continuations (e.g., left-to-right or right-to-left evaluation) and leads to the familiar-looking \ncontinuation\u00adpassing terms. Step 2 is completely reversible and Steps 1 and 3 form Galois connections. \nTogether they leading to the direct style (DS) transformation of our earlier work (including first-class \ncontinuations): 1. Intermediate continuations are named and sequencing order is abstracted. 2. Second-class \ncontinuations are eliminated. 3. Administrative reductions are performed.  A subset of these transformations \ncan leverage program manipulation systems: CPS-based compilers can modify se\u00adquencing to improve e.g., \nregister allocation; static program analyzers can yield more precise results; and overspecified CPS programs \ncan be rescheduled. Separating aspects of the CPS transformation also enablek a new programming style, \nwit h applications to non deterministic programming. As a byproduct, our work also suggests a new continuation \nsemantics for unspecified sequencing orders in programming languages (e.g., Scheme). *Bloomington, Indiana \n47405, USA. Part of this work was sup\u00adported by NSF under grant CCR-9000597. tManhattan, Kansas 66506, \nUSA. Part of this work was supported by NSF under grant CCR-9102625. Permission to copy without fee all \nor ,part of this material is granted provided that the copies are not made or distributed for direct \ncommercial advantage, the ACM copyright notice and the title of the publication and its date appbar, \nand notice is given that copying is by permission of the Association for Computing Machinery. To copy \notherwise, or to republish, requires a fee and/or specific permission. ACM-20th PoPL-1/93-S.C., USA G \n1993 ACM 0-89791 -561 -5/93 /0001 /0124 . ..$1 .50 1 introduction Continuation-passing style (CPS) terms \nenjoy a number of useful properties: They offer a good format for compil\u00ading and optimization [1, pages \n4 6] and the y enable non\u00adtrivial improvements for semantics-based program manipu\u00adlation [6, 28]. CPS \nterms are independent of their evaluation order [29, 30] and CPS is at the basis of several mathemat\u00adical \nsemantics of programming languages [12, 36]. On top of that, the CPS transformation occurs in several \nareas of theoretical Computer Science, including Category Theory [13, 26, 3 7] and Logic [17, 27]. In \nshort, there is something truly fundamental in the CPS transformation, even though it can be expressed \nin only three lines for the pure J-calculus [29]: [Z]K = ttz [Ax. e]K = K(A2. Ak. [e]k) [eo el] K = [co] \n(Avo . [elJJ(Azrl . w VI K)) Figure 1: Plot kin s call-by-value, left-to-right CPS transfor\u00ad mation The \nsimplicity of this transformation is, however, de\u00adceiving. At least-three things happen in the CPS transfor\u00admation: \nIntermediate values are named, continuations are introduced forcing terms into tail-recursive form, and \nterms are sequenced [1, 9, 14, 16, 29, 33]. Our thesis is that the CPS transformation is in fact a compound \ntransformation. We propose to stage the mapping between the DS world and the CPS world. We stage the \nmapping between DS and CPS terms as follows. First we restrict the DS and CPS languages so that continuations \ncan be introduced and eliminated with\u00adout dkturbing the structure of the source term. The re\u00adstricted \nlanguages are called CoreDS and CoreCPS, respec\u00adtively. Then we define a transformation between ordinary \nDS and CoreDS, and another transformation between ordi\u00adnary CPS and CoreCPS. Thus the CPS and DS transforma\u00adtions \nare staged as follows: DS ~ CoreDS ~ CoreCPS ~ CPS Our staged transformation shows a number of useful \nproperties: t-i-+$ kl-.i+k kt-i-ki k~i-i l-x-+x kl\u00ad k-k FAx.4+Ax. Ak. t I-io+;o I-t l+i] kEt o L=)-iJik \n Figure 2: Restricted CPS transformation it does not require any administrative reductions. So it is \nboth simpler to use and simpler to understand. For example, our earlier example f(Ax. x) directly yields \nthe CPS term Ak. f(Az. Ak. kz)k without any administrative reductions. Here is the BNF of the restricted \nCPS terms A k. e pro\u00adduced by this transformation: e ::= kt Is t ::= z IAz. Ak. e s ::= totl k where \nk is a meta-variable representing the set of all con\u00adtinuation identifiers.3 We call this language the \nCoreCPS language for the A-calculus. The transformation can be rewritten more expressively in a natural \nsemantics-style, as displayed in Figure 2. We use Stoy s diacritical convention [35] of overlining DS \nterms with an acute accent and CPS terms with a grave accent . Given a continuation k, a CoreDS term \ni is translated into a CoreCPS term, $ whenever k k 4-2. Correspondingly, a trivial DS term t 1s translated \ninto a CPS term t whenever 1-t + ~, and, given a continuation k, a serious DS term 4 is translated into \na CPS term ? whenever k 1-4 + ~. The resulting transformation is not only expressed with local and structure-preserving \nrewrite rules it is also re\u00adversible, as shown by Figure 3. Given a cent inuation k, a CoreCPS term \n&#38; is translated into a CoreDS term d when\u00adever k R d .-?. Correspondingly, a trivial CoreCPS term \n; is translated into a CoreDS term ~ whenever + t + ? and, given a continuation k, a CoreCPS term ; is \ntranslated into a serious CoreDS term i whenever k F 4 @ 3. We capture this symmetry by merging the contents \nof Figures 2 and 3 in Figure 4. 2.2 Restoring expressiveness The CoreDS and CoreCPS languages do allow \ninverse CPS\u00adand DS-transformations with local and structure-preserving rewrite rules, but these languages \ncannot directly express 3In the absence of control operators such as call/cc, one variable k is enough \n[8, 11, 31]. I Fi.mre 4: Restricted DS and CPS inter-transformations intermediate results, such as nested \nfunction calls. Let us now restore the expressiveness of the J-calculus. Essentially, the CPS-transformation \nnames intermediate results [1, Sec\u00adtion 1.1]. Observing that a name (i. e., a variable) is a trivial \nterm, we only need a special form for naming intermediate results, with the constraint that it can be \ntransformed into CPS with a local and structure-preserving rewrite rule. We choose to extend the BNF \nof serious CoreDS terms wit h a bind-expression: .. s .. ... I bind (xl, .... z~) = (s,, .... s~)in e \nA bind-expression provides a name x, for the result of evaluating a non-trivial term s: that may occur \nas an im\u00admediate sub-t erm in e. For example, a DS term such as (f X)(9x) yields the term bind (v, w) \n= (f c, gx)invw Symmetrically, unfolding all the bind-expressions yields a DS term. NB: In the rest of \nthe paper, we use bind-expressions interchangeably with their higher-order abstract-syntax counterpart, \ncombke (A(zl, .... xn) . e) (s1, .... sn) Correspondingly, we extend the BNF of serious CoreCPS terms \nwit h a schedule-expression: .. s .. ... I schedule (Ak. sl, .... ~k.s~) (J(zI, .... zn). e) A schedule-expression \nprovides a name x, for the result of performing a computation .-l k. sl. For example, a term such as \nbind (w, w) = (fz, gz)in v w yields the term Jk. schedule (Ak. fzk, Ak. grk) (A(v, w). vwk) Choosirm, \ne,g., left-to-right sequencing-order t hen yields the CPS term Ak. fz(Av. g%(Aw. vwk)) Accordingly, we \nextend the inter-transformations of Fig\u00adure 4 with the rule displayed in Figure 5. This rewrite rule \nis local and structure-preserving. 3 A Scheme-like language We now generalize the above transformations \nto a more re\u00adalistic DS language, by adding Scheme s constants, primi\u00adtive operations, conditional expressions, \nand let-and letrec\u00adexpressions [4]. We also consider n-ary functions. e ::=cli\\i I eo (cl, .... en) I \nop(el, .... em) I cond(el, e~, e~) I let (ii, .... in),= (cl, .... en)in e I letrec (il, .... Zn) = (1,, \n.... L)ine i ::= J(il, .... ire). e Constants are trivial. For simplicity we classify all the other new \nsynt act ic constructs as serious. We use continuation\u00adpsssing primitive operators in the CPS world. \nThe CPS transformation is displayed in Figure 6. Here is the BNF of the CPS terms A k. e this transformation \npro\u00adduces: e ::=kt I s t::= clili s .. . to (h, . . . . tn, Av. e) I op(tI, . . ..i~. Av. e) I cond(tl, \ne,, es) I letk=~v. eins I let (ii, .... in) = (tl, .... tn)in e I letrec (ii, .... in)= (11, .... L)ine \n1 ::= A(z1, .... in, k). e where v occurs once in J v. e. N13: Originally, we derived the BNF of CPS \nterms by in\u00adduction over DS terms and over the possible values of K, in Figure 6. Since then, we have \nverified it using Malmkjax s analysis that takes a two-level J-term and produces the BNF of its result, \nbased on abstract interpretation [22, 23]. Applying this analysis to the CPS transformer of Figure 6 \nyields the same BNF as the one derived by hand. We use Malmkjax s analysis again in the following section. \n3.1 CoreDS and CoreCPS Along the lines of Section 2.2, we now define CoreDS for the Scheme-like language. \nIn CoreDS again, all sub-terms but the actual parameters of bind-expressions are trivial. A CoreDS term \ne is defined by the following BNF: .. e .. tls ..   t .. Cizli to(tl,.. .%)I Op (h, .... k) cond(tl, \nez, es) s : let (i,, :.., in) = (tl,... tn)in e letrec(il, .... in) = (11, .... L)ine I combine (A(zl, \n.... z~). e) (s1, .... s~) i ::= A(il, .... t~ ) .e Symmetrically, a CoreCPS term for the Scheme-like \nlan\u00adguage has the form ~ k. e and is defined by the following BNii .. e .. ktls t ::= Clill to (tl, \n.... tn, k) I Op(tl, .... tm,k) cond(tl, ez, es) let (aI, .... in) = (tl,....tn)in e letrec(il, .... \nin) = (11, .... L)ine s :1 I schedule (~k. sl, .... ~k.s~) (A(zl, .... z~) .e) 1 ::= J(il, .... in, \nk). e As in Section 2.2, we can transform CoreDS terms into CoreCPS terms, using local and structure-preserving \nrewrite rules. These two transformations are exact inverses. They generalize Figure 5 and are presented \nin Figure 7. NB: These two transformations (minus the rule for combine-and schedule-expressions) can \nbe derived using Consel s partial evaluator [5]. Specializing the CPS t ransfor\u00admation with respect to \nCoreDS yields one half of the trans\u00adformation of Figure 7. Dually, specializing the DS transfor\u00admation \nwith respect to CoreDS yields the other half of the transformation of Figure 7. 3.2 Conclusion To summarize, \nwe have defined a CoreDS language as ex\u00adpressive as the ordinary DS language, for both the A-calculus \nand the Scheme-like language. In CoreDS, the result of all serious subcomput ations are named. Similarly, \nCoreCPS is as expressive as CPS. In CoreCPS, all intermediate e continu\u00adations are named (by the parameters \nof expressions in the tu\u00adple argument to schedule). We have derived local, structure\u00adpreserving, and \nreversible transformations between the two languages. Let us now turn to the transformations between \nDS and CoreDS, and between CPS and CoreCPS. These transforma\u00adtions are applicable to both the A-calculus \nand the Scheme\u00adlike language. They are described in the next two sections. 4 DS and CoreDS In CoreDS, \nserious subterms must be named, whereas in DS they may occur anywhere. Thus the transformations be\u00adtween \nthese two languages simply amount to introducing or eliminating names for serious expressions. Both transfor\u00admations \npreserve the implicit sequencing order of DS. This section is written in the spirit of the programming \nlanguage Scheme, where sequencing order is unspecified [4]. Further refinements may be possible in a \nlanguage that specifies a sequencing order, as, e.g., in Standard ML [25]. Nd DS ~ CoreDS u~ kl-k=i \nkkil~il ... kl-in+;n k 1-combine (A(rq, .. .. v~). d) (ii, .... .i~) + schedule (Ak. il, .... Ak.i~) \n(A(vI, . . . . vn). ?) Figure 5: Extended DS and CPS inter-transformations [c]X = Kc [2]K = Ki [/l(iI, \n.... in). e]tc = K(A (L, .... i~, k). [e]k) [eo (e], .... %)]K = [co] (AVO .[e~] (A VI . ...[en] (/lvn \n.vo (vi, .... vn, K)))) [ok(el, . . . . em)]6 = [cl] (AVI . ...[em] (Avm. (Jp (VI, .... v~, K)))) [cond(eo, \nel, ez)] 6 = [co] (A vo . let k = K in cond(vo, [cl] k, [e,] k) ~et (ii, .... in)= (cl, .... en)ine]~ \n= [cl] (~ q . ...[en] (A Vn let (21, .... in) = (VI, .... Vn) in [e]~)) IPetrec(fl, ...)= (-l(il, .... \ni~).e~, ...)ine]~ = letrec (~1, ...) = (A(il, .... am, k) .[e~jk, ...)in [e]~ where k is a fresh variable. \nA DS term e is transformed into CPS as A k . [e] k. Figure 6: The call-by-value CPS transformation kl-i~? \n1-A(il, . . . . in). 6++ A(il, ....in. k).&#38; E t o-io ... t-in-in t-io+ icl ... I-im++ im k+ t o(~1, \n....t n) U ~0(i, .... k k) k1-oj(~l, .... t ~) ~ o p(?I, ....?~,k) t-io ++ i) ... I-i .-in kki~i k h \nlet (i], ,.., in) = (~1, .... ~n) in.+ * let (21, .... in) = (;1, .... in) in? t iO ++ i) ... kin.-jn \nkt-t+eb k t-letrec (ii, .... in) = ([1, .... in)in L * letrec (ii, .... in) = (jl, .... in)in~ Fieure \n7: Inter-translation between CoreDS and CoreCPS 4.1 Naming DS terms 4.3 A Galois connection The transformation \nfrom DS to CoreDS, J%(d,maps a com\u00adpound term in which some immediate subterms are serious into a bind-expression \nnaming all these serious subterms. Replacing the serious terms by their names yields the body of the \nbind-expression. For example, ~d((~ z) (g x)) = bind (v, w)= (&#38;(j z), Md(9 z)) invw s combine (A \n(v, w) . w w) (~d(.f *), ~d(g $)) Naming all the serious subterms in a single bind-expression preserves \nthe sequencing order of the original term. The formals of the bind-expressions introduced by tid are \nfresh names, which makes it difficult to compare CoreDS terms. For simplicity we define equality on CoreDS \nterms to be equality up to a-equivalence of the formal parameters of bind-expressions. This definition \nof equality of CoreDS terms makes ~d into a function. In practice, CoreDS terms can be compared by simultaneously \nsubstituting fresh vari\u00adables for all the formal parameters of bind-expressions [34]. 4.2 Unnaming CoreDS \nterms Unnaming a CoreDS term with ~d corresponds to perform\u00ading the administrative reductions of the \nDS transformation. For each bind-expression, we either unfold all the bindings, substituting each named \nterm for its name, or we residualize the entire bind-expression as a let-expression. Our strategy is \nsimilar to that used in partial evaluation [3]. We first consider the image of ~d. Because there are \nno bind-expressions in DS, ~d never produces a bind-expression where the body is another bind-expression. \nAdditionally, each name bound by a bind-expression occurs exactly once, specifically as one of the t \ns in the BNF production of the body. If an expression denotes a CoreDS term in the image of tid, mapping \nit back to DS amounts to unfolding every bind-expression. On such terms, naming and unnaming are inverses. \nIn addition to the terms in the image of J%(d,~d must ac\u00adcommodate terms that result from transforming \nCPS terms into CoreDS. Thus we extend it to the full BNF of CoreDS terms. To maintain the sequencing \norder, we simply resid\u00adualize as a let-expression any bind-expression that is not in the image of the \ntransformation from DS to CoreDS. For example, bindv=fzingz is transformed into the DS term letv=jzingx \nMore precisely, ~d unfolds a bind-expression whenever 1. The body is an application, a primitive operation, \na conditional expression, a let-expression, or, degener\u00adately, an identifier and, 2. Each of the names \nbound by the bind-expression oc\u00adcurs exactly once and as one of the trivial subterms of the body.  \nOtherwise the bind-expression is residualized into a let\u00adexpression. We now examine the relationship \nbetween ~d and Ud. While the transformations are not inverses on all terms, there are some terms, called \nnormalized terms on which they are in\u00adverses. For each language we give a semantics-preserving mapping \nfrom terms to normalized terms. Then, we show that the partial orderings generated by these functions \nare related as a Galois connection. Naming and then unnaming a DS term produces the orig\u00adinal DS term, \nbut the converse is not true. As described in the previous section, a bind-expression in the CoreDS term \nmay be residualized into a let-expression in the DS term. These let-expressions remain let-expressions \nwhen translat\u00ading back to CoreDS. These problematic CoreDS terms are not in the image of ~d. Nd DS cc \nreDs u~ More formdy, ~d and .?/,j are related as fo~ows: L?/do Nd = IdentityD~ UdONdOUd = l?/d { We \ncan trivially define a mapping SDS from DS terms to normalized DS terms, because the transformations \nare inverses on all DS terms. Hence we define Although /.&#38; and ~d are not inverses on all CoreDS \nterms, the following equation NdOud O&#38; Oud=Nd O&#38; which can be easily verified using the above \nequations, shows that they are inverses on terms that have been unnamed and then named again. Thus we \ncan define a mapping SCO,CDS from all CoreDS terms into the set of normalized CoreDS terms, as follows \nb,.~s[e] = Nd(ud(e)) This mapping can be described inductively over CoreDS terms. We first give the \nrules for a bind-expression: 1. Whenever ud residualizes bind (ZI, . ..) = (sI, . ..) in e as a let-expression, \nSc~,@s[bind (XI, ...)= (sl, ...) in e] = fijt$l;,...) = (SCO,,DSISI], . ..) . ..) = (w, . ..) in SCoreDS[e] \n~c~,.ms~bind (zl, ...) = (s1, ...)in e] = bind (ZI, ...)= (Sco,,~s[sI~, ...)in &#38;oreDS[e] 3. Finally, \nSco,eDs[bind z = .sin z] = 5CoreDS[s] The rules for other terms are defined by straightforward in\u00adduction \nover the structure of the term, as in the second case above. Using the mappings SDS andSco,eDs wemay \ndefine the partial orders~Ds and ~c,,,Ds on DSand CoreDStermsre\u00adspectively. These partial orders relate \nterms tosemanticWy \u00adequivalent normalized terms, and are defined as follows. Again we use Stoy s diacritical \nconvention of overlining DS terms with an acute accent and CoreDS terms with a grave accent . e;~Ds \ne; * e 2= e> { Together these two equations imply a Galois connection [24]: 4.4 Conclusion CoreDS was \nintroduced as an intermediate language where the result of every serious expression is explicitly named, \non the motivation that, given a CoreDS term, continuations can be introduced with local and structure-preserving \nrewrite rules. In this section, we have described the mappings J$(d and ud between plain DS terms and \nCoreDS terms, Sev\u00aderal CoreDS terms may encode a single DS term. We have characterized this flexibility \nwith a Galois connection, and have specified the corresponding partial orders on DS and on CoreDS terms. \n5 CoreCPS and CPS In CoreC!PS, intermediate continuations must be named, whereas in CPS they may occur \nanonymously. Thus the transformations between these two languages simply amount to introducing or reducing \nredexes that name continuations. Because the sequencing order of CPS is explicit, while that of CoreCPS \nis implicit in the definition of schedule, some care must be taken in the transformation from CPS to \nCoreCPS to preserve the intent of the CPS term. u= CoreCPS ~ CPS N. 5.1 Unnaming Core CPS terms The \ntransformation U= unnames CoreCPS terms by (1) choosing a sequencing order and simplifying the schedule\u00adexpressions, \nand (2) by performing administrative reduc\u00adtions over the resulting CPS term [29]. Choosing a sequencing \norder amounts to selecting a def\u00adinition of schedule. The following function schedulez se\u00adquences two \ncomputations from left to right: schedule2 = A (cl, C2) . AK . c1 (Aol . oz (Aw . K (q, 02))) Simplifying \na schedule-expression amounts to moving its se\u00admantics into the syntax of the CPS term. 5.2 Naming CPS \nterms The primary task of the DS transformation is to encode a flat, tail-recursive term into a tree-like \n(in general non-tail recursive) term. This mapping from a flat, string-like term into a tree is reminiscent \nof parsing a concrete-syntax string into an abstract-syntax tree. Indeed, the analogy holds for CPS terms \nthat encode the sequencing order of the DS lan\u00adguage, e.g., left-to-right as in SML. For example, the \nCPS term Ak. f(z, Ave. g(z, Av,. wo(vl, Av2.h(z, Av3. v2(v3, k:))))) straightforwardly yields Terms that \ndo not encode the sequencing order or that may contain side-effects or control-effects require a more \nde\u00adtailed analysis, particularly if the DS language is Scheme, where the sequencing order of subexpressions \nis unspecified. For example, the term Ak. f(z, Awe. h(x, Av3. g(z, Avl. vo(ol, Av2. v2(v3, k))))) should \nyield Az.letvo =jzinlet v,=hzin(oo(g z)) v, We conjecture, however, that the sequencing order of most \nCPS terms is overspecified, and thus for most terms the straightforward tree-building algorithm applies. \nIn any case, an effect analysis would come in handy here. 5.2.1 Canonically-sequentiafized CPS terms \nThe task in naming a CPS term is to reconstruct the tree structure of the original DS term, hiding the \nsequencing cnr\u00adder in the definition of the schedule operator. A CPS term essentially encodes a postfix \ntraversal of this tree [7]. Each expression is evaluated tail-recursively and yields a value that is \ndenoted by the actual parameter of the continuation. Once a parameter is used in a term to yield a new \nvalue, it is never used again. Thus the problem of const rutting a tree from a CPS expression is analogous \nto the problem of pars\u00ading a program written in a postfix language. In particular, independent subexpressions \nin a CPS term can be naturally detected using a stack. The transformation A(C traverses the continuation, \npush\u00ading the temporaries (i. e., the continuation parameters) on a stack. At consumption points (i. e., \nwhere the continu\u00adation parameters occur), we pop the stack and match the temporaries according to the \nsequencing order of the target DS language. For example, if the target language is SML where the sequencing \norder is left-to-right, then a prefix of the stack must equal the sequence of the temporaries of the \nexpression. On the other hand, if the target language is Scheme where the sequencing order is unspecified, \nthen we only require that the temporaries in the expression be equal to the set of temporaries on top \nof the stack. In a CPS term that encodes the sequencing order of the DS language canonically, the temporaries \non the top of the stack are always the ones required. The evaluation of the corresponding expressions \ndoes not need to be explicitly se\u00ad quenced. 5.2.2 Other CPS terms A CPS term is not, however, constrained \nto reflect precisely a postfix traversal of some tree. Temporaries need not be declared in a last-in, \nfirst-out fashion. If a subexpression is evaluated early, then the CPS term reflects a sequencing constraint \nthat should be preserved in the CoreCPS term. In this case the temporaries at the top of the stack are \nnot the temporaries required for the current expression, since one denotes the subexpression evaluated \nearly. In that situation the expressions denoted by all the variables lower on the stack must be explicitly \nsequenced in the CoreCPS result. 5.2.3 Computational effects Side effects and control effects do not \nraise particular prob\u00adlems in a language where sequencing order is fixed. In a language such as Scheme, \nhowever, some kind of effect anal\u00adysis is needed to avoid assuming that every expression has a computational \neffect otherwise we would have to trans\u00adlate a term such as A(z, k). j(z, Ave. g(z, Avl. vo(v,, k))) \n iut o ~z.letffo=~zinvo(gz) instead of Am. jz(gz) Given safe effect information, a CPS term can be staged \nusing the above algorithm, modified to sequentialize sub\u00adterms that may contsin effects.  5.3 A Galois \nconnection We now examine to what extent tic and Uc are inverses. For each language we give a semantics-preserving \nmapping from terms to normalized terms. Then, we show that the partial orderings generated by these functions \nare related as a Galois connection. As in the case of the naming and unnaming of DS terms (ct. Section \n4.3), the naming and unnaming of CPS terms are not inverse transformations. In fact the transformations \nare not inverses on either CoreCPS or CPS terms. u= CoreCPS ~ * CPS N. Naming and unnaming are not inverse \ntransforma\u00adtions on CPS terms because let-expressions may name continuations that are used only once. \nN. trans\u00adforms all continuation-naming let-expressions into schedule\u00adexpressions. When the term is transformed \nback into CoreCPS a continuation-naming let-expression is introduced only if the continuation identifier \noccurs more than once. Thus the transformations are not inverses on CPS terms containing useless let-expressions. \nBecause U. does not pro\u00adduce these useless let-expressions, the transformations are inverses on the result \nof unnaming CoreCPS terms. The transformations are not inverses orI CoreCPS terms because Z4c moves nested \nlet-and letrec-expressions out\u00adward. Because AfC does not create nested let-and letrec\u00adexpressions, the \ntransformations are inverses on the result of naming CPS terms. More formally, NC and Z.4care related \nm follows: Ncouco Nc = N. U.ON. OU. = U. { These equations imply the following: u.o Ncou. oNc = t/coNC \nNcouco N.ou. = Ncoi% { which say that the transformations are inverses on terms that have been named \nand unnamed once. We can thus define the functions SCPS and Sco,ecP5, which normalize CPS and CoreCPS \nterms respectively. Scps[e] = U.(N.(e)) { Sc.,.cps[e] = N.(~.(e)) SCPS and SCo,eCpS can be defined inductively \nover source terms using the observations outlined above. ScOrecP5 must be defined relative to a fixed \nsequencing order. Using the mappings SCPS and SCO,,CPS we may define the partial orders gCpS and ~c.,ecps \non CPS and CoreCPS terms respectively. These partial orders relate terms to semantically-equivalent normalized \nterms, and are defined as follows. Again we use Stoy s diacritical convention of overlining CPS terms \nwith an acute accent and CoreCPS terms with a grave accent . et ~cps e> ~ e; = UC(iVC(e\\)) { e 1 gcorecps \n.5 * J2 = Nc(u.(e 1)) Together these two equations imply a Galois connection: ei ~CPS @(e 2) U &#38;(el) \nZcorecps s 2  5.4 Conclusion The administrative reductions of the full CPS transforma\u00adtion make it difficult \nto predict the shape of the result of transforming a DS expression. We have isolated these ad\u00administrative \nreductions in the transformation from CoreCPS to CPS. The definition of the partial order on CoreCPS \nterms shows how the administrative reductions affect the shape of a term. The partial order on CPS terms \nreflects a mismatch be\u00adtween how contexts may be specified in CPS and how they are specified in DS. Thus \nthere is some flexibility in the relationship between CoreCPS and CPS terms. We have characterized this \nflexibility as a Galois connection. 5.5 Caveat This particular partial order may not be the most appropri\u00adate \none, though, because the introduction and elimination of continuations is not order-preserving. For example, \nbindv=fz ~cO.eDs bind v = f z in bind w=gx in let w= v invw in bindw=gx invw  but introducing continuations \nin these two terms as specified by Figure 7 does not yield two terms that are related by the partial \norder LCO,,CPS. We are currently looking for partial orders that would be preserved by the introduction \nand elimination of continuations. 6 First-Class Continuations (outline) In an earlier work [11], we describe \nhow to handle first\u00adclaas continuations in the DS transformation. The technique scales up to the staged \ntransformation. Briefly stated, we instrument the BNF of CPS terms with the set of all the continuation \nidentifiers that are lexically visible, and we re\u00adlax the constraint that only the current continuation \nbeap\u00adplied. Instead we let any continuation identifier be applied. Then we analyze CPS terms to determine \nwhether a contin\u00aduation k is used exceptionally in a term e. We then need to distinguish terms where \ncontinuations are declared. Any declaration of a continuation that is used exceptionally is translated \ninto a call/cc. Conversely, any application of a continuation that is not the current one is translated \ninto a throw. Correspondingly, we instrument the BNF of DS terms with the set of all first-class continuation \nidentifiers that arelexically visible, and we enforce that only acontin\u00aduation identifier can be thrown \nvalues. 7 A complete example We consider the binary tree specified by the data type in Figure 8. This \ndeclaration and the corresponding control structure caseType are inspired by SML and usedin Consel s \npartial evaluator Schism [5]. We are using them for read\u00adability. Also for readability, we have maintained \ndirect-style primitive operators in Figures 11 and 12. Let us determine whether a given tree of positive \nnum\u00adbers weighs more than a given number. Rather than first computing the weight of the tree and then \ncomparing this weight with the number, we combine the comparison with computation of the weight, using \ncall/cc to abort the com\u00adputation as soon as the weight exceeds the number. This leads to more comparisons \non average but to a potentially shorter tree traversal. The corresponding direct-style pro\u00adgram is given \nin Figure 9. Let us transform this program into CoreDS by naming all intermediate (i. e., temporary) \nvalues. The result is given in Figure 10. We now turn this program into CoreCPS. Thanks to bind, the \nevaluation steps are explicit and we can express them as continuation abstractions, using a schedule \nexpres\u00adsion to sequentialize these abstractions. The resulting pro\u00adgram is given in Figure 11. Finally, \nwe can choose e.g., left-to-right sequencing order to transform the CoreCPS program into CPS. The resulting \nCPS program is given in Figure 12. Each step is reversible. First, continuations are named and continuation \nextensions (and thus sequencing order) are abstracted, thereby going from the CPS program of Figure 12 \nto the CoreCPS program of Figure 11. Second, continua\u00adtion abstractions are turned into evaluations of \nexpressions, thus going from the CoreCPS program of Figure 11 to the CoreDS program of Figure 10. Third, \nexpressions are in\u00adlined (provided the order of their evaluation does not con\u00adflict), leading from the \nCoreDS program of Figure 10 to the DS program of Figure 9. 8 Conclusion CPS is generally agreed to be \nuseful but it overcommits a program to a particular sequentialization. We have factored out this commitment \nby separating stages in the CPS trans\u00adformation: Proposition 1 (CPS transformation) Naming serious subterms \nin a DS term, introducing continuations in the res\u00ad ulting CoreDS term, choosing a sequencing order, \nand un\u00ad narning the resulting Core CPS term amount to transforming the DS term into 6 PS with respect \nto the same sequencing order. Proofi By composition. Unnaming the CoreCPS term amounts to performing \nthe administrative reductions of the CPS transformation. Proposition 2 (DS transformation) Naming contiraua\u00ad \ntions in a CPS term, eliminating continuations in the re\u00adsulting Core CPS term, and unnaming the resulting \nCoreLM term amount to transforming the CPS term into DS, re\u00adspecting the sequencing order. Proof: By \ncomposition. Unnaming the CoreDS term amounts to performing the administrative reductions of the DS transformation. \nc1 Fundamentally, we have isolated the heart of the CPS and of the DS transformations i.e., the int \nreduction and elimination of continuations with local, structure\u00adpreserving, and reversible rewrite \nrules. The CPS and the DS transformations looked complicated because of red tape the administrative \nreductions [29]. We have isolated th~ red tape and characterized it with two Galois connections: naming \nand unnaming of DS and CPS terms. naming introduction of naming ~ values Core -L Core .- CPS DS ~ DS \n- Cps ~ continuations Galois Galois connection btjection connection This staging scales up to the call-by-name \nCPS transfor\u00admation [29] because this transformation can be staged into (1) thunk introduction and (2) \ncall-by-value CPS transfor\u00admation [10]. Thus the call-by-value CPS transformation is a more basic CPS \ntransformation, in some sense. 9 Issues and Future Work The CPS transformation must offer useful properties \nbe\u00adcause it is used in many places: for flow analysis [32], for parallelization [19], for compiling [1, \n33], and for partial eval\u00aduation [2, 6], to name a few. Yet it is not easy to pinpoint which properties \nCPS offers that are not already therein DS, besides the obvious: CPS terms area subset of DS terms and \nthus they can be modeled and processed in a simpler way. A staged CPS-transformation such as the one \npresented here enables one to examine each stage to determine which one triggers which property. To this \nend, we need to refine the partial order over CoreDS and CoreCPS terms because the introduction and elimination \nof continuations is not order-preserving, cur\u00adrently (ct. Section 5.5). Better partial-orders would make \nit possible to consider the transformation over a least term. (defineType Tree (Leaf value) (node left \nright) ) Fiszure 8: Binarv tree datatvDe declaration (define overweight? (lambda (x t) (call/cc (lambda \n(c) (letrec ([traverse (lambda (t) (let ([v (caseTv~e t [(Le~2 value) value] [(mode left right) (+ (traverse \nleft) (traverse right) )1 )] ) (if (> w x) (throw c *t) 8)))1) (let (L (traverse t)]) *f)))))) Figure \n9: DS program ( define overweight? (lambda (x t) (call/cc (lambda (c) (letrec ( [traverse (lambda (t) \n(let ([w (casaType t [(I,eaf value) value] [(mlode left right) (bind ([1 (tr~verse left)] [r (traverse \nright )1 ) (+ 1 r))])]) (if O w x) (throw c *t) u)))]) (bind ([v (traverse t)] ) (let ([\u00ad vI) #f))))))) \nFigure 10: Staged DS program (all intermediate values are named) (define overweight? (lambda (X t C) \n(letrec ( [traverse (lambda (t k) (let ([k (lambda (v) (let ([u vI) (if (> =x) (c #t) (k w))))]) (caseType \nt [(Leaf value) (k value)] [(node left right) (schedule (list (lambda (k) (traverse left k)) (lambda \n(k) (traverse right k)) ) (lambda (1 r) (k (+ 1 r))))])))]) (schedule (list (lambda (k) (traverse t k))) \n(lambda (v) (let ([-v]) (c *f)))))) Figure 11: Staged CPS program (all intermediate continuations are \nabstracted) (define overweight? (lambda (x t c) (letrec ( [traverse (lambda (t k) (let ([k (lambda (v) \n(let ([w v]) (if (> u x) (c #t) (k u))))]) (caseTypa t [(1.eaf value) (k value)] [( Iiocle left right) \n(traverse left (lambda (1) (traverse right (lambda (r) (k (+ 1 r))) ))1)))1) (traverse t (lambda (v) \n(let ([\u00ad vI) (c *f))))))) Figure 12: CPS program with left-to-right sequencing order &#38; [(EII El \n. . . En)] p K = ap~is (permute (~ [Eo] p, .... &#38; [En] p)) (( A(to, .5,, .... en). fapplicate co \n(151,.... cn) 6) o rmperrnute) where K = E*+C permute : (K+ c)* + (K+ c)* unpermute : E* + E applicate \n: E+.E*+K+G single : (E+C)+K apli8 : (K + C)* +K-FC aplis = A(fo, .... fn). A K. fo (single(A 60 . ...fn \n(sing/e(A6n .K (60, .... En)))...)) and apks is the only new auxiliary function in the formal semantics \nof Scheme. Figure 13: Denotational semantics of applications, in Scheme (fragment) 133 &#38; [(Eo El \n. . . En)] p K = schedde (t [Eo] p, .... t [En] p) (A (60, 61, .... Cn) . applicateco (cI, .... fn) 6) \n where ~ schedule appkate = : : ll +c (K -C)* + E+E*4K-+C K+C and schedule is the only new auxiliary \nfunction in the formal semantics of Scheme. Figure 14: Denotational semantics of applications, in Scheme \n(revised fragment) Then we could determine to which extent it would be suf\u00ad ficient to simplify a term \nwithin the same style (i.e., DS, CoreDS, CoreCPS, or CPS) rather than transforming it into another style. \n 10 Applications 10.1 The Scheme programming language A few years ago [20], Jones and Muchnick proposed \na pro\u00ad gramming language design based on binding times in\u00adcluding lexica,l-andysis time, syntax-analysis \ntime, macro\u00adexpansion time, semantics-analysis time, code-generation time, link-time, and run-time. They \nalso proposed that the compiler for such an ideal programming language should be structured in phases \ncorresponding to each of the bindhg times, and that no reference should be made to an earlier binding \ntime. Our staged CPS-and DS-transformations fol\u00adlow this spirit by separating evaluation order from sequenc\u00ading \norder. They can also be applied to the Scheme program\u00adming language as follows. According to the semantics \nof Scheme [4], sub-expr\u00adessions in an application are evaluated in an unspecified order (a controversial \nissue among Scheme programmers). Our C H transformation remains uncommitted with respect to the sequencing \norder and thus it suggests we rephrase a part of the Scheme semantics as follows. The formal semantics \nof Scheme [4, Sections 7.2.3 &#38; 7.2.4] uses two inverse functions permute and unper\u00adrnute to ensure \nthe sequencing-order independence of sub\u00adexpressions in an application. These functions map a piece of \nabstract syntax into a new piece of abstract syntax, hiding the fact that this semantics is actually \ncompositional [35]. From the point of view of binding-times, this specification freezes the sequencing \norder at syntax-analysis time. Our uncommitted CPS transformation enables one to delay this permutation \nuntil a later binding-time, by permuting the cornput at ions of these sub-expressions. Figure 13 presents \nthe new fragment specifying the semantics of applications, and Figure 14 re-expresses it in terms of \n:schedule. In addition, unscheduled CPS introduces a new style of CPS programming, without oversequentialization, \ne.g., to simulate nondeterminism. 10.2 Compile-time analyses Let us now assess the new possibilities \noffered by this staged transformation, and whether we can isolate the use\u00adfulness of the CPS transformation \ninto one of the stages. CoreCPS seems quite promising as an intermediate language for compile-time analyses. \nIt enjoys many of the useful prop\u00aderties of both DS and CPS. Instantiating a CoreCPS term with any scheduling \nstrat\u00adegy yields a CPS term. Therefore, CoreCPS enjoys the same traditional advantages as CPS and leads \nto naturally for\u00adward program analyses4 [6] and simpler program analyzers [32]. Accordingly, compile-time \nanalyses still yield more pre\u00adcise results, both intra-procedurally and inter-procedurally [2, 6], given \nCoreCPS-transformed terms [28]. Compile-time analyses benefit from CPS to differing de\u00adgrees. Binding-time \nanalysis produces better information on CPS terms because information moves across the boundaries of \nconditionals and procedure calls. Here sequencing order is irrelevant, so ordinary CPS is adequate. On \nthe other hand, while the fact that sequencing order is explicit in CPS terms improves single-threading \ndetection, Fradet observes that whether a variable is classified to be single-threaded may depend on \nthe choice of the sequencing order [15]. By main\u00adtaining the boundaries between independent subexpressions, \nCoreCPS allows the single-threading analysis to determine the best sequencing order for each expression \nlocally, when it makes a difference. Finally, some analyses do not benefit from CPS transforming the \nsource program. For example, analyses for register allocation require reordering indepen\u00addent subexpressions \nto produce useful results. This reorder\u00ading is notoriously complex on CPS terms, but in CoreCPS the independent \nsubexpressions are easily accessible. The analysis can proceed almost unchanged, taking advantage of \nthe bijection between CoreCPS and CoreDS terms. Acknowledgments We are grateful to Dan Friedman for his \nsupport and to the other members of the programming language groups of In\u00addiana University and Kansas \nState University for their feed\u00adback. Special thanks to Andrzej Filinski, Jiirgen Koslowski, Karoline \nMalmkj=r, and David Schmidt for commenting an earlier draft, and to the referees for their guidance. \n4Since CPS terms are tail-recursive, there is nothing to propagate backwards but the final result. The \ndiagrams illustrating the separation of stages were [15] drawn with Kristoffer Rose s ~-pic package. \n[16]References [1] Andrew W. Appel. Compiling with Continuations. Cambridge University Press, 1992. [17] \n[2] Anders Bondorf. Improving binding times without ex\u00ad plicit CPS-conversion. In LFP 92 [21], pages \n1 10. [3] Anders Bondorf and Olivier Danvy. Automatic auto\u00ad projection of recursive equations with global \nvariables and abstract data types. Science of Computer Program\u00ad [18] ming, 16:151 195, 1991. [4] William \nClinger and Jonathan Rees (editors). Revised4 report on the algorithmic language Scheme. LISP Pointers, \n1V(3):1-55, July-September 1991. [19] [5] Charles Consel. Report on Schism 92. Oregon Graduate Institute, \nBeaverton, Oregon, October 1992. Research Report. [20] [6] Charles Consel and Olivier Danvy. For a better \nsupport of static data flow. In Hughes [18], pages 496-519. [7] Olivier Danvy. Three steps for the CPS \ntransforma\u00ad tion. Technical Report CIS-92-2, Kansas State Univer\u00ad [21] sity, Manhattan, Kansas, December \n1991. [8] Olivier Danvy. Back to direct style. In Bernd Krieg- Briickner, editor, Proceedings of the \nFourth European [22] Symposium on Programming, number 582 in Lecture Notes in Computer Science, pages \n130-150, Rennes, France, February 1992. [9] Olivier Danvy and Andrzej Filinski. Representing con\u00ad trol, \na study of the CPS transformation. Mathematical Structures in Computer Science, 2(4), 1992. To appear. \n[23] [10] Olivier Danvy and John Hatcliff. Thunks (contin\u00ad ued). In Proceedings o~ the Workshop on Static \nAnal\u00ad ysis WSA 92, volume 81-82 of Bigre Journal, pages 3 11, Bordeaux, France, September 1992. IRISA, \nRennes, France. [24] [11] Olivier Danvy and Julia L. Lawall. Back to direct style II: First-class continuations. \nIn LFP 92 [21], pages 299\u00ad 310. [12] Matthias Felleisen and Robert Hieb. The revised report on the syntactic \ntheories of sequential control and state. Theoretical Computer Science, 103(2):235-271, 1992. [25] [13] \nAndrzej Filinski. Declarative continuations: An inves\u00ad tigation of duality in programming language seman\u00ad \n[26] tics. In David H. Pitt et al, editors, Categorg Theory and Computer Science, number 389 in Lecture \nNotes in Computer Science, pages 224-249, Manchester, UK, September 1989. [27] [14] Michael J. Fischer. \nLambda calculus schemata. In Proceedings of the ACM Conference on Proving Asser\u00ad tions about Programs, \npages 104-109. SIGPLAN No\u00ad tices, Vol. 7, No 1 and SIGACT News, No 14, January 1972. [28] PSSCSJ Fradet. \nSyntactic detection of single-threading using continuations. In Hughes [18], pages 241-258. Daniel P. \nFriedman, Mitchell Wand, and Christopher T. Haynes. Essentials of Programming Languages. MIT Press and \nMcGraw-Hill, 1991. Timothy G. Griffin. A formulae-as-types notion of cou\u00adtrol. In Proceedings of the \nSeventeenth Annual ACM Symposium on Principles oj Programming Languages, pages 47-58, San Francisco, \nCalifornia, January 1990. ACM Press. John Hughes, editor. Proceedings of the Fifth ACM Conference on \nFunctional Programming and Computer Architecture, number 523 in Lecture Notes in Computer Science, Cambridge, \nMassachusetts, August 1991. William L. Harrison III. The interprocedural analy\u00adsis and automatic parallelization \nof Scheme programs. LISP and Symbolic Computationj 2(3/4):179-396, Oc\u00adtober 1989. Neil D. Jones and Steven \nS. Muchnick. Some thoughts towards the design of an ideal language. In ACM Con\u00adference on Principles \nof Programming Languages, pages 77-94, 1976. Proceedings of the 1992 ACM Conference on Lisp and Functional \nProgramming, San Francisco, California, June 1992. Karoline Malmkjzer. On static properties of specialized \nprograms. In Michel Billaud et al., editor, Analg9e Sta\u00adtigue en Programmation Equationnelle, Fonctionnelie \net Logique, volume 74 of Bigre Journal, pages 234 241, Bordeaux, France, October 1991. IRISA, Rennes, \nFrance. Karoline Malmkjzer. Predicting properties of residual programs. In Charles Consel, editor, ACM \nSIGPLAN Workshop on Partial Evaluation and Semantics-Based Program Manipulation, Research Report 909, \nDepart\u00adment of Computer Science, Yale University, pages 8 13, San Francisco, California, June 1992. Austin \nMelton, David A. Schmidt, and George Strecker. Galois connections and computer science applications. \nIn David H. Pitt et al., editors, Category Theory and Computer Programming, number 240 in Lecture Notes \nin Computer Science, pages 299 312, Guildford, UK, September 1986. Robin Milner, Mads Tofte, and Robert \nHarper. The Definition of Standard ML. The MIT Press, 1990. Eugenio Moggi. Computational lambda-calculus \nand monads. In Proceedings of the Fourth Annual Sgmpfi sium on Logic in Computer Science, pages 14 23, \nPa\u00adcific Grove, California, June 1989. IEEE. Chetan R. Murthy. An evaluation semantics for classi\u00adcal \nproofs. In Proceedings of the Sixth Symposium on Logic in Computer Science, pages 96 107, Amsterdam, \nThe Netherlands, July 1991. IEEE. Flemming Nielson. A denotational framework for data flow analysis. \nActs Injormatica, 18:265-287, 1982. [29] Gordon D. Plotkin. Call-by-name, call-by-value and the J-calculus. \nTheoretical Computer Science, 1:125-159, 1975. [30] John C. Reynolds. Definitional interpreters for higher\u00adorder \nprogramming languages. In Proceedings of 25th ACM National Conference, pages 717-740, Boston, 1972. [31] \nAm, Sabry and Matthias Felleisen.. Reasoning about programs in continuation-passing style. In LFP 92 \n[21], pages 288-298. [32] Olin Shivers. Control-Flow Analysis of Higher-Order Languages or Taming Lambda. \nPhD thesis, CMU, Pittsburgh, Pennsylvania, May 1991. Technical Report CMU-CS-91-145. [33] Guy L. Steele \nJr. Rabbit: A compiler for Scheme. Tech\u00adnical Report AI-TR-474, Artificial Intelligence L abo\u00adratory, \nMassachusetts Institute of Technology, Cam\u00adbridge, Massachusetts, May 1978. [34] Allen Stoughton. Substitution \nrevisited. Theoretical Computer Science, 59:317-325, 1988. [35] Joseph E. Stoy. Denotational Semantics: \nThe Scott-Stracheg Approach to Progravnming Language Theory. MIT Press, 1977. [36] Christopher Strachey \nand Christopher P. Wadsworth. Continuations: A mathematical semantics for handling full jumps. Technical \nMonograph PRG-11, Oxford Uni\u00adversit y Computing Laboratory, Programming Research Group, Oxford, England, \n1974. [37] Philip Wadler. The essence of functional programming (tutorial). In Proceedings of the Nineteenth \nAnnual ACM Symposium on Princtpies of Programming Lan\u00adguages, pages 1 14, Albuquerque, New Mexico, January \n1992. ACM Press. [38] Mitchell Wand. Correctness of procedure represen\u00ad tations in higher-order assembly \nlanguage. In Steve Brookes, Michael Main, Austin Melton, Michael Mis-Ilove, and David Schmidt, editors, \nMathematical Foun\u00addations of Programming Semantics, volume 598 of Lec\u00adture Notes in Computer Science, \npages 294 311, Pitts\u00adburgh, Pennsylvania, March 1991. 7th International (Conference.   \n\t\t\t", "proc_id": "158511", "abstract": "<p>The continuation-passing style (CPS) transformation is powerful but complex. Our thesis is that this transformation is in fact <italic>compound</italic>, and we set out to <italic>stage</italic> it. We factor the CPS transformation into several steps, separating aspects in each step: (1) Intermediate values are named; (2) Continuations are introduced; (3) Sequencing order is decided and administrative reductions are performed.</p><p>Step 1 determines the evaluation order (<italic>e.g.</italic>, call-by-name or call-by-value). Step 2 isolates the introduction of continuations and is expressed with local, structure-preserving rewrite rules &#8212; a novel aspect standing in sharp contrast with the usual CPS transformations. Step 3 determines the ordering of continuations (<italic>e.g.</italic>, left-to-right or right-to-left evaluation) and leads to the familiar-looking continuation-passing terms.</p><p>Step 2 is completely reversible and Steps 1 and 3 form Galois connections. Together they lead to the direct style (DS) transformation of our earlier work (including first-class continuations): (1) Intermediate continuations are named and sequencing order is abstracted; (2) Second-class continuations are eliminated; (3) Administrative reductions are performed.</p><p>A subset of these transformations can leverage program manipulation systems: CPS-based compilers can modify sequencing to improve <italic>e.g.</italic>, register allocation; static program analyzers can yield more precise results; and overspecified CPS programs can be rescheduled. Separating aspects of the CPS transformation also enables a new programming style, with applications to nondeterministic programming. As a byproduct, our work also suggests a new continuation semantics for unspecified sequencing orders in programming languages (<italic>e.g.</italic>, Scheme).</p>", "authors": [{"name": "Julia L. Lawall", "author_profile_id": "81100529486", "affiliation": "", "person_id": "PP15035219", "email_address": "", "orcid_id": ""}, {"name": "Olivier Danvy", "author_profile_id": "81100394275", "affiliation": "", "person_id": "PP15031217", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/158511.158613", "year": "1993", "article_id": "158613", "conference": "POPL", "title": "Separating stages in the continuation-passing style transformation", "url": "http://dl.acm.org/citation.cfm?id=158613"}