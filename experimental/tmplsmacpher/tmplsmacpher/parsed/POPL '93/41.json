{"article_publication_date": "03-01-1993", "fulltext": "\n Tutorial Notes on Charles Consel Pacific Software Research Center Department of Computer Science and \nEngineering Oregon Graduate Institute of Science &#38; Technology* consel@cse.ogi .edu Abstract The last \nyears have witnessed a flurry of new results in the area of partial evaluation. These tutorial notes \nsurvey the field and present a critical assessment of the state of the art. 1 Introduction Partial evaluation \nis a source-to-source program transforma\u00adtion technique for specializing programs with respect to parts \nof their input. In essence, partial evaluation removes layers of inter\u00adpretation. In the most general \nsense, an interpreter can be defined as a program whose control flow is determined by its input data. \nAs Abelson points out, [43, Foreword], even programs that are not themselves interpreters have important \ninterpreter-like pieces. These pieces contain both compile-time and run-time constructs. Partial evaluation \nidentifies and eliminates the compile-time con\u00adstructs. 1.1 A complete example We consider a function \nproducing formatted text. Such functions exist in most programming languages (e.g., format in Lisp and \nprintf in C). Figure 1 displays a formatting function written in Scheme [21]. Given a channel, a control \nstring, and a Iist of val\u00adues, this-function outputs text in the channel by interpreting the control \nstring to determine how to format the list of values. For conciseness, this function handles only three \nformatting directives: M, S and %. The first two directives specify that the corresponding element in \nthe list of values must be printed as a number or as a string, respectively. The third directive is interpreted \nas an end-of-line character. Any other character is printed verbatim. For simplicity, we assume that \nthe control string matches the list of values. Most of the time, format is called with a constant control \nstring. This situation is ideal for partial evaluation, particularly when format is called with the same \nconstant control string repeatedly. Specializing format with respect to this control string enables one \nto interpret it only once. The speciahzed function takes a channel 19600 N.W. von Neumann Drive, Beaverton, \nOregon 97006-1999, USA. tManhattan, Kansas 66506, USA. part of this work was supported by NSF under grant \nCCR9102625. Partial Evaluation Olivier Danvy Department of Computing and Info. Sciences Kansas State \nUniversity t danvyocis.ksu.edu and a list of values and returns an updated channel. It is built as a \ndedicated combination of printing operations. Figure 2 presents a version of format specialized with \nrespect to the control string -~ is not S-% . The interpretive overhead of i orrnet has been entirely \nremoved. All the operations manipulating the control string have been per\u00adformed at compile-time. No \nreferences to the control string are left in the residual program. The specialized function only con\u00adsists \nof operations manipulating the run-time arguments, i.e., the channel and the list of values to be formatted. \nThis example illustrates the essential purpose of partial evalu\u00adation: eliminating interpretive overhead \n here the interpretation of the control string.  1.2 Applications Because of its conceptual simplicity, \npartial evaluation has been applied to a wide variety of areas that include compiling and com\u00adpiler generation \n[31, 34, 39, 58, 68, 70, 73], string and pattern matching [28, 40, 69, 92, 102], computer graphics [83], \nnumerical computation [8], circuit simulation [5], and hard real-time systems [891. .. Partial evaluation \nhas been studied in the context of a wide variety of programming languages. In the area of logic program\u00adming, \npartial deduction is the focus of much work [44, 47, 77, 76]. Partial evaluators for imperative languages \nlike Pascal [82] and C [3] have been developed. In equational languages, partial evalu\u00adation have been \nused to optimize rewriting techniques [101], and more recently, it has been applied to Lafont s interaction \nnets [6]. A broader dwcussion and more detailed references on contem\u00adporary work in the area of partial \nevaluation and on applications of partial evaluation can be found in Jones, Gomard, and Sestoft s new \nbook [66]. Overview. This paper is organized as follows. Section 2 presents the principles and practice \nof partial evaluation. Section 3 briefly reviews the state of the art about partial evaluators for Scheme \nand imperative languages. Section 4 addresses the problem of termination. Section 5 analyzes the tradeoff \nbetween space and time when using partial evaluation. Section 6 presents some ap placations. Section \n7 addresses related work. Finally, Appendix A dwcusses self-application. 2 Principles and Practice of \nPartial Evaluation This section first describes the extensional aspects of partial eval\u00aduation. Then, \nwe present strategies to achieve partial evaluation and outlines work formalizing these strategies. Finally \nwe discuss generalized forms of partial evaluation. ;;; format: Port * ControlString * List (Value) > \nport (define format (lambda (port control-string values) (let ( [end (string-length control-string)] \n) (letrec ([traverse (lambda (port offset values) (if (= offset end) port (case (string-ref control-string \noff set ) [(#\\ ) (case (string-ref control-string (+ offset 1)) [( X\\ Ii) (traverse (write-number port \n(car values)) (+ offset 2) (cdr values) )1 [(#\\S) (traverse (Write-string port (car values)) (+ offset \n2) (cdr values) )1 [(#\\%) (traverse (twit e-ne=line port) (+ offset 2) values)] [else (error fomat illegal \ncontrol string: S control-string)] )1 [else (let ( [new-offset (letrec ( [upto-tilde (lambda (new-offset) \n(cond [(= new-offset end) nev-offsetl [(equal? (string-ref control-string ne=-off set ) #\\-) new-offset] \n[else (upto-tilde (+ neu-off set 1) )1 ) )1 ) (upto-tilde offset))]) (traverse (write-string port (substring \ncontrol-string offset new-offset)) new-offset values))])))]) (traverse port O values) ) ) ) ) ; ; ; given \nWrite-string: Port * String -> Port ... write-number: Port * Humber -> Port ... ,,> ,,, write-newline: \nPort -> Port Figure 1: A formatting function ; ; ; for any port p and list of tuo values vs, ;;; (format. \ni p VS) = (format p -~ is not s-% vs) (define format. i (lrurrbda (port values) (write-neuline (write-string \n(write-string (mite-nnrnber port (car values)) 1is not ) (car (cdr values)))))) Figure 2: Specialized \nversion of Figure 1 .1 Partial evaluation: what iiven a generaJ program and part of its input, we want \nto spe\u00adiahze this program with respect to this known information. Consider a program p and its input \ni, and say that somehow re can split i into a static (i.e. known) part s and a dynamic (i.e. nknown) \npart d. For example, p might take two arguments, one f which is a static value, and the other one of \nwhich is a dynamic alue. Given a specializing function S, we can specialize p with wpect to s: S(p, (s, \n.)) = p. By definition, running the residual program p~ must yield the same result as the general program \nwould yield, provided both terminate: runp (s, d) = run p. (-, d) As illustrated in the introduction, \np. can run faster than p. In fact, S is Kleene s S~-function [74]. This function is com\u00adputable and thus \nit can be implemented: the result is what is called a partial evaluator (hereafter denoted P-E). run \nPE (p, (s, J) = S(p, (s, J) 2.2 Partial evaluation: how Specializing a program with respect to all of \nits input amounts to running thk program and producing a constant residual program, i.e., a program that \ntakes an empty input and produces an already computed value. Therefore, a partial evaluator must include \nan interpreter to perform reductions at specialization-time. Dually, specializing a program with respect \nto none of its input amounts to produce a (possibly simplified) version of this program. Therefore, a \npartial evaluator must include a compiler to construct the residual program. In the common case where \nthe source and the target languages coincide this compiler essentially mimics the 2.4 Formalizing partial \nevaluation identity function. Otherwise, it is a simple translator [60]. When specializing a program \nwith respect to some known parts of its input, a partial evaluator corresponds to a non-standard interpreter: \nit evaluates the expressions depending on static data (i.e., data available at specialization-time) and \nit reproduces the expressions depending on dynamic data (i. e., data available only at run-time). A partial \nevaluator propagates constant values and folds constant expressions. It also irdines functions by unfolding \ncalls, and produces specialized functions by residualizing calls. A monovariant specialize produces at \nmost one specialized function for every source function. A polyvariant specirdizer can produce many specialized \nversions of a source function [18]. 2.3 Online vs. offline partial evaluation Contemporary partial evaluators \nare divided in two classes: on\u00adline, monolithic partial evaluators and oflirae, staged partial evalu\u00adators. \nBefore Jones s Mix system, all partial evaluators were online [7, 55, 79, 99]. Today, both online and \noffline partial-evaluation strategies are the subject of active research. An online partial-evaluator \nis a non-standard interpreter. The treatment of each expression is determined on the fly. Online partial-evaluators \nin general are very accurate but at the price of a considerable interpretive overhead, Most programming \nlanguages are not simply implemented with an interpreter. Instead, their implementation is structured \nwith a compiler and a run-time system. Implementing partial evalua\u00adtion makes no exception, and offline \npartial evaluators are struc\u00adtured with a preprocessing phase ~ and a processing phase ~. The preprocessing \nphase ~ usually includes a binding-time anal\u00adysis [68, 88]. Given the binding-time signature of a source \npro\u00adgram (i. e., which part of the input is static and which part is dynamic), the blndmg-time analyser \npropagates this information in the source program, determining for each expression whether it can be \nevaluated at compile-time or whether it must be evaluated at run-time. This binding-time information \nis then used to guide the processing phase ~ that performs the specialization proper. Binding-time analysis \n1s necessarily approximate and thus offline partial-evaluators are usually less accurate than online \nones. Binding-time analysis has been intensively studied in the framework of abstract interpretation \n[13, 25, 32, 35, 87] as well as in the framework of type theory [52, 59, 90, 98, 103]. In offline, binding-time \nbased partial-evaluators, accurate binding-time in\u00adformation is critical because it determines the degree \nof the actual specialisation. Existing binding-time analyses handle higher-order functions and data structures. \nNewer ones are polyvariant. The binding-time information determines whether an expres\u00adsion can be evaluated \nat partial-evaluation time or must be eval\u00aduated at run-time. In an earlier work, we proposed to stage \nPE further by shifting the interpretation of bindh-rg-times from ~ to ~ [24, 29]. This shift substantially \nsimplifies the specialize. To get the best of both worlds, online and oiliine partial\u00adevaluation can \nbe combined as follows [11, 107]. Whenever the exact binding-time property of an expression can be determined, \noffline partial evaluation is used. Otherwise, the treatment for this expression is postponed until specialization-time, \nwhen con\u00adcrete values are available. Bondorf s partial evaluator for term\u00adrewriting systems uses this \nstrategy [11]. 1For example, a conditional expression whose consequent and alternative branches are not \nbound at the same time is classified to have the latest binding-time of its components. Much effort is \ndevoted to specifying and proving offliie partial\u00adevacuation. Gomard defines the denotational semantics \nof the specialization process for the J-calculus and proves its correctness [53]. Launchbury formulates \na binding-time analysis for a first\u00adorder applicative language with projections [62, 78]. This line of \nwork aims at proving the correctness of offline partial-evaluation with respect to the standard semantics \nof the source language. More generally, Consel and Khoo formally define online and of\u00adfline partial-evaluation \nfor a first-order applicative language [35]. They relate the standard semantics of the language to an \nonline partial-evaluation semantics. Then they show that binding-time analysis is an abstraction of the \nonline partial-evaluation seman\u00adtics. Finally, they derive the specialization semantics from the binding-time \nanalysis and the online partial evaluation semantics. The method can be applied to other languages. \n2.5 Generalizing specialization In the traditional presentation of partial evaluation, specializing a \nprogram with respect to part of its input is implicitly under\u00adstood as given the actual value for some \nformal parameters of a program, construct a specialized program with fewer formal pa\u00adrameters . This \npoint of view haa been progressively refined over the years. For example, Launchbury s partial evaluator \nis based on projections [78]. The input of a source program is divided into a static projection and a \ndynamic projection. In their parameterized partial-evaluation, Consel and Khoo ai-Iow a program to be \nspecialized not only with actual values of its input, but also with respect to any property of this input \n[32]. The generalization applies for both online and offline strategies. Parameterized partial-evaluation \nhaa been implemented at CMU [22] and at Yale [72] for a first-order subset of ML. Both systems handle \npartially-static data. The latter implementation includes both online and offline partial-evaluation \nphases. 2.6 The structure of a source program In general, partial evaluation forces one to be very conscious \nabout the structure and properties of one s source programs. Typically, a program specializes well when \nit processes the static part of the input independently of the dynamic part. For example, consider the \nfollowing two functions: (lambda (x y z) (lambda (x y z) (+(+xy) z)) (+x(+yz))) Unless the partial evaluator \nis instructed that addition is associa\u00adtive, specializing these two functions with respect to their two \nfirst parameters (say, 2 and 3, respectively) does not produce the same result: (lambda (z) (lambda (z) \n(+5z)) (+2(+3z))) The first function specializes better than the second one because it processes the \nstatic part of the input (i. e., x and y) indepen\u00addently of the dynamic part of the input (i.e., z). \nThus some information about the binding-time signature of a source program (i.e., which part of the input \nis static and which part is dynamic) enables one to structure this program to make it specialize better. \n2,7 The structure of a residual program The residual program obtained by specializing a source program \nwith respect to some static data is structured like the static data. For example, the control string \n}-M is not S-% is built itera\u00ad tively with a formattirw directive for numbers. then a few charac\u00ad . ters, \nthen a formatting directive for strings, and then a newline. Correspondingly, the residual program of \nFigure 2 is built to it\u00aderatively output a number, then a constant string, then a string, and then a \nnewline. This observation makes it easier to read residual programs. In addition, occurrences of some \nstatic data in the residual program signify that, in the source program, the static data are not pre\u00adcessed \nindeDendentlv of the dvnamic data. This leads to st rate\u00adgies for res{ructurin~ source p~ograms to improve \ntheir blnding\u00adtimes [14, 15, 30]. Independently, one is often surprised to find redundant tests in one \ns residual programs usually a tell-tale of unexpected redundancies in the source programs. As such, \na partial evzduator is a useful programming tool. 3 State of the Art We first review the state of the \nart of partial evaluators for call\u00adby-value functional languages such as Scheme [21], and then of partial \nevaluators for imperative languages. 3.1 Applicative languages Weise s partial evaluator Fuse is an online \nsystem and aims at applying parti~ evaluation to practical problems, such w circuit simulation [107]. \nThe design of Fuse is distinct from earlier on\u00adline partial evaluators in that it uses graphs as an intermediate \nlanguage andhasa strategy forincreasing sharing in residual pro\u00adgrams [71,94, 95]. Bondorf s partial \nevaluator Similix is an offline polyvariant sys\u00adtem and was explicitly designed to be self-applicable \nand mostly automatic, based on a fixed specialization strategy [16]. The sys\u00ad tem handles recursive equations, \ncustomizable primitive operators and global side-effects. It also includes a binding-time debugger [86]. \nSince then, it has been extended to handle higher-order func\u00adtions [13] and more recently partially-static \nvalues [15]. Today, the new version of Similix is based on Henglein s efficient type\u00adinference for binding-time \nanalysis [59]. Similix is freely available and is used as a black box in Harnett and Montenyohl s investiga\u00adtion \nof programming languages [58]. Independently, Gengler and Rytz have extended the system with a polyvariant \nbinding-time analysis and with partially static values [48, 96]. Consel s partial evaluator Schism is \nan offline polyvariant sys\u00adtem with a flexible specialization strategy, higher-order functions, and partially-static \nvalues [27]. Both the binding-time analysis and the specialization are polyvariant. The system includes \na binding-time based programming-environment [36]. Both source programs and specialized programs are \nexpressed in Scheme, ex\u00adtended with ML-like datatypes. 3.2 Imperative languages Partial evaluation of \nimperative programs has received much at\u00ad tention recently. Meyer developed an online partial evaluator \nfor a subset of Pascal [82]. Nirkhe and Pugh [89] describe a partial eval\u00aduator for hard real-time problems \nwhere programs are constrained by the user to keep a tight control over the transformation process. Andersen \nreports a self-applicable partial evaluator for a subset of the C programming language where blndlng-time \nannotations are supplied by the user [3]. Partial evaluation of imperative programs is difficult because \nof the lack of referential transparency. The program transformation phase must take into account the \nnotion of state and thus is more complicated than in a functional setting [54]. Unless imperative features \nare encapsulated in some language constructs and so side\u00ad effects are disciplined, it is difficult to \nreason about the flow of static data. Duplication of side-effects and aliasing are the main concerns \n[1]. 4 Termination Due to its basic strategy unfolding calls and specializing func\u00adtions, partial evaluation \ncan loop in two ways: either by unfold\u00ading infinitely many function calls or by creating infinitely many \nspecialized functions. Both problems can be avoided naively by limiting the number of unfolded calls \nand the number of special\u00adized functions, but often this strategy appears unsatisfactory. In practice, \nsome programs require much unfolding while they are traversing static data and some others require many \nresidualiza\u00adtions. In this section, we review the strategies used in Mix, Similix, Schism, and Fuse. \nIn the Mix system, the problem was first treated by inserting annotations by hand in the source program, \nto indicate which call should be unfolded and which should be residualized [67]. Later on, static analyses \nwere devised to annotate first-order programs automatically [100]. The use of binding-time analysis enables \none to insert more accurate annotations. The strategy adopted in Sirnilix, for exam\u00adple, is very simple \nand appears to be applicable in most situations in an automatic way: dynamic conditional-expressions \n( z .e., con\u00additional expressions whose test do not evaluate to a static value) are selected as specialization \npoints and all procedure calls are unfolded [16]. Schism offers a more flexible annotation strategy: \nfilters, that can be used both in an online or in an offline strategy [23, 24, 27]. The user can equip \neach function with a filter, specifying under which conditions a call to this function should be unfolded \nand, if the call needs to be residualized, which parameters should be used for the specialization of \nthis function. Writing one s own fil\u00adters provides the user with full control over specialization. Filters \ncan also be generated automatically, based on any strategy. For example, an analysis corresponding to \nthe strategy of Similix is available in Schism [27]. As an online partial-evaluator, Fuse keeps a dynamic \ncache of program points [107, Section 3]. Specialization naturally termi\u00adnates when all loops that are \nunrolled statically terminate or are broken by a cache hit. Otherwise, an arbitrary bound is needed. \nIn general, the treatment of function calls not only determines the termination of the specialization \nprocess, but also has an im\u00adpact on the size and efficiency of the residual program. 5 Data vs. Code \nUsing partial evaluation is based on a tradeoff taking more space for programs and data may produce faster \ncomputations while taking less space for programs and data may produce slower com\u00adput ations. 5.1 Xphoon \nThe manual pages documenting Poskanzer and Leres program xphoon illustrate program specialization at \nits best. Xphoon dis\u00adplays a picture of the moon and was created by compiling the pro\u00adgram loading a \nfull-screen bitmap together with a bitmap repre\u00adsenting the moon: experience shows that the specialized \nprogram is both faster than loading a full-screen bitmap and smaller than the bitmap file representing \nthe moon. So in this case, program specialization wins both spacewise and time wise. 5.2 Pitfalls The \ntwo basic strategies of partial evaluation unfolding and spe\u00adcialization can of course lead to unsatisfactory \nresults, even if specialization terminates. For example, a program may be overly specialized and contain \nmany instances of the same piece of code, just differing with a single constant. At the other end of \nthe spec\u00adtrum, if partial evaluation is too conservative, the residual pro\u00adgram may contain many occurrences \nwhere further specialization actually would pay OR Care must also be taken when unfolding function calls \nto avoid duplicating computations. This can lead to the specialization of a linear program into an exponential \none [100]. This problem is met in C with the following macro. #clef ine inlinedplus (x) = x + x; Applying \ninlinedplus, for example, to a function call can make a linear-time looking code run in exponential time. \n6 Some Concrete Applications This section illustrates how partial evaluation can be used to de\u00adrive non-t \nrivial programs. In particular, this method stresses the fact that non-trivial programs are often instances \nof simpler ones [97, Chapter 5]. 6.1 Pattern Matching Let us consider the following string matching problem: \ndoes a string occur within a text? A variety of solutions have been pro\u00adposed for example, by Knuth, \nMorris, &#38; Pratt [75] and by Boyer &#38; Moore [17] that essentially solve this problem in lin\u00adear \ntime with respect to the size of the string and of the text. It is now folklore in the partial-evaluation \ncommunity how to derive the Knuth, Morris &#38; Pratt method out of the naive, quadratic program: After \na (partial) match, we know that a piece of the dy\u00adnamic input string is identical to (a prefix of) the \nstatic pattern. This means that we can now match the pattern against a shifted copy of itself, rather \nthan the input string; and the outcome of such a match can be decided at specialization-time [28]. Rather \nthan rewriting the source program to make it keep a static track of dynamic values, one can also obtain \nthis residual program by generalizing the partial evaluator [46, 57, 102]. For example, let us specialise \nthe following function by letting its first parameter be 10: (lambda (S d) (case d [(1) (+ S d)] [(2) \n(-s d)] [else d])) A naive strategy would yield the following residual program. (lambda (d) (case d \n[(i) (+ 10 d)] [(2) (-10 d)] [else d]) ) However, a strategy that keeps a static track of dynamic values \nacross conditional expressions can do a better job and produce the following residual program. (lambda \n(d) (case d [(1) 111 [(2) 81 [else d]) ) This simple step is enough to produce residual programs that \ntraverse the dynamic data linearly [40, 69]. Regarding string matching, any traversal of the string and \nthe text leads to a spe\u00adcialized program that is linear over the dynamic string. Thus if the traversal \ngoes from left to right and the string is static, the residual program mimics the effect of the Knuth, \nMorris, &#38; Pratt string-matching algorithm. If the traversal goes from left to right and the text \nis static, the residual program is structured like a Weiner tree [106] (named a position tree by Aho, \nHopcroft, and Unman [2]). If the traversal goes from right to left and the string is static, the residual \nprogram mimics the effect of the Boyer &#38; Moore string-matching algorithm. In fact, we have observed \nthat any traversal of the static data leads to a linear residual programz (which is remarkable considering \nhow much work has been in\u00advested to prove the expected linearity of some string-matching algorithms). \nHansen has identified several variations around the Boyer &#38; Moore string matching algorithm [56]. \nMore recently, Queinnec and Geffroy have identified several other traversals of the static data corresponding \nto other well-known matching algo\u00adrithms [92]. But even though partial evaluation can be used to generate \nlinear-time programs, there is no guarantee about the size of these programs, nor about the time taken \nby the partial evaluator to produce them. In particular, it would be surprising that the par\u00adtial evaluator \ngenerates a program mimicking the effect of Knuth, Morris, &#38; Pratt in time linear to the static string \n whereas Knuth, Morris, &#38; Pratt s algorithm first constructs a failure ta\u00adble in time linear to the \nstring, and then traverses the text in linear time. So partial evaluation does offer a safe way to remove \nthe inter\u00adpretive overhead of pattern matching and to construct linear-time residual programs. However, \nother insights are necessary to pro\u00adduce small programs quickly. 6.2 Partial evaluation applied to operating \nsystems An important trend in operating system development is the re\u00adstructuring of the traditional monolithic \noperating system kernel into independent servers running on top of a minimal/micro ker\u00adnel [50]. This \napproach results in modular and flexible operating systems. Also operating systems can be written in \nhigh-level pro\u00adgramming languages like Scheme [64]. However, these advantages come at a price: microkernel-baeed \nmodular operating systems do not provide performance comparable to monolithic ones. With the Synthesis \nkernel, Pu and his group have shown that microkernel-based operating systems can be optimized by generat\u00ading \nspecirdized kernel routines [91]. Their work demonstrated that efficiency can be obtained without compromise \non modularity and flexibtity. Although the Synthesis kernel has been a breakthrough in microkernel-based \noperating systems, it is based on an ad-hoc spe\u00adcialization process and requires the code to be specialized \nto be manually annotated. Such a process is tedious and error-prone. Since microkernel-baaed operating \nsystems can now be written in high-level programming languages there is no reason why partial evaluation \ncannot be used to perform the kind of specializations performed in the Synthesis kernel. In [37], Consel, \nPu and Walpole describe a research project aimed at using partiaJ evaluation to derive automatically \nimple\u00admentations of operating system components from generic specifica\u00adtions. They outline the necessary \nextensions to partial evaluation required for this derivation. 2We even proved this property, based on \na partial evaluator hke Similix that is guaranteed not to duplicate residual expressions. Related Work \n All optimizing compilers include constant propagation and folding [1]. The need for optimizing compilers \nto be efficient has moti\u00advated Wegman and Zadeck to study this subject on its own [105]. For another \nexample, Deutsch s interactive program verifier, like many other theorem provers, includes a simplification \nphase per\u00adforming static reductions [41]. Mosses s compiler generator S1S in\u00adcludes a phase for compiler-generation \ntime reductions and allows for compile-time reductions [85]. Appel s technique of reopening closures \nexplicitly aims at specializing functions at compile-time [4]. In fact, Lombardi and Raphael s main tool \nin their pioneer work on incremental computation was partial evaluation [79]. The investigations above \nhave one point in common: they use program trrmsformation as a phase in a larger system. Therefore this \nphase needs to be efficient. Alternatively, transforming a pro\u00adgram can be the main goaJ of a system, \nand then the emphasis is put first on understanding what is going on as a preliminary step to making \nthe transformation efficient [19]. We also observe a new trend in using one of the main techniques of \npartial evaluation polyvariance in modern compilers [20, 38]. Acknowledgements We are grateful to Anindya \nBanerjee, Andrzej FiLinski, John Hatcliff, Jim Hook, Julia Lawall, Jiirgen Koslowski, Karoline Malmkj=r, \nTim Sheard, and Erik Ruf for commenting earlier ver\u00adsions of these notes on short notice. References \n[1]A. D. Aho, R. Sethi, and J. D. Unman. Compilers: Pr-inci\u00adples, Techniques and Tools. Addison-Wesley, \n1986. [2] A. V. Aho, J. E. Hopcroft, and J. D. Unman. The Design and Analysis of Computer Algorithms. \nAddison-Wesley, 1974. [3] L. O. Andersen. Self-applicable C program specialization. In Consel [26], pages \n54-61. [4] A. W. Appel. Compiling with Continuations. Cambridge University Press, 1992. [5] W. Au and \nD. Weise. Automatic generation of compiler sim\u00adulation through program specialization. In IEEE Conference \non Design Automation, pages 205 210, 1991. [6] Denis Bechet. Partial evaluation of interaction nets. \nIn WSA 92 [109], pages 331-338. [7] L. Beckman, A. Haraldsson, O. Oskarsson, and E. San\u00addewall. A partial \nevaluator, and its use as a programming tool. Artificial Intelligence, 7(4):319 357, 1976. [8] A. Berlin. \nPartial evaluation applied to numerical computa\u00adtion. In ACM Conference on Lisp and Functional Prograrn\u00adrning, \npages 139 150, 1990. [9] D. Bj@rner, A. P. Ershov, and N. D. Jones, editors. Partial Evaluation and Mixed \nComputation. North-Holland, 1988. [10] C. Biihm. Subduing self-application. In 16th International Colloquium \non Automata, Languages and Programming, vol\u00adume 372 of Lecture Notes in Computer Science. Springer-Verlag, \n1989. [11] A. Bondorf. Towards a self-applicable partial evaluator for term rewriting systems. In Bj@rner \net al. [9]. [12] A. Bondorf. Self-Applicable Partial Evaluation. PhD thesis, University of Copenhagen, \nDIKU, Copenhagen, Denmark, 1990. DIKU Report 90-17. [13] A. Bondorf. Automatic autoprojection of higher-order \nrecur\u00adsive equations. Science of Computer Programming, 17:3 34, 1991. [14] A. Bondorf. Sirnilix manual, \nsystem version 3.0. Technical Report 91/9, Computer Science Department, University of Copenhagen, 1991. \n[15] A. Bondorf. Improving binding times without explicit CPS\u00adconversion. In ACM Conference on Lisp and \nFunctional Pro\u00adgramming, pages 1-10, 1992. [16] A. Bondorf and O. Danvy. Automatic autoprojection of \nrecursive equations with global variables and abstract data types. Science of Computer Programming, 16:151 \n195, 1991. [17] R. S. Boyer and J. S. Moore. A fast string searching algo\u00adrithm. Communications of the \nACM, 20(10):62 72, 1976. [18] M. A. Bulyonkov. Polyvariant mixed computation for ana\u00adlyzer programs. \nActs Informatica, 21:473-484, 1984. [19] R. M. Burst all and J. Darlington. A transformational sys\u00adtem \nfor developing recursive programs. Journal of ACM, 24(1):44 67, 1977. [20] C. Chambers and D, Ungar. \nCustomization: Optimizing compiler technology for SELF, a dynamically-typed object\u00adoriented programming \nlanguage. In ACM SIGPLA N Con\u00adference on Programming Language Design and Implementa\u00adtion, SIGPLAN Notices, \nVol. 24, No 7, pages 146 160, 1989. [21] W. Clinger and J. Rees (editors). Revised4 report on the algorithmic \nlanguage Scheme. LISP Pointers, IV(3): 1-55, July-September 1991. [22] C. Colby and P. Lee, An implementation \nof parameterized partial evaluation. In WSA 91 [108], pages 82-89. [23] C. Consel. New insights into \npartial evaluation: the Schism experiment. In ESOP 88, 2nd European Symposium on Pro\u00ad gramming, volume \n300 of Lecture Notes in Computer Sci\u00adence, pages 236 246. Springer-Verlag, 1988. [24] C. Consel, Analyse \nde Programmed, Evaluation Partielle et G4niration de Cornpilateurs. PhD thesis, Universitr5 de Paris \nVI, Paris, France, June 1989. [25] C. Consel. Binding time analysis for higher order untyped functional \nlanguages. In ACM Conference on Lisp and Func\u00adtional Programming, pages 264 272, 1990. [26] C. Consel, \neditor. ACM Workshop on Partial Evaluation and Semantics-Based Program Manipulation. Research Re\u00adport \n909, Department of Computer Science, Yale University, 1992. [27] C. Consel. Report on Schism 92. Research \nreport, Pacific Software Research Center, Oregon Graduate Institute of Sci\u00adence and Technology, Beaverton, \nOregon, USA, 1992. [28] C. Consel and O. Danvy. Partial evaluation of pat\u00ad tern matching in strings. \nInformation Processing Letters, 30(2):79-86, 1989. [29] [30] [31] [32] [33] [34] [35] [36] [37] [38] \n[39] [40] [41] [42] [43] [44]  C. Consel and O. Danvy. From interpreting to compiling binding times. \nIn ESOP 9o, .9rd European Symposium on Programming, volume 432 of Lecture Notes in Computer Sci\u00adence, \npages 88-105. Springer-Verlag, 1990. C. Consel and 0. Danvy. For a better support of static data flow. \nIn Hughes [63], pages 496-519. C. Consel and O. Danvy. Static and dynamic semantics pro\u00adcessing. In ACM \nSymposium on Principles of Programming Languages, pages 14-23, 1991. C. Consel and S. C. Khoo. Parameterized \npartial evalua\u00adtion. Research Report 865, Yale University, New Haven, Connecticut, USA, 1991. To appear \nin Transactions on Pro\u00adgramming Languages and Systems. Extended version of [33]. C. Consel and S. C. \nKhoo. Parameterized partial evaluation. In ACM SIGPLAN Conference on Programming Language Design and \nImplementation, pages 92-106, 1991. C. Consel and S. C. Khoo. Semantics-directed generation of a Prolog \ncompiler. In fd International Symposium on Programming Language Implementation and Logic Program\u00ad ming, \nvolume 528 of Lecture Notes in Computer Science, pages 135 146. Springer-Verlag, 1991. C. Consel and \nS.C. Khoo. On-line &#38; off-line partial evalu\u00adation: Semantic specifications and correctness prootk. \nRe\u00adsearch Report 896, Yale University, New Haven, Connecti\u00adcut, USA, 1992. C. Consel and S. Psi. A programming \nenvironment for binding-time based partisJ evaluators. In Consel [26], pages 62-66. C. Consel, C. Pu, \nand J. WaJpole. Incremental specializa\u00adtion: The key to high performance, modularity and portabil\u00adityy \nin operating systems. Research report, Pacific Software Research Center, Oregon Graduate Institute of \nScience and Technology, Beaverton, Oregon, USA, 1992. K. D. Cooper, M. W. Hall, and K. Kennedy. Procedure \ncloning. In Fourth IEEE International Conference on Com\u00adputer Languages, pages 96-105, 1992. Pierre Cr6gut. \nMachines d environnement pour la r%duction symbolique et 1 ~valuation partielle. PhD thesis, Universit6 \nParis VII, 1991. 0. Danvy. Semantics-dhected compilation of non-linear pat\u00adterns. Information Processing \nLetters, 37:315-322, March 1991. L. P. Deutsch. An interactive program verifier. TechnicaJ Report CSL-73-1, \nXerox PARC, May 1973. A. P. Ershov, D. Bj@rner, Y. Futamura, K. Furukawa, A. Haraldsson, and W. L. Scherlis, \neditors. Selected Pa\u00adpers from the Workshop on Partial Evaluation and Mixed Computation, volume 6 (2,3) \nof New Generation Comput\u00ading. OH MSHA. LTD. and Springer-Verlag, 1988. D. P. Friedman, M. Wand, and C. \nT. Haynes. Essentials of Programming Languages. MIT Press and McGraw-Hill, 1991. D. A. Fuller and S. \nAbramsky. Mixed computation of Prolog. In Bj@rner et ~. [9]. [45] Y. Futamura. Partial evacuation of \ncomputation process \u00adan approach to a compiler-compiler. Systems, Computers, Controls 2, 5, pages 45-50, \n1971. [46] Y. Futamura and K. Nogi. Generalized partial computation. In Bj@rner et al. [9]. [47] J. Gallager \nand M. Codish. Specialisation of Prolog and FCP programs using abstract interpretation. In Bj@rner et \nal. [9]. [48] M. Gengler and B. Rytz. A polyvariant binding time analysis handling partially known vsJues. \nIn WSA 92 [109], pages 322-330. [49] R. Gliick. Towards multiple self-application. In Hudak and Jones \n[61], pages 309-320. [50] D. Golub, R. Dean, A. Forin, and R. Rashid. Unix as an application program. \nIn Proceedings of the USENIX Summer Conference, 1990. [51] C. K. Gomard. Higher order partial evaluation \n-HOPE for the lambda calculus. Master s thesis, DIKU, University of Copenhagen, Copenhagen, Denmark, \n1989. [52] C. K. Gomard. Partial type inference for untyped functional programs. In ACM Conference on \nLisp and Functional Pro\u00adgramming, 1990. [53] C. K. Gomard. A self-applicable partial evaJuator for the \nlambda-calculus: Correctness and pragmatic. ACM Trans\u00adactions on Programming Languages and Systems, 14(2) \n:147 172, 1992. [54] C. K. Gomard and N.D. Jones. Compiler generation by partial evaluation: a case study. \nStructured Programming, 12:123-144, 1991. [55] M. A. Guzowski. Toward developing a reflexive partial \neval\u00aduator for an interesting subset of Lisp. Master s thesis, De\u00adpartment of Computer Engineering and \nScience, Case West\u00adern Reserve University, Cleveland, Ohio, 1988. [56] T. A. Hansen. Transforming a naive \npattern matcher into efficient pattern matchers. Technical report, DAIMI, 1991. [57] A. Haraldsson. A \nProgram Manipulation System Based on Partiat Evaluation. PhD thesis, Linkoping University, Swe\u00adden, 1977. \nLinkiiping Studies in Science and Technology Dissertations N 14. [58] S. Harnett and M. Montenyohl. Towards \nefficient compila\u00adtion of a dynamic object-oriented language. In Consel [26], pages 82 89. [59] F. Henglein. \nEfficient type inference for higher-order binding-time analysis. In Hughes [63], pages 448-472. [60] \nC. K. Hoist. Language triplets: The AMIX approach. In Bj@rner et al. [9], pages 167-185. [61] P. Hudak \nand N. D. Jones, editors. Partial Evaluation and Semantics based Program Manipulation. Vol. 26, No 9. \nACM SIGPLAN Notices, 1991. [62] J. Hughes. Backward analysis of functional programs. In [42], pages 187-208, \n1988. [63] John Hughes, editor. FPCA 91, 5*h International Confer\u00adence on Functional Programming Languages \nand Computer Architecture, number 523 in Lecture Notes in Computer Sci\u00adence, 1991. [64] S. Jagannathan \nand J. Philbin. A foundation for an efficient multi-threaded Scheme system. In ACM Conference on Lisp \nand Functional Programming, pages 345 357, 1992. [65] N. D. Jones. Partial evaluation, self-application \nand types. In 17th Inter-national Colloquium on Automata, Languages and Programming, volume 443 of Lecture \nNotes in Computer Science, pages 639 659. Springer-Verlag, 1990. [66] N.D. Jones, C. K. Gomard, and P. \nSestoft. Partial EvaL uation and Automatac Program Generation. Prentice-Hall International, 1993. To \nappear. [67] N. D. Jones, P. Sestoft, and H. S@ndergaard. Anexperiment in partizd evahation: the generation \nof a compiler genera\u00adtor. In J.-P. Jouannaud, editor, Rewriting Techniques and Applications, Dijon, France, \nvolume 202 of Lecture Notes in Computer Science, pages 124-140. Springer-Verlag, 1985. [68] N. D. Jones, \nP. Sestoft, and H. S@ndergaard. Mix: a self\u00adapplicable partial evaluator for experiments in compiler \ngen\u00aderation. LISP and Symbolic Computation, 2(1):9 50, 1989. [69] J. J@rgensen. Generating a pattern \nmatching compiler by partial evaluation. In Simon L. Peyton Jones, Guy Hutton, and Carsten Kehler Hoist, \neditors, Functional Programming, Glasgow 1990, pages 177-195. Springer-Verlag, 1991. [70] J. Jorgensen. \nGenerating a compiler for a lazy language by partial evaluation. In ACM Symposium on Principles O} Programming \nLanguages, pages 258-268, 1992. [71] M. Katz and D. Weise. Towards a new perspective on partial evaluation. \nIn Consel [26], pages 29 37. [72] S. C. Khoo. Parametrized Partial Evaluation: Theory and Practice. PhD \nthesis, Yale University, 1992. Forthcoming. [73] S. C, Khoo and R. S. Sundaresh. Compiling inheritance \nusing partial evaluation. In Hudak and Jones [61], pages 211-222. [74] S. C. Kleene. Introduction to \nMetamathematics. Van Nos\u00adtrand, 1952. [75] D. E. Knuth, J. H. Morris, and V. R. Pratt. Fast pattern matching \nin strings. SIAM, 6(2):323 350, 1977. [76] H. J, Komorowski. Partial evacuation as a means for infer\u00adencing \ndata structures in an applicative language: A theory and implementation in the case of Prolog. In ACM \nSympo\u00adsium on Principles of Programming Languages, 1982. [77] A. Lakhotia and L. Sterling. ProMiX: A \nProlog partial eval\u00aduation system. In L. Sterling, editor, The Practice of Prolog, chapter 5, pages 137 \n179. MIT Press, 1991. [78] J. Launchbury. Projection Factorisation in Partial Evalua\u00adtion. PhD thesis, \nDepartment of Computing Science, Uni\u00adversity of Glasgow, Scotland, 1990. [79] L. A. Lombardi and B. Raphael. \nLisp as the language for an incremental computer. In E. C. Berkeley and D. G. Bobrow, editors, The Programming \nLanguage Lisp: Its Operation and Apgdications, pages 204-219. MIT Press, Cambridge, Mas\u00adsachusetts, 1964. \n[80] K. Malmkjrer. On static properties of specialized programs. In WSA 91 [108], pages 234-241. [81] \nK, Malmkjzer. Predicting properties of residual programs. In Consel [26], pages 8 13. [82] U. Meyer. \nTechniques for partisJ evaluation of imperative languages. In Hudak and Jones [61], pages 94-105. [83] \nT. Mogensen. The application of partial evaluation to ray\u00adtracing. Master s thesis, University of Copenhagen, \nDIKU, Copenhagen, Denmark, 1986. [84] T. Mogensen. Binding Time Aspects of Partial Evaluation. PhD thesis, \nUniversity of Copenhagen, DIKU, Copenhagen, Denmark, 1989. [85] P. Mosses. S1S -Semantics Implementation \nSystem, refer\u00adence manual and user guide. University of Aarhus, Aarhus, Denmark, 1979. Version 1.0. [86] \nC. Mossin. Similix binding time debugger manual. Technical report, University of Copenhagen, Copenhagen, \nDenmark, 1991. [87] F. Nielson and H. R. Nielson. Two-Level Functional Lan\u00adguages. Cambridge University \nPress, 1992. [88] H. R. Nielson and F. Nielson. Automatic binding time analy\u00adsis for a typed A-calculus. \nIn ACM Symposium on Principles of Programming Languages, pages 98 106, 1988. [89] V. Nirkhe and W. Pugh. \nPartiaJ evaluation of high-level imp\u00aderative program languages with applications in hard real\u00adtime systems. \nIn ACM Symposium on Principles of Pro\u00adgramming Languages, pages 269 280, 1992. [90] J. Palsberg and M. \nI. Schwartzbach. Binding time analy\u00adsis: Abstract interpretation versus type inference. Technical report, \nDAIMI, 1992. [91] C. Pu, H. Massalin, and J. Ioannidis. The Synthesis kernel. ACM Computing Systems, \n1(1):11-32, 1988. [92] C. Queinnec and J. M. Geffroy. Partial evaluation applied to pattern matching \nwith intelligent backtracking. In WSA 92 [109], pages 109-117. [93] E. Ruf. Topics in Online Partial \nEvaluation. PhD thesis, Department of Computer Science, Stanford University, 1993. (in preparation). \n[94] E. Ruf and D. Weise. Using types to avoid redundant spe\u00adcialization. In Hudak and Jones [61], pages \n321 333. [95] E. Ruf and D. Weise. Improving the accuracy of higher\u00adorder specialization using control \nflow analysis. In Consel [26], pages 67-74. [96] B. Rytz and M. Gengler. A polyvariant binding time anal\u00adysis. \nIn Consel [26], pages 21 28. [97] W. L. Scherlis. Expression Procedure. and Program Deriva\u00adtion. PhD \nthesis, Department of Computer Science, Stanford University, 1980. Report No. STAN-CS-80-818. [98] D.A. \nSchmidt. Static properties of partial evaluation. In Bj@rner et sJ. [9], pages 465-483. [99] R. Schooler. \nPartial evaluation as a means of language ex\u00adtensibility. Master s thesis, M.I.T. (LCS), Massachusetts, \nU.S.A, 1984. TR-324. [100] P. Sestoft, Automatic call unfolding in a partial evaluator. In Bj@rner et \nal. [9]. 500 [101] D. Sherman, R. Strandh, and I. Durand. Optimization of equational programs using \npartial evaluation. In Hudak and Jones [61], pages 72-82. [102] D. A. Smith. Partial evaluation of pattern \nmatching in CLP domains. In Hudak and Jones [61], pages 62-71. [103] K. L. Solberg, H. R. Nielson, and \nF. Nielson. Inference systems for binding time analysis. In W SA 92 [109], pages 247 254. [104] R. S. \nSundaresh and P. Hudak. Incremental computation via partial evaluation. In ACM Symposium on Principles \nof Programming Languages, pages 1-13, 1991. [105] M. N. Wegman and F. K. Zadeck. Constant propagation \nwith conditional branches. ACM Transactions on Program\u00adming Languages and Systems, 3(2):181-210, 1991. \n[106] P. Weiner. Linear pattern matching algorithms. In l~thAn\u00adnual Sgmposium on Switching and Automata \nTheory, pages 1-11, 1973. [107] D. Weise, R. Conybeare, E. Ruf, and S. Seligman. Auto\u00admatic online partial \nevaluation. In Hughes [63], pages 165\u00ad 191. [108] Workshop on Static Analysis, volume 74 of Bigr-e Journal. \nIRISA, Rennes, France, 1991. [109] Workshop on Static Analysis, volume 81-82 of Bigre Journal. lktlSA, \nRennes, France, 1992.  A Optimizing Partial Evaluation: Self-Application   A.1 Self-application: \nwhat Often, we need to specialize a program p with respect to many different values. Then we are in the \nsituation of running PE several times on p. But PE is just another program that we need to run several \ntimes when a part of its input (p) does not change. Partial evaluation tells us that a faster way to \ndo th~ is first to produce a version of PE specialized with p (removing the inter\u00adpretive overhead of \nPE) and then to run this version instead: run PE (PE, (p, .)) = PEP By definition of program specialization, \nrunning the residual pro\u00adgram on s should yield the same result as PE would yield: run PE (p, (s, .)) \n= run PEP (s, .) Because PEP is obtained by specializing the specialize, this boot\u00adstrapping process \nis called self-application .3 For example, we might need to specialize the program format of the introduction \nwith respect to many different control strings so, 31, ... Instead of running the general-purpose partial-evaluator \non the same program format, we can build a specialiser dedicated to format (i. e., a program that only \nknows how to specialize format) first, and then run it as many times as needed on the different values: \nrun PE (PE, (format, -)) = PEforBat run PEfornat (-, SO,.) = forrnat~o run PEf Omat (., S1, .) = f ormat.l \n3The term self-application is confusing because the partial evaluator simply processes a copy of itself, \njust ss there are C compilers written in C. So in particular there is no danger of paradox or untypa~ility \nas in the A-calculus [10, 65]. In practice, PEformat is quite small. Most of this program co\u00adincides \nwith what a programmer would write by hand. The rest includes more specific features of the specialize. \nPartial evaluation can improve performance even further. Sup\u00adpose that several programs need to be specialized \nrepeatedly with respect to many different static values. We already know how to optimize the specialization \nof one program p with many static val\u00adues. This optimization requires us to specialize PE several times \nwith respect to many different programs p. In other terms, we need to run a program (PE) several times \nwhen a part of its input (PE) does not change, which is clearly inefficient. The remedy to this inefficiency \nis of course to specialize PE with respect to PE first, and then to use the residual program PEPE to \ntransform a source program p into a specialize PEP dedicated to specializing p without interpretive \noverhead. Since these optimizations are independent of the actual pro\u00adgram p, they are applicable to \nany kind of source programs. For example, when the source program p is the interpreter for a pro\u00adgramming \nlanguage, these optimizations are known aa the Fu\u00adt amura projections [45]. In this case, PEP has the \nfunctionality of a compiler and PEPE of a compiler generator. Sundaresh and Hudak point out that when \np is an incremental interpreter, then PEP has the functionality of an incrementalizer [104]. Self-application \nis an elegant idea in principle, and was at the basic motivation for Jones s Mix project [68] and for \nmuch of the work carried out at DIKU [12, 51, 84]. For example, Malmkjzer analyzes dedicated specializes \nPEP to predict generic properties of specialized versions of p [80, 81]. A.2 Self-application: how In \na partial evaluator, the static part of the input is the source program and the dynamic part of the input \nis the input to the source program. To make the partial evaluator specialize bet\u00adter (i. e., to make \nit more suitable for specialization), one should ensure that the source program is processed aa independently \nas possible of its input. During their PhD studies, Consel, Ruf, and Gluck have structured a partial \nevaluator along these lines to improve self\u00adamlication [ 2s, 49, 93]. This work aims at obtaining dedicated \nspecializes that are both reasonably small and reasonably effec\u00adtive. In his Mix project [68], Jones \nproposed a Gordian solution and stages PE explicitly into a static phase PE and a dynamic phase U. This \nstaging leads one to reformulate partial evaluation as follows: run PE (p, (s, -)) = run~ ((run ~p), \n(s, -)) In particular, self-application is reformulated as follows: run PE (PE, (p, -)) = run m ((run \n~~fl), ((run PE p), .)) In practice, ~ takes a second parameter, usually the binding\u00adtime signature of \nthe first parameter. Jones s staging effectively eliminates the problem of self\u00adapplication by ensuring \nthat dedicated specializes are only built out of the run-time part of PE (since they are specialized \ninstances of @ and that source programs are processed statically (since they are passed to ~ beforehand). \n\t\t\t", "proc_id": "158511", "abstract": "<p>The last years have witnessed a flurry of new results in the area of partial evaluation. These tutorial notes survey the field and present a critical assessment of the state of the art.</p>", "authors": [{"name": "Charles Consel", "author_profile_id": "81100552270", "affiliation": "", "person_id": "PP39048247", "email_address": "", "orcid_id": ""}, {"name": "Olivier Danvy", "author_profile_id": "81100394275", "affiliation": "", "person_id": "PP15031217", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/158511.158707", "year": "1993", "article_id": "158707", "conference": "POPL", "title": "Tutorial notes on partial evaluation", "url": "http://dl.acm.org/citation.cfm?id=158707"}