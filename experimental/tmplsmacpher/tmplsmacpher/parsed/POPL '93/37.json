{"article_publication_date": "03-01-1993", "fulltext": "\n Smartest Recompilation Zhong Shao and Andrew W. Appel Department of Computer Science, Princeton University \nPrinceton, NJ 08544-2087 ZSMCS. pxinceton. edu appel@cs .princeton. edu Abstract To separately compile \na program module in traditional statically-typed languages, one has to manually write down an import \ninterface which explicitly specifies all the external symbols referenced in the module. Whenever the \ndefinitions of these external symbols are changed, the module haa to be recompiled. In this paper, we \npresent an algorithm which can automatically infer the minimum import interface for any module in languages \nbaaed on the Damaa-Milner type discipline (e.g., ML). By minimum , we mean that the in\u00adterface specifies \na set of assumptions (for external symbols) that are just enough to make the module type-check and compile. \nBy compiling each module using its minimum import interface, we get a separate compilation method that \ncan achieve the following optimal property: A compilation unit never needs to be recompiled unless its \nown implemen\u00ad tation changes. Introduction Most traditional separate compilation methods rely on man\u00adually \ncreated contexts (e.g., Modula-3 interfaces, include\u00adfiles in C, and Ada package specifications) to enforce \ntype correctness across module boundaries. Using the proper con\u00adtexts, the compiler can check that each \nmodule uses its im\u00adported interfaces properly, and implements its exported in\u00adterface as expected. The \ndisadvantage of using these man\u00adually created contexts is that to guarantee consistency, all modules \nusing a changed context must be recompiled, no matter how small the change is. The conventional recom\u00adpilation \nrule (as described in Tichy [31]) is stated aa fol\u00adlows: A compilation wait must be recompiled whenever \n(1) its own implementation changes, or (2) a context changes upon which the compilation unit depends. \n This is obviously not satisfactory because adding a comment or adding a new declaration to a pervasive \ncontext may cause the unnecessary recompilation of the entire system. Tichy [31] presents an effective \ntechnique called smart recompilation that elimi\u00ad nates most of the redundant recompilation triggered \nby (2). Permission to copv without fee all or part of this material ia granted provided that the copies \nare not made or distributed for direct commercial advantage, tha ACM copyright notice and the title of \nthe publication and its date appear, and notice is given that copying is by permission of the Association \nfor Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. \nACM-20th PoPL-1 /93-S. C., USA @1993 ACM 0.89791-561-5/93/0001 /0439... $ 1.50 In Tichy s scheme, a compilation \nunit ia recompiled only if its implementation changea, or if it references a symbol defined elsewhere \nwhose definition haa changed. Schwanke and Kaiser [29] define smarter recompilation which can eliminate \neven more (but not all) redundant recompilation caused by (2). So a natural question to ask is: Can we \nelim\u00adinate all redundant recompilation? that ia, can we achieve the following smartest recompilation \nrule: A compila\u00ad tion unit never needs to be recompiled unless its own imple\u00ad mentation (source code) \nchanges? Standard ML (SML) [20] has a rather elaborate module system, but SML compilers have not supported \nseparate compilation very well. The problem ia that in SML, modules such as structures and functora can \nliberally reference exter\u00adnally defined identifiers wit bout even mentioning what are their specification. \nFor example, by using qualified (dot\u00adted) identifiers, a structure FOO can use BAR. QUX. f to refer\u00adence \nthe function f defined in the substructure QUX of the structure BAR, without even knowing what the type \nof f ia. Because of the lack of explicit import interfaces, structures and functors with free variables \n(Iet a call them open-formed modules) are not considered aa separately compilable units. So how can we \nseparately compile open-formed modules in SML? This paper presents a new separate compilation method \nwhich actually answers both of the above two queationa. Surprisingly, not only can we separately compile \narbitrary structures and functors in SML, but we can also accomplish the smarteat recompilation rule. \nOur idea ia simple: in order to separately compile a module with references to ex\u00adternal identifiera, \nwe have to know the specifications (e.g., types) of these external identifiers; since they are not explic\u00aditly \nspecified, we infer them by looking at how these exter\u00adnal identifiers are used inside the module; then \nwe compile the module by using this inferred import interface as ita context; finally, when all the modules \nare linked together, cross-module type errors are reported by checking whether the surroundings match \n(or satiafy) the specification in each inferred import interface. The catch here is that in order to \nachieve the smartest recompilation rule , we have to in\u00adfer the minimum import interface. Informally \nspeaking, this minimum import interface specifies a set of assump\u00adtions (on those external identifiers) \nthat are just enough to make the module type-check and compile; at link time, if the module s aurroundinga \nsatisfy this aet of assumptions, the compiled code can be reused, otherwise there must be cross-module \ntype errora. The inference algorithm ia dis\u00adcussed in detail in section 2 and 3. Now let s see an example \nof how our method works. From the following SML structure declaration, structure FOO . struct Vd X = \nBAR.f val y . (BAR.g 4, BAR. g true) end we know that in order to compile FOO, the context should contain \na structure named BAR. Inside BAR, there should be at least two val declarations: one is f, which can \nhave any type, say a; the other is g, which should be a function that can be applied to both integers \nand booleans, that is, g should have a type more general than int -+ P and bool + -y. Here, a, /3 and \n~ are just type variables we used to denote unknown types. Compiling FOO in this inferred interface will \nresult in a structure with two components: the variable x has type a and the variable y has type /3 * \ny. Now suppose that the real structure BAR is defined as follows: structure BAR = struct val f = 3 val \nh=true valg=fnz=>z end then at link time, when the real BAR is matched against the import interface \nof FOO, we find that the type variables a and /3 should be d and y should be bool, thus FOO. x will have \ntype int and FOO. y will have type int * booi. This is exactly what we will get if we compile FOO in \nthe environment that would result from compiling BAR. To achieve the smartest recompilation rule , the \nback end of the compiler must use only the type information specified in the module s inferred import \ninterface. This limitation is not a big problem. The back end of the cur\u00adrent SML/NJ compiler [4] uses \nalmost no type information from the front end but it still produces quite efficient code. Many optimization \ntechniques that do use the type informa\u00adtion, such as Leroy s representation analysis [17], can still \nbe partially incorporated into our separate compilation system. The details will be described in section \n4 and 5 of this paper. Our separate compilation method immediately has the fol\u00adlowing advantages over \ntraditional methods: e Because of the smartest recompilation rule, each module never needs to be recompiled \nunless its own implemen\u00adtation changes; so maximum reusability is achieved. e Because all modules are \ncompiled independently of each other, they can be compiled in any order. This also means that programmers \nneed no longer maintain de\u00adpendency files (e.g., Makefi.le [9]). * Open-formed modules can also be separately \ncompiled. e Cross-module type errors are now symmetric. In tra\u00ad ditional methods, if module A references \nidentifiers de\u00adfined in module l?, type inconsistencies between mod\u00adule A and module B will show up when \nA is compiled. If the programmer fixes the error by editing B, both A and B must be recompiled. But in \nour method, because cross-module type errors are reported at link time, only B will be recompiled. * \nThe compiler based on our method will automatically be a standalone compiler. As far as we know, no one \nhas yet built a standalone compiler for the complete SML module system. 1.1 Closed vs. open-formed modules \nStandard ML SJ1OWSprogramming in open-formed modules. The essential difference between closed and open-formed \nmodules can be seen from rewriting the above open-formed structure FOO in closed form, the SML functor \nFOO . functor FOO (BAR : sig val f : int val g: la .-> >a end) = struct val x = BAR.f val y = (BAR. \ng 4, BAR. g true) end  This shows that programmers have to give assumptions about the types of BAR. \nf and BAR. g based on a pro forma implement ation of the structure declaration BAR. If later, BAR. g \nis changed to have a type scheme Va.a -+int, the above declaration will no longer be useful and will \nhave to be modified and recompiled. An open-formed module such as structure FOO does not have this problem \nbecause it does not require that the specifications of imported identifiers be explicitly given. People \nmay want to write FOO using its minimum import interface as its argument signature, but this minimum \nis not expressible in the SML type system. Moreover, inferring the minimum import interface cannot be \neasily done by hand. We do not advocate writing large programs all in open\u00adformed modules. SML strongly \nencourages that every struc\u00adture declaration should be written with a result signature constraint (as \nits export interface). Programmers can write their programs all in the form of closed functors such as \nFOO . On the other hand, in practice, we find it extremely convenient and flexible to write parts of \nour programs as open-formed modules. For languages based on the Damas-Milner type disci\u00adpline [7] such \nas SML and Haskell [11], there is another reason in favor of writing certain modules in opened forms. \nOne of the most important features of the Damae-Milner type discipline is that the most general type \nfor arbitrary expressions can be automatically inferred by compilers. It is, however, nontrivial to infer \nthe most general type simply by hand, especially with the presence of polymorphic ref\u00aderences in SML \nor type classes in Haskell. This makes it also nontrivial to write explicit import interfaces for many \nmodules. The SML commentary [19] also suggests that pro\u00adgrammers will probably need to write down many \nsharing equations if they want to close every module and that it will be too restrictive to write everything \nin closed functors. 2 Assumption Inference in Core ML ML has a sophisticated type inference system. Given \nan ML expression e = Ar..f(z + 1), even though the type of newly introduced variable x is not specified, \nwe can still find the most general type of e if we know the type of j and +. For example, if ~ has type \nVa. cY ~ a and + has type int * int + int, e will have type int + int. Milner s type inference (or type \nreconstruction) algorithm W (as in Tofte [32]) takes two arguments, a type environment TE and an ML expression \ne; all the free variables in e (such as j and +) must be specified with a type in TE, and W( TE, e) will \nreturn the most generaJ type for e. To support smartest recompilation, we face the chal\u00ad lenge of doing \ntype inference without even knowing the type Def W*(e) = case e of Def lfono~nify(~, TS) = x =$ let \nassume TS = {7{,...,7;} let a be a new type variable in unifg({(~,r;),(r,~;), .... (~, ~;)}) in (a,{x \n~ a}) Def PoiyUnify((TVo, 7., Ao), TS) = Jz.el + let A= O,P=O let a be a new type variable assume TVO \n= {ci,, ~,, ....~n} (r~, Al)= W*(el) for each T c 7 S S = MorIo Lkifg(cY, Al(z)) /%, F?z,.....% be new \ntype variables in (S(a ~ r:), S(A1 \\{z})) s={at14/3, fori=l,..., n} A= A uS(AO) elez * P= Pu{(s 7,,7)} \nIet (r~, Al) = W*(el) in ( Vraify(P), A) (r~, A2) = W*(ez) a be a new type variable s = Unify({(r;,T;+ \na)}) Def Match( TE, A) = in (S(a), S(AI U Az)) let P=O for each (z,~) E A letz=e~ine~+-VCYI, ....an.rl \n= TE(z) let (rf, AI) = W (el) ,&#38;, /32,....A benew type variables (r~,A2) = W*(e2) S={a, w/?~fori=l,..., \nn} TV = tyuars(r~) U tyvars(A1) P= PU {(S r~, r)} (S, A)= PolyUnify((TV, rf, AI), Az(x)) in Unify(P) \nin (S(t-J), AI uS (AU (Az\\{t}))) Figure 1: The Inference Algorithm W* and Match of external identifiers. \nFor example, can we find out the most general type of the above expression e if we do not know the type \nof f and +? This seems impossible. But in the case of separate compilation, we assume that the types \nof external identifiers will be known at link time. We can divide the type inference into two phases: \nfirst (at compile time), we infer a type r for e and a set of assumptions A for the free variables in \ne, which essentially means that e will have type r if the free variables in e satisfy the constraints \nin A; then (at link time) when the types of those free vari\u00adables (i. e., TE) are known, we match them \nagainst those in A and magically recover the most general type of e in TE. To distinguish it from usual \nML type inference, we call the inference done in the first phase assumption inference . In this section \nand section 3 we discuss the details of our assumption inference algorithm and show how the matching \ndone at link time can successfully recover the correct type for each expression. To simplify the presentations, \nwe divide our algorithms into two parts: this section for Core ML and the next section for the ML module \nlanguage. We only give the details of our algorithm for the mini-ML language Exp and the skeletal module \nlanguage ModL used by Tofte [32]. However it is easy to extend our algorithm to the rest of SML. The \nexpressions in the mini-ML language Exp are defined by the following grammar: e::= z[Az.el lelez/letz=elin \nez Here is a brief review of the notation. %nmose TvVar is an .. infinite set of type variables and \nTyCon is a set of nullary type constructors, the set of types, Type, ranged over by r and the set of \ntype schetnet+ TypeScheme, ranged over by a are defined by T ::= rrlal~l~r2anda::=r /VCV.Ul. A type environment \nis a finite map from program variables to type schemes. tyvars(r), tyvars(a) and tgvars( TE) are the \nset of type variables that occur jree in r, a and TE re\u00adspectively. A type r is a generic instance of \na type scheme a=val, ..., a~.r, written as ~ < a, if there exists a substi\u00ad tution S with its domain \nbeing a subset of {al, .... cm} and # = S(T). A type scheme al is more general than crz, denoted as aZ \n< al, if all generic instances of az are also generic in\u00ad stances of al. The generalization of a type \nr in a type envi\u00adronment TE is denoted by gen( TE, r), it is the type scheme Val, ....an.i-where {al, \n....CYn} = tyvars(r) \\ tyvars( TE). The core ML type system, in the form of type deduction rules as TE \n* e : r, is omitted here because it is the same as in Tofte [32]. 2.1 The assumption inference algorithm \nW* We define a type assumption to be a pair (z,~) where x is a program variable and r is a type. An assumption \nenvi\u00ad ronment, ranged over by A, is a set of type assumptions; it is usually represented by a finite \nmapping from program variables to lists of types. In the following, we use A\\ {x } to denote the set \nof type assumptions in A except those for variable z, and A(z) to denote the set of types associated \nwith variable z in A. We ah% use Unify to denote Robinson s original unification algorithm on classical \nterm algebras [26]. Unify takes a set of pairs of types and returns a substitution (the most general \nunifier). Figure 1 gives the assumption inference algorithm W* and the matching algorithm Match. W* takes \nan ML expression, and returns a type and an assumption environment. Match takesan ML type environment \nand an assumption environ\u00adment, and ret urns a substitution. The other two procedures in figure 1 are \nMono Unify and Poly Unify. Mono Unify takes a type and a set of types, and returns a substitution. Poly \nU\u00adn i$y takestwo arguments: a triple of a TyVar set and a type and an assumption environment, and a set \nof types; it re\u00adturns a substitution and an assumption environment. Given an ML expression e, W*(e) delays \nthe type-checking of all free variables in e by recording their monomorphic type instances in an assumption \nenvironment A. In the case of lambda abstraction Az. el, the type a of z is treated as monomorphic; the \nprocedure Mono Unify checks whether the set of assumptions collected for z from el satisfies this constraint. \nOne the other hand, in the let expression, the type of z is treated as polymorphic; for each use of m \nin ez, the type and the assumption environment from el is re\u00adnamed with new type variables; the procedure \nPoly Unifi then checks the typing of z in ez and merges the assump\u00adtion environments collected from el \nand ez. When the real type environment TE for the free variables is known (at link time), the matching \nalgorithm Match( TE, A) precisely re\u00adcovers everything, including the result type of elaborating e in \nTE. For example, given an expression e= let g= Az.fz in g g, the free variable of e is ~; W*(e) will \nreturn an assumption environment A = {f H (~6 + cr7) + as, f + (~6 + cr7), f * (al -+ cr3)} and a type \na~ for e. If the real type environment TE is {f H VCY.CXA a}, Match( TE, A) will result in the substitution \nS = {we w /?1, a7 H /?I, crI I+ PI, ~3 w /11, as w (pl + /31)}. Thus the expression e will have the type \nS* (CY8) = (~1 -+ ,61). This is exactly what we will get if we apply Tofte s algorithm W to TE and e. \nIn fact we can show that the algorithm W* is equivalent to Milner s W [32] in the following sense: Theorem \n2.1 Given a type environment TE and an ex\u00adpression e, then (S, r) = W(TE, e) succeeds if and only if \nboth (r , A) = W (e) and S* == Match( TE, A) succeed; Moreover, there exists two substitutions R1 and \nR2, such that the following are true: (1) RJ o R2 = R2 o R1 = ID; (~) R1(S(TE), ~) = (S*(TE), S r ); \n(3) (s(TE), ~) = R2(S*(TE), S*T*). Proof By structural induction on the expression e. For details, see \nthe technical report [30]. QED. Notice that theorem 2.1 is not trying to show the sound\u00ad ness and completeness \nof W* directly. It is just proving that the result of W* and Match is equivalent to the result of W. \nProving this kind of equivalence is relatively easier. From the soundness and completeness of the algorithm \nW(which is proved in Damas s Ph. D thesis [6]), and the above theo\u00adrem 2.1, we can easily get the following \nsoundness and com\u00adpleteness results for our algorithm W*. Corollary 2.2 (Soundness of W* ) Given a type \nenvi\u00adronment TE and an ML expression e, if both (r , A) = W (e) and S* = Match( TE, A) succeed, then \nS*( TE ) k e : S:7- . Corollary 2.3 (Completeness of W* ) Given a type en\u00advironment TE and an ML expression \ne, suppose that TE1 = S1 ( TE) and TEI k e : rl, then both (r , A) = W*(e) and S* = Match( TE, A) will \nsucceed; Moreover there ex\u00adists a substitution S such that TE1 = S (S ( TE)) and T1 + S (gen(S*(TE), \nS*r*)). The algorithm W* itself is interesting. Recursive calls to W* in the algorithm will not interfere \nwith each other so they can be called in any order. If concurrency is used, W* can be efficiently implement \ned. The caae for the 1 et x = el in ez expression implies that we can link two pieces of programs, i.e., \nel and ez, even though both of them cent ain free vari\u00adables; this is done by the algorithm Po2y Unify \nin figure 1. The assumption environment A returned from W* may be big. A possible optimization is to \ninsert a simplifying proce\u00addure at each recursive call to W* in the algorithm. This sim\u00adplifying procedure \nwill identify all isolated type assump\u00adtions in A. Given (r, A) = W*(e), we define an equivalence relation \nN on type variables: a -,8 if there exists a type # such that either r = ~ or (z, ~ ) E A for some x \nis true, and both a and /3 are type variables of T . Let TV be the transitive closure of tyvars(r) under \nN, then all pairs (z, t) in A where tyvars(t) n TV = 0 are denoted as isolated assumptions. All type \nvariables occurred in isolated as\u00adsumptions need not to be renamed in Poiy Unify and most redundant isolated \nassumptions can be eliminated.  2.2 The assumption inference algorithm D One disadvantage of W* is that \nits sequential implementa\u00adtion may be not very efficient in practice. In most compilers, there is a pervasive \nbasis (or initial library) which tends to be referenced very frequently by user programs, thus the result\u00ading \nassumption environment from W* may be quite big (even if it uses certain optimizations mentioned above). \nIt turns out that this problem can be elegantly solved by extending ML types with type predicates and \nassumptions. This ex\u00adtension, which is called constrained type in Kaes [13] and qualified type in Jones \n[12], is normally used to reason about the ML type system in the presence of overloading and subtyping. \nThe algorithm D, which is presented in the appendix at the end of this paper, is the type reconstruction \nalgorithm for Kaes s constrained type system. By using a special set of type predicates, the algorithm \nD can efficiently solve the assumption inference problem even when there is a pervasive basis. For more \ndetails, please see the appendix. 3 Assumption Inference in the SML Mod\u00ad ule Language In this section, \nwe present an assumption inference algo\u00adrithm for the SML module language. To simplify the pre\u00adsentation, \nwe only consider the skeletal language ModL (as in Tofte [32]) in figure 2. Notice that signature expressions \nand declarations are intentionally left out because their elabora\u00adtions can be delayed to link time, \nthus are irrelevant to our assumption inference. Functor declarations are not consid\u00adered in our language \neither because only their body, which is a structure expression, is elaborated at compile time. How\u00adever, \nfunctor applications are considered in our language be\u00adcause they are structure expressions which must \nbe elabo\u00adrated at compile time. dec ::= m G StrName I strdec N G NameSet = Fin(StrName) I strdec decl \nGE c SigEnv = SigId ~ Sig FE ~ FunEnv = FunId ~ FunSig strexp ::= strid SE c StrEnv = StrId 9 Str I struct \ndec end S or (m, E) ~ Str = StrName x Env .drexp. strid E c Env = StrEnv I jctid(strexp) X or (N)S ~ \nSig = NameSet x Str N(S, IV (S )) c FunSig = NameSet x (Str x Sig) strdec ::= structure strid = strexp \nB c Basis = Nameset x SigEnv x FunEnv x Env Figure 2: left: Grammar; right: Semantic objects The static \nsemantics of ModL is discussed in detail in the definition [20] and Tofte [32]. Its deduction rule is \nin the form of B 1-phrase + A meaning that phrase is elabo\u00adrated into a semantic object A in the basis \nB. The semantic objects are also defined in figure 2. Here we give a quick review on notations and concepts \nused in the static seman\u00adtics: A structure S is a pair (m, E), where m is the name of the structure and \nE is an environment, which gives the static information about the components of the structure. To make \nthe presentation clear, from now on, we shall use str(m, E) to denote a structure (m,12). A signature \nis an object of the form (N)S, where S is a structure and N is a finite set of names. A functor signature \nQ is an object of the form N(S, N (S )) where N(S) is the principal signature for the parameter signature \nexpression of the functor and S is the body structure of the functor, the names bound in S are the names \nin S which have to be generated afresh upon each functor application. A structure environment SE is a \nfinite map from structure identifiers to structures, sim\u00adilarly for signature environment GE and functor \nenviron\u00adment FE. StrName is an infinite set of names: names that are specified in a signature expression \nand are not shared wit h already declared structures are called flexible names, denoted as FlexStrName; \nnames of declared structures are called rigid names, denoted as ftigStrName. Definition 3.1 A realization \nis a finite mapping from the set FlexStrName to the set StrName; a renaming realization p={m, I+ m; where \ni = I , .... k} is a realization where m~s m-e distinct rigid names. Definition 3.2 A structure S1 = \nstr(ml, SEI ) enriches a structure S2 = str(m2, SE2) if ml = m2 and the struc\u00adture environment SE1 enriches \nthe SE2, A structure en\u00advironment SE] enriches a structure environment SE2 if Dom(SEz) ~ Dom(SEl) and \nfor each s c Dom(SEg), SE1 (s) enriches S,?32(s). Definition 3.3 A structure S matches a signature Z \n= N(S) ij there ezists a realization p such that S enriches yJ(s). Because the SML module language is \nexplicitly typed, the elaboration of a module expression simply involves type\u00adchecking. The static semantics \nin the Definition [20] can be viewed as a type checking algorithm. Given a structure expression with \nfree identifiers, we want to infer the min\u00adimum constraints on these free identifiers with which the \nexpression will just type-check. Again the minimum con\u00adstraints are not expressible if we only use semantic \nobjects in figure 2. We introduce a new kind of structure variable which is similar to row variables \nused in typing record calculi. Let StrVar be an infinite set of structure variables; struc\u00adtures and \nstructure environments are now extended as Str = (StrName x StrEnv ) U StrVar and StrEnv = StrId ~ Str \n. Each structure variable tmust have a kind. Kinds are defined as (StrName x KindEnv) where KindEnv is \njust a fi\u00adnite mapping StrId ~ (Str U StrVar). To distinguish it from structures, a kind (m, KE) is represented \nas STR(m, KE). A kind assignment is a finite mapping from structure variables to kinds. A structure S \n= str(m, SE) has the kind k un\u00adder the kind assignment K, written as A 1-S :: k, if it is derivable from \nthe following set of kinding rules: (1) K k t:: STR(m,KE), if K(t)= STR(m, KE) (2) Af k str(m, SE) :: \nSTR(m, KE) if both Dom(SE) ~ Dom(KE) and Vs c Dom(KE), SE(s) = KE(s).  A substitution now consists \nof two parts: one from StrVar to Str , another from FlexStrName to StrName (i.e., realiza\u00adtion). A kinded \nsubstitution is a pair consisting of a kind as\u00adsignment and a substitution. A kinded substitution (KI \n,lt) respects a kind assignment K2 if, for all t in dom(K2), KI E R(t) :: R(Kz (t)) is a derivable kinding. \nA kinded substitu\u00adtion (Kl ,Rl ) is more general than (K2,1?2) if R2 = Rs o RI for some R3 such that \n(K2,R3 ) respects IYl. A kinded sub\u00adstitution (KI ,R) is an unifier of a kinded set of equations (Kz,P) \nif it respects KZ and R(tl ) = R(tz) for all (t,, -tz) in P. Figure 3 gives our inference algorithms \nWstrew on struc\u00ad ture expressions, W&#38;c on declarations, wfc~zd on functor identifiers, W8~~&#38;c \non structure declarations and the match\u00ading algorithm Mod/Match. The argument V and Af records those \nalready-used structure variables and flexible names. All functor applications are done by the matching \nalgorithm at link time. The thinning effect in functor applications (on the argument signature) is achieved \nby the set of con\u00adstraints generated by the GenRec algorithm. The inferred assumption environment A automatically \nrecords the min\u00ad Def W,t~~~P(se, V, M) = case se of x+ let t$+V, m@M,  V =vu{t}, iw=fwu{m} in ({t:: \nSTR(m,O)}, t, {x w t}, v , M ) s.a let &#38;l,ul,Al, K, Ml) = Wstrezp(S, V M) tl, tz$!Vl, andml, mz~Ml \nK2 = ~IU {tl::STR(ml,{a w tz})} Ii s= A-zU {t,:: STR(mz,@)} V2 = V1 u{tl, t2}, M2 = MI U{ml, mz} (KA,R) \n= Ifinduniig(~s,{(~l,tl)}) in (K,, R(tz),R(AI), k, Jf2) f(s) * let (KI, uI, A, U,~I] = ~f.t!d(f~~ ~) \n(K2, t42, A2, u,Jf2) = W$trer,(%w, fw) t@ V2, m$Ik12 V3 = VZ U{t}, M3 = M2U{m} K3 = IifI U KzU {t:: STR(m,O)} \n(K4,R) = I{indUnifg(K3,{ (u~,u2 -t)}) in (K,, R(t),IZ(A1 uAz), IL, M3) struct d end * let (KI, Envl, \nAl, VI, Ml) = Wciec(d) m$Ml, Mz=MILJ {m} in (Kl, str(m,Envl), Al, u, Mz) Def wf,t,~(f, V, M) = case \nf of x* let tl, tz$V; ml, mz$M VI =Vu{tl, tz}; Ml =MU{ml, m2} K = {tl::STR(ml,O), t2::STR(m2,@)} in (K,tl+ \ntz,{z+ tl+ t2}, IL, Ml) Def W.trdec(Sd, V, M) = case sd of structure s = se ~ let (KI, UI, Al, V1, MI) \n= WstrexP(se, ~ M) in (K 1, {s * al}, Al, Vi, Ml) Figure 3: Assumption Def Wdec(d, V, M) = case d of \nsd * W$trdec(sd, v, M) sd d+\u00ad let (K1, Envl, Al, K, Ml ) = w~td~c(sd, ~ M) assume Envl = {s # u1} (K~, \nEnvz, Az,15, Mz)=W~~~(d, W, Ml) assume Az(s) = {tl,....t~} A3 = (A, U (Az\\{s})) P = {(u,, t,),..., (ul, \ntk)} (K3,R) = KindUnifg(K~ U K2, P) in (Ifs, R(Env~ + Env2), R(A3), %, M2) Def Genl?ec(a, str(m, Env), \nK, V) let KE =0 for each s E Dom(Env) ,B@Vand V= VLJ{~} (K, V)= GenRec(@, Env(s), K, V) KE=KE+{s+-+f3} \nK = I?(U {a :: STR(m, KE)} in (K, V)  Def ModlMatch(B,T) = let (N, GE, FE, SE) = B (K, u, A,~M)=Tand \nP=O for each (z, t.)E A and z G StrId P= u {(t., P SE(Z))} for each (z, t.)c A and z E FunId assume \nN1(S1, N~(S 1)) = FE(z) a@ Vand V= VU {a} assume {ml, .... mk) = NI uN; m; ~....m~ ~ M M = M U {m~,..., \nmj} p={n, w rnj where t = 1,..., k} P = Pu {(cl + p(s; ),tz)} (K, V)= GenRec(a, P(SI), K , V) (K , R)= \nKindUni.fg(K, P) in R(u) Inference in ModL 444 Def h indUnify(K, P) = case (K, P) of (K,, O) =) (K,, \nID) (K,, P, u {(t, t)})* A indfwfy(x,, P,) (K,,P, U {(t,+ tz,t;+ t;)})a Kind17taify(K,, P, U {(f,, t~), \n(t,, tj)}) (1<1 u {tl::STR(ml, Env])}, PI U {(tl, str(rnz, J%oz))}) + let check Dom(Envl ) G Dom(Env2), \notherwise fail (R~, m) = Name Unify(ml, mz) and E..; = Rm(Envl) and Envj = Rm(Env2) R = ({tl + str(m, \nEnvj)})(R~) and IC2 = R(K1) Pz = R(P1 ) U {(1.hv~(s), Envj(s)) ] V.s c Dom(Envl)} (Ks, R )= KindUnify(Kz, \nPz) in (Kz, R oR) (KI U {ii :: STR(ml, &#38;wl), t2 :: STR(mZ, Env2)}, Pl U{(tl, t2)}) + let (Rm, m) \n= Name Uni~y(ml, mz) and R = ({tl H tz})(lt~) Envj = R(Envl) and Envj = R(Env2) and Env = Env~ U (Env~ \n\\Dom(Envj)) KZ = R(K1) U {t2 :: STR(m, Env )} P2 = R(P1) U {(llnv~ (s), Envj(s)) I Vs G Dom(Env~) n Dom(Env~)} \n(K3, R )= li indUnify(K2, P2) in (Ks, R oR) (Kl, PI u {(str(ml, Envl), str(mz, Envz))}) +\u00ad let check \nDom(.Envl ) = Dom(Env2), otherwise fail (R~, m) = Name Unify(ml, m2) and Env~ = Rm(Envl) and Envj = Rm(Env2) \nK2 = Rm(K1) and P2 = Rm(P1) U {( Env~(s), Envj(s)) I Vs c Dom(Envj)} (Ks, R )= KindUnify(K2, P2) in (Ks, \nR o Rm) Def Nanae Unify(ml, m2) = if ml = m2 then (ID,ml) else if ml, mz G RigStrName then fad else if \nml E RigStrName then ({m2 * ml}, ml) else ({ml w mz},mz) Figure 4: Kinded unification algorithm i mum \nsharing constraints required to make the structure expression elaborate. Figure 4 gives the unification \nalgorithms Kind Unify and Name Unify. The kinded unification algorithm Kind Unify presented there extends \nthe one in Ohori [24] with consider\u00adations on ML structure names. The following theorem can be proved \nin the same way as Ohori [24]. Theorem 3.1 Giuen any kinded set of equations, the al\u00adgorithm I<ind Unify \ncomputes a most general unifier if one exists and reports failure otherwise. The following lemma shows \nhow the thinning effect is achieved in our algorithm. Lemma 3.2 Given a signature Z = N(S) and a structure \nS , suppose that S does not contain any flexible names; let a be a structure variable and VI be any set \nof struc\u00adture variables; suppose that (K, V) = GenRec(a, S, 0, VI), then KindUnify(K, {(a, S )}) succeeds \nif and only if the structure S matches the signature Z, Moreover, if R = KindUnify(A , {(a, S )}), then \nS enriches R(S). The following theorem can be proved by structural induc\u00adtion on structure expressions. \nTheorem 3.3 Given an ML basis B and a structure ex\u00adpression strexp, then B F strexp : S succeeds if and \nonly if both (K, u, A, V, M) = W.trezp(stresp, 0, RigStrName) and S = ModlMatch(B, (K, u, A, V, M)) succeed, \nMoreover, there ezists a renaming realization y such that S = yJ(S ). The algorithm W.t.ezP possesses \nmost properties that W* has. It can also be modified to take a basis B as its argument (just as algorithm \nD) so that it can work more efficiently when we compile a structure expression in the pervasive ba\u00adsis. \nCode Generation Issues A compiling process usually contains two parts: elaboration (i.e., type inference \nor type-checking) and code generation (also code optimization). The assumption inference algo\u00adrithms \npresented in the last two sections successfully solve the problem in the elaboration phase. In order \nto achieve the smartest recompilation rule , our compiler should generate code that will be reusable \nas long as the surroundings satisfy the minimum import interface (i.e., match the assumption environment \n). This requires that our code generator should use no more type information than is specified in the \nmin\u00adimum import interface. Fortunately there are very few dependencies between the static semantics and \nthe dynamic semantics in SML. More\u00adover, although Leroy s representation analysis [17] shows that the \ncompiler can benefit a lot by using type information in the front end, the SML/NJ compiler [4] uses almost \nno type information in its back end but it still produces quite efficient code. In SML/NJ, the only things \nthat the back end needs to know from the front end are the correspond\u00ading dynamic interface for each \nsignature and the identifier status for each identifier. By delaying these dependencies to be resolved \nat link time, a program can be translated into machine code even before it is elaborated. Because of \nspace limitations, we only informally discuss the solutions to these issues here. Punctor application \nIn SML, a functor F with argument signature SIG can be applied to any structure S that matches SIG. A \nstructure does not have to agree exactly with a signature in order for it to match the signature, instead \nit can contain more com\u00adponents than required. In such cases, signature matching will coerce the structure \nagainst the signature, producing a thinned structure that exactly agrees with the signature in terms \nof number of components and their types. Suppose the corresponding dynamic code for the functor F and \nthe structure S is fd and sd, the code generated for a function application F(S) will be fd( th(sd)) \nwhere th is a thinning function from S to SIG. In our separate compilation scheme, it is possible that \nwe still do not know the argument signa\u00adture of F or the exact specification of structure S when we have \nto generate code for the functor application F(S). This is simply solved by addhrg an abstraction on \nth, and the code becomes Ath .(... fd(th(sd) )... ). The correct thinning function is filled in at link \ntime when the real specifications of SIG and S are known. Pattern matching In the SML/NJ compiler, the \nrepresentation of a user defined datatype is determined by its definition. For example, structure A = \nstruct dat atype color = RED i GREER I BLUE I IIIIX of real * real * real end structure B = struct fun \nredp(A. RED) = 1.0 I redp(A.ltIX(x,-, -)) = x I redp -= 0.0 end the datatype color in A may be represented \nwith integer tags 0,1,2,3 for the data constructors BLUE, GREEN, MIX, and RED. However this imposes some \nproblems if we want to sep\u00adarately compile structure B. What representations are we go\u00ading to use for \nA. RED and A. MIX in the redp function? Again this is solved by making the representation of data construc\u00adtors \nabstract (as in Aitken and Reppy s recent work [2]). A constant data constructor (such ae A. RED) is \ncompiled as a variable. A value carrying constructor (such aa A. MIX) is compiled as a pair of injection \nand projection functions. These details are filled in at link time when the definition of the datatype \nis known. Polymorphic equfllty function Nothing needs to be done to support our separate compi\u00adlation \nscheme if the equality function is implemented as it currently is in the SML/NJ compiler. In SML/NJ (ss \nin all ML compilers to our knowledge), the polymorphic equality function is implemented ae a runtime \nequality interpreter which checks equality of two objects based on their rnn\u00adtime tags. Another way to \nimplement polymorphic equality, which is used in Haskell [11], is to paas an equality fnnction for each \nformaJ parameter that is a polymorphic equality type variable. The code produced by this scheme closely \ndepends on the derivation tree of the elaboration phase. In our separate compilation scheme, because \nthe types of some external identifiers are not known at compile time, the derivation tree we get at compile \ntime is not accurate. For example, fun f 1[ = S.g (3,x) from assumption infereuce, we know S. g s type \nmust be in the form of id*a -+$ and f s in the form of a e /3. Because S. g may want to test the equality \non its 2nd argu\u00adment, the function f here has to be implemented with an equality function for type cr \nas its extra argument. This will have some runtime overhead in the common case that S. g actually never \ndoes equality test on its 2nd argument. Representational analysis Leroy [17] presented a program transformation \nthat allows polymorphic languages to be implemented with unboxed, multi-word data representation. The \nmain idea is to in\u00adtroduce coercions between various representations based on the typing derivation tree. \nIn our separate compilation sys\u00adtem, accurate type information for external identifiers are not available \nat compile time, so the typing derivation tree is not very specific. However the representation analysis \ncan still be carried out since all type instances of external identifiers are recorded in the assumption \nenvironment. At link time, the matching algorithm Match will find out the accurate type information of \nall external identifiers and co\u00aderce them into different type instances in the assumption environment. \nFor example, the above function f will be im\u00adplemented as a polymorphic function a ~ /3. When we find \nthat S .g has type *id + int, S .g has to be coerced iratto type int *cr-,6and f has to be coerced from \nu ~ /3to id + int.The code produced in this way will be less effi\u00adcient, but it should be acceptable \nif in practice there are not too many external identifiers in a module (especially when we use algorithm \nD). Open declaration The open declaration in SML causes several nasty problems for our separate compilation \nscheme. We have solved these problems by delaying certain operations to link time [30]. Our solutions \nmay increase the complexity of linking but they do not incur any runtime overhead (however, they may \nstop some inline-expansion optimizations). Implementation We are currently prototyping a separate compilation \nsys\u00adtem based on our algorithms into the SML/NJ compiler. In our system, a large ML program is composed \nof a set of top-level structure declarations, signature declarations and functor declarations. No two \ntop-level structures (or signa\u00adtures, functors) can have the same identifier name so that we can uniquely \ndetermine which definition each external iden\u00adtifier refers to. Every top-level declaration is considered \nas a compilation unit. Because signatures are usually small and compiling siguature declarations does \nnot take much time, their elaborations are delayed to be done at link time. To compile a structure or \nfunctor declaration, we apply the as\u00adsumption inference algorithm to its body (which is always a structure \nexpression), generate the machine code for the body, and then write both the inferred interface (i.e., \nthe assumption environment) and the machine code into its bi\u00adnary file. The final linking phase is done \nin certain order according to the dependency relation among different mod\u00adules. This dependency relation \nhas been already recorded in the inferred interface in each binary file. For each mod\u00adule, the linker \nsimply reads the binary file, elaborates every signature expression, applies the matching algorithm (i.e., \nMatch and ModLkfatch) to recover the correct static envi\u00adronment and detect cross-module type errors \nif there are any, and then concatenates the machine code with correct thinning functions. 6 Related Work \nMost dynamically-typed languages such as Lisp also allow independent compilations and can achieve the \nsame kind of smartest recompilation in the sense that a module never needs to be recompiled unless its \nimplementation changes. However, this is based on a big sacrifice: cross-module type errors will be detected \nonly at runtime. Our method, how\u00adever, will detect all cross-module type errors at link time. Levy [18] \npresents a separate compilation method very similar to ours for PASCAL-like languages. Its compiler also \nautomatically infers the import interface for each com\u00adpilation unit. Cross-module type errors are reported \nat link time. However he does not mention whether he achieves the smartest recompilation rule, and the \ntype systems of PASCAL-like languages are much simpler than that of SML. Traditional separate compilation \nsystems adopted in most statically typed languages all use manually created interface files. Each compilation \nunit contains an implementation plus several interface files. It has to be fully closed up to the per\u00advasive \nbasis so that the specifications of all external symbols will be found at compile time. The make system \n[9] is the simplest one along this line. It will trigger recompilation if the interface file a module \ndepends on changes. Tichy [31] and Schwanke [29] eliminated most recompilation by exam\u00adining finer-levels \nof dependency relations between interfaces and implementations. In their methods, if the interface file \na module depends on changes, but the set of symbols the module imports does not change, then the module \ndoes not need to be recompiled. SRC Modula-3 [22, 14] implements exactly the same idea: a version stamp \nwhich encodes the specification of a symbol is produced for each exported sym\u00adbol in an interface; modules \nimport the version stamps of the symbols that they import; a module only needs to be recom\u00adpiled if any \nof its imported version stamps are no longer ex\u00adported. Languages with very powerful module systems such \nas Mesa [21], the System Modeller in Cedar [15], and FX\u00ad87 [8] also adopt similar separate compilation \nmethods which are only applicable to closed modules. The compiler for Russell [5] does partially support \nseparate compilations on open-formed expressions, however its module system is very restrictive and all \nmodules must be loaded and com\u00adpiled in an order determined by their dependencies. In sum\u00admary, these \nprevious methods cannot achieve the smartest recompilation rule, neither can they be applied to compile \nopen-formed modules in SML. In SML, two kinds of separate compilation methods have been proposed: Rothwell \nand Tofte s import scheme [28] and Rollins s SourceGroup scheme [27]; both methods ap\u00adply only to closed \nfunctors. Recently, Emden Gansner [10] is implementing a make-like separate compilation system for open-formed \nmodules in the interactive SML/NJ compiler. In his method, all modules are loaded and compiled in a top \nlevel environment in an order determined by their depen\u00addencies. Whenever a module is compiled, a new \ntime stamp is generated; both the binary and the time stamp are then written out to the binary file. \nA module has to be recom\u00adpiled whenever its source changes or any of its predecessors (in the dependency \ngraph) have been recompiled. Gansner is also planning to export the static semantics of each mod\u00adule \ninto the binary file so that redundant recompilation can be detected and avoided if the static semantics \nof a module has not been changed. Aditya and Nikhil [I] have been working on similar kinds of assumption \ninference algorithms for their incremental compiler for Id [23]. However as far as we know, their algo\u00adrithm \ndoes not infer the minimum constraints, thus fails to achieve our theorem 2.1. Because their system allows \nmutu\u00adally recursive top-level declarations, it cannot fully recover the correct type information by simply \nusing our assumption inference and matching algorithm. In the SML module lan\u00adguage, however, top-level \ndeclarations cannot be mutually recursive. Damas [6] gave an inference algorithm called T which is very \nsimilar to our W* in section 2. His type system per\u00admits that a variable can be bound to several distinct \ntypes in the type environment (just like our assumption environ\u00adment). However since he mainly used the \nsystem to han\u00addle overloading, he did not try to prove our theorem 2.1. The soundness and syntactic completeness \nresults he proved for T are only for his particular type system, not for the usual M L type system [32], \nso they are not in the same sense as our corollary 2.2 and 2.3. The algorithm V in Leivant [16] is just \nDamas s T restricted to the type sys\u00adtem without ML-polymorphism. Its extension V2 is for the polymorphic \ndiscipline of rank 2 and the relation between W and VZ is not clear. On the side of the SML module lan\u00adguage, \nAponte [3] presented a type checking algorithm for ModL based on Remy s approach to record typing [25]. \nHer approach k very elegant; however, in practice it is proba\u00ad bly very difficult to implement efficiently. \nIt is also not clear whether her algorithm can be modified to do our assumption inference. Concluding \nRemarks We have presented a separate compilation method that achieves the smartest recompilation rule \nfor open-formed modules in Standard ML. In our method, each module is compiled independently without \nknowing the specifications of its external identifiers; its import interface, instead, is inferred by \nlooking at how each external identifier is used inside the module. Cross-module type inconsistencies \nare detected at link time by simply matching the real specifi\u00adcations against the inferred import interface \n(this process should be very fast because it only involves an unification of a set of types). The independent \ncompilation of each module may disable some inter-module optimizations, but we believe that the code \ngenerated by our recompilation method will be comparable to the quite efficient code generated by the \ncurrent SM L/NJ compiler [4]. We plan to implement and measure our algorithm in SML/NJ in the future. \nThe smartest recompilation technique in this paper is pre\u00ad sented in the framework of SML; however, it \ncan be easily applied to other polymorphic languages based on the Damas-Milner type discipline. It should \nbe straightforward to ex\u00adtend the algorithm D in section 2.2 to work on the extension of ML type system \nwith parametric overloadings [13]. The assumption inference algorithms presented in section 2 and 3 can \nalso be used as a basis to build incremental compilers for similar languages. On the other hand, we still \ndo not know how to extend the algorithm for A40dL to work on the ex\u00adtension of ML module system with \nhigh-order functors [33]. The smartest recompilation technique should also be ap\u00adplicable to languages \nin the Algol family. The type system in those languages are much simpler than that in ML, so it is not \nhard to infer the minimum import interface for each module. However, the code produced by smartest recompila\u00adtion \nwill be less efficient because the code generators of these languages usually rely much more on the inter-procedure \ntype and data flow information than those of polymorphic languages. For applications where reusability \nand recon\u00adfiguration are more import ant than efficiency, the smartest recompilation property is still \nvery desirable. Acknowledgements We would like to thank William Aitken, Carl Gunter, and QingMing Ma \nfor many valuable comments on an early ver\u00adsion of this paper. We are also grateful to David MacQueen \nand Pierre Cregut for interesting discussions on related sub\u00adjects. This research is supported by the \nNational Science Foundation Grant CC1l-9002 786 and CCR-9200790, and by the first author s summer research \ninternship at AT&#38;T Bell Laboratories. References [1] Shail Adit ya and Rkhiyur S. Nikhil. Incremental \npolymor\u00adphism. In The Fifth International Conference on Functional Programming Languages and Compute? \nArchitecture, pages 378405, New York, August 1991. Springer-Verlag. [2] William E. Aitken and John H. \nReppy. Abstract value con\u00adstructors. In ACM SIGPLAN Workshop on ML and its Ap\u00adplications, June 1992. \n[3] Maria Virginia Aponte. Typage d un systeme de modules paramet?iques avec partage: une application \nde l unification clans /es theo~ies egtiaiionnel[es. PhD thesis, University de Paris, February 1992. \n[4] Andrew W. Appel and David B. MacQueen. Standard ML of New Jersey. In Martin Wirsing, editor, Third \nInt 1 Symp. on Prog. Lang. Implementation and Logic Programming, pages 1 13, New York, August 1991. Springer-Verlag. \n[5] Hans Boehm and Alan J. Demers. Implementing Russell. In Symposium on Compiler Const?wction, pages \n186 195. ACM Sigpkm, June 1986. [6] Luis Damas. Type Assignment in Programming Languages. PhD thesis, \nUniversity of Edinburgh, Department of Com\u00adputer Science, Edinburgh, UK, 1985. [7] Lnis Damas and Robin \nMilner. Principal type-schemes for functional programs. In Nvnth Annual A GM Symp, on Pr-in\u00adciples of \nP?og. Languages, New York, Jan 1982. ACM Press. [8] David K. Gitford et al. FX-87 reference manual. Technical \nReport MIT/LCS/TR-407, M.I.T. Laboratory for Computer Science, September 1987. [9] [10] [11] [12] [1:3] \n[14] [15] [16] [17] [18] [19] [20] [21] [ 2] [2:3] ~24] [ 5] [ 6] [27] [.28] [29] Stuart I. Feldman. \nMake -a program for maintaining computer programs. Software Practice and Experience, 9(4):255-265, Apirl \n1979. Emdeu R. Gansner. AT&#38;T Bell Labs, personal communica\u00adtion, 1992. Paul Hudak, Simon Peyton Jones, \nand Philip Wadler et al. Report on the programming language Haskell a non-strict, purely functional language \nversion 1.2. SIGPLAN Notices, 21(5), May 1992. Mark P. Jones. A theory of qualitied types. In The lth \nEu-Topean Symposusm on Programming, pages 287 306, Berlin, February 1992. Spinger-Verlag. Stefan Kaes. \nType inference in the presence of overloading, subt yping and recursive types. In 1992 A CM ConfeTen \nce on Lasp and Fucntional Programming, New York, June 1992. ACM Press. Bill Kalsow and Eric Muller. SRC \nModula-3 version 1.6 man\u00adual, February 1991. Butler W. Lampson and Eric S. Schmidt, Pratical use of a \npolymorphic applicative language. in Tenth Annual ACM Symp. on Principles of Prog. Languages, New York, \nJan 1983. ACM Press. Daniel Leivant. Polymorphic type inference. In Tenth An\u00adnual ACM Symp. on Principles \nof pTog. Languages, New York, Jan 1983. ACM Press. Xavier Leroy. Unboxed objects and polymorphic typing. \nIn Nineteenth Annual ACM Symp. on Principles of Ps-og. Lan\u00adguages, New York, Jan 1992. ACM Press. Michael \nR. Levy. Type checking, separate compilation and reusability. SIGPLAN Notices (Proc. Sigplan 84 Symp. \non Compiler Construction), 19(6):285 289, .lune 1984. Robin Milner and Mads Tofte. Commentary on StandaTd \nML. MIT Press, Cambridge, Massachusetts, 1991. Robin Milner, Mads Tofte, and Robert Harper. The Defini\u00ad \ntion of Standayd ML. MIT Press, Cambridge, Massachusetts, 1990. J. Mitchell, W. Maybury, and R. Sweet. \nMesa language man\u00adual. Technical Report CSL-79-3, Xerox Palo Alto Research Center, Palo Alto, CA, 1979. \nGreg Nelson, editor. Systems programming with Modula-3. Prentice Hall, Englewood Cliffs, NJ, 1991. Rkhiyur \nS. Nikhil. Id version 90.0 reference manual. Techni\u00adcal Report TR-CSG-Memo 284-1, MIT Laboratory for \nCom\u00adputer Science, 1990. Atsushi Ohori. A compilation method for ML-style polymor\u00adphic record calculi. \nIn Nineteenth Annual ACM Symp. on Principles of PTog. Languages, New York, Jan 1992. ACM Press. Didier \nRemy. Typechecking records and variants in a nat\u00adural extension of ML. In Sixteenth Annual ACM Symp. \non Principles of Prog. Languages, pages 77 87, New York, Jan 1989. ACM Press. J. Robinson. A machine-oriented \nlogic based on the resolu\u00adtion principle. Journal of the A CM, 12(1):23 41, 1965. Eugene J. Rollins. \nSource Group: A selective recompilation system for SML. In Thtrd Intevsational WoTkshop on Stan\u00ad daTd \nML, Pittsburgh, September 1991. Carnegie Mellon Uni\u00ad versity. Nick Rothwe13 and Mads Tofte. Import command \nsource code. with Standard ML of New Jersey releases 0.65. Robert W. Schwanke and Gail E. Kaiser. Smarter \nrecompi\u00adlation. ACM TTiZnSaCti0n8 on Programming Languages and Systems, 10(4):627-632, October 1988. \n [30] Zhong Shao and Andrew W. AppeL Smartest recompilation. Technical Report CS-TR-395-92, Princeton \nUniv. Dept. of Computer Science, Princeton, NJ, October 1992. [31] Walter Tichy. Smart recompilation. \nACM Transactions on Programming Langtiages and Systems, 8(3):273 291, July 1986. [32] Mads Tofte. Operational \nSemantics and Polymorphic Type Inference. PhD thesis, University of Edinburgh, Edinburgh, UK, November \n1987. [33] Mads Toft e. Principal signatures for high-order ML fnnctors. in Nineteenth Annual ACM Symp. \non Principles of Prog. Languages, New York, Jan 1992. ACM Press. 8 Appendix: the assumption inference \nal\u00adgorithm D 8.1 An extension of ML with constrained types The extension of ML type system with constrained \ntypes (denoted as ML+) is discussed in detail by Kaes [13] and Jones [12] to solve the type inference \nproblem in languages that support overloading and subt yping. It turns out that it can also be used to \nsolve our assumption inference prob\u00adlem. The language syntax they use is essentially same as the mini-ML \nlanguage Exp. In the following, we give a quick re\u00adview of Kaes s framework of extending ML with constrained \ntypes. To ease the notation, m is used to denote a sequence ~1, . . ..~n. Definition 8.1 Let P be a finite \nn-indexed family of pred\u00adicate symbols. The set of predicate constraints over Type is defined as {p(~) \n] p G Pfi, r, G Type where i = 1, . . ..n}. An interpretation of P is a family oj total computable functions \n(~)pe~, WCh that for P E P~, j : (Type)n ~ 2. Definition 8.2 A set C of constraints is satisfiable if \nthere ezists a substitution S such that ifp(~) 6 C then ~(S(R)) is true. Satisfiablity will be denoted \nas S ~ C. Definition 8.3 A substitution S is a solution of C , if S o S ~ C for all substitutions S . \nA solution S is called the most general solution of a constraint set C , if for any solution R of C, \nthere exists a substitution S , such that R exist. S o S. In most cases, the most general solution \ndoes not The entaihnerstrelation on constraint sets, written Cl t+ Cz, may be defined once a particular \npredicate system is given. In the following, we only consider those predicate systems which satisfy the \nfollowing properties: if Cl l+-CZ then VS : S+ CI*S+C2. Definition 8.4 A constrained type is a pair rlC, \nconsisting of a type r and a set of constraints C. A constrained type scheme is of the form WK.TIC. A \nconstrained type environ\u00adment is now just a finite map from variables to constrained type schemes. Definition \n8.5 A constrained type r I C is a generic in\u00adstance of a constrained type scheme u = m.rl C, written \nas r lC < u, if there exists a substitution S with domain be\u00ading a subset of ~ such that r = S(T) and \nC H-S(C). We also denote u < c if all generic instances of u are generic instances ofu; and u z u if \na < u and u+ u . Def D(TE, e) = case e of x ~ let Z E(Z) = V~.r\\C and ~ be new type variables and S ={a~ \n* ~, for i = 1, ....n } in (Ill , S(r)\\ S(C)) kz.el ~ let a be a new type variable and (S1, rllCI) = \nD(TE &#38; {z = cr[O}, el) in (S, , (S,(a)+ TI)[CI) elez +-let (S1, TIICI) = D(TE, el) and (SZ.,T21CZ)= \nD(Sl(TE), ez) a be a new type variable and S s = UnZfy(SZ71, rz ~ w) in (L $so SZ 0.$1, (SSO )I(S3(SZ(CI) \nU C2))) let x = eli.nez+ let (S1, T1[CI) = D(Z E, el) and (SZ,721CZ) = D(S1(TE) + {z * gen(Sl(TE), rllCl)}, \nez) in (S2 o S, , 7_zl(SZ(CI) U Cz)) Figure 5: The Type Inference Algorithm D Typability of an expression \ne in ML+ is expressed as a judge\u00adment C, TE t-e : r, which can be read as e has type r under (constrained) \ntype environment TE, provided C is satisfiable. The generalization of a constrained type r I C in the \ncontext of a type environment TE is de\u00adnoted by gen( TE, r I C), it is the constrained type scheme V~.rlC \nwhere {al, .... an} = tgoars(rlC) \\tyvars( TE) and C ={p(F) c C where tyvars(p(~)) n G # 0}. Definition \n8.6 A typing C, TE + e : r is more general than C , TE 1-e : i- , if there exists a substitution S, such \nthat (1) x c dom(TE) +-TE (x) < S(TE(Z)) (2) gen(TE , r 1 C ) + S(gen( TE, TIC)). Kaes [13] presented \nthe type deduction rules (also listed in the appendix at the end of this paper for reference) and the \ntype inference algorithm D (as in figure 5) for the above ex\u00adtension. It can be proved that his type \ninference algorithm D is sound and (syntactically) complete in the following sense: Theorem 8.1 Let an \ninstance of a constraint based infer\u00adence system be given, e be an expression, TE and TE be type environments. \nSuppose for certain substitution S1, for each x G dom(TE), TE (z) < S1(TE(Z)); and C , TE F e : / is \na valid typing, then (S, ~ I C) = D( TE, e) succeeds. More\u00adover, C, S(TE) 1-e : T is valid and more general \nthan C , TE 1-e : T . 8.2 Application to assumption inference We can use the above extension to solve \nour assumption inference problem. The set of predicates we use, denoted by Pm, is {p.(~) where x is any \nprogram variable}. The interpretation of pz is p-~(r) = true if and only if r < a, assuming that the \ntype of x is a closed ML type scheme u. The entailment relation on constraint sets is defined as: Cl \ntt-Cz if and only if Cl is satisfiable and VS : S ~ C;l ~ S # CZ. This relation is decidable for our \nparticular predicate system Pm because of the following lemma: Lemma 8.2 For any constraint set C formed \nin the predi\u00adcate system Pm, either there is no solution or there exists a most general solution S. 450 \nProof If we consider each pm (~) as an assumption (z, r), also assume that the type of z is known, the \nMatch algorithm in figure 1 can be used to find the most general solution of C. The lemma then follows \nfrom Robinson s unification theorem. QED. Given a closed ML type scheme a = V~.T, it can be writ ten \nas ML+ constrained type schemes UI = V7iZ. T 10 or U2 = Va. crl{p=(a)}, but obviously al < UZ in ML+. \n The great thing about the algorithm D is that when it is running, it does not need any knowledge about \nthe interpre\u00adtation of the predicate system. This leads to the following theorem: Theorem 8.3 Given a \nML type environment TE = TE, + TEz, where Dom( TEl)nDom( TE2) = 0 and tyvars( TE2) = 0, we construct \na ML+ constrained type environment TE = TE~ + TEj where 7 Ej= {z * V~.(T I 0) where z E Dom(TEl) and \nTE1(z) = V~.r} and TEL= {z H Va.(rYl {pm(w)}) where z E Dom( TEz)} and the interpretation ofpm is fire(r) \n= true if and only if T < TEz(x) . Then (S, r) = TV( TE, e) succeeds it and onzy if (S , # I C ) = D( \nTE , e) succeeds and there exists a most general solution S* for Ct. Moreover, there exists two substitutions \nRI and R2, such that the following are true: (1) R1 o Rz = Rz o R1 = ID; (.2) R,(S(TE), r) = (S*(S (TE)), \nS*#); (3) (S(TE), ~) = R2(S*(S ( T.E)), S*#). Proof Follows from lemma 8.2 and theorem 8.1. For details, \nsee the technical report [30]. QED. In practice algorithm D will be more useful than W* be\u00adcause it resembles \nthe algorithm W and it also works more efficiently when the types of some free variables are known. Moreover, \nD can be easily extended to work on various ex\u00adtensions of the ML type system with overloading and sub\u00adtyping \nsuch as those in Kaes [13].   \n\t\t\t", "proc_id": "158511", "abstract": "<p>To separately compile a program module in traditional statically-typed languages, one has to manually write down an import interface which explicitly specifies all the external symbols referenced in the module. Whenever the definitions of these external symbols are changed, the module has to be recompiled. In this paper, we present an algorithm which can automatically infer the &#8220;minimum&#8221; import interface for any module in languages based on the Damas-Milner type discipline (e.g., ML). By &#8220;minimum&#8221;, we mean that the interface specifies a set of assumptions (for external symbols) that are just enough to make the module type-check and compile. By compiling each module using its &#8220;minimum&#8221; import interface, we get a separate compilation method that can  achieve the following optimal property: <italic>A compilation unit never needs to be recompiled unless its own implementation changes</italic>.</p>", "authors": [{"name": "Zhong Shao", "author_profile_id": "81351597965", "affiliation": "", "person_id": "PP94029260", "email_address": "", "orcid_id": ""}, {"name": "Andrew W. Appel", "author_profile_id": "81100498630", "affiliation": "", "person_id": "PP14174176", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/158511.158702", "year": "1993", "article_id": "158702", "conference": "POPL", "title": "Smartest recompilation", "url": "http://dl.acm.org/citation.cfm?id=158702"}