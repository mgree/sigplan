{"article_publication_date": "03-01-1993", "fulltext": "\n Array Data-Flow Analysis and its Use in Array Privatization Dror E. Maydan, Saman P. Amarasinghe and \nMonica S. Lam Computer Systems Laboratory Stanford University, CA 94305 Abstract Data-flow analysis \nof scalar variables and data depen\u00addence analysis on array elements are two important pro\u00adgram analyses \nused in optimizing and parallelizing com\u00adpilers. Traditional data-flow analysis models accesses of array \nelements simply as accesses to the entire array, and is inadequate for parallelizing loops in array-based \nprograms. On the other hand, data dependence analy\u00adsis differentiates between different array elements \nbut is flow-insensitive. This paper studies the combination of these two analyses-data-flow analysis \nof accesses to individual ar\u00adray elements. The problem of finding precise array data\u00adflow information \nin the domain of loop nests where the loop bounds and array indices are affine functions of loop indices \nwas first formulated by Feautrier. Feautrier s al\u00adgorithm, based on parametric integer programming tech\u00adniques, \nis general but inefficient. This paper presents an efficient algorithm that can find the same precise \ninfor\u00admation for many of the programs found in practice. In this paper, we argue that data-flow analysis \nof individual array elements is necessary for effative automatic parat\u00adlelization. In particular, we \ndemonstrate the use of array data-flow analysis -in an important optimization known as array privatization. \nBy demonstrating that array data-flow analysis can ~ computed efficiently and by showing the importance \nof M. research was supported in partby DARPAccmtre.t.s NOO039-91-C-013S and DABT63-91-K-OO03, and by \nfellowships frcm AT&#38;T BeU Laboratories and Intel Corporation. Permission to copy without fee all \nor part of this material is granted provided that the copies are not made or distributed for direct commercial \nadvantage, the ACM copyright notice and the title of the publication and its date appear, and notice \nis given that copying is by permission of the Association for Computing Machinery. To copy otherwise, \nor to republish, requires a fee andlor specific permission. ACM-20th PoPL-I /93-S. C., USA 01993 ACM \n0-89791 -561 -5/9310001 /0002 . ..+1.50 the optimization enabled by the analysis, this paper sug\u00adgests \nthat array data-flow analysis may become just as important in future optimizing and parallelizing compil\u00aders \nas data-flow and data dependence analysis are in to\u00adday s compilers. 1 Introduction Program analysis \nplays a crucial role in a compiler as it defines the kind of information that is made avail\u00adable to the \noptimizers. For a program analysis to be effwtive, it must capture sufficient information to enable sophisticated \noptitnizations and be amenable to efficient implementation. Previous research on scalar compiler optimization \nare based primarily on data-flow analysis while reseamh on parallelizing compilers are based pri\u00admarily \non data dependence analysis. This paper explores a less understood domain of program analysis, the data\u00adflow \nanalysis of individual array elements. We show that such analysis is viable and useful. In traditional \ndata-flow analysis, accesses of array ele\u00adments are simply modeled as accesses to the entire array. Such \nanalysis is inadequate, for example, to detect paral\u00adlelism within loops that access arrays. Data dependence \nis a powerful analysis whose results enable paralteliza\u00adtion of loops as well as various forms of loop \ntrans\u00adformations. The domain of data dependence analysis is that the array index functions and the loop \nbounds must be integer linear functions of loop indices and possibly other symbolic variables. The traditional \nformulation of the data dependence problem can best be described as memory disambiguation. Specifically, \ndata dependence asks if any of the dynamic instances of two array ac\u00adcesses in a loop nest may refer \nto the same location. This problem has been shown to be equivalent to integer programming. Fortunately, \nrecent results show that the data dependence problems that exist in most programs can be solved exactly \nand efficiently[ldl. Memory disambiguation does not capture some im\u00adportant information in programs. \nConsider the example below fori=lto 10do afi] = O; (s1) afi] = 1; (s2) . . . = a[lO]; (s3)  a data dependence \nanalyzer will say that there is a depen\u00addence from statement S1 to S2j a dependence from S1 to S3, and \na dependence from S2 to S3. This information is sufficient to determine that it is not legal to execute \nall the iterations of the loop in parallel. However, it is imprecise in two way~ Coverage. A data dependence \nanalyzer does not know if a write operation covers, or kills, an earlier write operation. In this example, \nprecise data-flow analy\u00adsis of the individual array eIements wilI find that the # definition in statement \nS2 kills the i* definition in statement S1. This information, for example, can be used to eliminate statement \nS1. Identify of &#38;pendence. The data dependence analyzer only knows that a dependence exists; it does \nnot know which pairs of instances are dependent. Pre\u00adcise data-flow analysis can determine that only \nthe tenth instance of statement S3 depends on the tenth instance of S2 This information can be used to \nparallelize the first nine iterations of the loop. Recent research in parallelization has uncovered the \nneed for new optimization that require program infor\u00admation beyond that provided by either scatar data-flow \nor data dependence analysis. For example, it is well known that array privatizatwn is crucial for automatic \nparal\u00adlelization to succeed[7, 17, 22]. A common program\u00adming practice is to use the same array to store \nworking data across different iterations of a loop. Such reuse renders the loop unparallelizable. Evaluation \nof today s compiler technology indicates that many more loops can be parallelized by privatizing these \nwork arrays, that is, assigning a separate copy of the array to each processor. To determine if each \niteration can have its own copy of the array, we need data-jiow analysis on individual ar\u00adray elements. \nWe need to know if any of the iterations requires the definitions in the preceding iterations of the \nsame 100W we also need to know how to initialize the private copy of the array and how to determine the \nfinal values of the original array. The need for data-flow analysis techniques is even more obvious in \nthe compilation for message-passing machines. The high communication cost on these ma\u00adchines makes it \nnecessary to minimize communication. Critical optimization inchule reducing total communi\u00adcation volume \nby eliminating redundant data movements and reducing start-up overhead per message by block\u00ading the communication. \nSuch optimization again need data-flow information for individual array elements. There have been hvo \nmajor approaches to finding data\u00adflow information for array elements. The first builds on scalar data-flow \nanalysis, and the second on data depen\u00addence. The first approach is based on the scalar data-flow framework[12, \n13, 14, 21]. Instead of representing an array with a single bit, the set of data touched within a regionhnterval \nin the flow graph is approximated by an area descriptor. These approaches handle arbitrary control flow, \nbut their accuracy is defined by the precision of the summary information. The operators in the data-flow \nequations are set oper\u00adations (e.g. intersection, union and difference) on area descriptors. There are \ntwo difficulties with this approach. FirsL these descriptors may not be closed under these operations; \nthe results must therefore be (conservative) approximates of the exact answer. Second, these area de\u00adscriptors \nare typically expressions of symbolic variables. Solving this problem exactly requires the same paramet\u00adric \ninteger programming techniques used in the second approach. In factj solving these equations in a bottom-up \nmanner, as in interval analysis, would require the atgo\u00adrithm to handle outer loop indices initially \nas symbolic variables. This would unnecessarily make the algorithm more expensive. The scalar data-flow \nfkarnework generalty associates the same data-flow information with all instances of an access in the \nprogram. TO capture the identity of de\u00adpendence requires distinguishing between different in\u00adstances; \nsuch distinction is not within the vocabulary of the static data-flow framework. For example, this ap\u00adproach \nwill summarize the data-flow information for the above code as writing Af7:10] and reading AIIOJ itwill \nnot detect that the tirst nine iterations of the loop do not read any of the values written in the loop. \nThe second approach, pioneered by Feautrier, uses the same framework as &#38;ta dependence analysis. \nFeautrier developed an algorithm that finds perfect data-flow in\u00ad formation for amays in the domain of \nloop nests where the loop bounds and array indices are affine functions of the loop indices[9, 10, 11]. \nWhile Feautrier s solu\u00ad tion does not handle general control flow directly, his techniques can be used \nto solve the fundamental prob\u00ad lems that must be addressed by any exact array &#38;ta-flow analysis. \nOther works tend m be more efficient but they are all either less accurate or only applicable in mo~ \nlimited domains. Brandes approach does not apply if dependence distances are coupled or if the loop is \nnon\u00ad rectangula.r[51. Ribas only discusses constant-distance dependence in perfectly nested 100PS[2O]. \nPugh and Wonmcott s algorithm is inaccurate when their Omega test generates disjmctions of constraints[19]. \nIt cannot handle cases when a read statement is covered by write insmnces from different loop nesting \nlevels. In addition, it may have difficulties when the data touched by an ac\u00adcess are not contiguous \n(i.e. a reference such as X2ij. Array privatization and optimization for message passing machines require \nthe precision of Feautrier s al\u00adgoritim. Unfortunately, our experience with our imple\u00admentation of his \nalgotithm suggests that it is too inef\u00adficient to use in practice. By exploiting the simplicity in real \ncode, we have developed an efficient algorithm that gives the same precise information as Feautrier s \nin most of the cases found in practice. We believe that the efficiency of this algorithm is important \nin making array data-flow analysis feasible to use in progmm optimiza\u00adtion. This paper also develops \nan array privatization algo\u00ad rithm that uses the data-flow information. Using the coverage information \nprovided by data-flow analysis, we have developed the concept of data-flow dependence vec\u00ad tors, the \ndata-flow analog of dependence vectors in the parallelization literature. We show how we use these vectors \nto determine when it is necessary and useful to privatize an array. Using the identity of the dependence, \nwe can derive the upwards-exposed uses and downwards\u00ad exposed definitions information. This information \nis nec\u00ad essary in array privatization to initialize the private array and to finalize the original army. \n 2 Last Write Trees The precise data-flow dependence relationships are cap turedin a representation \nwe call a Last Write Tree (LWT). We tirst motivate this representation with a simple exam\u00adple: for i=11 \nto20do forj =11 to20do a~]=... ... = a~-1] It is easy to see that the data read in the first iteration \nof the innermost loops ~=11) are not defined within the loop nest. For all other iterations, the data \nread are writ\u00adten in the very preceding iteration. This information can be organized in a binary tree \nas shown in Figure 1. We call thts a Last Write Tree (LWT); the tree represents a function that maps \nan instance of the read operation to the last instance of a write to the same location. The read and \nwrite instances are denoted by the values of their loop indices, ~r and ~W,respectively. The domain of \nthe func\u00adtion is the set of ;. that satisfy the constraints imposed by the loop bounds. Each internal \nnode in the tree con\u00adtains a further constraint on the value of the read instance ?.. It partitions the \ndomain of read instances into those satisfying the constraint, represented by its right descen\u00addants, \nand those not satisfying the constraint, represented by its left descendants. In other words, the constraints \nof a node s ancestors make up the node s context. The iter\u00adation set of a node is defined as the set \nof ~, that satisfy the node s context. Each leaf node has a solution which expresses the last write instance \nlW in terms of ;r, for all those ~ in its iteration set. If there is no preceding dependent write for \na particular iteration seL the solution of the corresponding leaf is denoted by -L. jr> 11 FT 1-iryjr \n-1 C/33 Figure 1: Example of an LWT The LWTS provide information on both coverage and identities of \ndependence, the two deficiencies in stan\u00addard data dependence analysis. First the LWT is a func\u00adtion \nthat specifies only the last write instance for a par\u00adticular read instance, instead of specifying all \nprevious writes to the same location. Second, the context of the LWT defines the identities of the dependent \npairs. The LWT function is defined formally below. Definition 2.1 Let;and ? be iterations of two state\u00ad \nments that are nested in n common loops and an arbi\u00ad trary number of non-common loops. Vector Z is iexico\u00ad \ngraplkically less than vector ?, written 14 ?, iff lk<n, il =i;,..,,ik_~=i k_~,ik <i~ Similarly, Z3 3 \nifl+ Z or; = ~,...,;n = ?n. Definition 2.2 Let;W,?W be iterations of a write state\u00ad ment in an m-deep \nloop nest, ~r bean iteratwn of a read statement in an n-&#38;ep loop nest, and $ be a p-element vector \nof symbolic constants used in the loop. Let iiw = iWl, ..%wm, sl. ..sp , )~= i Wl. .. Wm,slssp. .sp , \niip = ivl. ..arn, ssp. .sp . w) [) Let Pw, F, be aflne read and write access functions in theloop, \nand Bw, Br be aftine read and write loop bound constraints. If FW appears after F. lexi cally in the \nprogram, then Vi?, such that Br?l, >6, L WTFW,FT,BW,B7(G.) = ~W if and only ~ Bwiiw >6, FwiiW = Frii,, \n?W + ~, , and ~?W(# ~w) such that Bwil~ >6, Fwii~ = F, Z,, ZW+~W +:r If instead, the write statement \nappears lexicalll above the read, we must replace the condition ?W+ i, in the definition with the condition \n?W~ ~r. 3 Calculating LWTS Feautrier has developed a set of algorithms, based on parametric integer \nprogramming, that can be used to calculate LWTS in the domain of loop nests that con\u00adtain no IF statements \nand no procedure calls[9, 10, 11]. Feau@ier s algorithms require solving an exponential number of memory \ndisambiguadon problems. Feautrier found that his algorithm takes 1.7 seconds to solve a sim\u00adple input \nwith one write statement and four read state\u00adments on a low end SPARC[ll]. We have also imple\u00admented \nFeantrier s algorithms; we found them complex and obtained similar timing figures. We believe that the \nalgorithm is much too inefficient to be used in practice. The algorithm described below is designed to \nexploit the simplicity in real code. It can find exact data-flow information for the vast majority of \nprograms very effi\u00adciently. On Feautrier s example that took 1.7 seconds, our algorithm takes about 0.10 \nseconds on a similar ma\u00adchine, aDECstation3100, to both calculate the LWT and to compute the data-flow \ndependence vectors described in Section 4. The efficiency of our algorithm makes it feasible to use array \ndata-flow in program optimizwions. In the following, we first concentrate on finding the ar\u00adray data-flow \ninformation between one pair of read and write operations in a hop nest with affine loop bounds. To explain \nthe algorithm, we describe our observations on two degenerate casetc when a write access touches the \nsame locations in all instances and when a write ac\u00adcess touches all different locations in different \ninstances. We then show how we can use these ideas as building blocks to develop an algorithm applicable \nto many of the cases found in practice. We will back up the claim of its generality by some empirical \nevidence. Finally, we descni how to handIe multiple write operations. 3.1 Loop Independent References \nWe say that a Rference is loop-in&#38;pendent if it refers to the same location in every iteration. In \nthe following example doii =11 to 15 do iz =lltols . ..==a a =.. . end do end do  every read and write \niteration refers to the same location. The last dependent write before a read instance is simply the \nlastwritebefore the read instance. Except for the first iteration which reads an uninitialized value, \nevery itera\u00adtion mds the data written in the immediately prexxxling iteration. Figure 2(a) illustrates \nthe iteration space order\u00ading for the example. From the figure, we observe that if the second loop index \nof the read iteration, i,2, is not at its minimum value, 11, then one iteration before the read is (irl, \ni,2 -1). Otherwise, as long as the read iteration is not the first iteration of the loop (i ~1 > 11), \none itera\u00adtion before the read is (i~ 1 1,15). This information is represented by the LWT in Figure \n2(b). We now describe the general procedure for finding the LWT of a loop-independent read and write \nthat accesses the same location and are perfectly nested inn common loops. We Iirst assume that every \nloop has at least one itemtion. Let ~Wbe the last dependent write for read itera\u00adtion z. If the write \nstatement appears lexically before the rea~ then ~W= ~r. If the read statement appears lexically before \nthe write statement, we create a tree similar to the one in the example using a simple, recursive procedure. \nWe start our recursion with the innermost loop. At any le vel k in the recursion, we create a subtree \nwith the mot if i~~ > L13k , where LBk is the expression for the lower bound of i,,, and where ir, refers \nto the kti component of ~,. When k = 1, the false child is simply 1; other\u00adwise, the false child is the \nresult of applying the recursive step to level k 1. The true child is the solution node 1, iWk+,,m = \nUBk+~...n, = T1...k A 7 8Wk = %k %. k-l where each U B1 is the upper bound for i WI. If the loop is \ntrapezoidal, that is, UB1 is an expression of outer loop indices, we substitute in the values of the \nother compo\u00adnents of ?Wto find its value. If the loops are not perfectly nested, a write access may have \nadditional inner loop indices; however, these indices do not affect the relative ordering between the \nwrite and read instances. Suppose a write statement is nested in m loops and shares only the outer n \nloops with the read statement. We 6rst apply the procedure above on the commonly nested loops, then for \nevery non-1. solution, we set 2W.+1, ..., twm to be their upper bounds. We now consider the possibility \nof having degener\u00adate loop nests, nests where some of the loops may have =ro iterations. A loop is filly \ndegenerate if it will ei\u00adther always have iterations or never have iterations &#38;\u00adpending on the values \nof the symbolic constants. A loop is purtidy degenerate if it is degenerate only for some values of the \nouter loop indices. In the exam\u00adple doi=lto10 do j =ltorz a =,. . end do end do 11 151 14 13 12 11 \n12 11 12 13 1415+ (a) ll<i,2 F T 11< i,, Level ........................ ........ ................... \n................................. T2 i,l, i,2-1 F ........................ ....... . . .............................................. \n irl-1, L5 1 ....................... ................................................................... \no @ (53 - ........................................................................................... \n (b) Figure 2: LWT for loop-independent references the second loop is fully degenerate: it will always \nhave at least one iteration if n ~ 1, and none otherwise. In contrasg do il =nto15 doiz =12toil ..o=a \nla =... end do end do the second loop here is partially degenerate, because it has no iterations only \nwhen il e 12. Ancourt has developed an algorithm for loops with affine loop bounds based on Fourier-Mot&#38;in \nelimination that can be used to detect possibly degenerate loops [2]. For full degeneracies, we can use \nthe algorithm to derive the conditions under which the loop will execute, i.e. n ~ 1 in the above example. \nWe merely need to guard our LWT with these conditions; if these conditions are not satisfied, no writes \nare executed and the solution is J-. Our LWT algorithm relies on being able to find the first iteration \nof each loop, and thus cannot tolerate par\u00adtial degeneracies. Ancourt s algorithm can also rewrite the \nloop bounds to eliminate partial degeneracies; e.g. it will replace the lower bound of il in the partial \ndegener\u00adacy example with the value max(n, 12). This procedure solves the partial degeneracy problem if \nthe read and write statements are nested in the same loops. if the read and write statements have different \nenclosing loops, however, eliminating the partial degeneracy in one of these different loops may change \nthe actual bounds of a common loop for one of the statements. Our algorithm cannot handle this situation, \nand Feautrier s algorithm should be used. Fortunately, as shown in Section 3.3.1, this type of degeneracy \nis rare.  3.2 Writes That Do Not Self-Interfere We now consider the opposite scenario where a write \nop\u00aderation accesses a different location in each iteration. We say that such a write does not self-interfere \n. A write that never self-interferes writes at most one value into any location. For such writes, computing \nthe last depen\u00addent write before a read is equivalent to computing all the dependent writes before the \nre@ we can easily adapt our affine memory disambiguation system to generate the desired LWT. To simplify \nthe presentation in this section, we as\u00adsume that no symbolic constants are used in the access functions, \nand that the access function F. is invertible. Thus, for there to be a dependence, the read and write \nmust refer to the same location. i.e., FW~W = Fr ~r, or ZW= FJIF,;,. The domain of the LWT is the set \nof read iterations, 7,, that satisfy the constraints of the loop bounds, B,?, ~ O. The dependent writes \nmust be within the write bounds B. ?W ~ O, and that these writes must execute before the read. The LWT \ntemplate in Figure 3 ensures that these constraint are met for the case whe~ the mad statement appears \nIexically before the write statement, Assume that the write statement is nwted in m loops, n of which \nare common to the read. After expanding out the lexicographical ordering constraint and replacing in \nthe conditional each occurrence of ~Wwith its value, * z~ = F;lFr~,, we get the LWT in Figme 4, Note \nthat our lexicographic ordering constraint is expanded into two internal nodes for each of the n common \nloop indices, but our solution ~W= F; 1F,;, is a solution for all the write loop indices iW1,... ,i~fi. \nNote also that if the write statement appears lexically before the read, the true child of the lowest \ninternal node, (F; lF.~.). > irm, will b FJIFr~r rather than 1. Some nodes of the tree may be inconsistent. \nIf, for  I Write Boundd 1 \\T   -7---w 1 (@Fr&#38;) ,s (~ ~ Figure 3: Template LWT for non-self-interfering \nwrite example, there is no dependence such that i~. z i~m, the last condition is always false. We can \nprune away this node, replacing the subtree rooted at iwn ~ i,. with its false child. We can find the \ninconsistent nodes by testing each of the nodes for consistency. Checking all the nodes for consistency \nis relatively inexpensive. This is similar to finding if there is a data dependence for each dependence \nlevel, which is only a subset of the standard data dependence analysis. In general, FW and F, are affine \nfunctions of the loop indices and a symbolic constant vector. Thus, for some integer matrices F;, F:, \nF;, F:. Since Fwilw = Fril,, thus FJ~w = G(il,), where We can use the Smith normal form to express lW \nin terms of ii,. In a process similar to the extended GCD Test [4], which is similar to an integral version \nof LU de\u00ad composition, we can find matrices P, Q and S such that PF:Q = S, P and Q are unimodular matrices \nand S DO is of the form where D is a positive diag\u00ad 00 [1 onal matrix. The initial system has an integer \nsolution if and only if there exists an integer vector ~ such that Sf = PGt7,. Since D is a diagonal \nmatrix, the con\u00ad straint SF= PGii. is equivalent to the set of constraints D=,atO = (PGi2,)=. An integral \nt. satisfying this con\u00ad straint exists if and only if (PGil~)8%Ds,o = O where % is the integer modulo \noperation. If there exists an integral solution, then the solution is given by ~W = Q~ If there are no \nzero diagonal elements in S (S = D), ~ = D-l PGii,, whe~ D-l is a diagonal matrix such that (D-l )~,z \n= l/D~,~. TO create the LWT, we merely add to our tree the nodes (PGiip),%Dz,c = O. If these conditions \nare not meti (rwlFjJ ~2 (i) , \\TF (WW F,L)2 s (JJ2 F F WW F,;)22 (;)2 j II )- ...-T1 t F (FW s(iJ FT \n(F$f,ti > (i) F FT Level ................ ........ .......... ........ ...... .............. ........ \n.. ............. ..................\u00adrwlF,i? n ....... ........... ......... .. .... ............. .......................... \n................... HI -:, , 3 ;; ~ i ~~ ? : . 1i . !; c ........... ........ .. ... ............. \n........................... ................... b wlF k ........................... ................2. \n j .....g .... .............~ rw F,; 1 I --------------------------\u00ad . . . -------------------................... \n ...........d2...@?. ..d2. ..cb. ..C!2. ..Q. Figure 4: LWT for non-self-interfering write there is \nno dependent write, otherwise we create a tree as shown in Figure 4, replacing ?W = F; 1Fr ~~ with ~W= \nQD-lPGtir. If S has a O in the diagonal, there are multiple values of ~Wfor each i?~that satisfy F; \n~W= G(Z. ). However, for the write to qualify as not self-interfering, the loop bounds must impose sufficient \nadditional constraints on the problem to reduce the solution to at most one single value of ?W for each \nii~. For these rather uncommon cases, we can use a memory disambiguator to determine the unique ? [18]. \n Note that degenerate loops are not a problem for this case. If a particular solution for ~Wis invalid \ndue to a degenemcy, one of the loop bound constraints will fail, and our solution will be J-. 3.3 Computing \nLWT Efficiently We have shown how to construct LWTS for pairs whose write statements do not self-interfere \nand for pairs with loop-independent mfemnces. Both of these cases, by themselves, are not very interesting. \nWhen write state\u00adments do not self-interfere, LWTS contain no more infor\u00admation than standard memory \ndisambiguation. The ad\u00advantage of dara-flow dependence analysis is to eliminate dependence that are covered \nby other writes. Also, not many writes are loop-independent. We can, though, use AP Cs LG LW MT NA Oc \nSD SM SR TF TI Ws TOTAL S1 o 2 0 0 0 13 39 0 0 0 0 12 0 66 Non-SI 424 84 36 36 74 316 75 142 124 430 \n233 10 517 2,501 Table 1: Number of self-interfering (S1) and non-self-interfering (non-SI) writes after \nremoving unused indices our algorithms for these two cases as building blocks for an algorithm that is \nboth simple and widely applicable. 3.3.1 Domain of Algorithm To specify the domain of our algorithm \nprecisely, let us first introdttm the notion of unused loop indices. We define a loop index to be unused \nif it does not appear in the array reference function of the write nor in the bounds of any used loop \nindex. An unused index may be used by the mad statement. A loop-independent write corresponds to the \ndegenerate case when all the loop indices are unused by the write. Our algorithm can generate the LWT \nexactly and ef\u00adficiently if eliminating the unused loop indices reduces the write access into a non-interfering \none, and if An\u00adcourt s algorithm to eliminate partial degeneracies does not introduce read bounds that \nare different from the write bounds in any of the removed loops. To assess the generality of our algorithm, \nwe performed the following experiment. We inserted a pass in our SUIF (Stanford University Intermediate \nFormat) compiler to examine all of the army writes in the PERFECT Club programs that have affine array \nindex expressions and are nested in at least one loop. After eliminating the unused indices, we checked \nto see if each write statement self-interfered. This is equivalent to checking for an output memory dis\u00adambiguation \ndependence from the write to itself with a non-zero direction vector component. Table 1 shows that after \nremoving the unused indices, 2,501 out of 2,567 (over 97%) writes do not self-interfere. Some of these \nwrites may not have been self\u00adinterfering even before eliminating the unused variables and, for these \ncases, our analysis generates no more in\u00adformation than standard memory disambiguation. Even looking \nonly at writes which do originally self-interfere, we are able to handle 566 out of 632 (about 90%) cases. \nWe also performed a simple experiment to determine the frequency of Partial degeneracies. Partial degenera\u00ad \ncies can only occur in trapezoidal loops. For each trape\u00ad zoidal loop, we used a simple and efficient \nsubstitution step to determine if a trapezoidal loop can be partially degenerate[15]. Our simple algorithm \nwas able to prove that over 98% of the writes in the PERFECT Club were not nested in any partially &#38;generate \nloops. 3.3.2 Our Algorithm The outline of our algorithm is as follows: 1. Apply Ancourt s algorithm to \nfind the conditions for full degeneracy and to eliminate partial degenera\u00adties. The conditions for full \ndegeneracy are placed at the top of our LWT. 2. Remove the loops corresponding to those loop in\u00addices \nthat are unused by the write statement. We treat the unused limp indices remaining in the read statements \nas symbolic constants. 3. Check if the write in the reduced system self\u00adinterferes. If the write self-interferes, \nthe algorithm is not applicable. Otherwise: 4. Construct the LWT for the reduced system using the non-self-interfering \nalgorithm. 5. Construct the LWT for the original system by restor\u00ading the unused loop indices. We will \ndes~be this step in detail below. If Ancourt s algorithm has set any of the write bounds in the unused \nloops indices to be different from the corresponding read bounds, this step of our algorithm is not applicable, \nand we must use Feautrier s algorithm to restore the unused indices.  Note that even when our algorithm \ndoes not apply be\u00adcause of self-interference, our LWT is only inaccurate in that it conservatively describes \nmore &#38;pendent writes than just the last write before the read. Our algorithm is still useful in pruning \nsome of the false &#38;ta-flow de\u00adpendence in cases where only some of the writes to the same locations \noccur in iterations whose loop indices are unused. 3S.3 Restoring Unused Variables To construct the LWT \nfor the original system ffom the LWT for the reduced system, our algorithm restms the unused indices \none at a time starting with the innermost index. We show, at each stage, that our algorithm gener\u00adates \nthe correct tree for the partially reduced system with all loops corresponding to the not yet restored \nvariables removed. Recall that tAe values of the non-common indices do not affect the ordering between \nthe read and write. Thete\u00ad fore, for any non-l-solution node, we should set all non. common, unused write \nindices to their upper bounds, and we should set all the non-common, used indices to their corresponding \ncomponent in the solution, QD -1 PGiir. We can ignore the non-common loops for the remainder of our discussion. \n The algorithm to restore the index corresponding to the kti outer loop is as follows. Let T be the LWT \nbefore the restoration and T be the new tree. Each internal node in an LWT partitions the domain of the \nLWT function. A given value of ~, and symbolic variables falls in the iteration set of one and only one \nleaf. Thus, when cre\u00adating T , we must merely ensure that each leaf contains the correct solution for \nall the reads in its read iteration set. We consider non-L and L leaves in T separately below. non--l-leaves \nin Zl Every read in the read iteration set of a non-l leaf in the fully reduced system is preceded by \none and only one write to the same location. By definition, the unused indices do not affect the location \nbeing written. Therefore, the location vwitten in ?W in the full system is the same as the location read \nin ~, if and only if the solution for all the used indices in the full system is the same as the solution \nfor all these indices in the reduced system. Thus to restore the unused index, iW~, to the leaf in T, \nwe merely need to set the unused indices so that ~Wis the last write before the read. Each non-L leaf \nin T has a level, 1, corresponding to the minim% index such that iWl < irl. If no such index exists, \nthen ~W= ?~ in T, and we define / = n + 1 whete n is the number of common loops in the original, full \nsystem. Note that since we are restoring the index for the kti outer loop, there are no leaves with level \n1 = k m 1 , 1nus me two POssmlecases are 1. / < k. Since the algorithm restores the innermost unused \nindices tirst, and since index / appears in T, ir, must correspond to a used index. Since the restoration \nprocess cannot change the value of a used index, we know that i~l c i~l in T . What\u00adever vakte we give \niWk in T , the write will come before the read. We therefore set i., to be as large as possible, the \nupper bound for loop index . iks uBk(~wl Y. . . j Swh-1 ) . lftheupperboundisa constan~ we simply set \niw~ to its value. Otherwise, if the loop is trapezoidal, we can use simple substi\u00adtution to find its \nvahte. Note that we must ensure that any previously restored variable retains its cor\u00adrect value throughout \nthe restoration process. Since any previously restored variable, iw~, has m > k, we know that m > / and \ntherefore iw~ has previ\u00adously been set to its upper Mxu@ which is still its correct value. 2. / > k. \nSince in T, ~w ~ ;,, we must have iwl = %17. ..)2Wk_] = irk-l for all the existing indices in T. If we \nset iwk to be larger than irh, then the write instance will be later than the read S0 we set iwk = irk. \nThis is always possible since the bounds on the read are equivalent to the bounds on the writes. Now \nconsider once again each pre\u00ad viously restored variable, iw.. Our restoration pro\u00ad cess has not changed \nthe level of the let@ it is still /. Since the value of iw. was determined solely by the relative values \nof m and 1, the value for iwm in T is still the correct value for iwm in T . Thus for every non-l leaf \nin T, we can compute the solution in T . 1 leaves in 2V In the reduced system, there are three different \ntypes of J-leaves: 1. out of bounds writeq the writes will still be out of bounds in T . 2. reads without \nany wn tes rqfern ng to the same lo\u00adcatwn; since the unused indices do not affect the locations written, \nthere can also be no dependent writes in T . All such J-leaves in T remain as 1 leaves in T . 3. reads \nwith no preceding dependent writes the fact that there was no preceding dependent write in T does not \nnece+mrily imply that there is no preceding dependent write in T . In this category, there is an iteration \n;W that writes to the location read in ittxation ;r in the t%tly reduced system, except that thewritecomes \nafter theread  If there exists ~ 1<: k such thttt iW, > irl, m@d\u00adless of the value of iwk or any of \nthe other previously restored indices, the write will still come after the read and these leaves remain \n1. leaves in T . If no such / exists, then iw, = irl,..., iw,-, = irk-l for all the existing indices \nin T. In this case, as long as iw~ < irh, there is a preceding depen\u00addent write in T . Since our read \nand write bounds are identical, if i,, is greater than its lower bound LBk(ir,,..., ir,-l), the latest \nwrite occurs when iwk = irk -1, otherwise there is no iwk < i,, and the solution remains J-. Our non-self-interfering \nwrite algorithm makea it easy to find W the leaves where no such 1 exists. We merely search T for the \nnode with the constraint iw~, >_ ir,,, where H is the last used index less than k. We call this node \nour dividing node. If there is no used index, k < k, our dividing node is the last dependence node, (PGt7T)z%Da,a \n= O, the parent of the first lexicographic constraint node in T. From the context of this node, we know \nthat iwl = r19...9bk_1 = irk_l for all the existing indices in T. Since every node inherits the con\u00adtext \nof its ancestors, these equality conditions are true for every descendent of the true edge of the dividing \nnode. Thus, for any descendent L node of the true edge of our dividing node, no such 1 exists. We therefore \nreplace all such descendent 1 nodes with the subtree whose root is the condition if irk > Ll?k(ir,, . \n. . ,irk-, ) and whose false child is a 1. node. The true child is the solution node with iwk = irk \n1, every other unused variable set to its respective upper bound and every used variable set to its corresponding \ncomponent in the solutiom QD-lPGii,. Note that the subtree we have de\u00adscribed is the same as the tree \ngenerated by our loop-independent algorithm for the case of a mad statement followed by a write statement \nnested in one loop. Loop-independent references correspond to the case when all variables are unused. \n 3.3.4 Example of Restoring Unused Variables We now illustrate our restoration algorithm with an exam\u00adple. \nSuppose the example program has a read followed by a write statemen4 both nested within the same three \nloops. The second loop index is unused in the write ac\u00adcess function and has constant lower and upper \nbounds, LB2 and UB2, respectively. Assume that after moving the second loop index, the write in the reduced \nsystem does not self-interfere. The LWT for the reduced sys\u00adtem is shown in Figure 5. We label some of \nthe internal nodes No, IVl and N2 to aid in the discussion. BWQD-l20 PGiir \\ (PGiQ,%I)z ,.0 \\T (QD- \nPGQ< (i,) ~ ~ \\T (fW- pG@, ~(;,), No F \\T F (QD-1PG;J3 s (i,)3 N1 F \\T F (W PG~J# (:,)3 N2 F F I ...... \n........ ....... ............ ....................... 3 ..... ........ ...... ........... ................. \n...... .......... .................... 2 ..... ........ ....... ............ .................. ................................. \n...................  ............................................................................................................ \nFigure 5: LWT for reduced system We now wish to restore the second hop index, that is, k = 2. Consider \nfirst the two non-L leaves. The dependence level of the lower left leaf is 1 which is less than k = 2. \nWe therefore set iw, = UB2 for this leaf. The depen&#38;nce level of the upper right leaf is 3 which \nis greater than k = 2. We therefore set iWz= ir2 for this leaf. Now consider the L leaves. Node No, representing \nthe condition that i WI z i,i, is the dividing node. For any J\u00adnode descended from its true edge, we \nknow that iWl = i,,. There are two such 1 leaves: the false child of IVl and the true child of N2. We \nreplace both J-leaves with the subtree whose root is the condition if i,, > LB2 , whose false child is \na 1 node, and whose true child has the standard partial solution and i., = i,, 1. We show the LWT for \nthe full system in Figure 6. %%!\u00ad 172% I 1 , <T (QD- P;F J s f: J NO F F F F (QD-1PGi>32 (:,), N, &#38;z!z!&#38;el \n...... ....... ....... ......... .......T.~\\T . . ....3 HIT . . +i%f * I/Ii t!$ii 1 Ka?2iJ2 ~~~@~:~: \no ........................................................................................................... \n Figure 6: LWT after xestoring unused variables 3.3.5 Complexity of Algorithm Our technique offers substantial \nimprovement in time over Feautrier s algorithm for those problems that are within our algorithm s domain \n(as discussed in Section 3.3.1). The prepass of our algorithm needs to calculate the Smith normal form \nfor the write array access function. Also, in the rare cases where partial degeneracy may occur, the \nalgorithm eliminates the degeneracies using Fourier-Motzkin elimination, which is exponential in the \nworst case. The treebuilt by our algorithm has 0(n2) nodes where nisthenumber ofloopsin the nest. l%etree \nfor the fully reduced system represents the lexicographic order\u00ading constraints with at most 2n internal \nnodes, and 2n + 1 leaf nodes. When incorporating an unused index into the system, the only possible change \nto the tree structure is the replacement of L leaves by subtrees consisting of one internal nti one non-L \nleaf and one 1. leaf. Since the number of J-leaves remains unchang@ the final tree has 0(n2) nodes. Our \nalgorithm visits each node at most n times and the computation per node is trivial. In contrast, Feautrier \ns algorithm can generate exponen\u00adtial trees, where each node involves solving a parametric integer program, \nwhich in tmn is exponential in com\u00adplexity. 3.4 LWTS for Multiple Reads and Writes We have described \nan algorithm to calculate an LWT for a single pair of read and write statements. For multiple read statements \nwe can treat them as separate problems, and construct an LWT for each read statement. This is not possible \nwhen dealing with multiple writes. One approach is to find the LWT for each individual write statement \nwith respect to the same read statement, and find the last of all the last write solutions tlom the different \ntrees. A description of this algorithm can be found in [15 J. The complexity of the resulting tree and \ntherefore the complexity of ow algorithm can grow ex\u00adponentially with the number of write statements. \nThere are, though, hvo special cases that we believe are com\u00admon. If two write statements write to exactly \nthe same locations on each iteration, then one write statement al\u00adways overwrites the writes of the other \nstatement. The desired result is then simply the LWT for the covering write. The other simple case is \nwhen the two writes write to non-overlapping locations. The non-~ leaves in the two LW 13 refer to different \nlocations and thus must have disjoint read iteration sets. The solution is simply the union of the two \ntrees. 4 Array Privatization It is important that the information generated by a com\u00adplex program analyzer \nbe both sufficient and necessary for program optimization. We now use the example of array privatization \nto demonstmte that array data-flow information is indeed useful. Recent research by Eigen\u00admann et alp], \nby Singh et al.[22] and by US[17] has shown that array privatization is a critical transformation in \nparallelizing real programs. Eigenmann et al. dis\u00adcuss the techniques used to hand-parallelixe four of \nthe PERFECT Club programs[8]. They find that array pri\u00advatization is useful in all of the programs. In \ntwo of the four programs, 98% of the dynamic execution time of the program is spent in loops that require \narray privatizaticm before they can be parallelized. In the problem domain of nested loops with affine \nloop bounds and array indices, Feautrier developed an algo\u00adrithm that expands the array fully so that \nno single lo\u00adcation is ever written multiple times[9]. Such expansion eliminates all the spurious true \ndependence (those that are not also data-flow dependence), anti-and output de\u00adpendence in the loop nest. \nHowever, it is too expensive both in execution time and compilation time to fully ex\u00adpand an array indiscriminately. \nWe have developed a parallelization algorithm that uses array privatization if and when necessary. Our \nal\u00adgorithm will only privatize an array when it is legal and when it is necessary to improve parallelism, \nThe cost of privatization is the extm memory required to stem sep\u00adarate copies of the array. It is only \nnecessary to sto~ one copy per processor, as opposed to one copy per it\u00aderation for array expansion. \nThis minor space overhead is more than offset by tie increased parallelism. This is confirmed by Eigenmann \net al. s study which shows significant parallel speedups after hand privatizing real programs[q. In Section \n4.1, we first show why data dependence analysis fails to parallelize loops that muse arrays. We n3\u00adview \nthe terminology and theory developed using the data dependence analysis results. In Section 4.2, we present \nan algorithm that combines parallelization with army pri\u00advatization. We introduce the abstractions of \ndata-flow vectors, upwards exposed uses and downwards exposed definitions within a loop. We show how \nsuch informa\u00adtion is used in the algorithm. We show how to compute the information from the LWT in Section \n4.3. Finally, Section 4.4 shows an example of the code generated by our compiler. 4.1 Parallelizatiom \nwith Data Dependence Information Definition 4.1 Let s an$ St be two array statements in a program. There \nis a tkQ(NNkW? from 8 to s if there exist two iterations, ? and?, such that the location accessed by \nreference s in iteration ;is the same as the location accessed by reference d in iteratwn ?, and  iteration \n7 executes before ?.  Iteration ; executes &#38;fore iteration ? if;+ ~, or ~ tY= ? and statements \nappears Iem cally before statement s in the program text. The &#38;pendence is a true dependence z~s \nis a write and s is a read;  an anti-dependence z~s is a read and d is a write;  forj=lto Nlby2do \nfori=lto N.ZPdo ii= i+i worklii -1]= Afi]fi] worirfti] = A/j]Q + 1] fori=lto NZPdo A/iJ~] = work[i] A/i], \n+ 1]= WOZkfl + NW] Figure 7: Code flom PERFECT Club Benchmark OCS an output dependence z~s and St are \nboth write accesses. Definition 4.2 The direction vector ~ froms to s is of length m, where m is the \ncommon nesting level of the references. For all k, 1< k < m, + i~ ik>o dk= () i~ ik=o r ii ik <() { \nDefinition 43 A dependence vector ~ = (cJ1,..., dm) is sm d to be carried at level k tf (dl, . . . ,dk_l) \n= (o,..., O) and dk is + /1]. Theorem 4.1 The kth outer loop of a loop nest is par\u00adallelizable ~ theloop \nnest has no k-level &#38;pendence vectors. Consider the code extracted fkom the PERFECT Club benchmark \nprogmm OCS shown in Figure 7. We con\u00adsider only dependence involving the work array here since the references \nto the A array are parallelizable us\u00ading existing techniques. Dependence analysis detects: true dependence \nfrom each write reference work[fi\u00ad1] and work[li] to each read reference workfi] and WOrkfi+N~~ ( + ) \nand (0) e an anti-dependence from each read reference to each write reference: ( + ) e an output dependence \nfrom each write reference to itae~. ~+ ,0) By theorem 4,1, the outermost loop is not parallelizable because \nall the dependence are carried at level 1. 4.2 Paralleliza$ion with Data-Flow Infor\u00admation 4.2.1 Data-Flow \nDependence Vectors Analyzing the data-flow of the example program, it is clear that the only definitions \nthat reach the reads am those within the same outer iteration. The actual true dependence from the write \noperations to the read opera\u00adtions are more accurately described by only the direction vector (0)0 Thns \n*r eliminating the spurious true de\u00adpendence ( + ), this loop is parallelizable if the artificial anti-and \noutput dependence are eliminated by privati\u00adzation. Definition 4.4 There is a data-flow dependence from \na wn te reference w to a read reference r if at least one instance of r reaak a value written by w. We \ncan define data-jlow direction vectors accordingly. Data-flow direction vectors are a subset of true \ndepen\u00addence vectors. Those true depen&#38;nce directions that are not flow directions are artificial \ndependence that can be eliminated by renaming. 4.2.2 Privatizing an Array We say that an array is privatizable \nwithin a loop if there is no need to communicate any of the values written to the array between iterations \nof the loop. Definition 43 An array is privatizable in loop k if the loop has no k-level alata-jlow vectors. \nObviously, a loop k is not parallelizable if any of the arrays is not privatizable in loop k. While no \ncommunication is necessary between itera\u00adtions of the parallelized loop, an iteration maybe reading values \ndefined before the lcop, and the values written may be used after the loop. Therefore, each processor \nmay need to initialize its private copy and to copy back some data from the private copy to the original \nvariable at the end of the.execution. We refer to the former process as initialization and the latter \nasfinalization. A read of location / in an iteration n is upwards ex\u00adposed if iteration n does not contain \na definition (a write) of 1before the read. A write of 1in an iteration n is down\u00adwards exposed if it \nis the last definition of 1in iteration n. Obviously, if an array is privatizable in a loop, any read \nthat is upwards exposed within an iteration must also be upwards exposed in the enclosing loop. Each \niteration must initialize its local copy with values from the global version of the array at all those \nlocations for which there is an upwards exposed use. Finalization is slightly more complicated. Multiple \niterations in a loop may write to the same location; only the last iteration that writes to the location \nshould update the global copy. Definition 4.6 LetA be an array vm able, and; be an iteration 7 such that \nA[fl is an upwards INH (A, ~ = { exposed use in iteration; } I such that A~fl is a &#38;jinitwn in FIN(A, \n?) = iteratwn Z and it is downwardr { exposed in :he enclosing loop } 4.2.3 Algorithm l%eorem 4.2 The \nkth outer loop of a loop nest is paral\u00ad lelizable if the loop has no k-level data-jiow dependence vectors, \nand all arrays with k-level anti-and output de\u00adpendence are privatized. The algorithm to parallelize \na loop with may privati\u00adzation optirnizations is as follow~ 1. Compute the data-flow vectms. 2. Find \nall the pamllelizable loops, loops that carry no data-flow dependenees. We may choose to paral\u00adlelize \nall or just some of the loops. 3. For each loop k to be paralleli~ compute the INIT and FIN sets for \neach array that has k-level anti-and output dependence. 4. Modify the code by annotating the loop as \na paral\u00adlelizable loop. For eaeh array privatized, we declare a new local array, insert the code to initialize \nthe INIT set at the beginning of the iteration, and write back the FIN set at the end of each iteration. \nIn praetiee, multiple iterations ean be merged together and scheduled on the same processor.  4.3 Using \nLWTS for Array Privatization 4.3.1 Data-Flow Dependence Vectors Our algorithm calculates LWTS on a dependence \nlevel by dependence level basis. All the data-flow dependence pairs deseribed by a leaf me at the same \nlevel. This is a useful property for calculating data-flow dependence vec\u00adtors. Different algorithms \nfor constructing LW Ik might not guarantee that each leaf only describes singlelevel dependenees, but \nit is always possible to split a multi\u00adlevel leaf into several nodes with only single-level leaves. We \nean use an LWT to easily calculate the data-flow dependence vectors. We visit every leaf in the tree \nthat is not labeled L. The eontext and the solution of the leaf impose a set of linear constraints on \nthe iteration vari\u00ad ables which ean be expressed in matrix form as A;> O where ?is the combined vector \n(?W,?7,i?). This is exaetly the same formulation as for the affine memory disarn\u00ad biguation problem. \nWe can solve for the data-flow vec\u00ad tors in the same way that dependence analysis solves for direction \nvectors using a variation of Burke and Cytron s method[6], We then return the set union of all the data\u00ad \nfiow dependence vectors calculated at each leaf. The properties of our LWTS ean be exploited in opti\u00ad \nmizing this procedure. Burke and Cytron s method works by enumerating and verifying possible direction \nvectors. If the dependence level of a leaf is k, any data-flow de\u00ad pendence vector with a direction other \nthan O in any of its first k -1 dimensions is infeasible. We ean immedi\u00ad ately prune away any such vector. \nGiven an LWT, we can calculate data-flow dependence vectors this way as efficiently as calculating direction \nveetors. 43.2 IFII I and FIN Sets We ean use our LWT directly to calculate the INIT sets. A leaf at \nthe kti level indicates the presenee of a use that is upwards exposed in the iterations of the kti lcop. \nIf loop k is paralletizable, by definition, rhem are no k\u00adlevel leaves in the LWT. AU leaves of level \nless than k correspond to upwards exposed uses in the iterations of the privatized loops. The INIT. set \nis therefore the access locations of the leaves of these levels. The leaves with level greater than k \ndo notcontribute to the INIT set. It is then easy to genernte code that correctly initializes all the \naccess locations of a leaf in the LWT. We gen\u00aderate a copy of the original set of loops and a reference \nwith the original mad access function. We nest the ref\u00aderence within a set of conditional statements \nso that the reference is made only if the current loop indices satis~ the context of one of the given \nleaves. Ancourt shows how to optimize this basic algorithm using an extended version of Fourier-Motzkin \npair-wise elimination[3]. An-CWurtalso shows how to eliminate the duplicate access locations in multiple \ncontexts. The FIN set is not directly available from an LWT. Finding the FIN set, the last write to every \nlocation, is in fact a subproblem in calculating the LWT. However, when the LWT is generated using our \nsimplified algo\u00adrithm, finding FIN sets is trivial. When privatizing the kh loop If i~ is used, definitions \nof different iterations of i ~ are independent. l%exefore FIN(A, (il... ik)) is simply the set of all \nthe definitions in the iteration.  ~ ik k ~Us~ kfinitions of different i~~ons of ik are the same. lhls \nFIN(A, (il . . . ik)) is the set of definitions in the given iteration if i k! = UBk, and empty otherwise. \n  4.4 Implementation and An Example We have implemented our algorirhm in the SUIF eOm\u00ad piler system. \nOur pamllelizer marks all the parallelizable loops and privatizes arrays when needed. We show the output \nof our compiler on the code in Figure 7 as an example. Besides parallelization and pri\u00ad vatization, \nthe SUIF optimizer has to first perform loop normalization, constant propagation and induction vari\u00ad \nable identification on the code. The code to be run by every processor is shown below. (To simplify the \npre\u00ad sentation, we do not parallelize the inner loops although they can be parallelized using standard \ntechniques.) LB=o UB = floo@W-1)/2) Blk = ceil(@3-LB~.oc) if @id!=NutnRVc-1) then / notthe last processor \n*/ for j = LB+pid*Blk to LB+@id+l)*Blk -1 do P .workpriv is a local army */ fori=Ito N2Prlo -wor@xiv[2*i-1] \n= Afi][27+l] -workpn v[2*i] = Afi][27+2] end for f ori=lto NPdo A/1][29+1] = -workpxivli] AfiJ[27+2] \n= .workprivli+NW] end for end for eke P lastprocessor */ for j = LB+pid*Blk to UB do fori=lto NZPdo work[2*i-1] \n= Afi][27+l] work[2*i] = AJt][27+2] end for fori=lto N2Pdo Afi][29+l] = workfi] Afi][27+2] = workfi+NW] \nend for end for Note that it is noteasy to discover that initialization is unnecessary for this example. \nEach read gets part of its data from each write. Therefore, we cannot simply look at pairs of references. \nThe effects of the two writes must be combined by merging the two individual LWTS. Since the two writes \nwrite to non-overlapping locations, our system efficiently determines that initialization is un\u00adnecessary. \nAs for finalization, since all the iterations of the loop write to the same locations, the final values \nof the array are &#38;termined only by the last iteration. Since no initialization is needed, the last \nprocessor can directly operate on the original copy of work and no synchro\u00adnization is necessary. Conclusions \nResearchers in compiler technology have long realized that flow-insensitive alias analysk-~ is not sufficient \nfor advanced optimization. It is necessary to analyze the flow of data through the program. Similarly, \nwe be\u00adlieve that parallelizing compilers have now reached the stage where data dependence analysis, the \narray analog to scxdar alias analysis, is no longer sufficient for more advanced optimizations. Previous \ntechniques of array data-flow analysis are ei\u00adther too inefficient or too imprecise. We have developed \nan efficient algorithm that can find perfect information in the domain of loop nests with affine loop \nbounds and array indices in most of the programs found in praetiee. The efficiency of this algorithm \nis important in making array data-flow analysis feasible to use in program opti\u00admization. We have applied \nthis analysis to the problem of array privatization, itself an optimization that has been estab\u00adlished \nto be important. This optimization demonstrates the need for information on coverage and the identity \nof dependence, both missing in standard data dependence analysis. This paper provides the basic technology \nfor array privatization. Extension of the analysis across pro\u00adcedures is necessary to make it truly applicable \nto real w-s. More important than a particular optimization is the general framework of our approach. \nIn developing the algorithm for array privatization, we have defined several useful higher level abstractions: \ndata-flow dependence vectors, upwards-exposed uses and downwards-exposed definitions of individual array \nelements. These abstrac\u00adtions can be derived easily from the Last Write Tree, tie low-level representation \nfor the array &#38;ta-flow informa\u00adtion. These concepts will make possible many more ad\u00advanced optimizations \nsuch as minimizing communication on a distributed memory machine. References [1] R. Allen, D. Callahan, \nand K. Kennedy. Automatic decomposition of scientific programs for parallel execution. Technical Report \nRice COMP TR86-42, Department of Computer Science, Rice University, NOV 1986. [2] C, Ancourt and F. Irigoin. \nScanning polyhedra with do loops. In Proceedings of the ACM SIGPLAN 1991 Principles and Practice of Parallel \nProgram\u00adming, pages 39-50, 1991. [31 M. Ancourt. Generation automatique de codes &#38; tran$ert pour \nmultiprocesseurs a memoires locales. PhD thesis, University of Paris VI, March 1991. [4] U. Banerjee. \nDepen&#38;nce Analysis for Supercom\u00adputing. Kluwer Academic, 1988. [51 T. Brandes. The importance of \ndirect dependence for automatic parallelism. In Proceedings of 1988 lnternatwnal Conference on Supercomputing, \npages 407-424, 1988. [61 M. Burke and R. Cytron. Interprocedtual depen\u00addence analysis and parallelization. \nIn Proceed\u00adings of the SIGPLAN 1986 Symposium on Compiler Constructwn, pages 162-175, 1986. [;] R. Eigenmann, \nJ. Hoeflinger, Z. Li, and D. Padua. Experience in the automatic parallelization of four perfect-benchmark \nprograms, In Languages and Compilers for Parallel Computing, pages 65-83, Aug 1991. [8] M. Berry et \nal. The PERFECT Club benchmarks: effective performance evaluation of supercomput\u00aders. Technical Report \nUIUCSRD Rep. No. 827, University of Illinois Urbana-Champaign, 1989. [9] P. Feautrier. Array expansion. \nIn Internatwnal Con\u00adference on Supercomputing, pages 429442,1988. [10] P, Feautrier. Parametric integer \nprogramming. Technical Report 209, Laboratoire Methodologies and Architecture Des Systemes Informatiques, \nJan 1988. [11] P. Feautrier. DatafJow analysis of array and scalar refenmces. International Journal of \nParallel Pro\u00adgramming, 20(1):23-52, Feb 1991. [12] E. D. Granston and A. V. Veidenbaum. Detecting redundant \naccessesto array data. In Supercomputing 91, pages 854-865, 1991. [13] T. Gross and P. Steenkiste. Structured \ndataflow anal\u00adysis for arrays and its use in an optimizing compiler. Sofmare -Practice and Experience, \n20(2): 133-155, 1990. [14] Z. Li. Array privatization for parallel execution of loops. In Internatwnal \nConference on Supercom\u00adputing, 1992. [15] D. E. Maydan. Accurate Analysis of Array Ref\u00aderences. PhD thesis, \nStanford University, October 1992. [16] D. E. Maydan, J. L. Hennessy, and M. S, Lam. Efficient and exact \ndata dependence analysis. In Proceedings of the SIGPLAN 1991 Programming Language Design and Implementation, \npages 1-14, 1991. [17] D. E. Maydan, J. L. Hennessy, and M. S. Lam, Effectiveness of data dependence \nanalysis. In Pro\u00adceedings of the NSF-NCRD Workshop on Advanced Compilation Techniques for Novel Architectures, \n1992. [18] W. Pugh. The omega test a fast and practical in\u00adteger programming algorithm for dependence \nanal\u00adysis. ~ Supercomputing 91, pages 4-13, 1991. [19] W. Pugh and D. Wonnacott. Eliminating false data \ndependence using the omega tes~ In Proceedings of the SIGPLAN 1992 Programming Language De\u00adsign and Implementation, \npages 140-151, 1992. [20] H. Ribas, Obtaining &#38;pendence vectors for nested\u00adloop computations, In \nProceedings of 1990 inter\u00adnational Conference on Parallel Processing, pages 11-212 to 11-219, 1990. [21] \nC. Resend. Incremental Dependence Analysis. PhD thesis, Rice University, March 1990. [22] J. P. Singh \nand J. L. Hennessy. An empirical in\u00advestigation of the effectiveness and limitations of automatic parallelization. \nIn Proceedings of the In\u00adternational Symposium on Shared Memory Multi\u00adporcessing: Tokyo, Japan, pages \n25-36, 1991.  \n\t\t\t", "proc_id": "158511", "abstract": "<p>Data-flow analysis of scalar variables and data dependence analysis on array elements are two important program analyses used in optimizing and parallelizing compilers. Traditional data-flow analysis models accesses of array elements simply as accesses to the entire array, and is inadequate for parallelizing loops in array-based programs. On the other hand, data dependence analysis differentiates between different array elements but is flow-insensitive.</p><p>This paper studies the combination of these two analyses&#8212;data-flow analyses&#8212;data-flow analysis of accesses to individual array elements. The problem of finding precise array dataflow information in the domain of loop nests where the loop bounds and array indices are affine functions of loop indices was first  formulated by Feautrier. Feautrier's algorithm, based on parametric integer programming techniques, is general but inefficient. This paper presents an efficient algorithm that can find the same precise information for many of the programs found in practice. In this paper, we argue that data-flow analysis of individual array elements is necessary for effective automatic parallelization. In particular, we demonstrate the use of array data-flow analysis in an important optimization known as array privatization.</p><p>By demonstrating that array data-flow analysis can be computed efficiently and by showing the importance of the optimizations enabled by the analysis, this paper suggests that array data-flow analysis may become just as important in future optimizing and parallelizing  compilers as data-flow and data dependence analysis are in today's compilers.</p>", "authors": [{"name": "Dror E. Maydan", "author_profile_id": "81100138889", "affiliation": "", "person_id": "P70229", "email_address": "", "orcid_id": ""}, {"name": "Saman P. Amarasinghe", "author_profile_id": "81100533031", "affiliation": "", "person_id": "P258981", "email_address": "", "orcid_id": ""}, {"name": "Monica S. Lam", "author_profile_id": "81100237956", "affiliation": "", "person_id": "PP14092336", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/158511.158515", "year": "1993", "article_id": "158515", "conference": "POPL", "title": "Array-data flow analysis and its use in array privatization", "url": "http://dl.acm.org/citation.cfm?id=158515"}