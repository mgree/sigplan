{"article_publication_date": "10-22-2011", "fulltext": "\n Kismet: Parallel Speedup Estimates for Serial Programs Donghwan Jeon Saturnino Garcia Chris Louie MichaelBedfordTaylor \nDepartment of Computer Science&#38; Engineering University of California, SanDiego La Jolla, CA, USA \n {djeon,sat,cmlouie,mbtaylor}@cs.ucsd.edu Abstract Software engineers nowface the dif.cult taskofrefactoring \nserial programs for parallel execution on multicore proces\u00adsors. Currently, they are offered little guidance \nas to how much bene.t may come from this task, or how close they are to the best possible parallelization. \nThis paper presents Kismet, a tool that creates paral\u00adlel speedup estimates for unparallelized serial \nprograms. Kismet differs from previous approaches in that it does not require anymanual analysis or modi.cation \nof the program. This difference allows quick analysis of many programs, avoiding wasted engineering effort \non those that are fun\u00addamentally limited. To accomplish this task, Kismet builds upon the hierarchical \ncritical path analysis (HCPA) tech\u00adnique, a recently developed dynamic analysis that localizes parallelism \nto each of the potentially nested regions in the targetprogram.It then usesa parallelexecution time model \nto compute an approximate upper bound for performance, modeling constraints that stem from both hardware \nparame\u00adters and internal program structure. Our evaluation applies Kismet to eight high-parallelism NASParallelBenchmarksrunning \nona 32-coreAMD mul\u00adticore system, .ve low-parallelism SpecInt benchmarks, and six medium-parallelism \nbenchmarks running on the .ne\u00adgrained MIT Raw processor. The results are compelling. Kismet is able to \nsigni.cantly improve the accuracyof paral\u00adlel speedup estimates relative to prior workbased on critical \npath analysis. Categories and Subject Descriptors D.2.2[Software En\u00adgineering]:DesignTools andTechniques; \nD.1.3[Program\u00adming Techniques]: Concurrent Programming Parallel Pro\u00adgramming Permission to make digital \nor hard copies of all or part of this work for personal or classroomuseisgrantedwithout feeprovidedthat \ncopies arenot madeordistributed forpro.torcommercialadvantage andthatcopiesbearthisnoticeandthefullcitation \nonthe .rstpage.To copy otherwise,torepublish,topostonserversortoredistribute tolists,requirespriorspeci.cpermission \nand/ora fee. OOPSLA 11, October22 27,2011,Portland,Oregon,USA. Copyright c &#38;#169; 2011ACM978-1-4503-0940-0/11/10. \n. .$10.00 $> make CC=kismet-cc $> $(PROGRAM) $(INPUT) $> kismet --openmp Cores 1 2 4 8 16 32 64 Speedup \n1 2 3.8 3.8 3.8 3.8 3.8 (est.) Figure 1: Kismet s User Interface. After compiling and executing the \nprogram, Kismet produces estimated upper bounds on parallel speedups for the program. GeneralTerms Measurement,Performance \nKeywords Hierarchical CriticalPath Analysis, Expressible Self-Parallelism,Performance Estimation,Parallel \nSoftware Engineering 1. Introduction Software engineers currently face the enormous task of refactoring \ntheir programs to take advantage of multi-core processors. These multi-core processors provide extensive \nparallel resources, providing the potential for greatly im\u00adproved performance. However, this improved \nparallel per\u00adformance is typically unlocked only after extensive engi\u00adneering efforts that rely on the \nindividual engineer s expe\u00adrience rather than automated tools. Frequently, the parallel\u00adization process \nresults in dif.cult-to-diagnose bugs and a murkyunderstanding of the connection between code modi\u00ad.cations \nand resulting performance. Theperformanceofrefactored serialprograms oftenfalls short of optimistic speedups \nderived from raw hardware re\u00adsources.Worse-than-expected performance canbe causedby several factors. \nFirst, the implementation may be poor the result of missed parallelism opportunities or poorly executed \nparallelization attempts. Second, the program may have in\u00adherently low amount of parallelism possibly \nthe result of choosing an algorithm without considering its parallelizabil\u00adity. Finally, the target system \nmay be a poor choice for that program the result of a mismatch between the structure of the parallelism \nin the program and the ability of the system to ef.ciently exploit it.  An assortment of tools have \nbeen developed to help in the task of refactoring for parallelism. Some of these tools [6, 18, 47] help \nthe programmer debug performance prob\u00adlems in their parallel implementation. While these tools help the \nprogrammer overcome poorly executed parallelization, they fail to uncover missed parallelism opportunities \nor de\u00adtermine if otherfactors in poor performance are the limiting reagents. Other tools have looked \ninto measuring the paral\u00adlelismavailableinaprogram[27,30]but often lookonlyat abstract models of execution \nand do not provide realistic es\u00adtimates on the speedupof therefactoredprogram.Yet other tools examine \nthe scalability of a parallel program [10, 53] but look only at an existing implementation and therefore \ndo not provide insight into the fundamental scalability of a program. Furthermore, a vast majority of \nexisting tools as\u00adsume that there is already a mature parallel implementation. Relying on these tools \ngreatly increases the probability of undesirable sunk costs: programmers can waste signi.cant time and \nmoneyin refactoring code only to determine a fun\u00addamental limitation to its performance. Kismet s Purpose \nIn this paper we introduce Kismet, a parallel speedup estimation tool. Kismet performs dynamic program \nanalysis on an unmodi.ed, serial version of a pro\u00adgram to determine the amount of parallelism available \nin eachregion (e.g. loop and function)of theprogram.Kismet then incorporates system constraints to calculate \nan ap\u00adproximate upper bound on the program s attainable parallel speedup. These constraints include the \nnumber of cores, syn\u00adchronization overhead, cache effects, and expressible paral\u00adlelism types (i.e. loop \nand task parallelism for multicore chips; instruction level parallelism for VLIW-style chips; and data \nlevel parallelism for vector machines). Kismet provides a simple usage model in the style of gprof, as \nshown in Figure 1. The program is .rst compiled with a drop-in compiler replacement called kismet-cc. \nThe program is run with a representative input, which produces as a side-effect an output .le containing \npro.le information. The user then runs kismet, which analyzes the output .le and generates a table of \nestimated speedup upper-bounds for a spectrum of core counts. Kismet s Basic Structure In order to estimate \nthe paral\u00adlel performance of a serial program, Kismet uses a paral\u00adlel execution time model.Kismet s \nparallel execution time model is based on the major components that affect parallel performance, including \nthe amount of parallelism available, the serial execution time of the program, parallelization plat\u00adform \noverheads, synchronization and memorysystem effects which contribute in some cases to super-linear speedups. \nTo determine available parallelism, Kismet extends the recently developed hierarchical critical path \nanalysis, or HCPA [16, 21]. HCPA measures the critical path and work across manynested regions of the \nprogram hierarchy[15] in an ef.cient manner, which allows the average parallelism of region (including \nits children) to be determined. The key to leveraging HCPA is the self-parallelism metric [16], which \nprovides a mechanism for separating the parallelism of par\u00adentregionsfrom their children.With this metric, \nwe can as\u00adsign an average parallelism quantity to each region in the program, which quanti.es the potential \nspeedup if only that region were targeted. This is useful, for instance, in deter\u00admining which loop in \na triply-nested loop is most promising for parallelization. HCPA is an extension of critical path analysis \n(CPA) and therefore is able to identify many subtle forms of par\u00adallelism that potentially are available \nonly after signi.cant code refactoring that spans many loops and independent function calls.In caseswhereparallelismisexploitableonly \nafter majorrefactoring across many regionlevels,HCPA re\u00adports this parallelism at the highest level region. \nThis self\u00adparallelism value results in a prediction which is consistent with the majorrefactoring. ThusKismet \ncan postulate trans\u00adformations that greatly exceed the capabilities of today s par\u00adallelizing compilers.Kismet \ns optimistic viewof speedup at\u00adtempts to take into account the programmer s greater ability to perform \ncode transforms that would be unsafe in auto\u00admatic parallelizing compilers. Kismet improves uponHCPAso \nthat it can be used to es\u00adtimate attainable parallel speedups from serial code. In par\u00adticular, it employs \na summarizing variant of HCPA called summarizing hierarchical critical path analysis, or SHCPA, which \nimproves the scalability while maintaining the accu\u00adracy of the analysis by aggregating the data between \nsibling nodes in the region graph in a context-sensitive manner. Compelling Results To demonstrate the \neffectiveness of Kismet in creating realistic upper bounds on parallel per\u00adformance, we evaluated Kismet \nin three contexts. First, we looked at six medium-parallelism benchmarks running on the MIT Raw processor \n[35, 36]. Second, we examined the performance of 6 low-parallelism SpecInt benchmarks rel\u00adative to results \nreported in the literature. Finally, we ex\u00adamined Kismet s accuracy on high-parallelism benchmarks fromtheNASParallelBench[9]ona32 \ncoreAMDsystem. In all contexts, our results show that Kismet is able to cre\u00adate strong approximate upper \nbounds on the actual parallel performance. The remainder of this paper proceeds as follows. Sec\u00adtion \n2 overviews the architecture of the Kismet tool. Sec\u00adtion 3 continues with a description of the SHCPA \nimple\u00admentation, including the extensions that Kismet implements for speedup estimation. Section4 examineshowtheSHCPA \ndata is processed to account for machine and parallelization system properties. Section 5 overviews the \nKismet-based tools we built for the two platforms. Section 6 presents re\u00adsults, Section 7 presents related \nwork, and Section 8 con\u00adcludes.   2. Kismet Overview In this section, we provide a high-level overview \nof the Kismet system architecture, as shown in Figure 2. To use Kismet, programmers need only supply \nthe un\u00admodi.ed serial source code and a sample input data for a program. The output is the upper bounds \non parallel pro\u00adgram speedup as the number of cores is varied, as shown in Figure 1. Internally, Kismet \nalso makes use of a set of in\u00adput .les that describe the targeted machine. Since speedup results are \noften quite machine-dependent, this serves to im\u00adproveKismet s accuracy. Kismet operates in two phases; \nthe .rst phase is a pro.ler that collects the self-parallelism data and the second phase is a speedup \npredictor which applies the machine and system constraints. Self-ParallelismPro.ler Gathering of self-parallelism \nin\u00adformation in Kismet starts with a static instrumentation phase which instruments the target program \nwith code that implements the SHCPA dynamic analysis, and ends with running the instrumented program. \nThe static instrumentation phase transforms the input code to support SHCPA during execution of the sample \nin\u00adput. The inserted instrumentation consists of calls to a spe\u00adcial SHCPA library, which will perform \nthe dynamic analy\u00adsis during execution. In addition to adding instrumentation for calculating critical \npaths,Kismet also inserts instrumen\u00adtation to clearly delineate the regions of the code. Three types \nof regions loops, functions, and sequence are used, allowing SHCPA to calculate each region s self-parallelism \nand to determine the type of parallelism in each region. The Kismet code instrumentator utilizes LLVM \ns [31] static instrumentation infrastructure to perform a deeper level of analysis than is available \nwith dynamic instrumenta\u00adtion tools such asValgrind [41]. This allowsKismet to easily uncover the program \nstructure and account for false depen\u00addencies introduced by loop induction variables and reduc\u00adtion variables. \nStatic instrumentation also provides greater opportunity for optimizing the instrumented code in order \nto reduce the overhead associated with pro.ling. The dynamic analysis phase begins when the instru\u00admented \nbinary is run with the sample input to produce per\u00adregion statistics. For each dynamic region that is \nexecuted, the dynamic analysis computes three key pieces of data: the critical path length, the amount \nof work done, and the self-parallelism. Section 3 describes how this data is pro\u00adduced in more detail. \nThe data produced for each region is relatively small but the number of dynamic regions grows quickly, \nleading to a possibly unmanageable amount of data. Kismet improves the manageability of region data by \nsum\u00admarizing the data as it pro.les, creating summarized region pro.les. The summarized region pro.les \nreduce the number of recorded regions by orders of magnitude, leading to much smaller log sizes and allowing \nfor more ef.cient processing in later stagesofKismet. While thereducedlog sizefrom summarization is desirable, \nsummarization should not com\u00adpromise the qualityof self-parallelism information.Kismet s SHCPA provides \nrich call context-sensitive region informa\u00adtion, helping the speedup predictor not to underestimate the \npotential speedup from parallelization. We ll examine more details about this context-sensitive approach \nin Section 3. Speedup Predictor After running the instrumented binary on the sample input, Kismet has \ncaptured the underlying structure of the application in the form of the summarized region pro.le. The \nnext step is to combine this information with machine and parallelization system properties in order \nto make a prediction. Performance strongly depends on the target system. Kismet accepts a list of target-dependent \nparallelization con\u00adstraints and utilizes this information to provide more accu\u00adratepredictions.Typically \nconstraints includea simplehard\u00adware speci.cation (e.g. the number of available cores), the types of \nexpressible parallelism by that system, and func\u00adtions that quantify parallelization overheads such as \nsyn\u00adchronization. We have found that only a small number of  C 4WPVKOG 4GIKQP 6TGG D 5WOOCTK\\GF 4GIKQP \n2TQHKNG for (i= 1 to N) { foo (1); // callsite A  ORRS ORRS foo (N ); // callsite B } void foo(int \nsize) { for(i= 1 to size) { ... // loop body ... } }  Figure 3: Kismet s Program Representation. (a)Kismet \ns self-parallelism pro.ler builds a hierarchical region structure from source code, consisting of three \ntypes of regions. At runtime, it forms a region tree consisting of dynamic regions. (b) Kismet radically \nreduces the output .le size by compressing runtime region tree into a call context-sensitive region tree. \nThe summarized tree preserves context-sensitive parallelism information, exposing more parallelization \nopportunities. constraints are needed to accurately predict performance. This simpli.estheprocessofextendingKismettonewplat\u00adforms. \nWe were surprised at the ease with which our model could support two very different parallel systems \nan MIT Raw tiled processor and a 32-core AMD multicore system. The speedup predictor contains three sub-components. \nThe .rst sub-component is a modi.ed self-parallelism met\u00adric called expressible self-parallelism, or \nESP, which .lters out parallelism unexpressible by the speci.ed target system. The second sub-component \nis the parallel execution time model. The parallel execution time model allows the speedup predictor \nto estimate the parallel execution time of each pro\u00adgram region and the whole program based on a given \nparal\u00adlelism plan. This model incorporates self-parallelism, num\u00adber of allocated cores, and parallelization \noverhead. Kismet also provides an extended, cache-aware parallel execution time model that considers \nthe impact of caching on parallel execution. The parallel execution time model is used by the resource \nallocator to evaluate between completing parallel\u00adization plans and determines the .nal speedup numbers \nre\u00adportedbyKismet. Section4describes these models further. The .nal sub-component is the speedup planner. \nAn ideal parallel system will take advantage of all the expressible par\u00adallelism in a program. This desirable \nproperty is not avail\u00adable on most existing systems. These systems have other constraints such as limited \nhardware resources, synchro\u00adnization overhead, or poor support for nested parallelism that make expressible \nparallelism not be exploitable paral\u00adlelism. The speedup planner creates a mapping from regions to parallel \nresources, modeling at a high-level what the ex\u00adecution of the parallelized program would look like; \nwe re\u00adfer to this mapping as the parallelization plan.In Section 5, we describe how a parallelization \nplan is created, using two widely different systems as case studies.  3. Self-Parallelism Pro.ler Kismet \nextends the HCPA-based pro.ler introduced in [16] to quantify the parallelism in each region of the program \nin a summarizedfashion. Additionally, it modi.es the algorithm in a number of ways to facilitate speedup \nprediction and calculation of expressible self-parallelism. In this section, we describeKismet s implementation \nof SHCPA and self\u00adparallelism. 3.1 SummarizingHierarchical CriticalPath Analysis Summarizing hierarchical \ncritical path analysis extends tra\u00additional critical path analysis to incorporate the hierarchical region \nstructure of a program. SHCPA calculates the criti\u00adcal path of each dynamic region of the program, unlike \nCPA which looks only at the critical path of the whole program. This per-region calculation provides \nthe basis for improved localization of parallelism information. Types ofRegions Kismet demarcates region \nboundaries at static instrumentation time. Kismet includes all loops and functionsin the listofregionsbut \nintroduces the conceptof a sequence region, an important extension of prior work in HCPA. A sequence \nregion can be any single-entry piece of codebutKismetrestricts sequences to two important cases: loop \nbodies and self-work sequences. Loop body regions form a child region for each iteration of a loop region, \nal\u00adlowing Kismet to identify loop-level parallelism. Self-work sequence regions are sequences of code \nthat are contained in non-leaf regions and do not have any function calls or loops. Theseregions may \nseem unintuitivebut they address a concerninpriorwork onHCPA: the separabilityof differ\u00adent types of \nparallelism. Self-work sequences factor out the instruction level parallelism in regions that would otherwise \ncontain a mix of task-level parallelism (from its other chil\u00addren) and instruction level parallelism. \nFigure 3 (a) shows how regions are dynamically formed from a sample piece of code. SHCPAImplementation \nSHCPA uses shadow memory to track the earliest time that an instruction is available. When each operation \nexecutes, it reads from shadow memory the availability times of all of its dependencies. SHCPA adds the \nmaximum time amongst these dependencies to the la\u00adtency of the operation being performed and then stores \nthis value to the shadow memory location corresponding to the operation. Dependencies that are not true \ndependencies are .ltered out using two main mechanisms. First, Kismet op\u00aderates on LLVM s SSA form IR. \nThis eliminates false out\u00adput (i.e. write-after-write) dependencies. Next, Kismet de\u00adtects induction \nandreductionvariables thenbreaks thefalse dependencies that result from them.  Kismet tracks control \ndependencies through the use of control dependence analysis and a dynamic control depen\u00addence stack. \nControl dependence times are pushed to and popped from the control dependence stack whenever a con\u00adtrol \ndependent region is entered and exited. This stack has monotonically increasing values from the bottom \nto the top, allowingKismet to include only the topmost entryin the list of dependencies for each instruction. \nEach active region effectively has its own shadow mem\u00adory, enablingKismet to independently calculate \ntheregion s critical path length. All times in the region s shadow mem\u00adory are logically initialized \nto zero upon entry so that a ref\u00aderence to an instruction outside the region will be assumed be available \nimmediately at the beginning of the region (i.e. time 0). Regions track the largest time that was written \nto their view of shadow memory; this value is the critical path length. Each region also records the \ncombined latencies of all operations performed in that region; this summation is the amount of work in \nthat region. The ratio of work to crit\u00adical path length is the total parallelism of that region. SHCPA \ncan also recordother useful runtime information. For example, load and store counts are also included \nfor cache-aware performance estimation that will be explained in Section 4. IdentifyingIndependent Children \nKismet also determines if all of the children of a non-leaf region are independent and therefore can \nbe executed in parallel. This information is stored as the region s P bit and is calculated according \nto the following equation: P = CP (parent) == MAX(CP (child1), ..., CP (childn)) where CP (parent) is \nthe critical path length of the parent and CP (childi ) is the critical path length of the ith child. \nIf all children can be executed in parallel, then the length of the critical path will simply be the \nlength of longest critical path of all of the children. In this case, the P bit will be 1.Kismet uses \nthe information in the P bit to help identifyexpressible parallelism. Section4 will describe this in \nmore detail. SHCPAOptimizations Kismet s implementation of shadow memoryis based on a two-level table \nthat equally divides the memory address space. Kismet contains several optimiza\u00adtions of shadow memory \nfor increased performance and re\u00adduced memory usage, most notably shadow register tables Figure 4: Self-Parallelism \nCalculation on Regions with VaryingParallelism.Self-parallelism computes the amount of parallelism in \na parent region that is attributable to that region and not its children. The .gure above shows that \nKismet s self-parallelism calculation successfully quanti.es parallelism across a spectrum of loop types, \nranging from totally serial to partially parallel (DOACROSS) to totally parallel (DOALL). The shaded \nboxes are child regions, cor\u00adresponding to separate iterations of the loops. The relative scheduling \nof child regions is indicated spatially, with time running from left to right. The self-parallelism calculation \ncorrectly quanti.es parallelism in non-loop region hierar\u00adchies as well. Type Serial DOACROSS DOALL CP.(R). \nCP CP CP N.*.CP CP CP CP (N/2).*.CP CP CP CP CP SP.(R) N.*.CP N.*.CP =.1.0 N.*.CP (N/2).*.CP =.2.0 \nN.*.CP CP =.N and dynamic allocation of shadow memory. Shadow register tables optimize the common case \nof writing to local vari\u00adables by creating a direct access shadow register table. This avoids the overhead \nof indirectly accessing the two-level page table. Dynamic allocation of shadow memory reduces theoverhead \nthatwouldresultfromkeeping all shadow en\u00adtries in memory.  3.2 Calculating Self-Parallelism Although \nSHCPAproduces a total parallelism value for each region of the program, this alone is not enough to localize \nparallelism to speci.c regions of the program. Total paral\u00adlelism is computed without knowledge of the \nregion hier\u00adarchy and thus incorporates parallelism that originates from childregions.Kismet s self-parallelism \nmetric takes the next step in localizing parallelism. To determine the self-parallelism of a region R, \nSP (R), Kismet employs the following equation: . P n . cp(child(R, k)) . k=1 R is a non-leaf . . cp(R) \nSP (R)= . . . work(R) . R is a leaf cp(R) Heren is the number of children of R, child(R, k) is the kth \nchild of R, cp(R) is the critical path length, and work(R) is the amount of work in R. In contrast to \nearlierworkon self-parallelism [16],Kismet treats leaf and non-leaf regions differently because they \ncon\u00adtain different types of parallelism. Leaf regions contain no children and therefore will contain \nonly instruction level parallelism (ILP). Non-leaf regions have children that may provide opportunities \nfor either loop-level or task-level par\u00adallelism. Prior work in HCPA [16] employed self-work in the calculation \nof SP for non-leaf regions but this led to a mixture of ILP and task-level parallelism inside of a single \nregion. Kismet includes the new self-work sequence region to factor out the ILP in order to compute expressible \nself\u00adparallelism.  Figure 4 demonstrates the calculation of SP in three non-leaf regions, one totally \nserial, one partially parallel (DOACROSS), and the other totally parallel (DOALL). For simplicity, in \nthe example, each iteration s critical path length cp is the same. For the serial loop, the measured \ncp(R) will be equal to n * cp and the computed self\u00ad n *cp parallelism will be =1, which is expected \nsince serial n*cp dependences prevent overlapped execution of regions. For theDOACROSS loop shown, where \nhalf of an iteration can overlap with the next iteration, cp(R) will be the half of the n*cp cp(R) for \nthe serial loop. Thus SP (R) is =2.For (n/2)*cp the DOALL loop, cp(R) will be equal to cp, so SP (R) \nis n *cp cp = n. Although we show three relatively simple cases here, this method is a good approximation \nof self-parallelism even with more sophisticated child region interaction.  3.3 SummarizingHierarchical \nCriticalPath Analysis The number of dynamic regions quickly grows as nested loops with many iterations \nare executed. This large amount of regions poses practical challenges not only in the size of the pro.le \noutput but also in the runtime of algorithms that need to analyze this data. Garcia et al [16] developed \na dictionary-based compression algorithm to reduce the pro\u00ad.le size.However, this type of compression \nperforms poorly on programs with loop iterations that vary in their work or critical path length. In \nthis section we will describe a new technique that is used by Kismet for managing the number of regions. \nRegion Summarization Kismet combines all dynamic re\u00adgions that have the same region context into a single \nsumma\u00adrizedregion.Figure3depictshowtheruntimeregiontree(a) becomesa summarizedregionpro.le(b).In this \nmethod,all loop iterations collapse to a single node, greatly reducing the number of regions. Each node \ncalculates weighted averages for self-parallelism, work, and other pro.led data across all dynamic regions \ncorresponding to that node. Kismet maintains a current pointer that tracks the sum\u00admary node that corresponds \nto the current dynamic region. When a new region is entered, it updates the current pointer to one of \nits children node based on statically assigned call\u00adsiteIDinformation.Ifthere is no corresponding node, \nit cre\u00adates a new summary node and updates the current pointer. When a region exits, the region s pro.led \ninformation is added to the current node and the pointer returns to the par\u00adFigure 5: ParallelismIdenti.cationLogic.Kismet \nuses the program structure and parallelism information provided by SHCPA to help classify parallelism. \nThis .gure shows the simple classi.cation process. Kismet then uses the classi\u00ad.cation result to calculate \nthe expressible self-parallelism (ESP). ESP quanti.es the amount of expressible parallelism within a \nspeci.c region of the program. ent node. This process is similar to the call context tree de\u00adscribedin[5]but \nmodi.edforKismet sregion hierarchy. Utilizing Context Sensitivity The example summarized re\u00adgion pro.le \nshown in Figure 3(b) contains two nodes for the same function(foo)from what appears to be the same con\u00adtext. \nThis corresponds to two separate calls from the same loop. While this increases the number of nodes in \nthe sum\u00admarized pro.le, it allowsKismet to uncover new parallelism opportunities. To understand the merit \nof context-sensitive representa\u00adtion, consider the code in Figure 3. When the loop in func\u00adtion foo is \nparallel and N is large, the parallelism of this loop signi.cantly differs between callsitesA andB. Callsite \nA s loop will always have a self-parallelism of 1, providing no bene.t to parallelism and likely causing \nslowdown due to synchronization overhead. Callsite B s loop will have a self-parallelism of N and would \nlikely be a good candidate for parallel refactoring. Kismet can capitalize on the split contexts, incorporating \nthe speedup from callsite B into its estimates while ignoring callsite A.  4. Speedup Predictor Kismet \ns speedup predictor attempts to .nd the upper bound on parallel speedup of a program by examining a spec\u00adtrum \nof candidate parallelizations of the program on the target machine.Kismet s self-parallelismpro.lingprovides \nthe groundwork for calculating this speedup but it alone is not enough to determine a tight bound on \nspeedup. In this section we will describe how Kismet processes the self\u00adparallelism data to predict the \nmaximum parallel speedup. 4.1 Expressible Self-Parallelism (ESP) While Kismet s self-parallelism pro.le \nquanti.es the paral\u00adlelism in each region of the program, there is no guaran\u00adtee that the parallelism \nwill be expressible. Many systems have limitations on the type of parallelism that can effec\u00adtively be \nexpressed.Kismet transforms self-parallelism into expressible self-parallelism (ESP)in two steps. First, \nit clas\u00adsi.es the type of parallelism found in each region. Sec\u00adond, it uses this classi.cation to conditionally \nadjust self\u00adparallelism into ESP, as follows. Regions that have self\u00adparallelism that is unexpressible \nare assigned an ESP of 1. Regions with self-parallelism that is expressible have an ESP that is equivalent \nto their SP.  Figure5 illustrates theKismet s decision process when classifying parallelism. As described \nin Section 3,Kismet s region hierarchy has been designed to ensure that only leaf regionshave instructionlevel \nparallelism(ILP)and thatILP is foundonlyinleafregions.The.rststepinFigure5isthus to checkif theregionisa \nleaf.Iftheregionis nota leaf then the parallelism is either of the form of loop-or task-level parallelism. \nKismet checks the region type to determine if there is a loop or a function. Kismet further classi.es \nloop parallelism based on whether there are cross-iterations dependencies. Loops without cross\u00aditeration \ndependencies are classi.ed asDOALL while those with cross-iteration dependencies are classi.ed asDOACROSS. \nWhileKismet spro.le output does not contain statistics on the number of cross iteration dependencies, \nit does contain the information needed to quickly distinguish DOALL and DOACROSS loops. Namely, the P \nbit described in Sec\u00adtion 3 indicates if all iterations are independent. Kismet ex\u00adamines the P bit for \nthe region, classifying the region as DOALL if P == 1 andDOACROSS otherwise. As with any dynamic analysis \ntool,Kismet s identi.ca\u00adtion of parallelism is subject to differences across multiple inputs. In practice \nwe have found that while the amount of speedup mayvaryslightly across multiple inputs, theKismet classi.cation \nis consistent across these same inputs.  4.2 Parallel ExecutionTimeModel Although self-parallelism is \na major factor that affects the realizable speedup of a region, there are other major fac\u00adtors such as \nallocated core counts and parallelization over\u00adhead.Kismet usesa parallelexecution time model that cap\u00adtures \nmajor factors that affect parallel execution time. With the parallelexecution time model,Kismet s speeduppredic\u00adtor \ncan evaluate the effectiveness of parallelization plan it produces, and reports the plan that would bring \nthe high\u00adest speedup.We also showa cache-aware parallelexecution time model that incorporates changed \ncache miss rates after parallelization. Base Model The base parallel execution time model in\u00adcorporates \nregion structure, core count, and parallelization overhead in addition to self-parallelism. This model \nuses the following equation to determine the execution time of region R: .In ET (child(R, k)) . k=1 . \n+ O(R) non-leaf . . min(SP (R),A(R)) ET (R)= . . . work(R) . + O(R) leaf min(SP (R),A(R)) While there \nare different equations for leaf and non-leaf regions, they follow the same general model. The .rst term \nrepresents the time needed to execute the parallelized, as\u00adsuming that A(R) cores are allocated to that \nregion. The top of the fraction represents the serialized execution time the work of a leaf region, or \nthe sum of the children s work of a non-leaf region. This time is divided by the minimum of the self-parallelism \nof the region(SP (R))and A(R). Intu\u00aditively, this means that the speedup is either fundamentally limited \nby the parallelism available when SP (R) is the limitingfactor orby the amountof parallelresources allo\u00adcated \nto the region when A(R) is the limitingfactor. Note that the execution time of the non-leaf regions depend \non the execution time of their children; this forces a bottom-up approach to calculating the execution \ntime of the program. The second term, O(R) models target-dependent parallel\u00adizationoverhead.Parallelexecution \ntypicallyinvolvesover\u00adhead from several sources: thread management, synchro\u00adnization, communication, \netc..Asaresult, theoverheadfac\u00adtor is highly target dependent. For example, the synchro\u00adnization operation \ntakes less than20cyclesin theMITRaw processorbut takes several thousandcycles on shared mem\u00adory multicore \nprocessors. As such, Kismet allows target\u00addependent customization of O(R) by accepting paralleli\u00adzation \nconstraints. This overhead function directly impacts the parallelization granularity as the amount of \nwork in a region should offset parallelization overhead for a pro.table parallelization. Cache-Aware \nModel While the base model is able to ac\u00adcurately model benchmarks that have up to linear speedup, our \nresults showed that some benchmarks resulted in super\u00adlinear speedup when parallelized. For example, \nthe cg benchmark from the NAS Parallel Bench [9] showed sig\u00adni.cant super-linear speedup when using between \n4 and 16 cores on 32-core AMD Opteron system. We found that this was a result of increasing cache size \nwith a larger number of cores on this system, prompting us to include a cache-aware model of parallel \nexecution time. Kismet s cache-aware model extends the base model by including the memory service time \n(MST) in the calculation of ET (R). MST represents time spent in memory accesses that resulted in a cache \nmiss; it is calculated using the fol\u00adlowing equation:  .In MST (child(R, k)) . . k=1 non-leaf . . A(R) \n. MST (R) = . . . . . Idepth i=1 CMTi(R) leaf A(R) For both leaf and non-leaf equations,MST sums the \ntime spent and for cache misses either in that region for a leaf region, or among all children of a non-leaf \nregion in the level i cache, CMTi , and divides this by the number of cores allocated to the region, \nA(R). This optimistically assumes that the memory system of the target is scalable, distribut\u00ading memory \naccesses evenly across cores so that they may be simultaneously serviced without penalty. Although it \nis possible to model more complicated behaviors of memory systems, this simple cache model appears to \ndo a reason\u00adable job of predicting superlinear speedup effects due to the memorysystems. To calculate \nthe cache miss time at level i, Kismet uses the following equation: n CMTi(R)= MemCnt(R)*Missi(R, conf)*P \nenaltyi i=1 where MemCnt(R) is the number of memory accesses in region R, Missi(R, conf) is the cache \nmiss rate for level i, conf is a speci.c memory con.guration, and P enaltyi represents the penalty for \na level i cache miss. As more cores are allocated, the total cache size of conf increases, potentially \nleading to a decrease in Missi (R, conf).  5. Case Studies -Raw andMulticore In this section, we demonstrate \nhow Kismet can be con\u00ad.gured to a speci.c platform by examining two very dif\u00adferent platforms: the MIT \nRaw tiled multicore processor ( Raw ) [14, 35, 36, 48] and a conventional multicore pro\u00adcessor ( Multicore \n). Table 1 shows details of these two targets. We also model speci.c software platform because software \nplatforms also create constraints in parallelization, affecting the speedup even on the same hardware. \nSpecif\u00adically, we model the automatically parallelizing compiler RawCC [4, 32] forRaw, and manual OpenMPparallelization \nforMulticore.For each platform, we .rst introduce hardware characteristics and parallelization constraints, \nand describe how we model parallelization overhead in parallel execu\u00adtion time model, and then .nally \ndescribe the target-speci.c planning algorithm. 5.1 TargetingRaw inKismet Platform Description MIT Raw \nis an early tiled multi\u00adcore processor [17, 46, 49] featuring a fast, 1-cycle per hop, .ne-grained scalar \noperand network [50]. Although Table 1: Overview ofTwo Platforms -Raw andMulticore. These two targets \nhave different constraints in paralleli\u00adzation, expressible parallelism, and parallelization overhead. \nPlatform Core Type L1 Size L2 Size L3 Size Raw Modi.ed MIPS 32KB /Core -- Multicore AMD Opteron 64KB \n/Core 512KB /Core 6MB /Four Cores SW Platform RawCC OpenMP Expressible Parallelism ILP DOALL Non Reduction \nParallelization Overhead (cycles) 2 + 2 v N 250 * N Reduction Parallelization Overhead (cycles) 2 + 2 \nv N 500 * N manydifferent formsofparallelism(ILP, TLP,DLP, etc) are expressible on the Raw ISA, we model \nthe RawCC [4, 32] parallel compiler, for which onlyILPisexpressible. RawCC .nds ILP in each basic block \nand performs space-time scheduling toexploit it.For each instruction, the space-time scheduling determines \nwhich core executes the instruction for minimum totalexecution time.Inter-core data dependencies are \nresolved utilizing Raw s low latency net\u00adwork, and control .ow information is broadcast across cores \nto ensure all cores execute the same basic block. If needed, RawCC performs loop unrolling to increase \nthe amount of exploitableILPina loop. Modeling Parallelization Overhead Two sources can in\u00adcur parallelization \noverhead in Raw: control dependencies and data dependencies. To ensure control dependencies are respected, \nRawCC broadcasts the control dependency information to all cores via Raw s static network. At the end \nof every basic block, each core waits for the control dependence information and branches to the speci.ed \nbasic block when the information v arrives. The broadcast cost is 2+2 N, where N is the num\u00adber of cores. \nIn our parallel execution time model for Raw, we approximate this overhead based on [50]: an injection \nla\u00ad v tencyof2 cycles,a networkdiameterof 2 N, and a per-hop latencyof1 cycle. Data dependences between \ntwo instructions on differ\u00adent cores also incur communication overhead. Unlike with broadcasts, the cost \ncan be hidden if the communication is not on the critical path of the execution by RawCC. As Kismet aims \nto bound the achievable highest speedup, Kismet does not model this overhead. Planner Algorithm The planner \nalgorithm takes as input the summarized region pro.le which includes a region tree where each node is \na summarized region and each edge rep\u00adresents reachable relationship between them. As RawCC can express \nonly ILP in a program, the planner .rst .lters nodes with non-ILP parallelism and sets their ESP value \nto 1, effectively eliminating them from consideration. After ILP regions are identi.ed, producing the \nplan with highest speedup is straightforward. For each ILP region R, decide A(R)that minimizes ET(R)with \nthe given parallel execution time model.For nonILPregions,A(R)is simply setto one, representing serial \nexecution. When A(R)is determined, par\u00adallel execution time model calculates the estimated parallel execution \ntime of the root node with given A(R) function, and will compute the speedup against serial execution \ntime.  5.2 TargetingMulticore with OpenMP inKismet Platform Description The multicore platform represents \nconventional multicore processor systems such as Intel s Nehalem or AMD s Opteron line or processors. \nThey use shared memory for inter-core communication and as a re\u00adsult the latency is signi.cantly higher \ncompared to Raw s low-latency networks. For the software platform, we target the popular OpenMP platform \nthatexploits mainlyDOALL parallelism. In addition to the restricting expressible paral\u00adlelism toDOALL, \nwe also disallow nested parallelization although OpenMP supports nested parallelization, the fea\u00adture \nis rarely used in practice as synchronization overhead is typically too large. Modeling Parallelization \nOverhead OpenMP paralleli\u00adzation involves overhead in several aspects: thread creation, thread scheduling,reduction \noperations, and barrier cost.We found that thread creation cost is typically amortized with a thread \npool implementation and scheduling cost is negligible when static schedulingis used.We modelbarrierandreduc\u00adtion \ncosts since they signi.cantly impact performance. The values choseninTable1 was takenfromrunning theEPCC \nmicro-benchmark [11] on 32-core AMD Opteron machine. Planner Algorithm Once regions with unexpressible \npar\u00adallelism are .ltered out, the main constraint in theMulticore planner is prohibited nested parallelization. \nWhen nested parallelization is disallowed, the planner cannot choose more than one region among regions \nin the path from the root node to anynode in summarized region pro.le. To .nd the optimal solution with \nthe constraint, Kismet uses a dynamic programming algorithm. The core intuition of the algorithm is that \na region should be parallelized only when the bene.t of parallelization is greater than the bene.t from \nparallelizing any set of descendant regions. The plan\u00adnertraversestheregionpro.leina bottom-upfashion,from \nleaf nodes up to the root node, while saving the optimal plan P(R) at each region R. When the planner \nprocesses a new region, it compares the expected bene.t of parallelizing the region against the cumulative \nbene.t of the optimal plans of its childregions.Ifthe bene.tof parallelizingR exceeds the cumulative \nbene.t of child regions, P(R) is set to R; other\u00adwiseP(R)issettothe unionof childregions optimalplans. \n 5.3 Kismet Usage In this case study, we also address four commonly asked usability issues about theKismet \ntool. How Sensitive Is Kismet to Changing Inputs? Since Kismet s analysis is dynamic, it can take advantage \nof in\u00adformation that can only be extracted by observing the run\u00adtime execution of the program. This allows \nKismet to .nd opportunities for speedup that would be undiscovered by the more conservative analyses \nfound in parallelizing compil\u00aders. The sensitivity of a potential speedup of a program to the input varies \nby the underlying algorithms in the program. Although Kismet could mirror parallelizing compilers and \nprovide more worst-case speedup estimates, this fails to expose the opportunities that might be available \nin taking advantage of input-dependent parallelization strategies. As a result, our recommended usage \nmodel is that the user run Kismet on the application multiple times, across a spectrum ofrepresentative \ninputs,inordertogaina deeperknowledge of this issue. What Tasks are Performed by the User from Program \nto Program? Our expectation is that the maintainer of Kismetwould ship Kismet witha libraryofrepresentative \nmachine models and planners. When the user runs Kismet, they would select via commandline parameter the \nmachine model which most closely matches the target architecture. Thus, from the user s perspective, \nthe tool is push-button. Although this is clearly future work, we have also envi\u00adsioned the possibility \nof using auto-tuner techniques (i.e. as in FFTW) to automatically calibrate these components to a new \narchitecture, which alleviates the Kismet maintainers of the need to update the library. Finally, as \nlast resort, the user could extend the machine model and planner library themselves. What is Kismet s \nUtility in Providing Refactoring Assis\u00adtance? To be clear, Kismet does not try to make speci.c recommendations \nabout how the programmer should refac\u00adtor the program. Rather, it provides advanced information that \nhelps the programmer decide a) whether it may not be worth the effort to parallelize the piece of code \nand b) what kind of speedup might be reasonable to aim for. The latter item may also in.uence the programmer \ns choice of transfor\u00admations,but only in an indirectfashion. AlthoughKismet s speedup upperbounds are \nindeed approximate, our results show that they are rarely exceeded by actual parallelized code. A consistently \nlow estimated speedup upperbound is a strong signal to the user that attaining speedup of the ex\u00adisting \nserial program is likely to be verychallenging. What is Kismet s Bene.t Over Parallelizing Compilers? \nKismet derives its key advantages over parallelizing com\u00adpilers through an extension of CPA, which is \na dynamic analysis not commonly used in today s parallelizing compil\u00aders. Speedup estimates provided \nby Kismet are likely to be higher than those attainable by a parallelizing compiler, be\u00adcause they are \ndetermined by empirical measurements about program parallelism rather than the ability of an automatic \ntool to prove properties about the program. Kismet s opti\u00admistic view of speedup attempts to take into \naccount the pro\u00adgrammer s greater ability to perform code transforms that would be unsafe in automatic \nparallelizing compilers.   6. ExperimentalResults This sectionevaluatesKismet as follows.We .rst outline \nour evaluation methodology, including our selection of bench\u00admarks and target machines. Using this methodology, \nwe then quantifyKismet s accuracyby comparing bothpredicted and measured speedups from parallelization \nof three benchmark suites on three machine classes. Finally, we analyze the im\u00adpactof novel techniques \nfeaturedinKismet:expressible self\u00adparallelism, cache-aware prediction, and summarization. 6.1 Methodology \nKismet s goal is to provide realistic upper bounds on the parallel performance of serial programs. Our \nresults will therefore focus on examining the tightness of these upper bounds on a wide range of benchmarks \non several different platforms, both real and theoretical. In our evaluation, we worked hard to address \nthreats to validity by evaluating Kismet s performance across three very different architectures and \nby comparing against third\u00adparty parallelized codes from three benchmark suites, in\u00adcluding both low \nand high parallelism applications. We selected benchmarks using two primarycriteria. First, the set of \nbenchmarks needed to display a range of par\u00adallelism: from super-linear speedup down to very limited \nspeedup. Second, the benchmarks needed to have either 1) a parallel implementation that could be used \nto gather real results or 2) published performance results from a variety of sources. Programs that are \nhighly parallel tend to have a parallel implementation available while those with low amount of parallelism \ntend not to have parallel implemen\u00adtations available, possibly for reasons of vanity. The selected benchmarks \ncame from three benchmark suites, each targetinga different platform.Here weoverview these suites, describing \nthe amount and types of parallelism available and describing the steps necessary to obtain our results. \nRaw. We modeledRawCC sILPexploitation onRaw as described in Section 5.Kismet s estimates are compared \nagainst speedup numbers reported in [35]. These bench\u00admarks range from non-scalable to scalable. As mentioned \nbefore, RawCC utilizes loop unrolling to increase the amount ofILP. Unrolling also enables serial optimizations \nsuch as constant propagation and common sub-expression elimination. To control for these factors during \npro.ling, Kismet uses LLVM to unroll the loops before static instrumentation.  SpecInt2000. SpecInt2000 \nbenchmarks are widely known to have extremely limited parallelism. Luckily, a wide range of proposed \nparallelization systems especially those using speculative parallelization have attempted to parallelize \nthese benchmarks, providing a fertile source of published results. We chose to examine the bench\u00admarks \nfrom this suite that have most frequently been the target of parallelization, namely bzip2, gzip, mcf, \ntwolf, and vpr.  In general, these benchmarks are hard to parallelize due to complex dependence patterns \nin DOACROSS loops. The speedup numbers reported in literature typically re\u00adquired heroic code transformations, \nand often involved special speculative hardware support or simulation-only experiments [24, 43, 44, 59, \n60].To approximate the ma\u00adchine models in those aggressive scenarios, we modi\u00ad.ed theMulticore-OpenMPmodel \ndescribed in Section5 so that it allows the exploitation of both DOALL and DOACROSS with zero parallelization \noverhead. Even with these permissive settings, Kismet is able to create strong bounds.  NASParallel \nBench (NPB). In contrast to SpecInt2000, NPB [9] generally consists of benchmarks with large amountsof \neasy-to-exploit parallelism.We use theMulticore-OpenMP predictor targeting only DOALL parallelism with \nparameters for a 64-core system. We measured speedup with third-party parallelized version [2] of NPB, \nrunning these parallel versions on the 32-core AMD sys\u00adtem described in Table 1. For all NPB benchmarks, \nwe used the A input data set during both pro.ling and exe\u00adcution of the parallel versions.  What are \nCorrect SpeedupPredictions? In our evalua\u00adtion, we employbenchmarks that were parallelized by third\u00adparty \nexperts. To the extent that the benchmarks have been widely used in the research community, we have a \nreason\u00adable expectation that these parallelization efforts are not too far off from optimal. To us, correctly \npredict means 1) that the actual speedup did not exceed the predicted speedup upperbound (i.e.,Kismet \nsresults correspond to actual em\u00adpirical upperbounds) and 2) that the speedup experienced is close toKismet \nspredictions (i.e.Kismetprovides rela\u00adtive tight bounds.)To theextent thatKismet s bounds are not tight, \nit could be either due to insuf.cient modeling of machine constraints, or that there is remaining attainable \nspeedup in the application.  6.2 PredictionResults Raw Figure 6 shows predicted and measured speedup \non RAW. In all benchmarks, Kismet correctly predicts the speedup trend in both high parallelism benchmarks \n[8]  16   Program Speedup Program Speedup Program Speedup Program Speedup Program Speedup Program \nSpeedup 2 8 4 2 2 1 1 1 Core Count Core Count Core Count 64 64 64  32 32 16 8 4 2 32 16 8 4 2 16 \n8 4 2 1 1 1 Core Count Core Count Figure 6: Predicted andMeasured SpeedupforRAW Benchmarks onRAW hardware. \nKismet models theMITRaw pro-cessorandRawCC,targetingtheexploitationofILP.Fromlow-to high-parallelism \nbenchmarks,Kismetprovidedappropriate upper bounds. This successful speedupprediction resultsfromKismet \ns abilityto isolateILPfrom otherformsofparallelism based on summarizing hierarchical critical path analysis. \n8 8 8   Program Speedup 4 2 1 4 4 2 2 1 1 Core Count Core Count 8 8  Program Speedup 4 2 1 4 2 1 \nCore Count Figure 7: Predicted and Reported Speedup in Low-Parallelism SpecInt2000 Benchmarks using \nthird-party published results. Kismet correctly captures the low parallelism in SpecInt2000 benchmarks, \nproviding tight speedup upper bounds.Re\u00adported speedup numbers are from multiple sources that applied \naggressive hardware/software techniques to extract parallelism from these benchmarks[24,43,44,59,60].To \nmodel thoseexperimental systems,Kismetis con.guredtoexploit loop-level parallelism(DOALL andDOACROSS) \nwith zerooverhead.Kismet sexpressible self-parallelism (ESP)and parallelexecution time model enables \nsimilar modeling for a wide range of systems. 16 16 16 8   Program Speedup Program Speedup Program \nSpeedup Program Speedup Program Speedup Program Speedup 2 4 2 2 1 1 1 Core Count Core Count Core Count \n 64 64 32 16 8 4 2 64  32 16 8 4 2 32 16 8 4 2 1 1 1 Core Count Core Count 64 64  32 32 Program Speedup \n16 16 8 8 4 4 2 2 1 1 Core Count Core Count Figure 8: Estimated and Measured Speedup of NAS Parallel \nBench on 32-core AMD Multi-core System. The NAS benchmarks are generally much higher parallelism than \nthe other benchmarks considered in the paper. Kismet s cache\u00adaware prediction is able to bound the speedup \nof the benchmarks reasonably well, including for the cg benchmark which observed superlinear speedup \ndue to cache effects. The limited scalability of memorysystem becomes the bottleneck in several benchmarks \nwhen using 16+ cores. (jacobi, life) and low parallelism benchmarks (aes, fpppp, sha, unstruct). Super-linear \nspeedup is predicted and measured in both jacobi and life but only the former had actual super-linear \nspeedup. These benchmarks consist mainly of DOALL loops, allowing unrolling to linearly increase the \namount ofILP.In contrast, unstruct also bene.ts from unrolling and serial optimizations,but its loops \nareDOACROSS, limiting unrolling seffects and limiting scalability.For theremaining benchmarks, aes, fpppp, \nand sha, unrolling was ineffective as the parallelized regions were functions rather than loops. Kismet \ncorrectly bounded the speedup for all benchmarks except jacobi, which slightly outperformed Kismet s \nesti\u00admates. This anomaly can be attributed to the fact that in\u00adcluding more cores from the Raw processor \nincreases the number of registers, leading to decreased memory system delays; Kismet did not incorporate \nthis effect into its basic estimation model as its effect is generally negligible. SpecInt2000 Figure7 \nshowsKismet s speedup estimates and speedup numbersgatheredfrom third-partyeffortsrun\u00adning on aggressive \nhypothetical hardware [24, 43, 44, 59, 60]. These results con.rm the generally-held belief that SpecInt \nbenchmarks are fundamentally limited in their par\u00adallelism. Kismet predicted low speedups, plateauing \nat a speedup of 2 to 4 for all benchmarks except mcf. The re\u00adportedresults conform toKismet s upper bounds. \nNAS Parallel Bench (NPB) Figure 8 shows predicted and measured speedups for the benchmarks in NPB. As \nexpected, basedontheabundant, easily-exploitableDOALL parallelism of these benchmarks Kismet estimated \nrela\u00adtively high speedups in all benchmarks except is. The lower amount of speedup in is results from \nit having only a limited amount of execution spent in parallel regions. For ep and lu, measured speedup \nwas very close to pre\u00addicted speedup. Even though the communication cost on multicore processors typically \nlimit the scalability of bench\u00ad  Benchmark Estimated Speedup Suite Name Without ESP With ESP Ratio RAW \njacobi 8649 53.81 160.7X life 26840 153.73 174.6X sha 4.81 4.71 1.0X fpppp 1190 98.74 12.1X aes 39547 \n150.95 262.0X unstruct 4416 8.22 537.2X SpecInt2000 bzip2 17.4 3.39 5.1X gzip 4.27 1.37 3.1X mcf 67.12 \n5.92 11.3X twolf 11.35 1.68 6.8X vpr 15.77 3.1 5.1X NPB bt 161650 64.46 2507.8X cg 275 171.06 1.6X ep \n93.69 38.67 2.4X ft 10709 151.92 70.5X is 565 37.53 15.1X lu 43845 52.98 827.6X mg 2478 87.35 28.4X sp \n147873 65.18 2268.7X Total mean 23592 geomean 878 61 25 363.2X 34.5X Program Speedup 64 32 16 8 4 2 \n1 1 2 Core Count 4 8 16 32 64 100 80 % of Exec Time 60 40 20 0 Core Count 1 2 4 8 16 cg Memory Cache \nComputation 32 64 Table 2: Estimated Speedup with and without Express\u00adible Self-Parallelism. ESP helps \nthe tightening of speedup estimates by providing only expressible parallelism to Kismet.In these benchmarks, \nESP successfully reduced the speedup estimates by 363.2X, showing that it is indeed a central component \nin speedup estimation. marks, these benchmarks speedup continued to scale as they do not rely on inter-core \ncommunication. cg is an interesting benchmark that exhibits super-linear speedup in both predicted and \nmeasured speedup, thanks to Kismet s cache-aware performance model.We willexamine cg in more detail later \nin the results section. mg and sp scale up to 8 cores, but their speedup starts to decrease from that \npoint. The drop in performance can be attributed to shared-memory related overhead that is not capturedbyKismet \ns parallel execution time model. These benchmarks share data across cores and a data location is written \nby multiple cores, greatly increasing the sharing overhead. The gap between predicted and measured per\u00adformance \nin these benchmarks might be closed when in\u00adnovations in parallel computer architecture reduce the cost \nof shared-memory based communication. Alternately, more advanced modeling of coherence traf.c in Kismet \ncould be of assistance. Figure 9: Impact of Cache-aware Estimation incg Bench\u00admark. The baseline estimation \nfails to predict the super\u00adlinear speedup of cg. By incorporating potentially reduced cache miss rates \nin a parallel execution, cache-aware esti\u00admation successfully predicts the super-linear speedup. Exe\u00adcution \ntime breakdown clearly shows the time spent in cache and memoryis considerably reduced from two-core \nto four\u00adcore execution.  6.3 Impact of Expressible Self-Parallelism (ESP) OneofHCPA s majoradvantagesovertraditionalCPAisits \nability to localize parallelism using the self-parallelism met\u00adric. Kismet further improves the utility \nof self-parallelism by introducing the concept of expressible self-parallelism (ESP), a .ltering step \nthat removes self-parallelism that is unexpressible by the target system. To quantify the impact of ESP, \nwe compared the estimated speedup with and with\u00adout ESP in all benchmarks. We assumed zero overhead and \nin.nite cores in the speedup estimation, in order to isolate the impact from ESP from other speedup limitingfactors. \nTable 2 shows the estimated speedup number with and without ESP. By honoring only unexpressible parallelism, \nKismet tightens the speedup upper bound by up to 2508X, with an average reduction in speedup of 363.2X. \nThe results con.rm that ESP is an essential part in speedup estimation system.  6.4 Impact of Cache-aware \nSpeedup Estimation Cache-aware time estimation model incorporates potentially reduced cache service time \ncaused by increased cache sizes when additional cores are used in execution. The top part of Figure 9 \ndemonstrates the effectiveness of cache-aware estimation shown on the cg benchmark. Without cache\u00adawareness \nKismet predicts linear speedup, but measured speedup exhibits super-linear speedup. In cache-aware pre\u00addiction, \nKismet incorporates varying cache miss rates gath\u00adered from Cachegrind [41] for each core con.guration, \ncor\u00adrectly predicting super-linear speedup of cg. The lower part of Figure 9 shows the breakdown of exe\u00adcution \ntime on different number of cores. As the core count switches from one to two and from two to four, the \nportion of cache and memory service time is signi.cantly reduced. When the cache miss rate does not change, \nthe portion for cache and memoryshouldremain the same.Indeed, switch\u00adingfrom1 to4 cores,L1 cache missratedropsfrom \n23.3% to 6.5%, and the last level cache miss rate drops from 6% to 0.1%. 6.5 Effectiveness of the SummarizationTechnique \nTo examine the effectiveness of Kismet s summarization technique, we ran NPB and SpecInt2000 benchmarks \n1 with two different input sizes ( S and A for NPB, test and ref for SpecInt2000) andexamined dynamicregion \ncounts as well as output .le sizes. Figure3 shows theresults. Theresults show thatKismet s summarization \ntechnique scales well with increasing input sizes and is effective at re\u00adducing the output .le size. \nAs expected, the dynamic region count signi.cantly increases when we switch from small in\u00adput to larger \ninput 463X onaverage.With the larger input sets, dynamic region pro.le data runs as large as several \nter\u00adabytes, clearly too large to be conveniently stored to disk. With SHCPA, there is virtually no difference \nin the output .le size between small and large input sets. Moreover, the summarization technique results \nin very modest .le sizes only 85KB on average.  7. RelatedWork This section examines Kismet srelated \nwork according to four themes: parallelism pro.ling, performance prediction, parallel performance debugging, \nand optimizations for re\u00adducing memory and execution overheads of dynamic pro\u00adgram analyses. Parallelism \nPro.ling Approaches for parallelism-related pro.ling have generally fallen into two categories: critical \npath analysis and dependence testing. Critical path analysis (CPA) dates back several decades, with early \nimportantworks includingKumar and Austin [7, 28]. CPAapproaches seek to measure the number of concur\u00adrent \noperations at each time step along the critical path of 1Raw benchmarks have only a single input set. \nBench Dynamic Region Count (Mega Regions) Output File Size (Kilo Byte) Input S L Ratio S L Ratio bt 4 \n2665 666\u00d7 102 102 1.0\u00d7 cg 38 830 22\u00d7 15 15 1.0\u00d7 ep 50 805 16\u00d7 4 4 1.0\u00d7 ft 40 1526 38\u00d7 50 50 1.0\u00d7 is 0.7 \n104 149\u00d7 3 3 1.0\u00d7 lu 2 2208 1104\u00d7 45 45 1.0\u00d7 mg 2 969 485\u00d7 79 79 1.0\u00d7 sp 10 7452 745\u00d7 166 167 1.0\u00d7 bzip2 \n846 4086 5\u00d7 62 63 1.0\u00d7 gzip 141 4477 32\u00d7 96 137 1.4\u00d7 mcf 7.8 4758 595\u00d7 19 20 1.1\u00d7 twolf 11.4 23023 2093\u00d7 \n260 309 1.2\u00d7 vpr 42.1 3020 72\u00d7 104 107 1.0\u00d7 mean 92 4302 463\u00d7 77 85 1.1\u00d7 Table 3:Impact of SummarizationTechnique \non File Size in NPB. Switching from the small (S) to large (L) inputs causes 463\u00d7 more dynamic regions \nto execute on average, but the output .le size increases only 1.1\u00d7 on average, from 77KB to85KB. Thus,the \nsummarization techniqueisvery effective in keeping output .le size manageable even with large inputs. \nthe program.In contrast to these approaches,Kismet s hier\u00adarchical critical path analysis is able to \nlocalize parallelism within nested program regions, and provide concrete guid\u00adance on which program regions \nto target. Recently, Kulka\u00adrni et al [27] used a critical path based analysis to bring insight into the \nparallelism inherent in the execution of ir\u00adregular algorithms. In contrast to Kismet s focus on esti\u00admating \nspeedupin concrete coderegions viaHCPA,Kulka\u00adrni s approach attempts to transcend the details of the \nim\u00adplementation and to quantify the amount of latent paral\u00adlelism in irregular programs that exhibit \namorphous data par\u00adallelism. Other works have used CPA to perform limit stud\u00adies for processors that \ntarget instruction-level parallelism (ILP)[29, 52]. Dependence testing is another parallelism pro.ling \nap\u00adproach that strives to uncover the dependencies between different regions in the program. pp [30] \nis an early im\u00adportant work that proposed hierarchical dependence test\u00ading to estimate the parallelism \nin loop nests. Similar tech\u00adniques are used in Alchemist [54] and Prospector [25]. Al\u00adthough dependence \ntesting and Kismet s HCPA share simi\u00adlar goals,HCPAfocuses on localizing and quantifying paral\u00adlelism \nacross many different, nested program regions rather than establishing independence of pre-existing regions. \nAs a result, it can identify more nuanced forms of parallelism even if signi.cant code transformation \nwould be required to exploit it.Dependence testing is generally more pessimistic and sensitive to existing \nprogram structure.  Performance Prediction CilkView [18] and Intel Parallel Advisor s SuitabilityTool[1]arerecent \ntools whose motiva\u00adtion is similar toKismet. LikeKismet, theyalso predict par\u00adallel performance on a \ntarget with arbitrarynumber of cores. UnlikeKismet,however, CilkView andParallel Advisorrely on the user \ns parallelized code or annotations to predict speedup. Kismet minimizes user s efforts in prediction \nby automatically detecting parallelism in the serial program. Simulation has been used to predict the \nperformance of processors and systems that are still in development.In this case, a parallel version \nof the program exists, but the ma\u00adchine itself is not available to run it. ManySim [56] is one such simulator \nthat was designed to evaluate the perfor\u00admance potential and scalability of large-scale multicore pro\u00adcessors. \nGEMS[37] is a full-system functional simulator for multiprocessors. It separates the simulation from \nthe tim\u00ading models, allowing them build a detailed memory system timing simulator rather than focus on \nbasic functional sim-ulation.However, simulators stillrequire code that has been parallelized for these \nsystems, unlikeKismet. A number of works have looked at the limits of paral\u00adlelism and their impact on \nperformance. Theobald et al [51] examined the smoothability of a program s parallelism, i.e. the ability \nto which a program s parallelism could be equally spread throughout the program s entire execution to \nensure high utilization on a constrained multiprocessor. Rauchwerger et al [45] also looked at the ability \nto map ideal parallelism to a constrained processor, introducing the concept of slack to describe the \nability of parallelism to be pushed to later parts of the program.Kismet improves upon theseworksby usingHCPA \ns ability to localize parallelism; Kismet can examine the effect of parallelizing speci.c re\u00adgionsof \ntheprogramin ordertogaina better estimateof the program s parallel performance. There have been several \nefforts to predict serial perfor\u00admance [20, 23, 34, 42].In theory, these predictions could be combined \nwithKismet s speedup predictions to predict the parallel execution time of a program. Several works have \nlooked at predicting the scalability of parallel programs based on their performance on a small number \nof processors [10, 53]. Barnes et al [10] looked at several techniques for extrapolating performance \nof MPI programs, including one that measured the global critical path. Zhai et al [53] avoid performance \nextrapolation to predict performance; instead, they use deterministic replay to measure sequential time \nof each process using only a single node. Again, these systems differ from Kismet in that they predict \nperformance based on an existing parallel implementation. Hill and Marty [19] recently proposed a simple \nperfor\u00admance analytical model, extending Amdahl s law. Their model assumes future processors include \ndifferent types of cores and each program region can choose the more appro\u00adpriate core based on its workload. \nChung and Mai [12] fur\u00adther improved Hill and Marty s model with heterogeneous chip including ASIC, FPGA, \nand GPU. Although we kept Kismet s analytical modelrelatively simple,Kismet can eas\u00adily incorporate these \nsophisticated models if needed. Parallel Performance Debugging Tools Several systems have been developed \nin order to help debug the performance of pre-existing parallel programs [3, 13, 39]. SvPablo pro\u00advided \nan integrated viewing and instrumentation environ\u00adment that allowedperformancedebuggingofMPIprograms. \nAdve et al [3] performed similar analysis on data parallel FORTRAN.Paradyn [39] automatically searches \nfor perfor\u00admance problems in long running programs by dynamically instrumenting the program. Martonosi \net al [38] were able to examine the performance of the cache system with very little overhead by integrating \nperformance monitoring into existing cache-coherence mechanisms. These systems could be used in concert \nwithKismet to help determine whyactual performance does not match the predicted bound on program performance. \nSUIF Explorer [33] uses static and dynamic analyses to understand parallel-execution related properties, \nmuch likeKismet;however,Kismet does notrequire user in\u00adteraction, and uses a simplify hardware speci.cations \nto give reasonable speedup predictions of post-parallelized code. Reducing Dynamic Program Analysis Overheads \nDy\u00adnamic program analyses often have huge memory and stor\u00adage requirements as they can produce data for \neach dynamic instruction in a program that easily could run billions or trillions of instructions. To \nalleviate the severe memory re\u00adquirements of dynamic program analysis, compression tech\u00adniques have been \nused in whole program analysis [55], de\u00adpendence analysis [26], and HCPA [16]. Initially Kismet used \na compression technique similar to [16], but we found that handling more irregular programs like SpecInt \nnecessi\u00adtated the creation ofKismet s summarization-based HCPA variant, SHCPA. In addition to memory \noverhead, runtime overhead is also important for practical use. Speci.cally for program analysis that \nuses shadow memory, the implementation of shadow memory signi.cantly impacts the overall runtime as each \nload and store instruction will access the shadow memory.Valgrind [41] s shadow memoryimplementation \nis described in [40]. Umbra [58] and EMS64 [57] proposed ef.cient shadow memory implementation for 64-bit \naddress space, exploiting the sparse usage of memoryspace in 64-bit systems and cached shadow memory. \nAlthough techniques introduced in these papers can be incorporated in Kismet, Kismet s shadow memoryimplementation \ndiffers from other tools as it needs to ef.ciently store and retrieve multiple timestamps for each memory \naddress to track the critical path of multiple region levels.  Prior HCPA-Based Work Kismet extends \nthe HCPA re\u00adgion hierarchyproposed in [22] and [16] to include sequence regions. Sequence regions allow \nHCPA-based planners to separate ILP from other classes of parallelism. This is im\u00adportant for modeling \nperformance of both superscalar-based out-of-order systems, where the ILP is likely already ex\u00adploited \nby the base core (and thus is paradoxically unex\u00adpressible as far as the parallel programming is concerned), \nand also Raw-like systems with ILP compilers, where ILP is the primary source of parallelism. Sequence \nregions en\u00adable the implementofKismet sexpressible self-parallelism (ESP) metric, which allows Kismet \nto .lter regions that have parallelism unexpressible by the target platform, which greatly enhances accuracy. \nKismet also adds a cache-aware execution time model and models the effects of loop un\u00adrolling; both of \nthese enhancements enable prediction of super-linear speedup. Kismet introduces a new HCPA vari\u00adant with \nregion summarization, SHCPA, which is important for handling irregular applications for which trace compres\u00adsion \nis not effective.  8. Conclusion This paper presents Kismet, a tool that estimates the par\u00adallel speedup \nof serial programs. Kismet automatically lo\u00adcalizes the parallelism available throughout nested program \nregions and combines this with user-speci.ed constraints to provide approximate upper bounds for the \nparallel speedup attainable on a speci.ed system. Our preliminary results on 19 benchmarks and two classes \nof machines (AMDOpteron anda tiledprocessor)demonstrateKismet seffectivenessat providing accurate upper \nbounds across diverse programs and machine architectures.  Acknowledgment Thisresearchwas fundedby theUS \nNational ScienceFoun\u00addation under CAREER Award 0846152, Awards 0725357 and 1018850, andbya giftfrom AdvancedMicroDevices. \n References [1] Intel Parallel Advisor 2011. http://software.intel. com/en-us/articles/intel-parallel-advisor. \n[2] NASParallelBenchmarks 2.3; OpenMPC. www.hpcc.jp/ Omni/. [3]V.Adve,J.Mellor-Crummey,M. Anderson, J.-C.Wang,D.A. \nReed, and K. Kennedy. An integrated compilation and per\u00adformance analysis environment for data parallelprograms. \nIn SC 95: Proceedings of the ACM/IEEE conference on Super\u00adcomputing, 1995. [4] A. Agarwal, S. Amarasinghe, \nR. Barua, M. Frank, W. Lee, V. Sarkar,D. Srikrishna, andM.Taylor. TheRAW compiler project. In Proceedings \nof the Second SUIF CompilerWork\u00adshop, 1997. [5] G. Ammons, T. Ball, and J. R. Larus. Exploiting hardware \nperformance counters with .ow and context sensitive pro.l\u00ad ing. In PLDI 97:Proceedings of theACMSIGPLAN \nConfer\u00adence onProgramming Language Design andImplementation, 1997. [6]T.E. Anderson, andE.D. Lazowska. \nQuartz:A tool for tun\u00ading parallel program performance. InSIGMETRICS, vol. 18, 1990. [7] T. Austin, and \nG. S. Sohi. Dynamic dependency analysis of ordinary programs. In ISCA 92: Proceedings of the International \nSymposium on Computer Architecture, 1992. [8] J.Babb,M. Frank,V. Lee, E.Waingold,R.Barua,M.Taylor, J. \nKim, S. Devabhaktuni, and A. Agarwal. The raw bench\u00admark suite: computation structures for general purpose \ncom\u00adputing. In FCCM 97: Proceedings of the IEEE Symposium on FPGA-Based Custom ComputingMachines, 1997. \n[9] Bailey et al. The NAS parallel benchmarks. In SC 91: Proceedings of theConference on Supercomputing, \n1991. [10] B. J. Barnes, B. Rountree, D. K. Lowenthal, J. Reeves, B.de Supinski, andM. Schulz. A regression-based \napproach to scalability prediction. In ICS 08: Proceedings of the In\u00adternationalConference on Supercomputing, \n2008. [11] J. M. Bull, and D. O Neill. A microbenchmark suite for OpenMP 2.0. SIGARCH Computer Architecture \nNews, Dec 2001. [12] E. S.Chung,P. A.Milder, J. C.Hoe, andK.Mai. Single-chip heterogeneous computing: \nDoes the future include custom logic, fpgas, and gpgpus? In MICRO 10:Proceedings of the IEEE/ACM International \nSymposium on Microarchitecture, 2010. [13] L. De Rose, and D. Reed. Svpablo: A multi-language architecture-independent \nperformance analysis system. In ICPP 99:International Conference on Parallel Processing, 1999. [14] E.Waingold \net al. BaringIt All to Software:RawMachines. IEEEComputer, Sept 1997. [15] S. Garcia,D. Jeon,C. Louie, \nS.KotaVenkata, andM.B.Tay\u00adlor. Bridging the parallelizationgap: Automating parallelism discovery and \nplanning. In HotPar 10: Proceedings of the USENIX workshop on HotTopics inParallelism, 2010. [16] S. \nGarcia, D. Jeon, C. Louie, and M. B. Taylor. Kremlin: Rethinking and rebooting gprof for the multicore \nage. In PLDI 11: Proceedings of the Conference on Programming Language Design andImplementation, 2011. \n[17] N. Goulding, J. Sampson, G. Venkatesh, S. Garcia, J. Auric\u00adchio, J. Babb, M. Taylor, and S. Swanson. \nGreenDroid: A Mobile Application Processor for a Future of Dark Silicon. InHotchips, 2010. [18]Y.He,C. \nLeiserson, andW. Leiserson. The Cilkview Scala\u00adbility Analyzer. InSPAA 10:Proceedings of the Symposium \nonParallelism in Algorithms and Architectures, 2010. [19]M.D.Hill, andM.R.Marty. Amdahl slawin the multicore \nera. IEEE Computer, July 2008. [20]K.Hoste,A.Phansalkar,L. Eeckhout,A. Georges,L.K. John, and K. De Bosschere. \nPerformance prediction based on in\u00adherent program similarity. In PACT 06: Parallel Architec\u00adtures and \nCompilationTechniques, 2006.  [21] D. Jeon, S. Garcia, C. Louie, S. Kota Venkata, and M. B. Taylor. \nKremlin: Like gprof, but for Parallelization. In PPoPP 11:Principles andPracticeofParallelProgramming, \n2011. [22] D. Jeon, S. Garcia, C. Louie, and M. B. Taylor. Parkour: Parallel speedup estimates for serial \nprograms. In HotPar 11: Proceedings of the USENIX workshop on Hot Topics in Parallelism,May 2011. [23] \nT. S. Karkhanis, and J. E. Smith. A .rst-order superscalar processor model. In ISCA 04: Proceedings \nof the Interna\u00adtional Symposium on Computer Architecture. [24] H. Kim, A. Raman, F. Liu, J. W. Lee, and \nD. I. August. Scalable speculative parallelization on commodity clusters. In MICRO 10: Proceedings of \nthe IEEE/ACM International Symposium onMicroarchitecture, 2010. [25] M. Kim, H. Kim, and C. Luk. Prospector: \nA dynamic data\u00addependence pro.ler to help parallel programming. InHotPar 10: Proceedings of the USENIX \nworkshop on Hot Topics in parallelism, 2010. [26]M.Kim,H.Kim, and C.-K. Luk. SD3:A scalable approach \nto dynamic data-dependence pro.ling. MICRO 10: Pro\u00adceedings of the International Symposium on Microarchitec\u00adture, \n2010. [27] M. Kulkarni, M. Burtscher, R. Inkulu, K. Pingali, and C. Casc\u00b8aval. How much parallelism is \nthere in irregular ap\u00adplications? In PPoPP 09: Proceedings of the ACM SIG-PLAN Symposium onPrinciples \nandPractice ofParallelPro\u00adgramming, 2009. [28] M. Kumar. Measuring parallelism in computation-intensive \nscienti.c/engineering applications. IEEE TOC, Sep 1988. [29] M. S. Lam, and R. P. Wilson. Limits of control \n.ow on parallelism. In ISCA, 1992. [30] J.R. Larus. Loop-level parallelism in numeric and symbolic programs. \nIEEETrans.Parallel Distrib. Syst., 1993. [31] C. Lattner, and V. Adve. LLVM: A compilation framework \nfor lifelong program analysis &#38; transformation. In CGO 04: Proceedings of the International Symposium \non Code Generation and Optimization, 2004. [32]W. Lee,R.Barua,M.Frank,D.Srikrishna,J.Babb,V.Sarkar, and \nS. Amarasinghe. Space-time scheduling of instruction\u00adlevel parallelism on a Raw machine. In ASPLOS 98: \nInter\u00adnational Conference on Architectural Support for Program\u00adming Languages and Operating Systems, \nOct 1998. [33] S.-W.Liao,A.Diwan,R.P.Bosch,Jr.,A. Ghuloum,andM.S. Lam. SUIF Explorer: an interactive \nand interprocedural par\u00adallelizer. In PPoPP 99: Proceedings of the ACM SIGPLAN symposium on Principles \nand Practice of Parallel Program\u00adming. [34] G. Loh. Atime-stamping algorithm for ef.cient performance \nestimationof superscalarprocessors. In SIGMETRICS, 2001. [35] M. B. Taylor et al. Evaluation of the raw \nmicroprocessor: An exposed-wire-delay architecture for ilp and streams. In ISCA 04: Proceedings of the \nInternational Symposium on Computer Architecture, Jun 2004. [36]M.B.Tayloretal. TheRawMicroprocessor:AComputation \nFabric for SoftwareCircuits and General-PurposePrograms. InIEEEMicro,Mar/Apr 2002. [37] M. Martin, D. \nSorin, B. Beckmann, M. Marty, M. Xu, A. R. Alameldeen,K.Moore,M.Hill, andD.Wood. Multifacet s general \nexecution-driven multiprocessor simulator (GEMS) toolset. SIGARCH Comput. Archit.News, Nov 2005. [38] \nM. Martonosi, D. Felt, and M. Heinrich. Integrating perfor\u00admance monitoring and communication in parallel \ncomputers. InSIGMETRICS, 1996. [39] B. P. Miller, M. D. Callaghan, J. M. Cargille, J. K. Hollingsworth, \nR. B. Irvin, K. L. Karavanic, K. Kunchitha\u00adpadam, and T. Newhall. The Paradyn Parallel Performance MeasurementTool. \nIEEE Computer, 1995. [40] N. Nethercote, and J. Seward. How to shadow every byte of memory used by a \nprogram. In VEE 07: Proceedings of the 3rd international conference on Virtual Execution Envi\u00adronments, \n2007. [41] N. Nethercote, and J. Seward. Valgrind: A framework for heavyweight dynamic binary instrumentation. \nIn PLDI 07: Proceedings of the Conference on Programming Language Design andImplementation, 2007. [42] \nD. Ofelt, and J. L. Hennessy. Ef.cient performance predic\u00adtion for modern microprocessors. In SIGMETRICS, \n2000. [43]M.K.Prabhu, andK. Olukotun. Exposing speculative thread parallelism in spec2000. In PPoPP 05: \nProceedings of theACM SIGPLAN symposium onPrinciples andPractice of ParallelProgramming, 2005. [44] E. \nRaman, G. Ottoni, A. Raman, M. J. Bridges, and D. I. August. Parallel-stage decoupled software pipelining. \nIn CGO 08: Proceedings of the International Symposium on Code Generation and Optimization, 2008. [45] \nL.Rauchwerger,P.K.Dubey, andR. Nair. Measuring limits of parallelism and characterizing its vulnerability \nto resource constraints. InMICRO 93:Proceedings of the international symposium onMicroarchitecture, 1993. \n[46] S. Bell et al. TILE64 -Processor: A 64-Core SoC with MeshInterconnect. In ISSCC 08:IEEE Solid-StateCircuits \nConference, 2008. [47] N. R. Tallent, and J. M. Mellor Crummey. Effective per\u00adformance measurement and \nanalysis of multithreaded appli\u00adcations. In PPoPP 09: Proceedings of the ACM SIGPLAN symposium on Principles \nand practice of parallel program\u00adming, 2009. [48] M. B. Taylor. Design Decisions in the Implementation \nof a Raw ArchitectureWorkstation.Master s thesis,Massachusetts Institute ofTechnology, Sept 1999. [49] \nM. B. Taylor. Tiled Microprocessors. Ph.D. thesis, Mas\u00adsachusettsInstitute ofTechnology, 2007. [50] M. \nB. Taylor, W. Lee, S. P. Amarasinghe, and A. Agarwal. Scalar operand networks. IEEE Transactions on Parallel \nand Distributed Systems, Feb 2005. [51]K.B. Theobald,G.R. Gao, andL.J.Hendren. On the limits of program \nparallelism and its smoothability. InMICRO 92:  Proceedings of the International Symposium on Microarchi\u00adtecture, \n1992. [52] D. W. Wall. Limits of instruction-level parallelism. In Proceedings of the Conference on Architectural \nSupport for Programming Languages and Operating Systems, 1991. [53] J. Zhai, W. Chen, and W. Zheng. \nPhantom: predicting per\u00adformance of parallel applications on large-scale parallel ma\u00adchines using a single \nnode. In PPoPP 10: Proceedings of theACM SIGPLAN Symposium onPrinciples andPractice of ParallelProgramming, \n2010. [54] X. Zhang, A. Navabi, and S. Jagannathan. Alchemist: A transparent dependence distance pro.ling \ninfrastructure. In CGO 09: Proceedings of the International Symposium on Code Generation and Optimization, \n2009. [55] Y. Zhang, and R. Gupta. Timestamped whole program path representation and its applications. \nIn PLDI 01:Proceedings of theACM SIGPLAN Conference onProgramming Language Design andImplementation, \n2001. [56] L. Zhao, R. Iyer, J. Moses, R. lllikkal, S. Makineni, and D. Newell. Exploring Large-ScaleCMPArchitectures \nUsing ManySim. IEEEMicro, July 2007. [57] Q. Zhao,D.Bruening, and S. Amarasinghe. Ef.cient mem\u00adory shadowing \nfor 64-bit architectures. In ISMM 10: Pro\u00adceedings of the International Symposium on Memory Man\u00adagement, \nJun 2010. [58] Q. Zhao,D.Bruening, andS. Amarasinghe. Umbra:Ef.cient and scalable memory shadowing. In \nCGO 10: Proceedings of the IEEE/ACM international symposium on Code Genera\u00adtion and Optimization, 2010. \n[59] H. Zhong, M. Mehrara, S. Lieberman, and S. Mahlke. Un\u00adcovering hidden loop level parallelism in \nsequential applica\u00adtions. In HPCA 08: Proceedings of the International Sym\u00adposium on HighPerformanceComputer \nArchitecture, 2008. [60] D. A. Zier, and B. Lee. Performance evaluation of dy\u00adnamic speculative multithreading \nwith the cascadia architec\u00adture. IEEETransactions onParallel and Distributed Systems, Jan 2010.  \n\t\t\t", "proc_id": "2048066", "abstract": "<p>Software engineers now face the difficult task of refactoring serial programs for parallel execution on multicore processors. Currently, they are offered little guidance as to how much benefit may come from this task, or how close they are to the best possible parallelization. This paper presents Kismet, a tool that creates parallel speedup estimates for unparallelized serial programs. Kismet differs from previous approaches in that it does not require any manual analysis or modification of the program. This difference allows quick analysis of many programs, avoiding wasted engineering effort on those that are fundamentally limited. To accomplish this task, Kismet builds upon the hierarchical critical path analysis (HCPA) technique, a recently developed dynamic analysis that localizes parallelism to each of the potentially nested regions in the target program. It then uses a parallel execution time model to compute an approximate upper bound for performance, modeling constraints that stem from both hardware parameters and internal program structure.</p> <p>Our evaluation applies Kismet to eight high-parallelism NAS Parallel Benchmarks running on a 32-core AMD multicore system, five low-parallelism SpecInt benchmarks, and six medium-parallelism benchmarks running on the finegrained MIT Raw processor. The results are compelling. Kismet is able to significantly improve the accuracy of parallel speedup estimates relative to prior work based on critical path analysis.</p>", "authors": [{"name": "Donghwan Jeon", "author_profile_id": "81453639831", "affiliation": "UCSD, La Jolla, CA, USA", "person_id": "P2839223", "email_address": "djeon@cs.ucsd.edu", "orcid_id": ""}, {"name": "Saturnino Garcia", "author_profile_id": "81100378592", "affiliation": "UCSD, La Jolla, CA, USA", "person_id": "P2839224", "email_address": "sat@cs.ucsd.edu", "orcid_id": ""}, {"name": "Chris Louie", "author_profile_id": "81453654901", "affiliation": "UCSD, La Jolla, CA, USA", "person_id": "P2839225", "email_address": "cmlouie@cs.ucsd.edu", "orcid_id": ""}, {"name": "Michael Bedford Taylor", "author_profile_id": "81100517791", "affiliation": "UCSD, La Jolla, CA, USA", "person_id": "P2839226", "email_address": "mbtaylor@cs.ucsd.edu", "orcid_id": ""}], "doi_number": "10.1145/2048066.2048108", "year": "2011", "article_id": "2048108", "conference": "OOPSLA", "title": "Kismet: parallel speedup estimates for serial programs", "url": "http://dl.acm.org/citation.cfm?id=2048108"}