{"article_publication_date": "10-22-2011", "fulltext": "\n Bene.ts and Barriers of User Evaluation in Software Engineering Research Raymond P.L. Buse Caitlin \nSadowski Westley Weimer University of Virginia University of California Santa Cruz University of Virginia \nbuse@cs.virginia.edu supertri@cs.ucsc.edu weimer@cs.virginia.edu Abstract In this paper, we identify \ntrends about, bene.ts from, and barriers to performing user evaluations in software engineer\u00ading research. \nFrom a corpus of over 3,000 papers spanning ten years, we report on various subtypes of user evaluations \n(e.g., coding tasks vs. questionnaires) and relate user evalu\u00adations to paper topics (e.g., debugging \nvs. technology trans\u00adfer). We identify the external measures of impact, such as best paper awards and \ncitation counts, that are correlated with the presence of user evaluations. We complement this with a \nsurvey of over 100 researchers from over 40 different universities and labs in which we identify a set \nof perceived barriers to performing user evaluations. Categories and Subject Descriptors H.1.2 [Information \nSystems Applications]: Models and Principles User/Machine Systems; D.0 [Software]: General General Terms \nExperimentation, Human Factors Keywords Human study, User evaluation 1. Introduction Modern software \nengineering is an inherently human-centric activity. From requirements, architecture and design through \ndevelopment, testing and maintenance, the inputs, processes and outputs are primarily created, evaluated \nand performed by humans. While there have been many advances in au\u00adtomating various aspects of software \nengineering (e.g., spec\u00adi.cation mining, test input generation, component composi\u00adtion, software synthesis), \nmajor stakeholders such as devel\u00adopers, managers and consumers remain primarily human. It is thus perhaps \nsurprising that of 1,718 papers surveyed from ten years of selective software conferences, only about \n10% used humans to evaluate a research claim directly. Still, user studies in software engineering are \nalmost as old as the .eld itself, predating even FORTRAN. For example, Permission to make digital or \nhard copies of all or part of this work for personal or classroom use is granted without fee provided \nthat copies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. OOPSLA 11, October 22 27, 2011, Portland, Oregon, \nUSA. Copyright c &#38;#169; 2011 ACM 978-1-4503-0940-0/11/10. . . $10.00 in his seminal 1953 54 work \non SPEEDCODING, the .rst higher-level language for an IBM Computer, John Backus reported on user experiences: \nmany problems which might require two weeks or more to program in 701 language can be programmed in Speedcoding \nin a few hours. [1, p.6]. Since then, such studies have not always been common. Nonetheless, we .nd that \nthe use of user evaluations in research published at top venues has grown 500% since the year 2000. In \nthe case of OOPSLA, the user evaluation rate has more than tripled since 2007. This paper explores the \nrecent history and current state of such evaluations and identi.es bene.ts from and barriers to performing \nthem. In this paper, we use user evaluation to mean the pro\u00adcess of evaluating or understanding a technique, \ntool, or idea in terms of the needs, preferences, and abilities of humans. We collectively refer to humans \ninvolved in any stage of the software engineering process as users. From a research per\u00adspective, both \na developer considering a new programming language feature and a consumer navigating a carefully\u00addesigned \nGUI are users of the technique in question. This paper explores three main research questions: RQ1: \nWhat are trends in performing user evaluations within software engineering papers?  RQ2: Are there external, \nempirically measurable, notions of quality and impact correlated with performing user evaluations in \nsoftware engineering papers?  RQ3: What are the barriers software engineering re\u00adsearchers perceive \nin considering performing a user evaluation? How have they been mitigated by other re\u00adsearchers?  We \n.nd, for example, that the number of papers with user evaluations is increasing, both in absolute and \nrelative terms. With all papers considered, those with user evalua\u00adtions do not have higher citation \ncounts overall. However, when attention is restricted to highly-cited works, user eval\u00aduations are relevant: \nfor example, among the top quartile of papers by citation count, papers with user evaluations are cited \n40% more often than papers without. Highly-selective conferences accept a larger proportion of papers \nwith user evaluations than do less-selective conferences. Promisingly for resource-strapped investigators, \nlarge numbers of profes\u00adsional developers and real-world projects are not required to perform highly-cited \nuser evaluations. Each of these claims is made in a statistically signi.cant manner and is detailed in \nthe remainder of this paper.  Overall, this paper makes several related contributions: We present the \nresults of a survey of 107 researchers, highlighting the perceived dif.culties and bene.ts of performing \nuser evaluations. In particular, we .nd that 91% of participants who have performed user evalua\u00adtions \nagreed that they gained insights from performing a user evaluation which they may not otherwise have \nhad. We also highlight barriers to performing user evaluations identi.ed by researchers.  We present \na classi.cation of recent user evaluations based on 3,110 papers from ten years of selective confer\u00adences. \nThis includes longitudinal information about how reported research has changed over time. We .nd that \nuser evaluations are increasing in frequency and that pa\u00adpers containing human studies are associated \nwith impact indicators (e.g., awards and greater citation counts).  We develop a concise classi.cation \nscheme for user eval\u00aduations used in software engineering research. We val\u00adidate this classi.cation scheme \nwith both the survey of software engineering researchers and the classi.cation of papers containing a \nuser evaluation.  We develop a formal model relating words in research paper text to the presence of \na user evaluation described in the paper. Our model is 95% accurate, but perhaps more importantly, is \na concrete example of an approach to reducing the cost of a user evaluation. Previous work in such models \ninvolved large amounts of user annotation; we train on a small amount of annotated data, and then evaluate \nmodel accuracy.  The structure of this paper is as follows. In Section 2 we present background and place \nour results in the context of related work. We discuss our research methodologies in Sec\u00adtion 3. In Section \n4 we present the results of a study of 3,110 recent research papers and our survey of human researchers, \nincluding identi.cation of trends in Section 4.1, bene.ts in Section 4.2, and barriers in Section 4.3. \nWe discuss the re\u00adsults and identify possible threats to the validity of our work in Section 5 and conclude \nin Section 6. 2. Related Work This is not the .rst paper to analyze user studies in soft\u00adware engineering \nresearch. However, previous research has focused primarily on classifying experiments, whereas we focus \non identifying bene.ts and barriers to performing user evaluations. To this end, we broaden our analysis \nby sur\u00adveying researchers and examining previously-unused met\u00adrics such as best paper rate. Furthermore, \nwe are not aware of any previous papers which explore empirical relationships between an evaluation strategy \nand proxies of impact (e.g., citation count). Throughout this paper, we will use the terms user eval\u00aduation \n, user study , human study , and human evalua\u00adtion interchangeably to refer to user evaluations. User \neval\u00aduation in this context must include recruitment of external participants or else the use of speci.c \npredictive human mod\u00adels (e.g. cognitive models). Note that user evaluation does not encompass studies \nof human-created artifacts (e.g. min\u00ading software repositories). Some reports, often labeled case study \nor experience report , consist of investigator reports on personal experiences with self-made tools. \nWhen no ex\u00adternal participants are involved, we do not consider the study to be a user evaluation. Sj\u00f8eberg \net al. identi.ed 103 papers containing controlled experiments from a collection of 5,453 articles published \nin software engineering journals [24]. They discuss vari\u00adous facets of these experiments, such as threats \nto validity, whether they are replications, the subjects, and the tasks per\u00adformed. They found that very \nfew research papers contained controlled experiments, and many of those experiments may not be representative \nand have unaddressed threats to valid\u00adity. They also found that most published replications con\u00adducted \nby original authors con.rm the results of the initial experiment whereas most replications conducted \nby a new author differ with the results of the original paper, however, there were only 20 replications \nin the sample of 103 papers. Lung et al. [17] investigated the dif.culty of replicating human subjects \nstudies in software engineering. They found many key details were omitted from the study description. \nThey concluded that literal replication may not be the most effective strategy for validating the results \nof previous stud\u00adies. Tichy et al. [27] identi.ed computer science papers con\u00adtaining empirical studies, \nand compared them to papers in other disciplines. They found, for example, that there were fewer such \npapers than expected within computer science. Shaw [21] conducted a manual review of the abstracts of \npa\u00adper submitted to ICSE in 2002. She concluded that .eld ex\u00adperience and realistic examples tend to \nbe the most effective ways of validating a result. Glass et al. [8] categorized software engineering \narticles based on topic, research approach, research method, refer\u00adence discipline, and level of analysis. \nHost et al. [10] created a classi.cation scheme of user evaluations based on the in\u00adcentives of subjects. \nTheir classi.cation scheme is primarily designed to support understanding and replication. We em\u00adploy \na very similar classi.cation scheme in this paper and apply it to many more studies. Other researchers \nhave analyzed experiments in software engineering research in the context of taxonomy valida\u00adtion [4]; \nthis taxonomy was later used again to classify ex\u00adperiments in software engineering venues [28]. There \nare also a number of papers which review experiments within particular sub.elds of software engineering \n(e.g., [5, 6, 12, 13]). Previous researchers have also drawn attention to the general lack of consideration \nof human factors in software engineering and programming languages research [9].  3. Methodology In \nthis section we describe our classi.cation scheme for user evaluations, our large-scale empirical study \nof user evaluation in software engineering research, and our survey methodology. 3.1 Classi.cation Scheme \nWe desired a simple, easily applicable, and semantically useful classi.cation scheme for the types of \nuser evaluations found in software engineering papers. We developed a novel scheme based on a review \nof related literature (e.g., [24]) and re.ned it using data from a pilot survey. Table 1 shows the classi.cation \nscheme for user evaluations we adopted in this paper. Our scheme is based on the important split between \ntask\u00adbased studies (i.e., Comparative, Observation, Field) and non task-based studies (i.e., Judgment, \nDescriptive). In gen\u00aderal, task-based studies are more time consuming to admin\u00adister because of the additional \nburden of designing and ob\u00adserving the tasks. Not all task-based studies should be treated equally: the \nimportance of controlled experiments (cf. [24]) motives a division between Comparative and Observation \nstudies, and the separate explicit Field category was added based on pilot responses. Non task-based \nstudies primarily involve asking participants questions about their own expe\u00adriences (Descriptive) or \nparticipants rating artifacts (Judg\u00adment). This distinction is also found in previous work [18]. Finally, \nsome research models human interaction without human participants (Models). When categorizing papers, \nwe also employ a mostly\u00adorthogonal secondary classi.cation scheme based on the ar\u00adtifacts used. The artifacts \nclassi.cation scheme (Table 2) is simpli.ed from the scheme of Host et al. [10] and is de\u00adsigned primarily \nto capture the motivations of the study par\u00adticipants. In general, Real refers to an artifact that would \nhave existed even if the user study had not taken place, Arti.\u00adcial refers to an artifact and context \ncreated for the purposes of the study, and Isolated refers to objects, such as code snip\u00adpets, taken \nout of context. Of special note are user evalu\u00adations involving assignments undertaken by classroom stu\u00addents; \nwe classi.ed such projects as Arti.cial. In Section 4.1 we discuss why we elected not to include education-focused \npapers in our pool of software engineering papers for analy\u00adsis. Note that papers can contain multiple \nindependent stud\u00adies. In some cases, studies may contain elements from mul\u00adtiple categories. In these \ncases, we choose the category that best describes the study in question. We validated our classi.cation \nscheme in two ways: 1. We successfully used it to hand-annotate 211 papers identi.ed by our empirical \nanalysis (Section 3.2) as con\u00adtaining a user evaluation. 2. We asked survey participants to identify \nall types of user evaluations they had administered, and provided an op\u00adportunity for specifying additional \ncategories. None of the 64 participants who had experience performing a user evaluation speci.ed any \nadditional categories.1  Comparison with other schemes Many other researchers have proposed taxonomies \nof human studies and also of other empirical software engineering research methods [15]. For example, \nBasili et al. [2] present an extremely detailed classi.cation scheme for empirical software engineering \nex\u00adperiments. Zelkowitz et al. [4] developed a taxonomy of experimen\u00adtation schemes in software engineering, \nand validated their taxonomy by classifying software engineering papers. They identify 12 categories \n.tting into three broad groups: obser\u00advational methods (cf. our Observation and Field), historical methods \n(we do not consider these to be user evaluations), and controlled methods (cf. our Models, Comparative \nand Judgment). Descriptive studies are beyond the scope of their paper. When .nalizing our classi.cation \nscheme, we considered the eight strategies in four quadrants of McGrath s clas\u00adsic categorization of \nresearch strategies [18]. Quadrant 4 (theoretical strategies) maps to our Models category. The two strategies \nwithin Quadrant 3 (respondent strategies) correspond to Descriptive and Judgment. We also have an explicit \nField category corresponding to Quadrant 1 (.eld strategies). The two categories from McGrath s experimen\u00adtal \nstrategies Quadrant 2 map to Descriptive and Observa\u00adtion experiments, although McGrath divides based \non the realism of the environment. This correspondence with es\u00adtablished research strategies provides \nadditional con.dence that our scheme adequately covers the relevant possibilities.  3.2 Empirical Study \nWe begin by describing a manual process for characterizing the user evaluations in a research papers. \nWe then describe how, by training on a relatively small number of human\u00adannotated papers, we can create \na descriptive textual model capable of automatically identifying, with high accuracy, all papers in the \ncorpus containing a user evaluation. This clas\u00adsi.er allows us to quickly hand-annotate only those papers \nwhich likely contain a user evaluation. Using these tools we characterize the current state as well as \ntrends over the last ten years of user evaluations in several ways. For example, we can count and measure \nthe self\u00adreported subject matter of such papers to gain insight into 1 Figure 13 shows the percentage \nof participants who had performed a user evaluation for each of the categories.  Study Type Description \nExample Comparative Participants perform the same task under different situations. debugging code with/out \na tool Observation Participants are observed performing a speci.c task. case study of new language feature \nField Observations of people in the .eld performing various activities. counting daily interruptions \nJudgment Participants judge one or more artifacts; don t actively solve a problem. heuristic evaluation \nDescriptive Participants respond to questions, drawing on their own experiences focus groups; experience \nsurveys (performing no development task). Models Validation based on predictive models without human \nparticipants. prediction with cognitive models Table 1. Classi.cation scheme for user evaluation types. \nArtifact Type Description Example Real A pre-existing project is considered and the major objective of \nthe in situ studies subjects is not only to participate in the experimental study. Arti.cial Subjects \nconsider relationships to supporting material. designed programming tasks Isolated Objects of study are \npresented without additional context. participants consider code snippets No Speci.c No speci.c artifact \nis used. most surveys etc. Table 2. Classi.cation scheme for user evaluation artifacts. We adapt this \nscheme from Host et al. [10]. the sub-domains of software engineering where user studies are most applicable. \nData Set We restrict attention to .ve of the most promi\u00adnent software engineering publication venues: \nThe Inter\u00adnational Conference on Automated Software Engineering (ASE), The International Symposium on \nFoundations of Software Engineering (ESEC/FSE), The International Con\u00adference on Software Engineering \n(ICSE), The International Symposium on Software Testing and Analysis (ISSTA), and Object-Oriented Programming \nSystems, Languages and Ap\u00adplications (OOPSLA). In addition, and for the purpose of comparison, we also \nconsider The Conference on Human Factors in Computing Systems (CHI) which represents a community of researchers \nwho may also be associated with certain software engineering concerns (e.g., 18 of 107 survey respondents \nhad submitted to both CHI and ICSE) and who care about user evaluations. For each conference we mined \npapers published since the year 2000 (i.e., 2000 2010). Not every conference was held every year in that \nwindow. Ta\u00adble 3 gives a breakdown of the 3,110 total papers we ana\u00adlyzed, 1,718 of which were from software \nengineering (i.e., non-CHI) venues. Annotation We .rst manually annotated a random set of papers as training \ndata for a classi.er. For each of 100 pa\u00adpers, two researchers read enough of the paper to deter\u00admine \nthe presence or absence of a user evaluation (e.g., for some papers the abstract, introduction and conclusion \nsuf\u00ad.ce). The taxonomies introduced in Section 3.1 were em\u00adployed to label each user evaluation located. \nThe number and type of participants (students, practitioners, or both) was also recorded. The researchers \ndiscussed the annotations for each pa\u00adper until a consensus was reached. This step ensures that the classi.cation \nscheme is well-de.ned and can be consis\u00adtently applied. After completing the initial set of 100 papers \ntogether, one of the two researchers annotated an additional 82 papers to help ensure that the set was \nlarge enough for use as training data for a document classi.er (i.e., to mitigate the threat of over-.tting). \nPredictive Model We present a precise, empirically-derived, text-based model of research papers with \nuser evaluations. The model determines whether a given research paper is likely to contain a user study \nwithout human intervention. We adopt this approach for two primary reasons: First, an automatic system \nallows us to quickly and con\u00adsistently characterize a large number of papers. Automatic classi.cation \nis also highly adaptable in comparison to man\u00adual alternatives. Second, in our experience reading papers \nwhich describe user evaluations, we have often found that the abstracts alone contain no clues to indicate \nthat a user evaluation is present. This is especially true in the case of small studies. We therefore \nhypothesize that those previous studies which have relied on the ability of an annotator to classify \npapers solely from abstracts (e.g., [21, 24]) may be subject to a systematic bias. A textual model allows \nus to fully inspect the complete text of each paper. Our model is based on a term frequency vector (i.e., \nbag of words ) approach, where a document is characterized by a mapping from words to frequency counts. \nFor example, a document containing the word participant several times may likely contain a user evaluation. \nTo enhance the preci\u00adsion of this technique, we .ltered out paper sections with titles containing the \nterms related work and references as these were found to introduce errors when, for example,  Publication \nVenue Proceedings Mined Papers (2000-2010) International Conference on Automated Software Engineering \n(ASE) 7 280 Conference on Human Factors in Computing Systems (CHI) 11 1210 International Symposium on \nFoundations of Software Engineering (ESEC/FSE) 11 331 International Conference on Software Engineering \n(ICSE) 11 739 International Symposium on Software Testing and Analysis (ISSTA) 7 192 Object-Oriented \nProgramming Systems, Languages and Applications (OOPSLA) 11 358 total 58 3110 Table 3. Corpus of papers \nand publication venues examined in this study. Term Predictive Power asked 0.3519 participants 0.2259 \nstudents 0.2149 experience 0.1627 graduate 0.1549 human 0.1397 interview 0.1326 we asked 0.1326 conducted \n0.1290 programming experience 0.1160 Table 4. The ten most predictive textual terms for classifying \npapers with user evaluations using the ReliefF method [20]. A power of 1.0 indicates that the feature \nis a perfect predictor, 0.0 indicates it provides no information toward the classi.cation goal. authors \ndiscussed previous studies they they did not conduct. To learn which words are predictive we use a feature \nse\u00adlection technique called ReliefF [20], which characterizes the predictive power of features without \nassuming condi\u00adtional independence. Table 4 enumerates the ten most pre\u00addictive terms. As potential features \nwe consider both indi\u00advidual terms as well as two term tuples (e.g., in addition to asked we use phrases \nsuch as we asked ). For our model, we selected all terms that have an estimated predic\u00adtive power greater \nthan 0.01 (approximately 30 terms). A principle components analysis indicates that to explain 90% of \nvariance, 44 principle components are necessary. To test our model and check for over-.tting, we used \nleave-one-out cross validation on the 182 papers in our train\u00ading corpus. We experimented with several \nclassi.ers, includ\u00ading Logistic and Bayesian models, but found C4.5 decision trees [19] to give the best \nperformance: a correct classi.\u00adcation rate of 94.5% with an error rate of 5.5%. Table 5 presents additional \nstatistics relevant to the performance of the model. We employed the trained model to label each of the \nre\u00admaining 1,718 software engineering papers in our database, identifying 294 which are likely to contain \na user evaluation. The two researchers who annotated the original set of 182 Classi.er Evaluation Correctly \nClassi.ed Instances Incorrectly Classi.ed Instances Kappa statistic Mean absolute error Root mean squared \nerror 172 10 0.8105 0.0559 0.2344 94.5055% 5.4945% Class: No User Evaluation Precision Recall F-measure \n0.9603 0.9732 0.9667 Class: Yes User Evaluation Precision Recall F-measure 0.8814 0.9123 0.8966 Table \n5. Evaluation of the decision tree classi.er used to deter\u00admine if a paper contains a user evaluation. \nThe model has very high accuracy (94.5%) and behaves similarly on yes-instances and no\u00adinstances. papers \nthen annotated those 294 papers, applying the same annotation rubric to reveal the type and size of each \nstudy. In all, 211 user evaluations were positively identi.ed (see Ta\u00adble 6). In the following discussion, \nwe leave out the Models category since we did not identify any papers which contain a model-based user \nevaluation. Nonetheless, almost 20% of survey participants who had performed a user evaluation be\u00adfore \nsaid they had previously performed a model-based eval\u00aduation (Figure 13). Although model-based evaluations \nap\u00adpear in human computer interaction papers (e.g. [26]), they are less commonly reported in recent software \nengineering research literature. False positives are rejected manually and are thus not a concern. False \nnegatives, which we characterize by inspect\u00ading our training data, can be described as minor. Such studies \nhave little descriptive text and so are dif.cult to detect sim\u00adply by means of word counts. For an indicative \nexample, in a paper by Egyed published in ICSE 2006 [7] the following sentence appears: Yet, in interviews \nwith the engineers we were told that at no time they felt delays of any kind. While  Paper Category \n# of Papers (n = 3110) SE Papers Study Candidate SE Papers SE Papers w/ User Evaluation SE Education \nPapers 1718 1100 211 50 Study/Artifact Category # of Studies (n = 211) Comparative Observation 48 51 \nField 23 Judgment Descriptive 25 64 Real 62 Arti.cial 74 Isolated 44 No Spec.c 31 Table 6. A breakdown \nof major paper categories considered throughout this study. Study Candidate papers are those which involve \na topic wherein at least 10% of paper contain a user evalu\u00adation (see Section 4.1). this quali.es as \na user evaluation by our de.nition, insuf\u00ad.cient descriptive text is present for our automated model \nto correctly classify it. Thus, while our estimated false neg\u00adative rate is approximately 9%, the lack \nof descriptive text indicates that in very few of these cases is the user evalua\u00adtion tied to a major \nclaim of the paper. We conclude that the vast majority of user evaluations corresponding to a primary \npaper claim are correctly identi.ed by our approach.  3.3 Survey To better understand the perceptions \nof software engineering researchers related to user evaluation, we conducted a survey of authors of papers \nat top-tier software engineering venues in the past 10 years (see list of venues in Table 3). Employing \nthe classi.er described in Section 3.2, we identi.ed researchers who had and had not performed a user \nevaluation during the 10-year window. We used a strati.ed random sampled of 450 candidate participants \n(200 who had performed one and 250 who had not). Each researcher was sent an email inviting them to participate \nin the anonymous web-based survey. We also sent two follow-up emails to can\u00addidates who did not respond \nto the survey when it was ini\u00adtially sent out. As a repayment for participation, we entered participants \nin a drawing for a $25 USD gift certi.cate. We have awarded this certi.cate to one of the participants. \n107 researchers completed the survey (24% response rate). Those who our classi.er determined had previously \nconducted a user evaluation responded at a higher rate than others (overall, 61.7% of participants had \nperformed a user evaluation). Figure 1 shows the number of user evaluations performed by survey participants. \n Figure 2. For each given SE-related venue (x-axis), the percent\u00adage of 107 survey participants who \nhad submitted at least one paper to that venue within the last ten years. Of the participants, 54% were \nacademic faculty and 24% were students, while another 22% were non-academic (e.g., from research labs). \nParticipants responded from more than 40 different institutions. 30% of participants reviewed more than \n10 submissions to major research conferences or jour\u00adnals last year. Figure 2 shows the breakdown of \nwhat per\u00adcentage of participants submitted to different SE-related conferences in the past 10 years.2 \nMost participants (88%) had submitted a paper to ICSE. We asked participants to rate a variety of statements \nabout user evaluation such as the bene.ts of user evaluation gen\u00aderally outweigh the costs on a .ve-point \nLikert scale rang\u00ading from Strongly Agree to Strongly Disagree. We then asked participants to identify \nbarriers to using user eval\u00aduation in their own research. We also asked participants whether they had \nperformed a user evaluation in the past: participants who had were asked additional questions about the \nlast user evaluation they had performed. When reporting results from this survey, we use a Chi-Square \ntest for independence to compare the Likert-scaled perceptions of user evaluation against whether participants \n2 The International Symposium on Empirical Software Engineering and Measurement (ESEM) And IEEE Symposium \non Visual Languages and Human-Centric Computing (VL/HCC) were mentioned by survey partic\u00adipants in free-form \nresponse but are not studied in this paper.  had ever administered a user evaluation. This test is used \nto identify statistical association between two categorical variables. In situations where the number \nof participants in one category was too small, we instead used the Fisher-Irwin test to check for statistical \nsigni.cance [3], which can be extended beyond 2 x 2 matrices. We deem a result with a p-value below 0.05 \nto be statistically signi.cant. We piloted the survey with .ve graduate students and three researchers \nto verify that the questions were clear; pilot participants are not counted among the 107 and their results \nare not included. This iteration helped us identify a list of potential barriers to performing user evaluations, \nand re.ne the classi.cation scheme for different types of user evaluations. 4. Results In this section \nwe analyze our annotated corpus of research papers as well as responses to our survey. We begin by ex\u00adploring \ntrends in user evaluations by subject matter and over time (Section 4.1). Finding that the use of such \nevaluations is rapidly increasing, we quantify and qualify some of the important bene.ts of user evaluations \n(Section 4.2). Finally, we investigate the most common perceived barriers to such research (Section 4.3). \n 4.1 Trends We begin by exploring recent trends in user evaluations. First, we investigate which research \ntopics user evaluations are commonly associated with. Then we break down trends in such evaluations over \ntime. Topic Trends To measure the impact of user evaluations, we must .rst characterize their domains \n the research sub\u00ad.elds in which they are employed. This allows us to control for those topics for which \na user evaluation is an appropri\u00adate strategy. For example, if papers on the topic of formal veri.cation \n, or on other topics which generally do not con\u00adtain user-studies, are highly cited (or not highly cited), \nthis could prevent us from detecting signi.cant trends and eval\u00aduating the hypothesis (e.g., user evaluations \nare correlated with higher citation counts). To identify topics which pertain to user evaluation we use \nthe ACM Classi.cation system: generic categories are typ\u00adically added manually, by paper authors, when \npapers are submitted for publication. Figure 3 shows the top 30 most commonly used descriptors from this \nsystem. Each is la\u00adbeled with the percentage of papers in our data set bearing that description that \ncontain a user evaluation. As expected, papers pertaining to theory and veri.cation contain fewer user \nevaluations, while subjects such as programming en\u00advironments contain many. Note that we do not wish \nto sug\u00adgest which topics should contain user evaluations, we only present this characterization for the \npurpose of controlled analysis. Nonetheless, there are many popular topics con\u00adtaining few user studies \nwhich we believe would likely ben- Figure 3. Percent of papers containing a user evaluation with estimated \nfalse negative rate, in each of the top 30 most commonly used ACM category descriptors. e.t from them. \nIn particular, we were surprised at the low percentage of papers under d.2.5 testing and debugging that \ncontained user evaluations. We have identi.ed 19 subject and sub-subject areas for which at least 10% \nof the associated papers contain human studies. We term these study candidate subjects and use this set \nin subsequent experiments. In all, 1,100 papers, or about 57% of the software engineering papers we studied, \nare annotated with one of these descriptors and are thus a member of the study candidate category. Finally, \nwe also identify those papers in the area of soft\u00adware engineering education research. Because human \nstud\u00adies in this area are very different from standard user evalua\u00adtions (e.g., research on enrolled \nstudents about the effective\u00adness of a particular teaching technique) we conservatively elect not to \nconsider them in our pool of software engineer\u00ading papers. We match the title, abstract, and subject \ndescrip\u00adtors against the term education in order to identify such papers. A total of 50 were located \nin our dataset. Longitudinal Trends We employ the classi.cation method\u00adology described in Section 3.2 \nto explore trends in rates of user evaluations over time. We characterize by rate because the total number \nof publications in software engineering at the conferences in our dataset has increased greatly over \nthe last decade (nearly doubling from 127 papers published in 2000 to 232 in 2010).  Figure 4. User \nevaluation subtype trends over time. Note the increase evaluations, especially since 2004. Figure 5. \nUser evaluation trends over time by conference. Note the steady increase in such studies at many software \nengineering conferences including ASE, ISSTA, and OOPSLA. We explore whether user evaluations are employed \nmore often today than ten years ago: H1: The number of papers with user evaluations is in\u00adcreasing, in \nboth absolute and relative terms. Supported. Figure 4 indicates a signi.cant positive trend in the num\u00adber \nof user evaluations in SE research. Where such studies comprised as few as 3.5% of papers in 2000, about \n18% of papers published in 2010 contain a user evaluation (an in\u00adcrease of over 500%). The number of \nstudy candidate papers (i.e., papers in subject areas with more than average human studies) have not \nincreased signi.cantly as a fraction of all papers during that interval. While Descriptive, Comparative, \nand Judgment type studies are preformed more frequently today, we .nd that Observation and Field type \nstudies have not shown signi.cant growth. In general, this would indicate a trend toward controlled and \nempirical studies over stud\u00adies which are primarily qualitative and often require direct access to professional \ndevelopers. As indicated by Figure 5 this trend is particularly strong in the case of ASE, ISSTA, and \nOOPSLA which each show large increases in user evaluations in recent years. ICSE show a lesser but signi.cant \npositive trend and FSE exhibits noisy but relatively constant level of user evaluations for the study \nperiod.  4.2 Bene.ts Having observed that use of user evaluations is on the rise, we now explore the \nquestion of why this is the case. In par\u00adticular, we seek to qualify and quantify some of the potential \nbene.ts of user evaluations in software engineering research by exploring correlations with common in.uence \nmetrics. We test whether the presence of such studies show a signif\u00adicant correlation with citation count \n(as reported by ACM Digital Library), best paper awards, and selectivity of con\u00adference. While none of \nthese metrics are precise indications of quality, they are used throughout the research community as \nproxies when evaluating the impact of work and are thus important even in isolation. Bene.t: Impact H2: \nPapers with user evaluations have higher citation counts. Rejected. We .rst investigate citation rates \nbetween papers with and without a user evaluation (as predicted by our auto\u00admated model). Figure 6 shows \nthat, since 2002, papers with such studies have usually been cited slightly more frequently than papers \nwithout such studies. However, this difference is small and not statistically signi.cant.   H3: Among \nhigher-impact papers, papers with user eval\u00aduations have higher citation counts. Supported. In Figure \n9 we tease apart user evaluations, both by type of evaluation, and also by citation percentile (i.e., \nwhere the paper ranks among other papers by citation count). We mea\u00adsure citations per year so as not \nto bias our study toward older papers. We restrict attention to study candidate pa\u00adpers (without this \nrestriction, the trend we observe here is less signi.cant). Also, we do not include papers written in \n2010, since they have not been published long enough to be cited signi.cantly. Figure 9 shows that while \npapers that are not cited often (i.e., the 25th percentile) show no signi.cant difference with respect \nto whether they contain a user eval\u00aduation, higher-impact papers (i.e., 50th and 75th percentile papers) \nshow an appreciable and signi.cant difference. For example, among high-impact papers (75th percentile), \npa\u00adpers containing a user evaluation are cited 3.9 times per year, while papers without (the No UE bullet) \nare cited an av\u00aderage of 2.8 times per year. This difference is statistically signi.cant with t-test \np< 0.01. To put it another way, while papers without a user evaluation were ranked as the third most \nwidely cited amongst 25th percentile papers, they are ranked as least cited amongst 75th percentile paper. \nWe conclude then that the presence of a user evaluation, in and of itself, is not suf.cient to correlate \nwith a paper s impact. However, widely cited papers are often viewed as having signi.cant impact, and \namong such papers the pres\u00adence of a user study does correlate with citation count in a signi.cant manner. \nWe also note that light-weight Judgment and Descriptive studies, which are generally less expensive to \nconduct than many other types of studies, consistently rate high by this metric. We now consider the \nrelationship between of user evalu\u00adations and award-winning research. We mined the history of ACM Distinguished \nPaper awards, .nding 75 papers in our database associated with such an award. Of those, 11 con\u00adtain user \nevaluations. After controlling for study candidate papers (see Section 4.1) we .nd that those papers \ncontaining a user evaluation are about 30% more likely, on average, to win a best paper award than other \npapers (Figure 8). Bene.t: Selectivity of Publication Venue H4: Highly selective conferences tend to \npublish a larger proportion of papers containing user evaluations. Sup\u00adported. Finally, we consider conference \nacceptance rate as third proxy for impact. Figure 7 shows that mean acceptance rate is negatively correlated \nwith the rate of user evaluations, in\u00addicating that the most selective conferences publish a higher proportion \nof papers containing these studies.  Figure 9. The number of citations per year since publication for \n5 types of user evaluations as well as papers without studies di\u00advided into 3 percentiles. Note that \nfor paper in the 25th percentile of citations (papers which are cited relatively infrequently) the pres\u00adence \nof user evaluations has little impact on citation rate. However, for 75th percentile papers (those which \nare cited relatively often), papers with user evaluations are cited much more frequently than those without \nsuch studies. Bene.t: Insights 91% of survey participants who had per\u00adformed a user evaluation agreed \nthat they gained insights from performing a user evaluation which they may not oth\u00aderwise have had. We \nfound that participants who have per\u00adformed a user evaluation are more likely to agree that the bene.ts \nof user evaluation generally outweigh the costs: 80% of participants who had performed a user evaluation \nagreed, compared to 50% of those who have not. Most par\u00adticipants, even those who had not conducted a \nuser study, agreed that a user evaluation increases the impact of a paper (82%), and that an appropriately \ndesigned user evaluation contributes strongly to the likelihood of publication in a ma\u00adjor conference \n(76%). However, participants who have per\u00adformed a user evaluation before were more likely to strongly \nagree to these statements. The above differences are all sta\u00adtistically signi.cant, with p<.003 in all \ncases.  4.3 Barriers We have observed that user evaluations are correlated with a variety of important \nin.uence metrics. Nonetheless, histor\u00adically only about 10% of software engineering research pa\u00adpers \ncontain such studies. To better understand this discon\u00adnect we now investigate the barriers to user evaluations. \nWe employ survey data to characterize perceived barriers and then use data about actual papers to hypothesize \nways that researchers can mitigate some of these barriers. Most survey participants (84%) agreed that \nuser evalua\u00adtion is dif.cult. However, there was not a statistically signif\u00adicant difference between \nwhether participants had performed a user evaluation and their response to this question. Instead of \na general perception of dif.culty, there are speci.c bar\u00adriers which prevent software engineering researchers \nfrom engaging in user evaluations. The .rst step in overcoming barriers is identifying them. Through \nthe survey we identi.ed .ve major and four mi\u00adnor potential barriers for software engineering researchers \nin performing user evaluations (Figure 10). About 20% of par\u00adticipants cited that there was no barrier \nto them performing a user evaluation; this percentage is similar between partic\u00adipants who had or had \nnot performed a user evaluation in the past. In the following subsections, we will describe the most \nprevalent barriers and discuss some mitigating factors. We note that almost 25% of participants who had \nnever per\u00adformed a user evaluation identi.ed the fact that they did not know how to perform such an evaluation \nas a barrier to us\u00ading user evaluation in their research. This highlights the need for more resources \nrelated to performing user evaluations in software engineering research. Perceived Barrier: Recruiting \nOur survey respondents in\u00addicated that they perceive that user evaluations are dif.cult to conduct for \na variety of reasons, but especially because of concerns related to recruiting. Almost 60% of participants \ncited recruiting as a barrier to employing user evaluation in their own research. Unfortunately, we noticed \nthat very few of the user evaluation papers we read and annotated discuss how the authors handled participant \nrecruitment. Perceived Barrier: Experimental Design and Time The second two most prevalent barriers were \nfeeling that per\u00adforming a user evaluation takes too long (44% of partici\u00adpants), and not being sure \nhow phrase the research question (31%) in order to design a user evaluation. It is very dif.cult to measure \nhow long studies took to administer from read\u00ading the high-level study description in a research paper; \nthis information is typically not included. However, we believe that our classi.cation scheme does provide \nsome indication of time commitment in that task-based evaluations (Obser\u00advation, Comparative, Field) \nare often more time consuming than non task-based evaluations (Descriptive and Judgment).  Perceived \nBarrier: Institutional Review Board (IRB) An institutional review board (or independent ethics committee \nor ethical review board) is a committee that reviews and oversees studies involving human subjects to \nensure they are ethical and regulatory. In the United States, they are re\u00adlated to the National Research \nAct of 1974. Only one par\u00adticipant who had not performed a user evaluation identi.ed the institutional \nreview board (IRB) as a barrier, compared with 18 participants who had previous user evaluation ex\u00adperience. \nWe believe that many researchers without experi\u00adence using human subjects do not know about the role \nof the IRB in such research. Unfortunately, software engineering researchers rarely mention the IRB when \ndescribing studies, or discuss any challenges in getting IRB approval. We lo\u00adcated 14 papers in the corpus \nthat mentioned the IRB, but 12 out of those 14 were published at CHI. Other Perceived Barriers Our survey \nalso included a free\u00adform text .eld for describing barriers. Three major addi\u00adtional types of barriers \nwere mentioned frequently: 1. Reviewer expectations Reviewers who do not do this type of research, yet \nhave unrealistic expectations on those who do (as evidenced by their review comments). Survey Participant \n2. Dealing with biases User evaluations are dif.cult, and can be subject to bias and/or small numbers. \nAs such, they are not always appreciated ... Survey Participant 3. Interpreting the results It seems \ndif.cult to do in a meaningful way. Survey Participant Mitigating Barriers The two largest barriers identi.ed \nwere recruitment and the time commitment. To address these barriers, we investigate the type of user \nevaluations that par\u00adticipants employed in software engineering research papers and identify studies \nemploying lightweight methodologies. In this section we investigate the populations typically re\u00adcruited \nfor user evaluations and characterize them by stu\u00addents vs practitioners and by number of participants. \nWe conjecture that, in general, smaller studies are easier to re\u00adcruit for, as are studies of students \nwho are typically ac\u00adcessible for academic researchers. We also conjecture that smaller studies take \nless time to administer. Of course, many other factors can substantially in.uence the time commit\u00adment \nrequired for a user evaluation. To explore this notion more deeply, we also investigate the roll of study \ntypes and artifact types. In particular, we conjecture that some types of studies (e.g., controlled studies \ninvolving real artifacts) can be more time consuming to conduct than some other types (e.g., judgment \nstudies with arti.cial or even isolated arti\u00adfacts).  H5: Large numbers of professional developers are \nneeded for an effective user evaluation. Rejected. In Figure 11 we aggregate data on user evaluations. \nWe .nd that studies are conducted with a variety of ranges and with students and practitioners at about \nequal frequency. Al\u00adthough students may not always be appropriate subjects [22], they may be suitable \nsubjects for many areas [25]. Notably, a majority of studies involved fewer than 20 participants and \nstudies with less than ten are not uncommon. This .nding is validated by our survey data in which the \nmedian number of subjects reported for the last user evaluation performed was about 20.  Even a modest \nuser evaluation (e.g. watch three friends use your software) is tremendously insightful. Just do it! \nSurvey Participant Since studies with few participants are common, the ques\u00adtion then becomes whether \nthese small studies are as impact\u00adful as larger ones. Figure 12 shows that, excluding the largest studies \n(those with more than 45 participants) average cita\u00adtion count actually exhibits a negative correlation \nwith par\u00adticipant count; small controlled studies may be as valuable as larger studies (at least from \na citation correlation stand\u00adpoint). H6: Heavyweight techniques are needed for an effective user evaluation. \nRejected. Study type can play a signi.cant roll in the dif.culty of experimental design and time commitment. \nFor example, if one wishes to evaluate the utility of an engineering tool one might choose between several \nkinds of studies. Heavyweight options include: Comparative Participants use the tool to preform some \nrepresentative task in a realistic setting. Their perfor\u00admance is compared to a control group who are \nnot given the tool.  Observation Participants use the tool to preform some representative task in a \nrealistic setting. Their perfor\u00admance is observed.  Field Real developers adopt the tool into their \nwork\u00ad.ow.  Lightweight options include, for example: Judgment Participants are shown sample output \nof the tool and asked whenever they think it would be useful.  Descriptive Participants are asked whether \na tool matching the description of the tool in question would likely be useful to them.   Figure 13 \nshows the percentage of survey participants who had performed a user evaluation for each of these cat\u00adegories. \nWe also asked participants to list the methodolo\u00adgies employed in their last user evaluation; the results \nare shown in Figure 14. The results support the hypothesis that heavyweight studies are not always required: \nfor example, interviews, questionnaires, and web surveys are commonly employed. From this data we surmise \nthat there exists many popular means of conducting user studies which do not nec\u00adessarily require a large \ntime commitment. H7: Heavyweight user evaluations have higher citation counts on average. Rejected. We \n.nd that the more lightweight study categories (Judg\u00adment and Descriptive) are actually often correlated \nwith higher average citation counts (Figure 9). We also explore artifacts associated with studies. Figure \n15 shows the per\u00adcentage of each type of study conducted with real, arti.cial, isolated, and no speci.c \nartifact. Complete descriptions of artifact types can be found in Section 3.1. We .nd that user  Figure \n16. Average number of citations per year since publica\u00adtion for papers containing studies incorporating \neach of four artifact types. Error bars indicate standard deviation. A two-tailed T-test suggests that \nthere is no signi.cant difference between the mean ci\u00adtation rate of studies utilizing real artifacts \nand isolated ones. There is a signi.cant difference between real and arti.cial as well as real and no \nspeci.c. evaluations are not limited to real projects. On the contrary, researchers often use arti.cial \nor isolated artifacts when con\u00adducting studies. As compared with real artifacts, these can often allow \nresearchers to more easily translate research questions into successful experiments. For example, rather \nthan a study in which participants must become familiar with a real (and often complex) system to preform \nsome task of interest, one might instead elect to conduct an experiment where the task artifact is arti.cially \nsimpli.ed. While designing an arti.cial project may take time upfront, it can have several important \nadvantages. For example, it may reduce training time, simplify recruiting, and, perhaps more importantly, \nit can allow the researcher greater control over confounding factors. Moreover, Figure 16 indicates that \nthese artifacts actually are correlated with greater numbers of citations, indicating that impact may \nnot need to be sacri.ced for this type of experimental control. Finally, for more information on performing \nuser evalu\u00adations, the reader is referred to a variety of books [16], ar\u00adticles [14, 15, 18, 23], and \ncards [11] to help in deciding between different evaluation methodologies. 5. Threats to Validity Internal \nvalidity. In this study we identi.ed and explored a number of correlations between user evaluations and \nex\u00adternal metrics of quality and impact. It is important to note, however, that we do not know the extent \nto which such corre\u00adlations indicate causal relationships. Indeed, there are many factors which contribute \nto, say, the citation count of a pa\u00adper. We cannot say that simply including a user evaluation is suf.cient \nto yield a greater citation count. Nonetheless, qual\u00aditative feedback from our survey would tend to support \nsuch a claim. External Validity. We have endeavored to mitigate threats to external validity by studying \na large sample of papers from selective and respected publication venues. We miti\u00adgate potential bias \nin our annotation methodology by bor\u00adrowing from previous rubrics where possible and by using two annotators \nto develop a consistent classi.cation scheme. Intentional Validity. In this paper we used a number of \nproxy metrics for paper impact and quality, including selec\u00adtivity of publication venue, citation count, \nand awards. It is important to note that none of these metrics measure paper impact or quality directly. \nGroundbreaking research, for ex\u00adample, may not be cited often simply because few active re\u00adsearchers \nare working on related topics. The metrics were selected primarily because they are objective and are \nknown to carry considerable weight, for better or for worse, in the research community. In other words, \nthese metrics are valu\u00adable independent of whether they are truly reliable estimates of quality research \n(an important question that is beyond the scope of this paper). 6. Conclusion User evaluations remain \na critical pillar of software engi\u00adneering research, but their associated bene.ts and barriers are imperfectly \nunderstood. Using a corpus of over 3,000 pa\u00adpers spanning ten years, we studied subtypes of user evalua\u00adtions \n(e.g., coding tasks vs. questionnaires), and related user evaluations to paper topics (e.g., debugging \nvs. technology transfer). We identi.ed the 19 study candidate subject ar\u00adeas most likely to contain user \nevaluations and found that the number of papers with user evaluations is increasing, both in absolute \nand relative terms. We also performed a survey of over 100 researchers from over 40 different universities \nand labs, identifying perceived barriers to performing user eval\u00aduations. With all papers considered, \nthose with user evaluations do not have higher citation counts overall. However, when attention is restricted \nto highly-cited works, user evaluations matter: for example, among the top quartile of papers by citation \ncount, papers with user evaluations are cited 40% more often than papers without. Highly-selective confer\u00adences \naccept a larger proportion of papers with user evalua\u00adtions than do less-selective conferences. We identi.ed \nnine concrete barriers researchers identify to performing user evaluations, with recruitment as the most \ncommon barrier. Promisingly for resource-strapped investigators, large num\u00adbers of professional developers \nand real-world projects are not required to perform highly-cited user evaluations. Acknowledgments The \nauthors are indebted to Andrew Begel for insightful conversa\u00adtions and substantial commentary on an early \ndraft of this work. We thank Craig Anslow for suggested improvements to a later draft. We would also \nlike to thank all of our survey responders for participating. The authors gratefully acknowledge the \nsupport of NSF grants CCF 0954024 and CCF 0905373, AFOSR MURI grant FA9550-07-1-0532, AFOSR grant FA8750-11-2-0039, \nand DARPA grant FA8750-11-2-0039.  References [1] J. W. Backus. The IBM 701 Speedcoding system. Journal \nof the ACM, 1:4 6, January 1954. [2] V. Basili, R. Selby Jr, and D. Hutchens. Experimentation in software \nengineering. In P. Oman and S. L. P.eeger, editors, Applying Software Metrics. 1997. [3] I. Campbell. \nChi-squared and Fisher-Irwin tests of two-by\u00adtwo tables with small sample recommendations. Statistics \nin Medicine, 26:3661 3675, 2007. [4] T. Couronne, M. Zelkowitz, and D. Wallace. Experimental validation \nin software engineering. Information and Software Technology, 39(11):735 743, 1997. [5] I. Deligiannis, \nM. Shepperd, S. Webster, and M. Roumeliotis. A review of experimental investigations into object-oriented \ntechnology. Empirical Software Engineering, 7(3):193 231, 2002. [6] T. Dyb\u00b0a and T. Dings\u00f8yr. Empirical \nstudies of agile software development: A systematic review. Information and Software Technology, 50(9-10):833 \n859, 2008. [7] A. Egyed. Instant consistency checking for the UML. In International Conference on Software \nEngineering, 2006. [8] R. Glass, I. Vessey, and V. Ramesh. Research in software engineering: an analysis \nof the literature. Information and Software Technology, 44(8):491 506, 2002. [9] S. Hanenberg. Faith, \nhope, and love: an essay on software science s neglect of human factors. In Object-Oriented Pro\u00adgramming, \nSystems, Languages, and Applications, 2010. [10] M. Host, C. Wohlin, and T. Thelin. Experimental context \nclas\u00adsi.cation: incentives and experience of subjects. In Interna\u00adtional Conference on Software Engineering, \n2005. [11] IDEO. Method Cards: 51 Ways to Inspire Design. William Stout, 2003. [12] M. J\u00f8rgensen. A review \nof studies on expert estimation of software development effort. Journal of Systems and Soft\u00adware, 70(1-2):37 \n60, 2004. [13] N. Juristo, A. Moreno, and S. Vegas. Reviewing 25 years of testing technique experiments. \nEmpirical Software Engineer\u00ading, 9(1):7 44, 2004. [14] B. Kitchenham. Evaluating software engineering \nmethods and tools. Part 1: the evaluation context and evaluation methods. ACM SIGSOFT Software Engineering \nNotes, 21(1):11 14, 1996. [15] H. Lam, E. Bertini, P. Isenberg, C. Plaisant, and S. Carpen\u00addale. Seven \nguiding scenarios for information visualization evaluation. Technical Report 2011-992-04, University \nof Cal\u00adgary, 2011. [16] J. Lazar, J. H. Feng, and H. Hochheiser. Research Methods in Human Computer Interaction. \nWiley, 2010. [17] J. Lung, J. Aranda, S. M. Easterbrook, and G. V. Wilson. On the dif.culty of replicating \nhuman subjects studies in software engineering. In International Conference on Software Engi\u00adneering, \n2008. [18] J. McGrath. Methodology matters: Doing research in the be\u00adhavioral and social sciences. In \nHuman-computer interaction, pages 152 169. Morgan Kaufmann Publishers Inc., 1995. [19] J. R. Quinlan. \nC4.5: programs for machine learning. Morgan Kaufmann Publishers Inc., 1993. [20] M. Robnik-. Theoretical \nand em- Sikonja and I. Kononenko. pirical analysis of ReliefF and RReliefF. Machine Learning, 53:23 69, \nOctober 2003. [21] M. Shaw. Writing good software engineering research papers: minitutorial. In International \nConference on Software Engi\u00adneering, 2003. [22] D. Sjoberg, B. Anda, E. Arisholm, T. Dyba, M. Jorgensen, \nA. Karahasanovic, E. Koren, and M. Vok\u00b4ac. Conducting realistic experiments in software engineering. \nIn Empirical Software Engineering and Measurement, 2003. [23] D. Sjoberg, T. Dyba, and M. Jorgensen. \nThe future of empir\u00adical methods in software engineering research. In Future of Software Engineering \nWorkshop (FoSER), 2007. [24] D. Sj\u00f8eberg, J. Hannay, O. Hansen, V. Kampenes, A. Kara\u00adhasanovic, N.-K. \nLiborg, and A. Rekdal. A survey of con\u00adtrolled experiments in software engineering. IEEE Transac\u00adtions \non Software Engineering, 31(9):733 753, 2005. [25] M. Svahnberg, A. Aurum, and C. Wohlin. Using students \nas subjects-an empirical evaluation. In Empirical Software Engineering and Measurement, 2008. [26] L. \nTeo and B. John. Cogtool-Explorer: towards a tool for predicting user interaction. In Conference on Human \nfactors In computing systems (CHI), 2008. [27] W. Tichy, P. Lukowicz, L. Prechelt, and E. Heinz. Experi\u00admental \nevaluation in computer science: A quantitative study. Journal of Systems and Software, 28(1):9 18, 1995. \n[28] M. Zelkowitz. Techniques for Empirical validation. In V. Basili, D. Rombach, K. Schneider, B. Kitchenham, \nD. Pfahl, and R. Selby, editors, Empirical Software Engineer\u00ading Issues. Critical Assessment and Future \nDirections, pages 4 9. Springer, 2007.   \n\t\t\t", "proc_id": "2048066", "abstract": "<p>In this paper, we identify trends about, benefits from, and barriers to performing user evaluations in software engineering research. From a corpus of over 3,000 papers spanning ten years, we report on various subtypes of user evaluations (e.g., coding tasks vs. questionnaires) and relate user evaluations to paper topics (e.g., debugging vs. technology transfer). We identify the external measures of impact, such as best paper awards and citation counts, that are correlated with the presence of user evaluations. We complement this with a survey of over 100 researchers from over 40 different universities and labs in which we identify a set of perceived barriers to performing user evaluations.</p>", "authors": [{"name": "Raymond P.L. Buse", "author_profile_id": "81363602189", "affiliation": "University of Virginia, Charlottesville, VA, USA", "person_id": "P2839242", "email_address": "buse@cs.virginia.edu", "orcid_id": ""}, {"name": "Caitlin Sadowski", "author_profile_id": "81418599954", "affiliation": "University of California Santa Cruz, Santa Cruz, CA, USA", "person_id": "P2839243", "email_address": "supertri@cs.ucsc.edu", "orcid_id": ""}, {"name": "Westley Weimer", "author_profile_id": "81100631608", "affiliation": "University of Virginia, Charlottesville, VA, USA", "person_id": "P2839244", "email_address": "weimer@cs.virginia.edu", "orcid_id": ""}], "doi_number": "10.1145/2048066.2048117", "year": "2011", "article_id": "2048117", "conference": "OOPSLA", "title": "Benefits and barriers of user evaluation in software engineering research", "url": "http://dl.acm.org/citation.cfm?id=2048117"}