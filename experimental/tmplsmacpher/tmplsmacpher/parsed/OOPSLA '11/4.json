{"article_publication_date": "10-22-2011", "fulltext": "\n Testing Atomicity of Composed Concurrent Operations Ohad Shacham Nathan Bronson Alex Aiken Tel Aviv \nUniversity Stanford University Stanford University ohad.shacham@cs.tau.ac.il nbronson@cs.stanford.edu \naiken@cs.stanford.edu Mooly Sagiv Martin Vechev Eran Yahav * Tel Aviv University ETH Zurich and IBM Research \nTechnion msagiv@post.tau.ac.il martin.vechev@gmail.com yahave@cs.technion.ac.il Abstract We address \nthe problem of testing atomicity of composed concurrent operations. Concurrent libraries help program\u00admers \nexploit parallel hardware by providing scalable concur\u00adrent operations with the illusion that each operation \nis exe\u00adcuted atomically. However, client code often needs to com\u00adpose atomic operations in such a way \nthat the resulting com\u00adposite operation is also atomic while preserving scalability. We present a novel \ntechnique for testing the atomicity of client code composing scalable concurrent operations. The challenge \nin testing this kind of client code is that a bug may occur very rarely and only on a particular interleaving \nwith a speci.c thread con.guration. Our technique is based on modular testing of client code in the presence \nof an adver\u00adsarial environment; we use commutativity speci.cations to drastically reduce the number of \nexecutions explored to de\u00adtect a bug. We implemented our approach in a tool called COLT, and evaluated \nits effectiveness on a range of 51 real\u00adworld concurrent Java programs. Using COLT, we found 56 atomicity \nviolations in Apache Tomcat, Cassandra, MyFaces Trinidad, and other applications. Categories and Subject \nDescriptors D.2.5 [Testing and Debugging]; D.1.3 [Concurrent Programming] General Terms Veri.cation Keywords \nconcurrency, linearizability, testing, composed operations, collections * Deloro Fellow Permission to \nmake digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior speci.c permission and/or a fee. OOPSLA 11, October \n22 27, 2011, Portland, Oregon, USA. Copyright c &#38;#169; 2011 ACM 978-1-4503-0940-0/11/10. . . $10.00 \n1. Introduction Concurrent data structures are becoming critical components of many systems [27] but \nare notoriously hard to get right (e.g., [11]). To shield programmers from the complexity of concurrent \ndata structures, modern languages hide their im\u00adplementations in libraries (e.g., [1, 3, 19]). Data structures \nprovided by the library usually guarantee that their opera\u00adtions are atomic. That is, every individual \noperation appears to take place instantaneously at some point between its invo\u00adcation and its return, \nand the implementation details of the operation can be ignored by the programmer. Custom Concurrent Data \nStructures While the library provides basic concurrent data structures, client code often needs speci.c \nconcurrent data structures supporting addi\u00adtional operations. Such custom operations may combine sev\u00aderal \noperations of the concurrent library. The main challenge when composing atomic operations is to guarantee \nthat the composed operation is also atomic. It is important to note that programmers usually compose \nthe operations of underlying concurrent data structures with\u00adout using a wrapping lock, because guaranteeing \natomicity of the composed operation using a wrapping lock requires wrapping every other operation of \nthe concurrent data struc\u00adture with the same lock. Besides the major code modi.ca\u00adtions entailed by this \napproach, it severely limits concur\u00adrency and defeats the original purpose of using a concurrent data \nstructure. For example, Figure 1, taken from OpenJDK [4], imple\u00adments an increment operation for a concurrent \nhistogram (as a map of counters). The histogram is implemented as a subclass of ConcurrentHashMap and \nother map opera\u00adtions (e.g., remove, get, put) can be performed directly, and concurrently, on the map \nby client code. The incre\u00adment operation does not use any locks (for the reasons men\u00adtioned above), and \ninstead guarantees atomicity using an optimistic concurrency loop, using the (atomic) operations putIfAbsent \nand replace provided by the underlying  1 void inc(Class<?> key) { 2 for (;;) { 3 Integer i = get(key); \n4 if (i == null) { 5 if (putIfAbsent(key, 1) == null) //@LP if succeeds 6 return ; 7 } else { 8 if (replace(key, \ni, i + 1)) //@LP if succeeds 9 return ; 10 } 11 } 12 } Figure 1. A correct concurrent increment operation \nfor a map of concurrent counters, implemented using Concurren\u00adtHashMap operations (from OpenJDK [4]). \nConcurrentHashMap. While the code in Figure 1 does guarantee atomicity, we have found that many other \nopen source clients combine atomic operations of underlying con\u00adcurrent data structures in a non-atomic \nway. Checking Atomicity Given the dif.culty of writing com\u00adposed atomic operations, it is desirable to \nprovide program\u00admers with an automatic technique for checking atomic\u00adity. Existing approaches to dynamic \natomicity checking, such as Velodrome [15], identify violations of con.ict\u00adserializability [7] using \nan online computation of the happens\u00adbefore relation over program executions. Unfortunately, the con.ict-serializability \nproperty is in\u00adappropriate for concurrent data structures since correct data structures often violate \nit. For example, the increment code of Figure 1 is not con.ict-serializable. Indeed, many of the programs \nthat we investigated are not con.ict-serializable since they contain two memory accesses in the same \nmethod with an intervening write. (In Section 5, we show that for all of our benchmarks, correct methods \nwould have been rejected by a con.ict-serializability checker.) Therefore, in\u00adstead of using con.ict-serializability \nas the correctness crite\u00adrion, which results in many false alarms, we focus on check\u00ading linearizability \n[21] of the concurrent data structure. The Challenge Given an application, our goal is to test whether \nits composed concurrent operations might violate atomicity. In the rest of this paper, we refer to a \nmethod implementing the composed concurrent operation as the method under test (MUT). Since atomicity \nviolations often depend on speci.c thread con.gurations and schedules, very few program executions may \nexhibit the error. In fact, the biggest challenge for dy\u00adnamic atomicity checking tools such as Velodrome \n[15] and Lineup [8] is identifying the thread con.guration and sched\u00adule that expose an error. In our \nexperience, exposing a single bug in TOMCAT using a non-modular approach took about one week of work \nand required us to manually write an input test and introduce scheduling bias. COLT addresses the challenge \nof identifying thread con\u00ad.gurations and scheduling by: (i) modularly checking that composed operations \nare linearizable w.r.t. an open environ\u00adment relative to the collection; (ii) guiding execution by in\u00adterleaving \nnon-commutative collection operations. Modular Checking of Linearizability Rather than check\u00ading the \napplication as a whole, COLT tests it under an open environment with respect to collection operations. \nThe open environment over-approximates any possible manipulation of the environment on the collection \nused by the MUT. This allows us to test the MUT in an adversarial environment rather than trying to reproduce \nadversarial testing conditions within the original application. However, since COLT covers interleaving \nthat may not oc\u00adcur for the MUT in the context of its application, it might produce false alarms in cases \nwhere application-speci.c in\u00advariants create a restricted environment in which the MUT is actually atomic. \nFortunately, we found that such situa\u00adtions are rare, and on real programs COLT has a very low false \nalarm rate. Intuitively, this happy situation arises be\u00adcause operations can usually be executed in an \narbitrary or\u00adder without violating linearizabilty. Furthermore, checking the code for linearizability \nagainst all possible environments guarantees that the code is robust to future extensions of the client. \nIndeed, our experience has been that programmers .x linearization bugs discovered by COLT even if such \nbugs are unlikely or impossible in the current application, because they are concerned that future, apparently \nunrelated changes might trigger these latent problems. Adversarial Execution Guided by Non-Commutativity \nOur goal is to test the atomicity of the MUT. We would like to avoid exploring executions that result \nin collection values already observed by the MUT in earlier executions. Indeed, COLT aims for every newly \nexplored execution path to yield a new collection result for the MUT. This can be viewed as a kind of \npartial order reduction (e.g., [18]) where commutativity is checked at the level of atomic collection \noperations. Main Contributions The main contributions of this paper are: We present an approach for \nmodular testing of lineariz\u00adability for composed concurrent operations.  We demonstrate our approach \nin the context of Java con\u00adcurrent collections. We have implemented a dynamic tool called COLT that checks \nthe atomicity of composed con\u00adcurrent collection operations.  We use collection non-commutativity speci.cations \nto direct exploration. This signi.cantly increases the bug hunting capabilities of our tool, as con.rmed \nby our ex\u00adperimental results at Section 5. We show that this opti\u00admization cannot .lter out linearizability \nviolations.  We show that COLT is effective in detecting real bugs while maintaining a very low rate \nof false alarms. Using COLT, we were able to identify 56 linearizability viola\u00adtions in Apache Tomcat, \nCassandra, MyFaces Trinidad,   1 Attribute removeAttribute ( String name) {1 Attribute removeAttribute \n( String name) { 2 Attribute val 2 Attribute val = null ; 3 = attr.get(name) ; //@LP val ==null 3 synchronized(attr) \n{ 4 if (val != null) {4 found = attr .containsKey(name) ; 5 val = attr.remove(name); //@LP 5 if (found) \n{ 6}6 val = attr.get(name); 7 return val ; 7 attr.remove(name); 8}8} 9} (c) 10 return val ; 11 } (a) \n (b) Figure 2. (a) An erroneous code from Apache Tomcat [2] version 5 where attr is a HashMap. In version \n6 the attr implementation was changed to a ConcurrentHashMap and synchronized(attr) from line 3 was removed. \n(b) is an execution that shows an atomicity violation in removeAttribute at Tomcat version 6. (c) is \na .xed linearizable version of removeAttribute. Adobe BlazeDS, as well as in other real life applications. \nSome of these violations expose real bugs, while other violations represent potential bugs in future \nclient imple\u00admentations. 2. Motivation Figure 2a shows a composed operation taken from Apache Tomcat \n[2] version 5. In this version attr is a sequen\u00adtial HashMap. The atomicity of the composed operation \nis guaranteed by wrapping the composed operation with synchronized(attr) block. The operation maintains \nthe invariant that removeAttribute returns either the value it removes from attr or null. In Tomcat version \n6, the developers decided to utilize .ne-grain concurrency, therefore, attr s implementation was changed \nfrom a HashMap to a ConcurrentHashMap and consequently the programmers decided to remove the synchronized(attr) \nblocks from the program as well as from line 3 of Figure 2a. This caused a bug which was identi.ed by \nCOLT. Hard to cover rarely-executed traces Figure 2b shows an execution of removeAttribute revealing \nthe invariant vi\u00adolation in the composed operation in Tomcat version 6. In this execution trace, thread \nT1 returns a value different than null even though it does not remove a value from attr, which breaks \nthe invariant. This violation occurs due to the remove(\"A\") operation by thread T2 occurring between \nthe get(\"A\") and the remove(\"A\") of thread T1. In order to reveal the violation, it is not enough that \nremove occurs at a speci.c point in the trace, but also that the collection opera\u00adtions all use same \nkey \"A\". This composed operation is not linearizable and COLT successfully detects the violation. We \nreported this bug to Tomcat s developers as well as the .x in Figure 2c and they acknowledged the violation \nand accepted the .x. While COLT discovers this violation, tools that run the whole program are unlikely \nto succeed in particular, the above violation can only be triggered with a speci.c choice of keys. \n2.1 The Main Ideas in our Solution We now brie.y describe the main ideas in our tool. Composed Operations \nExtraction COLT includes a semi\u00adautomatic technique that signi.cantly helps the user in ex\u00adtracting composed \noperations out of whole applications. A simple static analysis identi.es methods that may invoke multiple \ncollection operations. These methods are returned to the user for further review. In some cases the static \nanal\u00adysis identi.es a composed operation inside a larger method and the user must manually extract and \ngenerate the MUT. Modular Checking of Linearizability COLT checks lin\u00adearizability [21] in a modular \nway by invoking one MUT at a time in an environment that performs arbitrary collec\u00adtion operations concurrently. \nAdversarial Execution Guided by Non-commutativity Due to the rare nature of many atomicity violations, \na dynamic tool for detecting atomicity violations must use some form of focused exploration to be effective. \nIn COLT, we use collec\u00adtion semantics to reduce the space of explored interleavings, without .ltering \nout interleavings leading to linearizability violations. As a .rst step, we assume that the underlying \ncollection implementation is linearizable and thus the internal repre\u00adsentation of the collection can \nbe abstracted away and col\u00adlection operations can be assumed to execute atomically. As done in [6], this \nallows us to consider interleavings at the level of collection operations, without considering interleav\u00adings \nof their internal operations. However, the key insight that we use is that before and after each collection \noperation op1 executed by the MUT, the environment chooses a col\u00adlection operation op2 to execute that \ndoes not commute with  1 V compute(K k) { 2 V val = m.get(k); 3 if (val == null) { 4 val = calculateVal(k); \n5 m.putIfAbsent(k, val); 6} 7 return m.get(k); 8}  (a) (b) Figure 3. (a) Non linearizable example, capturing \nbugs from Adobe BlazeDS and others; (b) sample executions of compute concurrently with a client running \narbitrary collection operations. Solid and dashed edges represent operations by compute and the client \nrespectively. op1. Note that scheduling operations only before every col\u00adlection operation of the MUT \nis insuf.cient, as this strategy can omit interleavings that lead to linearizability violations (see \nSection Section 4.5). Figure 3a shows an example of a non-linearizable method inspired by bugs from Adobe \nBlazeDS, Vo Urp and Ehcache-spring-annotations. The procedure compute(K k) uses an underling concurrent \ncollection to memoize the value computed by calcuateVal(k). When the value for a given key is cached \nin the collection it is re\u00adturned immediately; when the value for a given key is not available, it is \ncomputed and inserted into the collection. Figure 3b shows sample executions of compute(K k) running \nconcurrently with a general client that performs ar\u00adbitrary collection operations with arbitrary arguments. \nOper\u00adations of compute(K k) are shown with solid arrows, oper\u00adations of the general client are depicted \nwith dashed arrows. Out of the 8 sample executions in the .gure, only 1 exposes the atomicity violation; \nin practice a smaller fraction of ex\u00adecutions reveal the atomicity violation. As we show in Sec\u00adtion \n5, a random search of the space of executions fails to .nd even a single violation in practice. To trigger \nan atomicity violation in a MUT, particular collection operations, with a particular key, must interleave \nbetween the collection operations of the MUT. The set of client operations that can potentially trigger \nan atomicity violation in a point of execution in the MUT can be char\u00adacterized as operations that do \nnot commute with the col\u00adlection operation in the MUT. In the example, the execu\u00adtions that reveal the \natomicity violation are those in which a client operation affects the result of the m.get(k) in line \n7 of compute(K k). This suggests that an adversarial client should focus on scheduling an operation that \ndoes not com\u00admute with m.get(k) right before scheduling this MUT op\u00aderation. An example of an operation \nthat does not commute with m.get(k) is m.remove(k), as shown in the last exe\u00adcution of Figure 3b. 3. \nPreliminaries In this section we de.ne linearizability [21] and lineariza\u00adtion points, show two ways \nfor checking linearizability, and .nally de.ne a notion of commutativity [29]. 3.1 Linearizability Linearizability \n[21] is de.ned with respect to a sequential speci.cation (pre/post conditions). A concurrent object is \nlinearizable if each execution of its operations is equivalent to a legal sequential execution in which \nthe order between non-overlapping operations is preserved. An operation op is a pair of an invocation \nevent and a re\u00adsponse event. An invocation event is a triple (tid, op, args) where tid is the thread \nidenti.er, op is the operation identi\u00ad.er, and args are the arguments. Similarly, a response event is \na triple (tid, op, val) where tid and op are as just de.ned, and val is the value returned from the operation. \nFor an op\u00aderation op, we denote its invocation by inv(op) and its re\u00adsponse by res(op).A history is a \nsequence containing in\u00advoke and response events. A complete invocation of an oper\u00adation op is a history \nwith two events where the .rst event is inv(op) and the second event is res(op) A complete history is \na history without pending invocation events (that is, all in\u00advocation events have a matching response \nevent). A sequen\u00adtial history is one in which each invocation is a complete invocation. A thread subhistory, \nh|tid is the subsequence of all events in h with thread id tid. A sequential history h is le\u00adgal if it \nbelongs to the sequential speci.cation. Two histories Figure 4. Concurrent histories and possible sequential \nhis\u00adtories corresponding to them.  h1,h2 are equivalent when for every tid, h1|tid = h2|tid. An operation \nop1 precedes op2 in h, and write op1 <h op2, if res(op1) appears before inv(op2) in h. A history h is \nlinearizable when there exists an equivalent legal sequential history s, called a linearization, such \nthat for every two op\u00aderations op1, op2, if op1 <h op2 then op1 <s op2. That is, s is equivalent to h, \nand respects the global ordering of non\u00adoverlapping operations in h. Example Next we illustrate the concepts \nof linearizability and histories on a concurrent set data structure with methods add, remove, and contains. \nFigure 4 shows two concurrent histories H1 and H2, and a sequential history L1. All histories involve \ntwo threads invoking operations on a shared concurrent set. In the .gure, we abbreviate names of operations, \nand use a,r, and c, for add, remove, and contains, respectively. We use inv op(x) to denote the invocation \nof an operation op with an argument value x, and op/val to denote the response op with return value val. \nConsider the history H1. For now, ignore the + sym\u00adbols. In this history, add(4) is overlapping with \nopera\u00adtion remove(4), and add(7) overlaps contains(7). The history H1 is linearizable. We can .nd an \nequivalent sequential history that preserves the global order of non\u00adoverlapping operations. The history \nL1 is a possible lin\u00adearization of H1 (in general, a concurrent history may have multiple linearizations). \nIn contrast, the history H2 is non-linearizable. This is be\u00adcause remove(4) returns true (removal succeeded), \nand contains(4) that appears after remove(4) in H2 also returns true. Linearizability provides the illusion \nthat for each opera\u00adtion op, there exists a point between its invocation inv(op) and response res(op) \nin the history h where op appears to take effect instantaneously. This point is typically referred to \nas the linearization point lp(op) of the operation op. Given a concurrent history h, the (total) ordering \nbetween these points induces a linearization marked as lin(h). We refer to lin(h) as the reference history \nof h. Example Consider the history H1 of Figure 4, the + sym\u00adbols in the .gure denote the linearization \npoint in each oper\u00adation. The relative ordering between these points determines the order between overlapping \noperations, and therefore de\u00adtermines a unique linearization of H1, shown as L1. Checking Linearizability \nThere are two alternative ways to check linearizability [28]: (i) automatic linearization explore all \npermutations of a concurrent execution to .nd a valid linearization; (ii) lin\u00adearization points build \na linearization (lin(h)) on-the-.y during a concurrent execution, using linearization points. The .rst \ntechnique is fully automatic and checks whether there exists a linearization for the concurrent execution. \nThe second technique requires either user-provided linearization points or uses a heuristic for guessing \nlinearization points. This technique checks whether the concurrent execution is equivalent to a speci.c \nlegal sequential execution de.ned by the linearization points.  3.3 Commutativity Following [29], we \nsay that an operation op1 commutes with an operation op2 with respect to a complete history base, if \ns1 and s4 are complete invocations of op1, s2 and s3 are complete invocations of op2, and base\u00b7s1 \u00b7s3 \nand base\u00b7s2 \u00b7s4 are equivalent histories. Conversely, if base \u00b7 s1 \u00b7 s3 and base \u00b7 s2 \u00b7 s4 are not equivalent \nhistories then op1 and op2 does not commute with respect to base. Example An example of non-commutative \noperations is r(4) and c(4) in history L1 from Figure 4. These op\u00aderations are non-commutative with respect \nto the complete history a(4), because if the order between the two opera\u00adtions changes and c(4) is executed \nbefore r(4) then c(4) would instead return true. 4. Commutativity-Guided Checking COLT aims to check \nlinearizability in a modular fashion by checking for violations of a composed concurrent operation. In \nthis section we present our commutativity-based checking of linearizability. 4.1 General Setting COLT \nexploits the fact that collection operations are atomic and encapsulated (the state of the collection \nis only accessed through methods of the collection), which allows COLT to only explore interleavings \nbetween the (atomic) operations, without having to interleave the implementations of the op\u00aderations. \nSuch histories, consisting of atomic operations, can be generated by a single thread. Therefore, to test \nlinearizability, we generate histories via only two threads: one thread that runs the MUT, and another \n(adversarial) thread that runs arbitrary collection operations. The general technique used by COLT s \nenvironment contains three components:  Operation remove(k) get(k) putIfAbsent(k, v2) remove(k) get(k) \n= null get(k) = null true get(k) get(k) = null false get(k) = null putIfAbsent(k, v1) true get(k) = null \nget(k) = null Figure 5. Non-commutativity Speci.cation for Map. For simplicity, we assume that v1 and \nv2 are non-null. MUT CLIENT, which implements the thread running the MUT (MUT THREAD) and is responsible \nfor generating inputs for the MUT.  ENV CLIENT, which implements the thread running arbi\u00adtrary collection \noperations (ENV THREAD) and is respon\u00adsible for choosing collection operations together with their input \narguments.  SCHEDULER, which is responsible for schedulingthe ENV THREAD and the MUT THREAD.  Main \nQuestion The main question in building COLT is: How should we de.ne the behavior of MUT CLIENT, ENV CLIENT, \nand SCHEDULER to effectively .nd bugs?  4.2 Naive Approach The naive approach is to use randomness. \nHere, the MUT CLIENT would randomly select input values for the MUT, the ENV CLIENT would randomly select \ncollection operations together with their input arguments, and the SCHEDULER would randomly schedule \nbetween the MUT THREAD and the ENV THREAD. This approach is ineffective, as the space of executions using \nrandom values is huge, and only a few of these choices are likely to lead to executions that contain \nviolations. In\u00addeed, as shown in Section 5, the naive approach does not work well; the use of random \nchoices fails to expose even a single violation in realistic examples after hours of execu\u00adtion. Note \nthat even if we consider a systematic exploration of possible schedules (via say context-bounding [25]), \nwe will still not .nd many problematic behaviors unless we know the precise values with which to exercise \nthese sched\u00adules. 4.3 Commutativity Guided Scheduling The main insight behind our approach is to leverage \nthe commutativity of collection operations in order to de.ne the behaviors of ENV CLIENT and SCHEDULER. \nWith our approach, we do not produce histories that we know are certainly linearizable. This enables \nus to signi.cantly prune the massive search space of possible executions and focus on schedules that \ncan trigger violations. Commutativity Speci.cations A non-commutativity spec\u00adi.cation for some of the \nMap operations is shown in Table 5. Such speci.cations are easy to obtain from the speci.cation of Map \ns methods. The way to make use of this table is as follows: i) select an operation op1 from a column; \nii) select an operation op2 from a row; iii) pick a complete history base such that the state of the \ncollection at the end of base satis.es the condition in the box. Then, by the de.nition of commutativity, \nop1(k) and op2(k) do not commute from base. In the case where the box is false, it means the opera\u00adtions \nalways commute from any complete history base, i.e, they are never non-commutative. Conversely, when \nthe box is true, the operations never commute. That is, regardless of which complete history base is \nselected, the operations never commute with respect to base. General Approach It is possible to augment \nENV CLIENT and SCHEDULER to be commutativity-aware. That is, ENV CLIENT selects an operation with such \nkeys and values so that the operation does not commute with any of the preceding operations performed \nby MUT THREAD or with the operation that is about to be performed by the MUT THREAD. The SCHEDULER is \nresponsible for scheduling the ENV THREAD before and after each collection operation performed by the \nMUT THREAD. The intuition behind our approach is conceptually simple: if the ENV THREAD operation commutes \nwith all of the MUT THREAD s operations, then we can shift the ENV THREAD s operation to not interleave \nwith the MUT THREAD. That is, we can always produce a linearization of the concurrent his\u00adtory by simply \nmoving all of ENV THREAD s operations be\u00adfore or after the operations in MUT THREAD. Indeed, pro\u00adducing \nconcurrent histories that we know can always be lin\u00adearized is not useful for .nding linearizability \nviolations. In addition, we also augment the SCHEDULER so that un\u00adderlying collection operations are \nalways performed atom\u00adically. For instance, in Figure 3a, an operation such as remove() will be allowed \nto preempt compute(), yet we always schedule remove() to execute atomically, without preemption. This \nis because the underlying collection (ex\u00adcluding MUT) is already linearizable and due to the col\u00adlection \nencapsulation the MUT always accesses the col\u00adlection state through the underlying methods (i.e., get()) \nand hence the MUT cannot violate the linearizability of remove(). Implemented Approach In this work, \nwe implemented a simpler version of the more general approach. Rather than checking if a scheduled operation \ncommutes with all preced\u00ading operations performed by MUT THREAD, we only check whether the operation \ndoes not commute with the operation just performed by MUT THREAD or about to be performed by MUT THREAD. \nAlthough this approach may miss lineariz\u00adability violations, as we show later, it is still a very effective \napproach for discovering errors in practice.  4.4 One Thread Implementation Our approach uses two threads \nENV THREAD and MUT THREAD to detect linearizability violations. Using the (non) commutativity speci.cation \nof the map, we know that ENV THREAD may run operation op only before or after a map op\u00ad  1 void ENV( \noperation , args , map) { 2 if ( *){ 3 (nonCommutativeOp, args) = 4 getNonComOp(operation , args , map); \n 6 map.nonCommutativeOp(args); 7} 8} (a) 1 V compute(K k) { 2 ENV( get , k, m); 3 val = m.get(k); 4 \nENV( get , k, m); 5 if (val == null) { 6 val = calculateVal(k); 7 ENV( putIfAbsent , k, val , m); 8 \nm.putIfAbsent(k, val); 9 ENV( putIfAbsent , k, val , m); 10 } 11 ENV( get , k, m); 12 val = m.get(k); \n13 ENV( get , k, m); 14 return val ; 15 } (b) Figure 6. One thread instrumentation done by COLT for function \ncompute from Figure 3a. (a) shows function ENV imitating ENV THREAD and (b) shows the instrumentation \nof compute done by COLT. eration op' of MUT THREAD s.t. op and op' do not commute. Therefore, we can \nimplement both logical threads using a single actual thread as follows: given a MUT M we instru\u00adment \nM to include ENV THREAD operations before and after map operations of the MUT THREAD. Figure 6a shows \na method ENV, which takes an op\u00aderation (operation), its arguments (args), and a map. ENV imitates ENV \nTHREAD by make a non-deterministic choice whether to run an operation (given by getNonComOp) non-commutative \nto operation(args) at map. Method getNonComOp takes an operation, its arguments (args), and a map. This \nfunction returns one operation non commu\u00adtative to operation(args) at map. Figure 6b shows the instrumentation \ndone by COLT for function compute from Figure 3a. As Figure 6b shows, ENV is called before and after \nevery operation of m done by compute. This instrumented program uses one thread and imitates an environment \nthread running in parallel to compute.  4.5 Commutativity Guided Scheduling: An Example Let us illustrate \nhow our approach works on a simple exam\u00adple. Suppose that we would like to check the linearizability \nof the compute() operation from Figure 3a using a user\u00adprovided linearization point. The execution that \nwe will con\u00adstruct next is shown in Figure 7. The example shows a concurrent execution of the MUT THREAD \nand the ENV THREAD together with the concurrent history of the concurrent execution. In addition, the \nexample shows the REFERENCE THREAD representing the sequential run together with its corresponding sequential \nhistory. The execution starts when the MUT CLIENT selects an arbitrary key, say 7, for compute() and \nthe SCHEDULER invoked the MUT THREAD to perform its operations up to but not including putIfAbsent(7, \n14). Next, the SCHEDULER, using a non-commutativity speci.cation, de\u00adcides to run the ENV THREAD. The \nENV THREAD is in\u00advoked, and ENV CLIENT consults Table 5. To achieve non\u00adcommutativity with the next operation \nof compute(), i.e., putIfAbsent(7, 14), ENV CLIENT must also use the same key, i.e., key 7 (otherwise, \nif the keys are differ\u00adent, the operations will commute). From Table 5, we ob\u00adserve that putIfAbsent(7, \nv) does not commute with putIfAbsent(7, 14) when key 7 is not in the map (for any value of v). Therefore, \nENV CLIENT selects the opera\u00adtion putIfAbsent(7,12) for the ENV THREAD (the value 12 for v is picked \nrandomly). Since putIfAbsent(7,12) executes atomically, this is a linearization point for putIfAbsent(7,12), \nand hence we execute the REFERENCE THREAD that executes operation putIfAbsent(7,12) on the reference \nmap. This adds the .rst operation to the sequential history, and the results of the .rst operation putIfAbsent(7,12) \non the sequential and concurrent history are compared. In this case, the results are the same, i.e., \nnull, and therefore, a violation of linearizabil\u00adity is not yet detected. Next, the SCHEDULER lets the \nMUT THREAD run and the operation putIfAbsent(7, 14) operation is performed. Assuming that putIfAbsent(7,14) \nis a linearization point of compute(7), the reference implementation of compute is executed atomically \nand the compute operation (marked as c) is added to the sequential history with its re\u00adsult 12. Later, \ncompute(7) s result in the sequential history will be compared to compute(7) s result in the concurrent \nhistory (when it will be available). At this point, once again, the ENV THREAD is scheduled before get(7). \nAgain, ENV CLIENT consults Table 5, and it observes that remove(7) does not commute with get(7) as key \n7 is in the map at this point. Therefore, ENV CLIENT performs remove(7) after putIfAbsent(7,14) and be\u00adfore \nget(7). As remove(7) is executed atomically, this is a linearization point for remove(7) and hence we \nexecute the REFERENCE THREAD that executes remove(7) on the ref\u00aderence map. This adds an operation to \nthe sequential history (marked as r), and the results of the operation remove(7) on the sequential and \nconcurrent history are compared. In this case, the results are the same, i.e., 12, and therefore, a violation \nof linearizability is not yet detected. However, when the get(7) operation executes by the MUT THREAD \nand subsequently compute(7) returns, it will return the value null and the response and the result of \ncompute(7) is added to the concurrent history. When we compare the result of compute(7) on the concurrent \nand Figure 7. An example execution of our technique, testing method compute() from Figure 3a.  1 void \nadd(K k) { 2 val = m.get(k); // @LP (val != null) 3 if (val == null) { 4 m.put(k, 2); // @LP 5} 6} Figure \n8. An example showing that restricting ENV THREAD to perform operations only before map operation of \nthe MUT can miss linearizability violation. sequential histories, we obtain different results, i.e., \nnull vs. 12, triggering a linearizability violation. Scheduling environment operations only before MUT \nop\u00aderations Note that scheduling operations only before ev\u00adery collection operation of the MUT can sometimes \nlead to missing linearizability violations. Figure 8 shows a non\u00adlinearizable function add, where restricting \nENV THREAD to perform operations only before m operations performed by add would miss all non-linearizable \nexecutions of add. Function add receives an input key k and in case that k is not already in m adds k \nto m with the value 2. In this function the user provided two conditional linearization points. One at \nline 2, in case that val null and the second one at line = 4. Figure 9 shows an execution of add that \nreveals the lin\u00adearizability violation. All the violating executions of add are similar to the one shown \nat Figure 9. In this execution, the linearizability violation of add occurs due to the get opera\u00adtion \nthat occurs after the last operation of add. Therefore, re\u00adstricting the environment to perform operations \nonly before m operations performed by add would prune this violating execution of add. 5. Experimental \nEvaluation Inthissectionweexplain COLT simplementationanddemon\u00adstrate the effectiveness of COLT by evaluating \nit on a range Figure 9. An example execution of our technique, testing method add(K k) from Figure 8. \nNote that the linearizabil\u00adity violation revealed by the different return value of the get method occurs \nafter the last operation of add. of real-world applications. Using our approach we found a number of \nerrors, substantiating our hypothesis that pro\u00adgrammers often make incorrect assumptions when using concurrent \ncollections. Together with the error, we reported a suggested .x for that error to the development team. \nIn many cases, our .xes were accepted by the development team and incorporated in the application. All \nof our exper\u00adiments were carried out using 64-bit Linux with 8GB of RAM running on a dual-core, hyper-threaded \n2.4Ghz AMD Opteron. 5.1 Implementation An outline of COLT s implementation is shown in Figure 10. The \nprogrammer provides a multithreaded program to the MUT Extractor. The MUT Extractor uses a simple static \nanalysis that identi.es methods that may invoke multiple collection operations. These methods are returned \nto the user for review. In some cases the MUT Extractor identi.es a specialized concurrent data structure \noperation inside a large method and the user needs to manually extract and generate the MUT (which occurred \nin 19% of our examples). The user writes a driver that generates keys and values for the MUT and for \nthe collection. In addition, the user provides a non\u00adcommutative driver that takes an input object and \nreturns a different object. This driver is used by the non-commutative aware adversarial environment. \nIn order to help the user, we integrated into COLT simple and generic drivers for primitive and simple \ntypes such as Integers. These drivers, together with linearization points and the MUT, are given to the \nByte\u00adcode instrumentor. The Bytecode instrumentor instruments the MUT and generates our linearizability \nchecker using the library s non-commutativity speci.cation. Then, the instru\u00admented MUT is repeatedly \nexecuted until either a lineariz\u00adability violation is found or a time bound is exceeded.  Figure 10. \nCOLT overview 5.1.1 Checking Linearizability COLT has three modes for checking linearizability and in \ngen\u00aderal does not rely on user-speci.ed linearization points. The .rst mode enumerates linearizations, \nthe second enumerates candidate linearization points, and the third uses heuristics for guessing linearization \npoints. In our experiments, the third mode worked suf.ciently well and we never resort to the more expensive \nmodes or manually provided linearization points.  5.2 Applications The 51 real-world applications shown \nin Table 1 were se\u00adlected because their code is available and they make use of Java s concurrent collections. \nIn many cases concurrent collections were introduced to address observed scalability problems, replacing \nCollections.synchronizedMap or manual locking of a sequential map. Each of the applications contains \none or more methods that were extracted and tested by COLT. These methods were extracted using COLT s \nMUT Extractor (discussed in Sec\u00adtion 5.1). which identi.es methods that may invoke multiple collection \noperations. In 18 out of the 95 extracted methods, COLT identi.ed a specialized concurrent data structure \noper\u00adation inside a large method and we manually extracted and generated the MUT.  5.3 Results We tested \n95 methods in 51 applications. As Figure 11b shows 36 (38%) of these methods were linearizable in an \nopen environment (and hence also linearizable in their pro\u00adgram environment). Another 17 (18%) of these \nmethods were not linearizable in an open environment, but were safe when implicit (and unchecked) application \ninvariants were taken into account. Finally, 42 (44%) of these methods had atomicity violations that \ncould be triggered in the current ap\u00adplication. Figure 11a shows the results reported by COLT. The X \naxis shows the application number from Table 1 and the Y axis shows the number of MUTs checked for each \napplica\u00adtion. Non-Lin shows the methods in our experiments that are not linearizable in an open environment \nas well as in the Table 1. Applications used for experiments  (a) (b) client environment. Modular \nNL shows the methods in our experiments that are not linearizable in an open environment but are linearizable \nin the client s current environment. For example, application #9 is Apache Tomcat and has 10 non-linearizable \nmethods in an open environment as well as the client environment and 1 linearizable method in any environment. \nFor all of the non linearizable methods( Non-Lin and Modular NL ), COLT reported an interleaving in which \nthe MUT is not atomic. In cases where we observed the method to be linearizable under a restricted environment \n(e.g., no remove operations), we con.rmed that re-running COLT under an appropriately restricted adversary \nno longer reports the same violation. We manually inspected all of the methods where COLT timed out without \nreporting a violation after 10 hours. We managed to construct a linearizability proof for each one of \nthem, showing that: (i) these methods are indeed linearizable and COLT did not miss an error; and (ii) \nno false alarms were reported by COLT on these linearizable methods. These methods are presented in Figure \n11a as Lin . Naive Adversary Testing with a naive adversary failed to .nd a single violation within a \ntimeout of 10 hours per method, while COLT reported a violation for all of these non\u00adlinearizable methods \nin less than a second. Con.rmed Violations We reported all violations of lin\u00adearizability (even those \nthat were only present in an open environment) to the project developers. For each violation we included \nthe interleaving and a suggested .x. Interest\u00adingly, in some cases, even though the tool reported errors \nthat cannot occur in the program environment, the develop\u00adment team still decided to adopt our suggested \n.xes to make the code more robust to future program modi.cations. Apache Cassandra has two methods that \nimplement an optimistic concurrent algorithm on top of a concurrent col\u00adlection. COLT reports a violation \nfor both methods. The rea\u00adson for the violation is that this algorithm may throw a null pointer exception \nwhen running concurrently with a remove operation. We reported this violation to the development team \nand it turned out that under the program environment, remove operations are allowed only on local copies \nof the collection. Therefore, in the current program environment the method is linearizable. However, \nthe project lead decided to adopt our suggested .xes in order to make the method lin\u00adearizable in any \nfuture evolution of the program. Apache MyFaces Trinidad, Ektrop, Hazelcast, Grid-Kit, GWTeventservice, \nand DYProject use modules that try to atomically add and return a value from a collection or return a \nvalue if one is already in the collection for a given key. COLT reports a violation for all of these \nmethods. For Apache MyFaces Trinidad, we reported the potential vio\u00adlation, but the developers had .xed \nthis problem before we submitted the report. For Ektrop, the developers decided to keep our remarks in \ncase they opt to make program mod\u00adi.cations in the future. For Hazelcast, the developers ac\u00adknowledged \nthe violation and replied that the code is being re-factored. For GWTeventservice, the developers acknowl\u00adedged \nthe violations and adopted our .xes. For GridKit, the developers reported that the violation is not feasible \nin their environment. However, they still decided to adopt our .xes as a preventive measure. For DYProject, \nthe developers ac\u00adknowledged the violations and adopted our .xes.  In Apache Tomcat COLT found violations \nin 10 of the 11 methods checked. Two of the reported violations were approved as violations by the Tomcat \ndevelopment team. Most of the violations were caused by switching the im\u00adplementation from a HashMap \nto a ConcurrentHashMap while removing the lock that guarded the HashMap in the collection client. Another \ntype of violation occurs in FastHttpDateFormat and is caused by switching from a synchronizedMap to a \nConcurrentHashMap without changing the client code. In this case, violations were intro\u00adduced because \nthe implementation of synchronizedMap guards each operation by the object s lock while the im\u00adplementation \nof ConcurrentHashMap has internal locks which are different than the object lock. Therefore, two of the \nmethods are built by a set of collection opera\u00adtions guarded by the collection s lock. These methods \nwere linearizable under synchronizedMap and became non\u00adlinearizable under ConcurrentHashMap. For the \nrest of the violated methods we reported the vio\u00adlations but have not yet heard from the development \nteams.  5.4 Con.ict Serializability vs. Linearizability None of the application methods we tested are \ncon.ict seri\u00adalizable [15]. The con.ict serializability checker reports vi\u00adolation on all of our methods, \nwhether or not the method be\u00adhaves atomically. For the methods that required repair, none of the repairs \nwere con.ict serializable.  5.5 A Recurring Example: Memoization A recurring operation in our benchmarks \nwas memoization, in which a concurrent map was used to store the results of an expensive computation. \nThe authors of ConcurrentHashMap explicitly avoided including this functionality, because the optimal \ndesign depends on the hit rate of the memoization table, the cost of the computation, the relative importance \nof latency versus throughput, potential interference between duplicate work, the possibility that the \ncomputation might fail, etc. Consider a function compute(K k) that memoizes the result of calculateVal(k). \nThe desired functionality (note that Java does not actually have an atomic keyword) is: V compute(K k) \n{ atomic { V v = m.get(k); if (v == null) { v = computeVal(k); m.put(k, v); } return v; } } Figure \n12 shows some of the implementations that we en\u00adcountered for compute, including the buggy version from \nFigure 3a. The procedure compute(K k) uses an underlying con\u00adcurrent collection to memoize the value \ncomputed by func\u00adtion calculateVal(k). When the value for a given key is cached in the collection it \nis returned immediately; when the value for a given key is not available, it is computed and in\u00adserted \ninto the collection. Figure 12(i) shows a linearizable concurrent implemen\u00adtation of compute. This operation \nis linearizable because it has only one collection operation, putIfAbsent, there\u00adfore, the linearizability \nguarantee is provided by the collec\u00adtion library. Even though this implementation is correct and linearizable, \nin most cases, programmers avoid writing the compute operation in this way because the internal imple\u00admentation \nof putIfAbsent acquires a lock and this can be avoided in several cases. Another reason is that often \ncalculateData can be time consuming or cause a side ef\u00adfect, and hence should not be executed more than \nonce. The implementation in Figure 12(ii) is mostly used to avoid lock acquisition when k is already \ninside the collec\u00adtion. This implementation is linearizable and has two condi\u00adtional linearization points \nmarked by @LP [condition]. The .rst conditional linearization point occurs when the get operation returns \na value different than null. In this case, the value returned by get is returned without any additional \naccess to m. The second linearization point oc\u00adcurs at the putIfAbsent. In cases where putIfAbsent succeeds \nin updating m a null value is returned by the putIfAbsent and the updated value is returned. Otherwise, \nputIfAbsent returns the value from m and this value is re\u00adturned by compute. The implementations in Figure \n12(iv), Figure 12(v), and Figure 12(vi) are common non linearizable implementations of Figure 12(ii) \nand the implementation in Figure 12(iii) is a common non-linearizable implementation of Figure 12(i). \nFigure 7 displays an interleaving that reveals the non\u00adlinearizability of the implementation in Figure \n12(iv): the remove(7) operation at line 10 causes compute(7) to re\u00adturn a null result. A similar interleaving \ncan reveal the non\u00adlinearizability of Figure 12(iii) and Figure 12(vi) where a remove(k) operation occurs \nbetween the putIfAbsent(k) and the get(k). An interleaving that reveals the non-linearizability of Fig\u00adure \n12(v) occurs when put(k,*) with the same key k is ex\u00adecuted between if (val == null) and putIfAbsent \nof compute. In this case the putIfAbsent fails and the value returned by compute is not the value corresponding \nto k in the map m. Advanced Example When the calculateVal function is either time consuming or causes \na side effect the code in Figure 12(vii) is used. This implementation uses Java s Future construct to \nguarantee that only one execution of with @LP [condition].  calculateData is done for each added key. \nThis way only one thread is responsible for the calculation while the oth\u00aders may block until the calculation \ncompletes. If Future is canceled, the Future is removed from the map and a new memoization iteration \ncontinues until a Future terminates successfully and its value is returned. A linearization point for \nthis implementation occurs when f.get() returns suc\u00adcessfully (marked by @LP (f.isDone())) due to the \nfact that the f calculation terminated successfully. The reason this is a linearization point is that \nthere might be a case where an\u00adother thread canceled f s execution and then the compute execution should \ncontinue. Even though the implementation in Figure 12(vii) is lin\u00adearizable, COLT reports a violation \nof linearizability. The problem is that COLT is not aware of the Future semantics and should treat a \nFuture value that has not terminated suc\u00adcessfully as if its corresponding key is missing from the map. \nAugmenting COLT with this information solves the problem and COLT no longer reports a linearizability \nviolation in this case. In this example, the Future computation does not itself perform collection operations \nand thus no special handling of the scheduling of futures is required, even if the Future is added to \nthe collection (and such examples exist in prac\u00adtice). In general COLT warns the user if a Future performs \ncollection operations, but we have seen no such examples in practice. Benchmark Distribution Figure 13 \nshows the distribution of the different kinds of composed concurrent operations (as given in Figure 13) \nin our benchmark. Overall 68% of the MUTs we checked were memoization examples. The most common bug \npattern (20%) was the im\u00adplementation in Figure 12(v) followed by the implementa\u00adtion in Figure 12(iv) \n(9%).  5.6 Reasons for Success Source of Bugs most of the MUTs were written origi\u00adnally using a concurrent \ncollection. However, in some cases, the MUT was modi.ed while changing the collection im\u00adplementation \nfrom sequential to concurrent. Refactoring the code in most cases resulted in a linearizability violation. \nA clear example is Apache Tomcat where 10 out of 11 MUTs were refactored in a non-linearizable manner. \n Bugs Characteristics Observing the implementations in Figure 12 reveals that these implementations \nbehave uni\u00adformly for each given key. This characteristic occurs in all our checked MUTs and implies \nthat if the MUT is not lin\u00adearizable then for each key there exists an interleaving re\u00advealing the MUT \ns non-linearizability. The fact that programmers tend to write buggy special\u00adized concurrent collections \nand that their corresponding MUTs are uniform signi.cantly eases bug detection in real code. The uniformity \ncharacteristic implies that bugs can be detected using any input key so long as the adversary performs \nthe right operation with the right interleaving. Our non-commutativity aware adversarial environment \neasily de\u00adtects these non-linearizable interleavings. 6. Related Work Partial Order Reduction Partial \norder reduction tech\u00adniques, such as [18], use commutativity of individual mem\u00adory operations to .lter \nout execution paths that lead to the same global state. Indeed, our technique is a partial order re\u00adduction \nthat uses a similar observation. However, our tech\u00adnique works on an encapsulated ADT and leverages com\u00admutativity \nat the level of the ADT, rather than the level of memory operations. A traditional partial order reduction \ntechnique working at the level of memory operations would not be able to leverage the commutativity speci.cation \nat the level of ADT operations and would instead require ex\u00adposing the implementation of ADT operations \nand checking whether individual memory operations within them com\u00admute. In contrast, our approach treats \nADT operations as atomic operations and leverages commutativity of ADT op\u00aderations to signi.cantly increase \nthe bug hunting capabilities of our tool. Dynamic Atomicity Checking Dynamic atomicity check\u00aders such \nas [14, 15] check for violations of con.ict serial\u00adizability. As noted earlier, con.ict-serializability \nis inappro\u00adpriate for concurrent data structures. Therefore, in this work we check for violations of \nlinearizability. Vyrd [12] is a dy\u00adnamic checking tool that checks a property similar to lin\u00adearizability. \nVyrd required manual annotation of lineariza\u00adtion points. Line-Up [8] is a dynamic linearizability checker \nthat enumerates schedules. The formal result of [13] im\u00adplies that we need not generate schedules where \nlinearizable operations are executed non-atomically. This can reduce the number of interleavings that \nneed to be explored. This in\u00adsight has also been discussed and made use of in the pre\u00ademption sealing \nwork of [6]. In contrast with COLT, all of these existing tools are non-modular and not directed using \nnon-commutativity. As shown in Section 5, when commutativity information is not utilized, the ability \nto detect collection-related atomicity vi\u00adolations remains low. In fact, the poor results of the random \nadversary in Section 5 are obtained when underlying collec\u00adtion operations are considered atomic. GAMBIT \n[10] is a unit testing tool for concurrent li\u00adbraries built on top of the CHESS tool [25]. GAMBIT uses \nprioritized search of a stateless model checker using heuris\u00adtics, bug patterns, and user-provided information. \nEven though GAMBIT and COLT are unit testing tools for concur\u00adrent libraries, COLT is working on a specialized \nconcurrent data structure library built on top of an already veri.ed and well de.ned concurrent collection. \nCOLT leverages this fact together with non-commutativity to guide the scheduling to quickly reveal bugs. \nFrom the above reasons GAMBIT is unlikely to detect the collection-related atomicity violations. An active \ntesting technique for checking con.ict serializ\u00adabilty is presented in [26]. The technique uses bug patterns \nto control the scheduler directing the execution to error-prone execution paths. Even if the technique \nis adjusted to check linearizability, it is not modular and not directed using non\u00adcommutativity. Therefore, \nits ability to detect collection\u00adrelated violations would likely be low. Static Checking and Veri.cation \nSome approaches [17, 24] do static veri.cation and use the atomicity proof to sim\u00adplify the correctness \nproofs of multithreaded programs. Our work also uses the linearizability proof of the collection li\u00adbrary \nto prune non-violating paths. However, our technique is different than these works because we perform \nmodular testing and check only methods. In addition, we use collec\u00adtion non-commutativity speci.cations \nto direct exploration. Moreover, we are checking the linearizability of the meth\u00adods. Other work such \nas [28] focuses on model checking individual collections. Our work operates at a higher level, as a client \nof already veri.ed collections and leverages the speci.cations of the underlying collections to reduce \nthe search space. Shape analysis tools, such as [5], prove the lin\u00adearizability of heap manipulating \nconcurrent programs with a bounded number of threads. These tool are veri.ers and may produce false alarms \ndue to the overapproximating ab\u00adstractions they employed. Our work is different than these works because \nwe use dynamic analysis. The idea of using a general client over-approximating the thread environment \nis common in modular veri.cation. Pre\u00advious work represented the environment as invariants [22] or relations \n[23] on the shared state. This idea has also been used early on for automatic compositional veri.ca\u00adtion \n[9]. In addition, this approach has led to the notion of thread-modular veri.cation for model checking \nsystems with in.nitely-many threads [16] and has also been applied to the domain of heap-manipulating \nprograms with coarse\u00adgrained concurrency [20]. COLT uses the idea of an adver\u00adsarial environment for \ndynamic checking, leveraging non\u00adcommutativity information to effectively reveal atomicity vi\u00adolations. \n 7. Conclusions We have presented a modular and effective technique for testing composed concurrent \noperations. Our technique uses a specialized adversary that leverages commutativity infor\u00admation of the \nunderlying collections to guide execution to\u00adwards linearizability violations. We implemented our tech\u00adnique \nin a tool called COLT and showed its effectiveness by detecting 56 previously unknown linearizability \nviolations in 51 popular open-source applications such as Apache Tom\u00adcat. Acknowledgments We thank Karen \nYorav for reading a preliminary version of this paper and making valuable remarks. This research was \npartially supported by The Israeli Science Foundation (grant no. 965/10), NSF Grant CCF-0702681, and \na gift from IBM. References [1] Amino concurrent building blocks. http://amino-cbbs.sourceforge.net/. \n [2] Apache tomcat. http://tomcat.apache.org/. [3] Intel thread building blocks. http://www.threadingbuildingblocks.org/. \n[4] openjdk. http://hg.openjdk.java.net/jdk7/jaxp/jdk. [5] AMIT, D., RINETZKY, N., REPS, T. W., SAGIV, \nM., AND YAHAV, E. Comparison under abstraction for verifying lin\u00adearizability. In CAV (2007), pp. 477 \n490. [6] BALL, T., BURCKHARDT, S., COONS, K. E., MUSUVATHI, M., AND QADEER, S. Preemption sealing for \nef.cient con\u00adcurrency testing. In TACAS (2010), pp. 420 434. [7] BERNSTEIN, P. A., HADZILACOS, V., AND \nGOODMAN, N. Concurrency Control and Recovery in Database Systems. 1987. [8] BURCKHARDT, S., DERN, C., \nMUSUVATHI, M., AND TAN, R. Line-up: a complete and automatic linearizability checker. In PLDI (2010), \npp. 330 340. [9] CLARKE, JR., E. Synthesis of resource invariants for concur\u00adrent programs. TOPLAS 2, \n3 (1980), 338 358. [10] COONS, K. E., BURCKHARDT, S., AND MUSUVATHI, M. GAMBIT: effective unit testing \nfor concurrency libraries. In PPOPP (2010), pp. 15 24. [11] DOHERTY, S., DETLEFS, D. L., GROVES, L., \nFLOOD, C. H., LUCHANGCO, V., MARTIN, P. A., MOIR, M., SHAVIT, N., AND GUY L. STEELE, J. DCAS is not a \nsil\u00adver bullet for nonblocking algorithm design. In SPAA (2004), pp. 216 224. [12] ELMAS, T., TASIRAN, \nS., AND QADEER, S. Vyrd: verifying concurrent programs by runtime re.nement-violation detec\u00adtion. In \nPLDI (2005), pp. 27 37. [13] FILIPOVI C\u00b4 , I., O HEARN, P., RINETZKY, N., AND YANG, H. Abstraction for \nconcurrent objects. In ESOP (2009), pp. 252 266. [14] FLANAGAN, C., AND FREUND, S. N. Atomizer: A dynamic \natomicity checker for multithreaded programs. In POPL (2004), pp. 256 267. [15] FLANAGAN, C., FREUND, \nS. N., AND YI, J. Velodrome: a sound and complete dynamic atomicity checker for multi\u00adthreaded programs. \nIn PLDI (2008), pp. 293 303. [16] FLANAGAN, C., AND QADEER, S. Thread-modular model checking. In SPIN \n(2003), pp. 213 224. [17] FLANAGAN, C., AND QADEER, S. A type and effect systrm for atomicity. In PLDI \n(2003), pp. 338 349. [18] GODEFROID, P. Partial-Order Methods for the Veri.cation of Concurrent Systems: \nAn Approach to the State-Explosion Problem. Springer-Verlag New York, Inc., Secaucus, NJ, USA, 1996. \n[19] GOETZ, B., PEIERLS, T., BLOCH, J., BOWBEER, J., HOLMES, D., AND LEA, D. Java Concurrency in Practice. \nAddison Wesley, 2006. [20] GOTSMAN, A., BERDINE, J., COOK, B., AND SAGIV, M. Thread-modular shape analysis. \nIn PLDI (2007), pp. 266 277. [21] HERLIHY, M. P., AND WING, J. M. Linearizability: a cor\u00adrectness condition \nfor concurrent objects. TOPLAS 12,3 (1990). [22] HOARE, C. A. R. Towards a theory of parallel programming. \n1972. [23] JONES, C. B. Speci.cation and design of (parallel) programs. 1983. [24] MUSUVATHI, M., AND \nQADEER, S. Iterative context bound\u00ading for systematic testing of multithreaded programs. In PLDI (2007), \npp. 446 455. [25] MUSUVATHI, M., QADEER, S., BALL, T., BASLER, G., NAINAR, P. A., AND NEAMTIU, I. Finding \nand reproducing heisenbugs in concurrent programs. In OSDI (2008), pp. 267 280. [26] PARK, C.-S., AND \nSEN, K. Randomized active atomicity violation detection in concurrent programs. In SIGSOFT (2008), pp. \n135 145. [27] SHAVIT, N. Data structures in the multicore age. Commun. ACM 54 (March 2011), 76 84. [28] \nVECHEV, M., YAHAV, E., AND YORSH, G. Experience with model checking linearizability. In SPIN (2009), \npp. 261 278. [29] WEIHL, W. E. Commutativity-based concurrency control for abstract data types. IEEE \nTrans. Computers 37, 12 (1988), 1488 1505.    \n\t\t\t", "proc_id": "2048066", "abstract": "<p>We address the problem of testing atomicity of composed concurrent operations. Concurrent libraries help programmers exploit parallel hardware by providing scalable concurrent operations with the illusion that each operation is executed atomically. However, client code often needs to compose atomic operations in such a way that the resulting composite operation is also atomic while preserving scalability. We present a novel technique for testing the atomicity of client code composing scalable concurrent operations. The challenge in testing this kind of client code is that a bug may occur very rarely and only on a particular interleaving with a specific thread configuration. Our technique is based on modular testing of client code in the presence of an adversarial environment; we use commutativity specifications to drastically reduce the number of executions explored to detect a bug. We implemented our approach in a tool called COLT, and evaluated its effectiveness on a range of 51 real-world concurrent Java programs. Using COLT, we found 56 atomicity violations in Apache Tomcat, Cassandra, MyFaces Trinidad, and other applications.</p>", "authors": [{"name": "Ohad Shacham", "author_profile_id": "81100234221", "affiliation": "Tel Aviv University, Tel Aviv , Israel", "person_id": "P2839126", "email_address": "ohad.shacham@cs.tau.ac.il", "orcid_id": ""}, {"name": "Nathan Bronson", "author_profile_id": "81361595088", "affiliation": "Stanford University, Stanford , CA, USA", "person_id": "P2839127", "email_address": "nbronson@cs.stanford.edu", "orcid_id": ""}, {"name": "Alex Aiken", "author_profile_id": "81100399954", "affiliation": "Stanford University, Stanford, CA, USA", "person_id": "P2839128", "email_address": "aiken@cs.stanford.edu", "orcid_id": ""}, {"name": "Mooly Sagiv", "author_profile_id": "81100150928", "affiliation": "Tel Aviv University, Tel Aviv, Israel", "person_id": "P2839129", "email_address": "msagiv@post.tau.ac.il", "orcid_id": ""}, {"name": "Martin Vechev", "author_profile_id": "81100269652", "affiliation": "ETH Zurich and IBM Research, Zurich, Switzerland", "person_id": "P2839130", "email_address": "martin.vechev@gmail.com", "orcid_id": ""}, {"name": "Eran Yahav", "author_profile_id": "81100285431", "affiliation": "Technion, Haifa, Israel", "person_id": "P2839131", "email_address": "yahave@cs.technion.ac.il", "orcid_id": ""}], "doi_number": "10.1145/2048066.2048073", "year": "2011", "article_id": "2048073", "conference": "OOPSLA", "title": "Testing atomicity of composed concurrent operations", "url": "http://dl.acm.org/citation.cfm?id=2048073"}