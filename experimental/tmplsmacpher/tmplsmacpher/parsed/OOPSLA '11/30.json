{"article_publication_date": "10-22-2011", "fulltext": "\n Oracle Scheduling: Controlling Granularity in Implicitly Parallel Languages Umut A. Acar Arthur Chargu\u00b4eraud \nMike Rainey Max Planck Institute for Software Systems {umut,charguer,mrainey}@mpi-sws.org Abstract A \nclassic problem in parallel computing is determining whether to execute a task in parallel or sequentially. \nIf small tasks are executed in parallel, the task-creation overheads can be overwhelming. If large tasks \nare executed sequen\u00adtially, processors may spin idle. This granularity problem, however well known, is \nnot well understood: broadly appli\u00adcable solutions remain elusive. We propose techniques for controlling \ngranularity in im\u00adplicitly parallel programming languages. Using a cost se\u00admantics for a general-purpose \nlanguage in the style of the lambda calculus with support for parallelism, we show that task-creation \noverheads can indeed slow down parallel ex\u00adecution by a multiplicative factor. We then propose oracle \nscheduling, a technique for reducing these overheads, which bases granularity decisions on estimates \nof task-execution times. We prove that, for a class of computations, oracle scheduling can reduce task \ncreation overheads to a small fraction of the work without adversely affecting available parallelism, \nthereby leading to ef.cient parallel executions. We realize oracle scheduling in practice by a combination \nof static and dynamic techniques. We require the program\u00admer to provide the asymptotic complexity of \nevery func\u00adtion and use run-time pro.ling to determine the implicit, architecture-speci.c constant factors. \nIn our experiments, we were able to reduce overheads of parallelism down to between 3 and 13 percent, \nwhile achieving 6-to 10-fold speedups. Categories and Subject Descriptors D.1.3 [Programming Techniques]: \nConcurrent Programming General Terms Algorithms, Experimentation, Languages Keywords Scheduling, Granularity \nControl, Work Stealing Permission to make digital or hard copies of all or part of this work for personal \nor classroom use is granted without fee provided that copies are not made or distributed for pro.t or \ncommercial advantage and that copies bear this notice and the full citation on the .rst page. To copy \notherwise, to republish, to post on servers or to redistribute to lists, requires prior speci.c permission \nand/or a fee. OOPSLA 11, October 22 27, 2011, Portland, Oregon, USA. 1. Introduction Explicit parallel \nprogramming provides full control over parallel resources by offering primitives for creating and managing \nparallel tasks, which are small, independent threads of control. As a result, the programmer can, at \nleast in princi\u00adple, write ef.cient parallel programs by performing a careful cost-bene.t analysis to \ndetermine which tasks should be ex\u00adecuted in parallel and under what conditions. This approach, however, \noften requires reasoning about low-level execu\u00adtion details, such as data races or concurrent effects, \nwhich is known to be notoriously hard; it can also result in code that performs well in a particular \nhardware setting but not in others. The complexities of parallel programming with explicit languages \nhave motivated interest in implicitly parallel lan\u00adguages, such as Cilk [12], Manticore [16, 17], Multilisp \n[22], and NESL [8]. These languages enable the programmer to express opportunities for parallelism via \nlanguage con\u00adstructs, e.g., parallel sequences, parallel arrays, and paral\u00adlel tuples. This implicit \napproach enables a declarative pro\u00adgramming style by delegating the task of utilizing the par\u00adallelism \nexposed by the program to the compiler and the run-time system. As an implicit parallel program executes, \nit exposes opportunities for parallelism (as indicated by the parallel constructs) and the language run-time \nsystem creates parallel tasks as needed. To execute parallel tasks ef.ciently, implicit programming languages \nuse a scheduler to load bal\u00adance, i.e., distribute parallel tasks among processors. Various scheduling \ntechniques and practical schedulers have been developed, including work-stealing schedulers [1, 3, 11] \nand depth-.rst-search schedulers [7]. Experience with implicitly parallel programs shows that one of \nthe most important decisions that any implicit parallel language must make is determining whether or \nnot to exploit an opportunity for parallelism by creating a parallel task. Put another way, the question \nis to determine which tasks to execute in parallel and which tasks to execute sequentially. This problem, \noften referred to as the granularity problem, is important because creating a parallel task requires \nadditional overhead. If the task granularity is not handled effectively, c Copyright &#38;#169; 2011 \nACM 978-1-4503-0940-0/11/10. . . $10.00  task-creation overheads can easily obliterate the bene.t of \nparallelism. Many parallel programs are characterized by parallel slackness [41], a property which indicates \nthat the program exposes many more opportunities for parallelism than the number of available processors. \nIn such programs, effec\u00adtive granularity control is crucial because the program typi\u00adcally creates many \nsmall tasks, thereby ensuring signi.cant scheduling overhead. No known broadly applicable solution to \nthe granular\u00adity problem exists. Theoretical analyses often ignore task\u00adcreation overheads, yielding \nno signi.cant clues about how these overheads may affect ef.ciency. Practical implemen\u00adtations often \nfocus on reducing task-creation overheads in\u00adstead of attempting to control granularity. As a result, \npracti\u00adtioners often deal with this issue by trying to estimate the right granularity of work that would \nbe suf.ciently large to execute in parallel. Since the running time of a task de\u00adpends on the hardware, \nsuch manual control of granularity is dif.cult and bound to yield suboptimal results and/or non\u00adportable \ncode [40]. In this paper, we propose theoretical and practical tech\u00adniques for the granularity problem \nin implicit parallel\u00adprogramming languages. Our results include theorems that characterize how parallel \nrun time is affected by task\u00adcreation overheads, which we show to be signi.cant (Sec\u00adtion 3). To reduce \nthese overheads, we consider a granularity control technique that relies on an oracle for determining \nthe run-time of parallel tasks (Section 2). We show that if the oracle can be implemented ef.ciently \nand accurately, it can be used to improve ef.ciency for a relatively large class of computations (Section \n3). Based on this result, we describe how oracles can be realized in practice; we call this tech\u00adnique \noracle scheduling because it relies on an oracle to estimate task sizes and because it can be used in \nconjunction with practically any other scheduler (Section 4). Finally, we propose an implementation of \noracle scheduling that uses complexity functions de.ned by the user to approximate ac\u00adcurately run-time \nof parallel tasks (Section 4). We present an implementation and evaluation of the proposed approach by \nextending a subset of the Caml language (Sections 5 and 6). Brent s theorem [13], commonly called the \nwork-time principle, characterizes what is arguably the most important bene.t of parallel programs, which \nis that a parallel program can be executed on a multiprocessor to obtain near linear speedups. For a \ncomputation, let raw work, written w, re\u00adfer to the total number of executed instructions, and let raw \ndepth, written d, refer to the length longest dependent chain of executed instructions. Brent s theorem \nshows that we can execute a computation with w raw work and d raw depth in no more than w/P + d steps \non P processors using any greedy scheduler. 1 A greedy scheduler is a scheduler that can .nd available \nwork immediately. This assumption is rea\u00ad 1 Note that the bound is tight within a factor of two. sonably \nrealistic, as practical multiprocessor scheduling al\u00adgorithms, such as work-stealing, can match Brent \ns bound asymptotically for certain relatively large classes of compu\u00adtations, e.g., fork-join and nested \ndata-parallel computations. In the execution model with raw work and raw depth, each instruction implicitly \nis assigned unit cost. Unfortu\u00adnately, this model does not direcly account for task-creation overheads. \nTo assess the signi.cance of these overheads in implicitly parallel programs, we consider a lambda calculus \nwith parallel tuples and present a cost-semantics for evalu\u00adating expression of this language (Section \n2). The cost se\u00admantics accounts for task-creation overheads by assigning non-unit costs to the operations \ngenerating such overheads. In addition to raw work and raw depth, the cost seman\u00adtics yield total work, \ntotal depth of each evaluated expres\u00adsion. We de.ne total work, written W, as the total cost of the evaluated \ninstructions, and total depth, written D, as the total cost of the most expensive dependent chain of \nevalu\u00adated instructions total work and total depth include task\u00adcreation overheads. Using this cost semantics, \nwe show that task creation overheads can be a signi.cant multiplicative factor of the raw work. To understand \nthe understand the impact of the overheads, we adapt Brent s theorem to take them into ac\u00adcount (Section \n2). Speci.cally, we show that parallel com\u00adputations with total work W and total depth D can be exe\u00adcuted \nin no more than W/P +D steps. Intuitively, this bound shows that task-creation overheads contribute directly \nto the parallel run time just like any other work. Combined with the result that task-creation overheads \ncan increase total work by a multiplicative factor, the generalized Brent s theorem im\u00adplies that the \noverheads slow down parallel run time by a multiplicative factor. To reduce task-creation overheads, \nwe propose an alterna\u00adtive oracle semantics that capture a well-known principle for avoiding the task-creation \noverheads. We evaluate a task in parallel only if its is suf.ciently large, i.e., greater than some cutoff \nconstant .. We show that the oracle semantics can de\u00adcrease the overheads of task-creation by any desired \nconstant factor ., but only at the cost of increasing the total depth (Sections 2 and 3). These bounds \nsuggest that we can reduce the task-creation overheads signi.cantly, if we can realize the semantics \nin practice. This unfortunately is impossible because it requires determining a priori task-creation \nover\u00adheads. We show, however, that a realistic oracle that can give constant-factor approximations to \nthe task run times can still result in similar reductions in the overheads. We show that if we have prior \nknowledge of the raw work and the raw depth of a computation, then we can pick the optimal cutoff con\u00adstant \n. that yields the fastest parallel run time for a class of computations. We also show that, under some \nassump\u00adtions, there exists a constant . that reduces the task creation overheads to a small constant \nratio of the raw work, without increasing the depth of the computation in a way that would signi.cantly \naffect the run time.  To realize the oracle semantics in practice, we describe a scheduling technique \nthat we call oracle scheduling (Sec\u00adtion 4). Oracle scheduling relies on a task-size estimator that can \nestimate the actual run time of parallel tasks in constant\u00adtime within a constant factor of accuracy, \nand a conventional greedy scheduling algorithm, e.g., work-stealing, or a par\u00adallel depth-.rst scheduler. \nOracle schedulers perform ef.\u00adcient parallel task creation by selectively executing in paral\u00adlel only \nthose tasks that have a large parallel run-time. We describe an instance of the oracle scheduler that \nrelies on an estimator that uses asymptotic cost functions (asymptotic complexity bounds) and judicious \nuse of run-time pro.ling techniques to estimate actual run-times accurately and ef.\u00adciently. This approach \ncombines an interesting property of asymptotic complexity bounds, which are expressed with\u00adout hardware-dependent \nconstants, and pro.ling techniques, which can be used to determine these constants precisely. We present \na prototype implementation of the proposed approach (Section 5) by extending the OCAML language to support \nparallel tuples and complexity functions. The imple\u00admentation translates programs written in this extended \nlan\u00adguage to the PML (Parallel ML) language [17]. Although our implementation requires the programmer \nto enter the com\u00adplexity information, this information could also be inferred in some cases via static \nanalysis (e.g., [25] and references therein). In our implementation, for simplicity we only con\u00adsider \nprograms for which the execution time is (with high probability) proportional to the value obtained by \nevaluat\u00ading the asymptotic complexity expression. We extend the Manticore compiler for PML to support \noracle scheduling and use it to compile generated PML programs. Our exper\u00adiments (Section 6) show that \noracle implementation can re\u00adduce the overheads of a single processor parallel execution to between 3 \nand 13 percent of the sequential time. When using 16 processors, we achieve 7-to 15-fold speedups on \nan AMD machine and 6-to 10-fold speedups on an Intel machine. 2. Source Language To give an accurate \naccount of the cost of task creation, and to specify precisely our compilation strategy, we con\u00adsider \na source language in the style of the .-calculus and present a dynamic cost semantics for it. The semantics \nand the costs are parameterized by t and f, which represent the cost of creating a parallel task and \nthe cost of consulting an external oracle for predicting the sizes of its two branches respectively. \nBy using a known proof technique, we gener\u00adalize Brent s theorem to take task-creation overheads into \naccount. v ::= x | n | (v, v) | inl v | inr v | fun f.x.e e ::= v | let x = e1 in e2 | (vv) | fst v | \nsnd v | case v of {inl x.e, inr x.e}| (e, e) | (|e, e|) Figure 1. Abstract syntax of the source language \n 2.1 Cost semantics The source language includes recursive functions, pairs, sum types, and parallel \ntuples. Parallel tuples enable expressing computations that can be performed in parallel, similar to \nthe fork-join or nested data parallel computations. For simplicity of exposition, we consider parallel \ntuples of arity two only. Parallel tuples of higher arity can be easily represented with those of arity \ntwo. To streamline the presentation, we assume programs to be in A-normal form, with the exception of \npairs and paral\u00adlel pairs, which we treat symmetrically because our compila\u00adtion strategy involves translating \nparallel pairs to sequential pairs. Figure 1 illustrates the abstract syntax of the source language. \nWe note that, even though the presentation is only concerned with a purely-functional language, it is \neasy to include references; for the purposes of this paper, however, they add no additional insight and \nthus are omitted for clar\u00adity. We de.ne a dynamic semantics where parallel tuples are evaluated selectively \neither in parallel or sequentially, as de\u00adtermined by their relative size compared with some constant \n., called the cutoff value and such that . = 1. To model this behavior, we present an evaluation semantics \nthat is param\u00adeterized by an identi.er that determines the mode of execu\u00adtion, i.e., sequential or not. \nFor the purpose of comparison, we also de.ne a (fully) parallel semantics where parallel tu\u00adples are \nalways evaluated in parallel regardless of their size. The mode of an evaluation is sequential (written \nseq), paral\u00adlel (written par), or oracle (written orc). We let a range over modes: a ::= seq | par | \norc. In addition to an evaluating expression, the dynamic se\u00admantics also returns cost measures including \nraw work and raw depth denoted by w and d (and variants), and total work and total depth, denoted by \nW and D (and variants). Dy\u00adnamic semantics is presented in the style of a natural (big\u00adstep) semantics \nand consists of evaluation judgments of the form e .a v, (w, d), (W, D). This judgment states that evaluating \nexpression e in mode a yields value v resulting in raw work of w and raw depth of d and total work of \nW and total depth of D. Figure 2 shows the complete inductive de.nition of the dynamic cost semantics \njudgment e .a v, (w, d), (W, D). When evaluating any expression that is not a parallel tuple,  (value) \n v .a v, (1, 1), (1, 1) (let) e1 .a v1, (w1,d1), (W1, D1) e2[v1/x] .a v, (w2,d2), (W2, D2) (let x = \ne1 in e2) .a v, (w1 + w2 +1,d1 + d2 + 1), (W1 + W2 +1, D1 + D2 + 1) (app) (v1 = fun f.x.e) e[v2/x, v1/f] \n.a v, (w, d), (W, D) (v1 v2) .a v, (w +1,d + 1), (W +1, D + 1) (.rst) (second) (fst (v1,v2)) .a v1, (1, \n1), (1, 1) (snd (v1,v2)) .a v2, (1, 1), (1, 1) (case-left) e1[v1/x1] .a v, (w, d), (W, D) case (inl \nv1) of {inl x1.e1, inr x2.e2}.a v, (w +1,d + 1), (W +1, D + 1) (case-right) e2[v2/x2] .a v, (w, d), \n(W, D) case (inr v2) of {inl x1.e1, inr x2.e2}.a v, (w +1,d + 1), (W +1, D + 1) (tuple) e1 .a v1, (w1,d1), \n(W1, D1) e2 .a v2, (w2,d2), (W2, D2) (e1,e2) .a (v1,v2) , (w1 + w2 +1,d1 + d2 + 1), (W1 + W2 +1, D1 + \nD2 + 1) (ptuple-seq) e1 .seq e2 .seq v1, (w1,d1), (W1, D1) v2, (w2,d2), (W2, D2) (|e1,e2|) .seq (v1,v2) \n, (w1 + w2 +1,d1 + d2 + 1), (W1 + W2 +1, D1 + D2 + 1) (ptuple-par) e1 .par e2 .par v1, (w1,d1), (W1, \nD1) v2, (w2,d2), (W2, D2) (|e1,e2|) .par (v1,v2) , (w1 + w2 +1, max (d1,d2) + 1), (W1 + W2 +1+ t, max \n(D1, D2)+1+ t ) (ptuple-orc-parallelize) e1 .orc e2 .orc w1 = . . w2 = .v1, (w1,d1), (W1, D1) v2, (w2,d2), \n(W2, D2) (|e1,e2|) .orc (v1,v2) , (w1 + w2 +1, max (d1,d2) + 1), (W1 + W2 +1+ t + f, max (D1, D2)+1+ \nt + f) (ptuple-orc-sequentialize) w1 <. . w2 <. e1 .(if w1<. then seq else orc) e2 .(if w2<. then seq \nelse orc) v1, (w1,d1), (W1, D1) v2, (w2,d2), (W2, D2) (|e1,e2|) .orc (v1,v2) , (w1 + w2 +1,d1 + d2 + \n1), (W1 + W2 +1+ f, D1 + D2 +1+ f) Figure 2. Dynamic cost semantics we calculate the (raw or total) work \nand the (raw or total) Sequential mode. Parallel tuples are treated exactly like depth by summing up \nthose of the premises (subexpressions) sequential tuples: evaluating a parallel tuple simply con\u00adand \nadding one unit to include the cost of the judgment. tributes 1 to the raw and the total work (depth), \nwhich For all expressions, including parallel tuples, each evaluation are computed as the sum of the \nwork (depth) of the two step contributes 1 to the raw work or raw depth. When branches plus 1. In the \nsequential mode, raw and total calculating total work and total depth, we take into account work (depth) \nare the same. the cost of creating a parallel task t and the cost of making Parallel mode. The evaluation \nof parallel tuples induces an oracle decision f. an additional constant cost t. The depth is computed \nas Evaluation of parallel tuples vary depending on the mode.  the maximum of the depths of the two branches \nof the parallel tuple plus 1, and work is computed as the sum of the work of the two branches plus t \n. In the oracle mode, there are two cases. If the parallel tuple is scheduled sequentially, then its \ncosts 1 unit. Raw/total work and depth are both calculated as the sum of the depth of the branches plus \none. If the parallel tuple is evaluated in parallel, then an extra cost t is included in the total work \nand depth and the depth is computed as the maximum of the depth of the two branches. Oracle mode. The \nscheduling of a parallel tuple depends on the amount of raw work involved in the two branches. If the \nraw work of each branch is more than ., then the tu\u00adple is evaluated in parallel in the oracle mode. \nOtherwise, the raw work of at least one branch is less than ., and the tuple is executed sequentially. \nWhen evaluating a paral\u00adlel tuple sequentially, the mode in which each branch is evaluated depends on \nthe work involved in the branch. If a branch contains more than . units of raw work, then it is evaluated \nin oracle mode, otherwise it is evaluated in sequential mode. This switching to sequential mode on small \ntasks is needed for ensuring that the oracle is not called too often during the evaluation of a program. \n 2.2 Generalized Brent s theorem In order to relate the total work and total depth of a pro\u00adgram with \nits execution time, we rely on Brent s theorem. This theorem is usually formulated in terms of computation \nDAGs. A computation DAG is a directed acyclic graph that Figure 3. An example computation DAG. complex \nproofs. Another approach is to represent non-unit cost tasks with a sequence of unit tasks, e.g., we \ncan replace a task with weight three with a sequence of three unit-cost tasks. Since overheads are non-divisible \nwork, we would re\u00adquire that such tasks execute on the same processors back to back without interleaving \nwith other tasks. With this ap\u00adproach, typical proofs of Brent s theorem, which assume a level-by-level \nexecution schedule, do not work because they break up sequences. Fortunately, we have found that Arora \net al s proof [3] can be adapted easily for this purpose, because it makes no assumption about ordering \nof ready nodes, directly allowing us to generalize Brent s theorem to include task-creation overheads. \nTheorem 2.1 (Brent s theorem for computation DAGs) Let G be a computation DAG made of W nodes and whose \nlongest path has length D. Any greedy scheduler can exe\u00ad represents a parallel computation. Nodes in \nthe graph rep-cute this computation DAG in no more than PW + D steps on resent atomic computations. Edges \nbetween nodes represent precedence relations, in the sense that an edge from a to b in\u00addicates that the \nexecution of a must be completed before the execution of b can start. Every computation DAG includes \na source node and a sink node, representing the starting and the end points of the computation, respectively. \nThose nodes are such that all nodes of a computation DAG are reachable from the source node, and the \nsink node is reachable from all nodes. An example computation DAG appears in Figure 3. P processors. \nProof At each execution step, each processor places a token in the work bucket if it is busy at this \nstep, otherwise it places a token in the idle bucket. The work bucket contains exactly W tokens at the \nend of the execution. Let I be the number of tokens contained in the idle bucket at the end of the execution, \nand let T denote the total number of steps in the execution. Because a total TP tokens are created, we \nhave A node is said to be ready if all the nodes that points to it TP = W + I. In order to establish \nthe result T = PW + D, have already been executed. Brent s theorem gives a bound on the time required \nfor executing all the tasks in a computation DAG with a greedy scheduler, assuming that each node takes \na unit of time to execute. A scheduler is said to be greedy if it never stays idle unnecessarily, i.e., \nwhen there exists a ready node the scheduler .nds it at no cost and executes it. Typical proofs of Brent \ns theorem assume a unit cost model where each instruction costs a unit cost to execute and construct \na level\u00adby-level execution schedule. One way to extend the Brent s theorem to include task\u00adcreation overheads \nis to assign a weight to each node. Prov\u00ading such a generalization directly, however, turns out to be \nhighly nontrivial and in our attempts resulted in relatively it thus suf.ces to establish the inequality \nI = PD. Consider a given time step. If all processors are execut\u00ading then the idle bucket receives zero \ntokens. Otherwise, a number of processors are idle. In this case, the idle bucket receives between one \nand P - 1 tokens. We can bound the number of time steps at which this situation happens, as fol\u00adlows. \nIf one or more processors are idle, it means that those processors cannot .nd a ready task to execute. \nBecause the scheduler is assumed to be greedy, it must be the case that all the ready tasks are currently \nexecuting. Therefore, at such a time step, the maximal length of a path in the computation DAG starting \nfrom a ready node decreases by one unit. Be\u00adcause the maximal length of a path in the computation DAG \nis initially D, there can be at most D time steps at which not all processors are executing. It follows \nthat the .nal number of tokens in the idle bucket does not exceeed (P -1)D. This result entails the inequality \nI = PD. D  Observe that the proof does not impose any constraint on the order in which the ready tasks \nshould be executed by the processors. So, if one processor starts working on a sequence of several nodes, \nthen it can execute all the nodes in the sequence before looking for other ready tasks. Therefore, the \nproof accepts computation DAGs that encode non-unit tasks as sequences of unit tasks. We will make use \nof such an encoding in the proof of our next theorem, which relates our cost semantics to the computation \nDAG model. Theorem 2.2 (Brent s theorem for the cost semantics) Assume e .orc v, (w, d), (W, D) to hold \nfor some v, w and d. Any greedy scheduler can execute the expression e in no more than W + D computations \nsteps on P processors. P Proof In order to invoke the version of Brent s theorem that applies to computation \nDAGs, we build the computa\u00adtion DAG associated with the execution of the expression e, including nodes \nthat represent the cost of scheduling. To that end, we describe a recursive algorithm for turning an \nexpression e with total work W and total depth D into a corresponding computation DAG containing W nodes \nand whose longest path has length D. The algorithm follows the structure of the derivation that e has \ntotal work W and total depth D. If the last rule has zero premises, then e is an atomic expression and \nW = D =1. We build the corresponding DAG as a single node.  If the last rule has one premise, then W \ntakes the form W1 +1 and D takes the form D1 +1. Let G1 be the DAG corresponding to the sub-expression \ndescribed in the premise. We build G by extending G1 with one node at the bottom, that is, by sequentially \ncomposing G1 with a DAG made of a single node.  Otherwise the last rule has two premises. First, consider \nthe case where e is a let-expression. W takes the form W1 + W2 +1 and D takes the form D1 + D2 +1. Let \nG1 and G2 be the DAGs corresponding to the two sub\u00adexpressions. We build G by sequentially composing \nG1 with a single node and then with G2.  Consider now the case of a parallel tuple that is sequen\u00adtialized. \nW takes the form W1 + W2 +1+ f and D takes the form D1 + D2 +1+ f. Let G1 and G2 be the DAGs corresponding \nto the two branches. We build G by sequentially composing 1+ f unit-cost nodes with the sequential composition \nof G1 and G2.  Finally, consider the case of a parallel tuple that is par\u00adallelized. W takes the form \nW1 + W2 +1+ t + f and D takes the form max (D1, D2)+1+ t + f. Let G1 and G2 be the DAGs corresponding \nto the two branches. We  build G by sequentially composing 1+ t + f unit-cost nodes with the parallel \ncomposition of G1 and G2. It is straightforward to check that, in each case, W and D match the number \nof nodes and the total depth of the DAG being produced. D 3. Analysis We analyze the impact of task creation \noverheads on paral\u00adlel execution time and show how these costs can be reduced dramatically by using our \noracle semantics. For our analy\u00adsis, we .rst consider an ideal oracle that always makes per\u00adfectly accurate \npredictions (about the raw work of expres\u00adsions) without any overhead (i.e., f =0). Such an ideal or\u00adacle \nis unrealistic, because it is practically impossible to de\u00adtermine perfectly accurately the raw work \nof computations. We therefore consider a realistic oracle that approximates the raw work of computations \nby performing constant work. Our main result is a theorem that shows that the ideal oracle can reduce \nthe task-creation overheads to any desired con\u00adstant fraction of the raw work with some increase in depth, \nwhich we show to be small for a reasonably broad class of computations. 3.1 Ideal oracle We quantify \nthe relationships between raw work, raw depth and total work, total depth for each mode. Theorem 3.1 \n(Work and depth) Consider an expression e such that e .a v, (w, d), (W, D). Assume f =0. The following \ntight bounds can be obtained for total work and total depth, on a machine with P processors where the \ncost of creating parallel tasks is t . a Bound on total work Bound on total depth seq W = w D = d = \nw par W = (1 + t 2 ) w D = (1 + t ) d orc W = (1 + t .+1 ) w D = (1 + max (t, .)) d Proof The equations \nconcerning the sequential semantics follow by inspection of the semantics of the source language (Figure \n2). The inequalities for the parallel and the oracle modes follow directly by our more general bounds \npresented later (Theorems 3.2 and 3.3). To prove that the inequalities for the parallel and the oracles \nmodes are tight, we give example computation that achieve the bounds. Parallel mode. Consider an expression \nconsisting only of parallel tuples with n leaves, and thus n - 1 internal nodes . The raw work w is equal \nto n +(n -1) while the total work W is equal to n+(n-1)(1+t ). We therefore n1 nt have W =(1+ )w = 1+ \nt w. As n increases, 2n+1 n1 2 the bound approaches 1+ t w and thus the bound on 2 the total work is \ntight. To see that the depth bound is also tight, note that each parallel tuple adds 1 to the raw depth \nand 1+ t to the total depth. The total depth therefore can be as much as 1+ t times greater than the \nraw depth.  Oracle mode. Consider an expression with n nested par\u00adallel tuples, where tuples are always \nnested in the right branch of their parent tuple. The tuples are built on top of expressions that involve \n. units of work. In the oracle semantics, all the tuples are executed in parallel. Thus the raw work \nw is n+(n+1)., the total work W is n(1+t )+ i +i+ nt t (n+1)., and W = w 1+ = w 1+ . n(.+1)+..+1 i+ t \nAs n increases, the bound approaches 1+ w and .+1 thus the bound on the total work is tight. For the \ndepth bound, we consider two cases. In the .rst case, we have t = .. Using the same example, the raw \ndepth is d = n+1, the total depth is D = n(1+t )+., and i+ D = 1+ nt+.-1 d = (1 + t) d. As n increases, \nD n+1 approaches (1 + t) d and thus the bound is tight. For the second case when . = t , we change the \nexample slightly by reducing the amount of raw work in each leaf to just under .. This will cause all \nthe parallel tuples to be evaluated sequentially; the raw depth is d = n + . and the total depth is equal \nto the total work, i.e., D = n + i+ n. (n + 1). = 1+ d. As n increases, D approaches n+. (1 + .) d and \nthus the bound is tight. D This theorem leads to some important conclusions. First, the theorem shows \nthat task creation (scheduling) costs mat\u00adter a great deal. In a parallel evaluation, the total work \nand total depth can be as much as t times larger than the raw depth and raw work. This essentially implies \nthat a paral\u00adlel program can be signi.cantly slower than its sequential counterpart. If t is large compared \nto the number of pro\u00adcessors, then even in the ideal setting, where the number of parallel processors \nis small relative to t, we may observe no speedups. In fact, it is not uncommon to hear anecdotal evidence \nof this kind of slowdown in modern computer sys\u00adtems. Second, the theorem shows that evaluation of a \nprogram with an ideal oracle can require as much as . less work than 2 in the parallel mode. This comes \nat a cost of increasing the depth by a factor of . . Increasing the depth of a computa\u00ad t tion can hurt \nparallel execution times because many parallel schedulers rely on the availability of large degree of \nparal\u00adlelism to achieve optimal speedups. Unless done carefully, increasing the depth can dramatically \nreduce parallel slack\u00adness. In the common case, however, where there is plenty of parallelism, i.e., \nwhen w is far greater than d, we can safely P increase depth by a factor of . to reduce the task-creation \nt overheads. Concretely, if parallel slackness is high and . is w not too large, then .d remains small \ncompared to , and P tw w becomes much smaller than t , dramatically reducing .P 2 P task-creation overheads \nwithout harming parallel speedups.  3.2 Realistic oracles The analysis that we present above makes two \nunrealistic assumptions about oracles: 1) that they can accurately pre\u00addict the raw work for a task, \nand 2) that the oracle can make predictions in zero time. Realizing a very accurate oracle in practice \nis dif.cult, because it requires determining a pri\u00adori the execution time of a task. We therefore generalize \nthe analysis by considering an approximate or realistic oracle that can make errors up to a multiplicative \nfactor \u00b5 when es\u00adtimating raw work. For example, an oracle can approximate raw work up to a constant \nfactor of \u00b5 =3, i.e., a task with raw work w would be estimated to perform raw work be\u00ad w tween and 3w. \nAdditionally, we allow the oracle to take 3 some (.xed) constant time, written f, to provide its answer. \nWe show that even with a realistic oracle, we can reduce task creation overheads. We start with bounding \nthe depth; the result implies that the total depth is no larger than \u00b5. times the raw depth when . is \nlarge compared to t and f. Since with the ideal oracle this factor was ., the bound implies that the \nimprecision of the oracle can be in.uenced by changing the constant multiplicative factor. Theorem 3.2 \n(Depth with a realistic oracle) e .orc v, (w, d), (W, D) . D= (1+max (t, \u00b5.)+f) d Proof Let . denote \n1+ max (t, \u00b5.)+ f; we want to prove that D= .d. The proof is by induction on the derivation e .orc v, \n(w, d), (W, D). For a rule with zero premises, we have D = d =1. Because . = 1, it follows that D= .d. \n For a rule with one premise, we know by induction hypothesis that D= .d. Using again the fact that \n. = 1, we can deduce the inequality D +1 = .(d + 1).  For a rule with two premises, we can similarly \nestablish the conclusion D1 + D2 +1 = .(d1 + d2 + 1) using the induction hypotheses D1 = .d1 and D2 = \n.d2.  Now, consider the case of a parallel tuple. First, assume that the two branches of this tuple \nare predicted to be large. In this case, the tuple is executed in parallel and the branches are executed \nin oracle mode. We exploit the induction hy\u00adpotheses D1 = .d1 and D2 = .d2 to conclude as follows:  \nD = max (D1, D2)+1+ t + f = max (.d1, .d2)+1+ max (t, \u00b5.)+ f = max (.d1, .d2)+ . = . (max (d1,d2) + 1) \n= .d Consider now the case where both branches are pre\u00addicted to be small. In this case, the tuple is \nexecuted sequen\u00adtially. Because the oracle predicts the branches to be smaller than ., they must be actually \nsmaller than \u00b5.. So, we have w1 = \u00b5. and w2 = \u00b5.. Moreover, both branches are exe\u00adcuted according to \nthe sequential mode, so we have D1 = w1 and D2 = w2. It follows that D1 = \u00b5. and D2 < \u00b5.. Be\u00adlow, we \nalso exploit the fact that max (d1,d2) = 1, which comes from the fact that raw depth is at least one \nunit. We conclude as follows:  D = D1 + D2 +1+ f = \u00b5. + \u00b5. +1+ f = (1 + \u00b5. + f) * 2 = (1 + max (t, \u00b5.)+ \nf) \u00b7 (max (d1,d2) + 1) = .d It remains to consider the case where one branch is predicted to be smaller \nthan the cutoff while the other branch is predicted to be larger than the cutoff. In this case again, \nboth branches are executed sequentially. Without loss of generality, assume that the second branch is \npredicted to be small. In this case, we have w2 = \u00b5.. This .rst branch is thus executed according to \nthe sequential mode, so we have D2 = d2 = w2. It follows that D2 = \u00b5.. For the .rst branch, which is \nexecuted according to the oracle mode, we can exploit the induction hypothesis which is D1 = .d1. We \nconclude as follows: D = D1 + D2 +1+ f = .d1 + \u00b5. +1+ f = .d1 + (1 + max (t, \u00b5.)+ f) = . (d1 + 1) = . \n(max (d1,d2) + 1) = .d D This ends our analysis of the depth. Now, let us look at the work. The fact \nthat every call to the oracle can induce a cost f can lead the work to be multiplied by f. For example, \nconsider a program made of a complete tree built using n-1 sequential tuples, and leading to n parallel \ntuples generating 2n values as leaves. The raw work is equal to (n-1)+n+2n, and the total work is (n \n-1)+ nf +2n. Thus, W= f w and 4 this is tight for large values of n. This means that a program executed \naccording to the oracle semantics can slow down by as much as f/4. The problem with the above example \nis that the oracle is called infrequently only at the leaves of the computation preventing us from amortizing \nthe cost of the oracle towards larger pieces of computations. Fortunately, most programs do not exhibit \nthis pathological behavior, because parallel tuples are often performed close to the root of the computa\u00adtion, \nallowing us to detect smaller pieces of work early. One way to prevent the oracle from being called on \nsmaller pieces of work is to make sure that it is called at regular intervals. For proving a strong bound \non the work, we will simply assume that the oracle is not called on small tasks by restricting our attention \nto balanced programs. To this end, we de.ne balanced programs as programs that call the oracle only on \nexpressions that are no smaller than some constant . off from the value . , for some . = 1. Note that \n\u00b5 . we use as a target and not . so as to accomodate possible \u00b5 over-estimations in the estimations of \nraw work. The formal de.nition follows. De.nition 3.1 (Balanced programs) For . = 1, a program or expression \ne is .-balanced if evaluating e in the oracle mode invokes the oracle only for subexpressions whose raw \nwork is no less than . . \u00b5. Note that if a program is .-balanced and if .<.!, then this program is also \n.!-balanced. We will later give a suf.cient condition for proving that particular programs are balanced \n(\u00a73.4). Theorem 3.3 (Work with a realistic oracle) Assume e .orc v, (w, d), (W, D) where e is a .-balanced \nprogram. \u00b5(t + .f) W=1+ w. . +1 Proof We establish the following slightly tighter inequality. tf W=1+ \n+ w. ./\u00b5 +1 ./(\u00b5.)+1 The bound is indeed tighter because . = 1 and \u00b5 = 1. De.ne .! as a shorthand for \n./\u00b5 and .!! as a shorthand for + ./(\u00b5.). Note that, because . = 1, we have .!! = .!. Let xbe de.ned as \nthe value x when x is nonnegative and as zero otherwise. We prove by induction that: (w-. )+ (w-.)+ \nW= w + t+ f . +1. +1 This is indeed a strengthened result because we have: (w-. )+ wt t= t = w . +1. \n+1 ./\u00b5+1 (w-. )+ and f= f w = f w . +1. +1 ./(\u00b5.)+1 The proof is conducted by induction on the derivation \nof the reduction hypothesis. For a rule with zero premises, which describe an atomic operation, we have \nW = w =1, so the conclusion is satis.ed.  For a rule with a single premise, the induction hypoth\u00adesis \nis:  (w-. )+ (w-. )+ W= w + t+ f . +1. +1 So, we can easily derive the conclusion: ((w+1)-. )+ ((w+1)-. \n)+ W +1 = (w + 1) + t+ f . +1. +1  For a rule with two premises, we exploit the mathemat\u00ad nm n+m ical \ninequality + = . We have: qq q W = W1 + W2 +1 (w1-. )+ (w1-. )+ = w1 + t + f . +1 . +1 (w2-. )+ (w2-. \n)+ + w2 + t + f +1 . +1 . +1 (w1-. )+ +(w2-. )+ = w + t . +1 (w1-. )+ +(w2-. )+ + f . +1 To conclude, \nwe need to establish the following two mathe\u00admatical inequalities: ++ + (w1 - .!)+(w2 - .!)= ((w1 + w2 \n+ 1) - .!) (w1 - .!!)++ + +(w2 - .!!)= ((w1 + w2 + 1) - .!!) The two equalities can be proved in a similar \nway. Let us establish the .rst one. There are four cases to consider. First, if both w1 and w2 are less \nthan .!, then the right-hand side is zero, so we are done. Second, if both w1 and w2 are greater than \n.!, then all the expressions are nonnegative, and we are left to check the inequality w1 - .! + w2 - \n.! = w1 + w2 +1 - .!. Third, if w1 is greater than .! and w2 is + smaller than .!, then the inequality \nbecomes (w1 - .!)= ((w1 - .!)+(w2 + 1))+, which is clearly true. The case w1 = .! and w2 <.! is symmetrical. \nThis concludes the proof. Consider now the case of a parallel tuple where both branches are predicted \nto involve more than . units of work. This implies w1 = .! and w2 = .!. In this case, a parallel task \nis created. Note that, because .!! = .!, we also have w1 = .!! and w2 = .!!. So, all the values involved \nin the fol\u00adlowing computations are nonnegative. Using the induction hypotheses, we have: W = W1 + W2 \n+1+ t + f w1-.w1-. = w1 + t + f . +1 . +1 w2-.w2-. + w2 + t + f +1+ t + f . +1 . +1 w1-.w2-. = (w1 + \nw2 + 1) + t( + +1) . +1 . +1 w1-.w2-. + f( + +1) . +1 . +1 (w1-. )+(w2-. )+(. +1) = w + t . +1 (w1-. \n)+(w2-. )+(. +1) + f . +1 (w1+w2+1)-. (w1+w2+1)-. = w + t + f . +1 . +1 w-.w-. = w + t + f . +1 . +1 \nAssume now that the two branches are predicted to be less than the cutoff. This implies w1 = .! and w2 \n= .! . Both these tasks are executed sequentially, so W1 = w1 and W2 = w2. Since the program is .-balanced, \nwe have w1 = .!! and w2 = .!!. Those inequalities ensure that we are able to pay for the cost of calling \nthe oracle, that is, the cost f. Indeed, since we have w1 + w2 +1 - .!! = .!! +1, w1+w2+1-. we know that \n. +1 = 1. Therefore: W = W1 + W2 +1+ f = w1 + w2 +1+ f w1+w2+1-. = (w1 + w2 + 1) + f . +1 (w-. )+ w-. \n= w + t + f . +1 . +1 It remains to consider the case where one branch is predicted to be bigger than \nthe cutoff while the other is predicted to be smaller than the cutoff. For example, assume w1 = .! and \nw2 = .!. The parallel tuple is thus executed as a sequential tuple. The .rst task is executed in oracle \nmode, whereas the second task is executed in the sequential mode. For the .rst task, we can invoke the \ninduction hypothesis w1-.w1-. W1 = w1 + t + f . For the second . +1 . +1 task, which is executed sequentially, \nwe have W2 = w2. .!! Moreover, the regularity hypothesis gives us w2 = . Hence, we have w2+1 = 1. We \nconclude as follows: . +1 W = W1 + W2 +1+ f w1-.w1-. = w1 + t + f + w2 +1+ f . +1 . +1 w1-.w1-.w2+1 = \nw1 + t + f + w2 +1+ f . +1 . +1 . +1 w1+w2+1-.w1+w2+1-. = w + t + f . +1 . +1 w-.w-. = w + t + f . +1 \n. +1 D We are now ready to combine the version of Brent s theo\u00adrem adapated to our cost semantics with \nthe bounds that we have established for the total work and depth in .-balanced parallel programs executed \nunder the oracle semantics. Theorem 3.4 (Execution time with a realistic oracle) Assume an oracle that \ncosts f and makes an error by a factor not exceeding \u00b5. Assume .>t, which is always the case in practice. \nThe execution time of a parallel .-balanced program on a machine with P processors under the oracle semantics \nwith a greedy scheduler does not exceed the value \u00b5(t + .f) w 1+ +(.\u00b5 + f + 1) d. .P Proof The bound \nfollows by the version of Brent s theorem adpated to our cost semantics (Theorem 2.2), and by the bounds \nestablished in Theorem 3.3 and Theorem 3.2. For simplicity, we have replaced the denominator . +1 with \n.. This change does not loosen the bound signi.cantly because . is usually very large in front of a unit \ncost. D  Figure 4. An illustration of the run-time function i+ w 1+ \u00b5(t+.f) +(.\u00b5 + f + 1) d on P =4 \npro\u00ad .P cessors with constants \u00b5 =1, t =5, . =1, and f =2, and different work and depth values.  3.3 \nChoice of the cutoff Theorem 3.4 shows that the running time of a parallel pro\u00adgram can be controlled \nby changing the constant .; the for\u00admula, however, reveals an interesting tradeoff: we can re\u00adduce task-creation \noverheads but this comes at the cost of increasing the depth. To see this connection better, consider \nthe bound that appears in the statement of Theorem 3.4 and notice that as . increases the work (.rst) \nterm decreases but the depth (second) term increases. Figure 4 illustrates a con\u00adcrete instance of the \nbound for a hypothetical computation for .xed constants but different raw work and raw depth. The exact \nvalues of the constant and the raw work and depth are not relevant to our discussion; constants are .xed \nat some reasonable values consistent with our experimental obser\u00advations. The work and depth are consistent \nwith a program whose raw work is linear in the input size and whose raw depth is logarithmic in the input \nsize. As Figure 4 illustrates, the parallel run time decreases as we increase . up to some in.ection \npoint and then starts increasing. We compute the optimal value for . by solving for the root of the derivative. \nWe obtain: w . * =t + .f \u00b7. Pd Thus, with prior knowledge of the raw work and raw depth of a computation, \nwe can pick . to ensure ef.ciency of parallel programs. Such knowledge, however, is often unavailable. \nAs we now show, we can improve ef.ciency of parallel programs by selecting a .xed . that guarantees that \nthe task creation overheads can be bounded by any constant fraction of the raw work, without increasing \nthe depth of the computation signi.cantly. Theorem 3.5 (Run time with .xed .) Consider an oracle with \nf cost and \u00b5 error. For any . = 1 and for any constant r such that 0 <r< 1, there exists a constant . \nand a constant c such that the evaluation with the oracle semantics of a .-balanced program reduces task \ncreation overheads to a fraction r of the raw work, while in the same time increasing c the total depth \nby no more than a factor . With a greedy r scheduler, the total parallel run time on P processors of \nsuch a program therefore does not exceed (1 + r) w + c d. Pr Proof Consider a particular .-balanced program \nwith raw work w and raw depth d, and consider its evaluation under the oracle semantics. By Theorem 3.3 \nwe know that total work does not exceed \u00b5(t + .f) 1+ w. . To achieve the desired bound on execution time, \nwe take \u00b5(t+.f) . = . Plugging this value of . into the formula r yields (1 + r) w for total work, showing \nthat task creation overheads are reduced to a fraction r of the raw work. Furthermore, by Theorem 3.2 \nwe know that the total depth is bounded by (max (t, \u00b5.)+ f +1) d. Plugging in the same value for . yields \nthe following bound on total depth: \u00b52(t + .f) D= max t, + f +1 d. r Using \u00b5 = 1 and r< 1, we can derive \nthe inequality \u00b52(t + .f) f +1 D= + d. rr Choosing c = \u00b52(t + .f)+ f +1 therefore ensures that the total \ndepth does not exceed the desired bound d. The r run-time bound follows by an application of Brent s \ntheorem (Theorem 2.2). D This .nal theorem enables us to reduce task creation overheads to any desired \nconstant fraction of the raw work by choosing a . that is independent of the speci.c inputs. This comes \nat the cost of increasing the depth, but only c by a constant factor of . In the common case, when the \nr work is asymptotically greater than depth, e.g., T(n) versus O(log n), the resulting run-time guarantees \nthat the increase in depth remain small: speci.cally, the depth term itself is a fraction of the work \nterm for all but a constant number of small inputs.  3.4 Balanced programs Our bounds with the realistic \noracle hold only for what we called .-balanced programs, where the oracle is not called on small tasks. \nThis assumption can be satis.ed by call\u00ading the oracle regularly. It seems likely that this assump\u00adtion \nwould hold for many programs without requiring any changes to the program code. In this section, we show \nthat recursive, divide-and-conquer programs are .-balanced. To that end, we introduce the notion of .-regularity. \nIntu\u00aditively, a program is .-regular if, between any two calls to the oracle involved in the execution \nof this program, the amount of work does not reduce by more than a factor .. We will then establish that \nany .-regular program is a .-balanced program. Before giving the formal de.nition of .-regularity, we \nneed to formally de.ne what it means for a parallel tuple to be dominated by another parallel tuple. \n De.nition 3.2 (Domination of a parallel branch) A branch e of a parallel tuple is said to be dominated \nby the branch ei of another parallel tuple (|e1,e2|) if the expression e is involved in the execution \nof the branch ei. De.nition 3.3 (Regularity of a parallel program) A pro\u00adgram is said to be .-regular \nif, for any parallel branch in\u00advolving, say, w units of raw work, either w is very large compared with \n./(\u00b5.) or this branch is dominated by an\u00adother parallel branch that involves less than .w units of work. \nThe condition w is very large compared with ./(\u00b5.) is used to handle the outermost parallel tuples, which \nare not dominated by any other tuple. Note that the regularity of a program is always greater than 2. \nIndeed, if one of the branch of a parallel tuple is more than half of the size of the entire tuple, then \nthe other branch must be smaller than half of that size. On the one hand, algorithms that divide their \nwork in equal parts are .-regularity with . very close to 2. On the other hand, ill\u00adbalanced programs \ncan have a very high degree of regularity. Observe that every program is 8-regular. For example, consider \na program that traverses a com\u00adplete binary tree in linear time. A call on a tree of size n has raw work \nnc, for some constant c. If the tree is not a leaf, its size n has to be at least 3. The next recursive \ncall oa n-1 involves raw work c, The ratio between those two o 2 a n-1 values is equal n/ . This value \nis always less than 3 2 when n = 3. So, the traversal of a complete binary tree is a 3-regular algorithm. \nThe following lemma explains how the regularity as\u00adsumption can be exploited to ensure that the oracle \nis never invoked on tasks of size less than ./(\u00b5.). This suggests that, for the purpose of amortizing \nwell the costs of the oracle, a smaller regularity is better. Lemma 3.1 (From regularity to balanced) \nIf a program is .-regular then it is .-balanced. Proof We have to show that, during the execution of \na .\u00adregular program according to oracle semantics, the oracle is never invoked on subexpressions involving \nless than ./(\u00b5.) raw work. Consider a particular subexpression e involving w units of raw work, and assume \nthat the oracle is invoked on this subexpression. Because the oracle is being invoked, e must correspond \nto the branch of a parallel tuple. By the regularity assumption, either w is very large compared with \n./(\u00b5.), in which case the conclusion holds immediately, or the branch e is dominated by a branch ei that \ninvolves that involves w! units of work, with w! = .w. For the latter case, type cost type estimator \nval create: unit -> estimator val report: estimator\u00d7 cost\u00d7 float -> unit val predict: estimator\u00d7 cost-> \nfloat Figure 5. The signature of the estimator data structure we need to establish w = ./(\u00b5.). To that \nend, it suf.ces to prove that w! = ./\u00b5, which amounts to showing that the amount of raw work associated \nwith the dominating branch ei contains at least ./\u00b5 raw work. We conclude the proof by establishing the \ninequality ! w= ./\u00b5. Because the oracle is being invoked on the subexpression e, it means that e is being \nevaluated in the mode orc. Therefore, the call to the oracle on the dominat\u00ading branch ei must have predicted \nei to contain more than . raw work. (Otherwise ei and its subexpression e would have both been executed \nin the sequential mode.) Given that the oracle makes error by no more than a factor \u00b5, if ei is predicted \nto contain more than . units of raw work, then ei must contain at least ./\u00b5 units of raw work. So, w! \n= ./\u00b5. D 4. Oracle Scheduling As we describe in this section, we can realize the oracle semantics by \nusing a (f, \u00b5)-estimator that requires f time to estimate actual run-time of parallel tasks within a \nfactor of no more than \u00b5. We refer to the combination of an estimator with a parallel scheduler as an \n(f, \u00b5)-oracle-scheduler. Run-time estimators. To realize the oracle semantics, we require the user to \nprovide a cost function for each function in the program and rely on an estimator for estimating ac\u00adtual \nwork using the user-provided cost information. When applied to an argument v, a cost function of f returns \nthe abstract cost of the application of f to v. The cost is passed to the estimator, which uses the cost \nto compute an estimate of the actual execution time, that is, the raw work, of the ap\u00adplication. Figure \n5 shows a signature for the estimator. To perform accurate estimates, the estimator utilizes pro.ling \ndata obtained from actual execution times. The sampling op\u00aderation report (t, c, e) adds a cost c and \nan execution time e to the set of samples in an estimator t. An estimate of the actual execution time \nis obtained by calling predict. Given an estimator t and cost c, the call predict (t, c) returns a predicted \nexecution time. Compilation. To support oracle scheduling with estima\u00adtors, we need compilation support \nto associate an estimator with each function de.ned in the program code, to derive a sequential and an \noracle version for each function, and to evaluate tuples sequentially or in parallel depending on the \napproximations performed by the estimator. For simplicity, we assume that constituents of parallel tuples \nare function applications, i.e., they are of the form (|f1 v1,f2 v2|). Note that this assumption does \nnot cause loss of expressiveness, because a term e can always be replaced by a trivial application of \na thunk , a function that ignores its argument (typically of type unit ) and evaluates e to a dummy argument. \nThroughout, we write fun f.x.eb [ec] to denote a function fun f.x.eb for which the cost function for \nthe body eb is described by the expression ec. This expression ec, which may refer to the argument x, \nshould be an expression whose evaluation always terminates and produces an cost of type cost.  To associate \nan estimator with each function, in a sim\u00adple pass over the source code, we allocate and initialize an \nestimator for each syntactic function de.nition. For ex\u00adample, if the source code contains a function \nof the form fun f.x.eb [ec] , then our compiler allocates an estimator speci.c to that function de.nition. \nSpeci.cally, if the vari\u00adable r refers to the allocated estimator, then the translated function, written \nfun f.x.eb [ec|r] , is annotated with r. The second pass of our compilation scheme uses the allo\u00adcated \nestimators to approximate the actual raw work of func\u00adtion applications and relies on an MakeBranch function \nto determine whether an application should be run in the oracle or in the sequential mode. Figure 6 de.nes \nmore precisely the second pass. We write [v] for the translation of a value v, and we write [e]a for \nthe translation of the expression e according to the semantics a, which can be either seq or orc. When \nspecifying the translation, we use triples, quadru\u00adples, projections, sequence, if-then-else statements, \nand unit value; these constructions can all be easily de.ned in our core programming language. Translation \nof values other than functions does not de\u00adpend on the mode and is relatively straightforward. We trans\u00adlate \nfunctions, which are of the form fun f.x.eb [ec|r] , into a quadruple consisting of the estimator r, \na sequential cost function, the sequential version of the function, and the oracle versions of the function. \nTranslation of a function ap\u00adplication depends on the mode. In the sequential mode, the sequential version \nof the function is selected (by projecting the third component of the function) and used in the appli\u00adcation. \nSimilarly, in the oracle mode, the oracle version of the function is selected and used in the application. \nTo trans\u00adlate a tuple, we recursively translate the subexpression, while preserving the mode. Similarly, \ntranslation of the let, pro\u00adjections, and case constructs are entirely structural. In the sequential \nmode, a parallel tuple is turned into a simple tuple. In the oracle mode, the translation applies the \noracle-based scheduling policy with the aid of the meta\u00adfunction MakeBranch. This meta-function, shown \nin Fig\u00adure 7, describes the template of the code generated for preparing the execution of a parallel \ntuple. MakeBranch ex\u00adpects a (translated) function f and its (translated) argument v, and it returns \na boolean b indicating whether the applica\u00adtion of f to v is expected to take more or less time than \nthe cutoff ., and a thunk t to execute this application. On the MakeBranch (f, v) = let r = proj1 f in \nlet m = proj2 fv in let b = predict(r, m) >. in let fun kseq () = proj3 fv in let fun k! () = MeasuredRun(r, \nm, kseq) in seq let fun korc () = proj4 fv in let k = if b then korc else k! in seq (b, k) MeasuredRun \n(r, m, k) = let t = get time () in let v = k () in let t! = get time () in report (r, m, (t! - t)); v \nFigure 7. Auxiliary meta-functions used for compilation. one hand, if the application is predicted to \ntake more time than the cutoff (in which case b is true), then the thunk t cor\u00adresponds to the application \nof the oracle-semantics version of the function f. On the other hand, if the application is predicted \nto take less time than the cutoff (in which case b is false), then the thunk t corresponds to the application \nof the sequential-semantics version of the function f. Moreover, in the latter case, the time taken to \nexecute the application sequentially is measured. This time measure is reported to the estimator by the \nauxiliary meta-function MeasuredRun (Figure 7), so as to enable its approximations. Observe that the \ntranslation introduces many quadruples and applications of projection functions. However, in prac\u00adtice, \nthe quadruples typically get inlined so most of the pro\u00adjections can be computed at compile time. Observe \nalso that the compilation scheme involves some code duplica\u00adtion, because every function is translated \nonce for the se\u00adquential mode and once for the oracle mode. In theory, the code could grow exponentially \nwhen the code involves func\u00adtions de.ned inside the body of other functions. In practice, the code the \ngrowth is limited because functions are rarely deeply nested. If code duplication was a problem, then \nwe can use .attening to eliminate deep nesting of local func\u00adtions, or pass the mode a as an extra argument \nto functions. Cost as complexity functions. The techniques described in this section require the programmer \nto annotate each func\u00adtion de.ned in the program with a cost function that, when applied to the argument, \nreturns an abstract cost value. This abstract cost value is then used by an estimator, which is also \nleft abstract, to approximate the actual raw work of a task. For our bounds to apply, complexity expressions \nshould re\u00adquire constant time to evaluate. Predicting the raw work is only needed for sequential tasks, \nso the estimator actually needs to return an approx\u00adimation of the actual run time of a sequential task. \nA cru\u00ad  = x x](v1,v2)] = ([v1], [v2]) inl v = inl v inr v] = inr [ v] [ fun f.x.eb [ec|r]] = (r, (fun \n.x.[ec]seq), (fun f.x.[eb]seq), (fun f.x.[eb]orc)) a = v] [v] seq v1 v2 = proj3 v1 v2 v1 v2] orc = \nproj4 [ v1] [ v2] a a (e1,e2)] = ([e1], [e2]a) a a let x = e1 in e2]= let x = [e1]a in [e2] a fst \nv = fst v a snd v] = snd [ v] a a case v of {inl x.e1, inr x.e2}]= case [v] of {inl x.[e1], inr x.[e2]a} \nn 1 [ (|f1 v1,f2 v2|)]seq = proj3 [f1][v1], proj3 [f2][v2] . .let (b1,k1)= MakeBranch( f1 ,v1 ) in [(|f1 \nv1,f2 v2|)]orc = let (b2,k2)= MakeBranch([ f2] , [ v2] ) in . if (b1 &#38;&#38; b2) then (|k1 (),k2 \n()|) else (k1 (),k2 ()) Figure 6. Translation for oracle scheduling. type tree = cial property of the \nabstract cost is that it should be ab\u00adstract enough that the programmer can write the cost func-| Leaf \nof int tions without necessarily knowing the details of the hard-| Node of int * tree * tree ware that \nthe programs will be executed on. Yet, abstract costs should provide suf.cient information to estimate \nthe let size = function |Leaf _-> 1 actual run times. | Size (s,_,_) -> s Asymptotic complexity speci.cations \nserve as a natural cost function by satisfying both of these properties. Since let rec sum t = Oracle.complexity \n(size t); they eliminate hardware speci.c constants, they can be spec\u00adi.ed easily. Using complexity \nfunctions, we can approxi-match t with mate the actual run time of sequentially executed functions |Leaf \nn-> n | Node (size,t1,t2) -> by simply determining the constants hidden by the asymp\u00adtotic complexity \nnotation. Such an approximation can be let (n1,n2) = (| sum t1, sum t2 |) in performed by using the least \nsquares method or similar tech-n1+ n2 niques for data .tting from known samples. Figure 8. An example \nparallel program. In our implementation described in Section 5, we imple\u00adment an approach based on complexity \nfunctions. We de\u00adalect of the Caml language [26], which is a strict functional.ne cost as an integer, \nwhich represents the application of language. Our Caml dialect corresponds to the core Camlthe complexity \nfunction applied to the input size. We ap\u00adlanguage extended with syntax for parallel pairs and com\u00adproximate \nthe actual run time by calculating a single con\u00adplexity annotations. Figure 8 shows a program implementedstant, \nassuming that the constants in all terms of the asymp\u00adin our Caml dialect. This recursive program traverse \na binarytotic complexity are the same. Although assuming a single tree to compute the sum of the values \nstored in the leaves. constant can decrease the precision of the approximations, We use the Caml type \nchecker to obtain a typed syntaxwe believe that it suf.ces because we only have to compute tree, on which \nwe perform the oracle-scheduling translationlower bounds for our functions; i.e., we only need to deter\u00adde.ned \nin Figure 6. We then produce code in the syntax ofmine whether they are big enough for parallel execution. \nParallel ML (PML) [17], a parallel language close to Stan\u00addard ML. The translation from Caml to PML is \nstraight\u00ad 5. Implementation forward because the two languages are relatively similar. We compile our \nsource programs to x86-64 binaries us- In this section, we describe the implementation of our ing Manticore, \nwhich is the optimizing PML compiler. Thescheduling technique in an actual language and system. In Manticore \nrun-time system provides a parallel, generationalour approach, source programs are written in our own \ndi\u00adgarbage collector that is crucial for scaling to more than four processors, because functional programs, \nsuch as the ones we consider, often involve heavy garbage-collection loads. Further details on Manticore \ncan be found elsewhere [16]. In the rest of this section, we explain how we compute the constant factors, \nand we also give a high-level description of the particular work-stealing scheduler on top of which we \nare building the implementation of our oracle scheduler.  Run-time estimation of constants. The goal \nof the oracle is to make relatively accurate execution time predictions at little cost. Our approach \nto implementing the oracle consists of evaluating a user-provided asymptotic complexity func\u00adtion, and \nthen multiplying the result by an appropriate con\u00adstant factor. Every function has its own constant factor, \nand the value for this constant factor is stored in the estimator data structure. In this section, we \ndiscuss the pratical imple\u00admentation of the evaluation of constant factors. In order for the measurement \nof the constant to be lightweight, we simply compute average values of the con\u00adstant. The constant might \nevolve over time, for example if the current program is sharing the machine with another pro\u00adgram, a \nseries of memory reads by the other program may slow down the current program. For this reason, we do \nnot just compute the average across the entire history, but in\u00adstead maintain a moving average, that \nis, an average of the values gathered across a certain number of runs. Maintaining averages is not entirely \nstraightforward. One the one hand, storing data in a memory cell that is shared by all processors is \nnot satisfying because it would involve some synchronization problems. On the other hand, using a different \nmemory cell for every processor is not satisfying ei\u00adther, because it leads to slower updates of the \nconstants when they change. In particular, in the beginning of the execution of a program it is important \nthat all processors quickly share a relatively good estimate of the constant factors. For these reasons, \nwe have opted for an approach that uses not only a shared memory cell but also one data structure local \nto every processor. The shared memory cell associated with each estimator contains the estimated value \nfor the constant that is read by all the processors when they need to predict execution times. The local \ndata structures are used to accumulate statistics on the value of the constant. Those statistics are \nreported on a regular basis to the shared memory cell, by computing a weighted mean between the value \npreviously stored in the shared memory cell and the value obtained out of the local data structure. We \ntreat initializations somewhat specially: for the .rst few measures, a processor always begins by reporting \nits current average to the shared memory cell. This ensures a fast propagation of the information gathered \nfrom the .rst runs, so as to quickly improve the accuracy of the predictions. When implementing the oracle, \nwe faced three technical dif.culties. First, we had to pay attention to the fact that the memory cells \nallocated for the different processors are not allocated next to each other. Otherwise, those cells would \nfall in the same cache line, in which case writing in one of these cells would make the other cells be \nremoved from caches, making subsequent reads more costly. Second, we observed that the time measures \ntypically yield a few out\u00adliers. Those are typically due to the activity of the garbage collector or \nof another program being scheduled by the op\u00aderating system on the same processor. Fortunately, we have \nfound detecting these outliers to be relatively easy because the measured times are at least one or two \norders of magni\u00adtude greater than the cutoff value. Third, the default system function that reports the \ntime is only accurate by one mi\u00adcrosecond. This is good enough when the cutoff is greater than 10 microseconds. \nHowever, if one were to aim for a smaller cutoff, which could be useful for programs exhibit\u00ading only \na limited amount of parallelism, then more accurate techniques would be required, for example using the \nspeci.c processor instructions for counting the number of processor cycles. Work stealing. We implement \nour oracle scheme on top of the work stealing scheduler [11]. In this section we out\u00adline the particular \nimplementation of work stealing that we selected from the Manticore system. Our purpose is to un\u00adderstand \nwhat exactly contributes to the scheduling cost t in our system. In Manticore s work-stealing scheduler, \nall system pro\u00adcessors are assigned to collaborate on the computation. Each processor owns a deque (doubly-ended \nqueue) of tasks rep\u00adresented as thunks. Processors treat their own deques like call stacks. When a processor \nstarts to evaluate a parallel\u00adpair expression, it creates a task for the second subexpres\u00adsion of the \npair and pushes the task onto the bottom of the deque. Processors that have no work left try to steal \ntasks from others. More precisely, they repeatedly select a random processor and try to pop a task from \nthis processor s deque. Manticore s implementation of work stealing [34] adopts a code-specialization \nscheme, called clone translation, taken from Cilk-5 s implementation [19].2 With clone translation, each \nparallel-pair expression is compiled into two versions: the fast clone and the slow clone. The purpose \nof a fast clone is to optimize the code that corresponds to evaluating on the local processor, whereas \nthe slow clone is used when the sec\u00adond branch of a parallel-pair is migrated to another proces\u00adsor. \nA common aspect of between clone translation and our oracle translation (Figure 6) is that both generate \nspecialized code for the sequential case. But the clone translation dif\u00adfers in that there is no point \nat which parallelism is cut off entirely, as the fast clone may spawns subtasks. The scheduling cost \ninvolved in the fast clone is a (small) constant, because it involves just a few local operations, but \nthe scheduling cost of the slow clone is variable, because 2 In the Cilk-5 implementation, it is called \nclone compilation.  it involves inter-processor communication. It is well estab\u00adlished, both through \nanalysis and experimentation, that (with high probability) no more than O(P D) steals occur during the \nevaluation [11]. So, for programs that exhibit parallel slackness (W\u00bb P D), we do not need to take into \nac\u00adcount the cost of slow clones because there are relatively few of them. We focus only on the cost \nof creating fast clones, which correspond to the cost t. A fast clone needs to pack\u00adages a task, push \nit onto the deque and later pop it from the deque. So, a fast clone is not quite as fast as the correspond\u00ading \nsequential code. The exact slowdown depend on the im\u00adplementation, but in our case we have observed that \na fast clone is 3 to 5 times slower than a simple function call. 6. Empirical Evaluation In this section, \nwe evaluate the effectiveness of our imple\u00admentation through several experiments. We consider results \nfrom a range of benchmarks run on two machines with dif\u00adferent architectures. The results show that, \nin each case, our oracle implementation improves on the plain work-stealing implementation. Furthermore, \nthe results show that the ora\u00adcle implementation scales well with up to sixteen processors. Machines. \nOur AMD machine has four quad-core AMD Opteron 8380 processors running at 2.5GHz. Each core has 64Kb \neach of L1 instruction and data cache, and a 512Kb L2 cache. Each processor has a 6Mb L3 cache that is \nshared with the four cores of the processor. The system has 32Gb of RAM and runs Debian Linux (kernel \nversion 2.6.31.6\u00adamd64). Our Intel machine has four eight-core Intel Xeon X7550 processors running at \n2.0GHz. Each core has 32Kb each of L1 instruction and data cache and 256 Kb of L2 cache. Each processor \nhas an 18Mb L3 cache that is shared by all eight cores. The system has 1Tb of RAM and runs Debian Linux \n(kernel version 2.6.32.22.1.amd64-smp). For uniformity, we consider results from just sixteen out of \nthe thirty-two cores of the Intel machine. Measuring scheduling costs. We report estimates of the task-creation \noverheads for each of our test machines. To estimate, we use a synthetic benchmark expression e whose \nevaluation sums integers between zero and 30 million using a parallel divide-and-conquer computation. \nWe chose this particular expression because most of its evaluation time is spent evaluating parallel \npairs. First, we measure ws: the time required for executing a sequentialized version of the program \n(a copy of the pro\u00adgram where parallel tuples are systematically replaced with sequential tuples). This \nmeasure serves as the baseline. Sec\u00adond, we measure ww: the time required for executing the program using \nwork stealing, on a single processor. This measure is used to evaluated t. Third, we measure wo: the \ntime required for executing a version of the program with parallel tuples replaced with ordinary tuples \nbut where we still call the oracle. This measure is used to evaluate f. ww We then de.ne the work-stealing \noverhead cw = . ws We estimate the cost t of creating a parallel task in work ww-ws stealing by computing \n, where n is the number of n parallel pairs evaluated in the program. We also estimate the cost f of \ninvoking the oracle by computing wo-ws , where m m is the number of times the oracle is invoked. Our \nmeasures are as follows. Machine cw t (\u00b5s) f (\u00b5s) AMD 4.86 0.09 0.18 Intel 3.90 0.18 0.94 The .rst column \nindicates that work stealing alone can induce a slowdown by a factor of 4 or 5, for programs that create \na huge number of parallel tuples. Column two indi\u00adcates that the cost of creating parallel task t is \nsigni.cant, taking roughly between 200 and 350 processor cycles. The last column suggests that the oracle \ncost f is of the same order of magnitude (f is 2 to 5 times larger than t ). To determine a value for \n., we use the formula \u00b5(t+.f) r from \u00a73.2. Recall that r is the targette overhead for schedul\u00ading costs. \nWe aim for r = 10%. Our oracle appears to be always accurate within a factor 2, so we set \u00b5 =2. Our benchmark \nprograms are fairly regular, so we take . =3. We then use the values for t and f speci.c to the machine \nand evaluate the formula \u00b5(t+.f) . We obtain 13\u00b5s for the r AMD machine and 60\u00b5s for the Intel machine. \nHowever, we were not able to use a cutoff as small as 13\u00b5s because the time function that we are using \nis only accurate up to 1\u00b5s. For this reason, we doubled the value to 26\u00b5s. (One possibil\u00adity to achieve \ngreater accuracy would be to use architecture\u00adspeci.c registers that are able to report on the number \nof processor cycles involved in the execution of a task.) In our experiments, we used . = 26\u00b5s on the \nAMD machine and . = 61\u00b5s on the Intel machine. Benchmarks. We used .ve benchmarks in our empirical evaluation. \nEach benchmark program was originally written by other researchers and ported to our dialect of Caml. \nThe Quicksort benchmark sorts a sequence of 2 million integers. Our program is adapted from a functional, \ntree\u00adbased algorithm [6]. The algorithm runs with O(n log n) raw work and O(log2 n) raw depth, where \nn is the length of the sequence. Sequences of integers are represented as binary trees in which sequence \nelements are stored at leaf nodes and each internal node caches the number of leaves contained in its \nsubtree. The Quickhull benchmark calculates the convex hull of a sequence of 3 million points contained \nin 2-d space. The al\u00adgorithm runs with O(n log n) raw work and O(log2 n) raw depth, where n is the length \nof the sequence. The represen\u00adtation of points is similar to that of Quicksort, except that leaves store \n2-d points instead of integers. The Barnes-Hut benchmark is an n-body simulation that calculates the \ngravitational forces between n particles as they move through 2-d space [4]. The Barnes-Hut compu\u00ad  \nAMD Intel Figure 9. Comparison of the speedup on sixteen processors. Higher bars are better. tation \nconsists of two phases. In the .rst, the simulation vol\u00adume is divided into square cells via a quadtree, \nso that only particles from nearby cells need to be handled individually and particles from distant cells \ncan be grouped together and treated as large particles. The second phase calculates grav\u00aditational forces \nusing the quadtree to accelerate the compu\u00adtation. The algorithm runs with O(n log n) raw work and O(log \nn) raw depth. Our benchmark runs 10 iterations over 100,000 particles generated from a random Plummer \ndistri\u00adbution [33]. The program is adapted from a Data-Parallel Haskell program [31]. The representation \nwe use for se\u00adquences of particles is similar to that of Quicksort. The SMVM benchmark multiplies an \nm \u00d7 n matrix with an n \u00d7 1 dense vector. Our sparse matrix is stored in the compressed sparse-row format. \nThe program contains paral\u00adlelism both between dot products and within individual dot products. We use \na sparse matrix of dimension m = 500,000 and n = 448,000, containing 50,400,000 nonzero values. The DMM \nbenchmark multiplies two dense, square n\u00d7n matrices using the recursive divide-and-conquer algorithm \nof Frens and Wise [18]. We have recursion go down to scalar elements. The algorithm runs with O(n3) raw \nwork and O(log n) raw depth. We selected n = 512. Implementing complexity functions. Our aim is to make \ncomplexity functions fast, ideally constant time, so that we can keep oracle costs low. But observe that, \nin order to com\u00adplete in constant time, the complexity function needs access to the input size in constant \ntime. For four of our benchmark programs, no modi.cations to the algorithm is necessary, be\u00adcause the \nrelevant data structures are already decorated with suf.cient size information. The only one for which \nwe make special provisions is SMVM. The issue concerns a subprob\u00adlem of SMVM called segmented sums [9]. \nIn segmented sums, our input is an array of arrays of scalars, e.g., [[8, 3, 9], [2], [3, 1][5]] whose \nunderlying representation is in segmented format. The segmented format consists of a pair of arrays, \nwhere the .rst array contains all the elements of the subarrays and second contains the lengths of the \nsubarrays.  ([8, 3, 9, 2, 3, 1, 5], [3, 1, 2, 1]) The second array is called the segment descriptor. \nThe ob\u00adjective is to compute the sum of each subarray, [20, 2, 4, 5], There are two sources of parallelism \nin segmented sums: (1) within the summation of each subarray and (2) between different subarray sums. \nWe use divide-and-conquer algo\u00adrithms to solve each case. In the .rst case, our algorithm is just an \narray summation, and thus the complexity function is straightforward to compute in constant time from \nthe seg\u00adment descriptor. The second case is where we make the spe\u00adcial provisions. For this case, we \nuse a parallel array-map al\u00adgorithm to compute all the subarray sums in parallel. The is\u00adsue is that \nthe complexity of performing a group of subarray sums is proportional to the sum of the sizes of those \nsubar\u00adrays. So, to obtain this size information in constant time, we modify our segmented-array representation \nslightly so that we store a cached tree of subarray sizes rather than just a .at array of subarray sizes. \n([8, 3, 9, 2, 3, 1, 4, 5], 7 ) 43 3121 To summarize, in order to write a constant-time complexity function, \nwe changed the existing SMVM program to use a tree data structure, where originally there was an array \ndata structure. Building the tree can be done in parallel, and the cost of building can be amortized \naway by reusing the sparse matrix multiple times, as is typically done in iterative solvers. Performance. \nFor every benchmark, we measure several values. Tseq denotes the time to execute the sequential ver\u00adsion \nof the program. We obtain the sequential version of the program by replacing each parallel tuple with \nan ordinary tu\u00adple and erasing complexity functions, so that the sequential version includes none of \nthe task-creation overheads. T P par denotes the execution time with work stealing on P pro\u00adcessors. \nT P denotes the execution time of our oracle-based orc work stealing on P processors. The most important \nresults of our experiments come from comparing plain work stealing and our oracle-based work stealing \nside by side. Figure 9 shows the speedup on sixteen processors for each of our benchmarks, that is, the \nvalues T 16 and T 16 . The speedups show that, on sixteen cores, our oracle implementation is always \nbetween 4% and 76% faster than work stealing. The fact that some benchmarks bene.t more from our or\u00adacle \nimplementation than others is explained by Figure 10. This plot shows execution time for one processor, \nnormal\u00adized with respect to the sequential execution times. In other par/Tseq orc/Tseq  words, the \nvalues plotted are 1, Torc1 /Tseq and T 1 /Tseq par. The values T 1 range from 1.03 to 1.13 (with an \naverage orc/Tseq of 1.07), indicating that the task-creation overheads in the oracle implementation do \nnot exceed 13% of the raw work in any benchmark. The cases where we observe large im\u00adprovements in speedup \nare the same cases where there is a large difference bewteen sequential execution time and plain work-stealing \nexecution time. When the difference is large, there is much room for our implementation to improve on \nwork stealing, whereas when the difference is small we can only improve the execution time by a limited \nfactor. Figure 11 shows speedup curves for each of our experi\u00adments, that is, values of T P /Tseq and \nT P /Tseq against the parorc number of processors P on our Intel machine; the measure\u00adments on the AMD \nmachine show similar trends but quanti\u00adtatively better results for the oracle versions. The curves show \nthat our oracle implementation gener\u00adally scales well up to sixteen processors. There is one exception, \nwhich is the quickhull benchmark on the AMD machine. For this benchmark, the curve tails off after reaching \ntwelve processors. We need to conduct further experiments to understand the cause, which is probably \ndue to a lack of parallelism in the program. Notice, however, that our scheduler does not fall below \nwork stealing. 7. Related Work Cutting off excess parallelism. This study is not the .rst to propose \nusing cost prediction to determine when to cut off parallelism. One approach, developed in early work \nin func\u00adtional programing, uses list size to determine cut offs [24]. Using list size alone is limited, \nbecause the technique as\u00adsumes linear work complexity for every parallel operation. Another way to handle \ncost prediction is to use the depth and height of the recursion tree [30, 42]. But depth and height are \nnot, in general, the most direct means to pre\u00addict the execution time of subcomputations. In our oracle \nscheduling, we ask for either the programmer or compiler to provide for each function a cost function \nthat expresses the asymptotic cost of applying the function. Lopez et. al. take this approach as well, \nbut in the con\u00adtext of logic programming [27]. On the surface, their tech\u00adnique is similar to our oracle \nscheduling, except that their cost estimators do not utilize pro.ling to estimate constant factors. An \napproach without constant-factor estimation is overly simplistic for modern processors, because it relies \non complexity function predicting execution time exactly. On modern processors, execution time depends \nheavily on fac\u00adtors such as caching, pipelining, etc. and it is not feasible in general to predict execution \ntime from a complexity function alone. Reducing per-task costs. One approach to the granularity problem \nis to focus on reducing the costs associated with tasks, rather than limiting how many tasks get created. \nThis approach is taken by implementations of work stealing with lazy task creation [15, 19, 23, 28, 34, \n36]. In lazy task creation, the work stealing scheduler is implemented so as to avoid, in the common \ncase, the major scheduling costs, in particular, those of inter-processor communication. But, in even \nthe most ef.cient lazy task creation, there is still a non-negligable scheduling cost for each implicit \nthread. Lazy Binary Splitting (LBS) is an improvement to lazy task creation that applies to parallel \nloops [40]. The crucial optimization comes from extending the representation of a task so that multiple \nloop iterations can be packed into a single task. This representation enables the scheduler to both avoid \ncreating closures and executing deque operations for most iterations. A limitation of LBS is that it \naddresses only parallel loops whose iteration space is over integers. Lazy Tree Splitting (LTS) generalizes \nLBS to handle parallel aggregate operations that produce and consume trees, such as map and reduce [5]. \nLTS is limited, however, by the fact that it requires a special cursor data structure to be de.ned for \neach tree data structure. Amortizing per-task costs. Feitelson et al. study the gran\u00adularity problem \nin the setting of distributed computing [2], where the crucial issue is how to minimize the cost of inter\u00adprocessor \ncommunication. In their setting, the granularity problem is modeled as a staging problem, in which there \nare two stages. The .rst stage consists of a set of processor\u00adlocal task pools and the second stage consists \nof a global task pool. Moving a task to the global task pool requires inter\u00adprocessor communication. \nThe crucial decision is how of\u00adten each processor should promote tasks from its local task pool to the \nglobal task pool. We consider a different model of staging in which there is one stage for parallel evaluation \nand one for sequential evaluation. The approach proposed by Feitelson et al. is based on an online algorithm \ncalled CG. In this approach, it is assumed that the cost of moving a task to the global task pool is \nan integer constant, called g. The basic idea is to use amorti\u00adzation to reduce the scheduling total \ncost of moving tasks to the global task pool. In particular, for each task that is moved to the global \ntask pool, CG ensures that there are at least g +1 tasks added to the local task pool. Narlikar describes \na similar approach based on an algorithm called DFDeques [29]. Just as with work stealing, even though \nthe scheduler can avoid the communication costs in the common case, the scheduler still has to pay a \nnon-negligable cost for each implicit thread. Flattening and fusion. Flattening is a well-known pro\u00adgram \ntransformation for nested parallel languages [10]. Im\u00adplementations of .attening include NESL [8] and \nData Par\u00adallel Haskell [32]. Flattening transforms the program into a form that maps well onto SIMD architectures. \nFlattened pro\u00adgrams are typically much simpler to schedule at run time than nested programs, because \nmuch of the schedule is pre\u00addetermined by the .attening [38]. Controlling the granular\u00adity of such programs \nis correspondingly much simpler than in general. A limitation of existing .attening is that certain classes \nof programs generated by the translation suffer from space inef.ciency [7], as a consequence of the transforma\u00adtion \nmaking changes to data structures de.ned in the pro\u00adgram. Our transformation involves no such changes. \n The NESL [8] and Data Parallel Haskell [32] compilers implement fusion transformation in order to increase \ngranu\u00adlarity. Fusion transforms the program to eliminate redundant synchronization points and intermediate \narrays. Although fusion reduces scheduling costs by combining adjacent par\u00adallel loops, it is not relevant \nto controlling granularity within loops. As such, fusion is orthogonal to our oracle based ap\u00adproach. \nCost Semantics. To give an accurate accounting of task\u00adcreation of overheads in implicitly parallel languages \nwe use a cost semantics, where evaluation steps (derivation rules) are decorated with work and depth \ninformation or costs . This information can then be used to directly to bound run\u00adning time on parallel \ncomputers by using standard schedul\u00ading theorems that realize Brent s bound. Many previous ap\u00adproaches \nalso use the same technique to study work-depth properties, some of which also make precise the relationship \nbetween cost semantics and the standard directed-acyclic\u00adgraph models [6, 7, 39]. The idea of instrumenting \nevalu\u00adations to generate cost information goes back to the early 90s [35, 37]. Inferring Complexity Bounds. \nOur implementation of or\u00adacle scheduling requires the programmer to enter complex\u00adity bounds for all \nparallel tasks. In some cases, these bounds can be inferred by various static analyses, for example, \nusing type-based and other static analyses (e.g., [14, 25]), sym\u00adbolic techniques (e.g., [20, 21]). Our \napproach can bene.t from these approaches by reducing the programmer burden, making it ultimately easier \nto use the proposed techniques in practice. 8. Conclusion In this paper, we propose a solution to the \ngranularity-control problem. We prove that an oracle that can approximate the sizes of parallel tasks \nin constant time within a constant factor of accuracy can be used to reduce the task creation overheads \nto any desired constant fraction for a reasonably broad class of computations. We describe how such an \noracle can be integrated with any scheduler to support what we call oracle scheduling. We realize oracle \nscheduling in practice by requiring the programmer to enter asymptotic complexity annotations for parallel \ntasks and by judicious use of run\u00adtime pro.ling. Consistently with our theoretical analysis, our experiments \nshow that oracle scheduling can reduce task creation overheads to a small fraction of the sequential \ntime without hurting parallel scalability. References [1] Umut A. Acar, Guy E. Blelloch, and Robert D. \nBlumofe. The data locality of work stealing. Theory of Computing Systems (TOCS), 35(3):321 347, 2002. \n[2] Gad Aharoni, Dror G. Feitelson, and Amnon Barak. A run\u00adtime algorithm for managing the granularity \nof parallel func\u00adtional programs. Journal of Functional Programming, 2:387 405, 1992. [3] Nimar S. Arora, \nRobert D. Blumofe, and C. Greg Plaxton. Thread scheduling for multiprogrammed multiprocessors. In Proceedings \nof the tenth annual ACM symposium on Paral\u00adlel algorithms and architectures, SPAA 98, pages 119 129. \nACM Press, 1998. [4] Josh Barnes and Piet Hut. A hierarchical O(N log N) force calculation algorithm. \nNature, 324:446 449, December 1986. [5] Lars Bergstrom, Matthew Fluet, Mike Rainey, John Reppy, and Adam \nShaw. Lazy tree splitting. In ICFP 2010, pages 93 104. ACM Press, September 2010. [6] Guy Blelloch and \nJohn Greiner. Parallelism in sequential functional languages. In FPCA 95: Proceedings of the 7th International \nConference on Functional Programming Lan\u00adguages and Computer Architecture, pages 226 237, 1995. [7] Guy \nE. Blelloch and John Greiner. A provable time and space ef.cient implementation of NESL. In Proceedings \nof the 1st ACM SIGPLAN International Conference on Functional Programming, pages 213 225. ACM, 1996. \n[8] Guy E. Blelloch, Jonathan C. Hardwick, Jay Sipelstein, Marco Zagha, and Siddhartha Chatterjee. Implementation \nof a portable nested data-parallel language. J. Parallel Distrib. Comput., 21(1):4 14, 1994. [9] Guy \nE. Blelloch, Michael A. Heroux, and Marco Zagha. Seg\u00admented operations for sparse matrix computation \non vector multiprocessors. Technical Report CMU-CS-93-173, School of Computer Science, Carnegie Mellon \nUniversity, August 1993. [10] Guy E. Blelloch and Gary W. Sabot. Compiling collection\u00adoriented languages \nonto massively parallel computers. Jour\u00adnal of Parallel and Distributed Computing, 8:119 134, Febru\u00adary \n1990. [11] R.D. Blumofe and C.E. Leiserson. Scheduling multithreaded computations by work stealing. Annual \nIEEE Symposium on Foundations of Computer Science, 0:356 368, 1994. [12] Robert D. Blumofe, Christopher \nF. Joerg, Bradley C. Kusz\u00admaul, Charles E. Leiserson, Keith H. Randall, and Yuli Zhou. Cilk: an ef.cient \nmultithreaded runtime system. In PPOPP 95: Proceedings of the .fth ACM SIGPLAN Symposium on Principles \nand practice of parallel programming, pages 207 216. ACM, 1995. [13] Richard P. Brent. The parallel evaluation \nof general arithmetic expressions. Journal of the ACM, 21(2):201 206, 1974. [14] Karl Crary and Stephnie \nWeirich. Resource bound certi.ca\u00adtion. In Proceedings of the 27th ACM SIGPLAN-SIGACT sym\u00adposium on Principles \nof programming languages, POPL 00, pages 184 198. ACM, 2000.  [15] Marc Feeley. A message passing implementation \nof lazy task creation. In Proceedings of the US/Japan Workshop on Parallel Symbolic Computing: Languages, \nSystems, and Applications, pages 94 107, London, UK, 1993. Springer-Verlag. [16] Matthew Fluet, Mike \nRainey, and John Reppy. A scheduling framework for general-purpose parallel languages. In Pro\u00adceeding \nof the 13th ACM SIGPLAN international conference on Functional programming, ICFP 08, pages 241 252. ACM, \n2008. [17] Matthew Fluet, Mike Rainey, John Reppy, and Adam Shaw. Implicitly threaded parallelism in \nmanticore. Journal of Func\u00adtional Programming, 20(5-6):1 40, 2011. [18] Jeremy D. Frens and David S. \nWise. Auto-blocking matrix\u00admultiplication or tracking blas3 performance from source code. In Proceedings \nof the sixth ACM SIGPLAN Symposium on Principles and practice of parallel programming, PPOPP 97, pages \n206 216, New York, NY, USA, 1997. ACM. [19] Matteo Frigo, Charles E. Leiserson, and Keith H. Randall. \nThe implementation of the Cilk-5 multithreaded language. In PLDI, pages 212 223, 1998. [20] Simon F. \nGoldsmith, Alex S. Aiken, and Daniel S. Wilkerson. Measuring empirical computational complexity. In Proceed\u00adings \nof the 6th Joint Meeting of the European Software Engi\u00adneering Conference and the ACM Symposium on the \nFounda\u00adtions of Software Engineering, pages 395 404, 2007. [21] Sumit Gulwani, Krishna K. Mehra, and \nTrishul Chilimbi. Speed: precise and ef.cient static estimation of program com\u00adputational complexity. \nIn Proceedings of the 36th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Pro\u00adgramming Languages, \npages 127 139, 2009. [22] Robert H. Halstead. Multilisp: a language for concurrent symbolic computation. \nACM Transactions on Programming Languages and Systems, 7:501 538, 1985. [23] Tasuku Hiraishi, Masahiro \nYasugi, Seiji Umatani, and Taiichi Yuasa. Backtracking-based load balancing. Proceedings of the 2009 \nACM SIGPLAN Symposium on Principles &#38; Prac\u00adtice of Parallel Programming, 44(4):55 64, February 2009. \n[24] Lorenz Huelsbergen, James R. Larus, and Alexander Aiken. Using the run-time sizes of data structures \nto guide parallel\u00adthread creation. In Proceedings of the 1994 ACM conference on LISP and functional programming, \nLFP 94, pages 79 90, 1994. [25] Steffen Jost, Kevin Hammond, Hans-Wolfgang Loidl, and Martin Hofmann. \nStatic determination of quantitative re\u00adsource usage for higher-order programs. In Proceedings of the \n37th annual ACM SIGPLAN-SIGACT symposium on Prin\u00adciples of programming languages, POPL 10, pages 223 \n236, 2010. [26] Xavier Leroy, Damien Doligez, Jacques Garrigue, Didier R\u00b4emy, and J\u00b4er ome Vouillon. \nThe Objective Caml system, 2005. [27] P. Lopez, M. Hermenegildo, and S. Debray. A methodology for granularity-based \ncontrol of parallelism in logic programs. Journal of Symbolic Computation, 21:715 734, June 1996. [28] \nEric Mohr, David A. Kranz, and Robert H. Halstead Jr. Lazy task creation: a technique for increasing \nthe granularity of parallel programs. In Conference record of the 1990 ACM Conference on Lisp and Functional \nProgramming, pages 185 197, New York, New York, USA, June 1990. ACM Press. [29] Girija Jayant Narlikar. \nSpace-ef.cient scheduling for parallel, multithreaded computations. PhD thesis, Carnegie Mellon University, \nPittsburgh, PA, USA, 1999. [30] Joseph Pehoushek and Joseph Weening. Low-cost process creation and dynamic \npartitioning in qlisp. In Takayasu Ito and Robert Halstead, editors, Parallel Lisp: Languages and Systems, \nvolume 441 of Lecture Notes in Computer Science, pages 182 199. Springer Berlin / Heidelberg, 1990. [31] \nSimon Peyton Jones, Roman Leshchinskiy, Gabriele Keller, and Manuel M. T. Chakravarty. Harnessing the \nmulticores: Nested data parallelism in haskell. In Proceedings of the 6th Asian Symposium on Programming \nLanguages and Systems, APLAS 08, pages 138 138, 2008. [32] Simon L. Peyton Jones. Harnessing the multicores: \nNested data parallelism in haskell. In APLAS, page 138, 2008. [33] H. C. Plummer. On the problem of distribution \nin globular star clusters. Monthly Notices of the Royal Astronomical Society, 71:460 470, March 1911. \n[34] Mike Rainey. Effective Scheduling Techniques for High-Level Parallel Programming Languages. PhD \nthesis, University of Chicago, August 2010. [35] Mads Rosendahl. Automatic complexity analysis. In FPCA \n89: Functional Programming Languages and Computer Ar\u00adchitecture, pages 144 156. ACM, 1989. [36] Daniel \nSanchez, Richard M. Yoo, and Christos Kozyrakis. Flexible architectural support for .ne-grain scheduling. \nIn Proceedings of the .fteenth edition of ASPLOS on Architec\u00adtural support for programming languages \nand operating sys\u00adtems, ASPLOS 10, pages 311 322, New York, NY, USA, 2010. ACM. [37] David Sands. Calculi \nfor Time Analysis of Functional Pro\u00adgrams. PhD thesis, University of London, Imperial College, September \n1990. [38] Daniel Spoonhower. Scheduling Deterministic Parallel Pro\u00adgrams. Ph. D. dissertation, Carnegie \nMellon University, Pitts\u00adburg, PA, USA, 2009. [39] Daniel Spoonhower, Guy E. Blelloch, Robert Harper, \nand Phillip B. Gibbons. Space pro.ling for parallel functional programs. In International Conference \non Functional Pro\u00adgramming, 2008. [40] Alexandros Tzannes, George C. Caragea, Rajeev Barua, and Uzi Vishkin. \nLazy binary-splitting: a run-time adaptive work\u00adstealing scheduler. In Proceedings of the 2010 ACM SIGPLAN \nSymposium on Principles &#38; Practice of Parallel Program\u00adming. ACM Press, February 2010. [41] Leslie \nG. Valiant. A bridging model for parallel computation. Commun. ACM, 33:103 111, August 1990. [42] Joseph \nS. Weening. Parallel Execution of Lisp Programs. PhD thesis, Stanford University, 1989. Computer Science \nTechnical Report STAN-CS-89-1265.   \n\t\t\t", "proc_id": "2048066", "abstract": "<p>A classic problem in parallel computing is determining whether to execute a task in parallel or sequentially. If small tasks are executed in parallel, the task-creation overheads can be overwhelming. If large tasks are executed sequentially, processors may spin idle. This granularity problem, however well known, is not well understood: broadly applicable solutions remain elusive.</p> <p>We propose techniques for controlling granularity in implicitly parallel programming languages. Using a cost semantics for a general-purpose language in the style of the lambda calculus with support for parallelism, we show that task-creation overheads can indeed slow down parallel execution by a multiplicative factor. We then propose oracle scheduling, a technique for reducing these overheads, which bases granularity decisions on estimates of task-execution times. We prove that, for a class of computations, oracle scheduling can reduce task creation overheads to a small fraction of the work without adversely affecting available parallelism, thereby leading to efficient parallel executions.</p> <p>We realize oracle scheduling in practice by a combination of static and dynamic techniques. We require the programmer to provide the asymptotic complexity of every function and use run-time profiling to determine the implicit, architecture-specific constant factors. In our experiments, we were able to reduce overheads of parallelism down to between 3 and 13 percent, while achieving 6- to 10-fold speedups.</p>", "authors": [{"name": "Umut A. Acar", "author_profile_id": "81100077236", "affiliation": "Max Planck Institute for Software Systems, Kaiserslautern, Germany", "person_id": "P2839220", "email_address": "umut@mpi-sws.org", "orcid_id": ""}, {"name": "Arthur Chargu&#233;raud", "author_profile_id": "81372592434", "affiliation": "Max Planck Institute for Software Systems, Kaiserslautern, Germany", "person_id": "P2839221", "email_address": "charguer@mpi-sws.org", "orcid_id": ""}, {"name": "Mike Rainey", "author_profile_id": "81330497308", "affiliation": "Max Planck Institute for Software Systems, Kaiserslautern, Germany", "person_id": "P2839222", "email_address": "mrainey@mpi-sws.org", "orcid_id": ""}], "doi_number": "10.1145/2048066.2048106", "year": "2011", "article_id": "2048106", "conference": "OOPSLA", "title": "Oracle scheduling: controlling granularity in implicitly parallel languages", "url": "http://dl.acm.org/citation.cfm?id=2048106"}