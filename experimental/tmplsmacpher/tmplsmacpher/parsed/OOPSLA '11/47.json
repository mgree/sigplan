{"article_publication_date": "10-22-2011", "fulltext": "\n Variability-Aware Parsing in the Presence of Lexical Macros and Conditional Compilation Christian K\u00e4stner \nPaolo G. Giarrusso Tillmann Rendel Sebastian Erdweg Klaus Ostermann Philipps University Marburg, Germany \nAbstract In many projects, lexical preprocessors are used to manage different variants of the project \n(using conditional compila\u00adtion) and to de.ne compile-time code transformations (using macros). Unfortunately, \nwhile being a simple way to imple\u00adment variability, conditional compilation and lexical macros hinder \nautomatic analysis, even though such analysis is ur\u00adgently needed to combat variability-induced complexity. \nTo analyze code with its variability, we need to parse it without preprocessing it. However, current \nparsing solutions use un\u00adsound heuristics, support only a subset of the language, or suffer from exponential \nexplosion. As part of the TypeChef project, we contribute a novel variability-aware parser that can parse \nalmost all unpreprocessed code without heuristics in practicable time. Beyond the obvious task of detecting \nsyntax errors, our parser paves the road for further analy\u00adsis, such as variability-aware type checking. \nWe implement variability-aware parsers for Java and GNU C and demon\u00adstrate practicability by parsing \nthe product line MobileMedia and the entire X86 architecture of the Linux kernel with 6065 variable features. \nCategories and Subject Descriptors D.3.4 [Programming Languages]: Processors; D.2.3 [Software Engineering]: \nCoding Tools and Techniques General Terms Algorithms, Languages, Performance Keywords parsing, C, preprocessor, \n#ifdef, variability, con\u00additional compilation, Linux, software product lines 1. Introduction Compile-time \nvariability is paramount for many software sys\u00adtems. Such systems must accommodate optional or even alter- \nPermission to make digital or hard copies of all or part of this work for personal or classroom use is \ngranted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. OOPSLA \n11, October 22 27, 2011, Portland, Oregon, USA. Copyright &#38;#169; 2011 ACM 978-1-4503-0940-0/11/10. \n. . $10.00 Thorsten Berger University of Leipzig, Germany native requirements for different customers. \nIn software prod\u00aduct lines, variability is even regarded as a core strategic advan\u00adtage and planned accordingly \n[51]. Unfortunately, variability increases complexity because now many variants of a system must be developed \nand maintained. Hence, many researchers pursue a strategy to lift automated analysis and processing such \nas dead-code detection, type checking, model checking, refactoring, reengineering, and many more from \nindividual variants to entire software product lines in a variability-aware fashion [4, 12, 14, 23, 32, \n52, 61]. In practice, a simple and broadly used mechanism to im\u00adplement compile-time variability is conditional \ncompilation typically performed with lexical preprocessors such as the C preprocessor [29], Pascal s \npreprocessor [58], Antenna for Java ME [19], pure::variants [10, 53], and Gears [11]. With\u00adout loss of \ngenerality, we focus on the C preprocessor. A code fragment framed with #ifdef X and #endif directives \nis only processed during compilation if the .ag X is selected, for in\u00adstance because it is passed as \ncon.guration parameter to the compiler (as command-line option or using a con.guration .le). Using product-line \nterminology, we refer to such .ags as features and to products created for a given feature selec\u00adtion \nas variants. Conditional compilation is widely used in open-source C projects [39], and it is a common \nmechanism to implement software product lines [51]; examples include Linux with thousands of features \n[39, 54], HP s product line of printer .rmware with over 2000 features [49], or NASA s .ight control \nsoftware with 275 features [21]. One of the main problems of lexical preprocessors is that we cannot \npractically parse code without preprocessing it .rst. It is a common perception that parsing unpreprocessed \ncode is dif.cult or even impossible [46, 58]. The inability to parse unpreprocessed code poses a huge \nobstacle for applying variability-aware analysis and processing to code bases using lexical preprocessors, \nsuch as the vast amount of existing C code. Parsing is dif.cult, because lexical preprocessors are oblivious \nto the underlying host language and its structure. Hence, conditional compilation can be applied to arbitrary \ntoken sequences, i.e., it is not restricted to syntactic structures. In addition to conditional compilation, \nlexical macros (again oblivious to the underlying structure), .le inclusion, and their interaction with \nconditional compilation complicate the picture.  Parsing, analyzing, and processing unpreprocessed code \nis interesting for many tasks involving variability, such as variability-aware error detection [4, 14, \n32, 52, 57, 58, 61], program understanding [27, 35], reengineering [15, 54], refac\u00adtorings [1, 22, 23, \n64], and other code transformations [6, 46]. Variability-aware analysis and processing are especially \nim\u00adportant when the number of features grows, because there are up to 2n variants for n features. Without \nsuitable tools, devel\u00adopers typically analyze and process only few selected variants that are currently \ndeployed. This way, even simple syntax or type errors may go undetected until a speci.c feature combination \nis selected, potentially late in the development process, when mistakes are expensive to .x. Current \nattempts to parse unpreprocessed C code are unsound, incomplete, or suffer from exponential explosion \neven in simple cases. For instance, parsing all variants in isolation in a brute-force fashion does not \nscale for projects with more than a few features, due to the exponential number of variants. Alternatives \nuse either unsound heuristics (such as assuming that macro names can be identi.ed by capitalized letters) \nor restrict the way preprocessor directives can be used. Although such limitations are acceptable for \nsome tasks or small projects, our goal is a sound and complete parsing mechanism that can be used on \nexisting code to ensure consistency of an entire product line. We contribute a novel variability-aware \nparser framework that can accurately parse unpreprocessed code. With this framework, we have implemented \na sound and almost com\u00adplete parser for unpreprocessed GNU-C code and a sound and complete parser for \nJava with conditional compilation. In contrast to existing approaches, parsers written with our framework \nusually do not require manual code preparation and do not impose restrictions on possible preprocessor \nus\u00adage. Our variability-aware parsers can handle conditional compilation even when it does not align \nwith the underlying syntactic structure. In addition, we provide a strategy to deal with macros and .le \ninclusion, using a variability-aware lexer. As such, our parsers can be used on existing legacy code. \nWithout committing to a single variant, we detect syntax er\u00adrors in all variants and create a parse result \nthat contains all variability, so further analysis tools (such as type checkers or refactoring engines) \ncan work on a common representation of the entire product line. Although the worst case time and memory \ncomplexity of our parser and the produced abstract syntax tree are exponen\u00adtial, in practice, most source \ncode is well-behaved and can be parsed ef.ciently. We use SAT solvers during lexing and parsing to ef.ciently \nreason about features and their relation\u00adships. In addition, we avoid accidental complexity by making \ndecisions as local as possible. We demonstrate practicality with two case studies a small Java product \nline MobileMe\u00addia and the entire X86 architecture of the Linux kernel with 9.5 million lines of code \nand 6065 features. The parser is part of our long-term project TypeChef (short for type checking ifdef \nvariability) but can also be used in isolation. We open sourced the entire implementation, provide a \nweb version for easy experimentation, and publish additional data from our case studies at http://fosd.net/ \nTypeChef. In summary, we make the following novel contributions: We present a reusable variability-aware \nparser framework to parse code with conditional compilation, using SAT solvers for decisions during the \nparsing process. This framework is language-agnostic and can be reused for many languages using lexical \npreprocessors.  We developed variability-aware parsers for Java and GNU C; to the best of our knowledge, \nthese are the .rst parsers that can parse unpreprocessed code without excessive manual code preparation \nand without unsound heuristics.  We demonstrate practicality of our parsers by parsing the product line \nMobileMedia and the entire X86 architecture of the Linux kernel.  Our contributions cover both (a) developing \nthe novel concept of variability-aware parsing and (b) signi.cant engineering efforts combining the parsers \nwith other prior research re\u00adsults (i.e., reasoning about variability and variability models, variability-aware \ntype systems, variability-aware lexing) to build a tool infrastructure that scales to parsing Linux. \n2. Variability-aware parsing: What and why? Our goal is to build a parser that produces a single abstract \nsyntax tree for code that contains variable code fragments (optional or alternative). In the resulting \nabstract syntax tree, the code s variability is re.ected in optional or alternative subtrees. We illustrate \nthe problem and the desired result on two simple expressions in Figure 1, in which the C preprocessor \nis used on numeric expressions. The preprocessor prepares the code by removing code fragments between \n#if and #endif directives if the corresponding feature is not selected, and by replacing macros with \ntheir expansion. The .rst example expands to two different results, depending on whether feature X is \nde.ned. The second example is more complex and expands to six different results, depending on whether \nfeatures X, Y, and Z are selected. Note how macro B has two alternative expansions, depending on the \nfeature selection. Next to the listings, we show the desired parsing result in form of an abstract syntax \ntree with variability. The result re.ects variability with choice nodes over a feature F (denoted as \n.F ), the left child contains the parse result if feature F is selected, the right branch if it is not \nselected.1 By replacing choice nodes with their left or right branch, 1 Abstract syntax trees with variability \nare known from tools, such as fmp2rsm [13], FeatureMapper [26], and CIDE [30], and from formalisms, \n 1 2 3 4 5 6 3 * 7 + #ifdef X 1 #else 0 #endif + .X 01 * 73 1 3 4 5 6 7 9 10 11 12 13 14 15 16 17 18 \n19 20 21 22 23 #define A 3 #ifdef X #define B 1 #else #define B 2 #endif 3 #ifdef X * 7 #endif + #ifdef \nY #ifdef Z A #else 4 #endif * B #else 0 #endif .X 3* 73 + .Y * .X 21 .Z 43 0 Figure 1. Expressions with \nconditional fragments and corre\u00adsponding parse results with choice nodes. we could perform conditional \ncompilation on the abstract\u00adsyntax-tree representation to yield the individual parse results. In many \ncases however, we want to directly work on a single abstract syntax tree with variability. In our simple \nexpression example, a sample analysis would be an algorithm to approximate upper and lower bounds for \nthe result of numeric expression, which is trivial to implement on the abstract syntax tree with variability, \nbut not on the original source code with preprocessor directives. 2.1 Technical challenges Parsing unpreprocessed \ncode poses two main technical chal\u00adlenges, which are already visible in our initial expression examples \nand which we illustrate again on the small C-code snippets in Figure 2. Macro expansion and .le inclusion. \nLexical macros and .le inclusion interfere with the parsing process. Before parsing we must actually \ninclude .les and expand macros. In Figure 1, a numeric-expression parser would not understand the token \nA; in the .rst C-code example in Figure 2, a such as the choice calculus [17]. Our contribution lies \nnot in how to encode variability in syntax trees, but in parsing such trees from code. There are different \nequivalent abstract syntax trees for the same expression. We can move up the choice nodes in the tree \nby replicating tokens and we can move down choice nodes by refactoring common children. Two trees are \nequivalent when they can produce the same variants. Erwig and Walkingshaw [17] have formalized these \noperations on trees in their choice calculus and have de.ned normal forms. Some analysis tools may prefer \nchoice nodes at certain levels of granularity only. Moving choice nodes in trees after parsing is straightforward. \n 1 #define P(msg) \\ 1 #ifdef BIGINT 1 if (!initialized) 2 printf(msg); 2 #define SIZE 64 2 #ifdef DYNAMIC \n3 #endif 3 if (enabled) { 4 #ifdef SMALLINT 4 #endif 5 main() { 5 #define SIZE 32 5 init(); //... 6 P(\"Hello\\n\") \n6 #endif 6 #ifdef DYNAMIC 7 P(\"World\\n\") 7 } 8 } 8 allocate(SIZE) 8 #endif Figure 2. Challenges in parsing \nunpreprocessed code (macro expansion, conditional macros, and undisciplined annota\u00adtions) parser would \nnot even recognize two statements because the separating semicolon is added by a macro. Similar to macro \nexpansion, we must resolve includes before parsing, which could contain further macro de.nitions. In \naddition, macros may be de.ned conditionally (and .les may be included conditionally). In the example \nin Figure 1, macro B has two alternative expansions, depending on the feature selection. In the second \nC-code example, SIZE may be expanded in two ways or not expanded at all, depending on the feature selection. \nTo parse unpreprocessed code, we need a mechanism to handle macro expansion and .le inclusion (cf. Sec. \n4). Undisciplined annotations. Even without macros and .le inclusion, a parser must be able to deal with \nconditional\u00adcompilation directives that do not align with the underlying syntactic structure of the code. \nThe lexical nature of many preprocessors allows developers to annotate individual tokens, like the closing \nbracket in the third C-code example. Still, the parser must be able to make sense of such annotations \nand produce a variable abstract syntax tree that is equivalent to the source code in all possible feature \ncombinations. We call conditional-compilation directives on subtrees of the underlying structure disciplined \nannotations and those that do not align undisciplined annotations [31, 40]. In our expression example, \nmost annotations are disciplined: They just provide alternatives for subexpressions. However, the annotation \non *7 (Line 11) is undisciplined since it changes the structure of the resulting abstract syntax tree; \nin the result, we replicated token 3, so the abstract syntax tree covers both possible structures. Our \ngoal is a parser that can handle undisciplined annotations.  2.2 Soundness, completeness, and performance \nMany tool developers have tried to parse unpreprocessed C code with its variability, using different \nstrategies. Soundness, completeness, and performance are three characteristics that we can use to describe \ndesired properties of our parser and to distinguish it from other strategies. We consider a variability-aware \nparser as sound and complete if it yields a parse result that correctly represents the variability of \nthe parsed code fragment. That is, it should not matter whether we (a) .rst generate the source code \nof one variant (with a standard preprocessor given a desired feature selection) and then parse that variant \nwith a standard parser or (b) .rst parse the unpreprocessed code with a variability\u00adaware parser and \nthen generate the abstract syntax tree of the variant (by pruning subtrees not relevant for the desired \nfeature selection). A variability-aware parser is incomplete if it rejects a code fragment even though \npreprocessing and parsing would succeed for all variants. A variability-aware parser is unsound if it \nproduces a parse result which does not correctly represent the result from preprocessing and parsing \nin all variants (or if it produces a parse result even if at least one variant is not syntax-correct). \n Finally, performance largely depends on the complexity of the problem an approach is trying to solve. \nUnfortunately, no sound and complete parsing approach can avoid the inher\u00adent complexity of the problem, \nwhich is already exponential in the worst case. The challenge is to distinguish inherent complexity (exponential \nin the worst case, but usually man\u00adageable in real-world examples) from accidental complexity induced \nonly by the parsing strategy or tool. We illustrate soundness, completeness, and performance based on \nthree common strategies to parse unpreprocessed C code (we discuss these approaches in more detail as \npart of related work in Section 10.1): Brute force. For many purposes, a simple but effective strategy \nis to preprocess a .le for all (or all relevant) feature combinations in a brute-force fashion and to \nsubsequently parse and analyze the preprocessed variants in isolation. The brute force approach is our \nbenchmark for soundness and completeness. However, it suffers from exponential explosion and quickly \nbecomes infeasible in practice when the number of features grows. Even if the inherent complexity of \nthe problem is low (for example, features affect distinct structures in the same .le), the brute force \napproach parses all feature combinations. Already to parse a .le with 20 features, we would need to preprocess \nand parse the .le up to a million times, independent from the actual inherent complexity of the variability \nin that .le.  Manual code preparation. Another common strategy is to support only a subset of possible \npreprocessor usage. If we give up completeness, for example, by requiring that conditional compilation \nand macros align with the un\u00adderlying structure, parsing unpreprocessed code becomes possible in a sound \nand ef.cient fashion [6, 42, 68]: The parse results are correct, but we cannot parse all programs. This \nstrategy can only be used on code written according to certain guidelines; for existing code this would \nrequire manual rewrites.  Heuristics and partial analysis. Finally, giving up soundness, several researchers \nhave successfully ap\u00adplied heuristics to ef.ciently parse unpreprocessed C code [22, 23, 46]. They exploit \nrepeating patterns and idioms, such as the common include-guard pattern or cap\u00aditalized letters for macro \nnames. There are both complete  and incomplete heuristic-based parsers. Despite some reported success, \nunsound heuristics can lead to incor\u00ad rect parse results and undetected errors, especially in the presence \nof unusual macro expansions and undisciplined annotations. Being incomplete or unsound is not a problem \nper se. Incomplete or unsound approaches (and even the brute\u00adforce approach) have been successfully applied \nfor various tasks [6, 22, 47, 48, 64]. However, they do not .t our goals. Since it appears unrealistic \nto convince developers of projects as large as the Linux kernel to rewrite their code, and consider\u00ading \nthe vast amount of existing legacy code, we aim for a com\u00adplete approach that can parse code without \npreparation. Fur\u00adthermore, for precise type checking and other error detection, we aim for a sound parsing \nmechanism without heuristics to avoid both error reports on correct code and undetected errors. In addition, \nwe aim for acceptable performance in real-world settings; although we cannot avoid the inherent complexity, \nwe want to avoid accidental complexity as far as possible. Conceptually, we design a sound and complete \nsolution in Sections 3 6 and implement a sound and complete parser for Java with conditional compilation; \nhowever, due to im\u00adplementation issues in the lexer, we makes small sacri.ces regarding completeness \nfor C, as we will discuss in Section 9.  2.3 A .nal remark By no means do we intend to encourage developers \nto use lexical preprocessors. If they can encode variability with a better mechanism, they should. For \nexample, frameworks and module systems [51], syntactic preprocessors [42, 68], dedicated language constructs \nfor compile-time variability as in the language D, software composition mechanisms such as feature-oriented \nprogramming or aspect-oriented programming [2, 34], projectional workbenches [56, 66], and external variability \nmappings [13, 26, 30] may all have their own shortcomings, but they all do not depend on lexical preprocessing. \nPreprocessors have many well-known problems beyond parsing [16, 18, 59] and should have been replaced \ndecades ago. However, we acknowledge that they are still widely used in practice [16, 39] and that they \nare often the simplest path to introduce variability. We do not recommend using lexical preprocessors, \nbut we intend to support developers who are forced to use them in the vast amount of existing code. 3. \nArchitecture of TypeChef We consider variability-aware parsing in the larger context of our TypeChef \nproject, which consists of three main com\u00adponents as shown in Figure 3. First, a variability-aware lexer \nreads the target .le (and some con.guration parameters) to produce a token stream. The lexer propagates \nvariability from conditional compilation to conditions in the token stream and resolves macros and .le \ninclusion. Second, a variability\u00adaware parser reads the token stream and produces an abstract  TypeChef \nvariability-aware parser framework include directories  #ifdef A  + #define X4 #else #define X5  2 \n\u00b7*\u00b7 3 \u00b7 + \u00b7 4A \u00b7 5\u00acA * .A  #endif 2*3+X 2 34 5 partial con.guration  Figure 3. Architecture of the \nTypeChef project. syntax tree with variability. Parsers for speci.c languages are implemented using a \ngeneric variability-aware parser framework (a parser-combinator library). Finally, the parse result can \nbe further processed by a variability-aware type system. All components can also be used in isolation \nand for other purposes. Our main focus here is the development of the variability\u00adaware parser-combinator \nlibrary (Sec. 5), the variability\u00adaware parsers for GNU C and Java (Sec. 6), and the applica\u00adtion to \ntwo case studies (Sec. 7). We already presented the variability-aware lexer, based on earlier ideas of \npreprocessor analysis [27, 37], in prior work [33]. Nevertheless, we brie.y repeat its basic mechanisms \n(Sec. 4), because it is relevant for understanding how conditional-token streams are produced and how \nwe deal with (conditional) macros. 4. A variability-aware lexer The variability-aware lexer (formerly \nalso named partial pre\u00adprocessor) decomposes a code fragment into tokens, propa\u00adgates variability from \nconditional compilation into the pro\u00adduced token stream, includes all necessary header .les, and expands \nall macros. In the produced token stream, each token has a presence condition a propositional formula \nover features that eval\u00aduates to true if the token should be included in compilation.2 Hence, we speak \nof conditional tokens in a conditional-token stream. After a directive #if X (or similar directives for \nother preprocessors), all tokens receive the presence condi\u00adtion X until the corresponding #endif directive. \nFor nested #if directives, presence conditions are conjuncted (X . Y); also #if-#elif-#else-#endif chains \nare handled accordingly. Deriving presence conditions from #if directives is straight\u00adforward, see [57] \nfor a more formal description. We denote presence conditions as subscripts to tokens, we separate tokens \nby \u00b7 , and we denote the empty token sequence as \u00d8 . We omit the presence condition true on 2 For how \nto encode nonboolean features see Sec. 9. tokens that are included in all variants. For example, lexing \nthe .rst expression from Figure 1 yields the conditional-token stream 3 \u00b7*\u00b7 7 \u00b7 + \u00b7 1A \u00b7 0\u00acA. For preprocessors \nthat provide only conditional compila\u00adtion, such as Antenna for Java ME, tokenizing and detecting presence \ncondition is suf.cient and easy to implement. For the C preprocessor, we resolve also .le inclusion and \nmacros. File inclusion and macros. Handling .le inclusion and macros is straightforward in principle. \nWhen including a header .le, we simply continue reading tokens from that .le; when reading a token for \nwhich a macro expansion is de.ned, we return the expansion of the macro. When there are multiple de.nitions \nof a macro (e.g., de.ned in different conditional compilation blocks as in Figures 1 and 2), we return \nall possible expansions with corresponding presence conditions.3 Hence, the second expression in Figure \n1 results in the following conditional-token stream: 3 \u00b7*X \u00b7 7X \u00b7 + \u00b7 1Y.Z \u00b7 4Y.\u00acZ \u00b7*Y \u00b7 1Y.X \u00b7 2Y.\u00acX \n\u00b7 0\u00acY There are several nontrivial interactions between condi\u00adtional compilation, macros, and .le inclusion \nand several nontrivial constructs in macros, which are all handled in the variability-aware lexer (except \nfor two minor implementation issues discussed in Sec. 9; the interested reader may try our lexer online \nat the project s web page). For example, a macro may expand only under some condition, a macro can be \nused inside an #if expression, a second macro de.nition replaces previous de.nition, a macro may be unde.ned \nexplicitly with #undef, macros are used for include guards, macros can have parameters (and variadic \nparameters), macros may use stringi.cation, and many more. A detailed description would exceed the scope \nof this paper, but we refer the interested reader to related publications [33, 37]. 3 Internally, in \ncontrast to the macro table of an ordinary preprocessor, our variability-aware lexer stores alternative \nexpansions of a macro in a conditional macro table [33]. In that table, each macro expansion has a corresponding \npresence condition; for example, in Figure 1, B expands to 1 if X and to 2 if \u00acX.  1 static void rt_mutex_init_task(struct \ntask_struct *p) { 2 raw_spin_lock_init(&#38;p->pi_lock); 3 #ifdef CONFIG_RT_MUTEXES 4 plist_head_init_raw(&#38;p->pi_waiters, \n&#38;p->pi_lock); 5 p->pi_blocked_on = NULL; 6 #endif 7 } . macro expansion by variability-aware lexer \n. 1 static void rt_mutex_init_task(struct task_struct *p) { 2 #ifdef CONFIG_DEBUG_SPINLOCK 3 do { static \nstruct lock_class_key __key; __raw_spin_lock_init((&#38;p->pi_lock), \"&#38;p 4 ->pi_lock\", &#38;__key); \n} while (0) 5 #else 6 do { *(&#38;p->pi_lock) = (raw_spinlock_t) { .raw_lock = 7 #ifdef CONFIG_SMP 8 \n{0} 9 #else 10 {} 11 #endif 12 , 13 #ifdef CONFIG_DEBUG_LOCK_ALLOC 14 .dep_map = { .name = \"&#38;p->pi_lock\" \n} 15 #endif 16 }; } while (0) 17 #endif 18 ; 19 #ifdef CONFIG_RT_MUTEXES 20 plist_head_init_raw(&#38;p->pi_waiters, \n&#38;p->pi_lock); 21 p->pi_blocked_on = ((void *)0); 22 #endif 23 } Figure 4. Excerpt from .le kernel/fork.c \nin Linux illustrates how variability-aware lexing exposes variability from macros in header .les. Note \nthat variability-aware lexing introduces previously hidden variability from macros and header .les into \nthe token stream. In Figure 4, we illustrate one concrete excerpt from the Linux kernel, in which variability, \ndepending on several features de.ned in header .les, becomes apparent only by lexing (we serialize the \ntoken stream as C code for illustration purpose): raw_spin_lock_init in Line 3 is a macro with alternative \nexpansions, the body of which is expanded again. In the worst case, alternative macros could lead to \nan exponential explosion of the size of the produced token stream, but for common source code the increase \nis moderate.4 In our Linux evaluation, on average, partial preprocessing increases input size by 6.4 \ntimes compared to the .le preprocessed with a minimal con.guration. 5. A library of variability-aware \nparser combinators After describing how we create conditional-token streams (and eliminate of macros \nand .le inclusion in the process), we focus on parsing these token streams into abstract syntax trees \nwith choice nodes ( . ). 4 We discuss performance optimizations, e.g., preserving sharing to prevent \nexplosion due to iterative replication, elsewhere [25, 33]. Our strategy is to parse the conditional-token \nstream in a single pass, but split the parser context on conditional tokens and join the parser contexts \nagain to produce choice nodes in the abstract syntax tree. The parser context is a propositional formula \n(like presence conditions) that describes for which variants the parser is currently responsible. We \ndetermine split and join positions by reasoning about the token s pres\u00adence conditions and the parser \ncontext. We split only when necessary and join early to avoid parsing tokens repeatedly. Nevertheless, \nwe might need to parse some tokens multi\u00adple times to handle undisciplined annotations, but since we \nsplit and join locally, we avoid the accidental complexity of brute-force approaches. We illustrate the \nparsing strategy with an example in Figure 5. In this .gure, we explain the process in multiple steps \nand show produced abstract-syntax-tree nodes and the current position of all parser branches and their \ncontext. We have implemented our parser framework as a parser\u00adcombinator library [28] in Scala (and a \nminimal version in Haskell as well). The parser-combinator library implements recursive-descent parsers \n(also known as top-down, back\u00adtracking, or LL parsers). Although other parsing technologies likely would \nhave been possible and even more ef.cient, we opted for top-down parser combinators, because they are \neasy to understand and modify, which was valuable when exploring different design decisions. For explanation, \nwe use concise pattern-matching pseudo code in a functional style.    5.1 Parsers and results A parser \nin a standard parser-combinator library (e.g., in Scala [44, Ch. 31]) is a function that accepts a token \nstream and returns a parse result and the remaining token stream, or an error message: Parser[T ]= TokenStream \n. ParseResult[T ] ParseResult[T ]= Succ(T, TokenStream) | Fail(Msg) Two parsers p and q can be combined \nto form a new parser with the sequence combinator (p~q), the alternatives combi\u00adnator (p|q), a function-application \ncombinator (p^^f), and others. A standard sequence parser combinator produces a parser that .rst executes \nthe parser p and, if that does not fail, subsequently executes the parser q on the remaining in\u00adput stream; \nthe combinator returns the concatenated (tupled) results of both parsers. A standard alternative parser \ncombi\u00adnator creates a parser that executes parser p and returns either p s result, if it is successful, \nor the result of calling parser q on the original input. The standard function-application combinator \napplies a function f to the parse result of p to pro\u00adcess the parse result further, for example, to create \nabstract syntax from parse trees. In that way, semantic actions can be executed while parsing. Our variability-aware \nparsers are similar, but consider a context and multiple possible results (corresponding to  with initial \ncontext true, the parser is responsible for all variants (1) The parser consumes three tokens, but cannot \nprocess the fourth token, because that token ( \u00b7 \u00b7 + \u00b7 4A \u00b7 (\u00acA \u00b7 5\u00acA.B \u00b7 +\u00acA.B \u00b7 6\u00acA \u00b7 )\u00acA \u00b7 ) is not \npresent in all variants. the parser recognized this integer literal this parser assumes that A is selected \n \u00b7\u00b7\u00b7 \u00b7\u00b7\u00b7 ) (2) The parser splits into two branches, each with 4A (\u00acA 5\u00acA.B +\u00acA.B 6\u00acA )\u00acA a context responsible \nfor a distinct part of the 4A \u00b7 (\u00acA \u00b7 5\u00acA.B \u00b7 +\u00acA.B \u00b7 6\u00acA \u00b7 )\u00acA \u00b7 ) variant space. this parser assumes \nthat A is not selected ( \u00b7 3 \u00b7 + this integer literal is present if A is selected (3) The upper branch \nconsumes a token and  only responsible for tokens with A skips .ve tokens it is not responsible for; \nthe \u00b7 (\u00acA \u00b7 5\u00acA.B \u00b7 +\u00acA.B \u00b7 6\u00acA \u00b7 )\u00acA \u00b7 ) lower branch skips one token and needs to ( \u00b7 3 \u00b7 + split \nagain shortly after, because token 5 is 4A \u00b7 (\u00acA \u00b7 5\u00acA.B \u00b7 +\u00acA.B \u00b7 6\u00acA \u00b7 )\u00acA \u00b7 ) present only in some \nvariants of that branch s responsibility.  not responsible for tokens with A skipped, because A is not \nselected in this branch  4A \u00b7 (\u00acA \u00b7 5\u00acA.B \u00b7 +\u00acA.B \u00b7 6\u00acA \u00b7 )\u00acA \u00b7 ) (4) The lower branches both parse \nan expression and meet at the same position. We need ( \u00b7 3 \u00b7 + to process token 6\u00acA twice, because the \n \u00b7 \u00b7\u00b7\u00b7 5\u00acA.B +\u00acA.B )\u00acA ) annotations are not disciplined: 5 \u00b7 + does 4A \u00b7 (\u00acA not form a full expression. \n \u00b7 )\u00acA \u00b7 ) 4A \u00b7 (\u00acA \u00b7 5\u00acA.B \u00b7 +\u00acA.B \u00b7 6\u00acA \u00b7 )\u00acA \u00b7 ) (5) Two parser branches join, producing an inter-( \n\u00b7 3 \u00b7 + mediate result with a choice node. 4A \u00b7 (\u00acA )\u00acA \u00b7 ) intermediate results are joined with a \nchoice node .B (6) The remaining branches join before the .nal clos\u00ading bracket, producing an\u00adother choice \nnode; the parser produces the .nal result in a single branch.  Figure 5. Example of parsing the conditional \ntoken stream ( \u00b7 3 \u00b7 + \u00b7 4A \u00b7 (\u00acA \u00b7 5\u00acA.B \u00b7 +\u00acA.B \u00b7 6\u00acA \u00b7 )\u00acA \u00b7 ). multiple branches). A parser is a \nfunction from a token stream and a context (provided as propositional formula, type Prop) to a result \nwhich can either be (a) a parse result with the remaining tokens, (b) an error message, or (c) a split \nresult which contains two inner results depending on some condition (derived from the context and presence \ncondition when splitting). This way, a parser can return multiple results to implement context splits. \nVParser[T ] = Prop \u00d7 TokenStream . VParseResult[T ] VParseResult[T ] = Succ(T, TokenStream) | Fail(Message) \n| Split(Prop, VParseResult[T ], VParseResult[T ])  5.2 Context splitting The fundamental parser on which \nall further parsers are built, is the parser next which returns the next token. With variability, next \nmay return different results depending on presence conditions and context of the parser. Hence, parser \nnext implements context splitting and is a core mechanism of our variability-aware parser. Given the \nparser s context ctx and a token with presence condition pc, there are three possibilities: 1. The parser \nconsumes the token, if and only if it would be present in all variants that the parser is responsible \nfor. Technically, that is the case if ctx implies pc (i.e., ctx . pc), which we determine with a SAT \nsolver. 2. The parser skips the token, if and only if the parser is not responsible for a single variant \nin which the token would be present. Technically, that is the case if ctx contradicts pc (i.e., ctx . \n\u00acpc). 3. In all other cases, the parser is responsible for some, but not for all, variants in which \nthe token would be present. In this case, we split the parser context into two branches with contexts \nctx . pc and ctx .\u00acpc (the .rst of these parsers will consume the token, the second will skip it).  \nIn case we reach the end of the token stream, we return an error message. next : VParser[Token] next(ctx, \n\u00d8) = Fail(\"unexpected end of file\") next(ctx, tpc \u00b7 rest) Succ(tpc, rest) if ctx implies pc =next(ctx, \nrest) if ctx contradicts pc Split(pc, otherwise next(ctx . pc, tpc), next(ctx .\u00acpc, rest)) For example, \nnext(A,1\u00acA \u00b7 2B \u00b7 3) skips the .rst token, because it is never responsible (A contradicts \u00acA), then splits \nthe result, because it is responsible for some but not for all variants of the second token (A does not \nimply B), and returns Split(B, Succ(2, 3), Succ(3, \u00d8)). Our design is rather unusual, because we reason \nwith a SAT solver during parsing for every token. For each token, we compare parser state and presence \nconditions to determine split positions and skipping.5 Although such reasoning can be computational expensive \nin the worst case (determining tautologies and contradictions is NP-hard), there is evidence that SAT \nsolvers scale well for tasks in variability analysis [32, 43, 62]. We will demonstrate their ef.ciency \nalso for our parser in our evaluation. 5.3 Filtering Based on the next parser, we can create more sophisticated \nparsers, for example, parsers that expect a certain kind of token, such as identi.ers, numbers, or closing \nbrackets. For reasoning about variability, these parsers rely entirely on next. Function textToken creates \na parser that checks whether the next token has a given textual representation; for example, textToken \n+ would only accept tokens representing the plus sign. To implement textToken, we use a .lter function \nthat checks all successful parse results from next and replaces them by failures in case the expected \ntoken does not match. .lter :(T . Bool) \u00d7 VParseResult[T ] . VParseResult[T ] .lter(p, Succ(val, rest)) \nSucc(val, rest) if val satis.es p = Fail(\"unexpected \" + val) otherwise .lter(p, Fail(msg)) = Fail(msg) \n.lter(p, Split(prop, res1, res2)) = Split(prop, .lter(p, res1), .lter(p, res2)) textToken(\u00b7): String \n. VParser[Token] textToken(text)(ctx, input) = .lter(.t.t represents text, next(ctx, input))  5.4 Joining \ncontexts We implement a novel parser combinator for the joining of parse results (p!) that attempts to \njoin the results of another parser. The key idea is to move variability out of a split parse result into \nthe resulting abstract syntax tree. That is, instead of multiple parser branches, each with a different \ncontext and parse result, we produce a single parser branch with one parse result that contains a choice \nnode (cf. Steps 4 6 in Fig. 5). For 5 In addition, we can also reason about a variability model fm, which \ndescribe intended dependencies between features in software product lines, such as feature A requires \nfeature B and mutually excludes feature C. This way, we could restrict the parser only to a subset of \nvariants. Technically, such reasoning is straightforward, we start the parser with context fm instead \nof true or we consume a token if fm . ctx . pc holds and skip it if fm . ctx . \u00acpc; the latter has more \noptimization potential for reasoning with SAT solvers. However, a more detailed discussion is outside \nthe scope of this paper; see [7, 14, 32, 62] for more information about using variability models in variability \nanalysis.  example, join replaces Split(A, Succ(1, 3), Succ(2, 3))by Succ(.A(1, 2),3). Joining early \nhelps to reduce parsing effort for sequencing and alternative parser combinators (see below) and produces \nsmaller abstract syntax trees with choice nodes that are more local. To join two parser branches in a \nsplit parse result, (a) both parser branches must have succeeded and (b) both must, in their respective \ncontext, expect the next token at the same position. The second condition guarantees that the further \nbehind parser can safely catch up to the position of the other parser, because it would skip the tokens \nin between anyway. Therefore, after joining, the parser resumes at the position of the further advanced \nbranch as determined with auxiliary function rightmost. If two parse results cannot be merged, join returns \nthe unmodi.ed parse result. There are different possible encodings of choice nodes in abstract syntax \ntrees. Although easiest in an untyped setting (as typically used for ambiguity nodes in GLR parsing), \nwe want to preserve types in the abstract syntax tree. We therefore wrap a generic Conditional[T] decorator \ntype around each subterm of the abstract syntax tree that should support vari\u00adability. A Conditional[T]value \ncan either be Oneif there is no variability or Choice (. for short) in case of alternatives. For example, \nthe abstract syntax tree from the previous example is actually represented as Succ(.A(One(1), One(2)),3). \nIn most examples, however, we omit One for better readability, as already done in previous .gures. Conditional[T \n] = One(T) | .Prop(Conditional[T ], Conditional[T ]) join : Prop\u00d7 VParseResult[T ] . VParseResult[Conditional[T \n]] join(ctx, Succ(val, rest))= Succ(One(val), rest) join(ctx, Fail(msg))= Fail(msg)join(ctx, Split(prop, \nres1, res2)) (Succ(.prop(val1, val2), rightmost(rest1, rest2)) 1 equals Succ(val1, rest1) and . .. . \n. Split(a, Split(a .\u00acb, Split(b, Succ(2, rest2), . Succ(1, rest1), Split(a, rewrite Succ(2, rest2)), \nSucc(1, rest1), Succ(3, rest1)) Succ(3, rest1))) Figure 6. Restructuring of a Split tree allows joins \nacross the tree, while preserving variability. even though they are at the same position rest1 in the \ntoken stream. We improve the join mechanism by attempting to join every pair of results. To join two \nmatching nodes at dif\u00adferent positions in the Split tree, we simply restructure the tree until the two \njoinable results are siblings, as illustrated in Figure 6, at the right. For such restructuring, we com\u00adbine \ntwo simple rules: Split(a, x, y) = Split(\u00aca, y, x) and Split(a, Split(b, x, y), z) = Split(a . b, x, \nSplit(a, y, z)). The restructuring preserves the conditions for all results and allows us to apply the \noriginal join function subsequently. This improvement allows us to join earlier in some cases; since \nit is straightforward, but requires more code, we omit a listing. When to join? By selecting when to \njoin, we can precisely determine where in the resulting abstract syntax tree variabil\u00adity in the form \nof choice nodes is allowed. Technically, we can attempt to join parse result after every parsing step \nor after every production. For illustration purposes, we joined at expression level in Figure 1. In our \ncurrent parser implemen\u00adtation for Java and C, we attempt to join at manually de.ned positions: in lists, \nafter statements, and after declarations (for details see Sec. 6). The rationale for this design decision \nis based on the fol\u00adlowing trade off: Checking whether split parse results are join\u00adable produces a small \ncomputational overhead, hence joining too often may reduce performance. In contrast, joining too late \nleads to parsing code fragments in split parser branches if res unnecessarily. In our experience, joining \nafter typical .ne\u00ad res = 2 equals Succ(val2, rest2) and grained program structures seems to be a good \nbalance be\u00ad next(ctx . prop, rest1) equals next(ctx .\u00acprop, rest2) tween computation overhead and low \namount of repeated 1, res2) . . .. parsing. We have not performed an experimental analysis of Split(prop, \nres otherwise 1 is join(ctx . prop, res1) and 2 is join(ctx .\u00acprop, res2) how the selection of join positions \nin.uences the performance of the parsing algorithm. Note that joining at different posi\u00ad tions leads \nto different (but equivalent) resulting parse trees. where res res \u00b7 ! : VParser[T ] . VParser[Conditional[T \n]] (p!)(ctx, input)= join(ctx, p(ctx, input)) Optimization. The listed join mechanism attempts to join \nonly parse results that evolved from the same context split. Hence, we might miss some join opportunities. \nFor example, in Figure 6, at the left, we cannot join the branches with results 1 and 3, because they \ndo not share the same parent, For some downstream tools it may be convenient to place choice nodes only \nat preselected locations in the resulting tree, although rewriting a tree to move choice nodes is always \npossible as explored in the choice calculus [17].  5.5 Sequencing The sequence parser combinator (p~q) \nbecomes more com\u00adplex when alternative results are involved. The produced parser continues all successful \nresults (and only successful results) of the .rst parser with the second parser. The second parser might \nbe called multiple times with different contexts (for multiple successful results of the .rst parser). \nIf the sec\u00adond parser splits the result, the .rst parse result is copied.6  As example, consider parsing \nthe token sequence 1A \u00b7 2A \u00b7 3\u00acA \u00b7 4 with next~next. The .rst parser yields Split(A, Succ(1, 2A \u00b7 3\u00acA \n\u00b7 4), Succ(3, 4)), and the sec\u00adond parser is called twice, once with context A on 2A \u00b7 3\u00acA \u00b7 4 and once \nwith context \u00acA on 4. Overall, the parser combina\u00adtor yields Split(A, Succ(1~2, 3\u00acA \u00b7 4), Succ(3~4, \u00d8)). \nTechnically, a function seq calls the second parser with the token stream and context of the .rst parse \nresult (note, the context changes when propagating seq over splitted parse results). Auxiliary function \nconcat simply concatenates (tuples) successful results. seq(ctx, q, Succ(val1, rest))= concat(val1, q(ctx, \nrest)) seq(ctx, q, Fail(msg))= Fail(msg) seq(ctx, q, Split(prop, res1, res2)) = Split(prop, seq(ctx . \nprop, q, res1), seq(ctx .\u00acprop, q, res2)) concat(val1, Succ(val2, rest))= Succ(val1~val2, rest) concat(val1, \nFail(msg))= Fail(msg) concat(val1, Split(prop, res1, res2)) = Split(prop, concat(val1, res1), concat(val1, \nres2)) \u00b7 ~\u00b7 : VParser[S] \u00d7 VParser[T ] . VParser[S~T ] (p~q)(ctx, input)= seq(ctx, q, p(ctx, input)) \n  5.6 Alternatives The parser combinator for alternatives (p | q) is implemented similarly to sequencing. \nHowever, instead of concatenating all successful results, it replaces all failures with the result of \nthe second parser, called with the corresponding con\u00adtext. For example, if parser q returns a split parse \nresult Split(A, Succ(1, 2), Fail(\"...\")), the second parser is called with context \u00acA to replace the \nfailed result (of course the second parser may fail again or return a split parse result). Again, joining \nthe parser before replacing alternatives can reduce effort, so that the second parser is called less \noften. alt(ctx, input, q, Succ(val, rest))= Succ(val, rest) alt(ctx, input, q, Fail(msg))= q(ctx, input) \nalt(ctx, input, q, Split(prop, res1, res2)) = Split(prop, alt(ctx . prop, input, q, res1), alt(ctx .\u00acprop, \ninput, q, res2)) \u00b7 | \u00b7 : VParser[T ] \u00d7 VParser[T ] . VParser[T ] (p | q)(ctx, input)= alt(ctx, input, \nq, p(ctx, input)) 6 Since we use immutable parse results, a pointer to the same shared data structure \nis suf.cient. 5.7 Repetition Parser combinators for repetition (p * or p+) can be con\u00adstructed with \nsequencing and alternatives. For example, with a parser combinator e that always returns success without \nconsuming a token, we can implement p * as (p~p *) | e. This correctly deals with conditional tokens \nbut has performance problems when parsing long lists with optional entries. To illustrate the problem, \nconsider the following example: For token sequence 1A \u00b7 2 \u00b7 3 \u00b7 4, the parser next* splits at the .rst \ntoken and yields the intermediate result Split(A, Succ(1, 2 \u00b7 3 \u00b7 4), Succ(2, 3 \u00b7 4)), after which we \ncannot join because both parser branches are at different positions in the token stream. The next iteration \nwould yield Split(A, Succ(1~2, 3 \u00b7 4), Succ(2~3, 4)), which, again, we cannot join. Advancing only the \nbehind-most branch is not possible with our combinators so far. In the worst case, we can only join parser \nresults at the end of the list, which means that all list elements after an optional element will be \nparsed twice. In C code, this problem is critical, because C .les are essentially a long list of top-level \ndeclarations, several of which are typically optional. Therefore, we provide a specialized combinator \nrepOpt(p) for repetition, which returns a list of optional entries (instead of a choice of lists). Each \nentry in this result list has a presence condition. For example, parsing 1A \u00b7 2 \u00b7 3 with repOpt(next) \nyields the list OptA(1), Opttrue(2), Opttrue(3).7 Technically, repOpt tries the following strategies: \nFirst, it tries to suppress splitting and parses the .rst list element in isolation. If the parser does \nnot skip any tokens in the process, we can add its result with a corresponding presence condition to \nthe result list (i.e., in the example above, we would recognize 1A directly as OptA(1) without splitting). \nWith repOpt, we avoid excessive splitting and joining within lists. For some input streams, splitting \ncannot always be avoided; then, repOpt parses only in the branch that has consumed the fewest tokens \nso far, in the hope that branches behind catch up and we can join both branches again early. Similarly, \nwe constructed performance-optimized parser combinators for comma-separated lists (p~(q~p)*) and non\u00adempty \nlists (p+). 5.8 Function application Finally, we adjusted the parser combinator for function application \n(p^^f ), that is used to process parse results (e.g., for constructing abstract syntax out of parse trees). \nThe modi.cation is straightforward: We simply apply the function to all results in a split parse result. \n 5.9 Parsing effort As for the variability-aware lexer, worst-case complexity (parsing time and output \nsize) is exponential. Unfortunately, this worst-case complexity cannot be avoided it is inher\u00ad 7 Optc(v) \nis conceptually equivalent to a choice node with an empty branch .c(v, ) and counted as choice node for \nstatistics.  .A * + 3 \u00b7*\u00b7 (A \u00b7 2 \u00b7 + \u00b7 4 \u00b7 )A . 3 + * 4 2432 Figure 7. Undisciplined annotation leading \nto higher parsing effort and replication. ent in the task because variability can be used in such a way \nthat for n features there are 2n completely different outputs. Problematic are code fragments with undisciplined \nannotations that change the structure of the output, such as the example shown in Figure 7. In addition, \nwe again have to determine satis.ability of propositional formulas (for every combination of presence \ncondition and parser state) which is NP-hard.8 Nevertheless, typical source .les can be parsed ef.ciently. \nThere are several characteristics that contribute to ef.ciency in the common cases: We split parser \ncontext as late as possible and provide facilities to join parser context early. In contrast to the brute \nforce approach, we avoid accidental complexity by not parsing all tokens before and after a variable \nsegment multiple times.  If we attempt to join after each parsing step, we guarantee to consume each \ntoken only once in token streams with only disciplined annotations (tokens consumed by one parser branch \nare skipped by all other branches). With disciplined annotations, we do not replicate any nodes in the \nresulting abstract syntax tree. Only in the pres\u00adence of undisciplined annotations, we need to consume \ntokens multiple times and replicate nodes in the result. Although undisciplined annotations are quite \ntypical [40], they mostly occur locally, so that parsing overhead and replication are comparably low. \n Although we need to reason about the relationship be\u00adtween parser context and presence condition for \nevery single token (and once for each parser context), this can be done ef.ciently with contemporary \nSAT solvers, even for complex formulas with hundreds of features. We have implemented and .ne-tuned a \nlibrary for propositional formulas (with some extensions for the C preprocessor s facilities for integer \nconstants) in Scala [25] and connected it to the SAT solver sat4j [38]. In addition, since typically \n 8 Already the underlying parsing technology, recursive descent parsing with backtracking, is inef.cient. \nHowever, we exclude this aspect from discus\u00adsions within this paper, because we believe that other parsing \ntechnologies could be used as well (e.g., packrat parsing [20], LR parsing [24], GLR parsing [63]). We \nmerely used recursive descent parsers because parser com\u00adbinators made it easy to understand and explore \ndifferent strategies. In all statistics on consumed tokens, we ignore backtracking-related effort (i.e., \na token consumed multiple times in the same context). 1 // #ifdef includeMMAPI 2 public void showMediaList(String \nrecordName, ...) { ... 3 // #ifdef includeFavourites 4 if (favorite) { 5 if (medias[i].isFavorite()) \n6 mediaList.append(medias[i].getMediaLabel(), null); 7 } 8 else 9 // #endif 10 mediaList.append(medias[i].getMediaLabel(), \nnull); 11 ... } 12 // #endif Figure 8. Antenna preprocessor for Java (excerpt from Mo\u00adbileMedia with \nan undisciplined annotation). 1 statement: 2 labeled-statement 3 compound-statement 4 ... 5 labeled-statement: \n6 identifier : statement 7 case constant-expression : statement 1 def statement: MultiParser[Conditional[Statement]] \n= 2 (labeledStatement | compoundStatement | ...) ! 3 def labeledStatement: MultiParser[Statement] = 4 \n(id ~ COLON ~ statement ^^ 5 {case i~ ~s => LabelStatement(i, s)}) | _ 6 (textToken(\"case\") ~ constExpr \n~ COLON ~ statement ^^ 7 {case ~e~ ~s => CaseStatement(e, s)}) 8 def COLON: MultiParser[Token] = textToken(\":\") \nFigure 9. Language speci.cation of C and corresponding implementation using our parser combinators (excerpt). \na vast majority of tokens share the same presence condi\u00ad tions, caching is very effective. We empirically \ninvestigate parsing effort in two projects in Section 7. 6. Variability-aware parsers for C and Java \nWe have used our parser-combinator library to implement parsers for Java 1.5 and GNU C. Although preprocessor \nus\u00adage is less frequent in Java, there are several tools to introduce conditional compilation again. \nFor example, Antenna, often used for variability in Java ME projects for mobile devices, introduces #if \nand #endif statements in comments as il\u00adlustrated in Figure 8. For C, we implemented a parser that recognizes \nC code (speci.cally the GNU-C dialect used for the Linux kernel) with preprocessor directives of the \nC pre\u00adprocessor (again with GNU-speci.c extensions). We have implemented both parsers with our parser\u00adcombinator \nframework, which was essentially a straight\u00adforward adoption of existing grammars (from JavaCC and ANTLR) \nand involved some .ne-tuning to add missing GNU-C extensions. In Figure 9, we illustrate this implementation \nwith an excerpt from the C language speci.cation [29] and the corresponding Scala implementation using \nour parser\u00adcombinator library (the function-application combinator ^^ is used to create abstract-syntax-tree \nnodes from token se\u00adquences; this fragment also illustrates joins at statement level). Although only \nvisible from the type signature, the shown parser fragment is already variability aware by using our \nspe\u00adcialized parser combinators. As join points for the Java parser, we selected imports, type declarations, \nmodi.ers, class mem\u00adbers, and statements; for the C parser join points are external de.nitions, statements, \nattribute declarations, and type speci\u00ad.ers. We expect that implementing variability-aware parsers for \nother languages is similarly straightforward.  A technical note on parsing C: To distinguish types from \nvalues, a C parser requires a stateful symbol table during parsing. To track the state correctly across \nparser branches, we implemented a conditional symbol table that tracks under which condition a symbol \nis declared as type. 7. Parsing MobileMedia and Linux X86 To evaluate our parser for practical scenarios, \nwe conducted two case studies: We parse the entire unpreprocessed code of (a) the Java ME implementation \nof MobileMedia and (b) the Linux kernel (X86 architecture). MobileMedia is a favorable case study, due \nto its small size (5183 nonempty lines of Java code, 14 features) and the absence of macro expansion, \nso that we can look at parser results without in.uence of macros in the variability-aware lexer. The \nLinux kernel is larger by several orders of magnitude after resolving .le inclusion and macros during \nvariability-aware lexing, we parse a total of 899 million nonempty lines of C code (2.6 billion tokens) \nwith 6065 features. At this size, there is no meaningful way to calculate the exact number of valid feature \ncombinations, but the variant space is huge. Parsing Linux required some signi.cant engineering effort \nto set up and optimize our tools, because of its scale and because variability is additionally managed \nwith the build system (kcon.g, kbuild). At the same time, Linux is a good stress test for our tools; \nfor example, we found macro patterns that we never would have expected such as alternative expansions \nof a macro with different numbers of parameters. From both case studies, we describe the setup, report \nour experience, and collect statistics on parsing effort. All scripts and tools used in our case studies \nare available in the open-source repository of TypeChef. We welcome readers to replicate our evaluation \nand would gladly help to set up parsing other projects. 7.1 Parsing 2400 variants of MobileMedia MobileMedia \nis a medium-sized software product line of a Java ME application that manipulates photo, music, and video \non mobile devices developed at the University of Lancaster.9 According to MobileMedia s variability model, \nwe can derive 2400 distinct variants by selecting from 14 features. 36 of 51 .les contain variability, \ntypically with 1 to 5 and a maximum of 9 features per .le; in total, 14 379 of 23 938 (60 %) tokens are \nannotated. 9 http://mobilemedia.sf.net/, release 8 OO. Setting up the variability-aware parser is straightforward. \nVariability-aware lexing is trivial, because no include paths need to be con.gured and no macros are \ninvolved (i.e., .les do not grow during lexing). No external con.guration knowledge is necessary. We \ncan simply lex and parse each Java .le in isolation. Parsing all 51 .les in the entire project takes \nabout 3 seconds.10 The parser reports all .les as syntactically correct in all variants. During parsing, \nit makes 357 distinct calls to the SAT solver, which requires a negligible time of less than 0.1 seconds \nin total. The produced parse trees contain a total of 319 choice nodes. Due to undisciplined annotations, \nsuch as the one illustrated in Figure 8, we consume 117 of 23 938 tokens twice and a single token three \ntimes. That is, variability-aware parsing causes an overhead of only 0.5 % in terms of consumed tokens. \nAn exact comparison with a brute force strategy is dif.cult to make without actual preprocessing. However, \na rough estimate of a brute force approach per .le indicates a parsing overhead of 27 600 %. 7.2 Parsing \nthe Linux kernel with 6065 features Parsing Linux is more complicated and required some sub\u00adstantial \nadditional engineering effort. In a nutshell, we suc\u00adcessfully parsed the entire X86 architecture of \nLinux release 2.6.33.3 with 6065 variable features and 7665 .les (a total of 44 GB, 899 million nonempty \nlines of C code, and 2.6 billion tokens after variability-aware lexing).11 Parsing the entire code with \nour implementation takes roughly 85 hours on a single machine, but is easy to parallelize. For readers \nin\u00adterested in details, we describe the setup, practical challenges in the process, and some statistics \nin the remainder of this section. Variability implementation in the Linux kernel. To under\u00adstand the \nadditional challenges of parsing the Linux kernel, we .rst describe how Linux is implemented with C, \nthe C pre\u00adprocessor, and a sophisticated build system. A user can select from over 10 000 features, ranging \nfrom different architec\u00adtures, to different memory models, to numerous drivers, and to various debugging \nfeatures. Features and their dependen\u00adcies are described in a variability model, speci.ed in various \nKcon.g .les scattered over the source tree [9]. When users want to build a con.guration, they invoke \na con.guration dialog (make con.g/menucon.g/xcon.g) in 10 All times in this paper have been measured \non normal lab computers (Intel dual/quad-core 3 to 3.4 GHz with 2 to 8 GB RAM; Linux; Java 1.6, OpenJDK). \nWe did not perform low-level optimizations and still compute debug information and statistics. Measured \ntimes provide only rough indicators about what performance to expect and that variability-aware parsing \nis feasible; they are not meant as exact benchmarks. 11 Without preprocessing the analyzed Linux kernel \nsource is roughly 269 MB; but already ordinary preprocessing increases .le size dramatically, because \nmany headers are included in each .le. As described in Sec. 4, the output of the variability-aware lexer \nis roughly 6.4 times larger than the output of an ordinary preprocessor in the minimal con.guration. \n which they can select the desired features. Most features are of type bool or tristate, that is, they \neither have two possible values, include or do not include, or three possible values, do not include, \ncompile into the kernel, compile as module. Few features, such as Timer frequency (CONFIG_HZ), have nu\u00admeric \nor string values. The con.guration mechanism checks and propagates feature dependencies; for example, \nselecting a feature may deselect dependent features and may prevent to select other features later. The \nresulting feature selection is written into a con.guration .le (.con.g). Variability is implemented both \nat build system level (deciding which .les to compile with which parameters) and at source code level \nwith preprocessor directives (deciding which lines to compile and which macros to expand). Based on the \ncon.guration .le, the build system decides which .les to compile. For each .le, the build system can \nprovide alternative or additional directories in which the preprocessor searches for included .les (for \nexample, each architecture has a distinct directory for corresponding header .les). Files may be compiled \nand linked differently depending on whether a feature should be compiled as module or as part of the \nkernel. In few cases, the build system also runs scripts to generate additional .les. Finally, the build \nsystem passes con.gured features (as macros pre.xed with CONFIG_ ) to the preprocessor and C compiler, \npotentially together with additional de.nitions from the build script. In the source code, conditional \ncompilation decides which lines to compile and which macros to expand (e.g., #ifdef CONFIG_X queries \nwhether feature X is selected). Although our variability-aware lexer and parser work without heuristics, \nthe build system is more dif.cult to analyze. We use analysis tools developed by the research team led \nby Czarnecki at the University of Waterloo to extract information about features and their dependency \nfrom the variability model of the X86 architecture [9, 54, 55] and to extract information about presence \nconditions on .les from the build system [8]. Unfortunately, due to the expressiveness of Linux s variability-modeling \nlanguage [9], we could only work on a propositional approximation of the full constraints. Worse, the \nmapping from .les to presence conditions is hidden in an imperative build logic within make.les (using \na universal scripting language). To extract those mappings, the analysis tools rely on fuzzy parsing \n(with unsound heuristics) of the make.les to recognize patterns. As consequence, for now, we attempt \nto reduce our dependency on information extracted from the build system and variability model where possible \nand manually veri.ed involved dependencies in case of reported parsing errors. A partial con.guration. \nWe do not consider all variability, but only 6065 features of the X86 architecture. Speci.cally, from \nover 10 000 features in Linux, we deselect all features from other architectures and features that are \ndead in X86 ac\u00adcording to the variability model (i.e., features that may not be selected by a user). \nThe most important practical reason is that the variability-model and build-system extraction tools cur\u00adrently \nonly extract data for the X86 architecture. Furthermore, we exclude 30 features that expect numeric or \ntextual values (we simply use a default; cf. Sec. 9). We also do not consider #ifdef .ags without the \npre.x CONFIG_ , because they are not managed by the Linux variability model of course the variability-aware \nlexer handles these .ags as well if they are de.ned or unde.ned within the source code, we simply assume \nthem not to be de.ned externally as command line parameters.12 Using the information extracted from the \nbuild system, we determined which .les can be included at all in the considered partial con.guration. \nFrom that list, we excluded 28 .les that depend on .les generated by the build system (analyzing the \nbuild scripts to generate those .les is beyond our scope and using .les generated on our system would \nnot re.ect the variability available in the corresponding generators; cf. Sec. 9). Of 13 665 C .les in \nall architectures combined, we yield a list with 7665 relevant C .les. Parsing Linux. The parser setup \nfor Linux is straightfor\u00adward. We iterate over all 7665 C .les and run the variability\u00adaware lexer with \nour partial con.guration and the correspond\u00ading include paths. We feed the resulting token stream into \nthe parser, together with the presence condition of the entire .le extracted from the build system. The \nprocess is embarrassingly parallel, that is, trivial to parallelize, since every .le can be parsed in \nisolation from the others (we will need to consider dependencies between .les only for type checks, or \nmore accurately linker checks, in future work). Some caching of the results of header .les is theoretically \npossible, but does not seem to be worth the additional effort (given that we would need the exactly same \nsequence of header .les or some nontrivial analysis for sound caching). We simply start the parsing process \non multiple machines with a shared disk (usually seven lab computers) and let each computer process the \n.rst .le not yet started. Without further setup, the parser reports syntax errors for some feature combinations \non many .les. We show an excerpt of a typical example and the corresponding error message in Figure 10: \nIf feature CONFIG_SMP is not selected, a header .le de.nes a macro to replace move_masked_irq from the \nsource code; however if this macro is expanded in the function de.nition in migration.c it breaks the \nsyntax (by default we report errors with line numbers after macro expansion, because it corresponds to \ntypical debugging tasks; reporting line numbers of the original .les is possible as well). After some \ninvestigation, we found that the syntax error only occurs in feature combinations that are not allowed \nby Linux s variability model and cannot be selected when con.guring the kernel manually. The .le migration.c \nis only parsed when feature CONFIG_GENERIC_PENDING_IRQ is selected, 12 Flags without CONFIG_ are typical \nfor include guards or for compiler\u00adspeci.c variability, such as #ifdef __GNUC__. We de.ned all .ags used \nby the gcc 4.4.5 compiler on our system.  Source: kernel/irq/migration.c Header: include/linux/irq.h \n 1 #include <linux/irq.h> 240 #ifdef CONFIG_SMP 2 #include <linux/interrupt.h> 241 ... 242 void move_masked_irq(int \n4 #include \"internals.h\" irq); 243 ... 6 void move_masked_irq(int 244 #else /* CONFIG_SMP */ irq) 245 \n... 7 { 246 #define move_masked_irq(x) 8 ... 247 #endif /* CONFIG_SMP */ Code after macro expansion in \nlexer 277545 ... 277546 void 277547 #if !defined(CONFIG_SMP) 277549 #endif 277550 #if defined(CONFIG_SMP) \n277551 move_masked_irq(int irq) 277552 #endif 277553 { 277554 ... Parser output 1 if CONFIG_SMP: succeed \n2 if !CONFIG_SMP: failed: end of file expected at line: 277545 Figure 10. Conditional parser error in \nLinux, when not considering presence conditions of .les. which depends on CONFIG_SMP; hence, CONFIG_SMP \nis always selected when parsing the .le and the syntax error cannot occur. After adding a corresponding \ndependency to the variability model, our parser correctly accepts the .le. Whenever we found a parsing \nerror that cannot occur in practice due to feature dependencies, we added the depen\u00addency to an internal \nmodel that is used during parsing, leading to the exclusion of the problematic tokens. (We speci.cally \ndo not use the extracted approximated variability model, because it is not reliable enough.) As a side \neffect, we are actually reconstructing a small subset of the variability model from parser errors. For \nparsing the entire X86 architecture, we added 54 such dependencies. Performance and statistics. Parsing \nall 7665 .les takes ap\u00adproximately 85 hours and correspondingly less when paral\u00adlelizing the process \n(for orientation, parsing and compiling a single variant of the kernel requires roughly 15 to 20 minutes \nwith standard tools). On average parsing a .le takes 30 sec\u00adonds, 92 % of all .les require less than \none minute, only 0.4 % require more than .ve minutes and the worst case was 22 minutes (caused by many \nand complex presence condi\u00adtions in some driver code). Roughly 34 % of all time is spent on lexing and \nthe remaining 66 % on parsing. In general, with a couple of machines, one can run the parser over night \nto perform some analysis daily, or one can run the parser within minutes on modi.ed .les of a change \nrequest or commit. In total, we parsed 2.6 billion tokens with an overhead of 4.1 % (consuming tokens \nmultiple times due to undisciplined annotations). On average, per C .le, the variability-aware lexer \nincludes 353 header .les, de.ning 8590 distinct macros (of which 1387 are conditional and 340 have alternative \nex\u00adpansions). After lexing, the average .le contains 335 490 to\u00adkens, of which 72 % are conditional. \nWe have an average parsing overhead of 4.1 % due to undisciplined annotations. The average .le is affected \nby 207 distinct features (which clearly rules out a brute-force approach) and its tokens have 1779 distinct \npresence conditions over these features. The av\u00aderage parse result of a .le contains 20 097 choice nodes. \nAll averages are described by the median; we show distributions in Figure 11 as box plots. We could parse \nall code, which means that we did not .nd any syntax errors. All syntax errors that we found initially \nwere not actual errors, but were false alarms caused by missing features constraints in the variability \nmodel (or by bugs in our tools, which we .xed). For example, we found genuine syntax errors in two dead \n.les (mantis_core.c and i2o_config.c), .les which we accidentally parsed due to an inaccuracy in the \nbuild-system extraction. Despite the large code base and huge variants space, the absence of syntax errors \nis realistic, because we analyzed a released version (and with focus on X86, the most tested architecture) \nand because Linux developers have a commit process in which changes are carefully reviewed. Compile er\u00adrors \nare routinely .xed in the mainline branch before releases. Furthermore, although macros and conditional \ncompilation are sometimes used in extreme ways, the developers mostly follow guidelines how to use macros \nand conditional compi\u00adlation in a controlled fashion.13 We expect problems rather at type-system level \nthan at syntax level;14 our variability-aware parser lays the foundation for corresponding analysis. \n8. Perspective Beyond the most obvious use case of detecting syntax errors, there are many further use \ncases for parsing not only a single variant after preprocessing, but parsing the unpreprocessed code \nwith all variability. Development support. The parsed abstract syntax trees can be used to enhance integrated \ndevelopment environments. For example, we could provide previews of macro expansion (including all possible \ncombinations for different variants) or extend facilities such as code completion with information about \nvariability. Plenty of editor support for preprocessor\u00adbased languages has been proposed [e.g., 36, 65], \nbut, so far, none could rely on a sound abstract syntax tree encoding all variability. Code transformation. \nRefactoring engines [23, 64] and transformation systems [6, 46] typically perform transforma\u00adtions on \nabstract syntax trees and struggle with variability; for example, a rename-method refactoring usually \nshould rename a method in all variants. Additionally, parsing compile-time 13 cf. /Documentation/SubmittingPatches \nin the Linux source. 14 Type errors occurring only in speci.c features or feature combinations such as \nthe one reported in http://marc.info/?l=linux-kernel&#38;m= 130146346404118&#38;w=2.  variability allows \nrewrites of the variability implementation itself. For example, we could rewrite choice nodes into if \nstatements depending on global variables and, hence, defer variability decisions from compile time to \nload time. Error detection. On the resulting abstract syntax tree with choice nodes, many interesting \nopportunities for variability\u00adaware analysis arise. A prime candidate is variability-aware type checking, \nwhich we outline below. Similarly, extending other existing analyses in a variability-aware fashion, \nsuch as bug .nding, control-.ow analysis, and model checking appears as promising research avenue. Especially \nfor model checking, variability-aware approaches that analyze state sys\u00adtems with feature conditions \nhave already been explored [e.g., 12, 52]; our parser paves the way for translating existing C code with \nits variability into such state systems. Variability-aware type checking. The eventual goal of our TypeChef \nproject is to type check all variants of the Linux kernel. Without checking each variant in isolation, \nwe want to ensure that all, up to 26065, variants are well-typed or report corresponding error messages \notherwise. Such a type system detects type mismatches, but also dangling function calls. Checks at linker \nlevel are a useful extension as well. Variability-aware type systems have been explored before [3, 4, \n14, 32, 52, 62], but not on unprepared C code. For example, we implemented such a type system for Java, \nworking around the parser issue by supporting only disciplined annotations in a controlled environment \n[32]. With the parsing issue solved, variability-aware type checking for C is in reach. In a nutshell, \nthe idea is to build a conditional symbol table in which declarations of functions and variables are \nstored with a corresponding presence condition (much like the conditional macro table of our variability-aware \nlexer [33]). When we .nd a function call, we check whether there is one (or more) declaration in scope. \nWe compare the decl presence condition of call pccall and declarations pc i (presence conditions can \nbe deduced from choice nodes in the abstract syntax tree) and issue an error when there are variants \nin which the call but not a declaration is present call (i.e., if, with variability model VM, VM . (pc. \ndecl i pc) does not hold). Similarly, we check that multiple i function de.nitions are mutually exclusive \nand that types of parameters and return types are compatible in all variants. The type system will provide \nsimilar checks for other language constructs. As other variability-aware analyses, the type system directly \nworks on the compact representation of the abstract syntax tree with local variability. Implementing \na full variability-aware type system for C is part of our ongoing research.  For the various use cases, \nthe long parse times are of different concern. In many cases, parsing only a single .le or few changed \n.les is suf.cient (e.g., in editors or analyzing patches). Heavy analysis tasks on a large code base \ncan realistically run in nightly builds with some parallelization. 9. Limitations Although we were able \nto show that our setup scales even for the complexity faced in Linux, there are both conceptual and implementation-speci.c \nlimitations that are important to know to judge the capabilities of our parser. Although we cannot parse \narbitrary C code due to these limitations, the parser is nearly complete and can handle the vast majority \nof code fragments as demonstrated with Linux. We expect that all features are boolean and limit presence \nconditions to propositional formulas. We evaluate constraints (such as #if VER>2 ) only if the corresponding \nmacros are de.ned with #de.ne within the source code, but we do not accept numeric constraints over features \nprovided as open command-line parameters. However, we can always (manu\u00adally or even automatically) encode \ncountable parameters with boolean .ags by enumerating all possibilities in the source code (#if VER1 \n\u00b7 #de.ne VER 1 \u00b7 #elif VER2 \u00b7 #de.ne VER 2 ...). As consequence, we de.ned 30 nonboolean fea\u00adtures with \ndefault values as part of our partial con.guration of Linux (cf. Sec. 7.2). This limitation of completeness \nhas mainly performance reasons, because we can ef.ciently rea\u00adson about propositional formulas with SAT \nsolvers; in princi\u00adple other solvers would be possible as well. Variability-aware lexing performs essentially \nsome partial evaluation of macros and includes, which works well because the C preprocessor is simple \nand not Turing-complete (re\u00adcursion is limited; preprocessing is guaranteed to terminate; cf. [33]). \nLexing is even simpler for languages that do not contain macro expansion or .le inclusion, such as Antenna. \nHowever, there are also more expressive preprocessors that allow arbitrary computations, such as m4.15 \nTo what degree variability-aware lexing is possible for such preprocessors is an open question. Fortunately, \nhandling C preprocessor and simpler forms is suf.cient for most practical applications. A similar problem \ncomes up when considering not only the target language and its preprocessor but also the build system. \nFor example, the Linux build system can run arbitrary scripts and generates some .les. So far, we performed \nonly a shallow (and unsound) analysis of the build system and focus on parsing instead (which lead us \nto exclude 28 .les from our evaluation that depend on generated .les). This does not affect the soundness \nof the parser though. Discussions of suitable build systems and how to make them amenable to variability \nanalysis are interesting open research questions, but outside the scope of this paper. For the variability-aware \nlexer, we currently do not provide an operation to undo macro expansion and .le inclusion. 15 http://www.gnu.org/software/m4/ \nThat is unproblematic for our primary goal of type checking, but would be required for refactorings and \nother source-to\u00adsource transformations that should preserve the original code layout. We currently store \nthe original location of tokens for displaying meaningful error messages, but transforming a conditional-token \nstream back to the original code layout with macros and includes would require additional investigation \nand nontrivial engineering effort. Finally, in its current form, the variability-aware lexer is not capable \nto process some corner-case combinations of conditional compilation with macros using stringi.cation \n[29, \u00a76.10.3]. We manually prepared 13 lines of Linux code (out of 9.5 millions; documented in the repository) \nto work around these bugs, but we are currently working on a solution. In that regard, the lexer s implementation \nis not entirely complete, but this is an implementation limitation, not a conceptual one. 10. Related \nwork Our project touches and combines many areas of research, from parsing unpreprocessed C code, to \nparsers, to variability implementation (languages and tools), to partial evaluation (in the lexer), to \nvariability-model analysis, to variability\u00adaware type systems, and to several more. For brevity, we discuss \nonly work that is closest to our novel contributions in this paper parsing unpreprocessed code and practical \nanaly\u00adsis of Linux. For related work on implementing variability, on the variability-aware lexer, and \non variability-aware type systems, see our discussions in prior work [30, 32, 33]; for a comprehensive \ndiscussion on variability-model analysis and reasoning about variability, we recommend Benavides survey \n[7]. 10.1 Parsing of unpreprocessed C code In Section 2.2, we already introduced three main strategies \nto parse unpreprocessed C code: brute force, manual code preparation, and heuristics. The brute-force \nstrategy was used by Vittek to apply refactorings to C code [64]. He simply processed all 2n combinations \nof a .le separately, where a user has to specify the relevant features manually. While this process may \nbe feasible for the complexity observed in MobileMedia, we argue that it is unrealistic for Linux, except \nfor restricted partial con.gurations. Manual code preparation for sound but incomplete parsers was successfully \nused in projects reported by Baxter and Mehlich [6]. They enforce that conditional compilation di\u00adrectives \nmay only wrap selected syntactic structures (such as entire functions and entire statements; hence preventing \nconstructs with exponential parsing complexity). They extend the C grammar such that the C parser accepts \nconditional\u00adcompilation directives as C language constructs, just like compound statements [6]. Preparing \na grammar to understand disciplined annotations is straightforward, adding project\u00adspeci.c patterns (i.e., \nwhat is considered disciplined in this project) is also feasible, but preparing the grammar for all possible \nuses of preprocessor directives is considered impos\u00adsible [46]. Baxter and Mehlich report experience \nthat a small team of developers can rewrite an industrial project with 50 000 lines of code in an afternoon \nto make it parseable by this approach. Favre [18] and McCloskey and Brewer [42] provide migration tools \nto transform lexical preprocessors into disciplined forms (in a different implementation mech\u00adanism). \nHowever, such migration tools are faced with the same parsing problem and are currently based on unsound \nheuristics or require human interaction as well. Actually, our parser could be used to make such migration \ntools more ac\u00adcurate. Nonetheless, we argue that massive code rewrites are unrealistic for projects such \nas the Linux kernel. Our parser is nearly complete (with the exceptions discussed in Sec. 9) and can \nparse large code bases without manual preparation.  Finally, good results for parsing unpreprocessed \nsource code at a large scale have been achieved with heuristics [1, 22, 23, 41, 46]. For example, Garrido \nuses a reversible form of variability-aware lexing (called pseudo preprocessor) together with heuristics \nto perform refactorings on unpreprocessed C code [22, 23]. Padioleau presents a parser Yacfe that accepts \nmost Linux kernel code [46] using a set of heuristics carefully tailored for the project. Yacfe does \nnot expand macros, in that regard, the produced abstract syntax tree does not only contain C code, but \nanalysis tools need to understand (or ignore) additional macro nodes. Furthermore, the structure that \nis recognized is only correct if all assumptions made by the heuristics hold consistently in the entire \nproject; hence, the set of heuristics has to be adapted, .ne-tuned, and maintained for each project to \nparse. Parsing can already go wrong if developers write unusually indented code. Similarly, for an exploratory \nanalysis task, Adams et al. [1] wrote a parser that ignores all code fragments not understood. Badros \nand Notkin developed a parser PCp3 that investigates all #ifdef branches, but backtracks and considers \nonly a single path through the document, so alternative macros are not considered [5]. In addition, srcML \nis a tool frequently used to derive code metrics, which tries to roughly recognize code structures and \ntries to understand preprocessor directives as well (without expanding macros at all) [41]. In our experience \nwith srcML [39, 40], we frequently found incorrect results for nontrivial preprocessor usage. We are \ngenerally skeptical of heuristics, because it is dif.cult to judge correctness. In contrast, we used \nheuristics only for extracting information from the build system, but not for parsing. Although our parser \nframework allows to write sound and complete parsers, its performance is orders of magnitude worse compared \nto solutions based on code preparation or heuristics. For example, Yacfe needs 12 minutes to parse the \nwhole Linux kernel [46], compared to 85 hours for X86 in our evaluation. The code s inherent complexity \nof alternative macro expansions and undisciplined annotations is the root of this performance loss, because \nwe cannot ask developers to rewrite code in a less complex way and we cannot simply ignore complex cases. \nWe still regard the performance of our parser as acceptable for many tasks (usually less than one minute \nper .le, easy to process .les in parallel); however, for many tasks faster parsing may outweigh the disadvantages \nof unsound or incomplete parsing. At the same time, we avoid the accidental complexity of the (sound \nand complete) brute\u00adforce approach to a large degree; parsing the Linux kernel in a brute force fashion, \nwith 90 percent of all .les affected by between 124 and 255 distinct features, would be unrealistic. \n10.2 Variability-aware parsers There are a few approaches to parse unpreprocessed C code that are close \nto our idea of splitting and joining parse results using other parser formalisms. We implement our parser \nbased on parser combinators (the version for eager lan\u00adguages [67]) in Scala [44]. Despite performance \ndrawbacks (backtracking and suboptimal tail-call optimization of Scala in the Java VM), we use LL parsing \nand a parser-combinator interface because it is easy to understand and allowed us to explore different \ndesign decisions. It is possible to integrate our concepts of splitting and joining contexts also with \nother parser technologies, possibly yielding better performance, and different researchers have worked \non similar ideas for LR-based parsers. First, Platoff et al. sketched a similar parser as part of a general \nmaintenance framework [50]. At #ifdef directives, they clone the parser state of an LALR parser and join \nwhen both parsers reach an identical state. However, they do not support alternative macros and they \ndo not evaluate how their approach scales beyond medium-sized systems. Next, Overbey and Johnson outlined \na similar strategy, also based on modi.ed LR parsers [45]. They discussed how to handle alternative macros \nand how to keep a link back to the original source code to allow rewrites. However, they did not fully \nimplement the proposed concepts; currently, their parser Ludwig (part of the Photran environment) only \nprocesses single con.gurations without variability. Finally, in parallel to our work, Gazzillo and Grimm \nde\u00adveloped SuperC, a variability-aware parser based on forking and merging LALR-parser states [24]. In \naddition to a dif\u00adferent parsing technology, they use binary decision diagrams instead of SAT solvers, \nrepresent variability in token streams differently, and produce untyped abstract syntax trees. They evaluated \nperformance of their parser using our setup of the Linux kernel (including the information we extracted \nfrom the build system) and showed a four times faster performance compared to our parser. However, they \ndo not consider a fea\u00adture model and did not check for parser errors when parsing Linux (as discussed \nin Sec. 7.2, without considering depen\u00addencies from the feature model, there are many false alarms). \nIn general, the idea of splitting and joining parser states for variability is similar to GLR parsing, \nwhich splits and merges the parser state for ambiguities [63]. GLR parsers return alternative parse results \n(parse forests) that contain all matches in case of ambiguities. Technically, they also fork parse stacks \nsimilar to our context splits and use a concept called local ambiguity packing that is similar to our \njoins. In contrast to our parser combinators, but more similar to SuperC, GLR parsers use sophisticated \ntechniques to advance the parser with multiple contexts synchronously step by step without backtracking. \n 10.3 Analyzing variability in C code. There are several studies which investigated variability in C \ncode (often including Linux as case study). However, all studies we are aware of either use unsound heuristics \nor look only at the preprocessor directives but not at the underlying C code. In their intention, the \nworks of Tartler et al. are closest to our TypeChef project [57, 61]. They analyze #ifdef variabil\u00adity \nin the Linux kernel to .nd bugs, currently with a focus of .nding code fragments that are never included \nin any variant. They have reported and con.rmed a substantial number of inconsistencies and bugs. However, \nthey perform their analy\u00adsis entirely at the level of code blocks between preprocessor directives, without \nconsidering the underlying code structure. That is, they reason about lines of code and not about abstract \nsyntax trees. At their abstraction level, it is not possible or intended to perform type checking. In \naddition, they did not consider interactions between macro de.nition and condi\u00adtional compilation, as \nour variability-aware lexer does. In our parser, dead code is simply skipped by all parser branches. \nPadioleau s parser Yacfe has been used to build code transformations (called semantic patches) and static \nanalysis for Linux [47, 48]. In this line of work, the authors have identi.ed a series of bugs and rule \nviolations (such as do not use freed memory or do not use .oating point in the kernel ) , however analysis \nof variability was not in their focus. At the level of preprocessor directives, several researchers have \nsuggested tools to extract variability information [35] or to provide visualizations, such as #include \ntrees [65]. Some tools also trace macro expansion, but not their interaction with conditional compilation \n(i.e., neither conditional macros nor alternative macros) [36, 60]. All these approaches intend to support \ndevelopers in maintenance tasks, but do not analyze the underlying C code in a variability-aware fashion. \nErnst et al. quanti.ed macros and conditional compilation in open-source C code (with a focus on macro \nusage) [16], and in prior work, also we investigated how preprocessors are used to implement variability \n[39, 40]. However, those results are based on heuristics (PCp3 and srcML, see above) and do not focus \non producing abstract syntax trees or detecting errors. 11. Conclusion We have presented a novel framework \nfor variability-aware parsing, which, together with a variability-aware lexer, can be used to construct \nparsers that produce abstract syntax trees with variability for unpreprocessed code. Whereas existing \napproaches suffer either from exponential explosion, require manual code preparation, or are based on \nunsound heuristics, we have shown that our parser can effectively parse signi.cant code bases without \nheuristics and code preparation in rea\u00adsonable time. We have demonstrated practicality on a small Java-based \nsoftware product line and by parsing the entire X86 architecture of the Linux kernel with 6065 features. \nIn future work, in our TypeChef project, we plan to build a variability-aware type system that can type \ncheck the entire Linux kernel and report even type errors hidden in speci.c feature combinations. Furthermore, \nperforming other variability-aware analysis (e.g., bug patterns, data-.ow analysis, model checking) on \ntop of the produced abstract syntax trees is a promising avenue. Acknowledgments. We thank Sven Apel, \nSteven She, Rein\u00adhard Tartler, Krzysztof Czarnecki, Shriram Krishnamurthi, and James Noble for discussions \non this project. This work is supported in part by the European Research Council, grant #203099 ScalPL \n. References [1] B. Adams, W. De Meuter, H. Tromp, and A. E. Hassan. Can we refactor conditional compilation \ninto aspects? In Proc. Int l Conf. Aspect-Oriented Software Development (AOSD), pages 243 254. ACM Press, \n2009. [2] S. Apel and C. K\u00e4stner. An overview of feature-oriented software development. Journal of Object \nTechnology (JOT), 8(5):49 84, 2009. [3] S. Apel, C. K\u00e4stner, A. Gr\u00f6\u00dflinger, and C. Lengauer. Type safety \nfor feature-oriented product lines. Automated Software Engineering, 17(3):251 300, 2010. [4] L. Aversano, \nM. D. Penta, and I. D. Baxter. Handling preprocessor-conditioned declarations. In Proc. Int l Work\u00adshop \nSource Code Analysis and Manipulation (SCAM), pages 83 92. IEEE Computer Society, 2002. [5] G. J. Badros \nand D. Notkin. A framework for preprocessor\u00adaware C source code analysis. Software: Practice and Experi\u00adence, \n30(8):907 924, 2000. [6] I. Baxter and M. Mehlich. Preprocessor conditional removal by simple partial \nevaluation. In Proc. Working Conf. Reverse Engineering (WCRE), pages 281 290. IEEE Computer Society, \n2001. [7] D. Benavides, S. Seguraa, and A. Ruiz-Cort\u00e9s. Automated analysis of feature models 20 years \nlater: A literature review. Information Systems, 35(6):615 636, 2010. [8] T. Berger, S. She, K. Czarnecki, \nand A. W.asowski. Feature\u00adto-code mapping in two large product lines. In Proc. Int l Software Product \nLine Conference (SPLC), pages 498 499. Springer-Verlag, 2010. [9] T. Berger, S. She, R. Lotufo, A. W \n.asowski, and K. Czarnecki. Variability modeling in the real: A perspective from the operat\u00ading systems \ndomain. In Proc. Int l Conf. Automated Software Engineering (ASE), pages 73 82. ACM Press, 2010.  [10] \nD. Beuche, H. Papajewski, and W. Schr\u00f6der-Preikschat. Vari\u00adability management with feature models. Sci. \nComput. Pro\u00adgram., 53(3):333 352, 2004. [11] BigLevel Software, Inc., Austin, TX. BigLever Software Gears: \nUser s Guide, version 5.5.2 edition, 2008. [12] A. Classen, P. Heymans, P.-Y. Schobbens, A. Legay, and \nJ.-F. Raskin. Model checking lots of systems: Ef.cient veri.cation of temporal properties in software \nproduct lines. In Proc. Int l Conf. Software Engineering (ICSE), pages 335 344. ACM Press, 2010. [13] \nK. Czarnecki and M. Antkiewicz. Mapping features to models: A template approach based on superimposed \nvariants. In Proc. Int l Conf. Generative Programming and Component Engi\u00adneering (GPCE), volume 3676 \nof Lecture Notes in Computer Science, pages 422 437. Springer-Verlag, 2005. [14] K. Czarnecki and K. \nPietroszek. Verifying feature-based model templates against well-formedness OCL constraints. In Proc. \nInt l Conf. Generative Programming and Component Engineering (GPCE), pages 211 220. ACM Press, 2006. \n[15] S. Ducasse and D. Pollet. Software architecture reconstruction: A process-oriented taxonomy. IEEE \nTrans. Softw. Eng. (TSE), 35:573 591, 2009. [16] M. Ernst, G. Badros, and D. Notkin. An empirical analysis \nof C preprocessor use. IEEE Trans. Softw. Eng. (TSE), 28(12):1146 1170, 2002. [17] M. Erwig and E. Walkingshaw. \nThe choice calculus: A representation for software variation. ACM Trans. Softw. Eng. Methodol. (TOSEM), \n21(1), 2011. to appear. [18] J.-M. Favre. Understanding-in-the-large. In Proc. Int l Work\u00adshop on Program \nComprehension, page 29. IEEE Computer Society, 1997. [19] E. Figueiredo et al. Evolving software product \nlines with aspects: An empirical study on design stability. In Proc. Int l Conf. Software Engineering \n(ICSE), pages 261 270. ACM Press, 2008. [20] B. Ford. Packrat parsing: Simple, powerful, lazy, linear \n(func\u00adtional pearl). In Proc. Int l Conf. Functional Programming (ICFP), pages 36 47. ACM Press, 2002. \n[21] D. Ganesan, M. Lindvall, C. Ackermann, D. McComas, and M. Bartholomew. Verifying architectural design \nrules of the .ight software product line. In Proc. Int l Software Product Line Conference (SPLC), pages \n161 170. Carnegie Mellon University, 2009. [22] A. Garrido. Program Refactoring in the Presence of Pre\u00adprocessor \nDirectives. PhD thesis, University of Illinois at Urbana-Champaign, 2005. [23] A. Garrido and R. Johnson. \nAnalyzing multiple con.gurations of a C program. In Proc. Int l Conf. Software Maintenance (ICSM), pages \n379 388. IEEE Computer Society, 2005. [24] P. Gazzillo and R. Grimm. Parsing all of C by taming the preprocessor. \nTechnical Report TR2011-939, Computer Science Department, New York University, 2011. [25] P. G. Giarrusso. \nTypeChef: Towards correct variability analysis of unpreprocessed C code for software product lines. Master \ns thesis (tesi di diploma di licenza di 2\u00b0 livello), Scuola Superiore di Catania, 2011. [26] F. Heidenreich, \nJ. Kopcsek, and C. Wende. FeatureMapper: Mapping features to models. In Comp. Int l Conf. Software Engineering \n(ICSE), pages 943 944. ACM Press, 2008. [27] Y. Hu, E. Merlo, M. Dagenais, and B. Lagu\u00eb. C/C++ con\u00additional \ncompilation analysis using symbolic execution. In Proc. Int l Conf. Software Maintenance (ICSM), pages \n196 206. IEEE Computer Society, 2000. [28] G. Hutton and E. Meijer. Monadic parsing in Haskell. J. Functional \nProgramming, 8(4):437 444, July 1998. [29] International Organization for Standardization. ISO/IEC 9899\u00ad1999: \nProgramming Languages C, 1999. [30] C. K\u00e4stner, S. Apel, and M. Kuhlemann. Granularity in soft\u00adware product \nlines. In Proc. Int l Conf. Software Engineering (ICSE), pages 311 320. ACM Press, 2008. [31] C. K\u00e4stner, \nS. Apel, and M. Kuhlemann. A model of refactor\u00ading physically and virtually separated features. In Proc. \nInt l Conf. Generative Programming and Component Engineering (GPCE), pages 157 166. ACM Press, 2009. \n[32] C. K\u00e4stner, S. Apel, T. Th\u00fcm, and G. Saake. Type checking annotation-based product lines. ACM Trans. \nSoftw. Eng. Methodol. (TOSEM), 2011. accepted for publication. [33] C. K\u00e4stner, P. G. Giarrusso, and \nK. Ostermann. Partial preprocessing of C code for variability analysis. In Proc. Int l Workshop on Variability \nModelling of Software-intensive Systems (VaMoS), pages 137 140. ACM Press, 2011. [34] G. Kiczales, J. \nLamping, A. Menhdhekar, C. Maeda, C. Lopes, J.-M. Loingtier, and J. Irwin. Aspect-oriented programming. \nIn Proc. Europ. Conf. Object-Oriented Programming (ECOOP), volume 1241 of Lecture Notes in Computer Science, \npages 220 242. Springer-Verlag, 1997. [35] M. Krone and G. Snelting. On the inference of con.guration \nstructures from source code. In Proc. Int l Conf. Software Engineering (ICSE), pages 49 57. IEEE Computer \nSociety, 1994. [36] B. Kullbach and V. Riediger. Folding: An approach to enable program understanding \nof preprocessed languages. In Proc. Working Conf. Reverse Engineering (WCRE), page 3. IEEE Computer Society, \n2001. [37] M. Latendresse. Rewrite systems for symbolic evaluation of C-like preprocessing. In Proc. \nEuropean Conf. on Software Maintenance and Reengineering (CSMR), pages 165 173. IEEE Computer Society, \n2004. [38] D. Le Berre, A. Parrain, O. Roussel, and L. Sais. SAT4J: A satis.ability library for Java, \n2011. http://www.sat4j. org. [39] J. Liebig, S. Apel, C. Lengauer, C. K\u00e4stner, and M. Schulze. An analysis \nof the variability in forty preprocessor-based software product lines. In Proc. Int l Conf. Software \nEngineering (ICSE), pages 105 114. ACM Press, 2010. [40] J. Liebig, C. K\u00e4stner, and S. Apel. Analyzing \nthe discipline of preprocessor annotations in 30 million lines of C code. In Proc. Int l Conf. Aspect-Oriented \nSoftware Development (AOSD), 2011. [41] J. I. Maletic, M. L. Collard, and A. Marcus. Source code .les \nas structured documents. In Proc. Int l Workshop on Program Comprehension (IWPC), page 289. IEEE Computer \nSociety, 2002.  [42] B. McCloskey and E. Brewer. ASTEC: A new approach to refactoring C. In Proc. Europ. \nSoftware Engineering Conf./Foundations of Software Engineering (ESEC/FSE), pages 21 30. ACM Press, 2005. \n[43] M. Mendon\u00e7a, A. W.asowski, and K. Czarnecki. SAT-based analysis of feature models is easy. In Proc. \nInt l Software Product Line Conference (SPLC), pages 231 240. Carnegie Mellon University, 2009. [44] \nM. Odersky, L. Spoon, and B. Venners. Programming in Scala. Artima Press, Mountain View, CA, 2008. [45] \nJ. Overbey and R. Johnson. Generating rewritable abstract syn\u00adtax trees. In Proc. Int l Conf. Software \nLanguage Engineering (SLE), volume 5452 of Lecture Notes in Computer Science, pages 114 133. Springer-Verlag, \n2008. [46] Y. Padioleau. Parsing C/C++ code without pre-processing. In Proc. Int l Conf. Compiler Construction \n(CC), pages 109 125. Springer-Verlag, 2009. [47] Y. Padioleau, J. Lawall, R. R. Hansen, and G. Muller. \nDocu\u00admenting and automating collateral evolutions in Linux device drivers. In Proc. European Conference \non Computer Systems (EuroSys), pages 247 260. ACM Press, 2008. [48] N. Palix, G. Thomas, S. Saha, C. \nCalv\u00e8s, J. Lawall, and G. Muller. Faults in Linux: Ten years later. In Proc. Int l Conf. Architectural \nSupport for Programming Languages and Operating Systems (ASPLOS), pages 305 318. ACM Press, 2011. [49] \nT. T. Pearse and P. W. Oman. Experiences developing and maintaining software in a multi-platform environment. \nIn Proc. Int l Conf. Software Maintenance (ICSM), pages 270 277. IEEE Computer Society, 1997. [50] M. \nPlatoff, M. Wagner, and J. Camaratta. An integrated program representation and toolkit for the maintenance \nof C programs. In Proc. Int l Conf. Software Maintenance (ICSM), pages 192 137. IEEE Computer Society, \n1991. [51] K. Pohl, G. B\u00f6ckle, and F. J. van der Linden. Software Product Line Engineering: Foundations, \nPrinciples and Techniques. Springer-Verlag, Berlin/Heidelberg, 2005. [52] H. Post and C. Sinz. Con.guration \nlifting: Veri.cation meets software con.guration. In Proc. Int l Conf. Automated Soft\u00adware Engineering \n(ASE), pages 347 350. IEEE Computer So\u00adciety, 2008. [53] pure-systems GmbH, Magdeburg. pure::variants \nUser s Guide, version 3.0 edition, 2009. [54] S. She, R. Lotufo, T. Berger, A. W .asowski, and K. Czarnecki. \nReverse engineering feature models. In Proc. Int l Conf. Software Engineering (ICSE). ACM Press, 2011. \nto appear. [55] S. She, R. Lotufo, T. Berger, A. W .asowski, and K. Czarnecki. The variability model \nof the Linux kernel. In Proc. Int l Work\u00ad shop on Variability Modelling of Software-intensive Systems \n(VaMoS), pages 45 51. University of Duisburg-Essen, 2010. [56] C. Simonyi. The death of computer languages, \nthe birth of intentional programming. In NATO Science Committee Conference, 1995. [57] J. Sincero, R. \nTartler, D. Lohmann, and W. Schr\u00f6der-Preikschat. Ef.cient extraction and analysis of preprocessor-based \nvari\u00adability. In Proc. Int l Conf. Generative Programming and Component Engineering (GPCE), pages 33 \n42. ACM Press, 2010. [58] S. S. Som\u00e9 and T. C. Lethbridge. Parsing minimization when extracting information \nfrom code in the presence of conditional compilation. In Proc. Int l Workshop on Program Comprehension \n(IWPC), pages 118 125. IEEE Computer Society, 1998. [59] H. Spencer and G. Collyer. #ifdef considered \nharmful or portability experience with C news. In Proc. USENIX Conf., pages 185 198. USENIX Association, \n1992. [60] D. Spinellis. Global analysis and transformations in prepro\u00adcessed languages. IEEE Trans. \nSoftw. Eng. (TSE), 29(11):1019 1030, 2003. [61] R. Tartler, D. Lohmann, J. Sincero, and W. Schr\u00f6der-Preikschat. \nFeature consistency in compile-time-con.gurable system soft\u00adware: Facing the Linux 10,000 feature problem. \nIn Proc. Eu\u00adropean Conference on Computer Systems (EuroSys), pages 47 60. ACM Press, 2011. [62] S. Thaker, \nD. Batory, D. Kitchin, and W. Cook. Safe com\u00adposition of product lines. In Proc. Int l Conf. Generative \nProgramming and Component Engineering (GPCE), pages 95 104. ACM Press, 2007. [63] M. Tomita. An ef.cient \ncontext-free parsing algorithm for natural languages. In Proc. Int l Joint Conf. on Arti.cial Intelligence \n(IJCAI), pages 756 764. Morgan Kaufmann, 1985. [64] M. Vittek. Refactoring browser with preprocessor. \nIn Proc. European Conf. on Software Maintenance and Reengineering (CSMR), pages 101 110. IEEE Computer \nSociety, 2003. [65] K. Vo and Y. Chen. Incl: A tool to analyze include .les. In Proc. USENIX Conference, \npages 199 208. USENIX Association, 1992. [66] M. Voelter. Embedded software development with projectional \nlanguage workbenches. In Proc. Int l Conf. Model Driven Engineering Languages and Systems (MoDELS), volume \n6395 of Lecture Notes in Computer Science, pages 32 46. Springer-Verlag, 2010. [67] P. Wadler. How to \nreplace failure by a list of successes. In Proc. Conf. Functional Programming Languages and Computer \nArchitecture (FPCA), pages 113 128. Springer-Verlag, 1985. [68] D. Weise and R. Crew. Programmable syntax \nmacros. In Proc. Conf. Programming Language Design and Implementation (PLDI), pages 156 165. ACM Press, \n1993.    \n\t\t\t", "proc_id": "2048066", "abstract": "<p>In many projects, lexical preprocessors are used to manage different variants of the project (using conditional compilation) and to define compile-time code transformations (using macros). Unfortunately, while being a simple way to implement variability, conditional compilation and lexical macros hinder automatic analysis, even though such analysis is urgently needed to combat variability-induced complexity. To analyze code with its variability, we need to parse it without preprocessing it. However, current parsing solutions use unsound heuristics, support only a subset of the language, or suffer from exponential explosion. As part of the TypeChef project, we contribute a novel variability-aware parser that can parse almost all unpreprocessed code without heuristics in practicable time. Beyond the obvious task of detecting syntax errors, our parser paves the road for further analysis, such as variability-aware type checking. We implement variability-aware parsers for Java and GNU C and demonstrate practicability by parsing the product line MobileMedia and the entire X86 architecture of the Linux kernel with 6065 variable features.</p>", "authors": [{"name": "Christian K&#228;stner", "author_profile_id": "81331495728", "affiliation": "Philipps University Marburg, Marburg, Germany", "person_id": "P2839271", "email_address": "christian.kaestner@uni-marburg.de", "orcid_id": ""}, {"name": "Paolo G. Giarrusso", "author_profile_id": "81481648379", "affiliation": "Philipps University Marburg, Marburg, Germany", "person_id": "P2839272", "email_address": "pgiarrusso@Mathematik.Uni-Marburg.de", "orcid_id": ""}, {"name": "Tillmann Rendel", "author_profile_id": "81381610410", "affiliation": "Philipps University Marburg, Marburg, Germany", "person_id": "P2839273", "email_address": "rendel@Mathematik.Uni-Marburg.de", "orcid_id": ""}, {"name": "Sebastian Erdweg", "author_profile_id": "81490684973", "affiliation": "Philipps University Marburg, Marburg, Germany", "person_id": "P2839274", "email_address": "seba@Mathematik.Uni-Marburg.de", "orcid_id": ""}, {"name": "Klaus Ostermann", "author_profile_id": "81100028971", "affiliation": "Philipps University Marburg, Marburg, Germany", "person_id": "P2839275", "email_address": "kos@Mathematik.Uni-Marburg.de", "orcid_id": ""}, {"name": "Thorsten Berger", "author_profile_id": "81470641400", "affiliation": "University of Leipzig, Leipzig, Germany", "person_id": "P2839276", "email_address": "tberger@gsd.uwaterloo.ca", "orcid_id": ""}], "doi_number": "10.1145/2048066.2048128", "year": "2011", "article_id": "2048128", "conference": "OOPSLA", "title": "Variability-aware parsing in the presence of lexical macros and conditional compilation", "url": "http://dl.acm.org/citation.cfm?id=2048128"}