{"article_publication_date": "10-22-2011", "fulltext": "\n Catch Me If You Can: Performance Bug Detection in the Wild Milan Jovic Andrea Adamoli Matthias Hauswirth \nUniversity of Lugano University of Lugano University of Lugano Milan.Jovic@usi.ch Abstract Pro.lers help \ndevelopers to .nd and .x performance prob\u00adlems. But do they .nd performance bugs problems that real users \nactually notice? In this paper we argue that interactive applications traditional pro.lers .nd irrelevant \nproblems but fail to .nd relevant bugs. We then introduce lag hunting, an approach that identi\u00ad.es perceptible \nperformance bugs by monitoring the behav\u00adior of applications deployed in the wild. The approach trans\u00adparently \nproduces a list of performance issues, and for each issue provides the developer with information that \nhelps in .nding the cause of the problem. We evaluate our approach with an experiment where we monitor \nan application used by 24 users for 1958 hours over the course of 3-months. We characterize the resulting \n881 is\u00adsues, and we .nd and .x the causes of a set of representative examples. Categories and Subject \nDescriptors C[Performance of Systems]: Measurement techniques; D [Software Engineer\u00ading]: Metrics Performance \nmeasures General Terms Performance, Measurement, Human Fac\u00adtors Keywords Pro.ling, Latency bug, Perceptible \nperformance 1. Introduction Many if not most software applications interact with human users. The performance \nof such applications is de.ned by the users perception. As research in human-computer inter\u00adaction has \nshown, the key determinant of perceptible perfor\u00admance is the system s lag in handling user events [5, \n6, 24, 25]. Thus, to understand the performance of such applica\u00adtions, developers have to focus on perceptible \nlag. Permission to make digital or hard copies of all or part of this work for personal or classroom \nuse is granted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. OOPSLA \n11, October 22 27, 2011, Portland, Oregon, USA. Copyright c &#38;#169; 2011 ACM 978-1-4503-0940-0/11/10. \n. . $10.00  Figure 1. Perceptible Bug Hidden in Hotness Pro.le However, when developers try to understand \nthe perfor\u00admance of their applications, they use pro.lers to identify and optimize hot code. Figure 1 \nvisualizes the output of a tradi\u00adtional code hotness pro.ler in the form of a calling context tree rendered \nas a sunburst diagram1. The center of the dia\u00adgram represents the root of the tree (the application s \nmain method). The tree grows inside out. Each calling context is represented by a ring segment. The angle \nof a ring segment is proportional to the hotness of the corresponding calling context. The tree in Figure \n1 consists of 224344 calling con\u00adtexts and represents the production use of a real-world Java application \n(Eclipse). Figure 2. Developer vs. User-Centric View of Performance Following a traditional performance \ntuning approach, a developer will focus on the hottest regions (the ring seg\u00adments with the widest angles) \nof the calling context tree. In this paper we argue that this is the wrong approach for inter\u00adactive \napplications. As Figure 2 shows, code hotness pro.les 1 We use sunburst diagrams, also known as calling \ncontext ring charts [1], because they scale to the large calling context trees of real-world programs. \n produced by traditional pro.lers do not properly re.ect the perceptible performance of applications. \nThe left part of the .gure represents a traditional hotness pro.le, a developer\u00adcentric view of performance, \nshowing the three hottest meth\u00adods (methods that spent most CPU cycles). The right part represents a \ntrace of the corresponding program execution. The trace shows a signi.cant number of idle-time intervals \n(where the computer was waiting for user input). We use the term episode to denote the intervals of computational \nac\u00adtivity in-between idle intervals. More importantly, the trace shows that some (hot) methods were executed \noften but each invocation completed quickly, while other (cold) methods were executed rarely but an invocation \nmight have taken a long time. This example highlights that perceptible perfor\u00admance problems are not \nnecessarily caused by the hottest methods in a traditional pro.le. The real-world hotness pro.le in Figure \n1 con.rms this point: the calling context tree contains a node corresponding to a method that repeatedly \n(30 times) exhibited an excep\u00adtionally long (at least 1.5 seconds) latency. However, that context (the \nEditEventManager.mouseUp method indicated with the arrow) is barely visible in the tree, because the \nin\u00advocations of this method make up less than 1% of the appli\u00adcation s execution time. A developer looking \nat this pro.le would never have considered tuning that method to improve performance. Instead, he probably \nwould have focused on hot methods that are frequently called but do not necessarily contribute to perceptible \nlag. 1.1 Measuring latency instead of hotness In this paper we introduce an approach that automatically \ncatches perceptible performance bugs such as the one hidden in Figure 1. We do this by focusing on the \nlatency instead of the hotness of methods. At .rst sight, measuring latency may seem simple: just capture \ntime-stamps for method calls and returns. Unfortu\u00adnately, for real-world applications, the instrumentation \nand data collection overhead for such an approach signi.cantly slows down the application. This is the \nreason why com\u00admercial pro.lers use call-stack sampling instead of tracking method calls and returns. \nMoreover, the large overhead per\u00adturbs the time measurements and casts doubt on the validity of the measurement \nresults. A central goal of our approach is thus the ability to mea\u00adsure latency and to gather actionable \ninformation about the reason for long latency method calls, while keeping the over\u00adhead minimal. 1.2 \nCatching bugs in the wild Performance problems are highly dependent on the context in which an application \nruns. Their manifestation depends on (1) the underlying platform, (2) the speci.c con.guration of the \napplication, and the (3) size and structure of the inputs the application processes. For example, applications \nmay run on computers with different clock frequencies, numbers of cores, amounts of memory, and different \namounts of concur\u00adrent activity. Users may con.gure applications by installing a variety of plug-ins, \nsome of them might not even be known to the application developer (e.g. they may install a spell\u00adchecking \nplug-in into an editor, and that plug-in may per\u00adform unanticipated costly operations when getting noti.ed \nabout certain changes in the document). Or users may use an application to process inputs that are unexpectedly \nsized (e.g. they may browse a folder containing tens of thousands of .les, they may use a web server \nto serve mostly multi\u00adgigabyte .les, or they may want to edit an MP3 .le that contains 10 hours of audio). \nWithout knowing the exact us\u00adage scenarios of widely deployed applications, developers are unable to \nconduct representative performance tests in the lab. Thus, the second goal of our approach is the ability \nto directly detect performance problems in the deployed appli\u00adcations.  1.3 Contributions In this paper \nwe propose Lag Hunting. Our approach tar\u00adgets widely-deployed applications by gathering runtime in\u00adformation \nin the .eld. It targets interactive applications by focusing on perceptible lag. Without any user involvement, \nit produces a list of performance issues by combining data gathered from the different deployments. Moreover, \nit auto\u00admatically produces an actionable [20] report for each issue Figure 3. Perceptible Bug in Lag \nHunting Pro.le Figure 3 shows the calling context tree that is part of the report our approach produces \nfor the performance bug that is barely visible in Figure 1. Unlike the calling context tree derived from \na traditional sampling pro.ler, our tree only represents the behavior that is related to the given bug, \nand it shows all the method calls that contributed to the long latency of the problematic mouseUp method. \nWe demonstrate the practicality of our approach by im\u00adplementing it in LagHunter, our performance bug \ndetector for Java GUI applications. In a three-month experiment we deployed LagHunter to .nd performance \nissues in one of the most complex interactive Java applications, the Eclipse Figure 4. Lag Hunting  \n Figure 5. Session Report IDE. In that experiment, LagHunter identi.ed 881 perfor\u00admance issues. We characterized \nthese issues and we per\u00adformed three case studies to .nd and .x the performance bugs for representative \nissues. The remainder of this paper is structured as follows: Section 2 introduces the Lag Hunting approach. \nSection 3 presents LagHunter, our tool for lag hunting in Java GUI applications. Section 4 characterizes \nthe performance is\u00adsues LagHunter identi.ed in Eclipse. Section 5 evaluates our approach with three case \nstudies. Section 6 discusses the limitations of our approach and our evaluation, Section 7 presents related \nwork, and Section 8 concludes. 2. Approach Figure 4 provides an overview of our approach. Applica\u00adtions \nare deployed with an agent that collects performance information about each session. At the end of a \nsession, the agent sends a session report to an analysis engine running on a central server. The server \ncollects, combines, and analyzes the session reports to determine a list of performance issues. To the \ndevelopers, the analysis engine appears like a tradi\u00adtional bug repository. The only difference is that \nthe bug reports are generated automatically, without any initiative from the users, that all equivalent \nreports have been com\u00adbined into a single issue, that related issues are automatically linked together, \nand that issues contain rich information that points towards the cause of the bug. 2.1 What data to collect? \nPerceptible performance problems manifest themselves as long latency application responses. A complete \ntrace of each call and return of all methods of an application would allow us to measure the latency \nof each individual method call, and to determine all the callees of each method and their contri\u00adbutions \nto the method s latency. However, such an approach would slow down the program by orders of magnitude \nand thus could not be used with deployed applications. The chal\u00adlenge of any practical latency bug detection \napproach is to reduce this overhead while still collecting the information necessary to .nd and .x bugs. \nMoreover, the information needs to be collected in a way to enable a tool to effectively aggregate a \nlarge number of session reports, each containing possibly many occurrences of long-latency behavior, \ninto a short list of relevant performance issues. Figure 5 shows part of the execution of an interactive \nap\u00adplication. The x-axis represents time. The three .ash sym\u00adbols represent three incoming user requests \n(events) that need to be handled by the application. The call stack, shown in light-grey, grows from \nbottom to top. Each rectangle on the stack corresponds to a method activation. In our ap\u00adproach, the \nagent collects two kinds of information. First, it captures calls and returns from so-called landmark \nmeth\u00adods (black rectangles). This landmark trace contains the tim\u00ading information necessary to measure \nlag. Second, it cap\u00adtures rare, randomly spaced call stack samples of the run\u00adning threads. Each stack \nsample contains the complete call\u00ading context of the thread at the time the sample was taken. This sequence \nof stack samples provides information about what the thread was doing throughout a long-latency land\u00admark \nmethod invocation. Figure 5 shows information for only one thread. We do collect the same information \nfor all threads. However, for many interactive applications, most threads are blocked most of the time. \nMoreover, for most current GUI toolkits, only one thread, the event dispatch thread, is allowed to interact \nwith the user (process user input and update the GUI). For many applications, if the user is not interacting \nwith the application, the GUI thread becomes idle, waiting for new user requests2. The .gure shows such \nan idle interval in the center. While the thread is idle, no landmark method is on the stack, and we \nthus do not need any call stack samples3.  2.2 Landmark methods represent performance issues The most \ncrucial parameter in our approach is the choice of landmark methods. Landmark methods serve a double \npur\u00adpose. First, their calls and returns represent the points where we measure latency. Second, each \nlandmark method rep\u00adresents a potential performance issue. The lag hunting ap\u00adproach produces a list \nof potential performance issues that is identical to the list of landmark methods that incurred a 2 For \nsome applications, such as video players or games, the GUI thread receives periodic requests from a timer, \nwhich cause it to update the GUI from time to time. 3 In a practical implementation of our approach, \nsuch as LagHunter, it can be bene.cial to always sample the call stack and to drop the samples taken \nwhenever the thread is idle, because the low sampling rates of our approach only cause negligible sampling \noverhead, but there is a cost of communicating the start and end of idle intervals to the call stack \nsampler.  non-negligible latency (e.g., at least one invocation of that method took more than 3 ms). \nEach method in that list is annotated with a rich set of information that includes its in\u00advocation latency \ndistribution. A landmark method becomes a real performance issue, if a developer deems its latency distribution \nto be severe (e.g., over 100 ms on average). A chosen set of landmarks should ful.ll three properties: \nFirst, they need to cover most of the execution of the pro\u00adgram. Given that we only measure the latency \nof landmarks, we will be unaware of any long activity happening outside a landmark. Second, they need \nto be called during the han\u00addling of an individual user event. We want to exploit the hu\u00adman perceptibility \nthreshold (of roughly 100 ms) for deter\u00admining whether a landmark method took too long, and thus such \na method needs to correspond to a major activity that is part of handling a user event. A method executed \nin a back\u00adground thread may take much longer than 100 ms, but it will not affect the responsiveness of \nthe application, if the GUI thread continues to handle user events. Moreover, a top-level method of the \nGUI thread (e.g. the main method of the ap\u00adplication represented by the bottom rectangle in Figure 5), \nmay have a very long latency , but it is not responsible for delaying the handling of individual user \nevents. Third, land\u00admarks should be called infrequently. Tracing landmarks that are called large numbers \nof times for each user event would signi.cantly increase the overhead.  2.3 Landmark selection strategies \nWe now describe how to select good landmarks. While our approach focuses on interactive applications, \njust by chang\u00ading the set of landmarks, many of these ideas would also apply to the analysis of transaction-oriented \nserver applica\u00adtions. Event dispatch method. One possible landmark method is the single GUI toolkit method \nthat dispatches user events. This method will cover the entire event handling latency. However, when \nusing this method as the only landmark, the analysis will result in a list with this method as a single \nissue. This means that many different causes of long latency will be combined together into a single \nreport, which makes .nding and .xing the causes more dif.cult. Even though the event dispatch method \nis not very useful when used in isolation, it is helpful in combination with other, more speci.c, landmarks. \nAny left-over issue not appearing in the more speci.c landmarks (methods transitively called by the dispatch \nmethod) will be attributed to the dispatch method. Event-type speci.c methods. A more speci.c set of \nlandmarks could be methods corresponding to the different kinds of low-level user actions (mouse move, \nmouse click, key press). However, these methods are still too general. A mouse click will be handled \ndifferently in many different situations. Having a single issue that combines information about mouse \nclicks in multiple contexts is often not speci.c enough. Commands. Ideally, the landmarks correspond \nto the different commands a user can perform in an application. If the application follows the command \ndesign pattern [9] and follows a standard implementation idiom, it is possible to automatically identify \nall such landmarks. Observers. Even an individual command may consist of a diverse set of separate activities. \nCommonly, a com\u00admand changes the state of the application s model. In appli\u00adcations following the observer \npattern [9], the model (syn\u00adchronously) noti.es any registered observers of its changes. Any of these \nobservers may perform potentially expensive activities as a result. If the application follows a standard \nidiom for implementing the observer pattern, it is possible to automatically identify all observer noti.cations \nas land\u00admarks. Component boundaries. In framework-based applica\u00adtions, any call between different plug-ins \ncould be treated as a landmark. This would allow the separation of issues be\u00adtween plug-ins. However, \nthe publicly callable API of plug\u00adins in frameworks like Eclipse is so .ne grained, that the overhead \nfor this kind of landmarks might be too large. Application-speci.c landmarks. If application develop\u00aders \nsuspect certain kinds of methods to have a long latency, they may explicitly denote them as landmarks \nto trigger the creation of speci.c issues. 2.4 Data collection Given a speci.cation of the set of landmark \nmethods, the agent deployed with the application dynamically instru\u00adments the application to track all \ntheir invocations and re\u00adturns. Moreover, the agent contains code that periodically samples the call \nstacks of all the threads of the application. It also collects information about the platform, the applica\u00adtion \nversion, and installed plug-ins. At the end of a session, the agent combines the information into a session \nreport and uploads it to the analysis server.  2.5 Analysis For each session report, the analysis engine \nextracts all land\u00admark invocations from the landmark trace. For each land\u00admark invocation, it computes \nthe inclusive latency (landmark end time -landmark start time) and the exclusive latency (in\u00adclusive \nlatency -time spent in nested landmarks), and it up\u00addates the statistics of the corresponding issue. \nThe repository contains, for each issue, information about the distribution of its latencies, the number \nof occurrences, and the sessions in which it occurred. Moreover, to help in identifying the cause of \nthe latency of a given landmark, the repository con\u00adtains a calling context tree related to that landmark. \nThis tree is built from the subset of stack samples that occurred dur\u00ading that landmark (excluding samples \nthat occurred during nested landmarks). The tree is weighted, that is, each calling context is annotated \nwith the number of samples in which it occurred.  2.6 Connecting issues Based on the information about \nindividual issues, the anal\u00adysis engine then establishes connections between related is\u00adsues. It does \nthat by computing the similarities between the issues calling context trees. Two different issues (e.g. \ntwo different commands) may trigger the same cause of long la\u00adtency. If that is the case, their calling \ncontext tree will be similar. If a developer investigates a performance issue, the connections to similar \nissues can provide two key bene.ts: it can simplify the search for the cause (by providing more information \nabout the contexts in which it occurs), and it can help to prioritize the issue (given that a .x of the \nissue may also .x the related issues). 2.7 Call pruning Given an issue, its calling context tree, latency \ndistribution, and the list of related issues allow a developer to form hy\u00adpotheses about the reason for \nthe long latency. To test those hypotheses, developers would usually change the program and rerun it \nto con.rm that the change indeed reduced the perceptible latency. Our approach partially automates this \nstep. We do this by re-executing the application while automatically omitting various parts of the expensive \ncomputation. In particular, we omit calls to hot methods in the calling context tree. Note that we do \nnot omit the calls to the landmark method, but calls to hot methods that are (transitively) called by \nthe landmark method. Note that pruning a method call does not usually correspond to a .x. It often leads \nto crashes or an incorrectly working application. However, it is effective in locating and con.rming \nthe cause of long latency behavior.  2.8 Discussion The key idea behind our approach is to gather a \nlarge num\u00adber of session reports and to convert them into a small set of issues familiar to the developer. \nIn this way, the devel\u00adoper gains insight about the behavior of a landmark in many different execution \ncontexts. Note that we do not require de\u00advelopers to explicitly specify landmark methods. All but the \nlast class of landmarks we outline can be detected automat\u00adically. Moreover, commands and observers represent \nan in\u00ad.nite number of landmarks: the agent automatically recog\u00adnizes any commands or observers as a landmark. \nThe fact that we only trace a set of relatively infrequently executed landmarks keeps the performance \nimpact of data collection low. Moreover, the fact that many users provide session re\u00adports enables a \nlow call stack sampling rate with minimal overhead. 3. Implementation LagHunter is a lag hunting tool \ntargeting Java GUI applica\u00adtions. The LagHunter agent consists of a Java agent and a JVMTI agent that \nare shipped together with the application. The application does not have to be changed, the agents are \nactivated using command line arguments to the Java virtual machine. The Java agent dynamically rewrites \nthe loaded classes using ASM [22] to instrument all landmark method calls and returns. It is also responsible \nfor removing method calls during call pruning. The JVMTI agent is written in na\u00adtive code and loaded \ninto the Java virtual machine. It is re\u00adsponsible for call stack sampling. Moreover, it also traces the \nvirtual machine s garbage collection activity. At the shut\u00addown of the virtual machine, the JVMTI agent \ngathers the landmark trace, the stack sample sequence, and the garbage collection trace, compresses them, \nand ships them to the analysis server. If the upload fails, it keeps them on disk and tries to send them \nthe next time the application shuts down. 3.1 Landmarks LagHunter supports several types of landmarks: \nthe event dispatch method, observers, paint methods, and native calls. The event dispatch method is easy \nto identify, it corresponds to a speci.c method in the SWT or Swing GUI toolkits (we support both). Using \nthe event dispatch method as a land\u00admark ensures that we capture all episodes incurring long la\u00adtency. \nWe instrument all invocations of observer noti.cation methods that follow Java s EventListener idiom. \nThis kind of landmark is easy to detect automatically and usually pro\u00advides a meaningful level of abstraction. \nThe paint method landmark correspond to any paint() method in a subclass of java.awt.Component. It tracks \ngraphics rendering activ\u00adity in Swing applications (SWT uses the observer pattern for painting). Especially \nin visually rich applications, graph\u00adics rendering can be expensive and paint method landmarks represent \nabstractions meaningful to a developer. Finally, we instrument all calls sites to native methods. They \nrepresent cross-language calls to native code via JNI, and thus may be responsible for long-latency input \nor output operations. While the kinds of landmarks supported by LagHunter are targeted speci.cally at \ninteractive applications, LagHunter could be extended with further landmark types, e.g., to sup\u00adport \nevent-based server applications.  3.2 Overhead and perturbation Dynamic binary instrumentation takes \ntime, and classes are loaded throughout the application run, so our Java agent s instrumentation activity \nmight perturb its own timing mea\u00adsurements. To make our implementation practical, our agent uses a variety \nof optimizations, among them a persistent in\u00adstrumentation cache that stores instrumented classes to \navoid re-instrumentation in subsequent application runs. Most method calls complete quickly. This is \ntrue even for landmark methods. If we built a complete landmark trace, with every call and return, the \nsession reports would grow unreasonably large. Moreover, the constant tracing and I/O activity would \nalso perturb our latency measurements. We thus .lter out short landmarks online. Our .lter maintains \na shadow stack of active landmark methods. A call to a landmark method pushes a new element on the stack. \nA return from a landmark method pops the top-most element. It drops it if the latency was below a desired \nthreshold, and it writes it to the trace otherwise. Using this trace .ltering approach, we can reduce \nthe trace size by several orders of magnitude without losing relevant information.  Evaluation. To evaluate \nthe overhead and perturbation caused by our agent we conducted controlled experiments on three different \nplatforms. Three different users had to perform prede.ned tasks in two different large interactive applications, \nEclipse and NetBeans, which we ran on top of our agent. We .rst used the data collected by the JVMTI \nagent to verify that the Java agent did not cause excessive overhead. The stack samples collected in \nour initial experiment showed that we spent a signi.cant amount of time in our trace .lter\u00ading code, \nand the garbage collection trace showed an abnor\u00admally large number of garbage collections. We studied \nour trace .ltering implementation and realized that we were trig\u00adgering object allocations every time \nwe processed an event. We eliminated these allocations and reran the experiments. To evaluate the startup \noverhead due to our agent, we measured the startup time of the original applications (aver\u00adage across \nthree platforms; Eclipse: 1.1 s, NetBeans: 2.85 s), the startup time of the application with our agent, \nbut with\u00adout the persistent instrumentation cache (Eclipse: 3.44 s, NetBeans: 7.28 s), and the startup \ntime of the application with our agent, using the persistent instrumentation cache (Eclipse: 1.58 s, \nNetBeans: 3.2 s). All startup times represent the duration from launching the Java virtual machine until \nthe dispatch of the .rst GUI event. The above results show that the persistent instrumentation cache \nis effective in re\u00adducing the startup time of the application with the LagHunter agent to values close \nto those of the original application. We also measured the cost of stack sampling. The me\u00addian time to \ntake a stack sample varies between 0.5 ms and 2.23 ms. This is a small fraction of the perceptible latency \nof 100 ms, and thus is negligible with a low-enough sampling rate. Finally, we measured the effect of \nthe .lter threshold on trace size: a threshold of 3 ms reduces trace size by a factor of over 100. This \nshows that trace .ltering is essential in re\u00adducing the size of the session reports shipped from the \nusers to the central repository. We have deployed our agent with three different applica\u00adtions (BlueJ4, \nInforma5, and Eclipse) which the students in our undergraduate programming class have been using ex\u00adtensively \nduring their normal course work, and the only feed\u00adback we received was that the applications did not \nshut down immediately. This is to be expected, because we process and upload the session report when \nthe application quits. 4 http://bluej.org 5 http://informaclicker.org  3.3 Analysis Our repository collects \nall session reports as well as the gen\u00aderated issue information. We automatically run the analysis every \nnight to update the issues with newly received infor\u00admation. We have developed a small web interface \nto access our issue repository. This allows us to rank issues according to various criteria, and to look \nat the reports for each speci.c issue. Those reports contain visualizations of latency distri\u00adbutions \nand interactive visualizations of the calling context tree. We render the calling context trees, which \ncan contain hundreds or thousands of nodes, with the calling context ring chart visualization of the \nTrevis [1] interactive tree visual\u00adization and analysis framework. When establishing connec\u00adtions between \nrelated issues, we use Trevis weighted mul\u00adtiset node distance metric to compute the similarity between \ncalling context trees. 4. Issue Characterization In this section we characterize the repository of the \n1108 ses\u00adsion reports we gathered during a three-month experiment. Our experiment involved 24 users working \nfor a total of 1958 hours with Eclipse, one of the largest interactive Java appli\u00adcations. We con.gured \nLagHunter to take a call stack sample every 500 ms on average, and to drop all samples taken dur\u00ading \nidle time intervals. 4.1 Parameters LagHunter s analysis engine detected 881 issues in the given reports. \nTable 1 characterizes .ve key parameters of this set of issues. For each parameter it shows the mean, \n.rst quar\u00adtile, median, second quartile, 90-th percentile, and maximum value. The .rst parameter, the \naverage exclusive latency, rep\u00adresents the severity of a given issue: the longer its latency, the more \naggravating the issue. The next three parameters describe how prevalent a given issue is. From the perspec\u00adtive \nof a developer, the need to .x an issue increases when it is encountered by many users, occurs in many \nsessions, and occurs many times. The last parameter, the number of collected call stacks, is indicative \nof the chance of .xing the issue. The more call stack samples available about a given issue, the more \ninformation a developer has about the poten\u00adtial cause of the long latency. Avg Q1 Med Q3 p90 Max Avg.Excl.[ms] \n320 4 11 31 119 38251 Users Sessions Occurrence 5 49 896 1 1 2 2 3 6 8 14 48 19 24 115 1069 449 338622 \nStacks 65 0 0 3 42 28613 Table 1. Univariate characterization The average exclusive time of an issue \nvaries between less than a millisecond and over 38 seconds. However, 90% of these issues take less than \n119 ms. This shows that most\n\t\t\t", "proc_id": "2048066", "abstract": "<p>Profilers help developers to find and fix performance problems. But do they find performance bugs -- performance problems that real users actually notice? In this paper we argue that -- especially in the case of interactive applications -- traditional profilers find irrelevant problems but fail to find relevant bugs.</p> <p>We then introduce lag hunting, an approach that identifies perceptible performance bugs by monitoring the behavior of applications deployed in the wild. The approach transparently produces a list of performance issues, and for each issue provides the developer with information that helps in finding the cause of the problem.</p> <p>We evaluate our approach with an experiment where we monitor an application used by 24 users for 1958 hours over the course of 3-months. We characterize the resulting 881 issues, and we find and fix the causes of a set of representative examples.</p>", "authors": [{"name": "Milan Jovic", "author_profile_id": "81372592893", "affiliation": "University of Lugano, Lugano, Switzerland", "person_id": "P2839149", "email_address": "milan.jovic@usi.ch", "orcid_id": ""}, {"name": "Andrea Adamoli", "author_profile_id": "81442617366", "affiliation": "University of Lugano, Lugano, Switzerland", "person_id": "P2839150", "email_address": "andrea.adamoli@usi.ch", "orcid_id": ""}, {"name": "Matthias Hauswirth", "author_profile_id": "81332503330", "affiliation": "University of Lugano, Lugano, Switzerland", "person_id": "P2839151", "email_address": "matthias.hauswirth@usi.ch", "orcid_id": ""}], "doi_number": "10.1145/2048066.2048081", "year": "2011", "article_id": "2048081", "conference": "OOPSLA", "title": "Catch me if you can: performance bug detection in the wild", "url": "http://dl.acm.org/citation.cfm?id=2048081"}