{"article_publication_date": "10-22-2011", "fulltext": "\n Flow-Sensitive Type Recovery in Linear-Log Time * Michael D. Adams Andrew W. Keep Jan Midtgaard Indiana \nUniversity Indiana University Aarhus University Matthew Might Arun Chauhan R. Kent Dybvig University \nof Utah Indiana University Indiana University Abstract The .exibility of dynamically typed languages \nsuch as JavaScript, Python, Ruby, and Scheme comes at the cost of run-time type checks. Some of these \nchecks can be eliminated via control-.ow analysis. However, traditional control-.ow analysis (CFA) is \nnot ideal for this task as it ignores .ow-sensitive information that can be gained from dynamic type \npredicates, such as JavaScript s instanceof and Scheme s pair?, and from type-restricted operators, such \nas Scheme s car. Yet, adding .ow-sensitivity to a tra\u00additional CFA worsens the already signi.cant compile-time \ncost of traditional CFA. This makes it unsuitable for use in just-in-time compilers. In response, we \nhave developed a fast, .ow-sensitive type-recovery algorithm based on the linear-time, .ow\u00adinsensitive \nsub-0CFA. The algorithm has been implemented as an experimental optimization for the commercial Chez \nScheme compiler, where it has proven to be effective, justi\u00adfying the elimination of about 60% of run-time \ntype checks in a large set of benchmarks. The algorithm processes on av\u00aderage over 100,000 lines of code \nper second and scales well asymptotically, running in only O(nlog n) time.We achieve this compile-time \nperformance and scalability through a novel combination of data structures and algorithms. Categories \nand Subject Descriptors D.3.4[Programming Languages]: Processors Compilers, Optimization General Terms \nLanguages Keywords Control-Flow Analysis, Flow Sensitivity, Path Sensitivity,Type Recovery * This research \nwasfacilitatedin partbya NationalPhysical Science Con\u00adsortium Fellowship and by stipend support from \nthe National Security Agency. Permission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies are not made or distributed for \npro.t or commercial advantage and that copies bear this notice and the full citation on the .rst page.To \ncopy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci.c \npermission and/or a fee. OOPSLA 11, October 22 27, 2011, Portland, Oregon, USA. Copyright c &#38;#169;2011ACM \n978-1-4503-0940-0/11/10... $10.00 1. Introduction Dynamically typed languages such as JavaScript, Python, \nRuby, and Scheme are .exible, but this .exibility comes at the cost of type checks at run time. This \ncost can be reduced via type-recovery analysis [Shivers 1991], which attempts to discover variable and \nexpression types at compile time and thereby justify the elimination of run-time type checks. Since these \nare higher-order languages in which the call graph is not static, the type-recovery analysis generally \nmust bea formof control-.ow analysis[Shivers 1988].Acontrol\u00ad.ow analysis (CFA) tracks the .ow of function \nvalues to call sites and builds the call graph even as it tracks the .ow of other values to their use \nsites. To maximize the number of checks removed, the analysis must take evaluation order into account. \nThat is, it must be .ow sensitive [Banning 1979]. In the following expression, even a .ow-insensitive \ncontrol-.ow analysis can determine that x is a pair and thus car need not check that x is a pair. (let \n([x (cons e1 e2)]) (car x)) To make a similar determination in the following expres\u00adsions, however, evaluation \norder must be taken into account as the read function can return anytype of value. (let ([x (read)]) \n(begin (cdr x) (car x))) (let ([x (read)]) (if (pair? x) (car x) #f)) Because it does not take evaluation \norder into account, a .ow-insensitive analysis treats all references the same. On the other hand, a .ow-sensitive \nanalysis1 can determine that (car x) is reached in the .rst expression only after passing the pair check \nin (cdr x) and in the second expression only when the explicit pair? check succeeds. Thus the implicit \npair check in car can be eliminated in both expressions. In this paper, we present a .ow-sensitive, CFA-based \ntype-recovery algorithm that runs in linear-log time. Be\u00ad 1 Our use of the term .ow sensitive agrees \nwith the original de.nition of Banning [1979],in which an analysistakes orderofevaluation into account, \nas well as with the glossary de.nition of Mogensen [2000], in which separate results are computed for \ndistinct program points. Our analysis may also be considered path sensitive, depending on the de.nition \nused.  cause the analysis is intended to justify type recovery in a fast production compiler [Dybvig \n2010], it is based on sub-0CFA [Ashley and Dybvig 1998], a linear-time, .ow\u00adinsensitive variant of 0CFA \n[Shivers 1988]. We use a novel combination of data structures and algorithms to add .ow sensitivity to \nsub-0CFA at the cost of only an additional log\u00adarithmicfactor. The analysis has been implemented as an \nexperimental optimization for the commercial Chez Scheme compiler, where it has proven to be effective \nand justi.es eliminating about 60% of run-time type checks. The algorithm has also proventobefast, processingover \n100,000 linesof codeper second on average. Furthermore, since it runs in O(nlog n) time, it scales well \nto large input programs. The remainder of this paper reviews the semantics and implementation of 0CFA \nand sub-0CFA (section 2), de\u00adscribes the traditional technique for implementing .ow sen\u00adsitivity (section \n3), describes our more ef.cient technique for implementing .ow sensitivity (section 4), discusses practical \nconsiderations in a real-world implementation and presents benchmark results (section 5), reviews related \nwork (sec\u00adtion 6), and .nally concludes (section 7). 2. Background This section reviews two relevant \nforms of control-.ow anal\u00adysis, Shivers s 0CFA and Ashley s sub-0CFA. It also dis\u00adcusses their implementations \nin terms of .ow graphs, how top and escaped values are handled, and the representation of non-function \ntypes. Readersfamiliar with control-.ow anal\u00adysis may wish to skip forward to section 3. 2.1 0CFA Constraint \nrules for 0CFA on the call-by-value .-calculus are presented in .gure 1. The explicit representation \nof con\u00adtexts in our formulation differs from more traditional presen\u00adtations [Nielson et al. 1999]. It \nis used in the rest of this pa\u00adper as we extend and optimize the analysis. The operational semantics \nof this language is standard and is omitted. The analysis stores a reachability .ag, [e], for each in \nsubexpression of the program being analyzed. The .ag is .if the expression is reachable and .otherwise. \nIn addition, for each expression a .ow variable [e] out ranging over V Val records the abstract value \nthat .ows from the expression, i.e., a subset of the lambda terms that may be returned by the expression. \nFor example, the LAMBDA rule says that if .x.e is reachable, the result of that expression includes an \nabstract value representing the lambda. For V Val , the .relation is the usual partial order on power \nsets.For Bool , it is the usual ordering where .... We implicitly label all subexpressions and uniquely \nalpha-rename all variables before the analysis starts. Thus we distinguish duplicate expressions and \nvariable names. The CALLmid and CALLfun rules use K(e), which returns the source contextof e. These contexts \nare single-layer con- Expressions: e . Exp = x |.x.e |ee Contexts: C . Ctxt = 0 |(0 e1 ) |(e0 0) |(.x.0) \nSignatures: K . Exp . Ctxt [e in . Bool {-Whether e is reachable -} [e out Val {-What e evaluates to \n-} . dr . Bool = {.,.} Val = Lam) v . dP(J J Lam = {.x1 .e1 ,.x2 .e2 ,.x3 .e3 ,...} [.x.e .. [e0 e1 . \nr in in LAMBDA CALLin [.x.e .{.x.e} [e0 . r out in [e0 out .{v 0 } K(e0 )=(0 e1 ) CALLmid [e1 in .. [e0 \nout .{.x.e.} [e1 out .{v 1 } K(e1 )=(e0 0) CALLfun [e. .. [x .{v 1 } in out [e0 out .{.x.e.} [e1 .{v \n1 } [e. .{v } out out CALLout [e0 e1 out .{v } Figure 1: 0CFA with reachability texts instead of the \nmore usual multilayer contexts, but for specifying the constraint rules, only a single layer is needed. \nWhen multilayer contexts are needed, we represent them by the juxtaposition of contexts. For example, \nthe CALLfun rule says that if a lambda .ows to a subexpression that is contextually located inside an \napplication, i.e., K(e1)=(e0 0), and a value .ows to the operand of the expression, e1, then the body \nof the invoked lambda is reachable and the actual argument .ows to the formal parameter. To solve these \nconstraint rules for a particular program, the analysis initially assigns . to each [e]and the empty \nin set to each [e]. Then it iteratively uses the constraint rules out to update [e]and [e]until they \nconverge to a solution. in out In the process [e]and [e]monotonically climb the in out lattices for Bool \nand VVal respectively [Nielson et al. 1999].  2.2 Flow-graph implementation of CFA In order to solve \nthe constraint rules for CFAef.ciently, it is common to represent the problem as a .ow graph [Heintze \nand McAllester 1997a; Jagannathan and Weeks 1995] with graph nodes denoting the .ow variables [e]and \n[e]and in out directed edges denoting the .ow of abstract values from one node to another. For 0CFA augmented \nwith reachability, an edge into an expression node models reachability r , whereas an edge out of an \nexpression node models possible result values, v , as depicted in .gure 2a.   r e1 T ? F ?  e e2 \n e3       . v (a)A general .ow graph node     (b)Acomposite .ow graph Figure 2: Flow graphs \nin 0CFA For example, in an analysis for a language with condi\u00adtionals, the .ow graph for the expression \n(if e1 e2 e3) contains nodes for reachability and result values of the if, e1, e2, and e3 expressions. \nThis is depicted schematically in .gure 2b. The expressions e1, e2, and e3 are drawn in out\u00adline to indicate \nthat theymay contain other nodes internal to those expressions. The expression e1 is reachable if and \nonly if the if is reachable. Thus there is an edge from the input of the if to e1. Likewise e2 and e3 \nare reachable if and only if e1 outputsa true orfalsevalue respectively. Thus there are edges from e1 \nto e2 and e3 .ltered by T? and F? which test if the outputvalue contains true orfalsevalues respectively. \nFinally, the output node of the if computes the lattice join (.)of the output nodes ofe2 and e3. Once \nthe graph is constructed, it is iterated until conver\u00adgence usinga standardwork-list algorithm.Anode \nlisted on the work list has a potentially stale output that needs to be updated based on new inputs to \nthat node. Initially all nodes are on the work list. One by one, nodes are removed from the work list \nand a new value for their outputs calculated based on the values of their input edges. If the value of \nthe output changes, then any node connected by an outgoing edge of the current node is placed back on \nthe work list thus effec\u00adtively marking it as potentially stale. The algorithm contin\u00adues selecting nodes \nfrom the work list and recalculating stale nodes until the graph converges and no stale nodes remain. \nA crucial property is that, under appropriate conditions, .ow graphs quickly converge to a solution. \nIf (1) the values that .ow through a graph are members of a .nite-height lattice, L, (2) the output value \nof each node moves only monotonically up the lattice, and (3) the output value of a node can be computed \nin constant time from the input values, then the lattice will converge in O(|L|(|E|+ |N|)) time where \n|L|, |E|and |N|are the height of L, the number of edges, and the number of nodes in the graph, respectively. \nFor CFA, a minor modi.cation has to be made to the usual .ow-graph algorithm. The initial graph contains \nno edges between functions and call sites since not all calls are known, so the algorithm adds new edges \nas it discovers connections between functions and call sites. This does not affect convergence, however, \nsince the maximum number of edges is bounded, and edges are addedbut never removed. In the worst case, \nthe algorithm adds an edge between each of O(n) call sites and each of O(n) functions in a program of \nsize n, resulting in a graph with O(n2) edges. 0CFA uses a lattice over P( V Lam), which has a height \nequal to the number of functions in the program. We thus have a lattice of height O(n) and a graph of \nsize O(n2), so a naively implemented 0CFA takes O(n3) time to compute. Slightly faster techniques are \nknown for computing 0CFA but they still take O(n3/log n) time [Chaudhuri 2008; Melski and Reps 2000; \nMidtgaard andVan Horn 2009].  2.3 Top and escaped functions in CFA If a CFA is operating on a program \nthat contains free vari\u00adables such as variables imported from libraries outside the scope of the analysis, \nthe analysis does not know anything about thevaluesof thosevariables. Thisis handledby adding a top, \n., element to the lattice. This . denotes an unknown function [Shivers 1988] and is used for the value \nof free vari\u00adables. It represents not only any function from outside the scope of the analysis but also \nincludes any function inside the scope of the analysis. This is to say, it subsumes all other V functions \nin Lam. Likewise, if a function is assigned to a free variable or exported to a library outside the scope \nof the analysis, then it may be called in locations unknown to the analysis. That the analysis has lost \ntrack of all the places where the function .ows is represented by marking the function as escaped. Top \nvalues and escaped functions can cause more top values and escaped functions. First, if the function \nposition of a function call is ., the return value of the function call is ., and the arguments escape, \nsince they are passed to an unknown function. Second, if a function escapes, then its formal parameters \nbecome ., and its return value escapes, since it might .ow to places outside the scope of the analysis. \nFinally, when a set of functions are joined with ., the result is ., and since . does not explicitly \nmention the functions combined into it, those functions are marked as escaped. This handling of top and \nescaped functions is standard and is assumed throughout the rest of this paper even when not explicitly \nmentioned.  2.4 Sub-0CFA 0CFA takes O(n3) time even without .ow sensitivity but we are aiming for .ow \nsensitivity in O(nlog n) time. Thus rather than basing our type-recovery analysis on the more common \n0CFA, we base it on sub-0CFA which takes only O(n) time. Sub-0CFAbounds both the size of the graph and \nthe height of the lattice by approximating all non-singleton sets of functions with .. This conservative \napproximation of 0CFA s power-set lattice has a constant height and is shown in .gure 3. Whenever two \nor more different functions are joined by ., the result is .. As a result, the values that .ow to the \nfunction position of a particular call site either contain at most one function or are approximatedby \n.and thus add at most a linear number of edges to the graph.  This approach lets more functions escape \nthan in 0CFA, but it is not as bad as it might seem. Flowing to two different places does not cause a \nfunction to escape. Functions escape only when two or more .ow to the same point, i.e., when a call site \nperforms some sortof dispatch.Forexample, when running the analysis over the following both f and g escape, \nsince theyboth .ow into fg,but h is not affected. (let ([f (lambda (x) e1)] [g (lambda (y) e2)] [h (lambda \n(z) e3)]) (let ([fg (if (read) f g)]) (f(g (fg(h (h e4))))))) In the general case, Ashleyand Dybvig \n[1998] de.ne sub\u00ad0CFA by a projection or widening operator. When this op\u00aderator restricts sets of values \nto either singletons or ., it is equivalent to what we do here. Other projection operators produce lattices \nthat can be of constant or even logarithmic height and result in linear or nearly linear analyses. For \nex\u00adample, instead of sets of at most one function, the projection may limit sets to at most k functions \nfor some constant k.  2.5 Non-function types Programming languages usually have more values than just \nfunctions. Thus we add a .xed set of primitive types, e.g., INT , PAIR, etc., to the V Val lattice. Because \nof the lattice .attening in sub-0CFA, however, we split abstract values into a function part and a non-function \npart. The function part operates over the same .attened lattice as sub-0CFA, but since we have a .xed \nnumber of non-function types we allow the non-function part to operate over the full power\u00adset of non-function \ntypes. Nevertheless, we notationally treat abstractvalues as sets.Forexample, {INT ,PAIR,.x.e}is understood \nto mean ({INT ,PAIR}, {.x.e}). 3. Traditional .ow-sensitivity The control-.ow analyses described in section \n2 are .ow insensitive.This meansall referencestoavariableare treated as having the same value as the \nbinding site of the variable. Consider these examples from the introduction: (let ([x (cons e1 e2)]) \n(car x)) (let ([x (read)]) (if (pair? x) (car x) #f)) (let ([x (read)]) (begin (cdr x) (car x))) With \na .ow-insensitive analysis, all references to x in the .rst expression are known to be pairs while, in \nthe second and third expressions, they are treated as .. .   ... .x1.e1.x2.e2.x3.e3 .xn.en   . \nFigure 3: Sub-0CFAlattice of functions Type information can be gained, however, from the ex\u00adplicit and \nimplicit dynamic type checks in the second and third expressions. In the second expression, we can deduce \nfrom the explicit pair check, (pair? x), that x must be a pair when car is called. In the third expression, \nwe can also deduce that x must be a pair when car is called, since the implicit pair check in cdr guarantees \nthat it returns only if its argument is a pair. We call information constructive when it is learned from \noperations that are constructing values as with the .rst expression. We call information observational \nwhen it is learned from operations that are observing values as with the second and third expressions. \nObservational information is restrictive, since it restricts the type of a variable or value, as in the \nrestriction of x to the pair type in those expressions. If observational information restricts a type \nto two or more disjoint types, the type is .. In general, wherever a . type occurs, the following code \nis unreachable, i.e., dead, and can be discarded. To collect observationalinformation, we must use a \n.ow\u00adsensitive analysis as the type information about a variable is different at different points in the \nprogram, e.g., before and after an observation. We present such an analysis in two stages. First, we \npresent an analysis thatis .ow sensitive andgathers observa\u00adtional information only from functions like \ncar that uncon\u00additionally restrict the type of their argument. Our approach to this form of .ow sensitivityis \nstandard. Then we generalize this and present an analysis that also gathers observational information \nfrom functions that restrict the type of their ar\u00adgument conditionally. For example, the argument of \npair? islimited to pairs if and only if pair? returns true. Our ap\u00adproach to this form of .ow sensitivity \nis novel. 3.1 Flow-sensitivityfor unconditional observers To recover observational information from \nfunctions like car, an analysis must be .ow sensitive. A .ow-insensitive analysis takes information about \na variable s abstract value directly from its binding site to each reference. The infor\u00admation about \na variable is the same at all references to the variable. To be .ow-sensitive we adjust the abstract \nvalues of variables as we trace the .ow of the program. Consider the earlier example that used (cdr x) \nand (car x). With .ow-sensitivity, the variable x starts at its binding site with the abstract value \n.. It then .ows to (cdr x). On entry to (cdr x), x still has the abstract value .. Since cdr throws an \nerror and does not return unless its argument is a pair, the analysis learns that x is a pair on exit \nfrom (cdr x). This then .ows to (car x). Thus (car x) is only ever called with a pair as argument. The \ncdr prevents non-pair values from .owing to the car, so we can safely omit the implicit pair check in \nthe car.  To gather restrictive information, each function is anno\u00adtated with the abstract value that \neach argument must be for the function to return. For example, if car returns, then its argument mustbeapair.Thisisnot \nlimitedtobuilt-inprim\u00aditives. For user-de.ned functions we examine the abstract value of each formal \nparameter after .owing through the function body. In the following example, if g returns, then we know \nits argument, y, is a pair. Thus x must be a pair af\u00adter returning from (g x) and consequently the implicit \npair check in cdr is redundant and can be safely omitted. (let ([x (read)] [g (lambda (y) (+ (car y) \n1))]) (g x) (cdr x)) The formal semantics for an analysis with .ow sensitivity for unconditional observers \nis a straightforward extension of the analysis in .gure1for .ow-insensitive 0CFA. It includes sequencing \nby threading the environment through the execu\u00adtion .ow of the program. Each expression still has an \nasso\u00adciated reachability .ag and result value, but now each ex\u00adpression also has two environments associated \nwith it. One tracks the types of variables entering the expression. The other tracks the type of variables \nexiting the expression. At each function call, arguments are restricted to only those ab\u00adstract values \nthat are compatible with the particular func\u00adtion returning. After (car x) for example, x is restricted \nto pairs. Both the entering and exiting environments are treated as reduced abstract domains [Cousot \nand Cousot 1979] and equate abstract elements with the same meaning (concretiza\u00adtion). Hence, if anycomponent \nof an environment is ., then all components of the environment are forced to be .. For example, if x \nis known to be an integer, then x is . after (car x). This causes all components of the exiting envi\u00adronment \nto be .. In addition, as part of the reduced abstract domain, the return value of (car x) is .. This \nmodels the fact that (car x) does not return if x is an integer. This simple form of .ow sensitivity \nhandles functions like car that unconditionally provide observational informa\u00adtion when they return. \nHowever, itfails to handle predicates like pair?.Thefact thata callto pair? returns says nothing about \nthe argument. The information is conditional, and it is whether it returns a true or false value that \ntells us whether the argument is a pair.  3.2 Flow-sensitivityfor conditional observers To handle conditional \nor predicated observers, we generalize the environments .owing through the program. For the exit of an \nexpression, we store one environment for when the expression returns a true value and another for when \nthe expression returns a false value. The environment for entry to the expression remains the same as \nbefore. Figure 4 presents this formally. It includes two environ\u00adments in [e]. One contains abstract \nvalues for when e out returns true values and the other for when e returns false values. For example, \n[(pair? x)]has x as a pair in the out true environment and asa non-pairin thefalse environment. These \ntrue andfalse environments are usedby the IFmid rule. The true environment of the test .ows to the entering \nenvi\u00adronment of the true branch. Thefalse environment of the test .ows to the entering environment of \nthefalse branch. In the abstract semantics of .gure 4, gathering restric\u00adtive information from a function \ncall, e.g., (car x) or (pair? x), is implemented by the CALLout rule. First, the values and environments \nthat .ow out of e0 and e1 are col\u00adlected. Next, RET examines any functions, f , .owing out of e0 and \nreturns three abstract values. One is the return value of f . The other two are the values that the argument \nto f must be for f to return either true orfalse respectively. Finally, ARG determines for both the true \nandfalse cases if the function could return to this call site given the abstract valueof the call site \ns argument.Forexample, (car 3) does not return. If the argument is a variable, ARG restricts the variable \nin the environment to the appropriate abstract value. Thus after (pair? x), x is restricted to pairs \nand non-pairs in the true andfalse cases respectively. Each primitive or function has one abstract value \nfor its argument for when it returns true and another for when it returns false. For example, with pair?, \nthe RET function returns the abstract value for pairs in the true case and the abstractvalue for non-pairsin \nthefalse case. Unconditional observers like car have the same abstract value in both the true andfalse \ncases.For user-de.ned functions, the same in\u00adformation is obtained from the exiting true and false envi\u00adronments \nof the body of the function.  3.3 Flow-graph representation of .ow-sensitivity As with standard .ow-insensitive \nCFAthese constraint rules canbe implementedbya .ow graph.Yet, while information can .ow directly from \nvariable bindings to variable refer\u00adences in a .ow-insensitive CFA, this is not suf.cient in a .ow-sensitive \nCFA. Instead, the abstract value for a variable must be threaded through each expression. In addition \nto the usual reachability .ags and result abstract values, we asso\u00adciate an environment with each entry \nedge of an expression anda true andafalse environment with eachexit edgeof an expression. This is depicted \nin .gure 5a. Lines with single\u00adheaded arrows represent the .ow of single values and lines with double-headed \narrows represent the .ow of environ\u00adments. Figure 5b shows how to extend the composite .ow graph for \nif from .gure 2b to account for the extra edges and illustrates the .ow of the true andfalse environments. \nThis graph formulation still has only linearly manyedges, but some of these edges now contain environments. \nThe lat\u00ad  Expressions: e . Exp = x |.x.e |ee |if eee |e; e |c Contexts: C . Ctxt = 0 |(0 e1 ) |(e0 0) \n|(.x.0) |(0; e1 ) |(e0 ; 0) |(if 0 e1 e2 ) |(if e0 0 e2 ) |(if e0 e1 0) Signatures: [e . Env [e out . \ndEnv \u00d7 dK . Exp . in Bool \u00d7 dVal \u00d7 dEnv Ctxt dJ r . Bool = {.,.} Env = Var . d Val = Fun \u00d7 P( df . d= \nLam + . . dVal v . dTag) Fun P(JPrim) Tag {FALSE ,TRUE ,INT ,FLOAT ,PAIR,.t = d.f = t . d= ...} Val \\{FALSE \n}{FALSE } J= {.x1 .e1 ,.x2 .e2 ,.x3 .e3 ,...} o . J= {pair?,car,cdr,...} Lam Prim ABS(#f)= FALSE ABS(#t)= \nTRUE ABS(n)= INT ... G RET(f ) = RET(f) RET(.x.e)= ( .t(x), where v, .f)= [e out v, .f(x))( .t, f.f \n RET(car)= (., {PAIR}, {PAIR}) RET(pair?)= ({FALSE,TRUE}, {PAIR}, .\\{PAIR}) ... 8 >. if v = . < ARG( \n., e, v ) = . if v = ..e/. Var > : . [e . . (e) .v ] if e . Var [c . (.,. ) [.x.e . (.,. ) in in CONST \nLAMBDA [c out ., [.x.e .(.x.e, .) .(ABS(c), .) out ., [x . (.,. ) [e0 e1 .( .) r, in in VAR CALLin \n[x out .(x), .(x) ..t], .(x) ..f]) in r, .( .[x . .[x . [e0 .( .) [e0 out .(v 0 ,. t,. f) K(e0 )=(0 e1 \n) CALLmid [e1 in . (.,. t .. f) .e0 .e0 .e1 .e1 [e0 out . ({.x.e.}, t , f ) [.x.e. in . (.,. .) [e1 \nout .(v 1 , t , ) K(e1 )= 0) f (e0 CALLfun . (.,. .[x . v 1 ]) [e. in .e0 .e0 .e1 .e1 . ' .e1 .e1 [e0 \n.( t , ) .( t , ) v, vf) = RET(f ) i = t . ,e1 , vi) f, [e1 v1 , ( vt, .i .{t,f}. ARG( v1 . out fout \nf f CALLout . ' . ' [e0 e1 out .( t, f) v, [e0 ; e1 .( .) .( .t, K(e0 )=(0; e1 ) r, [e0 v0 , .f) in \nout SEQin SEQmid SEQout [e0 .( .) .(., .f). [e1 r, [e1 .t . [e0 ; e1 in in out out e1 r, .( .t, K(e0 \n) [if e0 e2 in .( .) [e0 out v0 , .f) =(if 0 e1 e2 ) IFin IFmid [e0 .( .) .(., [e2 .f) r, [e1 .t) .(., \nin inin IFout [if e0 e2 . [e1 out .[e2 e1 out out Figure 4: Analysis constraint rules r . in T ? 'e1. \nl f.Cf F ?.    e2 e l 'e3 ... v . t . f S  (a)A .ow-sensitive node(b)Acomposite .ow graph Figure \n5: Flow graphs for .ow-sensitive 0CFA tice of an environment is theCartesian product of the lattice for \neach variable, so the lattice height of an environment is linear in the number of variables. Thus, the \n.ow graph has a linear number of edges with linear height lattices and conse\u00adquently takes quadratic \ntime to converge in the worst case. 4. Ef.cient .ow-sensitivity The .ow-graph based algorithm for .ow-sensitive \nCFA de\u00adscribed in section 3 is quadratic because environments are threaded through each expression. Contrast \nthis with .ow\u00adinsensitive sub-0CFA where the abstract value of a variable .ows directly from its binding \nlocation to each reference without threading through intervening expressions. To im\u00adplement .ow sensitivity \nef.ciently, we adopt a similar ap\u00adproach. Instead of .owing abstract values from a variable binding site \nto each reference, however, we .ow values from one occurrence of the variable to the next, following \nthe con\u00adtrol .ow of the program. The abstract value of the variable is adjusted as appropriate at each \noccurrence. When moving from one occurrence to the next, the true andfalsevalues for eachvariable mayjoin \nor swap with each other. For example, consider how abstract values for x .ow from (pair? x) to (car x) \nin the following expression. It is of the sort that may arise via the expansion of boolean connectives \nsuch as and, or, and not. Assume x is not referenced in e1 or e2. (if (if (pair? x) e1 e2) (car x) e3) \nWhat the .ow-sensitive analysis learns about x at (car x) depends on the return values of e1 and e2. \nIf e1 evaluates to only true values and e2 evaluates to only false values, then x at (car x) is always \na pair. If, however, e1 evaluates to only false values and e2 evaluates to only true values, then x at \n(car x) is never a pair. Other cases arise if either expressiondiverges or returns both true andfalsevalues. \nLikewise, consider the following expression where re\u00adstrictive information is learned from (car x) in \none of the branches of the inner if. As before, assume x is not refer\u00adenced in expressions e0, e1, or \ne2. (let ([x (read)]) (if (if e0 (begin (car x) e1) e2) (cdr x) e3)) After (car x), it is known that \nx is a pair, but the abstract value of x at (cdr x) depends upon the return values of e1 and e2. If e2 \nreturns a true value then (cdr x) is reachable without passing through (car x). If e2 evaluates to only \nfalse values, however, then all paths to (cdr x) go through (car x), and x at (cdr x) must be a pair. \nDealing with how the abstract values of variables change as they .ow through the program but still taking \nonly O(nlog n) time is the primary technical challenge of this analysis. The remainder of this section \nshows how to do this. The result is an analysis that takes only linear-log time and produces the same \nresults as the analysis in section 3. First, section 4.1 de.nes a skipping function VC,e that allows \nvalues to move from one point in the program to another without threading through each intervening expres\u00adsion, \nwhile accounting for changes that happen as they .ow through each expression. Next, section 4.2 explains \nhow to determine where to use the skipping functions versus the constraint rules. Then section 4.3 describes \nthe data struc\u00adtures used to cache the skipping functions ef.ciently so that each one can be computed \nor updated in logarithmic time. Since the algorithm in section 4.2 ensures that only a linear number \nof skipping functions are used, the entire analysis then takes only linear-log time. Finally, section \n4.4 puts all of these together into a linear-log time algorithm that com\u00adputes results identical to those \nof the less ef.cient algorithm. 4.1 Context skipping In the earlier example with pair?, the traditional \nalgorithm .rst .ows the abstract value of x from the exiting environ\u00adments of (pair? x) into the entering \nenvironments of e1 and e2, then through e1 and e2, and .nally from the exiting environments of e1 and \ne2 out of both if expressions and into (car x). When .owing through e1 and e2, the abstract value of \nx is threaded through every subexpression of e1 and e2. However, if x is not referenced in e1 and e2, \nthen these expressions can be skipped. Intuitively, the true andfalse en\u00advironments that contain x might \nbe swapped or joined, but the value of x in each environment does not fundamentally change. The following \nlemma re.ects this intuition. Lemma 4.1 (Expression Skipping). If x is a variable not mentioned in e \nthen (. t(x),. f (x))= (Gt(e, . in(x)), Gf (e, . in(x))) where ( .in)= [e]and v, .f )= [e] r, ( .t, in \nout and Gt(e, u) and Gf (e, u) are as de.ned in .gure6. Proof. By induction on e and constraint rules \nin .gure 4.  Gt(e, u) = ( v ..t)= . ? u : . where ( .t,. f )= [e] v, out Gf (e, u) = ( v ..f )= . ? \nu : . where ( .t,. f )= v, [e] out Figure6:True andfalseexpression guards This lemma allows us to directly \ncompute the abstract values of x at the end of e1 and e2 given the abstract values at the start of e1 \nand e2. The Gt(e, u) and Gf (e, u) functions act as guards and return the abstract value u if and only \nif [e] out contains true orfalsevalues respectively. This lemma deals only with the .ow of abstract val\u00adues \nfrom the entry of an expression to its exit. As seen in the examples, however, we are also interested \nin how abstract values .ow from an expression through its sur\u00adrounding context. In the preceding examples, \nabstract val\u00adues .ow from the outputs of (pair? x) and (car x) to the output of the surrounding (if (pair? \nx) e1 e2) and (if e0 (begin (car x) e1) e2) respectively. To account for this we compute the .ow across \na context by means of the context-skipping function VC,e de.ned in .g\u00adure 7. Given an expression, e, \nin a single-layer context, C, it computes the abstract value of a variable in the exit environ\u00adments \nof C[e] given the abstract value in the exit environ\u00adments of e and the entry environment of C[e]. The \nfollowing lemma states this formally. Here again, the intuition is that the true andfalse information \naboutavariable might join or swapbut do not fundamentally change. Lemma 4.2 (Single-Layer Context Skipping). \nIf x is a vari\u00adable not mentioned in the single-layer context C then (. ct (x),. c (x),. in(x))= VC,e(. \net (x),. ef (x),. in(x)) f .e .e where (v e, t , )= [e] f out (v c,. ct ,. c )= [C[e]] f out (r, = [C[e]] \n.in)in . Proof. By lemma 4.1, constraint rules and unfolding. There are a corresponding context-skipping \nfunction and lemma for values entering a context, but they are omitted here as they are straightforward \nand are equivalent to a simple reachability check. For example, consider (if (pair? x) e1 e2) and the \nresulting VC,e. The context of (pair? x) is (if 0 e1 e2). Thus, if e1 returns only true values and e2 \nreturns onlyfalse values, the skipping function, VC,e, does not change the values in the environment \nas they .ow from (pair? x). On the other hand, if e1 returns only false values and e2 returns only true \nvalues, the skipping function swaps the values of the true andfalse environments. Other possibilities \narise depending on the values returned by e1 and e2. ' '' VC,e(v t,v f,v in)= (Gt(C[e],v t), Gf(C[e],v \nf),v in) ''' ' where (v t,v f,v in)= VC(v t,v f,v in) ' '' V(if D e2 e3) (v t,v f,v in)= (v t,v f,v in) \nwhere v t ' = Gt(e2 ,v t) .Gt(e3 ,v f) v f ' = Gf(e2 ,v t) .Gf(e3 ,v f) ' '' V(if e1 D e3) (v t,v f,v \nin)= (v t,v f,v in) where v t ' = v t .Gt(e3 ,v in) v f ' = v f .Gf(e3 ,v in) ' '' V(if e1 e2 D) (v \nt,v f,v in)= (v t,v f,v in) where v t ' = v t .Gt(e2 ,v in) v f ' = v f .Gf(e2 ,v in) V(' .x.D) (v t,v \nf,v in)= (v in,v in,v in) V(' e2 D) (v t,v f,v in)= (v t .v f,v t .v f,v in) V(' D e2) (v t,v f,v in)= \n(v t .v f,v t .v f,v in) V(' D;e2) (v t,v f,v in)= (v t .v f,v t .v f,v in) V(' e1;D) (v t,v f,v in)= \n(v t,v f,v in) Figure 7: Context skipping function While lemma 4.2 handles only single layer contexts, \nthe following theorem generalizes this to multilayer contexts.2 Theorem 4.3 (Multilayer Context Skipping). \nIf x is a vari\u00adable not mentioned in a single-layer or multilayer context C then the equation from lemma \n4.2 holds where VC,e on a composite context is VC2C1,e = VC2,C1[e] .VC1,e Proof. By induction on C and \nlemma 4.2. Note that even for multilayer contexts the universe of pos\u00adsible VC,e is .nite and small. \nAny particular VC,e can be represented in a canonical form of constant size. Theorem 4.4 (Canonical Skipping \nFunctions). There exist T,F .{v t,v f ,v in} for any C, e, [e]and [C[e]] out out such that VC,e(v t,v \nf ,v in)= ( T, F, v in) Proof. By induction on C and unfolding VC,e. Intuitively, there are only so many \nways to join and swap the true and false values of a variable. Diagrammatically, these canonical forms \nare all sub-graphs of the graph in .gure 8 that omit zero or more of the dashed edges that lead to the \ntwo join(.) nodes. Functions of this form have compact, constant-size representations, are closed under \ncomposition, 2This theorem is the reason why the tuple returned by VC,e has a third component even though \nit is unused in lemma 4.2.  v t v f v in .. '' ' v v v tf in Figure 8: Graph form of canonical skipping \nfunctions and form a .nite height lattice that theymonotonically climb when [e]and [C[e]]climb the value \nlattice. out out When composing VC,e we always reduce the composi\u00adtion to this canonical form. Thus all \nVC,e can be applied to a given value in constant time even if the VC,e is from the com\u00adposition of many \nVC,e.We take advantage of this to .ow ab\u00adstract values quickly across multilayer contexts. Since VC,e \nis the same for all variables not in C, we can compute VC,e once and use it for all variables not in \nC. Section 4.3 shows how to compute and update these compositions ef.ciently. This skipping function \nis thekeyinsight of our technique. We still must choose which contexts to skip and how to com\u00adpute VC,e \nef.ciently, but those aspects of the algorithm are only so that we can use skipping functions to .ow \ninforma\u00adtion through the program more ef.ciently.  4.2 Selecting context skips We now have two ways \nto .ow abstract values through a program. The .rst is via the constraint rules in .gure 4. The second \nis via the skipping function, VC,e. For each variable, we use a combination of these methods that ensures \nthe analysis takes only linear-log time while maintaining semantic equivalence with the traditional algorithm. \nWe do this by selecting the longest contexts to be skipped for which theorem 4.3 is valid for a given \nvariable. The length of a context is measured by the number of layers in the context. We fall back to \nthe constraint rules in .gure 4 when theorem 4.3 is not valid. A different set of skips is selected for \neach variable, and we select longestskips for a particular variable, x, by starting with each reference \nto it, e, and .nding the longest context, C, of e that does not contain x. C is then one of the contexts \nthat we skip. Since C is the longest context of e not containing x, the parent of C[e], p, contains references \nto x other than the ones in e. Thus we cannot use theorem 4.3 to skip past p, and at p wefall back to \nthe constraint rules from .gure 4.We repeat the process by .nding the longest context of pthat does not \ncontain x and choose that context as one to be skipped. This if1 23 ee if4 if5  6789 eeee 12 13 if10 \nif11 xx / / // // 14 1516 17 eeee 18 1920 21 xxx x Figure 9: Example AST for skipping context selection \nrepeats until we have all the skips needed to .ow x through the entire program. As an example, consider \nthe abstract syntax tree in .g\u00adure 9 and longest contexts skipped for x. The dotted edges in the diagram \nrepresent multiple layers of the abstract syn\u00adtax tree that are omitted and which do not contain references \nto x. The two children of each if are the consequent and the alternative. The test part of if is omitted \nfor simplicity. To select skips, all references to x are examined. In this case they are expressions \n12, 13, 18, 19, 20 and 21. For each such expression, the longest context not containing x is selected.Forexpression12,thisisthe \ncontextgoingfrom expression 12 to just past expression 8. For expression 13, this is the context going \njust past expression 9, and so on. The parents of these contexts are places where the constraint rules \nare used instead of the context skipping function. For example, because of the reference to x in expression \n9, the other child of expression 5, theorem 4.3 does not hold for moving type information for x from \nexpression 8 to its parent, expression 5. Thus the context skipping function cannot be used there. The \nprocess repeats with the parents of each of the skips. Forexample,expression5is the parentof the contexts \nend\u00adingatexpressions8and9sothe algorithm selectsthe longest contextofexpression5that does not contain \nx. Likewise for expressions 10 and 11. In the end the only places where the algorithm uses the constraint \nrules are expressions 1, 4, 5, 10, and 11. Every\u00adwhere else it uses context skipping functions. The entire \nscope of x is thus tessellated by the skipping contexts and the points where wefall back to the constraint \nrules. This part of the algorithm is linear because the selected context skips form a tree structure. \nThe expressions at which we use the constraint rules are the nodes of the tree. The con\u00adtexts being skipped \nare the edges of the tree. The references to variables are the leaves. Since the number of edges and \nthe number of nodes in a tree are both linearly bounded by the number of leaves, the number of skips \nand the number of uses of the constraint rules for a particular variable are both  if0 if1 (car x1) \ne1 . . VCn \u00b7\u00b7\u00b7C1,en+1 . ifn-1 (car x2) e2 VCn+1 \u00b7\u00b7\u00b7C2,e2 . . ifn (car xn) en VC2n-1 \u00b7\u00b7\u00b7Cn ,e1 ifn+1 (car \nx1) en+1 . . . . if2n- 1 (car x2) en+2 . (car xn) e2n . . Figure 10: Example of quadratic VC,e calculation \nlinearly bounded by the number of references to that vari\u00adable. Summing over all variables we are thus \nlinear in the size of the program. Finding the longest context not containing a particular variable is \nthe most computationally complex part of this process. It is implemented in terms of a lowest common \nancestor algorithm [Aho et al. 1973; Alstrup et al. 2004] that takes linear time for construction and \nconstant time for each query. Finding the longest skips amounts to .nding the lowest common ancestor \nof an expression and the immedi\u00adately preceding and following references to the variable be\u00ading considered. \n 4.3 Caching context skips Theorem 4.3 allows us to skip over a context and move information about variables \nquickly across multiple layers. Once the skipping function, VC,e, is computed and reduced to the canonical \nform in theorem 4.4, it takes only constant time to move information across C for any variable not referenced \nin C. We must be careful that the total time to construct all the skipping functions does not exceed \nour linear-log time bound. For example, consider the abstract syntax tree in .gure 10, where a different \nVC,e is needed for each of the n variables, and each context is n layers deep. Computing the VC,e for \neach variable separately takes O(n2) time. To ensurea linear-log time bound wekeepa cacheofVC,e for selected \nC such that - only linear-log many VC,e are stored in the cache, - for any C,a VC,e can be computed from \nthe composition of only logarithmically many VC,e from the cache, and - when more information is learned \nabout an expression, only logarithmically many VC,e in the cache need to be updated and each VC,e takes \nonly constant time to update.     Figure 11: Layered structure of the VC,e cache Figure 11 shows \nan example of the cached values for one path down a program s abstract syntax tree. Together all of these \ncached VC,e form a tree structure. The same structure occurs on all other paths down the program tree. \nEach VC,e shared between paths is stored only once in the cache. The cache can be thought of as starting \nwith the VC,e single-layer contexts. That is, it stores the skipping informa\u00adtion necessary to .ow any \nvariable by a single step from the exit environment of one expression to the enclosing expres\u00adsion s \nexit environment. If the cache stores only these, then when the abstract value of an expression changes, \nit takes only constant time to update. Next, the single-layer VC,e are paired together. Each sin\u00adgle \nlayer C that goes from depth 2kto depth 2k+1 is paired with each of its single-layer, child contexts \nwhich go from depth 2k+1 to 2k+2. The VC,e for each of these pairings is included in the cache. These \ndouble-layer VC,e are then also paired together. Each double-layer C that goes from depth 4k to depth \n4k +2 is paired with each of its double-layer, child contexts which go from depth 4k +2 to 4k +4. The \nVC,e for each of these pairings is also included in the cache. This process continuesiteratively, pairing \neach 2i-layer con\u00adtext that goes from depth 2i+1kto depth 2i+1k+2i with each of its 2i-layer child contexts \nwhich go from depth 2i+1k+2i to 2i+1k+2i+1 . This selection of cached VC,e has the three important properties \nthat ensure our linear-logarithmic bound. First, only O(nlog n) skipping functions are cached, since \nonly logarithmically many VC,e are cached for any particular e. Second, any VC,e that is not cached can \nbe computed if0  VC1,e1 if1 VC1C2,e3 VC2,e2 if2 VC1 \u00b7\u00b7\u00b7C4,e5 VC3,e3 if3 VC3C4,e5 VC4,e4 if4 VC1 \n\u00b7\u00b7\u00b7C8 ,e9 VC5,e5 if5 VC5C6,e7 VC6,e6 if6 VC5 \u00b7\u00b7\u00b7C8,e9 VC7,e7 if7 VC7C8,e9 VC8,e8 . . . t e 1 t e \n2 t e 3 t e 4 t e 5 t e 6 t e 7 t e 8 f e 1 f e 2 f e 3 f e 4 f e 5 f e 6 f e 7 f e 8  from the \ncomposition of logarithmically many cached VC,e. Third, when the VC,e for a single-layer context is up\u00addated, \nthe double-layer VC,e composed from it are also up\u00addated. If the new double-layer VC,e changes as a result, \nthe quadruple-layer VC,e composed from it are updated, and so on. Thus, when abstract-value information \nis learned about an expression, at most logarithmically many VC,e in the cache are updated. Each update \ntakes constant time since each multilayer VC,e in the cache is composed of two VC,e. This caching strategy \ncan be improved by considering the path from each expression to the root. Storing this path as a perfectly \nbalanced variation of a skip list [Pugh 1990] is equivalent to the caching strategy just described. However, \nby using a variation of Myers applicative random access stacks [Myers 1984], the number of cached values \nand the total time spent updating the cache both become linear in the size of the program.For an arbitrary \nC, computing VC,e may still require logarithmically manycached values, so this does not improve the overall \nasymptotic bounds, but it improves the constants involved. This is the representation used by the implementation \ndescribed in section 5.  4.4 Algorithm summary Putting all these pieces together, the optimized algorithm \nworks as follows. First, as described in section 4.3, the cache of skipping functions is constructed \nas a .ow graph. This creates linearly many nodes in linear time. Next, for each variable, context skips \nare selected as described in sec\u00adtion 4.2 and .ow-graph nodes are constructed that take log\u00adarithmically \nmany VC,e from the cache andbuilda VC,e for the skipped context. In total there are linearly many con\u00adtext \nskips and each one involves composing logarithmically many skipping functions. Each composition requires \none .ow-graph node, so this process creates O(nlog n) nodes. Finally, for each non-skipping point where \na variable is ref\u00aderenced or the constraint rules are used for a particular vari\u00adable, a .ow-graph node \nis constructed that computes the type of the variable at that point in terms of the non-skipping points \nthat .ow to the point and the VC,e that skips from them to the non-skipping point. A similar process \nis used for entering rather than exiting a context. Since in total there are linearly manyskipping points, \nthis creates linearly many nodes. Overall, this entire process takes linear-log time to construct the \n.ow graph, and it produces a .ow graph with a linear-log number of nodes. The values .owing over the \nedges of the graph all monotonically increase over constant\u00adheight lattices, and nodes recompute in terms \nof their inputs in constant time. Thus, the .ow-graph for the optimized analysis converges in linear-log \ntime. 5. Implementation Wehaveimplemented the algorithm describedin section4as an experimental optimization \nfor the Chez Scheme [Dybvig 2010] compiler. It is used to perform type recovery and jus\u00adtify the elimination \nof run-time type checks. The implemen\u00adtation supports the full Scheme language and successfully compiles \nand runs both Chez Scheme itself and the entire Chez Scheme test suite without errors. 5.1 Implementation \nstructure To implement type recovery, a post-processing pass is added after the CFA pass. The post-processing \npass uses the type information gathered during the CFA pass to determine where run-time type checks are \nunnecessary. Primitive calls where some or all of the run-time type checks are unneces\u00adsary are replaced \nby an unsafe variant of the call which does not perform the unnecessary run-time type check. For instance \n(car x) is replaced by (unsafe-car x) when x is determined to be a pair. If a primitive makes multiple \nrun\u00adtime type checks and only some type checks can be omitted, then a semi-unsafe variant is used. These \ncases arise when a primitive does more than one run-time type check or when the checks involve information \nnot tracked by the analy\u00adsis.Forexample,avector range check cannotbe eliminated because the analysis \ndoes not track the lengths of vectors. Another example is when the analysis determines that the vector \nargument of a vector-ref isalwaysavectorbut not that the index argument is always a nonnegative integer. \n 5.2 Implementation notes Our implementation handles a variety of language constructs and features that \nare not described in section 4. Among these are mutable variables and the unspeci.ed evaluation order \nfor function call arguments and let bindings. Amutable variable s type can change between where type \ninformation is recovered and it is used. For instance, an in\u00adtervening function call could arbitrarily \nmutate the variable and invalidate what is learned.3 Thus, for mutable variables, our implementationgathers \nonly constructive information. The unspeci.ed evaluation order for function-call argu\u00adments and let bindings \ncan be handled by choosing a .xed evaluation order prior to this analysis. At present, however, the decision \nis made later in the compiler during register al\u00adlocation. We therefore process function-call arguments \nin\u00addependently, as we do for the branches of an if. While the resulting environments are unioned for \nif, they are inter\u00adsected for function-call arguments. The bindings of let are handled similarly.  5.3 \nEffectiveness We tested the effectiveness of the type-recovery algorithm on a standard set of Scheme \nbenchmarks [Clinger 2008]. Each test was run both with type recovery enabled and with type recovery disabled. \nThe number of type checks per\u00adformed in these two cases were then compared with each 3 This issue arises \nonly in higher-order languages. The analysis can pro\u00adcess restrictiveinformation for mutable variablesin \n.rst-order languages, including, for example, the output language of a closure-conversion pass in a typical \ncompiler for a higher-order language.  100% 80% 60% 40% 20% 0% Flow-insensitive Flow-insensitive Flow-sensitive \nFlow-sensitive sub-0CFA 0CFA sub-0CFA 0CFA Figure 12: Percent of type checks removed other. Our tests \nlook only at type checks caused by pair and vector primitives (e.g. car, cdr, cadr, vector-ref, vector-set!, \netc.). An average of 69.1% of type checks are eliminated from the code, which results in 55.35% fewer \ntype checks at run-time.We compared these results witha .ow-insensitive version of the analysis which \nperformed signi.cantly worse, eliminating 41.3% of run-time type checks. We also com\u00adpared the .ow-sensitive \nsub-0CFA results with a .ow\u00adsensitive 0CFA. The 0CFA version performed only slightly better, eliminating \n55.39% of type checks at run time. A .ow-insensitive 0CFA performed slightly better then the .ow-insensitive \nsub-0CFA, eliminating 41.7% of run-time type checks. Figure 12 compares the average percent of type checks \neliminated for the .ow-insensitive sub-0CFA, .ow\u00adinsensitive 0CFA, .ow-sensitive sub-0CFA, .ow-sensitive \n0CFA. Figure 14 gives the percent of checks eliminated for each individual program. While these results \nare promising, they are not necessar\u00adily a good predictor for how well type-recovery will perform in \ngeneral. Using the same set of tests, and counting only the checks made by pair primitives, only 36.4% \nof checks can be eliminated, resulting in the elimination 30.6% of run\u00adtime checks. Pairs are a hard \ncase for type-recovery. While vectors store a number of items, pairs store only two. At best we expect \nto see a call to car paired with a call to cdr and can eliminate only one of the two type checks. In \noperations such as cadr and cddr only the .rst of two pair checks can be eliminated. This is because \nthe contents of pairs are not tracked by the analysis. Hence, it cannot determine that the cdr of a pair \nis also a pair, so the nested pair must always be type checked. It is also common in Scheme to structure \ndata into proper lists, and use an explicit null? check to de\u00adtermine when the end of the list is reached. \nUnfortunately, the null? check does not eliminate the need for the pair? checks implicit in car and cdr, \nsince it tells the analysis only that the value is not null. Beyond the dif.culties in han\u00addling pairs, \nsome opportunities for eliminating type checks are already handled by the source optimizer before getting \nto the type recovery analysis. Time 1 0.1 0.01 0.001 0.0001 100 1000 10000 100000 AST node count Figure \n13: Source node count versus analysis time Although the analysis does not require the evaluation or\u00adder \nof let bindings and function-call arguments to be spec\u00adi.ed, type information learned in one argument \nor binding might be useful for eliminating a type check in another ar\u00adgument or binding.For instance,in \n(f (car x) (cdr x)) a speci.ed evaluation order would allow the implicit pair check to be eliminated \nfrom one of the two argument expres\u00adsions.To determinethe impactof .xingtheevaluation order, we tested \nwith both left-to-right and right-to-left evaluation orders. In both cases, although the bene.t is signi.cant \nin a few cases, the average number of type checks at run time improved by only around 5%. These results \nare encouraging, and we expect to be able to make additional improvements as we re.ne the implementa\u00adtion. \nThe analysis currently treats all pairs and all vectors the same, although we could treat each occurrence \nof cons and make-vector in the source code as a separate element in the lattice analogously to the way \nwe handle lambda ex\u00adpressions, and thus get more information about the contents of pairs and vectors. \n 5.4 Ef.ciency Beyond the effectiveness of our analysis, we also veri.ed its asymptotic behavior and \nmeasured its speed by counting the number of source-tree nodes on input to the type-recovery algorithm \nand measuring the time it takes for the algorithm to run. For this test, we used the same Scheme benchmarks \nas before along with the compilation units that comprise the Chez Scheme compiler. Figure 13 plots these \ntimes on a log\u00adarithmic scale along with linear (lower) and linear-log (up\u00adper) reference lines. The \nquantization of the numbers at the lower end of the graph results from timer granularity. The graph shows \nthat the processing times trend between the lin\u00adear and worst-case linear-log lines as expected. The \ntype re\u00adcovery is also acceptablyfast, handling 100,000 AST nodes (approximately 30,000 lines of code) \nin less than a second for the largest of the programs. Averaging over all of the programs, the implementation \nhandles about 281,500 AST nodes (approximately 100,000 lines of code) per second.  100%90%80%70%60%50%40%30%20%10%0% \n100% 80% 60% 40% 20% 0% Sub-0CFA flow-insensitive0CFA flow-insensitiveSub-0CFA flow-sensitive0CFA flow-sensitive \nSub-0CFA flow-insensitive0CFA flow-insensitiveSub-0CFA flow-sensitive0CFA flow-sensitive 100%90%80%70%60%50%40%30%20%10% \n0% 100% Sub-0CFA flow-insensitive0CFA flow-insensitive 90% Sub-0CFA flow-sensitive0CFA flow-sensitive80%70%60%50%40%30%20%10%0% \nSub-0CFA flow-insensitive0CFA flow-insensitiveSub-0CFA flow-sensitive0CFA flow-sensitive Figure 14: \nPercent of type checks removed 6. Related work 6.1 CFA and CFA-based type recovery Shivers [1991] uses \nan extension of 0CFA to perform type recovery. Instead of directly discovering type information about \nvariables, he adds a level of indirection and discovers information about the quantity a variable contains. \nThis ap\u00adproach allows information learned about one variable to be shared with its aliasesbut leads to \npotential correctness prob\u00adlems if multiple quantities .ow to the same variable. Shivers addresses this \nby introducing a re.ow semantics to correct for the problems caused by the indirection. We do not treat \nquantity information in our analysis, and instead rely on a pass earlier in the compiler that performs \ncopy propagation and aggressive inlining. This keeps our analysis relatively simple while still yielding \nsome of the bene.ts of his quanti\u00adties. Since it is based on 0CFA rather than sub-0CFA, Shiv\u00aders s analysis \nis more precise though asymptotically more expensive than ours. Serrano [1995] argues that 0CFA is useful \nin compilers for functional languages by presenting two use cases: an analysis for reducing closure allocation \nand an analysis for reducing dynamic type tests. Serrano reports that the latter algorithm eliminates \n65% of dynamic type tests. This differs from our results, which show that a 0CFA-based analysis eliminates \nonly 41.7% of type tests. The difference is likely attributable to different sets of benchmarks being \nused and to different strategies for inserting and counting type checks. As it is based on 0CFA, Serrano \ns analysis takes O(n3) time in the worst-case. Heintze and McAllester [1997b] describe a linear-time \nCFA. It is targeted at typed languages and assumes bounds on the sizes of types. In linear time it can \nlist up to a constant number of targets for all call sites, and in quadratic time it can list all targets \nfor all call sites. Mossin [1998] indepen\u00addently developed a similar quadratic analysis for explicitly \ntyped programs based on higher-order .ow graphs. Whereas these analyses are based on inclusion, Henglein \ns simple clo\u00adsure analysis [Henglein 1992b] computes a cruder approx\u00adimation based on equality constraints \nand can be solved in almost linear time via uni.cation. None of these analyses are .ow-sensitive. Our \nnotion of sub-0CFAis close to that of Ashleyand Dy\u00adbvig [1998]. They effectively use a more restrictive \nlattice than ours but provide a general framework through which more general lattices can be constructed. \nTheir analysis achieves a limited form of .ow sensitivity when the test of an if is a type predicate \napplied to a variable by creating new bindings for the variable in the then and else parts of the if \nwhose abstract values are restricted by the test. Theyalso describe a more general form of .ow sensitivity \nthat tracks variable assignments. It does notgather observational infor\u00admation from nested conditionals, \ntype-restricted primitives, or user-de.ned functions, and they do not make any claims about its asymptotic \nbehavior. 6.2 Type recovery based on type inference Soft typing [Cartwright andFagan 1991] and more \nrecently, gradual typing [Siek and Taha 2006] are designed to pro\u00adduce, through type inference, statically \nwell-typed programs from dynamically typed programs by introducing run-time checks or casts. CFA-based \ntype recovery can be seen as an alternative mechanism for accomplishing a similar ef\u00adfect. While soft \ntyping andgradual type systems might reject some programs, our implementation never rejects programs, \nbecause type errors are semantically required to cause run\u00adtime exceptions. Henglein [1992a] presents \na fast O(na(n)) tagging op\u00adtimization algorithm for Scheme. Using the terminology of Steenkiste [1991], \nthe goal of the algorithm is to statically eliminate dynamic taginsertion and tag removal operations. \nIn contrast, we seek to eliminate dynamic tag checks. Val\u00adues in Chez Scheme are always tagged as the \ngarbage col\u00adlector relies upon them. Henglein reports that his algorithm is able to eliminate around \n40% of the executed tag inser\u00adtion operations and around 55% of the executed tag removal operations across \nsix non-numerical benchmarks. Although related, these numbers are not comparable to our number of eliminated \ntag checking operations. In a companion paper, Henglein [1994] addresses the theory of dynamic typing \nin the form of a calculus with explicit type coercions and an equational theory. The concept of occurrence \ntyping developed in the con\u00adtextofTyped Scheme[Tobin-Hochstadt and Felleisen 2010], is closely related \nto the present analysis in that different oc\u00adcurrences of the same variable are typed differently depend\u00ading \non the control .ow through type-testing predicates. The type systemofTyped Schemeexpresses typesas formulasin \na propositional logic that has some similarities to the lattice structure underlying our analysis.  \n6.3 Recent type-recovery applications Jensen et al. [2009] develop a type analysis for JavaScript. Their \nanalysis is context-sensitive and incorporates both re\u00adcency abstraction and abstract garbage collection. \nTheyfo\u00adcus however on precision over computational complexity. As a result, their analysis sometimes \nrequires a few minutes to process JavaScript programs of only several hundred lines. Vardoulakis and \nShivers [2010] describe a summariza\u00adtion-based CFA with a degree of .ow sensitivity. In addi\u00adtion to \nprecise call-return matching, their analysis models precisely the top stack frame of arguments. However, \ntheir focus is more on precision than ef.ciency. The analysis has since been re-targeted to JavaScript \nin the form of Doc\u00adtorJS [Mozilla Corporation 2011]. For type checking dynamically typed programs, Guha \net al. [2011] combine a type system and a .ow analysis such that the latter boosts the precision of the \nformer. Like our analysis, their .ow analysis is .ow-sensitive and computes tag sets for each occurrence \nof a variable. Unlike our anal\u00adysis, it is not interprocedural. Instead it relies on the type system \nat function boundaries. It has a quadratic worst-case time complexity.  6.4 Other related work Wegman \nand Zadeck [1991] formulatedfast constant propa\u00adgation algorithms for a .rst-order imperative language. \nTheir conditional constant propagation relates to our CFA in that they track reachability and maygain \ninformation from con\u00additionals. Wegman and Zadeck list elimination of run-time type checks in a Lisp \ndialect as a possible use of their approach. Whereas they consider multiple ways to handle functions, \nincluding aliasing of pass-by-reference parame\u00adters, theydo not consider how to handle .rst-class functions. \nAs an illustration of a general property simulation al\u00adgorithm in ESP, Das et al. [2002] instantiate \ntheir general framework to a .ow-sensitive constant-propagation algo\u00adrithm. However, the resulting work-list \nalgorithm is polyno\u00admial as it involves invoking a theorem prover at each condi\u00adtional expression for \nsymbolic evaluation. 7. Conclusions and future work This paper describes a .ow-sensitive type-recovery \nalgo\u00adrithm based on sub-0CFAthat runs in linear-log time. It jus\u00adti.es, on average, the removal of about \n60% of run-time type checks in a standard set of benchmarks for the dynamically typed language Scheme. \nIt handles, on average 100,000 lines of code in less then a second. The implementation conservatively \nhandles the unspec\u00adi.ed evaluation order of arguments and bindings. Making evaluation-order decisions \nearlier in the compiler would al\u00adlow the analysis to be more precise, particularly if the de\u00adcisions \nwere in.uenced by the needs of the analysis. Our experiments show that the typical bene.t is likely minimal, \nbut the bene.t in some cases would be substantial. Employing an extended lattice that differentiates \npairs and vectors based on their allocation sites as the analysis already does for functions should also \nlead to more precise information. In a statically typed variant of the analysis, the lattice can also \nbe re.ned to differentiate functions with different static types. Even in a dynamically typed language, \nfunctions can be grouped by arity. Another avenue for further investigation is to supplement the current \ntechniques with an ef.cient must-alias analysis, such that for two aliased variables x and y, information \nlearned about x is re.ected in y. The higher-order must-alias analysis by Jagannathan et al. [1998] is \na natural starting point for such an investigation. Finally, we conjecture that the same techniques we \nhave used to extend sub-0CFA with .ow sensitivity can be ap\u00adplied more generally to kCFA with the addition \nof a single logarithmicfactor to the asymptotic cost. Acknowledgments Comments from Dan Friedman, JeremiahWillcock, \nLindsey Kuper, Alex Hearn, and the anonymous reviewers led to several improvements in this paper s presentation. \nReferences AlfredV.Aho,JohnE. Hopcroft,andJeffreyD. Ullman.On .nding lowest common ancestors in trees. \nIn Proceedings of the .fth annual ACM symposium on Theory of computing, pages 253 265.ACM, 1973. doi: \n10.1145/800125.804056. Stephen Alstrup, Cyril Gavoille, Haim Kaplan, and Theis Rauhe. Nearest common \nancestors:A survey anda new algorithm fora distributed environment. Theory of Computing Systems, 37(3): \n441 456, May 2004. doi: 10.1007/s00224-004-1155-5. J. Michael Ashley and R. Kent Dybvig. A practical \nand .exible .ow analysis for higher-order languages. ACMTransactions on Programming Languages and Systems \n(TOPLAS), 20(4):845 868, July 1998. doi: 10.1145/291891.291898. John P. Banning. An ef.cient way to .nd \nthe side effects of procedure calls and the aliases of variables. In Proceedings of the 6th ACM SIGACT-SIGPLAN \nsymposium on Principles of programming languages, pages 29 41. ACM, 1979. doi: 10.1145/567752.567756. \nRobert Cartwright and MikeFagan. Soft typing. In Proceedings of theACM SIGPLAN 1991 conference on Programming \nlanguage design and implementation, pages 278 292. ACM, 1991. doi: 10.1145/113445.113469. Swarat Chaudhuri. \nSubcubic algorithms for recursive state ma\u00adchines. In Proceedings of the 35th annual ACM SIGPLAN-SIGACT \nsymposium on Principles of programming languages, pages 159 169.ACM, 2008. doi: 10.1145/1328438.1328460. \nWilliam D. Clinger. Description of benchmarks, 2008. URL http://www.larcenists.org/benchmarksAboutR6.html. \nPatrick Cousot and Radhia Cousot. Systematic design of program analysis frameworks. In Proceedings of \nthe 6th ACM SIGACT-SIGPLAN symposium on Principles of programming languages, pages 269 282.ACM, 1979. \ndoi: 10.1145/567752.567778. Manuvir Das, Sorin Lerner, and Mark Seigle. ESP: Path-sensitive program veri.cation \nin polynomial time. In Proceedings of the ACM SIGPLAN 2002 Conference on Programming language design \nand implementation, pages 57 68. ACM, 2002. doi: 10.1145/512529.512538. R. Kent Dybvig. Chez Scheme Version \n8 User s Guide. Cadence Research Systems, 2010. Arjun Guha, Claudiu Saftoiu, and Shriram Krishnamurthi. \nTyping local control and state using .ow analysis. In Programming Languages and Systems, volume 6602, \npages 256 275. Springer Berlin/Heidelberg, 2011. doi: 10.1007/978-3-642-19718-5 14. Nevin Heintze and \nDavid McAllester. On the complexity of set\u00adbased analysis. In Proceedings of the secondACM SIGPLAN in\u00adternational \nconference on Functional programming, pages 150 163.ACM, 1997a. doi: 10.1145/258948.258963. Nevin Heintze \nand David McAllester. Linear-time subtransitive control .ow analysis. In Proceedings of the ACM SIGPLAN \n1997 conference on Programming language design and imple\u00ad mentation, pages 261 272.ACM, 1997b. doi: 10.1145/258915. \n258939.  Fritz Henglein. Global tagging optimization by type inference. ACM SIGPLAN Lisp Pointers, V(1):205 \n215, January 1992a. doi: 10.1145/141478.141542. Fritz Henglein. Dynamic typing: Syntax and proof theory. \nScience of Computer Programming, 22(3):197 230, June 1994. doi: 10.1016/0167-6423(94)00004-2. Fritz Henglein. \nSimple closure analysis. Semantics Report D-193, DIKU, University of Copenhagen, 1992b. Suresh Jagannathan \nand Stephen Weeks. A uni.ed treatment of .ow analysis in higher-order languages. In Proceedings of the \n22nd ACM SIGPLAN-SIGACT symposium on Principles of programming languages, pages 393 407. ACM, 1995. doi: \n10.1145/199448.199536. Suresh Jagannathan, Peter Thiemann, StephenWeeks, and Andrew Wright. Single and \nloving it: Must-alias analysis for higher\u00adorder languages. In Proceedings of the 25th ACM SIGPLAN-SIGACT \nsymposium on Principles of programming languages, pages 329 341.ACM, 1998. doi: 10.1145/268946.268973. \nSimon Jensen, Anders M\u00f8ller, and Peter Thiemann. Type anal\u00adysis for JavaScript. In Static Analysis, volume \n5673, pages 238 255. Springer Berlin / Heidelberg, 2009. doi: 10.1007/ 978-3-642-03237-0 17. David Melski \nand Thomas Reps. Interconvertibility of a class of set constraints and context-free-language reachability. \nTheoretical Computer Science, 248(1 2):29 98, October 2000. doi: 10. 1016/S0304-3975(00)00049-9. Jan \nMidtgaard and DavidVan Horn. Subcubic control .ow analysis algorithms. Computer Science Research Report \n125, Roskilde University, Roskilde, Denmark, May 2009. Revised version to appear in Higher-Order and \nSymbolic Computation. Torben \u00c6. Mogensen. Glossary for partial evaluation and related topics. Higher-Order \nand Symbolic Computation, 13(4):355 368, December 2000. doi: 10.1023/A:1026551132647. Christian Mossin. \nHigher-order value .ow graphs. NordicJournal of Computing, 5(3):214 234, 1998. Mozilla Corporation. Doctor \nJS, 2011. http://doctorjs.org/. EugeneW. Myers. Ef.cient applicative data types. In Proceedings of the \n11th ACM SIGACT-SIGPLAN symposium on Principles of programming languages, pages 66 75. ACM, 1984. doi: \n10.1145/800017.800517. Flemming Nielson, Hanne Riis Nielson, and Chris Hankin. Princi\u00adples of Program \nAnalysis. Springer-Verlag, 1999. ISBN 978-3\u00ad540-65410-0. William Pugh. Skip lists: A probabilistic alternative \nto balanced trees. Communications of the ACM, 33(6):668 676, June 1990. doi: 10.1145/78973.78977. Manuel \nSerrano. Control .ow analysis: A functional languages compilation paradigm. In Proceedings of the 1995ACM \nsympo\u00adsium on Applied computing, pages 118 122. ACM, 1995. doi: 10.1145/315891.315934. Olin Shivers. \nData-.ow analysis and type recovery in Scheme. In Peter Lee, editor, Topics in Advanced Language Implemen\u00adtations, \npages 47 87. The MIT Press, 1991. Olin Shivers. Control .ow analysis in Scheme. In Proceedings of theACM \nSIGPLAN 1988 conference on Programming Language design and Implementation, pages 164 174. ACM, 1988. \ndoi: 10.1145/53990.54007. Jeremy G. Siek and Walid Taha. Gradual typing for functional languages. In \nScheme and Functional Programming Workshop, pages 81 92, September 2006. Peter A. Steenkiste. The implementation \nof tags and run-time type checking. In Peter Lee, editor, Topics in Advanced Language Implementations, \npages 3 24. The MIT Press, 1991. SamTobin-Hochstadt and Matthias Felleisen. Logical types for un\u00adtyped \nlanguages. In Proceedings of the 15thACM SIGPLAN in\u00adternational conference on Functional programming, \npages 117 128.ACM, 2010. doi: 10.1145/1863543.1863561. Dimitrios Vardoulakis and Olin Shivers. CFA2: \nA context-free approach to control-.ow analysis. In Programming Languages and Systems, volume 6012, pages \n570 589. Springer Berlin / Heidelberg, 2010. doi: 10.1007/978-3-642-11957-6 30. Mark N. Wegman and F. \nKenneth Zadeck. Constant propagation with conditional branches. ACM Transactions on Programming Languages \nand Systems (TOPLAS), 13(2):181 210, April 1991. doi: 10.1145/103135.103136.    \n\t\t\t", "proc_id": "2048066", "abstract": "<p>The flexibility of dynamically typed languages such as JavaScript, Python, Ruby, and Scheme comes at the cost of run-time type checks. Some of these checks can be eliminated via control-flow analysis. However, traditional control-flow analysis (CFA) is not ideal for this task as it ignores flow-sensitive information that can be gained from dynamic type predicates, such as JavaScript's 'instanceof' and Scheme's 'pair?', and from type-restricted operators, such as Scheme's 'car'. Yet, adding flow-sensitivity to a traditional CFA worsens the already significant compile-time cost of traditional CFA. This makes it unsuitable for use in just-in-time compilers. In response, we have developed a fast, flow-sensitive type-recovery algorithm based on the linear-time, flow-insensitive sub-0CFA. The algorithm has been implemented as an experimental optimization for the commercial Chez Scheme compiler, where it has proven to be effective, justifying the elimination of about 60% of run-time type checks in a large set of benchmarks. The algorithm processes on average over 100,000 lines of code per second and scales well asymptotically, running in only O(n log n) time. We achieve this compile-time performance and scalability through a novel combination of data structures and algorithms.</p>", "authors": [{"name": "Michael D. Adams", "author_profile_id": "81316487801", "affiliation": "Indiana University, Bloomington, IN, USA", "person_id": "P2839214", "email_address": "adamsmd@cs.indiana.edu", "orcid_id": ""}, {"name": "Andrew W. Keep", "author_profile_id": "81392616901", "affiliation": "Indiana University, Bloomington, IN, USA", "person_id": "P2839215", "email_address": "akeep@cs.indiana.edu", "orcid_id": ""}, {"name": "Jan Midtgaard", "author_profile_id": "81100381173", "affiliation": "Aarhus University, Aarhus, Denmark", "person_id": "P2839216", "email_address": "jmi@cs.au.dk", "orcid_id": ""}, {"name": "Matthew Might", "author_profile_id": "81309498719", "affiliation": "University of Utah, Salt Lake City, UT, USA", "person_id": "P2839217", "email_address": "might@cs.utah.edu", "orcid_id": ""}, {"name": "Arun Chauhan", "author_profile_id": "81100266897", "affiliation": "Indiana University, Bloomington, IN, USA", "person_id": "P2839218", "email_address": "achauhan@cs.indiana.edu", "orcid_id": ""}, {"name": "R. Kent Dybvig", "author_profile_id": "81100181541", "affiliation": "Indiana University, Bloomington, IN, USA", "person_id": "P2839219", "email_address": "dyb@cs.indiana.edu", "orcid_id": ""}], "doi_number": "10.1145/2048066.2048105", "year": "2011", "article_id": "2048105", "conference": "OOPSLA", "title": "Flow-sensitive type recovery in linear-log time", "url": "http://dl.acm.org/citation.cfm?id=2048105"}