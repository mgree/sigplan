{"article_publication_date": "10-22-2011", "fulltext": "\n * Why Nothing Matters: The Impact of Zeroing XiYang , StephenM. Blackburn , Daniel Frampton , JenniferB. \nSartor , KathrynS. McKinley* Australian National University EPFL *Microsoft Research *UniversityofTexasat \nAustin Abstract Memory safety defends against inadvertent and malicious misuse of memory that may compromise \nprogram correct\u00adness and security. A critical element of memory safety is zero initialization. The direct \ncost of zero initialization is surprisingly high: up to 12.7%, with average costs ranging from 2.7 to \n4.5% on a high performance virtual machine on IA32 architectures. Zero initialization also incurs indirect \ncosts due to its memory bandwidth demands and cache dis\u00adplacement effects. Existing virtual machines \neither: a) mini\u00admize direct costs by zeroing in large blocks, or b) minimize indirect costs by zeroing \nin the allocation sequence, which reduces cache displacement and bandwidth. This papereval\u00aduates the \ntwo widely used zero initialization designs, show\u00ading that theymake different tradeoffs to achievevery \nsimilar performance. Our analysis inspires three better designs: (1) bulk ze\u00adroing with cache-bypassing(non-temporal)instructions \nto reduce the direct and indirect zeroing costs simultaneously, (2) concurrent non-temporalbulk zeroing \nthatexploits par\u00adallel hardware to move work off the application s critical path, and (3) adaptive zeroing, \nwhich dynamically chooses between (1) and (2) based on available hardware paral\u00adlelism. The new software \nstrategies offer speedups some\u00adtimes greater than the direct overhead, improving total per\u00adformance by \n3% on average. Our .ndings invite additional optimizations and microarchitectural support. Categories \nand Subject Descriptors D3.4[Programming Languages]: Processors Memory management (garbage collection); \nOptimization; Run-time environments GeneralTerms Performance, Measurement Keywords Memory safety, Zero \ninitialization * This work is supported by ARC DP0666059, NSF SHF0910818, NSF CSR0917191, NSF CCF0811524, \nNSF CNS0719966, Intel, Google and Microsoft Research. Any opinions, .ndings and conclusions expressed \nherein are the authors and do not necessarily re.ect those of the sponsors. Permission to make digital \nor hard copies of all or part of this work for personal or classroom use is granted without fee provided \nthat copies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page.To copyotherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. OOPSLA 11 October 22 27, 2011, Portland, Oregon, \nUSA. Copyright c &#38;#169; 2011ACM 978-1-4503-0940-0/11/10... $10.00 1. Introduction Memory safety is \nan increasingly important tool for the cor\u00adrectness and security of modern language implementations. \nAkeyelement of memory safety is initializing memory be\u00adfore giving it to the program. In managed languages, \nsuch as Java, C#, and PHP, the language speci.cations stipulate zero initialization.For the same reason, \nunmanaged native languages, such as C and C++, have begun to adopt zero initialization to improve memory \nsafety [26].We show that existing approachesof zero initializationaresurprisinglyex\u00adpensive. On three \nmodern IA32 architectures, the direct cost is around 2.7-4.5% on average and as much as 12.7% of all \ncycles,ina high-performanceJavaVirtual Machine (JVM), without accounting for indirect costs due to cache \ndisplace\u00adment and memory bandwidth consumption. Hardware trends towards chip multiprocessors (CMPs) are \nexacerbating these expenses because of their increasing demands on memory bandwidth [9, 15, 16, 24, 28, \n33, 34] and pressures on shared memory subsystems, such as shared on-chip caches and memory controllers.Forexample, \nZhao et al. and Inoue et al. showthat the memory bandwidth needs of both managed and unmanaged languages \nare a large per\u00adformance bottleneck on CMPs [16, 34]. Furthermore, energy is now constraining memory \nbandwidth [8]. If architects add processor cores without adding commensurate memory re\u00adsources (memory \nbandwidth and shared caches), the over\u00adhead of existing zero initialization techniques is likely to grow. \nAlthough hardware parallelism increases pressure on the memory system, it offers an optimization opportunity, \nsuch as of.oading critical system services that must be done ina timely manner.To our knowledge, this \npaperis the .rst to explore the zero initialization design space and show that zero initialization is \ncostly. Existing zero initialization strategiesface two problems: the direct cost of executing the requisite \nzeroing instructions and the indirect cost of memory bandwidth consumption and cache pollution. The two \nstandard designs in JavaVir\u00adtual Machines today arebulk zeroing (JikesRVM and op\u00adtionally HotSpot) and \nhot-path zeroing (Azul, HotSpot de\u00adfault, and J9 [14]). Bulk zeroing attacks the direct cost by zeroing \nmemory in large chunks and exploiting instruction level parallelism, loop optimizations, and zeroing \na cache line or more at a time. Bulk zeroing, however, introduces a signi.cant reuse distance between \nwhen the VM zeroes a cache line and when the application .rst uses it. This dis\u00adtance increases cache \npollution. Hot-path zeroing injects ze\u00adroing instructions into the allocation sequence, attacking in\u00addirect \ncosts by minimizing reuse distance and exploiting the hardware prefetchertoavoid stallsin modern fetch-on-write \ncaches. Hot-path zeroing,however,expandsand complicates the performance-critical allocation sequence \nand reduces op\u00adportunities for software optimization of the zeroing instruc\u00adtions. The two designs are \nthus at poles, addressing either, but not both, of the direct and indirect costs of zeroing.  Although \nthis cost is signi.cant, very little research ex\u00adplores zeroing costs or optimizations.We performa detailed \nstudyinJikesRVMand con.rmthe resultsviaa preliminary implementationintheHotSpotJVM.Weuse19 benchmarks \nfrom DaCapo [6], SPECjvm98 [32], and pjbb2005 [3],execut\u00ading on three mainstream CMPs: an Intel Core2 \nQuad Q6600, an AMD Phenom II X6 1055T, and an Intel Sandy Bridge Core i7-2600. We measure the allocation \nrates of real and microbenchmarks to explore performance limits and costs. Our analysis reveals opportunities \nand tradeoffs in zeroing strategies.We show that an effective hardware prefetcher is critical to the \nperformance of hot-path zeroing. We introduce three better solutions. (1) Non-temporal bulk zeroing targets \nboth direct and indirect costs using cache-bypassing instructions. (2) Concurrent non-temporal bulkzeroingtargetsdirectcostsbyusing \nparallelismtomove zeroing offthe application s critical path. (3) Adaptive zero\u00ading chooses betweenthe \n.rsttwo designs basedonavailable hardware parallelism. Our zeroing designs take advantage of non-temporal \nin\u00adstructions and unutilized hardware parallelism to minimize zeroing costs.We demonstrate that non-temporal \nstores im\u00adprove memory throughput and mitigate cache pollution due tobulk zeroing.Takingadvantageofavailable \nhardwarepar\u00adallelism to move zeroing off the application s critical path further reduces the direct cost \nof zeroing. The best strat\u00adegy adaptively chooses between concurrent and synchronous non-temporalbulk \nzeroing, adjusting based on theavailabil\u00adity of unused hardware parallelism. The adaptive approach improves \nperformance by 3.2% on average and up to 9.2% on the i7-2600. It is most effective on highly allocating, \nmemory intensive benchmarks, which stress the memory systemthe most. Nonetheless,the total numberofcyclesde\u00advoted \nto zero-initialization is often substantial, which sug\u00adgests that further optimization of zeroing will \nbe useful. The contributions of this paper are: (1) the .rst detailed study of the cost of zero initialization \nwhich shows zero ini\u00adtializationis oftenexpensiveon modern processors,(2)ade\u00adtailed microarchitectural \nanalysis of existing designs which shows they make different tradeoffs but have very similar performance, \nand (3) identi.cation and evaluation of three newdesigns. The adaptivedesign uses non-temporal instruc\u00adtions \nand concurrency to provide speedups that sometimes exceed the direct cost of zero initialization. 2. \nBackground and RelatedWork Our work sits at the boundary of programming language implementation and microarchitecture. \nThis section presents key background ideas and related work in hardware and software. Language design. \nManaged languages such as Java and C# have long touted memory safety as a software engineer\u00ading and security \nbene.t and native languages, such as C and C++, are now embracing memory safety using compiler and library \nsupport [26]. Data initialization and pointer dis\u00adciplines are the principal techniques for ensuring \nmemory safety. Pointer safety disciplines protect against unintended or malicious access to memoryby \nensuring that the program accesses only valid references to reachable objects. Pointer safety is achieved \nthrough a combination of language speci\u00ad.cation and implementation techniques that enforce pointer declarations \nin static or dynamic type systems. The language speci.cation forbids reference forging, and the implementa\u00adtion \nchecks array indices, and usesgarbage collection rather than manual freeing to avoid dangling references. \nThe run\u00adtime also zero initializes all data before the program reads it. This approach is conservative \na program will often ex\u00adplicitly initialize the data before use as well, rendering the runtime s zeroing \nredundant. Both pointer safety and data initialization offer software engineering and security bene\u00ad.ts,but \ntheyincrease the numberof memory operations. Memory system design. Meanwhile, the era of chip multi\u00adprocessors \nis increasing pressure on memory performance [9, 15, 16, 24, 28, 33, 34]. Adding hardware parallelism \nin\u00adcreases computational power, but scaling memory perfor\u00admance to keep pace is challenging. Zhao et \nal. show that allocation-intensive Java programs create an allocation wall on modern chip multiprocessors \n(CMPs) that limits appli\u00adcation scaling and performance [34]. Studying partially scalable benchmarks, \nthey found a strong correlation be\u00adtween object allocation rates and memorybus write traf.c, which is \nquickly saturated and limits scalability. Inoue et al. show that bandwidth problems are common to more \nthan just managed languages highly allocating web server ap\u00adplications written in native languages also \nhave extremely high memory bandwidth demands that compromise perfor\u00admance on CMPs [16]. More generally, \nall shared elements of the memory subsystem, including shared caches, are in\u00adcreasingly subject to contention \nas hardware parallelism in\u00adcreases, both due to CMPs and simultaneous multithreading. Memory subsystems \non modern processors support in\u00adtense memory activity when accessesexhibit either:a)ahigh degreeof locality,orb)no \nlocality whatsoever.Acachehi\u00aderarchy ensures accesses that exhibit good temporal local\u00adity within a cache \nblock have low latency. Modern hardware prefetchers hide latencywhen programs exhibit predictable spatial \nlocality.For accesses lacking temporal locality, non\u00adtemporal streaming instructions go directly to memory \nwith higher memory throughput and do not displace useful data in the cache.  Jouppi investigated various \ncache policies and their ef\u00adfect on performance [21]. In particular, write-validate of\u00adfered the best \nperformance it combines no-fetch-on-write and write-allocate. This policy requires per-byte valid bits \nto partially instantiate cache lines. This policyfurther moti\u00advates zeroing cache lines without reading \nthem from mem\u00adory and would improve zeroing performance; however, no modern caches use it. Modern caches \nuse a write-back with fetch-on-write se\u00admantics [11, 20, 22, 25]. On a write hit, the hardware writes \nto and marks the cache line dirty. On a write miss, the hard\u00adware .rst fetches the cache line, and then \nwrites and marks it as dirty. When the cache line is evicted or is synchronized with lower-level caches, \ndirty lines are written back to the next lower level of the hierarchy. For memory references that exhibit \ngood temporal locality, write-back caches work well by reducing write transactions and speeding up mem\u00adory \nreferences. When temporal locality is poor, this design limitsthe memory throughput since,intheworst \ncase,every write generatesa storeto memoryanda cachelineloadfrom memory, which is useless in the case \nwhen the line will not be read. ISA support. Some instruction set architectures (ISAs) in\u00adclude special \ninstructions that initialize the cache without fetching data from. The PowerPC ISA includes a data cache \nblock zero(dcbz)instruction that zeros a cache-line directly without fetching it from memory [30]. The \nprocessors de\u00adsignedbyAzul[10]haveasimilar instruction,(CLZ),that di\u00adrectly zeros a cache line without \nfetching old memory. The x86 [18] ISA includes non-temporal cache bypass instruc\u00adtions for reads and \nwrites that have no temporal locality. Non-temporal store instruction such as movnti bypass the cache \nhierarchy. Theysend writes directly to memory via a write combiningbuffer withouta cache access. When \nused effectively, they have two bene.ts: a) they do not displace otherdatainthe cache,andb)theymaximize \nmemory band\u00adwidth utilization because, unlike normal stores that can gen\u00aderate one fetch and one write-back \ntransaction, non-temporal stores only generate one write transaction. However, non-temporal stores are \nexpensive when incor\u00adrectly applied to temporal data. If the target of the write is currently cached, \nthe hardware must invalidate all cache res\u00adident copies of the line, which is costly.Furthermore, if \nthe program reuses the data soon after the non-temporal write, an additional bus transaction is required \nto fetch the data. Non-temporal stores are weakly ordered, which requires that the programmer use explicit \nfences when the semantics re\u00adquire consistency of writes. Because fences are expensive, theymake non-temporal \nstores unsuitable for writes that re\u00adquire .ne-grained consistency. Ef.cient zeroing. Programming language \nand OS imple\u00admentations highly optimize zeroing, memory copying, and memory initialization.Forexample, \nthe standardC library provides the memset() function to initialize memory. Since memset() has no semantic \nknowledge of the reuse distance between the initialized memory and its next use, it resorts to a simple \nheuristic to switch to non-temporal instructions. For x86 processors, GNU sClibrary (glibc) [13] uses \nnon\u00adtemporal stores when the region being zeroed is larger than the processor s last level cache. Otherwise \nit uses standard writes. The open64 compiler [1] provides a -CG:movnti=N .ag. Whenit writestoa memory \nblocklarger thanNKB, the compiler generates non-temporal store instructions. Zero initialization strategies. \nWe examined the details of zero initialization in the open source versions of Ora\u00adcle HotSpot [23] VM. \nWe extracted further details of the Azul [10] and IBM J9 [14] JVMs from talks and publica\u00adtions. Each \nof these VMs zero initializes memory on the allocation hot path, minimizing reuse distance between ini\u00adtialization \nand .rst use. Where practical theyalso selectively zeroonly thosepartsofthe objectsthatarenotexplicitlyini\u00adtialized \nwhen they are constructed.To save memory band-width,J9and Azul VMs use dcbz and CLZ instructions when \ntargeting PPC and Azul hardware, respectively. Java s semantics require that a constructor be executed \nimmediately after each objectis allocated.Aconstructor in\u00adcludes arbitrary user code and may include \nthe explicit ini\u00adtialization of all or part of the object, resulting in a dupli\u00adcation of effort. If \nthe implicit zeroing and explicit initial\u00adization are both statically visible to an optimizing compiler, \nthe compiler can remove redundant hot-path zeroing. The opportunities for performance improvement are \nmodest be\u00adcause hardware ef.ciently elides redundant writes with good temporal locality. Correctly implementing \nthis optimization is dif.cult because it requires an analysis to guarantee that all object .elds are \ninitialized before either the program or the garbage collector observes them. The Oracle HotSpot VM implements \nsuch an optimization, but when we mea\u00adsured it, we found that it provides limited bene.t, on aver\u00adage \nonly 0.4% compared with hot-path zeroing across our benchmarks. Due to this weak result and the complexity \nin\u00advolved in implementing the optimization, we do not consider it further. Jikes RVM [4] and, optionally, \nHotSpot both bulk zero memory before providing it to the allocator. This approach forgoes temporal locality \nbetween initialization and .rst use, but minimizes the direct cost of zeroing by using a tight loop that \ncan use coarse-grained zeroing instructions to utilize available memory bandwidth. We found that the \nHotSpot implementationofbulk zeroingisextremely naive. We were able to substantially improve its performance \nby using memset() to perform the zeroing.  Benchmark Suite compress SPECjvm98 114 105 0.01 1.00 No jess \nSPECjvm98 114 265 1.03 0.99 No db SPECjvm98 114 74 0.09 1.01 No javac SPECjvm98 198 175 0.29 1.03 No \nmpegaudio SPECjvm98 78 0.21 0.00 1.00 No mtrt SPECjvm98 120 75 0.43 1.39 Yes jack SPECjvm98 102 254 0.68 \n1.00 No antlr DaCapo MR2 144 217 0.46 1.01 No avrora DaCapo Bach 300 54 0.03 2.88 Yes bloat DaCapo MR2 \n198 1096 0.59 1.02 No eclipse DaCapo MR2 480 2752 0.31 0.92 Yes fop DaCapo MR2 240 48 0.10 1.04 No hsqldb \nDaCapo MR2 762 118 0.22 0.94 Yes jython DaCapo Bach 240 1395 0.70 1.05 Yes luindex DaCapo Bach 132 34 \n0.09 0.99 Yes lusearch DaCapo Bach 204 8152 8.24 6.34 Yes lusearch-.x DaCapo Bach 204 1071 2.57 7.22 \nYes pmd DaCapo Bach 294 385 0.79 2.31 Yes sun.ow DaCapo Bach 324 1832 1.47 7.33 Yes xalan DaCapo Bach \n324 1104 1.92 7.06 Yes pjbb2005 SPECjbb2005 1200 1930 0.92 4.77 Yes Table 1. Benchmark characteristics \n3. Methodology Empirical evaluation is used throughout the remainder of this paper, .rst to provide motivating \nanalyses, then as part of a detailed analysis of existing design points, and .nally to analyze our three \nnew designs. So we now present the software, hardware, and measurement methodologies that we use. 3.1 \nSoftware platform Benchmarks. Table1 shows the benchmarks we use, the heap size we use, the total allocation \nand allocation rates of the benchmarks, their CPU utilization and whether the benchmarks are multi-threaded. \nThe zeroing workload and CPU utilization of these benchmarks is discussed in Sec\u00adtion 4. We draw the \nbenchmarks from DaCapo [6] suite, the SPECjvm98 [32] suite, and pjbb2005 [3]. (A .xed work\u00adloadversionof \nSPECjbb2005 [31] with8 warehouses that executes 10,000 transactions perwarehouse.)We use bench\u00admarks \nfrom both 2006-10-MR2 and 9.12 Bach releases of DaCapo to enlarge our suite and because a few 9.12 bench\u00admarksdo \nnotexecute on JikesRVM. We omit two outliers mpegaudio and lusearch from our .gures andaverages,but \ninclude them grayed-outin ta\u00adbles, for completeness. The mpegaudio benchmark is a very small benchmark \nthat performs almost zero allocation while lusearch allocates at three times the rate of any other. The \nlusearch benchmark derives from the 2.4.1 stable release of Apache Lucene. Investigating the source of \nits high allo\u00adcation rate, we found a performance bug in the method QueryParser.getFieldQuery(), which \nrevision r803664 of Lucene .xes [29]. The heavily executed getFieldQuery() method unconditionally allocated \na large data structure. In the .xedversion the code only allocates the large data struc\u00ad 1 static int[] \nfresh; 2 public static void initnone() { 3 for (int i=0; i < 1<<26; i++) { 4 fresh = new int[8]; 5 } \n6 } (a) initnone // 64 million 1 static int[] fresh; 2 public static void initfresh() { 3 for (int i=0; \ni < 1<<26; i++) { 4 fresh = new int[8]; 5 6 // initialize the fresh array 7 for (int j=0; j < 8; j++) \n8 fresh[j] = j; 9 } 10 } (b) initfresh // 64 million 1 static 2 static 3 public int[] fresh; int[] stale; \nstatic void initstale() { 4 stale = new int[8]; 5 for (int i=0; i < 1<<26; i++) { // 64 million 6 fresh \n= new int[8]; 7 8 // (re)initialize the stale array 9 for (int j=0; j < 8; j++) 10 stale[j] = j; 11 } \n12 } (c) initstale Figure 1. Zero initialization locality microbenchmarks ture if it is unable to reuse \nan existing one. This .x cuts to\u00adtal allocationbyafactorof eight, speedsthe benchmarkup considerablyand \ncutsthe allocation ratebyoverafactorof three.We patched the DaCapo lusearch benchmark with just this \n.x and we call the .xed benchmark lusearch-.x. The pres\u00adence of this anomaly for over a year in public \nreleases of a widely used package suggests that the behavior of lusearch is of interest and we occasionally \ncall out lusearch as anexample ofa highly allocatingworkload. Our zeroing approaches im\u00adprove the performance \nof lusearch by up to 30% on i7-2600, but we uselusearch-.x in our results. Microbenchmarks. To better \nunderstand the behavior of zeroing, we use three simple microbenchmarks, illustrated in Figure 1. The \ninitnone benchmark allocates 64 million arrays, each of eight integers. In our VM, this array consumes \n44 bytes(8\u00d74bytes plus 12 bytes of header). Theinitfresh bench\u00admark does the same, and then explicitly \ninitializes each 44 byte array immediately after allocation. This benchmark has good temporal locality \nand we use it to explore the locality effects of the zeroing strategies. The third microbenchmark, initstale, \nallocates a single array, stale before executing the tight allocation loop and explicitly (re)initializes \nstale af\u00adter each array is allocated. Explicit initialization of stale generates very little additional \nmemory traf.c, but it adds computation to the hot loop, which throttles the allocation rate. \n\t\t\t", "proc_id": "2048066", "abstract": "<p>Memory safety defends against inadvertent and malicious misuse of memory that may compromise program correctness and security. A critical element of memory safety is zero initialization. The direct cost of zero initialization is surprisingly high: up to 12.7%, with average costs ranging from 2.7 to 4.5% on a high performance virtual machine on IA32 architectures. Zero initialization also incurs indirect costs due to its memory bandwidth demands and cache displacement effects. Existing virtual machines either: a) minimize direct costs by zeroing in large blocks, or b) minimize indirect costs by zeroing in the allocation sequence, which reduces cache displacement and bandwidth. This paper evaluates the two widely used zero initialization designs, showing that they make different tradeoffs to achieve very similar performance. Our analysis inspires three better designs: (1) bulk zeroing with cache-bypassing (non-temporal) instructions to reduce the direct and indirect zeroing costs simultaneously, (2) concurrent non-temporal bulk zeroing that exploits parallel hardware to move work off the application's critical path, and (3) adaptive zeroing, which dynamically chooses between (1) and (2) based on available hardware parallelism. The new software strategies offer speedups sometimes greater than the direct overhead, improving total performance by 3% on average. Our findings invite additional optimizations and microarchitectural support.</p>", "authors": [{"name": "Xi Yang", "author_profile_id": "81490653895", "affiliation": "Australian National University, Canberra, Australia", "person_id": "P2839187", "email_address": "xi.yang@anu.edu.au", "orcid_id": ""}, {"name": "Stephen M. Blackburn", "author_profile_id": "81100547435", "affiliation": "Australian National University, Canberra, Australia", "person_id": "P2839188", "email_address": "Steve.Blackburn@anu.edu.au", "orcid_id": ""}, {"name": "Daniel Frampton", "author_profile_id": "81314488699", "affiliation": "Australian National University, Canberra, Australia", "person_id": "P2839189", "email_address": "daniel.frampton@anu.edu.au", "orcid_id": ""}, {"name": "Jennifer B. Sartor", "author_profile_id": "81100262404", "affiliation": "EPFL, Lausanne, Switzerland", "person_id": "P2839190", "email_address": "jennifer.sartor@epfl.ch", "orcid_id": ""}, {"name": "Kathryn S. McKinley", "author_profile_id": "81100402805", "affiliation": "Microsoft Research and University of Texas at Austin, Austin, USA", "person_id": "P2839191", "email_address": "mckinley@cs.utexas.edu", "orcid_id": ""}], "doi_number": "10.1145/2048066.2048092", "year": "2011", "article_id": "2048092", "conference": "OOPSLA", "title": "Why nothing matters: the impact of zeroing", "url": "http://dl.acm.org/citation.cfm?id=2048092"}