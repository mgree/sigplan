{"article_publication_date": "10-22-2011", "fulltext": "\n Trustworthy Numerical Computation in Scala Eva Darulova Viktor Kuncak * School of Computer and Communication \nSciences (I&#38;C) -Swiss Federal Institute of Technology (EPFL), Switzerland .rstname.lastname@ep..ch \nAbstract Modern computing has adopted the .oating point type as a default way to describe computations \nwith real numbers. Thanks to dedicated hardware support, such computations are ef.cient on modern architectures, \neven in double pre\u00adcision. However, rigorous reasoning about the resulting pro\u00adgrams remains dif.cult. \nThis is in part due to a large gap bet\u00adween the .nite .oating point representation and the in.nite\u00adprecision \nreal-number semantics that serves as the deve\u00adlopers mental model. Because programming languages do not \nprovide support for estimating errors, some computa\u00adtions in practice are performed more and some less \nprecisely than needed. We present a library solution for rigorous arithmetic com\u00adputation. Our numerical \ndata type library tracks a (double) .oating point value, but also a guaranteed upper bound on the error \nbetween this value and the ideal value that would be computed in the real-value semantics. Our implemen\u00adtation \ninvolves a set of linear approximations based on an extension of af.ne arithmetic. The derived approximations \ncover most of the standard mathematical operations, inclu\u00adding trigonometric functions, and are more \ncomprehensive than any publicly available ones. Moreover, while interval arithmetic rapidly yields overly \npessimistic estimates, our approach remains precise for several computational tasks of interest. We evaluate \nthe library on a number of examples from numerical analysis and physical simulations. We found it to \nbe a useful tool for gaining con.dence in the correctness of the computation. Categories and Subject \nDescriptors D.2.4 [Software En\u00adgineering]: Software/Program Veri.cation General Terms Algorithms, Languages, \nVeri.cation * This research is supported by the Swiss NSF Grant #200021_132176. Permission to make digital \nor hard copies of all or part of this work for personal or classroom use is granted without fee provided \nthat copies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. OOPSLA 11, October 22 27, 2011, Portland, Oregon, \nUSA. Copyright &#38;#169; 2011 ACM 978-1-4503-0940-0/11/10. . . $10.00 1. Introduction Numerical computation \nhas been one of the driving forces in the early development of computation devices. Floating point representations \nhave established themselves as a de\u00adfault data type for implementing software that approximates real-valued \ncomputations. Today, .oating-point-based com\u00adputations form an important part of scienti.c computing \napplications, but also of cyber-physical systems, which rea\u00adson about the quantities describing the physical \nworld in which they are embedded. The IEEE standard [59] establishes a precise interface for .oating \npoint computation. Over the past years, it has be\u00adcome a common practice to formally verify the hardware \nimplementing this standard [28, 47, 55]. On the other hand, the software using .oating point arithmetic \nremains dif.cult to reason about. As an example, consider the experiment in N-version programming [29], \nin which the largest discrepan\u00ad cies among different software versions were found in nume\u00adrical computation \ncode. One of the main dif.culties in dealing with numerical code is understanding how the approximations \nperformed by the individual arithmetic operation steps (precisely speci.ed by the standard) compose into \nan overall error of a com\u00adplex computation. Such roundoff errors can accumulate to the point where the \ncomputed value is no longer a precise enough approximation of the real value. Currently, the deve\u00adlopers \nhave no reliable automated method to determine these approximation errors. It is striking that, for many \nimportant program correctness properties we now have type systems and static analyzers that can establish \nthem over all execu\u00adtions [7, 15, 19, 22, 36, 37, 54], whereas for numerical errors, we lack practical \nmethods to estimate errors even for one, given, execution. Namely, it is dif.cult to determine whether \na given execution is correct in the sense that its .nal result is close to the result of the corresponding \nsequence of ope\u00adrations in real analysis. In program analysis terminology, we observe a great desire \nin the community to expand veri.ca\u00adtion techniques to numerical code [39]. At the same time, we do not \neven know yet how to test numerical code. Com\u00adpounding the problem is that the correctness of these app\u00adlications \nis dif.cult to asses by manual inspection (whether we try to inspect the source code or the computed \nresult). Both formal and informal reasoning about programs with .oating-points is therefore challenging. \nAs a result, we have little con.dence in the correctness of an increasingly impor\u00adtant set of applications \nthat reason about the real world.  To remedy this unfortunate situation, we introduce an easy-to-use \nsystem for estimating roundoff errors. Our sys\u00adtem comes in the form of new data types for the Scala \n[49] programming language. These data types act as a drop-in replacement for the standard .oating point \ndata types, such as Double. They offer support for a comprehensive range of operations, with a greater \nprecision than in any previously documented solution. We deploy our techniques in Scala for easy use \non many platforms, although they apply in any pro\u00adgramming language using .oating-point computation. \nWhen faced with the problem of .oating point approxi\u00admation errors, many existing approaches propose \ninterval arithmetic (IA) as a solution. However, intervals give too pessimistic estimates in many cases. \nThe problem is easy to demonstrate; its essence can be seen already on a very simple example. If x is \nan interval [0,a], then interval arithmetic ap\u00adproximates the expression x - x with [-a, a], although \nit is, in fact, always equal to zero. Essentially, interval arithmetic approximates x-x in the same way \nas it would approximate x - y when x and y are unrelated variables that both belong to [0,a]. Furthermore, \nwhen such approaches are used to es\u00adtimate the behavior over a range of input values, they fail to distinguish \ntwo sources of uncertainty: uncertainty in the error between the ideal and the .oating point value; \n uncertainty in the actual values of .oating point variables when analyzing code, if the initial values \ncan belong to any point in a given interval.  Any approach that lumps together these two sources of \nun\u00adcertainty will quickly become imprecise. To avoid the above problems, we .rst examine af.ne arithmetic, \nwhich was introduced in [17] and can more pre\u00ad cisely track the relations between variables. It turns \nout, however, that af.ne arithmetic by itself cannot be as ea\u00adsily adapted for reasoning about roundoff \nerrors as inter\u00adval arithmetic, because it uses mid-points of intervals for its estimates of nonlinear \nfunctions, and roundoff errors in the end-points of intervals can be greater than for the mid\u00adpoint value. \nWe describe a methodology that we used to derive the appropriate sound approximations. (The actual approximation \nrules that we use are publicly available in our system s source code.) Building on these, we de.ne a \ndata type that tracks a .oating-point computation and provides, in addition to the computed value, a \nguaranteed estimate on the roundoff error committed. Furthermore, we introduce an approach that allows \nthe library to track errors over a range of values. We can therefore answer both of the following questions: \nWhat is an upper bound on the roundoff error of the result of a .oating-point computation run, for a \nconcrete input? What is the maximum roundoff error of the result, for inputs ranging over a given input \ninterval? By introducing a freely available library that addresses these questions, we provided developers \nand researchers with an easy-to-use tool that helps them understand .oating-point properties of code, \na tool that provides sound guarantees on the .oating-point roundoff errors committed. We expect that \nour system can be easily integrated into veri.cation and testing systems in the future. Contributions. \nWe make the following contributions: We develop and implement an Af.neFloat data type that supports \ntesting of concrete numerical computa\u00adtions against their real-valued semantics. Our data type computes \npractically useful error bounds while retaining compatibility with the standard Double data type: not \nonly are the operations entirely analogous, but the underlying Double value that it computes is identical \nto the one com\u00adputed with the standard Double type alone. This compa\u00adtibility is important in practice, \nbut requires changes to the way roundoff errors and af.ne forms are supported compared to the existing \ntechniques. As a safe-guard, our technique falls back onto intervals when the linear appro\u00adximation is \nnot appropriate. Furthermore, our solution goes beyond the (very few) available af.ne arithmetic implementations \nby accurately supporting a substantial set of non-linear and transcendental functions. The li\u00adbrary also \nimplements a technique to soundly bound the number of af.ne error terms, ensuring predictable perfor\u00admance \nwithout sacri.cing much precision.  We develop and implement a SmartFloat data type that generalizes \nAf.neFloat to estimate upper bounds on roundoff errors over an entire range of input values. SmartFloat \nalso accepts user-speci.ed errors on input vari\u00adables (arising from, e.g. physical measurements, or iter\u00adative \nnumerical methods). Thanks to SmartFloat, the de\u00adveloper can show, using a single program run, that the \nroundoff error within the entire interval remains small. Existing methods that merge initial interval \nwidth with roundoff estimates cannot perform such estimates. We also provide a nested af.ne implementation, \nwhich uses a linear function of input to represent error terms them\u00adselves. This technique provides an \nimproved estimate of relative errors for the input ranges that contain zero.  We evaluate the precision \nand performance of our im\u00adplementation on a number of benchmarks from physics simulations and numerical \nanalysis, including: Nbody and spring simulations, spectral norm computation, the Scimark, Fbench and \nWhetstone benchmarks [4, 42, 50, 61]. The results show that our library produces (possi\u00ad bly after an \ninitial interval subdivision) precise estimates that would otherwise require expensive constraint solving \ntechniques. It also shows that the library scales to long\u00adrunning computations.   Our implementation \nis available at http://lara.epfl.ch. Paper outline. We continue by illustrating our system through two \nexamples. We then provide a quick overview of the basic af.ne arithmetic approach (Section 3). This back\u00ad \nground gives the high-level idea of the approach, but is not suf.cient to obtain our results. We characterize \nthe precision and the performance of our implementation in Section 4. We show further applications enabled \nby our system in Sec\u00adtion 5. We then present the new techniques that we introduced to achieve our results: \n.rst for Af.neFloat (Section 6) and then for SmartFloat (Section 7). We describe the integration into \nScala in Section 8, .nishing with related work and conclusions.  2. Examples Cube root. Intervals have \nthe unfortunate property of ig\u00adnoring correlations between variables and thus often over\u00adapproximate \nroundoff errors by far too much to be useful. As an illustration, consider the following code fragment \nthat uses Halley s method [57] to compute the cube root of a = 10, starting from an initial value of \nxn = 1.6: for (i + 1 until 5) xn = xn * ((xn*xn*xn + 2.0*a) / (2.0*xn*xn*xn + a)) Compare the results \ncomputed with Double against the re\u00adsult to 30 digits precision from a popular computer algebra system \n(denoted CAS), and the result returned by interval arithmetic: Double 2.1544346900318834 CAS 2.154434690031883721... \nInterval [2.1544346900317617, 2.154434690032006] It turns out that the Double value differs from the \ntrue real re\u00adsult only in the very last digit, which amounts to an absolute error on the order of unit \nin the last place, . 4.44 8 10-16 . Interval arithmetic however, would quantify this error as . 1.23 \n8 10-13. On the other hand, using our af.ne-arithmetic\u00adbased type we compute an absolute error of 1.34 \n8 10-15 , which is (by the correctness of our approach) sound, yet two decimal orders of magnitude more \nprecise than the result in interval arithmetic. If we relied only on intervals, we might be led to believe \nthat we cannot compute the value with the desired precision using Halley s method. We might have thus \ndecided to (unnecessarily) adopt a more expensive computa\u00adtional method, even though Halley s method \nactually worked .ne. Area of a triangle. As another example, consider the code in Figure 1. triangleTextbook \ncomputes the area of a triangle using the well-known textbook formula. On the other hand, triangleKahan \nuses an improved version by Kahan [34]. Run\u00ad ning both versions with our SmartFloat type and with inter\u00advals, \nwe get the results listed in Table 1. def triangleTextbook(a: SmartFloat, b: SmartFloat, c: SmartFloat): \nSmartFloat = { val s = (a + b +c)/2.0 sqrt(s * (s - a) * (s - b) * (s - c)) } def triangleKahan(a: SmartFloat, \nb: SmartFloat, c: SmartFloat): SmartFloat = { if(b < a) { val t=a if (c < b) {a = c; c= t } else { if \n(c < a) {a = b; b = c;c = t} else { a= b;b = t} }} else if (c <b){ val t = c;c = b; if (c < a) {b = a; \na= t } else {b=t} } sqrt((a+(b+c)) * (c-(a-b)) * (c+(a-b)) * (a+(b-c))) / 4.0 } Figure 1. Code for \ncomputing the area of a triangle using the classic textbook formula and Kahan s improved version. The \nlatter sorts the triangle sides by their lengths (a being the smallest) and refactors the .nal formula \nsuch that computa\u00adtions are minimized, and performed in an order that mini\u00admizes precision loss. A.ne \n2.1544346900318834 \u00b1 1.34 \u00b7 10-15 Interval Arithmetic area rel.roundoff triangleTextbook a = 9.0, b \n= c = [4.71, 4.89] a = 9.0, b = c = [4.61, 4.79] a = 9.0, b = c = [4.501, 4.581] [6.00, 8.96] [4.32, \n7.69] [0.42, 3.93] ? ? ? triangleKahan a = 9.0, b = c = [4.71, 4.89] a = 9.0, b = c = [4.61, 4.79] a \n= 9.0, b = c = [4.501, 4.581] [6.13, 8.79] [4.41, 7.54] [0.42, 3.89] ? ? ? SmartFloat triangleTextbook \na = 9.0, b = c = [4.71, 4.89] a = 9.0, b = c = [4.61, 4.79] a = 9.0, b = c = [4.501, 4.581] [6.25, 8.62] \n[4.50, 7.39] [0.41, 3.86] 1.10e-14 1.97e-14 1.95e-12 triangleKahan a = 9.0, b = c = [4.71, 4.89] a = \n9.0, b = c = [4.61, 4.79] a = 9.0, b = c = [4.501, 4.581] [6.25, 8.62] [4.49, 7.39] [0.39, 3.86] 3.11e-15 \n5.26e-15 5.07e-13 Table 1. Area and relative roundoffs computed on the code from Figure 1 with SmartFloat \nand intervals for selected va\u00adlues. Although interval arithmetic does not over-approximate the range \nby much more than af.ne arithmetic on this parti\u00adcular example, it fails to quantify the roundoff errors. \nBased only on intervals, it is impossible to tell that one version of the code behaves better than the \nother. Our SmartFloat on the other hand shows an improvement of about one order of magnitude in favor \nof Kahan s formula. Also note that the computed roundoff errors indicate that for thin triangles re\u00adlative \nroundoff errors grow, which is indeed what happens. This illustrates that our library allows for both \nformal rea\u00adsoning (by establishing correspondence to real-valued se\u00admantics), as well as high-level informal \nreasoning and ana\u00adlysis.  Using our implementation of SmartFloat s, we obtain not only a more accurate \ninterval for the result, but in fact an up\u00adper bound on the error across the entire input interval. In \nin\u00adterval arithmetic, one could in principle use the width of the actual interval as the roundoff error \nbound, but this would yield unrealistically large errors. In this particular exam\u00adple the bound on roundoff \nerrors is more than 1014 times smaller than the actual width of the interval in which the output ranges! \nTherefore, any attempt to use an interval-like abstraction to simultaneously represent 1) the input range \nand 2) the error bound, will spectacularly fail. In contrast, our technique distinguishes these different \nquantities, and is among the .rst ones to do so. Thanks to this separation, it can establish that roundoff \nerror is small even though the in\u00adterval is relatively large.  3. A Quick Tour of Interval and Af.ne \nArithmetic Throughout this paper, we use the following general nota\u00adtion: F denotes .oating-point values; \nif not otherwise stated, in double precision (64 bit).  R denotes (mathematical) real numbers.  IF, \nIR denote the sets of all closed intervals of .oating\u00adpoint and real numbers, respectively; an interval \nis given by its two endpoints.  [a] is a notation to denote the interval represented by an expression \na, according to some speci.ed semantics.  .x. and txt denote the result of some expression x rounded \ntowards -0 or +0 respectively. That is, if x * R is a number not representable in a given .oating point \nformat, .x. evaluates to the next smaller number representable in binary. Similarly, txt evaluates to \nthe nearest larger .oating-point number. If x has a .oating point representation, then .x. = txt = x. \n3.1 IEEE Floating-point Arithmetic Throughout this paper we assume that .oating-point arith\u00admetic conforms \nto the IEEE 754 .oating-point standard [59]. Recent general-purpose CPUs conform to it, and it is also \ngenerally respected in main programming languages. The JVM (Java Virtual Machine), on which Scala runs, \nsupports single-and double-precision .oating-point values accord\u00ading to the standard, as well as rounding-to-nearest \nrounding mode [41]. Also by the standard, the basic arithmetic ope- J rations {+, -, 8, /, } are rounded \ncorrectly, which means that the result from any such operation must be the closest representable .oating-point \nnumber. Hence, provided there is no over.ow, the result of a binary operation in .oating\u00adpoint arithmetic \n.F satis.es x.F y =(x.R y)(1+\u00df), |\u00df| .M , .*{+, -, 8,/} (1) where .R is the ideal value in real numbers \nand .M is the machine epsilon that determines the upper bound on the relative error. This model provides \na basis for our roundoff error estimates. Thanks to dedicated hardware .oating-point units, .oating-point \ncomputations are fast, and our library is cur\u00adrently set up for double-precision .oating-point values \n(i.e. .M =2-53). This is also the precision of choice for most numerical algorithms. It is straightforward \nto adapt our tech\u00adniques for single precision, or any other precision with an analogous semantics. 3.2 \nInterval Arithmetic One possibility to perform guaranteed computations in .oating-point arithmetic is \nto use standard interval arith\u00admetic [48]. Interval arithmetic computes a bounding interval for each \nbasic operation as x .F y =[ .(x . y). , t(x . y)t ] (2) Rounding outwards guarantees that the interval \nalways con\u00adtains the real result and thus ensures soundness. The error for square root is computed analogously. \nSection 2 already illustrated how quickly interval arith\u00ad metic becomes imprecise. This is a widely recognized \nphe\u00adnomenon; to obtain a more precise approximation, we there\u00adfore use af.ne arithmetic.  3.3 Af.ne \nArithmetic Af.ne arithmetic was originally introduced in [17] and de\u00ad veloped to compute ranges of functions \nover the domain of reals, with the actual calculations done in double (.nite) pre\u00adcision. Af.ne arithmetic \naddresses the dif.culty of interval arithmetic in handling correlations between variables. It is one \npossible range-based method to address this task; we discuss further methods in Section 9. Given a function \nf : R = R, we wish to compute its approximation in terms of .oating-point numbers. Let A be a set of \nrepresentations of intervals, with [a] * IR for a * A. The goal is then to compute an approximation of \nf by a function g : A = A that satis.es the fundamental invariant of range analysis: PROPERTY 1. For \na * A, x * R, if x * [a], then f (x) * [g(a)]  Note that a range-based arithmetic as such does necessarily \nattempt to quantify the roundoff errors itself. A possible ap\u00adplication of af.ne arithmetic, as originally \nproposed, is .nd\u00ading zeroes of a function in a given initial interval. The idea is to bisect the initial \ninterval and to compute an estimate of the function value over each subdomain. If the output range for \none subdomain does not include a zero, then that part of the domain can be safely discarded. Af.ne arithmetic \nrepresents possible values of variables as af.ne forms n formula for calculating the roundoff for composed \nexpres\u00adsions (e.g. . 8 x0 + .). We determine the maximum round\u00adoff error of an expression f (v1,...,vm) \nusing the following procedure [17]: z = f (v1,v2,...,vm) z-o = .f (v1,v2,...,vm). z+o = tf (v1,v2,...,vm)t \nL = max(z+o - z, z - z-o) x = x0 + i=1 where x0 denotes the central value (of the represented in\u00adterval) \nand each noise symbol .i is a formal variable de\u00adnoting a deviation from the central value, intended \nto range over [-1, 1]. The maximum magnitude of each noise term is given by the corresponding xi. Note \nthat the sign of xi does not matter in isolation, it does, however, re.ect the re\u00adlative dependence between \nvalues. For example, take x = x0 + x1.1, then in real number semantics, x - x = x0 + x1.1 - (x0 + x1.1) \n= x0 - x0 + x1.1 - x1.1 =0 If we subtracted x = x0 -x1.1 instead, the resulting interval would have width \n2 8 x1 and not zero. The range represented by an af.ne form is computed as n [ x]=[x0 - rad( x),x0 + \nrad( x)], rad( x) = L L |xi| xi.i That is, the program computes three results: (1) the .oating\u00adpoint \nresult z using rounding to-nearest, (2) the result z-o assuming worst-case roundoff errors when rounding \ntowards -0, and the analogous result z+o with rounding towards +0 at each step. As the worst-case committed \nroundoff error L we use the maximum difference (L) between these values. A possible use of af.ne arithmetic \nfor keeping track of roundoff errors is to represent each double precision value by an af.ne form. That \nis, the actually computed double precision value is equal to the central value and the noise terms collect \nthe accumulating roundoff errors. One expects to obtain tighter bounds than with interval arithmetic, \nespe\u00adcially when a computation exhibits many correlations bet\u00adween variables. However, a straightforward \napplication of af.ne arithmetic in the original formulation is not always sound, as we show in Section \n6. Namely, the standard af.ne arithmetic takes the liberty of choosing a convenient central i=1 When \nimplementing af.ne forms in a program, we need to take into account that some operations are not performed \nexactly, because the central value and the coef.cients need to be represented in some .nite (e.g. double) \nprecision. As suggested in [17], the roundoff errors committed during the computation can be added with \na new fresh noise symbol to the .nal af.ne form. A general af.ne operation .x + .y + . consists of addition, \nsubtraction, addition of a constant (.) or multiplication by a constant (., .). Expanding the af.ne forms \nx and y we get n .x + .y + . =(.x0 + .y0 + .)+ L (.xi + .yi).i + L.n+1 i=1 value in a range, which does \nnot preserve the compatibility with Double. Using such computation on non-af.ne opera\u00adtions (such as \ndivision or trigonometric functions) can shift the central value away from the actually computed Double \nvalue. Roundoff errors computed using this method would be those of a different computation and would \nthus be un\u00adsound. Our implementation therefore provides a modi.ed approximation that ensures soundness. \nBefore proceeding with the description of the technique we use in our solution, we show in the next section \nhow the library behaves in practice. The library provides two data types, Af.neFloat and SmartFloat, \nthat replace ordinary .oating-point numbers in a program, track a computation and provide estimates on \nthe roundoff errors committed. (3) with ., ., . * F and where L denotes the accumulated internal errors, \nthat is, the roundoff errors committed when computing the individual terms of the new af.ne form. Each \noperation carries a roundoff error and all of them must be taken into account to achieve the parameter \nL for the rigorous bound. The challenge hereby consists of account\u00ading for all roundoff errors, but still \ncreating a tight approxi\u00admation. While for the basic arithmetic operations the round\u00adoff can be computed \nwith Equation 1, there is no such simple An Af.neFloat variable represents exactly one .oating-point \nnumber and thus replaces the computation one-to-one, a SmartFloat variable represents a range of values \nand com\u00adputes the maximum roundoff error over this range. The tech\u00adnical implications of this difference \nwill be described in de\u00adtail in Section 6 and Section 7. Note that whereas SmartFloat must compute worst-case \nroundoff errors over the entire interval, Af.neFloat only needs to do this for one value. Af.neFloat \ncan therefore generally provide a more precise estimate.   4. Evaluation of Precision and Performance \nWe have selected several benchmarks for evaluating our library. Many of them were originally written \nin Java or C; we ported them to Scala as faithfully as possible. Once written in Scala, we found that \nchanging the code to use our Af.neFloat type instead of Double is a straightforward process and needs \nonly few manual edits. Scala compiler s type checker was particularly helpful in this process. Many of \nthe existing benchmarks we adopted were ori\u00adginally developed for performance and not numerical preci\u00adsion \nevaluation. We hope that our library and examples will stimulate further benchmarking with precision \nin mind. The benchmarks we present are the following:1 Nbody simulation is a benchmark from [4] and \nis a simula\u00ad tion that should model the orbits of Jovian planets, using [a] (...) simple symplectic-integrator \n. Spectral norm is a benchmark from [4] and should calcu\u00ad late the spectral norm of an in.nite matrix \nA, with entries 11111 a11 =1, a12 = , a21 = , a13 = , a22 = , a31 = , 23456 etc. Scimark [50] is \na set of Java benchmarks for scienti.c com\u00ad putations. We selected three benchmarks that best suit our \npurpose: the Fast Fourier Transform (FFT), Jacobi Suc\u00adcessive Over-relaxation (SOR) and a dense LU matrix \nfactorization to solve the matrix equation Ax = b. The exact dimensions of the problems we used are noted \nin Table 6. Fbench was orginally written by Walker [60] as a Trigonometry Intense Floating Point Benchmark \n. We used the Java port [61] for our tests. Whetstone [42] is a classic benchmark for performance evaluation \nof .oating-point computations. Spring simulation is our own code from Figure 3, however for benchmarking \nwe removed the added method errors. We have also implemented an interval arithmetic type that can, in \nthe same way as Af.neFloat and SmartFloat, replace all Double types in a program. This type is used throughout \nthis paper when comparing our library to interval arithmetic. 4.1 Af.neFloat Precision Because Af.neFloats \nrepresent exactly one .oating-point value, we can compare its precision in computing roundoff errors \nto that of interval arithmetic, where each value is ana\u00adlogously represented by one interval. The width \nof the re\u00adsulting interval provides the roundoff error. Table 2 presents our measurements of precision \non three of our benchmarks. These results provide an idea on the or\u00adder of magnitude of roundoff error \nestimates, as well as the scalability of our approach. For the Nbody problem we com\u00ad 1 All benchmarks \nare available from http://lara.epfl.ch/w/smartfloat . Benchmark rel. error AF rel. error IA SOR 5 iter. \n2.327e-14 4.869e-14 SOR 10 iter 4.618e-13 3.214e-12 SOR 15 iter 8.854e-12 2.100e-10 SOR 20 iter 1.677e-10 \n1.377e-8 NBody, initial energy 5.9e-15 6.40e-15 Nbody, 1s, h=0.01 1.58e-13 1.28e-13 Nbody, 1s, h=0.0156 \n1.04e-13 8.32e-14 Nbody, 5s, h=0.01 2.44e-10 7.17e-10 Nbody, 5s, h=0.015625 1.42e-10 4.67e-10 Spectral \nnorm 2 iter 1.8764e-15 7.1303e-15 Spectral norm 5 iter 4.9296e-15 2.4824e-14 Spectral norm 10 iter 7.5071e-15 \n5.6216e-14 Spectral norm 15 iter 1.0114e-14 8.8058e-14 Spectral norm 20 iter 1.7083e-14 1.1905e-13 Table \n2. Comparison of the relative errors computed by Af.neFloat and interval arithmetic. actual error Af.neFloat \nIA with pivoting LU 5x5 2.22e-16 1.04e-13 6.69e-13 LU 10x10 8.88e-16 7.75e-12 2.13e-10 LU 15x15 4.44e-16 \n6.10e-10 1.92e-8 no pivoting LU 5x5 1.78e-15 2.50e-11 1.24e-9 LU 10x 10 5.77e-15 2.38e-10 4.89e-6 LU \n15x15 7.15e-13 - - FFT 512 1.11e-15 9.73e-13 6.43e-12 FFT 256 6.66e-16 3.03e-13 2.38e-12 Table 3. Maximum \nabsolute errors computed by Double, Af.neFloat and interval versions for the LU factorization and FFT \nbenchmarks. The matrices were random matrices with entries between 0 and 1. pute the energy at each step, \nwhich changes due to method errors but also due to accumulated roundoffs. For the Spec\u00adtral norm we measure \nthe roundoff error of the result after different numbers of iterations. In the case of SOR, the re\u00adported \nerrors are average relative errors for the matrix en\u00adtries. Because we do not have a possibility to obtain \nthe hypothetical real-semantics results, we compare the errors against the errors that would be computed \nwith interval arithmetic. Note that none of these benchmarks is known to be particularly unstable for \n.oating-point errors, so that we cannot observe some particularly bad behavior. We can see though that \nexcept for the second and third (short) run of the Nbody benchmark our Af.neFloat gives consistently \nbetter bounds on the roundoff errors. The numbers for the SOR benchmark also suggest that the library \nscales better on longer computations. Table 3 shows measurements of precision with AffineFloat for those \nbenchmarks. These results can actually be checked knowing the properties of this particular application. \nIn our example application, an LU factorization of the matrix A is used to compute the solution to the \nsystem of linear equations Ax = b, with b a vector. From the solution x we can compute Ax and the actual \nroundoff errors committed as Ax - b. Note, that because Ax - b is a vector, we only consider the maximum \nroundoff error from the entries. This error is then compared to the maximum roundoff error attached to \nx when the solution is computed with Af.neFloats and intervals. For the FFT benchmark, we can compute \nthe transform and its inverse and compare it to the original input. We again compare the maximum roundoff \nerrors from the matrix entries. We applied the LU factorization to random matrices with and without pivoting.2 \nWe compared the error bounds against interval arithmetic and the actual error. (Note that the computation \nof the error for the LU transform involves some multiplication, hence these error bounds are not very \nprecise themselves.) Our Af.neForm can show the pivoting approach to be clearly more accurate and provides \nconsistently better bounds than interval arithmetic. For LU factorization of size 15x15 both af.ne and \ninterval arithmetic compute bounds that are too large to be useful.  In general, the type of computation \nhas a strong in.u\u00adence on how fast the over-approximation of error bounds grows. Af.ne as well as interval \narithmetic compute larger roundoff bounds for longer computations, because they ac\u00adcumulate worst-case \nerrors at each step. We have shown that Af.neFloats limit this over-approximation better and provide \nsmaller bounds than interval arithmetic. In addition, our li\u00adbrary detects the rare cases when a computation \nis precise and then includes no new error.  4.2 SmartFloat Precision In contrast to Af.neFloat, one \nSmartFloat variable represents a whole interval of values and computes the worst-case round\u00adoff error \nover the entire interval. Doppler example. For an evaluation of the SmartFloat type, consider the Doppler \nfrequency shift. The following equa\u00adtion computes the frequency change due to the Doppler ef\u00adfect dv \n-(331.4+0.6T )v z == du (331.4+0.6T + u)2 by decomposing it into the following sub-calculations: q1 = \n2 331.4+0.6T , q2 = q1v, q3 = q1 + u, q4 = q3 , z = q2/q4. The parameters used are -30\u00b0CT 50\u00b0C, 20Hz \nv 20000Hz and -100m/s u 100m/s. We compare the results to [35], who chose an SMT-based approach, and \nsummarize them in Table 4. We can compare not only the range bounds but also the roundoff errors to the \nminimum number of bits required as determined in [35]. Our estimates show precisely which calculations \nrequire more precision, namely the ones with the largest roundoff errors. 2 Pivoting attempts to select \nlarger elements in the matrix during factoriza\u00adtion to avoid numerical instability. B-splines example. \nNow consider the B-spline basic func\u00adtions commonly used in image processing [33] B0(u) = (1 - u)3/6 \nB1(u) = (3u 3 - 6u 2 + 4)/6 B2(u)=(-3u 3 +3u 2 +3u + 1)/6 B3(u)= u 3/6 with u * [0, 1]. Zhang et al. \n[62] use these functions to test their new way to approximate non-linear multiplication in af.ne arithmetic. \nIn line with the use case for testing (subsection 5.3), we use SmartFloat to estimate the ranges and \nroundoffs of these functions on the given input interval. For this purpose, we divide the input interval \ntwice and four times respectively. Observe the results in Table 5, where we compare the computed bounds \nagainst the ones from [62], and intervals (with the same dividing procedure). [62] im\u00ad proves the multiplication \nalgorithm for af.ne arithmetic, but can only provide the .nal ranges, whereas our SmartFloat is able \nto bound the roundoff errors of the results as well. Their strategy is sophisticated, but also computationally \nex\u00adpensive. However, Table 5 shows that with a suitable stra\u00ad tegy, SmartFloat can indeed produce very \nuseful and precise results while at the same time being ef.cient. 4.3 Performance Our technique aims \nto provide much more information than ordinary .oating point execution while using essentially the same \nconcrete execution. We therefore do not expect the performance to be comparable to that of an individual \ndou\u00adble precision computation on dedicated .oating-point units. Nonetheless, our technique is effective \nfor unit testing and for exploring smaller program fragments one at a time. The runtimes of Af.neFloat \nand SmartFloat are summa\u00adrized in Table 6. The SmartFloat uses the extra higher-order information as \ndescribed in subsection 7.4, which accounts for the larger runtimes. Note that the computations are long \nrunning (indicated by the operation count of a double pre\u00adcision computation). This is more than any \nof the tools we know can handle, yet the runtimes remain acceptable. Simi\u00adlarly, total memory consumption \nin these benchmarks was not measurably large.  4.4 Compacting of Noise Terms Existing af.ne arithmetic \ndescriptions generally give no guidelines on how to choose bounds on the number of linear terms used \nin the approximation and how to compact them once this number is exceeded. Our algorithm for compacting \nnoise symbols is described in subsection 6.10. In this para\u00ad graph we brie.y describe its effect on performance. \nWe ran experiments with Af.neFloat on all our benchmarks and con\u00adcluded that in general the runtime grows \nand precision in\u00adcreases according to the maximum number of noise symbols allowed. The results are summarized \nin Table 7 and Figure 2.  AA [35] SMT [35] bits [35] SmartFloat (outward-rounded) abs. roundoff q1 \nq2 q3 q4 z [313, 362] [-473252, 7228000] [213, 462] [25363, 212890] [-80, 229] [313, 362] [6267, 7228000] \n[213, 462] [45539, 212890] [0, 138] 6 23 8 18 8 [313.3999,361.4000] [6267.9999,7228000.0000] [213.3999,461.4000] \n[44387.5599,212889.9600] [-13.3398,162.7365] 8.6908e-14 3.3431e-09 1.4924e-13 1.6135e-10 6.8184e-13 \nTable 4. Doppler example from [35]. Our values were rounded outwards to 4 digits. The third column indicates \nthe minimum number of bits needed to compute the result. true ranges ranges [62] Intervals 2 div. Intervals \n4 div. SmartFloat 2 div. SmartFloat 4 div. errors for 4 div. B0 B1 [0, 1 6 ] [ 1 6 , 2 3 ] [-0.05, 0.17] \n[-0.05, 0.98] [0, 0.1667] [-0.2709, 0.9167] [0, 0.1667] [-0.1223, 0.6745] [-0.0079, 0.1667] [0.0885, \n0.8073] [-3.25\u00b710-4 , 0.1667] [0.1442, 0.6999] 1.43e-16 6.98e-16 B2 B3 [ 1 6 , 2 3 ] [- 1 6 , 0] [-0.02, \n0.89] [-0.17, 0.05] [0.0417, 1.1042] [-0.1667, 0] [0.1588, 0.9558] [-0.1667, 0] [0.1510, 0.8230] [-0.1667, \n0.0261] [0.1647, 0.7097] [-0.1667, 0.0033] 7.2e-16 1.3e-16 time 358s < 1s < 1s < 1s < 1s Table 5. B-splines \nwith SmartFloat compared against intervals and [62]. The errors given are absolute errors. double interval \nAf.neFloat SmartFloat Nbody (100 steps) 2.1 21 779 33756 Spectral norm (10 iter.) 0.6 31 198 778 Whetstone \n(10 repeats) 1.2 2 59 680 Fbench 0.2 1.3 10 1082 Scimark -FFT (512x512) 1.2 18 1220 39987 Scimark -SOR \n(100x100) 0.8 25 698 127168 Scimark -LU (50x50) 2.6 30 2419 4914 Spring sim. (10000 steps) 0.2 46 1283 \n4086 + -*/, . trig 9530 3000 14542 2006 0 4020 0 4020 4002 0 1470 510 600 110 0 115 120 89 94 13806 15814 \n19438 37 36 8416 1 19209 0 0 0 45425 44100 99 0 20002 20003 30007 10002 0 Table 6. Running times (in \nms) of our set of benchmarks, compared against the running time in pure doubles. The numbers on the right \ngive the numerical operation count of each benchmark, i.e. the number of operations a double precision \ncomputation performs. Tests were run on a Linux machine with 2.66GHz and 227MB of heap memory available. \n Figure 2. The effect of the number of noise symbols on the average running time (in ms). 20 40 60 80100 \nNbody 1.60e-13 1.58e-13 1.57e-13 1.55e-13 1.56e-13 Spectral 1.94e-14 9.25e-15 6.63e-15 8.73e-15 5.94e-15 \nFbench 3.09e-13 1.28e-13 9.48e-14 1.20e-13 5.22e-14 FFT 1.17e-16 1.63e-16 1.17e-16 1.57e-16 2.04e-16 \nSOR 1.17e-16 1.17e-16 1.32e-16 1.32e-16 1.32e-16 LU 1.16e-16 1.18e-16 1.54e-16 1.14e-16 2.09e-16 Spring \n1.81e-09 1.77e-09 1.73e-09 1.69e-09 1.64e-09 Table 7. The effect of the number of noise symbols on accuracy. \nThe peaks in the runtime graph for very small thresholds can be explained by the library spending too \nmuch time com\u00adpacting than doing actual computations. The irregular peaks for the SOR and FFT benchmarks \nillustrate that sometimes the precise characteristics of the calculation problem can in\u00ad.uence running \ntimes. Every operation s runtime is roughly proportional to the number of noise symbols, and different \ncompacting thresholds change the number of noise sym\u00adbols at each operation. It can then happen that \nwith a larger threshold the compacting happens just before a critical oper\u00adation, e.g. one that is executed \noften, but for a lower thresh\u00adold the compacting happens earlier and by the time that oper\u00adation is reached \nthe number of symbols has grown again and thus the overall running time is longer. It is thus necessary \nthat the user has some control over the compacting proce\u00addure. Note that the precision is not signi.cantly \naffected in most of our experiments, hence the library has a default limit of around 40 noise symbols \nas a good compromise between performance and accuracy.3 The developer may change this value for particular \ncalculations. Summary. We have demonstrated in this section that our library provides an improvement \nover currently used tech\u00adniques. In the examples we examined, it provided compara\u00adble results to those \nfrom a SMT solver, at a fraction of the time (see Table 4). Furthermore, it often provides dramati\u00ad cally \nbetter results compared to intervals. An interval subdi\u00advision bene.ts our approach just as it bene.ts \ninterval ana\u00adlysis. Note, however, that systematic interval subdivision is not feasible when the inputs \nare, e.g., matrices, as in Table 3. Namely, the total number of subdivided points increases ex\u00adponentially \nwith the number of elements in the matrix, so the total cost quickly dwarfs the added computational cost \nof af.ne arithmetic. In such cases, our af.ne arithmetic app\u00adroach is essential to obtain additional \norders of magnitude more precision over interval arithmetic.  5. Further Applications Our SmartFloat \ntype can be used to soundly estimate ranges of .oating-point numbers, roundoff errors over an entire \nrange of .oating-point numbers, or both. So far, we have only discussed immediate applications of these \ntypes. 3 Actually, the number used by default is 42. In this section we suggest further possible use \ncases, which also point to a possible integration of our tool into larger frameworks. 5.1 User-De.ned \nError Terms The ranges of a computation are determined chie.y by in\u00adput intervals and roundoff errors \nincurred along a computa\u00adtion path. However, errors can come from different sources as well. For instance, \nduring the integration of an ordinary differential equation the numerical algorithm accumulates method \nerrors, i.e. errors due to the discrete nature of the in\u00adtegration algorithm. One, albeit simple, example \nwhere this is the case is the simulation of a (undamped and unforced) spring in Figure 3. For simplicity, \nwe use Euler s method. Although this method is known to be too inaccurate for many applications, it provides \na good application showcase for our library. The comparison failed! line is explained in subsection 5.2, \nfor now note the method addError in line 14. In this example, we compute a coarse approximation of the \nmethod error by computing the maximum error over the whole execution. What happens behind the scenes \nis that our library adds an additional error to the af.ne form represen\u00adting x, i.e. it adds a new noise \nterm in addition to the errors already computed. Now consider the output of the simulation using our \nli\u00adbrary. Notice that using step sizes 0.1 and 0.01, time t can\u00adnot be computed precisely, whereas using \nt =0.125, which is representable in binary, the result is exact. Now consider x. We can see that choosing \nsmaller step sizes, the enclo\u00adsure of the result becomes smaller and thus more accurate, as expected. \nBut note also, that the use of a smaller step size also increases the overall roundoff errors. This is \nalso to be expected, because we have to execute more computations. Note that this precise analysis of \nroundoff errors is only possible with the separation of roundoff errors from other uncertainties. Our \nSmartFloat type can thus be used in a more general framework that guarantees soundness with respect to \na .oating-point implementation but that also includes other sources of errors.  5.2 Robustness Our library \ncan also show code to be robust in certain cases. A computation is robust if small changes in the input \ncause only small changes in the output. There are two ways in which a program, starting from some given \ninput, can change the output. Change in input causes the control .ow to change. In this case, the program \ntakes a different branch or executes a loop more or less many times, so that it actually executes differ\u00adent \ncode, causing the output to differ. In each case, this in\u00advolves a comparison in a guard. Our library \nhandles these comparisons in a special way, by keeping a global boolean .ag which is set if a comparison \nfails. What we mean by a comparison failing is that the information computed by  def springSimulation(h: \nSmartFloat) = { 2 val k: SmartFloat = 1.0 val m: SmartFloat = 1.0 4 val xmax: SmartFloat = 5.0 var x: \nSmartFloat = xmax //curr. horiz. position 6 var vx: SmartFloat = 0.0 //curr. velocity var t: SmartFloat \n= 0.0 //curr. time 8 var methodError = k*m*xmax * (h*h)/2.0 10 while(t < 1.0) { 12 val x_next = x + \nh * vx val vx_next = vx - h * k/m * x 14 x = x_next.addError(methodError) vx = vx_next 16 t=t+h } 18 \nprintln(\"t:\" + t +\",x:\" + x) } Spring simulation for h = 0.1: comparison failed! t: [1.099,1.101] (8.55e-16) \nx: [2.174, 2.651] (7.4158e-15) Spring simulation for h = 0.125: t: [1.0,1.0] (0.00e+0) x: [2.618, 3.177] \n(4.04e-15) Spring simulation for h = 0.01: comparison failed! t: [0.999, 1.001] (5.57e-14) x: [2.699, \n2.706] (6.52e-13) Figure 3. Simulation of a spring with Euler s method. The numbers in parentheses are \nthe maximum absolute roundoff errors committed. We have rounded the output outwards for readability reasons. \naf.ne forms is not suf.cient to determine whether this value is smaller or bigger than another one. Hence, \nfor the com\u00adparison x<y, the difference x - y includes zero. The user may, when noti.ed by such a situation, \nchoose to re.ne the input intervals until no such warning occurs. In addition, the user may choose that \nthe library emits a war\u00adning (comparison failed!) as seen in Figure 3. The same comparison procedure \nis also used for the methods abs, max, min so the library sets the .ag, if it detects a robust\u00adness problem, \nfor these functions as well. Computation is numerically unstable. In this case, the control .ow may stay \nthe same, but the input range of the variables gets ampli.ed, yielding a much larger output in\u00adterval. \nThe programmer can also detect this case with our library, as he only needs to compare the input to the \nout\u00adput widths of the intervals. Note that our library only gives estimates on the upper bounds on roundoff \nerrors, but not on the lower bounds. That is, our library makes inevitably over-approximations, so the \ncomputed output interval may be larger than the true interval. However, the user can, if such a case \nis suspected, rerun the computation using AffineFloats, which in general give tighter bounds. Illustration \nof control-.ow robustness check. As an example for the .rst case of robustness problem, consider again \nthe code in Figure 3. Our library set the global com\u00ad parison .ag in two of the three runs. Because there \nis only one comparison in this code, it is clear that the (possible) problem occurred in line 12. And \nin fact, we can see that in the .rst case for h =0.1, the loop was actually executed once too many, thus \nalso giving a wrong result for the value of x. In the second case h =0.125, because the computation of \ntime is exact, the .ag is correctly not set.  5.3 Testing Numerical Code We can take SmartFloat s ability \nto detect when a program takes different paths within an input interval even further. Suppose we have \na piece of code and, for simplicity, one in\u00adput variable, for which we assume that the input is within \nsome .nite range [a, b]. To generate a set of input intervals that exercise all possible paths through \nthe program, we pro\u00adpose the following procedure: Start with the entire interval [a, b] and run the program \nwith our SmartFloat type. If the li\u00adbrary does not indicate any robustness problem, we are done and can \nread off the maximum roundoff error incurred. If it detects a possible problem, split the interval and \nrerun the program on each of the new input intervals. Repeat until no problem occurs, or until an error \nin the program is found. In addition to test inputs, SmartFloats also provide guaranteed bounds on the \nerrors for each of the paths. The splitting can, in addition to control .ow changes, also be triggered \non too large roundoff error bounds. In this way, the testing proce\u00addure can be re.ned to obtain error \nbounds for each run to a desired precision.  6. Af.neFloat Design and Implementation We will now discuss \nour contributions in developing an af.ne arithmetic library suitable for evaluating .oating-point computations. \nThe main challenge are non-linear approxi\u00admations, and this basically for two reasons. The precision \nis unsatisfactory if implemented in a sim\u00adple way.  The roundoff error estimation is not sound if using \na standard approximation method.  6.1 Different Interpretations of Computations When using a range-based \nmethod like interval or af.ne arithmetic, it is possible to have different interpretations of what such \na range denotes. In this paper we consider the following three different interpretations of af.ne arithmetic. \n  (a) Chebyshev approximation (b) Min-range approximation Figure 4. Linear pproximations of the inverse \nfunction. INTERPRETATION 1 (Original Af.ne Arithmetic). In the original af.ne arithmetic, an af.ne form \nx represents a range of real values, that is [ x] denotes an interval [a, b] for a, b * R. This is also \nthe interpretation from [17]. INTERPRETATION 2 (Exact Af.ne Arithmetic). In exact af.ne arithmetic x\u00afrepresents \none .oating-point value and its deviation from an ideal real value. That is, if a real valued computation \ncomputed x * R as the result, then for the corresponding computation in .oating-points that x * [\u00afx]. \nThe difference to Interpretation 1 is that the central value x0 has to be equal to the actually computed \ndouble value at all times. We will discuss the reason for this in subsection 6.3. INTERPRETATION 3 (Floating-point \nAf.ne Arithmetic). In .oating-point af.ne arithmetic x represents a range of .oating-point values, that \nis [ x] denotes an interval [a, b] for a, b * F. Interpretation 3 corresponds to our SmartFloat type \nand Interpretation 2 to Af.neFloat, the details of which are dis\u00adcussed in this section. Usually implementation \nissues are of minor interest, how\u00adever in the case of .oating-point computations they are an important \naspect: our tool itself uses .oating-point values to compute roundoff errors, so that we are faced with \nthe very same problems in our own implementation that we are trying to quantify.  6.2 Nonlinear Operations \nAf.ne operations are computed as n L .x + .y + . =(.x0 + .y0 + .)+ (.xi + .yi).i + L.n+1 i=1 For nonlinear \noperations like multiplication, inverse, or square root, this formula is not applicable so that the ope\u00adrations \nhave to be approximated. Multiplication is derived from expanding and multiplying two af.ne forms: n \nL x y = x0y0 +(x0yi + y0xi).i +(r + L).n+1 i=1 where L contains the internal errors and r an over\u00adapproximation \nof the nonlinear contribution. To compute the latter, several possibilities exist of varying degree of \naccu\u00adracy. In the case of tracking a single .oating-point value, the simplest way r = rad( x) \u00b7 rad( \ny) is suf.cient as the radii will be in general several orders of magnitude smaller than the central \nvalues. For larger ranges, the nonlinear part of multiplication unfortunately becomes a notable problem \nand is discussed in subsection 7.4. Division y is computed as x/ x \u00b7 (1/y ) so that it remains only to \nde.ne unary nonlinear function approximations. For the approximation of unary functions, the problem \nis the following: given f ( x), .nd ., ., \u00df * F such that [f ( x)] . [.x + . \u00b1 \u00df] . and . are determined \nby a linear approximation of the function f and \u00df represents all (roundoff and approximation) errors \ncommitted, thus yielding a rigorous bound. [17] suggests two approximations for computing ., ., and \u00df: \na Chebyshev (min-max) or a min-range appro\u00adximation. Figure 4 illustrates these two on the example of \nthe inverse functionf ( x)= x -1. For both approxima\u00adtions, the algorithm .rst computes the interval \nrepresented by x =[a, b] and then works with its endpoints a and b. In both cases we want to compute \na bounding box around the result, by computing the slope (.) of the dashed line, its intersection with \nthe y-axis (.) and the maximum deviation from this middle line (\u00df). This can be done in the following \ntwo ways:  Min-range Compute the slope . at one of the endpoints a or b. Compute the intersections of \nthe lines with this slope that go through either a or b. Fix . to be the average of the two. Compute \n\u00df as the maximum deviation, which occurs by construction at either a or b. Chebyshev Compute the slope \n. of the line through both a and b. This gives one bounding side of the wanted box (parallelepiped). \nTo .nd the opposite side, compute the point where the curve takes on the same slope again. Again, compute \n. as the average of the intersections of the two lines and \u00df as the maximum deviation at either the middle \npoint v, a or b. In general, the Chebyshev approximation computes smaller parallelepipeds, especially \nif the slope is signi.cantly differ\u00adent at a and b. However, it also needs the additional com\u00adputation \nof the middle point. Especially for transcendental functions like acos, asin, etc., this can involve \nquite com\u00adplex computations which are all committing internal round\u00adoff errors. On big intervals, like \nthe one considered in [17] and [18] these are (probably) not very signi.cant. How\u00ad ever, when keeping \ntrack of roundoff errors, our library deals with intervals on the order of machine epsilon. From the \nexperience with several versions of transcendental func\u00adtion approximations we concluded that min-range \nis the bet\u00adter choice. Chebyshev approximations kept returning unex\u00adpected and wrong results. As discussed \nin [18], the Cheby\u00ad shev approximation would be the more accurate one in long running computations, however \nwe simply found it to be too numerically unstable for our purpose. To our knowledge, this problem has \nnot been acknowledged before. Obviously, any linear approximation is only valid when the input range \ndoes not cross any in.ection or extreme points of the function. Should this occur, our library resorts \nto computing the result in interval arithmetic and converting it back into an af.ne form. Error estimation \nfor nonlinear library functions like log, exp, cos, etc. requires specialized rounding, because the returned \nresults are correct to 1 ulp (unit in the last place) only [1] (for the standard Scala math library), \nand hence are less accurate than elementary arithmetic operations, which are correct to within 1/2 ulp. \nThe directed rounding pro\u00adcedure is thus adapted in this case to produce larger error bounds, so that \nit is possible to analyze code with the usual Scala mathematical library functions without modi.cations. \n 6.3 Guaranteeing Soundness of Error Estimates What we have described so far applies to the original \naf.ne arithmetic as well as our Af.neFloat. However, our goal is to quantify roundoff errors, and original \naf.ne arithmetic has not been developed to quantify them, only to compute sound bounds (i.e. intervals) \non output values, interpreted over ranges of real numbers. It turns out that if af.ne arith\u00admetic is \nmodi.ed appropriately, it can be used for the quan\u00adti.cation of roundoff errors as in Interpretation \n2. For this, we assume that the central value x0 is exactly the .oating\u00adpoint value computed with double \nprecision and the noise symbols xi represent the deviation due to roundoff errors and approximation inaccuracies \nfrom non-af.ne operations. A straight-forward re-interpretation of af.ne arithmetic from Section 6 is \nnot sound as the following observation shows. OBSERVATION 2. The algorithm for approximating non\u00adaf.ne \noperations using the min-range approximation as de\u00ad.ned in subsection 6.2 is unsound under the interpretation \nof Interpretation 2. Namely, the interpretation of af.ne arithmetic as in Interpre\u00adtation 2 relies on \nthe assumption that the central value x0 is equal to the .oating-point value of the original compu\u00adtation. \nThis is important, as the roundoff for af.ne opera\u00adtions is computed according to Equation 1, i.e. by \nmultipli\u00ad cation of the new central value by some \u00df. If the central value does not equal the actual .oating-point \nvalue, the computed roundoff will be that of a different result. Af.ne operations maintain this invariant. \nHowever, non-af.ne operations de\u00ad.ned by computing ., . and \u00df such that the new af.ne form is z = . \u00b7 \nx + . + \u00df.n+1 do not necessarily enforce that the actual double value computed in the operation is equal \nto the new central value z0 = . \u00b7 x0 + .. That is, in general (and in most cases), the new z0 will be \nslightly shifted. In general the shift is not large, however soundness cannot be guaranteed any more. \nFortunately, an easy solution exists and is illustrated in Figure 5. For non-linear operations, the new \ncentral value is computed as z0 = .\u00b7x0 +. and we want f (x0)= .\u00b7x0 +.. Hence, our library computes . \nas . = f (x0) - . \u00b7 x0 The min-range approximation computes for an input range [a, b] an enclosing parallepiped \nof a function as . \u00b7 x + . \u00b1 \u00df that is guaranteed to contain the image of the nonlinear function from \nthis interval as computed in .oating-point precision. Suppose that . = f (x0)-.\u00b7x0, with . computed at \none of the endpoints of the interval. Because we compute the deviation \u00df with outwards rounding at both \nendpoints and keep the maximum, we soundly over-approximate the function f in .oating-point semantics. \nClearly, this approach only works for input ranges where the function in question is monotonic. By the \nJava API [1], the implemented library functions are guaranteed to be semi-monotic, i.e. whenever the \nreal function is non-decreasing, so is the .oating-point one. It is clear from Figure 5 that our modi.ed \napproximation computes a bigger parallelepiped than the original min-range approximation. However, in \nthis case, the intervals are very small to begin with, so the over-approximations do not have a big effect \non the precision of our library.  Figure 5. Modi.ed min-range approximation of the inverse function. \n 6.4 Double-Double Precision for Noise Terms It turns out that even when choosing the min-range appro\u00adximation \nwith input ranges with small widths (order 10-10 and smaller), computing the result of a nonlinear function \nin interval arithmetic gives better results. The computation of . and . in our approximation cannot be \nchanged for sound\u00adness reasons, but it is possible to limit the size of \u00df. In order to avoid arbitrary \nprecision libraries for performance rea\u00adsons, our library uses double-double precision (denoted as DD) \nas a suitable compromise. Each value is represented by two standard double precision values. Algorithms \nhave been developed and implemented [16, 51] that allow the compu\u00ad tation of standard arithmetic operations \nwith only ordinary .oating-point operations, making the performance trade-off bearable. In this way, \nour library can compute range reduc\u00adtions for the sine and cosine functions accurately enough. It can \nalso avoid using intervals to bound \u00df, an approach we found not to be suf.ciently effective for our purpose. \nOne condition for these algorithms to work is that the operations are made in exactly the order as given \nand with\u00adout optimizations or fused-multiply instructions. We have enforced this for our code (which \nruns on the JVM) by using the strictfp modi.er for the calculations. Our library uses double-double precision \ntypes for the noise symbols and computations involving them. Keeping the noise symbols in extended precision \nand thus reduc\u00ading also the internal roundoff errors, we have found that the accuracy of our library \nincreased suf.ciently for most nonlinear function approximations. To ensure soundness of double-double \ncomputations, we use the outward rounding mode. 6.5 Precise Handling of Constants A single value, say \n0.03127, is represented in a real valued interval semantics as the point interval [0.03127, 0.03127] \nor in af.ne arithmetic as x =0.03127, i.e. without noise terms. This no longer holds for .oating-point \nvalues that cannot be represented exactly in the underlying binary representa\u00adtion. Our library tests \neach value for whether it can be re\u00adpresented or not and adds noise terms only when necessary. In the \ncase of the above example, it creates the following af.ne form: 0.03125+(.M \u00b70.03125).n. This limits \nthe over\u00adapproximations committed and provides more precise anal\u00adyses when possible. For an error estimate \naccording to Inter\u00adpretation 2, our runtime library has the exact values available and can thus generally \ncompute tighter bounds compared to a static analysis-based approach. 6.6 Computing Roundoff Errors The \nJVM does not provide access to the different round\u00ading modes of the .oating-point unit, so that the expressions \nthat need directed rounding are implemented as native C me\u00adthods. It turns out that this approach does \nnot incur a big per\u00adformance penalty, but provides the needed precision, which cannot be achieved by \nsimulated directed rounding. The na\u00adtive C code has to be compiled for each architecture sep\u00adarately, \nbut because no specialized functionality is needed this is a straightforward process and does not affect \nthe por\u00adtability of our library. Using directed rounding also enables the library to determine when a \ncalculation is exact so that no unnecessary noise symbols are added. 6.7 Soft Policy to Avoid Too Many \nFalse Warnings Our solution follows the soft policy advocated in [17], whereby slight domain breaches \nfor functions that work only on restricted domains are attributed to the inaccuracy of our over-approximations \nand are ignored. For example, with a hard policy computing the square root of [-1, 4] results in a run-time \nerror, as the square root function is not de.ned on all of the input interval. It is possible however \nthat the true interval (in a real semantics) is [0, 4] and the domain problem is just a result of a previous \nover-approximation. In order to not interrupt computations unnecessarily with false alarms, a soft policy \ncomputation will give the result [0, 2]. Note, that our library nonetheless generates warnings in these \ncases if the user chooses so, so that the policy only affects the tool s ability to continue a computation \nin ambiguous cases, but not its rigorousness.  6.8 Correctness The correctness of each step of the interval \nor af.ne arith\u00admetic computation implies the correctness of our overall approach: for each operation \nin interval or af.ne arithmetic the library computes a rigorous over-approximation, and thus the overall \nresult is an over-approximation. This means, that for all computations, the resulting interval is guaranteed \nto contain the result that would have been computed on an ideal real-semantics machine. The use of assertions \ncertifying that certain invariants always hold support the correctness of our implementation. Example \ninvariants for AffineFloat include the statement that the computed double precision value has to be exactly \nthe same as the central value of the af.ne form, a prerequisite for our roundoff analysis.  In addition, \nwe have tested our library extensively on se\u00adveral benchmarks (see Section 4) and our implementation \nof nonlinear functions against the results from 30 digit pre\u00adcision results from Mathematica. We are \nable to avoid several pitfalls related to .oating\u00adpoint numbers [9, 46] by writing our library in Scala \nand not for example in C, as the JVM is not as permissive to optimizations that may alter the actual \nexecution of code.  6.9 Using a Library on an Example To illustrate the use of Af.neFloat we present \nits use on a classic example, the quadratic formula in Figure 6. This example illustrates the effect \nof roundoff errors because it produces less accurate results (two orders of magnitude in this particular \ncase), when one root is much smaller. Our library shows the result of rewriting this code following the \nmethod in [24]. Our library con.rms that both roots are now computed with approximately the same accuracy: \nclassic r1 = -18.655036847834893 (5.7133e-16), r2 = -0.0178874602678082 (1.4081e-13) smarter r1 = -18.655036847834893 \n(5.7133e-16), r2 = -0.0178874602678077 (7.7584e-16) The values in parentheses give the relative errors. \nNote that the code looks nearly the same as if it used the standard Double type, thanks to the techniques \nwe used to integrate it into Scala (see section 8).  6.10 Managing Noise Symbols in Long Computations \nThe runtime performance of our library depends on the num\u00adber of noise terms in each af.ne form, because \neach opera\u00adtion must access each term at least once. Hence, an appro\u00adpriate compacting strategy of noise \nsymbols becomes crucial for performance. Compacting too little means that our app\u00adroach becomes unfeasible, \nwhereas compacting too much means the loss of too much correlation information. Compacting algorithm. \nThe goal of compaction is to take as input a list of noise terms and output a new list with fewer terms, \nwhile preserving the soundness of the roundoff er\u00adror approximation and, ideally, keeping the most important \ncorrelation information. Our library performs compaction by adding up the absolute values of the smallest \nterms and intro\u00adducing them as a fresh noise symbol along with the remain\u00ading terms. We propose the following \nstrategy to compute the fresh noise term: Compact all error terms smaller than 10-33. These errors are \nsmaller than the smallest double value and are thus internal errors. Our library can manipulate such \nsmall values because it uses double-double precision internally (section 6.4).  Compute the average \n(avrg) and the standard deviation (stdDev) of the rest of the noise terms. Compact all terms smaller \nthan avrg \u00b7a+ stdDev \u00b7b and keep the rest.  var a = Af.neFloat(2.999) var b = Af.neFloat(56.0001) var \nc = Af.neFloat(1.00074) val discr = b * b - a * c * 4.0 //classical way var r2 = (-b + sqrt(discr))/(a \n* 2.0) var r1 = (-b - sqrt(discr))/(a * 2.0) println(\"classic r1 =\" + r1 +\",r2 = \" + r2 ) //smarter way \nval (rk1: Af.neFloat, rk2: Af.neFloat) = if(b*b - a*c > 10.0) { if(b > 0.0) ((-b - sqrt(discr))/(a * \n2.0), c * 2.0 /(-b - sqrt(discr))) else if(b < 0.0) (c * 2.0 /(-b + sqrt(discr)), (-b + sqrt(discr))/(a \n* 2.0)) else ((-b - sqrt(discr))/(a * 2.0), (-b + sqrt(discr))/(a * 2.0)) } else { ((-b - sqrt(discr))/(a \n* 2.0), (-b + sqrt(discr))/(a * 2.0)) } println(\"smarter r1 = \" + rk1 + \", r2 = \" + rk2) Figure 6. Quadratic \nformula computed in two different ways. The factors a and b are user-controllable positive param\u00adeters, \nand can be chosen separately for each computation. (The result is sound regardless of the particular \nvalues.) In some cases the above steps are still not enough to en\u00adsure that the number of symbols is \nbelow the threshhold. This occurs, for example, if nearly all errors have the same magnitude. If our \nlibrary detects this case, it re\u00adpeats the above procedure one more time on the newly computed noise \nterms. In our examples, at most two iter\u00adations were suf.cient. In pathological cases in which this does \nnot suf.ce, the library compacts all noise symbols into a single one.   7. SmartFloat Design and Implementation \nThe implementation described in Section 6 provides a way to estimate roundoff errors for one single computation. \nIt provides reasonably tight bounds for the most common ma\u00adthematical operations and is fast enough for \nmiddle sized computations, hence it can be used to provide some intuition about the behavior of a calculation. \nIt does not provide, how\u00adever, any guarantee as to how large the errors would be if one chose (even slightly) \ndifferent input values or constants. In this section we investigate the following two aspects: 1. computation \nof a rigorous range of .oating-point values (according to Interpretation 3);  2. computation of sound \nroundoff error estimates for this range. Unfortunately, a straightforward reinterpretation of neither \nthe original af.ne arithmetic, nor the modi.ed version for Af.neFloat gives a sound range arithmetic \nfor .oating-point numbers. OBSERVATION 3. The roundoff computation of af.ne ope\u00adrations as de.ned in \nEquation 3 is unsound under the inter\u00ad pretation of Interpretation 3. Namely, when tracking a range \nof .oating-point numbers and computing the roundoff errors of each computation, we need to consider the \nroundoff errors for all values in the range, not only the central value as is the case in Equation 3. \nIn addition, the non-linear approximation algorithm does not explicitly compute the roundoff errors, \nthey are implicitly included in the computed \u00df. If we now have input values given by (possibly wide) \nranges, the computed \u00df will be so large that no roundoff estimate from them is meaningful. Our library \nprovides a new type, SmartFloat as a solution for this problem. A SmartFloat can be constructed from \na double value or a double value with an uncertainty, providing thus a range of inputs. A SmartFloat \nvariable x , then keeps the initial double value and the following tuple L L L x = (x0, xi.i + xiui, \nriPi), xi, ri * DD (4) where x0 * F is the central value as before and xi.i and xiui are the noise terms \ncharacterizing the range. We now mark those that come from user-de.ned uncertainties by spe\u00adcial noise \nsymbols ui, which we call uncertainties. We keep these separately, so that for instance during the noise \nterm compacting, these are preserved. riPi are the error terms quantifying the roundoff errors committed. \nThe sum|ri|gives a sound estimate on the current maximum commit\u00adted roundoff error for all values within \nthe range. We now need to de.ne the computation and propagation of roundoff errors; the noise terms are \nhandled as before. 7.1 Computation of Roundoffs To compute the roundoff error of an operation, our library \n.rst computes the new range. It uses either Equation 3 for af.ne operations or the min-range approximation \nfor non\u00adlinear ones and then computes the maximum roundoff from the resulting range. Following the de.nition \nof roundoff from Equation 1 the maximum roundoff is the maximum absolute value in the range multiplied \nby .M . For the other operations, correct to within 1 ulp, we adjust the factor to 2 \u00b7 .M . 7.2 Propagation \nof Roundoffs The already committed errors e x =riPi in some af.ne form x have to be propagated correctly \nfor each operation. Af.ne. The propagation is given straightforwardly by Equation 3. That is, if the \noperation involves the computa\u00ad tion .x + .y + ., the errors are transformed as .e x + .e y + (L + .)Pn+1, \nwhere L corresponds to the internal errors com\u00admitted and . to the new roundoff error. Multiplication. \nThe linear part is computed as usual by multiplication by x0 and y0. The non-linear part in multi\u00adplication \nposes the dif.culty that it involves cross-terms bet\u00adween the noise and error terms. We derive the new \npropa\u00adgation, by appending the error terms to the noise term sum and then compute the multiplication. \nThen, removing those terms not involving any error terms, we get the new re: re = rad( x) \u00b7 rad(ey)+ \nrad( y) \u00b7 rad(ex)+ rad(ex) \u00b7 rad(ey) Note that this produces an over-approximation, because some of the \nerrors from the error terms are also included already in the noise terms. Non-af.ne. Because the nonlinear \nfunction approxima\u00adtions compute ., . and \u00df, the propagation of errors reduces to an af.ne propagation, \nwith one exception. The factor used to propagate the roundoff errors must be, instead of ., the maximum \nslope of the function over the given input range to ensure soundness. Because this value does not necessarily \nequal ., we need to compute that factor separately. 7.3 Additional Errors Additional errors, e.g. method \nerrors from a numerical in\u00adtegration, can be added to the af.ne form in the following way. Given x =(x0,xi.i,riPi) \nand the error to be added y =(y0,yi.i,siPi), the resulting af.ne form is given by L L z =(x0,xi.i +(|y0| \n+ rad( yi.i)).n+1, LL riPi, +(rad( siPi)Pm+1) That is, the maximum magnitude of the error is added as \na new noise term, and the maximum magnitude of the round\u00adoff committed when computing this error is added \nas a new error term.  7.4 Treatment of Range Explosion due to Multiplication In Section 6 the naive \ncomputation of the non-linear part for multiplication was suf.ciently accurate due to the relatively \nsmall radii of the involved af.ne forms. This is in general no longer the case if we consider arbitrary \nranges of .oating\u00adpoint numbers. To illustrate this problem, consider x = 3+2.1 and y = 4+3.2 Both values \nare clearly positive, hence their product should be positive as well. Now, z = x \u00b7 y = 12+8.1 +9.2 +6.3 \nwhich gives as resulting interval [ z]=[-11, 35]. This result is unacceptable, if this value is subsequently \nused in for instance division. [58, 62] have suggested some approaches, however they either change the \nunderlying structure by using matrices in\u00adstead of af.ne forms or are simply not scalable enough. Be\u00adcause \nwe do not want to change the underlying data struc\u00adture, we chose a different solution for this problem. \nThe  nn problem is, that by computing r =( |xi|) \u00b7 ( |yi|) i=1 i=1 and appending it with a fresh noise \nsymbol, correlation in\u00adformation is lost. Af.ne forms do not provide the possibi\u00adlity to keep quadratic \nterms. We can keep, however, source information with each noise term. For example, if a noise term is \ncomputed as x1x2.1.2, it results in a new fresh noise symbol x3.3[1, 2], where the indices in brackets \ndenote the information that is additionally stored. Similarly, if the prod\u00aduct involves two noise terms \nthat already contain such infor\u00admation, it is combined. Currently, our library supports up to 8 indices, \nhowever this value can be extended as needed -at a performance cost of course. Most operations work exactly \nas before; this information is only used when the interval of an af.ne form is computed and is essentially \nan optimiza\u00adtion problem. One option is to use a brute force approach and to substitute all possible \ncombinations of -1, 1, 0 for all .i. Because an af.ne form represents a convex range of va\u00adlues, the \nmaximum and minimum value of this range has to necessarily be at .i having one of these values. Clearly, \nthis approach is not very ef.cient, but for up to 11 noise terms is still feasible. We use our compacting \nalgorithm to reduce the number of noise symbols before this optimization is run to make this approach \nef.cient enough in practice. The example from Section 4.2 demonstrates the impact of this simple solution \non the Doppler shift problem. [35] choose an SMT-based approach, precisely for the reason that af.ne \narithmetic produces too large over-approximations. We compare the results from the approaches in Table \n4. Note that our library obtains its results in under half a second. Clearly, the library could compute \nbetter bounds if a better optimization method is used, for instance by using a dedicated solver.  7.5 \nNested Af.ne Arithmetic An even smarter version of SmartFloat can provide informa\u00adtion on how the output \nroundoff error depends on the input error, thus providing additional insight about the computa\u00adtion. \nThis is possible, provided that roundoff errors are com\u00adputed as functions of the initial uncertainties, \nand not just absolute values. Note that if we restrict ourselves to linear functions, we can use af.ne \nforms for the new roundoffs. That is, for the af.ne form representing the roundoff errors e x = riPi, \nthe library now keeps each ri as an af.ne form (i.e. af.ne form of af.ne forms). It keeps only the lin\u00adear \nterms of uncertainties (and not the noise terms from the computation itself) and compacts all other terms \nfor perfor\u00admance reasons. Finally, the computation of the actual round\u00adoff errors becomes an optimization \nproblem similar to the one for multiplication. Our library currently reports the as\u00adsignment that minimizes \nand maximizes the roundoff errors. Note that due to over-approximations this reported assign\u00adment may \nnot necessarily be the one giving the real smallest or largest roundoff. For this reason, the user may \nwant to examine the, say, three smallest or largest assignments res\u00adpectively. Although this feature \nis so far only experimental, we believe that it can become very useful. We will demonstrate this by returning \nto the triangle example from Table 1. With the modi.ed SmartFloat, we can now run the following code: \nval area = triangleArea(9.0, SmartFloat(4.7, 0.19), SmartFloat(4.7, 0.19)) area.analyzeRoundoff The output \nis analyzing the roundoffs... maximum relative error: 4.728781774296841E-13 maximizing assignment: 10 \n-> -1.0, 7 -> -1.0 minimum relative error: 8.920060990068312E-14 minimum assignment: 10 -> 1.0, 7 -> \n1.0 To explain this output, the numbers 10 and 7 denote the in\u00addices of the uncertainties that were assigned \nto b and c res\u00adpectively, that is, those are the indices of their uncertainty noise symbols. The .nal \nanalysis reveals that for the assign\u00adment of -1.0 to both noise symbols, the roundoff is maxi\u00admized. \nLooking back at the de.nition of the values we can see that the assignment of -1.0 corresponds to the \ninput value of 4.51 for both b and c. This corresponds exactly to the known property that the relative \nroundoff errors are largest for the thinnest triangles. Similarly, the assignment of 1.0 corresponds \nto the least thin triangles, as expected.  8. Integration into a Programming Language This section explains \nhow we integrated Af.neFloat and SmartFloat data types into Scala in a seamless way. Our de\u00adcision to \nimplement a runtime library was in.uenced by se\u00adveral factors. Firstly, a runtime library is especially \nuseful in the case of .oating-point numbers, because the knowledge of exact values enables us to provide \na much tighter analysis that cannot be achieved in the general case in static analysis. Also, with our \ntight integration it is possible to use any Scala construct, thus not restricting the user to some subset \nthat an analyzer can handle. 8.1 Our Deployment as a Scala Library Our library provides wrapper types \nAf.neFloat and SmartFloat that track the computation errors. These types are meant to replace the Double \ntypes in the user-selected parts of a program. All that is needed to put our library into action are \ntwo import statements at the beginning of a source .le, import smart.oats.SmartFloat import smart.oats.SmartFloat._ \n and the replacement of Double types by one of the Af.neFloat or SmartFloat types. Any remaining con.icts \nare signaled by the compiler s strong typechecker. The new data types han\u00addle de.nitions of variables \nand the standard arithmetic ope\u00adrations, as well as many scala.math library functions, inclu\u00adding the \nmost useful:  log, expr, pow, cos, sin, tan acos, asin, atan  abs, max, min  constants Pi (a) and \nE (e)  The library also supports special values N aN and \u00b10 with the same behavior as the original code. \nTo accomplish such an integration, we needed to address the following issues. Operator overloading. Developers \nshould still be able to use the usual operators +, -, *, / without having to rewrite them as functions, \ne.g x.add(y). Fortunately, Scala allows xmy as syntax for the statement x.m(y) and (nearly) arbitrary \nsymbols as method names [49] , including +, -, *,/. Equals. Comparisons should be symmetric, i.e., the \nfol\u00adlowing should hold val x: SmartFloat = 1.0 val y: Double = 1.0 assert(x == y &#38;&#38; y == x) The \n== will delegate to the equals method, if one of the operands is not a primitive type. However, this \ndoes not result in a symmetric comparison, because Double, or any other built-in numeric type, cannot \ncompare itself correctly to a SmartFloat. Fortunately, Scala also provides the trait (similar to a Java \n[25] interface) ScalaNumber which has a special semantics in comparisons with ==. If y is of type ScalaNumber, \nthen both x == y and y == x delegate to y.equals(x) and thus the comparison is symmetric [52]. Mixed \narithmetic. Developers should be able to freely combine our SmartFloats with Scala s built-in primitive \ntypes, as in the following example val x: SmartFloat = 1.0 val y = 1.0 + x if (5.0 < x) {...} This is \nmade possible with Scala s implicit conversions, strong type inference and companion objects [49]. In \naddi\u00ad tion to the class SmartFloat, the library de.nes the (singleton) object SmartFloat, which contains \nan implicit conversion sim\u00adilar to implicit def double2SmartFloat(d : Double): SmartFloat = new SmartFloat(d) \n As soon as the Scala compiler encounters an expression that does not type-check, but a suitable conversion \nis present, the compiler inserts an automatic conversion from the Double type in this case to a SmartFloat. \nTherefore, implicit conver\u00adsions allow a SmartFloat to show a very similar behavior to the one exhibited \nby primitive types and their automatic con\u00adversions. Library functions. Having written code that utilizes \nthe standard mathematical library functions, developers should be able to reuse their code without modi.cation. \nOur li\u00adbrary de.nes these functions with the same signature (with SmartFloat instead of Double) in the \ncompanion SmartFloat ob\u00adject and thus it is possible to write code such as val x: SmartFloat = 0.5 val \ny = sin(x) * Pi Concise code. For ease of use and general acceptance it is desirable not having to declare \nnew variables always with the new keyword, but to simply write SmartFloat(1.0). This is possible as this \nexpression is syntactic sugar for the special apply method which is also placed in the companion object. \n 8.2 Applicability to Other Languages The techniques described in this paper can be ported to other \nlanguages, such as C/C++, or to languages speci.cally tar\u00adgeted for instance for GPU s or parallel architectures, \npro\u00advided the semantics of .oating-point numbers is well speci\u00ad.ed. In fact, language virtualization \nin Scala [53] can be used to ultimately generate code for such alternative platforms in\u00adstead of the \nJVM.  9. Related Work Estimating Roundoff Errors in Fluctuat. Closest to our work is the Fluctuat static \nanalyzer [26] which analyzes nu\u00ad merical code with operations +, -, 8,/ for roundoff errors and their \nsources based on abstract interpretation and af.ne arithmetic. A comparison solely based on the results \nre\u00adported [26] is dif.cult, because the results are obtained by using different user-supplied settings \nfor different examples (for example, using interval subdivisions, or a larger number of bits for internal \ncomputations). If we do the subdivision into tens of sub-intervals in our system (using a simple for\u00adloop \nin the user s code), we .nd that the accuracy on many of our examples increases further by a decimal \norder of mag\u00adnitude. A direct experimental comparison with the Fluctuat implementation was not possible \ndue to the commercial na\u00adture of the tool, but there are several important conceptual differences. We \ndo not view our approach as abstract interpretation, because our library is closer to the actual program \nexecu\u00adtion, and this makes it more appropriate for (and easier to use for) testing. We never perform \njoins and never approxi\u00admate objects and other data structures using alias analysis as in Fluctuat. We \ndo not expect to reach a .xpoint, because we maintain the exact .oating-point value of the numbers. Be\u00adcause \nwe only replace the .oating-point number data type and keep the structure of the data exact, we generate \nthe same trace and terminate in the same number of steps as the original program. This lack of approximation \ngives us more precision. Furthermore, our tool supports mathemati\u00adcal functions beyond the standard four \narithmetic operators.  No other work we know of documents the actual approxima\u00adtions used. As described \nin Section 6 in detail, this task turns out to be non-trivial. We also introduced the noise symbol packing \ntechnique (subsection 6.10), which allows us to sup\u00ad port longer-running computations with many .oating-point \noperations, while having stable time and memory overhead. [26] does not describe how Fluctuat deals with \nthe grow\u00ading numbers of noise symbols, nor does it indicate whether the computations performed were simply \nnot of the size that would require it, as in our benchmarks. We note that in addition to SmartFloat, \nwe also introduced Af.neFloat, which tracks the error of a speci.c computation, with concrete input points, \ninstead of an error over an inter\u00adval. Af.neFloat is even closer to concrete execution: it gives more \nprecise results for one execution (with fewer guar\u00adantees about other executions). We have further enhanced \nAf.neFloat precision by using directed rounding to recognize the cases where the error is actually zero, \ninstead using the worst case of machine epsilon relative error for each opera\u00adtion. Because of this, \nwe avoid introducing unnecessary error terms. As a result, entire series of basic arithmetic computa\u00adtions \ncan be found to have zero error. We have not seen this precision enhancement mentioned elsewhere, or \nperformed by other tools. Abstract interpretation Further work in abstract interpre\u00adtation includes the \nAstr\u00e9e analyzer [14] and APRON [13, 31]. These systems provide abstract domains that work cor\u00ad rectly \nin .oating-point semantics, but they do not attempt to quantify roundoff errors (an attempt to treat \nintervals them\u00adselves as roundoff errors gives too pessimistic estimates to be useful). In general, tools \nbased on abstract interpretation (including Fluctuat) attempt to rigorously establish global ranges for \nvariables, regardless of what these ranges corre\u00adspond to. In contrast, we are interested in the correspondence \nbetween the computed value and the ideal mathematical re\u00adsult. Another difference is that, instead of \nfocusing only on embedded domain, we are interested also in scienti.c com\u00adputations, which often have \ntranscendental functions (so we needed to develop approximation rules for them), and non\u00adtrivial data \nstructures. Data structures contain many .oating\u00adpoint numbers whose values cannot be simply approximated \nby joint values as in many static analysis approaches, other\u00adwise too much precision would be lost. A \nrecent approach [30] statically detects loss of precision in .oating-point computations using bounded \nmodel check\u00ading with SMT solvers, but uses interval arithmetic for scal\u00adability reasons. [21] uses af.ne \narithmetic to track round\u00ad off errors using a C library; this work is speci.c to the signal processing \ndomain. Further approaches to quantify roundoff errors in .oating-point computations are summa\u00adrized \nin [44], of which we believe af.ne arithmetic to be the most useful one. This also includes stochastic \nestimations of the error, which have been implemented in the CADNA library [32]. However, the stochastic \napproach does not pro\u00ad vide rigorous bounds, because, for example, in loops, round\u00adoff errors are not \nuniformly distributed. Af.ne Arithmetic. Existing implementations of af.ne arithmetic include [2] and \n[3]. Researchers [62] have pro\u00ad posed a solution to the over-approximation problem of mul\u00adtiplication \nby repeatedly re.ning the approximation until a desired precision is obtained. While the method provides \na possible solution to the over-approximation problem, it is also computationally expensive. None of \nthese implemen\u00adtations can be used to quantify roundoff errors, only to com\u00adpute ranges in the way described \nby the original af.ne arith\u00admetic [17]. As a result, the problems we describe in this pa\u00ad per do not \narise, and the existing systems cannot be used as such for our purpose. Other range-based methods are \nsur\u00adveyed in [45] in the context of plotting curves. We have de\u00ad cided to use af.ne arithmetic, because \nit seems to us to be a good compromise between complexity and functionality. A library based on Chebyshev \nand Taylor series is presented in [20], however it does not provide correlation information as af.ne \narithmetic does, so its use is directed more towards non-linear solvers. Af.ne arithmetic is used in \nseveral ap\u00adplication domains to deal with uncertainties, for example, in signal processing [27]. Our \nlibrary is developed for general\u00ad purpose calculations and integrated into a programming lan\u00adguage to \nprovide information about .oating-points for any application domain. Robustness Analysis. Our library \ncan detect the cases when the program would continue to take the same path in the event of small changes \nto the input, thanks to the use of the global sticky .ag set upon the unresolved compar\u00adisons. Therefore, \nwe believe that our library can be useful for understanding program robustness and continuity prop\u00aderties, \nfor which sophisticated techniques have been investi\u00adgated [12, 43]. Finite-Precision Arithmetic. [38] \nuses af.ne arithmetic for bit-width optimization and provides an overview of re\u00adlated approaches. [56] \nuses af.ne arithmetic with a spe\u00ad cial model for .oating-points to evaluate the difference bet\u00adween a \nreduced precision implementation and normal .oat implementation, but uses probabilistic bounding to tackle \nover-approximations. Furthermore, it only allows addition and multiplication. [35] employs a range re.nement \nmethod based on SMT solvers and af.ne arithmetic, which is one way to deal with the division-by-zero \nproblem due to over\u00adapproximations. The authors use a timeout to deal with the case when such computation \nbecomes too expensive. An\u00adother approach uses automatic differentiation [23] to sym\u00ad bolically compute \nthe sensitivity of the outputs to the inputs. While this approach could be used for roundoff error analy\u00adsis \nas well, the symbolic expressions need to be evaluated, and thus need to ultimately rely on methods such \nas interval or af.ne arithmetic.  Theorem Proving Approaches. Researchers have used theorem proving \nto verify .oating-point programs [6, 8, 28, 47, 55]. These approaches provide high assurance and gua\u00ad \nrantee deep properties. Their cost is that they rely on user\u00adprovided speci.cations and often require \nlengthy user inter\u00adactions. [40] extend previous work using af.ne arithmetic by considering the problem \nof reducing precision for perfor\u00admance reasons. However, the resulting system still requires interactive \neffort. [11] presents a decision procedure for checking satis.ability of a .oating-point formula by encod\u00ading \ninto SAT. Even this approach requires the use of approx\u00adimations, because of the complexity of the resulting \nformu\u00adlas. A symbolic execution technique that supports .oating\u00adpoint values was developed [10], but \nit does not quantify roundoff errors. There is a number of general-purpose app\u00adroaches for reasoning \nabout formulas in non-linear arith\u00admetic, including the MetiTarski system [5]. Our work can be used as \na .rst step in veri.cation and debugging of nume\u00adrical algorithms, by providing the correspondence between \nthe approximate and real-valued semantics. 10. Conclusions We have presented a library that introduces \nnumerical types, SmartFloat and Af.neFloat, into Scala. Like the standard Double type, our data type \nsupports a comprehensive set of operators. It subsumes Double in that it does compute the same .oating \npoint value. In addition, it also computes a roundoff error an estimate of the difference between this \n.oating-point value and the value of the computation in an ideal real-number semantics. SmartFloat can \ncompute the roundoff error not only for a given value, but also for the va\u00adlues from a given interval, \nwith the interval being possibly much larger than the roundoff error. It can be notoriously dif.cult \nto reason about computa\u00adtions with .oating-point numbers. Running a computation with a few sample values \ncan give us some understand\u00ading for the computation at hand. The newly developed data types allow developers \nto estimate the error behavior on en\u00adtire classes of inputs using a single run. We have found the performance \nand the precision of these data types to be ap\u00adpropriate for unit-testing of numerical computations. \nWe are therefore con.dent that our implementation is already very helpful for reasoning about numerical \ncode, and can be em\u00adployed for building future validation techniques.   References [1] DocWeb -Java \nSE 6 -java.lang.Math. http://doc.java. sun.com/DocWeb/#r/JavaSE6/java.lang.Math/ columnMain. [2] J. \nStol. s general-purpose C libraries. http://www.ic. unicamp.br/~stolfi/EXPORT/software/c/Index. html#libaa, \n2005.  [3] aa.ib -An Af.ne Arithmetic C++ Library. http:// aaflib.sourceforge.net/, 2010.  [4] The \nComputer Language Benchmarks Game. http:// shootout.alioth.debian.org/, Jan 2011.  [5] B. Akbarpour \nand L. C. Paulson. MetiTarski: An Automatic Theorem Prover for Real-Valued Special Functions. J. Autom. \nReason., 44(3), 2010. [6] A. Ayad and C. March\u00e9. Multi-Prover Veri.cation of Floating-Point Programs. \nIn IJCAR, 2010. [7] T. Ball, E. Bounimova, R. Kumar, and V. Levin. Slam2: Static driver veri.cation \nwith under 4% false alarms. In FMCAD, pages 35 42, 2010. [8] S. Boldo, J.-C. Filli\u00e2tre, and G. Melquiond. \nCombining Coq and Gappa for Certifying Floating-Point Programs. In CICM, 2009. [9] S. Boldo and T. M. \nT. Nguyen. Hardware-independent proofs of numerical programs. In Proceedings of the Second NASA Formal \nMethods Symposium, 2010. [10] B. Botella, A. Gotlieb, and C. Michel. Symbolic execution of .oating-point \ncomputations. Softw. Test. Verif. Reliab., 2006. [11] A. Brillout, D. Kroening, and T. Wahl. Mixed Abstractions \nfor Floating-Point Arithmetic. In FMCAD, 2009. [12] S. Chaudhuri, S. Gulwani, and R. Lublinerman. Continuity \nanalysis of programs. In POPL, 2010. [13] L. Chen, A. Min\u00e9, J. Wang, and P. Cousot. A Sound Floating-Point \nPolyhedra Abstract Domain. In APLAS, 2008. [14] P. Cousot, R. Cousot, J. Feret, L. Mauborgne, A. Min\u00e9, \nD. Monniaux, and X. Rival. The ASTR\u00c9E Analyser. In ESOP, 2005. [15] Manuvir Das, Sorin Lerner, and Mark \nSeigle. ESP: Path-sensitive program veri.cation in polynomial time. 2002. [16] M. Davis. DoubleDouble.java. \nhttp:// tsusiatsoftware. net/dd/main.html. [17] L. H. de Figueiredo and J. Stol.. Self-Validated Numerical \nMethods and Applications. IMPA/CNPq, Brazil, 1997. [18] L. H. de Figueiredo and J. Stol.. Af.ne Arithmetic: \nConcepts and Applications. Numerical Algorithms, 2004. [19] Isil Dillig, Thomas Dillig, and Alex Aiken. \nSound, complete and scalable path-sensitive analysis. In PLDI, 2008. [20] A. G. Ershov and T. P. Kashevarova. \nInterval Mathematical Library Based on Chebyshev and Taylor Series Expansion. Reliable Computing, 11, \n2005. [21] C.F. Fang, Tsuhan C., and R.A. Rutenbar. Floating-point error analysis based on af.ne arithmetic. \nIn ICASSP, 2003. [22] Stephen Fink, Eran Yahav, Nurit Dor, G. Ramalingam, and Emmanual Geay. Effective \ntypestate veri.cation in the presence of aliasing. In ISSTA 06, 2006. [23] A.A. Gaffar, O. Mencer, and \nW. Luk. Unifying bit-width optimisation for .xed-point and .oating-point designs. In Field-Programmable \nCustom Computing Machines, 2004. FCCM 2004. 12th Annual IEEE Symposium on, pages 79 88, april 2004. \n[24] D. Goldberg. What every computer scientist should know about .oating-point arithmetic. ACM Comput. \nSurv., 23(1), 1991.  [25] J. Gosling, B. Joy, G. Steele, and G. Bracha. Java(TM) Language Speci.cation, \nThe 3rd Edition. Addison-Wesley, 2005. [26] E. Goubault and S. Putot. Static Analysis of Finite Precision \nComputations. In VMCAI, 2011. [27] Ch. Grimm, W. Heupke, and K. Waldschmidt. Re.nement of Mixed-Signal \nSystems with Af.ne Arithmetic. In DATE, 2004. [28] J. Harrison. Formal Veri.cation at Intel. In LICS, \n2003. [29] L. Hatton and A. Roberts. How Accurate is Scienti.c Software? IEEE Trans. Softw. Eng., 20, \n1994. [30] F. Ivancic, M. K. Ganai, S. Sankaranarayanan, and A. Gupta. Numerical stability analysis of \n.oating-point computations using software model checking. In MEMOCODE, 2010. [31] B. Jeannet and A. Min\u00e9. \nApron: A Library of Numerical Abstract Domains for Static Analysis. In CAV, 2009. [32] F. J\u00e9z\u00e9quel and \nJ.-M. Chesneaux. CADNA: a library for estimating round-off error propagation. Computer Physics Communications, \n178(12), 2008. [33] J. Jiang, W. Luk, and D. Rueckert. FPGA-Based Computation of Free-Form Deformations. \nIn Field -Programmable Logic and Applications. 2003. [34] W. Kahan. Miscalculating Area and Angles of \na Needle-like Triangle. Technical report, University of California Berkeley, 2000. [35] A.B. Kinsman \nand N. Nicolici. Finite Precision bit-width allocation using SAT-Modulo Theory. In DATE, 2009. [36] Etienne \nKneuss, Philippe Suter, and Viktor Kuncak. Runtime instrumentation for precise .ow-sensitive type analysis. \nIn International Conference on Runtime Veri.cation, 2010. [37] Viktor Kuncak, Patrick Lam, Karen Zee, \nand Martin Rinard. Modular pluggable analyses for data structure consistency. IEEE Transactions on Software \nEngineering, 32(12), December 2006. [38] D.-U. Lee, A. A. Gaffar, R. C. C. Cheung, O. Mencer, W. Luk, \nand G. A. Constantinides. Accuracy-Guaranteed Bit-Width Optimization. IEEE Trans. on CAD of Integrated \nCircuits and Systems, 25(10), 2006. [39] X. Leroy. Veri.ed squared: does critical software deserve veri.ed \ntools? In POPL, 2011. [40] M. D. Linderman, M. Ho, D. L. Dill, T. H. Meng, and G. P. Nolan. Towards program \noptimization through automated analysis of numerical precision. In CGO, 2010. [41] T. Lindholm and F. \nYellin. Java Virtual Machine Speci.cation. Addison-Wesley Longman Publishing Co., Inc., 2nd edition, \n1999. [42] R. Longbottom. Whetstone Benchmark Java Version. http://www.roylongbottom.org.uk/online/ whetjava.html, \n1997. [43] R. Majumdar and I. Saha. Symbolic Robustness Analysis. In IEEE Real-Time Systems Symposium, \n2009. [44] M. Martel. An overview of semantics for the validation of numerical programs. In VMCAI, 2005. \n[45] R. Martin, H. Shou, I. Voiculescu, A. Bowyer, and G. Wang. Comparison of interval methods for plotting \nalgebraic curves. Comput. Aided Geom. Des., 19(7), 2002. [46] D. Monniaux. The pitfalls of verifying \n.oating-point computations. ACM Trans. Program. Lang. Syst., 30(3), 2008. [47] J. S. Moore, T. W. Lynch, \nand M. Kaufmann. A Mechanically Checked Proof of the AMD5 K86 Floating Point Division Program. IEEE Trans. \nComputers, 47(9), 1998. [48] R.E. Moore. Interval Analysis. Prentice-Hall, 1966. [49] M. Odersky, L. \nSpoon, and B. Venners. Programming in Scala: a comprehensive step-by-step guide. Artima Press, 2008. \n[50] R. Pozo and B. R. Miller. Java SciMark 2.0. http://math.nist.gov/scimark2/about.html, 2004. [51] \nD. M. Priest. Algorithms for Arbitrary Precision Floating Point Arithmetic. In Proceedings of the 10th \nSymposium on Computer Arithmetic, 1991. [52] Scala Programming Language Blog. == and equals. http://scala-programming-language. \n1934581.n4. nabble.com/and-equals-td2261488. html, June 2010. [53] T. Rompf and M. Odersky. Lightweight \nmodular staging: a pragmatic approach to runtime code generation and compiled DSLs. In GPCE, 2010. [54] \nP. M. Rondon, M. Kawaguci, and R. Jhala. Liquid types. In PLDI, pages 159 169, 2008. [55] D. M. Russinoff. \nA Mechanically Checked Proof of Correctness of the AMD K5 Floating Point Square Root Microcode. Formal \nMethods in System Design, 1999. [56] R. A. Rutenbar, C. F. Fang, M. P\u00fcschel, and T. Chen. Toward ef.cient \nstatic analysis of .nite-precision effects in DSP applications via af.ne arithmetic modeling. In DAC, \n2003. [57] T. R. Scavo and J. B. Thoo. On the Geometry of Halley s Method. The American Mathematical \nMonthly, 102(5), 1995. [58] H. Shou, R.R. Martin, I. Voiculescu, A. Bowyer, and G. Wang. Af.ne Arithmetic \nin Matrix Form for Polynomial Evaluation and Algebraic Curve Drawing. Progress in Natural Science, 12, \n2002. [59] IEEE Computer Society. IEEE Standard for Floating-Point Arithmetic. IEEE Std 754-2008, 2008. \n[60] J. Walker. fbench -Trigonometry Intense Floating Point Benchmark. http://www.fourmilab.ch/fbench/ \nfbench.html, 2007. [61] J. White. Fbench.java. http://code.google.com/p/ geo-reminder/source/browse/trunk/ \nbenchmark-android/ src/com/benchmark/suite/ Fbench.java?r=108, 2005. [62] L. Zhang, Y. Zhang, and W. \nZhou. Tradeoff between Approximation Accuracy and Complexity for Range Analysis using Af.ne Arithmetic. \nJournal of Signal Processing Systems, 61, 2010.  \n\t\t\t", "proc_id": "2048066", "abstract": "<p>Modern computing has adopted the floating point type as a default way to describe computations with real numbers. Thanks to dedicated hardware support, such computations are efficient on modern architectures, even in double precision. However, rigorous reasoning about the resulting programs remains difficult. This is in part due to a large gap between the finite floating point representation and the infinite-precision real-number semantics that serves as the developers' mental model. Because programming languages do not provide support for estimating errors, some computations in practice are performed more and some less precisely than needed.</p> <p>We present a library solution for rigorous arithmetic computation. Our numerical data type library tracks a (double) floating point value, but also a guaranteed upper bound on the error between this value and the ideal value that would be computed in the real-value semantics. Our implementation involves a set of linear approximations based on an extension of affine arithmetic. The derived approximations cover most of the standard mathematical operations, including trigonometric functions, and are more comprehensive than any publicly available ones. Moreover, while interval arithmetic rapidly yields overly pessimistic estimates, our approach remains precise for several computational tasks of interest. We evaluate the library on a number of examples from numerical analysis and physical simulations. We found it to be a useful tool for gaining confidence in the correctness of the computation.</p>", "authors": [{"name": "Eva Darulova", "author_profile_id": "81490689453", "affiliation": "EPFL, Lausanne, Switzerland", "person_id": "P2839192", "email_address": "eva.darulova@epfl.ch", "orcid_id": ""}, {"name": "Viktor Kuncak", "author_profile_id": "81100277693", "affiliation": "EPFL, Lausanne, Switzerland", "person_id": "P2839193", "email_address": "viktor.kuncak@epfl.ch", "orcid_id": ""}], "doi_number": "10.1145/2048066.2048094", "year": "2011", "article_id": "2048094", "conference": "OOPSLA", "title": "Trustworthy numerical computation in Scala", "url": "http://dl.acm.org/citation.cfm?id=2048094"}