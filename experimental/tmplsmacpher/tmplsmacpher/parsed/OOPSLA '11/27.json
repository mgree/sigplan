{"article_publication_date": "10-22-2011", "fulltext": "\n A Step Towards Transparent Integration of Input-Consciousness into Dynamic Program Optimizations KaiTian \nEddyZ. Zhang Xipeng Shen Computer Science Department The CollegeofWilliamandMary,Williamsburg,VA,USA \n{ktian,eddy,xshen}@cs.wm.edu Abstract Dynamic program optimizations are critical for the ef.\u00adciencyof \napplications in managed programming languages and scripting languages. Recent studies have shown that \nex\u00adploitation of program inputs may enhance the effectiveness of dynamic optimizations signi.cantly. \nHowever,current so\u00adlutions for enabling the exploitation require either program\u00admers annotations or intensive \nof.ine pro.ling, impairing the practical adoption of the techniques. This current work examines the basic \nfeasibility of trans\u00adparent integration of input-consciousness into dynamic pro\u00adgram optimizations, particularly \nin managed execution en\u00advironments. It uses transparent learning across production runs as the basic \nvehicle, and investigates the implications of cross-run learning on each main component of input\u00adconscious \ndynamic optimizations. It proposes several tech\u00adniquesto address somekeychallengesforthe transparentin\u00adtegration, \nincluding randomized inspection-instrumentation for cross-user data collection, a sparsity-tolerant algorithm \nfor input characterization, and selective prediction for ef.\u00adciencyprotection. These techniques make \nit possible to au\u00adtomatically recognize the relations between the inputs to a program and the appropriate \nways to optimize it. The whole process happens transparently across production runs; no need for of.ine \npro.ling or programmer intervention. Exper\u00adiments ona numberofJava programs demonstrate theeffec\u00adtiveness \nof the techniques in enabling input-consciousness for dynamic optimizations, revealing the feasibility \nand po\u00adtential bene.ts of the new optimization paradigm in some basic settings. Permission to make digital \nor hard copies of all or part of this work for personal or classroom use is granted without fee provided \nthat copies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page.To copyotherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. OOPSLA 11, October 22 27, 2011, Portland, Oregon, \nUSA. Copyright c . 2011ACM 978-1-4503-0940-0/11/10... $10.00 Categories and Subject Descriptors D.3.4[Programming \nLanguages]: Processors optimization,compilers GeneralTerms Languages, Performance Keywords Program inputs, \nDynamic optimizations, Java Virtual Machine, Proactivity, Seminal behaviors, Dynamic version selection, \nJust-In-Time Compilation 1. Introduction Dynamic program optimizations play a central role for en\u00adhancing \nthe performance of applications in managed pro\u00adgramming languages (e.g., Java and C#) [3], as well as \nscripting languages (e.g., Javascript) [13]. Even though re\u00admarkable progresses have been achieved, most \nexisting dy\u00adnamic optimizations have not systematically exploited an importantfactor: program inputs. \nProgram inputs refer to all the data that are accessedbut not generated by the program, including command-line \nop\u00adtions, content of input .les, and so on. Many studies have reported the importance of program inputs \nin determining program behaviors and hence appropriate optimization deci\u00adsions [19,23,32]. The strong \ncorrelation suggests the poten\u00adtial of using program inputs as hints to predict large-scoped behaviors \nof a program and hence assist dynamic optimiza\u00adtions. Recent studies [32] show that the hints from program \ninputs may help make dynamic optimizations more proac\u00adtive (e.g., optimizing a method appropriately before \nanyof its invocations) and long-sighted (i.e., optimizing for the ef\u00ad.ciencyof the entire execution rather \nthan a small interval), leading to signi.cant speedups.For instance,Tian and oth\u00aders have shown 10 29% \nspeedup for a set of Java programs and 5 13% for a number of C programs when inputs are considered in \nprogram dynamic optimizations [32], Li and others have observed 44% performance potential for sorting \nlibrary construction when certain attributes of input data sets are used [23]. However, program inputs \nare often complex a plausi\u00adble reason for the lack of exploitation of inputs in exist\u00ading dynamic optimizations. \nAn input .le, for instance, may havevarious syntactic structures and semantics (e.g.,atree,a graph,a \nvideo,a document, ora program).Toexploit inputs for optimizations, it is necessary to obtain a clean \nstructure that captures the important input features and is amenable to automatic processing. But the \ncomplexity of inputs makes thistaskextremely challenging.Someprior studieshavepro\u00adposed the use of annotations \n[25] or of.ine pro.ling-based solutions[19,32].But bothputextraburdenson program\u00admers.In addition,the \nannotation approach requiresextensive knowledge of the programmers on both the application and its interactions \nwith the underlying execution stack, while theof.ine solutions demandalarge numberof representative inputs \nand manyof.ine pro.ling runs with detailed code in\u00adstrumentation. These limitations impair the adoption \nof these solutions in practice. This current study is one step towards addressing this open question. \nIt examines the basic feasibility of transpar\u00adent integration of input-consciousness into dynamic program \noptimizations, particularly in managed execution environ\u00adments (e.g., JavaVirtual Machines) equipped \nwith Just-In-Time compilers (JIT). It uses transparent, continuous learn\u00ading across production runs as \nthe basic vehicle, proposes several techniques to address some key challenges for the transparent integration, \nand investigates the implications of cross-run learning on each main component of input\u00adconscious dynamic \noptimizations1. 2. Overviewof ThisWork As prior work [32] describes, input-conscious dynamic optimizations \nmainly consists of three components: input characterization, input-behavior modeling, and input-based \nadaptiveoptimizations. Inputcharacterization identi.es fea\u00adtures of program inputs that are important \nfor optimizations. Input-opt modeling constructs predictive models, which mapsthevaluesofthe featuresofaninputtothe \nappropriate optimization decisions for the correspondingexecution.We name the models input-opt models. \nThese two components provide the foundation for the .nal component, input-based adaptive optimizations, \nwhich feeds the values of input fea\u00adtures of the current run into the constructed input-opt models to \nguide the dynamic optimizations for the present run. In previous work, most parts of input-conscious \ndynamic optimizations, except the third component, happen in an of.ine training process, requiring the \napplication developers to conduct a large number of of.ine-pro.ling runs with detailed instrumentation \nand data collection [32]. The solution investigated in this work tries to integrate all components into \na continuous learning process that hap\u00adpens across production runs. It is fully automatic, imposing no \nspecial requirement on either application developers or users. 1We use input-conscious rather than the \nprevious term, input\u00adcentric [32], for its intuitiveness.  Figure 1. As Figure1 illustrates, in this \nparadigm, all three com\u00adponents of input-conscious dynamic optimizations mingle intoa continuouslyevolving \nprocess.Aproduction run may bene.t from the current results of input characterization and input-opt modeling \nwhen dynamic optimizers use them to help optimize the current execution. On the other hand, af\u00adter each \nproduction run, new observations are added into the database, which is used periodically (during the \nidle time of a machine) to re.ne the input characterization and input-opt models to better serve future \nruns. This cross-run learning scheme circumvents the needs for of.ine pro.ling, and hence overcomes the \nlimitations of prior solutions. But for the scheme to work effectively, several new challenges must be \naddressed for each of the three components. The challenges to input characterization are the most dif\u00ad.cult \nof all. Due to the complexity of inputs, the only ex\u00adisting solution for automatic input characterization \nis the seminal-behavior identi.cation approach, proposedbyJiang and others [19]. It however requires \nthe collection of many runtime behaviors of a large number of runs through de\u00adtailed instrumentation, \nwhich is unaffordable for produc\u00adtion runs. Runtime sampling is a natural direction. How\u00adever, how to \nachieve large coverage quickly without dis\u00adturbing performance of production runs is a challenge, es\u00adpecially \nfor seminal-behavior identi.cation because 1) it is based on statistical correlation analysis, the data \nrequiredby which is usually tremendous; 2) dynamic instrumentation is necessary, which complicates overhead \ncontrol as both the inserted instructions and the instrumentation process itself cause overhead. In this \nwork, we propose randomized inspection-instrumentation to solve the problem. The solu\u00adtion is based on \na cross-user sampling scheme [24] that is, accumulating data sampled from manyusers executions with two \nextensions. Its inspection-instrumentation mech\u00adanism helps control instrumentation-incurred overhead. \nIts randomization feature accelerates the coverage of the data collection by randomizing the coverage \nof the samples among users. Section 4.2 describes these two techniques. The data collected through the \nlightweight pro.ling tend to be sparse, causing dif.culty for the prior approach, seminal-behavior identi.cation, \nto effectively characterize program inputs. We solve the problem by developing a sparsity-tolerant algorithm, \nwhich capitalizes on the par\u00adtial overlaps of the data from different runs to incrementally propagate \nthe extracted knowledge on program inputs. Sec\u00adtion 4.3 presents the algorithm. The challenges to the \nother components of input-conscious dynamic optimizations mainly come from the continuous evolvement \nof input features and input-opt models, a phe\u00adnomenon that exists in no prior of.ine-based scheme. The \nissue is how to exert the full power of the incrementally en\u00adhanced predictive models without risking \nmuch the negative effects of prediction errors. Sections5and6describeour solutiontothisissue.Ituses self-assessment \nand selective prediction to control the risks of wrong predictions. By maintaining a con.dence value, \nit prevents immature uses of input-opt models without be\u00ading too conservative, coordinates the different \ncomponents of input-conscious optimizations, and ensembles them intoa concerted continuous optimization \nsystem to proceed trans\u00adparently and pro.tably. Experiments based on a JVM, namely Jikes RVM [3], demonstrate \nthat the proposed techniques are effective in addressing some major obstacles for transparent input\u00adconscious \ndynamic optimizations. On 18 Java benchmarks, we observe 10 26%average speedup compared to theirexe\u00adcutionsonthedefaultJikesRVM \n(Section8), outperforming the previous input-oblivious approach substantially. We stress that creating \na complete transparent input\u00adconscious dynamic optimizer that is ready to deployin prac\u00adticeis not the \ngoalof this currentwork.To reach that goal, there are manyother obstacles (e.g., differences in platforms \nand software versions as detailed in Section 7) to conquer, which require many efforts from the community \nthat are probably far beyond what can .t into a single paper. The contributions of this paper are at \nthe proposal of solutions to some of thekeyobstacles, and the demonstration of the feasibility of the \ndesired scheme in some basic setting. In summary, this work makes four main contributions. Asthe .rstexplorationtowards \ntransparent input-conscious dynamic optimizations, it reveals the main challenges and demonstrates the \nfeasibility of the paradigm in some basic settings.  It proposes randomized inspection-instrumentation \nto overcome dif.culties for data collection for automatic input characterization over production runs. \n It develops a sparsity-tolerant algorithm to enable in\u00adput characterizationover data collected across \nproduction runs.   Figure 2. The default cost-bene.t model (with simpli.ca\u00adtion for illustration purpose) \nin Jikes RVM for determin\u00ading the optimizationlevel foraJava method.(Tfuture: the estimated time the \nmethod is expected to take (if not op\u00adtimized) in the rest of the current execution. speedup(i): the \nexpected speedup of the method after being optimized at level i. compileSpeed(i): the compilation speed \nat level i.) It proposesa continuous learning framework that enables incremental evolvement of input-conscious \ndynamic op\u00adtimizations with risks tightly controlled. We organize the rest of this paper around the \nchallenges each of the three key components of input-conscious dy\u00adnamic optimizations has to meet so \nthat the entire integration of input-consciousness can occur transparentlyover produc\u00adtion runs.But .rst,wegiveabrief \ndescriptionofthe underly\u00ading platform we use as it is closely relevant to the remaining discussions. \n3. Platform: JikesRVM We use JikesRVM [3], an open-source JVM originally from IBM, as our main platform \nfor its representativeness as a dynamic optimization system.We brie.y describe some of its features that \nare closely relevant to the following sections. JikesRVM uses method-level JIT compilation. Like most \nexisting dynamic optimization systems, the optimizer in JikesRVMis reactive: Duringanexecution,it observesthe \nbehaviors of the application through sampling, whereby, it determines the importance of each Java method \nin the ap\u00adplication, and invokes the JIT compiler to (re)optimize the method accordingly. As compilation \nincurs runtime over\u00adhead, the JIT offers four compilation levels. The high-level optimizations (more \nsophisticated and hence taking more time) are supposed to be used only for important Java meth\u00adods, and \nlow-level optimizations for the others. During anexecution, the default JikesRVM usesa cost\u00adbene.t model \nto determine whether a method should be re\u00adcompiled at a higher optimization level. As shown in Fig\u00adure \n2, in the cost-bene.t model, the cost is the time needed to compile the Java method, estimated from the \nsize of the method and some predetermined compilation speeds at vari\u00adous compilation levels. This cost \ncalculation is directly used in the inspection-instrumentation scheme described in the next section. \nThe bene.t is estimated as the expected time savings in the rest of the execution because of this recom\u00adpilation. \nIn JikesRVM, there are some predetermined con\u00adstants that represent the average speedup each optimization \nlevel produces [3]. The parameter Tfuture in the bene.t formula means the timethatthe methodisexpectedtotakeintherestofthecur\u00adrentexecution. \nJikesRVM assumes that Tfuture equals the timethismethodhasalreadytaken.Asaconsequence,inone run,a methodmaybe \nrecompiledseveral timesat increasing optimizing levels, as JikesRVM realizes the importance of the method \ngradually. 4. Transparent Input Characterization Among the many challenges for transparently integrating \ninput-consciousness into dynamic optimizations, the most dif.cult ones reside in the .rst component, \ninput characteri\u00adzation. The main goal of input characterization is to reduce the raw program inputs \nto a set of features. These features criti\u00adcally determine the behaviors of the program that are essen\u00adtial \nto its performance. The dif.culty comes from the complexities in program inputs. An application may allow \nhundreds of options; those options may overshadow each other; input .les may con\u00adtain millions of data \nelements, organized in complex struc\u00adtures and representing various semantics (e.g., trees, graphs, videos). \nOur solutionis basedonarecently proposed concept, pro\u00adgram seminal behaviors.We .rst brie.y review the \nconcept and then describe our techniques for transparently character\u00adizing program inputs over production \nruns. 4.1 Review on Program Seminal Behaviors The concept of program seminal behaviors is proposed by \nJiang and others [19]. It comes from the strong statistical correlations among program behaviors. For \nexample, two loops in a program have iterations (15, 41, 52, 89, 101), and (69, 173, 217, 365, 413) in \n.ve runs respectively. Statistical analysis can easily determine that the trip-counts (i.e., the numbers \nof iterations) of these two loops have a linear rela\u00adtion as C2 =4*C1 +9,where,C1 and C2 are the trip-counts \nof the two loops. Jiang and others have shown that such sta\u00adtistical correlations widelyexist both among \nloop trip-counts and from loop trip-counts to other types of behaviors, includ\u00ading function invocations, \ndata values, and so on. Based on those observations, they developed an auto\u00admatic approach to recognizing \na small set of behaviors in a program, named a seminal behavior set. These behaviors have two properties. \nFirst, they have strong statistical cor\u00adrelations with manyother behaviors in the program so that knowingtheirvalueswouldleadto \naccurate predictionofthe valuesof otherbehaviors. Second,thevaluesof thosebehav\u00adiors becomeknowninanearlystageina \ntypicalexecutionof the program. The seminal behavior set of a SPEC CPU2006 program mcf, for example, \nis composed of 10 behaviors: the trip-counts of .ve of its loops, the values of four of its vari\u00adables \nwhose values come directly from command line argu\u00adments or input .les, and its input .le size. In all \nmeasured runs, the values of most of these seminal behaviors become known during the .rst 10% portion \nof an execution. Recall that the essence of input-conscious dynamic opti\u00admizations is to use program \ninputs as the hints to predict the behaviors of the program and guide the optimizations. The properties \nof seminal behaviors suggest that they can play the same role that program inputs play in the optimization \nparadigm. A seminal behavior set can be hence viewed as one form of characterization of program inputs. \nThe previously proposed approach to recognizing semi\u00adnal behaviors includes a pro.ling step and an analysis \nstep. The pro.ling step collects the values of some candidate be\u00adhaviors (e.g. all loop trip-counts) \nby running the program which has been instrumented in detail on many different inputs. The analysis step \ngreedily classi.es the behaviors into some af.nity lists. Let S be the set of all candidate be\u00adhaviors. \nIn each iteration of the analysis step, one behavior b is taken out of S. Using the data collected in \nthe pro.ling step, the algorithm .nds all behaviors in S that strongly cor\u00adrelate with b, extracts them \nout of S, and puts them into a newly created list (called an af.nity list); b is called the head of the \naf.nity list. This process repeats until S is empty. The union of the heads of all af.nity lists is taken \nas the seminal behavior set. Although the approach has shown effective for program behavior predictionand \noptimizations[19],itsdesignfailsto meet the needs of transparent input characterization. There are two \nmajor hurdles. First, as a statistical approach, the method requires tremendous data collected through \ndetailed instrumentation, which is apparently unaffordable for pro\u00adduction runs. Second, the algorithm \nfor seminal behavior identi.cation works for large, dense pro.ling results. But data collected through \nproduction runs tend to be sparse. How to recognize seminal behaviors over sparse data re\u00admains unclear. \nThe following two sub-sections present our solutions to each of them. 4.2 Randomized Inspection-Instrumentationfor \nData Collection There have been much work on lightweight runtime pro\u00ad.ling [7, 9, 17, 20]. In this work, \nwe use cross-user sam\u00adpling[24]asthe underlyingvehicleforits strengthin quickly accumulatingalargesetofsamples.Thebasicideaof \ncross\u00aduser sampling is simple. In each run of a given program, the JIT instruments a small portion of \nthe program and record the pro.ling results. The sampled information from differ\u00adent runs and by different \nusers (e.g., all the customers of a software) of the program are accumulated together to form one data \nset. Initially, applying cross-user sampling to input character\u00adization appears to be a straightforward \nprocess. But when we take into consideration the distinctive properties of input characterization, it \nturned out to be a challenging task. The special dif.culties exist in two aspects. First, as the data \nto collect are for thorough statistical correlation analy\u00adsis, the required instrumentations are intensive.For \ninstance, one type of data needed is the trip-count of every loop, at\u00adtaining which often requires the \ninsertion of counter update instructions that needs to be executed in every iteration of the loop. Second, \nthe instrumentation must happen on the .y. Most previous designs of cross-user sampling [24] are for \nstatic instrumentation, which inserts some monitoring instructions into the software before the deployment \nof the software. But our problem requires instrumentation to adapt to each run because of our focus on \nthe in.uence of pro\u00adgram inputs. As a consequence, the overhead of the sam\u00adpling comesnotonlyfromtheexecutionofthe \ninstrumented instructions,but also from the instrumentation process itself, making it more dif.cult than \nbefore to control the total over\u00adhead to a given limit a requirement critical for production runs. This \nsub-section describes two techniques, inspection-instrumentation and randomization, designed to address \nthese dif.culties. Behaviors to Pro.le Before looking at the solutions, we .rstexplainthebehaviors neededto \npro.le.Followingprior observations [19], we focus on two kinds of program behav\u00adiors, from which, seminal \nbehaviors will be recognized later through statistical correlation analysis. The .rst kind of behaviors \nis interface behaviors. (Please note that the name has nothing to do with Java Interfaces.) They consist \nof the values obtained directly from program inputs suchasthevaluesof command-line optionsandval\u00adues \nfrom .le reading operations.We ignorea .le operation thatfallsinaloop,the trip-countof whichis eitherlargeor \nunknown during compile time. Such an operation tends to be accessing some massive data set, the values \nof which may not in.uence the coarse-grained behaviors much, but may signi.cantly in.ate the candidate \nbehavior set and compli\u00adcate the recognition of seminal behaviors. Interface behav\u00adiors have two appealing \nproperties: They usually correlate with program inputs strongly; theytend to reside in the ini\u00adtialization \npart of a program, and hence their values often become known in the early stage of an execution an im\u00adportant \nproperty for the uses of the predictive modelsbuilt later on (Section 5.2). The second kind of behaviors \nare the trip-counts of all the loops in the program thanks to the importance of loops and their strong \ncorrelations with other program behaviors [19]. 4.2.1 Overhead Control through Inspection-Instrumentation \nCollection of interface behaviors (that are not loop trip\u00adcounts) requires only the recording of some \nvariable values, incurringnegligibleoverhead.The focusofoverhead control is on the collection of loop \ntrip-counts. Before describing the overhead control strategy, we .rst explain the source of overhead. \nThere are two sources of overhead for collecting loop trip-counts. The .rstis compilationoverhead.It \nrelatestothewayJIT works.We use JikesRVM for explanation; many other managedenvironmentshavesimilar schemes.Bydefault, \nthe JIT in Jikes RVM compiles a Java method using a basic compiler when encountering the method for the \n.rst time. The compilation is essentially a simple byte code translation with little data or control \n.ow analysis. Later recompilations are through an optimizing compiler. Only compilation by the optimizing \ncompiler (at levels 0, 1, or 2) exposes loop structures. Our loop instrumentation is implemented in the \noptimizing compiler. So in order to collect loop information, the selected Java method is compiled by \nthe optimizing compiler (at level 0) rather than by the basic compiler when it is loaded for the .rst \ntime. Because compilations happen during runtime, the extra time incurred by the optimizing compilation \nover that by the default basic compilation is the .rst source of overhead.  The second sourceisexecutionoverhead.Togetthetrip\u00adcount \nof a loop, the compiler inserts a counter-increase instruction into the loop body. Executions of these \nin\u00adstructionshappenineveryloop iteration,formingthesec\u00adond source of overhead.  Without a careful control, \nthe two kinds of overhead may cause unacceptable slowdown to the program executions. Our solutionisa \nguarded adaptive schemefor instrumen\u00adtation, named inspection-instrumentation. The basic idea is simple: \nIf we can estimate the overhead of an instrumen\u00adtation, we would be able to control the amount of instru\u00admentations \nso that the total overhead is within an acceptable limit. But because the inspection and selective instrumenta\u00adtion \nboth have to happen over production runs, theymust be carefully designed to work hand-in-hand over the \noften in\u00adcomplete view exposed by production runs on program be\u00adhaviors. The designed inspection and \ninstrumentation mingle to\u00adgether through all production runs. But for clarity of expla\u00adnation, we describe \nthem separately as follows. Inspection The purpose of inspection is to estimate the compilation and execution \noverhead that an instrumentation may incur. 1) Compilation Overhead. Compilation overhead mainly relates \nwith the size of a Java method. Typically, a JIT is ableto estimatethetime neededto compilea methodateach \noptimizinglevel.For instance, thereisa tableinthedefault Jikes RVM that lists the compilation speeds \nof the JIT at various compilation levels (Section 3). So with the size of a Java method revealed, the \ncompilation overhead can be easily estimated from the compilation speeds. The size of a Java method \nis obtained incrementally across runs. In each run, as the JIT compiles a Java method, it gets the size \nof the method for free. It records the size into a database if it is not there yet. The .rst-time runs \nby all the users (likely on many different inputs) typically give a good coverage of all the Java methods \nin the program. If a later run encounters some new methods, these methods are excluded from instrumentation \nin that run. Their size will be added to the database for guiding the instrumentations in future runs. \nThe recording of a method size happens once per Java method, incurring negligible overhead. 2) Execution \nOverhead. The estimation of execution overhead is based on the following assumption: After a loop is \ninstrumented, it becomes 1/S or less slower than its default run, where S is the size of the loop body \nin terms of the number of instructions. This assumption comes from thefact that the instrumen\u00adtation \ninserts only one counter increase instruction into the loop body. We acknowledge that the assumption \nmay not hold in certain cases (e.g., with early returns). But most other parts of the overhead estimation \nalgorithm are conservative. Overall, the assumption causes no noticeable effects as ex\u00adperiments show \n(Section 8). Estimation of execution overhead takes place during the compilation of a Java method by \nthe optimizing compiler. The compiler lists the loops in an ascendingly ordered se\u00adquence based on their \nbody size.Fora nested loop, the size of the outer loop does not include the inner loops. Let Li be the \nith loop in that ordered sequence(i =1, 2, \u00b7\u00b7\u00b7 ,M), with M for the total number of loops in the sequence. \nThe overhead estimation mainly uses the following proposition: PROPOSITION 1. For any giveni, when all \nloops, Lj (i<= j<= M), are instrumented, their incurred execution over\u00adhead (normalizedbytheexecutiontimeoftheprogram \nsde\u00adfault run) is no more than 1/size(Li).(size(Li) is the num\u00adber of instructions in Li.) To see the \ncorrectness, one needs to notice that because of our assumption described two paragraphs earlier, after \nthe instrumentation, the total execution time of the program . ' becomes T * (1 + 1/size(Lj)), = Trest \n+ j=i,\u00b7\u00b7\u00b7,M TLj where, Trest is the time the non-loop parts of the program take in the default run of \nthe program, and TLj is the time loop Lj takes in the default run of the program with the time spent \nin its inner loops excluded. Because size(Lj ) >= size(Li) (i<= j<= M), we have . ' T<= Trest + (1 + \n1/size(Li)) .j=i,\u00b7\u00b7\u00b7,M TLj <= (1+1/size(Li))(Trest +) j=i,\u00b7\u00b7\u00b7,M TLj = (1+1/size(Li))Tdef , where Tdef \nistheexecutiontimeofthe programinitsdefault run. The correctness of the proposition follows. static int \ntotalCost=0; Procedure methodProcess (Methodj ){ if (Cj == null){ recordSize (Methodj ); defaultCompile \n(Methodj ); } else if (totalCost + Cj >H* T) defaultCompile (Methodj ); else{ totalCost += Cj ; compileWithInstrument \n(Methodj ); }} Procedure compileWithInstrument (Methodj ){optCompile (Methodj ); LoopList = sortLoops \n(Methodj ); // from small to large instruB = false; foreach e in LoopList{ if (instruB) instrument (e); \nelse if (totalCost/T + 1/e.size < H){ instruB = true; instrument (e); }}} Figure 3. The online algorithm \nfor guarded adaptive instru\u00admentation. Instrumentation Aided by the inspection component, the algorithm \nof instrumentation ensures that the ratio between the estimated total overhead and the default running \ntime does not exceed a prede.ned threshold, H (a small number between0and1;2%in ourexperiments). Figure3outlines \nthe algorithm.For simplicityofexpla\u00adnation, .rst assume that the default execution time of the current \nrun, Tdef , is known beforehand. The runtime system (JVM) uses a variable totalCost to track the total \nestimatedoverhead (normalizedbythe default execution time) that may be incurred by instrumentations thathavebeendoneinthe \ncurrent run.Itsvalueis zeroatthe beginning of an execution. The instrumentation algorithm consists of \nthree steps: Step 1) When a method, Mj, is loaded, the JIT checks whether its size has been recorded \n(by the inspection in previous runs). If not, the method will be excluded from the instrumentation,and \ncompiledinthedefaultway. Otherwise, this method may need to be instrumented; the algorithm proceeds to \nthe second step. Step 2) Recall that instrumentation can only happen through optimizing compilation. \nIn this step, the JIT com\u00adputes the overhead (Cj ) that may be incurred by com\u00adpiling Mj with the optimizing \ncompiler. By comparing (totalCost + Cj /Tdef ) against H, it determines whether using the optimizing \ncompiler to compile this method is af\u00adfordable. If not, the method is compiled in the default way with \nno instrumentation. Otherwise, tries to instrument Mj by following the next step. Step 3) The JIT increases \ntotalCost by Cj/Tdef , and does the optimizing compilation, during which, it tries to instrument the \nloops in the method selectively as follows. Itexaminestheloopsinthemethodinan ascendingorderof their \nbody size. Its examination stops when it encounters a loop, denoted as L, that meets the condition (1/size(L)+ \ntotalCost) < H. (1) From Proposition 1, we know that the instrumentation of loop L and all loops that \nare larger than it incurs no more overhead (normalized by Tdef )than1/size(L). Therefore, meeting condition \n1 means that all these loops can be in\u00adstrumented without incurring too much overhead. The JIT increases \ntotalCost by 1/size(L), and then instruments all these loops. The program execution continues. In the \nabove description, we assume that Tdef is known. Itis rarely truein reality.To circumvent the problem, \nwe in\u00adstead use the approximated shortest execution time, Tshort, of the program. The approximation of \nTshort is over the .rst-time runs of all users.TheJVM recordstheexecution timesof those runs (likely \non various inputs). The idle-time analyzer computes the mean(m)and standard deviation(d)of those times. \nThe valuem-3d is taken for Tshort. Using m-3d rather than the minimumofallruntimesistoavoidthenoisefrom \nabnormal executions of the program; it is a standard way in statistics for outliers .ltering (including \nthe use of 3 ) [16]. As typically Tshort <= Tdef , that replacement only in.ates the estimated overhead, \nhence adding no risks but extra conservativeness to the selective instrumentation. Coverage Maximization \nthrough Randomization A fac\u00adtor critically determining the coverage of the lightweight pro.ling is the \ntime when the instrumentation algorithm startstoruninanexecution.For instance,ifitalways startsat the \nbeginning of an execution, due to the limited affordabil\u00adity, only the methods invoked early in the executions \nwould get a chance to be instrumented. Tohelp achievealarge coverage quickly,we designaran\u00addomized scheme.ForeachcopyofaJava \napplication(likely owned by different users), the JVM maintains a variable, insStart, which determines \nthe time when the instrumen\u00adtation algorithm starts to run. After the .rst execution of the application, \nthe JVM as\u00adsignsarandomvalueto insStart.Thevalueisan integerbe\u00adtween zero and N (the number of methods \nin the program). Inanexecutionofthe program(exceptthe .rst-timerun),the instrumentation algorithm starts \nafter the number of methods that have been loaded equals insStart. After each run, the Figure4. An illustration \nof the dif.culty for seminal behav\u00adior recognition causedby data sparsity.  value of insStart is updated \nto (insStart + m)%N, where m is the number of methods getting instrumented in the just\u00ad.nished run. If \nno methods were instrumented (e.g., when insStart is greater than the number of methods loaded), m issettobe1to \nencouragethe continuationofthe instrumen\u00adtation in the next run. The randomization of the initial value \nof insStart helps to diversify the instrumentation coverage of the execu\u00adtions by different users. Meanwhile, \nthe regular updates to insStart within the executions by the same user ensure a systematic coverage of \nthe entire program among the execu\u00adtions by that user.  4.3 CorrelationPropagationfor Seminal Behavior \nRecognition over Sparse Data Based on the data accumulated through cross-user sampling, the idle-time \nanalyzer tries to identify seminal behaviors. Recall that the goal is to .nd a small set of behaviors \nthat strongly correlate with other behaviors by processing the collected data set. A special complexity \nimposed by the transparent data collection is that the collected data set tends to be sparse because \nof the low tolerance of overhead by production runs. The sparsity complicates the correlation analysis.For \nexample, as Figure4illustrates, Loop1 and Loop2 are never sampledin one common run.Soevenifthe trip-countsofthe \ntwoloops actually correlate with each other strongly,adirect correlation calculation on their sampled \ntrip-counts cannot uncover that. We circumvent the dif.culty by exploiting the transitiv\u00adityof correlations. \nThe basic observationis thatifeventA has strong statistical correlation with event B and event B strongly \ncorrelates witheventC,eventA andeventCtend to correlate. For the example in Figure 4, as the samples \nof Loop3 overlap with those of both Loop1 and Loop2, we can use the overlapped runs to compute the correlation \nbe\u00adtween Loop3 and Loop1, and the correlation between Loop3 and Loop2. If the two correlations are both \nhigh, it can be inferred that Loop1 and Loop2 have strong correlations as well. Figure 5 outlines our \nalgorithm for identifying seminal behaviors. The algorithm iteratively partitions all sampled loopsintoa \nnumberoffamilies.Theloopsinafamilyhave strong correlations with one another in terms of trip-counts. \nDuring this process, the algorithm examines every pair of loops in order of loop ID. For each pair, it \nfeeds the data //IB: the interface behavior set //Hc: a predefined correlation threshold Procedure SemRec( \n){ semBeh = {}; LoopFam = buildLoopFam( ); foreach f in LoopFam { l = getRepresentive (f); c = calCor \n(l, IB); if (c< Hc) { s = getEarliest (f); semBeh = semBeh U s; } } } Procedure buildLoopFam (){ loopList \n= sortLoops (); // based on ID foreach l1 in loopList { loopList = loopList -l1; for each l2 in loopList \n{ c = calCor (l1, l2); if (c > Hc) { f = getFamily (l1); // create one if none addToFam (l2, f); } } \n} } Figure 5. Algorithm for seminal behavior recognition. collected from theiroverlapped runstoacorrelation \nanalyzer (a component of the idle-time analyzer). If the analyzer regards that the loops have strong \ncorrelations (either linear or non-linear), the loop with the larger ID is added to the familyto whichtheloopwiththe \nsmallerID belongs.Anew familyis createdif thereis no suchfamily. After the loops are partitioned intofamilies, \nthe next step is to determine the seminal behaviors. This step starts with the interface behaviors, which \nare put into seminal behav\u00adior set by default. Recall that interface behaviors are typi\u00adcally cold behaviors \nand are collected in every run. The en\u00adtire set of interface behaviors is regarded as one predictor. \nThe algorithm examines the correlation between this predic\u00adtorandone representativeloopineachfamily.The \nrepresen\u00adtative is selected to be the loop that has the largest samples in the family for the stableness \nof the correlation analysis results. Whena strong correlationis found, the wholefam\u00adily of loops are \nremoved from further considerations as they are predictable from the current seminal behavior set. If \nthe correlation is low or uncomputable (when there are too few overlapped runs), the earliest loop of \nthatfamily is taken as a new seminal behavior and added into the seminal behavior set. We elaborate on \ntwo details. First, the earliness of a loop is de.ned as the earliest time that its trip-counts is known. \nWe make changes to the JIT so that the instrumentation in\u00adserts a load and store bytecode before and \nafter each sam\u00adpled loop to record how much time (in timerTicks in Jikes RVM) has passed since the start \nof the program. The ear\u00adliness of a loop is computed as the average earliness of all samples of the loop \nin all runs. Selecting the earliest loop fromafamily asa seminal behavior helps the early useof the to-be-built \ninput-opt models (see Section 5.2). Second, the correlation analyzer is a statistical tool we have devel\u00adoped. \nIt consists of standard statistical functions for corre\u00adlation analysis: Least Median of Squares (LMS) \nregression for linearregression,RegressionTreesfor non-linearregres\u00adsion, step-wise function and principal \ncomponent analysis (PCA) for feature selection. Theyare similar to the analyzer in previous work [19]. \nDetails are elided. The technique designed in this work for seminal behav\u00adior identi.cation shares certain \ncommonality with the prior technique [19] in that both are based on statistical correla\u00adtion analysis. \nHowever, there are two important differences. First, the previous technique works on dense data sets \nrather than sparsedata sets. Second,thepreviousworkbuildsaf.n\u00adity lists in a greedy manner. It cannot \nexploit the correlation transitivity and hence is not amenable to sparse data sets. For instance, for \nthe example in Figure 4, the prior tech\u00adnique fails in recognizing the correlations between Loop1 and \nLoop2. In the implementation of the algorithms, we use 0.8 as thevalue for the threshold Hc to judge \nwhethera correlation is high enough. It is the same as the threshold value used in the previous work, \neasing the comparison between the two techniques (in Section 8). The complexity of the algorithm is O(N2), \nwhere N is the number of loops in the program. As this step happens during idle time of a machine, the \ncomplexity is typically tolerable. 5. Input-Opt Modeling and Adaptive Dynamic Optimizations This section \nbrie.y discusses some issues the cross-run learning paradigm brings to the other two components of of \ntransparent input-conscious dynamic optimizations. Al\u00adthough these issues are not as dif.cult as those \ndiscussed in the previous section, appropriate treatment to them is no less important for the transparent \ninput-conscious dynamic optimizations to work effectively. 5.1 Input-Opt Modeling Recall thatthe objectiveof \ninput-opt modelingistobuildup apredictivemodelmappingfromthevaluesofinput features (i.e., seminal behaviors) \nto the appropriate optimization de\u00adcisions (e.g., appropriate unrolling levels for a loop, suitable optimizing \nlevels for a method, etc.) for an execution. The mapping can be represented as Btarget = f(Bsem), with \nBtarget forthevalueofa predictiontarget, Bsem for theval\u00adues of seminal behaviors, and f() for the predictive \nmodels. The goal of input-opt modeling is to determine f(). The previous work [32] has treated this \nproblem as a statistical learning problem. The solution is to collect a data set consisting of the values \nof Btarget and Bsem in many runs of a program on different inputs, and then apply a statistical learning \ntool to the data set to compute f(). The paradigm of learning across production runs imposes new implications \nto input-opt modeling in two aspects. Data Collection The .rst array of implications relate with the \ncollection of the data sets(Btarget and Bsem). The sem\u00adinal behavior set(Bsem)consists of mostly interface \nbe\u00adhaviors (which are usually outside hot code regions) and a small number of loop trip-counts. The overhead \nfor collect\u00ading those behaviors is typically negligible. Collection of target behaviors(Btarget)is more \ncomplex; the overhead depends on what the target optimizations are. We categorize various optimizations \ninto three classes. Class 1) For some optimizations, the default runtime en\u00advironment eventually exposes \nthe appropriate decisions. An example is the appropriate level for optimizing a Java method. Even though \nthe default JikesRVM cannot de\u00adtermine the appropriate level for a method during an ex\u00adecution because \nit does not know how much time that method takes in the entire execution, it can do so at the end of \nthe execution. So, the .nal optimizing level the JVM decidesona methodis usuallythe appropriatelevel \nforthe entire run.Forthisclassof optimizations,thecol\u00adlection of Btarget is simple, just recording the \n.nal deci\u00adsions at the end of an execution. As the overhead occurs only after the execution, it is typically \nnegligible.  Class 2) For some optimizations, the default runtime en\u00advironment does not directly expose \nthe appropriate de\u00adcisions,but can produce such decisions as long as some necessary information is provided. \nOne example is func\u00adtion inlining. The inlining decisions madeby JikesRVM during an execution may be \ninappropriate due to the lack of information. However, Jikes RVM contains a model that produces the appropriate \ninlining decisions as soon as the hotness of all methods and their sizes are provided. Often,the information \nneededis recorded throughtheex\u00adecutionby default; the method hotnessin JikesRVMis such an example. If \nnot, the behaviors have to be col\u00adlected using the overhead-controlled sampling scheme as described in \nthe previous section.  Class 3) There are some other optimizations, the appro\u00adpriate decisions of which \nare hard to model, and are of\u00adten better to resort to empirical cross-trial comparisons. An example is \nthe best unrolling levels of a loop. As the trials may negatively affect the production run per\u00adformance, \nthe number of trial runs must be minimized. For some target optimizations, the trials of different deci\u00adsionscanhappeninoneexecution,suchasloop \nunrolling fora loop thatisinvoked manytimesina run.For oth\u00aders, the trials of different decisions may \nneed to happen  on different runs of the program. An example is the se\u00adlectionof thegarbage collection \nthat best .ts anexecu\u00adtion [26, 29]. In this case, the comparison of the quality of the different decisions \nis tricky. If the trials happen on the same inputs, the comparison is simple. But because the trials \nare on production runs, the same inputs may not be seen until many runs later(orever).Fortunately, from \nseminal behaviors values, one can infer the similarity or relations among different inputs, and hence \nmakeapprox\u00adimated comparison. Detailed explorations are out of the scope of this paper. Model Self-Assessment \nand Evolvement The second fold of implications are on model construction. Because now training data come \nincrementally across runs, it becomes es\u00adpecially important to track the quality of the current models \nso that wrong predictions can be prevented from hurting the optimizations. We use ten-fold cross-validation \n[16] to compute the con\u00ad.denceof each constructed model.Ten-fold cross-validation is a standard statistical \napproach. It uses nine tenth of all training data for model construction and the rest for test\u00ading. This \nprocess repeats for ten times.A standard statisti\u00adcal analysis is then applied to the testing results \nto derive a con.dence value for that model. Meanwhile, the model construction step records the boundaries \nof the part of the input feature space that has been covered by the training data. Prediction inside \nthese regions is typically safer than outside. The usage of the boundaries is seen in the next section. \nAs more runs .nish, more data are collected. With the input-opt models reconstructed periodically using \nthe up\u00addated data set, the con.dence levels and covered regions boundaries are updated accordingly. The \nself-assessment process happens in the idle-time analyzer and do not in\u00adterfere with the production executions. \n 5.2 Adaptive Dynamic Optimizations The third component of input-conscious optimizations is to employ \nthe constructed input-opt models to guide runtime optimizers. During runtime, as soon as the seminal \nbehav\u00adiorsvalues becomeknown,the runtimeenvironmentinvokes the already constructed input-opt models to \nattain the appro\u00adpriate optimization decisions and use them for runtime opti\u00admizations. As an implication \nfrom the paradigm of learning across production runs, the usage of the models must be select as the model \nquality takes some runs to enhance. The con.\u00addence levels and region boundaries described in the previous \nsub-section come at handy. The principle is that the runtime environment uses a model only if the seminal \nbehavior val\u00aduesofthe currentrunfallsintothecoveredregionandatthe sametime,the con.denceofthemodelishighenough(over \n70% in our experiments). 6. AConcerted Assembly That Evolves Continuously Another implication from the \nnew paradigm is that unlike the prior of.ine schemes, the three components now must happen throughout \nthe entire life time of an application. It is important to assemble them together into a concert to work \nsynergistically. As Figure1shows,a continuous learning framework uni\u00ad.es the three components together. \nAlthough all three com\u00adponents remain activethrough the life time of an application, the degrees of their \nactiveness differ in different stages of the life time. The initial certain numberof runsofa program \nare purely dedicated to the .rst component for identi.cation of the seminal behaviors. The second and \nthird components do not need to be invoked as the seminal behaviors set is not available yet. The criterion \nwe use for the initial activation of the sec\u00adond component is as follows. Let ni,j be the number of runs \nduring which both loop i and j are sampled. Let n\u00afbe theav\u00aderage of all ni,j . Thevalueof n\u00afre.ects the \ndensity of the ac\u00adcumulated data set. When it exceeds a prede.ned threshold (e.g., 3 in our experiments), \nthe second component, input\u00adopt modeling, gets activated. All the runs before the reach\u00ading of the prede.ned \nthreshold form the initial stage of the optimization paradigm. In every following run, the runtime optimizer \ntries to use the current input-opt model for optimizations. After every such run, the collected seminal \nbehaviors and the observed learning target (e.g., method optimization levels) are put into the local \ndatabase. Periodically, the local databases of dif\u00adferent users are accumulated together (e.g., into \na remote server), upon which, the idle-time analyzer re.nes the semi\u00adnal behavior set and the input-opt \nmodels. So over time, the seminal behavior set may become smaller (as more correla\u00adtions among seminal \nbehaviors are discovered), the input-opt model may become more accurate, and the program is likely to \nrunfaster. 7. Other Complexities The previous sections have described our solutions to some core obstacles. \nThis section lists some other complexi\u00adties related with practical deployment of the optimization paradigm. \nResolving these complexities is beyond the scope of this paper. We list them, hoping that they may trigger \nsome research interest of the community so that the new op\u00adtimization paradigm can be practically materialized \nin the near future. Data Communication and Pro.le Management Concep\u00adtually, the idle-time analyzer resides \nin a machine that con\u00adnects with all the users of a target software. All the sam\u00adpled data of that software \nare sent to this central machine periodically (when the local machine is idle) from all users for the \nanalyzer to process. The processing results, including the IDs of the recognized seminal behaviors and \nthe input\u00adopt models, are sent back to all the users for helping their respective runtime optimizers \n(e.g., a JVM) to optimize the futureexecutionsofthe software.As neitherthe samples nor the models are \nlarge, the amount of data transfer should be modest, unless the customer base is massive. The frequency \nof the communication can be con.gured to strike a good tradeoffbetween the timeliness of the model update \nand the communication cost. The concrete design of the communi\u00adcation systemandtheef.cientwayto manage \npro.lesonthe servers may depend on the scale of the problem, the frequen\u00adcies of required updates, and \nso on. Differences in Platforms and Libraries The second com\u00adplexity for real-world deployment of the \nparadigm is the dif\u00adferences among platforms and software copies. Two users may happen to run a program \non two different architec\u00adture or libraries; the data collected may have to be recon\u00adciled. Studies (e.g. \n[38]) in matching pro.les across plat\u00adforms may be helpful. Another possible solution is to con\u00adcentrate \non behaviors that are largely platform-independent (e.g. method calling frequency) during input-opt modeling. \nPrediction from such models may still be useful as the run\u00adtime optimizer has the knowledge of the speci.c \nplatform, and hence may translate the predicted program-level behav\u00adiors into platform-speci.c optimization \ndecisions. Software Update The third complexity comes from soft\u00adware update. Software update may cause \nchanges to the behaviors of the program, hence invalidating some results learned sofar. But on the other \nhand, an update to a soft\u00adware rarely changes the program entirely. It is worth ex\u00adploring how the continuous \nlearning framework can adapt to the changes smoothly, without discarding the entire pro.le database and \nstarting from scratch. Some techniques (e.g., code matching) in software test prioritization (e.g. [30]) \nmay be helpful to solve this problem. Server Applications and Program Phases The inputs to a server application \ntypically come continuously through an entire execution. The input-conscious continuous optimiza\u00adtions \nmay need to happen at the arrival of each input. Simi\u00adlarly, for a program with phase shifts, the integration \nof the phase knowledge into the paradigm may be necessary. 8. Evaluation Ourevaluation focusesontheeffectivenessofthe \ntechniques for overhead control, and the feasibility and potential of the transparent input-conscious \nparadigm for dynamic optimiza\u00adtions in some basic settings. Speci.cally, we aim at answer\u00ading three-fold \nquestions: 1) Control of Overhead. Can the runtime data collection quickly collect many samples without \ncausing too much interference to production runs? 2) Potential for Optimizations. How effective is the \nparadigm in characterizing program inputs and exerting the power of input-consciousness? Can the paradigm \ncontinu\u00adously enhance program performance? Is the enhancement signi.cant? 3) Prevention of Risks. How \neffective is the selective pre\u00addiction in preventing wrong predictions from hurting pro\u00adgram performance? \n 8.1 Methodology Platform Our implementation is on Jikes RVM (v. 3.1), which has been brie.y described \nin Section 3. All experi\u00adments happen on machines equipped with Intel Xeon E5310 processors that run \nLinux 2.6.22; the heap size ( -Xmx ) is 512MB for all. Benchmarks Aspecial obstacle for our experiments \nis in .nding benchmark suites. Because of the focus on input in\u00ad.uence, we require many different inputs \nper benchmark. However, most existing benchmark suites come with no more than three inputs. A Java benchmark \nsuite that comes with many inputs is the one developed in a previous study on of.ine in\u00adput characterization \n[32]. The suite contains 10 Java pro\u00adgrams selected based on the criterion that extra inputs for these \nbenchmarks are relatively easier to collect than for other benchmarks in the original suites, and meanwhile, \nthe benchmark comes with source code as it is necessary for the previous analysis. Despite the previous \nefforts, some of the programs in the suite (e.g., Search)still have only a small numberof inputs.We include \nthem for completeness. In addition to including all the benchmarks in the previ\u00adous suite, we add all \nthe other programs from the Dacapo (2006) benchmark suite, except Chart, for comprehensive\u00adness of the \ntest. (We have not .gured out how to get new inputs for Chart.)For each of the added benchmark, we try \nto collect extra inputs that are typical in the normal execu\u00adtions of the benchmarks. More speci.cally, \nwe collect or de\u00adrive the inputs by searching the real uses of the correspond\u00ading applications, consulting \nthe authors of the Dacapo suite (Our special thanks to Blackburn!), and reading the source codeof the \nprograms andexample inputs.For the usageof the benchmarks to be close to that of real applications, some \nprograms (e.g., Mtrt, Antlr, Bloat)are modi.ed to reactivate some of their command-line options that \nwere disabled by the benchmark suite interface. Table1lists all the benchmarks. These benchmarks cover \na variety of domains, from utility tools to compiler tools to computational applications. The inputs \nexhibit large varia\u00adtions, re.ected by the large differences in the corresponding running times shown \nin the 5th and 6th columns of the table. Experimental Setting Weuseacontrolled environment for experiments. \nIt helps us concentrate on the main goal of the evaluation (i.e., the questions listed at the beginning \nof this section), without getting distracted by the complexities beyond the scope of this work, such \nas the design of the distributed communication system, variations in platforms and library versions. \n In the controlled setting, there are 100 virtual users, run\u00adning a benchmark on identical platforms. \nInstead of using 100 machines and getting distracted by complexities in data communications, we put all \nruns on a single machine. Each time, one virtual user runs the benchmark once, on an input randomly selected \nfrom the input set. The pro.les from all runs are accumulated into a single database. We acknowledge \nthat the setting has apparent distance from practical settings; but we maintain that the setting is still \nusable for answering the three-fold questions in the fo\u00adcus of this study. For instance, the overhead \nincurred by the sampling scheme is about the current execution by the current user, largely independent \nof how all users are con\u00adnected, how pro.les are managed, or anyother complexities excluded by the controlled \nsetting; the same for the eval\u00aduation of risks prevention. We acknowledge that the exact bene.ts from \nthe optimizations may differ from those in real settings. However, the measurement in this controlled \nset\u00adting can still indicate whether the paradigm is promising in continuously enhancing program performance, \nand whether this direction is worth further investigations. 8.2 Data Collection Ef.ciency and Incurred \nOverhead This sub-section concentrates on the ef.ciency of the data collection scheme. Speci.cally, it \nexamines the effective\u00adness of the two sampling techniques proposed in this work, randomization and inspection-instrumentation, \nin helping achieve a large coverage quickly without causing too much sampling overhead. Overhead Among \nthe executions of the continuous opti\u00admization paradigm, the initial stage is subject to the largest \nrisks of exhibiting slowdowns due to the instrumentation for data collection. Our evaluation of overhead \nconcentrates on that stage. Because multiple runs of a Java program tend to show considerable variations \nof running times even if all those runs are on the same input, we use a statistical approach ad\u00advocated \nby some previous studies [14] to examine the in.u\u00adenceoftheoverhead.For each program,we randomlypick \none input.We run the program on that input for20 times us\u00ading the default JikesRVM, and record the times.Wethen \nuse the same program and input to conduct 20 runs with the ran\u00addomized sampling based on the inspection-instrumentation \nscheme. Figure6showsthe distributionofthe running times in the two scenarios. The times are normalized \nwith the av\u00aderage time of the 20 default runs. Weusethe standard statisticalhypothesistestingtoexam\u00adine \nwhether a program s performance in the two scenarios differs signi.cantly. The approach applies T-testing \nto the time samples to compute a statistical metric, p-value. The higher the p-value is, the less likely \nthe two kinds of runs Table 1. Benchmarks and Their Properties Program Description Code #Inputs Running \ntime (s) #Runs in Sampled loops lines Min Max init stage per run (%) Compressj compression tool 927 20 \n0.94 9.33 618 6.7 Dbj database tool 1028 54 0.59 98.16 80 18 Mtrtj multithreaded ray tracer tool 3842 \n100 0.26 6.37 1161 4.2 Eulerg computational .uid dynamics 1179 20 0.93 7.79 55 17.4 MolDyng molecular \ndynamics simulation 583 20 0.11 63.05 38 21 MonteCarlog Monte Carlo simulation 3073 21 9.07 15.81 204 \n11.1 Searchg Alpha-Beta pruned search 712 9 2.74 210.36 106 20 RayTracerg 3D ray tracer 1224 21 3.10 \n236.57 83 16.7 Antlrd parser generator 32263 175 0.15 0.19 1270 4.8 Bloatd bytecode-level optimization \n73563 100 0.08 41.46 1815 3.9 Eclipse d multi-language IDE 1903219 80 0.572 86.648 1856 1.5 Fopd print \nformatter 88846 70 0.385 2.039 943 3.4 Hsqldb d SQL relational database engine 151915 75 0.455 8.888 \n1146 2.9 Jython d python interpreter in Java 91982 60 0.594 34.02 1258 1.9 Luindex d text indexing tool \n8570 50 0.363 7.299 645 3.2 Lusearch d text search tool 12709 63 0.482 1.443 1630 1.8 Pmd d Java source \ncode analyzer 49331 53 0.323 4.475 1051 4.3 Xalan d transform XML documents 243516 60 0.229 5.723 924 \n3.1 j: jvm98 [2]; d: dacapo [6]; g: grande [1] Figure 6. Distributions of the running times of the \ndefault and instrumented runs. The P-values at the bottom indicate whether the two differ signi.cantly(< \n0.05) or not(> 0.05).  Figure 7. The number of runs per user in the initial stage. The solid-line curves \nare the results when randomization is used; the broken-line curves are when randomization is not used. \ndiffer signi.cantly in time. A typical statistical practice is to reject thehypothesis that the two differ \nsigni.cantly if p\u00advalue is greater than 0.05 [16]. As shown in the bottom of Figure 6, only the p-values \nof Euler, Search, and Eclipse are lessthan 0.05.Theaveragetimedifferencesofthe threepro\u00adgrams are respectively \n0.85%, 0.97%, and 4.7%, con.rming that the inspection-instrumentation scheme effectively limits the overhead \nof most programs to be negligible. Collection Ef.ciency The second to the rightmost column in Table 1 \nreports the total number of sampling runs that the initial stage of our continuous optimization paradigm \nrequires before the second and third components can start. (Recall that the criterion is that on average, \na pair of loops must have been sampled in at least three common runs.) As we have 100 users, on average \neach user needs to have 0.38 to 18.56 runs to reach that coverage. As a comparison, if the randomized \nscheme is not used and every user s sampling starts from the beginning of the program, based on the right\u00admost \ncolumninTable1,the estimated numberof total runs would be 1.4X to 7.5X more for them to cover every loop \njust at least once. It is worth noting that because of the randomization in our sampling scheme, the \naverage number of needed runs per user decreases almost linearly as the number of users increases,asFigure7shows.(Forlegibility,the \n.gureshows only5benchmark curves.The othershavethe similar trend.) But the average number remains virtually \nconstant when the randomization scheme is not used, because no matter how many users there are, the sampling \nwindow always moves through the entire program gradually and sequentially for every user s executions. \n 8.3 Prediction Accuracy andPerformance Enhancement As Section 5.1 describes, the input-conscious dynamic \nop\u00adtimizations may be applied to help different classes of opti\u00admizations. In this experiment, we take \na speci.c optimiza\u00adtion decision problem as a concrete example to examine the basic effectiveness of \nthe new optimization paradigm. 8.3.1 Targetfor Enhancement In this example use, the objective is to \nenhance the compila\u00adtion strategyinJikesRVM.In Section3,wehave mentioned that Jikes RVM realizes the \nimportance of a Java method only gradually after some invocations of the method. The weakness causes \ntwo kinds of inef.ciency. First, because the JIT recompilesa methodata higher optimizinglevel whenit \nsees the increased importance of the method, a method may be recompiled multiple times in one run, causing \nunneces\u00adsarily large compilation overhead. Second, the highly opti\u00admized code is produced late, throttling \nthe bene.ts of the optimizations. An extreme case is that many methods that are used heavily in the initialization \nstage of an application may get highly optimized at the end of the stage;but after that, the methods \nare never invoked again [15]. As many dynamic optimization systems use the similar strategy, this weakness \nis shared by almost all of them. Sev\u00aderal studies [4, 15, 32] have reported the importance of this weakness.For \ninstance, Arnold and others[4]have reported over47% potential speedup whenthe weakness canbeover\u00adcome \nin IBM commercial JVM, J9. Despite manyrecent ef\u00adforts on this issue, the state-of-the-art solutions \nrequire ei\u00adther extensive of.ine pro.ling [32] or are subject to input\u00adobliviousness [4]. 8.3.2ASolutionfromtheNewParadigm \nIn this experiment, we try to apply the transparent input\u00adconscious paradigm to overcome the limitations \nof existing solutions. Speci.cally, we set the appropriate optimization level for each Java method as \nthe prediction target in the input-opt models. As described earlier, the models are transparently built \nacross production runs of the program. When a Java method is encountered and the values of the seminal \nbe\u00adhaviors of the current run are known already, the modi.ed JikesRVM uses the input-opt models to predict \nthe best op\u00adtimization level for the method. If the prediction is con.\u00addent, the JIT optimizes the method \nat that level immediately. Otherwise, the default compilation scheme is applied to the method. This approach \nhelps avoid repetitive recompilations of a method. At the same time, as seminal behaviors typically become \nknown at the early stage of an execution, this ap\u00adproach helps the JIT produce optimized code early, \nhence alleviating both kinds of inef.ciencyof the default strategy. Itovercomesthe limitationsofprior \nsolutions[4,32]by re\u00admoving the needs for of.ine pro.ling and enabling input\u00adconsciousness. 8.3.3 Results \nFigure 8 reports how the programs performance changes across runs as the knowledge base (i.e., the input-opt \nmod\u00adels) grows incrementally. The X-axis starts with the .rst run following the .nish of the initial \nstage of the paradigm. In the experimental setting, the idle-time analyzer re.nes the knowledge base \nafterevery.ve runs. For lackof space,it contains only the .gures for the top 12 benchmarks listed in \nTable 1. The results of the other benchmarks show the simi\u00adlar trend. The con.dence and accuracy curves \nin each .gure show the quality of the constructed input-opt models. TheY-axis value of each point on \nthe accuracy curve is the percentage of the Java methods whose appropriate optimization levels are predicted \ncorrectly in the corresponding run. Being cor\u00adrect here means that the predicted optimization level of \na method equals the ground truth, which is obtained through the default Jikes RVM as explained in the \nClass 1 bul\u00adlet in Section 5.1. The con.dence value is computed using cross-validation on the existing \ndata base, as described in Section 5.2. The arising trend exhibited by the curves indi\u00adcates that the \ncontinuous learning framework is able to incre\u00admentally increase the quality of the input-opt models. \nSome runs prediction accuracies are zero because in those runs, the runtime system .nds that their seminal \nbehavior values fall out of the space that the previous runs have covered. As Section 5.2 describes, \nthanks to the self-assessment and se\u00adlective prediction scheme, in such cases, the runtime system doesnotdo \npredictionandfallsbacktothedefaultexecu\u00adtion; no performance penalty is incurred. Similar fallback executions \nhappen for those runs in which the con.dence is lower than the threshold (0.7). As the model becomes \ngood enough, the JIT starts to use the predicted levels to do optimizations. The resulting speedup starts \nto show. On different inputs, the speedup dif\u00adfers. Overall, for most of the programs, signi.cant speedups \nare exhibited. Comparisons Even though the speedup brought by the input-conscious optimizationsisquite \nsigni.cantasFigure8 shows, the bene.ts come from multiple sources. it is unclear how much bene.ts the \ninput-consciousness really brings. Will a simple input-oblivious re.nement of the default re\u00adcompilation \nscheme be suf.cient? And how much bene.t of input-conscious optimizations is compromised because of the \ndata loss caused by the cross-run sampling scheme? To answer the two questions, we compare the speedups \nbrought by our technique with two other results. One is from the repository-based approach by Arnold \nand his col\u00adleagues [4].It learns froma repositoryof history runs,but does not tailor optimization strategies \nto program inputs. More speci.cally, it produces an optimization strategy for each method in a program \nbased on some optimization his\u00adtograms that arebuilt through history runs of the program. The optimization \nstrategy contains a number of pairs. Each pair, say < k,o >, indicates that the method should be (re)compiled \nusing level o whenthe samplerintheRVM en\u00adcounters the kth samples of the method. The cross-run learn\u00ading \nin the technique ensures that the produced optimization strategy produce the best average performance \nfor history runs. Prior studies have shown that this scheme enhances the optimizationbyJavaVirtual Machines(J9) \nsubstantially. Despitebeingagood re.nementtothedefault recompilation scheme, it is input-oblivious. The \ncomparison with this ap\u00adproach will indicate the value of being input-conscious. The authors of the technique \ndid their implementation in IBM J9; we implement their approach on JikesRVMby follow\u00ading their paper. \n The other result to compare with is from the of.ine pro\u00ad.ling approach [32].We use detailed instrumentation \nto run each program on all its inputs to collect a complete train\u00ading data set, then apply the techniques \nproposed in a re\u00adcentwork [32] to characterize the inputs andbuild predic\u00adtive models for optimization \nlevel selection. After that, we use the models to help JIT in the same way as in our tech\u00adnique. As the \ncomplete data set is used for training, the obtained performance enhancements are expected to be the \nupper-bound for our approach.Acomparison with these re\u00adsults will indicate the bene.t compromise caused \nby the lightweight data collection in our approach. Figure 9 shows the minimum, mean, and maximum speedups \nfrom the three techniques on the benchmarks (40 runs per program). The cross-input adaptivity helps our \ntech\u00adnique to outperform the repository-based approach substan\u00adtially, accelerating the programs over \ntheir default runs by 10 26% on average. Some of the results, especially those of repository results, \nshow less than 1 speedup. Those indicate that some slowdown is caused due to wrong pre\u00addictions. In most \ncases, the minimum speedups of our ap\u00adproach are higher than those of the repository approach, demonstrating \nthat the selective prediction technique helps our technique to avoid negative effects from prediction \ner\u00adrors. The small average distance from the of.ine pro.ling\u00adbased results indicates that the automatic \ncomponents in our technique are able to well exert the potential of the input\u00adconscious continuous optimization \nparadigm. (In several cases, our approach shows even better performance than the of.ine one. It is due \nto the imperfect design of the compiler, just another indication of the complexity and sub-optimality \nof the current compiler construction.) 9. RelatedWork Given the large body of literatures on program \noptimiza\u00adtions, this section concentrates on the studies closest to cross-run program optimizations, \ninput-based optimizations, and sampling. There have been some proposals on continuous compila\u00adtion across \nruns, including the design of CoCo by Childers and others [10] and the CPO frameworkbyWisniewski and \nothers [37]. Their focuses are on the design of high-level architectures, loop transformations, or the \nexploitation of multiple levels of the software stack. Their design contains  (a) compress (b) db (c) \nmtrt (e) euler (f) moldyn (g) montecarlo (h) search (i) raytracer (j) antlr (k) bloat (l) eclipse \n(m) fop   Figure 8. The cross-run changes of the prediction con.dence and accuracy of the input-opt \nmodels, along with the corre\u00adsponding performance enhancementover theexecutionsin the default JikesRVM. \n Figure 9. The overall speedups from repository-based approach, input-conscious continuous optimizations, \nand the upper\u00adbounds obtained throughof.ine-pro.ling basedexperiments. no systematic treatment to program \ninputs. Our work dif\u00adfers from the repository-based cross-run learning system by Arnold and his colleagues \n[4] in three main aspects. First, we propose an automatic way to tackle input complexity over production \nruns, which is not addressed in their study. Sec\u00adond, our technique tailors the optimization strategy \nfor ev\u00adery input rather than producing a single strategy that max\u00adimizes the average performance of all \npast runs. Finally, our technique uses self-evaluation to selectively predict op\u00adtimal strategies with \ncon.dence; their technique applies the learned strategy to new inputs with no guarding. Mao and Shen \nhave developed a framework for cross-input learning and optimizing programs [25]. Their work uses manually \ncharacterized input features without addressing the dif.cul\u00adties in automatic input characterization \nthrough production runs. The required manualefforts areextraburden that im\u00adpairs the adoption of cross-input \nlearning and optimizations. To the best of our knowledge, this current work is the .rst that enables \nfully automatic cross-run optimizations with input-adaptivity. Some prior studies have noticed the importance \nof pro\u00adgram inputsand triedtoexploit themfor optimizations.Tian and others propose an input-centric framework \n[32]. Their technique is heavily based on of.ine pro.ling for both in\u00adput characterization and the recognition \nof the relations be\u00adtween inputs and optimizations. There are some other stud\u00adies that manually specify \na set of input features that are im\u00adportant for the execution of the application, and then use search \nor machine learning techniques to derive a model to help the execution of the application adapt to those \nfea\u00adtures in an arbitrary input. Examples include the parametric analysis for computation of.oading [35], \nmachine learning\u00adbased compilation [22], adaptive sorting [23], and some li\u00adbrary constructions[5,12,18,28,31,36]. \nBecauseofthe re\u00adquired manual efforts, those explorations have been focused on some particular applications \norkernels. The optimization strategyis constructed throughalarge numberofof.inepro\u00ad.ling runs.Acomplementary \napproach to helping JIT is to enhance the compilation decisions by training over a large number of code \nfeatures. An example is the method-speci.c dynamic compilationbyCavazos and others [8]. Theirwork also \nrelies on a large number of of.ine training runs. The term, continuous program optimizations, was also \nused to refer to pure runtime adaptive optimizations [3, 11, 21, 27, 33]. They typically use runtime \nlightweight pro.l\u00ading to guide dynamic optimizers. Theydo not use cross-run knowledge, and do not deal \nwith input complexities explic\u00aditly. Muchwork has used sampling for program optimizations (e.g. [9, \n17]) and debugging (e.g., [7, 20]). Cross-user data collection has been used for bug isolation [24] and \ncom\u00adpilation [34]. The data collection scheme used in this cur\u00adrent work differs from the previous work \nin that it uses randomization to speedup coverage, and employs the over\u00adhead pre-inspection to guard \ninstrumentation. The two tech\u00adniques show effectiveness for both coverage maximization andoverhead control;wearenotawareofpriorusesofthese \ntwo techniques. 10. Conclusion In this paper, we report an investigation in the basic feasi\u00adbility of \ntransparent integration of input-consciousness into dynamic program optimizations, particularlyin managedex\u00adecutionenvironments.The \nunderlyingvehicleofthenewap\u00adproach is transparent learning across production runs. Af\u00adter examining the \nimplications of the new paradigm on each main component of input-conscious dynamic optimizations, we \npropose several techniques to address some key chal\u00adlenges, including randomized inspection-instrumentation \nfor cross-user data collection, a sparsity-tolerant algorithm for input characterization, and selective \nprediction for ef.ciency protection. Together, these techniques make it possible to automatically recognize \nthe relations between the inputs to a program and the appropriate ways to optimize it. The new approach \neliminates the needs for of.ine pro.ling or programmers annotations, overcoming some limitations of prior \nsolutions. Meanwhile, the paper points out some com\u00adplexities that require further explorations. Experiments \nin a JVM demonstrate the feasibility and potential bene.ts of the new optimization paradigm in some basic \nsettings. Acknowledgments We owe the anonymous reviewers our gratitude for their helpful suggestions \non the paper.We are especially grateful for Steve Blackburn and John Zigman s help on the collec\u00adtion \nand creation of extra inputs of the Dacapo benchmark suite.We thank Michael Bond for answering our questions \nabout JikesRVM, and Matthew Arnold for commenting on the repository-based approach. This material is \nbased upon work supportedby the National ScienceFoundation under Grant No. 0811791 and 0954015. Any opinions, \n.ndings, and conclusions or recommendations expressed in this ma\u00adterial are those of the authors and \ndo not necessarily re.ect the viewsof the National ScienceFoundation. References [1] JavaGrande benchmark. \nhttp://www2.epcc.ed.ac.uk/javagrande/. [2] Spec jvm98. http://www.spec.org/jvm98/. [3] M. Arnold, M. \nHind, and B. G. Ryder. Online feedback\u00addirected optimization of Java. In Proceedings of ACM Conference \non Object-Oriented Programming Systems, Languages and Applications, pages 111 129, 2002. [4] M. Arnold, \nA. Welc, and V. Rajan. Improving virtual machine performance using a cross-run pro.le repository. In \nthe Conference on Object-Oriented Programming, Systems, Languages, and Applications, pages 297 311, 2005. \n[5] J. Bilmes, K. Asanovic, C.-W. Chin, and J. Demmel. Optimizing matrix multiply using PHiPAC:Aportable, \nhigh\u00adperformance, ANSICcoding methodology. In Proceedings of theACM International Conference on Supercomputing, \npages 340 347, 1997. [6] S. M. Blackburn et al. The DaCapo benchmarks: Java bench\u00admarking development \nand analysis. In Proceedings ofACM SIGPLAN Conference on Object-Oriented Programming Sys\u00adtems, Languages \nand Applications, October 2006. [7] M. D. Bond, K. E. Coons, and K. S. McKinley. Pacer: Proportional \ndetection of data races. In Proceedings ofACM SIGPLAN Conference on Programming Language Design and Implementation, \n2010. [8] J. Cavazos and M. O Boyle. Method-speci.c dynamic compilation using logistic regression. In \nProceedings ofACM SIGPLAN Conference on Object-Oriented Programming Systems, Languages and Applications, \n2006. [9] W. Chen, S. Bhansali, T. M. Chilimbi, X. Gao, and W. Chuang. Pro.le-guided proactive garbage \ncollection for locality optimization. In Proceedings of PLDI, pages 332 340, 2006. [10] B. Childers, \nJ. Davidson, and M. L. Soffa. Continuous compilation: A new approach to aggressive and adaptive code \ntransformation. In Proceedings of NSF Next Generation SoftwareWorkshop, 2003. [11] P. Diniz and M. Rinard. \nDynamic feedback: an effective technique for adaptive computing. In Proceedings ofACM SIGPLAN Conference \non Programming Language Design and Implementation, pages 71 84, LasVegas, May 1997. [12] M. Frigo and \nS. G. Johnson. The design and implementation of FFTW3. Proceedings of the IEEE, 93(2):216 231, 2005. \n[13] A. Gal, B. Eich, M. Shaver, D. Anderson, et al. Trace\u00adbased just-in-time type specialization for \ndynamic languages. In Proceedings of the ACM SIGPLAN Conference On Programming Language Design and Implementation, \n2009. [14] A. Georges, D. Buytaert, and L. Eeckhout. Statistically rig\u00adorous Java performance evaluation. \nIn Proceedings ofACM SIGPLAN Conference on Object-Oriented Programming Sys\u00adtems, Languages and Applications, \n2007. [15]D.GuandC.Verbrugge. Phase-basedadaptiverecompilation ina JVM. In Proceedings of the International \nSymposium on Code Generation and Optimization, pages 24 34, 2008. [16]T. Hastie,R.Tibshirani, andJ. Friedman. \nThe elements of statistical learning. Springer, 2001. [17] X. Huang, S. M. Blackburn, K. S. McKinley, \nJ. E. Moss, Z.Wang, andP. Cheng. Thegarbage collection advantage: improving program locality. In the \nConference on Object-Oriented Programming, Systems, Languages, and Applica\u00adtions, 2004.  [18] E.-J. \nIm, K.Yelick, and R.Vuduc. Sparsity: Optimization framework for sparse matrixkernels. Int.J. HighPerform. \nComput. Appl., 18(1):135 158, 2004. [19]Y. Jiang,E. Zhang,K.Tian,F. Mao,M. Geathers,X. Shen, andY. Gao. \nExploiting statistical correlations for proactive prediction of program behaviors. In Proceedings of \nthe Inter\u00adnational Symposium on Code Generation and Optimization (CGO), pages 248 256, 2010. [20] G. \nJin, A.V. Thakur, B. Liblit, and S. Lu. Instrumentation and sampling strategies for cooperative concurrency \nbug isolation. In Proceedings of ACM SIGPLAN Conference on Object-Oriented Programming Systems, Languages \nand Applications, 2010. [21]T.P. KistlerandM. Franz. Continuous program optimization: a case study. ACMTransactions \nonProgramming Languages and Systems, 25(4):500 548, 2003. [22] H. Leather, E. Bonilla, and M. O Boyle. \nAutomatic feature generation for machine learning based optimizing compilation. In Proceedings of the \nInternational Symposium on Code Generation and Optimization (CGO), 2009. [23] X. Li, M. J. Garzaran, \nand D. Padua. A dynamically tuned sorting library. In Proceedings of the International Symposium on Code \nGeneration and Optimization, pages 111 124, 2004. [24] B. Liblit, A. Aiken, A. X. Zheng, and M. I. Jordan. \nBug isolation via remote program sampling. In Proceedings ofACM SIGPLAN Conference on Programming Language \nDesign and Implementation, 2003. [25] F. Mao and X. Shen. Cross-input learning and discriminative prediction \nin evolvable virtual machine. In Proceedings of the International Symposium on Code Generation and Optimization \n(CGO), pages 92 101, 2009. [26] F. Mao, E. Zhang, and X. Shen. In.uence of program inputs on the selectionofgarbage \ncollectors. In Proceedings of the International Conference onVirtual ExecutionEnvironments (VEE), pages \n91 100, 2009. [27] M.Paleczny, C.Vic, and C. Click. The Java Hotspot(TM) server compiler. In USENIXJavaVirtual \nMachine Research andTechnology Symposium, pages 1 12, 2001. [28] M. Puschel, J. Moura, J. Johnson, D. \nPadua, M. Veloso, B. Singer, J. Xiong,F. Franchetti, A. Gacic,Y.Voronenko, K. Chen, R. Johnson, and N. \nRizzolo. SPIRAL: code generation for DSP transforms. Proceedings of the IEEE, 93(2):232 275, 2005. [29] \nJ. Singer, G. Brown, I.Watson, and J. Cavazos. Intelligent selection of application-speci.c garbage collectors. \nIn Proceedings of the International Symposium on Memory Management, pages 91 102, 2007. [30] A. Srivastavaand \nJ. Thiagarajan. Effectively prioritizing tests in development environment. In Proceedings of International \nSymposium on SoftwareTesting and Analysis, 2002. [31] N. Thomas, G. Tanase, O. Tkachyshyn, J. Perdue, \nN. M. Amato, and L. Rauchwerger. A framework for adaptive algorithm selection in STAPL. In Proceedings \nof theTenth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, pages 277 288, \n2005. [32] K. Tian, Y. Jiang, E. Zhang, and X. Shen. An input\u00adcentric paradigm for program dynamic optimizations. \nIn the Conference on Object-Oriented Programming, Systems, Languages, and Applications (OOPSLA), 2010. \n[33]M.VossandR. Eigenmann. High-level adaptive programop\u00adtimization with ADAPT. In ProceedingsofACM Symposium \non Principles and Practice ofParallel Programming, pages 93 102, Snowbird, Utah, June 2001. [34] B.Wagner. \nCollaborative compilation. PhD thesis, Computer Science Dept., MIT, 2006. [35] C.Wang andZ. Li.Parametric \nanalysis for adaptive compu\u00adtation of.oading. In ProceedingsofACM SIGPLAN Confer\u00adence on Programming \nLanguages Design and Implementa\u00adtion, pages 119 130, 2004. [36] R. C. Whaley, A. Petitet, and J. Dongarra. \nAutomated empirical optimizations of software and theATLAS project. Parallel Computing, 27(1-2):3 35, \n2001. [37] R.W.Wisniewski,P.F. Sweeney,K. Sudeep,M. Hauswirth, E. Duesterwald, C. Cascaval, and R. Azimi. \nPerfor\u00admance and environment monitoring for whole-system char\u00adacterization and optimization. In PAC2 \nConference on Power/Performance Interaction with Architecture, Circuits, and Compilers, 2004. [38] X. \nZhuang, S. Kim, M. Serrano, and J. Choi. Perfdiff: a framework for performance difference analysis in \na virtual machine environment. In Proceedings of the International Symposium on Code Generation and Optimization, \n2008.   \n\t\t\t", "proc_id": "2048066", "abstract": "<p>Dynamic program optimizations are critical for the efficiency of applications in managed programming languages and scripting languages. Recent studies have shown that exploitation of program inputs may enhance the effectiveness of dynamic optimizations significantly. However, current solutions for enabling the exploitation require either programmers' annotations or intensive offline profiling, impairing the practical adoption of the techniques.</p> <p>This current work examines the basic feasibility of transparent integration of input-consciousness into dynamic program optimizations, particularly in managed execution environments. It uses transparent learning across production runs as the basic vehicle, and investigates the implications of cross-run learning on each main component of input-conscious dynamic optimizations. It proposes several techniques to address some key challenges for the transparent integration, including randomized inspection-instrumentation for cross-user data collection, a sparsity-tolerant algorithm for input characterization, and selective prediction for efficiency protection. These techniques make it possible to automatically recognize the relations between the inputs to a program and the appropriate ways to optimize it. The whole process happens transparently across production runs; no need for offline profiling or programmer intervention. Experiments on a number of Java programs demonstrate the effectiveness of the techniques in enabling input-consciousness for dynamic optimizations, revealing the feasibility and potential benefits of the new optimization paradigm in some basic settings.</p>", "authors": [{"name": "Kai Tian", "author_profile_id": "81430601479", "affiliation": "The College of William and Mary, Williamsburg, VA, USA", "person_id": "P2839331", "email_address": "ktian@cs.wm.edu", "orcid_id": ""}, {"name": "Eddy Zhang", "author_profile_id": "81375606973", "affiliation": "The College of William and Mary, Williamsburg, VA, USA", "person_id": "P2839332", "email_address": "eddy@cs.wm.edu", "orcid_id": ""}, {"name": "Xipeng Shen", "author_profile_id": "81452603368", "affiliation": "The College of William and Mary, Williamsburg, VA, USA", "person_id": "P2839333", "email_address": "xshen@cs.wm.edu", "orcid_id": ""}], "doi_number": "10.1145/2048066.2048103", "year": "2011", "article_id": "2048103", "conference": "OOPSLA", "title": "A step towards transparent integration of input-consciousness into dynamic program optimizations", "url": "http://dl.acm.org/citation.cfm?id=2048103"}