{"article_publication_date": "10-22-2011", "fulltext": "\n Product Lines of Theorems Benjamin Delaware William R. Cook Don Batory Department of Computer Science \nUniversity of Texas at Austin {bendy,wcook,batory}@cs.utexas.edu Abstract Mechanized proof assistants \nare powerful veri.cation tools, but proof development can be dif.cult and time-consuming. When verifying \na family of related programs, the effort can be reduced by proof reuse. In this paper, we show how to \nen\u00adgineer product lines with theorems and proofs built from fea\u00adture modules. Each module contains proof \nfragments which are composed together to build a complete proof of correct\u00adness for each product. We \nconsider a product line of pro\u00adgramming languages, where each variant includes metathe\u00adory proofs verifying \nthe correctness of its semantic de.ni\u00adtions. This approach has been realized in the Coq proof as\u00adsistant, \nwith the proofs of each feature independently certi.\u00adable by Coq. These proofs are composed for each \nlanguage variant, with Coq mechanically verifying that the composite proofs are correct. As validation, \nwe formalize a core calcu\u00adlus for Java in Coq which can be extended with any combi\u00adnation of casts, interfaces, \nor generics. Categories and Subject Descriptors D.3.1 [Programming Languages]: D.3.1 Formal De.nitions \nand Theory General Terms Design, Theory, Veri.cation Keywords Feature-Orientation, Mechanized Metatheory, \nProduct Line Veri.cation 1. Introduction Mechanized theorem proving is hard: large-scale proof de\u00advelopments \n[13, 16] take multiple person-years and consist of tens of thousand lines of proof scripts. Given the \neffort in\u00advested in formal veri.cation, it is desirable to reuse as much of the formalization as possible \nwhen developing similiar proofs. The problem is compounded when verifying mem\u00adbers of a product line \n a family of related systems [2, 5] Permission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior \nspeci.c permission and/or a fee. OOPSLA 11, October 22 27, 2011, Portland, Oregon, USA. Copyright c &#38;#169; \n2011 ACM 978-1-4503-0940-0/11/10. . . $10.00 in which the prospect of developing and maintaining indi\u00advidual \nproofs for each member is untenable. Product lines can be decomposed into features units of functionality. \nBy selecting and composing different features, members of a product line can be synthesized. The challenge \nof feature modules for software product lines is that their contents cut across normal object-oriented \nboundaries [5, 25]. The same holds for proofs. Feature modularization of proofs is an open, fundamental, \nand challenging problem. Surprisingly, the programming language literature is re\u00adplete with examples \nof product lines which include proofs. These product lines typically only have two members, con\u00adsisting \nof a core language such as Featherweight Java (FJ) [14], and an updated one with modi.ed syntax, seman\u00adtics, \nand proofs of correctness. Indeed, the original FJ paper also presents Featherweight Generic Java (FGJ), \na modi.ed version of FJ with support for generics. An integral part of any type system are the metatheoretic \nproofs showing type soundness a guarantee that the type system statically en\u00adforces the desired run-time \nbehavior of a language, typically preservation and progress [24]. Typically, each research paper only \nadds a single new fea\u00adture to a core calculus, and this is accomplished manually. Reuse of existing syntax, \nsemantics, and proofs is achieved by copying existing rules, and in the case of proofs, fol\u00adlowing the \nstructure of the original proof with appropriate updates. As more features are added, this manual process \ngrows increasingly cumbersome and error prone. Further, the enhanced languages become more dif.cult to \nmaintain. Adding a feature requires changes that cut across the normal structural boundaries of a language \n its syntax, operational semantics, and type system. Each change requires arduously rechecking existing \nproofs by hand. Using theorem provers to mechanically formalize lan\u00adguages and their metatheory provides \nan interesting testbed for studying the modularization of product lines which in\u00adclude proofs. By implementing \nan extension in the proof as\u00adsistant as a feature module, which includes updates to ex\u00adisting de.nitions \nand proofs, we can compose feature mod\u00adules to build a completely mechanized de.nition of an en\u00adhanced \nlanguage, with the proofs mechanically checked by the theorem prover. Stepwise development is enabled, \nand it  Figure 1: Selected FJ De.nitions with FGJ Changes Highlighted is possible to start with a core \nlanguage and add features to progressively build a family or product line of more detailed languages \nwith tool support and less dif.culty. In this paper, we present a methodology for feature\u00adoriented development \nof a language using a variant of FJ as an example. We implement feature modules in Coq [8] and demonstrate \nhow to build mechanized proofs that can adapt to new extensions. Each module is a separate Coq .le which \nincludes inductive de.nitions formalizing a language and proofs over those de.nitions. A feature s proofs \ncan be independently checked by Coq, with no need to recheck existing proofs after composition. We validate \nour approach through the development of a family of feature-enriched lan\u00adguages, culminating in a generic \nversion of FJ with generic interfaces. Though our work is geared toward mechanized metatheory in Coq, \nthe techniques should apply to different formalizations in other higher-order proof assistants. 2. Background \n2.1 On the Importance of Engineering Architecting product lines (sets of similar programs) has long existed \nin the software engineering community [18, 23]. So too has the challenge of achieving object-oriented \ncode reuse in this context [26, 30]. The essence of reusable designs be they code or proofs is engineering. \nThere is no magic bullet, but rather a careful trade-off between .exibility and specialization. A spectrum \nof common changes must be explicitly anticipated in the construction of a feature and its interface. \nThis is no different from using abstract classes and interfaces in the design of OO frameworks [7]. The \nplug-compatibility of features is not an after-thought but is essential to their design and implementation, \nallowing the easy integration of new features as long as they satisfy the assumptions of existing features. \nOf course, unanticipated features do arise, requiring a refactoring of existing modules. Again, this \nis no different than typical software development. Exactly the same ideas hold for modularizing proofs. \nIt is against this backdrop that we motivate our work.  2.2 A Motivating Example Consider adding generics \nto the calculus of FJ [14] to pro\u00adduce the FGJ calculus. The required changes are woven throughout the \nsyntax and semantics of FJ. The left-hand column of Figure 1 presents a subset of the syntax of FJ, the \nrules which formalize the subtyping relation that establish the inheritance hierarchy, and the typing \nrule that ensures expressions for object creation are well-formed. The corre\u00adsponding de.nitions for \nFGJ are in the right-hand column. The categories of changes are tagged in Figure 1 with Greek letters: \n  Figure 2: An Example FJ Proof with FGJ Changes Highlighted a. Adding new rules or pieces of syntax. \nFGJ adds type vari\u00adables to parameterize classes and methods. The subtyping relation adds the GS-VAR \nrule for this new kind of type. \u00df. Modifying existing syntax. FGJ adds type parameters to method calls, \nobject creation, casts, and class de.nitions. .. Adding new premises to existing typing rules. The up\u00addated \nGT-NEW rule includes a new premise requiring that the type of a new object must be well-formed. d. Extending \njudgment signatures. The added rule GS-VAR looks up the bound of a type variable using a typing context, \n.. This context must be added to the signature of the subtyping relation, transforming all occurrences \nto a new ternary relation. .. Modifying premises and conclusions in existing rules. The type parameters \nused for the parent class D in a class de.nition are instantiated with the parameters used for the child \nin the conclusion of GS-DIR. In addition to syntax and semantics, the de.nitions of FJ and FGJ include \nproofs of progress and preservation for their type systems. With each change to a de.nition, these proofs \nmust also be updated. As with the changes to de.nitions in Figure 1, these changes are threaded throughout \nexisting proofs. Consider the related proofs in Figure 2 of a lemma used in the proof of progress for \nboth languages. These lem\u00admas are used in the same place in the proof of progress and are structurally \nsimilar, proceeding by induction on the derivation of the subtyping judgment. The proof for FGJ has been \nadapted to re.ect the changes that were made to its de.nitions. These changes are highlighted in Figure \n2 and marked with the kind of de.nitional change that triggered the update. Throughout the lemma, the \nsignature of the sub\u00adtyping judgment has been altered include a context for type variablesd. The statement \nof the lemma now uses the auxil\u00adiary bound function, due to a modi.cation to the premises of the typing \nrule for .eld lookup.. These changes are not simply syntactic: both affect the applications of the induc\u00adtive \nhypothesis in the GS-TRANS case. The proof must now include a case for the added GS-VAR subtyping rulea. \nThe case for GS-DIR requires the most drastic change, as the ex\u00adisting proof for that case is modi.ed \nto include an additional statement about the behavior of bound. As more features are added to a language, \nits metatheo\u00adretic proofs of correctness grow in size and complexity. In addition, each different selection \nof features produces a new language with its own syntax, type system, operational se\u00admantics. While the \nproof of type safety is similar for each language, (potentially subtle) changes occur throughout the \nproof depending on the features included. By modularizing the type safety proof into distinct features, \neach language variant is able to build its type safety proof from a com\u00admon set of proofs. There is no \nneed to manually maintain separate proofs for each language variant. As we shall see, this allows us \nto add new features to an existing language in a structured way, exploiting existing proofs to build \nmore feature-rich languages that are semantically correct.  We demonstrate in the following sections \nhow each kind of extension to a language s syntax and semantics outlined above requires a structural \nchange to a proof. Base proofs can be updated by .lling in the pieces required by these changes, enabling \nreuse of potentially complex proofs for a number of different features. Further, we demonstrate how this \nmodularization can be achieved within the Coq proof assistant. In our approach, each feature has a set \nof as\u00adsumptions that serve as variation points, allowing a feature s proofs to be checked independently. \nAs long as an extension provides the necessary proofs to satisfy these assumptions, the composite proof \nis guaranteed to hold for any composed language. Generating proofs for a composed language is thus a \nstraightforward check that all dependencies are satis.ed, with no need to recheck existing proofs. 3. \nThe Structure of Features Features impose a mathematical structure on the universe of programming languages \n(including type systems and proofs of correctness) that are to be synthesized. In this section, we review \nconcepts that are essential to our work. 3.1 Features and Feature Compositions We start with a base language \nor base feature to which extensions are added. It is modeled as a constant or zero\u00adary function. For \nour study, the core Featherweight Java cFJ language is a cast-free variant of FJ. (This omission is not \nwithout precedent, as other core calculi for Java [28] omit casts). There are also optional features, \nwhich are unary functions, that extend the base or other features: cFJ core Featherweight Java Cast adds \ncasts to expressions Interface adds interfaces Generic adds type parameters Assuming no feature interactions, \nfeatures are composed by function composition (\u00b7). Each expression corresponds to a composite feature \nor a distinct language. Composing Cast with cFJ builds the original version of FJ: cFJ // Core FJ Cast \n\u00b7 cFJ // Original FJ [14] Interface \u00b7 cFJ // Core FJ with Interfaces Interface \u00b7 Cast \u00b7 cFJ // Original \nFJ with Interfaces Generic \u00b7 cFJ // Core Featherweight Generic Java Generic \u00b7 Cast \u00b7 cFJ // Original \nFGJ Generic \u00b7 Interface \u00b7 cFJ // core Generic FJ with // Generic Interfaces Generic \u00b7 Interface // FGJ \nwith \u00b7 Cast \u00b7 cFJ // Generic Interfaces 3.2 Feature Models Not all compositions of features are meaningful. \nSome fea\u00adtures require the presence or absence of other features. An if statement, for example, requires \na feature that introduces some notion of booleans to use in test conditions. Feature models de.ne the \ncompositions of features that produce meaningful languages. A feature model is a context sensitive grammar, \nconsisting of a context free grammar whose sen\u00adtences de.ne a superset of all legal feature expressions, \nand a set of constraints (the context sensitive part) that eliminates nonsensical sentences [6]. The \ngrammar of feature model P (below) de.nes eight sentences (features k, i, j are optional; b is mandatory). \nContraints limit legal sentences to those that have at least one optional feature, and if feature k is \nselected, so too must j. P :[k][i][j] b; // context free grammar k . j . i; // additional constraints \nk . j; Given a sentence of a feature model ( kjb ) a dot-product is taken of its terms to map it to an \nexpression (k \u00b7 j \u00b7 b). A language is synthesized by evaluating the expression. The feature model L that \nused in our study is context free: L : [Generic][Interface][Cast] cFJ; 3.3 Multiple Representations \nof Languages Every base language (cFJ) has multiple representations: its syntax scFJ, operational semantics \nocFJ, type system tcFJ , and metatheory proofs pcFJ . A base language is a tuple of representations cFJ \n= [scFJ ,ocFJ,tcFJ,pcFJ]. An optional feature i extends each representation: the language s syn\u00adtax is \nextended with new productions Lsi, its operational se\u00admantics are extended by modifying existing rules \nand adding new rules to handle the updated syntax Loi, etc. Each of these changes is modeled by a unary \nfunction. Feature i is a tuple of such functions i =[Lsi, Loi, Lti, Lpi] that update each representation \nof a language. The representations of a language are computed by com\u00adposing tuples element-wise. The \ntuple for language FJ = Cast \u00b7 cFJ is: FJ = Cast \u00b7 cFJ =[LsC, LoC, LtC, LpC] \u00b7 [sFJ, oFJ, tFJ, pFJ] =[LsC \n\u00b7 sFJ, LoC \u00b7 oFJ, LtC \u00b7 tFJ , LpC \u00b7 pFJ] That is, the syntax of FJ is the syntax of the base sFJ com\u00adposed \nwith extension LsC, the semantics of FJ are the base semantics oFJ composed with extension LoC, and so \non. In this way, all parts of a language are updated lock-step when features are composed. See [5, 12] \nfor generalizations of these ideas.  3.4 Feature Interactions Feature interactions are ubiquitous. Consider \nthe Interface feature which introduces syntax for interface declarations: J ::= interface I {Mty}  This \ndeclaration may be changed by other features. When Generic is added, the syntax of an interface declaration \nmust be updated to include type parameters: J ::= interface I (XN) {Mty} Similarly, any proofs in Generic \nthat induct over the deriva\u00adtion of the subtyping judgement must add new cases for the subtyping rule \nintroduced by the Interface feature. Such proof updates are necessary only when both features are present. \nThe set of additional changes made across all representations is the interaction of these features, written \n1 Generic#Interface. Until now, features were composed by only one operation (dot or \u00b7). Now we introduce \ntwo additional operations: product (\u00d7) and interaction (#). When designers want a set of features, they \nreally want the \u00d7-product of these features, which includes the dot-product of these features and their \ninteractions. The \u00d7-product of features f and g is: f \u00d7 g =(f#g) \u00b7 f \u00b7 g (1) where # distributes over \ndot and # takes precedence over dot: f#(g \u00b7 h)=(f#g) \u00b7 (f#h) (2) That is, the interaction of a feature \nwith a dot-product is the dot-product of their interactions. \u00d7 is right-associative and # is commutative \nand associative.2 The connection of \u00d7 and # to prior discussions is sim\u00adple. To allow for feature interactions, \na sentence of a feature model ( kjb ) is mapped to an expression by a \u00d7-product of its terms (k \u00d7 j \u00d7 \nb). Equations (1) and (2) are used to re\u00adduce an expression with \u00d7 operations to an expression with only \ndot and #, as below: p = k \u00d7 j \u00d7 b // def of p = k \u00d7 ( j#b \u00b7 j \u00b7 b ) //(1) = k#(j#b \u00b7 j \u00b7 b) \u00b7 k \u00b7 (j#b \n\u00b7 j \u00b7 b) //(1) = k#j#b \u00b7 k#j \u00b7 k#b \u00b7 k \u00b7 j#b \u00b7 j \u00b7 b // (2) (4) 1 Our Generic#Interface example is isomorphic \nto the classical example of .re and .ood control [15]. Let b denote the design of a building. The flood \ncontrol feature adds water sensors to every .oor of b. If standing water is detected, the water main \nto b is turned off. The fire control feature adds .re sensors to every .oor of b. If .re is detected, \nsprinklers are turned on. Adding .ood or .re control to the building (e.g. flood \u00b7 b and fire \u00b7 b) is \nstraightforward. However, adding both (flood \u00b7 fire \u00b7 b) is problematic: if .re is detected, the sprinklers \nturn on, standing water is detected, the water main is turned off, and the building burns down. This \nis not the intended semantics of the composition of the flood, fire, and b features. The .x is to apply \nan additional extension, labeled flood#fire, which is the interaction of flood and fire. flood#fire represents \nthe changes (extensions) that are needed to make the flood and fire features work correctly together. \nThe correct building design is flood#fire\u00b7flood\u00b7 fire \u00b7 b. 2 A more general algebra has operations \u00d7, \n#, and \u00b7 that are all commutative and associative [4]. This generality is not needed for this paper. \nLanguage p is synthesized by evaluating expression (4). Interpreting modules for individual features \nlike k, j, and b as 1-way feature interactions (where k#j denotes a 2-way interaction and k#j#b is 3-way), \nthe universe of modules in a feature-oriented construction are exclusively those of feature interactions. \nAn \u00d7-product of n features results in O(2n) interactions (i.e. all possible feature combinations). Fortunately, \nthe vast majority of feature interactions are empty, meaning that they correspond to the identity transformation \n1, whose proper\u00adties are: 1 \u00b7 f = f \u00b7 1 = f (3) Most non-empty interactions are pairwise (2-way). Occa\u00adsionally \nhigher-order interactions arise. The \u00d7-product of cFJ, Interface, and Generic is: Generic \u00d7 Interface \n\u00d7 cFJ = Generic#Interface#cFJ \u00b7 Generic#Interface \u00b7 Generic#cFJ \u00b7 Generic \u00b7 Interface#cFJ \u00b7 Interface \n\u00b7 cFJ = Generic#Interface \u00b7 Generic \u00b7 Interface \u00b7 cFJ which means that all 2-and 3-way interactions, \nexcept Generic#Interface, equal 1. In our case study, the com\u00adplete set of interaction modules that are \nnot equal to 1 is: Module Description cFJ core Featherweight Java Cast cast Interface interfaces Generic \ngenerics Generic#Interface generic and interface interactions Generic#Cast generic and cast interactions \n Each of these interaction modules is represented by a tuple of de.nitions or a tuple of changes to these \nde.nitions. 4. Decomposing a Language into Features We designed features to be monotonic: what was true \nbe\u00adfore a feature is added remains valid after composition, al\u00adthough the scope of validity may be quali.ed. \nThis is stan\u00addard in feature-based designs, as it simpli.es reasoning with features [2]. All representations \nof a language (syntax, operational semantics, type system, proofs) are themselves written in distinct \nlanguages. Language syntax uses BNF, operational semantics and type systems use standard rule notations, \nand metatheoretic proofs are formal proofs in Coq. Despite these different representations, there are \nonly two kinds of changes that a feature makes to a document: new de.nitions can be added and existing \nde.nitions can be mod\u00adi.ed. Addition is just the union of de.nitions. Modi.cation requires de.nitions \nto be engineered for change. In the following sections, we explain how to accomplish addition and modi.cation. \nWe alert readers that our tech\u00adniques for extending language syntax are identical to ex\u00adtension techniques \nfor the other representations. The critical contribution of our approach is how we guarantee the cor\u00adrectness \nof composed proofs, the topic of Section 5.1.  4.1 Language Syntax We use BNF to express language syntax. \nFigure 3a shows the BNF for expressions in cFJ, Figure 3b the production that the Cast feature adds to \ncFJ s BNF, and Figure 3c the composition (union) of these productions, that de.nes the expression grammar \nof the FJ = Cast \u00b7 cFJ language (Figure 1). e ::= x | e.f e ::= x| e.m(e) | e.f | new C(e); | e.m(e) \n(a) | new C(e) |(C) e ; e ::= (C) e ; (c) (b) Figure 3: Union of Grammars Modifying existing productions \nrequires foresight to an\u00adticipate how productions may be changed by other features. (This is no different \nfrom object-oriented refactorings that prepare source code for extensions visitors, frameworks, strategies, \netc. as discussed in Section 2.) Consider the impact of adding the Generics feature to cFJ and Cast: \ntype parameters must be added to the expression syntax of method calls and class types now have type \nparameters. What we do is to insert variation points (VP), a standard concept in product line designs \n[1], to allow new syntax to appear in a production. For syntax rules, a VP is simply the name of an (initially) \nempty production. Figure 4a-b shows the VPs TPm added to method calls in the cFJ expression grammar and \nTPt added to class types in the cFJ and Cast expression grammars. Figure 4c shows the composition (union) \nof the revised Cast and cFJ expression grammars. Since TPm and TPt are empty, Figure 4c can be inlined \nto produce the grammar in Figure 3c. Now consider the changes that Generic makes to ex\u00adpression syntax: \nit rede.nes TPm and TPt to be lists of type parameters, thereby updating all productions that reference \nthese VPs. Figure 5a shows this de.nition. Figure 5b shows the productions of Figure 4c with these productions \ninlined, building the expression grammar for Generic \u00b7 Cast \u00b7 cFJ. Replacing an empty production with \na non-empty one is a standard programming practice in frameworks (e.g. EJB [19]). Framework hook methods \nare initially empty and users can override them with a de.nition that is speci.c to their context. We \ndo the same here. e ::= x | e.f e ::= x | TPm e.m (e) | e.f | new ( TPt C) (e); | TPm e.m (e) TPm ::= \nE; | new ( TPt C) (e) TPt ::= E; | ( TPt C) e; (a) TPm : E; TPt : E; e ::= ( TPt C) e; (c) (b) Figure \n4: Modi.cation of Grammars e ::= x | e.f TPm ::= (T); | (T) e.m (e)TPt ::= (T); | new ((T) C)(e) |((T) \nC) e; (a) (b) Figure 5: The Effect of Adding Generics to Expressions These are simple and intuitively \nappealing techniques for de.ning and composing language extensions. As readers will see, these same ideas \napply to rules and proofs as well.  4.2 Reduction and Typing Rules The judgments that form the operational \nsemantics and type system of a language are de.ned by rules. Figure 6a shows the typing rules for cFJ \nexpressions, Figure 6b the rule that the Cast feature adds, and Figure 6c the composition (union) of \nthese rules, de.ning the typing rules for FJ. fields(C)= Vf fields(C)= Vf G f e : U U <:V G f e : U U \n<:V G f new C(e): C (T-NEW) G f new C(e): C . (T-NEW) . . . . .(a) G f e0: D D <:C G f e0: D D <:C G \nf e0: C G f e0: C (T-UCAST) (T-UCAST) (c) (b) Figure 6: Union of Typing Rules Modifying existing rules \nis analogous to language syn\u00adtax. There are three kinds of VPs for rules: (a) predicates that extend \nthe premise of a rule, (b) relational holes which extend a judgement s signature, and (c) functions that \ntrans\u00adform existing premises and conclusions. Predicate and rela\u00adtional holes are empty by default. The \nidentity function is the default for functions. This applies to both the reduction rules that de.ne a \nlanguage s operational semantics and the typing rules that de.ne its type system.  To build the typing \nrules for FGJ, the Generic feature adds non-empty de.nitions for the WFc (D, TPt C) predicate and for \nthe D relational hole in the cFJ typing de.nitions. (Compare Figure 6a to its VP-extended counterpart \nin Fig\u00adure 7a). Figure 7b shows the non-empty de.nitions for these VPs introduced by the Generic feature, \nwith Figure 7c showing the T-NEW rule with these de.nitions inlined. . f(T)C ok WFc(D, TPt C) WFc(., \n(T) C) C)= Vf D; G f e : U D := . D f U <:V (b) D; Gfnew( TPt C)(e) : TPt C . f(T)C ok (T-NEW) fields((T)C)= \nVf T .; G f e : U WFc(E, C, E) . f U <:V .; G fnew((T)C)(e):(T)C D := E (GT-NEW) (a) (c) Figure 7: Building \nGeneric Typing Rules  4.3 Theorem Statements Variation points also appear in the statements of lemmas \nand theorems, enabling the construction of feature-extensible proofs. Consider the lemma in Figure 8 \nwith its seven VPs. TPt : VP for Class Types TPm : VP for Method Call Expression \u00b5 : VP for Method Types \nD : Relational Hole for Typing Rules WFmc(D, \u00b5, TPm) : Predicate for T-INVK WFne(D, TPt C) : Predicate \nfor T-NEW FM(TPm, \u00b5, T) : Transformation for Return Types Lemma 4.1 (Well-Formed MBody). If mtype(m, \nTPt C)= \u00b5 V . V, and with WFne(D, TPt C) mbody(TPm, m, TPt C)= x.e, where WFmc(D, \u00b5, TPm), then there \nexists some N and S such that D f TPt C <:N and D f S <:FM(TP m, \u00b5, V) and D; x : FM(TPm, \u00b5, V), this \n: N f e : S. Figure 8: VPs in a Parameterized Lemma Statement Different instantiations of VPs produce \nvariations of the original productions and rules, with the lemma adapting accordingly. Figure 9 shows \nthe VP instantiations and the corresponding statement for both cFJ and FGJ (E stands for empty in the \ncFJ case) with those instantiations inlined for clarity. Without an accompanying proof, feature-extensible \nthe\u00adorem statements are uninteresting. Ideally, a proof should adapt to any VP instantiation or case \nintroduction, allow\u00ading the proof to be reused in any target language variant. Of course, proofs must \nrule out broken extensions which do not guarantee progress and preservation, and admit only cor\u00adrect \nnew cases or VP instantiations. This is the key chal\u00adlenge in crafting modular proofs. 5. Implementing \nFeature Modules in Coq The syntax, operational semantics, and typing rules of a lan\u00adguage are embedded \nin Coq as standard inductive data types. The metatheoretic proofs of a language are then written over \nthese encodings. Figure 10a-b gives the Coq de.nitions for the syntax of Figure 3a and the typing rules \nof Figure 7a. A feature module in Coq is realized as a Coq .le containing its de.nitions and proofs. \nThe target language is itself a Coq .le which combines the de.nitions and proofs from a set of Coq feature \nmodules. Definition TP_m := unit. Definition TP_t := unit. Inductive C : Set := | ty : TP_t . Name . \ne. Inductive e : Set := | e_var : Var . e | fd_access : e . F . e | m_call : TP_m . e . M . List e . \ne | new : C . List e . e. (a) Definition Context := Var_Context. Definition WF_c (gamma : Context)(c \n: C):= True. Inductive Exp_WF : Context . e . Ty . Prop := | T_New : forall gamma c us tp d_fds es, \nWF_c gamma (ty tp c) . fields (ty tp c) d_fds . Exps_WF gamma es us . subtypes gamma us d_fds . Exp_WF \ngamma (new (ty tp c) es) (ty tp c). . . . (b) Figure 10: Coq Encoding of Fig. 4a and Fig. 7a. As shown \nin Figure 10, each feature includes the default de.nitions for its variation points. When composed with \nfea\u00adtures that provide new de.nitions for a variation point, these de.nitions are updated for the target \nlanguage. In the case of syntax, the .nal de.nition of a VP is the juxtaposition of  TPt : E; TPm : \nE; \u00b5 : E; D := E T WFmc(E, E, T) T WFne(E, C) FM(E, E, T) := T Lemma 4.1 (cFJ Well-Formed MBody). If \nmtype(m, C) = V . V and Twith mbody(m, C) = x.e where T, then there exists some N and S such that f C \n<:N and f S <:V and x : V, this : N f e : S. TPt : T; TPm : T; \u00b5 : (Y P); D := . . f U ok . f U <:[U/Y]P \nWFmc(., (Y P), U) . f (T)C ok WFne(., (T)C) FM((T), (Y P), U) := [T/Y]U Lemma 4.1 (FGJ Well-Formed MBody). \nIf mtype(m, (T)C) = (Y P)V . V and . f (T)C ok with mbody((U), m, (T)C) = x.e, where . f U ok and . f \nU <:[U/Y]P, then there exists some N and S such that . f (T)C <:N and . f S <:[U/Y]V and .; x : [U/Y]V, \nthis : N f e : S Figure 9: VP Instantiations for cFJ and Generic and the resulting statements of Lemma \n4.1 for cFJ and FGJ the de.nitions from each of the features. For abstract pred\u00adicates, the target predicate \nis the conjuction of all the VP de.nitions. The Coq encoding of expressions the Cast, and Generic features \nand the result of their composition with cFJ is given in Figure 11. Inductive e : Set := | cast : C . \ne . e. Cast Definition TP_m := list Type. Definition TP_t := list Type. Generic Definition TP_m := (list \nType, unit). Definition TP_t := (list Type, unit). Inductive C : Set := | ty : TP_t . Name . e. Inductive \ne : Set := | e_var : Var . e | fd_access : e . F . e | m_call : TP_m . e . M . List e . e | new : C . \nList e . e | cast : C . e . e. Cast \u00b7 Generic \u00b7 cFJ Figure 11: Coq Encoding of Fig. 3b and Fig. 5a-b. \nIn the discussion so far, composition has been strictly syn\u00adtactic: de.nitions are directly unioned together \nor defaults are replaced. Modular reasoning within a feature requires a more semantic form of composition \nthat is supported by Coq. OO frameworks are implemented using inheritance and mixin layers [3], techniques \nthat are not available in most proof assistants. Our feature modules instead rely on the higher-order \nparameterization mechanisms of the Coq the\u00adorem prover to support case extension and VPs. Modules can \nnow be composed within Coq by instantiating param\u00adeterized de.nitions. Using Coq s native abstraction \nmecha\u00adnism enables independent certi.cation of each of the feature modules. Figure 12 shows a concrete \nexample of crafting an ex\u00adtensible inductive de.nition in Coq. The target language of FJ = Cast \u00b7 cFJ \nis built by importing the Coq modules for features cFJ and Cast. The target syntax is de.ned as a new \ndata type, e, with data constructors cFJ and Cast from each feature. Each constructor wraps the syntax \nde.nitions from their corresponding features, closing the inductive loop by instantiating the abstract \nparameter e with e , the data type for the syntax of target language. Inductive e (e : Set): Set := | \ne_var : Var . e | fd_access : e . F . e | m_call : e . M . List e . e | new : Ty . List e . e. cFJ.v \nInductive e (e : Set) : Set := | e_cast : Ty . e . e. cast.v Require Import cFJ. Require Import cast. \nInductive e : Set := | cFJ : cFJ.e e . e | cast : cast.e e . e. FJ.v Figure 12: Syntax from cFJ and Cast \nFeatures and their Union. These parameters also affect data types which reference open inductive de.nitions. \nIn particular, the signature of typ\u00ading rules and the transition relation are now over the pa\u00adrameter \nused for the .nal language. Exp_WF from Fig. 10b ranges over the complete set of expressions from the \n.nal language, so its signature becomes . e : Set, Context . e . Ty . Prop. Of course, within a feature \nmod\u00adule these rules are written over the actual syntax de.nitions it provides. In order for the signatures \nto sync up, these rules are parameterized by a function that injects the syn\u00adtax de.ned in the feature \nmodule into the syntax of the .nal language. Since the syntax of a module is always included alongside \nits typing and reduction rules in the target lan\u00adguage, such an injection always exists.  Parameterization \nalso allows feature modules to include VPs, as shown in Figure 13. The VPs in each module are ex\u00adplicitly \nrepresented as abstract sets/predicates/functions, as with the parameter TP_m used to extend the expression \nfor method calls in cFJ.v. Other features can provide appropriate instantiations for this parameter. \nIn Figure 13, for example, FGJ.v builds the syntax for the target language by instantiat\u00ading this VP \nwith the de.nition of TP_m given in Generic.v. Alternatively, the syntax of cFJ can be built from the \nsame inductive de.nition from cFJ using the default de.nition of TP_m it provides. Definition TP_m := \nunit. Inductive cFJ_e (e : Set) (TP_m : Set): Set := | e_var : Var . cFJ_e | fd_access : e . F . cFJ_e \n| m_call : TP_m . e . M . List e . cFJ_e | new : C . List e . cFJ_e. cFJ.v Definition TP_m := List Ty. \nGeneric.v Require Import cFJ. Require Import Generic. Definition TP_m := Generic.TP_m. Inductive e : \nSet := | cFJ : cFJ_e e TP_m . e FGJ.v Figure 13: Coq Syntax from cFJ with a Variation Point, and its \nInstantiation in FGJ. 5.1 Crafting Modular Proofs Rather than writing multiple related proofs, our goal \nis to create a single proof for a generic statement of a theorem. This proof is then specialized for \nthe target language by in\u00adstantiating the variation points appropriately. Instead of sep\u00adarately proving \nthe two lemmas in Figure 2, the cFJ feature has a single proof of the generic Lemma 5.1 (Figure 14). \nThis lemma is then specialized to the variants FJ and FGJ shown in Figure 2. The proof now reasons over \nthe generic subtyping rules with variation points, as in the case for S-Dir in Figure 14. The de.nition \nof these holes depends on the set of features included in the .nal language, so from the (human or computer) \ntheorem prover s point of view, they are opaque. Thus, this proof becomes stuck when it requires knowledge \nabout behavior of Ff. In order to proceed, the lemma must constrain possible VP instantiations to those \nthat have the properties required by the proof. In the case of Lemma 5.1, this behavior is that Ff must \nbe the identity function for non-variable types and that FSD maps class types to class types. For this \nproof to hold for the target language, the instantiations of Ff and FSD must have this property. More \nconcretely, the proof assumes this behavior for all instantiations of Ff and FSD, produc\u00ading the new \ngeneric Lemma 5.2. In order to produce the de\u00adsired lemma, the target language instantiates the VPs and \nprovides proofs of all the assumed behaviors. Each feature which supplies a concrete realization of a \nVP also provides the necessary proofs about its behavior. The assumptions of a proof form an explicit \ninterface against which it is written. The interface of a feature module is the union of all these as\u00adsumptions \ntogether with the the set of lemmas about the be\u00adhavior of its VP instantiations and de.nitions it provides. \nAs long as the features included in the target language provide proofs satisfying this interface, a feature \ns generic proofs can be specialized and reused in their entirety. Lemma 5.1. If . f S <:T and fields(Ff(., \nT)) = T f, then fields(Ff(., S)) = S g, Si = Ti and gi = fi for all i = #(f). Case S-DIR S = TP0 C, CP0 \nclass C extends TP1 D {S g; . . .}, T = FSD(TP0, CP0, TP1 D). By the rule F-CLASS, fields(FSD(TP0, CP0, \nTP1 D)) = U h with fields(TP0 C) = U h; FSD(TP0, CP0,S) g. As\u00adsuming that for all class types TP2 D/, \nFf(., TP2 D/) = TP2 D/ and FSD(TP0, CP0 , TP2 D/) returns a class type, Ff(., FSD(TP0 , CP0, TP1 D)) \n= FSD(TP0 , CP0, TP1 D). It follows that T f = fieldsFSD(TP0 , CP0, TP1 D)) = U h from which the conclusion \nis immediate. Figure 14: Generic Statement of Lemmas 2.2 and 2.1 and Proof for S-Dir Case.  Lemma 5.2. \nAs long as Ff(., V)= V for all non-vari\u00ad able types V and FSD maps class types to class types, if . f \nS <:T and fields(Ff(., T)) = Tf, then fields(Ff(., S)) = Sg, Si= Ti and gi= fi for all i = #(f). We also \nhave to deal with new cases. Whenever a new rule or production is added, a new case must be added to \nproofs which induct over or case split on the original production or rule. For FGJ, this means that a \nnew case must be added to Lemma 5.2 for GS-Var. When writing an inductive proof, a feature provides cases \nfor each of the rules or productions it introduces. To build the proof for the target language, a new \nskeleton proof by induction is started. Each of the cases is discharged by the proof given in the introducing \nfeature.  5.2 Engineering Extensible Proofs in Coq Each Coq feature module contains proofs for the \nextensi\u00adble lemmas it provides. To get a handle on the behavior of opaque parameters, Coq feature modules \nmake explicit as\u00adsumptions about their behavior. Just as de.nitions were pa\u00adrameterized on variation \npoints, proofs are now parameter\u00adized on a set of lemmas that de.ne legal extensions. These assumptions \nenable separate certi.cation of feature mod\u00adules. Coq certi.es that a proof is correct for all instantiations \nor case introductions that satisfy its assumptions, enabling proof reuse for all compatible features. \nAs a concrete example, consider the Coq proof of Lemma 5.3 given in Figure 16. The cFJ feature provides \nthe state\u00adment of the lemma, which is over the abstract subtype rela\u00adtion. Both the Generic and cFJ features \ngive proofs for their de.nitions of the subtype relation. Notably, the Generic feature assumes that if \na type variable is found in a Context Gamma, it will have the same value in app_context Gamma Delta for \nany Context Delta. Any compatible extension of Context and app_Context can thus reuse this proof. Lemma \n5.3 (Subtype Weakening). For all contexts G and ., if G f S<:T, G; . f S<:T. Figure 15: Weakening lemma \nfor subtyping. To build the .nal proof, the target language inducts over subtype, as shown in the .nal \nbox of Figure 16. For each constructor, the lemma dispatches to the proofs from the corresponding feature \nmodule. To reuse those proofs, each of their assumptions has to be ful.lled by a theorem (e.g. TLookup_app \nsatis.es TLookup_app). The inductive hy\u00adpothesis is provided to cFJ_Weaken_subtype_app for use on its \nsubterms. As long as every assumption is satis.ed for each proof case, Coq will certify the composite \nproof. There is one important caveat: proofs which use the induc\u00adtive hypothesis can only do so on subterms \nor subjudge\u00adments. By using custom induction schemes to build proofs, features can ensure that this check \nwill always succeed. The cFJ_subtype_ind induction scheme used to combine cFJ s cases in the .rst box \nof Figure 16 is an example.  5.3 Feature Composition in Coq Each feature module is implemented as a \nCoq .le which contains the inductive de.nitions, variation points, and proofs provided by that feature. \nThese modules are certi.ed independently by Coq. Once the feature modules have been veri.ed, a target \nlanguage is built as a new Coq .le. This .le imports the .les for each of the features included in the \nlanguage, e.g. Require Import cFJ. in Figure 12. First, each target language de.nition is built as a \nnew inductive type using appropriately instantiated de.nitions from the included feature modules, as \nshown in Figures 12 and 13. Proofs for the target language are then built using the proofs from the constituent \nfeature modules per the above discus\u00adsion. Proof composition requires a straightforward check by Coq \nthat the assumptions of each feature module are satis.ed, i.e. that a feature s interface is met by the \ntarget language. Currently each piece of the .nal language is com\u00adposed by hand in this straightforward \nmanner; future work includes automating feature composition directly. Variables (app_context : Context \n. Context . Context) (FJ_subtype_Wrap : forall gamma S T, FJ_subtype gamma S T . subtype gamma S T). \nDefinition Weaken_Subtype_app_P delta S T (sub_S_T : subtype delta S T) := forall gamma, subtype (app_context \ndelta gamma) S T. Lemma cFJ_Weaken_Subtype_app_H1 : forall (ty : Ty) (gamma : Context), Weaken_Subtype_app_P \n_ _ _ (sub_refl ty gamma). Lemma cFJ_Weaken_Subtype_app_H2 : forall c d e gamma (sub_c : subtype gamma \nc d) (sub_d : subtype gamma d e), Weaken_Subtype_app_P _ _ _ sub_c . Weaken_Subtype_app_P _ _ _ sub_d \n. Weaken_Subtype_app_P _ _ _ (sub_trans _ _ _ _ sub_c sub_d). Lemma cFJ_Weaken_Subtype_app_H3 : forall \nce c d fs k ms te te delta CT_c bld_te, Weaken_Subtype_app_P _ _ _ (sub_dir ce c d fs k ms te te delta \nCT_c bld_te). Definition cFJ_Weaken_Subtype_app := cFJ_subtype_ind _ cFJ_Weaken_Subtype_app_H1 cFJ_Weaken_Subtype_app_H2 \ncFJ_Weaken_Subtype_app_H3. Variables (app_context:Context . Context . Context) (TLookup_app : forall \ngamma delta X ty, TLookup gamma X ty . TLookup (app_context gamma delta) X ty). (GJ_subtype_Wrap : forall \ngamma S T, GJ_subtype gamma S T . subtype gamma S T). Definition Weaken_Subtype_app_P := cFJ_Pinitions.Weaken_Subtype_app_P \n_ _ subtype app_context. Lemma GJ_Weaken_Subtype_app : forall gamma S T (sub_S_T : GJ_subtype gamma S \nT), Weaken_Subtype_app_P _ _ _ sub_S_T. cbv beta delta; intros; apply GJ_subtype_Wrap. inversion sub_S_T; \nsubst. econstructor; eapply TLookup_app; eauto. Qed. Fixpoint Weaken_Subtype_app gamma S T (sub_S_T : \nsubtype gamma S T) : Weaken_Subtype_app_P _ _ _ sub_S_T := match sub_S_T return Weaken_Subtype_app_P \n_ _ _ sub_S_T with | cFJ_subtype_Wrap gamma S T sub_S_T . cFJ_Weaken_Subtype_app _ _ _ _ _ _ cFJ_Ty_Wrap \n_ _ _ CT _ subtype GJ_Phi_sb cFJ_subtype_Wrap app_context _ _ _ sub_S_T Weaken_Subtype_app | GJ_subtype_Wrap \ngamma S T sub_S_T . GJ_Weaken_Subtype_app _ _ Gty _ TLookup subtype GJ_subtype_Wrap app_context TLookup_app \n_ _ _ sub_S_T end. Figure 16: Coq proofs of Lemma 5.3 for the cFJ and Generic features and the composite \nproof.  6. Implementation of the FJ Product Line We have implemented the six feature modules of Section \n3.4 in the Coq proof assistant. Each contains pieces of syntax, semantics, type system, and metatheoretic \nproofs needed by that feature or interaction. Using them, we can built the seven variants on Featherweight \nJava listed in Section 3.23.  Module Lines of Code in Coq cFJ 2612 LOC Cast 463 LOC Interface 499 LOC \nGeneric 6740 LOC Generic#Interfaces 1632 LOC Generic#Cast 296 LOC Figure 17: Feature Module Sizes While \nwe achieve feature composition by manually in\u00adstantiating these modules, the process is straightforward \nand should be mechanizable. Except for some trivial lemmas, the proofs for a .nal language are assembled \nfrom proof pieces from its constituent features by supplying them with lemmas which satisfy their assumptions. \nImportantly, once the proofs in each of the feature modules have been certi.ed by Coq, they do not need \nto be rechecked for the target language. A proof is guaranteed to be correct for any language which satis.es \nthe interface formed by the set of assumptions for that lemma. This has a practical effect as well: certifying \nlarger feature modules takes a non-trivial amount of time. Figure 18 lists the certi.cation times for \nfeature modules and all the possible language variants built from their com\u00adposition. By checking the \nproofs of each feature in isolation, Coq is able to certify the entire product line in roughly the same \namount of time as the cFJ feature module. Rechecking the work of each feature for each individual product \nwould quickly become expensive. Independent certi.cation is par\u00adticularly useful when modifying a single \nfeature. Recertify\u00ading the product line is a matter of rechecking the proofs of the modi.ed features \nand then performing a quick check of the products, without having to recheck the independent fea\u00adtures. \n 7. Discussion and Future Work Relying on parameterization for feature composition allows feature modules \nto be built and independently certi.ed by Coq out of the box with the same level of assurance. With this \napproach, a familiar set of issues is encountered when a new feature is added to a product line. Ideally, \na new fea\u00adture would be able to simply update the existing de.nitions and proofs, allowing language designers \nto leverage all the hard work expended on formalizing the original language. Some foresight, called domain \nanalysis [22], allows lan\u00adguage designers to predict VPs in advance, thus enabling a smooth and typically \npainless addition of new features. What our work shows is a path for the structured evolution of languages. \nBut of course, when unanticipated features are added using this style of composition, additional engineering \nmay be required. Existing de.nitions can be extended and reused as long as they already have the appropriate \nVPs and their inductive de.nitions are left open. For example, once class de.nitions have a variation \npoint inserted for interfaces, the same VP can also be extended with type parameters for generics. Similarly, \nonce the de.nition of subtyping has been left open, both interfaces and generics can add new rules for \nthe target language. Proof reuse is almost as straightforward: as long as an extension is compatible \nwith the set of assumptions in a proof s interface, the proof can be reused directly in the .nal language. \nA new feature is responsible for providing the proofs that its extension satis.es the assumptions of \nthe original base language. Refactoring is necessary when a new feature requires VPs that are not in \nexisting features. A feature which makes widespread changes throughout the base language (i.e. Generic), \nwill probably make changes in places that the original feature designer did not anticipate. In this situ\u00adation, \nas mentioned in Section 2.1, existing features have to be refactored to allow the new kind of extension \nby inserting variation points or breaking open the recursion on inductive de.nitions. Any proofs over \nthe original de.nition may have to be updated to handle the new extensions, possibly adding new assumptions \nto its interface. Feature modules tend to be inoculated from changes in another, unless they reference \nthe de.nitions of another fea\u00adture. This only occurs when two features must appear to\u00adgether: modules \nwhich resolve feature interactions, for ex\u00adample, only appear when their base features are present. Thus, \nit is possible to develop an enhanced language incre\u00admentally: starting with the base and then iteratively \nrefac\u00adtoring other features, potentially creating new modules to handle interactions. Once a new feature \nhas been fully in\u00adtegrated into the feature set, all of the previous languages in the product line should \nstill be derivable. If two features F and G commute (i.e. F \u00b7 G = G \u00b7 F) their integration comes for \nfree as their interaction module is empty (i.e. F#G = 1).  A new feature can also invalidate the assumptions \nof an existing feature s proofs. In this case, assumptions might have to be weakened and the proof refactored \nto permit the new extension. Alternatively, if an extension breaks the assumption of an existing proof, \nthe offending feature can simply build a new proof of that lemma. This proof can then be utilized in \nany other proofs which used that lemma as an assumption, replacing the original lemma and allowing the \nother proofs to be reused. In this manner, each proof is insulated from features which break the assumptions \nof other lemmas. Again, all of this is just another variation of the kinds of problems that are encountered \nwhen one generalizes and refactors typical object-oriented code bases. Future work includes creating \na new module-level com\u00adposition operator that eases the burden of integrating new features. Ideally, \nthis operator will allow subsequent features to extend a feature s de.nitions with new cases or variations \nwithout modifying the feature and to provide patches to al\u00adlow existing proofs to work with the extended \nde.nitions. As alluded to earlier, by operating at the module level, this oper\u00adator would automate the \ntedious piece-by-piece composition currently employed to build target languages. 8. Related Work The \nTinkertype project [17] is a framework for modularly specifying formal languages. Features consist of \na set of variants of inference rules with a feature model determining which rule is used in the .nal \nlanguage. An implementation of these ideas was used to format the language variants used in Pierce s \nTypes and Programming Languages [24]. This corresponds to our notion of case introduction. Importantly, \nour approach uses variations points to allow variations on a single de.nition. This allows us to construct \nof a single generic proof which can be specialized for each variant, as opposed to maintaining a separate \nproof for each variation. Levin et al. consider using their tool to compose handwrit\u00adten proofs, but \nthese proofs must be rechecked after compo\u00adsition. In contrast, we have crafted a systematic approach \nto proof extension that permits the creation of machine\u00adcheckable proofs. After a module s proofs are \ncerti.ed, they can be reused without needing to be rechecked. As long as the module s assumptions hold, \nthe proofs are guaranteed to hold for the .nal language. St\u00e4rk et. al [27] develop a complete Java 1.0 \ncompiler through incremental re.nement of a set of Abstract State Machines. Starting with ExpI, a core \nlanguage of impera\u00adtive Java expressions which contains a grammar, interpreter, and complier, the authors \nadd features which incrementally update the language until an interpreter and compiler are de\u00adrived for \nthe full Java 1.0 speci.cation. The authors then write a monolithic proof of correctness for the full \nlanguage. Later work casts this approach in the calculus of features [2], noting that the proof could \nhave been developed incremen\u00adtally. While we present the incremental development of the formal speci.cation \nof a language here, many of the ideas are the same. An important difference is that our work fo\u00adcuses \non structuring languages and proofs for mechanized proof assistants, while the development proposed by \n[2] is completely by hand. Th\u00fcm et. al [29] consider proof composition in the veri\u00ad.cation of a Java-based \nsoftware product line. Each product is annotated with invariants from which the Krakatoa/Why tool generates \nproof obligations to be veri.ed in Coq. To avoid maintaining these proofs for each product, the authors \nmaintain proof pieces in each feature and compose the pieces for an individual product. Their notion \nof composition is strictly syntactic: proof scripts are copied together to build the .nal proofs and \nhave to be rechecked for each prod\u00aduct. Importantly, features only add new premises and con\u00adjunctions \nto the conclusions of the obligations generated by Krakatoa/Why, allowing syntactic composition to work \nwell for this application. As features begin to apply more subtle changes to de.nitions and proofs, it \nis not clear how to effec\u00adtively syntactically glue together Coq s proof scripts. Using the abstraction \nmechanisms provided by Coq to implement features, as we have, enables a more semantic notion of com\u00adposition. \nThe modular development of reduction rules are the fo\u00adcus of Mosses Modular Structural Operational Semantics \n[20]. In this paradigm, rules are written with an abstract la\u00adbel which effectively serves as a repository \nfor all effects, allowing rules to be written once and reused with different instantiations depending \non the effects supported by the .\u00adnal language. Effect-free transitions pass around the labels of their \nsubexpressions: X d -. d/ X(R-LETB) let d in e -. let d/ in e Those rules which rely on an effectual \ntransition specify that the .nal labeling supports effects: {p=p1[p0]...} / e - ------. e (R-LETE) {p=p1...} \nlet p0 in e - ------. let p0 in e These abstract labels correspond to the abstract contexts used by \nthe cFJ subtyping rules to accommodate the updates of the Generic feature. In the same way that R-LETE \nde\u00adpends on the existence of a store in the .nal language, S-VAR requires the .nal context to support \na type lookup op\u00aderation. Similarly, both R-LETB and S-TRANS pass along the abstract labels / contexts \nfrom their subjudgements. Both Boite [9] and Mulhern [21] consider how to extend existing inductive de.nitions \nand reuse related proofs in the Coq proof assistant. Both only consider adding new cases and rely on \nthe critical observation that proofs over the ex\u00adtended language can be patched by adding pieces for \nthe new cases. The latter promotes the idea of proof weaving for merging inductive de.nitions of two \nlanguages which merges proofs from each by case splitting and reusing exist\u00ading proof terms. An unimplemented \ntool is proposed to au\u00adtomatically weave de.nitions together. The former extends Coq with a new Extend \nkeyword that rede.nes an existing inductive type with new cases and a Reuse keyword that creates a partial \nproof for an extended datatype with proof holes for the new cases which the user must interactively .ll \nin. These two keywords explicitly extend a concrete de.ni\u00adtion and thus modules which use them cannot \nbe checked by Coq independently of those de.nitions. This presents a problem when building a language \nproduct line: adding a new feature to a base language can easily break the proofs of subsequent features \nwhich are written using the original, .xed language. Interactions can also require updates to exist\u00ading \nfeatures in order to layer them onto the feature enhanced base language, leading to the development of \nparallel fea\u00adtures that are applied depending on whether the new feature is included. These keyword extensions \nwere written for a previous version of Coq and are not available for the current version of the theorem \nprover. As a result of our formula\u00adtion, it is possible to check the proofs in each feature module independently, \nwith no need to recheck proof terms when composing features.  Chlipala [10] proposes a using adaptive \ntactics written in Coq s tactic de.nition language LTac [11] to achieve proof reuse for a certi.ed compiler. \nThe generality of the approach is tested by enhancing the original language with let expres\u00adsions, constants, \nequality testing, and recursive functions, each of which required relatively minor updates to exist\u00ading \nproof scripts. In contrast to our approach, each re.ne\u00adment was incorporated into a new monolithic language, \nwith the new variant having a distinct set of proofs to maintain. Our feature modules avoid this problem, \nas each target lan\u00adguage derives its proofs from a uniform base, with no need to recheck the proofs in \nexisting feature modules when com\u00adposing them with a new feature. Adaptive proofs could also be used \nwithin our feature modules to make existing proofs robust in to the addition of new syntax and semantic \nvaria\u00adtion points. 9. Conclusion Mechanically verifying artifacts using theorem provers can be hard work. \nThe dif.culty is compounded when verifying all the members of a product line. Features, transformations \nwhich add a new piece of functionality, are a natural way of decomposing a product line. Decomposing \nproofs along fea\u00adture boundaries enables the reuse of proofs from a common base for each target product. \nThese ideas have a natural ex\u00adpression in the evolution of formal speci.cation of program\u00adming languages, \nusing the syntax, semantics, and metathe\u00adoretic proofs of a language as the core representations. In \nthis paper, we have shown how introductions and variation points can be used to structure product lines \nof formal lan\u00adguage speci.cations. As a proof of concept, we used this approach to imple\u00adment features \nmodules that enhance a variant of Feather\u00adweight Java in the Coq proof assistant. Our implementation \nuses the standard facilities of Coq to build the composed languages. Coq is able to mechanically check \nthe proofs of progress and preservation for the composed languages, which reuse pieces of proofs de.ned \nin the composed fea\u00adtures. Each extension allows for the structured evolution of a language from a simple \ncore to a fully-featured language. Harnessing these ideas in a mechanized framework trans\u00adforms the mechanized \nformalization of a language from a rigorous check of correctness into an important vehicle for reuse \nof de.nitions and proofs across a family of related lan\u00adguages. Acknowledgments. This work was supported \nby NSF s Science of Design Project CCF 0724979. Also we appreciate comments from Thomas Th\u00fcm and the \nreferees on earlier drafts of this paper. References [1] Paul Bassett. Frame-based software engineering. \nIEEE Soft\u00adware, 4(4), 1987. [2] D. Batory and E. B\u00f6rger. Modularizing theorems for software product lines: \nThe jbook case study. Journal of Universal Computer Science, 14(12):2059 2082, 2008. [3] D. Batory, Rich \nCardone, and Y. Smaragdakis. Object\u00adoriented frameworks and product-lines. In SPLC, 2000. [4] D. Batory, \nJ. Kim, and P. H\u00f6fner. Feature interactions, prod\u00aducts, and composition. In GPCE, 2011. [5] D. Batory, \nJ.N. Sarvela, and A. Rauschmayer. Scaling Step-Wise Re.nement. IEEE TSE, 30, June 2004. [6] Don Batory. \nFeature models, grammars, and propositional formulas. Software Product Lines, pages 7 20, 2005. [7] Don \nBatory, Rich Cardone, and Yannis Smaragdakis. Object\u00adoriented framework and product lines. In SPLC, pages \n227 247, 2000. [8] Yves Bertot and Pierre Cast\u00e9ran. Interactive Theorem Proving and Program Development. \nSpringer-Verlag, Berlin, 2004. [9] Olivier Boite. Proof reuse with extended inductive types. In Theorem \nProving in Higher Order Logics, pages 50 65, 2004. [10] Adam Chlipala. A veri.ed compiler for an impure \nfunctional language. In POPL 2010, January 2010. [11] David Delahaye. A tactic language for the system \ncoq. In Proceedings of Logic for Programming and Automated Rea\u00adsoning (LPAR), Reunion Island, volume \n1955 of LNCS, pages 85 95. Springer, 2000. [12] Feature oriented programming. http://en.wikipedia. org/wiki/Feature_Oriented_Programming, \n2008. [13] Georges Gonthier. In Deepak Kapur, editor, Computer Math\u00adematics, chapter The Four Colour \nTheorem: Engineering of a Formal Proof, pages 333 333. Springer-Verlag, Berlin, Hei\u00addelberg, 2008.  \n[14] Atsushi Igarashi, Benjamin C. Pierce, and Philip Wadler. Featherweight java: a minimal core calculus \nfor java and gj. ACM Trans. Program. Lang. Syst., 23(3):396 450, 2001. [15] K.C. Kang. Private Correspondence, \n2005. [16] Xavier Leroy. Formal veri.cation of a realistic compiler. Commun. ACM, 52:107 115, July 2009. \n[17] Michael Y. Levin and Benjamin C. Pierce. Tinkertype: A lan\u00adguage for playing with formal systems. \nJournal of Functional Programming, 13(2), March 2003. A preliminary version ap\u00adpeared as an invited paper \nat the Logical Frameworks and Metalanguages Workshop (LFM), June 2000. [18] M. D. McIlroy. Mass-produced \nsoftware components. Proc. NATO Conf. on Software Engineering, Garmisch, Germany, 1968. [19] R. Monson-Haefel. \nEnterprise Java Beans. O Reilly, 3rd edition, 2001. [20] Peter D. Mosses. Modular structural operational \nsemantics. J. Log. Algebr. Program., 60-61:195 228, 2004. [21] Anne Mulhern. Proof weaving. In Proceedings \nof the First Informal ACM SIGPLAN Workshop on Mechanizing Metathe\u00adory, September 2006. [22] J. Neighbors. \nThe draco approach to constructing software from reusable components. IEEE TSE, September 1984. [23] \nD.L. Parnas. On the design and development of program families. IEEE TSE, SE-2(1):1 9, March 1976. [24] \nBenjamin C. Pierce. Types and Programming Languages. MIT Press, 2002. [25] Y. Smaragdakis and D. Batory. \nMixin Layers: An Object-Oriented Implementation Technique for Re.nements and Collaboration-Based Designs. \nACM TOSEM, December 2001. [26] Yannis Smaragdakis and Don Batory. Implementing reusable object-oriented \ncomponents. In In the 5th Int. Conf. on Soft\u00adware Reuse (ICSR 98, pages 36 45. Society Press, 1998. [27] \nRobert St\u00e4rk, Joachim Schmid, and Egon B\u00f6rger. Java and the java virtual machine -de.nition, veri.cation, \nvalidation, 2001. [28] Rok Strnisa, Peter Sewell, and Matthew J. Parkinson. The Java module system: core \ndesign and semantic de.nition. In OOPSLA, pages 499 514, 2007. [29] T. Th\u00fcm, I. Schaefer, M. Kuhlemann, \nand S. Apel. Proof com\u00adposition for deductive veri.cation of software product lines. In Software Testing, \nVeri.cation and Validation Workshops (ICSTW) 2011, pages 270 277, march 2011. [30] Michael VanHilst and \nDavid Notkin. Decoupling change from design. SIGSOFT Softw. Eng. Notes, 21:58 69, October 1996.   \n \n\t\t\t", "proc_id": "2048066", "abstract": "<p>Mechanized proof assistants are powerful verification tools, but proof development can be difficult and time-consuming. When verifying a family of related programs, the effort can be reduced by proof reuse. In this paper, we show how to engineer product lines with theorems and proofs built from feature modules. Each module contains proof fragments which are composed together to build a complete proof of correctness for each product. We consider a product line of programming languages, where each variant includes metatheory proofs verifying the correctness of its semantic definitions. This approach has been realized in the Coq proof assistant, with the proofs of each feature independently certifiable by Coq. These proofs are composed for each language variant, with Coq mechanically verifying that the composite proofs are correct. As validation, we formalize a core calculus for Java in Coq which can be extended with any combination of casts, interfaces, or generics.</p>", "authors": [{"name": "Benjamin Delaware", "author_profile_id": "81414620111", "affiliation": "University of Texas at Austin, Austin, TX, USA", "person_id": "P2839235", "email_address": "bendy@cs.utexas.edu", "orcid_id": ""}, {"name": "William Cook", "author_profile_id": "81406596033", "affiliation": "University of Texas at Austin, Austin, TX, USA", "person_id": "P2839236", "email_address": "wcook@cs.utexas.edu", "orcid_id": ""}, {"name": "Don Batory", "author_profile_id": "81100296626", "affiliation": "University of Texas at Austin, Austin, TX, USA", "person_id": "P2839237", "email_address": "batory@cs.utexas.edu", "orcid_id": ""}], "doi_number": "10.1145/2048066.2048113", "year": "2011", "article_id": "2048113", "conference": "OOPSLA", "title": "Product lines of theorems", "url": "http://dl.acm.org/citation.cfm?id=2048113"}