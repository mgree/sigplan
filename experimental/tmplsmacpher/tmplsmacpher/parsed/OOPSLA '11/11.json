{"article_publication_date": "10-22-2011", "fulltext": "\n PREFAIL: A Programmable Tool for Multiple-Failure Injection Pallavi Joshi Haryadi S. Gunawi Koushik \nSen EECS, UC Berkeley, USA EECS, UC Berkeley, USA EECS, UC Berkeley, USA pallavi@cs.berkeley.edu haryadi@cs.berkeley.edu \nksen@cs.berkeley.edu Abstract As hardware failures are no longer rare in the era of cloud computing, \ncloud software systems must prevail against multiple, diverse failures that are likely to occur. Testing \nsoft\u00adware against multiple failures poses the problem of combi\u00adnatorial explosion of multiple failures. \nTo address this prob\u00adlem, we present PreFail, a programmable failure-injection tool that enables testers \nto write a wide range of policies to prune down the large space of multiple failures. We integrate PreFail \nto three cloud software systems (HDFS, Cassandra, and ZooKeeper), show a wide variety of useful pruning \npoli\u00adcies that we can write for them, and evaluate the speed-ups in testing time that we obtain by using \nthe policies. In our experiments, our testing approach with appropriate policies found all the bugs that \none can .nd using exhaustive testing while spending 10X 200X less time than exhaustive testing. General \nTerms Reliability, Veri.cation Keywords fault injection, distributed systems, testing Categories and \nSubject Descriptors D.4.5 [Operating Sys\u00adtems]: Reliability; D.2.5 [Software Engineering]: Testing and \nDebugging 1. Introduction With the arrival of the cloud computing era, large-scale distributed systems \nare increasingly in use. These systems are built out of hundreds or thousands of commodity ma\u00adchines \nthat are not fully reliable and can exhibit frequent fail\u00adures [13, 20, 34, 37, 41]. Due to this reason, \ntoday s cloud software (i.e., software that runs on large-scale deploy\u00adments) does not assume perfect \nhardware reliability. Cloud software has a great responsibility to correctly recover from diverse hardware \nfailures such as machine crashes, disk er\u00adrors, and network failures. Permission to make digital or hard \ncopies of all or part of this work for personal or classroom use is granted without fee provided that \ncopies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. OOPSLA 11, October 22 27, 2011, Portland, Oregon, \nUSA. Copyright c &#38;#169; 2011 ACM 978-1-4503-0940-0/11/10. . . $10.00 Even if existing cloud software \nsystems are built with re\u00adliability and failure tolerance as primary goals [10, 13, 16], their recovery \nprotocols are often buggy. For example, the developers of Hadoop File System [38] have dealt with 91 \nrecovery issues over its four years of development [18]. There are two main reasons for this. Sometimes \ndevelopers fail to anticipate the kind of failures that a system can face in a real setting (e.g., only \nanticipate fail-stop failures like crashes, but forget to deal with data corruption), or they in\u00adcorrectly \ndesign/implement the failure recovery code. There have been many serious consequences (e.g., data loss, \nun\u00adavailability) of the presence of recovery bugs in real cloud systems [5, 7, 8, 18]. To test recovery, \nthere has been some work that has pro\u00adposed novel failure-injection tools and frameworks, but they primarily \naddress single failures during program execution. However, cloud software systems face frequent, multiple, \nand diverse failures. In this regard, there is a need to advance the state-of-the-art of failure testing \n multiple failures need to be systematically explored in program execution. Unfor\u00adtunately, exercising \nmultiple failures is not straight-forward. The challenge to deal with is the combinatorial explosion \nof multiple failures that can be exercised. From our personal experience and our conversation with some \ndevelopers of cloud software systems, we found that a tester can employ many different heuristics to \nprune the large combinations of multiple failures. For example, a tester might only want to fail a representative \nsubset of the compo\u00adnents of a system, or inject only a subset of all possible fail\u00adure types, or reduce \nthe number of failure-injection points with some optimizations, or explore failure-injection points that \nsatisfy some code-coverage objectives, or fail proba\u00adbilistically. Furthermore, the tester might want \nto use mul\u00adtiple heuristics together. To enable testers to express many different pruning heuristics \nor policies, we design, implement, and evalu\u00adate PREFAIL, a programmable failure-injection tool for multiple-failure \ninjection. More speci.cally, we make the following contributions in this paper. 1. We build a programmable \nfailure-injection tool that al\u00adlows testers to write policies to express the set of multiple-failure \ncombinations (or sequences) that they want to explore. This way, we alleviate the need to ex\u00adplore all \npossible multiple-failure combinations, which can be too huge to test with reasonable resources and time. \nTo enable programmability, we decouple PREFAIL into two pieces: the failure-injection engine which is \nca\u00adpable of interposing different execution points of the sys\u00adtem under test and is responsible for performing \nfailure injection at those points, and the failure-injection driver where testers can write pruning policies \nthat drive the engine (i.e., make decisions about which failures to in\u00adject). PREFAIL provides suitable \nabstractions of failures and the execution points where the failures can be in\u00adjected, and also pro.les \nof executions where failures are injected. These abstractions can be used by testers to eas\u00adily write \na wide variety of pruning policies.  2. We present a number of pruning policies that we have written \nfor distributed systems. If a tester has a good knowledge about the system being tested, then she can \neasily write appropriate policies to explore the failures that meet her testing objectives. But, even \nif the tester does not know much about the system, we show that she can still use generic coverage based \npolicies (e.g., code-coverage and recovery-coverage based policies) to systematically test the system. \nIn our experiments, we found all bugs in a system with appropriate policies in time that is much lesser \nthan the time to exhaustively test all possible failure sequences (e.g., 20 hours vs. 1/2 hour). 3. \nWe have integrated PREFAIL to three popular cloud sys\u00adtems: Hadoop File System (HDFS) [38], ZooKeeper \n[23], and Cassandra [29]. We provide a thorough evaluation of the speed-ups in the testing process that \nwe obtain by us\u00ading the pruning policies that we wrote. In terms of bug .nding, so far we have focused \nmore on HDFS. We found all of the 16 new bugs in HDFS that we had found in pre\u00advious work [18], and also \nfound 6 newer bugs.  We have made PREFAIL publicly available for download from http://sourceforge.net/projects/prefail/. \nWe have, in fact, already worked with engineers from Cloud\u00adera Inc. to test their version of Hadoop software. \nMore real\u00adworld adoption of PREFAIL is in progress. In our previous work [18], we had begun the quest \nof .nd\u00ad ing techniques to prune down multiple-failure sequences. In this prior work, we only presented \ntwo rigid pruning policies which are hard-coded in the failure-injection tool that we built. Based on \nmore experience and conversation with some developers of cloud software systems, we found that there \nwere many more pruning policies that a tester would like to use. This led us to re-think and re-structure \nour failure\u00adinjection tool so that it can let testers easily and rapidly write various kinds of policies. \nIn the rest of the paper, we present an extended motivation for having a programmable failure-injection \ntool (\u00a72), the design and implementation of PREFAIL (\u00a73), examples of a wide range of pruning heuristics \nthat we can write in Node A Node B A1. write(B, msg); B1. write(A, msg); A2. read(B, header); B2. read(A, \nheader); A3. read(B, body); B3. read(A, body); A4. write(B, msg); B4. write(A, msg); A5. write(Disk, \nbuf); B5. read(Disk, buf); Figure 1. Example code. PREFAIL (\u00a74), evaluation of PREFAIL (\u00a75), limitations \n(\u00a76), related work (\u00a77), and .nally conclusion (\u00a78) .  2. Extended Motivation In this section, we present \nan extended motivation for having a programmable tool for multiple-failure injection. 2.1 The Combinatorial \nExplosion of Multiple Failures Testing systems against multiple failures is unfortunately not straight-forward \n the challenge to deal with is the combi\u00adnatorial explosion of multiple failures. This explosion is at\u00adtributed \nto the complex characteristics of failures that can arise: different types of failures (e.g., crashes, \ndisk failures, rack failures, network partitioning), different parts of the hardware (e.g., two among \nfour nodes fail), and different timings (e.g., failures happen at different stages of the pro\u00adtocol). \nExhaustively exploring all possible failure sequences can take a lot of computing resources and time. \nLet s consider the code segment in Figure 1 that runs on a distributed system with two nodes, A and B. \nThis program executes reads and writes (to the network and the disk) in each node. Given this program, \nhardware failures such as transient I/O failures, crashes, data corruptions, and network failures, can \nhappen around the I/O operations. Thus, we would like to test the tolerance of this code against different \nkinds of failures by injecting those failures during its exe\u00adcution. For example, we can inject a transient \nI/O failure at the write call on line A1 by executing code that throws an IOException instead of executing \nthe write call. Let us suppose that a tester wants to test against crashes before read and write calls, \nand that she wants to inject two crashes in an execution. One possible combination is to crash before \nthe write at A4 and then to crash before the write at B5. Overall, since there are 5 possible points \nto inject a crash on every node, there are 52*N(N -1) possible ways to inject two crashes, where N is \nthe number of participating nodes (N = 2 in the above example). Again, considering many other factors \nsuch as different failure types and more failures that can be injected during recovery, the number of \nall possible failure sequences can be too many to explore with reasonable computing resources and time. \n 2.2 The Need For Programmable Failure-Injection To address the aforementioned challenge, we believe \nthat there are many different ways in which a tester could reduce the number of failures to inject. Below, \nwe present some ex\u00adamples based on our personal experience and our conversa\u00adtion with some developers \nof cloud software systems. Our goal is to allow testers to express failure space pruning poli\u00adcies of \ndifferent complexities so that they can choose a suit\u00adable policy based on testing budget and requirement. \n Failing a component subset: Let s suppose a tester wants to test a distributed write protocol that \nwrites four replicas to four machines, and let s suppose that the tester wants to inject two crashes \nin all possible ways in this execution to show that the protocol could survive and continue writing to \nthe two surviving machines. A brute-force technique will inject failures on all possible combinations \nof two nodes () 4 (i.e., ). However, to do this quickly, the tester might wish 2 to specify a policy \nthat just injects failures in any two nodes. Failing a subset of failure types: Another way to prune \ndown a large failure space is to focus on a subset of the possible failure types. For example, let s \nimagine a testing process that, at every disk I/O, can inject a machine crash or a disk I/O failure. \nFurthemore, let s say the tester knows that the system is designed as a crash-only software [6], that \nis, all I/O failures are supposed to translate to system crash (fol\u00adlowed by a reboot) in order to simplify \nthe recovery mecha\u00adnism. In this environment, the tester might want to just inject I/O failures but not \ncrashes because it is useless to inject ad\u00additional crashes as I/O failures will lead to crashes anyway. \nAnother good example is the rack-aware data placement pro\u00adtocol common in many cloud systems to ensure \nhigh avail\u00adability [14, 38]. The protocol should ensure that .le replicas should be placed on multiple \nracks such that if one rack goes down, the .le can be accessed from other racks. In this sce\u00adnario, if \nthe tester wants to test the rack-awareness property of the protocol, only rack failures need to be injected \n(e.g., vs. individual node or disk failures). Coverage-based policies: A tester might want to speed up \nthe testing process with some coverage-based policies. For example, let s imagine two different I/Os \n(A and B) that if failed could initiate the same recovery path that performs another two I/Os (M and \nN). To ensure correct recovery, a tester should inject more failures in the recovery path. A brute-force \nmethod will perform 4 experiments by injecting two failures at AM, AN, BM, and BN (M and N cannot be \nexercised by themselves unless A or B has been failed). But a tester might wish to .nish the testing \nprocess when she has satis.ed some code coverage policy, for example, by stopping after all I/O failures \nin the recovery path (at M and N) have been exercised. With this policy, she only needs to run 2 experiments \nwith failures at AM and AN. Domain-speci.c optimization: In some cases, system\u00adspeci.c knowledge can \nbe used to reduce the number of failures. For example, consider 10 consecutive Java read I/Os that read \nfrom the same input .le (e.g., f.readInt(), f.readLong(), ...). In this scenario, disk failure can start \n Figure 2. PREFAIL Architecture. The .gure shows the sep\u00adaration of failure-injection engine (mechanism) \nand driver (policy). The pruning policies written in the driver make failure decisions that drive the \nengine. to happen at any of these 10 calls. In a brute-force man\u00adner, a tester would run ten experiments \nwhere disk failure begins at 10 different calls. However, with some operating system knowledge, the tester \nmight inject disk failure only on the .rst read. The reasoning behind this is that a .le is typically \nalready buffered by the operating system after the .rst call. Thus, it is unlikely (although possible) \nto have ear\u00adlier reads succeed and the subsequent reads fail. In our ex\u00adperience, by reducing these individual \nfailures, we greatly reduce the combinations of multiple failures. Failing probabilistically: Multiple \nfailures can also be re\u00adduced by only injecting them if the likelihood of their oc\u00adcurrence is greater \nthan a prede.ned threshold [36, 40]. This technique is useful especially if the tester is interested \nin cor\u00adrelated failures. For example, two machines put within the same rack are more likely to fail together \ncompared to those put across in different racks [14]. A tester can use real-world statistical data to \nimplement policies that employ some fail\u00adure probability distributions. In summary, there are many different \nways in which a tester can reduce the number of failures and their combina\u00adtions to be injected. Thus, \nwe believe that there is a need for a programmable failure-injection tool that enables testers to express \ndifferent pruning policies. In the following section, we describe our approach in detail.  3. Programmable \nFailure Injection In this section we present the design of PREFAIL. To enable programmability, we borrow \nthe classic principle of sepa\u00adration of mechanism and policy [2, 30, 39]. With this prin\u00ad ciple, we decouple \nour failure-injection framework into two pieces: the FI engine and the FI driver as depicted in Figure \n2 (FI stands for failure-injection). The FI engine is the compo\u00adnent that injects failures in the system \nunder test, and the FI driver is the component that takes tester-speci.ed policies to decide where to \ninject failures. The FI engine exposes fail\u00adure related abstractions to the FI driver that can be used \nby the testers in their policies. In the following sections, we .rst illustrate the test work.ow in PREFAIL \n(\u00a73.1), and then we explain the FI engine (\u00a73.2), the abstraction interface (\u00a73.3), the FI driver and \npolicies (\u00a73.4), and .nally the detailed al\u00ad  1 failure/exp 2 failures/exp 3 failures/exp  Figure \n3. PREFAIL Test Work.ow. An alphabetical symbol represents a failure-injection point or task. A box represents \na fail\u00adure sequence (a sequence of failure-injection tasks) to be exercised. A letter in a circle represents \na failure-injection point observed in an experiment. For ease of reading, a letter in a box represents \na crash failure-injection task (e.g., A in a box should be read as A c). gorithm of the test work.ow \n(\u00a73.5). We give some examples of policies in Section 3.4.3, but for more examples and de\u00ad tails, readers \ncan refer to Section 4. 3.1 Test Work.ow Figure 3 shows an example scenario of the testing process in \nPREFAIL. The tester speci.es three failures as the maximum number of failures to inject in an execution \nof the system under test. The FI engine .rst runs the system with zero failure during execution (i.e. \nwithout injecting any failure during execution). During this execution, it obtains the set of all execution \npoints where failures can be injected (i.e., failure-injection points as described in Section 3.3): A, \nB, and C. Let us assume that we are interested only in crashes, and let Ac, Bc, and Cc denote the injection \nof crashes (i.e., failure-injection tasks as described in Section 3.3) at the failure-injection points \nA, B, and C, respectively (for ease of reading, a failure-injection task Xc is represented as X in a \nbox in Figure 3). Using the tester-speci.ed policies, suppose PREFAIL prunes down the set of failure-injection \ntasks to Ac and Bc, and then exercises each failure-injection task in the pruned down set. After exercising \na failure-injection task, the FI en\u00adgine records all failure-injection points seen where further crashes \ncan be injected. For example, after exercising Ac (that is, injecting a crash at A), the FI engine observes \nthe failure-injection points D and E. From this information, the FI engine creates the set of sequences \nof two failure\u00adinjection tasks AcDc and AcEc that can be exercised while in\u00adjecting two crashes in an \nexecution. Similarly, it creates BcEc after observing the failure-injection point E in the execution \nthat exercises Bc. As mentioned before, the number of all sequences of failure-injection tasks that can \nbe exercised tends to be large. Thus, PREFAIL again uses the tester-speci.ed policies to reduce this \nnumber. For example, a tester might want to test just one sequence of two crashes that exercises Ec as \nthe fip B5 Table 1. Failure-Injection Point (fip) and Failure-Injection Task (fit). The left table illustrates \na fip with label B5 . More context can be added by adding more key-value map\u00adpings. The right-hand side \ntable shows different fit s that can be formed for the fip . second crash. Thus, PREFAIL would automatically \nexercise just one of AcEc and BcEc to satisfy this policy instead of exercising both of them. The step \nfrom injecting two failures to three failures per execution is similar. 3.2 FI Engine The failure-injection \ntasks described above are created by the FI engine. The FI engine interposes different execu\u00adtion points \nin the system under test and injects failures at those points. The target failure-injection points and \nthe range of failures that can be injected all depend on the objec\u00adtive of the tester. For example, interposition \ncan be done at Java/C library calls [18, 31], TCP-level I/Os [12], disk\u00ad level I/Os [35], POSIX system \ncalls [28], OS-driver inter\u00ad faces [24], and at many other points. Depending on the tar\u00ad get failure-injection \npoints, the range of failures that can be injected varies. In our work, we use a failure-injection tool \nthat we had built in prior work [18] as the FI engine. This particular FI engine interposes all I/O related \nto calls to Java libraries and emulates hardware failures by supporting diverse failure types such as \ncrashes, disk failures, and network partitioning at node and rack levels. The FI driver tells the FI \nengine to run a set of experi\u00adments that satisfy the written policies. An experiment is an execution \nof the system under test with a particular failure scenario (could be one or multiple failures). For \nexample, using the example in Figure 1, the FI driver could tell the FI engine to run one experiment \nwith one speci.c failure (e.g., a crash before the write at A4) or two concurrent failures (e.g., the \nsame crash plus a crash before the write at B4).  3.3 Abstractions In this section, we provide the abstractions \nthat bridge the FI engine and the FI driver. The FI engine provides the following abstractions of failures \nand execution points where failures can be injected, and of failure-injection experiments. These abstractions \ncan be used by testers in their pruning policies.  1. Failure-Injection Point (fip). A failure-injection \npoint (fip)isamapfromasetofkeys K to a set of values V.It is a static abstraction of an execution point \nwhere a failure can be injected. A key k in K represents a part of the static or dynamic context associated \nwith the execution point. For example, k could be func to represent the function call being executed. \nIt would be mapped to the name of the function in the fip. Other examples for k are: loc for the location \nof the function call in the source code, node for the node ID on which the execution occurs, target for \nthe target of the I/O executed by the function call (e.g., the name of the .le being written to in case \nof a disk write I/O), and stack for the stack trace. Table 1 shows the fip corresponding to the execution \npoint at the read call at line L5 in node B in Figure 1. We denote the set of all failure-injection points \nby P. 2. Failure-Injection Task (fit). A failure-injection task (fit) is a pair of a failure type (e.g., \ncrash, disk failure) and a failure-injection point. Thus, a fit f .F \u00d7P, where F denotes the set of all \nfailure types. Given a failure-injection point, there are different types of fail\u00adures that can be injected \nat that point. For example, Ta\u00adble 1 shows different fits that can be formed for the fip illustrated \nin the same table for three different types of failures (crash, data corruption, and disk failure). Exer\u00adcising \na fit f =(ft, fp) means injecting the failure type ft at the fip fp. Since we are interested in injecting \nmultiple failures dur\u00ading execution in addition to single failures, we also con\u00adsider sequences of failure-injection \ntasks. We denote the set of all sequences of failure-injection tasks by Q.We call a sequence of failure-injection \ntasks as a failure se\u00adquence in short. 3. Per-experiment pro.le. To allow powerful policies (a variety \nof policies) to be written, the FI driver pro.les the execution of the system in every experiment, and \nmakes the pro.ling information available to testers. Testers can use the pro.les of already executed \nexperiments to decide in their policies which failure sequences to exercise in future experiments. Our \nstrategy in pro.ling an execution is by recording the set of failure-injection points observed during \nthe execution. The reasoning behind this is that failure-injection points are typically built out of \nI/O calls, library calls, or system calls, and these calls can be used to approximately represent an \nexecution of the system under test. Thus, an execution pro.le exp . 2P .  Let allFips: Q. 2P and postInjectionFips: \nQ. 2P be the functions that return execution pro.les of failure-injection experiments. Given a failure \nsequence fs, allFips(fs) returns the execution pro.le consisting of all fips observed during the experiment \nin which fs is injected, and postInjectionFips(fs) returns the set of all fips observed after fs has \nbeen injected. For the  Algorithm 1 fpGen 1: Inputs: A filter predicate flt and a set of failure sequences \nFS 2: Output: A set of failure sequences FSP 3: FSP = {} 4: for fs in FS do 5: if flt(fs) then 6: FSP \n= FSP .{fs} 7: end if 8: end for 9: return FSP  Algorithm 2 cpGen 1: Inputs: A cluster predicate cls \nand a set of failure sequences FS 2: Output: A set of failure sequences FSP 3: FSP = {} 4: E = FS/Rcls \n5: for e in E do 6: fs = select an element from e randomly 7: FSP = FSP .{fs} 8: end for 9: return FSP \nempty sequence (), allFips and postInjectionFips both return the set of all fips seen in the execution \nin which no failure is injected. 3.4 FI Driver Based on the abstractions above, the FI driver provides \nsup\u00adport for writing predicates that it uses to generate policies that express how to prune the failure \nspace. For convenience and brevity, whenever we say that a tester writes a policy, we mean that the tester \nwrites the predicate that is later used by the FI driver to generate the policy. A policy is a func\u00adtion \np :2Q . 2Q. It takes a set of failure sequences, and returns a subset of the sequences to be explored \nby the FI engine. Testers can use the failure and execution point ab\u00adstractions, and execution pro.les \nprovided by the FI engine in their predicates. There are two different kinds of predi\u00adcates that can \nbe written to generate two different kinds of policies: .lter and cluster policies. PREFAIL can also \ncom\u00adpose the policies generated from different predicates to ob\u00adtain more complex policies. 3.4.1 Filter \nPolicy A .lter policy uses a tester-written predicate flt: Q. Boolean. The predicate takes a failure \nsequence fs as an ar\u00adgument and implements a condition that decides whether to exercise fs or not. Algorithm \n1 explains how a .lter policy works. Given a predicate flt, the function fpGen :(Q. Boolean) . (2Q . \n2Q) (implemented in PREFAIL) gener\u00adates a .lter policy out of it. The policy takes a set of failure sequences \nFS, applies the flt predicate on each sequence  . fs.( let ((ft1, fp1), ..., (ftn, fpn)) = fs in let \nisCrash(ft)=(ft == crash) in let inSetup(fp)= fp[ stack ] has setup in isCrash(fti) . inSetup(fpi) i.{1,...,n} \n) Figure 4. Setup-stage .lter. Return true if all fit s(ft1,fp1), ..., (ftn,fpn) in fs correspond to \na crash within the setup function. fs, and retains fs in its result set FSP if the predicate holds for \nit.  3.4.2 Cluster Policy A cluster policy uses a tester-implemented predicate cls: Q\u00d7Q . Boolean. The \npredicate takes two failure se\u00adquences as arguments, and returns true if the tester consid\u00aders them to \nbe similar (e.g., exercising either of them would result in the same test coverage), and false otherwise. \nThe predicate implicitly implements an equivalence relation Rcls = {(fs1, fs2) | cls(fs1, fs2)}. Algorithm \n2 shows how a clus\u00ad ter policy works. Given a cls predicate, the function cpGen :(Q\u00d7Q . Boolean) . (2Q \n. 2Q) (implemented in PREFAIL) generates a cluster policy out of it. The policy uses the predicate to \npartition its argument set of failure se\u00adquences FS into disjoint subsets FS/Rcls . It then randomly \nselects one failure sequence fs from each equivalence class. Thus, the tester implements her notion of \nequivalence of fail\u00adure sequences, and the policy uses the equivalence relation to select failure sequences \nsuch that all equivalence classes in its argument set of failure sequences are covered. 3.4.3 Example \nPolicies We give brief examples of how one can use the .lter and cluster policies. Suppose that a tester \nis interested in testing the tolerance of the setup stage of a distributed systems pro\u00adtocol against \ncrashes. The tester can write the flt predicate in Figure 4. The .lter policy fpGen(flt) would retain \na fail\u00adure sequence fs only if every fit in fs corresponds to a crash in the setup stage (execution of \nthe setup function). In failure testing, since we are concerned with testing the correctness of recovery \npaths of a system, one way to re\u00adduce the number of failure sequences to test would be to cluster them \naccording to the recovery paths that they would lead to. Out of all failure sequences that would lead \nto a par\u00adticular recovery path, we can just choose and test one. To achieve this, we can write the cluster \npredicate in Figure 5. If two failure sequences fs1 and fs2 have the same last fit, and their pre.xes \nthat leave the last fit out (fs1P and fs2P respectively) result in the same recovery path, then we can \nconsider fs1 and fs2 to be equivalent in terms of the recovery paths that they would lead to since they \ninvolve injecting the same failure at the same execution point in the same recovery . fs1, fs2.( let \nrec(fs) = allFips(fs) \\ allFips(()) in let eq(fs1, fs2) = (rec(fs1) == rec(fs2)) in let (f11, ..., f1m) \n= fs1 in let fs1P =(f11, ..., f1(m-1)) in let (f21, ..., f2n) = fs2 in let fs2P =(f21, ..., f2(n-1)) \nin (eq(fs1P , fs2P ) . (f1m == f2n) . (m = 2) . (n = 2)) ) Figure 5. Recovery path cluster. Cluster two \nfailure se\u00adquences if their last fit s are the same and their pre.xes (that exclude the last fit s) result \nin the same recovery path. path. PREFAIL s test work.ow is such that when deciding whether to test a \nfailure sequence (e.g., fs1), all of its pre\u00ad.xal sequences (e.g., fs1P ) would have already been tested, \nand thus we would have already seen the recovery paths that they lead to. Figure 5 uses the function \nrec to characterize a recovery path. It uses the set of all fips seen in the recovery path to characterize \nit. From all fips observed during an ex\u00adecution in which a failure sequence is injected, we subtract \nout the fips that are observed during normal program exe\u00adcution (that is, when no failure is injected) \nto obtain the fips seen in the recovery path. More details about recovery path clustering can be found \nin Section 4.4. PREFAIL also enables composition of policies. For ex\u00adample, the policies that use the \npredicates in Figures 4 and 5 (fpGen(flt) and cpGen(cls)) can be composed to .rst .l\u00adter out only those \nfailure sequences that have crashes in the setup stage, and then to cluster the .ltered sequences accord\u00ading \nto the recovery paths that they would lead to. Section 4 shows how to write the policies in Python in \nPREFAIL, and also gives many other examples of policies.  3.5 Test Work.ow Algorithm Having outlined \nthemajor components of PREFAIL, this sec\u00adtion presents the detailed algorithm of PREFAIL s test work\u00ad.ow \n(Algorithm 3). PREFAIL takes a system Sys to test, a list of tester-written predicates Preds, and the \nmaximum number of failures N to inject in an execution of the sys\u00adtem. The testing process runs in N \n+1 steps. At step i (0 = i = N), the FI engine of PREFAIL executes the sys\u00adtem Sys once for each failure \nsequence of length i that it wants to test, and injects the failure sequence during the ex\u00adecution of \nthe system. FSc is the set of all failure sequences that should be tested in the current step, and FSn \nis the set of failure sequences that should be tested in the next step. Initially FSc is set to a singleton \nset with the empty failure sequence as the only element. Therefore, in step 0 the FI en\u00adgine executes \nSys and injects an empty sequence of failures, i.e. it does not inject any failure. The FI engine observes \nthe fips that are seen during execution, computes fits from them, and adds singleton failure sequences \nwith these fits  Algorithm 3 PREFAILTest Work.ow moves to the next step. This process is repeated till \nthe last 1: INPUT: System under test (Sys), List of flt step. and cls predicates (Preds), Maximum number \nof failures per execution (N) 4. Crafting Pruning Policies 2: FSc = {()}3: FSn In this section, we present \nthe pruning policies that we = {}4: for 0 = i = N do have written and their advantages. More speci.cally, \nwe 5: for each failure sequence fs in FSc do present our integration of PREFAIL to Hadoop File Sys\u00ad 6: \nExecute Sys and inject fs during execution tem (HDFS) [38], an underlying storage system for Hadoop \n7: Profile execution using fips observed MapReduce [1], and show the policies that we wrote for it. \nduring execution We begin with an introduction to HDFS and then present the 8: for each fit f computed \nfrom a fip in policies. postInjectionFips(fs) do Overall, we make three major points in this section. \nFirst, 9: fs' = Append f to fs . fs' by clearly separating the failure-injection mechanism and 10: FSn \n= FSn 11: end for policy and by providing useful abstractions, we can write 12: end for many different \npruning policies clearly and concisely. Sec\u00ad 13: FSn = Prune(Preds, FSn) ond, we show that policies can \nbe easily composed together 14: FSc = FSn to achieve different testing objectives. Finally, we show that \n15: FSn = {} some policies can be reused for different target systems. We 16: end for believe these advantages \nshow the power of PREFAIL.We chose Python as the language in which testers can write poli\u00ad cies in PREFAIL, \nthough any other language could have also Algorithm 4 Prune(Preds, FS) been chosen. 1: FSP = FS 4.1 HDFS \nPrimer 2: for predicate pr in Preds do 3: if pr is a filter predicate then HDFS is a distributed .le \nsystem that can scale to thousands 4: p = fpGen(pr) of nodes. Here we describe the HDFS write protocol \nin 5: end if detail. Figure 6 shows a simpli.ed illustration of the write 6: if pr is a cluster predicate \nthen I/Os (both .le system and network writes) occurring within 7: p = cpGen(pr) the protocol. The protocol \nby default stores three replicas in 8: end if three nodes, and is divided mainly into two stages: the \nsetup 9: FSP = p(FSP ) stage and the data transfer stage; later, we will see how the 10: end for recovery \nfor each stage is different. 11: return FSP Our FI engine is able to emulate hardware failures on every \nI/O (every box in Figure 6). As illustrated, there are 13 failure points that the FI engine interposes \nin this write to FSn. Therefore, FSn has failure sequences that the FI en\u00ad protocol. (Note that, in reality, \nthe write protocol performs gine can exercise in the next step, i.e. in the i =1 step. more than 40 I/Os). \nAt every I/O, the FI engine can inject a Before PREFAIL proceeds to the next step, it prunes down crash, \na disk failure (if it s a disk I/O), or a network failure the set FSn using the predicates written by \ntesters. The pred\u00ad (if it s a network I/O). The .gure also depicts many possible icates in Preds are \nused to generate policies that are then ways in which multiple failures can occur. For example, applied \nto FSn (Algorithm 4). The policy generated from two crashes can happen simultaneously at failure-injection \nthe .rst predicate is applied .rst to FSn, the second policy points B1 and B2, or a disk failure at D1 \nand a network failure is then applied to the result of the .rst policy and so on. at E3, and many more. \nInterested readers can learn more Note that the order of predicates is important since the poli\u00ad about \nHDFS from here [38, 42] and our extended technical cies generated from them may not commute. In step \ni =1, report (which depicts the write protocol in more detail) [25]. FSc is set to the pruned down FSn \nfrom the previous step, and FSn is reset to the empty set. For each failure sequence 4.2 Pruning by Failing \na Component Subset fs in FSc, the failure-injection tool executes Sys and injects In distributed systems \nlike HDFS, it is common to have fs during execution. For each fit f that it computes from multiple nodes \nparticipating in a distributed protocol. As a fip observed after fs has been injected (that is, a fip \nmentioned earlier, let s say we have N participating nodes, in postInjectionFips(fs)), it generates a \nnew failure se\u00ad and the developer wants to inject two failures on two nodes. quence fs' by appending \nf to fs, and adds fs' to FSn.Af- Then there are (N 2 ) failure sequences that one could inject. ter Sys \nhas been executed once for each failure sequence in Worse, on every node (as depicted in Figure 6), there \ncould FSc,PREFAIL prunes down the set FSn with predicates and be many possible points to exercise the \nfailure on that node.   A Forward setup message to downstream nodes. (The last node does not need \nto forward the setup message) B After receiving setup message, create temporary block and meta .les. \nC Stream data bytes to downstream nodes. (The last node does not need to stream the data bytes) D Write \nbytes to the block and meta .les. E Send commit acks to upstream nodes.  Figure 6. HDFS Write Protocol. \nThe .gure presents a simpli.ed illustration of the HDFS write protocol. Each box represents an I/O (.le \nsystem or network), and thus a failure-injection point. For ease of reading, we label each failure-injection \npoint with an alphabetical symbol plus the node ID. The protocol begins with the client forming a pipeline \n(Client-N1-N2-N3) to the three nodes where replicas of a .le will be stored. The client obtains these \ntarget nodes from the master (communication between client and master is not shown). For simplicity, \nwe don t show many other I/Os such as other acknolwedgment and disk I/Os. We also do not show the rack-aware \nplacement of replicas. 1 def cls (fs1, fs2): 2 rs1 = abstractOut(fs1, node ) 3 rs2 = abstractOut(fs2, \nnode ) 4 return ( rs1 == rs2 ) Figure 7. Ignore nodes cluster. Return true if two failure sequences have \nthe same failures with the same contexts not con\u00adsidering the nodes in which they occur. The function \nabstractOut removes the mappings for nodes from the fip s in its argument fail\u00adure sequence. To reduce \nthe number of failure sequences to test, a devel\u00adoper might just wish to inject failures at all possible \nfailure\u00adinjection points in any two nodes. She can write a cluster policy that uses the function in Figure \n7 to cluster failure sequences that have the same context when the node is not considered as part of \nthe context. With this policy, the devel\u00adoper can direct the FI engine to exercise failure sequences \nwith two failures such that if the FI engine has already ex\u00adplored failures on a pair of nodes then it \nshould not explore the same failures on a different pair of nodes. Using Figure 6 as an example, a failure \nsequence with simultaneous crashes at D1 and D2 is equivalent to another with crashes at D2 and D3. We \nalso want to emphasize that this type of pruning pol\u00adicy could be used for other systems. Consider a \nRAID sys\u00adtem [33] with N disks that a tester wishes to test by injecting failures at any two of its N \ndisks. To do this, we de.nitely need a FI engine that works for RAID systems, but we can re-use much \nof the policy that we wrote for distributed sys\u00adtems for RAID systems. The only difference would be in \nthe keys in the fips whose mappings we want to remove (i.e., for distributed systems we removed the mappings \nfor the node key in Figure 7, for RAID systems we remove the mappings for disk key). 1 def flt (fs): \n2 last = FIP (fs [ len(fs) -1 ]) 3 return not explored (last, loc ) Figure 8. New source location .lter. \nReturn true if the source location of the last fip has not been explored. The function FIP returns the \nfip in the argument fit . 1 def cls (fs1, fs2): 2 last1 = FIP (fs1[len(fs1) -1]) 3 last2 = FIP (fs2[len(fs2) \n-1]) 4 return (last1[ loc ] == last2[ loc ]) Figure 9. Source location cluster. Return true if the fip \nsin the last fit s have the same source location.  4.3 Pruning via Code-Coverage Objectives Developers \ncan achieve high-level testing objectives using policies. One common objective in the world of testing \nis to have some notion of high coverage . In the case of failure testing, we can write policies that \nachieve different types of coverage. For example, a developer might want to achieve a high coverage of \nsource locations of I/O calls where failures can happen. To achieve high code-coverage with as few experiments \nas possible, the tester can simply compose the policies that use the flt function shown in Figure 8 and \nthe cls function shown in Figure 9. The .lter policy explores failures at pre\u00ad viously unexplored source \nlocations by .ltering out a failure sequence if the fip in its latest fit (last) has an unex\u00adplored source \nlocation. The function FIP in Figure 8 returns the fip in the argument fit. The function explored re\u00adturns \ntrue if a failure has already been injected at the source location in the last fip in a previous failure-injection \nex\u00adperiment. For brevity, we do not show the source code of  fit Recovery Path (Fig. 10) SL SL+N A1c \n{ABCDE}\u00d7{234} DD B2c {ABCDE}\u00d7{134} D D C1c {FGI}\u00d7{23}, {CDE}\u00d7{23} \u00a6\u00a6 C2c {FGJ}\u00d7{13}, {CDE}\u00d7{13} .. D1c \n{FG}\u00d7{23}, {CDE}\u00d7{23} 00 E2c {FG}\u00d7{13}, {CDE}\u00d7{13} 0 V Table 2. HDFS Write Recovery. The table shows \nthe de\u00adtailed recovery I/Os of some fit s within the HDFS write protocol. The .rst column shows the fit \ns. A1c is the fit for crash at the I/O A1. (For simplicity, we do not distinguish here between an I/O \nand the failure-injection point that corresponds to the execution of the I/O). The second column shows \nthe recovery paths returned by the getRecoveryPath function (Figure 10) for every fit shown in the .rst \ncolumn1. To save space, we use \u00d7; {AB}\u00d7{12} represents the I/Os A1, A2, B1, and B2. The third and fourth \ncolumns represent two ways of characterizing the recovery path; the same shape rep\u00adresents the same class \nof recovery path. For example, the third col\u00adumn represents the characterization shown in Figure 11 which \nuses source location (SL) to characterize recovery. The fourth column uses source location and node ID \n(SL+N) to characterize recovery. these functions. The cls function in Figure 9 clusters fail\u00ad ure sequences \nthat have the same source location in their last fits. Thus, after the .lter policy has .ltered out failure \nse\u00adquences that have unexplored source locations, the cluster policy would cluster the failure sequences \nwith the same un\u00adexplored source location into one group. With these policies, PREFAIL would exercise \na failure sequence for each unex\u00adplored source location.  4.4 Pruning via Recovery-Coverage Objectives \nIn failure testing, since we are concerned with testing the correctness of recovery paths of a system, \nanother useful testing goal is to rapidly explore failures that lead to dif\u00adferent recovery paths. To \ndo this, a tester can write a cluster policy that clusters failure sequences leading to the same re\u00adcovery \npath into a single class. PREFAIL can then use this policy to exercise a failure sequence from each cluster, \nand thus exercise a different recovery path with each failure se\u00adquence. Below, we .rst describe the \nHDFS write recovery protocol, and then explain the whole process of recovery\u00adcoverage based pruning in \ntwo steps: characterizing recovery path, and clustering failure sequences based on the recovery characterization. \n1 def getRecoveryPath ( f s ) : 2 a = allFips ( f s ) 3 a 0 = allFips ([]) 4 rPath = a -a 0 5 return \nrPath Figure 10. Obtaining Recovery Path FIPs. Line 2 uses the function allFips (\u00a73.3) to get the set \nof all fip s, a , observed during the execution in which fs is injected. Line 3 obtains the set of fip \ns observed when no failure is injected (represented by [] ). Line 4 performs the diff of the two sets \nto obtain the fip sinthe recovery path taken when fs is injected. 4.4.1 HDFS Write Recovery As mentioned \nbefore, the HDFS write protocol is divided mainly into two stages: the setup stage and the data trans\u00adfer \nstage. The recovery for each stage is different. Table 2 shows in detail the recovery I/Os, that is, \nthe I/Os that occur during execution while recovering from an injected failure (or failure sequence). \nWe will gradually discuss the contents of the table in the following sections. In the setup stage, if \na node crashes, the recovery protocol will repeat the whole write process again with a new pipeline. \nFor example, in the .rst row of Table 2, after N1 crashes at I/O A1 (A1c), the protocol executes the \nentire set of I/Os again (ABCDE) in the new pipeline (N2-N3-N4). However, if a node crashes in the second \nstage, the recovery protocol will only repeat the second stage with some extra recovery I/Os on the surviving \ndatanodes. For example, in the .fth row of Table 2, after N1 crashes at D1 (D1c), the protocol .rst performs \nsome syn\u00adchronization I/Os (FG), and then repeats the second stage I/Os (CDE) on the surviving nodes \n(N2 and N3).  4.4.2 Characterizing Recovery Path To write a recovery clustering policy, a tester has \nto .rst decide how to characterize the recovery path taken by a system. One way to characterize would \nbe to use the set of fips observed in the recovery path. Figure 10 returns the difference of the fips \nobserved in the execution in which a failure sequence is injected and the fips in the execution in which \nno failure is injected. The difference can be thought of as the fips that are observed in the extra execution \nthat results or the recovery path that is taken when the failure sequence is injected. A tester can use \nthe set of fips observed in the recovery path to characterize the recovery path. Thus, two failure sequences \nthat result in the same set of fips in the recovery path are considered to be equivalent. Instead of \nusing all of 1 The reader might wonder why the I/Os A, B, C, D, and E appear again in the recovery paths \neven though the getRecoveryPath function returns the diff between the I/Os in the execution with failures \nand in the normal execution path, and thus should exclude those I/Os. The answer is that these I/Os are \nexecuted in the recovery path too, but with different contexts (e.g. different message content, different \ngeneration number) that we incorporate in the fip. For simplicity, we do not discuss these detailed contexts \nhere.  8 def eqvBySrcLoc (fs1, fs2): 9 r1 = getRecoveryPath ( fs1 ) 10 r2 = getRecoveryPath (fs2) 11 \nc1 = abstractIn(r1, loc ) 12 c2 = abstractIn(r2, loc ) 13 return c1== c2 Figure 11. Equivalence of recovery \npaths. Return true if two failure sequences result in the system executing I/Os at the same set of source \nlocations during recovery. The function abstractIn retains only the mappings for the source locations \n( loc ) in the fip s in its argument set. the context in the fips, the tester might abstract out the \nfips and use only part of the context in them to characterize a recovery path. For example, the tester \nmight want to use only the source locations of fips. Thus, she might consider two recovery paths to be \nthe same if the I/Os in them occur at the same set of source locations. The function in Figure 11 considers \nthis relaxed characterization of recovery paths. Thus, in PREFAIL, a tester has the power and .exibility \nto decide how to characterize and cluster recovery paths. If we use the equivalence function in Figure \n11 to cluster failure sequences that result in the same recovery path into the same class, then we would \nobtain four different equiv\u00adalence classes for the HDFS write protocol. The third col\u00adumn in Figure 2 \nshows the four classes: 0, \u00a6, ., and 0 which represent the recovery paths {ABCDE}, {CDEF}, {CDEG}, and \n{CDE} respectively. Note that the recovery paths of A1c and B2c are considered to be equivalent (0)as \nthey have I/Os at the same set of source locations {ABCDE}even if the I/Os are executed in different \nnodes. However, if the tester decides to characterize recovery paths using both source location and node \nID, then the recovery paths of A1c and B2c would be considered to be different (0 and .), as shown in \nthe last column in Table 2. Figure 12 provides more details of how different I/Os shown in Figure 6 \nare grouped into different recovery classes. The left .gure shows 4 recovery classes that result from \nthe use of only source location to distinguish between different recovery paths. Even by just using source \nlocation, PREFAIL is able to distinguish between the two main recov\u00adery classes in the protocol (0 and \n0). Furthermore, PRE-FAIL also .nds two unique cases of failures that result in two more recovery classes \n(\u00a6 and .). In the .rst one (\u00a6), a crash at C1 leaves the surviving nodes (N2 and N3) with zero\u00adlength \nblocks, and thus the recovery protocol executes I/Os at a different source location (labeled with I in \nTable 2). In the second one (.), a crash at C2 leaves the surviving nodes (N1 and N3) with different \nblock sizes (the .rst node has re\u00adceived the bytes, but not the last node), and thus I/Os at yet another \ndifferent source location (labeled as J) are executed. Figure 12b shows the 8 recovery classes that result \nwhen node ID is used in addition to the source location to char-   (a) Src. Loc. (b) Src. Loc. + Node \nID. 4 recovery classes: 8 recovery classes: 0, \u00a6, ., 0. 0, ., O, \u00a6, ., 0, v, .. Figure 12. Recovery \nClasses of HDFS Write Protocol. The symbols (e.g., A1, A2) represent I/Os described in Figure 6. A shape \n(e.g., D) surrounding an I/O #X represents the equivalence class of the I/O with regard to the recovery \npath that is taken by HDFS when a crash occurs at that I/O. Different shapes represent different equivalence \nclasses. The two .gures show how the I/Os are grouped differently into equivalence classes when recovery \npaths are characterized in different ways (e.g., (a) using source location only and (b) using source \nlocation and node ID). 1 def cls (fs1, fs2): 2 last1 = fs1 [ len(fs1) -1 ] 3 last2 = fs2 [ len(fs2) -1 \n] 4 prefix1 = fs1 [ 0 : len(fs1) -1 ] 5 prefix2 = fs2 [ 0 : len(fs2) -1 ] 6 isEqv = eqvBySrcLoc ( prefix1 \n, prefix2 ) 7 return isEqv and ( last1 == last2 ) Figure 13. Equivalent-recovery clustering. Cluster \ntwo failure sequences if their pre.xes (that exclude the last fit s) result in the same recovery path \nand their last fit s are the same. Line 6usesthe eqvBySrcLoc function in Figure 11 to compute the equivalence \nof the recovery paths of the pre.xes. acterize recovery paths. If the tester uses all of the con\u00adtext \npresent in a fip, the I/Os in the write protocol will be grouped into 10 recovery classes. Interested \nreaders can .nd the explanations behind the different numbers of recovery classes in [25]. In general, \nthe more context information in fips considered, the more we can distinguish between dif\u00adferent recovery \npaths, and hence the more the number of re\u00adcovery classes of I/Os. Lesser context leads to fewer recov\u00adery \nclasses and thus fewer failure-injection experiments, but might miss some corner-case bugs.  4.4.3 Clustering \nFailure Sequences After specifying the characterization of a recovery path, the tester can simply write \na cluster policy that uses the cls function in Figure 13. Given this policy, if there are two failure \nsequences, (prefix1, last) and (prefix2, last), such that prefix1 and prefix2 result in the same recovery \npath, then PREFAIL will exercise only one of the two sequences.  1 def flt (fs): 2 for f in fs: 3 fp \n= FIP(f) 4 isCrash = (fp[ failure ] == crash ) 5 isWrite = (fp[ ioType ] == write ) 6 isBefore = (fp[ \nplace ] == before ) 7 if isCrash and ( not ( isWrite and isBefore )): 8 return False 9 return True Figure \n14. Generic crash optimization. The function accepts a failure sequence if all crash failures in the \nsequence are injected before write I/Os. If a failure sequence has a crash that is not injected before \na write I/O, then that sequence is rejected, and thus not exercised by the FI engine. To illustrate the \nresult of this policy, let s consider the ex\u00adample in Table 2. The fit Fc (crash at I/O F) can be exer\u00adcised \nafter any of the crashes at {DE}\u00d7{123} (i.e.,6 fits). Without the speci.ed equivalent-recovery clustering, \nPRE-FAIL will run 6 experiments (D1cFc.. E3cFc). But with this policy, PREFAIL will group all of the \n6 failure sequences into a single class (D1c/../E1c +Fc) as all the pre.xes have the same recovery class \n(0, as shown in Figure 12), and thus will run only 1 experiment to exercise any of the 6 failure sequences. \nIf the tester changes the clustering function such that it uses both source location and node ID to characterize \na recovery path (Figure 12b), then PREFAIL will run three experiments as the pre.xes now fall into three \ndifferent re\u00adcovery classes (0, v, and .).  4.5 Pruning via Optimizations In general, failures can \nbe injected before and/or after every read and write I/O, system call or library call. For some types \nof failures like crashes or disk failures, there are optimiza\u00adtions that can be performed to eliminate \nunnecessary failure\u00adinjection experiments. In the following sections, we present policies that implement \noptimizations for crashes and disk failures in distributed systems. Appendix A describes the op\u00ad timizations \nfor network failures and disk corruption. By re\u00adducing the number of individual failure-injection tasks, \nthese optimizations also help in reducing the number of multiple\u00adfailure sequences. 4.5.1 Crashes In \na distributed system, read I/Os performed by a node affect only the local state of the node, while write \nI/Os potentially affect the states and execution of other nodes. Therefore, we do not need to explore \ncrashing of nodes around read I/Os. We can just explore crashing of nodes before write I/Os. Figure 14 \nshows a flt function that can be used to implement this optimization. The second optimization that we \ncan do for crashes is that we do not crash a node before the node performs a network write I/O that sends \na message to an already crashed node. This is because crashing a node before a network write I/O can \nonly affect the node to which the message is being sent, but the receiver node is itself dead in this \ncase. The flt function that implements this optimization is shown in Figure 16 in Appendix A.  4.5.2 \nDisk Failures For disk failures (permanent and transient), we inject failures before every write I/O \ncall, but not before every read I/O call. Consider two adjacent Java read I/Os from the same input .le \n(e.g., f.readInt() and f.readLong()). It is unlikely that the second call throws an I/O exception, but \nnot the .rst one. This is because the .le is typically already buffered by the OS. Thus, if there is \na disk failure, it is more likely the case that an exception is already thrown by the .rst call. Thus, \nwe can optimize and only inject read disk failures on the .rst read of every .le (i.e., we assume that \n.les are always buffered after the .rst read). The subsequent reads to the .le will naturally fail. The \npolicy for this optimization is similar to the one for network failure optimization that is explained \nin Appendix A (Figure 17).  4.6 Failing Probabilistically Finally, a tester can inject multiple failures \nif they satisfy some probabilistic criteria. We have not explored this strat\u00adegy in great extent because \nwe need some real-world failure statistic to perform real evaluation. However, we believe that specifying \nthis type of policy in PREFAIL will be straight\u00adforward. For example, the tester can write a policy as \nsimple as: return true if prob(fs) > 0.1. That is, inject a failure se\u00adquence fs only if the probability \nof the failures happening together is larger than 0.1. The tester needs to implement the prob function \nthat ideally uses some real-world failure statistic (e.g., a statistic that shows the probability distribu\u00adtion \nof two machine crashes happening at the same time). In summary, the programmable policy framework allows \ntesters to write various failure exploration policies in order to achieve different testing and optimization \nobjectives. In addition, as different systems and workloads employ differ\u00adent recovery strategies, we \nbelieve this programmability is valuable in terms of systematically exploring failures that are appropriate \nfor each strategy.  5. Evaluation In this section, we evaluate the different aspects of PREFAIL. We \n.rst list our target systems and workloads, along with the bugs that we found (\u00a75.1 and \u00a75.2). Then, \nwe quantify the effectiveness of pruning policies that we have written (\u00a75.3). Finally, we show the implementation \ncomplexity of PREFAIL (\u00a75.4).  5.1 Target Systems, Workloads, and Bugs We have integrated PREFAIL on \ndifferent releases of three popular cloud systems: HDFS [38] v0.20.0, v0.20.2+320,  and v0.20.2+737 \n(the last one is a release used by Cloud\u00adera customers [9]), ZooKeeper [23] v3.2.2 and v3.3.1, and Cassandra \n[29] v0.6.1 and v0.6.5. These integrations show that it is easy to port our tool to real-world systems \nand re\u00adleases. We evaluate PREFAIL on four HDFS workloads (log recovery, read, write, and append), two \nCassandra workloads (key-value insert and log recovery), and one ZooKeeper workload (leader election). \nIn this work, we focused more on Cloudera s HDFS, and thus present extensive evaluation numbers for it. \nWe present partial results for other releases.  5.2 Bugs Found With PREFAIL, we were able to .nd all \nof the 16 bugs in HDFS v0.20.0 that we had reported in our previous work [18]. We were told that many \ninternal designs of HDFS have changed since that version. After we integrated PRE-FAIL to a much newer \nHDFS version (v0.20.2+737), we found 6 more previously unknown bugs (three have been con.rmed, and three \nare still under consideration). Impor\u00adtantly, the developers believe that the bugs are crucial ones and \nare hard to .nd without a multiple-failure testing tool. These bugs are basically availability (e.g., \nthe HDFS master node is unable to reboot permanently) and reliability bugs (e.g., user data is permanently \nlost). For brevity of space, we explain below only one of the new recovery bugs. This bug is present \nin the HDFS append protocol, and it happens be\u00adcause of multiple failures. The task of the append protocol \nis to atomically append new bytes to three replicas of a .le that are stored in three nodes. With two \nnode failures and three replicas, append should be successful as there is still one working replica. \nHowever, we found a recovery bug when two failures were injected; the append protocol returns error to \nthe caller and the surviving replica (that has the old bytes) is inaccessi\u00adble. Here are the events that \nlead to the bug: The .rst node\u00adcrash causes the append protocol to initiate a quite complex distributed \nrecovery protocol. Somewhere in the middle of this recovery, a second node-crash happens, which leaves \nthe system in an unclean state. The protocol then initiates an\u00adother recovery again. However, since the \nprevious recovery did not .nish and the system state was not properly cleaned, this last initiation of \nrecovery (which should be successful) cannot proceed. Thus, an error is returned to the append caller, \nand worse since the surviving replica is in an unclean state, the .le cannot be accessed.  5.3 Effectiveness \nof Policies We now report the effectiveness of some of the pruning policies that we have written. We \n.rst present the code\u00adcoverage (Section 4.3) and recovery-coverage (Section 4.4) based policies, and \nthen the optimization-based policies (Section 4.5). Pruning speedup with diverse policies  (2 failures/run) \n(3 failures/run) Figure 15. #Experiments run with different coverage\u00adbased policies. The y-axis shows \nthe number of failure-injection (just crash-only failure) experiments for a given policy and a work\u00adload. \nThe x-axis shows the workloads: the write (Wrt), append (App), and log recovery (LogR) protocols from \nCloudera s ver\u00adsion of HDFS. We also run workloads from the old HDFS release v0.20.0 (marked with *), \nwhich has a different design (and hence different results). Two and three crashes were injected per exper\u00adiment \nfor the bars on the left-and right-hand sides respectively. CC and BF represent the code-coverage policy \nand brute-force ex\u00adploration, respectively. R-L, R-LN, and R-All represent recovery\u00adcoverage policies \nthat use three different ways to characterize re\u00adcovery (\u00a74.3): using source location only (L), source \nlocation and node (LN), and all information in fip (All). We stopped our ex\u00adperiments when they reached \n10,000 (Hence, the maximum number of experiments is 10,000). 5.3.1 Coverage-Based Policies We show the \nbene.ts of using different coverage-based fail\u00adure exploration policies to prune down the failure space \nin different ways. Figure 15 shows the different number of ex\u00ad periments that PREFAIL runs for different \npolicies. An ex\u00adperiment takes between 5 to 9 seconds to run. Here, we inject crash-only failures so \nthat the numbers are easy to compare. The .gure only shows numbers for multiple-failure experi\u00adments \nbecause injecting multiple failures is where the major bottleneck is. With PREFAIL, a tester can choose \ndifferent policies, and hence different numbers of experiments and speed-ups, de\u00adpending on her time \nand resource constraints. For example, the code-coverage policy (CC) gives two orders of magni\u00adtude improvement \nover the brute-force approach because it simply explores possible crashes at source locations that it \nhas not exercised before (e.g., after exploring two crashes, there is no new source location to cover \nin 3-crash cases). Recovery clustering policies (R-L, R-LN, etc.) on the other hand run more experiments, \nbut still give an order of magni\u00adtude improvement over the brute-force approach. The more  #Failed Workload \n#F Exps #Bugs #BugsR Write 2 000 3 46 1 1 Append 2 142 2 3 31 (*)2 (*)2 LogRecovery 2 6 3 0 3 3 (*)3 \n0 Table 3. #Bugs found. The table shows the number of failed experiments (#Failed Exps) for a given workload \nusing the simplest recovery clustering policy (R-L in Figure 15) and the number of crashes per run (#F), \nalong with the actual number of bugs that trigger the failed experiments (#Bugs). The last column (#BugsR) \nis the number of bugs that can be found using randomized failure injection. (*) implies that these are \nthe same bugs (i.e., bugs in 2\u00adfailure cases often appear again in 3-failure cases). relaxed the recovery \ncharacterization, the lesser the number of experiments (e.g.,R-L vs.R-All). Pruning is not bene.cial \nif it is not effective in .nding bugs. In our experience, the recovery clustering policies are effective \nenough in rapidly .nding important bugs in the system. To capture recovery bugs in the system, we wrote \nsimple recovery speci.cations for every target workload. For example, for HDFS write, we can write a \nspeci.cation that says if a crash happens during the data transfer stage, there should be two surviving \nreplicas at the end . If a speci.cation is not met, the corresponding experiment is marked as failed. \nTable 3 shows the number of bugs that we found even with the use of the most relaxed recovery clustering \npolicy (R-L, which only uses source location to characterize recov\u00adery). But again, a more exhaustive \npolicy could .nd bugs that were not caught by a more relaxed one. For example, we know an old bug that \nmight not surface with R-L pol\u00adicy, but does surface with R-LN policy which uses source location and \nnode ID to characterize recovery. The last col\u00adumn in the table shows the number of bugs that we can \n.nd by using randomized failure injection, that is, by randomly choosing the execution points at which \nto inject crashes. For each workload, we execute the system as many times as we do for the recovery clustering \npolicy, and randomly inject crashes in each execution. Randomized failure injection can .nd the bugs \nfor the write and append workloads, but not for the log recovery workload. This is because the bugs for \nthe log recovery workload are corner-case bugs; the propor\u00adtion of failure sequences that lead to a log \nrecovery bug is much smaller than that for a write or an append bug. This shows that randomized failure \ninjection, though simple to implement, is not effective in .nding corner-case bugs that manifest only \nin speci.c failure scenarios. Workload Crash Disk Net Data Failure Failure Corruption H. Read 2/42 1/4 \n4/17 1/4 H. Write 57/454 27/27* 45/200 N.A. H. Append 111/880 43/60 117/380 1/18 H. LogR 36/128 39/64 \nN.A. 3/28 C. Insert 33/102 25/25* 12/26 N.A. C. LogR 84/196 89/98 N.A. 5/14 Z. Leader 39/132 21/21* \n31/45 N.A. Table 4. Bene.ts of Optimization-based Policies. The table shows the bene.ts of the optimization-based \npolicies on four HDFS workloads (H), two Cassandra workloads (C), and one ZooKeeper workload (Z). Each \ncell shows two numbers X/Y where Y and X are the numbers of failure-injection experiments for single \nfailures without using and with using the optimization respectively. N.A. represents a not applicable \ncase; the failure type never occurs for the workload. For write workloads, the replication factor is \n3 (i.e., 3 nodes participating). (*) These write workloads do not perform any disk read, and thus the \noptimization does not work here.  5.3.2 Optimization-Based Policies Table 4 shows the effectiveness \nof the optimizations of dif\u00ad ferent failure types that we described in Section 4.5. The optimizations \nfor network failures and data corruption are shown in Appendix A. Each cell presents two numbers X/Y \nwhere Y and X are the numbers of failure-injection exper\u00adiments for single failures without using and \nwith using the optimization respectively. Overall, depending on the work\u00adload, the optimizations bring \n21 to 1 times (5 on average) of reduction in the number of failure-injection experiments.  5.4 Complexity \nThe FI engine is based on our previous work [18], which is written in 6000 lines of Java code. We added \naround 160 lines of code to this old tool so that it passes on appropri\u00adate failure and execution abstractions \nto the FI driver. The FI driver is implemented in 1266 lines of Python code. It implements a library \nof functions that testers can use to ac\u00adcess fits, fips and execution pro.les passed on by the FI engine. \nIt also uses the policies written by testers to prune down the set of failure sequences that can be exercised \nby FI engine. We have written a number of different pruning poli\u00adcies in Python using the library provided \nby the FI driver. On an average, we wrote a policy in 17 lines of Python code.  6. Limitations PREFAIL \ndoes not control all kinds of non-determinism present in a system execution (e.g., network message order\u00ading). \nTherefore, two executions of the same system against the same workload might be different, and PREFAIL \nmight not be able to inject a failure sequence that seemed possible to inject from a previous system \nexecution. In future work, we plan to control the non-determinism that arises out of network message \nordering and also expose it to testers and provide support for writing policies that can express the \nmes\u00adsage orderings to test for.  7. Related Work In this section, we compare our work with other work \nthat re\u00adlates to failure-injection. More speci.cally, we discuss other related work that provide some \nlanguage support for speci\u00adfying failure-injection tasks, and present techniques to prune down large \nfailure spaces. There has been some work in designing a clear language support for expressing which failures \nto inject. FAIL (Fault Injection Language) is a domain-speci.c language that de\u00adscribes failure scenarios \nfor Grid middleware [21]. FIG also uses a domain-speci.c language to inject failures at library level \n[4]. Orchestra uses TCL scripts to inject failures at TCP level [12]. Genesis2 uses a scripting language \nto specify service-level failures [26]. LFI uses an XML-based language to trigger failures at library \nlevel [31]. These works however do not describe how a wide range of policies can be writ\u00adten in their \nlanguages. Furthemore, the tester might need to write code from scratch to build the failure-injection \ntasks in these languages. In contrast, in our work, we abstract out a failure-injection task, and let \ntesters easily use the informa\u00adtion in the abstraction to write policies. Our work is motivated by the \nneed to exercise multi\u00adple failures especially to test cloud software systems. As mentioned before, one \nmajor challenge is the large num\u00adber of combinations of failures to explore. One direct way to explore \nthe space is via randomness. For example, ran\u00addom injection of failures is employed by the developers \nat Google [7], Yahoo! [40], Microsoft [43], Amazon [20], and other places [22]. Random failure-injection \nis relatively sim\u00ad ple to implement, but the downside is that it can easily miss corner-case bugs that \nmanifest only when speci.c failure se\u00adquences are injected. Another approach is to exhaustively explore \nall possible failure scenarios by injecting sequences of failures in all pos\u00adsible ways during execution. \nHowever, we found that within the execution of a protocol (e.g., distributed write protocol, log recovery), \nthere are potentially thousands of possible combinations of failures that can be exercised, which can \ntake hundreds of hours of testing time [19]. Thus, exhaus\u00ad tive testing is plausible only if the tester \nhas enough time budget and computing resources. Other than random and exhaustive approaches, there has \nbeen some work in devising smart techniques that systemat\u00adically prune down large failure spaces. Extensible \nLFI [32] for example automatically analyzes the system to .nd code that is potentially buggy in its handling \nof failures (e.g.,sys\u00adtem calls that do not check some error-codes that could be returned). AFEX [27] \nautomatically .gures out the set of failure scenarios that when explored can meet a certain given coverage \ncriterion like a given level of code coverage. It uses a variation of stochastic beam search to .nd the \nfailure sce\u00adnarios that would have the maximal effect on the coverage criterion. Fu et al. [15] use compile-time \nanalysis to .nd which failure-injection points would lead to the execution of which error recovery code. \nThey use this information to guide failure injection to obtain a high coverage of recov\u00adery code. To \nthe best of our knowledge, the authors of these works do not address pruning of combinations of multiple \nfailures in distributed systems. The multiple-failure combinatorial explosion problem is similar to the \nstate explosion problem in model check\u00ading. Existing system model-checkers [43, 44] use domain\u00ad speci.c \noptimization techniques to address the state explo\u00adsion problem. However, when it comes to multiple failures, \nwe did not .nd any system model-checker that is able to effectively prune down combinations of multiple \nfailures. We believe that some of the pruning strategies that we have introduced in our work can be integrated \nwithin a system model checker. There has been some work in program testing [3, 11, 17] that uses tester-written \nspeci.cations or input generators to produce all non-isomorphic test inputs bounded by a given size. \nThe speci.cations or generators can be thought of as being analogous to the tester-written pruning policies \nin PREFAIL, and the process of generating inputs from them by pruning down the input space can be thought \nof as being analogous to the process of pruning down the failure space using policies. The speci.cations \nare used only for the pur\u00adpose of generating test inputs, and there is no support to ad\u00address failures \nin the speci.cations.  8. Conclusion We have presented PREFAIL, a programmable failure\u00adinjection tool \nthat provides appropriate failure abstractions and execution pro.les to let testers write a wide variety \nof policies to prune down large spaces of multiple-failure combinations. Currently, we are adding two \nother important features to PREFAIL: support for triaging of failed experi\u00adments, and parallelizing the \nwhole architecture of PREFAIL. Since debugging each failed experiment can take a signi.\u00adcant amount of \ntime (many hours or even days), being able to automatically triage failed experiments according to the \nbugs that caused them can be very useful. Policies in PRE-FAIL already prune down a failure space and \nresult in a speed-up of the entire failure testing process, but paralleliz\u00ading PREFAIL would lead to \nan even greater speed-up. The test work.ow of PREFAIL can in fact be very easily paral\u00adlelized. Overall, \nour goal in building PREFAIL is to help to\u00adday s large-scale distributed systems prevail against pos\u00adsible \nhardware failures that can arise. Although so far we use PREFAIL primarily to .nd reliability bugs, we \nenvision PREFAIL will empower many more program analyses un\u00adder failures . That is, we note that many \nprogram analy\u00adses (related to data races, deadlocks, security, etc.) are of\u00adten done when the target \nsystem faces no failure. However, we did .nd data races and deadlocks under some failure sce\u00adnarios. \nTherefore, for today s pervasive cloud systems, we believe that existing analysis tools should also run \nwhen the target system faces failures. The challenge is that some pro\u00adgram analyses might already be \ntime-consuming. Running them with failures will prolong the testing time. We believe the pruning policies \nthat PREFAIL supports will be valuable in reducing the testing time for these analyses. And again, we \nhope that our work attracts other researchers to present other pruning alternatives.  9. Acknowledgments \nThis material is based upon work supported by the NSF/ CRA Computing Innovation Fellowship and the National \nScience Foundation under grant numbers CCF-1016924, CNS-0720906, CCF-0747390, CCF-1018729, and CCF\u00ad0747390. \nThe third author is supported in part by an Alfred P. Sloan Foundation Fellowship. We also thank Eli \nCollins and Todd Lipcon from Cloudera Inc. for helping us con.rm the HDFS bugs that we found. Any opinions, \n.ndings, con\u00adclusions, or recommendations expressed in this material are those of the authors and do \nnot necessarily re.ect the views of NSF or other institutions. References [1] Hadoop MapReduce. http://hadoop.apache.org/ \nmapreduce. [2] Jonathan Aldrich and Craig Chambers. Ownership Domains: Separating Aliasing Policy from \nMechanism. In Proceedings of the 18th European Conference on Object-Oriented Pro\u00adgramming (ECOOP 04), \nOslo, Norway, June 2004. [3] Chandrasekhar Boyapati, Sarfraz Khurshid, and Darko Mari\u00adnov. Korat: Automated \nTesting Based on Java Predicates. In Proceedings of the International Symposium on Software Test\u00ading \nand Analysis (ISSTA 02), pages 123 133, Rome, Italy, July 2002. [4] Pete Broadwell, Naveen Sastry, and \nJonathan Traupman. FIG: A Prototype Tool for Online Veri.cation of Recovery Mech\u00adanisms. In Workshop \non Self-Healing, Adaptive and Self-Managed Systems. [5] Mike Burrows. The Chubby lock service for loosely-coupled \ndistributed systems. In Proceedings of the 7th Symposium on Operating Systems Design and Implementation \n(OSDI 06), Seattle, Washington, November 2006. [6] George Candea and Armando Fox. Crash-Only Software. \nIn The Ninth Workshop on Hot Topics in Operating Systems (HotOS IX), Lihue, Hawaii, May 2003. [7] Tushar \nChandra, Robert Griesemer, and Joshua Redstone. Paxos Made Live -An Engineering Perspective. In Proceed\u00adings \nof the 26th ACM Symposium on Principles of Distributed Computing (PODC 07), Portland, Oregon, August \n2007. [8] Fay Chang, Jeffrey Dean, Sanjay Ghemawat, Wilson C. Hsieh, Deborah A. Wallach, Michael Burrows, \nTushar Chandra, An\u00addrew Fikes, and Robert Gruber. Bigtable: A Distributed Stor\u00adage System for Structured \nData. In Proceedings of the 7th Symposium on Operating Systems Design and Implementation (OSDI 06), Seattle, \nWashington, November 2006. [9] Eli Collins and Todd Lipcon. Contact Persons at Cloudera Inc., 2011. [10] \nBrian Cooper, Adam Silberstein, Erwin Tam, Raghu Ramakr\u00adishnan, and Russell Sears. Benchmarking Cloud \nServing Sys\u00adtems with YCSB. In Proceedings of the 2010 ACM Sympo\u00adsium on Cloud Computing (SoCC 10), Indianapolis, \nIndiana, June 2010. [11] Brett Daniel, Danny Dig, Kely Garcia, and Darko Marinov. Automated Testing of \nRefactoring Engines. In Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering \n(ESEC/FSE 07), Dubrovnik, Croatia, September 2007. [12] Scott Dawson, Farnam Jahanian, and Todd Mitton. \nExperi\u00adments on Six Commercial TCP Implementations Using a Soft\u00adware Fault Injection Tool. Software Practice \nand Experi\u00adence, 27:1385 1410, 1997. [13] Jeffrey Dean. Underneath the covers at google: Current sys\u00adtems \nand future directions. In Google I/O, 2008. [14] Daniel Ford, Franis Labelle, Florentina I. Popovici, \nMurray Stokely, Van-Anh Truong, Luiz Barroso, Carrie Grimes, and Sean Quinlna. Availability in Globally \nDistributed Storage Systems. In Proceedings of the 9th Symposium on Operating Systems Design and Implementation \n(OSDI 10), Vancouver, Canada, October 2010. [15] Chen Fu, Barbara G. Ryder, Ana Milanova, and David Won\u00adnacott. \nTesting of Java Web Services for Robustness. In Pro\u00adceedings of the International Symposium on Software \nTesting and Analysis (ISSTA 04), Boston, Massachusetts, July 2004. [16] Garth Gibson. Reliability/Resilience \nPanel. In High-End Computing File Systems and I/O Workshop (HEC FSIO 10), Arlington, VA, August 2010. \n[17] Milos Gligoric, Tihomir Gvero, Vilas Jagannath, Sarfraz Khurshid, Viktor Kuncak, and Darko Marinov. \nTest gener\u00adation through programming in UDITA. In Proceedings of the 32nd ACM/IEEE International Conference \non Software Engineering (ICSE 10), pages 225 234, Cape Town, South Africa, May 2010. [18] Haryadi S. \nGunawi, Thanh Do, Pallavi Joshi, Peter Alvaro, Joseph M. Hellerstein, Andrea C. Arpaci-Dusseau, Remzi \nH. Arpaci-Dusseau, and Koushik Sen. FATE and DESTINI:A Framework for Cloud Recovery Testing. In Proceedings \nof the 8th Symposium on Networked Systems Design and Imple\u00admentation (NSDI 11), Boston, Massachusetts, \nMarch 2011. [19] Haryadi S. Gunawi, Thanh Do, Pallavi Joshi, Joseph M. Hellerstein, Andrea C. Arpaci-Dusseau, \nRemzi H. Arpaci-Dusseau, and Koushik Sen. Towards Automatically Check\u00ading Thousands of Failures with \nMicro-speci.cations. In The 6th Workshop on Hot Topics in System Dependability (HotDep 10), Vancouver, \nCanada, October 2010.  [20] Alyssa Henry. Cloud Storage FUD: Failure and Uncertainty and Durability. \nIn Proceedings of the 7th USENIX Symposium on File and Storage Technologies (FAST 09), San Francisco, \nCalifornia, February 2009. [21] William Hoarau, Sebastien Tixeuil, and Fabien Vauchelles. FAIL-FCI: Versatile \nfault injection. Journal of Future Gener\u00adation Computer Systems archive, Volume 23 Issue 7, August, 2007. \n[22] Todd Hoff. Net.ix: Continually Test by Failing Servers with Chaos Monkey. http://highscalability.com, \nDecember 2010. [23] Patrick Hunt, Mahadev Konar, Flavio P. Junqueira, and Ben\u00adjamin Reed. ZooKeeper: \nWait-free coordination for Internet\u00adscale systems. In Proceedings of the 2010 USENIX Annual Technical \nConference (ATC 10), Boston, Massachusetts, June 2010. [24] Andreas Johansson and Neeraj Suri. Error \nPropagation Pro.l\u00ading of Operating Systems . In Proceedings of the International Conference on Dependable \nSystems and Networks (DSN 05), Yokohama, Japan, June 2005. [25] PallaviJoshi,HaryadiS.Gunawi,andKoushikSen.PREFAIL: \nA Programmable Failure-Injection Framework. UC Berkeley Technical Report UCB/EECS-2011-30, April 2011. \n[26] Lukasz Juszczyk and Schahram Dustdar. Programmable Fault Injection Testbeds for Complex SOA. In \nProceedings of the 8th International Conference on Service Oriented Computing (ICSOC 10), San Francisco, \nCalifornia, December 2010. [27] Lorenzo Keller, Paul Marinescu, and George Candea. AFEX: An Automated \nFault Explorer for Faster System Testing, 2008. [28] Philip Koopman and John DeVale. Comparing the Robustness \nof POSIX Operating Systems. In Proceedings of the 29th In\u00adternational Symposium on Fault-Tolerant Computing \n(FTCS\u00ad29), Madison, Wisconsin, June 1999. [29] Avinash Lakshman and Prashant Malik. Cassandra -a decen\u00adtralized \nstructured storage system. In The 3rd ACM SIGOPS International Workshop on Large Scale Distributed Systems \nand Middleware (LADIS 09), Florianopolis, Brazil, October 2009. [30] R. Levin, E. Cohen, W. Corwin, F. \nJ. Pollack, and W. Wulf. Policy/mechanism separation in Hydra. In Proceedings of the 5th ACM Symposium \non Operating Systems Principles (SOSP 75), Austin, TX, November 1975. [31] Paul Marinescu and George \nCandea. LFI: A Practical and General Library-Level Fault Injector. In Proceedings of the In\u00adternational \nConference on Dependable Systems and Networks (DSN 09), Lisbon, Portugal, June 2009. [32] Paul D. Marinescu, \nRadu Banabic, and George Candea. An Extensible Technique for High-Precision Testing of Recovery Code. \nIn Proceedings of the 2010 USENIX Annual Technical Conference (ATC 10), Boston, Massachusetts, June 2010. \n[33] David Patterson, Garth Gibson, and Randy Katz. A Case for Redundant Arrays of Inexpensive Disks \n(RAID). In Proceed\u00adings of the 1988 ACM SIGMOD Conference on the Manage\u00adment of Data (SIGMOD 88), Chicago, \nIllinois, June 1988. [34] Eduardo Pinheiro, Wolf-Dietrich Weber, and Luiz Andre Bar\u00adroso. Failure Trends \nin a Large Disk Drive Population. In Proceedings of the 5th USENIX Symposium on File and Stor\u00adage Technologies \n(FAST 07), San Jose, California, February 2007. [35] Vijayan Prabhakaran, Lakshmi N. Bairavasundaram, \nNitin Agrawal, Haryadi S. Gunawi, Andrea C. Arpaci-Dusseau, and Remzi H. Arpaci-Dusseau. IRON File Systems. \nIn Proceed\u00adings of the 20th ACM Symposium on Operating Systems Prin\u00adciples (SOSP 05), Brighton, United \nKingdom, October 2005. [36] C. J. Price and N. S. Taylor. Automated multiple failure FMEA. Reliability \nEngineering and System Safety, 76(1):1 10, April 2002. [37] Bianca Schroeder and Garth Gibson. Disk failures \nin the real world: What does an MTTF of 1,000,000 hours mean to you? In Proceedings of the 5th USENIX \nSymposium on File and Storage Technologies (FAST 07), San Jose, California, February 2007. [38] Konstantin \nShvachko, Hairong Kuang, Sanjay Radia, and Robert Chansler. The Hadoop Distributed File System. In Proceedings \nof the 26th IEEE Symposium on Massive Stor\u00adage Systems and Technologies (MSST 10), Incline Village, Nevada, \nMay 2010. [39] Alex C. Snoeren and Barath Raghavan. Decoupling Policy from Mechanism in Internet Routing. \nACM SIGCOMM Com\u00adputer Communication Review, 34(1), January 2004. [40] Hadoop Team. Hadoop Fault Injection \nFramework and Devel\u00adopment Guide. http://hadoop.apache.org/hdfs/docs/ r0.21.0/faultinject_framework.html. \n [41] Kashi Vishwanath and Nachi Nagappan. Characterizing Cloud Computing Hardware Reliability. In Proceedings \nof the 2010 ACM Symposium on Cloud Computing (SoCC 10), Indianapolis, Indiana, June 2010. [42] Tom White. \nHadoop The De.nitive Guide. O Reilly, 2009. [43] Junfeng Yang, Tisheng Chen, Ming Wu, Zhilei Xu, Xuezheng \nLiu, Haoxiang Lin, Mao Yang, Fan Long, Lintao Zhang, and Lidong Zhou. MODIST: Transparent Model Checking \nof Un\u00admodi.ed Distributed Systems. In Proceedings of the 6th Sym\u00adposium on Networked Systems Design and \nImplementation (NSDI 09), Boston, Massachusetts, April 2009. [44] Junfeng Yang, Paul Twohey, Dawson Engler, \nand Madanlal Musuvathi. Using Model Checking to Find Serious File Sys\u00adtem Errors. In Proceedings of the \n6th Symposium on Oper\u00adating Systems Design and Implementation (OSDI 04),San Francisco, California, December \n2004.  A. Appendix We explain the optimizations that we perform to eliminate redundant failure-injection \nexperiments for network failures and disk corruption. A.1 Network Failures For network failures, we \ncan perform an optimization simi\u00adlar to disk failures (Section 4.5.2). Since there is no notion of .le \nin network I/Os, we keep information about the latest  1 def flt (fs): 2 for i in range ( len ( fs )): \n3 fp = FIP(fs[i]) 4 isNet = (fp[ ioTarget ] == net ) 5 isWrite = (fp[ ioType ] == write ) 6 isCrash = \n(fp[ failure ] == crash ) 7 rNode = fp[ receiver ] 8 pfx = fs[0:i] 9 if isNet and isWrite and isCrash \nand 10 nodeAlreadyCrashed(pfx, rNode): 11 return False 12 return True Figure 16. Crash optimization \nfor network writes. The function accepts a failure sequence if for each crash at a network write to a \nreceiver node rNode in the sequence, there is no pre\u00adceding crash in the sequence that occurs in the \nnode rNode .The function nodeAlreadyCrashed (also implemented by the tester but not shown) takes a failure \nsequence and a node as arguments, and returns true if there is a crash failure in the sequence that oc\u00adcurs \nin the given node. network read that a thread of a node performs. If a particu\u00adlar thread performs a \nread call that has the same sender as the previous call, then we assume that it is a subsequent read \non the same network message from the same sender to this thread (potentially buffered by the OS), and \nthus we do not explicitly inject a network failure on this subsequent read. In addition, we clear the \nread history if the node performs a network write, so that we can inject network failures when the node \nperforms future reads on different network mes\u00adsages. Also, we do not inject a network failure if one \nof the nodes participating in the message is already dead. Figure 17 shows the flt function that can \nbe used to implement the optimization for network failures.  A.2 Data Corruption In the case of disk \ncorruption, after data gets corrupted, all reads of the data give unexpected values for the data. It \nis possible but very unlikely that the .rst read of the data gives a non-corrupt value and the second \nread in the near future gives a corrupt one. Thus, we can perform an optimization similar to the disk-failure \ncase (Section 4.5.2). 1 def flt (fs): 2 for i in range ( len ( fs)): 3 fp = FIP(fs[i]) 4 isNetFail = \n(fp[ failure ] == netfail ) 5 isRead = (fp[ ioType ] == read ) 6 sender = fp[ sender ] 7 node = fp[ \nnode ] 8 thread = fp[ thread ] 9 time = fp[ time ] 10 pfx = fs[0:i] 11 allFS = allFitSeqs () 12 if isNetFail \nand isRead and 13 ( not first(pfx, node, thread, time , sender , allFS)): 14 return False 15 return True \nFigure 17. Network failure optimization. The function checks for each network failure at a read I/O in \na failure sequence to see if it is the .rst read of data in its thread that is sent by its sender to \nits node. The function first (also implemented by the tester, but not shown) determines this condition \nfor each network failure in the failure sequence. The key time in a fip records the time when the fip \nwas observed during execution in the FI engine. This key helps in determining the temporal position of \na read in the list of all failure sequences allFS passed on by the FI engine.   \n\t\t\t", "proc_id": "2048066", "abstract": "<p>As hardware failures are no longer rare in the era of cloud computing, cloud software systems must \"prevail\" against multiple, diverse failures that are likely to occur. Testing software against multiple failures poses the problem of combinatorial explosion of multiple failures. To address this problem, we present PreFail, a programmable failure-injection tool that enables testers to write a wide range of policies to prune down the large space of multiple failures. We integrate PreFail to three cloud software systems (HDFS, Cassandra, and ZooKeeper), show a wide variety of useful pruning policies that we can write for them, and evaluate the speed-ups in testing time that we obtain by using the policies. In our experiments, our testing approach with appropriate policies found all the bugs that one can find using exhaustive testing while spending 10X--200X less time than exhaustive testing.</p>", "authors": [{"name": "Pallavi Joshi", "author_profile_id": "81336490218", "affiliation": "UC Berkeley, Berkeley, CA, USA", "person_id": "P2839152", "email_address": "pallavi@cs.berkeley.edu", "orcid_id": ""}, {"name": "Haryadi S. Gunawi", "author_profile_id": "81100603773", "affiliation": "UC Berkeley, Berkeley, CA, USA", "person_id": "P2839153", "email_address": "haryadi@cs.berkeley.edu", "orcid_id": ""}, {"name": "Koushik Sen", "author_profile_id": "81100399070", "affiliation": "UC Berkeley, Berkeley, CA, USA", "person_id": "P2839154", "email_address": "ksen@cs.berkeley.edu", "orcid_id": ""}], "doi_number": "10.1145/2048066.2048082", "year": "2011", "article_id": "2048082", "conference": "OOPSLA", "title": "PREFAIL: a programmable tool for multiple-failure injection", "url": "http://dl.acm.org/citation.cfm?id=2048082"}