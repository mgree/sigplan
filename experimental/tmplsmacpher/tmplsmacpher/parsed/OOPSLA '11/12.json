{"article_publication_date": "10-22-2011", "fulltext": "\n Synthesizing Method Sequences for High-Coverage Testing * Suresh Thummalapenta1,Tao Xie2, Nikolai Tillmann3, \nJonathan de Halleux3, Zhendong Su4 1IBM Research, Bangalore, India 2Department of Computer Science, North \nCarolina State University, Raleigh, USA 3Microsoft Research, Redmond, USA 4Department of Computer Science, \nUniversity of California, Davis, USA surthumm@in.ibm.com, xie@csc.ncsu.edu, {nikolait, jhalleux}@microsoft.com, \nsu@cs.ucdavis.edu Abstract High-coverage testing is challenging. Modern object-oriented programs present \nadditional challenges for testing. One key dif.culty is the generation of proper method sequences to \nconstruct desired objects as method parameters. In this pa\u00adper, we cast the problem as an instance of \nprogram synthesis that automatically generates candidate programs to satisfy a user-speci.ed intent. \nIn our setting, candidate programs are method sequences, and desired object states specify an intent. \nAutomatic generation of desired method sequences is dif.cult due to its large search space sequences \noften involve methods from multiple classes and require speci.c primitive values. This paper introduces \na novel approach, called Seeker, to intelligently navigate the large search space. Seeker synergistically \ncombines static and dynamic analy\u00adses: (1) dynamic analysis generates method sequences to cover branches; \n(2) static analysis uses dynamic analysis in\u00adformation for not-covered branches to generate candidate \nsequences; and (3) dynamic analysis explores and elimi\u00adnates statically generated sequences. For evaluation, \nwe have implemented Seeker and demonstrate its effectiveness on four subject applications totalling 28K \nLOC. We show that Seeker achieves higher branch coverage and def-use cover\u00adage than existing state-of-the-art \napproaches. We also show that Seeker detects 34 new defects missed by existing tools. Categories and \nSubject Descriptors: D.2.3 [Software En\u00adgineering]: Coding Tools and Techniques Object-oriented programming; \nD.2.5 [Software Engineering]: Testing and Debugging Symbolic execution; D.2.6 [Software Engi\u00ad * The majority \nof the research reported in this submission was conducted while the .rst author was associated with North \nCarolina State University. Permission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies are not made or distributed for \npro.t or commercial advantage and that copies bear this notice and the full citation on the .rst page. \nTo copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci.c \npermission and/or a fee. OOPSLA 11, October 22 27, 2011, Portland, Oregon, USA. Copyright c &#38;#169; \n2011 ACM 978-1-4503-0940-0/11/10. . . $10.00 neering]: Programming Environments Integrated environ\u00adments; \nGeneral Terms: Languages, Experimentation Keywords: Object-oriented testing, Symbolic execution 1. Introduction \nHigh-Coverage Testing. An important goal of software testing is to achieve full or at least high coverage \n(either structural coverage such as branch coverage or data .ow coverage such as def-use coverage) of \nthe code under test. Achieving high coverage of object-oriented code requires de\u00adsired object states \nfor the receiver or arguments of a method under test (MUT). These desired object states help cover true \nor false branches of the conditional statements (such as if statements) in the MUT. For example, consider \nthe two classes from the C# QuickGraph [36] library shown in Fig\u00adure 1. A desired object state for covering \nthe true branch of Statement 24 (Branch B4) in Figure 1 is that the graph object should include at least \none edge. There exist two common approaches for producing de\u00adsired object states: sequence generation \n[8, 15, 17, 32, 35, 43, 45] and direct construction [3]. With sequence generation, desired object states \nare produced via generating method se\u00adquences that create and mutate objects, while with direct construction, \ndesired object states are produced via directly setting values to member .elds. In this paper, we adopt \nthe general sequence-generation approach since directly setting values to member .elds such as private \n.elds of a class can easily lead to invalid object states. For example, the follow\u00ading method sequence \n(S1) produces the preceding desired object state for the graph object, thereby covering B4. 00: AdjacencyGraph \nag = new AdjacencyGraph(); 01: Vertex v1 = new Vertex(0); 02: ag.AddVertex(v1); 03: ag.AddEdge(v1, v1); \nIn this sequence, AddVertex should precede AddEdge to satisfy the requirement that the vertices passed \nas arguments should already exist in the graph object (Statements 7 and 10).  00:class AdjacencyGraph \n: IVEListGraph { 01: private Collection edges; 02: private ArrayList vertices; 03: public void AddVertex \n(IVertex v){ 04: vertices.Add(v); // B1 05: } 06: public Edge AddEdge (IVertex v1, IVertex v2){ 07: if \n(!vertices.Contains(v1)) 08: throw new VNotFoundException(\"\"); 09: // B2 10: if (!vertices.Contains(v2)) \n11: throw new VNotFoundException(\"\"); 12: // B3 13: // create edge 14: Edge e = new Edge(v1, v2); 15: \nedges.Add(e); 16: } ... 17:} //UDFS:UndirectedDepthFirstSearch 18:class UDFSAlgorithm { 19: private IVEListGraph \ngraph; 20: private bool isComputed; 21: public UDFSAlgorithm (IVEListGraph g){ 22: ... } 23: public void \nCompute (IVertex s){ ... 24: if(graph.GetEdges().Size() > 0){ // B4 25: isComputed = true; 26: foreach \n(Edge e in graph.GetEdges()){ 27: ... // B5 28: } 29: } 30: } ... 31:} Figure 1. Two classes from C# \nQuickGraph library [36]. Program Synthesis. In this paper, we cast the problem of generating method sequences \nas an instance of general pro\u00adgram synthesis that automatically generates candidate pro\u00adgrams to satisfy \na user-speci.ed intent. The intent can be expressed in various forms such as high-level speci.cations, \nnatural language, or input-output examples. Based on the in\u00adtent, synthesizers, unlike compilers that \nperform transforma\u00adtions, search for programs that satisfy the user-speci.ed in\u00adtent over a space of \nall possible candidate programs. Automatic program synthesis has seen interesting ad\u00advances in recent \nyears due to the availability of more ad\u00advanced computing resources and better reasoning techniques such \nas SMT solvers [47]. In contrast to previous work [12] that focuses on synthesizing algorithms such as \nsorting or bit-manipulation routines, this paper focuses on synthesiz\u00ading object-oriented programs that \ninvolve method sequences. In particular, we accept a user-speci.ed intent in the form of a desired object \nstate and automatically synthesize a method sequence that produces the desired object state. For exam\u00adple, \ngiven the user-speci.ed intent as a conditional branch Client Code: 00: public static void foo (UDFSAlgorithm \nudfs) { 01: ... 02: if(udfs.GetIsComputed()) { 03: ... // B6 04: } 05: // B7 06: } Figure 2. User-speci.ed \nintent expressed as a desired object state. 01: Vertex s1 = new Vertex(0); 02: AdjacencyGraph ag = new \nAdjacencyGraph(); 03: ag.AddVertex(s1); 04: ag.AddEdge((IVertex)s1, (IVertex)s1); 05: UDFSAlgorithm ud \n= new UDFSAlgorithm(ag); 06: ud.Compute((IVertex)null); Figure 3. An example method sequence. (such as \nBranch B6 in Statement 2, Figure 2) that describes a desired object state, the goal of our approach is \nto automat\u00adically synthesize a method sequence (such as the sequence shown in Figure 3) that produces \nthe desired object state. Challenges. Automatic synthesis of target sequences that produce a desired \nobject state, is challenging due to three major factors. First, target sequences often include methods \nfrom multiple classes, resulting in a large search space of candidate sequences. Second, target sequences \nrequire spe\u00adci.c primitive values that help exercise desired paths in the code under analysis. Third, \nobject-oriented programming features such as encapsulation pose additional challenges, since values cannot \nbe directly set to member .elds. Our Approach. To address these preceding challenges, we propose a novel \nsystematic approach, called Seeker, that in\u00adtelligently navigates the large search space via synergisti\u00adcally \ncombining static and dynamic analyses. We next ex\u00adplain why either pure dynamic or static analysis alone \ncan\u00adnot address this problem. A major issue with pure dynamic\u00adanalysis-based approach is that dynamic \nanalysis does not have the knowledge of methods that are not yet explored. Furthermore, it is not possible \nto explore all methods es\u00adpecially in real-world applications, since real-world appli\u00adcations often include \nclasses and methods from system li\u00adbraries such as .NET framework libraries. On the other hand, static \nanalysis alone cannot generate target sequences due to imprecision of static analysis. For example, consider \ngenerating the desired object state pro\u00adduced by the preceding Sequence S1 using static analysis alone. \nStatic analysis, being conservative, identi.es three methods (AddVertex, RemoveVertex, and ClearVertex \nof the AdjacencyGraph class) that modify the .eld vertices as can\u00addidates for Statement 2. For simplicity, \nwe did not show methods RemoveVertex and ClearVertex in Figure 1. Simi\u00adlarly, static analysis identi.es \nsix candidates for Statement 3, resulting in a total of 18 (3 * 6) candidate sequences. Among these candidates, \nstatic analysis alone cannot identify the tar\u00adget sequence due to its imprecision. Although dynamic anal\u00adysis \ncan be used to identify the target sequence among these candidates, the number of such candidate sequences \ncould be quite high in practice.  To address these aforementioned challenges, Seeker in\u00adcludes three \nsteps that form a feedback loop between dy\u00adnamic and static analyses. First, given a desired object state \nin the form of a conditional branch, dynamic analysis at\u00adtempts to generate a target sequence. Second, \nif dynamic analysis fails to generate the target sequence, static anal\u00adysis uses information for not-covered \nbranches from dy\u00adnamic analysis to generate candidate sequences. Third, dy\u00adnamic analysis explores and \neliminates statically generated sequences. For example, in Statement 2 of S1, Seeker stat\u00adically identi.es \nthree methods (AddVertex, RemoveVertex, and ClearVertex) as candidates. Seeker next uses dynamic analysis \nto .lter out RemoveVertex and ClearVertex meth\u00adods that do not help produce the desired object state. \nThis feedback loop between static and dynamic analyses is the key essence of Seeker and helps systematically \nexplore a po\u00adtentially large space of candidate sequences, thereby scaling Seeker to large real-world \napplications. Furthermore, Seeker stores the knowledge gained regarding individual method calls while \ngenerating a target sequence, and reuses the knowledge while generating other target sequences, thereby \nincreasing ef.ciency. To handle encapsulation in object-oriented programs, Seeker includes a novel technique \nbased on method-call graphs. A method-call graph is a directed graph that includes caller-callee relations \namong methods. This technique helps synthesize sequences that generate desired values for mem\u00adber .elds \n(both primitive and non-primitive) including pri\u00advate .elds. Evaluation. We developed a prototype based \non our Seeker approach for object-oriented test generation. We compared our approach with two state-of-the-art \ntest-generation ap\u00adproaches: Pex [41] and Randoop [35] that are represen\u00adtative of dynamic-symbolic-execution-based \nand random approaches, respectively. Our evaluation results show that Seeker achieves higher coverage \n(both structural and data\u00ad.ow) than Pex and Randoop. Achieving such higher cover\u00adage compared to Pex \nand Randoop is signi.cant, since the branches that are not covered by these approaches are gener\u00adally \nquite hard to cover. This paper makes the following major contributions: A novel approach, called Seeker, \nthat accepts a user\u00adspeci.ed intent in the form of a desired object state and automatically synthesizes \na method sequence that pro\u00adduces the desired object state. An application of our approach to automatically \ngenerate test inputs for object-oriented programs and a prototype implementation based on an existing \ntest-generation ap\u00adproach [41].  A technique based on method-call graphs to handle en\u00adcapsulation. Our \ntechnique also effectively handles pri\u00advate member .elds that are of non-primitive types.  Evaluation \nresults on four popular applications (totalling 28 KLOC) to show the effectiveness of Seeker approach. \nOur results show that Seeker achieves 12% (653 new branches) and 26% (1571 new branches) higher branch \ncoverage than Pex and Randoop, respectively. Our re\u00adsults also show that Seeker achieves 15.7% (428 pairs) \nand 15.3% (416 pairs) higher def-use coverage than Pex and Randoop, respectively. Seeker also detects \n34 new defects, including an in.nite loop defect in Quick-Graph [36].  The rest of the paper is organized \nas follows. Section 2 presents background on existing test-generation approaches. Section 3 explains \nour Seeker approach with illustrative ex\u00adamples. Section 4 presents formal de.nitions of terms used in \nthe paper. Section 5 presents the key algorithms of Seeker. Section 6 describes implementation details \nof the prototype developed for our approach. Section 7 presents the evalua\u00adtion results. Section 8 discusses \nthe limitations of our ap\u00adproach. Section 9 presents the related work. Finally, Sec\u00adtion 10 concludes. \n 2. Background In this section, we present two state-of-the-art test generation approaches Pex [41] \nand Randoop [35] that are used as repre\u00adsentative approaches for systematic and random approaches, respectively, \nin the rest of the paper. Pex. Pex [41] is a systematic approach based on a test\u00adgeneration technique, \ncalled Dynamic Symbolic Execution (DSE) [7, 11, 19, 22, 41]. DSE is a recent state-of-the-art test generation \ntechnique that explores an MUT and generates test inputs that can achieve high structural coverage of \nthe MUT. Pex, developed based on DSE, explores an MUT with default inputs. During exploration, Pex collects \nconstraints on inputs from the predicates in branch statements. Pex negates collected constraints and \nuses a constraint solver to generate new inputs that guide future program explorations along different \npaths. To generate method sequences, Pex uses a simple heuristic-based approach that generates .xed sequences \nbased on static information of constructors and other methods (of classes under test) that set values \nto mem\u00adber .elds, hopefully helping produce desired object states. Randoop. Randoop [35] is a random \napproach that gener\u00adates sequences incrementally by randomly selecting method calls. For each randomly \nselected method call, Randoop uses random values and previously generated sequences for primi\u00adtive and \nnon-primitive arguments, respectively. For each gen\u00aderated test input, Randoop avoids reusing or extending \nprevi\u00adously generated sequences that throw uncaught exceptions.  3. Example We next explain our approach \nusing the same illustrative ex\u00adamples shown in Figure 1. The .gure shows two classes un\u00adder test AdjacencyGraph \nand UDFSAlgorithm from the Quick-Graph library [36]. AdjacencyGraph represents a graph struc\u00adture including \nvertices and edges, which are added using AddVertex and AddEdge, respectively. UDFSAlgorithm per\u00adforms \nan undirected depth .rst search on the graph structure. We added an additional method IsComputed for \nillustrative purposes. Consider the foo method (Figure 2), where the user-speci.ed intent is expressed \nas an if condition (State\u00adment 2), describing the desired object state that isComputed should be true. \nHere, synthesizing a method sequence that produces the desired object state can be transformed as a testing \nproblem of generating a test input that covers the true branch (B6) of Statement 2. A necessary requirement \nto achieve the desired object is that the graph object in UDFSAlgorithm should contain both vertices \nand edges. We .rst present the branch coverage achieved by Pex and Randoop on classes shown in Figures \n1 and 2 and next de\u00adscribe our Seeker approach. The test inputs generated by Randoop and Pex achieved \nbranch coverage of 36.8% (21 out of 57) and 35.1% (20 out of 57), respectively. The rea\u00adson for low coverage \nis that neither Randoop nor Pex could satisfy the requirement of AddEdge to successfully add an edge \nto the graph object (Branch B3 in Statement 12 of Fig\u00adure 1). Therefore, neither Pex nor Randoop could \ngenerate a sequence that helps cover Branch B6 in Statement 2 (Fig\u00adure 2). As shown through this example, \nit is quite challeng\u00ading to achieve high branch coverage of these classes under test due to the requirement \nof complex sequences. Such re\u00adquirement is often encountered when testing object-oriented code. We next \npresent how our Seeker approach achieves high branch coverage by synthesizing sequences using a combi\u00adnation \nof dynamic and static analyses. In particular, Seeker leverages DSE for dynamic analysis and applies \nDSE to gen\u00aderate a target sequence. If DSE cannot generate the target se\u00adquence, Seeker statically analyzes \nthe branches that are not covered by DSE and synthesizes method sequences. Seeker next uses DSE with \nthe assistance of statically synthesized sequences. In our approach, we use Pex, which is based on DSE, \nfor dynamic analysis. Although we describe our ap\u00adproach in the context of Pex, our approach is independent \nof Pex and can be used to assist any other DSE-based ap\u00adproach [1]. Initially, Seeker applies DSE to \nexplore Branch B6 in the foo method. DSE explores foo but fails to generate a target sequence that covers \nBranch B6. Seeker statically analyzes B6 and suggests B4 (in the Compute method) as a pre-target branch \nthat could help cover B6. In particular, the sequence that helps cover B4 can be leveraged to cover B6 \nas well. Due to imprecision of static analysis, Seeker may suggest more than one pre\u00adtarget branches \nand not all those pre-target branches can help cover B6. To address this issue, Seeker applies DSE on \npre-target branches and .lters out irrelevant pre-target branches. Since DSE alone cannot cover B4, Seeker \nin turn uses static analysis to identify further pre-target branches for B4. This feedback loop eventually \nidenti.es the pre-target branches as follows: B6 . B4 . B3 . B2 . B1 . Here, the notation B6 . B4 indicates \nthat B4 is a pre-target branch for B6. Consider that Seeker successfully covered B3. In this scenario, \nSeeker generates the following sequence: 01: AdjacencyGraph ag = new AdjacencyGraph(); 02: Vertex v1 \n= new Vertex(0); 03: ag.AddVertex(v1); 04: ag.AddEdge(v1, v1); Seeker next uses this sequence to assist \nDSE for cover\u00ading the next target branch B4 and the process continues. Fig\u00adure 3 shows the .nal target \nsequence (generated by Seeker) that covers Branch B6. The sequence includes four classes and six method \ncalls. Using this sequence, Seeker achieved 84.2% (48 out of 57) branch coverage. The remaining not\u00adcovered \nbranches are related to the event handling mecha\u00adnism, which is currently not handled by our implemented \nprototype. It is quite challenging to generate such sequences either randomly or using heuristics, since \nthese three classes include 39 methods. However, the feedback loop between static analysis (that suggests \ncandidate methods) and dy\u00adnamic analysis (that identi.es correct candidate method and generates data) \ngenerates target sequences, thereby achiev\u00ading high structural coverage of the code under test.  4. \nProblem Formulation This section formalizes the problem of method sequence gen\u00aderation and introduces \nthe terminology that we use through\u00adout the rest of the paper. For a given application A under test, \nlet C and M denote its sets of classes and methods, respectively. Let PrimTy and PrimVal represent the \nset of all primitive types, such as int or bool, and primitive values, respectively. Each method M .M \nis represented by the method s type signature: C \u00d7 T1 \u00d7 \u00b7\u00b7\u00b7 \u00d7 Tn . T , where C .C is the type of the \nreceiver object, Ti .C . PrimTy denotes the type of the i\u00adth argument for i . [1..n], and T .C . PrimTy \n.{void}denotes the type of the return value. Since Ti .C. PrimTy, M s arguments can be either primitive \nvalues or objects. DEFINITION 4.1. Method Sequence (MCS). A method se\u00adquence is a sequence of method \ncalls (m1,...,mr), such that for i . [1..r],wehave  mi = o.Mi(a1,...,an) where M. Mi : C \u00d7 T1 \u00d7 \u00b7\u00b7\u00b7\u00d7 \nTn . T . In other words, mi is well-typed: o : C1 and aj : Tj for all j . [1..n]; o = ret(mk) for some \nk . [1..i), and for all j . [1..n], aj . PrimVal . aj = null . aj = ret(ml) for some l . [1..i). In other \nwords, the sequence is well-formed with the proper data dependence. In the preceding de.nition, ret(mk) \ndenotes the return value of the method mk. Also note that for brevity of presen\u00adtation, we do not explicitly \nmodel constructor calls and static methods in the preceding de.nition. To model them, one can simply \ndrop the conditions on the receiver object o. For each method call mi = o.Mi(a1,...,an) in an MCS, the \nreceiver object o should be the return object ret(mk) of another method call mk that precedes mi within \nthe se\u00adquence. Furthermore, each mi in the MCS can have either primitive or non-primitive arguments. \nFor primitive argu\u00adments, the preceding de.nition requires that the arguments should take on primitive \nvalues of the corresponding types, such as true for the bool type. For non-primitive arguments, they \nmust be null or return values of some preceding method calls within the sequence. For example, in the \nsample se\u00adquence shown in Figure 3, the non-primitive argument s1 of AddVertex in Statement 3 is the \nreturn value of another preceding method call new Vertex() in Statement 1. Our de.nition ensures that \nmethod sequences are well-formed, and executable code can be generated directly from those sequences. \nIt is helpful to also de.ne the notion of a sequence skele\u00adton. Intuitively, a sequence skeleton is an \nMCS except that primitive arguments are not required to take on concrete val\u00adues. The de.nition below \nprovides a precise description. DEFINITION 4.2. Skeleton (SKT). A sequence skeleton is a sequence of \nmethod calls (m1,...,mr), such that for i . [1..r],wehave mi = o.Mi(a1,...,an) where M. Mi : C \u00d7 T1 \u00d7 \n\u00b7\u00b7\u00b7\u00d7 Tn . T . o = ret(mk) for some k . [1..i), and for all j . [1..n], aj : PrimTy . aj . PrimVal . aj \n= null . aj = ret(ml) for some l . [1..i). The de.nition for SKT is essentially the same as that for \nMCS, except that some values of primitive-type argu\u00adments are not required: aj . PrimVal for MCS versus \naj : PrimTy . aj . PrimVal for SKT. DEFINITION 4.3. Target Branch (TB). A target branch is a true or \nfalse branch of a conditional statement2. In our setting of program synthesis, we use a not-covered target \nbranch to denote the user intent. 1 The notation o : C indicates that the receiver object o is of type \nC. 2 We model a switch statement as a series of if-then-else state\u00adments. DEFINITION 4.4. Method Sequence \nSynthesis. Given a method under test M .M and a target branch tb within M, synthesize a method sequence \n(m1,...,mr) that constructs the receiver object and arguments of M and drives M to successfully cover \ntb.  5. Seeker Algorithm Algorithms 1 and 2 show the two key algorithms DynAnalyzer (dynamic analysis) \nand StatAnalyzer (static analysis) of our Seeker approach. Seeker leverages DSE for dynamic analy\u00adsis \nto synthesize sequences that can cover a given target branch tb. In particular, given a target branch, \nSeeker .rst applies DSE and checks whether DSE can generate a se\u00adquence (referred to as target sequence) \nthat covers the target branch. In case, DSE cannot generate the target sequence, Seeker uses static analysis \nto synthesize skeletons and again applies DSE to generate data, forming a feedback loop. Here, DSE assists \nstatic analysis in two major ways. First, DSE helps generate data for skeletons synthesized by static \nanal\u00adysis. Second, DSE eliminates candidate methods (identi.ed by static analysis) that do not help cover \nthe target branch. The novelty of Seeker is that this feedback loop helps over\u00adcome individual limitations \nof static and dynamic analyses, thereby effectively synthesizing sequences3. We next explain each algorithm \nin detail using illustrative examples shown in Figures 1 and 2. Consider that the DynAnalyzer is invoked \nwith the target branch tb as B6 in Figure 2 and inpseq as null. Here, Branch B6 represents the true branch \nof State\u00adment 2 (Figure 2). 5.1 DynAnalyzer Algorithm DynAnalyzer accepts a target branch tb and an \ninput se\u00adquence inpseq as inputs, and generates a target sequence that covers tb. Initially, DynAnalyzer \nidenti.es the method m that includes tb (using GetMethod). DynAnalyzer next appends the method m to inpseq \nusing AppendMethod (Lines 1 and 2) and generates the skeleton tmpskt. Since AppendMethod does not know \nthe parameter values for m, AppendMethod uses symbolic values as parameters for m in the skeleton tmpskt. \nIf the method m is a non\u00adstatic method and there exists no constructor in inpseq, AppendMethod automatically \nadds relevant constructors to tmpskt.For the tb B6, when DynAnalyzer(B6, null) is invoked, AppendMethod \nreturns the skeleton foo(<sym>), where <sym> represents a symbolic variable. Here, no con\u00adstructor is \nadded to tmpskt, since foo is a static method. DynAnalyzer next applies DSE (referred to with a func\u00adtion \ncall DSE in Line 3 of Algorithm 1) to explore tmpskt for generating a target sequence that covers tb. \nDSE accepts 3 An astute reader can identify that, in a few scenarios, the recursion be\u00adtween our two \nalgorithms can result in an in.nite loop. Seeker includes techniques for detecting and avoiding such \nin.nite loops. For brevity, we ignore such details while presenting our algorithms.  two arguments of \ntypes SKT and TB as inputs. DSE outputs three values described as follows: targetseq of type MCS: MCS \nthat covers the given tb of type TB or null  CovB: Set of covered branches  NotCovB: Set of not covered \nbranches  During exploration of DSE, if DSE happens to gener\u00adate a target sequence, then DSE returns \nthe target sequence; otherwise, it returns null. Apart from the target sequence, DSE also returns covered \n(CovB) and not-covered branches (NotCovB) in the method m. For example, when DSE is in\u00advoked with the \nskeleton foo(<sym>), DSE generates a se\u00adquence that helps cover Branch B7, but not Branch B6. The reason \nis that DSE could not generate a target sequence that can help cover B6. Therefore, DSE returns null, \n{B7}, and {B6} for targetseq, CovB, and NotCovB, respectively. Note that the set CovB . NotCovB does \nnot represent the entire set of branches in the method m. The primary reason is that DSE, being a pure \ndynamic analysis technique, does not have the knowledge of those branches where both the branch and its \nalternative branch4 are not explored by DSE. After exploration using DSE, there can be three possible \nscenarios for the target branch. Scenario 1: The target branch tb is covered. In this sce\u00adnario, DynAnalyzer \nreturns targetseq.  Scenario 2: The target branch tb is not covered and tb . NotCovB. This scenario \nhappens when DSE suc\u00adcessfully covers the alternative branch of tb and could not cover tb. In this scenario, \nDynAnalyzer invokes StatAnalyzer to generate a sequence that can help cover tb.  Scenario 3: The target \nbranch tb is not covered and tb /. NotCovB. This scenario happens when DSE could not cover all the dominant \nbranches of tb in the method m. In this scenario, DynAnalyzer invokes ComputeDominants to identify dominant \nbranches. In particular, Compute Dominants .rst identi.es the dominant branch, referred to as prime dominant, \nwhose alternative branch is cov\u00adered by DSE. ComputeDominants next identi.es all other dominant branches \nof tb between the prime dominant and tb. DynAnalyzer next recursively invokes itself for each such dominant \nbranch starting from the prime dominant branch. DynAnalyzer returns a method sequence if all dominant \nbranches are covered along with tb; otherwise, it returns null.   5.2 StatAnalyzer Algorithm StatAnalyzer \nanalyzes a target branch tb and identi.es other branches (referred to as pre-target branches) that can \n4 Given a branch b (such as the true branch) of a conditional statement, we use alternative branch to \nrefer to the other branch (such as the false branch) of that conditional statement. Algorithm 1 DynAnalyzer(tb, \ninpseq) Require: tb of type TB Require: inpseq of type MCS Ensure: targetseq of type MCS covering tb \nor null 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: \n27: Method m = GetMethod(tb) SKT tmpskt = AppendMethod(inpseq, m) DSE(tmpskt, tb, out targetseq, out \nCovB, out NotCovB) //Scenario 1  if tb . CovB then return targetseq end if //Scenario 2 if tb . NotCovB \nthen return StatAnalyzer(tb, inpseq) end if //Scenario 3 if tb /. NotCovB then List<TB> tblist = ComputeDominants(tb) \nfor all TB domtb . tblist do inpseq = DynAnalyzer(domtb, inpseq) if inpseq == null then Break end if \nend for if inpseq = null then return DynAnalyzer(tb, inpseq)  end if end if return null help cover \ntb. We .rst explain the two major functions DetectField (Line 1) and SuggestTargets (Line 2) used by \nStatAnalyzer. Since examples shown in Figures 1 and 2 are complex, we use a simple example shown in Figure \n4 to explain DetectField and SuggestTargets. Consider that StatAnalyzer is invoked with tb as Branch \nB8 and inpseq as null. DetectField. Given a tb, the function DetectField pre\u00adcisely identi.es the target \nmember .eld tfield that needs to be modi.ed to produce a desired object state for cov\u00adering tb. It is \ntrivial to identify tfield for branches such as if(stack.size == 10), where tfield (such as size) is \ndirectly included in the branch. However, in object\u00adoriented code, branches often involve method calls \nsuch as if(!vertices.Contains(v1)) in Statement 7 (Figure 1) rather than .elds. It is challenging to \nidentify target .elds in the presence of method calls, since the return statements in these method calls \nmay in turn can include further method calls, where the actual member .eld is returned.  Algorithm 2 \nStatAnalyzer(tb, inpseq) Require: A target branch tb Require: A sequence inpseq Ensure: A sequence targetseq \ncovering tb 1: Field tfield = DetectField(tb) 2: List<TB> tblist = SuggestTargets(tfield) 3: for all \nTB pretb . tblist do 4: MCS targetseq = DynAnalyzer(pretb, inpseq) 5: if targetseq = null then 6: targetseq \n= DynAnalyzer(tb, targetseq) 7: if targetseq = null then 8: return targetseq 9: end if  10: end if 11: \n//Try other alternative target branches 12: end for  13: return null To address this issue, the function \nDetectField uses an inter-procedural execution trace (hereby referred to as trace), gathered during the \nruntime exploration with DSE. This trace includes the statements executed in each method. DetectField \nperforms backward analysis of the trace start\u00ading from the method call involved in tb.Weuse retvar to \nrefer to the variable or value associated with the return state\u00adment in a method call. DetectField uses \nthe following .ve steps with respect to retvar to identify tfield. 1. If retvar is a member .eld, DetectField \nidenti.es retvar as tfield. This scenario can happen with methods such as getter methods. 2. If retvar \nis data-dependent on a member .eld, the func\u00adtion DetectField identi.es that member .eld as tfield. \n3. If retvar is data-dependent on the return of a nested method call, the function DetectField repeats \nthese .ve steps with the nested method call to identify tfield. 4. If retvar is control-dependent on \na member .eld, Detect Field identi.es that member .eld as tfield. This sce\u00adnario can happen when DSE \nfailed to generate other ob\u00adject states for that member .eld. 5. If retvar is control-dependent on the \nreturn of a nested method call, DetectField repeats these .ve steps with that nested method call to identify \ntfield. The method HasElements (Lines 6-9 in Figure 4) shows an example of this scenario, where retvar \nis control-dependent on the return of another nested method call stack.size(). In this scenario, DetectField \nrepeats the preceding .ve scenarios with that method call stack.size().  To illustrate these .ve steps, \nconsider Branch B8 as tb. Given this tb, DetectField applies the preceding steps and detects size (in \nArrayList)as tfield. Here, Stack in\u00adcludes a member .eld list of type ArrayList. Initially, 00: public \nclass IntStack { 01: private Stack stack; 02: public IntStack() { 03: this.stack = new Stack; } 04: public \nvoid Push(int item) { 05: stack.Push(item); } 06: public bool HasElements() { 07: if(stack.size() > 0) \n{ return true; } 08: else { return false; } 09: } 10: } 11: public class MyCls { 12: private IntStack \nints; 13: public MyCls(IntStack ints) { 14: this.ints = ints; } 15: public void MyFoo() { 16: if(ints.HasElements()) \n{ 17: ...// B8 18: } 19: } 20: } Figure 4. An integer stack class. DetectField analyzes the method IntStack.HasElements. \nSince the executed return statement (Statement 8) is control\u00addependent on a nested method call Stack.size, \nDetectField analyzes the Stack.size method. Eventually, DetectField reaches the getter method that returns \nsize member .eld of ArrayList, and thereby identi.es size as tfield. Note that, in a few scenarios, there \ncan be multiple tfields for cover\u00ading tb. However, currently we handle only those tb that can be covered \nby achieving desired value for a single tfield. We plan to handle multiple tfields in our future work. \nAlong with identifying tfield, DetectField also cap\u00adtures two other pieces of information. First, DetectField \nidenti.es the condition on tfield that is not satis.ed. For example, DetectField identi.es size > 0 \n(Statement 7) as the condition that should be satis.ed to cover tb. DetectField applies a constraint \nsolver on the preced\u00ading condition to get a desired value for tfield. Second, DetectField also captures \nthe hierarchy of .elds, referred to as .eld hierarchy, that includes all objects starting from the object \nenclosing tb to tfield. For Branch B8 as tb,the identi.ed .eld hierarchy is as follows: FH: MyCls root \n. IntStack is . Stack stack . ArrayList list . int size . This .eld hierarchy describes that size of \ntype int is con\u00adtained in the object list of type ArrayList, which is in turn contained in the object \nstack of type Stack and so on. Here, root represents the object of type MyCls. This .eld hierarchy is \nused by SuggestTargets discussed next. In our setting, in\u00adheritance is not an issue, since DetectField \nanalyzes the execution trace produced by dynamic analysis, where actual objects are described.  SuggestTargets. \nGiven a target .eld tfield such as size, its current and desired values, and .eld hierarchy, the func\u00adtion \nSuggestTargets identi.es pre-target branches that need to be covered to cover the original target branch \ntb. Initially, SuggestTargets traverses the .eld hierarchy and identi.es the object tobject (in the .eld \nhierarchy) that can be mod\u00adi.ed to achieve a desired value for tfield. The objective of this traversal \nis to identify the tobject that is nearest to tfield and can be modi.ed by either assigning a value directly \nor by invoking its public methods. The reason is that the shorter the distance between tobject and tfield, \nthe smaller the amount of code that needs to be explored to achieve a desired value for tfield. For example, \nconsider the preceding .eld hierarchy FH. Here, the object ArrayList list is near size (tfield) compared \nto IntStack stack.How\u00adever, the list object cannot be tobject, since list, a private member .eld, cannot \nbe modi.ed directly or by invoking its public methods. To identify tobject, SuggestTargets traverses \nthe .eld hierarchy from root and considers the object whose next object cannot be modi.ed either directly \nor through public methods as tobject. For example, root is not considered as tobject, since ints can \nbe modi.ed as ints is set through the constructor. For this .eld hierarchy, SuggestTargets identi.es \nints as tobject, since stack cannot be modi.ed outside the ints object. After identifying tobject, SuggestTargets \nidenti.es meth\u00adods (and pre-target branches within those methods) that help produce a desired value for \ntfield. Identifying the meth\u00adods of tobject that modify tfield is non-trivial, since there can be intermediate \nobjects between tobject and tfield, as identi.ed by the .eld hierarchy. To address this issue, SuggestTargets \nuses a novel technique based on method\u00adcall graphs. DEFINITION 5.1. Method-Call Graph. A method-call \ngraph is a strati.ed-directed graph G =(V, E), where V and E represent the set of all nodes and edges, \nrespectively, such that V = {root}. V1 . ... . Vn. Here, root represents the node corresponding to t.eld, \nand for all i . [1..n], Vi = {M1,...,Mr} represents all nodes at level i. For all j . [1..r], Mj .M and \nall methods Mj in Vi belongs to the same class or its parent classes;  E = {(root, Ma) | Ma . V1}. {(Mb,Mc) \n| Mb . Vi . Mc . Vi+1}.In  i.[1...n-1] essence, there can be two kinds of edges. The .rst kind of edges \nrepresent the edges from root to nodes at level 1. An edge between root and Ma . V1 (nodes at level 1) \nrepresent that Ma modi.es tfield. The second type of edges represent the edges between the nodes in any \ntwo successive levels. An edge between Mb . Vi and Mc . Vi+1 represent that Mc invokes Mb. Figure 5. \nA sample method-call graph. SuggestTargets constructs the method-call graph on de\u00admand based on the .eld \nhierarchy identi.ed by DetectField. Figure 5 shows a sample method-call graph constructed for the .eld \nhierarchy FH. The root node of the graph in\u00adcludes tfield. The .rst level of the graph includes the meth\u00adods \n(in the declaring class) that modi.es tfield. Initially, SuggestTargets statically analyzes all public \nmethods of the declaring class of tfield to identify the target methods that modify tfield. In particular, \nSuggestTargets identi.es assignment statements, where tfield is on the left hand side. For example, SuggestTargets \nidenti.es the methods such as Add, Insert, and Reset of ArrayList as target methods for the tfield size. \nFrom the second level, the graph includes the methods from the declaring classes of .elds in the .eld \nhierarchy. The graph includes an edge from a method Mb in one level to a method Mc in the next level, \nif Mb is called by Mc. For example, Stack.Push invokes the List.Add method and the corresponding edge \nis shown from Levels L1 to L2. SuggestTargets next traverses constructed graph from the top to bottom \nto identify methods that can be invoked on tobject to achieve a desired value for tfield. Furthermore, \nSuggestTargets identi.es pre-target branches within each method that need to be covered to invoke the \nmethod call of the preceding level. For example, SuggestTargets identi.es that the IntStack.Push method \ncan help achieve a desired value for tfield. Furthermore, SuggestTargets identi.es the pre-target branch \nin IntStack.Push that helps invoke Stack.Push method. This pre-target branch is considered as a new target \nbranch that needs to be covered, so as to cover the original target branch tb. SuggestTargets returns \nseveral candidate pre-target branches. StatAnalyzer. We next describe the StatAnalyzer algo\u00adrithm. Given \na target branch tb, StatAnalyzer .rst iden\u00adti.es other pre-target branches that need to be covered using \nDetectField and SuggestTargets. Due to impreci\u00adsion of static analysis, not every pre-target branch identi\u00ad.ed \nby SuggestTargets can help cover tb. For example, along with IntStack.Push, SuggestTargets can also iden\u00adtify \nthat IntStack.Pop can help cover tb. To address this issue, StatAnalyzer invokes DynAnalyzer to generate \na se\u00adquence that covers these pre-target branches. If DynAnalyzer successfully covers any pre-target \nbranch, StatAnalyzer  07: DynAnalyzer(B1, null) 06: StatAnalyzer(B2, null) 05: DynAnalyzer(B2, null) \n04: DynAnalyzer(B3, null) 03: StatAnalyzer(B4, null) 02: DynAnalyzer(B4, null) 01: StatAnalyzer(B6, null) \n00: DynAnalyzer(B6, null) Figure 6. A snapshot of the execution stack. Subject Version # Classes # Methods \nKLOC # Downloads QuickGraph 1.0 88 634 5.1 62,734 Dsa 0.6 27 308 3.3 6,724 XUnit 1.6.1 151 1267 11.9 \n86,702 NUnit 2.5.7 225 2344 8.1 193,563 TOTAL 491 4553 28.4 349,723 Table 1. Subjects and their characteristics. \nuses the generated sequence to cover the original target branch tb. 5.3 Example We next describe how \nSeeker generates the sequence shown in Figure 3 for the tb B6 in Figure 2. Initially, Seeker in\u00advokes \nDynAnalyzer(B6, null). Since B6 . NotCovB after exploration with DSE, DynAnalyzer(B6, null) in\u00advokes \nStatAnalyzer(B6, null). StatAnalyzer analyzes B6 and identi.es B4 in the Compute method as a pre\u00adtarget \nbranch. Therefore, StatAnalyzer(B6, null) invokes DynAnalyzer(B4, null). This process continues and even\u00adtually \nreaches a stage where StatAnalyzer(B2, null) in\u00advokes DynAnalyzer(B1, null). Figure 6 shows the contents \nof the execution stack when DynAnalyzer(B1, null) is in\u00advoked. At this point, DynAnalyzer(B1, null) covers \nBranch B1, and returns the following sequence, say S2: 01: Vertex s1 = new Vertex(0); 02: AdjacencyGraph \nag = new AdjacencyGraph(); 03: ag.AddVertex(s1); StatAnalyzer(B2, null) next invokes DynAnalyzer(B2, \nS2), which successfully generates a sequence that covers B2, resulting in the following sequence, say \nS3: 01: Vertex s1 = new Vertex(0); 02: AdjacencyGraph ag = new AdjacencyGraph(); 03: ag.AddVertex(s1); \n04: ag.AddEdge(s1, null); Seeker continues further and eventually covers the orig\u00adinal target branch \nB6, thereby synthesizing the method se\u00adquence shown in Figure 3.   6. Implementation We implemented \na prototype of Seeker using Pex 0.92 [41] for dynamic analysis. Pex provides extensible Application Programming \nInterfaces (APIs) that can be overridden to add new functionalities. Given a tb, Seeker launches Pex \nmultiple times to synthesize target sequences. Seeker uses a .le-based repository to cache information \nacross multiple launches of Pex. After each launch, Pex returns branches that are not yet covered due \nto lack of desired method sequences. Seeker analyzes those not-yet-covered branches statically and generates \nskeletons. Seeker persists these generated skeletons in the repository and reuses them during the next \nlaunch of Pex. Seeker re\u00adlaunches Pex with new skeletons to generate primitive data for the method calls \nin the skeletons. Due to these multiple launches of Pex and using a .le-based repository, our current \nprototype has high runtime overhead. We expect that the run\u00adtime performance of our prototype can be \nsigni.cantly im\u00adproved by implementing Seeker within Pex or by adopting a memory-based repository, which \nis left as our immediate future work. Our DynAnalyzer algorithm assumes that DSE (shown in Step 3) could \nnot cover the target branch tb due to the lack of proper sequence. However, in practice, there can be \nother issues such as environment dependency due to which DSE could not cover tb. Seeker identi.es such \nother issues and returns from Step 3 of DynAnalyzer. Our current open\u00adsource prototype can be downloaded \nfrom http://pexase. codeplex.com/releases/view/50822.  7. Evaluation To show the effectiveness of our \nSeeker approach, we ap\u00adplied Seeker to automatically generate test inputs for object\u00adoriented programs. \nWe also compared our approach with two categories of approaches: random and DSE-based ap\u00adproaches. We \nused two state-of-the-art tools Randoop [35] and Pex [41] as representative tools for random and DSE\u00adbased \napproaches, respectively. We applied all three ap\u00adproaches on four popular real-world applications. Along \nwith these two approaches, we compared our results with the results of existing manually written tests \navailable with the subject applications. We also compared Seeker with our previous approach, called MSeqGen \n[40]. MSeqGen, unlike Seeker, generates method sequences based on how method calls are used in practice. \nSince Seeker and MSeqGen com\u00adplement each other, this comparison helps show the bene.ts and limitations \nof Seeker, MSeqGen, and their combined ap\u00adproach. More details of the subjects and results of our evalu\u00adations \nare available at http://research.csc.ncsu.edu/ ase/projects/seeker/. All evaluations were conducted on \na machine with 3.33GHz Intel Core 2 Duo processor with 4GBRAM.  Subject Namespace # Branches Randoop \nPex Seeker Manual #Tests Cov Time #Tests Cov Time #Tests Cov Time #Tests Cov QuickGraph OVERALL 1119 \n10140 51.2 0.2 334 31.6 4.4 1923 68.2 3.2 21 26 Algorithms 572 - 38.1 - - 24.8 - - 52.1 - - 24.8 Collections \n269 - 87.7 - - 17.8 - - 94.0 - - 11.2 ... (5 more) Dsa OVERALL 665 10493 14.9 1.0 552 83.8 3.7 961 90 \n0.9 298 93.2 Algorithms 198 - 41.9 - - 100 - - 100 - - 88.3 DataStructures 433 - 0 - - 76.7 - - 86.4 \n- - 90.8 ... (2 more) xUnit OVERALL 2379 10148 24.9 6.1 1265 38.6 4.5 1360 41.1 2.0 282 62.7 Gui 432 \n- 34.3 - - 40.8 - - 46.1 - - 17.8 Sdk 706 - 25.1 - - 35.6 - - 40.2 - - 86.3 ... (6 more) NUnit Util 1810 \n10129 16.1 1.7 816 35.3 7.5 1804 43.5 3.7 319 63.9 TOTAL 5973 40910 26 9.0 2967 41.3 20.1 6048 52.3 9.8 \n920 59.2 Table 2. Branch coverage achieved by Randoop, Pex, Seeker, and manually written tests. 7.1 \nResearch Questions In our evaluation, we addressed the following research ques\u00adtions. RQ1: How much higher \npercentage of branch coverage is achieved by Seeker compared to Randoop and Pex, respec\u00adtively? This \nresearch question helps show that Seeker per\u00adforms better than Randoop and Pex in achieving high struc\u00adtural \ncoverage such as branch coverage of the code under test. RQ2: How much higher percentage of def-use coverage \nis achieved by Seeker compared to Randoop and Pex, respec\u00adtively? This research question helps show that \nSeeker per\u00adforms better than Randoop and Pex in achieving high data\u00ad.ow coverage [10] such as def-use \ncoverage of the code un\u00adder test. RQ3: How many new defects are detected by Seeker com\u00adpared to Randoop \nand Pex, respectively? This research question helps address whether Seeker has higher defect\u00addetection \ncapabilities compared to Randoop and Pex, respec\u00adtively. RQ4: How high percentage of branch coverage \nis achieved by Seeker, MSeqGen [40], and their combination? 7.2 Subjects We used four popular applications \nas subjects in our evalu\u00adations. Table 1 shows their various characteristics, such as the number of classes \nand methods, their site, and number of downloads. QuickGraph [36] is a popular C# graph li\u00adbrary that \nprovides various graph data structures and algo\u00adrithms such as depth-.rst search. Data structures and \nalgo\u00adrithms (Dsa)5 provides various data structures, complement\u00ad 5 http://dsa.codeplex.com/ ing those \nfrom the .NET framework. xUnit6 and NUnit7 are widely used open source unit testing frameworks for all \n.NET languages. For NUnit, we focused on applying all three approaches on its core component, the util \nnamespace (including 8.1 KLOC). We used these applications as subject applications, since these applications \nare popularly used (as shown by their total downloads count as of March 2011 in Column Downloads ) and \nalso by previous work [40]. The subjects include a total of 28 KLOC.  7.3 Evaluation Setup We next describe \nour evaluation setup for addressing the preceding four research questions. Seeker and Pex accept Parameterized \nUnit Tests (PUTs) [42] as input and generate conventional unit tests. Unlike conventional unit tests, \nPUTs accept parameters. Since PUTs are not available with our subjects, we automatically generated PUTs \nfor each public method by using the PexWizard tool, which is provided with Pex. A PUT generated for the \nCompute method in Figure 1 using PexWizard is shown below. 00:[PexMethod] 01:public void Compute01( \n02: [PexAssumeUnderTest]UDFAlgorithm target, 03: [PexAssumeUnderTest]IVertex s) { 04: target.Compute(s); \n05: Assert.Inconclusive(\"this test needs review\"); 06:} We .rst applied Seeker on PUTs generated for \neach sub\u00adject application. We measured four metrics for generated test inputs: the branch coverage, def-use \ncoverage, num\u00adber of distinct defects detected, and the time taken. For branch coverage, we used a coverage \nmeasurement tool, 6 http://xunit.codeplex.com/ 7 http://www.nunit.org/  called NCover8, to measure the \nbranch coverage achieved by generated test inputs. For def-use coverage, we developed a tool for C#, \ncalled DUCover, based on the techniques described in previous work [31, 34]. The reason for developing \nthis new tool is that, to the best of our knowledge, there exist no def-use cov\u00aderage measurement tool \nfor C#. In object-oriented code, def\u00adinitions and uses for member .elds can occur in different member \nmethods of classes under analysis. DUCover auto\u00admatically measures coverage of such def-use pairs based \non method sequences among generated test inputs. For defects, we measured distinct defects, since multiple \nfailing test in\u00adputs could detect the same defect. Section 7.6 presents more details on how we identify \ndefects from failing test inputs. As mentioned in Section 6, our implementation has run\u00adtime performance \noverhead, since we launch Pex multiple times. To ensure that our results are not biased by the limi\u00adtations \nof our implementation, we used customized settings for Pex and Randoop rather than using their default \nsettings, respectively. These customized settings allow Pex and Ran\u00addoop to run for the same or higher \namount of time compared to Seeker. In essence, our settings favor Pex and Randoop compared to Seeker. \nWe used the following customized set\u00adtings for Pex and measured the three metrics for the test in\u00adputs \ngenerated by Pex. Timeout = 500 sec. (default:120) MaxConstraintSolverTime = 10 sec. (default:2) MaxRunsWithoutNewTests \n= 2147483647 (default:100) MaxRuns = 2147483647 (default:100) The values in brackets represent the default \nvalues. For example, the default value of the timeout parameter is 120 seconds. Instead, we used 500 \nseconds for the timeout pa\u00adrameter. For Randoop, the default timeout value is 120 sec\u00adonds. Since, Randoop \nis a random approach, we ran Ran\u00addoop multiple times (each time with the timeout parameter as 120 seconds) \nfor each subject so that the total time is equal or higher than the amount of time taken by Seeker for \nthat subject. However, we observed that Randoop may generate thousands of test inputs that are too many \nto be compiled within Visual Studio for measuring metric values. Therefore, we limited the number of \ngenerated test inputs to 10, 000. For one subject under analysis, although we tried compil\u00ading remaining \ntest inputs into several other Visual Studio projects and measured coverage, we found that there was \nno increase in the branch coverage. To compare Seeker with MSeqGen, we .rst applied MSe\u00adqGen alone on \nQuickGraph, which is the only subject pre\u00adviously used for evaluating MSeqGen (integrated with Pex), \nand measured the three metrics9. For the combined approach, 8 http://www.ncover.com/ 9 We could not apply \nMSeqGen on other subject applications due to lack of usage information currently with us for these subject \napplications. In future, we plan to collect this usage information and compare Seeker with MSeqGen on \nthe remaining subjects as well. Subject Randoop Pex Seeker Avg. SD Max Avg. SD Max Avg. SD Max QuickGraph \n21.6 21.6 191 4.4 3.0 14 5.6 2.9 17 Dsa 3.0 2.5 20 2.7 2.0 12 3.2 1.9 12 xUnit 6.1 5.8 65 3.3 4.9 58 \n2.4 2.0 37 NUnit 4.7 5.0 121 4.1 3.0 20 4.3 2.9 19 Table 3. Statistics of generated sequences. we used \nsequences extracted by MSeqGen as input to Seeker. In this setting, Seeker enhances the sequences extracted \nby MSeqGen to generate more sequences that could help pro\u00adduce desired object states.  7.4 RQ1: Coverage \nWe next address the .rst research question. Table 2 shows our results for all subject applications. For \neach subject, due to space constraint, we show results for a few selected names\u00adpaces (Column Namespace \n) that help provide insights de\u00adscribed in subsequent sections, instead of all namespaces. Column Branches \nshows the number of branches in each application. Among the remaining columns, subcolumns # Tests , Cov \n, and Time show the number of test inputs generated by each approach (Randoop, Pex, Seeker, and manually \nwritten tests), branch coverage achieved, and time taken in hours, respectively. Table 3 shows further \ndetails re\u00adgarding the sequences generated by each approach. Columns Avg. , SD , and Max show the average \nlengths, standard deviation, and maximum lengths of sequences generated by each approach, respectively. \nWe next summarize our results. Randoop. Our results show that Randoop achieved the lowest coverage among \nall approaches for all applications, except for Quickgraph. For QuickGraph, Randoop achieved higher coverage \nthan Pex. Randoop could not achieve any coverage for the DataStructures namespace of Dsa, since Randoop \ncannot handle generics. Furthermore, Table 3 shows that sequences generated by Randoop are often longer \nthan the sequences generated by other approaches. In sum\u00admary, our results show that target sequences \ncannot be gen\u00aderated by combining method calls randomly to form longer sequences. Pex and Seeker. Our \nresults show that Pex, which is a DSE\u00adbased approach, can effectively handle generation of primi\u00adtive \ndata, but cannot generate target sequences. For exam\u00adple, Pex achieved 100% coverage for the algorithms \nnames\u00adpace of Dsa. This namespace does not require sequences and includes implementations of various \nsorting algorithms such as mergesort. On the other hand, Pex achieved only 31.6% for QuickGraph, which \nrequires complex sequences for achieving high coverage. In our evaluations, we used cus\u00adtomized settings \nfor Pex instead of default values, thereby favoring Pex compared to Seeker. For example, our settings \nhelp Pex run for 20 hours (for all subjects) compared to  Subject # Def-Use pairs Randoop Pex Seeker \nManual # Covered % # Covered % # Covered % # Covered % QuickGraph 892 402 45.1 198 22.2 447 50.1 152 \n17.0 Dsa 583 0 0 96 16.5 222 38.1 185 31.7 xUnit 1256 196 15.6 316 25.2 357 28.4 24 1.9 TOTAL 2731 598 \n21.9 610 22.3 1026 37.6 361 13.2 Table 4. Def-Use coverage achieved by Randoop, Pex, Seeker, and manually \nwritten tests. 9.8 hours for Seeker. Still, Seeker achieved 12% (653 new branches) higher branch coverage \nthan Pex. Indeed, allow\u00ading Seeker to run for longer time could help achieve more coverage. Therefore, \nour results show that it is dif.cult to achieve higher coverage by letting Pex run for longer time, showing \nthe signi.cance of our Seeker approach. Although Seeker achieved higher coverage than Randoop and Pex, \nthe coverage achieved is still not close to 100%. Moreover, coverage achieved by Seeker is lower than \nmanu\u00adally written tests for all subjects, except for QuickGraph and Gui namespace of xUnit. Section 8 \ndiscusses limitations on why Seeker could not achieve coverage close to 100%.  7.5 RQ2: Def-Use Coverage \nWe next address the second research question on whether Seeker achieves higher def-use coverage compared \nto Pex and Randoop. Table 4 shows the def-use coverage achieved by Randoop, Pex, Seeker, and manually \nwritten tests, respec\u00adtively. We could not apply our DUCover tool on test inputs generated for NUnit, \ndue to a technical limitation of execut\u00ading NUnit tests using NUnit. Along with def-use coverage, we \nalso measure all-defs coverage to provide more insights. All-defs criteria describe that for each de.nition \nin the code under test, some use of this de.nition is being exercised by a test input. Table 5 shows \nthe all-defs coverage achieved by all approaches for each subject. Our results show that Seeker achieved \nhigher def-use and all-defs coverage compared to both Pex and Randoop, re\u00adspectively, for all subjects. \nThe results also show that Seeker achieved higher def-use coverage than manually written tests. A primary \nreason could be that programmers may not write tests to achieve high def-use coverage. Although Seeker \nachieved higher def-use coverage than Pex and Ran\u00addoop, the coverage achieved by Seeker is not close \nto 100%. There are two major reasons. First, some of the def-use pairs are infeasible. For example, consider \nthe Deque class shown in Figure 7. In this class, the m deque .eld is de.ned in State\u00adment 7 in the Clear \nmethod. On the other hand, the m deque .eld is accessed in Statement 12 in the DequeFront method, forming \na def-use pair. However, this def-use pair is an infea\u00adsible pair, since the Clear method sets the value \nof the Count .eld to zero (Statement 8) and the DequeFront method in\u00adcludes an additional condition check \n(in Statement 11) that throws an exception if the value of the Count .eld is zero. In future work, we \nplan to identify such infeasible pairs by 00:public class Deque<T> { 01: private DoublyLinkedList<T> \nm deque; 02: ... 03: public override void Add(T item) { 04: EnqueueBack(item); 05: } 06: public override \nvoid Clear() { 07: m deque.Clear(); 08: Count = 0; 09: } 10: public override T DequeFront() { 11: Guard.InvalidOperation(Count \n== 0, Resources.DequeDequeueEmpty); 12: T item = m deque.Head.Value; 13: m deque.RemoveFirst(); 14: Count--; \n15: return item; 16: } 17: ... 18:} Figure 7. The Deque class from Dsa. constructing inter-procedural \ncontrol-.ow graphs and by us\u00ading constraint solving to detect infeasible paths. Detecting such inter-procedural \ninfeasible paths helps detect infeasible def-use pairs. Second, Seeker, which is developed around Pex, \nis primarily intended for achieving higher branch cov\u00aderage rather than def-use coverage. In future work, \nwe plan to develop a new search strategy for Seeker that guides Pex to achieve higher def-use coverage \nalong with higher branch coverage.  7.6 RQ3: Defects We next address the third research question regarding \ncom\u00adparing defect-detection capabilities of Randoop, Pex, and Seeker. Table 6 shows our results. Subcolumns \nAT , FT , and D show the total number of generated test inputs, num\u00adber of failing test inputs, and number \nof distinct defects de\u00adtected, respectively, by each approach. For Randoop, due to the large number of \nfailing test inputs, we regenerated test in\u00adputs with its default parameters, instead of analyzing all \ntest inputs generated with the setting described in Section 7.4. Furthermore, all our test inputs are \nautomatically generated and do not include test oracles. Therefore, we used uncaught exceptions as test \noracles with focus on robustness issues.  Subject # All Defs Randoop Pex Seeker Manual # Covered % # \nCovered % # Covered % # Covered % QuickGraph 136 97 71.3 65 47.8 109 80.1 31 22.8 Dsa 112 0 0 34 30.3 \n59 52.7 59 52.7 xUnit 922 97 10.5 144 15.6 156 16.9 13 1.4 TOTAL 1170 194 16.6 243 20.8 324 27.7 103 \n8.8 Table 5. All defs coverage achieved by Randoop, Pex, Seeker, and manually written tests. Subject \nRandoop Pex Seeker AT FT D AT FT D AT FT D QuickGraph 6956 456 10 334 14 11 1923 117 34 Dsa 687 17 3 \n552 34 15 961 61 20 xUnit 112 0 0 1265 12 5 1360 12 5 NUnit 528 76 3 816 10 7 1804 16 13 Total 8283 549 \n11 2967 70 38 6048 206 72 AT: All Tests, FT: Failing Tests, D: Defects Table 6. Defects detected by \nall approaches. In particular, we considered the test inputs that throw ex\u00adceptions as failing test inputs. \nHowever, we considered the failing test inputs that throw expected exceptions as passing test inputs. \nFurthermore, we ignored the defects related to NullReferenceExceptions that are thrown by passing null \nvalues to arguments of public methods. The primary reason is that often open source applications do not \ncheck null val\u00adues for the arguments of public methods, and can also be .xed by automatically adding \na null check on arguments of all public methods. To classify a failing test as a defect or ex\u00adpected \nexception, we inspected the source code of subjects under analysis and its associated Javadocs and comments. \nSince manually written tests of these subjects do not include any failing tests, we consider all defects \ndetected by Ran\u00addoop, Pex, and Seeker as new defects. Our results show that Randoop, Pex, and Seeker \nde\u00adtected 11, 38, and 72 distinct defects, respectively. We re\u00adported detected defects on hosting websites \nof our sub\u00adject applications. In all subjects, defects detected by Ran\u00addoop are related to NullReferenceExceptions. \nSimilarly, except for Dsa, all defects detected by Pex are also re\u00adlated to NullReferenceExceptions. \nIn Dsa, Pex detected two and .ve defects related to OverflowException and IndexOutOfRange exceptions, \nrespectively. Seeker detected all defects detected by Randoop and Pex, and also de\u00adtected new defects \nrelated to InvalidOperationException in QuickGraph. This exception is thrown when an attempt to modify \na collection is made after an enumerator is created on that collection. It requires speci.c method sequences \nto cause this exception. Furthermore, Seeker detected a defect related to an in.nite loop in QuickGraph. \nFigure 8 shows the test input that detected the in.nite loop. The test input includes .ve classes and \nsix method calls. Along with the 00: BidirectionalGraph bidGraph; 01: Random random; 02: VertexAndEdgeProvider \ns0 = new VertexAndEdgeProvider(); 03: Vertex s1 = new Vertex(); 04: bidGraph = new BidirectionalGraph \n((IVEProvider)s0, PexSafeHelpers. ByteToBoolean((byte)16)); 05: bidirectionalGraph.AddVertex((IVertex)s1); \n06: random = new Random(); 07: RandomGraph.Graph((IEdgeMutableGraph)bidGraph, 0, 1, random, false); Figure \n8. A test input (generated by Seeker) that detected an in.nite loop in QuickGraph. Namespace # Branches \nPex M S M+S Algorithms 572 24.8 27.4 52.1 44.2 Collections 269 17.8 63.2 94.0 95.6 Concepts 51 39.2 74.5 \n74.5 74.5 Exceptions 5 80.0 80.0 80.0 80.0 Predicates 58 93.1 93.1 100 98.3 Providers 5 60.0 80.0 80.0 \n80.0 Representations 159 52.2 64.8 67.9 67.3 TOTAL 1119 31.6 47.3 68.2 64.3 Table 7. Branch coverage \nachieved by MSeqGen (M) and Seeker (S) for QuickGraph. skeleton generated by Seeker, the values 0 and \n1 gener\u00adated by Pex in Statement 7 helped trigger the in.nite loop in the RandomGraph.Graph method. In \nsummary, our results show that Seeker has higher defect-detection capabilities compared to Randoop and \nPex.  7.7 RQ4: MSeqGen Comparison We next address the fourth research question regarding com\u00adparing \nbranch coverage achieved by Seeker with MSeqGen. MSeqGen took 1.3 hours to generate test inputs for Quick-Graph. \nTable 7 shows our results. Columns Pex , M , and S show branch coverage achieved by Pex, MSeqGen, and \nSeeker, respectively. Column M + S shows branch cover\u00adage achieved by combining MSeqGen and Seeker. In \npar\u00adticular, we used the sequences extracted by MSeqGen as beginning sequences for Seeker rather than \nstarting Seeker from the scratch. Although MSeqGen achieved higher cov\u00aderage than Pex, our results show \nthat Seeker achieved much higher coverage than MSeqGen, especially for complex namespaces such as Algorithms \nand Collections. There are two major reasons for the lower coverage of MSe\u00adqGen compared to Seeker. First, \nsequences extracted by MSeqGen from the existing code bases do not include se\u00adquences for many classes \nunder test. For example, although we used 3.85MB of .NET assembly code for extracting se\u00adquences, none \nof these code bases include sequences for the EdgeDoubleDictionary or EdgeStringDictionary classes. Therefore, \nMSeqGen could not achieve any coverage for these classes. On the other hand, Seeker achieved 100% cov\u00aderage \nfor these two classes. Second, MSeqGen-extracted sequences are different from desired sequences required \nfor producing desired object states.  In contrast to our original expectation, M + S achieved lower \ncoverage than Seeker alone, except for the namespace Collections. Through our inspection, we found that \nM + S often resulted in more sequences, thereby increasing the exploration space for Pex. Although we \ncan address this is\u00adsue by using customized settings for Pex (similar to those used for RQ1), the limitations \nof the current Seeker proto\u00adtype prevents from using such customized settings. In fu\u00adture work, we plan \nto combine both these approaches by improving the performance of Seeker. In summary, Seeker achieved \nhigher branch coverage than MSeqGen, and unlike MSeqGen, Seeker does not require any additional informa\u00adtion \nsuch as usage information.  8. Discussion and Future Work In our evaluation, we used code coverage \nas a criterion for showing the effectiveness of Seeker compared to other ap\u00adproaches. Our criterion is \nbased on a recent case study [33], which showed that .eld defects reduce with increased test coverage. \nThis case study helps show that achieving high coverage can help improve the quality of code under test. \nFurthermore, our evaluation showed results for def-use cov\u00aderage, which is a stronger criterion compared \nto branch cov\u00aderage. The reason is that achieving higher coverage with re\u00adspect to a stronger coverage \ncriterion such as def-use cover\u00adage further helps to show the effectiveness of Seeker com\u00adpared to other \nexisting approaches. We next summarize major limitations due to which Seeker could not achieve branch \ncoverage close to 100%. These limitations can be broadly classi.ed into two cate\u00adgories: general limitations \nof DSE and limitations speci.c to our Seeker approach. These general limitations of DSE also affect Seeker, \nsince Seeker inherently uses DSE for dynamic analysis. 8.1 General limitations of DSE We next describe \ngeneral limitations of DSE that also apply for our Seeker approach. Path explosion. Although Seeker suggests \nshorter skele\u00adtons (as shown in Table 3), we identify that skeletons sug\u00adgested by Seeker increase the \nnumber of paths to be explored by Pex. Note that Seeker helps reduce the number of can\u00addidate sequences \nby using a combination of static and dy\u00adnamic analyses, but do not reduce the number of paths to be explored \nwithin a suggested candidate sequence. For exam\u00adple, for the algorithms namespace of QuickGraph, Seeker \nachieved 52.1%. Although Seeker suggested desired skele\u00adtons to Pex, Pex could not generate test inputs \nusing those skeletons for this namespace. The primary reason is that Pex, by default, attempts to cover \nall feasible paths among method calls within the suggested sequences. In future work, we plan to address \nthis issue by developing a search strategy that can guide Pex. The insight for our future work is that \nnot all paths in the method calls of suggested skeletons need to be explored for producing desired object \nstates. Environment dependency. A primary reason for the low coverage achieved by Seeker for xUnit and \nNUnit is their dependency on environments; dealing with such dependen\u00adcies is currently beyond the scope \nof Seeker. For exam\u00adple, in xUnit, majority of the classes requires assembly .les that include tests \nor project .les in XML formats. However, Seeker achieved 28.3% (121 new branches) higher coverage than \nmanually written tests for the Gui namespace, which includes some classes that require sequences and \ndo not de\u00adpend on the environment. In future work, we plan to address this issue by combining Seeker \nwith other approaches [30] that mock environments, thereby isolating the environment dependency.  8.2 \nSpeci.c limitations of Seeker We next describe two major limitations of our Seeker ap\u00adproach. Loop-based \nSequences. Seeker is effective in generating sequences that involve multiple methods (that can be from \ndifferent classes as well). Figure 8 shows an example se\u00adquence that includes six method calls from four \ndifferent classes. However, Seeker faces challenges in generating sequences that require method calls \nto be repeated mul\u00adtiple times to produce the desired object state. For exam\u00adple, consider the IntStack \nclass shown in Figure 4. Seeker can easily handle target branches with conditions such as if(stack.count \n> 0), which requires the Push method to be invoked only once. Instead, consider the following target \nbranch B9. 00: public static void foo1(IntStack ints) { 01: if(ints.size() > 3) { 02: ... // B9 03: } \n04: } To cover the target branch B9, the target sequence should invoke the Push method at least four \ntimes. However, our Al\u00adgorithms 1 and 2 cannot handle this scenario. In particular, when StatAnalyzer(B9, \nnull) is invoked, StatAnalyzer identi.es the pre-target as Push.After DynAnalyzer success\u00adfully covers \nthis pre-target (Line 4 of Algorithm 2), Stat Analyzer invokes DynAnalyzer(B9, IntStack.Push).How\u00adever, \nDynAnalyzer still cannot cover B9, since Push is in\u00advoked only once in the suggested sequence. To address \nthis issue, Seeker includes the a heuristic-based technique de\u00adscribed next.  Along with suggesting \na method (and a pre-target in that method), Seeker also observes how the suggested method such as Push \nmodi.es the target .eld tfield. We refer to this information as side-effect information. Using the desired \nvalue for tfield and the side-effect information, Seeker com\u00adputes the number of times the suggested \nmethod has to be invoked to produce a desired value for tfield. For example, the desired value for the \ntfield size is four for covering the target branch B9. Therefore, Seeker identi.es that Push method has \nto be invoked for four times in the suggested sequence, since each invocation of Push increases the value \nof the tfield size by one. Our technique can handle only a limited set of scenarios and cannot handle \nall scenarios that require method calls to be repeated multiple times. For example, our technique cannot \nhandle the scenario where more than one method has to be repeated multiple times. In future work, we \nplan to address this issue by developing a .tness-based approach, where a .tness function incremen\u00adtally \nguides the number of times suggested methods have to be invoked to achieve a desired value for tfield. \nAbstract classes, interfaces, and callback methods. All our subjects are libraries or frameworks that \ninclude ele\u00adments such as abstract classes or interfaces, whose imple\u00admentations are often not available \nwithin those libraries or frameworks. These libraries or frameworks expect client ap\u00adplications to provide \nsuch implementations. For example, Dsa provides three abstract classes such as CommonBinaryTree. Without \nthese abstract classes, Seeker achieved 94.3% cov\u00aderage (higher than manually written tests) for the \nnamespace DataStructures of Dsa. Similarly, xUnit includes methods (such as ExecutorCallback.Wrap) that \nrequire a callback method. We identify that manually written tests achieved higher coverage than Seeker, \nsince those tests include nec\u00adessary implementations. In future work, we plan to address this issue by \ndeveloping a technique similar to mocking en\u00advironments.  9. Related Work Our Seeker approach is related \nto three major research areas to be discussed next. 9.1 Object-oriented Test Generation Existing approaches \nfor object-oriented test generation can be broadly classi.ed into two major categories: implementation\u00adbased \nand usage-based approaches. Implementation-based approaches. These approaches use the implementation \ninformation of classes under test for gen\u00aderating test inputs. These approaches can further be classi\u00ad.ed \ninto two sub-categories: direct construction [3] and se\u00adquence generation [8,15, 17,32, 35,43, 45]. The \ndirect construction approaches such as Korat [3] construct desired object states by directly assigning \nvalues to member .elds of classes under test. However, these ap\u00adproaches require speci.cations such as \nclass invariants [26], which are rarely documented by developers. In contrast, Seeker is a sequence-generation \napproach and does not re\u00adquire class invariants. Among sequence-generation approaches, Buy et al. [5] \nproposed an approach that generates sequences for exer\u00adcising the def-use pairs associated with member \n.elds of classes under test. Their approach can be used for testing classes in isolation to achieve def-use \ncoverage. Bounded\u00adexhaustive approaches [45] generate sequences exhaustively up to a small bound of sequence \nlength. However, target se\u00adquences involving classes from real-world applications often require longer \nsequences beyond the small bound handled by bounded-exhaustive approaches. Another category of approaches, \ncalled evolutionary ap\u00adproaches [2, 14, 15, 24, 43], accept an initial set of se\u00adquences and evolve those \nsequences to produce new se\u00adquences that can generate desired object states. Two of these approaches \n[15, 43] can be used to test individual classes only and cannot generate target sequences that involve \nmeth\u00adods from multiple classes (as shown in their evaluations). Testful [2] addressed some of the issues \nfaced by these two approaches and proposed a semi-automated approach, where the user has to provide data \nto augment the ef.ciency. Harman and McMinn [14] further presented a theoretical and empirical analysis \nof a global search technique used in evolutionary approaches. Based on their empirical results, they \nproposed a hybrid global-local search (a memetic) algo\u00adrithm. Although a direct comparison of Seeker \nwith these approaches helps show the bene.ts of Seeker, we could not perform such comparison due to language \nrestrictions. In particular, prototypes developed for these evolutionary approaches target C or Java \nprograms, whereas Seeker tar\u00adgets .NET (C#) programs. Nevertheless, Lakhotia et al. [24] conducted an \nempirical study that compares a search-based test generation approach, called AUSTIN [23], with a DSE\u00adbased \napproach, called CUTE [22]. Their study on testing C code shows that both the approaches achieved similar \nbranch coverages. Their study also shows that neither of the approaches achieved more than 50% branch \ncoverage. A major issue identi.ed by their study for the DSE-based ap\u00adproach is related to the path exploration \nstrategy used by the approach. In particular, CUTE could not achieve high cover\u00adage due to unbounded \ndepth-.rst search strategy that often cannot handle programs with loops effectively. Recent ap\u00adproaches \nsuch as Fitnex [46] integrated within Pex [41] can help address those issues. Based on this empirical \nstudy, we expect that our Seeker approach can perform better than evo\u00adlutionary approaches too, since \nour evaluation results show that Seeker performs better than Pex.  Randoop [35] is a random approach \nthat generates se\u00adquences by randomly combining method calls. Zheng et al. [48] proposed a heuristic \napproach that assists a random approach with sequences that mutate member .elds accessed by a method \nunder test. However, due to the large search space of possible sequences, there is often a low probabil\u00adity \nfor randomly generating target sequences. In contrast to these approaches, ours is a systematic approach \nthat gener\u00adates sequences incrementally based on the branches that are not yet covered, thereby signi.cantly \nreducing the number of candidate target sequences. Korel [20, 21] proposed a chaining approach that iden\u00adti.es \nalternate target branches that need to be covered to cover a given target branch. Seeker also uses a \nsimilar tech\u00adnique. However, their approach can handle only procedural code such as C code and cannot \nhandle object-oriented code that includes additional challenges such as inheritance and nested classes. \nMcMinn and Holcombe [32] proposed an ex\u00adtended chaining approach that identi.es a sequence of meth\u00adods \nthat need to be executed to cover a target branch. Our approach signi.cantly differs from their approach \nin two ma\u00adjor aspects. First, similar to Korel s approach, their approach can handle only procedural \ncode. Second, their approach re\u00adquires users to provide a bound on the length of the desired sequence \nand method calls that can be included in that se\u00adquence. In contrast, our approach does not require any \nman\u00adual effort and automatically synthesizes sequence that pro\u00adduces desired object states. A recent \napproach, called Covana [44], precisely iden\u00adti.es the problems that prevent tools from achieving high \nstructural coverage. Covana focuses on two major problems: (1) external-method-call problem; (2) object-creation \nprob\u00adlem. Covana reports these problems to developers, so that developers can provide guidance to tools \nin achieving high structural coverage. Similar to Covana, Seeker focuses on object-creation problem. \nHowever, in contrast to Covana that reports problems to reduce effort of developers in guiding tools, \nSeeker automatically synthesizes sequences (that as\u00adsist tools such as Pex) and does not require any \nmanual ef\u00adfort. Usage-based approaches. In our previous work, we pro\u00adposed a mining-based approach, called \nMSeqGen [40], which statically mines method-call sequences based on their usages from existing code bases. \nMSeqGen uses mined se\u00adquences to assist random and DSE-based approaches. A ma\u00adjor issue with MSeqGen \nis that it is not effective in the sce\u00adnarios where no code bases that use classes required for tar\u00adget \nsequences are available or code bases include sequences that are different from target sequences. For \nexample, if a class c is newly introduced, it is not possible to .nd code bases using class c. Furthermore, \nmined sequences may not include all necessary method calls required for producing desired object states. \nJaygarl et al. proposed OCAT [16] that captures object states dynamically during program executions and \nreuses captured object states to assist a random approach. Simi\u00adlarly, another approach, called DyGen \n[38], mines dynamic traces recorded during program executions and generates regression test inputs from \nmined sequences. A major is\u00adsue with OCAT and DyGen is that these approaches re\u00adquire system test inputs \nfor capturing object states and se\u00adquences, respectively. Furthermore, captured object states or sequences \ncan be different from desired ones. Although OCAT includes a mutation technique, the mutation tech\u00adnique \nrequires class invariants to effectively mutate private member .elds. Seeker complements these approaches \nand does not require any additional information. Furthermore, Seeker can also effectively handle private \nmember .elds through method-call graphs.  9.2 Program Synthesis Earlier research in program synthesis \nfocused on program\u00adming by demonstration [9, 25], where programs are synthe\u00adsized automatically by observing \nthe manual actions per\u00adformed by the user. Further efforts in end-user program\u00adming attempted to bridge \nthe gap between natural languages and programming languages by developing structured edi\u00adtors [18] or \nproviding semantics to natural-language inter\u00adfaces [28]. In contrast to these approaches, Seeker targets \nat reducing efforts of programmers rather than end users. Little and Miller [27] proposed an approach \nthat allows end users to leverage scripting interfaces provided by appli\u00adcations such as Microsoft Word. \nIn particular, their approach allows end users to specify a task, such as formatting a docu\u00adment, using \nkeywords. Their approach attempts to map those keywords to APIs of particular system. In contrast to \ntheir ap\u00adproach that focuses on identifying APIs, Seeker focuses on generating method sequences that \nproduce a desired object state. However, in future work, we plan to adopt a similar strategy of accepting \ndesired object states in the form of key\u00adwords and automatically generate method sequences. Gulwani [12] \nproposed a framework that describes three dimensions of program synthesis: a user-speci.ed intent, the \nspace of candidate programs, and the search technique. Our Seeker approach can be formulated based on \nthis framework. Gulwani et al. [13] also proposed another approach for syn\u00adthesizing loop-free programs. \nIn contrast to these approaches that focus on synthesizing algorithms such as sorting or bit-manipulation \nroutines, Seeker focuses on synthesizing object-oriented programs that involve method sequences. There \nexist two other approaches [29, 39] accept queries of the form Source . Destination . These approaches \ngen\u00aderate method sequences that accept an object of type Source as input and produce an object of type \nDestination. In con\u00adtrast to these approaches that generate some object of the Destination type, Seeker \nfocuses on generating a desired ob\u00adject state of the Destination type.  9.3 Static and Dynamic Analyses \nSeeker uses a combination of static and dynamic analy\u00adses to intelligently navigate through a large search \nspace. Similar to Seeker, there exist other dynamic-analysis ap\u00adproaches [4, 6, 37] that also leverage \nstatic analysis. How\u00adever, static analysis used in Seeker differs signi.cantly from the static analysis \nused in these approaches. In particular, these existing approaches analyze control-.ow, data-.ow, or \nprogram-dependence graphs to assist dynamic analysis. In contrast to these approaches, Seeker uses method-call \ngraphs. Furthermore, these approaches handle procedural code such as C, whereas Seeker handles object-oriented \ncode.  10. Conclusion Over the past decade, program synthesis has gained focus due to the recent advances \nin computing and reasoning techniques. In this paper, we proposed an approach, called Seeker, that accepts \na user-speci.ed intent as a desired ob\u00adject state and synthesizes programs in the form of method sequences \nthat produce the desired object state. We have shown the effectiveness of Seeker by applying it to the \nprob\u00adlem of object-oriented test generation. In our evaluation, we have shown that Seeker achieved higher \ncoverage (both structural and data-.ow coverage) than existing state-of-the\u00adart DSE-based and random \napproaches on four subject appli\u00adcations (totalling 28KLOC). We have also shown that Seeker detected \n34 new defects. In future work, we plan to extend Seeker to accept a user-speci.ed intent in the form \nof natu\u00adral language. In particular, we plan to transform the intent into a series of desired object \nstates and leverage Seeker to automatically synthesize programs. Acknowledgments This work is supported \nin part by NSF grants CCF-0725190, CCF-0845272, CCF-0915400, CCF-0546844, CCF-0702622, CCF-1117603, CNS-0958235, \nCNS-0627749, CNS-0917392, ARO grant W911NF-08-1-0443, and US Air Force grant FA9550-07-1-0532.  References \n[1] S. Anand, C. S. Pasareanu, and W. Visser. JPF-SE: A symbolic execution extension to Java PathFinder. \nIn Proc. TACAS, pages 134 138, 2007. [2] L. Baresi and M. Miraz. Testful: automatic unit-test genera\u00adtion \nfor Java classes. In Proc. ICSE, pages 281 284, 2010. [3] C. Boyapati, S. Khurshid, and D. Marinov. Korat: \nAutomated testing based on Java predicates. In Proc. ISSTA, pages 123 133, 2002. [4] J. Burnim and K. \nSen. Heuristics for scalable dynamic test generation. In Proc. ASE, pages 443 446, 2008. [5] U. Buy, \nA. Orso, and M. Pezze. Automated testing of classes. In Proc. ISSTA, pages 39 48, 2000. [6] C. Cadar, \nD. Dunbar, and D. Engler. Klee: unassisted and auto\u00admatic generation of high-coverage tests for complex \nsystems programs. In Proc. OSDI, pages 209 224, 2008. [7] L. Clarke. A system to generate test data and \nsymbolically execute programs. IEEE Trans. Softw. Eng., 2(3):215 222, 1976. [8] C. Csallner and Y. Smaragdakis. \nJCrasher: An automatic robustness tester for Java. Softw. Pract. Exper., 34(11):1025 1050, 2004. [9] \nA. Cypher, D. C. Halbert, D. Kurlander, H. Lieberman, D. Maulsby, B. A. Myers, and A. Turransky, editors. \nWatch what I do: Programming by demonstration. MIT Press, Cam\u00adbridge, MA, USA, 1993. [10] P. G. Frankl \nand E. J. Weyuker. An applicable family of data .ow testing criteria. IEEE Trans. Softw. Eng., 14(10):1483 \n1498, 1988. [11] P. Godefroid, N. Klarlund, and K. Sen. DART: Directed automated random testing. In Proc. \nPLDI, pages 213 223, 2005. [12] S. Gulwani. Dimensions in program synthesis. In Proc. PPDP, pages 13 \n24, 2010. [13] S. Gulwani, S. Jha, A. Tiwari, and R. Venkatesan. Synthesis of loop-free programs. In \nProc. PLDI (to appear), 2011. [14] M. Harman and P. McMinn. A theoretical and empirical study of search-based \ntesting: Local, global, and hybrid search. IEEE Trans. Softw. Eng., 36:226 247, March 2010. [15] K. Inkumsah \nand T. Xie. Improving structural testing of object-oriented programs via integrating evolutionary testing \nand symbolic execution. In Proc. ASE, pages 297 306, 2008. [16] H. Jaygarl, S. Kim, T. Xie, and C. K. \nChang. OCAT: object capture-based automated testing. In Proc. ISSTA, pages 159 170, 2010. [17] Parasoft. \nJtest manuals version 5.1. Online manual, 2006. http://www.parasoft.com. [18] C. Kelleher and R. Pausch. \nLowering the barriers to program\u00adming: A taxonomy of programming environments and lan\u00adguages for novice \nprogrammers. ACM Comput. Surv., 37:83 137, June 2005. [19] J. C. King. Symbolic Execution and Program \nTesting. Com\u00admunications of the ACM, 19(7):385 394, 1976. [20] B. Korel. Dynamic method for software \ntest data generation. Software Testing, Veri.cation and Reliability, 2(3):203 213, 1992. [21] B. Korel. \nAutomated test generation for programs with proce\u00addures. In Proc. ISSTA, pages 209 215, 1996. [22] S. \nKoushik, M. Darko, and A. Gul. CUTE: A concolic unit testing engine for C. In Proc. ESEC/FSE, pages 263 \n272, 2005. [23] K. Lakhotia, M. Harman, and P. McMinn. Handling dynamic data structures in search based \ntesting. In Proc. GECCO, pages 1759 1766, 2008.  [24] K. Lakhotia, P. McMinn, and M. Harman. An empirical \nin\u00advestigation into branch coverage for C programs using CUTE and AUSTIN. J. Syst. Softw., 83:2379 2391, \nDecember 2010. [25] H. Lieberman, editor. Your wish is my command: Program\u00adming by example. Morgan Kaufmann \nPublishers Inc., San Francisco, CA, USA, 2001. [26] B. Liskov and J. Guttag. Program Development in Java: \nAbstraction, Speci.cation, and Object-Oriented Design. Addison-Wesley Longman Publishing Co., Inc., Boston, \nMA, USA, 2000. [27] G. Little and R. C. Miller. Translating keyword commands into executable code. In \nProc. UIST, pages 135 144, 2006. [28] H. Liu and H. Lieberman. Programmatic semantics for natural language \ninterfaces. In Proc. CHI, pages 1597 1600, 2005. [29] D. Mandelin, L. Xu, R. Bodik, and D. Kimelman. \nJungloid mining: helping to navigate the API jungle. In Proc. PLDI, pages 48 61, 2005. [30] M. R. Marri, \nT. Xie, N. Tillmann, J. de Halleux, and W. Schulte. An empirical study of testing .le-system\u00addependent \nsoftware with mock objects. In Proc. AST, pages 149 153, 2009. [31] V. Martena, A. Orso, and M. Pezz`e. \nInterclass testing of object oriented software. In Proc. ICECCE, pages 145 154, 2002. [32] P. McMinn \nand M. Holcombe. Evolutionary testing of state\u00adbased programs. In Proc. GECCO, pages 1013 1020, 2005. \n[33] A. Mockus, N. Nagappan, and T. T. Dinh-Trong. Test cover\u00adage and post-veri.cation defects: A multiple \ncase study. In Proc. ESEM, pages 291 301, 2009. [34] A. Orso and B. Kennedy. Selective capture and replay \nof program executions. SIGSOFT Softw. Eng. Notes, 30(4):1 7, 2005. [35] C. Pacheco, S. K. Lahiri, M. \nD. Ernst, and T. Ball. Feedback\u00addirected random test generation. In Proc. ICSE, pages 75 84, 2007. [36] \nQuickGraph: A 100% C# graph library with Graphviz Sup\u00adport, Version 1.0, 2008. http://www.codeproject.com/ \nKB/miscctrl/quickgraph.aspx. [37] D. Song, D. Brumley, H. Yin, J. Caballero, I. Jager, M. G. Kang, Z. \nLiang, N. James, P. Poosankam, and P. Saxena. Bit\u00adblaze: A new approach to computer security via binary \nanaly\u00adsis. In Proc. ICISS, pages 1 25, 2008. [38] S. Thummalapenta, J. de Halleux, N. Tillmann, and S. \nWadsworth. DyGen: Automatic generation of high\u00adcoverage tests via mining gigabytes of dynamic traces. \nIn Proc. TAP, pages 77 93, 2010. [39] S. Thummalapenta and T. Xie. PARSEWeb: A programmer assistant for \nreusing open source code on the web. In Proc. ASE, pages 204 213, 2007. [40] S. Thummalapenta, T. Xie, \nN. Tillmann, J. de Halleux, and W. Schulte. MSeqGen: Object-oriented unit-test generation via mining \nsource code. In Proc. ESEC/FSE, pages 193 202, 2009. [41] N. Tillmann and J. de Halleux. Pex -white box \ntest generation for .NET. In Proc. TAP, pages 134 153, 2008. [42] N. Tillmann and W. Schulte. Parameterized \nUnit Tests. In Proc. ESEC/FSE, pages 253 262, 2005. [43] P. Tonella. Evolutionary testing of classes. \nIn Proc. ISSTA, pages 119 128, 2004. [44] X. Xiao, T. Xie, N. Tillmann, and J. de Halleux. Precise identi.cation \nof problems for structural test generation. In Proc. ICSE, pages 611 620, 2011. [45] T. Xie, D. Marinov, \nW. Schulte, and D. Notkin. Symstra: A framework for generating object-oriented unit tests using symbolic \nexecution. In Proc. TACAS, pages 365 381, 2005. [46] T. Xie, N. Tillmann, P. de Halleux, and W. Schulte. \nFitness\u00adguided path exploration in dynamic symbolic execution. In Proc. DSN, pages 359 368, 2009. [47] \nZ3: An ef.cient theorem prover, 2005. http://research. microsoft.com/en-us/um/redmond/projects/z3/. [48] \nW. Zheng, Q. Zhang, M. Lyu, and T. Xie. Random unit-test generation with MUT-aware sequence recommendation. \nIn Proc. ASE, pages 293 296, 2010.  \n\t\t\t", "proc_id": "2048066", "abstract": "<p>High-coverage testing is challenging. Modern object-oriented programs present additional challenges for testing. One key difficulty is the generation of proper method sequences to construct desired objects as method parameters. In this paper, we cast the problem as an instance of program synthesis that automatically generates candidate programs to satisfy a user-specified intent. In our setting, candidate programs are method sequences, and desired object states specify an intent. Automatic generation of desired method sequences is difficult due to its large search space---sequences often involve methods from multiple classes and require specific primitive values. This paper introduces a novel approach, called Seeker, to intelligently navigate the large search space. Seeker synergistically combines static and dynamic analyses: (1) dynamic analysis generates method sequences to cover branches; (2) static analysis uses dynamic analysis information for not-covered branches to generate candidate sequences; and (3) dynamic analysis explores and eliminates statically generated sequences. For evaluation, we have implemented Seeker and demonstrate its effectiveness on four subject applications totalling 28K LOC. We show that Seeker achieves higher branch coverage and def-use coverage than existing state-of-the-art approaches. We also show that Seeker detects 34 new defects missed by existing tools.</p>", "authors": [{"name": "Suresh Thummalapenta", "author_profile_id": "81339532651", "affiliation": "IBM Research, Bangalore, India", "person_id": "P2839155", "email_address": "surthumm@in.ibm.com", "orcid_id": ""}, {"name": "Tao Xie", "author_profile_id": "81314487030", "affiliation": "North Carolina State University, Raleigh, USA", "person_id": "P2839156", "email_address": "xie@csc.ncsu.edu", "orcid_id": ""}, {"name": "Nikolai Tillmann", "author_profile_id": "81100175291", "affiliation": "Microsoft Research, Redmond, USA", "person_id": "P2839157", "email_address": "nikolait@microsoft.com", "orcid_id": ""}, {"name": "Jonathan de Halleux", "author_profile_id": "81435593783", "affiliation": "Microsoft Research, Redmond, USA", "person_id": "P2839158", "email_address": "jhalleux@microsoft.com", "orcid_id": ""}, {"name": "Zhendong Su", "author_profile_id": "81100108298", "affiliation": "University of California, Davis, USA", "person_id": "P2839159", "email_address": "su@cs.ucdavis.edu", "orcid_id": ""}], "doi_number": "10.1145/2048066.2048083", "year": "2011", "article_id": "2048083", "conference": "OOPSLA", "title": "Synthesizing method sequences for high-coverage testing", "url": "http://dl.acm.org/citation.cfm?id=2048083"}