{"article_publication_date": "10-22-2011", "fulltext": "\n Reactive Imperative Programming with Data.ow Constraints Camil Demetrescu Irene Finocchi Andrea Ribichini \nDept. ofComputer and System Sciences Dept. of Computer Science Dept. of Computer andSystem Sciences Sapienza \nUniversityof Rome Sapienza UniversityofRome Sapienza UniversityofRome demetres@dis.uniroma1.it .nocchi@di.uniroma1.it \nribichini@dis.uniroma1.it Abstract Data.ow languages provide natural support for specifying constraints \nbetween objects in dynamic applications, where programs need to react ef.ciently to changes of their \nenvi\u00adronment.Researchers havelonginvestigatedhowtotakead\u00advantage ofdata.owconstraintsbyembeddingthemintopro\u00adcedurallanguages.Previous \nmixedimperative/data.ow sys\u00adtems, however, require syntactic extensions or libraries of ad hoc datatypesfor \nbindingtheimperative programtothe data.ow solver. In this paper we propose a novel approach that smoothly \ncombines the two paradigms without placing undueburden ontheprogrammer. In our framework, programmers \ncan de.ne ordinary statements of the imperative host language that enforce constraints between objects \nstored in special memory lo\u00adcations designated as reactive . Differently from previous approaches, reactive \nobjects can be of any legal type in the host language, including primitive data types, pointers, ar\u00adrays, \nand structures. Statements de.ning constraints are au\u00adtomatically re-executed every time their input \nmemory lo\u00adcations change,letting a program behavelike a spreadsheet where the values of some variables \ndepend upon the values of othervariables.Theconstraintsolvingmechanismis han\u00addled transparently by altering \nthe semantics of elementary operations of the host language for reading and modifying objects.We provide \naformal semanticsand describeacon\u00adcrete embodiment of our technique into C/C++, showing how to implement \nit ef.ciently in conventional platforms using off-the-shelf compilers. We discuss common coding idioms \nand relevant applications to reactive scenarios, in\u00adcluding incremental computation, observer design \npattern, and data structure repair. The performance of our imple\u00admentation is compared to ad hoc problem-speci.c \nchange propagation algorithms, as well as to language-centric ap- Permission to make digital or hard \ncopies of all or part of this work for personal or classroom useisgranted without feeprovided that copies \nare not made ordistributed forpro.tor commercial advantage andthat copiesbearthis notice and thefull \ncitation onthe .rstpage.Tocopy otherwise,torepublish,topost onserversortoredistribute tolists, requiresprior \nspeci.cpermission and/or afee. OOPSLA 11, October22 27,2011,Portland,Oregon,USA. Copyright c &#38;#169;2011ACM978-1-4503-0940-0/11/10. \n. .$10.00 proaches such as self-adjusting computation and subject/ob\u00adserver communication mechanisms, \nshowing that the pro\u00adposed approachisef.cientin practice. Categories and Subject Descriptors D.3.3[Programming \nLanguages]:LanguageConstructs andFeatures Constraints General Terms Algorithms, design, experimentation, \nlan\u00adguages. Keywords Reactive programming, data.ow programming, imperative programming, constraint solving, \nincremental computation, observer design pattern, data structure repair. 1. Introduction A one-way, data.ow \nconstraint is an equation of the form y = f(x1 ,...,xn)in which the formula on the right side is automatically \nre-evaluated and assigned to the variable y whenever any variable xi changes. If y is modi.ed from outsidetheconstraint,theequationislefttemporarily \nunsat\u00adis.ed, hence the attribute one-way . Data.ow constraints are recognized as a powerful programming \nmethodology in a variety of contexts because of their versatility and sim\u00adplicity [46]. The most widespread \napplication of data.ow constraintsis perhapsembodiedbyspreadsheets[2, 36].In a spreadsheet, the user \ncan specify a cell formula that de\u00adpends onothercells: whenanyofthosecellsis updated,the valueofthe .rstcellisautomatically \nrecalculated.Rulesin a Makefile are another example of data.ow constraints: a rule sets up a dependency \nbetween a target .le and a list of input .les, and provides shell commands for rebuilding the targetfromtheinput \n.les.Whenthe Makefile is run,ifany input.leinaruleisdiscovered tobe newerthanthetarget, thenthetargetisrebuilt.The \ndata.owprinciplecanalsobe applied to software development and execution, where the role ofacell/.leisreplacedbya \nprogramvariable.Thisap\u00adproach has been widely explored in the context of interac\u00adtive applications, multimedia \nanimation, and real-time sys\u00adtems[13, 15, 29, 35, 41, 47]. Since the values of program variables are \nautomatically recalculateduponchangesofothervalues,thedata.owcom\u00adputational model is very different from \nthe standard imper\u00adative model, in which the memory store is changed explic\u00aditlybythe program viamemoryassignments.Theexecution \n.ow of applications running on top of a data.ow environ\u00adment is indeed data-driven, rather than control-driven, \npro\u00advidinga naturalgroundforautomaticchangepropagationin all scenarios where programs need to react to \nmodi.cations oftheirenvironment.Implementations ofthe data.ow prin\u00adciple share some commonissues with \nself-adjusting compu\u00adtation, in which programs respond to input changes by up\u00addatingautomatically their \noutput[3 5, 30]. Differently from purely declarativeconstraints[7],data\u00ad.ow constraints are expressed \nby means of (imperative) methods whose execution makes a relation satis.ed. This programming style is \nintuitive and readily accessible to a broad range of developers[46].The abilityto smoothlycom\u00adbine different \nparadigms in a uni.ed framework makes it possible to take advantage of different programming styles in \nthe context of the same application. The problem of in\u00adtegratingimperativeand data.ow programming hasalready \nbeen the focus of previous work in the context of speci.c application domains[11,40,41,46].Previousmixedimper\u00adative/data.ow \nsystems are based on libraries of ad hoc data types andfunctionsfor representing constraint variables \nand for binding the imperative program to the constraint solver. One drawback of these approaches is \nthat constraint vari\u00adables can only be of special data types provided by the run\u00adtime library, causing \nloss of .exibility and placing undue burden ontheprogrammer.Anaturalquestioniswhetherthe data.ow model \ncan be made to work with general-purpose, imperative languages, such as C, without adding syntactic extensions \nand ad hoc data types. In this paper we af.rma\u00adtively answer this question. Our Contributions. We present \na general-purpose frame\u00adwork where programmers can specifygeneric one-way con\u00adstraints between objects \nof arbitrary types stored in reac\u00adtive memory locations. Constraints are written as ordinary statementsoftheimperative \nhostlanguageandcanbeadded andremoveddynamically at runtime.Sincethey canchange multiple objects within \nthe same execution, they are multi\u00adoutput. The main feature of a constraint is its sensitivity to modi.cations \nof reactive objects: a constraint is automati\u00adcally re-evaluated whenever any of the reactive locations \nit depends onischanged,eitherbytheimperativeprogram, or by another constraint. A distinguishing feature \nof our ap\u00adproach is that the whole constraint solving mechanism is handled transparently by altering \nthe semantics of elemen\u00adtary operations of the imperative host language for reading and modifying objects. \nNo syntax extensions are required and no newprimitivesare needed exceptforadding/remov\u00ading constraints, \nallocating/deallocating reactive memory lo\u00adcations, and controlling the granularityof solver activations. \nDifferentlyfrompreviousapproaches,programmersare not forcedto useanyspecialdatatypesprovidedbythelanguage \nextension, and can resort to the full range of conventional constructsforaccessingandmanipulating objects \nofferedby the host language. In addition, our framework supports all the otherfeaturesthathavebeenrecognized \ntobeimportant inthe design of data.owconstraintsystems[46],including: Arbitrary code:constraints consist \nof arbitrary codethatis legalinthe underlyingimperativelanguage,thusinclud\u00adingloops, conditionals,function \ncalls, and recursion. Address dereferencing: constraints are able to reference variablesindirectlyvia \npointers. Automatic dependency detection: constraints automatically detectthereactivememorylocationstheydepend \non dur\u00ading their evaluation, so there is no need for program\u00admers to explicitly declare dependencies, \nwhich are also allowed to vary over time. We embodied these principles into an extension of C/C++ that \nwe called DC. Our extension has the same syntax as C/C++,but operationsthatread ormodifyobjectshaveadif\u00adferentsemantics.Allotherprimitives,includingcreatingand \ndeleting constraints and allocating and deallocating reactive memoryblocks,are provided asruntimelibraryfunctions. \nOur main contributions are re.ected in the organization ofthe paperandcan besummarized asfollows: In \nSection 2 we abstract our mechanism showing how to extend an elementary imperative language to support \none-waydata.owconstraints usingreactivememory.We formallydescribe ourmixedimperative/data.owcompu\u00adtational \nmodel, provide a formal semantics, and discuss convergence properties of our data.ow constraint solver \nbymodelingthecomputationasaniterative processthat aims at .nding a common .xpoint for the current set \nof constraints.  InSection3wedescribetheconcreteembodimentof our technique into C/C++, introducing the \nmain features of DC.  In Section 4 we give a variety of elementary and ad\u00advanced programmingexamplesand \ndiscuss howDC can improveC/C++ programmabilityinthreerelevantappli\u00adcation scenarios: incremental computation, \nimplementa\u00adtion of the observer software design pattern, and data structure checking and repair. To the \nbest of our knowl\u00adedge,theseapplicationshave notbeenexploredbeforein the context of data.ow programming. \n In Section 5 we describe how DC can be implemented using off-the-shelf compilers on conventional platforms \nvia a combination of runtime libraries, hardware/operat\u00ading system support, and dynamic code patching, \nwithout requiring any source code preprocessing.  In Section 6 we discuss the results of an extensive \nex\u00adperimental analysis of DC in a variety of settings, show\u00ading that our implementation is effective \nin practice. We consider both interactive applications and computation\u00adally demanding benchmarks that \nmanipulate lists, grids, trees, and graphs. We assess the performances of DC   against conventional \nC-based implementations as well as against competitors that can quickly react to input changes,i.e., \nad hoc dynamicalgorithms[20, 44],incre\u00admental solutions realizedinCEAL[30](a state-of-the\u00adart C-based \nframework for self-adjusting computation), and Qt s signal-slot implementation of the subject/ob\u00adserver \ncommunication mechanism[26]. Relatedworkis discussedinSection7 and directionsforfu\u00adtureresearch aresketchedinSection \n8.Anextended version of this paper, including proofs and additional experiments, can befoundintheaccompanyingtechnical \nreport[23]. 2. Abstract Model To describe our approach, we consider an elementary im\u00adperative language \nand we show how to extend it to support one-waydata.owconstraints.WestartfromWHILE [43],an extremely \nsimple language of commands including a sub\u00adlanguage of expressions. Although WHILE does not sup\u00adportmany \nfundamentalfeatures of concreteimperativelan\u00adguages(including declarations, procedures, dynamic mem\u00adory \nallocation,typechecking,etc.),it providesall thebuild\u00ading blocks for a formal description of our mechanism, \nab\u00adstracting away details irrelevant for our purposes. We dis\u00adcuss how to modify the semantics of WHILE \nto integrate a data.ow constraint solver. We call the extended language DWHILE. DWHILE is identical to \nWHILE except for a dif\u00adferent semantics and additional primitives for adding/delet\u00adingconstraints dynamicallyandforcontrollingthegranular\u00adity \nof solver activations. As we will see in Section 3, these primitivescan besupportedin procedurallanguagesasrun\u00adtimelibraryfunctions. \n2.1 The DWHILE Language The abstract syntax of DWHILE is shown in Figure 1. The language distinguishes \nbetweencommands(i.e.,statements) and expressions. We use c, c1 , c2 as meta-variables e . Exp::= l |v \n|(e)|... ranging over the set of c . Comm::= commands Comm, and e, skip | e1 , e2 as meta-variables l:= \ne | ranging over the set of c1 ;c2 | expressions Exp. Canon\u00ad if e then c1 else c2 | ical forms of expressions \nwhile e do c | newcons c |are either storagelocations delcons c | l . Loc, or storable values begin at \nc end at v over some arbitrary do\u00ad main Val.Expressions can Figure 1. Abstract syntax also be obtained \nby apply\u00adof DWHILE. ing to sub-expressions any primitive operationsde.nedoverdomain Val (e.g.,plus, mi\u00adnus, \netc.).Commandsinclude: Assignments of valuesto storagelocations(l:=e).These commands are the basic state \ntransformers. write to reactive constraint memory location termination S=\u00d8 S.\u00d8 Figure 2. Transitions \nbetween different execution modes. Constructsfor sequencing, conditional execution, andit\u00aderation, with \nthe usual meaning.  Two new primitives, newcons and delcons, for adding and deleting constraints dynamically. \nNotice that a con\u00adstraintinDWHILE isjustan ordinary command.  An atomic block construct, begin at c \nend at, that executes a command c atomically so that any constraint evaluation is deferred until the \nend of the block. This offers .ne-grained control over solver activations.  In Section 3 we will show \na direct application of the con\u00adcepts developed in this section to the C/C++ programming languages. 2.2 \nMemoryModel andExecutionModes Ourapproachhinges upontwokeynotions:reactive memory locations and constraints.Reactive \nmemory can be read and writtenjustlike ordinarymemory.However,differentlyfrom ordinary memory: 1. Ifa \nconstraint c reads a reactive memorylocation lduring its execution, a dependency (l,c)of c from l is \nadded to a set D of dependencies. 2. If the value stored in a reactive memory location l is changed, \nall constraints depending on l (i.e., all con\u00adstraints c such that (l,c) . D) are automatically re\u00adexecuted. \n  Point 2 states that constraints are sensitive to modi.cations ofthecontents ofthereactivememory.Point1 \nshowshowto maintain dynamically the set D of dependencies needed to trigger the appropriate constraints \nupon changes of reactive memorylocations.We remarkthat re-evaluating a constraint c may completely change \nthe set of its dependencies: prior to re-execution, all the old dependencies (-,c) . D are discarded,and \nnewdependenciesareloggedin D duringthe re-evaluation of c. As shown in Figure 2, at any point in time \nthe execution can be in one of three modes: normal execution, constraint execution, or scheduling.As \nwe will see moreformallylater inthissection, differentinstructions (suchasreadingare\u00adactive memory location \nor assigning it with a value) may have different semantics depending on the current execution mode. We \nassume eager constraint evaluation, i.e., out-of-date constraints are brought up-to-date as soon as possible. \nThis choiceisbettersuited to ourframework and,aspreviousex\u00adperience has shown, lazy and eager evaluators \ntypically de\u00adlivercomparable performancein practice[46].Eagereval\u00aduation is achieved as follows. A scheduler \nmaintains a data structure S containing constraints to be .rst executed or re\u00adevaluated. As an invariant \nproperty, S is guaranteed to be emptyduring normal execution.Assoonasareactivemem\u00adory location l is written, \nthe scheduler queries the set D of dependencies and addsto S all the constraints depending on l. These \nconstraints are then run one-by-one in constraint execution mode, and new constraints may be added to \nS throughout this process. Whenever S becomes empty, nor\u00admal executionis resumed. An exception to eager \nevaluation is related to atomic blocks.Theexecution ofanatomic block c is regardedas an uninterruptible \noperation: newconstraintscreatedduringthe evaluation of c arejust added to S. When c terminates, for \neach reactive memory location l whose value has changed, all the constraints depending on l are also \nadded to S, and the solver is eventually activated. Constraint executions are uninterruptible as well. \nWe remark that any scheduling mechanism may be used for selecting from S the next constraint to be evaluated: \nin this abstract model we rely on a function pick that imple\u00adments any appropriate scheduling strategy. \n 2.3 Con.gurations Given a set X, we denote by 2X its powerset.A con.gura\u00adtionof oursystemisasix-tuple \n(.,a,s,D,S,cself ).R\u00d7Bool\u00d7S\u00d7Dep\u00d72Cons \u00d7Cons where: R is a set of Boolean store attributes of the form \n. : Loc .{normal,reactive }, where function . .R speci.es which memorylocations are reactive.  a .{true,false}isaBoolean \n.ag thatis true inside atomicblocksandis usedfordeferringsolveractivations.  S= {s : Loc .Val}is a set \nof stores mapping storage locations to storable values.  D .Loc \u00d7Consis a set of dependencies,Consis \na set of constraints, and Dep =2Loc\u00d7Cons. A constraint can be any commandin DWHILE,i.e., Cons = Comm.We \nusedifferent namesforthesakeofclarity.  S . Consis the scheduling data structure.  cself is ameta-variablethatdenotesthecurrent \nconstraint (i.e., the constraint that is being evaluated) in constraint executionmode,andis unde.nedotherwise.Ifthesched\u00aduler \nwere deterministic, cself may be omitted from the con.guration,butwedo notmakethisassumptioninthis paper. \n  2.4 OperationalSemantics Most of the operational semantics of the DWHILE lan\u00adguage can be directly \nderived from the standard semantics ofWHILE.Themostinterestingaspectsof ourextensionin\u00adclude reading \nand writing the reactive memory, adding and deleting constraints, executing commands atomically, and \nde.ning the behavior of the scheduler and its interactions with the otherexecutionmodes.Rulesfortheseaspectsare \ngiveninFigure4 andare discussedbelow. Let .e . (S\u00d7Exp)\u00d7Val and .c . (S\u00d7Comm)\u00d7S bethestandardbig-steptransitionrelations \nusedinthe oper\u00adational semantics oftheWHILE language[43].Besides .e and .c,we useadditionaltransitionrelationsforexpression \nevaluation in constraint mode (.ce), command execution in normal mode (.nc), command execution in constraint \nmode(.cc), and constraint solver execution in scheduling mode(.s), as de.ned in Figure 3. Notice that \nexpression evaluation in normal mode can be carried on directly by means of transition relation .e of \nWHILE. As discussed below, relation .ce is obtained by appropriately modifying .e. Similarly, relations \n.nc and .cc are obtained by ap\u00adpropriately modifying .c.All therules notreportedinFig\u00adure4 canbe derivedinastraightforward \nwayfromthecor\u00adrespondingrulesinthestandard semantics ofWHILE [43]. The evaluation of a DWHILE program \nis started by rule EVAL,whichinitializestheatomic .ag a to false and both the scheduling queue S andthe \nset D of dependenciestothe empty set. Writing Memory. Assigning an ordinary memory loca\u00adtionin normal \nexecutionmode(ruleASGN-N1)justchanges the store as in the usual semantics of WHILE. This is also the \ncase when the new value of the location to be assigned equalsitsold value orinsideanatomicblock.Otherwise,if \nthe location l to be assigned is reactive, the new value dif\u00adfersfromtheoldone,andexecutionis outsideatomicblocks \n(rule ASGN-N2), constraints depending on l are scheduled in S andareevaluated one-by-one.Aswewill see,thetran\u00adsition \nrelation .s guarantees S tobeemptyattheend ofthe constraint solving phase. In conformity with the atomic \nex\u00adecution ofconstraints,assignmentinconstraintmode(rule ASGN-C)justresortsto ordinaryassignmentinWHILE \nfor both normal and reactive locations. We will see in rule SOLVER-2, however, that constraints can be \nnevertheless scheduledby otherconstraintsif theirexecutionchangesthe contents ofreactivememorylocations. \nReading Memory. Reading an ordinary memory location in constraint execution mode(rule DEREF-C1)justevaluates \nthelocation toits valuein the current store: thisis speci.ed by transition relation .e of the WHILE semantics. \nIf the location l to be read is reactive (rule DEREF-C2), a new dependencyoftheactiveconstraint cself \nfromlis alsoadded to the set D of dependencies. Executing Atomic Blocks. To execute an atomic block in \nnormal mode (rule BEGINEND-N2), the uninterruptible command c is .rst evaluated according to the rules \nde.ned by transition .nc. If the content of some reactive loca\u00ad s ' .. (R\u00d7S\u00d7 Comm)\u00d7 S (.,s,c). .ce \n. (R\u00d7S\u00d7 Cons\u00d7 Dep\u00d7 Exp)\u00d7(Dep\u00d7 Val) (.,s,cself ,D,e).ce (D ' ,v) ' . nc . (R\u00d7Bool\u00d7 S\u00d7 Dep\u00d7 2Cons \u00d7 Comm)\u00d7 \n(S\u00d7 Dep\u00d7 2Cons) (.,a,s,D,S,c).nc (s ' ,D ' ,S ) . (R\u00d7S\u00d7 Dep\u00d7 2Cons ' . cc \u00d7 Cons\u00d7 Comm)\u00d7 (S\u00d7 Dep\u00d7 2Cons) \n(.,s,D,S,cself ,c).cc (s ' ,D ' ,S ) . (R\u00d7S\u00d7 Dep\u00d7 2Cons . s )\u00d7 (S\u00d7 Dep) (.,s,D,S).s (s ' ,D ')  Figure \n3. Transition relations for DWHILE program evaluation (.), expression evaluation in constraint mode (.ce), \ncommandexecutionin normal mode(.nc), command executionin constraint mode(.cc), and constraint solver \nexecutionin scheduling mode(.s). (., a, s,D, S, c) .nc (s ' ,D ' ,S) (.,s, c) . s ' where: 8 < : a = \nfalse D = \u00d8 S = \u00d8 (EVAL) (.,s, cself ,D, e) .ce (D ' , v) s ' = s|lb.v (.,s, D, S, cself , l := e) .cc \n(s ' , D ' , S) (ASGN-C) s . e .e v s ' = s|lb.v (.,a, s, D,S,l := e) .nc (s ' , D, S) if.(l)= normal \nor s ' (l)= s(l)or a = true (ASGN-N1) S = \u00d8 S ' = {c |(l, c). D} s . e .e v s ' = s|lb.v (., s ' , D, \nS ') .s (s '' ,D ') (.,a, s, D,S,l := e) .nc (s '' , D ' , S) if .(l)= reactive and s ' (l) = s(l)and \na = false (ASGN-N2) s . l .e v (.,s, cself ,D, l) .ce (D, v) if .(l)= normal (DEREF-C1) s . l .e v D \n' = D .{(l, cself )} (., s,cself , D, l) .ce (D ' , v) if.(l)= reactive (DEREF-C2) (., s,D, S, cself \n,c) .cc (s ' , D ' , S ') (.,s, D, S, cself ,begin at c end at) .cc (s ' , D ' , S ') (BEGINEND-C) (.,a, \ns,D, S, c) .nc (s ' , D, S ') (., a, s,D, S,begin at c end at) .nc (s ' , D, S ') if a = true (BEGINEND-N1) \n '' '' S = \u00d8 a = true (.,a ,s,D,S,c).nc (s ,D,S ) '' ' ' ''''' ' S = S .{c |(l,c). D .s(l) s (l)}(.,s \n,D,S ).s (s ,D = ) ifa = false (.,a,s,D,S,begin at c end at).nc (s '' ,D ' ,S) (BEGINEND-N2) S = \u00d8 S \n' = {c} (.,s, D,S ') .s (s ' ,D ') (.,a, s,D, S,newcons c) .nc (s ' ,D ' ,S) ifa = false (NEWCONS-N1) \nS ' = S .{c} (.,a, s,D, S,newcons c) .nc (s,D, S ') ifa = true (NEWCONS-N2) D ' = D \\{(l,x). D|x = c} \nS ' = S \\{c} (.,a, s, D,S,delcons c) .nc (s, D ' , S ') (DELCONS-N) S ' = S .{c} (., s,D, S, cself ,newcons \nc) .cc (s, D, S ') (NEWCONS-C) D ' = D \\{(l,x). D|x = c} S ' = S \\{c} (.,s, D, S, cself , delcons c) \n.cc (s,D ' ,S ') (DELCONS-C) (., s,D, S) .s (s,D) ifS = \u00d8 (SOLVER-1) (.,s,D ' ,S \\{cself },cself ,cself \n).cc (s ' ,D '' ,S ') 8 = pick(S) < cself ' '' '''' ''' (.,s ,D ,S ).s (s ,D ) where: D ' = D \\{(l,x). \nD|x = cself } if S = \u00d8 '' ''': '' ' D '' s ' (.,s,D,S).s (s ,D ) S = S .{c |(l,c)..s(l) (l)} = (SOLVER-2) \nFigure 4. DWHILE program evaluation. tion changes due to the execution of c, the solver is then activated \nat the end of the block. The begin at /end at command has instead no effect when execution is already \natomic,i.e.,in constraint mode (rule BEGINEND-C)andin\u00adsideatomic blocks(ruleBEGINEND-N1),exceptforexecut\u00ading \ncommandc. Creating and Deleting Constraints. In non-atomic nor\u00admal execution mode, rule NEWCONS-N1 creates \na new con\u00adstraint and triggers its .rst execution as speci.ed by .s. In atomic normal execution and in \nconstraint mode, rules NEWCONS-N2 and NEWCONS-C simply add the constraint to the scheduling queue. Similarly, \nrules DELCONS-N and DELCONS-C remove the constraint from the scheduling queueandclean upits dependenciesfrom \nD. Activating the Solver. Rules SOLVER-1 and SOLVER-2 specify the behavior of the scheduler, which is \nstarted by rules ASGN-N2 and BEGINEND-N2. Rule SOLVER-1 de\u00ad.nes the termination of the constraint solving \nphase: this phase ends only when there are no more constraints to be evaluated(i.e., S = \u00d8).RuleSOLVER-2 \nhasaninductive de.nition.If S is notempty,function pick selectsfrom S a new active constraint cself , \nwhichisevaluatedinconstraint mode after removing from D its old dependencies. The .\u00adnal state(s '' )and \ndependencies(D ''' )are those obtained by applying the scheduler on the store s ' obtained after the \nex\u00adecution of cself and on a new set S '' of constraints. S '' is derived from S by adding any new constraints(S \n' ) result\u00ading from the execution of cself along with the constraints depending onreactivememorylocationswhosecontenthas \nbeen changed by cself .The de.nition of S '' guarantees that constraintscantrigger otherconstraints(eventhemselves), \neven if each constraint execution is regarded as an atomic operationandis neverinterruptedbythescheduler. \n 2.5 ConvergenceProperties The approach we follow in our work consists of model\u00ading data.ow constraint \nsolving as an iterative process that aims at .nding a common .xpoint for the current set of constraints. \nIn our context, a .xpoint is a store that satis\u00ad.es simultaneously all the relations between reactive \nmem\u00adory locations speci.ed by the constraints. As we will see inSection2.6,thisprovidesa unifyingframeworkforsolv\u00ading \ndata.ow constraint systems with bothacyclic and cyclic dependencies. So far, we have assumed that the \nschedul\u00ading orderofconstraintexecutionsisspeci.edbyafunction pick givenasaparameterofthesolver.A naturalquestion \nis whether there are any general properties of a set of con\u00adstraintsthatlet oursolverterminateandconvergetoacom\u00admon \n.xpoint independently of the scheduling strategy used byfunction pick. Weadaptthetheoryin[7,17]to ourframework,model\u00ading \nconstraint executions as the application of functions on stores: let fc :S . S be the function computed \nby a con\u00adstraint c . Cons, where fc(s)= s ' if(s,c).c s ' .A store s . S is a FIXPOINT for fc if fc(s)= \ns. To simplify the discussion, in this section we assume that constraints only operate on reactive cells \nand focus our attention on stores where all locations are reactive. The de.nition of in.ation\u00adary functions \nassumes that a partial ordering is de.ned on the set of stores S: DEFINITION 1 (INFLATIONARY FUNCTIONS). \nLet (S,j) be any partial ordering over the set of stores S and let f : S . S be a function on S. We say \nthat f is in.ationary if s jf(s)for all s . S. Arelevantpropertyofpartial orderingsin ourcontextisthe \n.nite chain property: DEFINITION 2 (FINITE CHAIN PROPERTY). A partial order\u00ading(S, j)over Ssatis.esthe \nFINITE CHAIN PROPERTY ifev\u00adery non-decreasing sequence of elements s0 j s1 j s2 j ... fromSeventually \nstabilizes at some element s inS,i.e., if there exists j= 0such that si = s for all i = j. To describe \nthe store modi.cations due to the execution of the solver, we use the notion of iteration of functions \non stores. Let F = {f1 ,...,fn}, (a1 ,...,ak). [1,n]k , and s . S be a .nite set of functions on S, a \nsequence of indices in [1,n], and an initial store, respectively. An iteration of functions of F starting \nat s is a sequence of stores (s0 ,s1 ,s2 ,...)where s0 = s and si = fai (si-1 ) for i> 0. We say that \nfunction fai is activated at step i. Iterations of functions that lead to a .xed point are called regular: \nDEFINITION 3 (REGULAR FUNCTION ITERATION). A func\u00adtion iteration (s0 ,s1 ,s2 ,...)is REGULAR if it satis.es \nthe following property: for all f . F and i = 0, if si is nota .xpointfor f, then f is activated at some \nstep j>i. Using arguments from Chapter 7 of [7], it can be proved that any regulariteration ofin.ationaryfunctionsstartingat \nsome initial store stabilizes in a .nite number of steps to a common .xpoint. A relevant convergence \nproperty of our solveris thefollowing: THEOREM 1. Let C = {c1 ,...,ch} be any set of con\u00adstraints, let \nF = {fc1 ,...,fch }be the functions computed by constraints in C, and let (S,j)be any partial ordering \nover S satisfying the .nite chain property. If functions in F are in.ationary on S and {f . F |f(s)= \ns}. S . F, then (.,s,D,S).s (s ' ,D ')and s ' is a common .xpoint of the functionsin F such that s js \n' . If functions in Theorem 1 are also monotonic (i.e.,s j s ' implies f(s) j f(s ' ) for all s,s ' . \nS), then the solver always converges to the least common .xpoint[7],yielding deterministicresultsindependentlyofthescheduling \norder. 2.6 Cyclic vs.AcyclicSystems ofConstraints Itis possibletoprovethattheclassofin.ationaryconstraints \ndiscussedinTheorem1includes anyprogramthat can be de\u00ad insert(u,v,w): E := E .{(u,v)} w(u,v) := w newcons( \nifd[u]+ w(u,v) <d[v] then d[v] := d[u]+ w(u,v)) | {z } cuv decrease(u,v,d): w(u,v) := w(u,v) -d Figure \n5. Incremental shortestpath updatesinamixedim\u00adperative/data.ow style. scribed in terms of an acyclic \ndata.ow graph, such as com\u00adputational circuits[6], non-circularattributegrammars[37], andspreadsheets[36].Hence,if \nasystem ofconstraintsis acyclic, then our solver always converges to the correct re\u00adsult, without the \nneed for programmers to prove any stabi\u00adlization properties oftheirconstraints.Thisisthemostcom\u00admoncaseinmany \napplications [6, 19, 34],andseveralef\u00ad.cient techniques can be adopted by constraint solvers to automatically \ndetect cycles introduced by programming er\u00adrors[46]. In addition to the acyclic case, our abstract machine \ncan also handle the most general case of cyclic constraints em\u00adbedded within an imperative program. This \nopens up the possibility to address problems that would not be solvable usingacyclic data.ow graphs.Weexemplifythisconceptby \nconsidering the well known algorithmic problem of main\u00adtaining distances in a graph subject to local \nchanges to its nodes oredges.Intheremainderofthissectionweshowhow to specify an incremental variant of \nthe classical Bellman\u00adFord ssingle-sourceshortestpathalgorithm[8]interms ofa (possiblycyclic) system \nof one-way constraints. Compared to purelyimperativespeci.cations[20],theformulationof theincremental \nalgorithmin ourmixedimperative/data.ow framework is surprisingly simple and requires just a few lines \nof code. Example. LetG =(V,E,w)be a directedgraph with real edge weights w(u,v), andlet s be a source \nnode in V.The incremental shortest path problem consists of updating the distances d[u]of all nodes u \n. V from the source s afterin\u00adsertingany newedgeinthegraph, ordecreasingtheweight of any existing edge. \nTo solve this problem in our frame\u00adwork,wekeep edgeweightsand distancesinreactivemem\u00adory.Assumingtostartfromagraphwithnoedges,weinitial\u00adize \nd[s]:= 0and d[u]:= +8for all u = s.The pseudocode of update operationsthatinsert a newedgeanddecreasethe \nweight of an existing edge by a positive amount d are shown in Figure 5. Operation insert(u,v,w)adds \nedge (u,v)to the graph with weight w and creates a new constraint cuv for the edge:cuv simply relaxes \nthe edgeifBellman sinequality d[u]+ w(u,v) = d[v]is violated[8].Theconstraintisim\u00admediately executed \nafter creation (see rule NEWCONS-N1 in Figure 4) and the three pairs (d[u],cuv), (d[v],cuv), and (w(u,v),cuv)are \nadded to the set of dependencies D. Any later change to d[u], d[v]or w(u,v), which may violate the inequalityd[u]+w(u,v)= \nd[v], will causethe re-execution of cuv. Decreasing the weight of an existing edge (u,v)by any positive \nconstant d with decrease(u,v,d)can be done byjust updating w(u,v). In view of rule ASGN-N2 of Fig\u00adure \n4, the system reacts to the change and automatically re\u00adexecutes cuv and any other affected constraints. \n Using the machinery developed in Section 2.5 and suit\u00adably de.ning a partial order on S and an appropriate \npick function, it can be proved that our solver .nds a correct so\u00adlutionwithinthebest knownworst-casetime \nboundsforthe problem.InSection6.3 wewill analyzeexperimentally our constraint-based approach showingthatin \npracticeit can be orders of magnitude faster than recomputing from scratch, even when all weights are \nnon-negative. 2.7 GlitchAvoidance The performances of our solver depend on the ability to minimize glitches \n[15], i.e., redundant evaluations of con\u00adstraints. In the case of acyclic systems, glitch freedom can \nbeachievedbycreatingconstraintsintopological orderand by letting the pick function return the least recently \ncreat\u00aded/executed constraint.For cyclic systems, glitch avoidance is problem-speci.c, and relies on the \nde.nition of appropri\u00adate pick functions. In some cases, glitches may be entailed by algorithmic properties \nof the program at hand, and there\u00adforemaybe unavoidable.Forinstance,glitchesmay arisein theshortest pathsexample \ndiscussedinSection 2.6if multi\u00adpleedgeinsertionsareperformed asa batchinsideanatomic block.Thisisaconsequence \nofthefactthateach edgemay be relaxed up to |V| times by Bellman-Ford s algorithm. De.ning a pick functionthat \nguaranteestheglitchfreedom property in this situation would improve the long-standing O(|V|\u00b7|E|)time \nbound of the shortest paths problem with arbitrary edge weights. 3. EmbodimentintoC/C++ In this section \nwe show how to apply the concepts devel\u00adoped in Section 2 to the C and C++ languages, deriving an extension \nthat we call DC. DC has exactly the same syntax as C/C++, but operations that read or modify objects \nhave a different semantics. All other primitives, including creat\u00ading/deleting constraints, allocating/deallocating \nreactive ob\u00adjects, and opening/closing atomic blocks, are provided as runtimelibraryfunctions1(seeFigure \n6). Reactive Memory Allocation. Similarlyto otherautomatic changepropagationapproaches(e.g.,[5,41]),inDC \nall ob\u00adjects allocatedstatically or dynamically are non-reactive by default. Reactive locations are allocated \ndynamically using libraryfunctions rmalloc and rfree, which workjustlike malloc and free,but on a separate \nheap. 1A detailed documentation of the DC application programming in\u00adterface, including stricter library \nnaming conventions and several ad\u00additional features not covered in this paper, is available at the URL: \nhttp://www.dis.uniroma1.it/~demetres/dc/ typedef void (*cons_t)(void*); int newcons(cons_t cons, void* \nparam); void delcons(int cons_id); void* rmalloc(size_t size); void rfree(void* ptr); void begin_at(); \nvoid end_at(); void arm_final(int cons_id, cons_t final); void set_comp(int (*comp)(void*, void*)); \nFigure 6. Mainfunctions oftheDClanguageextension. Opening and Closing Atomic Blocks. Atomic blocks are \nsupportedinDC usingtwolibrary functions begin at and end at. Calling begin at opens an atomic block, \nwhich should be closed with a matching call to end at. Nested atomic blocks are allowed, and are handled \nusing a counter of nestinglevelssothatthesolveris only resumed attheend of the outer block, processing \nany pending constraints that need to be .rst executed or brought up to date as a result of the block \ns execution. Creating and Deleting Constraints. For the sake of sim\u00adplicity, in Section 2 constraints \nhave been modeled as ordi\u00adnary statements. DC takes a more .exible approach: con\u00adstraints are speci.ed \nas closures formed by a function that carries out the computation and a user-de.ned parame\u00adter to be \npassed to the function. Different constraints may therefore share the same function code, but have differ\u00adent \nuser-de.ned parameters. New constraint instances can be created by calling newcons, which takes as parameters \na pointer cons to a function and a user-de.ned parame\u00adter param. When invoked in non-atomic normal execution \nmode, newcons executes immediately function cons with parameter param, and logs all dependencies between \nthe created constraintandthereactivelocationsread duringthe execution. If a constraint is created inside \nan atomic block (orinsideanotherconstraint),its .rstevaluationis deferred until theend oftheexecution \nofthecurrent block(orcon\u00adstraint). All subsequent re-executions of the constraint trig\u00adgered by modi.cations \nof the reactive cells it depends on will be performed with the same value of param speci.ed at the creation \ntime. newcons returns a unique id for the created constraint, which can be passed to delcons. Reading \nand Modifying Objects. Reading and modifying objectsinreactivememory canbe doneinDCbyevaluating ordinary \nC/C++ expressions. We remark that no syntax ex\u00adtensions orexplicit macro/functioninvocationsarerequired. \nCustomizing the Scheduler. Differently from other ap\u00adproaches [41], DC allows programmers to customize \nthe execution order of scheduled constraints. While the default pick function of DC (which gives higher \npriority to least recently executed constraints) works just .ne in practice for a large class of problems \n(see Section 6), the ability to replace it can play an important role for some speci.c problems. DC provides \na function set comp that installs a struct robject { void* operator new(size_t size) { return rmalloc(size); \n} void operator delete(void* ptr) { rfree(ptr); } }; static void con_h(void*), fin_h(void*); class rcons \n{ int id; public: virtual void cons() = 0; virtual void final() {} rcons() { id = -1; } ~rcons() { disable(); \n} void enable() { if (id == -1) id = newcons(con_h, this); } void disable() { if (id != -1) { delcons(id); \nid = -1; } } void arm_final() { if (id != -1) arm_final(id, fin_h); } void unarm_final() { if (id != \n-1) arm_final(id, NULL); } }; void con_h(void* p) { ((rcons*)p)->cons(); } void fin_h(void* p) { ((rcons*)p)->final(); \n} Figure 7. C++ wrapping ofDCprimitives. user-de.ned comparator to determine the relative priorityof \ntwo constraints. The comparator receives as arguments the user-de.nedparametersassociated withtheconstraintsto \nbe compared. Final Handlers. AnadditionalfeatureofDC,built ontop of the core constraint handling mechanisms \ndescribed in Section 4, is the ability to perform some .nalization oper\u00adations only when the results \nof constraint evaluations are stable, i.e., when the solver has found a common .xpoint. For instance, \na constraint computing the attribute of a wid\u00adgetinagraphic userinterfacemay alsoupdatethescreenby calling \ndrawing primitives oftheGUI toolkit:if aredrawing occurs at each constraint execution, this may cause \nunnec\u00adessary screen updates and .ickering effects. Another usage example ofthisfeaturewill begiveninSection \n4.3. DC allows users to specify portions of code for a con\u00adstraintto beexecuted as .nal actions just \nbeforeresuming the underlyingimperativeprograminterruptedbythesolver activation.Thiscanbe donebycallingfunction \narm final during constraint solving: the operation schedules a .nal handler to be executed at the end \nof the current solving ses\u00adsion.Thefunctiontakes as parametersa constraintid and a pointer to a .nal \nhandler, or NULL to cancel a previous re\u00adquest. A .nal handler receives the same parameter as the constraint \nit is associated to, but no dependencies from re\u00adactive locations are logged during its execution. All \n.nal handlers are executed in normal execution mode inside an atomic block. C++ Wrapping of DC Primitives. \nThe examplesin the re\u00admainder of this paper are based on a simple C++ wrapping of the DC primitives, \nshown in Figure 7. We abstract the concepts of reactive object and constraint using two classes: robject \nand rcons.Theformerisa baseclassfor objects stored in reactive memory. This is achieved by overloading \nthe new and delete operators in terms of the correspond\u00ading DC primitives rmalloc and rfree, so that \nall member variablesofthe objectarereactive.Class rcons is a virtual baseclassfor objectsrepresentingdata.owconstraints.The \nclassprovidesa purevirtualfunctioncalled cons, to be de\u00ad.nedinsubclasses,whichprovidesthe usercodeforacon\u00adstraint. \nAn additional empty final function can be option\u00adally overridden in subclasses to de.ne the .nalization \ncode for a constraint. The class also provides functions enable and disable to activate/deactivate the \nconstraint associated withthe object,andfunctions arm final and unarm final toschedule/unscheduletheexecution \nof .nalhandlers. 4. ApplicationsandProgrammingExamples In this section, we discuss how DC can improve \nC/C++ programmability in three relevant application scenarios. To the best of our knowledge, these applications \nhave not been exploredbeforeinthecontext of data.owprogramming.All the code we showis real. 4.1 IncrementalComputation \nInmanyapplications,inputdataissubjecttocontinuous up\u00addatesthat need tobeprocessed ef.ciently.Forinstance,ina \nnetworkingscenario,routersmustreact quickly tolinkfail\u00aduresby updatingroutingtablesin ordertominimizecommu\u00adnication \ndelays.Whentheinputissubjecttosmall changes, aprogrammayincrementally.xonly theportionofthe out\u00adput affected \nby the update, without having to recompute the entire solutionfrom scratch.For manyproblems, ef.cient \nad hoc algorithmsare knownthat can updatethe outputasymp\u00adtotically faster that recomputing from scratch, \ndelivering in practice speedups of several orders of magnitude[19, 21]. Such dynamic algorithms, however, \nare typicallydif.cult to design and implement, even for problems that are easy to be solvedfrom-scratch.Alanguage-centric \napproach, which was extensively explored in both functional and imperative programminglanguages,consists \nofautomatically turninga conventional staticalgorithmintoanincremental one,byse\u00adlectivelyrecomputingthe \nportionsofacomputationaffected byan updateoftheinput.Thispowerfultechnique, knownas self-adjustingcomputation[3,5], \nprovidesa principled way of deriving ef.cient incremental code for several problems. We nowshowthatdata.owconstraintscanprovideaneffec\u00adtive \nalternative for specifying incremental programs. Later in this section we discuss differences and similarities \nof the two approaches. Example. Toput ourapproachintotheperspectiveofpre\u00advious work on self-adjusting \ncomputation, we revisit the problem of incremental re-evaluation of binary expression trees discussedin[30].This \nproblemisaspecial case ofthe circuit evaluation: input values are stored at the leaves and thevalueofeachinternal \nnodeisdeterminedbyapplyinga binary operator(e.g.,sum orproduct) onthevaluesofits children. The .nal result \nof the evaluation is stored at the root. We start from the conventional node structure that a programmerwould \nuseforabinary expressiontree,contain\u00adingthetypeofthe operationcomputedatthe node(only rel\u00ad template<typename \nT> struct node : robject, rcons { enum op_t { SUM, PROD }; T val; op_t op; node *left, *right; node(T \nv): val(v), left(NULL), right(NULL) { enable(); } node(op_t o): op(o), left(NULL), right(NULL) { enable(); \n} void cons() { if (left == NULL || right == NULL) return; switch (op) { case SUM: val = left->val + \nright->val; break; case PROD: val = left->val * right->val; break; } } }; Figure 8. Incremental evaluation \nof expression trees. evant for internal nodes), the node s value, and the pointers tothe subtrees.OurDC-based \nsolution(seeFigure 8) sim\u00adply extends the node declaration by letting it inherit from classes robject \nand rcons, and by providing the code of a constraintthat computesthevalueofthe nodeintermsofthe values \nstored atits children.Everything elseis exactly what theprogrammerwouldhave doneanyway tobuild theinput \ndatastructure.Anexpressiontreecan beconstructedbyjust creating nodesandconnectingtheminthe usual way: \nnode<int> *root = new node<int>(node<int>::SUM); root->left = new node<int>(10); root->right = new node<int>(node<int>::PROD); \nroot->right->left = new node<int>(2); root->right->right = new node<int>(6); The example above creates \nthe tree showninFigure9(left). Since all .elds of the node are reactive and each node is equipped with \na constraint that computes its value, at any time during the tree construction, root->value con\u00adtains \nthe correct result of the expression evaluation. We remark that this value not only is given for free \nwithout the need to compute it explicitly by traversing the tree, but is also updated automatically after \nany change of the tree. For instance, changing the value of the rightmost leaf with root->right->right->val \n= 3 triggers the propaga\u00adtionchainshowninFigure9(right).Other possible updates that would be automatically \npropagated include changing the operation type of a node or even adding/removing entire subtrees. Notice \nthat a single change to a node may trigger the re-execution of the constraints attached to all its ances\u00adtors, \nso the total worst-case time per update is O(h), where h is the height of the tree. For a balanced expression \ntree, this is exponentially faster than recomputing from scratch. If a batch of changes are to be performed \nand only the .nal value ofthetreeis ofinterest, performancecan beimproved by grouping updates with begin \nat() and end at() so that the re-execution of constraints is deferred until the end of the batch, e.g.: \nbegin_at(); // put the solver to sleep root->op = node<int>::SUM; // change node operation type delete \nroot->right->left // delete leaf ... // etc... end_at(); // wake up the solver  Figure 9. Reactiveexpressiontree(left) \nandchangepropagationchainafteraleaf value update(right). Discussion. DC and imperative self-adjusting \ncomputa\u00adtionlanguagessuch asCEAL[30] sharethe basicidea of change propagation, and reactive memory is \nvery similar to CEAL smodi.ables.However,thetwoapproaches differin a numberofimportantaspects.InCEAL,thesolutionisini\u00adtially \ncomputedbyacorecomponentandlater updatedbya mutator, whichperforms changestotheinput.InDCthereis noexplicit \ndistinction betweenaninitial runandasequence of updates,andinparticularthereis nostaticalgorithmthat \nisautomaticallydynamized.Instead,programmersexplicitly break down the solution of a complex problem into \na col\u00adlection of reactive code fragments that locally update small portionsoftheprogramstateasafunctionofother \nportions. This implies a paradigm shift that may be less straightfor\u00adward for the average programmer \nthan writing static algo\u00adrithms,butit canmakeit easiertoexploit speci.c properties of the problem at \nhand, which can be crucial for coding al\u00adgorithms provablyfasterthanrecomputingfromscratch.  4.2 Implementing \ntheObserverDesignPattern As a second example, we show how the reactive nature of our framework can be \nnaturally exploited to implement the observer software design pattern. A common issue arising from partitioning \na system into a collection of cooperating software modules is the need to maintain consistency be\u00adtween \nrelated objects. In general, a tight coupling of the involved software components is not desirable, as \nit would reduce their reusability. For example, graphical user inter\u00adface toolkits almost invariably \nseparate presentational as\u00adpectsfromthe underlyingapplicationdatamanagement,al\u00adlowing data processing \nand data presentation modules to be reusedindependently.The observer software design pat\u00adtern [14] answers \nthe above concerns by de.ning one-to\u00admany dependencies between objects so that when one ob\u00adject(the subject)changesstate,allits \ndependents(the ob\u00adservers)areautomatically noti.ed.Akey aspectisthat sub\u00adjects send out noti.cations \nof their change of state, without having to know who their observers are, while any number of observers \ncan be subscribed to receive these noti.cations (subjectsand observersaretherefore nottightlycoupled).A \nwidely deployed embodiment of this pattern is provided by the Qt application developmentframework[26]. \n Qt is based on a signal-slot communication mechanism: asignalisemitted whenaparticularevent occurs,whereasa \nslotisafunctionthatiscalledinresponsetoa particularsig\u00adnal. An object acting as a subject emits signals \nin response to changes of its state by explicitly calling a special mem\u00adber function designated as a \nsignal. Observers and subjects can be explicitly connected so that any signal emitted by a subjecttriggerstheinvocationof \noneormoreobserverslots. Programmers can connect as many signals as they want to a single slot, and a \nsignal can be connected to as many slots as they need. Since the connection is set up externally af\u00adter \ncreating the objects, this approach allows objects to be unaware of the existence of each other, enhancing \ninforma\u00adtion encapsulation and reuse of software components. Sub\u00adjects and observers can be created inQt \nas instances of the QObject base class. Qt s signal-slot infrastructure hinges uponanextensionoftheC++languagewith \nthree newkey\u00adwords: signal and slot, to designate functions as signals or slots, and emit, to generate \nsignals. A Minimal Example: Qt vs. DC. Toillustratethe concepts discussed above and compare Qt andDC \nas toolsforimple\u00admenting the observer pattern, we consider a minimal exam\u00adple excerptedfromthe Qt 4.6 \nreference documentation.The goalistoset upa programinwhich twocountervariables a and b are connected \ntogether so that the value of b is auto\u00admatically kept consistent with the value of a. The example startswith \nthesimpledeclarationshowninFigure 10(a)(all excepttheframed box),which encapsulatesthecounterinto an \nobject with member functions value/setValue for ac\u00adcessing/modifyingit.Figure 10(b)shows howthe Counter \nclass can be modi.ed in Qt so that counter modi.cations can be automatically propagated to other objects \nas pre\u00adscribedbythe observerpattern.Firstofall,theclassinherits from Qt s QObject base class and starts \nwith the Q OBJECT class Counter : public robject { public: Counter() { m_value = 0; } int value() const \n{ return m_value; } void setValue(int value) { m_value = value; } private: int m_value; }; (a) Acounter \nclass and its DC observer pattern version (framed box). class Counter : public QObject { Q_OBJECT public: \nCounter() { m_value = 0; } int value() const { return m_value; } public slots: void setValue(int value); \nsignals: void valueChanged(int newValue); private: int m_value; }; void Counter::setValue(int value) \n{ if (value != m_value) { m_value = value; emit valueChanged(value); } } (b)Qt observer pattern version \nof the counter class. Figure 10. Observer pattern example excerptedfromtheQt   4.6 reference documentation:DC \nvs. Qt implementation. macro. Function setValue is declared as a slot and it is augmented by calling \nexplicitly the valueChanged signal with the emit keyword every time an actual change occurs. SinceQt \nCounter objectscontain both signal andslotfunc\u00adtionsthey canact both assubjectsandas observers.Thefol\u00adlowing \ncode snippetshows how two counters can be created and connected so that each change to the former triggers \na change ofthelatter: Counter *a = new Counter, *b = new Counter; QObject::connect(a, SIGNAL(valueChanged(int)), \nb, SLOT(setValue(int))); a->setValue(12); // a->value() == 12, b->value() == 12 b->setValue(48); // a->value() \n== 12, b->value() == 48 The QObject::connect callinstallsaconnection between counters a and b: everytime \nemit valueChanged(value) isissuedbya with a given actualparameter, setValue(int value) isautomaticallyinvokedon \nb with the same param\u00adeter. Therefore, the call a->setValue(12) has as a side\u00adeffect that the value of \nb is also set to 12. Conversely, the call b->setValue(48) entails no change of a as no con\u00adnection existsfrom \nb to a. The same result can be achieved in DC by just letting the Counter class ofFigure 10(a)inheritfromthe \nrobject base class ofFigure 7.As a result,the m value member vari\u00adableis storedin reactive memory.The \nprescribed connection between reactive counters can be enforced with a one-way data.ow constraint that \nsimply assigns the value of b equal to the value of a: Counter *a = new Counter, *b = new Counter; struct \nC : rcons { Counter *a, *b; C(Counter *a, Counter *b) : a(a), b(b) { enable(); } void cons() { b->setValue(a->value()); \n} } c(a,b); a->setValue(12); // a->value() == 12, b->value() == 12 b->setValue(48); // a->value() == \n12, b->value() == 48 We notice that the role of the QObject::connect of the Qt implementation is now \nplayed by a data.ow constraint, yielding exactly the same program behavior. Discussion. The example above \nshows that DC s run\u00adtime system handles automatically a number of aspects that wouldhavetobesetupexplicitlybytheprogrammers \nusing Qt s mechanism: there is no need to de.ne slots and signals, relieving programmersfromtheburden \nofextendingthe de.nition ofsubject and observerclasseswith extramachinery(see Figure 10); only actual \nchanges of an object s state trigger propaga\u00adtionevents,soprogrammersdo nothavetomakeexplicit checks \nsuch as in Counter::setValue s de.nition to preventin.niteloopinginthecase ofcyclicconnections (seeFigure \n10(b));  DC does notrequireextensionsofthelanguage,andthus the code does not have to be preprocessed \nbefore being compiled.  We list below further points that make data.ow constraints a .exibleframeworkforsupportingsomeaspects \nofcompo\u00adnent programming, putting it into the perspective of main\u00adstream embodiments of the observer \npattern such as Qt: inDC, onlysubjects needtobereactive,while observers can be ofanyC++ class,even ofthird-partylibraries \ndis\u00adtributedin binary codeform.In Qt, third-partyobservers must be wrapped using classes equipped with \nslots that act as stubs;  relations between Qt objects are speci.edby creating ex\u00adplicitly one-to-one \nsignal-slot connections one at a time; a single DC constraint can enforce simultaneously any arbitrary \nset of many-to-many relations. Furthermore, as the input variables of a data.ow constraint are de\u00adtected \nautomatically, relations may change dynamically depending on the state of some objects;  Qt signal-slot \nconnections let subjects communicate copies of values to their observers; in contrast, DC con\u00adstraintscancomputethevaluesreceivedbythe \nobservers as an arbitrary function of the state of multiple subjects, encapsulating complex update semantics. \n  0 template<class T, class N> class snode : public rcons { 1 map<N**, snode<T,N>*> *m; 2 N *head, \n**tail; 3 snode *next; 4 int refc; 5 public: 6 snode(N *h, N **t, map<N**, snode<T,N>*> *m) : 7 m(m), \nhead(h), tail(t), next(NULL), refc(0) { 8 (*m)[tail] = this; 9 enable(); 10 } 11 ~snode() { 12 m->erase(tail); \n 13 if (next != NULL &#38;&#38; --next->refc == 0) delete next; 14 } 15 void cons() { 16 snode<T,N>* \ncur_next; 17 18 if (*tail != NULL) { 19 typename map<N**, snode<T,N>*>::iterator it = 20 m->find( \n&#38;(*tail)->next ); 21 if (it != m->end()) 22 cur_next = it->second; 23 else cur_next = new snode<T,N>(*tail, \n 24 &#38;(*tail)->next, m); 25 } else cur_next = NULL; 26 27 if (next != cur_next) { 28 if (next \n!= NULL &#38;&#38; --next->refc == 0) 29 next->arm_final(); 30 if (cur_next != NULL &#38;&#38; cur_next->refc++ \n== 0) 31 cur_next->unarm_final(); 32 next = cur_next; 33 } 34 if (head != NULL) T::watch(head); \n35 } 36 void final() { delete this; } 37 }; 38 template<class T, class N> class watcher { 39 snode<T,N> \n*gen; 40 map<N**, snode<T,N>*> m; 41 public: 42 watcher(N** h) { gen = new snode<T,N>(NULL, h, &#38;m); \n} 43 ~watcher() { delete gen; } 44 }; Figure 11. Data structure checking and repair:listwatcher. 4.3 \nDataStructureChecking andRepair Long-living applicationsinevitablyexperience variousforms of damage, \noften due to bugs in the program, which could lead to system crashes or wrong computational results. \nThe ability of a program to perform automatic consistency checks and self-healing operations can greatly \nimprove re\u00adliabilityinsoftware development.One ofthemost common causes of faults is connected with different \nkinds of data structure corruptions, which can be mitigated using data structure repair techniques[24]. \nIn this section, we show how data.ow constraints can be used to check and repair reactive data structures. \nWe ex\u00ademplify this concept by considering the simple problem of repairing a corrupt doubly-linked list \n[38]. We .rst show how to build a generic list watcher, which is able to de\u00adtect any changes to a list \nand perform actions when modi\u00ad.cations occur. This provides an advanced example of DC programming, where \nconstraints are created and destroyed by other constraints. Differently from the expression trees ofSection4.1,whereconstraintsareattributesof \nnodes,the main challenge here is how to let the watched list be com\u00adpletely unaware of the watcher, while \nstill maintaining au\u00adtomatically a constraint for each node. The complete code of the watcher is shown \nin Figure 11. The only assumption ourwatchermakes onlist nodestobemonitored(ofgeneric typeN)isthattheyare \nreactive and contain anext .eldpoint\u00adingtothe successor.The mainideaistomaintaina shadow list ofconstraintsthatmirrorsthewatchedlist(Figure \n12). Shadow nodes are snode objects containing pointers to the monitored nodes (head) and to their next \n.elds (tail). A special shadowgenerator node(gen)is associated to the re\u00adactive variable(list)holding \nthe pointer to the .rst node of the input list. A lookup table(m)maintains a mapping from list nodes \nto the corresponding shadow nodes. The heart of the watcher is the constraint associated with shadow \nnodes (lines 15 35).It.rstchecksifthesuccessorofthemonitored node,if any,isalreadymapped toashadow node(lines \n18 21). If not,itcreatesa newshadow node(line 23). Lines 27 33 handle the case where the successor of \nthe shadow node haschanged andits next .eldhastobe updated.Line 34 calls a user-de.ned watch function(providedbytemplate \nparameter T), whichperforms anydesiredchecks and repairs foraninputlist node.Todisposeofshadow nodeswhenthe \ncorresponding nodes are disconnected from the list, we use a simple reference counting technique, deferring \nto a .nal handlerthetaskofdeallocatingdeadshadow nodes(line36).  Thefollowingcodesnippetshows howtocreateasimple \nrepairer for a doubly-linked list based on the watcher of Figure 11: struct node : robject { int val; \nnode *next, *prev; }; struct myrepairer { static void watch(node* x) { // check if (x->next != NULL &#38;&#38; \nx != x->next->prev) // repair x->next->prev = x; } }; // create reactive list head and repairer node** \nlist = ...; watcher<myrepairer,node> rep(list); // manipulate the list ...  The repairer object rep \nchecks if the invariant property x == x->next->prev issatis.edforallnodesinthelist,and recovers it to \na consistent state if any violation is detected during the execution of the program. We notice that several \ndifferentwatchersmaybecreated tomonitorthesamelist. 5. Implementation In this section we discuss how \nDC can be implemented via a combination of runtime libraries, hardware/operating sys\u00adtem support, and \ndynamic code patching, without requiring any source code preprocessing. The overall architecture of our \nDC implementation, which was developed on a Linux IA-32platform,isshowninFigure 13.At averyhighlevel, \nof f wheretraces are padded withtrailing nop instructions the DC runtime library is strati.ed into two \nmodules: 1) a reactive memory manager, which de.nes the rmalloc and rfree primitives and provides support \nfor tracing accesses to reactive memory locations; 2) a constraint solver, which ' f schedulesand dispatchestheexecution \nofconstraints,keep\u00ading track of dependencies between reactive memory loca\u00adtionsandconstraints.Westart \nourdescriptionbydiscussing how to support reactive memory, which is the backbone of the whole architecture. \n 5.1 ReactiveMemory Takinginspirationfromtransactional memories[1], weim\u00adplemented reactive memory using \noff-the-shelf memorypro\u00adtection hardware. Our key technique uses access violations sothatthesmallesttraceisatleast \n5-byteslong. Shadow Memory and Address Redirecting. To avoid ex\u00adpensive un-protect and re-protect page \noperations at each ac\u00adcess to reactive memory, we mirror reactive memory pages withunprotected shadow \npages thatcontainthe actualdata.Theshadowmemory regioniskept 32 2 under the control of our reactive \nmemory allo\u00ad cator, which maps it onto the swap space with Reactive memory Stack mmaps Shadow memory \nReactive memory info BSS Heap Data Text kernel space the mmap system call. Any access to a reactive \n(AV) combined with dynamic binary code patching as a object is transparently redirected to the corre\u00adbasic \nmechanism to trace read/write operations to reactive sponding object in the shadow memory. As a memorylocations. \nresult, memory locations at addresses within 30 fixed 2 offset Access Violations and Dynamic Code Patching. \nReactive the reactive memory region are never actually 30 2 + memoryiskeptin a protected region of the \naddress space so that any read/write access to a reactive object raises an AV. Sinceaccessviolationhandlingisveryinef.cient,we \nuseit just to incrementallydetect instructions that access reactive memory.When aninstruction x .rsttriesto \naccess a reactive location, a segmentation fault with offending instruction x is raised. In the SIGSEGV \nhandler, we patch the trace t containing x byoverwritingitsinitial5 byteswith ajump to a dynamically \nrecompiled trace t ' derived from t, which is placed in a code cache. In trace t ' , x is instrumented \nwith additional inline code that accesses reactive locations withoutgeneratingAVs,and possiblyactivatestheconstraint \nsolver. Trace t ' ends with a jump that leads back to the end of t so that control .ow can continue normally \nin the original code. Since t ' may contain several memory access instructions, it is re-generated every \ntime a new instruction that accesses reactive memory is discovered. To identify traces in the code, we \nanalyze statically the binary code whenitisloaded and we construct alookup table that maps 2 read orwrittenbytheprogram.Toavoid \nwast-31 ing memory without actually accessing it, re\u00adactive memory can be placed within the Ker\u00adnel space, \nlocated in the upper 1GB of the address space on 32-bit Linux machines with 31 2 the classical 3/1 virtual \naddress split. Kernel space is .agged in the page tables as exclu\u00adsivetoprivileged code(ring2 orlower),thus \nan AV is triggered if a user-mode instruction tries to touch it. More recent 64-bit platforms offer even \nmore .exibility to accomodate reac\u00adtive memoryin protected regions ofthe address space.Weletthe reactivememory \nregion start brk at address 230 +231 = 0xC000000 and grow upwardasmorespaceis needed(seethe .g\u00adure on \nthe right). The shadow memory region starts at address 231 = 0x8000000 and grows upward, eventually hitting \nthe memory map\u00ad 0 ping segment used by Linux to keep dynamic the address of each memory access instruction \nto the trace libraries, anonymous mappings, etc. Any reactive object at containingit.Tohandlethecaseswhereatraceinafunction \naddress x is mirrored by a shadow object at address x -d, 230 f is smaller than 5 bytes and thus cannot \nbe patched, we where d == 0x4000000 is a .xed offset. This makes overwrite the beginning of f with ajump \ntoa newversion addressredirectingvery ef.cient.  5.2 ConstraintSolver Our implementation aggregates \nreactive locations in 4-byte words aligned at 32 bit boundaries. The solver is activated everytime such \na wordis readin constraint execution mode, or its value is modi.ed by a write operation. The main involvedunitsare(seeFigure \n13): 1. A dispatcher that executes constraints, maintaining a global timestamp that grows by one at each \nconstraint execution.Foreach constraint,wekeep thetimestampof itslatestexecution. 2. A memory accesslogger \nthat maintains the set of depen\u00addencies D and a list W of all reactive memory words written by the execution \nof the current constraint cself , along with their initial values before the execution. To avoid logging \ninformation about the same word multi\u00adpletimes duringtheexecutionofaconstraint,thelogger stamps each \nword with the time of the latest constraint execution that accessed it. Information is logged only if \ntheaccessed word hasatimestampolderthanthecurrent global timestamp, which can only happen once for any \nconstraint execution.To represent D,theloggerkeepsfor each word v theaddressofthehead nodeofalinkedlist \ncontainingtheid sofconstraintsdepending upon v. 3. A constraint scheduler that maintains the set of \nsched\u00aduledconstraints S.Bydefault Sis a priorityqueue, where the priority of a constraint is given by \nthe timestamp of its latest execution: the scheduler repeatedly picks and letsthe dispatcherexecutetheconstraintwiththe \nhighest priority, until S gets empty. Upon completion of a con\u00adstraint s execution, words are scanned \nand removedfrom  W: for each v . W whose value has changed since the beginning of the execution, the \nconstraint id s in the list of nodes associated with v are added to S,if not already there. Nodes of \nthe linked lists that represent D and data struc\u00adtures S and W arekeptin contiguous chunks allocated \nwith malloc. To support direct lookup, timestamps and depen\u00addency list heads for reactive memory words \nare stored in a contiguous reactive memoryinfo regionthat starts at address 231 = 0x8000000 and grows \ndownward, eventually hitting the heap s brk. A critical aspect is how to clean up old dependencies in \nD when a constraint is re-evaluated. To solve the problem ef.ciently in constant amortized time per list \noperation, we keep for each node its insertion time into the linked list. We say that a node is stale \nif its timestamp is older than the timestamp of the constraint it refers to, and up to date otherwise. \nOur solver uses a lazy approach and disposes of stale nodes only whentheword they refertoismodi.ed and \nthelinkedlistis traversedto add constraintsto S.To prevent the numberofstale nodesfromgrowingtoolarge,weusean \nincrementalgarbage collection technique. 6. Experimental Evaluation In this section we present an experimental \nanalysis of the performances ofDCinavarietyof differentsettings,show\u00adingthat ourimplementationiseffectiveinpractice. \n6.1 BenchmarkSuite WehaveevaluatedDC onasetofbenchmarksthatincludes a variety of problems on lists, grids, \ntrees, and graphs, as well asfull and event-intensiveinteractive applications. Linked Lists. We considered \nseveralfundamental primitives on linear linked data structures, which provide a variety of data manipulation \npatterns. Our benchmarks include data structures for: computing the sum of the elements in a list (adder), \n.ltering the items of a list according to a given function (filter), randomly assigning each element \nof a listto one oftwo outputlists(halver), mapping the items of a list onto new values according to a \ngiven mapping function (mapper), merging two sorted lists into a single sorted outputlist(merger), producing \na sorted version of an input list(msorter), producing a reversedversion of an input list (reverser), \nand splitting a list into two output lists, each containing only elements smaller or, respectively, greaterthan \na given pivot(splitter). All benchmarks are subjectto operationsthatadd orremove nodesfromtheinput lists. \nGraphs and Trees. Benchmarks in this class include classi\u00adcal algorithmic problems for routing in networks \nand tree computations: sp: given a weighteddirected graph and a source node s, computesthedistancesofallgraph \nnodesfrom s.Graph edges are subject to edge weight decreases.  exptrees:computesthevalue ofanexpressiontreesub\u00adject \nto operations that change leaf values or operators computedbyinternal nodes(seeSection4.1).  InteractiveApplications. \nWeconsidered bothfull real appli\u00adcations and synthetic worst-case scenarios,including: othello: full \napplication that implements the well\u00adknown board game in which two players in turn place coloredpieces \non a square board, with the goalof revers\u00ading as manyof their opponent s pieces as possible; buttongrid: \nevent-intensive graphic user interface ap\u00adplication with a window containing n \u00d7 n push buttons embedded \nin a grid layout. This is an extreme arti.cial scenario in which many events are generated, since a quadratic \nnumberofbuttons needtoberesized andrepo\u00adsitionedtomaintainthe prescribedlayoutat eachinterac\u00adtive resize \nevent. Some benchmarks, suchas sp, are very computationallyde\u00admanding. For all these benchmarks we have \nconsidered an implementation based on DC, obtained by making the base data structures(e.g.,theinputlist) \nreactive, and a conven\u00adTable 1. Performanceevaluation ofDC versusCEAL,foracommonset of benchmarks.Inputsizeis \nn =1,000,000 for all tests except msorter,for which n = 100,000. From-scratch time (secs) Propagation \ntime (msecs) Mem peak usage (Mbytes) DC statistics Benchmark conv dc ceal dc conv ceal conv ceal dc dc \nceal ceal dc dc ceal ceal dc avg cons per update instr time patched instr adder 0.10 1.44 1.40 14.40 \n14.00 0.97 0.68 85.80 126.17 211.54 232.87 1.10 1.5 0.030 26 exptrees 0.14 1.02 1.07 7.28 7.64 1.04 4.11 \n5.46 1.32 143.30 225.32 1.57 15.6 0.028 72 filter 0.19 2.08 1.11 10.94 5.84 0.53 0.63 2.49 3.95 265.78 \n189.47 0.71 0.5 0.032 39 halver 0.20 2.08 1.33 10.40 6.65 0.63 0.61 3.95 6.47 269.10 218.22 0.81 0.5 \n0.030 38 mapper 0.19 2.04 1.30 10.73 6.84 0.63 0.61 2.63 4.31 261.53 214.34 0.81 0.5 0.032 39 merger \n0.19 2.12 1.37 11.15 7.21 0.64 0.66 4.43 6.71 284.41 218.21 0.81 0.5 0.031 57 msorter 0.91 5.18 3.91 \n5.69 4.29 0.75 5.55 15.91 2.86 689.59 820.14 1.18 37.6 0.031 75 reverser 0.18 2.04 1.30 11.33 7.22 0.63 \n0.62 2.63 4.24 267.45 214.34 0.80 0.5 0.030 37 splitter 0.18 2.27 1.31 12.61 7.27 0.57 1.54 3.92 2.54 \n344.60 222.34 0.64 1.5 0.031 56 Update times - Mapper benchmark Total update times - Adder benchmark \nPerformance ratio CEAL/DC - Adder benchmark 1000 180 90 DC CEAL conv          CEAL DC    \n  CEAL/DC     160 80 100 140 70 10 1 0.1 0.01 Milliseconds 120 60 Seconds Ratio 50 40 100 80 60 \n30 40 20 0.001 20 10 0.0001 0 0 0.00010.001 0.01 0.1 1 10 100 1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 \n9 10 Percentage of input changed Number of nodes x 100000 Number of nodes x 100000 (a) (b) (c) Figure \n14. (a)Changepropagationtimes onthemapper benchmarkforcomplex updateswithinputsize n = 100,000;(b-c) \nperformancecomparisonofthechangepropagationtimesofDC andCEAL onthe adder benchmark. tionalimplementationinCbased \non non-reactivedatastruc\u00adtures.Interactive applications (othello and buttongrid) are written in the Qt-4 \nframework: change propagation throughouttheGUIisimplemented either usingconstraints (DCversions), or \nusing the standard signal-slot mechanism providedby Qt (conventionalversions).To assessthe perfor\u00admancesofDC \nagainst competitorsthat can quickly respond to input changes, we have also considered highly tuned ad-hoc \ndynamicalgorithms[20, 44]andincremental solu\u00adtions realizedin CEAL[30], a state-of-the-artC-basedlan\u00adguage \nfor self-adjusting computation. Benchmarks in com\u00admon with CEAL are adder, exptrees, filter, halver, \nmapper, merger, msorter, reverser, and splitter.For thesebenchmarks,wehave used theoptimizedimplementa\u00adtions \nprovidedbyHammer et al. [30].  6.2 PerformanceMetrics andExperimentalSetup We tested our benchmarks \nboth on synthetic and on real test sets, considering a varietyof performance metrics: Running times: \nwe measured the time required to initialize the datastructureswith theinput data(from-scratch execu\u00adtion), \nthe time required by change propagation, and binary codeinstrumentationtime.All reportedtimes are wall-clock \ntimes, averaged over three independent trials. Times were measured with gettimeofday(), turning off \nanyother pro\u00ad cessesrunninginthe background. Memory usage: we computed the memory peak usage as well \nas a detailed breakdown to assess which components of our implementation take up most memory (constraints, \nshadow memory, reactive memory, stale and non-stale de\u00adpendencies, etc.). DC-related statistics: we \ncollected detailed pro.ling infor\u00admation including counts of patched instructions, stale de\u00adpendencies \ncleanups, allocated/deallocated reactive blocks, created/deleted constraints, constraints executed per \nupdate, and distinct constraints executedper update. AllDCprogramsconsideredinthissection,exceptforsp \nthat will be discussed separately, use the default timestamp\u00adbased comparatorfor constraint scheduling. \nTheexperimentswereperformed onaPC equipped with a 2.10 GHz Intel Core 2 Duo with 3 GB of RAM, running \nLinux Mandriva 2010.1 with Qt 4.6. All programs were compiled with gcc 4.4.3 and optimization .ag -O3. \n 6.3 IncrementalComputation Thereactivenatureof ourmixedimperative/data.owframe\u00adwork makes it a natural \nground for incremental computa\u00adtion. In this section, we present experimental evidence that Graph n \nNY BAY COL FLA 1,070 NW 1,207 NE 1,524 Road network \u00b7 103 m \u00b7 103 264 733 321 800 435 1,057 2,712 2,840 \n3,897 From-scratch time (msec) sq 50.99 59.99 79.98 192.97 236.96 354.94 Propagation time (msec) sp rr \n0.16 0.07 0.15 0.07 0.28 0.17 0.63 0.35 0.87 0.54 0.27 0.16 Speedup sq sp sq rr 318.6 728.4 399.9 857.0 \n285.6 470.4 306.3 551.3 272.3 438.8 1314.5 2218.3 Mem peak usage (Mbytes) sp rr sq 76.75 26.62 26.19 \n84.84 30.21 29.82 108.61 39.09 38.97 251.26 93.42 93.29 270.66 102.15 101.53 350.86 132.85 132.15 Statistics \nsp cons per update rr node scans per update 143.9 143.9 170.6 170.5 378.3 378.2 687.5 687.3 1002.4 1002.3 \n320.2 320.1 Table 2. PerformanceevaluationofDCforincremental routinginUS road networks usingupto1.5 \nmillionconstraints. Incremental routing - comparison  Road network aconstraint-based solutionin ourframework \ncanrespondto input updates very ef.ciently. We .rst show that the prop\u00adagation times are comparable to \nstate of the art automatic change propagation frameworks, such as CEAL [30], and for some problems can \nbe orders of magnitude faster than recomputingfromscratch.Wethenconsiderarouting prob\u00adlem on real road \nnetworks, and compare our DC-based so\u00adlution bothtoaconventionalimplementationandtoahighly optimized \nad hoc dynamic algorithm supporting a class of speci.c update operations. Comparison to CEAL. Table 1 \nsummarizes the outcome of our experimental comparison with the conventional ver\u00adsion and with CEAL for \nall common benchmarks. Input size is n =1,000,000 foralltests(withtheexception of msorter, for which \nn = 100,000), wheren is the length oftheinputlistforthelist-basedbenchmarks,andthe num\u00adber of nodesinthe(balanced)inputtreefor \nexptrees. Ta\u00adble 1 reports from-scratch execution times of both DC and CEAL(comparedto the corresponding \nconventionalimple\u00admentations),average propagationtimesinresponsetosmall changesoftheinput,memory usageandsomeDC \nstats(av\u00aderage number of executed constraints per update, executable instrumentation time, and total \nnumber of patched instruc\u00adtions). The experiments show that our DC implementation performs remarkably \nwell. From-scratch times are on aver\u00adageafactor of 1.4 higher than those of CEAL, while prop\u00adagation \ntimes are smaller by a factor of 4 on average for all tests considered except the adder, yielding large \nspeed\u00adups over complete recalculation. In the case of the adder benchmark, DC leads by a huge margin \nin terms of prop\u00adagation time (see Figure 14a and Figure 14b), which can be attributed to the different \nasymptotic performance of the algorithms handling the change propagation (constant for DC, and logarithmic \nin the input size for the list reduc\u00adtionapproach usedbyCEAL).Weremark thatthelogarith\u00admic bound of self-adjusting \ncomputation could be reduced toconstantby usingatraceableaccumulator[5];however, support for traceable \ndata structures is not yet integrated in CEAL. We alsoinvestigatedhowDCandCEAL scaleinthe case of batches \nof updates that change multiple input items si\u00admultaneously.TheresultsarereportedinFigure 14aforthe representative \nmapper benchmark, showing that the selec\u00adtive recalculations performed by DC and CEAL are faster thanrecomputingfromscratchforchanges \nuptosigni.cant percentages oftheinput. Comparison to ad hoc Incremental Shortest Paths. We now consider \nan application of the shortest path algorithm discussedinSection2.6 toincremental routinginroad net\u00adworks.Weassesstheempiricalperformance \nofaconstraint\u00adbased solution implemented in DC (sp) by comparing it with Goldberg s smart queue implementation \nof Dijkstra s algorithm (sq), a highly-optimized C++ code used as the reference benchmark in the 9th \nDIMACS Implementation Challenge[22], and withan engineered version ofthe ad hoc incremental algorithm \nbyRamalingam andReps(rr)[20, 44].Ourcodesupports update operationsfollowingthehigh\u00adlevel description \ngiven in Figure 5, except that we create one constraint per node, rather than one constraint per edge. \nWe used as input data a suite of US road networks of size up to n =1.5 million nodes and m =3.8 million \nedges derivedfromtheUACensus 2000TIGER/LineFiles[45]. Edge weights are large and represent integer positive \ntravel times.Weperformed oneachgraphasequenceof m/10 ran\u00addom edge weight decreases, obtained by picking \nedges uni\u00ad Total and change propagation times per resize event Total and change propagation times per \nmove (b) Average time per move (msec)  8x8 12x12 16x16 20x20 Grid size (n x n) Board size (n x n) formlyat \nrandom and reducingtheir weights by afactor of 2. Updates that didnot change anydistances were not counted. \nThe results of our experiments are shown in Table 2 and Figure 15. Both sp and rr were initialized with \ndistances computed using sq,hencewereportfrom-scratch time only for this algorithm. Due to the nature \nof the problem, the average number of node distances affected by an update is rathersmall andalmostindependent \nofthesize ofthe graph. Analogously to the incremental algorithm of Ramalingam and Reps, the automatic \nchange propagation strategy used by oursolvertakesfull advantageofthisstronglocality,re\u00adevaluating only \naffected constraints and delivering substan\u00adtial speedups over static solutions in typical scenarios. \nOur DC-basedimplementation yields propagationtimesthatare, onaverage,afactor of 1.85 higher than the \nconventional ad hoc incremental algorithm, but it is less complex, requires fewer lines of code, is fully \ncomposable, and is able to re\u00adspondseamlesslytomultiple datachanges,relievingthepro\u00adgrammer from the \ntask of implementing explicitly change propagation.We alsotested sp withdifferenttypes ofsched\u00adulers.By \ncustomizingthe pick function ofthe defaultprior\u00adity queuescheduler(givinghighestpriority to nodesclosest \nto the source), a noticeable performance improvement has beenachieved(seeFigure 15).Wealsotried asimplestack \nscheduler,which, however,incurred aslowdown ofafactor of4 overthe default scheduler.  6.4 Comparison \ntoQt sSignal-slotMechanism Maintainingrelationsbetweenwidgetsinagraphic userin\u00adterface is one of the \nmost classical applications of data.ow constraints[46].Weassessthe performance ofDCinevent\u00adintensive \ninteractive applications by comparing the DC im\u00adplementations of buttongrid and othello with the con\u00adventional \nversionsbuilt atop Qt s signal-slot mechanism. In buttongrid, each constraint computes the size and position \nof a button in terms of the size and position of adjacent buttons. We considered user interaction sessions \nwith continuous resizing, which induce intensive schedul\u00ading activity along several propagation chains \nin the acyclic data.ow graph.In othello, constraints are attachedto cells ofthegame board(storedinreactivememory)andmain\u00adtain \na mapping between the board and its graphical repre\u00adsentation:inthisway,thegamelogiccanbecompletely un\u00adaware \nof the GUI backend, as prescribed by the observer pattern(seeSection4.2).For bothbenchmarks,weexperi\u00admented \nwith different grid/board sizes. Figure 16 plots the averagetime perresizeevent(buttongrid)and per game \nmove(othello), measuredover 3 independent runs. Both the total time and the change propagation time are \nreported. For all values of n, the performance differences of the DC and Qt conventionalimplementationsare \nnegligibleandthe curves are almost overlapped. Furthermore, the time spent in change propagation is only \na small fraction of the total time,showingthattheoverheadintroducedbyaccess viola\u00adtions handling, instrumentation, \nand scheduling in DC can be largely amortized over the general cost of widget man\u00adagement and event propagation \nin Qt and in its underlying layers. 7. RelatedWork The ability of a program to respond to modi.cations \nof its environment is a feature that has been widely explored in a large variety of settings and along \nrather different research lines. While this section is far from being exhaustive, we discuss some previous \nworks that appear to be more closely related to ours. GUI and Animation Toolkits. Although data.ow pro\u00adgramming \nis a general paradigm, data.ow constraints have gained popularity in the 90 s especially in the creation \nof interactive userinterfaces.Amulet[41]isagraphic userin\u00adterfacetoolkitbased onthedata.owparadigm.Itintegrates \na constraint solver with a prototype-instance object model implemented on top of C++, and is closely \nrelated to our work. Each object, created by making an instance of a pro\u00adtotype object,consists ofasetofproperties(e.g.,appearance \nor position)that arestoredinreactivevariables,called slots. Constraints are created by assigning formulas \nto slots. Val\u00adues of slots are accessed through a Get method that, when invokedfrominside ofaformula,setsupa \ndependencybe\u00adtweenslots.A varietyofapproaches have beentestedbythe developers to solve constraints[46]. \n FRAN (Functional Reactive Animation) provides a re\u00adactive environment for composing multimedia animations \nthroughtemporal modeling[25]: graphicalobjectsinFRAN use time-varying, reactive variables to automatically \nchange their properties, achieving an animation that is function of both events and time. The data-drivenAlphalanguage \nprovidedbytheLeonardo software visualization system[18] allows programmers to specify declarative mappings \nbetween the state of a C pro\u00adgramanda graphical representation ofits datastructures. Reactive Languages. \nThe data.ow model of computation can also be supported directly by programming languages. Most of them \nare visual languages, often used in industrial settings [10], and allow the programmer to directly man\u00adagethedata.owgraphbyvisuallyputtinglinksbetweenthe \nvarious entities. Only a few non-visual languages provide a data.ow environment, mostly for speci.c domains. \nAmong them,Signal[29] andLustre[13]are dedicated to program\u00adming real-time systems found in embedded \nsoftware, and SystemC[28]isasystem-levelspeci.cationand designlan\u00adguage based onC++. Functional Reactive \nProgramming (FRP) is a declara\u00adtive programming model for constructing interactive appli\u00adcations [25, \n42, 47]. FRP offers two kinds of reactive in\u00adputs: behaviors (e.g.,time-continuous variables whose value \nchangesareautomatically propagatedbythelanguage),and events (e.g., potentially in.nite streams of discrete \nevents, each of which triggers additional computations). Early im\u00adplementations ofFRP have beenembeddedinthe \nprogram\u00adminglanguageHaskell[33].FrTime[15] extendsa purely functional subset of PLT Scheme with an instantiation \nof the FRP paradigm, supporting eager evaluation and benign impurities (e.g., imperative commands for \ndrawing, and for creating and varying mutable references). Additionally, in FrTimebehaviorsandeventscanbeturnedinto \noneanother through the use of primitives such as hold and changes.The former takes as input an initial \nvalue and an event stream, and returns a behavior that starts with the initial value and changestothelasteventvalueeverytimea \nneweventoccurs. The latter consumes a behavior and returns an event stream thatemitsthe value ofthe behavior \nwheneveritchanges.The problem ofintegratingFrTime and object-oriented graphics toolkits hasalso beenthe \nobject ofresearch[35].Recently, Meyerovich et al. [40] have introduced Flapjax, a reactive extension \nto the JavaScript language targeted at Web ap\u00adplications, whose approach is mainly informed by FrTime. \nFrapp\u00b4e[16]integratestheFRP model withtheJavaBeans technology,allowingreactive programminginJava. SugarCubes \n[12] and ReactiveML [39] allow reactive programming(inJava andOCAML, respectively) by rely\u00ading not on \noperating system and runtime support, as our ap\u00adproach does, but rather on causality analysis and a custom \ninterpreter/compiler. Both systems, however, track depen\u00addenciesbetweenfunctionalunits,throughthe useofspeci.c \nlanguage constructs, such as events, and explicit commands for generatingandwaitingforevents. Severalother \nsystems withreactive capabilities have been proposedinrecent years,especiallyinthe .eld ofWeb pro\u00adgramming. \nFor a more comprehensive discussion on these systems, we refer the interested reader to the paper by \nMeyerovichet al. [40] and the references therein. Constraint Programming. Data.ow constraints .t within \nthemore general.eldofconstraint programming[7].Terms such as constraint propagation and constraint solving \nhaveoftenbeen usedinpapersrelated todata.owsincethe early developments ofthearea[11, 41, 46].However,the \ntechniques developed so far in data.ow programming are quite distantfromthose appearinginthe constraint \nprogram\u00admingliterature[9].Inconstraint programming,relations be\u00adtweenvariablescan bestatedintheform ofmulti-way \ncon\u00adstraints, typically speci.ed over restricted domains such as realnumbers,integers, orBooleans.Domain-speci.csolvers \nuse knowledge of the domain in order to forbid explicitly values or combinations of values for some variables \n[9], while data.ow constraint solvers are domain-independent. Kaleidoscope[27]integratesimperative object-oriented \nprogramming with constraint programming, by allowing the speci.cation of multi-way constraints between \nuser de.ned objects. Constraints are asserted by statements containing a durationkeyword(e.g., once, \nalways), an optionalcon\u00adstraintstrength parameter(e.g., weak, strong, required), effectively supporting \nconstraint hierarchies, and an arbi\u00adtrary object-oriented expression. Differently from our ap\u00adproach,constraintsatisfactionis \nguaranteedbyaspecialized compiler/interpreter pair. Inthe .eld ofConstraintLogicProgramming, attributed \nvariables, a new data type that associates variables with arbitrary attributes, have proven to be a powerful \nmech\u00adanism for extending logic programming systems with the ability ofconstraintsolving[31, 32].Attributesmay \nrepre\u00adsent user-de.ned constraints, and extensible uni.cations are supported: when an attributed variable \nis to be uni.ed with aterm(possibly another attributed variable), a user-de.ned uni.cationhandlerisinvokedtoprocessthe \nobjectsinvolved andchangethevariables attributes.While beingarelatively low-level construct for constraint \nprogramming, attributed variables have also served as a basis for Constraint Han\u00addlingRules[32],a highlevel \ndeclarativelanguagetowrite constraintsolvers.Recently,ActionRules[48]hasextended logic programminglanguages,suchasProlog,with \nthecon\u00adcept of agents that can be usedfor both event-handling and constraintpropagation.Agentscarryoutspeci.cactionsand \nare activated when certain events are posted. Algorithms for Constraint Satisfaction. Moving from early \nwork onattributegrammars[19,37],avarietyofin\u00adcremental algorithms for performing ef.cient data.ow con\u00adstraint \nsatisfaction have been proposed in the literature and integratedin data.owsystemssuch asAmulet.Thesealgo\u00adrithmsarebased \neither onamark-sweep approach[19,34], or on a topological ordering [6]. In contrast, DC uses a priority-based \napproach, which allows users to customize the constraint scheduling order. Mark-sweep algorithms are \npreferable when the data.ow graph can change dynamically during constraint evaluation: this may happen \nif constraints use indirection and conditionals, and thus cannot be stat\u00adically analyzed. With both approaches, \nif there are cyclic dependencies between constraints, they are arbitrarily bro\u00adken, paying attention \nto evaluate each constraint in a cycle at most once.Comparedto ouriterativeapproach,thislimits the expressive \npower of constraints. Self-adjusting Computation. A .nal related area, that we have extensively discussed \nthroughout the paper, is that of self-adjusting computation, in which programs respond to inputchangesby \nupdatingautomaticallytheir output.Thisis achievedby recording data and control dependencies during the \nexecution of programs so that a change propagation algorithm can update the computation as if the program \nwererunfromscratch,butexecuting only thosepartsofthe computationaffectedbychanges.Wereferto[3 5, 30]for \nrecent progressinthis .eld. 8. FutureWork The work presented in this paper paves the road to several \nfurther developments. Although conventional platforms of\u00adfer limited support for implementing reactive \nmemory ef.\u00adciently,webelievethatourapproachcangreatlybene.tfrom advances in the hot .eld of transactional \nmemories, which shareswith usthesamefundamental needfora.ne-grained, highly-ef.cient control over memory \naccesses. Multi-core platforms suggest another interesting direction. Indeed, ex\u00adposing parallelism was \none of the motivations for data.ow architectures, since the early developments of the area. We regardit \nasachallenging goaltodesigneffectivemodelsand ef.cientimplementationsof one-waydata.owconstraintsin multi-core \nenvironments. Acknowledgments We wish to thank Umut Acar and Matthew Hammer for many enlightening discussions \nand for their support with CEAL. We are also indebted to Alessandro Macchioni for hiscontributionstotheimplementation \nofreactivememory, andtoPietroCenciarelli andIvanoSalvoforproviding use\u00adfulfeedback ontheformal aspectsof \nourwork. This work was supported in part by the Italian Ministry ofEducation,University,andResearch(MIUR)underPRIN \n2008TFBWL4 national researchproject AlgoDEEP:Algo-rithmicChallengesforData-IntensiveProcessing onEmerg\u00adingComputingPlatforms \n. References [1] M.Abadi,T.Harris, andM.Mehrara. TransactionalMemory with Strong Atomicity Using Off-the-Shelf \nMemory Protec\u00adtionHardware. In PPoPP,pages185 196,2009. [2] R. Abraham, M. M. Burnett, and M. Erwig. \nSpreadsheet Programming. In Wiley Encyclopedia of Computer Science andEngineering.JohnWiley&#38;Sons,Inc.,2008. \n[3] U.A.Acar. Self-AdjustingComputation:(anOverview). In PEPM,pages1 6,2009. [4] U. A. Acar, G. E. Blelloch,M. \nBlume, and K.Tangwongsan. AnExperimentalAnalysis ofSelf-AdjustingComputation. In PLDI,pages96 107,2006. \n[5] U. A. Acar, G. E. Blelloch, R. Ley-Wild, K. Tangwongsan, andD.T\u00a8urkoglu. TraceableDataTypesforSelf-Adjusting \nComputation. In PLDI,pages483 496,2010. [6] B. Alpern, R. Hoover, B. K. Rosen, P. F. Sweeney, and F. \nK. Zadeck. Incremental Evaluation of Computational Circuits. InSODA,pages32 42,1990. [7] K.R.Apt. Principles \nofConstraintProgramming.Cambridge UniversityPress,2003. [8] R. Bellmann. On a Routing Problem. Quarterly \nof Applied Mathematics,16:87 90,1958. [9] C.Bessiere. ConstraintPropagation. InF.Rossi,P. vanBeek, andT.Walsh, \neditors, HandbookofConstraintProgramming. 2006. [10] P.A.Blume. TheLabVIEWStyleBook. PrenticeHall,2007. \n[11] A. Borning. The Programming Language Aspects of ThingLab, a Constraint-Oriented Simulation Laboratory. \nACMTransactions on ProgrammingLanguages andSystems, 3(4):353 387,1981. [12] F.BoussinotandJ.-F.Susini.TheSugarCubesToolBox:aRe\u00adactive \nJava Framework. Software: Practice and Experience, 28(14):1531 1550,1998. [13] P. Caspi, P. Pilaud, N. \nHalbwachs, and J. Plaice. Lustre, a Declarative Language for Programming Synchronous Sys\u00adtems. In POPL,pages178 \n188,1987. [14] C. Chambers, B. Harrison, and J. Vlissides. A Debate on Language and Tool Support for \nDesign Patterns. In POPL, pages277 289,2000. [15] G. H. Cooper and S. Krishnamurthi. Embedding Dynamic \nData.owin aCall-by-ValueLanguage. In ESOP,pages294 308,2006. [16] A. Courtney. Frapp\u00b4 e: Functional Reactive \nProgramming in Java. InPADL,pages29 44,2001. [17] P. Cousot and R. Cousot. Abstract Interpretation: \nA Uni.ed Lattice Model for Static Analysis of Programs by Construc\u00adtionorApproximationofFixpoints.InPOPL,pages238 \n252, 1977. [18] P. Crescenzi, C. Demetrescu, I. Finocchi, and R. Petreschi. Reversible Execution and \nVisualization of Programs with Leonardo. Journal of Visual Languages and Computing, 11 (2):125 150,2000. \n[19] A. J. Demers, T. W. Reps, and T. Teitelbaum. Incremen\u00adtal Evaluation for Attribute Grammars with \nApplication to Syntax-DirectedEditors. InPOPL,pages105 116,1981. [20] C.Demetrescu. FullyDynamicAlgorithmsforPathProblems \non Directed Graphs. PhD thesis, Sapienza University of Rome,2001. [21] C. Demetrescu, I. Finocchi, and \nG. Italiano. Handbook on Data Structures and Applications, chapter 36: Dynamic Graphs. D.Mehta andS.Sahni(eds.),CRCPress,2005. \n[22] C. Demetrescu, A. V. Goldberg, and D. S. Johnson, editors. The Shortest Path Problem: Ninth DIMACS \nImplementation Challenge. AmericanMathematicalSociety,2009. [23] C. Demetrescu, I. Finocchi, and A. Ribichini. \nReactive Im\u00adperative Programming with Data.ow Constraints. Technical Report arXiv:1104.2293,April2011. \n[24] B. Demsky and M. Rinard. Automatic Detection and Repair ofErrorsinDataStructures. In OOPSLA,pages78 \n95,2003. [25] C. Elliott and P. Hudak. Functional Reactive Animation. In ICFP,pages263 273,1997. [26] \nA. Ezust and P. Ezust. An Introduction to Design Patterns in C++ withQt4. PrenticeHall,2006. [27] B. \nFreeman-Benson and A. Borning. Integrating Constraints with an Object-Oriented Language. In ECOOP, pages \n268 286,1992. [28] T. Groetker,S. Liao, G.Martin, andS. Swan. System Design withSystemC. KluwerAcademicPublishers,2002. \n[29] P.L.Guernic,A.Benveniste,P.Bournai, andT.Gautier. SIG\u00adNAL-ADataFlow-OrientedLanguageforSignalProcessing. \nIEEE Transactions on Acoustics, Speech and SignalProcess\u00ading,34(2):362 374,1986. [30] M. Hammer, U. A. \nAcar, and Y. Chen. CEAL: a C-based Language for Self-Adjusting Computation. In PLDI, pages 25 37,2009. \n[31] C. Holzbaur. Metastructures vs. Attributed Variables in the ContextofExtensibleUni.cation-AppliedfortheImplemen\u00adtation \nofCLPLanguages. In PLILP,pages260 268,1992. [32] C. HolzbaurandT.Fr\u00a8CompilingConstraintHan\u00aduhwirth. dling \nRulesintoProlog withAttributedVariables. In PPDP, pages117 133.1999. [33] P.Hudak,S.PeytonJones,andP.Wadler(editors). \nReport on the Programming Language Haskell, A Non-strict Purely FunctionalLanguage(Version1.2). ACMSIGPLANNotices, \n27(5),1992. [34] S. E. Hudson. Incremental Attribute Evaluation: A Flexible AlgorithmforLazy Update. \nACM Transactions on Program\u00admingLanguages andSystems,13(3):315 341,1991. [35] D. Ignatoff, G. H. Cooper, \nand S. Krishnamurthi. Crossing State Lines: Adapting Object-Oriented Frameworks to Func\u00adtionalReactiveLanguages. \nIn FLOPS,pages259 276,2006. [36] A.Kay.Computer software. Scienti.cAmerican,251(3):191 207,1984. [37] \nD. E. Knuth. Semantics of Context-free Languages. Theory ofComputingSystems,2(2):127 145,1968. [38] M. \nZ. Malik, K. Ghori, B. Elkarablieh, and S. Khurshid. A CaseforAutomatedDebuggingUsingDataStructureRepair. \nInASE,pages620 624,2009. [39] L.MandelandM.Pouzet. ReactiveML, aReactiveExtension toML. In PPDP,pages82 \n93,2005. [40] L. A. Meyerovich, A. Guha, J. Baskin, G. H. Cooper, M.Greenberg,A.Brom.eld,andS.Krishnamurthi.Flapjax:a \nProgramming Language for Ajax Applications. In OOPSLA, pages1 20,2009. [41] B. A. Myers, R. G. McDaniel, \nR. C. Miller, A. S. Ferrency, A. Faulring, B. D. Kyle, A. Mickish, A. Klimovitski, and P. Doane. The \nAmulet Environment: New Models for Effec\u00adtiveUserInterfaceSoftwareDevelopment. IEEETransactions onSoftwareEngineering,23(6):347 \n365,1997. [42] H.Nilsson,A.Courtney, andJ.Peterson. FunctionalReactive Programming,Continued. InHASKELL,pages51 \n64,2002. [43] S.PrasadandS.Arun-Kumar. AnIntroductiontoOperational Semantics. In Compiler Design Handbook: \nOptimizations and Machine Code,pages841 890.CRC Press,BocaRaton, 2002. [44] G. Ramalingam and T. Reps. \nAn Incremental Algorithm for a Generalization of the Shortest-Path Problem. Journal of Algorithms,21(2):267 \n305,1996. [45] U.S. Census Bureau, Washington, DC. UA Census 2000 TIGER/LineFiles. http://www.census.gov/geo/www/tiger/, \n2002. [46] B. T. Vander Zanden, R. Halterman, B. A. Myers, R. Mc-Daniel, R. Miller, P. Szekely, D. A. \nGiuse, and D. Kosbie. LessonsLearned aboutOne-Way,Data.owConstraintsin the GarnetandAmuletGraphicalToolkits. \nACMTransactions on ProgrammingLanguages andSystems,23(6):776 796,2001. [47] Z.Wan andP.Hudak.FunctionalReactiveProgrammingfrom \nFirstPrinciples. InPLDI,pages242 252,2000. [48] N.-F. Zhou. Programming Finite-Domain Constraint Propa\u00adgators \nin Action Rules. Theory and Practice of Logic Pro\u00adgramming,6:483 507,2006.   \n\t\t\t", "proc_id": "2048066", "abstract": "<p>Dataflow languages provide natural support for specifying constraints between objects in dynamic applications, where programs need to react efficiently to changes of their environment. Researchers have long investigated how to take advantage of dataflow constraints by embedding them into procedural languages. Previous mixed imperative/dataflow systems, however, require syntactic extensions or libraries of ad hoc data types for binding the imperative program to the dataflow solver. In this paper we propose a novel approach that smoothly combines the two paradigms without placing undue burden on the programmer. In our framework, programmers can define ordinary statements of the imperative host language that enforce constraints between objects stored in special memory locations designated as \"reactive\". Differently from previous approaches, reactive objects can be of any legal type in the host language, including primitive data types, pointers, arrays, and structures. Statements defining constraints are automatically re-executed every time their input memory locations change, letting a program behave like a spreadsheet where the values of some variables depend upon the values of other variables. The constraint solving mechanism is handled transparently by altering the semantics of elementary operations of the host language for reading and modifying objects. We provide a formal semantics and describe a concrete embodiment of our technique into C/C++, showing how to implement it efficiently in conventional platforms using off-the-shelf compilers. We discuss common coding idioms and relevant applications to reactive scenarios, including incremental computation, observer design pattern, and data structure repair. The performance of our implementation is compared to ad hoc problem-specific change propagation algorithms, as well as to language-centric approaches such as self-adjusting computation and subject/observer communication mechanisms, showing that the proposed approach is efficient in practice.</p>", "authors": [{"name": "Camil Demetrescu", "author_profile_id": "81100357279", "affiliation": "Sapienza University of Rome, Rome, Italy", "person_id": "P2839204", "email_address": "demetres@dis.uniroma1.it", "orcid_id": ""}, {"name": "Irene Finocchi", "author_profile_id": "81100246662", "affiliation": "Sapienza University of Rome, Rome, Italy", "person_id": "P2839205", "email_address": "finocchi@di.uniroma1.it", "orcid_id": ""}, {"name": "Andrea Ribichini", "author_profile_id": "81309483171", "affiliation": "Sapienza University of Rome, Rome, Italy", "person_id": "P2839206", "email_address": "ribichini@dis.uniroma1.it", "orcid_id": ""}], "doi_number": "10.1145/2048066.2048100", "year": "2011", "article_id": "2048100", "conference": "OOPSLA", "title": "Reactive imperative programming with dataflow constraints", "url": "http://dl.acm.org/citation.cfm?id=2048100"}