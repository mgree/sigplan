{"article_publication_date": "10-22-2011", "fulltext": "\n A Simple Abstraction for Complex Concurrent Indexes Pedro da Rocha Pinto Thomas Dinsdale-Young Mike \nDodds Imperial College London Imperial College London University of Cambridge pmd09@doc.ic.ac.uk td202@doc.ic.ac.uk \nmike.dodds@cl.cam.ac.uk Philippa Gardner Imperial College London pg@doc.ic.ac.uk Abstract Indexes are \nubiquitous. Examples include associative arrays, dictionaries, maps and hashes used in applications such \nas databases, .le systems and dynamic languages. Abstractly, a sequential index can be viewed as a partial \nfunction from keys to values. Values can be queried by their keys, and the index can be mutated by adding \nor removing mappings. Whilst appealingly simple, this abstract speci.cation is in\u00adsuf.cient for reasoning \nabout indexes accessed concurrently. We present an abstract speci.cation for concurrent in\u00addexes. We \nverify several representative concurrent client ap\u00adplications using our speci.cation, demonstrating that \nclients can reason abstractly without having to consider speci.c underlying implementations. Our speci.cation \nwould, how\u00adever, mean nothing if it were not satis.ed by standard imple\u00admentations of concurrent indexes. \nWe verify that our speci\u00ad.cation is satis.ed by algorithms based on linked lists, hash tables and BLink \ntrees. The complexity of these algorithms, in particular the BLink tree algorithm, can be completabely \nhidden from the client s view by our abstract speci.cation. Categories and Subject Descriptors D.2.4 \n[Software/Pro\u00adgram Veri.cation]: Formal Methods General Terms Algorithms, Theory, Veri.cation. Keywords \nConcurrency, Indexes, B-Trees, Separation Logic. 1. Introduction An index is a data structure where data \nis associated with identifying keys, through which the data can be ef.ciently Permission to make digital \nor hard copies of all or part of this work for personal or classroom use is granted without fee provided \nthat copies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. OOPSLA 11, October 22 27, 2011, Portland, Oregon, \nUSA. Copyright c &#38;#169; 2011 ACM 978-1-4503-0940-0/11/10. . . $10.00 Mark Wheelhouse Imperial College \nLondon mjw03@doc.ic.ac.uk retrieved. Indexes are ubiquitous in computer systems: they are integral to \ndatabases, caches, .le systems, and even the objects of dynamic languages such as JavaScript. Concurrent \nsystems use indexes for: database sanitation to concur\u00adrently remove patients who have been cured or \ntransferred; graphics rendering to clip all objects outside horizontal or vertical bounds; garbage collection \n to concurrently mark reachable objects; and web applications to allow multiple clients to add and remove \npictures and comments, for in\u00adstance. A variety of implementations of indexes exist, such as skip lists, \nhash tables and B-trees. Different implementa\u00adtions offer different performance characteristics, but \nall ex\u00adhibit the same abstract behaviour. To a sequential client, an index can be viewed abstractly as \na partial function from keys to values. A client can query or mutate the index without having to take \ninto account the complexities of its underlying implementation. This simple, yet powerful, abstract speci.cation \nlargely accounts for the popularity of indexes. However, this abstraction breaks down if an index is \naccessed concurrently. When several threads insert, remove and query keys, clients can no longer model \nthe whole index by a single partial function. Each client must take account of potential interference \nfrom other threads. In this paper, we present a novel abstract speci.cation for concurrent indexes, and \nuse it to verify a number of client programs. Crucially, clients can reason abstractly using our speci.cation \nwithout having to consider speci.c underlying implementations. However, we can also verify our speci.ca\u00adtion \nagainst complex concurrent index implementations. Our approach is based on concurrent abstract predi\u00adcates \n[7], recently introduced to reason about concurrent modules. With this technology, we can view the index \nas divisible: keys are a resource which can be divided between the threads. When threads operate on disjoint \nkeys, they can do so independently of each other. When threads operate on shared keys, concurrent abstract \npredicates can account for the interference caused by other threads.  Intuitive description of the approach. \nFirst, consider the disjoint case, where each key is manipulated by a single thread. In this case, we \ncan verify each thread in terms of the keys it uses, and combine the results to understand the composed \nsystem. In our speci.cation, we have the predi\u00adcates in(h, k, v) and out(h, k): in(h, k, v) declares \nthat the key k is mapped to value v in index h; out(h, k) declares that there is no mapping of k. A thread \nmust hold one of these predicates in order to modify k. A disjointness axiom enforces that only one thread \ncan hold such a predicate on k at any one time. We describe these predicates as abstract, because they \ndo not reveal how they are implemented. Given these predicates, we can give the following speci\u00ad .cation \nto remove: where the sum of permissions held by all threads cannot ex\u00adceed 1. In addition, if the current \nthread holds exclusive per\u00admission, we have axioms to change the type of the interfer\u00adence restriction \nwithout violating the expectations of other threads, such as: indef (h, k,v)1 . inrem(h, k,v)1. Using \nour speci.cation and these axioms, we can prove a natural speci.cation for parallel remove on a shared \nkey: indef (h, k,v)1 inrem(h, k,v)1 (h, k,v)* inrem(h, k,v) inrem 12 12 inrem (h, k,v) 12 inrem \n(h, k,v) 12 in(h, k,v)remove(h, k)out(h, k) remove(h, k) remove(h, k)  (h, k)(h, k) outrem outrem \n1212 With this speci.cation, we can prove the following property of a simple client program performing \nparallel removes (our (h, k) * outrem(h, k) outrem 12 12 outdef (h, k)1 proof assumes that k1 = k2 \n): in(h, k1 ,v1) * in(h, k2 ,v2)  This speci.es the strong property that, if we de.nitely know that \nkey k has a value then, after the parallel remove, we in(h, k1 ,v1) in(h, k2 ,v2) remove(h, k1 ) remove(h, \nk2 )  de.nitely know that the value has been removed. We do not know which thread has performed the \nremove, but this fact out(h, k1 ) out(h, k2 )  is irrelevant to correctness. out(h, k1 ) * out(h, k2 \n) In this proof, we reason about the parallel threads individ\u00adually. We then join the disjoint pre-and \npostconditions to form the overall proof. Disjointness is expressed by the sep\u00adarating conjunction, *, \nof concurrent separation logic [16]. The disjointness axiom requires that k1 = k2 . Now consider the \nshared case, where threads can inter\u00adfere with each other: for example, when k1 = k2 in the parallel \nremoves. We introduce the more re.ned predicates indef (h, k, v)i, outdef (h, k)i, inrem(h, k, v)i and \noutrem(h, k)i. These predicates are extended in two ways: 1. def and rem are restrictions on the type \nof interference that is allowed on the key: def prohibits any interference, while rem only permits removal \nof the key. All threads must agree on the type of interference for a given key. 2. The interference \npermissions i . (0, 1] determine whether a thread has shared (0 <i< 1) or exclusive (i =1) ac\u00adcess to \na key. If a thread holds shared permission, it can only perform operations that respect the interference \nre\u00adstrictions.  Using the rem predicates, we can give the following speci.\u00adcation for remove: inrem(h, \nk,v)iremove(h, k)outrem(h, k)i Predicates can be split and joined by permission, so for example we have \nthe axiom: inrem(h, k, v)i+j .. inrem(h, k, v)i * inrem(h, k, v)j , Verifying clients and index implementations. \nOur concur\u00adrent index speci.cation allows us to present a single abstract interface to clients, irrespective \nof the choice of underly\u00ading implementation. We demonstrate that our speci.cation is useful by verifying \nseveral representative client programs such as function memoization, a prime number sieve and a mapping \nof a function onto an index. We also verify that several concurrent index algorithms satisfy our speci.cation: \nin particular, a na\u00a8ive linked list algorithm with coarse-grained locking for expository pur\u00adposes; a \nsimple algorithm using a hash table linked to a set of (abstract) secondary indexes, to demonstrate the \nveri.cation of a more complex implementation; and Sagiv s substantial BLink tree algorithm [20] to demonstrate \nthe scalability of our techniques to a real-world algorithm. During veri.cation, we found a subtle bug \nin the BLink tree algorithm. We use the concurrent abstract predicate methodol\u00adogy [7] to hide low-level \nsharing in the implementations from clients. In particular, the underlying sharing mecha\u00adnism used by \nthe BLink tree algorithm to permit non-blocking reads is exceedingly complex. This complexity is completely \nhidden from the client s view by our abstract speci.cation. Related work. We build directly on concurrent \nabstract predicates (CAP) [7], which provides a logic for verifying concurrent modules based on separation \nlogic. CAP devel\u00adoped from three lines of work: racy concurrent variants of separation logic such as \nRGSep [9, 10, 23]; sequential mod\u00ad ular reasoning based on abstract predicates [17]; and .ne\u00ad grained \nmodular reasoning based on context logic [3, 8]. We originally used RGSep [23] to verify concurrent B-trees \n[4]. However, RGSep and similar approaches depend on global conditions; consequently, they cannot verify \nabstract spec\u00adi.cations such as our index speci.cation. This observation formed part of our original \nmotivation for CAP.  Our concurrent index speci.cation descends from the set speci.cation veri.ed in \n[7]. In that paper, we focussed on building a sound logic, and veri.ed only simple, disjoint speci.cations \nagainst small implementations. As far as we are aware, our speci.cation is the .rst in separation logic \nto allow thread-local reasoning combined with races over elements of a shared structure. We have veri.ed \nour index speci.cation against Sagiv s real-world concurrent BLink tree algorithm [20]1, a substantial \njump in the complexity of the veri.cation compared with [7]. Our work is beginning to develop the idioms \nnecessary to scale to large examples. Others have worked on reasoning abstractly about index\u00adlike data \nstructures for sequential clients. For example, Dillig et al. propose a static analysis for C-like programs \nwhich represents the abstract content of containers [6]. Kuncak et al. propose an analysis that represents \nvarious kinds of data by abstract sets, while proving these abstractions [14]. One of the most challenging \nparts of our work was veri\u00adfying that the concurrent BLink tree implementation satis.es our speci.cation. \nSome prior work exists on verifying se\u00adquential B-trees. In [21], B-tree search and insert operations \nare veri.ed as fault-free in a simpli.ed sequential setting. In [15], a sequential B-tree implementation \nis veri.ed in Coq as part of a relational database management system. The au\u00adthors comment that the proof \nwas dif.cult and in need of ab\u00adstraction. They go on to state that verifying the correctness of high-performance, \nconcurrent B+ trees will be a particu\u00adlarly challenging problem . The only prior veri.cation of a concurrent \nB-tree we are aware of is a highly-abstracted version of the algorithm mod\u00adelled in process algebra [18]. \nIt veri.es a global speci.ca\u00ad tion, rather than allowing elements to be divided between threads. We believe \nthat our work provides the .rst direct, formal veri.cation of Sagiv s widely-used algorithm [20]. Paper \nstructure. \u00a72 gives technical background. \u00a73 give the disjoint index speci.cation, and \u00a74 extends it \nto sharing. \u00a75 discusses iteration over indexes. \u00a76 describes verifying our speci.cation against index \nimplementations. \u00a73-5 can be understood from the simple summary in \u00a72. A complete understanding of the \ntechnicalities in \u00a76 requires knowledge of the original CAP paper [7]. 2. Separation Logic &#38; Abstraction \nThis paper is based on separation logic [19], a Hoare-style program logic for reasoning locally about \nprograms that ma\u00adnipulate resource: for example, C programs that manipulate the heap. Local reasoning \nfocusses on the speci.c part of 1 Without compression, which is beyond the scope of this paper. the resource \nthat is relevant at each point in the program. This supports scalable and compositional reasoning, since \ndisjoint resource neither impinges upon nor is affected by the behaviour of the program at that point. \nSeparation logic speci.cations have a fault-avoiding partial-correctness interpretation. Consider the \nfollowing speci.cation for a command C(here P , Q are assertions): {P } C {Q} The interpretation of this \nspeci.cation is that (1) executing C in a state satisfying assertion P will result in a state satisfying \nassertion Q, if the command terminates; and (2) the resources represented by P are the only resources \nneeded for Cto execute successfully. Other resources can be conjoined with such a speci.ca\u00adtion without \naffecting its validity. This is expressed by the following proof rule: {P } C {Q} FRAME (side-condition) \n{P * F } C {Q * F } This rule allows us to extend a speci.cation on a small re\u00adsource with an unmodi.ed \nframe assertion F , giving a larger resource. Here, * is the so-called separating conjunction. Combining \ntwo assertions P and F into a separating con\u00adjunction P *F asserts that both resources are independent \nof each other. The side-condition simply states that no variable occurring free in F is modi.ed by the \nprogram C. Separation logic provides straightforward reasoning about sequential programs. It also handles \nconcurrency [16], using the following rule: {P1} C1 {Q1}{P2} C2 {Q2} PAR {P1 * P2} C1 C2 {Q1 * Q2} In \na concurrent setting, the precondition and postcondition are interpreted as resources owned exclusively \nby the thread. Reasoning using PAR is thread-local. We reason about each thread purely using the resources \nthat are mentioned in its precondition, without requiring global reasoning about in\u00adterleaving. As with \nsequential reasoning, locality is the key to compositional reasoning about threads. Abstraction. Abstract \nspeci.cations are a mechanism for specifying the external behaviour of a module s functions, while hiding \ntheir implementation details from clients. Re\u00adsources are represented by abstract predicates [17]. Clients \ndo not need to know the concrete de.nitions of these predi\u00adcates; they can reason purely in terms of \nthe module s oper\u00adations. For example, insert in a set module might be spec\u00adi.ed as: set(x,S) insert(x, \nv) set(x,S .{v}) insert updates the abstract contents of the set at address x from S to S .{v}. A client \ncan reason about the high-level behaviour of insert without knowing about the concrete de.nition of the \nset predicate.  Abstract predicates, however, can only represent the set as a single entity, because \nimplementation details disrupt .ner\u00adgrained abstractions. Concurrent abstract predicates [7], on the \nother hand, can achieve .ner abstractions. We can break the set down into predicates representing individual \nele\u00adments: in(x, v) if v belongs to the set x; out(x, v) if it does not. Different threads can hold access \nto different set el\u00adements. When element v is not in the set, the command insert can be speci.ed by: \nout(x, v) insert(x, v) in(x, v) Concurrent abstract predicates provide a .ner granularity of local reasoning, \nwhilst still hiding implementation details from clients. We follow the concurrent abstract predicate \napproach in our reasoning about concurrent indexes. 3. Index Speci.cation: Disjointness We start by giving \na simple speci.cation which divides an index up into its constituent keys. Our speci.cation ensures that \neach key is accessed by at most one thread (in \u00a74 we discuss a re.ned speci.cation that supports sharing). \nOur speci.cation hides the fact that each key is part of an under\u00adlying shared data structure, allowing \nstraightforward high\u00adlevel reasoning about keys and values. Abstractly, the state of an index can be \nseen as a partial function mapping keys to values2: H : Keys -Vals There are three basic operations on \nan index search, insert and remove which operate on index h (with cur\u00adrent state H) as follows: search(h, \nk) looks for the key k in the index. It returns H(k) if it is de.ned, and nil otherwise.  insert(h, \nk, v) tries to modify H to associate the key k with value v. If k . dom(H) then insert does nothing. \nOtherwise it modi.es the shared index to H I{k . v}.  remove(h, k) tries to remove the value of the \nkey k from the index. If k ./dom(H) then remove does nothing. Otherwise it rewrites the index to H \\{k}. \n This view of operations on the index is appealingly simple, but cannot be used for practical concurrent \nreasoning. This is because it depends on global knowledge of the underlying index H. To reason in this \nway, a thread would require perfect knowledge of the behaviour of other threads. To avoid this, we give \na speci.cation that breaks the index up by key value. Our speci.cation allows threads to 2 Where possible, \nwe treat the key and value sets abstractly. Implementa\u00adtions require certain properties of these sets, \nhowever: all require keys to be comparable for equality, hash tables require the ability to compute hashes \nof keys, and B-trees require a linear ordering on keys. hold the exclusive ownership of an individual \nkey. Each key in the index is represented by a predicate, either in or out depending on whether the key \nis associated with a value or not. The predicates have this intuitive interpretation: in(h, k, v): there \nis a mapping in the index h from k to v, and only the thread holding the predicate can modify k. out(h, \nk): there is no mapping in the index h from k, and only the thread holding the predicate can modify k. \nThese predicates combine knowledge about state whether a key is in the index with knowledge about ownership \nwhether the thread is allowed to alter that key. A thread holding the predicate for a given key knows \nthe value of the key, and can be sure that no other thread will modify it. This entangling of state with \nownership is essential to our approach: each predicate is invariant under the behaviour of other threads, \nmeaning its implementation can be abstracted. The index operations have the following speci.cations with \nrespect to these predicates: in(h, k, v) r := search(h, k) in(h, k, v) . r = v out(h, k) r := search(h, \nk) out(h, k) . r = nil in(h, k, vi) insert(h, k, v) in(h, k, vi) out(h, k) insert(h, k, v) in(h, k, v) \nin(h, k, v) remove(h, k) out(h, k) out(h, k) remove(h, k) out(h, k) Predicates can be composed using \nthe separating conjunc\u00adtion *, indicating that they hold independently of each other. Note that our speci.cation \nallows us to reason about an in\u00addex as a collection of disjoint, independent elements, despite the fact \nthat indexes are generally implemented as a single shared data structure. Each predicate represents exclusive \nownership of a par\u00adticular key. Our speci.cation represents this fact by exposing the following axiom: \n (in(h, k, v) . out(h, k)) * =. false (in(h, k, vi) . out(h, k)) Given the above speci.cations, we can \nreason locally about programs that use concurrent indexes. Consider for example the following simple \nprogram: r := search(h, k2 ); insert(h, k1 , r) remove(h, k2 ) This program retrieves the value v associated \nwith the key k2 . It then concurrently associates v with the key k1 and removes the key k2 . When the \nprogram completes, k1 will be associated with v, and k2 will have been removed from the index. This speci.cation \ncan be expressed as: out(h, k1 ) * in(h, k2 ,v) - in(h, k1 ,v) * out(h, k2 )  j . (out(h,i) . i/. keys(S)) \n. \u00aek1 =i=k2 (.v. in(h, i, v) . (i, v) . S) . We can prove this speci.cation as follows: out(h, k1 ) * \nin(h, k2 ,v) (k1 = k2){ rangeMap(h, k1,k2){ if j r := search(h, k2 ); . out(h, k1 ) * in(h, k2 ,v) . \nr = v k1 = k2 . ((out(h, k1 ) . k1 ./keys(S)) . (.v. in(h, k1 ,v) . (k1 ,v) . S)) out(h, k1 ) . r = v \nin(h, k2 , v) insert(h, k1 , r) remove(h, k2 ) in(h, k1 , v) out(h, k2 ) := search(h, k1);r ((out(h, \nk1 ) . k1 ./keys(S) . r = nil) . j . (in(h, k1 , r) . (k1 , r) . S)) . k1 = k2 in(h, k1 ,v) * out(h, \nk2 ) = in(h, k1 , r) . (k1 , r) . S . k1 = k2 In this proof, the search operation .rst uses the predicate \nremove(h, if.. (rnil) { \u00af k1); \u00af in(h, k2 ,v) to retrieve the value v. Then, the parallel rule out(h, \nk1 ) . (k1 , r) . S . k1 = k2 hands insert and remove the out(h, k1 ) and in(h, k2 ,v) predicates respectively. \nThe postcondition of the program := f(r);r .v. out(h, k1 ) . (k1 ,v) . S . r = f(v) . k1 = k2 .\u00af consists \nof the separating conjunction of the two thread postconditions. k1, r);insert(h, .v. in(h, k1 ,f(v)) \n. (k1 ,v) . S . k1 = k2 . \u00af  }j8>>< k1 = k2 . ((out(h, k1 ) . k1 ./keys(S)) . } else (.v. in(h, i, v) \n. (i, v) . S) 3.1 Example: Map A common operation on a concurrent index is applying a particular function \nto every value held in the index: mapping the function onto the index. We consider a simple algorithm \nrangeMap that maps function f (implemented by f) onto keys within a speci.ed range. We implement rangeMap \nwith a divide-and-conquer approach, which splits the key range . (.v. in(h, k1 ,f (v)) . (k1 ,v) . S)) \n{ 9>>= \u00ab (out(h,i) . i/. keys(S)) . jk k1 +k2 \u00aek1 =i= 2 * . \u00ab (out(h,i) . i/. keys(S)) . >>: >>; \u00ae \nj// Apply k k1 +k2 . (.v. in(h, i, v) . (i, v) . S) <i= k2 2 the PAR into sub-intervals on which the \nmap operation is recursively rule. rangeMap(h, k1,k1+((k2-k1)/2)) applied in parallel. || 8>>< jk k1 \n+k2 \u00aek1 =i= 2 rangeMap(h, k1+((k2-k1)/2)+1, k2) 9>>= \u00ab rangeMap(h, k1,k2){ if (k1 = k2){ (out(h,i) . \ni/. keys(S)) . * . (.v. in(h, i, f(v)) . (i, v) . S) (out(h,i) . i/. keys(S)) . \u00ab r := search(h, k1); \nif (r= nil) { >>: >>; \u00ae j k k1 +k2 . (.v. in(h, i, f(v)) . (i, v) . S) <i= k2 2 remove(h, k1); r := f(r); \ninsert(h, k1, r); j }} \u00aek1 =i=k2 . . (out(h,i) . i/. keys(S)) . (.v. in(h, i, f(v)) . (i, v) . S) } } \nelse { rangeMap(h, k1,k1+((k2-k1)/2)) || rangeMap(h, k1+((k2-k1)/2)+1, k2) }} We specify rangeMap as \nfollows, where S is a set of key\u00advalue pairs: \u00aek1 =i=k2 . (out(h,i) . i/. keys(S)) . (.v. in(h, i, v) \n. (i, v) . S) rangeMap(h, k1 , k2 ) \u00aek1 =i=k2 . (out(h,i) . i/. keys(S)) . (.v. in(h, i, f(v)) . (i, \nv) . S) (Here, \u00ae is the iterated separating conjunction. That is, \u00aex.{1,2,3}.P is equivalent to P [1/x] \n* P [2/x] * P [3/x]. The set keys(S) is the set of keys associated with values in S.) In the speci.cation, \nthe logical variable S describes the initial state of the index (in the key range [k1 , k2 ]). Assuming \nthat S contains at most one key-value pair for each key, the key i (for k1 = i = k2 ) initially has value \nv if and only if Figure 1. Proof for rangeMap. (i, v) . S. After execution of rangeMap, the postcondition \nensures that if the key i had and initial value v, then it now has value f(v), and if it had no value \nthen it still has no value. A proof that rangeMap conforms to this speci.cation is given in Figure 1. \nrangeMap might not be considered truly typical of map operations, as it maps over a range of keys rather \nthan the entire index. In \u00a75, we introduce a speci.cation for iterators, allowing all keys in an index \nto be enumerated. Using an iterator, we implement and verify a map function over all values in the index. \n4. Index Speci.cation: Sharing The speci.cation we de.ned in the previous section requires that each \nkey in the index is accessed by at most one thread. However, often threads read and write to keys at \nthe same time. In this section, we de.ne a re.ned speci.cation that allows for concurrent access to keys. \nAs before, our speci\u00ad.cation hides implementation details and allows threads to reason locally.  Consider \nthe following program: remove(h, k) r := search(h, k) (1) If we know at the start of the program that \nkey k maps to some value v, we should be able to establish that there will not be a mapping from the \nkey k at the end. However, we will not know the value of r, because we do not know at which point during \nthe remove operation that the search operation will read the value associated with k. Implementations \nhave many different ways of handling the sharing of keys (for example using mutual exclusion locks or \ntransactions), but at the abstract level they all behave in the same way. If a thread reads a key multiple \ntimes, the reads all return the same result, unless another thread also writes to that key. Our re.ned \nspeci.cation is based on abstract predicates that express three facts about a given key: 1. whether there \nis a mapping from the key to some value in a set; 2. whether the thread holding the predicate can add \nor re\u00admove the value of the key in the index; 3. whether any other concurrently running threads (the \nen\u00advironment) can add or remove the value of the key in the index.  These facts are related. If a key \nmaps to a value in the index, but other threads are allowed to remove the value of the key, the current \nthread cannot assume the value will remain in the index. Our predicates therefore re.ect the uncertainty \ngenerated by sharing in a local way. We de.ne the following set of predicates, parametric on key k and \nindex h: unk(h, k, S)i : there may be a mapping from key k to a value v in set S and threads can search, \nremove and insert any value in set S at this key. read(h, k) : there may be a mapping from key k to some \nvalue, the current thread may not change it, but other threads can make any modi.cation. The subscripts \ndef, ins and rem and the fractional compo\u00adnents i . (0, 1] record the behaviours allowed by the current \nthread and its environment on key k. Access to keys can be shared between threads. We rep\u00adresent this \nin our speci.cation by splitting predicates. Our speci.cation includes axioms which de.ne the ways that \npredicates can be split and joined. For example: inrem(h, k, v)i+j .. inrem(h, k, v)i * inrem(h, k, v)j \nif i + j = 1 As in Boyland [2], fractional permissions are used to record splittings. A permission i \n. (0, 1) records that a key is shared with other threads, while i =1 records it is held exclusively by \nthe current thread. When a thread holds exclusive access to a key (i =1), the thread can add or remove \nthe key freely. When a thread shares access to the key (i . (0, 1)), the subscripts def, ins and rem \nrestrict what the thread and its environment are able to do. Subscript def speci.es that no thread is \nable to modify the key. Subscript ins speci.es that both thread and environment can insert on the key, \nbut not remove the key, while subscript rem speci.es the converse. Modifying keys concurrently can result \nin different threads holding different predicates for the same key. For example, suppose a thread holds \nthe inrem(h, k, v)1 predicate, which denotes that the key k has value v in the index. Since the permission \nis 1, this knowledge is assured. However, we indef (h, k, v)i : there is a mapping from key k to value \nv can split this predicate into two halves, inrem(h, k, v) 1 2 and and a thread can only modify this \nkey if it has exclusive inrem (h, k, v) 1 2 , and give each half to two sub-threads. As\u00ad permission (i \n= 1). sume the .rst thread does not modify the key, but the second calls remove(h, k), which has the \nfollowing speci.cation: outdef (h, k)i : there is no mapping from key k and a thread can only modify \nthis key if it has exclusive permission {inrem(h, k,v)i} remove(h, k) {outrem(h, k)i}(i = 1). The result \nis uncertainty: one thread holds the outrem(h, k)inins(h, k, S)i : there is a mapping from key k to a \nvalue in 1 2 set S and threads can only insert values in set S at this predicate, stating that k is \nnot in the index, while the other key. holds the inrem(h, k, v) 1 2 predicate, stating that k may have \nassociated value v. We de.ne joining axioms that resolve outins(h, k, S)i : there may be a mapping from \nkey k to a value in set S, threads can only insert values in set S at this key, and the current thread \nhas not made such an insertion so far. inrem(h, k, v)i : there may be a mapping from key k to value v, \nthreads can only remove the value at this key, and the current thread has not done this so far. outrem(h, \nk)i : there is no mapping from key k and threads can only remove the value at this key. this uncertainty. \nSince rem allows removal but not insertion, we know that once the key has been removed from the index, \nit stays removed. So outrem dominates inrem, which is re.ected in the following axiom: inrem(h, k, v)i \n* outrem(h, k)j =. outrem(h, k)i+j if i + j = 1 Some predicates take sets of value arguments, while oth\u00aders \ntake singleton values. We use singleton values when we The proof starts with the predicate indef (h, \nk,v)1, which speci.es that there is a mapping from key k to a value v in the index. The def subscript \nasserts that no other thread can modify the value mapped by this key. We use the following axiom to create \na read(h, k) predicate:  Xi .. Xi * read(h, k) This allows the right-hand thread to perform a simple \nsearch operation, although the postcondition establishes Thread Env. Predicate Perm. Ins. Rem. Ins. Rem. \nindef / outdef 1 Yes Yes No No indef / outdef i No No No No inins / outins 1 Yes No No No inins / outins \ni Yes No Yes No inrem / outrem 1 No Yes No No inrem / outrem i No Yes No Yes unk i Yes Yes Yes Yes read \n- No No Yes Yes Figure 2. Predicates and their interference. know a key has that value. We use a set \nof values when con\u00adcurrent inserts are possible (that is, in the ins and unk cases), because we cannot \nknow which thread will be the .rst to insert. However, if a value is inserted, it will be one of the \nvalues in the set S. Our full speci.cation is given in Figure 3. The choice of predicates is not arbitrary; \neach represents a stable combina\u00adtion of facts about the key k and the behaviours permitted by the thread \nand environment. Figure 2 shows how various combinations of fractional permissions and subscripts cor\u00adrespond \nto various behaviours. Our predicates give almost complete coverage of all possible combinations. The \nmiss\u00ading combinations are either cases where the current thread has no access to a key, or where it is \nonly safe to conclude that a key has an unknown value, in which case we can use one of the read or unk \npredicates. We do not claim that our speci.cation is de.nitive, just one natural choice. We ex\u00adpect to \nadapt our speci.cation when looking at real-world applications such as the POSIX .le system, the concurrent \ndatabase algorithm ARIES, and java.util.concurrent. We believe that our speci.cation is robust enough \nto be able to support such applications with minor modi.cation. nothing about the result. This captures \nthe fact that we do not know at which point during the remove opera\u00adtion the search operation will read \nthe key s value. The indef (h, k,v)1 predicate allows the left-hand thread to re\u00admove the value successfully, \nas we know that it is the only thread changing the shared state for the key k. When both threads .nish \ntheir execution we use the same axiom to merge read(h, k) back into the outdef (h, k)1. We can prove \nthe second speci.cation in a similar fashion. We can establish natural speci.cations for all the various \ncombinations of insert, remove and search. For example, consider the parallel composition of two removes \non the same key k: remove(h, k) remove(h, k) Regardless of whether k is in the index, we de.nitely know \nthat there will be no mapping from key k afterwards. By splitting the predicates, we can share this knowledge \nbe\u00adtween the threads. indef (h, k,v)1 inrem(h, k,v)1 (h, k,v) * inrem(h, k,v) 12 inrem 4.1 Proving Simple \nExamples 12 (h, k,v) inrem (h, k,v) inrem 1 2 1 2 Recall the program labelled (1) with which we began \nthis remove(h, k) remove(h, k) section. This program satis.es the following speci.cations: outrem (h, \nk) 12 outrem (h, k) 12 outrem indef (h, k,v)1 - outdef (h, k)1 (h, k) * outrem(h, k) 1 2 1 2 outdef (h, \nk)1 - outdef (h, k)1 Using our abstract speci.cations, we can prove the .rst of these speci.cations as \nfollows: indef (h, k,v)1 indef (h, k,v)1 * read(h, k) read(h, k) indef (h, k,v)1 remove(h, k) r := search(h, \nk) read(h, k) outdef (h, k)1 outdef (h, k)1 * read(h, k) outdef (h, k)1 outdef (h, k)1 We cannot always \nestablish the exact state of an index at all points during a program, but our speci.cation will always \nallow us to be as precise as possible. For example, consider the following program: remove(h, k) insert(h, \nk, v) remove(h, k) When run in a state where key k is initially unassigned, we will not know if there \nis a mapping from key k in the index at the end of the parallel call. However, after the .nal remove \n SPECIFICATIONS: indef (h, k,v)i r := search(h, k) indef (h, k,v)i . r = v outdef (h, k)i r := search(h, \nk) outdef (h, k)i . r = nil inins(h, k,S)i r := search(h, k) inins(h, k,S)i . r . S outins(h, k,S)i r \n:= search(h, k)(outins(h, k,S)i . r = nil) . (inins(h, k,S)i . r . S) inrem(h, k,v)i r := search(h, k)(inrem(h, \nk,v)i . r = v) . (outrem(h, k)i . r = nil) outrem(h, k)i r := search(h, k) outrem(h, k)i . r = nil unk(h, \nk,S)i r := search(h, k) unk(h, k,S)i . (r . S . r = nil) read(h, k) r := search(h, k) read(h, k) indef \n(h, k,v)i insert(h, k, vi) indef (h, k,v)i outdef (h, k)1 insert(h, k, v) indef (h, k, v)1 (inins(h, \nk,S)i . outins(h, k,S)i) . v . S insert(h, k, v) inins(h, k,S)i unk(h, k,S)i . v . S insert(h, k, v) \nunk(h, k,S)i indef (h, k,v)1 remove(h, k) outdef (h, k)1 outdef (h, k)i remove(h, k) outdef (h, k)i \ninrem(h, k,v)i . outrem(h, k)i remove(h, k) outrem(h, k)i unk(h, k,S)i remove(h, k) unk(h, k,S)i AXIOMS: \n Xi * Xj . Xi+j if i + j = 1 inins(h, k, S)i * outins(h, k, S)j . inins(h, k, S)i+j if i + j = 1 inrem(h, \nk, v)i * outrem(h, k)j . outrem(h, k)i+j if i + j = 1 indef (h, k, v)1 . inrem(h, k, v)1 .v . S. indef \n(h, k, v)1 . inins(h, k, S)1 outdef (h, k)1 . outrem(h, k)1 . outins(h, k, S)1 Xi . Xi * read(h, k) read(h, \nk) . read(h, k) * read(h, k) unk(h, k, S)1 . outdef (h, k)1 ..v . S. indef (h, k, v)1 CONTRADICTION AXIOMS: \n Xi * Xj . false if i + j> 1 indef (h, k, v)i * Xj . false if X = indef (h, k, v) outdef (h, k)i * Xj \n. false if X = outdef (h, k) (inins(h, k, S)i . outins(h, k, S)i) * Xj . false if X = inins(h, k, S) \n. X = outins(h, k, S) (inrem(h, k, v)i . outrem(h, k)i) * Xj . false if X = inrem(h, k, v) . X = outrem(h, \nk) (inins(h, k, S)i * inins(h, k, Si)j) . (outins(h, k, S)i * outins(h, k, Si)j ) . false if S = Si unk(h, \nk, S)i * Xj . false if X = unk(h, k, S) Figure 3. Full speci.cation for concurrent indexes. X denotes \nindef (h, k, v), outdef (h, k), inins(h, k, S), outins(h, k, S), inrem(h, k, v), outrem(h, k) or unk(h, \nk, S) in the axioms. . \u00af. \u00af .i . (0, 1]. \u00aevl . unk(memo,v', {f(v')})i .i . (0, 1]. \u00aevl . unk(memo,v', \n{f(v')})i memoized_f(v) { evict_f() { .\u00af \u00aevwhile (...) { '' l . unk(memo,v', {f(v')})i .\u00af // frame \nthe irrelevant values off l . unk(memo,v, {f(v)})i .\u00af \u00aev unk(memo, v, {f (v)})i k := nondet(); r := search(memo, \nv); // frame the irrelevant values off . \u00af.\u00af unk(memo, v, {f (v)})i . (r = f(v) . r = nil) unk(memo, \nk, {f(k)})i if (r = nil) { remove(memo,k); .\u00af .\u00af unk(memo, v, {f(v)})i unk(memo, k, {f(k)})i r := f(v); \n} } .\u00af. \u00af unk(memo, v, {f(v)})i . r = f(v) .i . (0, 1]. \u00aevl . unk(memo,v', {f(v')})i insert(memo, v, \nr); .\u00af unk(memo, v, {f(v)})i . r = f(v) Figure 5. Proof outline for evict f. } .\u00af unk(memo, v, {f (v)})i \n. r = f(v) // frame the values back on memoized_f(v) { .\u00af r = f (v) . \u00aevl . unk(memo,v', {f (v')})i \nr := search(memo, v); return r; if (r = nil) { }r := f(v); .\u00af insert(memo, v, r); ret = f(v) ..i . (0, \n1]. \u00aevl . unk(memo,v', {f(v')})i } return r; Figure 4. Proof outline for memoized f. } We give memoized \nf the following speci.cation: operation we know that the key k will be unassigned. memo r := memoized \nf(v) r = f(v) . memo outdef (h, k)1 outdef (h, k)1 . indef (h, k, v)1 where the abstract predicate memo \nis unk(h, k, {v})unk(h, k, {v}) unk(h, k, {v})1 * unk(h, k, {v})unk(h, k, {v}) 12 12 . memo = i .i . \n(0, 1]. \u00aevl . unk(memo,v , {f(v i)})i 11 22 The de.nition of memo states that, for each value vi, we \ndo remove(h, k) insert(h, k, v) not know if vi is in the index. The predicate is splittable: that unk(h, \nk, {v}) 12 unk(h, k, {v}) 12 is, memo . memo * memo. The memoized f speci.cation 11 therefore allows \ncalls to f to be replaced with memoized f, 22 unk(h, k, {v})* unk(h, k, {v}) unk(h, k, {v})1 outdef (h, \nk)1 . indef (h, k, v)1 remove(h, k) outdef (h, k)1 The key step in this proof is the use of the .nal \naxiom from Figure 3 to convert a complete unk predicate into the disjunction of an in and out predicate. \nIn both cases, the remove operation results in an index where the key k is de.nitely unassigned.  4.2 \nExample: Memoization A common client application of indexes is memoization: storing the results of expensive \ncomputations to avoid having to recompute them. Our speci.cation can verify that a mem\u00adoized function \ngives the same result as the original function. Suppose that f is a side-effect free procedure implement\u00ading \nthe (mathematical) function f. A memoized version of f, memoized f, can be implemented using the index \nmemo as follows: even in parallel. A proof of the speci.cation for memoized f is shown in Figure 4. Evicting \nmemoised values. We may want to periodically evict memoised values from the index, for example to ensure \nthat the number of stored values does not grow inde.nitely. Using our index speci.cation, we can show \nthat values can be evicted in parallel with memoised_f(). We model eviction by the function evict_f, \nwhich non\u00addeterministically removes keys from the index: evict_f() { while (...) { k := nondet(); remove(memo,k); \n}} where nondet() returns an arbitrary key value and the Boolean assertion for while is not given. (A \nmore nuanced eviction function might store timestamps along with the memoised values, and evict only \nold values. For simplic\u00adity, we choose not to model this.)  We give evict_f the following speci.cation: \nmemo evict f() memo A proof of this speci.cation is given in Figure 5. Because we can split and join \nmemo arbitrarily, we can reason as follows: memo memo * memo memo memo evict f() r := memoised f(v) \nmemo memo . r = f(v) memo * (memo . r = f(v)) memo . r = f(v) Consequently, it is safe to run the memoised \nversion of f in parallel with eviction from the index.  4.3 Example: The Sieve of Eratosthenes Let us \nconsider an example where many threads require write access to the same shared value in a concurrent \nin\u00addex. We choose the Sieve of Eratosthenes [1, 13], an algo\u00adrithm for generating all of the prime numbers \nup to a given maximum value max. The sieve is a simple algorithm, but it is representative of a class \nof algorithms where threads co\u00adoperatively race to delete elements of shared data. Similar behaviour \noccurs in databases when deleting stale records, and in rendering when removing objects outside of a \nclipped region. The algorithm starts by constructing a set of integers from 2 (since 1 is not a prime \nnumber) to max. We use an index to represent the set of (candidate) prime numbers. A set can be viewed \nas an instance of an index where the set of values is a singleton (in this example, we use {0}). A key \nis either present, representing that it is in the set, or not: the value itself conveys no information. \nWe assume a function idxrange that creates an index with mappings for keys in a speci.ed range. v For \neach integer in the range 2 .. lmaxJ, a thread is created that removes multiples of that integer from \nthe set. Once all threads have completed, the remaining elements of the set are exactly those with no \nfactors in the range v 2 .. lmaxJ (excluding themselves), and hence exactly the prime numbers less than \nor equal to max. The code for the implementation is given in Figure 6. The procedure sieve is the main \nsieve function, which uses the recursive parwork procedure to run each worker thread in parallel. The \nprocedure worker is the implementation of the worker threads. The speci.cation for sieve is emp . max \n> 1 x := sieve(max) \u00aei.[2..max]. isPrime(i) . indef (x, i, 0)1 .\u00acisPrime(i) . outdef (x,i)1 sieve(max) \n{ idx := idxrange(2, max); parwork(2, max, idx); return idx; } parwork(v, max, idx) { worker(v, max, \nidx) { if (v = sqrt(max)) { c := v + v; worker(v, max, idx) while (c = max) || remove(idx, c); parwork(v+1, \nmax, idx) c := c + v; } } } } Figure 6. Prime sieve functions. where the predicate emp denotes no resource \nat all, and the predicate isPrime(i) holds exactly when i is prime. We also de.ne the predicate fac(i, \nv, vi) , which holds when i has a factor (distinct from itself) in the range [v .. vi]: . fac(i, v, vi)= \n.j. v = j = v i . j = i . (i mod j)=0 The proof that sieve meets its speci.cation is given in Figure \n7. This proof requires we establish the following speci.cation for worker: 2 = v . \u00aei.[2..max]. inrem(idx, \ni, 0)t worker(v, max, idx) \u00aei.[2..max]. fac(i, v, v) . outrem(idx,i)t . \u00acfac(i, v, v) . inrem(idx, i, \n0)t This speci.cation expresses that the worker removes all mul\u00adtiples of v from the set; any other elements \nwill still be present unless they are removed by another thread. The fact that (for v = vi) fac(i, v, \nv) . fac(i, v +1,v i) .. fac(i, v, vi) allows us to conclude that the parwork procedure eliminates exactly \nthe set elements with factors different from them\u00adselves in the range v .. max. Since > 1 is prime if \nand only p v 1 if it has no factor in the range 2 .. p , for i . [2 .. max] pv 1 \u00acfac(i, 2, max ) .. \nisPrime(i). Together with the index axioms that allow rem predicates to be switched to def predicates \nwhen full permission is held, this lets us establish the postcondition of sieve. 5. Iterating an Index \nThe high-level speci.cation discussed so far does not allow us to explore the contents of an arbitrary \nindex. To use search, we must know which keys we seek. If we do not (and the set of keys is in.nite), \nwe cannot write a program that examines all the values stored in the index. To handle this case, we add \nimperative iterators, based loosely on those in Java. Iterators have three operations:  \u00af . emp . max \n> 1 sieve(max) { it := createIter(h) creates a new iterator for index h.  (k, v) := next(it) returns \nsome key-value pair in the  index for which it is an iterator. The returned pair will be one that has \nnot been returned by a previous call to next on it. When all key-value pairs have been returned, the \ncall returns (nil, nil). idx max); \u00aei.[2..max]. inrem(idx, i, 0)1 parwork(2, max, \u00af . idxrange(2, := \nidx); v // By properties of prime numbers and To iterate an index, one creates a new iterator, calls \nnext () destroyIter(it) frees the iterator it. \u00aei.[2..max]. fac(i, 2, LmaxJ) . outrem(idx,i)1 . v \u00acfac(i, \n2, LmaxJ) . inrem(idx, i, 0)1 until it returns (nil, nil), then frees the iterator. Notice that the next \nprocedure just returns some key-value pair, placing no order on the iteration. This keeps the iterator \nspeci.ca\u00ad tion general, as many underlying implementations have no // \u00aei.[2..max]. isPrime(i) . indef \n(idx, i, 0)1 .\u00acisPrime(i) . outdef (idx,i)1 ( index axioms ) return idx; natural ordering. ret = idx \n. \u00aei.[2..max]. isPrime(i) . indef (idx, i, 0)1 .\u00acisPrime(i) . outdef (idx,i)1 }( ) As in Java, we do \nnot allow full mutability of an index be\u00ad ing iterated. We allow partial mutability: keys can be safely \nmodi.ed once they have been returned by next. \u00af . 2 = v . \u00aei.[2..max]. inrem(idx, i, 0)t parwork(v, max, \nidx) { Iterator speci.cation. An iterator is represented by the ab\u00ad if (v 8< = sqrt(max)) { stract predicate \niter(it, h, S, K, i), which describes an itera\u00ad 9 2 = v . \u00aei.[2..max]. inrem(idx, i, 0) = * tor it, \niterating over index h. The set S contains the key\u00advalue pairs that are in the index and have not yet \nbeen re\u00adturned by next, while K is the set of keys that are not as\u00ad t  2 : ; max, . t 2 max, idx) 2 \n= v +1 . \u00aei.[2..max]. inrem(idx, i, 0) worker(v, I parwork(v+1, 8>>>>< idx) 9 >>>>= t 2 signed in the \nindex. The iterator has de.nite permission i for every key in keys(S) . K. Our speci.cation for the three \niterator operations is \u00aei.[2..max]. fac(i, v, v) . outrem(idx,i)\u00acfac(i, v, v) . inrem(idx, i, 0)maxJ) \n. outrem(idx,i) t 2 v \u00aei.[2..max]. fac(i, v+1, L v .\u00acfac(i, v+1, L shown in Figure 8. Creating an iterator \nfor an index requires de.nite information about the state of each key in that index, in the form of indef \nand outdef predicates for all keys. It is not >>>>: >>>>; t 2 t 2 maxJ) . inrem(idx, i, 0) // Using permission \ncombination axioms v ( ) sensible for two threads to share the same iterator, as each thread will iterate \nover an unknown subset of the underlying index. As such, the iter predicate cannot be split for sharing \n\u00aei.[2..max]. fac(i, v, L\u00acfac(i, v, L maxJ) . outrem(idx,i)t . v maxJ) . inrem(idx, i, 0)t \u00aei.[2..max]. \nfac(i, v, L\u00acfac(i, v, L }( } between threads. However, notice that we can create multi\u00ad ple iterators \nfor a single index, as createIter requires only fractional permission for each key. ) v maxJ) . outrem(idx,i)t \n. v maxJ) . inrem(idx, i, 0)t The two speci.cations for next handle the case where the client has not \nyet seen all key-value pairs in the iterator (in which case, a pair is returned non-deterministically), \nand \u00af . 2 = v . \u00aei.[2..max]. inrem(idx, i, 0)t worker(v, max, idx) { c := v+ v; when it has (in which \ncase, nil is returned for both the key and value). Destroying an iterator liberates all of the index \npredicates that have not been returned by next, including (c = max) { while 8>< 9>= -1)]. fac(i, v, \nv) . outrem(idx,i)t . \u00aei.[2..(c the outdef predicates. \u00acfac(i, v, v) . inrem(idx, i, 0)t * \u00aej.[c..max]. \ninrem(idx, j, 0)t c); >: >; remove(idx, 5.1 Example: a more powerful map. c := c + v; }} ( \u00aei.[2..max]. \nfac(i, v, v) . outrem(idx,i)t . \u00acfac(i, v, v) . inrem(idx, i, 0)t In \u00a73.1, we veri.ed rangeMap, an algorithm \nthat mapped all values in an index from a given key range through a function, replacing the values with \nthe result. Using an iterator, we ) can de.ne a concurrent map that does not require a key range, and \nworks over all entries in an index. To avoid having Figure 7. Proofs for the sieve and worker programs. \nto reason about function pointers, we assume the particular function f is baked into the algorithm source. \n \u00ae(k,v).S indef (h, k, v)i * \u00aek .keys(S) outdef (h,k)i it := createIter(h) iter(it, h, S, keys(S),i) \n(k, v) . S . iter(it, h, S \\{(k, v)}, K, i) * iter(it, h, S, K, i) . S = \u00d8 (k, v) := next(it) indef (h, \nk, v)i iter(it, h, \u00d8, K, i)(k, v) := next(it) iter(it, h, \u00d8, K, i) . k = nil . v = nil iter(it, h, S, \nK, i) destroyIter(it) \u00ae(k,v).S indef (h, k, v)i * \u00aek.K outdef (h, k)i Figure 8. Speci.cation for iterators. \nFor createIter, set S denotes the key-value pairs of h, keys(S) denotes the assigned keys of h, and keys(S) \ndenotes the unassigned keys. . \u00af \u00ae(k,v).S indef (h, k, v)1 * \u00aek .keys(S) outdef (h,k)1 map_f(h) { it \n:= createIter(h); . \u00af iter(it, h, S, keys(S), 1) map_worker(it, h); . \u00af iter(it, h, \u00d8, keys(S), 1) * \n\u00ae(k,v).S indef (h, k, f(v))1 destroyIter(it); } . \u00af \u00ae(k,v).S indef (h, k, f(v))1 * \u00aek .keys(S) outdef \n(h,k)1 .\u00af iter(it, h, S, K, 1) map_worker(it, h) { (k, v) := next(it); j . (k, v) . S . iter(it, h,S \n\\{(k, v)}, K, 1) * indef (h, k, v)1 . (iter(it, h, \u00d8, K, 1) . k = nil . v = nil) if (k = nil){ . \u00af (k, \nv) . S . iter(it, h,S \\{(k, v)}, K, 1) * indef (h, k, v)1 ( . \u00af (k, v) . S . indef (h, k, v)1 remove(h, \nk); insert(h, k, f(v)); . \u00af (k, v) . S . indef (h, k,f(v))1 ) || . \u00af iter(it, h,S \\ (k, v), K, 1) map_worker(it, \nh); . \u00af iter(it, h, \u00d8, K, 1) * \u00ae(kl,vl).S\\(k,v) indef (h,k ' ,f(v ' ))1 }} . \u00af iter(it, h, \u00d8, K, 1) * \n\u00ae(k,v).S indef (h, k, f(v))1 Figure 9. Proof outline for map f. map_f(h) { map_worker(it, h) { it := \ncreateIter(h); (k,v) := next(it); map_worker(it, h); if (k = nil){ destroyIter(it); ( remove(h, k); } \ninsert(h, k, f(v));) || map_worker(it, h); }} A proof of correctness for map f is given in Figure 9. \n 5.2 Example: counting distinct values. We can use an index to store discovered information, and then \nuse iteration to summarise what has been discovered. To illustrate this, we give an algorithm which counts \nthe number of distinct values stored in a tree. Both the tree and the secondary store is used for recording \ndistinct values are implemented using our index speci.cation. Our algorithm is de.ned as follows: count(it,k,is) \n{ fetch(it,k,is) { fetch(it,k,is); if (k = nil) { itr := createIter(is); (k1,k2,v) := num := 0; search(it,k); \n(k,v) := next(itr); insert(is,v,k); while(k = nil) { (fetch(it,k1,is) || num := num + 1; fetch(it,k2,is)); \nremove(is,k); } (k,v) := next(itr); } } destroyIter(itr); return num; } The function count calls fetch \nto construct the index is from the values of the tree in the index it. Values can appear at more than \none tree node, but are only recorded once in the is index. count then iterates the index is, counting \nthe number of distinct values discovered. We de.ne a tree predicate annotated with the set of values \nstored in the tree: . tree(h, k, vs)= .k1,k2, vs1, vs2, v. (k = nil . vs = \u00d8. emp) . . . tree(h, k1, \nvs1) * tree(h, k2, vs2) . . * indef (h, k, (k1,k2,v))1 . vs = vs1 . vs2 .{v} fetch and count satisfy \nthe following speci.cations: tree(it, k, vs) * \u00aekl . outins(is,ki , Keys)i fetch(it, k, is) tree(it, \nk, vs) * \u00aekl./vs. outins(is,ki , Keys)i * \u00aekl.vs. inins(is,ki , Keys)i tree(it, k, vs) * \u00aekl . outdef \n(is,ki)1 count(it, k, is) tree(it, k, vs) * \u00aekl . outdef (is,ki)1 . ret = |vs| Figure 10 shows an outline \nproof of these speci.cations. The part of the proof associated with searching the tree is similar in \nstructure to O Hearn et al s proof of tree disposal  \u00af . tree(it, k, vs) * \u00aekl . outins(is,k ' , ItKeys)i \nfetch(it,k,is) { that proving implementations is an obligation on the writer of the module clients can \nreason using our speci.cation without any knowledge of such proofs. We .rst introduce a (k = nil) { 9= \nsimple list-based implementation and show that it satis.es if 8< : .k1,k2, vs1, vs2, v. tree(it,k1, vs1) \n* tree(it,k2, vs2) the disjoint speci.cation of \u00a73. This example is given to de\u00ad velop our technical \napproach. We then prove that a hash ta\u00adble implementation satis.es the sharing speci.cation of \u00a74. Finally, \nwe show that our approach scales to quite complex * indef (it, k, (k1,k2,v))1 * \u00aekl . outins(is,k ' , \nItKeys)i . vs = vs1 . vs2 .{v} ;9 (k1,k2,v) .vs1, vs2. tree(it, k1, vs1) * tree(it, k2, vs2) 8< := search(it,k); \n= implementations, by outlining our proof that the BLink tree * indef (it, k, (k1, k2, v))1 * \u00aekl . outins(is,k \n' , ItKeys)i . vs = vs1 . vs2 .{v} : ; algorithm satis.es the sharing speci.cation. Approach: Concurrent \nAbstract Predicates. We use the insert(is,v,k); .vs1, vs2. tree(it, k1, vs1) * tree(it, k2, vs2) 8>>>< \n9>>>= i 2 techniques developed in the work on concurrent abstract predicates (CAP) [7] to prove that \nindex implementations * indef (it, k, (k1, k2, v))1 * inins(is, v, ItKeys) * \u00aekl=v . outins(is,k ' , \nItKeys) * \u00aekl . outins(is,k '  satisfy our speci.cation. This approach extends separation logic with \nboth explicit reasoning about sharing within mod\u00ad ules, and a powerful abstraction mechanism that can \nhide >>>: >>>; i 2 . vs = vs1 . vs2 .{v} , ItKeys) i 2 ( fetch(it,k1,is) || fetch(it,k2,is) ); } sharing \nfrom clients. } Sharing between threads is represented in CAP by shared tree(it, k, vs) * \u00aekl . outdef \n(is,k ' )1 ifying type of mutations threads can perform on P . Asser\u00ad count(it,k,is) { tions on shared \nregions behave additively under *, that is, fetch(it,k,is); 8< j . tree(it, k, vs) * \u00aekl./vs. outins(is,k \n' r , ItKeys)i regions, denoted by boxed assertions of the form . The P I * \u00aekl.vs. inins(is,k ' , ItKeys)i \nassertion P describes the contents of the region, r is the name of the region, and I is an interface \nenvironment spec\u00ad \u00af . 9= r . = r * r P . QP Q tree(it, k, vs) * \u00aekl./vs. outdef (is,k ' )1 * I I I '' \n' \u00aekl.vs. .v . ItKeys. (k ' ,v ) . S . indef (is,k ' ,v )1 A shared region can be mutated by the environment \nthreads. This means that assertions about shared regions must be stable: that is, invariant under other \nthreads interference. : ; . vs = keys(S) itr := createIter(is); := 0; num tree(it, k, vs) * iter(itr, \nis, S, vs, 1) . vs =keys(S) . num =0 . Often, different threads can perform different operations \u00af over \na shared resource: for example, they may be able to (k,v) := next(itr); mutate different keys in a shared \nindex. To represent this be\u00ad while(k = nil) { num:=num +1; haviour, CAP introduces capabilities. These \nare resources giving a thread the ability to perform particular operations. remove(is,k); .vs ' ,S ' \n. tree(it, k, vs) * iter(itr, is,S ' , vs, 1) * 8< 9= Threads can hold both non-exclusive and exclusive \ncapabil\u00ad ities. When an exclusive capability is held, no other thread \u00aekl.vs\\vsl . outdef (is,k ' )1 \n.|vs ' | + num = |vs| : ; can perform the associated operation. Shared regions and capabilities can be \nabstracted using . vs ' = keys(S ' (k,v) := next(itr); ) predicates in the manner described in \u00a72. Each \npredicate rep\u00ad } tree(it, k, vs) * iter(itr, is, \u00d8, vs, 1) * . resents both some information about \na shared region, and \u00aekl.vs. outdef (is,k ' )1 . num = |vs| some ability held by the thread to modify \nthe shared re\u00addestroyIter(itr); gion. If the combination of capabilities held ensures that the j return \nnum; shared assertion is invariant, then stability need not be con\u00ad . } tree(it, k, vs) * \u00aekl . outdef \n(is,k ' )1 . ret = |vs| \u00af Figure 10. Outline proofs of count and fetch. using concurrent separation \nlogic [16]. The difference is that we are able to reason abstractly about concurrently inserting into \nthe is index. 6. Verifying Index Implementations In this section, we verify three quite different concurrent \nin\u00addex implementations against our abstract speci.cation. Note sidered by clients, and the predicate \ncan be treated abstractly. In the discussion below, we assume the proof system and semantics given in \n[7], and only give details necessary for understanding the proof structure. The interested reader is \nreferred to [7] for other technical details, including a proof of soundness for the CAP logic. 6.1 Linked \nList Implementation To illustrate our approach, we consider a very simple index implementation which \nuses a linked list with a single lock protecting the entire list3. The code for this implementation 3 \nThis example is quite similar to the coarse-grained set example from [7].  search(h, k) { remove(h, \nk) { lock(h.lk); lock(h.lk); e := h.nxt; e := h.nxt; while (e = nil) { prev := h; if (e.key = k) { while \n(e = nil) { unlock(h.lk); if(e.key = k) { return e.val; prev.nxt := e.nxt; } disposeNode(e); e := e.nxt; \nunlock(h.lk); } return; unlock(h.lk); } return nil; prev := e; } e := e.nxt; } insert(h, k, v) { unlock(h.lk); \nlock(h.lk); } e := h.nxt; while (e = nil) { if (e.key = k) { unlock(h.lk); return; } e := e.nxt; } e \n:= makeNode(k,v,h.nxt); h.nxt := e; unlock(h.lk); } Figure 11. Linked list operations. is given in Figure \n11. In order to simplify the presentation, we only consider the disjoint speci.cation of \u00a73 in this section. \nAdditional technicalities are required to handle the full sharing speci.cation of \u00a74. We give these technicalities \nin \u00a76.3, when we verify the BLink tree implementation against the sharing speci.cation. Before performing \nany operation on the list, the thread .rst acquires the lock. The search operation traverses the list \nchecking if an element matches the key; if so, it returns the corresponding value. The insert operation \nis similar to search. However, if it cannot .nd the key, it creates a new node and adds it to the head \nof the list. The remove operation searches for the key to be removed. If it .nds the key, it updates \nthe previous node in the list to point to the following node. The node, having been thus removed from \nthe list, is then deleted. Interpretation of abstract predicates. In order to prove that the operations \nof the implementation are correct with respect to our speci.cation, we .rst give concrete interpreta\u00adtions \nto the abstract predicates. We begin by de.ning a predicate ls(a, H), corresponding to list with address \na and representing the index state H : Keys -Vals. This is de.ned in terms of the inductive predicate \nlseg(a, b, H), which represents a list segment with address a and .nal pointer b, having key-value elements \ngiven by H. A list segment is either empty, in which case a = b and H = \u00d8, or it consists of a node at \naddress a whose key and value are taken from H, and whose nxt .eld points to a list segment of the rest \nof the keys and values. The de.nition of lseg is, in turn, de.ned in terms of the predicate node(a, k, \nv, n), which simply represents a node at address a whose key, val and nxt .elds are k, v, and n respectively. \nThe formal de.nitions of these predicates are as follows: . node(a, k, v, n)= a.key . k * a.val . v * \na.nxt . n . lseg(a, b, H)=(a = b . H = \u00d8) . .k, v, n, Hi.H = Hi I{k . v}. node(a, k, v, n) * lseg(n, \nb, Hi) . ls(a, H)= lseg(a, nil,H) Using the ls predicate, we can give a concrete interpreta\u00adtion to our \nindex predicates for the linked list implementa\u00adtion of an index, as follows: . in(h, k, v)= .r, l, H. \nH(k)= v . [LOCK(k)]r 1 * r lock(h.lk, r, k) * h.nxt . l * ls(l, H) I(r,h) . out(h, k)= .r,l,H.k /. dom(H) \n. [LOCK(k)]1 r * r lock(h.lk, r, k) * h.nxt . l * ls(l, H) I(r,h) Here, the boxed assertion describes \nthe region r shared between all the threads that can access the list. This boxed assertion says that \nregion r contains a lock at h.lk (we de.ne the predicate lock below) and a pointer h.nxt to a list representing \nthe contents of the index. The index state H is existentially quanti.ed; the assertions only specify \nwhether the key k is in the index, and its value, if any. Both predicates also include the (unshared) \ncapability re\u00adsource [LOCK(k)]r. A thread with such a capability in its lo\u00ad 1 cal state is able to update \nthe contents of the corresponding region r by performing the LOCK(k) action that is de.ned in the interference \nenvironment I(r, h) associated with the re\u00adgion. We will give the formal de.nition of I(r, h) presently; \nintuitively, the LOCK(k) action allows a thread to acquire the lock in order to subsequently add or remove \nthe key k. The subscript 1 in the capability denotes that it is an exclu\u00adsive capability: no other thread \ncan perform the action. The exclusivity of the permission ensures that the predicates are stable, since \nthe state of key k in the index cannot be changed by any other thread. We de.ne the predicate lock(x, \nr, k) as follows: unlocked(x, r) . [MOD(i)]r = x . 0 * 1 i.\u00aeKeys locked(x, r, j) . [MOD(i)]r = x . 1 \n* 1 i.Keys\\{j} \u00ae lock(x, r, k) . = unlocked(x, r) ..j = k. locked(x, r, j) This lock predicate contains \na shared lock bit and a collec\u00adtion of capabilities. Each capability [MOD(k)]r 1 controls the  \u00af . out(h, \nk) ability to add or remove a particular key k from the shared list in region r. When these capabilities \nare in the shared re\u00ad gion, no thread is able to modify the list; such is the case when the lock is unlocked. \nWhen the lock is locked, a sin\u00ad insert(h, .r, l, H. k ./dom(H) . [LOCK(k)]r 1 * ( k,v) { ) lock(h.lk, \nr, k) * h.nxt . l * ls(l, H) r I(r,h) gle [MOD(j)] r 1 capability is held by some thread, allowing it \nto perform the necessary update, but only to the key j. The lock(h.lk, r, k) predicate ensures, that \nno other thread can have the [MOD(k)] r 1 capability, and hence update key k. lock(h.lk); capability \n[LOCK(k)]r 1. ) .r, l, H. k ./dom(H) . [MOD(k)]r 1 * [LOCK(k)]r 1 * ( // use the r locked(h.lk, r, k) \n* h.nxt . l * ls(l, H) I(r,h) e := h.nxt; Describing Interference. The meaning of the capabilities [LOCK(k)] \nr 1 and [MOD(k)] r 1 is determined by the interfer\u00ad ence environment associated with the region r: I(r, \nh). This while .r, l, H, H1,H2,k ' ,v ' , n. k ./dom(H) . 8>>>>>< (e = nil) { 9>>>>>= [MOD(k)]r 1 * [LOCK(k)]r \n1 * r de.nes the possible state mutations that can occur over a locked(h.lk, r, k) * h.nxt . l * lseg(l, \ne,H1) * node(e,k ' ,v ' >>>>>: >>>>>; given shared region. The environment de.nes the meaning of capabilities \nin terms of actions, written P \" Q. When a ,n) * ls(n, H2) . H1 H2 {k ' . v ' } = H I(r,h) thread holds \na capability mapped to an action P \" Q, it is if (e.key \u00af . part matching Q. To perform the action, \na thread may trans-unlock(h.lk); fer resource between the region and its own local state, and return; \nmay mutate it in an atomic operation. } =k) { permitted to replace a part of the region matching P with \na false // this branch is for k in the set For the linked list implementation, the interference envi\u00ad \nronment I(r, h) is de.ned as follows: := e.nxt; e8>>>< .r, l, H, H1,H2. k ./dom(H) . [MOD(k)]r 1 * [LOCK(k)]r \n1 * r 9>>>= . . . h.nxt . l * ls(l, H) \" h.nxt . li * ls(li,H I{k . v}) >>>: locked(h.lk, r, k) * h.nxt \n. l * lseg(l, e,H1) * ls(e,H2) . H1 H2 = H I(r,h) >>>;) MOD(k): }( .r, l, H. k ./dom(H) . [MOD(k)]r 1 \n* [LOCK(k)]r 1 * h.nxt . l * ls(l, H) . . \" h.nxt . li * ls(li,H \\{k}) r I(r,h) locked(h.lk, r, k) * \nh.nxt . l * ls(l, H) h.lk . 0 * [MOD(k)] r 1 \" h.lk . 1 := makeNode(k,v,h.nxt); e8>< .r, l, H. k ./dom(H) \n. node(e, k, v,l) * 9>= LOCK(k): h.lk . 1 \" h.lk . 0 * [MOD(k)] r 1 [MOD(k)]r 1 * [LOCK(k)]r 1 * r locked(h.lk, \nr, k) * h.nxt . l * ls(l, H) >; >; unlock(h.lk); 8>< ( h.nxt capability [MOD(k)]r 1. .r, l, H. k ./dom(H) \n. [MOD(k)]r 1 * [LOCK(k)]1 r * r >: The de.nition of MOD(k) says that a thread holding a ca\u00ad pability \n[MOD(k)] r 1 is allowed to update the list by adding or removing the key k. The de.nition of LOCK(k) \nsays that the thread is allowed to set or unset the lock bit. Recall that I(r,h) := e; // use the 9>= \nlocked(h.lk, r, k) * h.nxt . e * >: actions replace part of the shared state, so the de.nition of node(e, \nk, v,l) * ls(l, H) I(r,h) LOCK(k) implies that a thread acquiring the lock also ac\u00ad quires the capability \n[MOD(k)] r 1 , which leaves the shared state. Similarly, when releasing the lock it must give up the \n// use the ) capability [LOCK(k)]r 1. .r, l, H. H(k)= v . [LOCK(k)]r 1 * r I(r,h) lock(h.lk, r, k) * \nh.nxt . l * ls(l, H) capability [MOD(k)] r 1 . In this way, acquiring the lock gives a thread the ability \nto modify the contents of the list. } . in(h, k, v) \u00af Verifying the operations. Having given concrete \nde.ni-Figure 12. Proof outline for linked list insert. tions to the index predicates, we can verify \nthat the mod\u00adule s implementations of add, remove and search match our high-level speci.cation. Figure \n12 shows one such proof, h.nxt:=e assigns to the shared location h.nxt. This mu\u00adestablishing that the \nimplementation of insert matches the tation corresponds to performing the .rst of the actions as\u00ad sociated \nwith the [MOD(k)] r 1 capability, held in the localfollowing abstract speci.cation: out(h, k) insert(h, \nk, v) in(h, k, v) In the proof, mutations of the shared state require that the thread holds a capability \npermitting the mutation. These points in insert are annotated by program comments. For example, towards \nthe end of insert, the assignment state. The action requires that initially h.nxt should point to a list \nrepresenting some index state H, and that after the assignment it should point to a list representing \nthe state H I{k . v} for some v. By considering the predicate de.nitions, this is clearly the case. It \nis necessary to check that the all assertions in the proof are stable. In fact, once the lock has been \nacquired, the only actions which can affect the shared state are MOD(k) and LOCK(k). Since full permission \nto both is held in local state, no interference can happen, and so the assertions are stable.  For the \npre-and postconditions, the list may be locked or unlocked, but it can only be modi.ed with respect to \nkeys other than k. Since no information about such keys is contained in these assertions, they are also \nstable. Verifying the axioms. As well as proving the speci.ca\u00adtions for the operations, our other obligation \nis establishing that implementation satis.es the axioms of the abstract spec\u00adi.cation. To do this, we \nuse the concrete de.nitions for the abstract predicates. For example, we prove the following ax\u00adiom from \nthe disjoint speci.cation: in(h, k, v) * out(h, k)=. false If we expand the predicate de.nitions on the \nleft-hand side of this implication, we end up with the following assertion: .r, l, H. H(k)= v . [LOCK(k)]r \n1 * r lock(h.lk, r, k) * h.nxt . l * ls(l, H) * I(r,h) .r,l,H.k /. dom(H) . [LOCK(k)]r 1 * r lock(h.lk, \nr, k) * h.nxt . l * ls(l, H) I(r,h) The memory location h.nxt cannot belong to more than one region at \nonce, so we can infer that both existentially\u00adquanti.ed rs must refer to the same shared region. The \nca\u00adpability [LOCK(k)]r is exclusive, denoted by the 1 subscript. 1 Now [LOCK(k)]r 1 * [LOCK(k)]r =. false \n1 and so the axiom holds.  6.2 Hash Table Implementation We now consider a second index implementation \nwhich uses a hash table. The hash table algorithm consists of a .xed\u00adsize array and a hashing function \nmapping from keys to offsets in the array. Each element of the array is a pointer to a secondary index \nstoring the key-value pairs that hash to the associated array offset. Secondary indexes are often implemented \nas linked lists, but in fact any kind of index implementation can be used. In this section, we assume \nthat secondary indexes are imple\u00admented by some module matching our abstract speci.cation, but do not \nspecify which. (To avoid naming con.icts, we rename the methods and predicates of the secondary index \ni to searchi , inserti , remove, ini , etc..) We then def , ini rem show that the resulting hash table \nmodule also matches our abstract speci.cation. That is, we show that we can build a concurrent index \nusing a (different) index module. The hash table implementations of search, insert and remove are given \nin Figure 13. This code assumes a pure hashing function hash which takes a key k and returns an integer \nhash(k) between 0 and max - 1, where max is the size of the hash table array. search(h, k) { w := hash(k); \na := [h+w]; return (search ' (a, } k)); remove(h, k) { w := hash(k); a := [h+w]; remove ' (a, k); } insert(h, \nk, v) { w := hash(k); a := [h+w]; insert ' (a, k, v); } Figure 13. Hash table operations.  Although \nthe implementation we consider here is very simple, it captures the essence of more complicated im\u00adplementation \ns such as Java s ConcurrentHashMap, which uses resizable hash tables as a secondary index. It would be \ninvaluable to consider such real-world implementations in detail, but this is beyond the scope of the \npresent work. Interpretation of abstract predicates. All of our index predicates inins, outins, inrem, \nand so on consist of a shared region containing a hash table pointer, and a local predicate representing \nthe associated secondary index. Picking an ar\u00adbitrary example, we de.ne inrem(h, k, v)i as follows: r \n. h + hash(k) . hi * true inrem(h, k, v)i = .r, hi . * ini (hi, k, v)i rem (The predicates have exactly \nthe same form. Only the predi\u00adcate pertaining to the secondary index changes.) The shared region contains \na pointer from h + hash(k) to the address of the secondary index, hi. The rest of the hash table array \nalso belongs to the shared region; it is represented in the assertion by true. The array of pointers \nrepresenting the hash table is read only, so the interference environment for the shared region is empty. \nThe secondary index is represented by the predicate ini (hi, k, v)i. Note that this de.nition hides completely \nrem the implementation of the secondary index. The hash table simply knows that this element of the index \ncan be queried according to the abstract index speci.cation. State mutations on the secondary index are \nalready captured by the predicate representing it, meaning that they need not be considered when verifying \nthe hash table implementation. Verifying the operations. A sketch-proof for the hash table implementation \nof search is given in Figure 14. Notice that this proof appeals to the speci.cation of searchi when retrieving \na value from the appropriate secondary index. Since there are no actions de.ned for the shared region, \nstability of our assertions is trivial. Verifying the axioms. The axioms follow from the axioms of the \nsecondary index. In particular, two predicates involv\u00ading the same key will be de.ned in terms of predicates \nwhich must be on the same secondary index.  .\u00af indef (h, k,v)i search(h, k) { no (h + hash(k)) . h ' \n* true .r, h ' . r * in ' , k,v)i def (h ' w := hash(k); a := [h+w]; no .r. (h + hash(k)) . a * true \nr * in ' def (a, k,v)i return (search ' (a, k)); // search ' specification j r . .r, h ' . h + hash(k) \n. h ' * true * in ' , k,v)i def (h ' . ret = v } .\u00af indef (h, k,v)i . ret = v Figure 14. Proof outline \nfor hash table search.  6.3 BLink Tree Implementation Our .nal index implementation is Sagiv s BLink \ntree algo\u00adrithm [20]. (Note that we only consider the algorithm with\u00ad out compression here.) A BLink \ntree is a balanced search tree. An example is shown in Figure 15. The leaves of the tree contain they \nkey-value pairs stored in the index in order. Non-leaf (or inner) nodes associate keys with pointers \nto nodes at the next level down, which direct the traversal of the tree. In addition, the .nal pointer \nin each node s list, the link pointer, points to the next node at that level (if it exists). The tree \nis accessed through a prime block which holds pointers to the .rst node at each level in the tree. During \ninserts, nodes of the tree that are at full capacity may be split by creating a new right sibling and \ntransferring half of the keys to the new node. This new node must then be attached to the level above, \nwhich might require further splittings. However, other operations may still need to tra\u00adverse the tree \nbefore this operation is completed. A traversal in progress may therefore have to use link pointers to \n.nd the correct leaf. Since the minimum values of nodes are al\u00adways preserved, and every leaf with a \nminimum value no less than that of a given node is reachable from that node, such traversals are always \npossible. Search operations on a BLink tree are lock-free, and insert and remove operations lock only \none node (or two if they are modifying the root node) at a time, making this a highly concurrent implementation \nof an index. This index algorithm is much more complex than the list or hash table, and is therefore \nconsiderably more challenging to verify. The full details of the BLink tree implementation are too lengthy \nto describe in detail. We only consider search in any depth here; for full details, see the technical \nreport [5]. Node notation. We use the notation node(l, k, p, D, ki,pi) to denote the contents of a node, \nwhere: l determines whether the node is locked (l =1) or not (l =0),  k is the minimum key for the \nnode (less than or equal to all keys that are reachable from it),  p is the pointer to the left-most \nchild for an inner node, or nil for a leaf node,  D is a list of pairs of keys and child pointers (for \ninner nodes), or keys and stored values (for leaf nodes),  ki is the maximum key for the node,  pi \nis the pointer to the next sibling of the node (or nil if it is the last).  We also use inner(l, k, \np, D, ki,pi) to denote an inner node with the given contents (requiring that p = nil) and leaf(l, k, \nD, ki,pi) to denote a leaf node. In this notation, the contents of node 3 in Figure 15 would be represented \nas inner(0, -8, 1, [(22, 2), (38, 7)], 44, 8). Interpretation of abstract predicates. All of our index \npredicates are de.ned as a shared region containing a BLink tree and a collection of shared capabilities, \nas well as some thread-local capabilities. For example, the predicate indef (h, k, v) is de.ned as follows: \nr . B.(h, k, v) indef (h, k, v)i = .r. * dcaps(k, r, i) I(r,h) The shared assertion B.(h, k, v) denotes \na BLink tree at ad\u00address h containing the key-value pair (k, v). We omit the for\u00admal de.nition of this \npredicate here, which may be found in the technical report [5]. The predicate dcaps(k, r, i), which is \nde.ned in Figure 16, consists of capabilities associated with the current thread. The permission subscripts \nof the capabilities are more complex than those we have seen so far: they are deny\u00adguarantee permissions \n[9]. A guarantee permission, indi\u00ad cated by the subscript (g, i) for 0 <i = 1 (or simply g when we do \nnot care about the exact value of i), allows a thread to perform the associated action. If the permission \nis less than 1, other threads may have guarantee permissions to perform the same action it is a non-exclusive \npermission. A deny permission, indicated by the subscript (d, i) (or, again, sim\u00adply d), does not allow \nthe thread to perform the associated action, but precludes the possibility that any other thread will \neither. Fractions of the same type may be combined by ad\u00addition, and (g, 1) = 1 = (d, 1) represents exclusive \npermis\u00adsion; however, a deny permission and a guarantee permission cannot be combined, since they are \ncon.icting. (For further details, see Dodds et al. [9].) The intuitive meaning of the capabilities in \nthe dcaps predicate is as follows. The [LOCK]r capability says that the g current thread is allowed to \nlock nodes in the region r. The [SWAP]r capability allows the indef predicate to be modi.ed g to represent \ndifferent behaviour (for example, by converting it to inrem or unk) provided i =1. The [REM(0,k)]r (d,i) \ncapability says that neither the current thread, nor any other thread, is allowed to remove the key k \nfrom the BLink tree in region r. However, if i =1, then the current thread has the exclusive capability \nto remove key k from the tree. The  dcaps(k, r, i)=[.LOCK]r * [SWAP]r * [REM(0,k)]r * [INS(0, k, v)]r \ngg (d,i)(d,i) v.\u00aeVals  r i pi . node(-,ki , -, -, -, -) * true niceNode(N, k, v, r, h) . ,p ki =+8. \n= .k0,p0, D, ki . . I(r,h)  N = inner(-,k0,p0, D, ki,pi) ..(k, p) . D. N = leaf(-,k0, D, ki,pi) . r \n. p . node(-, k, -, -, -, -) * true (k0 <k = ki . (k, v) . D) I(r,h) Figure 16. Predicates used in \nthe BLink tree proofs. [INS(0, k, vi)]r capabilities similarly restrict the ability to (d,i) insert value \nvi at key k. The other index predicates are de.ned in a similar way to indef . For example, the de.nition \nof the inrem(h, k, v)i pred\u00adicate will include a REM capability for k with permission (g, i), so that \nany thread may remove the key from the tree, as well as all INS capabilities for k with permission (d, \ni), so that no thread may insert values for the key into the tree. The full de.nitions of the predicates \nmay be found in the technical report [5]. Describing Interference. The interference environment, I(r, \nh), for the BLink tree implementation is markedly more complex than for the list or hash table. It involves \na sub\u00adstantial amount of capability swapping to track changes to the shared state and to thread behaviour. \nFigure 17 gives a few examples of de.nitions in the interference environment. These de.nitions can be \nread as follows: LOCK allows a thread to lock a node in the BLink tree. When locking, the thread acquires \nthe exclusive capabil\u00adity [UNLOCK(x)]r, allowing it to unlock the node again.  REM(t, k) allows a thread \nto give up [REM(t, k)]r  1 (g,i) and [UNLOCK(x)]r and acquire the exclusive capability 1 [MODLR(t, x, \nk, i)]r 1. This means that a thread which is allowed to remove the key k from the tree and holds the \nlock on a node x can acquire the right to remove the key k from the leaf node x (the value t is used \nto track capability transfer in some environments). MODLR(t, x, k, i) allows a thread to remove a key-value \npair (k, -) from a leaf node. In doing so, the thread gives up the capability [MODLR(t, x, k, i)]r and \nreacquires the 1 capability [UNLOCK(x)]r, and, if t =0, the capability 1 [REM(k)]r . (We write - to indicate \nan unspeci.ed, (g,i) existentially quanti.ed value.) Full details of the interference environment may \nbe found in the technical report [5]. Note that both [REM(0,k)]r and [REM(1,k)]r ca\u00ad (g,i)(g,i) pabilities \nallow a thread to remove the key k; however, the latter requires the thread to leave a [REM(1,k)]r capa\u00ad \n(g,i) bility behind in the shared state when it does so. This is used to implement the inrem predicate: \nif none of the threads with inrem(k, v) predicates remove k then between them they must still be able \nto produce the full [REM(1,k)]r capability, 1 proving that none of them did so. Thus the inrem(k, v)1 \ncan be converted to indef (k, v)1. Verifying the operations. We give a sketch proof in Fig\u00adure 18, showing \nthat the BLink tree implementation of search matches the following speci.cation: indef (h, k,v)i r := \nsearch(h, k) indef (h, k,v)i . r = v The search operation only mutates thread-local state, so the thread \ndoes not require capabilities to perform actions. However, by owning deny permissions (d, i) on all the \nREM and INS capabilities for key k, the thread can establish that no other thread can modify the value \nassociated with k.  LOCK : x . node(0,k0, p, D, ki,pi) * [UNLOCK(x)]r x . node(1,k0, p, D, ki,pi) 1 \n REM(t, k): [MODLR(t, x, k, i)]r [REM(t, k)]r * [UNLOCK(x)]r 1(g,i)1 .. .. x . leaf(1,k0, D, ki,pi) \n* [UNLOCK(x)]r 1 x . leaf(1,k0,Di,ki,pi) .. MODLR(t, x, k, i): . * [REM(t, k)]r . t =0 . emp . t =1 .. \n* [MODLR(t, x, k, i)]r . (g,i)1 . D = Di I (k, -) . (k, -) . D Figure 17. Example actions from the BLink \ntree interference environment. Thus, the assertion that the key-value pair (k,v) is contained inthe BLink \ntree is stable. The proof uses the predicate niceNode(N, k, v, r, h), de\u00ad.ned in Figure 16. The de.nition \nof niceNode asserts that the node descriptor N contains legitimate information about the \u00af . indef (h, \nk,v)i search(h, B.(h, k,v) n k) { o tree. If N is an inner node, then the children and link pointers \nr * dcaps(k, r, i) of N must all point to extant nodes in the tree, which have I(r,h) PB := getPrimeBlock(h); \nthe minimum values speci.ed by N this ensures that fol- N8>< cur := root(PB); lowing a pointer reaches \nan appropriate node. If N is a leaf node into whose range the key k falls, then the key-value pair (k, \nv) must be stored in N this ensures that the search := get(cur); 9>= r * dcaps(k, r, i) B.(h, k,v) \nI(r,h) * niceNode(N, k, v, r, h) will return the correct value. >:>; . N = node(-,k0, p, D, k ' ,p ' \n) . k0 = -8 Assertions in the proof must be stable that is, invari\u00ad }8>< while(isLeaf(N) = false) { \nant under interference from other threads. The stability of cur := next(N, k); niceNode is ensured by \nthe fact that the capabilities held by N := get(cur); the thread do not allow nodes to be removed, the \nminimum 9>= values of nodes to change, or key k to be changed. r * dcaps(k, r, i) B.(h, k,v) A bug in \nthe BLink tree algorithm. I(r,h) While verifying the al\u00ad * niceNode(N, k, v, r, h) >:>; gorithm, we discovered \na subtle bug in the original presen\u00ad . N = leaf(-,k0, D, k ' ,p ' ) . k0 < k tation [20]. The bug can \noccur during an insert, when a while(k > highValue(N)) { thread splits a tree node which itself was the \nresult of another cur := next(N, k); }8>< thread splitting the tree root. In order to insert the new \nnode N := get(cur); into the tree, the .rst thread will look in the prime block for 9>= the node s parent. \nHowever, the second thread might not yet r * dcaps(k, r, i) I(r,h) B.(h, k,v) have written a pointer \nto the new root, resulting in an invalid * niceNode(N, k, v, r, h) >:>;9> dereference. Our solution was \nto require that a thread split\u00ad < k = k '' . N = leaf(-,k ' , D, k '' , -) . k ' ting the current the \nroot locks the new node. A thread trying to insert must wait until the creation of the root is complete. \nA detailed trace exhibiting this bug can be found in [5]. if(isIn(N, 8>< B.(h, k,v) k)) { r * dcaps(k, \nr, i) = I(r,h) * niceNode(N, k, v, r, h) . N = leaf(-,k ' , D, k '' , -) . (k,v) . D  >:>; 7. Conclusions \n return( lookup(N, k) ); We have proposed a simple, abstract speci.cation for reason\u00ad \u00af. n } else satility \nof our speci.cation, verifying a representative range return nil; } { ing about concurrent indexes. We \nhave demonstrated the ver\u00ad false of client applications ranging from common programming o patterns such \nas memoization and map, to algorithms such B.(h, k,v) r * dcaps(k, r, i) . ret = v I(r,h) as a prime \nnumber sieve. We have demonstrated that our par\u00ad } . indef (h, k,v)i . ret = v \u00af ticular choice of \nindex speci.cation is satis.ed by three rad\u00ad ically different concurrent implementations, based on sim\u00adply \nlinked lists, hash tables, and Sagiv s complex and highly Figure 18. Proof outline for the BLink tree \nsearch. concurrent BLink trees respectively. Relationship to linearizability. Linearizability [12] is \nthe current de-facto correctness criterion for concurrent algo\u00adrithms. It requires that the methods of \nconcurrent objects behave as atomic operations, thus providing a proof tech\u00adnique for observational re.nement \n[11]. We could employ linearizability, or other atomicity re.nement techniques such as [22], as a proof \ntechnique for verifying that implementa\u00ad tions meet our abstract speci.cation: an implementation that \nmeets the sequential speci.cation of an index and whose operations behave atomically can easily be shown \nto meet the concurrent speci.cation. However, this simply shifts the proof burden; our approach is able \nto verify clients and im\u00adplementations in a single coherent proof system.  While linearizability assures \nthat index operations behave atomically, our abstract speci.cation makes no such guaran\u00adtee. Instead, \nour client proofs enforce abstract constraints on the possible interactions between threads, such as \nonly al\u00adlowing removals on a certain key. Consequently, while all linearizable indexes can be shown to \nimplement our speci\u00ad.cation, our speci.cation also admits implementations that are not linearizable. \nFor instance, an index that implemented removal by performing the operation twice in succession could \nmeet our speci.cation, but would not be linearizable. As a more realistic example, consider the program: \ninsert(k, 0) insert(k, 1) x := search(k) y := search(k) Linearizability ensures that, after executing \nthe program, the variables x and y will be equal (one or the other insert must come .rst, and a second \ninsert has no effect). How\u00adever, our speci.cation does not ensure this. An implemen\u00adtation in which writes \nare cached, for instance, may satisfy our speci.cation, but fail to provide this stronger guaran\u00adtee. \nIn practice, the strength of speci.cations is often traded against performance. We have shown how our \napproach can provide weak (\u00a73) and strong (\u00a74) speci.cations of concur\u00ad rent behaviour. Our approach \ncould therefore be seen as a .exible alternative to linearizability as a correctness crite\u00adrion for concurrent \nprograms. Acknowledgments. Special thanks to Moshe Vardi for challenging us to verify Sagiv s concurrent \nBLink tree algo\u00adrithm, and to Adam Wright for substantial contributions to the section on iteration (\u00a75), \nand invaluable discussions and feedback overall. Thanks also to Bornat, Jones, Shapiro, and many researchers \nat Cambridge, Imperial and Queen Mary working on separation logic, for discussions and feedback. We acknowledge \nfunding from an EPSRC DTA (da Rocha Pinto), EPSRC programme grant EP/H008373/1 (da Rocha Pinto, Dinsdale-Young, \nGardner and Wheelhouse) and EP-SRC grant EP/H010815/1 (Dodds). References [1] BLELLOCH, G. E. Programming \nparallel algorithms. Com\u00admun. ACM 39 (March 1996), 85 97. [2] BOYLAND, J. Checking interference with \nfractional permis\u00adsions. In Static Analysis (2003). [3] CALCAGNO, C., GARDNER, P., AND ZARFATY, U. Context \nLogic and tree update. In POPL (2005), ACM. [4] DA ROCHA PINTO, P. Reasoning about Concurrent Indexes. \nMaster s thesis, Imperial College London, Sept. 2010. [5] DA ROCHA PINTO, P., DINSDALE-YOUNG, T., DODDS, \nM., GARDNER, P., AND WHEELHOUSE, M. A simple abstraction for complex concurrent indexes. Tech. rep., \nImperial College London, 2011. [6] DILLIG, I., DILLIG, T., AND AIKEN, A. Precise reasoning for programs \nusing containers. SIGPLAN Not. 46 (2011). [7] DINSDALE-YOUNG, T., DODDS, M., GARDNER, P., PARKINSON, \nM., AND VAFEIADIS, V. Concurrent abstract predicates. In ECOOP (2010). [8] DINSDALE-YOUNG, T., GARDNER, \nP., AND WHEEL-HOUSE, M. Abstraction and Re.nement for Local Reasoning. In VSTTE (2010). [9] DODDS, M., \nFENG, X., PARKINSON, M., AND VAFEIADIS, V. Deny-guarantee reasoning. In ESOP (2009). [10] FENG,X.,FERREIRA,R., \nAND SHAO,Z.Ontherelationship between concurrent separation logic and assume-guarantee reasoning. In ESOP \n(2007). [11] FILIPOVIC, I., O HEARN, P., RINETZKY, N., AND YANG, H. Abstraction for concurrent objects. \nIn ESOP (2010). [12] HERLIHY, M. P., AND WING, J. M. Linearizability: a cor\u00adrectness condition for concurrent \nobjects. ACM Trans. Pro\u00adgram. Lang. Syst. 12 (July 1990), 463 492. [13] HOARE,C.A.R.Proofofastructuredprogram: \nThesieveof Eratosthenes . The Computer Journal 15, 4 (1972), 321 325. [14] KUNCAK, V., LAM, P., ZEE, \nK., AND RINARD, M. C. Mod\u00adular pluggable analyses for data structure consistency. IEEE Trans. Softw. \nEng. 32 (December 2006), 988 1005. [15] MALECHA, G., MORRISETT, G., SHINNAR, A., AND WIS-NESKY, R. Toward \na veri.ed relational database management system. In POPL (2010). [16] O HEARN, P. W. Resources, concurrency, \nand local reason\u00ading. Theor. Comput. Sci. 375 (April 2007), 271 307. [17] PARKINSON, M., AND BIERMAN, \nG. Separation logic and abstraction. In POPL (2005). [18] PHILIPPOU, A., AND WALKER, D. A process-calculus \nanal\u00adysis of concurrent operations on b-trees. J. Comput. Syst. Sci. 62, 1 (2001), 73 122. [19] REYNOLDS, \nJ. Separation logic: a logic for shared mutable data structures. In LICS (2002). [20] SAGIV,Y.ConcurrentoperationsonB*-treeswithovertaking. \nJournal of Computer and System Sciences 33 (October 1986). [21] SEXTON,A., AND THIELECKE,H.ReasoningaboutB+trees \nwith operational semantics and separation logic. ENTCS 218 (2008). [22] TURON, A. J., AND WAND, M. A \nseparation logic for re.ning concurrent objects. In POPL (2011). [23] VAFEIADIS, V., AND PARKINSON, M. \nA marriage of re\u00adly/guarantee and separation logic. CONCUR (2007).    \n\t\t\t", "proc_id": "2048066", "abstract": "<p>Indexes are ubiquitous. Examples include associative arrays, dictionaries, maps and hashes used in applications such as databases, file systems and dynamic languages. Abstractly, a sequential index can be viewed as a partial function from keys to values. Values can be queried by their keys, and the index can be mutated by adding or removing mappings. Whilst appealingly simple, this abstract specification is insufficient for reasoning about indexes accessed concurrently. We present an abstract specification for concurrent indexes. We verify several representative concurrent client applications using our specification, demonstrating that clients can reason abstractly without having to consider specific underlying implementations. Our specification would, however, mean nothing if it were not satisfied by standard implementations of concurrent indexes. We verify that our specification is satisfied by algorithms based on linked lists, hash tables and B-Link trees. The complexity of these algorithms, in particular the B-Link tree algorithm, can be completely hidden from the client's view by our abstract specification.</p>", "authors": [{"name": "Pedro da Rocha Pinto", "author_profile_id": "81490690271", "affiliation": "Imperial College London, London, United Kingdom", "person_id": "P2839282", "email_address": "pmd09@doc.ic.ac.uk", "orcid_id": ""}, {"name": "Thomas Dinsdale-Young", "author_profile_id": "81458657026", "affiliation": "Imperial College London, London, United Kingdom", "person_id": "P2839283", "email_address": "td202@doc.ic.ac.uk", "orcid_id": ""}, {"name": "Mike Dodds", "author_profile_id": "81418593776", "affiliation": "University of Cambridge, Cambridge, United Kingdom", "person_id": "P2839284", "email_address": "mike.dodds@cl.cam.ac.uk", "orcid_id": ""}, {"name": "Philippa Gardner", "author_profile_id": "81351597297", "affiliation": "Imperial College London, London, United Kingdom", "person_id": "P2839285", "email_address": "pg@doc.ic.ac.uk", "orcid_id": ""}, {"name": "Mark Wheelhouse", "author_profile_id": "81351608810", "affiliation": "Imperial College London, London, United Kingdom", "person_id": "P2839286", "email_address": "mark.wheelhouse03@imperial.ac.uk", "orcid_id": ""}], "doi_number": "10.1145/2048066.2048131", "year": "2011", "article_id": "2048131", "conference": "OOPSLA", "title": "A simple abstraction for complex concurrent indexes", "url": "http://dl.acm.org/citation.cfm?id=2048131"}