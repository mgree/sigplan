{"article_publication_date": "10-22-2011", "fulltext": "\n HAWKEYE: Effective Discovery of Data.ow Impediments to Parallelization Omer Tripp * Greta Yorsh John \nField Tel Aviv University ARM Google omertrip@post.tau.ac.il greta.yorsh@arm.com j.eld@google.com Mooly \nSagiv Tel Aviv University msagiv@post.tau.ac.il Abstract Parallelization transformations are an important \nvehicle for improving the performance and scalability of a software sys\u00adtem. Utilizing concurrency requires \nthat the developer .rst identify a suitable parallelization scope: one that poses as a performance bottleneck, \nand at the same time, exhibits con\u00adsiderable available parallelism. However, having identi.ed a candidate \nscope, the developer still needs to ensure the cor\u00adrectness of the transformation. This is a dif.cult \nundertaking, where a major source of complication lies in tracking down sequential dependencies that \ninhibit parallelization and ad\u00addressing them. We report on HAWKEYE, a dynamic dependence-analysis tool \nthat is designed to assist programmers in pinpointing such impediments to parallelization. In contrast \nwith .eld\u00adbased dependence analyses, which track concrete memory con.icts and thus suffer from a high \nrate of false reports, HAWKEYE tracks dependencies induced by the abstract se\u00admantics of the data type \nwhile ignoring dependencing arising solely from implementation artifacts. This enables a more concise \nreport, where the reported dependencies are more likely to be real as well as intelligible to the programmer. \nCategories and Subject Descriptors D.2.5 [Software En\u00adgineering]: Testing and Debugging * Research supported \nby an Open Cooperative Research grant from IBM Research to Stanford and Tel Aviv Universities. Research \ndone when author was a staff member at IBM Research. Permission to make digital or hard copies of all \nor part of this work for personal or classroom use is granted without fee provided that copies are not \nmade or distributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. OOPSLA 11, October 22 27, 2011, Portland, Oregon, USA. Copyright \nc . 2011 ACM 978-1-4503-0940-0/11/10. . . $10.00 General Terms Measurement,Experimentation Keywords loop \nparallelization, commutativity, dependence analysis, dynamic analysis, abstract data types 1. Introduction \nProgram dependence graphs are a useful aid in software par\u00ad allelization [13]. However, parallelization \nof existing pro\u00ad grams using dependence analysis remains challenging. One reason is the inherent dif.culty \nof computing static depen\u00ad dencies in programs with complex data structures. Yet even the dynamic dependencies \nexhibited by a single execution trace may embody super.uous information. Consider the following illustrative \nexample (in Java): 1 Pair[] pairs = makePairs(); 2 Map m = new ConcurrentHashMap (); 3 for (int i= 0; \ni<pairs.length; ++i) { 4 m.put(pairs[i].fst, pairs[i].snd); 5 compute(pairs[i].snd); } Assume that (i) \nthe fst .eld of each Pair object points to a unique object, but there are two distinct Pair objects whose \nsnd .elds point to a shared object, and (ii) compute modi.es some .elds of the object pointed-to by its \nargument. A dynamic dependence-analysis tool based on .eld reads and writes will infer that the two relevant \ncompute invo\u00ad cations depend on each other, which would in turn inhibit parallelization of the loop. \nHowever, it would also indicate dependencies between put calls. These dependencies arise from access \nto the internal .elds of ConcurrentHashMap.1 These dependencies are spurious, in the sense that they \ndo not re.ect genuine constraints on parallelization. Spurious dependencies obscure the real constraints \non parallelization, and thus stand in the developer s way toward 1 For example, when we ran this code \nusing the IBM V9 JRE, de\u00adpendencies were reported on ConcurrentHashMap$HashEntry.hash, ConcurrentHashMap$Segment.modCount, \nand several other internal .elds of ConcurrentHashMap.  arriving at a parallel version of the code. \nIn the above exam\u00adple, instead of focusing the developer s attention on the only real impediment to parallelization \nthe two compute calls operating on the same object a concrete dependence anal\u00adysis would also overwhelm \nthe developer with (potentially many) put/put con.icts that can all safely be ignored. Scope This paper \ntakes a step toward reporting useful dy\u00adnamic dependencies in support of programmer-guided paral\u00adlelization. \nWe assume that the programmer has already iden\u00adti.ed a suitable scope for parallelization, where there \nare both performance bottlenecks and a great deal of available parallelism. The remaining challenge is \nto uncover (the few) dependencies that impede parallelization and address them. Na\u00a8ive reporting of (concrete) \ndata dependencies often yields a prohibitive amount of spurious dependencies. It is then the developer \ns responsibility to distill the real con\u00adstraints on parallelization, a task that in many cases obviates \nthe value of using a dependence analysis in the .rst place. This paper tackles the challenge of reporting \ndata.ow im\u00adpediments to parallelization in a precise, concise and intelli\u00adgible manner. Our Approach \nWe build on the centrality of abstract data types (ADTs) in the design and development of object\u00adoriented \nsoftware. Data structures implementing ADTs can come from libraries, but may also be user-de.ned data \ntypes. In our approach, concrete data structures are considered in terms of the semantics of the ADTs \nthey represent during dependence analysis. This allows the analysis to suppress spurious dependencies \ndue to the speci.cs of the ADT im\u00adplementation, and report con.icts at the semantic level. We have implemented \nour approach in HAWKEYE, a dynamic dependence analysis that accounts for ADT semantics. Past research \nhas already addressed the problem of spu\u00adrious dependencies, though there are some important dif\u00adferences, \nwhich we now brie.y discuss. The commutativity analysis framework [30 32], used in parallelizing compil\u00aders, \nemploys symbolic reasoning and other specialized al\u00adgorithms to recognize and exploit commuting operations. \nHAWKEYE, instead, is a dynamic analysis. HAWKEYE also utilizes abstraction, rather than commutativity \nproofs, to ig\u00adnore spurious dependencies. HAWKEYE is also reminiscent of recent works in the area of \ntransactional memory [16, 20 23] that leverage ADT se\u00admantics for more robust online con.ict detection. \nUnlike these works, HAWKEYE performs of.ine con.ict detection. Moreover, the HAWKEYE speci.cation is \nin the form of a representation function, rather than a commutativity speci\u00ad.cation, which in our experience \nfacilitates support for user types. Contributions This paper makes the following contribu\u00adtions: Effective \ndependence analysis. Our approach augments dynamic dependence analysis with con.ict detection based on \nADT semantics. This supports our objective of concise and precise dependencies, as our experimental results \ncon.rm. The reported dependencies are also more intelligible to the user being at the semantic level. \n Flexible speci.cation language. HAWKEYE enables a wide spectrum of speci.cations for a given ADT that \nrepresent different tradeoffs between the complexity of the speci.cation and the cost and precision of \nthe anal\u00adysis. Our experience suggests that the simplest form of speci.cation, whereby commutativity \nbetween ADT op\u00aderations is inferred automatically based (only) on a de.\u00adnition of the ADT s representation \nfunction, suf.ces for most ADTs. This facilitates the de.nition and incorpora\u00adtion of user types into \nthe analysis.  Uniform con.ict-detection framework. The theoretical underpinnings of our analysis, which \nwe evolve in Sec\u00adtions 3 6, enable uniform treatment of concrete and se\u00admantic dependencies. This property \nof our framework simpli.es reasoning on the behavior of the analysis, as well as implementation effort. \n Implementation and evaluation. We have implemented our approach in HAWKEYE, and evaluated it via two \nsets of experiments. First, we compared HAWKEYE and an analogous analysis that is unaware of ADT semantics \nby using both to discover loop-carried dependencies in seven real-world benchmarks. We then applied parallelization \ntransformations in three of these benchmarks guided by the dependencies reported by HAWKEYE to assess \nits value in end-to-end parallelization. The results are highly encouraging: HAWKEYE reported signi.cantly \nfewer de\u00adpendencies than the baseline analysis, and the surviv\u00ading dependencies represented real impediments \nto par\u00adallelization, which greatly facilitated manual paralleliza\u00adtion of the benchmarks we studied. \n Organization In the remainder of this paper, we .rst illus\u00adtrate our approach via a real-world example \nin Section 2. Then, in Sections 3 6, we establish a uniform framework for detecting both semantic con.icts \nbetween ADT opera\u00adtions and concrete con.icts between non-ADT operations. The implementation of HAWKEYE \nis described in Section 7, which also reports on its experimental evaluation. Section 8 discusses related \nwork. We conclude in Section 9.  2. Overview Consider the (pseudo-)code fragment in Fig. 1 taken from \nthe JGraphT library [4], which builds the block-cutpoint graph [33] representation of a connected undirected \ngraph. The block-cutpoint graph of connected undirected graph G is a bipartite graph connecting cutpoints \nin G to their containing blocks , where node v in G is called a cutpoint if its removal disconnects G, \nand subgraph b of G is considered a block if it is a maximal subgraph of G not containing a cutpoint \nas an independent graph (i.e., disregarding the rest of G). Fig. 2 presents a simple example of a connected \ngraph (G) and its corresponding block-cutpoint graph (G/).  iter.hasNext() iter.hasNext() iter.hasNext() \n cutpoint = iter.next() 1 for (Vertex cutpoint : this.cutpoints) { 2 UndirectedGraph subgraph = new \nSimpleGraph (); 3 subgraph.addVertex(cutpoint ); 4 this.cutpointGraphs.put(cutpoint, subgraph); 5 this.addVertex(subgraph \n); 6 Set blocks = this.vertex2blocks.get(cutpoint); 7 for (UndirectedGraph block : blocks) { 8 int oldHitCount \n= this.block2hits.get(block); 9 this.block2hits.put(block, oldHitCount+1); 10 this . addEdge ( subgraph \n, block ); } } Figure 1. Simpli.ed pseudo-code version of the JGraphT algorithm for building a block-cutpoint \ngraph  (G)(G/) Figure 2. Connected undirected graph G and its corre\u00adsponding block-cutpoint graph G/ \nThe code in Fig. 1 builds a BlockCutpointGraph in\u00adstance, G/ (pointed-to by the this pointer), where \nthe nodes of G/ are themselves graphs, by iterating over the cutpoints in the graph G underlying G/. \nFor each cutpoint c,(i)a fresh graph enclosing c (line 3) is added to G (line 5), (ii) the blocks c is \ncontained in (which were computed ahead of the loop) are retrieved (line 6), and (iii) these blocks are \ntra\u00adversed by an inner loop (line 7) that inserts an edge into G/ between c and each of them via an addEdge \ncall (line 10). Prior to the edge insertion, a counter counting the number of times each block was encountered \nis incremented (line 9). Program Parallelization The loop in Fig. 1 has a great deal of available parallelism: \nDifferent iterations process dis\u00adtinct cutpoints, and thus manipulate disjoint portions of the block-cutpoint \ngraph. Dependence analysis is a useful tech\u00adnique for testing this observation: Loop-carried dependen\u00adcies \nreported by the analysis pose as potential impediments to parallelization, which the developer can review \nand if needed address before parallelizing the loop. Dually, fail\u00adure to .nd dependencies (by a sound \nanalysis) is a correct\u00adness proof for the parallelization transformation. Unfortunately, existing dependence \nanalyses, which track con.icts at the level of conrete memory locations, are overly pessimistic. Fig. \n3, which visualizes the dynamic loop\u00adcarried dependencies recorded when running the loop with graph G \nfrom Fig. 2 as input, illustrates this. The many reported dependencies obscure the parallelization potential \nlatent in the loop. Most of these dependencies are due to the internals of collection implementations \n(e.g., the HashMap implementing vertex2block), and are thus spurious. Par\u00adallelizing compilers are likely \nto arrive at an even more con\u00ad subgraph = new SimpleGraph(...) subgraph = new SimpleGraph(...) subgraph.addVertex(...) \nsubgraph.addVertex(...) vertex2block.put(...) vertex2block.put(...) addVertex(...)  addVertex(...) \nblocks = ... blocks = ... iterator = ... iterator = ... iterator.hasNext() iterator.hasNext() oldHitCount \n= ... oldHitCount = ... block2hits.put(...) block2hits.put(...) block = ... block = ... addEdge;.. \naddEdge;.. iterator.hasNext() iterator.hasNext() oldHitCount = ... oldHitCount = ... block2hits.put(...) \n block2hits.put(...) block = ... block = ... addEdge(...) addEdge(...) iterator.hasNext() iterator.hasNext() \nFigure 3. Dynamic dependence graph for the program in Fig. 1 running on the graph in Fig. 2 servative \nresult due to their limited ability to handle aliasing and track graph invariants maintained by G (which \nis a tree). The important dependencies (denoted in red) are (i) those involving the induction variable \n(iter), as well as (ii) a speci.c pair of calls to put on block2hits where block {v3,v5}, which is shared \nby cutpoints v3 and v5,isused as the key. Each of these essential dependencies can be addressed by standard \nparallelization techniques. For (i), cutpoints can be changed into a random-access list, or alter\u00adnatively, \nthe sequential loop structure can be preserved while making the loop body asynchronous. For (ii), atomicity \ncan be guaranteed for the statements at lines 8 9 via appropriate synchronization (e.g., locks or transactions), \nor block2hits can be privatized and its different copies can be merged after the loop. As this example \ndemonstrates, often the challenge in par\u00adallelization is not how to transform the code, but where to \napply the transformation. A developer confronted with the dependence graph of Fig. 3 is likely to spend \na long time separating the wheat from the chaff, or even give up at some point. Our goal is to distill \nthe report by a dependence anal\u00adysis toward more concise and precise reporting of candidate impediments \nto parallelization. Dependence Analysis with ADTs In our approach, ADT instances are treated at the semantic \nrather than the con\u00adcrete level. The developer speci.es the ADT implemented by a concrete type (that \nis known to have a concurrent im\u00adplementation), and the analysis then computes dependencies according \nto this speci.cation, which enforces a more pre\u00adcise notion of commutativity between ADT operations. \nThe result by the analysis is then more accurate and thus also more actionable.  Representation Functions \nfor Map and UndirectedGraph MAP(m): state = \u00d8 for each e in m.entrySet key(e) state = state . m . e.key \nvalue(e) state = state . e.key . e.value return state UNDIRECTEDGRAPH(g): state = \u00d8 for each n in g.nodeSet \nnode(n) state = state . g . n for each e in g.edgeSet target(e) target(e) state = state .{e.n1 . e.n2,e.n2 \n. e.n1}return state Figure 4. Pseudo-code version of the representation func\u00adtions for the Map and UndirectedGraph \nADTs used by the program in Fig. 1 Our choice of description for ADTs is as a store mapping logical .elds \nto their respective values. For example, the Map ADT can represent keys as logical .elds that point to \ntheir associated values, which is the natural way of perceiving a mapping. A pleasing property of this \nrepresentation is that it enables a uniform con.ict-detection framework based on data dependencies, which \nwe formalize in Sections 3 6. HAWKEYE accepts ADT speci.cations in the form of a Java method receiving \na concrete data-structure instance as input and returning the object s state as an ADT. This is il\u00adlustrated \nin Fig. 4, where for example the speci.ca\u00adtion for UndirectedGraph lets the state of graph instance \ng map g to the nodes it contains via entries of the form (g, node(n)). n, and node u map to its neighbors \nvia en\u00adtries of the form (u, target(e)). v (where e = {u, v}). For the program in Fig. 1, HAWKEYE reports \nall the essen\u00adtial dependencies (those in red), and only these dependen\u00adcies, based only on a user-provided \nrepresentation function for the UndirectedGraph ADT. (HAWKEYE already has built-in representation functions \nfor the Java collections.) Technical Outline The algorithm undelying HAWKEYE is evolved incrementally \nin Sections 3 6. Sections 3 4 formal\u00adize the connection between parallelization and dynamic data dependencies. \nFirst, in Section 3, we de.ne a correctness cri\u00adterion for parallel execution of a single trace (assuming \nan interleaving semantics of concurrency) whereby a parallel schedule is correct iff all the interleavings \nit permits of state\u00adments from its underlying trace yield the .nal state of the trace. We further discuss \nhow (local) reasoning about com\u00admutativity between adjacent trace transitions can be utilized toward \nconservative enforcement of this criterion. Section 4 then links the criterion to data dependencies based \non the ob\u00adservation that absence of data dependencies implies commu\u00adtativity. Importantly, this section \nde.nes the read and write accesses made by a trace transition in a way that we can later use both in \nconcrete and in abstract semantics, which is key for the uniformity of our framework. Sections 5 6 build \non the results of the two preceding sections. Section 5 instantiates the de.nitions of Section 4 both \nin a standard concrete semantics where a state is en\u00adcoded as a store mapping stack and heap locations \nto their corresponding values, and in an abstract semantics where states are structured as mappings from \nabstract locations to abstract values. Finally, Section 6 considers abstraction us\u00ading representation \nfunctions, and presents an ef.cient algo\u00adrithm for computing dependencies between ADT operations based \non the de.nitions of Section 4.  3. Correct Parallelization We start by introducing the notations we \nuse for program states and traces. We then discuss how commutativity be\u00adtween statements in a trace can \nbe used to reason about the correctness of its parallel execution. Low-level Program Semantics A state \ns : Ly. V is a mapping from memory locations to values. We use dom(s) to denote the domain of the mapping, \ni.e., the subset of memory locations for which the partial function s is de.ned. The intended meaning \nis that dom(s) includes the memory locations that are allocated in state s (see Section 5.1). The set \nof all states is denoted by S.Weuse [[p]](s) to denote the state resulting from executing program statement \np in state s . S. A transition t is a triple (s, p, s/), where p is a program statement and s, s/ . S \nare states such that s/ =[[p]](s). The set of all transitions is denoted by T .A trace t is a sequence \n(t1,t2,...,tn) of transitions, such that ti = ). T for 1 = i = n and s/si+1 for (si,pi,s/= ii 1 = i<n. \nParallel Execution Semantics Let t = (t1,t2,...,tn) be a trace, where pi is the i-th statement executed \nin t. Consider a partitioning of the statements pi into consecutive segments I1,...,Ik according to indices \n1= i1 < ... < ik+1 = n + 1, where Ij = (pij ,...,pij+1-1). Ij intuitively corresponds to a single command, \nwhich preserves the ordering between the statements comprising it under parallelization. We de.ne an \nexecution schedule for I1,...,Ik via the standard composition operators: 1. Sequential composition. We \ndenote the sequential exe\u00adcution of Ij and Ij+1 by Ij ; Ij+1. 2. Parallel composition. We denote concurrent \nexecution of consecutive segments Ij and Ij+1 by Ij IIj+1. I is transitive in that IjIIj+1IIj+2 denotes \nthat Ij , Ij+1 and Ij+2 will be run concurrently to each other.  An execution schedule of segments I1,...,Ik \ncorrespond\u00adingtotrace t is thus of the form I1 * 1 I2 * 2 ... * k-1 Ik, (1) where * m .{; , I} for 1 \n= m = k.  For example, schedule I1; I2II3II4; I5 prescribes that .rst I1 is run, then I2, I3 and I4 \nrun concurrently to each other, and .nally I5 is executed. We assume an interleaving semantics of concurrency \n[27, 28], where the single steps of concurrent segments are in\u00adterleaved with each other (in an unscheduled \nway). For Ij I ... IIj+m, all possible permutations of the statements comprising these segments that \npreserve the internal order\u00ading of statements in Ij,...,Ij+m are possible. Commutativity Let t = (t1,t2,...,tn) \nbe a trace. We denote the set of statements executed in t by P (t)= {pi | 1 = i = n}. We say that consecutive \nstatements pi,pi+1 . P (t) commute in t if the trace from running (p1,...,pi+1,pi,...,pn) starting at \ns1 has .nal state s/. n That is, swapping between pi and pi+1 does not change the .nal state of the run. \nWe refer to two traces as equivalent if (i) they are both over the same set of statements, and (ii) their \nstarting and .nal states are identical: t = (t1,...,tn)= tt = (tt1,...,ttn). s1 = st1 . P (t)= P (tt) \n. sn/= st/n. (2) Our de.nition of commutativity between statements sup\u00adports this notion of equivalence. \nDenote by Pp(t) the organi\u00adzation of the statements in P (t) as a sequence (according to their ordering \nin t). Considering traces t and tt,if(i) s1 = st1, and (ii) Pp(tt) is a transposition of Pp(t) where \nthe transposed statements are commutative in t, then by de.nition t and tt are equivalent according to \n(2). We record this connection between t and tt via (symmetric) relation =1: t =1 tt. We can now de.ne \n=n inductively: //// . t// =1 t/ t =n t/ ..t.t =n-1 t. Note that for all n . N,if t =n t/ then t = t/. \nCorrect Parallel Schedule Let t be a trace and S a (par\u00adallel) schedule based on t (according to (1)). \nWe say that S is correct with respect to t if every possible trace t/ re\u00adsulting from executing S is \nequivalent to t. If we consider t as a single run of some program P , then a complementary perspective \nis enabled: A transformation of P that makes P more concurrent is incorrect if it results in a parallel \nexecu\u00adtion schedule based on t yielding traces that are inequivalent to t. In this sense, information \nfrom a single trace provides an upper bound on available parallelism that can be used to test a proposed \ntransformation. Note that we can use any subset of the equivalence rela\u00adtion de.ned in (2) toward a conservative \njudgment about the correctness of a schedule, which may reject valid schedules being unable to prove \nthem as such. In particular, we can use =k.= or even a subset of =k that is built k.N k.N based on a \nsafe approximation of the commutativity relation between statements, where a safe commutativity judgment \nis one where two statements are said to be commutative (in some trace) if they actually commute, but \nthe opposite is not necessarily true. The motivation for using a conservative correctness judg\u00adment stems \nfrom the cost and tractability of direct reasoning about equivalence between traces. In what follows, \nwe ap\u00adproximate the commutativity property by considering data dependencies between trace transitions. \n 4. Formalizing Dependencies In this section, we formally de.ne the notion of data depen\u00addencies. We \nthen restate the (well-known) connection be\u00adtween data dependencies and commutativity [13], whereby absence \nof data dependencies implies commutativity, in our setting of dynamic dependencies recorded based on \na single execution trace. 4.1 Trace Dependence Graph We .rst de.ne the notion of best read and write \nsets, as well as and their conservative approximation. We then use these sets to de.ne dependencies in \na standard way. Read and Write Sets Each transition t = (s, p, s/) is associated with a write set write(t) \nand a read set read(t) of memory locations, which we now de.ne. The write set is the set of memory locations \nmodi.ed by the transition: write(t ) \u00a3 mod(t) . alloc(t) . dealloc(t ) (3) where mod(t ) \u00a3 {m . dom(s) \nn dom(s/) | s(m)= s/(m)}alloc(t) \u00a3 dom(s/) \\ dom(s) dealloc(t) \u00a3 dom(s) \\ dom(s/) The .rst line handles \nchange of values of existing loca\u00adtions. The second and third lines handle memory allocation and deallocation, \nrespectively. We assume that identities of memory locations do not change, and that deallocation is an \nexplicit transition (i.e., the garbage collector is treated as part of the program). The read set is \nthe set of memory locations whose values determine the effect of p. Intuitively, a memory location m \nis in the read set of (s, p, s/) if altering its value in s affects the result of executing p. Formally, \nto ensure its uniqueness, the read set is de.ned as the union of all minimal suf.cient read sets:  read(t) \n\u00a3 {M . R(t ) |.M/ . R(t).M/ . M} (4) where R(t) is the set of all suf.cient read sets of transition t \n, i.e., M . R(t ) if and only if M . dom(s) and for every  ts/ . S, s, t if M . dom(st) .m . M.s(m)= \nst(m) tts, p, t = (ts/). T then mod(t)= mod(tt) (5) alloc(t )= alloc(tt) dealloc(t )= dealloc(tt) .m \n. write(t) n dom(s/).s/(m)= st/(m) Intuitively, M is a suf.cient read set if whenever the states s and \nstagree on the values of all memory locations in M (but may differ in other values), the results of applying \np to them also agree on what locations change and how. Dependencies There is a con.ict between two transitions \nwhen one of them writes a memory location that is accessed (for read or write) by the other. Let r, w \n: T .P(L) de.ne some read and write sets. The following de.nitions are (implicitly) parameterized by \nr and w.Weuse cm(t, tt) to denote the set of locations that participate in a con.ict between two transitions, \nt and tt: cm(t, tt) \u00a3 w(t ) n (w(tt) . r(tt)) . w(tt) n (w(t) . r(t )) (6) We say that there is a con.ict \nbetween t and tt(or t and ttare con.icting) when cm(t, tt)= \u00d8. The con.ict relation is symmetric. The \nset of dependencies of trace t = (t1,...,tn), de\u00ad.ned with respect to a given w and r, is the set of \npairs of con.icting transitions of the trace t, ordered by time: D[r, w](t)= {(ti,tj)| i<j,cm(ti,tj )= \n\u00d8} (7) We use ti . tj to denote that the dependency (ti,tj )is in D[r, w](t), when t, r, w are clear \nfrom the context. For simplicity, we do not distinguish between RAW, WAR, and WAW dependencies. It is \neasy to see that by using larger read and write sets, we get more dependencies. A weaker condition is \nstated in the following. // Lemma 4.1. Let r,w,r,w: T .P(L) such that for all t . T , r(t ) . r/(t) . \nw/(t ) and w(t) . w/(t). For every / trace t, D[r, w](t) . D[r,w/](t). Proof. See Appendix A. In particular, \na location read by t according to r can be in either r/(t) or w/(t ). Mappings r, w : T .P(L) de.ne conservative \nread and write sets when for all t . T , read(t) . r(t) . w(t ) and write(t) . w(t) (8) When referring \nto the best read and write sets, we intend those de.ned in (4) and (3). Otherwise, when we mention read \nand write sets, we refer to conservative read and write sets as de.ned in (8). It follows from Lemma \n4.1 that by using conservative read and write sets, we do not miss any dependencies that arise using \nthe best read and write sets. Trace Dependence Graph In the rest of this paper, we refer to the (directed \nacyclic) graph induced by D[r, w](t) as a trace dependence graph of t. The nodes of this graph are the \ntransitions of t, and the edges are the dependencies of D[r, w](t).2 The trace depen\u00addence graph of t \nis denoted by G[r, w](t). We write D(t) and G(t) instead of D[r, w](t) and G[r, w](t), respectively, \nwhen the parameters r, w are clear from the context. We lift the de.nition of G(t) to a (possibly in.nite) \nset of traces by taking the union of the individual trace dependence graphs in the set.  4.2 Connection \nto Commutativity Our goal in recording data dependencies is to enable reason\u00ading about correct parallelization. \nWe must therefore clarify the connection between dependencies and commutativity to be able to use the \ncriterion speci.ed in Section 3. The fol\u00adlowing establishes this connection. Lemma 4.2. Consider trace \nt = (t1,...,tn), and let D[r, w](t) denote the set of dependencies of t according to conservative read \nand write sets r and w. Then 1. every pair (pi,pi+1) of consecutive statements in Pp(t) such that (ti,ti+1)./D[r, \nw](t) is commutative in t. 2. Moreover, for every pair t and tt of traces and permuta\u00adtion d, such that \n(i) Pp(tt)= d(Pp(t)), (ii) st1 = s1, and  (iii) for all (ti,tj ). D[r, w](t), d(i) <d(j), there exists \na natural number m . N such that t =m tt. Proof. See Appendix B. The second clause of the claim states \nthat if trace tt is the result of running the statements in t in a different order that does not break \nany data dependency, then the two traces are equivalent according to (2). Furthermore, we can arrive \nat p P (tt) starting from Pp(t) via a .nite number of permitted transpositions (according to the conservative \nnotion of com\u00admutativity imposed by r and w). This observation enables simple reasoning about the cor\u00adrectness \nof (parallel) schedule S based on trace t: As long as S enables only traces that satisfy the second claim \nabove, and in particular, every possible trace of S satis.es the con\u00adstraints imposed by D[r, w](t), \nS is correct.  5. Concrete and Abstract Dependencies We now explore the meaning of dependencies under \nabstrac\u00adtion. We show how the general de.nition of dependencies we gave in the previous section can be \nused with both con\u00adcrete and abstract semantics, including specialized abstrac\u00adtions for ADTs and loop \nparallelization. Fig. 5 summarizes the notations for these semantic domains. 2 Note the difference from \nstandard program dependence graphs, where nodes are program statements. Also, our presentation is in \nterms of data dependencies only, without loss of generality, because control dependencies are subsumed \nby trace order.  Low-level Semantics s : Ly. V s . S t \u00a3 (s, p, s') t . T Simple Concrete Semantics \nVal \u00a3 Obj l{null}. : VarId y. Val . . Env heap : Obj \u00d7 FieldId y. Val heap . Heaps state = (., heap) \nstate . States L \u00a3 (Obj \u00d7 FieldId) l VarId V \u00a3 Val s = heap l . Concrete Semantics with Methods Val \u00a3 \nObj l{null}. : VarId y. Val . . Env stack \u00a3 (.1,...,.m) stack . Stacks heap : Obj \u00d7 FieldId y. Val heap \n. Heaps state = (stack, heap) state . States L \u00a3 (N \u00d7 VarId) l (Obj \u00d7 FieldId) V \u00a3 Val s = heap l n \ni=1{i}\u00d7 .i Abstract Semantics s# : L# y. V # s# . S# t# \u00a3 (s#,p,s'#) t # . T # Figure 5. Summary of \nnotations for semantic domains, where Obj is an unbounded set of dynamically allocated objects, VarId \nis the set of local variable identi.ers, and FieldId is the set of .eld identi.ers Instruction Read Set \nWrite Set putfield(b,f,v) v (.(b), f) r=getfield(b,f) (.(b), f) r r=checkcast(x,T) x r r=unaryop(\u00ac,v) \nv r r=binaryop(+,x,y) x, y r Figure 6. Read and write sets for several of the Java byte\u00adcode instructions \non a concrete state (stack, heap), where . is the top frame on the stack ADT Operation Read Set Write \nSet m.put(k,v):r \u00d8 {(m, k) , r } m.get(k):r {(m, k)} { r } m.containsKey(k):r {(m, k)} { r } m.remove(k):r \n\u00d8 {(m, k) , r } Figure 7. Conservative abstract read and write sets of sev\u00aderal operations of the Map \nADT on abstract state s#, where m = .(m), k = .(k) and v = .(v) (. being the top stack frame) 5.1 Concrete \nDependencies We assume a standard concrete semantics for sequential pro\u00adgrams, where a state consists \nof a stack of method invoca\u00adtions mapping local variables to values, and a heap mapping object .elds \nto values (see Fig. 5). A memory location is a data storage that can be used by the instructions of the \nprogramming language to store and retrieve data values. In our semantics, there are two types of memory \nlocations: heap and environment locations. A heap location is an object s .eld. In state (stack, heap), \neach pair (o, f) in dom(heap) de.nes a unique memory location. An environment location is a local variable \nin the context of an invocation of a method (i.e., a stack frame). In state (stack, heap), where stack \nis (.1,...,.n), each pair (i, v)de.nes a unique memory location, where v . VarId is a local variable \nidenti.er of a method executed by stack frame i, i.e., v . dom(.i) and 1 = i = n. To apply the uniform \nde.nition of dependencies from Sec\u00adtion 4.1, we encode concrete states of the form (stack, heap)as states \nin the low-level semantics by simply mapping from memory locations to values. Given a state (state, heap),we \nde.ne the low-level mapping s to be the (disjoint) union of the mapping de.ned by heap and the mappings \nde.ned by .i for i =1,...,n, as shown in Fig. 5. This can be extended in the usual way to handle threads, \ntypes, arrays and static variables. Fig. 6 shows read and write sets for several interest\u00ading Java bytecode \ninstructions. These sets are conservative. For example, consider a transition t for putfield(b,f,v) from \na state where the value of b.f before the update is al\u00adready .(v). The best write set for this transition \ndoes not contain memory location (.(b), f), because the value at (.(b), f) has not changed. The best \nread set for this transi\u00adtion contains memory location (.(b), f), because by chang\u00ading the value of this \nlocation, the best write set changes. The read set of this transition according to Fig. 6 does not contain \n(.(b), f), but the write set does. Fig. 6 is therefore conservative: The read and write sets it prescribes \nmay give rise to more dependencies than the best read and write sets. In return, the speci.cation in \nFig. 6 is simpler than (3) and (4), because the read set in Fig. 6 depends only on the state in which \nthe statement executes, whereas (4) depends on all possible executions of the same statement. In the \nabove example, the best read set takes into account that there exists another state in which the value \nof (.(b), f) is not .(v). For a transition that starts in that state, write contains (.(b), f), and therefore \ndiffers from the best write set for t.  5.2 Abstraction of Trace Segments We de.ne an abstraction of \ntrace t by breaking the sequence of transitions constituting t into multiple segments whose concatenation \nis the original trace. Each segment is mapped to an abstract transition, and intermediate states between \nthe concrete transitions represented by the abstract transition are omitted.  Let trace t be (t1,...,tn). \nWe de.ne segments of t using aset I = {a1,b1,...,ak,bk} of indexes, where 1= a1 = b1,b1 +1= a2 = b2,...ak \n= bk = n For i =1,...,k,the i-th segment of t consists of transitions between indexes ai and bi, inclusive. \nThe abstraction of t is the trace I(t) \u00a3 (t1,...,t k), such that ti \u00a3 (sai ,pi,s/) bi and pi is a sequential \ncomposition of the statements in the i-th segment of t. A read (resp. write) set of an abstract transition \nt i can be naturally de.ned as the union of all the read (resp. write) sets of the concrete transitions \nin segment i of t: w(t i)= w(tj ) ai=j=bi (9) r(t i)= r(tj ) ai=j=bi This de.nition is conservative \nwith respect to the best read and write sets: read(t i) . w(ti) . r(ti) and write(t i) . w(ti) for i \n=1,...,k assuming that w, r are conservative. For the de.nition of the best read set of ti according \nto (4), we use the semantics of sequential composition to compute the effect of pi on st. Next, we de.ne \ntwo useful instances of trace segments abstraction by specifying different ways of partitioning a trace \ninto segments. Loop-based Segments Segments can be de.ned by loop it\u00aderations. We use this for loop parallelization, \nbecause we are interested in dependencies between different loop iterations only, and not between statements \nwithin the iteration body. Method-based Segments Segments can be de.ned by calls and returns to certain \nmethods, e.g. methods implementing ADT operations. Instructions executed by the body of such a method, \nincluding instructions executed by methods called indirectly from it, are represented by a single transition \nin the abstract trace. Dependencies computed from the result of this abstraction are between ADT operations, \nbut based on concrete memory locations, which re.ect the internals of an ADT implementation. To abstract \naway from internals, we combine this abstraction with an abstraction of states, described in the next \nsection.  5.3 Abstraction of States The de.nition of dependencies from Section 4.1 can be ap\u00adplied to \nany abstract trace that satis.es a simple requirement whereby abstract states essentially map (abstract) \nmemory locations to (abstract) values: s# : L# y. V #. Intuitively, abstract memory locations enable \nmore precise approxima\u00adtion of commutativity between statements. Instead of testing for con.icing accesses \nto concrete memory, we consider a more abstract notion of con.ict that stems from a semantic dependency \nbetween the statements. Let \u00df :S . be an abstraction function mapping S# concrete to abstract states \n(see examples of \u00df in Section 6). ## \u00df is the trace t \u00a3 (t,...,t#), where for i =1,...,n, \u00df 1 n # t \n\u00a3 (\u00df(si),p,\u00df(s/)). (In what follows, we omit the \u00df ii subscript when \u00df is clear from the context.) A \ntransition t# = (s#,p,s/#) is in T # if and only if there exist s, s/ . S such that \u00df(s)= s# , \u00df(s/)= \ns/# and s/ =[[p]](s). Note that we cannot compare read (or write) sets of con\u00adcrete and abstract traces, \nbecause these sets refer to different domains of memory locations, L and L# (unless abstrac\u00adtion a has \nsome special properties, which we do not require here). However, dependencies are comparable, because \nthere is a one-to-one correspondence between the transitions in a given trace and its abstraction (when \nwe ignore cm annota\u00adtions of memory locations on dependencies).  6. Representation Functions The abstractions \npresented in Sections 5.2 5.3 provide the basis for de.ning and utilizing ADTs, which is the focus of \nthis section. To this end, we introduce a simple notion of representation allowing the programmer to \nde.ne a logical view for a concrete store [18], as illustrated in Fig. 8. A key question in the representation \nof data structures is what type of encapsulation to enforce on them to guaran\u00adtee representation independence. \nAnother question is how to express representation functions (or relations). In this paper, we employ \na restrictive de.nition of a representation func\u00adtion based on the notion of ownership as domination \n[29]. Considering concrete trace t and ADT a with set ops of op\u00aderations, we assume our ability to interpret \ninvocation state\u00adments in (transitions in) t as operations from ops.Wefur\u00adther assume that the concrete \nimplementation of an ADT is rooted at some base object. The internal state of the ADT implementation \nis only accessible via the root object. For example, in call m.put(k,v) corresponding to the put op\u00aderation \nof the Map ADT, argument m points to the root of the Map implementation.  A representation function \nfor ADT a, repa, operates on a concrete state s by replacing certain concrete memory loca\u00adtions by a \nset of abstract locations. Intuitively, the removed locations comprise the concrete state of data structures \nim\u00adplementing a, and the abstract locations introduced in their stead represent the abstract state of \nthese data structures ac\u00adcording to a. repa operates only on heap locations. repa may operate on heap \nlocation m = (o, f) de.ned in s only if it is owned by the root r of some implementation of a, where \nownership is equated with domination: Every heap path in s (consisting of zero or more edges) reaching \no goes through r. For simplicity, we assume that ADT operations do not ac\u00adcess heap locations outside \nthe ADT. We note that more permissive de.nitions of a represen\u00adtation function can be used at the cost \nof complicating the formal discussion [10]. For our needs, the above de.nition suf.ces. It is also compatible \nwith the ADTs we de.ned for our evaluation (described in Section 7), at least with regard to the traces \nwe considered. 6.1 Abstraction via Representation Functions Based on the speci.cations of ADTs a1,...,ak, \nwe would like to de.ne a state abstraction function \u00df :S . . S# For this, we assume that for all i = \nj, ai and aj operate on distinct memory locations and produce disjoint sets of abstract memory locations. \nWe can thus de.ne \u00df as follows: .. .. \u00df = .s . S.(s \\ rem(repai ,s)) . add(repai ,s)), 1=i=k 1=i=k (10) \nwhere rem(repa,s) denotes the set of concrete locations de.ned by s that are abstracted away by repa, \nand add(repa,s) the set of abstract locations introduced by repa. Parallelization with ADTs Information \ncollected about de\u00adpendencies between ADT operations is actionable for paral\u00adlelization assuming that \nthe following two properties hold: 1. Encapsulation. If the client treats the ADT as such, and is unaware \nof its implementation details, then the exist\u00ading ADT implementation can be replaced with relative ease. \nThis amounts to the requirement that the memory locations the representation function operates on are \nnot manipulated outside of ADT operations. (That is, these locations are neither read nor written by \ntransitions not corresponding to operations of the ADT.) This can easily be veri.ed for a single trace \n(or .nitely many traces). 2. Atomicity. As discussed in Section 3, we assume an in\u00adterleaving semantics \nof concurrency. The trace I(t) af\u00adter method-based segments abstraction according to ADT operations represents \n(long) sequences of concrete in\u00adstructions as single ADT invocations. Reasoning about interleavings at \nthe level of ADT invocations (i.e., entire segments), and not their constituent instructions, assumes \n Basic Computation of Abstract Read and Write Sets Inputs: {(a1,repa1 ),..., (ak,repak )}: representation \nfunctions t = (s, p, s ' ): transition in I[a1,...,ak](t) [r, w]: read and write sets ABSREADWRITE: t \n# = ABS(t) return READWRITE(t#) ABS(t ): for each ai .{a1,...,ak}if t . T ai return (\u00df[repai ](s),p,\u00df[repai \n](s ' ))return t READWRITE(t#): if t . T return (args(t #) . abs(t#),write(t#))else return (r(t#),w(t \n#)) Figure 9. Basic algorithm for computing abstract read and write sets using representation functions \nthat the invoked operations execute without interruption. For a transformation based on this to be valid, \nthe ADT operations must be atomic. This guarantee can be pro\u00ad vided by a linearizable [17] implementation \nof the ADT. Given that these two requirements are met, Lemma 4.2 holds for I(t).  6.2 Computing Dependencies \nTo compute dependencies in a sound manner relative to (4) and (3), we must introduce a tractable approximation \nof (4), which is not computable in general (e.g., if S# is in.nite). Assuming encapsulation, as well \nas disjointness between the operations of distinct ADTs, we can use the algorithm in Fig. 9 for computing \nthe abstract read and write sets of a transition, where (i) T a is the set of (segment) transitions k \ncorresponding to ADT a and T = T ai , (ii) \u00df[repa] i=1 denotes the state abstraction function obtained \naccording to (10) by using (only) repa, (iii) args is an auxiliary func\u00adtion returning the set of all \nenvironment locations used and de.ned by the statement in its argument transition, and (iv) abs returns \nthe set of abstract locations de.ned by the entry (abstract) state of its argument transition. Importantly, \nthe algorithm in Fig. 9 uses repa, and not repa1 ,...,repak , for abstracting the states of a transition \ncorresponding to an operation of ADT a, and does not ap\u00adply abstraction at all to compute the read and \nwrite sets of concrete transitions. The following claim asserts that no de\u00adpendencies are lost. Lemma \n6.1. Let Rep = {repa1 ,...,repak } be repre\u00adsentation functions for ADTs a1,...,ak, and r, w read and \nwrite sets that agree with ABSREADWRITE, instanti\u00adated with Rep, on the transitions in segments abstraction \nI[a1,...,ak](t) of concrete trace t. Then if we assume that (i) all ADTs are encapsulated, and (ii) there \nis no sharing between ADTs in terms of their operations, then  D[read, write](I[a1,...,ak](t)#) . D[r, \nw](I[a1,...,ak](t)), where I[a1,...,ak](t)# is obtained by applying \u00df[a1,...,ak] to I[a1,...,ak](t). \nProof. See Appendix C. A primary source of imprecision in ABSREADWRITE is its conservative resolution \nof the read set of an ADT opera\u00adtion as the entire state of the ADT at the entry state, which consists \nof all the abstract locations introduced by the ADT s reprsentation function. Another concern, related \nto perfor\u00admance, is that computing the write set can be expensive, as it entails exhaustive comparison \nbetween the abstract states before and after an operation. We now address these con\u00adcerns. 6.2.1 Exploiting \nthe Concrete Frame Consider abstract transition t = (s, p, s/). I[a1,...,ak](t) corresponding to ADT \nai and its respective read and write sets according to (9), r(t) and w(t). We rewrite the entry and exit \nstates of t as follows: s.= s \\{(l, s(l))| l . dom(s) \\ (r(t) . w(t))} s./ = s/ \\{(l, s/(l))| l . dom(s/) \n\\ (r(t ) . w(t))} That is, we omit the frame of t from the entry and exit states, which yields entry \nand exit states s.and s./ that are by de.nition indistinguishable from s and s/, respectively, from the \nperspective of t . t (. Applying repai to .= s, p, s./), and not to t,isprefer\u00adable since portions of \nthe concrete implementation(s) of a are absent from s.and s./. This suggests a smaller abstract read \nset, as well as more ef.cient computation of the abstract write set. While reliance on concrete memory \naccesses may im\u00adprove the accuracy of the analysis, it comes at the cost of tracking concrete reads and \nwrites during the execution of ADT operations. Moreover, there is still the threat that the implementation \nof an ADT operation is na\u00a8ive in that its foot\u00adprint is much larger than the best read and write sets. \nConsider for example an implementation of the Map ADT where the get operation performs linear traversal \nof all the keys stored in the map until a match is found. In the worst case, the concrete read set of \nthe get implementation may translate into an abstract read set consisting of the entire state of the \nMap, which is not much of an improvement compared to the read set prescribed by ABSREADWRITE. This can \nbe remedied by capturing the frame of the get operation at a more abstract level.  6.2.2 Exploiting \nthe Abstract Frame The semantics of ADT operation op may enable partition\u00ading the abstract state immediately \npreceding its invocation into two disjoint areas corresponding to (a safe approxima\u00adtion of) the frame \nand the footprint of op, where the abstract locations in op s frame are not needed to establish a sound \ncommutativity speci.cation. This is exempli.ed by the par\u00adtial speci.cation in Fig. 7 provided for the \nMap ADT. Importantly, the Map spci.cation in Fig. 7 relies on a prohibitive representation of the state \nof a Map where a Map instance m contains a .eld k for every allocated object k, and not only for stored \nkeys. This allows, e.g., a sound yet precise read set for containsKey even if the queried key is not \nstored in the concrete Map instance. A sound read set can also be obtained using a more concise description \nof a Map, but it would be less precise than the one in Fig. 7 in terms of its corresponding commutativity \nspeci.cation. Knowledge of ADT semantics enables specialization of the representation function by taking \ninto account the per\u00adformed operation along with its arguments. In practice, this translates into specialization \nof the representation function based on the call statements in the trace in the context of their entry \nstate. Since at each point we need only represent the abstract footprint of an operation, we can implicitly \nde.ne an abstract state whose explicit representation is prohibitive by referring only to parts of it \nwhen describing the footprint of an opera\u00adtion, as is done in Fig. 7. Even when explicit representation \nof the abstract state is possible, there is still an important gain by considering only a (small) portion \nof it for the rea\u00adsons mentioned in Section 6.2.1; namely, a smaller read set, as well as more ef.cient \ncomputation of the write set. 6.2.3 Advanced Version of ABSREADWRITE An overload of ABSREADWRITE incorporating \nthe en\u00adhancements described above is presented in Fig. 10. Note that conceptually, CONCFILTER and SPECIALIZE \ncommute. We can either start by removing the concrete frame of the transition, and then build an abstraction \nof the entry and exit states that also takes into account the abstract frame of the transition (as shown \nin Fig. 10), or vice versa. This is because the concrete implementation is known to comply with the ADT \ns semantics, and so the concrete footprint al\u00adready enables a safe approximation of the abstract locations \nneeded toward sound commutativity constraints. Further note that both CONCFILTER and SPECIALIZE are optional, \nand moreover, any combination of these enhance\u00adments is valid. By letting r(t)= L and w(t )= L/,weef\u00adfectively \ndisable the concrete-frame heuristic. Analogously, by letting .(t)= repa where t corresponds to an operation \nof ADT a and repa is the generic representation function of a, we skip the abstract-frame representation. \n Enhanced Computation of Abstract Read and Write Sets Inputs: t = (s, p, s ' ): transition in I[a1,...,ak](t) \n. =[t . rept | t . T ]: specialized representations [r, w]: read and write sets ABSREADWRITE: if t/. \ndom(.) return (r(t),w(t)) C t = CONCFILTER(t) # Ct = SPECIALIZE(Ct ) ## # return (args(Ct ) . abs(Ct \n),write(Ct )) CONCFILTER(t): sC= s \\{(l, s(l))| l . dom(s) \\ (r(t) . w(t))}sC' = s ' \\{(l, s ' (l))| \nl . dom(s ' ) \\ (r(t) . w(t))}return (Cs') s, p, C SPECIALIZE(t): return (\u00df[.(t )](s),p,\u00df[.(t)](s ' )) \nFigure 10. Advanced version of the algorithm for comput\u00ading abstract read and write sets using representation \nfunc\u00adtions   7. Implementation and Evaluation In this section, we .rst provide details about the implemen\u00adtation \nof HAWKEYE (Section 7.1). We then describe two sets of experiments we conducted to evaluate HAWKEYE s \nutil\u00adity. In the .rst experiment (Section 7.3), we measure the re\u00adduction in reported dependencies due \nto semantic con.ict detection. The second experiment (Section 7.4) goes beyond these raw numbers to assess \nthe value of HAWKEYE as an aid in end-to-end parallelization. 7.1 Implementation Details HAWKEYE is implemented \natop Chord [1], an extensible static and dynamic program-analysis framework for Java bytecode based on \nthe Joeq compiler infrastructure [37] and the Javassist bytecode-instrumentation library [12]. HAWK-EYE \naccepts as input (i) a program description in the form of a Chord properties .le, which includes the \nprogram s class .les, main entry point, and input data for one or more runs of the program, (ii) a (dynamic) \nparallelization scope where dependencies should be tracked (e.g., a loop, a method-stack con.guration, \netc), and (iii) a speci.cation for one or more ADTs. The algorithm implemented by HAWKEYE is illustrated \nin Fig. 11. For clarity, the algorithm in Fig. 11 is a slightly simpli.ed version of the actual HAWKEYE \nalgorithm, where the parallelization scope is a single loop executing at most once. This suf.ces for \nthe experiments described in this section. The dependencies computed by the analysis are at the granularity \nof ADT operations (and not loop iterations): Each of the transitions involved in a dependency is either \nan ADT operation or a concrete transition. The loop-based seg- Computation of Loop-carried Dependencies \nInputs: t: concrete trace {a1,...,ak}: ADT speci.cations l: loop identi.er [r, w]: read and write sets \n DEPENDENCIES: (tm,tl) = SEGMENTS(t, {a1,...,ak},l) G(tm)= DEPGRAPH(M(t), [r, w]) G(tl)= DEPGRAPH(L(M(t)), \n[r, w]) return '' '' {(t1,t2). EG(tm) |(t1,t2). EG(tl).t1 . t1 . t2 . t2} SEGMENTS(t, {a1,...,ak},l): \nM(t)= METHODSEGMENTS(t, {a1,...,ak}) L(M(t)) = LOOPSEGMENTS(M(t),l) return (M(t),L(M(t))) DEPGRAPH(t, \n[r, w]): V = {t . t}E = {(t1,t2). D[r, w](t)}return (V, E) Figure 11. Algorithm for computing loop-carried \ndepen\u00addencies ments abstraction is used to eliminate dependencies where both transitions map to the same \nloop iteration. 7.2 Experimental Setup We performed our experiments with the IBM J9 V1.6.0 VM running \non 32-bit Linux atop a Lenovo X201 laptop with 4GB of RAM. ADTs from the Java Collections Framework, \nas well as the graph ADTs in JGraphT and Boruvka, were speci.ed using the abstract-frame enhancement. \nWe man\u00adually speci.ed the abstract footprint of operations exposed by these ADTs, as depicted in Fig. \n7. For all the remain\u00ading ADTs (and benchmarks), a single representation func\u00adtion was de.ned, and the \nanalysis inferred a re.ned read set according to the concrete-frame heuristic described in Sec\u00adtion 6.2.1. \n 7.3 Number of Reported Dependencies The purpose of the .rst experiment was to gain a quantita\u00adtive \nmeasure of the decrease in the number of reported de\u00adpendencies under abstraction. Our research hypotheses \nwere the following: 1. Importance of abstraction. There is a signi.cant gap between the number of dependencies \nreported with and without abstraction. 2. Important of user ADTs. Use of user-provided repre\u00adsentation \nfunctions is signi.cant in dependency reduc\u00adtion compared to the baseline analysis that relies solely \non built-in representations for library ADTs.   Name Ver. Description Trace Len. Boruvka 2.1 Solver \nfor the minimum spanning tree problem 813,382 PMD 4.2 Java source code analyzer 2,190,213 JGraphT [4] \n0.8.1 Graph library 710,580 JFileSync [3] 2.2 Utility for synchronizing pairs of directories 1,733,552 \nWeka [6] 3.6.4 Machine-learning library for data-mining tasks 17,945,255 Cobertura [2] 1.9 (rc2) Java \ncode coverage analysis 2,629,457 WebLech [5] 0.0.3 Web-site download and mirror tool 4,840,544 Figure \n12. Benchmark characteristics  To test these hypotheses, we implemented a variant of HAWKEYE, which \nwe dubbed MOLEYE, that differs from HAWKEYE only in the value it assigns to [r, w]: Where HAWKEYE uses \nabstract read and write sets according to ADT semantics, MOLEYE relies on (9), which amounts to localizing \nconcrete dependencies within ADT calls to their corresponding call sites. We then ran both analyses on \nseven real-world bench\u00admarks. The benchmarks details appear in Fig. 12. From each benchmark, we selected \na single loop as the analysis parallelization target, as follows: We .rst de.ned the bench\u00admark s entry \npoint (either its main method or a unit test ex\u00adercising its core functionality). We then ran a pro.ling \nanal\u00adysis several times, each time with a different input. Loops whose number of iterations was not a \nfunction of the input (e.g., initialization loops) were discarded. Of the surviving loops, we chose the \none with the highest average number of instructions per iteration. The analyses were ran in two con.gurations: \n1. The Collections Only (CO) con.guration treats only Java collections (such as Set, Map and List) as \nADTs. 2. The Collections and User Types (CU) con.guration subsumes the CO con.guration by also treating \ncertain interfaces and classes in user code as ADTs. These ADTs are benchmark speci.c.  Results Table \n1 summarizes the results. For each con.gu\u00adration, we provide the results by both analyses according to \ntwo counts: The All Dep.s count is the total number of dependencies reported by the analysis according \nto the al\u00adgorithm in Fig. 11. ADT Dep.s considers the subset of all dependencies where both the source \nand the target transi\u00adtions occurred during ADT operations. Dependencies out\u00adside ADT operations, such \nas those involving the induction variable, are not included in the ADT Dep.s count. Fig. 13 visualizes \nthe differences between the CO (blue) and CU (blue and red) con.gurations in trace coverage.  7.4 End-to-end \nParallelization using HAWKEYE The second experiment was designed to provide qualitative insight into \nthe value of HAWKEYE as a user aid in end-to\u00adend parallelization. For this, we selected three of the \nseven benchmarks used for the .rst experiment. We then applied the following parallelization methodol\u00adogy: \n1. Run HAWKEYE on several representative execution traces. 2. Perform shallow review of the reported \ndependencies to decide whether additional (user) ADTs should be speci\u00ad.ed. If further ADTs are required, \nthen augment the spec\u00adi.cation and return to the .rst step. 3. Perform in-depth review of the surviving \ndependencies, and if needed transform the code to address these dependencies. If the program was changed, \nthen return to the .rst step using execution traces of the transformed program. 4. Manually verify the \ncorrectness of the transformed pro\u00ad  gram. PMD PMD is part of the DaCapo suite [9]. It accepts a set \nof Java classes as input, and analyzes them for a range of source-code problems. PMD has both a sequential \nand a parallel version. In the concurrent version of PMD, different classes are analyzed in parallel \nto each other. Our .rst run of HAWKEYE on PMD yielded close to 300 dependencies. Many of these dependencies \ninvolved the Report and BenchmarkResult classes, which moti\u00advated treatment of these types as ADTs. Rerunning \nthe analysis with the augmented speci.cation yielded many fewer depedencies, of which .ve were due to \noperations of BenchmarkResult and Report. These .ve dependen\u00adcies turned out to be important (e.g., con.icting \nupdates to global counters maintained by BenchmarkResult, such as the total time spent in each rule and \nthe number of AST nodes it visited). We addressed these dependencies by privatizing shared data and merging \nbetween its different copies at the end of  Benchmark Collections Only Collections and User Types HAWKEYE \nMOLEYE HAWKEYE MOLEYE All Dep.s ADT Dep.s All Dep.s ADT Dep.s All Dep.s ADT Dep.s All Dep.s ADT Dep.s \nBoruvka 227 90 414 277 51 30 289 268 PMD 289 3 307 21 88 5 103 20 JGraphT 58 3 106 51 40 0 66 26 JFileSync \n155 5 178 28 19 13 42 36 Weka 11,859 0 11,859 0 21 0 103 82 Cobertura 448 0 496 48 15 0 18 3 WebLech \n48 5 63 20 44 6 58 20 Table 1. Benchmark statistics according to the algorithm in Fig. 11 the parallel \nloop. We then repeated all steps of the paral\u00adlelization process, and con.rmed that the important depen\u00addencies \nare no longer present and no more code changes are required. Comparison to the parallel version of PMD \nshowed that our transformed program was correct (though the de\u00advelopers of PMD chose to use lock-based \nsynchronization, instead of privatization, for some of the shared data). JFileSync JFileSync is a utility \nfor comparing and synchro\u00adnizing pairs of directories. The JFileSync code makes heavy use of design patterns, \nand in particular, the singleton pat\u00adtern, which forces sharing. Running HAWKEYE on JFileSync with only \ncollection representations yielded a rather noisy report containing 160 candidate impediments to parallelization. \nThe vast ma\u00adjority of reported dependencies were traceable to classes JFSProgress and JFSComparisonMonitor. \nThis led us to add a speci.cation for these classes. With the augmented speci.cation in place, many fewer \ndependencies were re\u00adported. Of the 19 remaining .ndings, 13 were still due to JFSProgress and JFSComparisonMonitor. \nAll these de\u00adpendencies were found to be real, and stemmed from use of these classes as singletons. To \nresolve the discovered impediments, we applied priva\u00adtization transformations that exploded the singleton \nobjects into multiple copies and then, after the main loop, again re\u00adduced the separate copies into a \nsingle object. We veri.ed that the found dependencies were absent from the resulting program by running \nthe parallelization process from scratch on it. We then manually con.rmed that the resulting program \nwas correct by (i) exercising it on various test inputs and (ii) carefully reviewing its code. WebLech \nWebLech is a website download and mirror tool that emulates standard web-browser behavior. WebLech sup\u00adports \nmultithreaded execution, but can also be run sequen\u00adtially. For our experiment, we disabled all forms \nof synchro\u00adnization in WebLech, so that it could be treated as a gen\u00aduinely sequential application. The \nresults from running HAWKEYE on WebLech mo\u00adtivated consideration of the Spider class as an ADT. This \nyielded negligible reduction in the number of reported de\u00adpdendencies, but those were relatively few \nto begin with (< 50). Manual review of the ADT-related dependencies showed them to be real impediments \nto parallelization due to dependent accesses to internal Spider data structures that must occur atomically \n(e.g., checking whether a URL is scheduled for download via a contains call on the set of scheduled URLs, \nand attempting to add the URL to the download queue only if the answer is negative). We transformed WebLech \nby unifying dependent calls into linearizable Spider operations, and then reran the par\u00adallelization \nprocess on the transformed program. This con\u00ad.rmed that the impediments were no longer present and no \nfurther code changes were required. Comparison to the orig\u00adinal code of WebLech indicated that the program \nwe have arrived at was correct.  7.5 Discussion The numbers in Table 1 provide strong con.rmation our \nhy\u00adpothesis on the importance of abstraction: The overall num\u00adber of ADT-related dependencies reported \nwith abstraction is 106 in the CO con.guration and 54 in the CU con.gu\u00adration, compared to 445 and 455, \nrespectively, without ab\u00adstraction. With only collection ADTs, 77% of the ADT\u00adrelated dependencies are \nsuppressed, and if user types are also treated as ADTs, then 89% of the original dependen\u00adcies are omitted \nleaving the developer with an average of 7.7 dependencies to review (30 being the maximum). As for the \nvalue of user ADTs, na\u00a8ive interpretation of the results is misleading. User ADTs yield a more abstract \ntrace (with fewer abstract transitions that represent more concrete transitions) compared to using collection \nADTs alone, and so the number of dependencies is bound to decrease regard\u00adless of whether user ADTs actually \nimprove matters. Still, the results on JGraphT, Weka and Cobertura are an encour\u00adaging indication that \nthis extra speci.cation effort is worth\u00adwhile. In the .rst case, the addition of user ADTs annihi\u00adlated \nall ADT-related depedencies, and in the second and third cases, the overall number of dependencies decreased \nby a considerable factor (of 582 in Weka and 30 in Cober\u00adtura) without introducing a single ADT-related \ndependency. In terms of raw coverage, Fig. 13 indicates that the gap be\u00adtween the CO and CU con.gurations \nis nonnegligible with an average difference of 32% between the two con.gura\u00adtions.  Our qualitative \nstudy on end-to-end parallelization also supports our hypothesis on user ADTs. Augmenting the speci.cation \nwith user types proved to be key to arriving at a manageable report that can be processed manually, and \nsimultaneously also removing large sources of noise. More generally, we found the methodology used for \nthe second experiment to be highly effective. It required few iterations, allowed us to quickly converge \non the real impediments to parallelization, and led us in all three cases to acceptable programs that \nare amenable to parallelization. For the transformations we performed which either reduced sharing via \nprivatization or encapsulated accesses to shared data in ADT operations we could also verify that the \nimpediments found in the original program were absent from the transformed program. We note, however, \nthat this may not be the case in general. For example, a synchronization transformation that governs \nunsafe accesses to shared data by locks is likely to preseve dependencies, in which case HAWKEYE may \nstill report (some of) the original dependencies on the transformed program.  8. Related Work In this \nsection, we survey closely related research on soft\u00adware parallelization. For other studies where abstraction \nis applied during dynamic analysis, the reader is referred to [24, 25] and references therein. Research \non program slicing, where accurate tracking of data dependencies is a key challenge, is discussed in \n[19, 36] and works they cite. Parallelizing Compilers Compile-time code parallelization is a long-standing \nresearch problem that dates back to the early days of the Fortran compiler, which exploited Fortran s \nstrong aliasing guarantees to prove the safety of its trans\u00adformations. The SUIF [14] compiler framework \nis designed to study parallelization in shared-memory and distributed\u00adshared-memory machines, and has \nbeen the basis of several parallelization techniques, including af.ne partitions [26] and linear inequalities \n[7]. A survey of compiler optimizations of imperative pro\u00adgrams for parallel architectures, which typically \nrely on tracking the properties of arrays using loop dependence anal\u00adysis, is provided in [8]. [34] describes \nhow to compute the transitive reduction of the data-dependence relation, an opti\u00admization we use in our \ndynamic analysis. Commutativity analysis, a parallelization technique that exploits commutativity between \noperations on objects to un\u00adcover parallelization opportunities, is introduced in [30 32]. This technique \nwas implemented in a compilation system as a set of automatic analysis algorithms. Our approach follows \na similar motivation, though it bases commutativity judg\u00adments on ADT semantics rather than automated \nanalysis of implementation code. Moreover, the instantiation of our ap\u00adproach as a dynamic analysis entails \ndifferent challenges in performance and accuracy, and consequently also different algorithms. Transactional \nMemory Transactional boosting [16, 20] is a methodology developed to avoid redundant con.icts in traditional \nsoftware transactional memory (STM) systems, which synchornize on the basis of read/write con.icts. In\u00adstead \nof checking for competing memory accesses, transac\u00adtional boosting promotes the notion of abstract locks: \nEach invocation of a boosted object (which is assumed to be lin\u00adearizable) is associated with an abstract \nlock; two abstract locks con.ict if their corresponding invocations do not com\u00admute. The Galois system \n[23] facilitates parallelization of ir\u00adregular applications, which manipulate pointer-based data structures \nlike trees and graphs. Galois provides syntactic constructs for expressing optimistic parallelism, as \nwell as a runtime scheme for detecting potentially unsafe accesses to shared memory and performing recovery. \nSimilar to transac\u00adtional boosting and our approach, Galois bases con.ict de\u00adtection on semantic rather \nthan concrete commutativity be\u00adtween operations, which depends on ADT semantics. A framework for reasoning \nabout con.ict-detection schemes expressed as commutativity conditions is presented in [22]. Different \ncommutativity speci.cations for the same ADT, which are each phrased as a set of predicates associated \nwith pairs of operations, are mapped into a uni.ed representa\u00adtion, the commutativity lattice. The commutativity \nlattice orders the speci.cations by the amount of parallelism they permit. [22] also shows ways of systematically \nconstructing commutativity checkers based on commutativity speci.ca\u00adtions from the lattice. Our analysis \nalso tracks dependencies at the semantic level (and can thus be used to uncover STM-based paral\u00adlelization \nopportunities, as shown for Boruvka), but is not speci.c to transactional memory. Also, representation \nfunc\u00adtions are a different way of expressing commutativity than the speci.cations of [22] and the abstract \nlocks of [16]. Pro.ling The critical path, de.ned as the longest path whose instruction instances must \nbe executed sequentially, is used by [15] as an optimistic measure of available par\u00adallelism. Their tool \noperates on concrete traces, and com\u00ad n putes the ratio between trace length (n) and the length k of \nthe critical path (k) for several sequential benchmarks from the DaCapo suite [9]. Loops L exhibiting \ngood paral\u00adlelization potential are reported as promising parallelization candidates, where a loop s \npotential is de.ned as the ratio l(Li) SLi , where Li is an instance of L, and l(Li) and |Li| SLi|Li| \ndenote the length of Li s critical path and the overall number of instructions it executed, respectively. \nParaMeter [21] produces parallelism pro.les for irregu\u00adlar programs iterating over worklists using Galois \nset itera\u00adtors [23]. These pro.les show how many worklist items can be executed concurrently at each \nstep of the algorithm as\u00adsuming an idealized execution model. ParaMeter also com\u00adputes the parallelism \nintensity of an algorithm by dividing the amount of work executed in each round by the total amount of \nwork available for execution at that time, i.e., the size of the worklist.  Determinism The guarantees \nmade by our analysis regard\u00ading the parallel execution of a sequential trace relate to an extensive body \nof research on .nding and preventing un\u00adwanted behaviors in parallel programs due to nondetermin\u00adistic \nthread interleavings. [11] proposes an assertion frame\u00adwork for specifying that regions of a parallel \nprogram behave deterministically despite arbitrary thread interleavings. The framework allows a programmer \nto specify that if block P of parallel code is executed twice (with potentially differ\u00ad / ent schedules), \nfrom initial states s0 and ssatisfying user\u00ad 0 provided precondition Pre, then the respective .nal states \ns and s/ must satisfy user-provided postcondition Post. SingleTrack [35] is a dynamic analysis that veri.es \na stronger form of determinism, where the parallel compu\u00adtation must be free of both external interference \n(external serializability) and race conditions due to communication between threads (con.ict freedom). \nThese conditions ensure that every schedule produces bitwise identical results. The notion of equivalence \nemployed by our analysis (cf. Section 3) can be expressed in terms of the Pre and Post predicates of \n[11], where Pre enforces equality of the entry states and Post demands that the .nal states modulo the \nADT implementations be identical, and the ADT imple\u00admentations have identical abstract states.  9. Conclusion \nDependence analysis provides useful information for paral\u00adlelization, but its value is often impaired \nby conservative re\u00adporting of dependencies. We have investigated the bene.t of leveraging ADT semantics \ntoward more accurate reporting of dynamic dependencies. Experiments we have conducted on seven real-world \nbenchmarks show that a signi.cant por\u00adtion of the execution (86%) is spent in ADT operations, and more \nimportantly, the vast majority of concrete dependen\u00adcies (89%) reported between ADT operations are annihilated \nwhen ADT semantics are taken into account, and the remain\u00ading dependencies are largely real. Our tool, \nHAWKEYE,was further demonstrated to be effective as a parallelization aid. References [1] Chord: A static \nand dynamic program-analysis framework for java. [2] Cobertura: A test coverage tool for java. //http://cobertura.sourceforge.net. \n[3] The j.lesync java .le synchronization utility. http://jfilesync.sourceforge.net. [4] The jgrapht \njava graph library. http://www.jgrapht.org. [5] The weblech download and mirror tool. http://sourceforge.net/projects/weblech/. \n[6] The weka machine-learning library. http://sourceforge.net/projects/weka. [7] Saman P Amarasinghe. \nParallelizing compiler techniques based on linear inequalities. Technical report, 1997. [8] David F. \nBacon, Susan L. Graham, and Oliver J. Sharp. Com\u00adpiler transformations for high-performance computing. \nACM Comput. Surv., 26:345 420, 1994. [9] S. M. Blackburn, R. Garner, C. Hoffman, A. M. Khan, K. S. McKinley, \nR. Bentzur, A. Diwan, D. Feinberg, D. Frampton, S. Z. Guyer, M. Hirzel, A. Hosking, M. Jump, H. Lee, \nJ. E. B. Moss, A. Phansalkar, D. Stefanovi\u00b4 c, T. VanDrunen, D. von Dincklage, and B. Wiedermann. The \nDaCapo benchmarks: Java benchmarking development and analysis. In OOPSLA 06: Proceedings of the 21st \nannual ACM SIGPLAN confer\u00adence on Object-Oriented Programing, Systems, Languages, and Applications, pages \n169 190, 2006. [10] Chandrasekhar Boyapati, Barbara Liskov, and Liuba Shrira. Ownership types for object \nencapsulation. In Proceedings of the 30th ACM SIGPLAN-SIGACT symposium on Principles of programming languages, \npages 213 223, 2003. [11] Jacob Burnim and Koushik Sen. Asserting and checking determinism for multithreaded \nprograms. Commun. ACM, 53:97 105, 2010. [12] Shigeru Chiba and Muga Nishizawa. An easy-to-use toolkit \nfor ef.cient java bytecode translators. In Proceedings of the 2nd international conference on Generative \nprogramming and component engineering, pages 364 376, 2003. [13] J. Ferrante, K. Ottenstein, and J. Warren. \nThe program depen\u00addence graph and its use in optimization. toplas, 3:319 349, 1987. [14] Mary W. Hall, \nJennifer M. Anderson, Saman P. Amaras\u00adinghe, Brian R. Murphy, Shih-Wei Liao, Edouard Bugnion, and Monica \nS. Lam. Maximizing multiprocessor performance with the suif compiler. Computer, 29:84 89, 1996. [15] \nClemens Hammacher, Kevin Streit, Sebastian Hack, and An\u00addreas Zeller. Pro.ling java programs for parallelism. \nIn Pro\u00adceedings of the 2009 ICSE Workshop on Multicore Software Engineering, pages 49 55, 2009. [16] \nMaurice Herlihy and Eric Koskinen. Transactional boosting: a methodology for highly-concurrent transactional \nobjects. In PPOPP, pages 207 216, 2008. [17] Maurice P. Herlihy and Jeannette M. Wing. Linearizability: \na correctness condition for concurrent objects. ACM Trans. Program. Lang. Syst., 12:463 492, 1990. [18] \nC. A. R. Hoare. Proof of correctness of data representations. Acta Inf., 1:271 281, 1972. [19] S. Horwitz, \nT. Reps, and D. Binkley. Interprocedural slicing using dependence graphs. In Proceedings of the ACM SIG-PLAN \n1988 conference on Programming Language Design and Implementation, pages 35 46, June 1988. [20] Eric \nKoskinen, Matthew Parkinson, and Maurice Herlihy. Coarse-grained transactions. In Proceedings of the \n37th an\u00adnual ACM SIGPLAN-SIGACT symposium on Principles of programming languages, pages 19 30, 2010. \n [21] Milind Kulkarni, Martin Burtscher, Rajasekhar Inkulu, Ke\u00adshav Pingali, and Calin Cascaval. How \nmuch parallelism is there in irregular applications? In PPoPP 09: Proceedings of the 14th ACM SIGPLAN \nsymposium on Principles and prac\u00adtice of parallel programming, pages 3 14, 2009. [22] Milind Kulkarni, \nDonald Nguyen, Dimitrios Prountzos, Xin Sui, and Keshav Pingali. Exploiting the commutativity lattice. \nIn PLDI, 2011. [23] Milind Kulkarni, Keshav Pingali, Bruce Walter, Ganesh Ra\u00admanarayanan, Kavita Bala, \nand L. Paul Chew. Optimistic par\u00adallelism requires abstractions. In PLDI, pages 211 222, 2007. [24] Ondrej \nLhot\u00b4 Context-sensitive ak and Laurie J. Hendren. points-to analysis: Is it worth it? In CC, pages 47 \n64, 2006. [25] Percy Liang, Omer Tripp, Mayur Naik, and Mooly Sagiv. A dynamic evaluation of the precision \nof static heap abstrac\u00adtions. In OOPSLA, pages 411 427, 2010. [26] Amy W. Lim and Monica S. Lam. Maximizing \nparallelism and minimizing synchronization with af.ne partitions. Paral\u00adlel Comput., 24:445 475, 1998. \n[27] Susan Owicki and David Gries. Verifying properties of par\u00adallel programs: an axiomatic approach. \nCommun. ACM, 19:279 285, 1976. [28] Susan S. Owicki. Axiomatic Proof Techniques for Parallel Programs. \nOutstanding Dissertations in the Computer Sci\u00adences. Garland Publishing, New York, 1975. [29] John Potter, \nJames Noble, and David Clarke. The ins and outs of objects. In In Australian Software Engineering Conference, \npages 80 89, 1998. [30] Martin C. Rinard and Pedro C. Diniz. Commutativity analy\u00adsis: A new analysis \nframework for parallelizing compilers. In PLDI, pages 54 67, 1996. [31] Martin C. Rinard and Pedro C. \nDiniz. Commutativity analy\u00adsis: A technique for automatically parallelizing pointer-based computations. \nIn IPPS, pages 14 22, 1996. [32] Martin C. Rinard and Pedro C. Diniz. Semantic foundations of commutativity \nanalysis. In Euro-Par, Vol. I, pages 414 423, 1996. [33] Daniel J. Rosenkrantz, Sanjay Goel, S. S. Ravi, \nand Jagdish Gangolly. Structure-based resilience metrics for service\u00adoriented networks. In EDCC, pages \n345 362, 2005. [34] John L. Ross and Mooly Sagiv. Building a bridge between pointer aliases and program \ndependences. Nordic Journal of Computing, 8:221 235, 1998. [35] Caitlin Sadowski, Stephen N. Freund, \nand Cormac Flana\u00adgan. Singletrack: A dynamic determinism checker for mul\u00adtithreaded programs. In ESOP, \npages 394 409, 2009. [36] Manu Sridharan, Stephen J. Fink, and Rastislav Bod\u00b4ik. Thin slicing. In PLDI, \npages 112 122, 2007. [37] John Whaley. Joeq: A virtual machine and compiler infras\u00adtructure. Sci. Comput. \nProgram., 57:339 356, 2005.  A. Proof of Lemma 4.1 / Let r, w and r,w/ be two pairs of read and write \nsets satis\u00adfying that for all t . T , r(t) . r/(t) . w/(t) and w(t ) . w/(t), and consider trace t. Assume \nthat (t, tt). D[r, w](t). Then by (6) and (7), at least one of the following is true: w(t ) n w(tt)= \n\u00d8, w(t ) n r(tt)= \u00d8,or r(t) n w(tt)= \u00d8. If w(t )nw(tt)= \u00d8, then necessarily w/(t )nw/(tt)= \u00d8. As for \nthe option that w(t ) n r(tt)= \u00d8,let l . w(t ) n r(tt). Then either l . w/(t) n r/(tt) or l . w/(t ) \nn w/(tt).In both cases, (t, tt). D[r/,w/](t). The third and .nal option is handled symetrically. B. \nProof of Lemma 4.2 The .rst part of the claim follows from the observation that given transition ti = \n(si,pi,si/), pi+1 cannot distinguish between si and s/, since all the memory locations it accesses i \nin s/= si+1 are also de.ned in si and have the same value i there. The same reasoning applies to pi. \nWe prove the second part of the claim by induction on the length n of a shortest sequence of adjacent \ntranspositions required to bring d (the permutation transforming Pp(t) into p P (tt)) into order. (Such \na sequence is guaranteed to exist.) We further argue that the natural number m satisfying t =m tt is \nn. For the base case where n =1, we can use the .rst claim proved above, since the transposed statements \nare adjacent, and their corresponding transitions (in t) are not dependent. Commutativity between the \nstatements implies that traces t and tt have the same .nal state, and thus t =1 tt. For the induction \nstep, we assume that a shortest sequence of adjacent transpositions transforming d into the identity \npermutation is of length n +1. We rely on the fact that we can choose a shortest sequence where at the \nk-th step, transposition pk = (i, i +1) operates on a descent of d as modi.ed so far (i.e., dk(i) >dk(i \n+1), where d1 = d and dm+1 = pm(dm)). Let S be such a sequence and p1 = (i, i +1) the .rst transposition \nin S. Applying p1 to Pp(tt) removes descent i. Observe that for all (ti,tj ). D[r, w](t), dp1(i) <dp1(j), \nsince the application of p1 to d merely removes an inversion. Thus, by the induction hypothesis, if we \nconsider the trace .t corresponding to schedule Pp(tt)p1 run from starting state s1, then t =n .t. Since \ntt =1 .t,we conclude that t =n+1 tt, which completes our proof.  C. Proof of Lemma 6.1 We show that \nif memory location l is in the read set of t # = (s#,p,s/#), then it is either in the read set or the \nwrite set computed by ABSREADWRITE for t = (s, p, s/), and if l is in the write set of t #, then it is \nin the write set computed by ABSREADWRITE for t. We split our proof into four cases: Case 1: l is an \nenvironment location, and t is a non-ADT transition. Since ABSREADWRITE does not apply abstraction and \nuses conservative read and write sets, l is treated conservatively. Case 2: l is an environment location, \nand t is an ADT transition. l is preserved by the relevant abstraction function,  \u00df[repai ], since repai \nonly operates on heap locations. If l is read by t#, then it must be an argument of p (or the result, \nif the value of the de.ned variable remains unchanged), in which case it is also in the read set assigned \nto t.If l is written by t#, then this is because its value has changed between the entry and exit states. \nABSREADWRITE also uses write, and thus l is guaranteed to also be in the write set of t . Case 3: l is \na heap location, and t is a non-ADT transition. By our assumption of encapsulation, l is a concrete heap \nlo\u00adcation. ABSREADWRITE treats l conservatively in refrain\u00ading from applying abstraction and using conservative \nread and write sets. Case 4: l is a heap location, and t is an ADT transition. By our (simplifying) assumption \nthat ADT operations do not manipulate heap locations outside the ADT, we conclude that l is part of the \nstate of some ADT a. By our assump\u00adtions regarding encapsulation and disjointness of ADT op\u00aderations, \nwe know that it suf.ces to base \u00df only on repa to observe accesses to l in transition t#.If l is read \nby t#, then it is also read under ABSREADWRITE since the entire state of a is considered read. If l is \nwritten by t # , then ABSREADWRITE also marks it as read since it applies write to \u00df[repa](s) and \u00df[repa](s/). \n  \n\t\t\t", "proc_id": "2048066", "abstract": "<p>Parallelization transformations are an important vehicle for improving the performance and scalability of a software system. Utilizing concurrency requires that the developer first identify a suitable parallelization scope: one that poses as a performance bottleneck, and at the same time, exhibits considerable available parallelism. However, having identified a candidate scope, the developer still needs to ensure the correctness of the transformation. This is a difficult undertaking, where a major source of complication lies in tracking down sequential dependencies that inhibit parallelization and addressing them.</p> <p>We report on Hawkeye, a dynamic dependence-analysis tool that is designed to assist programmers in pinpointing such impediments to parallelization. In contrast with field-based dependence analyses, which track concrete memory conflicts and thus suffer from a high rate of false reports, Hawkeye tracks dependencies induced by the abstract semantics of the data type while ignoring dependencing arising solely from implementation artifacts. This enables a more concise report, where the reported dependencies are more likely to be real as well as intelligible to the programmer.</p>", "authors": [{"name": "Omer Tripp", "author_profile_id": "81435610768", "affiliation": "Tel Aviv University, Tel Aviv, Israel", "person_id": "P2839160", "email_address": "omertrip@post.tau.ac.il", "orcid_id": ""}, {"name": "Greta Yorsh", "author_profile_id": "81315492721", "affiliation": "ARM, Cambridge, United Kingdom", "person_id": "P2839161", "email_address": "greta.yorsh@arm.com", "orcid_id": ""}, {"name": "John Field", "author_profile_id": "81100419562", "affiliation": "Google, New York, NY, USA", "person_id": "P2839162", "email_address": "jfield@google.com", "orcid_id": ""}, {"name": "Mooly Sagiv", "author_profile_id": "81100150928", "affiliation": "Tel Aviv University, Tel Aviv, Israel", "person_id": "P2839163", "email_address": "msagiv@post.tau.ac.il", "orcid_id": ""}], "doi_number": "10.1145/2048066.2048085", "year": "2011", "article_id": "2048085", "conference": "OOPSLA", "title": "HAWKEYE: effective discovery of dataflow impediments to parallelization", "url": "http://dl.acm.org/citation.cfm?id=2048085"}