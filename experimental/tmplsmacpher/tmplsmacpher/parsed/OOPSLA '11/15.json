{"article_publication_date": "10-22-2011", "fulltext": "\n Safe Parallel Programming using Dynamic Dependence Hints Chuanle Ke and Lei Liu Chao Zhang Tongxin Bai, \nBryan Jacobs, Institute of Computing Technology, Intel Labs, China and Chen Ding Chinese Academy of Sciences \nzhangchaospecial@gmail.com University of Rochester {kechuanle,liulei}@ict.ac.cn Abstract Speculative \nparallelization divides a sequential program into possibly parallel tasks and permits these tasks to \nrun in paral\u00adlel if and only if they show no dependences with each other. The parallelization is safe \nin that a speculative execution al\u00adways produces the same output as the sequential execution. In this \npaper, we present the dependence hint, an interface for a user to specify possible dependences between \npossibly parallel tasks. Dependence hints may be incorrect or incom\u00adplete but they do not change the \nprogram output. The inter\u00adface extends Cytron s do-across and recent OpenMP order\u00ading primitives and \nmakes them safe and safely composable. We use it to express conditional and partial parallelism and to \nparallelize large-size legacy code. The prototype system is implemented as a software library. It is \nused to improve per\u00adformance by nearly 10 times on average on current multicore machines for 8 programs \nincluding 5 SPEC benchmarks. Categories and Subject Descriptors D.1.3 [Concurrent Programming]: Parallel \nprogramming General Terms Languages, Performance Keywords do-across parallelism, post-wait, speculative \nparallelization, safe parallel programming 1. Introduction Speculative parallelization divides a sequential \nprogram into possibly parallel tasks for example as safe futures [33], ordered transactions [32] or PPRs \n[12] and uses a run\u00adtime system to ensure sequential equivalence. Speculation is useful in addressing \nthe problems of uncertain parallelism due to either implementation or program input. It enables safe \nparallelization of programs that use legacy code and programs that have frequent but not de.nite parallelism. \nPermission to make digital or hard copies of all or part of this work for personal or classroom use is \ngranted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. OOPSLA \n11, October 22 27, 2011, Portland, Oregon, USA. Copyright c . 2011 ACM 978-1-4503-0940-0/11/10. . . $10.00 \n{bai,jacobs,cding}@cs.rochester.edu Most previous systems allow speculation to succeed only if program \ntasks are completely independent (also called do\u00adall parallelism or embarrassingly parallel). Two tasks \nare serialized if they have a con.ict. However, in many cases tasks are partially parallel (called do-across \nparallelism). An example is result collection, where a set of tasks compute on different data and then \ncombine their results into a shared counter. Another one is pipelined parallelism, where each task is \ndivided into stages, and parallelism exists between stages rather than tasks as a whole. To safely express \npartial parallelism, we present the de\u00adpendence hint, an interface for a user to suggest post-wait dependences \nbetween possibly parallel tasks. It enables tasks to speculatively synchronize with each other. For example, \na user can parallelize a compression utility by dividing the in\u00adput into chunks and then use dependence \nhints to assemble the compressed chunks after parallel compression. Dependence annotations have been \nstudied in the past, including post-wait by Cytron for static parallelization [10], signal-wait by Zhai \net al. for thread-level speculation (TLS) [35], and .ow by von Praun et al. for ordered trans\u00adactions \n[32]. In these systems, parallel tasks shared data directly. In the latter two, speculation required \nspecial hard\u00adware. Speculation support may be implemented on conven\u00adtional hardware, using a two-step \nstrategy. The .rst is copy\u00adon-write in speculative tasks to isolate them from each other. The second \nis serial commit to merge concurrent changes and resolve con.icts. This software-only strategy is used \nby BOP [12, 17, 36] and Cord [30] for speculative parallelization, Grace [7] for race-free threaded execution, \nisolation and revision types in C# [8] and implicit copying and explicit commit in C [13] for safe parallel \nprogramming, DoublePlay [31] for deter\u00administic replay, CoreDet [6] and Determinator [3] for deter\u00administic \nparallel execution. While these systems eliminate the interaction between parallel tasks to ensure safety, \nthese tasks cannot coordinate with each other during execution. Any interaction, e.g. en\u00adforcing a dependence, \nwould require explicit communica\u00adtion. The dependence hint uses a channel to express communi\u00adcation \nbetween copy-on-write tasks. The channel abstraction has three bene.ts. First, a channel communicates \ndynami\u00adcally allocated data, i.e. a task can receive new data from a peer task without knowing the address. \nSecond, a channel communicates aggregate data, which allows staged execu\u00adtion such as pipelining. Finally, \nchannels may be chained to handle conditional dependences. We design the dependence hint with the following \ngoals: Safety. Dependence hints are added to a sequential pro\u00adgram. The program with hints always produces \nthe iden\u00adtical result as the program without hints (hence no need for parallel debugging). When the hints \nare wrong, the program should not run slower than the sequential ver\u00adsion.  Expressiveness. The hint \ncan express regular and condi\u00adtional dependences, on data allocated before or during parallel execution. \n Concision. Not all dependences need hints. The needed ones can be combined in a few hints or expressed \nthrough high-level constructs.  These properties by themselves do not imply that depen\u00addence hints can \nbe effectively used by average programmers. There are inherent dif.culties in reasoning about concurrent \nvalue .ows on replicated data. Our focus is the safe imple\u00admentation that ensures correct program output \nagainst all possible errors. A system may help a user to correct erro\u00adneous hints or insert hints automatically, \nalthough these ex\u00adtensions are outside the scope of this paper. Because of the cost of software implementation, \nwe fo\u00adcus on coarse-grained task parallelism in sequential code. We target small-scale parallelism to \nutilize spare cores on today s workstations rather than for scalable performance on massively parallel \nmachines. The rest of the paper is organized as follows. Section 2 in\u00adtroduces software speculative parallelization. \nSection 3 de\u00adscribes the dependence hint: the interface, the safe imple\u00admentation, the conditional and \nhigh-level hints. Section 4 demonstrates their use on four examples. Section 5 shows the performance. \nFinally, the last two sections discuss re\u00adlated work and summarize. 2. Background on The Parallelism \nHint Software speculative parallelization is pioneered by Rauch\u00adwerger and Padua [24]. Our work is based \non behavior-based parallelization (BOP), which provided a manual interface to suggest possibly parallel \nregions (PPR) [12]: bop ppr{ PPR code } marks a block of code and sug\u00adgests task parallelism the code \nin the PPR block may be parallel with the code after the PPR block. BOP uses a process to implement a \nPPR task. A process is heavy weight, but it is .exible and fully protected. A PPR task can be forked \nanywhere in a program and can be aborted by simply killing it. More interesting is the effect on data \nsharing. When a process writes to a page for the .rst time, it makes a copy of the page. This is known \nas copy-on-write. Data copying insulates a PPR task against changes happening in other tasks. More importantly, \ndata copying removes all false dependences write-after-read, write-after-write con.icts between PPR tasks. \n When PPR tasks .nish, BOP checks their data access for true dependences (read-after-write) and if none \nfound, merges the data changes. Since a PPR task may be wrong, an understudy process re-executes speculative \nwork non\u00adspeculatively. The understudy is always correct because it is the same as sequential execution. \nListing 1 shows a simple program with two PPR tasks.  # try setting g[x] and using g[y] # in parallel \n bop ppr { g[x] =foo( x ) }bop ppr { bar( g[y] ) } t1 = fork { # copy-on-write g[x] g[x] =foo( x ) \n# understudy for error recovery t2 undy = fork { bar( g[y] ) # safe re -execution kill t2 # abort speculation \n }}t2 = fork { bar ( g[y] ) # speculation # wait for t1 join(t1) # check correctness of t2 if g[x] and \ng[y] are the same array cell exit # abort speculation else copy g[x] from t1 to t2 # commit kill t2 undy \n# abort understudy end } # either t2 or t2 undy succeeds/continues  Listing 2 shows the BOP implementation \nwith three pro\u00adcesses: the .rst and the third fork operations are for the two PPRs and the second fork \nstarts the understudy. The imple\u00admentation is simpli.ed for the purpose of illustration and does not \nrepresent the complete design [12]. Safe parallelization relies on the following: fork-without-join. \nA PPR task commits when it .nishes. A user does not specify when a task should .nish.  copy-on-write \ndata replication. PPR tasks are isolated from each other. All false dependences are eliminated.  access \nmonitoring. A PPR task uses virtual memory sup\u00adport to monitor access (to global and heap data) at page \ngranularity. The .rst read and write access to each page is recorded. The read and write sets are compared \nafter the parallel execution to check for dependences.  recovery via understudy. The understudy process \nruns the original code and aborts PPR tasks if they are wrong or too slow.  BOP supports only iterative \nparallelism with no nesting. A PPR is ignored if it is encountered in a PPR execution. Nesting can be \nsupported using established techniques in race checking [5, 20]. In addition to BOP, a number of speculation \nsystems are based on Unix processes, including Grace [7], SMTX [23] and recently DoublePlay [31]. Process-based \nspeculation has been extended to support speculative memory allocation in Grace [7], irregular task sizes \nand adaptive speculation in BOP [36? ]. As an interface, the parallelism hint has a major limitation \nPPR tasks cannot collaborate with each other while they are executing. Dependences between active PPR \ntasks have to be enforced by rollbacks and sequential re\u00adexecution. We next show the dependence hint \nto allow tasks to utilize partial parallelism and avert rollbacks. 3. The Dependence Hint This section \npresents the interface, the safe implementation, and improvements in ef.ciency and expressiveness. If \nthe basic interface in Section 3.1 feels too low-level for manual use, treat it as the implementation \ninterface for building high-level hints in Section 3.4. 3.1 The Interface The dependent hint has three \nprimitives, which are matched at run time by a unique channel identi.er. The identi.er is either a number \nor a string. bop .ll(channel id, addr, size) is called by a sender task to register an address range \nwith a channel. The data is copied when the channel is posted.  bop post(channel id [, ALL]) is called \nto post the chan\u00adnel and its data. An optional ALL, if speci.ed, means to  post all modi.ed data.1 A \nchannel is single post post\u00ading to a posted channel has no effect (a no-op). bop wait(channel id) stalls \nthe receiver task until the channel is posted. The received data is placed in the receiver task in the \nsame address as in the sender. If a task waits for the same channel multiple times, later waits have \nno effect (no-ops). Multiple tasks may wait for the same channel and receive data from the same sender. \n A sender task .lls a channel with data and then posts the channel. A receiver task waits for a channel \nand then copies the channel data. The hint represents both synchronization and communication: the post \nis a non-blocking send, and the wait is a blocking receive. An example Listing 3 shows a processing loop \nthat com\u00adputes on each input from a queue and inserts the result to an output queue. The processing step \nmay be parallel, but the queuing step is not each iteration adds a new node to the output queue. Speculation \nin Listing 3 would always fail because of the dependence.2 The solution in Listing 4 serializes the queuing \nstep using a dependence hint. Each (non-.rst) iteration calls bop wait to wait until the previous iteration \nposts the result. Then it re\u00adceives the queue tail, appends a new node, and calls bop .ll and bop post \nto send the new tail to the next iteration. The variable cid is used to create a new channel id at each \nit\u00aderation, so there is no reuse of channels and no cross-talk between non-adjacent iterations. Listing \n4 uses three primitives. To make the coding sim\u00adpler, we can replaced them by a high-level hint, bop \nordered, which we show in Listing 5 and will describe in Section 3.4. Next we explain the three features \nof the dependence hint necessary for this example to be parallelized correctly. Sender-side addressing \nAs a sender .lls a channel, it records both the data and its address. A receiver does not say what data \nto receive or where to place the received data the data from the channel is placed in the same address \nas the sender speci.ed. Sender-side addressing has three bene.ts. First, it simpli.es the receiver interface \nand avoids the problems of mismatch. An example mismatch, as can hap\u00adpen in MPI, is when a task sends \na message of size n, but the receiver expects a message of size 2n. Sender-side address\u00ading removes all \npossible sender-receiver disagreements. Second, sender-side addressing allows a task to commu\u00adnicate \ndynamically allocated data (of a dynamic size). This is necessary for the solution in Listing 4. The \noutput queue grows node by node. The next iteration can receive the new tail, even though it did not \nhave the node in its address space and had no knowledge of its allocation in the previous iter\u00ad 1 which \nmeans all data modi.ed since the last .ll/post/wait call or if there is none, all data modi.ed since \nthe start of the task. 2 Note that the speculation would still fail if the queue insertion is moved outside \nthe PPR block, since the insertion needs the datum computed inside the PPR.  while (has more ( inputs \n)) begin w= get next ( inputs ) # try computing w in parallel bop ppr { t= compute (w) # allocate a new \nnode n n= new qnode(t) # make n the new tail append(outputs , n) } end cid = 0 # channel id while (has \nmore ( inputs )) begin w= get next ( inputs ) bop ppr { t= compute (w) n= new qnode(t) # wait for the \nlast tail bop wait (cid - 1) if cid >0 append(outputs , n) # send the new tail bop.ll(cid, n, sizeof(qnode) \n) bop post(cid) } cid ++ end while (has more ( inputs )) begin w= get next ( inputs ) bop ppr { t= \ncompute (w) bop ordered { n= new qnode(t) append(outputs , n) } } end ation. Communicating dynamic \ndata is as simple to code as communicating static data. Third, the run-time system can dynamically change \nthe content of communication. This feature is critical in the safe implementation which we will describe \nin Section 3.2. Selective dependence marking Enumerating all depen\u00ad 2 dences is impracticable because \nthere may be ndepen\u00addences in an n-statement program. The dependence hint is for selective marking, i.e. \nfor only dependences from a PPR task to its continuation. We call these PPR dependences. Other dependences \ndo not require hints, including depen\u00addences within a PPR task, dependences within inter-PPR code and \nfrom inter-PPR to PPR code. PPR dependences do not need hints if they are too infre\u00adquent to affect \nperformance. The rest, more regularly occur\u00adring dependences can be divided into two types: short range \nand long range. Short-range dependences happen between nearby PPR tasks, which are likely to require \nhints for coor\u00addination. Long-range dependences happen between distant PPR tasks, which are most likely \nalready serial and do not need hints. For example when parallelizing a loop, hints are needed for short-range \ndependences between consecutive it\u00aderations but not for long-range dependences, e.g. between the loop \nand the subsequent code. Furthermore, multiple dependences can share a single channel and be marked by \na single hint. For example, in pipelining, each stage needs just one post and one wait. An\u00adother example \nof enforcement en masse is to suggest a join point to serialize two PPR tasks and satisfy all dependences \nbetween them. For these reasons, the number of hints can be few even though the dependences may be many. \nA careful reader may note that long-range dependences, although they do not need synchronization, still \nneed com\u00admunication. Such communication is done at the commit time when a PPR task .nishes and its modi.ed \ndata copied into later tasks. Data commits and dependence hints are the two ways by which PPR tasks share \ndata. Data commits move data asynchronously and do not block an active task. Depen\u00addence hints are synchronous \nand may stall the receiver task. Dependence hints require an explicit hint, while data com\u00admits do not. \nNow we can explain a subtlety in the solutions in List\u00ading 4 and Listing 5. The last PPR task, the one \nto create the last node, is supposed to have the full queue, but it does not. From the dependence hint, \nit has only the node it creates and the one before it just the two nodes, not the full queue. How and \nwhen is the entire queue assembled? The rest of the queue is pieced together by data commits. As tasks \n.n\u00adish, their data is copied out and merged. The construction happens asynchronously as the loop progresses. \nSelective dependence marking bene.ts both programma\u00adbility and performance. In the example, parallelization \nis simple since only the tail node requires a hint. It is also ef.\u00adcient and more scalable since the \ncommunication is constant size rather than linear size. Each node is copied just once. Safety and determinism \nUnlike in non-speculative sys\u00adtems where communication primitives must be perfectly paired, dependence-hint \nprimitives are suggestions and may mismatch. To ensure determinism, a channel accepts at most one post. \nIf we were to allow multiple posts, we would be un\u00adcertain how many of the posts had happened at the \ntime of a bop wait. In addition, the restriction means that a channel cannot be .lled by multiple tasks. \nIf two PPR tasks could .ll the same channel, we would be uncertain which task .nished .rst placing data \nin the channel. A third bene.t is ef.ciency bop .ll is a task-local operation since a channel is never \nused by more than one writer. A programmer may not know completely about the code she is parallelizing. \nFor example the loop shown in Listing 3 may have abnormal entries and early exits in the compute call. \nThe call may have hidden dependences including ac\u00adcess to the output queue. Next we describe the safe \nimple\u00admentation to guard against potential errors.  3.2 Safe Implementation Incomplete knowledge of \na program can cause three types of errors in hint speci.cation: under-speci.cation, where an actual dependence \nis not hinted; over-speci.cation, where a hinted dependence does not happen in execution; or incor\u00adrect \nspeci.cation, where the location or the cause of a de\u00adpendence is wrong. Some errors (e.g. a missed dependence) \nmay cause the speculation to fail, some (e.g. a widow wait) may delay a task unnecessarily, and some \n(e.g. a widow post and intra-task post-wait in our example) may add unneces\u00adsary overhead but do not \notherwise harm parallel execution. To present our implementation and show its correctness, we .rst de.ne \na con.ict, which happens when a task in a parallel execution reads a value different from the would-be \nvalue in a sequential execution. It follows that a parallel ex\u00adecution produces the sequential result \nif there is no con.ict. The absence of con.icts are ensured by .ltering and three checks as follows: \n .ltered posting At bop post, only modi.ed data in the channel is posted. The .ltering avoids useless \ndata transfer and simpli.es correctness checking.  the sender con.ict The con.ict happens when a task \nmodi.es the data it has already posted. The check detects the transmission of stale data.  the receiver \ncon.ict There is a con.ict if a task ac\u00adcesses (reads or modi.es) some data and then later re\u00adceives \nit by a wait. The check detects premature use of dependent data.  the last-writer check For every piece \nof data received by a task p, its sender must be the last writer before p in the sequential program order. \n The .ltering and the .rst two checks are task local and performed during speculation. The last-writer \ncheck in\u00advolves potentially all speculation tasks. It is performed dur\u00ading the commit process. Consider \nan example in Figure 1. There are 3 tasks: T1 and T2 write x, and T3 reads x. The correct post-wait is \nto pass x from T1 to T2 and then from T2 to T3. In the example, however, T1 sends x to T2 and T3. task \n1 task 2 task 3 Figure 1: A misuse of post-wait: task 3 reads incorrect x. The three checks detect \nthe error as follows. The sender\u00adcon.ict check ensures that T1 send last modi.ed version of x. The receiver-con.ict \ncheck ensures that T2 and T3 do not use x before getting its new value. The last-writer check ensures \nthe right pairing of communication. In T2, the last writer and the sender of x are both T1, so the check \npasses. In Task 3, the sender is T1, but the last writer is T2, so the check fails. As a result, the \nsystem aborts and re-executes T3. Consider an extension of this example in which T2 sends correct x to \nT3, but T3 does not wait for it. The last-writer check still fails since T3 consumed the wrong version \nof x. Next we show the correctness formally using a proof similar to that of the Fundamental Theorem \nof Dependence (Sec. 2.2.3 in [2]) and the one in [12]. Theorem 3.1. The three checks, if all passed, \nrule out all possible con.icts in a parallel execution. Proof We assume that PPR tasks are numbered by \ntheir sequential order, all inter-task dependences are marked by hints, and all tasks have passed the \nthree checks. We show that the parallel execution has no con.ict. We prove by contradiction. Let task \ntq be the .rst task to have a con.ict it reads location x but the value is different from reading x in \nthe sequential execution. Since all values seen by tq before this read were correct, the incorrect value \nmust come from out\u00adside tq. Let s assume that in the sequential execution, task tp is the last to write \nx before tq. Since tq is the .rst to have a con.ict, tp must be correct and write x correctly. The ques\u00adtion \nis whether tp communicates to tq properly and whether other tasks may interfere by interjecting some \nother x to tq. Because all three checks are passed, tp and tq must com\u00admunicate properly. First, tp sends \nx after its last write, tq receives x from tp before its .rst read. Second, tasks after tp do not post \nx because tp is the last writer. Third, a task be\u00adfore tp may send x to tq. However, tq must read the \nversion of x from tp (to pass the last-writer check). Therefore, there is no way for tq to receive an \nincorrect x, which contradicts the assumption that x is incorrect. Progress guarantee Usually a blocking \nreceive such as bop wait may entangle a set of tasks into a deadlock. As a dependence primitive, BOP \ncommunication .ows in one direction in the increasing order of the task index. In addi\u00adtion, BOP uses \nan understudy process to execute speculative tasks non-speculatively. Understudy does not speculate and \nignores all hints. If there is a wait with no matching post, the unmatched wait is dropped when the understudy \n.nishes the re-execution of the waiting PPR. Dependence hints are free of concurrency errors such as \ndeadlocks, live locks, and data races. The implementation is wait free, which means that all tasks .nish \nwithin bounded time the time taken by the sequential execution. Con.ict handling If PPR i incurs a sender \ncon.ict, we abort and re-start all PPR j for j>i. With suf.cient book\u00adkeeping, we can identify and rollback \nonly those tasks that are affected by the offending send. In addition, a sender can remove and update \ndata in a posted channel if it has not been received, or if it is received but not consumed. To avoid \nre\u00adcurring con.icts, the BOP runtime can ignore a post opera\u00adtion if it caused a sender con.ict before, \nleveraging the learn\u00ading strategies studied by Jiang and Shen [? ]. At a receiver con.ict, the faulting \ntask is aborted. A special case is when a task receives the same data, say x, from different senders \n(note that it cannot receive multiple x from the same sender because of the modi.cation and sender-con.ict \nchecks). We rank the recency of received values by the task index of their sender. A larger index means \na later sender and a newer value. There are three cases. First, x is .rst accessed before the .rst receive, \nwhich means a receiver con.ict. Second, x is .rst accessed between two receives. If the value from the \n.rst receive is less recent than that of the second receive, a receiver con.ict is triggered. If the \nvalue from the .rst receive is newer, the second receive can be silently dropped without raising a con.ict. \nIn the third case, x is .rst accessed after two or more receives. We keep the newest value of x and continue. \nWe call the last two extensions silent drop and reordered receive. Inter-PPR post-wait A post in an inter-PPR \ngap is unneces\u00adsary since its writes are visible to all later execution. bop .ll is ignored, and bop \npost marks the channel as posted with\u00adout communicating any data. Consider for example a depen\u00addence \nbetween two inter-PPR gaps. The dependence is al\u00adways satis.ed since the gaps run sequentially. There \nis no need to send any data, and there will be none sent. We call it inter-PPR hint override. Finally, \na wait in an inter-PPR gap is treated normally since it may be the target of a PPR depen\u00addence. Outdated \npost and channel de-allocation If a post-wait pair spans many PPR tasks, it is possible that the receiver \nalready has the sent data. The channel data is freed when the sender commits and all active tasks have \nthe data. The system still stores the identi.er of posted channels in case a task later waits for one \nof these channels. When it happens, the task does not block and does not receive the data again. We call \nit distant-PPR hint override. The storage cost is small since the system stores only channel identi.ers \nnot channel data. It is worth noting that most techniques in this section .ltered posting, silent drop, \nreordered receive, the inter-PPR and the distant-PPR override dynamically change the con\u00adtent of a channel. \nAs mentioned before, they are possible be\u00adcause of sender-side addressing. The feature bene.ts both programming \nand performance. For example with .ltered posting, a task may change sub-parts of an array, and the programmer \ncan simply post the whole array without incur\u00adring unneeded communication. 3.3 Conditional Dependence \nby Channel Chaining If every task modi.es shared data, we enforce in-order ac\u00adcess by all of them. Sometimes \nhowever, there may be non\u00adcontributing tasks that decide not to write the shared data. Assuming the decision \nis dynamic, how should the next task know whether to wait for its predecessor? There are two sim\u00adple \nsolutions: the next task waits until the previous task .n\u00adishes, or the previous task tells the next \ntask not to wait. The .rst loses parallelism. The second requires a communica\u00adtion. Neither does any \nbetter whether there is a dependence or not. We note that this problem does not exist in concur\u00adrency \nconstructs such as transactions and atomic sections. It is unique with dependence annotations because \nof their ad\u00adherence to sequential semantics. An ef.cient solution is to dynamically chain multiple channels \nto bypass non-contributing tasks. We call it channel chaining. The primitive is bop cc: bop cc(channel \n1, channel 2) is called by a task to chain two channels. After chaining, a receiver of either channel \nwaits for both channels, and a sender of either channel posts to both channels. The two channels are \neffectively aliases. We revise the previous example. In Figure 2, an iteration outputs to the queue \nonly conditionally in some cases; oth\u00aderwise, it calls bop cc to chain cid minus 1 with cid, so the next \niteration waits for the post by the previous iteration. If only a few iterations generate output, most \nchannels will be chained, and only the generating tasks will synchronize and communicate queue data. \nChannel chaining has four bene.ts. First, any number of tasks may decide not to contribute. More than \ntwo channels may be chained. Second, a task may decide at any time not to contribute, regardless whether \nthe peer tasks have posted or waited. Third, no data is transferred to non-contributing tasks. Last, \nincorrect chaining will be detected, for example, when a task mistakenly connects two channels. The imple\u00admentation \nin Section 3.2 is safe against all misuses of chan\u00adnel chaining. 3.4 The Ordered Hint Dependence, although \nfundamental, may be too low level for a programmer. In this section, we build a high-level hint as follows: \n while (has more ( inputs )) begin w= get next ( inputs ) # try computing w in parallel bop ppr { t= \ncompute (w) # conditional queue insertion if t =.nil begin n= new qnode(t) append(outputs , n) end } \nend cid = 0 # channel id while (has more ( inputs )) begin w= get next ( inputs ) bop ppr { t= compute \n(w) n= new qnode(t) if t =.nil begin .. . # same as Listing 4 else # bypass the sync /comm bop cc(cid \n-1,cid ) end } cid ++ end Figure 2: Channel chaining to handle a conditional depen\u00addence. Communication \nhappens only between iterations that perform queue insertion. bop ordered{ code } marks a block of code \nand sug\u00adgests an ordered execution PPR tasks running the code should execute one at a time and in their \nsequential order. Figure 3(a) shows an ordered block in a function called foo, and the function is called \nin a while-loop. Figure 3(b,c) show two implementations of bop ordered depending on whether the number \nof foo calls is known. When the last call is known, we post after the last call; otherwise, we post at \nthe end of the task. A special case is when foo is not called at all. bop cc is used. OpenMP provides \nordered as a directive for loop paral\u00adlelization [21]. Gossamer introduced it as a general directive \nfor task parallelism [26]. The examples in these papers show a single ordered call in each task or loop \niteration. In fact, the OpenMP standard requires that ordered be used exactly once in each iteration \na conforming example ... each iter\u00adation will execute only one ordered region. [21] It is unclear how \nerrors are handled, e.g. dependence between ordered and unordered code, and how they may affect other \nparallel primitives in the language. Like Gossamer ordered, bop ordered is a general primi\u00adtive and \ncan be used to serialize code in different functions. Unlike in OpenMP and Gossamer, however, bop ordered \nis a safe hint and can be used amidst uncertain control .ow. For example a program may branch into or \nout of an or\u00addered block, which would be illegal in OpenMP. This shows the dependence hint as a .rm foundation \nfor the high-level hints. A user can de.ne high-level hints using BOP post-wait to de.ne desirable semantics \nespecially with conditional ex\u00adecutions (as we have done in Figure 3). The high-level hints are safe \nin composite uses and against misuse. 4. Programming with Dependence Hints We show four examples of parallelization. \nBecause of hid\u00adden or unpredictable dependences, the .rst two are dif.cult to express, and the second \ntwo are dif.cult to implement safely, if we were to use conventional methods. 4.1 String Substitution \nConsider the task of sequentially replacing all occurrences of the 3-letter pattern aba with bab . The \nprocess may be parallel, e.g. when the input has no aba . But it may be completely sequential, e.g. when \nthe string is abaa...a and should become bb...bab after conversion. For safe paral\u00adlelization, we mark \nthe inner loop a possibly parallel task. To count the number of substitutions, we use an ordering hint \nto add per PPR task counts into a global counter num. The pro\u00adgram in Listing 8 processes the input in \nm-letter blocks. The code uses the range syntax. For example, str[lo...hi] refers to the series of letters \nstarting from str[lo] and end\u00ading at (and including) str[hi]. 4.2 Time skewing Iterative solvers are \nwidely used to compute .xed-point or equilibrium solutions. Listing 9 shows the structure of a typi\u00adcal \nsolver as a two-level loop. An outer iteration, often called a time step, computes on domain data .rst \nand then checks for convergence. There is no static or dynamic parallelism between time steps: the convergence \ncannot be checked un\u00adtil all computations .nish, and the next step cannot start until the convergence \ncheck is done. But there is specula\u00adtive parallelism the computations do not converge until the last \niteration, so time steps may overlap before then. The transformation is known as time skewing [34]. Previous \nlit\u00aderature shows large performance bene.ts for both sequen\u00adtial [27, 34] and parallel [18] executions. \nParallelization hints can express time skewing with two PPRs and two ordering hints. The .rst PPR (safely) \nparal\u00adlelizes the inner loop. The second PPR makes the conver\u00adgence check asynchronous, so the program \ncan skip the con\u00advergence check and start the next time step, allowing two while not done bop_ppr { \n// parallel work ... call foo ... } end while func foo bop_ordered { // serial work }  end func (a) \nan example parallel loop andordered block Figure 3: Using post-wait to implement the bop ordered region \nhint. while not done bop_ppr { if (no foo call)bop_cc( ppr_id ,ppr_id+1) ... call foo ... } end while \n func foo bop_wait( ppr_id) // serial work if (last foo call) bop_post(ppr_id+1, ALL) end func (b) \nimplementation by post-wait if the numberof foo calls is known while not done bop_ppr { ... call foo \n... bop_post(ppr_id+1, ALL) } end while  func foo bop_wait( ppr_id) // serial work  end func (c) \nimplementation if the number of foo calls is not known.  str[0...n], src= aba , target= bab num = 0 \n# number of substitutions for ii in 2...n with step b do # try a string block in parallel bop ppr { for \ni= ii ...min(ii+b -1, n) cnt =0 if matches( str [i -2... i], src) str [i -2... i] = target cnt ++ end \nend # update num sequentially bop ordered { num += cnt } } end time steps to overlap. The convergence \ncheck must wait for the computations to .nish. This is done by two ordering hints: the results of the \ndomain computation is combined in the .rst ordered region, and the convergence check is then made in \nthe second ordered region. In the last iteration, the write to the converged variable triggers a (true-dependence) \ncon.ict with the speculative execution of the next time step. The speculation is then rolled back, and \nthe loop .nishes normally as if by a sequential execution. converged = false while not converged for \ni in 1...n # try inner loop in parallel bop ppr { r= compute( data[i] ) bop ordered { s= s.add result( \nr ) } } end # try next time step in parallel bop ppr { bop ordered { if good enough ?( s ) converged \n= true end } } end  4.3 TCA Pipelining Thies, Chandrasekhar, and Amarasinghe de.ned an interface for \nexpressing pipeline parallelism in a loop [29]. We refer to the interface as TCA pipeline after the initials \nof the authors. The original version is not speculative, and a recent system called SMTX added the speculation \nsupport [23]. The body of a pipeline loop is divided into stages. Each stage is separated from the preceding \nstage by a pipeline label. By default, a stage is sequential and its label takes no parameter. A parallel \nstage has a parameter p to indicate the number of parallel processors to use for the stage. Figure 4 \n(a) shows an example TCA pipeline with 3 stages: stages 1 and 3 are sequential, and stage 2 is parallel. \nThe implementation uses one process running each sequen\u00adtial stage and p processes running the parallel \nstage. Thies et al. developed pro.ling support to identify and transfer shared data and to divide the \nstages evenly so that all pro\u00adcesses are fully utilized in the steady state. for i in 1 ... n bop_ppr \n{ bop_wait( <my_ppr-1, s1> ) for i in 1 ... n // serial stage 1 begin_pipelined_loop  bop_post( <my_ppr, \ns1> ) // serial stage 1 bop_wait( <my_ppr-1, s1> )pipeline( p ) // parallel stage 2  bop_post( <my_ppr, \ns2> ) // parallel stage 2  bop_wait( <my_ppr-1, s3> )pipeline // serial stage 3// serial stage 3  bop_post( \n<my_ppr, s3> ) end_pipelined_loop } end for end for (b) implementation by(a) 3-stage TCA pipeline dependence \nhints Figure 4: Using post-wait to safely implement the pipeline loop construct of Thies et al. [29] \nThe pipeline parallelism can be implemented by post\u00adwait, as shown in Figure 4 (b). Each stage starts \nwith a wait and ends with a post. The channel identi.ers are set up to wait for the same stage in the \nprevious task, if the stage is sequential. A parallel stage has two cases. If it is the .rst stage, it \nshould not wait for anyone; otherwise, it waits for the previous stage in the previous task. Note that \nthe implementation can be encapsulated so the programmer is provided with the same interface as the TCA \npipeline, e.g. through a bop pipeline hint. The BOP pipeline exploits the same parallelism as TCA pipeline \nand its safe version SMTX [23], but the implemen\u00adtation is different. In BOP, the same task uses the \nsame pro\u00adcess, which simpli.es error recovery. In TCA and SMTX, the same stage uses the same process(es), \nwhich reuses pro\u00adcesses. The TCA and SMTX pipelines are likely more ef.\u00adcient when computations are regular \nand regularly chunked into stages. On the other hand, the .xed stage partition has trouble handling variable \nlength iterations or dependence be\u00adtween non-consecutive tasks. In implementation, TCA and SMTX have \nthe advantage of process reuse over the original BOP [12]. The current BOP also reuses processes, which \nwe will discuss in Section 5.1.  4.4 Hmmer from SPEC 2006 Hmmer is a genetic search program developed \nat Washing\u00adton University with nearly 36,000 lines of C code. Most of the execution happens in two steps: \ncalibration and search. The calibration loop is shown below. Most of the time is spent in the function \nP7Viterbi. The loop traverses through a series of genetic sequences. It is parallel as far as we know \nexcept in the call to AddToHistogram zc, which adds the result computed in each iteration to a histogram. \nIt can be parallelized by a bop ppr and a bop ordered hint as shown below. The entire histogram data \n(2 memory pages in the test) is marked for posting in every task. for (i = 0; i < parallelism; i++) { \nbop_ppr { // begin possibly parallel region (PPR) mx = CreatePlan7Matrix(1, hmm->M, 25, 0); for (idx=i*temp; \nidx<(i+1)*temp &#38;&#38; idx<nsample; idx++) { dsq = DigitizeSequence(seq[idx], sqlen[idx]); if (P7ViterbiSize(sqlen[idx], \nhmm->M) <= RAMLIMIT) score = P7Viterbi(dsq, sqlen[idx], hmm, mx, NULL); else score = P7SmallViterbi(dsq, \nsqlen[idx], hmm, mx, NULL); hhu[idx%temp] = score; free(dsq); free(seq[idx]); } FreePlan7Matrix(mx); \nbop_ordered { // implemented by post-wait for (idx=i*temp; idx<(i+1)*temp &#38;&#38; idx<nsample; idx++) \n{ length_zc = AddToHistogram_zc(&#38;(post_zc.a), hhu[idx%temp]); if (hhu[idx%temp] > post_zc.b) post_zc.b \n= hhu[idx%temp]; } } // end bop_ordered } // end bop_ppr }  The search loop has more dependent operations \nat the end of each iteration to perform a signi.cance test and add signi.cant matches to a result list. \nThe serial block is several times longer in code and transfers 40 times more data (about 79 pages in \nthe test run) than in the calibration loop. The matched genes are inserted into the result list in the \nsame order as they were read from the input .le. 5. Evaluation 5.1 Experimental Setup BOP implementation \nBOP hints are implemented as run\u00adtime library calls. We have completely re-designed and re-implemented \nthe system three times to improve its ef\u00ad.ciency. The current design has three important features: Process \nreuse. Instead of forking a process for each PPR, we fork a set of processes at the .rst PPR. Each one \nis assigned the next unexecuted PPR and returns for a new assignment after .nishing. We designate a main \nprocess to serve as the understudy and always maintain a correct state. In case of a speculation error, \nthe offending pro\u00ad  Table 1: The 8 test programs test source code lines/changes num. PPRs seq. time \norig bop omp str-sub Section 4.1 80 9 - 279 4.4s k-means textbook 260 8 3 200 110s qt-cluster [15] &#38; \nSection 4.2 303 11 4 1,600 438s art SPEC 2k 1,270 29 707 2,480 1211s bzip2 SPEC 2k 4,649 19 - 373 115s \nhmmer SPEC 06 &#38; Section 4.4 33,992 22 62 85,000* 93s parser SPEC 2k 11,391 4 - 7,756 140s sjeng SPEC \n06 13,847 15 - 15 500s Parallel performance of all 8 tests 2 proc4 proc8 proc15 proc  str.sub k.means \nqt.cluster hmmer art bzip2 parser sjeng mean Figure 5: Summary of BOP performance. On average, the reduction \non end-to-end, wall-clock execution time is a factor of 2.1 on 2 processors, 3.6 on 4 processors, 6.1 \non 8 processors, and 9.1 on 15 processors. cesses are killed, and new ones are forked (from the main \nprocess) in their place. In case of unrecoverable opera\u00adtions such as system calls (e.g. mmap), only \nthe main process survives. It starts a new pool at the start of the next PPR. Dynamic load balancing. \nPPR tasks have unpredictable sizes. For load balancing, we use delegated correctness checking. When a \nprocess .nishes a PPR task, it submits the changes and then returns to run the next PPR task. Correctness \nchecking is delegated to the main process and by doing so, all processes are busy working as long as \nthere is enough parallelism.  Byte-granularity checking. BOP provides an annotation interface for recording \ndata access at byte granular\u00ad  ity [11]. If a programmer or a compiler knows all the places a variable \nis accessed, the data can be moni\u00adtored precisely, which avoids false sharing and the page\u00adprotection \noverhead. Otherwise, page-level protection is used as in the base BOP. We use the byte-granularity in\u00adterface \nto evaluate the cost of page-level protection and to implement .ne-grained PPR tasks. The use of a main \nprocess complicates task counting. Most of the times the main process does not contribute to actually \nexecuting a program. Therefore we count only speculative processes as tasks. Test Machines We test two \nmachine platforms. One has four 2.5GHz quad-core AMD Opteron (8380) processors with 512KB cache per core. \nThe other has two 2.3GHz quad-core Intel Xeon (E5520) processors with 8MB second\u00adlevel cache per processor. \nThe Intel processors are hyper\u00adthreaded, so we run our tests up to 15 tasks. The test pro\u00adgrams and the \nBOP code are compiled by GCC 4.1 with -O3 on the AMD machine and (due to errors when compiled with -O3 \n) by GCC 4.4 with -g3 on the Intel machine. The two machines have 32GB and 8GB of physical mem\u00adory respectively. \nBoth run Linux, Red Hat 4.1.2 and Linux 2.6.30. We run each version of a test on each task count from \n1 to 15 for three times (for a total of 45 runs per test) and re\u00adport the average result. Test programs \nTable 1 shows the eight test programs, in\u00adcluding the three examples in Section 4 and .ve full-size SPEC \nbenchmarks. The programs have between 80 and 34\u00adthousand lines of C code. Except for the two clustering \ntests, all make heavy use of pointer data. To parallelize them, BOP adds between 4 and 29 lines of hints \nand access annotations. Most tests use dependence hints except for parser and sjeng, which are included \nto compare with the previous BOP [12]. The number of PPRs in these programs ranges from 186 to 85 thousand. \nWe test two PPR counts for hmmer, marked by a star in the table. The average length of a parallel task \nis as small as 1 millisecond and as large as half second. Not all code is included in the source form. \nThe test bzip2 is a block-sorting data compressor. Through binary instru\u00admentation we found that up to \n25% of executed instructions are inside the glibc library. We use full address space pro\u00adtection for \ncorrectness. OpenMP parallelization For comparison, we test the OpenMP version for half of the tests. \nThe OpenMP code of Art comes from SPEC OMP 2001. A diff between SPEC 2K and SPEC OMP 2001 shows 707 lines \nof difference. Much of the code changes in the OpenMP version are due to new data structures needed to \nimplement reduction. The two clustering tests have a regular loop structure and are easy to parallelize \nusing OpenMP. We also created an OpenMP version for hmmer. OpenMP cannot parallelize string substi\u00adtution, \nwhich has input-dependent parallelism. The remain\u00ading three programs would require signi.cant changes \nto the source code to use OpenMP (e.g. some loops have early exits), which we did not endeavor to perform. \n 5.2 BOP Performance Figure 5 shows an overview of BOP performance. On aver\u00adage, BOP parallelization \nreduces the end-to-end, wall-clock program run time by a factor of 2.1 on 2 processors, 3.6 on 4 processors, \n6.1 on 8 processors, and 9.1 on 15 processors. We next discuss the programs, the cost of dependence hints, \nand the comparison with OpenMP. String substitution The test program .nds and replaces a pattern in 558MB \nof text, with a block size of 2MB per PPR and a total of 279 PPRs. We test the program with 5 different \nlevels of con.icts: no con.ict, 1%, 5%, 10%, and 50% con.icts. The sequential run time ranges from 4.4 \nseconds with no con.ict to 4.7 seconds with 50% con.icts. The ordering hint is used to measure the total \nnumber of substitutions. When there is no match, the program is sped up by 1.9 to 5.1 times with 2 to \n8 processors, as shown in Figure 6. The execution time is reduced from 4.4 seconds to 0.9 second. The \nimprovement decreases when there are con\u00ad.icts, as the four other curves show. The maximal speedup drops \nto 4.6 for 1% (3) con.icts, to 2.4 for 5% (14) con.icts, and to 1.6 for 10% (28) con.icts. With 50% con.icts, \nevery other PPR fails and requires a rollback. The program runs 7% to 14% slower. String substitution0% \n- 50% conflicts num. processors p Figure 6: The speedup of parallel string substitution with and without \nthe ordering hint. K-means The program has regular loop parallelism and dependence inside each clustering \nstep when it updates the centroids data. The ordering hint is used with the addi\u00adtional bene.t of maintaining \nthe same numerical precision for the centroid coordinates as in the sequential execution. For this test, \nthe program divides 8 million points in a 20\u00addimensional space into 10 clusters in 10 steps. BOP protects \n15627 pages or 64MB data. It uses depen\u00addence hints at page granularity to serialize the centroid data \nupdates. During commit and post-wait, it copies and trans\u00adfers 168740 pages or 691MB data. As shown in \nFigure 5 and in more detail later in Figure 9, the performance increases linearly to a factor of 4 with \n.ve processors and then slowly by anther 20% to 15 processors. Bzip2 We parallelized two versions of \nbzip2, . The original code is revision 0.1p12, dated 1997. In this version, post\u00adwait calls are inserted \nin two separate functions in the code. This cannot be done with any of the ordered hint. A newer version, \n1.0.3, is included in SPEC 2K, which we use to report performance. In this version, the serialized code \nis placed in a single function and bracketed by bop ordered. The input to bzip2 is a 300MB .le (Intel \nFortran com\u00adpiler). The speedups are 1.8 by 2 tasks, 2.8 by 4, 4.1 by 8, and 5.3 by 15 tasks. The compression \ntime was reduced from 85 seconds to 18 seconds, a signi.cant reduction. Compared to others, however, \nthe speedup is low. The reason is the large amount of disk reads. BOP runs a pool of processes, each \nof which has to read the .le separately. The total amount of .le reads multiply. 15 tasks would read \n15 times 300MB (4.5GB). The problem may be solved by using copy-on\u00adwrite in the .le system, as advocated \nby the Determinator OS [3]. Art and Hmmer The program art is used to train a neural network for image \nrecognition using the adaptive resonance theory. The training process is parallelized, and the results \nare combined using an ordering hint. As shown in Figure 5 and in more detail later in Figure 10, the \nperformance of art increases linearly but at two different rates: a faster rate from 2 to 8 processors \nand then a slower rate from 8 to 15. The speedup is 6.2 at 8 processors and 7.4 at 15 processors. For \nhmmer, we test the smallest and the largest granu\u00adlarity. The smallest is 85 thousand PPRs in 93 seconds \nor on average 1 millisecond per PPR. To support tasks at this granularity, we use the byte-granularity \ninterface instead of page-based monitoring. The dependence hint is too costly so we move the serial code \nout of the parallel region. On the other extreme, we divide the loop into p PPRs when running with p \ntasks and use the dependence hint. Hmmer has near perfect scaling with 14 times performance on 15 processors. \nQT-clustering Quality threshold (QT) clustering is an it\u00adment is a factor of 2.0 by 2 tasks, 3.8 by 4 \ntasks, 7.4 by 8 tasks, and 13 by 15 tasks. Parser obtains a speedup of 2.0 by 2 tasks, 3.8 by 4, 7.0 \nby 8, and 10.6 by 15 tasks. The two programs have do-all loops. They do not need dependence hints but \nshow the bene.ts of the current BOP design espe\u00adcially process reuse. The average task size in parser \nis 0.02 second, which cannot be parallelized ef.ciently if a process is created for every task.  5.3 \nThe Dependence-hint Overhead K-means The dependence hint requires serialization of PPR tasks. It is implemented \nusing a series of locks, one be\u00adtween each pair of tasks. To quantify the serialization cost, we measure \nthe difference between the time when the last process in a group ends and the time when the whole group \n.nishes. We call the time difference the serial pause.We collect the data from the test runs of k-means. \nWe show in Figure 7 the min, mean, and max serial pauses for two ver\u00adsions: BOP and skim BOP. BOP communicates \ndata, a total of 690MB, but skim BOP does not (and is incorrect as a result). The .gure shows the mean \npause using a solid bar, and the range from the min to the max pause using a vertical line. BOP and Skim \nBOP Overhead Comparison 1 2 3 4 5 6 7 8 9101112131415 erative algorithm proposed in 1999 for grouping \nrelated genes [15]. In each step, the algorithm .nds the cluster around every node (unclustered neighbors \nwithin a distance) and picks the largest cluster. The next step repeats the pro- Number of Processes \n Figure 7: The time takes to serialize BOP parallel tasks, with cess for the remaining nodes until all \nnodes are clustered. QT clustering overcomes two shortcomings of k-means cluster\u00ading. It needs no a priori \nknowledge of the number of clusters. The result is deterministic and does not depend on the ini\u00ad tial \nchoices of cluster centroids. The drawback is that QT clustering is more computationally intensive. We \nimplemented the QT algorithm in C and tested both intra-time step parallelism (shown in Figure 5) and \ntime skewing (see Section 5.5). There is suf.cient parallelism, 400 PPRs, in each time step. BOP shows \nhighly scalable performance, with speedups of 2.0, 3.8, 7.0, and 14 times for 2, 4, 8, and 15 parallel \ntasks, as shown in Figure 5. and without data copying. Without data copying, the average serial pause \nincreases from 0.004 second when there is no parallel execution to 0.04 seconds when 15 tasks are running. \nWith data copying, the average cost is within 3% of that without data copying, showing that data communication \nhas a negligible effect on the length of a serial pause. The cost in a sequential run comes mostly from \nre-locking the memory pages accessed during the preceding task. The cost in parallel runs is due entirely \nto synchronization. On average, the serialization takes up to 0.02 seconds for up to 6 tasks, 0.03 seconds \nfor 7 to 9 task, and 0.04 seconds Sjeng and parser Sjeng is a computer-chess program. The for 10 to 15 \ntasks. The pause time dictates the minimal task input is based on a chess-board .le in the set of reference \nin-granularity when dependence hints are used. If a PPR takes puts provided as part of the SPEC 2006 \nbenchmark package. half a second or more, the serialization overhead will be We increased the number \nof tasks in the input. The improve-insigni.cant. String substitution The k-means analysis does not con\u00adsider \nthe effect of process reuse, which can hide the cost of serialization. In string substitution, the average \nPPR size is 0.016 second, but BOP shows scalable improvements up to 8 tasks. To quantify the cost of \nthe ordering hint, we removed it and measured the performance, which is shown in Figure 6. Without the \nordering hint, the program runs 3.5% faster on average overall and 6-7% faster on average for tests with \n0%, 5%, and 10% con.icts.  5.4 Comparison with Original BOP The original BOP does not support dependence \nhints. As a result, it can parallelize 2 of the 8 programs (it can run the other programs correctly but \nnot in parallel). In addi\u00adtion, there are signi.cant differences in the implementation. The original \nBOP creates a process for each PPR and uses only OS page-protection for access monitoring. It was im\u00adplemented \nfor only the 32-bit address space. The current version has process reuse and byte-granularity interface \nfor access monitoring and supports the 64-bit address space. Next we evaluate the overhead of page protection \nin two tests. They are short programs. We compare the automatic page-granularity monitoring with manually \ninserted byte\u00adgranularity monitoring. String substitution In this test, the average length of a PPR task \nis 0.016 second. The cost of page protection is sig\u00adni.cant. Figure 8 shows that byte-granularity monitoring \nis 30% to 50% faster than page-granularity monitoring. For easy viewing, the graph does not show 1% and \n10% con\u00ad.ict curves. Byte-granularity monitoring with 1% con.icts, shown previously in Figure 6, is about \n25% faster than page\u00adgranularity monitoring with no con.ict. K-means In this test, the average length \nof a PPR task is 0.55 second. The cost of page protection is negligible. We have tested a version, skim \nBOP, which does not protect data and does not use dependence hints. It is incorrect paralleliza\u00adtion. \nWe use it for performance evaluation since skim BOP is free of most of the speculation overheads. Because \nof coarse granularity, we found that the overheads from data monitor\u00ading, commit, and post-wait costed \nno more than 2% of the overall performance.  5.5 Comparison with OpenMP K-means We test two OpenMP versions. \nOpenMP ordered uses an ordered region to perform the reduction on the cen\u00adtroid data. OpenMP atomic uses \na critical section instead. Figure 9 shows their difference, 1% to 4% at p = 10 15 ex\u00adcept -2% at p = \n12 and 11% at p = 14. There is a slight performance bene.t from out of order access to shared data. For \nBOP, however, its large arrays and large amounts of data writes make it challenging for copy-on-write \nto obtain good performance, especially in comparison with OpenMP which modi.es data in place. In Figure \n9, safe paralleliza\u00adtion by BOP and unsafe parallelization by OpenMP ordered String substitution0% - \n50% conflicts num. processors p Figure 8: String substitution (without the ordering hint) with and \nwithout the page protection overhead. K-means num. processors p Figure 9: Comparison of BOP with OpenMP \nusing ordered and critical sections. have less than 3% difference when running with 5 or fewer processors. \nThen OpenMP is faster on average by 7% for p = 7 12. BOP becomes faster by 1% to 2% for p = 13 15. The \ntwo have the same peak speedup of 4.7X. Art and Hmmer Figure 10 shows that BOP has a similar performance \nas OpenMP. In art, as the number of tasks is increased from 1 to 15, the performance of both the BOP \nand the OpenMP versions increases almost identically to a factor of over 7. The execution time is reduced \nfrom 1,211 seconds level speculation (TLS), a dependence can be speci.ed us\u00adto 164 seconds by BOP and \n172 seconds by OpenMP. ing signal-wait [35]. Like Cytron s post-wait [10], signal and wait are paired \nby an identi.er, which is usually a data ad\u00address. Another construct is .ow in an ordered transaction. \nIt Hmmer and Art speci.es that a variable read should wait until a new value is produced by the previous \ntransaction [32]. In these systems, speedup 0 5 1015  x + BOP hmmer max x o +xoo OpenMP hmmerBOP art \n+ x + BOP hmmer min o o +x memory is shared, so a construct can implicitly synchronize +xo#  interface \nfor software multi-threaded transactions. It pro\u00ad +x- o# 0 5 1015 dependences on other data as well. \nThe correctness in guar\u00ad anteed by the user or special hardware support. - + xo The .ow construct is \na data trigger and useful when a +x o # OpenMP art + xo read needs to wait for an unidenti.ed write \nin the predeces\u00ad +x o - - # +xo - -# # -# -  # - # # sor task [32]. A problem may arise if \nthe previous task does  - +x o  -# - #  # not write or writes multiple times. As shown in Section \n3.4, +xo- #+xo - # - BOP provides a more programmable solution. SMTX is an - +x# o vides primitives \nto accessed versioned data [23]. The read and write accesses specify a variable name and are matched \nby the name and version number. The channel identi.er in BOP can serve the purpose of a version number. \nBOP chan\u00ad 2 4 6 8 101214 num. processors p Figure 10: BOP reduces hmmer time from 93 seconds to 6.7 \n nels use one-sided addressing (to allow dynamic changes to the channel content) and can communicate \naggregate and dynamically-allocated data, which would require additional seconds and art time from 1,211 \nseconds to 164 seconds. annotations if using .ow or versioned access. Both improvements are similar to \nOpenMP. The .gure shows three versions of hmmer: coarse-grained The Galois system lets a user express \nparallelism at an ab\u00adstract level of optimistic iterators instead of the level of reads and writes [22]. \nDependence hints also address the problem BOP with dependence hints, .ne-grained BOP without de\u00adin complex \ncode where access tracking is dif.cult to specify, pendence hints, and .ne-grained OpenMP without the \ncriti\u00adbut with a different solution which is to mark larger units of cal section (moved out of the compute \nloop). All three ver\u00addata and to combine with speculation. Optimistic iterators sions show almost identical \nlinear speedups. The running in Galois can specify semantic commutativity and inverse time is reduced \nfrom 93 seconds to 6.7 seconds by both BOP methods, which permits speculative parallelism beyond the \nversions and 6.3 seconds by OpenMP. QT-clustering The OpenMP version parallelizes within each time step, \nusing a critical section to combine results and an implicit barrier to separate time steps. When there \nis suf.cient parallelism, 400 PPRs, in each time step, both BOP and OpenMP obtain a highly scalable performance. \nThe speedup by BOP is 2.0, 3.8, 7.0, and 14 times for 2, 4, 8, and 15 parallel tasks. The speedup by \nOpenMP is 2.0, 4.0, 7.9, and 15.6. When there is an insuf.cient number of PPR tasks limit of dependence \nhints. However, these extensions are not hints and must be used correctly. An increasing number of software \nsystems use copy-on\u00ad write data replication to implement speculation [12, 13, 23, 30] or race-free threaded \nexecution [7, 8, 31]. Tasks do not physically share written memory and must exchange depen\u00ad dent data \nexplicitly. A solution, transactional communicator, addresses the problem in transactional memory [19]. \nBOP hints provide a solution for parallelization and use channels supported by sender-side addressing, \nselective dependence marking, and channel chaining to reduce the programming effort and implementation \ncost. Dependence in dynamic parallelization Instead of enu\u00ad merating dependences, the Jade language uses \ndata speci\u00ad .cation to derive dependence automatically [25]. Jade iden\u00adin time steps, time skewing can \nhelp. We set the inner loop to have 15 iterations and run it on a machine with 8 processors. In OpenMP, \nthe 15 iterations are divided into 3 groups when p =5, 6, 7. As a result, OpenMP has less than 5% improve\u00adment \nfrom p =5 to p =7. With time skewing, however, different time steps may overlap, so time skewing is 7% \nand 18% faster when using 6 and 7 parallel tasks than OpenMP. 6. Related Work ti.es all dependences \nwithout having to specify any of them. The dependence hint provide a different solution through partial \ndependence speci.cation. Being speculative, depen- Dependence in speculative parallelization Software \nspec\u00adulative loop parallelization was pioneered by Rawchwerger and Padua in the LRPD test [24]. Java \nsafe future and BOP PPR provided an interface for expressing possible paral\u00adlelism but not dependence \n[12, 33]. In hardware thread\u00addence hints enforce all dependences without having to spec\u00adify all of them. \nIn comparison, Jade speci.cations are not hints and may lead to program error if used incorrectly. The \ntwo approaches are fundamentally different. Jade is aimed for automatically optimized parallelization. \nDepen\u00addence hints focus on parallelization with incomplete pro\u00adgram knowledge but with direct control. \n In many programs especially irregular scienti.c code, de\u00adpendences can be analyzed through the inspector-executor \napproach. Recent advances include leveraging the OpenMP interface [4] and utilizing powerful static tools \nsuch as the use of uninterpreted function symbols in an integer\u00adset solver [28]. The combination of static \ntechniques such pointer and inter-procedural analysis with run-time depen\u00addence counting has also enabled \ndynamic parallelization of Java programs by OoOJava [16]. Fork-join parallel languages Fork-join primitives, \nin\u00adcluding spawn/sync in Cilk [14], future/get in Java, and async/.nish in X10 [9], come in pairs: one \nfor fork and one for join. PPR has fork but no explicit join. bop ppr uses speculation to abandon a task \nif it does not .nish in time, thus providing a safe join. It is useful when the join point of a task \nis unknown, unpredictable, or too complex to specify. In fact, not relying on user is a requirement for \nfull safety. One use of the dependence hint is in suggesting a task join. Parallel languages provide \nprimitives for synchronization such as critical (atomic) section or transactions. New lan\u00adguages such \nas Fortress have primitives for reduction [1]. These primitives maximize parallelism in dependent opera\u00adtions \nand may be necessary for highly scalable performance. BOP cannot support these constructs because it \ncannot auto\u00admatically guarantee sequential semantics for them. The or\u00addering hint loses parallelism, \nbut BOP recoups the loss by speculating on the later PPRs while waiting. Ordered opera\u00adtions are often \nuseful. For example in bzip2, it ensures that the compressed data is generated in order, which cannot \nbe done by a critical section or a transaction. 7. Summary Copy-on-write data replication is increasingly \nused in safe program parallelization to isolate parallel tasks and eliminate their false dependences. \nIn this paper, we have presented the dependence hint, an interface for a user to express partial parallelism \nso copy-on-write tasks can speculatively com\u00admunicate and synchronize with each other. We have presented \nsingle-post channel, sender-side ad\u00addressing and selective dependence marking to simplify pro\u00adgramming; \nchannel chaining to express conditional depen\u00addences; the three correctness checks for safe implementa\u00adtion; \nand a set of techniques including .ltered posting, silent drop, reordered receive, inter-PPR and distant-PPR \nhint over\u00adrides to enhance parallelism and reduce communication. We show that dependence hints can be \nused to build high-level constructs such as ordering and pipelining hints and make them safe and safely \ncomposable. We have shown example uses of dependence hints and demonstrated their expressiveness and \nsafety. In evaluation, we found that despite of the cost of data copying, access monitoring and task \nserialization, the safe parallelization by BOP achieves on average 9.1 times performance improve\u00adment \nfor 8 programs on today s multicore computers. Acknowledgments The idea of process reuse was originated \nin a parallelization system developed by Long Chen and others at ICT. Long Chen also provided the initial \ncode for several of the SPEC benchmark tests. The initial implementation of BOP-malloc was assisted by \nJingliang Zhang. Yang Chen helped result processing. Xiaoming Gu helped with the implementation of an \nearlier version of the system. Finally, we wish to thank Michael Scott, Xipeng Shen, Kai Shen, Peng Wu, \nChristopher Hill, Ron Rostan, Mark Hapner, and others for the helpful discussions. The research is supported \nby NSF (Contract No. CCF-1116104, CCF-0963759, CNS-0834566, CNS-0720796), and IBM CAS Faculty Fellowships. \nReferences [1] E. Allen, D. Chase, C. Flood, V. Luchangco, J. Maessen, S. Ryu, and G. L. Steele. Project \nfortress: a multicore lan\u00adguage for multicore processors. Linux Magazine, pages 38 43, September 2007. \n[2] R. Allen and K. Kennedy. Optimizing Compilers for Modern Architectures: A Dependence-based Approach. \nMorgan Kauf\u00admann Publishers, Oct. 2001. [3] A. Aviram, S.-C. Weng, S. Hu, and B. Ford. Ef.cient system\u00adenforced \ndeterministic parallelism. In Proceedings of the Sym\u00adposium on Operating Systems Design and Implementation, \n2010. [4] A. Basumallik and R. Eigenmann. Optimizing irregular shared-memory applications for distributed-memory \nsystems. In Proceedings of the ACM SIGPLAN Symposium on Princi\u00adples and Practice of Parallel Programming, \npages 119 128, 2006. [5] M. A. Bender, J. T. Fineman, S. Gilbert, and C. E. Leiser\u00adson. On-the-.y maintenance \nof series-parallel relationships in fork-join multithreaded programs. In Proceedings of the ACM Symposium \non Parallel Algorithms and Architectures, pages 133 144, Barcelona, Spain, 2004. [6] T. Bergan, O. Anderson, \nJ. Devietti, L. Ceze, and D. Gross\u00adman. CoreDet: a compiler and runtime system for determin\u00adistic multithreaded \nexecution. In Proceedings of the Interna\u00adtional Conference on Architectural Support for Programming Languages \nand Operating Systems, pages 53 64, 2010. [7] E. D. Berger, T. Yang, T. Liu, and G. Novark. Grace: Safe \nmultithreaded programming for C/C++. In Proceedings of the International Conference on Object Oriented \nProgramming, Systems, Languages and Applications, 2009. [8] S. Burckhardt, A. Baldassin, and D. Leijen. \nConcurrent pro\u00adgramming with revisions and isolation types. In Proceedings of the International Conference \non Object Oriented Program\u00adming, Systems, Languages and Applications, pages 691 707, 2010. [9] P. Charles, \nC. Grothoff, V. Saraswat, C. Donawa, A. Kielstra, K. Ebcioglu, C. von Praun, and V. Sarkar. X10: an object\u00adoriented \napproach to non-uniform cluster computing. In Pro\u00ad  ceedings of the International Conference on Object \nOriented Programming, Systems, Languages and Applications, pages 519 538, 2005. [10] R. Cytron. Doacross: \nBeyond vectorization for multiproces\u00adsors. In Proceedings of the 1986 International Conference on Parallel \nProcessing, pages 836 844, St. Charles, IL, Aug. 1986. [11] C. Ding. Access annotation for safe speculative \nparalleliza\u00adtion: Semantics and support. Technical Report URCS #966, Department of Computer Science, \nUniversity of Rochester, March 2011. [12] C. Ding, X. Shen, K. Kelsey, C. Tice, R. Huang, and C. Zhang. \nSoftware behavior oriented parallelization. In Proceedings of the ACM SIGPLAN Conference on Programming \nLanguage Design and Implementation, pages 223 234, 2007. [13] M. Feng, R. Gupta, and Y. Hu. SpiceC: scalable \nparallelism via implicit copying and explicit commit. In Proceedings of the ACM SIGPLAN Symposium on \nPrinciples and Practice of Parallel Programming, pages 69 80, 2011. [14] M. Frigo, C. E. Leiserson, and \nK. H. Randall. The implemen\u00adtation of the cilk-5 multithreaded language. In Proceedings of the ACM SIGPLAN \nConference on Programming Language Design and Implementation, pages 212 223, 1998. [15] L. Heyer, S. \nKruglyak, and S. Yooseph. Exploring expres\u00adsion data: Identi.cation and analysis of coexpressed genes. \nGenome Research, 9:1106 1115, 1999. [16] J. C. Jenista, Y. H. Eom, and B. Demsky. OoOJava: Software out-of-order \nexecution. In Proceedings of the ACM SIGPLAN Symposium on Principles and Practice of Parallel Program\u00adming, \npages 57 68, 2011. [17] Y. Jiang and X. Shen. Adaptive software speculation for en\u00adhancing the cost-ef.ciency \nof behavior-oriented paralleliza\u00adtion. In Proceedings of the International Conference on Par\u00adallel Processing, \npages 270 278, 2008. [18] L. Liu and Z. Li. Improving parallelism and locality with asynchronous algorithms. \nIn Proceedings of the ACM SIG-PLAN Symposium on Principles and Practice of Parallel Pro\u00adgramming, pages \n213 222, 2010. [19] V. Luchangco and V. J. Marathe. Transaction communicators: enabling cooperation among \nconcurrent transactions. In Pro\u00adceedings of the ACM SIGPLAN Symposium on Principles and Practice of Parallel \nProgramming, pages 169 178, 2011. [20] J. M. Mellor-Crummey. On-the-.y detection of data races for programs \nwith nested fork-join parallelism. In Proceedings of Supercomputing, pages 24 33, 1991. [21] OpenMP application \nprogram interface, version 3.0, May 2008. http://www.openmp.org/mp-documents/spec30.pdf. [22] K. Pingali, \nD. Nguyen, M. Kulkarni, M. Burtscher, M. A. Hassaan, R. Kaleem, T.-H. Lee, A. Lenharth, R. Manevich, \nM. M\u00b4endez-Lojo, D. Prountzos, and X. Sui. The tao of paral\u00adlelism in algorithms. In Proceedings of the \nACM SIGPLAN Conference on Programming Language Design and Imple\u00admentation, pages 12 25, 2011. [23] A. \nRaman, H. Kim, T. R. Mason, T. B. Jablin, and D. I. August. Speculative parallelization using software \nmulti\u00adthreaded transactions. In Proceedings of the International Conference on Architectural Support \nfor Programming Lan\u00adguages and Operating Systems, pages 65 76, 2010. [24] L. Rauchwerger and D. Padua. \nThe LRPD test: Speculative run-time parallelization of loops with privatization and reduc\u00adtion parallelization. \nIn Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Imple\u00admentation, La Jolla, \nCA, June 1995. [25] M. C. Rinard and M. S. Lam. The design, implementation, and evaluation of Jade. ACM \nTransactions on Programming Languages and Systems, 20(3):483 545, 1998. [26] J. A. Roback and G. R. Andrews. \nGossamer: A lightweight approach to using multicore machines. In Proceedings of the International Conference \non Parallel Processing, pages 30 39, Washington, DC, USA, 2010. IEEE Computer Society. [27] Y. Song and \nZ. Li. New tiling techniques to improve cache temporal locality. In Proceedings of the ACM SIGPLAN Con\u00adference \non Programming Language Design and Implementa\u00adtion, pages 215 228, Atlanta, Georgia, May 1999. [28] M. \nM. Strout, L. Carter, and J. Ferrante. Compile-time com\u00adposition of run-time data and iteration reorderings. \nIn Pro\u00adceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation, pages \n245 257, San Diego, CA, June 2003. [29] W. Thies, V. Chandrasekhar, and S. P. Amarasinghe. A practi\u00adcal \napproach to exploiting coarse-grained pipeline parallelism in c programs. In Proceedings of the ACM/IEEE \nInternational Symposium on Microarchitecture, pages 356 369, 2007. [30] C. Tian, M. Feng, V. Nagarajan, \nand R. Gupta. Copy or Discard execution model for speculative parallelization on multicores. In Proceedings \nof the ACM/IEEE International Symposium on Microarchitecture, pages 330 341, 2008. [31] K. Veeraraghavan, \nD. Lee, B. Wester, J. Ouyang, P. M. Chen, J. Flinn, and S. Narayanasamy. DoublePlay: parallelizing se\u00adquential \nlogging and replay. In Proceedings of the Interna\u00adtional Conference on Architectural Support for Programming \nLanguages and Operating Systems, pages 15 26, 2011. [32] C. von Praun, L. Ceze, and C. Cascaval. Implicit \nparallelism with ordered transactions. In Proceedings of the ACM SIG-PLAN Symposium on Principles and \nPractice of Parallel Pro\u00adgramming, Mar. 2007. [33] A. Welc, S. Jagannathan, and A. L. Hosking. Safe futures \nfor Java. In Proceedings of the International Conference on Object Oriented Programming, Systems, Languages \nand Applications, pages 439 453, 2005. [34] D. Wonnacott. Achieving scalable locality with time skewing. \nInternational Journal of Parallel Programming, 30(3), June 2002. [35] A. Zhai, J. G. Steffan, C. B. Colohan, \nand T. C. Mowry. Com\u00adpiler and hardware support for reducing the synchronization of speculative threads. \nACM Transactions on Architecture and Code Optimization, 5(1):1 33, 2008. [36] C. Zhang, C. Ding, X. Gu, \nK. Kelsey, T. Bai, and X. F. 0002. Continuous speculative program parallelization in software. In Proceedings \nof the ACM SIGPLAN Symposium on Princi\u00adples and Practice of Parallel Programming, pages 335 336, 2010. \nposter paper.   \n\t\t\t", "proc_id": "2048066", "abstract": "<p>Speculative parallelization divides a sequential program into possibly parallel tasks and permits these tasks to run in parallel if and only if they show no dependences with each other. The parallelization is safe in that a speculative execution always produces the same output as the sequential execution.</p> <p>In this paper, we present the dependence hint, an interface for a user to specify possible dependences between possibly parallel tasks. Dependence hints may be incorrect or incomplete but they do not change the program output. The interface extends Cytron's do-across and recent OpenMP ordering primitives and makes them safe and safely composable. We use it to express conditional and partial parallelism and to parallelize large-size legacy code. The prototype system is implemented as a software library. It is used to improve performance by nearly 10 times on average on current multicore machines for 8 programs including 5 SPEC benchmarks.</p>", "authors": [{"name": "Chuanle Ke", "author_profile_id": "81490659116", "affiliation": "Institute of Computing Technology, Chinese Academy of Sciences, Beijing, NY, USA", "person_id": "P2839170", "email_address": "kechuanle@ict.ac.cn", "orcid_id": ""}, {"name": "Lei Liu", "author_profile_id": "81548023856", "affiliation": "Institute of Computing Technology, Chinese Academy of Sciences, Beijing, NY, USA", "person_id": "P2839171", "email_address": "liulei@ict.ac.cn", "orcid_id": ""}, {"name": "Chao Zhang", "author_profile_id": "81490651671", "affiliation": "Intel Labs, China, Beijing, NY, USA", "person_id": "P2839172", "email_address": "zhangchaospecial@gmail.com", "orcid_id": ""}, {"name": "Tongxin Bai", "author_profile_id": "81372591012", "affiliation": "University of Rochester, Rochester, NY, USA", "person_id": "P2839173", "email_address": "bai@cs.rochester.edu", "orcid_id": ""}, {"name": "Bryan Jacobs", "author_profile_id": "81490660993", "affiliation": "University of Rochester, Rochester, NY, USA", "person_id": "P2839174", "email_address": "jacobs@cs.rochester.edu", "orcid_id": ""}, {"name": "Chen Ding", "author_profile_id": "81309499457", "affiliation": "University of Rochester, Rochester, NY, USA", "person_id": "P2839175", "email_address": "cding@cs.rochester.edu", "orcid_id": ""}], "doi_number": "10.1145/2048066.2048087", "year": "2011", "article_id": "2048087", "conference": "OOPSLA", "title": "Safe parallel programming using dynamic dependence hints", "url": "http://dl.acm.org/citation.cfm?id=2048087"}