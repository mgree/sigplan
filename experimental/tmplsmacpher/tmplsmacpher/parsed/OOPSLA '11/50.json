{"article_publication_date": "10-22-2011", "fulltext": "\n Composable, Nestable, Pessimistic Atomic Statements Zachary Anderson David Gay ETH Z\u00a8urich Intel Labs \nBerkeley zachary.anderson@inf.ethz.ch dgay@acm.org Abstract In this paper we introduce a new method \nfor pessimistically implementing composable, nestable atomic statements. Our mechanism, called shelters, \nis inspired by the synchroniza\u00adtion strategy used in the Jade programming language. Unlike previous lock-based \npessimistic approaches, our mechanism does not require a whole-program analysis that computes a global \nlock order. Further, this mechanism frees us to imple\u00adment several optimizations, impossible with automatically \ninserted locks, that are necessary for scaling on recent multi\u00adcore systems. Additionally we show how \nour basic mech\u00adanism can be extended to support both open-and closed\u00adnesting of atomic statements, something \nthat, to our knowl\u00adedge, has not yet been implemented fully-pessimistically in this context. Unlike optimistic, \ntransactional-memory-based approaches, programmers using our mechanism do not have to write compensating \nactions for open-nesting, or worry about the possibly awkward semantics and performance im\u00adpact of aborted \ntransactions. Similar to systems using locks, our implementation re\u00adquires programmers to annotate the \ntypes of objects with the shelters that protect them, and indicate the sections of code to be executed \natomically with atomic statements. A static analysis then determines from which shelters protection is \nneeded for the atomic statements to run atomically. We have implemented shelter-based atomic statements \nfor C, and ap\u00adplied our implementation to 12 benchmarks totaling over 200k lines of code including the \nSTAMP benchmark suite, and the sqlite database system. Our implementation s perfor\u00admance is competitive \nwith explicit locking, Autolocker, and a mature software transactional memory implementation. Categories \nand Subject Descriptors D.1.3 [Programming Techniques]: Concurrent Programming; D.3.3 [Program\u00adming Languages]: \nLanguage Constructs and Features Permission to make digital or hard copies of all or part of this work \nfor personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior \nspeci.c permission and/or a fee. OOPSLA 11, October 22 27, 2011, Portland, Oregon, USA. Copyright &#38;#169;c2011 \nACM 978-1-4503-0940-0/11/10. . . $10.00 General Terms Languages Keywords Atomic statements, Locks, Concurrency \n1. Introduction Given recent advances in hardware, writing multithreaded programs that manipulate shared \nstate is an increasingly important task. However, it is also very challenging, even for experienced programmers. \nThough explicit locking can yield highly e.cient code, its use is prone to errors such as data-races \nand deadlocks. Indeed, as of June 2011, according to their bugzilla databases, there were 38 known, outstanding \nrace conditions in the Linux kernel, and 136 in Firefox; and there were 39 known, outstanding deadlocks \nin the Linux kernel, and 107 in Firefox [22, 27]. Atomic statements are a convenient language construct \nfor controlling access to shared state in multithreaded pro\u00adgrams. They ensure that the statements within \nthem execute atomically. That is, the e.ects of operations in atomic state\u00adments become visible to other \nthreads only all at once when execution leaves the atomic statement, much like a database transaction. \nBecause they simply declare what to run atom\u00adically, rather than fully specifying how to ensure atomicity, \nuser studies have shown that the use of atomic statements is less error prone than explicit locking [34]. \nIn particular, atomic statements make it much easier to compose indepen\u00addently written code as there \nis no confusion created by hav\u00ading di.erent locking disciplines in di.erent modules. In this paper we \nintroduce an implementation of atomic statements suitable for use in systems-level C code. Our mechanism, \ncalled shelters, is inspired by the synchroniza\u00adtion strategy used in the Jade programming language [33]. \nOur approach is pessimistic, able to support multiple seman\u00adtics for nested atomic statements, allows \natomic statements to coexist with explicit locks, and does not require whole\u00adprogram analysis. These \nbene.ts come at the cost of only a small number of simple annotations on types and functions. Further, \nour implementation achieves performance competi\u00adtive with explicit locking on benchmarks and a few applica\u00adtions, \nincluding the sqlite database system. Existing optimistic and pessimistic systems have signif\u00adicant drawbacks. \nSTM systems execute atomic statements optimistically, at least in part. Atomic sections are allowed to \nexecute concurrently, but when two or more threads make con.icting accesses, transactions must be aborted, \nrolled\u00adback, and retried. Some TM implementations achieve good performance. However, if transactions \nare large, or if data is highly contended, con.icts may be frequent and expen\u00adsive. Furthermore, roll-back \nmay not be possible if, for ex\u00adample, a thread performed I/O during a failed transaction. STM systems \nsupport multiple semantics for nested trans\u00adactions: closed-nesting, in which the e.ects of nested trans\u00adactions \nbecome visible only at the end of the outer transac\u00adtion; and open-nesting, in which the e.ects of nested \ntrans\u00adactions become visible at their respective ends. However, in order to use open-nesting, a programmer \nmay have to write compensating actions, and reason directly about the seman\u00adtics of roll-back [26]. We \nview this requirement as overly\u00ad burdensome.  These considerations lead us to prefer a pessimistic ap\u00adproach. \nHowever, previous pessimistic approaches have re\u00adlied on whole-program analysis for determining a global \nlock order [25], or for inferring .ne-grained lock hierar\u00ad chies [13, 19]. But whole-program analysis \nis often prob\u00ad lematic in practice. First, it is often expensive for large pro\u00adgrams. Second, the source \ncode for the whole program may not always be available. Furthermore, previous pessimistic, lock-based \napproaches have only supported closed-nesting semantics. Our approach avoids these problems, thereby \nmaking it suitable for systems-level C programs. In Sections 2 and 3 we give a detailed description of \nour design, which we for\u00admalize and prove correct in Section 4. Further, we have im\u00ad plemented compiler \ntransformations and a runtime for the C language, described in Section 5, and applied our im\u00ad plementation \nto 12 programs including the STAMP bench\u00admark suite [11], and a few representative applications, in\u00ad \ncluding the sqlite database system, totaling over 200k lines of code. In Section 6, we present a thorough \ncomparison of the runtime and programming cost of shelter-based atomic statements with the Intel STM \nimplementation [21] and Au\u00ad tolocker [25] for the above mentioned benchmarks. In summary, we make the \nfollowing contributions: We present the design and implementation of shelters,a pessimistic method for \nimplementing atomic statements, including straightforward semantics for both open-and closed-nesting, \nand interaction with explicit locking, that requires no whole-program analysis.  We formalize our design \nto show that shelters enforce useful concurrency guarantees and are deadlock-free.  We present the results \nor our experiments comparing the runtime performance and annotation overheads of shelter-based atomic \nstatements with explicit locking, Autolocker, and an STM system. In summary, both our annotation burden \nand our performance burden are com\u00adpetitive with other systems. We also show that acquiring all locks \nupon entering an atomic statement, an obvious  alternative to our system, has worse performance than \nshelters. 2. Overview In this section we give an overview of what shelters are, how they work, and our \nextensions to C. Then in Section 3, using a few examples, we explain in detail the features of our system. \n 2.1 Extensions to C We add to the C language a few basic constructs: atomic statements, shelters, a \ntype annotation, and a function anno\u00adtation. Atomic statements in our extension have the usual seman\u00adtics: \nthe e.ects of a closed atomic statement on shared ob\u00adjects, including those in any nested atomic statements, \ndo not become visible to other threads until the atomic state\u00adment is .nished. We also provide open atomic \nstatements, in which the e.ects of any nested atomic statements on shared objects become visible at their \nrespective ends. In our system, the programmer identi.es what constitutes a single shared object through \na combination of shelters and type an\u00adnotations. Shelters, like locks, are .rst-class objects, and can \nbe de\u00adclared as variables, structure .elds, and function arguments. Further, as in Autolocker [25], the \nprogrammer speci.es the shelter that protects a location by writing a type anno\u00adtation. Thus, what constitutes \nan object for concurrency\u00adcontrol purposes are those locations in memory whose C types, at the static \nsource-code level, are annotated with a sheltered by(s) quali.er using the same shelter s. Shel\u00adters, \nthen, are simply a mechanism for storing all of the concurrency-control metadata in one place for a collection \nof locations that may not be accessed concurrently. From here on, we simply refer to such a collection \nof locations as a single object identi.ed by a single shelter. Since our approach is fully pessimistic, \nat the beginning of an atomic statement, we must know what objects may be touched within it. We discover \nthese objects by performing a backwards data.ow analysis over atomic statements. For the implementation \nof open atomic statements, the analysis also notes in which nested atomic statement an object is accessed. \nOur compiler transforms a program such that at the beginning of an atomic statement, the collected shelters \nare passed to functions of our runtime, which are described below. Unlike previous pessimistic approaches, \none of our de\u00adsign gaols is to avoid whole-program analysis. Since we must know what objects may be accessed \nin an atomic state\u00adment we require the programmer to write function annota\u00adtions describing the objects \na function will access. These take the form of needs shelter() annotations on func\u00adtion types. Our implementation \nperforms a per-module call\u00adgraph analysis so that programmers must only annotate the module boundaries \nand function pointers of those functions that may be called with in an atomic statement. Further, at \ncompile time, we provide warnings and errors as appropri\u00adate when needs shelters() annotations are missing. \nWe describe the conditions under which warnings and errors are emitted in Section 3.  The programmer \nis responsible for placing the shel\u00adtered by(s) annotation on shared objects protected by shel\u00adter s. \nThe programmer is also responsible for using the atomic statement with the correct semantics for his \nsituation. Though our implementation provides errors if annotated ob\u00adjects are accessed outside of atomic \nstatements, if type anno\u00adtations are missing, then data-races, but not deadlocks, may result. Through \na straightforward combination with our ear\u00adlier sharing-checker work [5], we can provide compile-time \nand run-time errors for these missing-annotation mistakes. Next, we give a brief description of how the \nsemantics of our atomic statements are implemented by our runtime.  2.2 Runtime Semantics Shelters are \ninspired by Jade in the sense that both are based on timestamp-based concurrency control as devel\u00adoped \nin the database community [8]. In databases with ba\u00adsic timestamp-based concurrency control, each transaction \nreceives a timestamp when it begins. Each object in the database also has an associated timestamp that \nrecords the timestamp of the transaction that most recently accessed it. The timestamps are needed to \nimplement optimistic concur\u00adrency control the decision to abort a transaction (and main\u00adtain consistency) \nis made by comparing transaction times\u00adtamps with the object timestamps. If a transaction would read \nan object that was written by a newer transaction, or overwrite a value read or written by a newer transaction, \nthen it must be aborted, rolled-back, and restarted. This approach is akin to how transactional memory \nsystems operate. The approach we use with shelters and the approach used in Jade is more similar to strict \ntimestamp-based con\u00adcurrency control, in which transactions that began later must block before accessing \nobjects that may be accessed by transactions that began earlier, and must remain blocked un\u00adtil these \nother transactions .nish. As one might expect, be\u00adcause transactions may block, additional logic is needed \nin the transaction manager to avoid deadlock. So, in essence, shelters solve the problem of using strict \ntimestamp-based concurrency control for deadlock-free, systems-level pro\u00adgramming. In our system and \nJade, the timestamp takes the form of a distinct value obtained by a thread from a strictly increasing \nthread-safe counter at the beginning of each atomic state\u00adment. The implementation details of this counter \nare dis\u00adcussed in Section 5. At the beginning of an atomic statement, a thread atom\u00adically registers \nits timestamp with all of the shelters pro\u00adtecting the objects it will touch in the atomic statement, \nas found by the data.ow analysis described above. This regis\u00adtration operation is the function shelter \nregister() in our runtime (register() in our formalism). Atomically, in the reg\u00adister operation, a thread \ngets a timestamp and records its identi.er and timestamp in each of the shelters it is regis\u00adtering for. \nThen, to maintain atomicity, threads block before access\u00ading an object unless they have the smallest \ntimestamp among all threads recorded by the shelter that protects the object. This timestamp check is \nthe function shelter wait(s) in our runtime, and is captured by the transition semantics in our formalism. \nHere a thread examines all of the other threads registered for the shelter to see if it has the smallest \ntimestamp. Since threads in our system may block in an atomic state\u00adment, we must ensure that deadlock \nis not possible. In the presence of only closed atomic statements, this is straight\u00adforward. A timestamp \nis only acquired for the outermost atomic statement, and if two threads will touch the same ob\u00adjects \nin concurrently running atomic statements, the thread with the larger timestamp blocks right before accessing \nthe .rst object it would touch that is shared by both atomic state\u00adments, while the thread with the smaller \ntimestamp runs its atomic statement to completion. Thus, the apparent serial or\u00adder of the atomic statements \nis given by the timestamps of the threads that run them. In other words, a thread that be\u00adgins an atomic \nstatement earlier never sees the e.ects from an atomic statement that started later. In the presence \nof open atomic statements, the situation is more complicated. In particular, since threads must acquire \na timestamp for each nested atomic statement, it is possible that waits-for cycles will be created when \nthreads registered for the same shelters have intersecting ranges of timestamps. To avoid this situation, \nwe require threads to block in the register operation if the registration would create a cycle. This \ncycle avoidance relies on knowing for which shelters a thread will attempt to register in the future. \nTherefore, be\u00adfore registering for a shelter, a thread must declare its intent to do so, which we call \na reservation. This approach is simi\u00adlar to the Resource-Allocation-Graph deadlock-avoidance algorithm \nas described by Silberschatz, et al. [37]. The reser\u00ad vation is carried out by the shelter reserve() \nfunction of our runtime, (reserve() in our formalism). In the reserve op\u00aderation, a thread adds the indicated \nshelters to its reservation set. If a thread will never again register for a shelter within the scope \nof the current outermost atomic statement, then it no longer needs the reservation. The deletion of the \nreservation is performed by the shelter unreserve() function of our runtime. In the unreserve operation, \na thread removes the indicated shelters from its reservation set. This operation is also performed by \nthe reserve() operation in our formalism. The precise conditions under which a waits-for cycle may exist \nin our system are given in the formalism by the im\u00adpedes relation between two threads, described informally \nin Section 3 and formally in Section 4. Which threads impede each other depends on which shelters they \nare registered for, and which shelters they have reserved. We detect the poten\u00adtial for deadlock by looking \nfor cycles in a graph in which threads are nodes, and the directed edges are given by this relation. \n In open-nesting, a thread completes an atomic statement by unregistering only from the shelters for \nthe immediately enclosing atomic statement. In closed-nesting, a thread com\u00adpletes an atomic statement \nby unregistering from all shel\u00adters when exiting only the outermost atomic statement. This unregistration \nis carried out by the shelter register pop() function of our runtime (pop() in our formalism). It causes \nthe relevant shelters to forget about the calling thread, which allows previously blocked threads with \nlarger timestamps to continue. Now that we have given an overview of our system s ex\u00adtensions to C and \nthe runtime library for these extensions, we brie.y describe two features that are necessary for handling \nreal code. In our extensions to C, atomic statements are translated into calls into the runtime library \ndescribed brie.y above. This translation is guided by a static analysis that discov\u00aders what shelters \nmust be registered for at the start of each atomic statement. Any static analysis will be imprecise somewhere. \nWhere ours is imprecise, we automatically in\u00adtroduce a hierarchy of shelters, in which shelters higher \nin the hierarchy subsume lower-level shelters. Our hierarchy is type-based, and we use a two-level hierarchy \nin which shelters in the upper level subsume shelters that are .elds of structures. An example of this \nsituation is described in Section 3. Our compiler transformations detect where the shelter\u00adhierarchy \nmay be needed and add calls to set it up when the program begins, as well as to add new shelters when \nthey are created dynamically. All a programmer must do is declare the leaf shelters. If the static analysis \nis imprecise, a thread will register directly for one of these ancestor shelters. Then, in the shelter \nwait(s) function, a thread must wait not only when a thread with a smaller timestamp is registered for \ns, but also when a thread with a smaller timestamp is registered for a descendant or ancestor of s. Finally, \nshelters may coexist with explicit locking. In or\u00adder to avoid deadlock, our runtime must be informed \nof the existence of any explicit locks that may be used within atomic statements, in addition to their \nruntime state. To achieve this, we introduce shadow shelters, which capture the state of explicit locks \nfor use in our deadlock avoidance algorithm. 3. Examples In this section we explain each of the features \ndescribed in Section 2 using examples. In the course of explaining these 1 typedef struct { 2 int sheltered \nby(s) id; 3 float sheltered by(s) balance; 4 shelter t s; 5 } account t; 6 7 needs shelters(a->s) 8 \nvoid deposit(account t *a, float d) { 9 a->balance += d; // shelter wait(a->s); 10 } 11 12 needs shelters(a->s) \n13 void withdraw(account t *a, float w) { 14 a->balance -= w; // shelter wait(a->s); 15 } 16 17 needs \nshelters(to->s, from->s) 18 void transfer(account t *to, account t *from, 19 float amount) { 20 atomic \n{ 21 //shelter reserve((W,to->s), (W,from->s)) 22 //shelter register((W,to->s), (W,from->s)); 23 //shelter \nunreserve((W,to->s), (W,from->s)); 24 withdraw(from, amount); 25 deposit(to, amount); 26 } //shelter \nregister pop() 27 } Figure 1. Code using atomic statements for the atomic transfer of funds between \ntwo accounts. Statements inserted by the shelters compiled are in comments. examples, we also give more \ndetail about the operation of our system. 3.1 Example In this example we show how shelter-based atomic \nstate\u00adments avoid the pitfalls of explicit locking while achieving the convenience of atomic statements \nfor the example of the atomic transfer of funds between two bank accounts. Consider the type and function \ndeclarations in Figure 1. The account t structure type contains a balance .eld, and an id .eld. The structure \nalso contains a shelter t .eld s. In our design, shelters, like locks, are .rst class objects, and can \nbe declared as variables, structure .elds, and function arguments. The types of the balance and id .elds \nare anno\u00adtated with sheltered by(s). This indicates that concurrent access to these structure .elds is \nmediated by the shelter s in the same structure. In the transfer function an atomic block indicates that \nthe withdraw from the from account and the deposit to the to account happen atomically. That is, no other \nthread will see some intermediate inconsistent state. The beginning of the atomic statement in the transfer \nfunction is translated into the shelter register() call on Line 22. Here a thread registers for shelters \nto->s and from\u00ad>s, which are indicated by the results of the static analysis.  In this call, W indicates \nthat the shelters are needed for write access. The calls to reserve (and unreserve) shelters needed to \nsupport open nesting of atomic sections are discussed further in Section 3.3. Before accessing a sheltered \nobject, a thread must wait unless it is the thread registered for the shelter with the smallest timestamp. \nTherefore, where the program in our example accesses objects with types annotated with shel\u00adtered by(s), \ncalls to shelter wait(s) are added that cause the calling thread to wait until it is the thread with \nthe small\u00adest timestamp on s. These calls appear for the accesses to the balance .eld of account structures \non Lines 9 and 14. When a thread exits an atomic statement, it unregisters itself from the shelters associated \nwith that atomic statement. Accordingly, we add a call to shelter register pop() on Line 26, which unregisters \nthe calling thread from shelters to->s and from->s. The deposit and withdraw functions adjust the balance \nof an account. They must be annotated as needs shelter(a\u00ad>s) because they access .elds of a that have \nbeen annotated as sheltered by(s). Additionally, the transfer function must be annotated with needs shelters(to->s,from->s) \nif it is ever going to be called from within an atomic statement because it calls functions that access \ndata protected by those shelters. Whether an error or a warning is emitted for missing needs shelters() \nannotations depends on where a function is de.ned and called. If a function may be called within an atomic \nstatement, lacks the correct annotation, is called in the same module it is de.ned in, and our system \ncannot deduce the annotation automatically (for example, when a shelter is the result of a function call), \nthen an error is emit\u00adted. However if the called function is de.ned in a di.erent module, then we infer \nthat no shelters are required by the function, but emit a warning asking for an annotation in the header \n.le. We found this arrangement to be the most expe\u00addient in converting our benchmarks because very few \nlibrary calls touch sheltered objects. Now, consider that one thread calls transfer(A, B, 0.10), and \na second thread calls transfer(B, A, 0.10) at the same time. If the example in Figure 1 were written \nwith explicit locking, care would have to be taken when implementing or calling the transfer function \nin order to avoid deadlock. With shelter-based atomic statements, how\u00adever, deadlock is avoided automatically \nbecause the two threads will have distinct timestamps, one smaller than the other. If two threads are \nregistered for the same shelter s, and both arrive at a shelter wait(s) call, the thread with the smaller \ntimestamp proceeds while the thread with the larger timestamp must wait until the .rst thread calls shel\u00adter \nregister pop(). In the examples that follow, we show how this basic mechanism is extended to support \nboth open-and closed\u00adnesting of atomic statements, something that, to our knowl\u00adedge, has not yet been \nimplemented fully-pessimistically. Fi\u00adnally, we show how our mechanism can coexist with soft\u00adware using \nother synchronization mechanisms such as .le locking. First, though, we explain how our design uses the \nshelter hierarchy to cope with imprecision in our static anal\u00adysis.  3.2 Shelter Hierarchy Consider \nthe following alternate version of the transfer function from the example: void idTransfer(int toId, \nint fromId, float a) { atomic { account t *to = accountLookup(toId); account t *from = accountLookup(fromId); \nwithdraw(from, a); deposit(to, a); }} In this example the function accountLookup takes the ac\u00adcount ID, \nlooks up the account t structure in some data structure implementing a map, and returns a pointer to \nit. It might be preferable to write the function like this in case, for example, accounts may be deleted \nfrom the system. De\u00adpending on how the map data structure is implemented, it may not be possible for \na static analysis to determine exactly what shelters are needed at the beginning of the atomic state\u00adment. \nIn this situation, a thread registers for a coarser grained shelter protecting all account t structures, \nwhich subsumes the shelters in the individual account t structures. We call these coarser-grained shelters \ntype-shelters, and will refer to particular type-shelters as T.s, where T is the structure type name, \nand s is the shelter .eld, for example account t.s. In our implementation they are only needed to subsume \nthe shelters that are .elds of structure types.1 Our system transforms programs such that calls to shel\u00adter \nwait() are always made on leaf shelters. A thread must then examine each ancestor of this shelter to \ndetermine whether or not it must wait. For example, suppose thread T1 is registered for a shelter protecting \na particular account t structure, a->s, and has timestamp 3. Further suppose that thread T2 is in the \natomic statement in the alternate trans\u00adfer implementation, and is registered directly for the ances\u00adtor \nof a->s, account t.s, with timestamp 2. Even if T1 is the thread with the smallest timestamp registered \nfor shelter a->s, it must wait because T2 is registered with a smaller timestamp for account t.s, an \nancestor of a->s. On the other hand, if T2 had only been registered for the shelter for another account \nt structure, say b->s, then both threads would be able to proceed. 1 The precision of the static analysis \nwill a.ect the granularity of the hier\u00adarchy. An analysis that goes beyond our type-based aliasing assumptions \nwill probably be .ner-grained than the two-level hierarchy described here, and may achieve better performance \nat the cost of a whole-program pointer analysis. Our runtime implementation is able to support .ner-grained \nhier\u00adarchies, but they are not generated by our static analysis.  3.3 Open Atomic Statements Now consider \na function that processes a list of transfers. The list of transfers is shared among threads, but a thread \nprocessing a list of transfers requires that the list does not change during processing. However, a thread \nonly requires exclusive access to a pair of accounts when executing a particular transfer. It is safe \nwith respect to the semantics of the program for other threads to access the other accounts while the \nrest of the list is being processed. open atomic { for (t = l->head; t; t = t->next) { atomic { withdraw(t->from, \na); deposit(t->to, a); }}} Open atomic statements have the same semantics as reg\u00adular atomic statements \nwith the exception that the e.ects occurring in any nested atomic statement becomes visible as soon as \nthe nested atomic statement .nishes. Here, the open atomic statement indicates that accesses to l->head \nwill not be interfered with during the execution of the en\u00adclosed statements. This would also be true \nif an atomic state\u00adment had been used. The open atomic statement di.ers be\u00adcause it allows the e.ects \nof the transfers performed within the nested atomic statements to become visible at their re\u00adspective \nends. Typically in implementations of open-nesting, it is the inner atomic statements that specify the \nsemantics. In our design it is possible to use either convention. However, we felt it would be clearer \nif, by default, outer atomic statements could constrain the semantics of atomic statements nested within \nthem. Consider that the inner atomic statement in the above example could be replaced by a call to the \ntransfer function. If, by default, inner atomic statements speci.ed the nesting semantics, then we would \nneed two versions of this function, one with an open atomic statement, and the other with a closed atomic \nstatement. When the default is for the outer atomic statement to specify the semantics, only one version \nis needed. On the other hand, in some situations, it is always safe for the e.ects of an atomic statement \nto become visible im\u00admediately, regardless of where it is nested, for example for a malloc call. To handle \nthese situations, we include a forced\u00adopen atomic statement, written force open atomic {...}. They have \nthe same semantics as open atomic statements with the exception that the e.ects occurring in them become \nvisible as soon as they .nish, regardless of where they are nested. To prevent deadlock in the presence \nof open nesting, the system needs to know an approximation of all shel\u00adters a thread will register for \nbefore the end of the outer\u00admost atomic statement (see Section 3.4). Recall that our static analysis \ntracks not only the shelters needed for the present atomic statement, but also about the shelters needed \nfor nested atomic statements, and the atomic statement nesting structure where they are needed. This \ninforma\u00adtion is used to construct calls to shelter reserve() and shelter unreserve(). Our implementation \ninstruments the above example as follows: // open atomic { shelter reserve((R,l->s), (W,account t.s)); \nshelter register((R,l->s)); shelter unreserve((R,l->s)); shelter wait(l->s); for (t = l->head; t; t = \nt->next) { // atomic { shelter register((W,t->from->s), (W,t->to->s)); withdraw(t->from, a); deposit(t->to, \na); shelter register pop(); // } } shelter unreserve((W,account t.s)); shelter register pop(); // } \nThe call to shelter reserve() expresses the fact that forthcoming nested atomic statements will register \nfor read\u00adonly access to l->s and for write access to account t structures. However, no actual registration \noccurs. The .rst call to shelter register() registers the thread on shelter l->s with a .rst timestamp. \nIn the second call to shel\u00adter register() for the nested atomic statement, the thread registers itself \non the indicated account t shelters with a second timestamp. The calls to shelter unreserve() indi\u00adcate \nthat the thread will perform no new registrations (until the end of the outermost atomic statement) on \nthe speci\u00ad.ed shelters. In the .rst call to shelter register pop() the thread only unregisters from t->from->s \nand t->to->s, but does not unregister from l->s or forget that it may still in the future attempt to \nregister for an account t shelter.  3.4 Deadlock Avoidance As described above, to prevent deadlock in \nthe presence of open atomic statements, the static analysis ensures that we reserve with shelter reserve() \nall the shelters that may be needed for the current and for nested atomic statements. We explain the \nreason for this by way of the arti.cial example of function foo. In foo, deposits are made into two accounts, \none in an outer atomic statement, and one in a nested atomic state\u00adment with open semantics. Now suppose \nthat Thread 1 calls foo(x,y), and Thread 2 calls foo(y,x) at the same time. Further suppose that Thread \n1 obtains timestamp 1 when making its .rst registration call for x->s. Without any sort of deadlock avoidance, \nThread 2 could then succeed at its registration with timestamp 2 for y->s. Thread 1 would then proceed \nthrough the .rst call to deposit on account x, and  void foo(account t *a, account t *b) { open atomic \n{ //shelter reserve((W,a->s),(W,b->s)); //shelter register((W,a->s)); //shelter unreserve((W,a->s)); \ndeposit(a,0.10); atomic { //shelter register((W,b->s)); //shelter unreserve((W,b->s)); deposit(b,0.10); \n} //shelter register pop(); } //shelter register pop(); } obtain timestamp 3 when it registers for shelter \ny->s, and Thread 2 would proceed to obtain timestamp 4 when it reg\u00adisters for shelter x->s. Then, when \nboth threads make their second call to deposit, they will be stuck. Registered for y->s, Thread 2 has \nan earlier timestamp than Thread 1, and registered for x->s, Thread 1 has an earlier timestamp than Thread \n2. However, when Thread 2 reaches its .rst registration call, it can observe that Thread 1 is already \nregistered for shelter x->s and may try to register for y->s in a nested atomic statement, and that if \nit were to complete its own registration, a cycle like the one described above could be created. We formalize \nthis notion of a cycle, and specify precisely our deadlock avoidance algorithm in Section 4, but here \nwe give an informal description. First, we say that two shelters may interfere with each other if they \nare the same shelter or if one is an ancestor of the other. Next, we say that thread A impedes thread \nB if thread A has a shelter registration that may cause thread B to block. Thread A may cause thread \nB to block if they are registered for shelters that interfere and thread A is regis\u00adtered with a smaller \ntimestamp than thread B. Thread A is also said to impede thread B if thread A is registered for a shelter \nthat interferes with a shelter that thread B has only reserved that is, thread B could block waiting \nfor thread A after it registers one of its reserved shelters. If there is a cycle in this relation, for \nexample if we have impedes(A,B), impedes(B,C), and impedes(C,A), then a deadlock may oc\u00adcur. We .nd cycles \nin the relation by constructing a graph in which threads are nodes, and in which there is a directed \nedge between a pair of nodes when the relation is true of the corresponding pair of threads. It may seem \nsuper.uous for reservations to occur even before the beginning of a closed-atomic statement. However, \nthis is necessary because we must perform deadlock avoid\u00adance not only before registration for an open \natomic state\u00adment, but also for closed ones, as well.  3.5 Coexisting with Explicit Locking In low-level \nsystems code it is often necessary to retain some explicit locking, for example .le locking in a database \nsystem. To see why this might be problematic, consider the following example. void accountFileLock(account \nt *a, int mode) { fcntl(a->file, mode); } void account file open(account t *a) { atomic { accountFileLock(a, \nWRITE); }} void account file close(account t *a) { atomic { accountFileLock(a, UNLOCK); }} In this example \nwe have added a file .eld to our ac\u00adcount t structure, which is also protected by the account structure \ns shelter. The atomic statements here protect ac\u00adcount structures from concurrent access while an account \ns .le is being opened and locked, and closed and unlocked. But the possibility for deadlock exists when, \nfor example, the thread holding the .le lock must wait on the shelter pro\u00adtecting the account structure \nbefore releasing the lock on the .le. When code like this is exposed as library calls, as in sqlite, \nit is not possible to .x the problem by adding addi\u00adtional atomic statements. We address this problem \nby introducing shadow shel\u00adters that follow the state, and track the owning thread, of an explicit lock \nin the program. Shadow shelters are declared with type shadow shelter t. The shadow shelters change state \nwhen explicit locking calls are made based on program\u00admer supplied annotations on the explicit locking \nfunctions. The annotation shadow change(s,c) on a function indicates that the function changes the state \n(e.g. locked, unlocked) of an explicit lock. Here, s is an expression for the shadow shelter for the \nlock, and c is an integer expression for the new state, both in terms of formal parameters and global \nvariables. Our static analysis determines that an atomic state\u00adment requires shadow shelter s when a \nfunction annotated with shadow change(s,c) is called within it, where s is s with actual arguments substituted \nfor formal arguments. In the above example, we would add a shadow s .eld to the account t structure of \ntype shadow shelter t, and anno\u00adtate the accountFileLock function with shadow change(a\u00ad>shadow s, mode). \nOur implementation then instruments calls to accountFileLock with calls to a function in our runtime \ncalled shadow change state(a->shadow s,mode), which updates the shadow shelter a->shadow s to re.ect \nthe state of the explicit lock, mode. In our implementation, we assume that the external lock is unlocked \nwhen the mode is zero, and locked otherwise. Further, shadow change state(a\u00ad>shadow s,mode) must block \nwhen a thread is attempting to set the mode to a locked mode, but the thread does not have the smallest \ntimestamp on the shadow shelter. We leave as future work extending this mechanism to support other lock\u00ading \nmodes.  Declaration Trace Statement Shelter (S) d T s s ::= ::= ::= |||::= int v sheltered by x (t1, \ns1), . . . , (tm, sm) reserve(s1, . . . , sm) register(s1, . . . , sm) pop v := v1 + v2 + n vs | x Identi.ers \nv, x Integers t, n, m Figure 2. Traces of shelter-based programs. Finally, a thread may only complete \na registration when the shadow shelter says that the .le lock is unlocked, or when the current thread \nis the .le lock s owner. Thus, the call to accountFileLock will never block and cause dead\u00adlock. This \nstrategy will also work for explicit locking calls where both the lock and unlock call are in the same \natomic statement.  3.6 Discussion This preponderance of annotations undoubtedly makes our design seem \nno less complex and confusing than simply using explicit locks. In particular, our insistence on avoiding \nwhole-program analysis, and the inclusion of open-nesting, and shadow shelters, requires us to add several \nconstructs beyond simple atomic statements and the sheltered by type quali.er. However, this increase \nin complexity comes only in response to the complex protocols and invariants that we see in existing \nsoftware. If we removed some of these annotations for the sake of simplicity, our design would be demonstrably \nless e.cient, or less complete. 4. Atomicity We give a trace-based formalism for our design in order \nto clarify the requirements on a shelter compiler and show that our system provides two useful guarantees: \nprogress (no deadlock) and partial atomicity (atomic statement e.ects are visible once committed to those \nstatements that started later). Our formalization is fairly di.erent to that used for Jade [32] as we \nare proving a di.erent property (progress and atomicity vs. equivalence to a sequential program). A trace \nT (Figure 2) captures the essential operations executed by a shelter-based program. A trace is executed \nin the context of declarations d1,..., dn of the global integer variables used in the trace. Each variable \nv is protected by its own .ne-grained shelter vs and a coarse-grained shelter s (possibly shared with \nother variables). These declarations implicitly de.ne a partial order on shelters (vs = vs, vs = s, s \n= s). This order mirrors our implementation s hierarchy, with the .ne-grained shelters matching shelter \nt objects and the coarse-grained shelters matching the type-based shelters like account t.s. We say that \na shelter s1 subsumes a shelter s2 if s2 = s1. For simplicity, we omit the distinction between read and \nwrite shelter access. The trace itself is a sequential interleaving of statements from multiple threads, \nwith each thread identi.ed by a dis\u00adtinct integer t. The statements, executed atomically, are ei\u00adther \nassignments of a computed value to a variable v,2 or one of the three low-level operations used to implement \natomic statements: reserve,3 register and pop. The reserve operation allows future registration for shelters, \nor shelters subsumed by, s1,...,sm, while the register operation begins an atomic statement, registering \nfor shelters, s1,...,sm. Finally, the pop operation releases the shelters registered for by the most \nrecent, still active reserve statement. For instance, a possible trace for atomic { j :=3 } is (0, reserve( \njs)), (0, register( js)), (0, reserve()), (0, j := 3), (0, pop) Thread 0 simply reserves then registers \nfor the required shelter, performs the atomic statement body, then releases all its shelters. The reserved \nshelters are released as soon as they will no longer need to be registered for, to avoid impeding other \nthreads (see below). An open atomic statement is more complicated. A possi\u00adble trace for open atomic \n{ p := p+1; atomic { q := p+q; }} in a context where both p and q are sheltered by s is: (0, reserve(s)), \n(0, register(ps)), (0, reserve(qs)), (0, p := p + 1), (0, register(qs)), (0, reserve()), (0, q := p + \nq), (0, pop), (0, pop) Thread 0 initially reserves the coarse-grained shelter s which protects both p \nand q. The .rst open atomic statement only registers for .ne-grained shelter ps. The thread then re.nes \nits reservation to the qs shelter, which it will subsequently register for. To start the inner atomic \nstatement, it addition\u00adally registers for the qs shelter and releases all its reserva\u00adtions as it no \nlonger needs them. Finally, it ends by terminat\u00ading both atomic statements, releasing all shelters. Traces \ndo not necessarily represent a valid execution of a shelter-based program. For instance, if p is sheltered \nby s, (0, p = 2) accesses p without being registered for either its .ne-grained shelter ps or its coarse-grained \nshelter s. This corresponds to a thread accessing a shelter-protected object outside of an atomic statement, \nwhich is an error in the program itself. We call this type of error a single-threaded error. Our trace \noperational semantics will send traces with this kind of error into an error state. 2 The atomic execution \nof v := v1 + v2 + n is not essential and could easily be relaxed with the addition of per-thread variables \nto the trace formalism. 3 The shelter unreserve function is represented in the trace by a reserve on \nthe resulting reservation set.  regfor(H, t, v) regfor(H, t, v1) regfor(H, t, v2) R, H |= (t, v := v1 \n+ v2 + n) H(t) = \u00d8. subsumed({s1,...,sm}, R(t)) R, H |= (t, reserve(s1,...,sm)) subsumed({s1,...,sm}, \nR(t)) m = 1 R, H |= (t, register(s1,...,sm)) H(t) * \u00d8 R, H |= (t, pop) R, H |= (t1, s1) M, R, H, a :(t1, \ns1),..., (tm, sm) .., ., ., . : E regfor(H, t, v) = .(a,s) . H(t).vs = s interferes(s1,s2) = s1 = s2 \n. s2 = s1 access(H, t, v) access(H, t, v1) access(H, t, v2) M, R, H, a :(t, v := v1 + v2 + n) . M[v \n. M(v1) + M(v2) + n], R, H, a R' = R[t .{s1,...,sm}] M, R, H, a :(t, reserve(s1,...,sm)) . M, R', H, \na H' = H[t . H(t) .{(a,s1),..., (a,sm)}] \u00accycle(impedes(R, H')) M, R, H, a :(t, register(s1,...,sm)) \n. M, R, H', a + 1 b = max {a | (a,s) . H(t)} H' = H[t .{(a,s) | (a,s) . H(t) . a * b}] M, R, H, a :(t, \npop) . M, R, H', a ' R, H |= (t1, s1) M, R, H, a :(t1, s1) . M', R', H', aM, R, H, a :(t1, s1),..., \n(tm, sm) . M', R', H', a' :(t2, s2),..., (tm, sm) subsumed(S1, S2) = .s1 . S1..s2 . S2.s1 = s2 access(H, \nt, v) = .(a, s) . H(t).(vs = s . .t' * t..(a', s') . H(t').interferes(vs, s') . a < a') impedes(R, H)(t1, \nt2) = .(s1, a1) . H(t1).(.(s2, a2) . H(t2).a1 < a2 . interferes(s1, s2). .s2 . R(t2). . interferes(s1, \ns2)) Figure 3. Trace Operational Semantics A di.erent kind of error is present in the following trace: \n(0, reserve(ps), (0, register(ps)), (0, reserve()), (1, reserve(ps), (1, register(ps)), (1, reserve()), \n(1, p := 1) Thread 1 accesses p, but it should have been blocked because thread 0 registered the shelter \nps earlier in the trace. We call this kind of error a multi-threaded error. We will show that these kinds \nof errors are forbidden by our trace operational semantics; there will be an implicit shelter wait() \nopera\u00adtion in front of each variable access. Figure 3 gives an operational semantics for traces that \nmakes a clear distinction between these two kinds of invalid traces. The state of the operational semantics \nis a four-tuple M, R, H, a where M : id . N maps variables to their values, R : N .P(S) maps threads \nto their current shelter reservations, H : N .P(N\u00d7S) maps threads to the shelters they are registered \nfor, and a : N is the timestamp of the last register statement. Registrations for a shelter are described \nby a pair (a,s) of the register statement s timestamp and the shelter itself. The single-threaded errors \nare explicitly checked for by the |= relation: R, H |= (t, s) holds if executing s is not a single-threaded \nerror for thread t in state R, H it is easy to verify by inspection of the rules in Figure 3 that violations \nof the |= relation can only be caused by the thread itself. The M, R, H, a :(t, s) . M', R', H', a' transition \nmodels the atomic execution of a single statement s by thread t and the corresponding state change. This \ntransition is only de.ned when it does not cause a multi-threaded error; the requirements of the . transition \ne.ectively represent the conditions for which a thread must wait until it can execute s. The single-threaded \nerrors formalize the requirements on (and freedoms of) the shelter compiler: it must never gener\u00adate \ncode that would violate the |= relation. That is, in the trace semantics and our implementation, the \n|= relation checks at runtime that the results of the static analysis that gener\u00adated the calls to reserve() \nand register() was correct. The two assignment rules use the regfor relation to check that the thread \nis registered for a shelter that subsumes the vari\u00adable s shelter. The reserve statement can reserve \nnew shel\u00adters when the thread is not currently registered for any shel\u00adters (H(t) = \u00d8): this corresponds \nto the requirement dis\u00adcussed in the overview to conservatively estimate the shel\u00adters required by an \natomic statement prior to entering it. The reserve statement can also release or re.ne its current reser\u00advation \n(checked by the subsumed relation); this e.ectively allows a shelter compiler to change the reservation \nat any time to its best approximation of the shelters that will still be registered for in the remainder \nof the outermost atomic state\u00adment. The register statement can only register for shelters that have been \npreviously reserved, and the pop statement cannot occur when no atomic statement is active.  The . transitions \nperform straightforward updates to the current state. Only two rules include preconditions that could \nlead to a multi-threaded error: the two assignment rules use the access rule to check that no other thread \nhas registered for a shelter for the variable with a smaller timestamp. The access rule encodes the conditions \nunder which a thread does not need to block for a shelter wait() call. The rule for register requires \nthat registering for the shel\u00adters will not create a cycle in the the directed graph formed by considering \nthe threads to be nodes, and the edges given by the impedes(R, H) relation (we call this the impediment\u00adgraph). \nEssentially, impedes captures when a thread might block the progress of another thread sometime in the \nfuture: when a thread is registered before another for an interfering shelter, or when a thread is registered \nfor a shelter that inter\u00adferes with a shelter that another thread plans to register for later. The impedes \nrelation itself depends on the current state of registrations and reservations, so we write it as a function \nof these mappings. Throuh a little abuse of notation, we write \u00accycle(impedes(R, H)) to indicate that \nthere are no cycles in the impediment-graph given the registrations and reser\u00advations in R and H. This \ncycle-detection is necessary for us to avoid deadlock while supporting open atomic statements as described \nearlier. In particular, it deals with the fact that threads may be registered for di.erent shelters with \ndi.erent timestamps. De.nition 1. Trace evaluation. In the context of declara\u00adtions d1,..., dn, trace: \n(t1, s1),..., (tm, sm) evaluates to M, R, H, a if * M0, R0, H0, 0:(t1, s1),..., (tm, sm) - . M, R, H, \na : E where the initial state has M0(v) = 0 for all variables v in d1,..., dn, and R0(t) = H0(t) = \u00d8 \n(no shelters registered for or reserved) for all threads t in t1,..., tm. Trace evaluation ends in the \nerror state (., ., ., .) if the trace has a single-threader error, but is not de.ned in the presence \nof a multi-threaded error. Theorem 1. Atomic-statement progress. If trace (t1, s1),..., (tm, sm) evaluates \nto M, R, H, a in context d1,..., dn then the ex\u00adtended trace (t1, s1),..., (tm, sm), (t, s) evaluates \nto M' , R' , H' , a ', where either: if .t ' .H(t ') = \u00d8, then t is any thread, s is any statement, or \nH(t) * \u00d8, and s is any statement Proof: By induction over traces, using the fact the trace semantics \nensures that \u00accycle(impedes(R, H)). As a conse\u00adquence, some thread t with H(t) * \u00d8 has no incoming edge \nfrom any other thread in the impediment-graph. This thread can attempt to execute any statement s: either \nthis statement will cause a single-threaded error, or by construction of t, ' M, R, H, a :(t, s) . M' \n, R' , H' , a . In essence, the above theorem states that programs that have no single-threaded errors \nand wait for the preconditions of . can always make progress, and, in particular, it can never be the \ncase that all threads in atomic statements are blocked. Theorem 2. Partial Atomicity. In a trace (t1, \ns1),..., (tn, sn) that evaluates to M, R, H, a, let Mi, Ri, Hi, ai be the state of M, H, R, a before \nthe ith step of the trace. If si = register(s1,..., sm) and sj = pop is the end of this atomic statement \nthen, for all variables v such that \u00acregfor(Hi, ti, v).regfor(Hi+1, ti, v) (the atomic statement gave \naccess to v) the e.ects of the writes by thread ti to v between si and sj are visible to exactly those \natomic statements of other threads that started after si and ended after sj. We prove Theorem 2 in Appendix \nA. It essentially states that updates to objects protected by an atomic statement be\u00adcome visible to \nother threads only after the end of the atomic statement. Note that it is safe for threads with concurrently \nrunning atomic statements to access the same objects, but the atomic statement that started later may \nnot access these ob\u00adjects until the atomic statement that started earlier ends. This theorem applies \nto both open and closed atomic statements. 5. Implementation Our implementation is written in about 3600 \nlines of OCaml using the CIL [29] library, with a runtime library written in about 4000 lines of C. We \nuse a combination of lock\u00adfree algorithms and other optimizations to ensure that our implementation scales. \nIn this section we describe optimizations. We also discuss issues that arise when using shelters in existing \nC programs, such as condition variables, library calls, polymorphism, and interaction with synchronization \nstrategies not based on atomic statements. 5.1 Optimizations In practice, a number of optimizations \nare required for shel\u00adters to scale. In particular, the following optimizations were critical to achieving \nperformance similar to, and in some cases, better than explicit locking. Without any one of these optimizations, \nat least one of our benchmarks will fail to ex\u00adceed sequential performance when explicit locks do.  \nFirst, we track shelter registration using a queue on each shelter of registered threads, sorted by timestamp. \nFurther\u00admore, threads are also placed on the queues of a shelter s an\u00adcestor, with an indication that \ntheir initial registration was for a descendant. This allows us to e.ciently implement shel\u00adter wait() \n(and the cycle-detection for deadlock-avoidance in shelter register()); we only ever need to inspect \na short initial segment of the queues at each level of the hi\u00aderarchy. When a shelter is acquired for \nread-only access, waiting for a shelter can be optimized: a thread must only wait until it has a timestamp \nsmaller than threads on the shelter s queue that have registered for write access. This mechanism is \nessentially equivalent to explicit read/write locking, however our shelter-based atomic statement implementation \ninvokes it automatically wherever possible. It is often the case that atomic statements are used to protect \naccess to objects for which there is not much con\u00adtention. To take advantage of this, instead of adding \nitself to a shelter s queue, our runtime allows a thread to acquire a spinlock or to increment a counter \nin the case of read\u00adonly access when there are no writers during the registra\u00adtion phase in the case \nthat there are no other threads on the shelter s queue. This also reduces contention for the global timestamp \ncounter. Furthermore, not all programs will use the various levels of the shelter hierarchy, and those \nthat do will not be using them at all times. Therefore, our runtime includes a mech\u00adanism to activate \nshelters higher up in the hierarchy only when they are needed that is, when a thread attempts to register \nfor them directly. When a shelter is inactive, threads registering for its children must simply read \na .ag that indi\u00adcates inactivity, and check that its queue is empty to see that no further action is \nrequired. Threads also record for which inactive shelters they have registered. When a thread wishes \nto register directly for a shelter with children, it registers as usual, but during a wait call, it must \nalso wait until there are no threads registered for a child shelter that make use of the inactivity of \nthe parent shelter. This check is made by ex\u00adamining the inactive shelters that each thread has registered \nfor. When there are no more such threads, the thread unsets the inactive .ag in the parent shelter, and \nproceeds when the usual conditions for a shelter wait() are met.4 As the number of threads and cores \nincreases, contention for the global sequence number counter increases. To ad\u00address this, when the compare-and-swap \noperation used to in\u00adcrement the counter fails, after retrying immediately a small number of times, we \nuse a binary exponential back-o. algo\u00ad 4 It may seem simpler for threads registering for a child shelter \nto simply acquire read-access to the inactive parent shelter by way of a counter, as in a reader-writer \nlock. However, in practice, contention for this counter can incur an unacceptably high overhead. We shift \nthe cost to the uncommon operation of waiting on a non-leaf shelter. rithm [17]. In our experiments, \nthis approach reduced by sev\u00ad eral orders of magnitude the number of compare-and-swap failures without \ncompromising performance. Potential Optimizations Our static analysis can tell when a shelter will no \nlonger be used before the end of an atomic statement, when a shelter will no longer be used for write \naccess, and when a child shelter can be held instead of a parent shelter. These optimizations are all \nsound in the absence of open atomic statements, but we have not yet determined the full conditions under \nwhich they can be used with nested atomic statements (see Appendix A), so we leave them for future work. \nAnother potential optimization that we leave for future work involves the way that timestamps are allocated \nto threads. If it can be determined that two threads will never use the same shelter, then their timestamps \nneed not be dis\u00adtinct. An analysis supporting such an optimization would likely rely on some form of \na must-not-alias analysis [28]. Finally, another potential optimization for timestamp al\u00adlocation proceeds \nas follows. Each shelter could be ran\u00addomly placed in one of a .xed number of sets of shelters. Each \nset would have a separate timestamp counter. When a thread registers for a set of shelters at the start \nof an atomic statement it would atomically acquire timestamps from each of the sets the shelters belong \nto. That is, for each reg\u00adistration, a thread would be granted a set of timestamps. shelter wait would \nproceed as before, using the appro\u00adpriate timestamp for each shelter. We have not yet implemented these \ntechniques for reduc\u00ading contention on the timestamp counter because the backo. optimization was e.ective \nenough on our benchmarks that this contention was no longer the performance bottleneck.  5.2 Condition \nVariables Our implementation includes support for condition vari\u00adables. That is, threads may send signals \nand wait on con\u00addition variables based on shared state that is protected by shelters. Shelter condition \nvariables are much like tradi\u00adtional condition variables. They are declared like pthread condition variables, \ne.g. shelter cond t scv, and are sig\u00adnaled in the same way, e.g. shelter cond signal(scv). However, a \nconditional wait on shelter protected state is slightly di.erent. We introduce the following construct: \nshelter cond wait(scv,e) s. The meaning of this statement is as follows. The thread waits on the shelter \ncondition variable scv while the condi\u00adtion, e, is false. If the thread is then signaled, and the con\u00addition \nis true, it executes the statement s atomically. This is accomplished by our analysis treating the shelter \nwait block as an atomic statement and collecting the shelters necessary for protecting both the block \nand the condition e. Then, the above construct can be translated as follows. The thread reg\u00adisters for \nthe shelters found by the analysis, waits on them, and then checks e. If e is false, it atomically releases \nthe shel\u00adters and sleeps on scv. If e is true, then the thread proceeds as in a normal atomic statement. \nWe leave as future work an extension to our implementation that ensures that condition variables are \nsignaled when appropriately speci.ed state is updated.  5.3 Library Calls and Polymorphism If a call \nis made into a library that does not use shelters, and does not use any callbacks, then it will not cause \na thread to register for any shelters. Therefore, it is only necessary to know what objects such a library \ncall will access in case any of these locations are protected by a shelter. We allow programmers to indicate \nthis by providing annotations that summarize the read and write behavior of library calls, so that our \nimplementation can automatically place the appro\u00adpriate wait calls ahead of them. Library calls invoking \ncall\u00adbacks that access shelter protected state are not currently supported by our system, though we believe \nthis issue could be addressed by allowing the programmer to describe the behavior of callbacks with an \nannotation. In our current implementation we do not support type\u00adquali.er polymorphism for the sheltered \nby(s) annota\u00adtions. This sort of feature has not been needed in the bench\u00admarks we analyze in Section \n6 due to the limited use of polymorphism in C programs. However, more modern lan\u00adguages may require increased \nsupport of polymorphism to allow code-reuse, and other good software engineering prac\u00adtices. We leave \nsupport for polymorphism as future work.  5.4 Other Synchronization Strategies It is not realistic to \nassume that all shared data will be protected by shelters and accessed within atomic statements. For \ninstance, some shared data will be read-only and need no synchronization, while other data will be protected \nby other means: barrier synchronization, data obtained from work queues and worked on exclusively by \na single thread, etc. The programmer must ensure that data is shared correctly using consistent mechanisms. \nOur own previous work in SharC uses sharing annotations on all types [5], while Martin et al. [24] use \ndynamic ownership assertions to detect where such rules are violated. Furthermore, external libraries \nmay already use locks to protect their own data converting these libraries to use shelters may not be \ndesirable, practical or even possible. For these cases in our sqlite benchmark, our shadow shelters mechanism \nhas proved adequate. 6. Evaluation We have modi.ed a number of programs to use shelters as the mechanism \nfor enforcing atomic statements, and we have measured the runtime performance of these programs on re\u00adalistic \ninputs. The purpose of this evaluation is to investigate the convenience of using shelter-based atomic \nstatements, and to compare the runtime performance of our implemen\u00adtation with .ve other mechanisms for \nenforcing atomicity, namely explicit locking, Autolocker, software transactional memory, a single global \nlock, and shelters implemented with pthread reader-writer locks. We chose these implementa\u00adtions for \ncomparison because they cover existing techniques for C not requiring special hardware or by-hand instrumenta\u00adtion. \nThe STM implementation allows us to compare against an optimistic approach, and Autolocker and the others \nal\u00adlow us to compare against the various ways of implementing atomic statements pessimistically. In the \nalternate implementation of shelters using reader\u00adwriter locks (RWLocks) each shelter contains a lock. \nWhen registering for shelters at the beginning of atomic statements, the locks are sorted by address \nto avoid deadlock before being acquired. When a .ne-grained shelter is registered, the shelter s lock \nis acquired in read mode if the section only reads the sheltered data, and in write mode otherwise. When \na coarse-grained shelter is registered, the lock is acquired in write mode. A shelter s ancestors locks \nare always acquired in read mode. 6.1 Experimental Setup All of our experiments were performed on a 2.27GHz \nIntel Xeon X7560 machine with four processors each with eight cores having 32GB of main memory running \nLinux. Hy\u00adperthreading was turned o.. We chose to compare against an Intel compiler for C/C++ that includes \nan STM imple\u00admentation [35]. Other STM implementations may give better performance [11], but the Intel \nSTM compiler has an anno\u00ad tation burden that is similar to that of our implementation, and requires no \nadditional real or simulated hardware. We also used the Intel compiler with the STM features disabled \nas the back-end of our implementation, and to compile the other versions of the benchmarks. The compile-time \nanaly\u00adses used for our implementation did not add signi.cantly to compilation time.  6.2 Benchmarks \nOur benchmark programs consist of the STAMP STM benchmark suite [11], along with: pbzip2, a parallel \nver\u00ad sion of bzip2; pfscan, a parallel .le scanning tool; ebarnes, an n-body simulation using the Barnes-Hut \nalgorithm [6]; open-atomic, a micro-benchmark that uses open atomic statements; and the sqlite database \nsystem. Table 1 shows the size of each of the benchmark programs, the number of atomic statements in \neach, and the number of other anno\u00adtations that were needed to use Intel s STM, shelters, and Autolocker, \nrespectively. The other annotations for Intel s STM are the tm callable annotations that must be placed \non functions called from transactions. The other annotations for our system are the sheltered by() annotations \nand the needs shelters() function annotations. Autolocker s only annotations are its protected by annotations. \nThe STAMP benchmarks are distributed with the tm callable annotations already placed, some of which are \nredundant. We also made redundant annotations when adapt\u00ad  (a) bayes -v32 -r768 -p40 -n10 -e8 -i1 -s37 \n(b) genome -g16384 -n4194304 -s64 (c) intruder -a10 -l256 -n16384 -s1 (d) kmeans -m10 -n10 -t0.00001 \n(e) labyrinth -i random-x256-y256-z3-n256 (f) ssca2 -s19 -i1.0 -u1.0 -l3 -p3 (g) vacation -t262144 (h) \nyada -a20 -i ttimeu100000.2 (i) ebarnes (1,000,000 bodies)  Figure 4. Graphs (a) -(k) show speedup \nover sequential runs versus the number of threads used for our benchmark programs when run with explicit \nlocking (Locks), Autolocker (AL), shelters (Shelters), Intel STM (STM), a single global lock (SGL), and \nshelters implemented with reader-writer locks (RWLocks). Higher is better. ing the application benchmarks \nfor shelters as we also found the annotations to be useful for documentations purposes. The annotation \ncounts in the table include these redundant annotations. This is why in the STAMP benchmarks the an\u00adnotation \ncount for STM is sometimes higher than it is for shelters. The annotation count for shelters on the sqlite \nap\u00adplication would have been as high as the count for STM, but our call-graph analysis allowed us to \navoid making module\u00adlocal function annotations. A similar analysis would bene.t STM to the same extent. \nThe results of all our experiments except sqlite are given by the graphs in Figure 4. The graphs show \nspeedup over sequential runs (i.e. Tsequential/Tparallel) versus the number of cores used. Each reported \nresult is the average of 50 runs. Command line arguments passed to the STAMP benchmarks are also given \nin the captions. Below, we describe all the results we observe before discussing the conclusions we draw \nfrom them. 6.2.1 STAMP Benchmarks The intended usage of the STAMP benchmark suite is to compare di.erent \ntransactional memory implementations. In addition, we believe that it is also a suitable benchmark suite \nfor the more general task of comparing di.erent implemen\u00adtations of atomic statements. The algorithms \nused in some of the benchmarks have synchronization strategies that are very challenging to handle well \nwith any implementation. We note that the suite covers variation across the follow\u00ading dimensions: contention, \nlength of time in atomic state\u00adments, and the number of shared memory accesses in atomic statements. \nBecause it attempts to cover this space, on sev\u00aderal of the benchmarks, none of the automatic software\u00ad \n Name Size (kloc) Atm. stms. STM Anns. Shelt. Anns. AL Anns. Seq. Time bayes 12.0 15 47 42 11 9.97s \ngenome 10.0 5 16 25 11 8.58s intruder 11.3 3 61 64 9 2.26s kmeans 3.9 3 4 7 6 9.17s labyrinth 8.2 3 50 \n46 13 3.02s ssca2 9.2 1 5 12 9 9.73s vacation 11.0 3 159 122 6 1.53s yada 13.4 6 105 86 12 4.19s ebarnes \n13.4 3 8 9 7 16.07s pbzip2 10.0 10 4 14 12 10.46s pfscan 2.8 6 4 11 12 2.56s oatomic 0.3 4 0 9 8 8.56s \nsqlite 131.0 97 2229 161 156 - total 236.5 149 2692 608 272 Table 1. Program size, number of atomic \nstatements, number of annotations for Intel STM, Shelters, and Autolocker, and sequential runtime for \nour benchmark programs. based approaches we investigate here manage to scale. Scal\u00ading on these benchmarks \nseems to require either specialized hardware, or by-hand instrumentation along with a knowl\u00adedge of implementation \ndetails [11]. For the bayes, intruder, labyrinth, vacation, and yada benchmarks contention is high, and \nthreads make many shared memory accesses in atomic statements. The genome benchmark includes lightly \ncontended atomic statements that make many shared memory accesses. The kmeans benchmark includes highly \ncontended atomic sections that make many updates to shared memory. The ssca2 bench\u00admark includes lightly \ncontended atomic statements that make only a few shared memory accesses. Scaling of ssca2 is limited \nby the sequential portion of the benchmark. The bayes, vacation, and yada benchmarks required ac\u00adtivation \nof our shelter hierarchy. The others did not. Au\u00adtolocker initially reported deadlock for these benchmarks \ndue to its inability to compute a global lock-order. To ad\u00address this, following the technique outlined \nby Autolocker s authors, we added global locks by hand until it accepted the programs.  6.2.2 Open Atomic \nStatements To show the bene.t of open nesting, we constructed a micro\u00adbenchmark to compare our implementation \nof open-nesting with explicit locking, Autolocker, and Intel STM. The form of this micro-benchmark is \ntaken from a pattern that appears a number of times in the Linux kernel, the MySQL database server, and \nsqlite. The micro-benchmark resembles the open\u00adnesting example in Section 3. Occasionally one thread \ncomputes a summary of data contained in a collection of linked lists. While this thread is computing, \nother threads are permitted to access individual linked-lists, but not to add or remove lists from the \ncollec\u00adtion: lock acquire(S.lock); for (i = 0; i < S.len; i++) { list *l = S.lists[i]; lock acquire(l->lock); \nlist ops(l); lock release(l->lock); } lock release(S.lock); A system having only closed atomic statements \nwould be unable to capture this safe way of increasing concurrency. In our oatomic benchmark, one thread \ncomputes a sum\u00admary of the collection of linked lists, while a number of other threads modify randomly \nselected lists from the collection. The results of this experiment are presented in Figure 4(l). 6.2.3 \nApplication Benchmarks Our three multithreaded C applications that use threads and locks were chosen \nto compare the real-world performance of shelters with the other implementations. The pbzip2 bench\u00admark \nis a parallel implementation of the popular block-based compression algorithm. Atomic statements are \nshort, lightly contended, and make only a small number of shared memory accesses. For this benchmark, \na 50MB text .le was com\u00adpressed. The pfscan benchmark is a tool that searches for a string in all .les \nunder a given directory tree. For this benchmark, we searched for the string ab in the Linux source code \ntree. The atomic statements are short, lightly contended, and make only a few shared memory accesses. \nCalls into the operating system and limited workload size impeded performance gains beyond 8 cores for \neach of the synchronization mechanisms. The ebarnes benchmark is an n-body simulation adapted from the \nBarnes-Hut Splash2 benchmark [38]. Performance of its oct-tree building phase is very sensitive to the \nsynchronization strategy. Contention is low, but many shared memory accesses are made in atomic statements. \nFor this benchmark, we simulated 1 million bod\u00adies so that building the oct-tree in parallel would give \na sig\u00adni.cant performance advantage.  6.2.4 Sqlite To show that our implementation is capable of scaling \nto large pieces of systems-level software, we ported the sqlite database system to use shelters instead \nof explicit locking with mutexes. Sqlite is composed of about 130k lines of code, and uses mutexes to \nprotect database connections, btrees, a shared page cache, the memory-management sub\u00adsystem, and its \npseudo-random number generator. Addition\u00adally, it uses .le locks and locking of pages in btrees to ensure \nthe atomicity of database transactions. These .le and page locks are frequently held across API calls. \nIn porting sqlite, we exercised all of the features of our implementation. In particular, we used open-nesting \nfor the memory, page-cache, and random number generator subsys\u00adtems. Furthermore, shadow shelters were \nrequired to prevent locks on .les and btree pages from causing deadlock, as in\u00addicated by the example \nin Section 3. This deadlock problem was not created by our use of atomic statements and shel\u00adters, though \nin the absence of our shadow shelters, it is ex\u00adposed more readily when imprecision in our static analysis \nrequires the use of the shelter hierarchy. The default explicit locking version of sqlite deals with \nthis issue by requiring application code to manually roll-back database transactions when certain lock \nacquisitions fail. Our shadow shelters hide this issue from client applications, however it is likely \nsome\u00adtimes useful for applications to deal directly with contention of this sort. We leave the addition \nof customizable contention management to our shelters-based implementation for future work.  Statistics \nfor the sqlite benchmark appear in Table 1. We evaluated the performance of our implementation by running \nsqlite s multithreaded regression suite, which uses between 2 and 12 threads, and comparing only against \nthe original explicit locking version; attempting to use Autolocker, the Intel STM, or a single global \nlock resulted in deadlock due to the .le locking problem. The unmodi.ed sqlite runs the multithreaded \nregression suite in roughly 4 minutes, whereas the shelters version took around 8 minutes. Because of \nthe imprecision of our static analysis, namely the type-based aliasing assumptions, many of the atomic \nstatements acted e.ectively as a single global lock. We believe that an automatic approach to acquiring \n.ner-grained locks or shelters in this context may only be possible in the presence of at least some \ninformation about the shape of data structures. It is also interesting to note that in the process of \nconvert\u00ading sqlite to use atomic statements, we were able to remove a few hundred lines of code dedicated \nto deadlock avoidance in its btree implementation.  6.2.5 E.ects of Workload Size We also did an experiment \nin which the number of bodies simulated in the ebarnes benchmark was varied from 100k bodies, which .t \ncomfortably in the CPU caches, to 8 mil\u00adlion bodies, which exceeded the capacity of the caches. We ran \nthe simulations with each implementation, and on 4, 8, 16, and 32 cores. We did not observe any changes \nin rela\u00adtive overhead with respect to the explicit locking version as workload size increased.  6.2.6 \nDiscussion Our implementation obtains performance comparable to ex\u00adplicit locking while having much of \nthe convenience of a ma\u00adture STM implementation. The graph in Figure 5 shows the average percent slowdown \nover all of the benchmarks (ex\u00adcluding sqlite and oatomic) of the Autolocker, shelters, sin\u00adgle global \nlock, reader/writer locks, and STM runs with re\u00adspect to the locking runs versus the number of threads \nused. Our implementation scales up where possible as threads are Figure 5. Average percent slowdown \nwith respect to explicit locking over all benchmarks versus the number of threads. Lower is better. added \nin many cases where the the other implementations fail to do so. In particular, the Intel STM scales \nwell when atomic statements are short or touch little shared memory, but suf\u00adfers signi.cant performance \npenalties when atomic sections are long and make many accesses to shared memory due to the cost of rollback \nand the cost of the instrumentation of shared memory accesses. Autolocker has performance simi\u00adlar to \nexplicit locking and our implementation, however it is occasionally necessary to add locks by hand in \norder for it to accept a program. This could be avoided if it also used a hierarchy like our implementation \ndoes. However, we have shown that the performance of a hierarchy built out of locks would be poor (RWLocks). \nFurthermore, Autolocker fails to allow open-nesting. On the other hand, Autolocker requires a smaller \nannotation burden due to its use of whole program analysis. Finally, our implementation outperforms the \nuse of a single global lock, and a naive implementation of shelters with a hierarchy of reader/writer \nlocks, demonstrating that our techniques help achieve good performance in this space. On the other hand, \nour implementation does not scale as well as explicit locking in the cases where contention is very low, \nbut in which the ratio of synchronization calls to actual work is somewhat high. This overhead is exposed \nby the ebarnes benchmark, but is not an issue in the pbzip2 benchmark, for example. 7. Related Work Many \nresearchers have investigated the implementation of atomic statements, and more generally, language constructs \nenabling correct concurrency. Fortress [3], Chapel [12], and X10 [14] are recent languages intended to \nbe used for writ\u00ad ing highly scalable, high-performance code. Each of them includes atomic statements \nfor protecting shared state. The relative merits of optimistic and pessimistic con\u00adcurrency control have \nbeen investigated by the database community. The consensus seems to be that optimistic approaches are \ndesirable in the presence of abundant re\u00adsources, so that the cost of roll-back and retry is not sig\u00adni.cant, \nwhereas pessimistic approaches are desirable when resources are scarce [2]. A few projects are more closely \nrelated to our own. We have already discussed Autolocker in detail [25]. Cherem et al. [13] present a \nsystem in which the locks required for an atomic statement are inferred from the structure of expres\u00adsions \naccessed in the atomic statement. The granularity of the locks may vary depending on the precision of \nthe un\u00adderlying analyses. Like Autolocker, this approach requires a whole program analysis for calculating \npointer aliasing and for re.ning the expressions accessed in an atomic statement. The approach of Hicks \net al. [19] is somewhat similar, re\u00ad quiring a whole program alias analysis to acquire a coarse\u00adgrained \nlock for the possible targets of a pointer when a precise target cannot be determined. The coarsening \nstrat\u00adegy for our system is similar, however instead of perform\u00ading a whole-program alias analysis, we \nsimply assume that pointers of the same type may alias. A more precise anal\u00adysis could be integrated \ninto our implementation to obtain a .ner-grained shelter hierarchy, however, due to the practi\u00adcal problems \nit presents, we chose to avoid whole-program analysis.  In the Jade [33] programming language, programmers \nmake annotations to describe how concurrent tasks will ac\u00adcess shared state so that the Jade compiler \ncan then automat\u00adically extract concurrency. We use the shelter mechanism to enforce atomic statements \nin an explicitly parallel program rather than as a way to help a compiler extract parallelism in an implicitly \nparallel language. Our system s implemen\u00adtation also expands on this mechanism by introducing ex\u00adplicit \nshelter objects that allow the programmer to declare what objects need protection, eliminating the need \nfor the programmer to make annotations at each atomic block, and by introducing a shelter hierarchy. \nThere also exist several transactional memory systems that can be used to implement atomic statements, \nboth hard\u00adware [4, 31] and software [16, 23, 36] based. We believe that the STM system for C most similar \nto our own in terms of programmer convenience is the Intel STM implemen\u00adtation [21]. Other STM implementations \nfor C give better performance [11], but they require by-hand instrumentation of reads and writes of shared \nmemory. The Intel STM im\u00adplementation incurs overhead from the instrumentation of all shared memory reads \nand writes inside of transactions, and from the rollback of transactions during which con\u00ad.icts are detected. \nBecause our system is pessimistic, it does not incur these overheads. Boehm argues that transactional \nmemory should be viewed as a mechanism for providing atomicity rather than a programming interface [9]. \nSome re\u00ad searchers have suggested that atomic statements with trans\u00adactional semantics are for concurrency \nwhat garbage collec\u00adtion is for memory management [18]. The nesting seman\u00ad tics of transactions and atomic \nsections has been explored by both TM [1, 26, 30] and database [7] researchers. Open\u00adnesting in a pessimistic \nsetting is desirable because there is no need to write compensating actions, or to worry about complicated \nroll-back semantics. Type systems with annotations describing locking rules have been used to prevent \ndata races [15], in particular for Java [10]. Boyapati s ownership type-system [10] allows the expression \nand checking of sophisticated locking schemes. Our system could be extended with similar ownership or \nre\u00adgion types as an alternate way to arrive at a .ner-grained shelter hierarchy. We leave these extensions \nfor future work. Also for Java, Hindman and Grossman [20] translate pro\u00ad grams with atomic statements \nto ones which acquire locks just before they are needed. Deadlock is avoided at runtime through rollback. \n8. Conclusion We have implemented a system in which atomic statements in C programs are implemented with \nshelters. We avoid the current problems associated with optimistic implementa\u00adtions of atomic statements \nby using a pessimistic approach, and unlike previous pessimistic approaches, our design al\u00adlows us to \navoid whole-program analysis, and provides for open-nesting. A wide range of benchmarks shows that our \ntechniques perform well. In the future, we plan to investigate the use of lightweight shape speci.cations \nthat may allow our analysis to make use of a .ner-grained shelter hierarchy to further improve us\u00adability \nand performance. Further, recording the di.erences between the timestamps obtained by threads, may provide \nan e.cient method for deterministically replaying threaded programs. Additionally, we have not yet thoroughly \ninvesti\u00adgated the fairness properties of our design. Fairness and live\u00adness issues did not arise in any \nof our benchmark programs, but in the future we wish to obtain more rigorous guarantees. 9. Acknowledgements \nWe would like to thank the anonymous reviewers for their valuable feedback and suggestions, and our shepherd \nMichael Hicks for guidance in improving the clarity and organization of this paper. References [1] Agrawal, \nK., Leiserson, C. E., and Sukha, J. Memory models for open-nested transactions. In MSPC 06. [2] Agrawal, \nR., Carey, M. J., and Livny, M. Concurrency control performance modeling: alternatives and implications. \nACM Trans. Database Syst. 12, 4 (1987), 609 654. [3] Allen, E., Chase, D., Luchangco, V., Jr., J.-W. \nM. S. R. G. L. S., and Tobin-Hochstadt, S. The Fortress language speci.cation version 1.0, 2008. http://research.sun.com/projects/plrg/fortress.pdf. \n[4] Ananian, C. S., Asanovic, K., Kuszmaul, B. C., Leiserson, C. E., and Lie, S. Unbounded transactional \nmemory. In HPCA 05, pp. 316 327. [5] Anderson, Z., Gay, D., Ennals, R., and Brewer, E. SharC: checking \ndata sharing strategies for multithreaded C. In PLDI 08, pp. 149 158.  [6] Barnes, J., and Hut, P. A \nhierarchical O(N log N) force\u00adcalculation algorithm. Nature 324 (Dec. 1986), 446 449. [7] Beeri, C., \nBernstein, P. A., and Goodman, N. A model for concurrency in nested transactions systems. J. ACM 36,2 \n(1989), 230 269. [8] Bernstein, P. A., Hadzilacos, V., and Goodman, N. Concur\u00adrency control and recovery \nin database systems. Addison-Wesley Longman Publishing Co., Inc., Boston, MA, USA, 1987. [9] Boehm, H.-J. \nTransactional memory should be an implemen\u00adtation technique, not a programming interface. In HotPar 09. \n[10] Boyapati, C. SafeJava: A Uni.ed Type System for Safe Pro\u00adgramming. PhD thesis, MIT. [11] Cao Minh, \nC., Chung, J., Kozyrakis, C., and Olukotun, K. STAMP: Stanford transactional applications for multi\u00adprocessing. \nIn IISWC 08. [12] Chamberlain, B., Callahan, D., and Zima, H. Parallel pro\u00adgrammability and the chapel \nlanguage. Int. J. High Perform. Comput. Appl. 21, 3 (2007), 291 312. [13] Cherem, S., Chilimbi, T., and \nGulwani, S. Inferring locks for atomic sections. In PLDI 08. [14] Ebcioglu, K., Saraswat, V., and Sarkar, \nV. X10: Program\u00adming for hierarchical parallelism and non-uniform data ac\u00adcess. In OOPSLA 04. [15] Flanagan, \nC., and Abadi, M. Object types against races. In Conference on Concurrent Theory (CONCUR (1999). [16] \nFraser, K., and Harris, T. Concurrent programming without locks. ACM Trans. Comput. Syst. 25, 2 (2007), \n5. [17] Goodman, J., Greenberg, A. G., Madras, N., and March, P. Stability of binary exponential backo.. \nJ. ACM 35, 3 (1988), 579 602. [18] Grossman, D. The transactional memory / garbage collection analogy. \nIn OOPSLA 07, pp. 695 706. [19] Hicks, M., Foster, J. S., and Pratikakis, P. Inferring locking for atomic \nsections. In TRANSACT 06. [20] Hindman, B., and Grossman, D. Atomicity via source-to\u00adsource translation. \nIn MSPC 06. [21] Intel. Intel C++ STM Compiler Prototype Edition 3.0, 2008. [22] kernel.org. Kernel bug \ntracker. http://bugzilla.kernel.org/. [23] Marathe, V., Spear, M., Heriot, C., A.Acharya,Eisenstat, D., \nIII, W. S., and Scott, M. Lowering the overhead of software transactional memory. In TRANSACT 06. [24] \nMartin, J.-P., Hicks, M., Costa, M., Akritidis, P., and Castro, M. Dynamically checking ownership policies \nin concurrent C/C++ programs. In POPL 10, pp. 457 470. [25] McCloskey, B., Zhou, F., Gay, D., and Brewer, \nE. Au\u00adtolocker: synchronization inference for atomic sections. In POPL 06, pp. 346 358. [26] Moss, J. \nE. B. Open nested transactions : Semantics and support. In Workshop on Memory Performance Issues (WMPI) \n(2006). [27] mozilla.org. Bugzilla@mozilla bug tracker. http://bugzilla.mozilla.org/. [28] Naik, M., \nand Aiken, A. Conditional must not aliasing for static race detection. In PLDI 07, pp. 327 338. [29] \nNecula, G. C., McPeak, S., and Weimer, W. CIL: Interme\u00addiate language and tools for the analysis of C \nprograms. In CC 04, pp. 213 228. http://cil.sourceforge.net/. [30] Ni, Y., Menon, V. S., Adl-Tabatabai, \nA.-R., Hosking, A. L., Hudson, R. L., Moss, J. E. B., Saha, B., and Shpeisman, T. Open nesting in software \ntransactional memory. In PPoPP 07, pp. 68 78. [31] Rajwar, R., Herlihy, M., and Lai, K. Virtualizing \ntransac\u00adtional memory. In ISCA 05, pp. 494 505. [32] Rinard, M. C., and Lam, M. S. Semantic foundations \nof Jade. In POPL 92, pp. 105 118. [33] Rinard, M. C., and Lam, M. S. The design, implementation, and \nevaluation of jade. ACM Trans. Program. Lang. Syst. 20, 3 (1998), 483 545. [34] Rossbach, C. J., Hofmann, \nO. S., and Witchel, E. Is transac\u00adtional memory programming actually easier? In PPoPP 10, pp. 47 56. \n[35] Saha, B., Adl-Tabatabai, A.-R., Hudson, R. L., Minh, C. C., and Hertzberg, B. Mcrt-stm: a high performance \nsoftware transactional memory system for a multi-core runtime. In PPoPP 06, pp. 187 197. [36] Shavit, \nN., and Touitou, D. Software transactional memory. In PODC 95, pp. 204 213. [37] Silberschatz, A., Galvin, \nP. B., and Gagne, G. Operating System Concepts, 6th ed. John Wiley &#38; Sons, Inc., New York, NY, USA, \n2001. [38] Woo, S. C., Ohara, M., Torrie, E., Shingh, J. P., and Gupta, A. The SPLASH-2 Programs: Characterization \nand Method\u00adological Considerations. In ISCA 95, pp. 24 36. A. Proofs Here, we prove Theorem 2 from Section \n4. Additionally we argue that releasing shelters early, before the end of an atomic statement, or downgrading \na shelter from one higher in the hierarchy to one lower in the hierarchy may have unexpected semantics. \nThis implies that though it is an attractive seeming optimization, it must be weighed against the desired \nsemantics for atomic statements. Theorem. Partial Atomicity. In a trace (t1, s1),..., (tn, sn) that evaluates \nto M, R, H, a, let Mi, Ri, Hi, ai be the state of M, H, R, a before the ith step of the trace. If si \n= register(s1,..., sm) and sj = pop is the end of this atomic statement then, for all variables v such \nthat \u00acregfor(Hi, ti, v).regfor(Hi+1, ti, v) (the atomic statement gave access to v) the e.ects of the \nwrites by thread ti to v between si and sj are visible to exactly those atomic statements of other threads \nthat started after si and ended after sj.  Proof: In the proof, we say the atomic statement at i to \nstand for the atomic statement started by the statement si = register(...) , Let (ai, s) . Hi+1(t) - \nHi(t) be such that vs = s. Consider the following cases for the atomic statement at k in threads t * \nti that can access v: 1. k < i and \u00acregfor(Hk, t, v) . regfor(Hk+1, t, v) (the atomic statement gave \naccess to v). Then .(ak, s ') . Hk+1(t) - ' Hk(t).vs = s ', and, obviously, ak < ai . interferes(s , \nvs). Thus thread ti cannot modify v until the atomic statement started at sk terminates. 2. k > i, and \nthe atomic statement ended at statement sl with l < j, and \u00acregfor(Hk, t, v) .regfor(Hk+1, t, v) (the \natomic statement gave access to v). Then this atomic statement never accessed v, as .k < d = l.\u00acaccess(Hd, \ntk, v). 3. k > i, and the atomic statement ended at statement sl with l > j, and \u00acregfor(Hk, t, v) .regfor(Hk+1, \nt, v) (the atomic statement gave access to v). Then .(a, s ') . Hk+1(t) -  ' Hk(t).a = ak . (vs = s \n. ' inter f eres(s , vs). As ak > ai, the atomic statement can only access v after the atomic statement \nat si terminates, so sees the e.ects of that atomic statement s writes. 4. regfor(Hk, t, v) (an enclosing \natomic statement gave ac\u00adcess to v): consider the (unique) enclosing atomic state\u00adment at l that gave \naccess to v: \u00acregfor(Hl, t, v) . regfor(Hl+1, t, v) If l < i, then the atomic statement at i cannot access \nv until the atomic statement started at l ends, which must necessarily be after the atomic statement \nat k ends. There\u00adfore the e.ects of the writes by the atomic statement at i are not visible to the atomic \nstatement started at k, as re\u00adquired. If l > i, the same arguments as above can be applied to show that \neither the atomic statement at k does not access v, or it terminates after the atomic statement at k. \nCorollary 1. Atomicity. In the trace of a program that does not use open-nested transactions, the e.ects \nof an atomic statement are visible to exactly those atomic statements that started after it. Proof: In \nthe absence of nested atomic statements, the con\u00addition \u00acregfor(Hi, ti, v) . regfor(Hi+1, ti, v) holds \nfor all vari\u00adables v accessed in the atomic statement at i. All atomic statements at j with j > i that \naccess any vari\u00adable v accessed by the atomic statement at i will necessarily end after the atomic statement \nat i ends, so will all see the e.ects of the writes of the atomic statement at i. Conversely, all statements \nthat start at j with j < i will not see the e.ects of the atomic statement at i. Finally, atomic statements \nthat start at j with j > i and end before si end cannot access any variable accessed by the atomic statement \nat i or, by induction, depend on any atomic statement that accesses such a variable, so the e.ects of \nthe atomic statement at i can be viewed as visible to the atomic at j. The requirement that the e.ects \nof an atomic statement not be visible to atomic statements that end before it is nec\u00adessary to prevent \nsurprising behaviors where the e.ects of an incomplete atomic statement become visible to inner atomic \nstatements of other threads. Consider a slightly modi.ed trace language where pop is replaced by re.ne(s1,...,sm) \nwhich allows a thread to release shelters at any time or re\u00adplace a coarser shelter by a .ner one. The \nsemantics for re.ne allow a thread to re.ne any shelters it is registered for: .(a,s) . H(t).ai = a . \nsi = s R, H |= (t, re.ne((a1,s1),..., (am,sm)) H ' = H[t .{(a1,s1),..., (am,sm)}] M, R, H, a :(t, re.ne((a1,s1),..., \n(am,sm)) . M, R, H ' , a In this modi.ed trace language, a variant of Theorem 2 where the and ended . \n. . clause is removed can easily be proved. However, the following trace has the surprising behavior \nthat thread 2 s atomic statement must complete after thread 1 s atomic statement (it accesses state shared \nwith thread 1 s atomic statement and started after it), but the e.ects of thread 2 s atomic statement \nare visible to thread 1 s inner atomic statement before thread 2 s atomic statement completes: (1, reserve(as, \nbs)), (1, register(as)), (1, reserve(bs)), (2, reserve(as, cs)), (2, register(as, cs)), (2, reserve()), \n(2, c = c + 1), (2, re.ne(as)), (3, reserve(bs, cs), (3, register(bs, cs)), (3, reserve()), (3, b = b \n+ c)), (3, re.ne()), (1, register(bs)), (1, reserve()), (1, b = b + a), (1, re.ne()), (2, a = 7), (2, \nre.ne()) The write in thread 2 to c a.ects the write in thread 3 to b, and therefore the write in thread \n1 s inner atomic statement to b. While one could argue that this behavior is acceptable (thread 1 s inner \natomic statement does occur after thread 2 s atomic statement), we believe it would be very counter\u00adintuitive \nfor programmers and should hence be forbidden. In essence, allowing early shelter release exposes an \natomic statement s e.ects too early . It is however worth noting that in the absence of nested atomic \nstatements, early shelter release or re.nement is sound (i.e. preserves atomic\u00adity). We attempted to \nprove that shelters could be understood as providing a mutual exclusion property similar to locks: De.nition \n2. A trace is lock equivalent if after every step, .t * t ' ..(a,s) . H(t), (a ' ,s ' ) . H(t ' ).\u00acinterferes(s, \ns ' ) i.e. two threads are never simultaneously registered for in\u00adterfering shelters.  Conjecture 1. \nLock equivalence. For every trace T which ' evaluates to M, R, H, a there exists a lock equivalent trace \nT which evaluates to M, R, H, a (in the same context). Further\u00admore, for every thread t, ops(t, T) = \nops(t, T '), where ops (t, T) is the subsequence of statements in trace T executed by thread t. However, \nthis conjecture is invalid, as the following vari\u00adation on the example above shows: (1, reserve(as, bs)), \n(1, register(as)), (1, reserve(bs)), (1, a = 10), (2, reserve(as, cs)), (2, register(as)), (2, register(cs)), \n(2, reserve()), (2, c = c + 1), (2, pop), (3, reserve(bs, cs), (3, register(bs, cs)), (3, reserve()), \n(3, b = b + c)), (3, pop), (1, register(bs)), (1, reserve()), (1, b = b + a), (1, pop), (1, pop), (2, \na = a + 1), (2, pop) Here thread 2 s inner atomic statement must happen af\u00adter the start of thread 1 \ns outer atomic statement starts and before the start of thread 1 s inner atomic statement. It is thus \nnot possible to reorganize this trace so that thread 1 and thread 2 are not simultaneously registered \nfor interfer\u00ading shelters. This trace also contradicts a weaker form of conjecture 1 which only requires \nnon-interference at assign\u00adment statements (the trace element (1, b = b + a) cannot be made to satisfy \nthis weaker conjecture).    \n\t\t\t", "proc_id": "2048066", "abstract": "<p>In this paper we introduce a new method for pessimistically implementing composable, nestable atomic statements. Our mechanism, called shelters, is inspired by the synchronization strategy used in the Jade programming language. Unlike previous lock-based pessimistic approaches, our mechanism does not require a whole-program analysis that computes a global lock order. Further, this mechanism frees us to implement several optimizations, impossible with automatically inserted locks, that are necessary for scaling on recent multi-core systems. Additionally we show how our basic mechanism can be extended to support both open- and closed-nesting of atomic statements, something that, to our knowledge, has not yet been implemented fully-pessimistically in this context. Unlike optimistic, transactional-memory-based approaches, programmers using our mechanism do not have to write compensating actions for open-nesting, or worry about the possibly awkward semantics and performance impact of aborted transactions.</p> <p>Similar to systems using locks, our implementation requires programmers to annotate the types of objects with the shelters that protect them, and indicate the sections of code to be executed atomically with atomic statements. A static analysis then determines from which shelters protection is needed for the atomic statements to run atomically. We have implemented shelter-based atomic statements for C, and applied our implementation to 12 benchmarks totaling over 200k lines of code including the STAMP benchmark suite, and the sqlite database system. Our implementation's performance is competitive with explicit locking, Autolocker, and a mature software transactional memory implementation.</p>", "authors": [{"name": "Zachary Anderson", "author_profile_id": "81332488087", "affiliation": "ETH Zurich , Zurich, Switzerland", "person_id": "P2839287", "email_address": "zachary.anderson@inf.ethz.ch", "orcid_id": ""}, {"name": "David Gay", "author_profile_id": "81100039538", "affiliation": "Intel Labs Berkeley, Berkeley, CA, USA", "person_id": "P2839288", "email_address": "dgay@acm.org", "orcid_id": ""}], "doi_number": "10.1145/2048066.2048132", "year": "2011", "article_id": "2048132", "conference": "OOPSLA", "title": "Composable, nestable, pessimistic atomic statements", "url": "http://dl.acm.org/citation.cfm?id=2048132"}