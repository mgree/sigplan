{"article_publication_date": "10-22-2011", "fulltext": "\n Null Dereference Veri.cation via Over\u00adapproximated Weakest Pre-conditions Analysis Ravichandhran Madhavan \n* Microsoft Research India, Bangalore t-rakand@microsoft.com Abstract Null dereferences are a bane of \nprogramming in languages such as Java. In this paper we propose a sound, demand\u00addriven, inter-procedurally \ncontext-sensitive data.ow analy\u00adsis technique to verify a given dereference as safe or poten\u00adtially unsafe. \nOur analysis uses an abstract lattice of formu\u00adlas to .nd a pre-condition at the entry of the program \nsuch that a null-dereference can occur only if the initial state of the program satis.es this pre-condition. \nWe use a simpli.ed domain of formulas, abstracting out integer arithmetic, as well as unbounded access \npaths due to recursive data struc\u00adtures. For the sake of precision we model aliasing relation\u00adships explicitly \nin our abstract lattice, enable strong updates, and use a limited notion of path sensitivity. For the \nsake of scalability we prune formulas continually as they get prop\u00adagated, reducing to true conjuncts \nthat are less likely to be useful in validating or invalidating the formula. We have im\u00adplemented our \napproach, and present an evaluation of it on a set of ten real Java programs. Our results show that the \nset of design features we have incorporated enable the analy\u00adsis to (a) explore long, inter-procedural \npaths to verify each dereference, with (b) reasonable accuracy, and (c) very quick response time per \ndereference, making it suitable for use in desktop development environments. Categories and Subject Descriptors \nD[2]: 4 Assertion checkers; F [3]: 1 Assertions, Mechanical veri.cation General Terms Algorithms,Experimentation,Veri.cation \n* Part of this work was done while the author was af.liated with Indian Institute of Science, Bangalore. \nPermission to make digital or hard copies of all or part of this work for personal or classroom use is \ngranted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. OOPSLA \n11, October 22 27, 2011, Portland, Oregon, USA. Copyright c &#38;#169; 2011 ACM 978-1-4503-0940-0/11/10. \n. . $10.00 Raghavan Komondoor Indian Institute of Science, Bangalore raghavan@csa.iisc.ernet.in 1: foo(a,d) \n{ 2: b=null; 3: if (a == null) 4: b=d; 5: c=a; 6: if (c != null) 7: b.g=10; 8: } Figure 1. Computation \nof weakest at-least once pre\u00adcondition at line 1 for condition b = null at line 7 1. Introduction We \naddress the problem of verifying the non-nullness of dereferences in Java programs, via over-approximated \nweak\u00adest pre-conditions analysis. Given a program point p and a condition C,a pre-condition [5] for (p, \nC) is a constraint on the initial state of the program that guarantees that the program state will satisfy \nC every time control reaches the point p; control, however, will not necessarily reach p in every execution \nthat begins by satisfying the pre-condition. A weakest pre-condition wp(p, C) is the weakest such pre\u00adcondition \nfor (p, C). Clearly, a property C at point p always holds iff wp(p, C) is logically equivalent to true \n(we as\u00adsume that there are no external constraints on the possible initial states of a program). We de.ne \na notion of the weak\u00adest at-least once pre-condition, denoted wp1(p, C), as the weakest constraint on \nthe initial state of the program that guarantees that execution will reach p at least once in a state \nthat satis.es C; control, however, may reach p other times in states that do not satisfy C. Note: (a) \nfor any (p, C), wp1(p, C)= \u00acwp(p, \u00acC), and (b) a property A always holds at a program point p iff wp1(p, \n\u00acA) is logically equiv\u00adalent to false. Weakest at-least-once preconditions are not computable precisely \nin the presence of loops and recursion. Our ap\u00adproach is to use an abstract interpretation [4] to over\u00adapproximate \nthe weakest at-least-once pre-condition; i.e., for variable v dereferenced at a given program point p, \nwe use a backwards, demand-driven data.ow analysis to compute a condition at the entry point to the program \nthat is equal to or implied by wp1(p, v = null). We declare the dereference safe if this condition is \nequal to false. Our data-.ow lattice is a lattice of predicates, whose literals are access paths (of \nthe form v.f1.f2. . .fk), and null. The lattice elements are ordered by implication, where weaker predicates \ndominate strong predicates, while our join operation is OR. We illus\u00adtrate our analysis using a small \nexample in Figure 1; as can be seen, the weakest at-least-once pre-condition at line 1 for b to be null \nin line 7 is a  = null . In our implementation, in order to ensure termination, and for the sake of \nscalabil\u00adity, we ensure .nite access path lengths, abstract out arith\u00admetic, use summary tables [18] \nto cache the results of inter\u00adprocedural analysis, and use custom predicate-simpli.cation rules rather \nthan a full-blown satis.ability solver. For the sake of precision our analysis is inter-procedurally \ncontext\u00adsensitive, keeps track of aliasing relationships precisely, and uses a limited form of path sensitivity. \n 1.1 Contributions Our chief contributions are: A novel set of design features that together enable \nsound demand-driven veri.cation of dereferences in real Java programs, with very quick response time \nper dereference, and reasonable precision. Our approach would be useful if integrated in a development \nenvironment, letting a pro\u00adgrammer check the safety of the dereferences in their cur\u00adrent scope of interest \n(e.g., the methods in the class they are editing) in near real time.  An implementation of this approach. \n An evaluation of the approach on a set of real Java pro\u00adgrams, with detailed analysis of the running \ntime and pre\u00adcision. To the best of our knowledge ours is the .rst prac\u00adtical weakest pre-conditions-based \nveri.cation approach to be demonstrated on large, real Java programs.  Over-approximating weakest at-least-once \npreconditions for Java is challenging for multiple reasons. Loops and re\u00adcursion are dif.cult to handle \nsoundly and with reasonable precision. Even for loop-free programs there is potential for exponential \nexplosion due to the number of distinct paths in the program. Within straight-line code itself aliasing \ncan increase the number of disjunctions in the preconditions. In addition to these issues, the computation \nshould handle lan\u00adguage features like virtual method dispatch, type checks, fre\u00adquent accesses to heap, \ndeeply nested chains of method calls, and call backs (from library methods to application meth\u00adods). \nAll of these, and also the sheer scale of real Java pro\u00adgrams can make the analysis extremely expensive \nor very imprecise unless it is designed carefully. Therefore, we make a set of design decisions for our \nanalysis (outlined above, and described in detail in the subsequent sections) that yield good precision \nin many (but not all) scenarios, without being impractical expense-wise. Compared to previous related \nwork on this topic, e.g., Xylem [15] and Salsa [13], our technical innovations are in terms of how we \n(a) perform strong updates (for precision) in the presence of aliasing, (b) handle recursion soundly \nin the backwards, demand-driven setting, and (c) enable more paths, and longer paths with deep call chains \nto be explored, by continually simplifying the formulas being propagated. Our formula simpli.cation is \nbased on dropping conjuncts that are less likely to eventually play a role in the validation or invalidation \nof the formula, and is with the intent of keep\u00ading formula sizes in check, which could otherwise explode \nwith increasing path lengths. As a result of these innova\u00adtions we are able to ef.ciently analyze large \nprogram scopes, thereby refuting a conjecture by previous researchers [13] that such an analysis would \nbe infeasible. In fact, the pri\u00admary strength of our approach is being able to verify a class of dereferences \nwhere the pointer value is not transmitted through recursive data structures or arrays (which are ex\u00adpensive \nto model precisely, and which we deal with conser\u00advatively), but are nonetheless dif.cult for other approaches \nto reason about due to the large number of paths, and long paths, that go from the program entry to the \ndereference. Ours is a veri.cation approach to ef.ciently .nd a su\u00adperset of all potential null-dereference \nsites; this is in con\u00adtrast with bug-.nding approaches [11, 15], that may miss bugs (and also report \nfalse positives). Our approach comple\u00adments precise, expensive, and incomplete approaches such as Snugglebug \n[3] that attempt to .nd a concrete input to a program that disproves a desired safety property, in that \nthese approaches could be used to try to validate our bug reports. We have evaluated our implementation \non a set of ten medium-to-large real programs. The approach turns out to be highly ef.cient, taking less \nthan 250 milli seconds per dereference on at least 93% of the dereferences in each of 9 (out of 10) programs, \nand on 85% of the dereferences in one program. The memory footprint of the analysis is very small. It \nis reasonably precise, reporting fewer than 16% of the dereferences as potentially unsafe in 7 out of \nthe 10 pro\u00adgrams, and between 21% and 29% of the dereferences as po\u00adtentially unsafe in the remaining \n3 programs. We have also identi.ed various interesting characteristics of the derefer\u00adences that were \nveri.ed as safe, such as lengths and context\u00addepths of analyzed paths, as well characteristics of derefer\u00adences \nthat were found to be potentially unsafe. An advantage of our approach is that it is demand driven, in \ncontrast to previous veri.cation approaches that are based on a forward analysis, such as Salsa [13]. \nA demand-driven analysis means that an individual programmer working on a portion of a large application \ncould try to verify derefer\u00adences in his or her code, by taking into account all paths in  AccessPath \n(AP) . Variable.Fields | Variable Fields . .eld.Fields | E Atom . AP | null Predicate . Atom op Atom \n| true | false op . = | = 2Predicate Disjunct = 2Disjunct Formula = Figure 2. Structure of formulas (lattice \nelements) the program that lead to these dereferences, without paying the price of verifying all dereferences \nin the program. Our analysis is currently highly ef.cient for this purpose. In the future, we will look \ninto trading off some of this ef.ciency to address more precisely certain complex features such as arrays \nand recursive heap data structures. The rest of this paper is structured as follows. We discuss our analysis \nin the intra-procedural setting in Section 2, and its extension to the inter-procedural setting in Section \n3. Section 4 contains a few details about our implementation, while Section 5 discusses our experiments \nin detail. We discuss related work in Section 6, and conclude the main paper in Section 7. Finally, we \nsketch a formulation of our analysis as an abstract interpretation in the Appendix. 2. The basic intra-procedural \napproach As mentioned in the Introduction, our approach is a standard backwards data-.ow analysis. In \nthe rest of this section we discuss in the intra-procedural setting our abstract lattice, transfer functions, \nthe analysis, as well as the key features and properties of this analysis. 2.1 Abstract lattice, and \ntransfer functions Our abstract lattice elements are formulas in disjunctive nor\u00admal form, represented \nas shown in Figure 2. Note that a Dis\u00adjunct is a conjunction of Predicates, and a Formula (i.e., a lattice \nelement) is a disjunction of Disjuncts. The lattice el\u00adements are ordered intuitively by implication: \nin particular, given two Formulas f1 and f2, f1 = f2 if f1 . f2 (con\u00adsidering each Formula as a set of \nDisjuncts). Therefore, our join operation is set union (which implements logical OR), bottom element \nis the empty set of disjuncts (which repre\u00adsents logical falsehood), and the top element is the set of \nall Disjuncts (which represents logical truth). While the lattice as shown is of in.nite size due to \nunbounded sequences of .elds in access paths, in the analysis we effectively make the lattice .nite by \nbounding the access paths (see details in Section 2.1.1). We assume that the program is in an Intermediate \nRepre\u00adsentation (IR) form like three-address code. We also assume that variables have been renamed in \na way that the name of a variable fully identi.es the scope it is declared in. The (backward) transfer \nfunctions for the individual statements in the IR are shown in Figure 3. Since the functions are all \ndistributive, we express each one as taking a single disjunct in the statement s post-state as input, \nand returning a set (i.e., disjunction) of disjuncts f' in the statement s pre-state. Our notation and \nterminology is as follows. For a formula f and variables v, w, f[w/v] means f in which v is replaced \nsyntactically with w. Vars(pred) is a set containing the (one or two) program variables (without the \n.eld names) referred to in pred. SubAPs(f) is a set containing all pre.xes (proper as well as improper) \nof all access paths that are operands of the predicates in f. The symbol = s denotes syntactic identity \nof two terms (as opposed to value identity). Note that the PUTFIELD rule (in its second of three cases) \nis the only one to return a f' containing two disjuncts. All other rules return a f' containing a single \ndisjunct; therefore, for brevity, we omit the curly braces around f' in these rules. The root dereference \nis the given dereference that we wish to verify; e.g., in Figure 1 it is the dereference of b at line \n7. Every disjunct in the analysis has zero or one root predicates, which are always of the form AP = \nnull. At the program point preceding the root dereference of an access path ap the sole disjunct (i.e., \nthe post-condition) contains a single predicate ap = null, which is the root predicate of this disjunct. \nWhenever a disjunct d with root predicate pd gets transformed by a statement into a disjunct d', the \nroot predicate in d' is that predicate that is equal to pd or the result of the rewriting applied to \npd by the transfer function of the statement. In our notation we underline the root predicate in each \ndisjunct; for instance, see Figure 1, where the disjunct at the start of the analysis (i.e., the post-condition) \nis b = null at the point before Line 7. The objective of the transfer function of each statement is to \naccept a post-state f, and return a pre-state f' that s an over-approximation of the weakest at-least-once \npre\u00adcondition of f wrt the statement. The functions for COPY, NULLASGN, and RETURN are self-explanatory; \nwe discuss below some of the more interesting ones. The EXPRASGN rule reduces all predicates in f that \ndepend on v to true, because we abstract out all arithmetic from our disjuncts. GETARRAY does a similar \nreduction, while PUTARRAY has an identity transfer function, because the incoming fact (i.e., Formula) \ncan contain no array references. NEWASGN uses an approach that is standard in many previous approaches \n of representing all objects allocated at an allocation-site i by a single variable ti (that is not present \nin the original program). 2.1.1 Bounding access paths, and path sensitivity We .nitize our lattice by \nbounding access path lengths, as follows. In rule GETFIELD, whenever a predicate in the computed pre-condition \nf' contains an access path in which some .eld f repeats more than once in the sequence of .elds, we drop \nthis predicate (i.e., reduce it to true). This effectively .nitizes our lattice, because no access path \ncan be longer than the total number of distinct .elds (statically) in the program. This does render our \nanalysis imprecise in the  Instruction Transfer Function: .f . Disjunct.f ', where f ' . 2Disjunct , \nand is = COPY NULLASGN NEWASGN GETFIELD ASSUME EXPRASGN GETARRAY PUTARRAY PUTFIELD RETURN v = w v = null \nv = new T v = r.f assume(b) v = v1 op v2 v = a[i] a[i]= v r.f = v return v f[w/v] f[null/v] f[ti/v], \nwhere ti is a variable representing all objects allocated at this instruction i f[r.f/v] .{r = null} \n(See Section 2.1.1 regarding access-path bounding.) '' f .{b}, if b = s AP op null f, otherwise f - \nS, where S = {pred . f | v . Vars(pred)} f - S, where S = {pred . f | v . Vars(pred)} f f[r.f, v, ap1.f][r.f, \nv, ap2.f] ... [r.f, v, apn.f], where {ap1.f, ap2.f, . . . , apn.f} are the access paths in SubAPs(f) \nthat end with .eld f, and f[r.f, v, api.f] = f[v/api.f] .{r = null}, if MustAlias(r, api) after r.f \n= v = {f[v/api.f] .{r = api,r = null},f .{r = api,r = null}}, if MayAlias(r, api) after r.f = v = f .{r \n= null}, otherwise f[v/ret], where ret is a place-holder for the return value Figure 3. Abstract transfer \nfunctions presence of recursive data structures; we postpone further discussion of this to Section 4.2. \nWe implement a limited notion of path-sensitivity as follows. The ASSUME rule conjuncts the condition \nb in assume(b) with f to yield f ' if b is an access path being compared to null; this predicate (i.e., \nb) could get validated or invalidated later during the propagation, based on other assignments or assumes \nthat are encountered.  2.1.2 Strong updates in the presence of aliasing Strong updates of .elds are \na requirement for precision in null-dereference analysis, but are dif.cult to perform in the presence \nof aliasing. Even precise pre-computed may-alias and must-alias information may not enable strong updates \noften enough, because at a given program point two vari\u00adables may be aliased under some paths and not \naliased un\u00adder other paths. Since our approach is to propagate disjuncts selectively along paths (depending \non the predicates in the disjuncts), we basically incorporate a .ow-and context\u00adsensitive and limited \npath-sensitive points-to analysis within our main analysis, and always perform strong updates at put\u00ad.eld \nstatements. Given a put-.eld statement r.f = v, and for each access path api that occurs in f, we (a) \nhypoth\u00adesize that api and r are aliases, and generate a disjunct in f ' in which occurrences of api.f \nhave been replaced with v, and also (b) hypothesize that api and r are not aliases, and generate a disjunct \nin f ' that is identical to f. To the .rst of the two disjuncts above we add an aliasing predi\u00adcate r \n= api, and to the second one we add the aliasing predicate r = api. The aliasing predicate(s) in a disjunct \nencode the aliasing hypotheses under which the disjunct is valid, and are used by the analysis to subsequently \nvalidate (resp. invalidate) the disjunct as it gets propagated to state\u00adv = w; <x.g = null> w.f = x; \n<x.g = null . v=w>, <v.f.g = null . v = w> v.f.g.h; <v.f.g = null> Figure 4. Illustration of our bottom-up \nanalysis in the pres\u00adence of PUTFIELD statement. Each entry on the right-hand side shows the formula \nat the program point that precedes the corresponding statement. Root predicates are underlined. ments \nthat con.rm (resp. contradict) the hypotheses. In con\u00adtrast, previous approaches such as Xylem [15] and \nSalsa [13] are not able to take advantage of path-sensitive aliasing rela\u00adtionships as deeply as we do. \nNote that in the PUTFIELD rule we do make use of pre\u00adcomputed MayAlias and MustAlias information (if \navail\u00adable). This is simply an optimization for ef.ciency; the pre\u00adcision of the pre-computed alias information \ndoes not in\u00ad.uence the ultimate precision of our analysis (in the intra\u00adprocedural setting). For instance, \nif there is no pre-computed may-alias information available then every access path can be assumed to \nbe may-aliased with every other access path (provided their static types are compatible); similarly, \nif there is no pre-computed must-alias information available (which is the case in our implementation), \nwe can treat api and apj to be must-aliased iff api = s apj . We illustrate the above technique using \nFigure 4. The left hand side shows a toy program, wherein we wish to verify the dereference of the .eld \nv.f.g . To the right of each statement we show the disjuncts at the point above the statement. Propagating \nthe disjunct v.f.g = null from the .nal point upwards, we encounter the put-.eld statement, which leads \nto the generation of two disjuncts. Both these disjuncts are propagated upwards, but the second one gets \ninvalidated by the statement v = w , meaning the .rst one alone reaches the top (and hence is the weakest \nat-least-once pre-condition).  Given a put-.eld statement r.f = v, if there are k pre.xes of access \npaths of the form ap.f in f such that ap may be aliased with r, the resultant f ' will have 2k disjuncts. \nWhile this blowup sounds excessive on paper, it is not a problem in practice, due to several reasons. \nOne reason for the blow up, in which case the blowup gets compensated for very soon after it happens, \nis that many IRs, such as the one we use Wala [21] always copy program variables into temporaries before \ndoing .eld accesses. Thus, the put-.eld statement above becomes something like tk = r; tk.f = v . While \npropagating a disjunct that refers to r in the backwards direction the analysis would not know initially \nif tk and r are aliased or not. Thus, it would blow up the disjunct. Then, upon encountering tk = r it \nwould invalidate the disjunct that has the aliasing predicate tk = r. When programmers introduce aliasing \nin their code, then invalidation may not happen so quickly, and the number as well as sizes of disjuncts \ncould increase inordinately when propagated through long inter-procedural paths (al\u00adthough, due to the \n.nitized lattice, there is no danger of non\u00adtermination). To deal with this, we associate an age with \nevery predicate, which is the number of statements it has been propagated through; we have a threshold \nk1 (which we set to 1000 in our implementation), and drop (i.e., re\u00adduce to true) a predicate whenever \nits age increases beyond k1. Additionally, we have another threshold k2 (set to 3 in our implementation): \nwhenever a disjunct has more than k2 predicates, excluding aliasing predicates, we drop its oldest of \nits predicates. The idea behind dropping old predicates is our observation that branch correlations in \npaths typically occur between branches that are near each other in the code, than very far from each \nother. The root predicate alone is never dropped using these heuristics. Note that these heuris\u00adtics \nare conservative; in fact, dropping any predicate at any point only results in a weaker pre-condition \nthan the ideal one. Xylem also bounds disjunct sizes, but in a more coarse\u00adgrained manner. They drop \nan entire disjunct when its size crosses a threshold. We, on the other hand, drop individual predicates \nfrom disjuncts, and that too old ones, which are least likely to get validated or invalidated going forward. \nWe found that our approach serves the purpose of improving ef.ciency, but with greater precision.  \n2.2 Simpli.cation rules Inspired by the Snugglebug [3] approach, we apply a light\u00adweight custom simpli.er \non each disjunct after it is produced by a propagation step, to validate, invalidate, or simplify the \ndisjunct. Figure 5 shows a sampling of the rules used in our simpli.er. In Figure 5, the di s represent \ndisjuncts (or a sub\u00adset of all the predicates in a disjunct), api s represent ac\u00ad (1) (ap = ap) -. true \n(2) {ap1 = ap2, ap1 = ap2} -. {false} (3) {ap1 = null, ap1 = null} -. {false} (4) (ti = tj ) -. false \n (5) (ti = tj ) -. true  (6) (ti = null) -. false (7) (ti = null) -. true  (8) (ti = ap) -. false \n (9) (ti = ap) -. true  Figure 5. Rules for simplifying disjuncts cess paths, and each ti represents \nthe special variable intro\u00adduced corresponding to the allocation-site i by the analysis to represent \nall concrete objects allocated at this site (see the NEWASGN rule in Figure 3). Rules 1-3 are straightforward. \nRules 4 and 5 are based on the fact that the sets of objects allocated at two different sites are disjoint. \nRules 6 and 7 are based on the fact that a newly allocated object cannot be null (we do not model the \npotential failure of memory al\u00adlocation). Rules 8 and 9 are based on the fact that a newly allocated \nobject is not the same as any other existing object. Note that for any statement st, its abstract transfer \nfunc\u00adtion fst actually is the function shown in Figure 3, composed with repeated applications of the \nsimpli.cation rules until a .x-point is reached.  2.3 Putting it all together Let ap be a given root \ndereference, i.e., a given access path that is dereferenced at a program point p that we wish to verify. \nWe start the data.ow analysis with the fact (i.e., formula) at program point p set to C = ap = null, \nand facts at all other program points set to the empty set. When the analysis terminates, we read off \nthe fact at the program entry as an over-approximation of wp1(p, C). If this over\u00adapproximation is the \nempty set (i.e., logical falsehood) then the root dereference is safe. Since the transfer functions are \ndistributive, we mark and propagate individual disjuncts, not formulas (sets of dis\u00adjuncts), using Kildall \ns [12] algorithm. This greatly reduces the time to reach a .x point. We have also implemented a few optimizations \nto the propagation: (a) Whenever any disjunct anywhere gets simpli.ed to the empty set (i.e., logical \ntruth) we terminate the analysis right away and call the root deref\u00aderence unsafe, without waiting for \na .x point to be reached, and (b) whenever a disjunct gets simpli.ed to {false} we unmark it, and do \nnot propagate it any more. It would be possible to take a slice of the program starting from the root \ndereference, and do our analysis only on the slice. The result would be the same as performing the analysis \non the whole program. However, we do not do this, because we found that the time spent on computing the \nslice was not compensated by the time saved during the subsequent analysis.  Since we don t model array \nreferences in our abstract lattice, we don t analyze root dereferences that have array references in \nthem (we call them unsafe by default). As with previous techniques [13, 15], our technique does not model \nconcurrency soundly, nor dynamic features such as re.ection and dynamic class loading. Our approach may \nmiss null-dereference errors that manifest themselves due to these aspects. See the right-side of Figure \n1 for an illustration of our approach. The root dereference is b.g; hence we start the analysis with \nthe singleton disjunct b = null at the point above this dereference, and the empty set all other points. \nThe disjuncts at each program point after the propagation is over are shown in the .gure (we omit false \ndisjuncts at the program points before Lines 1 and 2). Note the path sensitivity in this analysis: the \ndisjunct d = null . a = null at the true branch of the .rst conditional (in line 3) becomes false (and \nis not shown) after it propagates up through the conditional. The end result is that the dereference \nin line 7 may be unsafe if a = null at the entrance to the program; in this example, this turns out to \nbe the precise weakest at\u00adleast-once pre-condition, and not an over-approximation. 3. Inter-procedural \nanalysis Our inter-procedural analysis algorithm is based on the one used in Xylem. Their analysis, in \nturn, is based on Sharir and Pnueli s tabulation based approach to inter-procedural analysis [18], with \nan important modi.cation: when a call\u00adsite is encountered, the analysis proceeds to analyze the callee \n(together with all its transitive callees) to completion before proceeding with the analysis of the caller. \nIn other words, a depth-.rst propagation strategy is followed, rather than a chaotic strategy or a breadth-.rst \nstrategy (we dis\u00adcuss later the advantages offered by this strategy). Xylem s algorithm is sound and \nfully context-sensitive in the absence of recursion, but is unsound in the presence of recursion (it \nskips recursive calls). We .rst discuss our basic approach (ignoring recursion) below, and then discuss \nin Section 3.1 how we handle recursion by iterating for a .x-point. Figure 6 shows our inter-procedural \nanalysis procedure. The arguments to this procedure are a method m, a state\u00adment stmt in m, and a post-condition \nfstmt. Let p be the program point after statement stmt. The return value from this procedure is a set \nof disjuncts, which represents an over\u00adapproximation of wp1(p, fstmt) at the entry of method m. The procedure \nshown in Figure 6 uses two globally scoped data structures viz. a stack CS and a summary ta\u00adble S. The \nstack CS, as well as lines 6, 12-13, and 33-36 in the .gure are about handling recursion, and are discussed \nin Section 3.1. We maintain for each method m a summary table S[[m] : Disjunct .P(Disjunct), which is \na par\u00adtial map from a disjunct at the exit of m to the set of dis\u00adjuncts that would result at the entry \nof m after propagating the disjunct through the method (and its transitive callees). 1: Procedure wpWrtMethod(m, \nstmt,fstmt ) 2: worklist W = {(stmt,fstmt )} 3: result = \u00d8{A set of disjuncts} 4: if stmt = exit(m) then \n5: update S[[m]][fstmt . emptyset] 6: push(CS,(m, fstmt )) 7: end if 8: while W = \u00d8 do 9: Select (S, \nf) . W {f is a post-cond. after stmt S} 10: output = \u00d8{A set of disjuncts} 11: if S is a call instruction \nvr = c(v1,v2,...,vn) then 12: if (c, f) . CS then 13: output = S[[c]](f) 14: else 15: if S[[c]](f) is \nde.ned then 16: output = S[[c]](f) {Summary hit} 17: else {Summary miss} 18: output = wpWrtMethod(c, \nexit(c),f) 19: end if 20: end if 21: else if S is Entry then 22: Add f to result 23: else 24: output \n= fS(f) 25: end if 26: for all Predecessor Statement SP of S do 27: for all f ' . output do 28: W = W \n.{(SP ,f ')} 29: end for 30: end for 31: end while 32: if stmt = exit(m) then 33: pop(CS) 34: if (S[[m]](fstmt \n)= result) then 35: update S[[m]][fstmt . result] 36: result = wpWrtMethod(m, stmt,fstmt ) 37: else 38: \nupdate S[[m]][fstmt . result] 39: end if 40: end if 41: return result Figure 6. Computing wp1(stmt,fstmt \n) at the entry of method m for post-cond. fstmt at point after stmt in m. The algorithm uses a worklist \nW to propagate a disjunct intra-procedurally through the statements in the method to the entry of the \nmethod. Each element in the worklist is a pair (S, f), where f is post-condition at the the program point \naf\u00adter the statement S. The lines 2 5 in Figure 6 performs the initialization of the worklist, summary \ntable and a variable result that is used to store the precondition computed at the entry of the method. \nAs shown in lines 8 31, the algorithm iteratively processes  1: Procedure wpWrtPgm(m, stmt,fstmt ) 2: \npropagated = propagated . (stmt,fstmt ) 3: result = wpWrtMethod(m, stmt,fstmt ) 4: precond = \u00d8 5: if \n. a predecessor for m then 6: for all disjunct d ' in result do ' 7: for all callsite s of m in predecessor \np of m do 8: if (s ' ,d ') . propagated then 9: precond = precond . wpWrtPgm(p, s '' ,d '), '' ' 10: \nwhere s is the statement that precedes s 11: end if 12: end for 13: end for 14: else 15: precond = result \n16: end if 17: return precond 18: Procedure analyzeDeref (m, stmt, ap) 19: set CS, propagated to empty \n20: set all entries in S to unde.ned 21: result = wpWrtPgm(m, stmt, ap = null) 22: if result is the empty \nset then 23: return safe 24: else 25: return potentially unsafe 26: end if Figure 7. wpWrtPgm: Computing \nwp1(stmt,fstmt ) at the entry of the program for post-condition fstmt just after statement stmt in m. \nanalyzeDeref : Computing wp1(stmt, ap = null) at the entry of the program, where ap is an access path \nin stmt. the elements in the worklist. For each element (S, f) in the worklist, where S is a statement \nand f is a post-condition after S, the algorithm computes the pre-condition (as a set of disjuncts) at \nthe point before the statement S and stores it in a variable output (lines 10 25). For every statement \nS other than a call statement and a method entry statement, the intra-procedural transfer function fS \nas de.ned in Sec\u00adtion 2.2 is used to compute the pre-condition before the statement S (line 24). If S \nis a call statement of the form vr = c(v1,...,vn), the procedure wpWrtMethod is recur\u00adsively invoked \n(at line 18) to process the callee c with the post-condition f only if there is no result in the summary \nta\u00adble for (c, f) (this check is done in line 15). Note that when we analyze a callee (in line 18) we \nneed to map (i.e., replace) the occurrences of the actual parameters (at the call-site) in the access \npaths in f with the corresponding formal param\u00adeters, and similarly unmap the access paths in the condition \nthat was returned from the callee analysis (i.e., in output). This mapping and unmapping is straightforward, \nso we omit the details. The pre-condition output computed before the statement S is the post-condition \nfor each predecessor SP of S. Hence, for each disjunct f ' . output (SP ,f ') is added to the worklist \nW so that it would be processed in the subse\u00adquent iterations (this is done in lines 26 30). Finally, \nonce the pre-condition result is computed at the entry of the method, the summary table S is updated \nto map the input post-condition fstmt to the precondition result (at line 38) provided the input post-condition \nwas for the exit statement of the method m (this check is done at line 32). Procedure wpWrtPgm in Figure \n7 has the same signature as procedure wpWrtMethod, but computes the weakest at\u00adleast-once pre-condition \nof fstmt (at the point after stmt in method m) at the entry of the entire program, rather than at the \nentry of m. It does so by .rst computing the pre-condition at the entry of m (see call to wpWrtMethod \nin line 3), and then propagating the disjuncts in this pre\u00adcondition through all predecessors (i.e., \ntransitive callers) of m until the disjuncts reach the entry of the program (see tail\u00adrecursive call \nin wpWrtPgm to itself in line 9). The variable propagated is a global variable, and is used to remember \nthe facts that were propagated up to the various call-sites, in order to ensure termination. Procedure \nanalyzeDeref is the main routine to analyze the safety of the dereference of a given access path ap at \na point after statement stmt of method m. It is basically a wrapper around procedure wpWrtPgm. The depth-.rst \napproach described above (which was fol\u00adlowed in Xylem, too) has the advantage that if a disjunct be\u00adcomes \nvalidated, i.e., becomes true at some point or reaches the program s entry while being satis.able, then \nwe termi\u00adnate the analysis (and call the root dereference unsafe). If we adopted a chaotic or breadth-.rst \napproach it would poten\u00adtially take much more analysis time before any single dis\u00adjunct is propagated \ndeep enough to become validated. An\u00adother key advantage of our method is that it is space ef.cient. We \nconstruct a control-.ow graph of a method only on de\u00admand, i.e., when we encounter a call to the method. \nAlso, once we have analyzed this method (which is a callee), the memory allocated to it for storing the \ndata-.ow facts (or dis\u00adjuncts) at every program point, the intra-procedural worklist, etc., can be freed \nup. (However, as in the tabulation based approach, we re-analyze a method when it is re-encountered with \na different data-.ow fact at the exit.) Similarly, in pro\u00adcedure wpWrtPgm, we can de-allocate memory \nused for an\u00adalyzing any method m before we go onto analyzing its pre\u00addecessors. 3.1 Handling recursion \nby iterating for a .x-point We identify recursive calls during the analysis using a context-stack CS, \nwhose entries are pairs of the form (c, f); the presence of such an entry in the stack means that an \nanalysis of method c with post-condition f (at the method s exit) is ongoing and not yet completed. Given \nthis, if we encounter another call to c with the same post-condition f, as checked for in line 12 in \nFigure 6, we have detected recursion; we then pick the (potentially intermediate non\u00adFigure 8. Bottom-up \nanalysis of recursive method f with post-condition b = null, done repeatedly until .x-point. Each iteration \ni uses summary table computed in previous iteration (see top row in iteration i - 1) to analyze call \nat line 4. Each entry shows formula at the program point that precedes the corresponding statement. \n S[[f]](b = null) = false b = null b = null . b = null . a = null a = null 1: 2: f(b,a) {if (*) { \nb = null b = null . a = null b = null . a = null 3: b = a; false a = null a = null 4: f(b,a); false \nb = null b = null . a = null 5: 6: }}Iteration: b = null b = null 1 b = null b = null 2 b = null b = \nnull 3 .x-point) result S[[c]][f] from the summary table (instead of starting a re-analysis of c), complete \nthe analysis of the caller (i.e., method m, with post-state fstmt), and then re\u00adanalyze the caller in \norder to reach a .x point (see lines 33 36). In other words, if during the analysis of m with post\u00adcondition \nfstmt we detect a recursive call, we iteratively analyze m with the same post-condition until S[[m]](fstmt) \nreaches a .x-point. During every iteration i = 1, we use S[[m]](fstmt) computed in the iteration i - \n1, which we de\u00adnote here as S[[m] i-1(fstmt)), to compute S[[m] i(fstmt) (S[[m] 0(fstmt) being false). \nWe repeat this process until S[[m] i-1(fstmt) = S[ m] i(fstmt) for some i = 1. We illustrate the analysis \nof the recursive method f in Fig\u00adure 8 with post-condition b = null at the exit of the method. In each \niteration of the analysis, to the right of each state\u00adment of code, we show the formula computed at the \npoint just above the statement. Consider Iteration 1, where we propagate b = null upwards from the exit. \nWhen we en\u00adcounter the recursive call in line 4, we pick up the value of S[[f]](b = null) from the initial \nsummary table, which is false (see the top of the .gure, above the program). There\u00adfore, the disjunct \nthat reaches the top of the method is false, via the true branch of the if , and b = null (the post\u00adcondition \nat the end of the method), via the false branch. The disjunction of these two facts is b = null. Therefore, \nwe update S[[f]](b = null) to b = null (as shown at the top of the .gure along Iteration 1), and start \nIteration 2 from the bottom. This time, at line 4, since the post-state is the same (b = null), we pick \nup the value b = null from S[[f]](b = null) (as updated in the previous iteration). This gets transformed \nto a = null after propagation through b = a; . Therefore, the fact at the beginning of the method be\u00adcomes \nb = null . a = null, where b = null came, as explained before, via the false branch. This fact is updated \ninto the summary table (as shown at the top of the .gure, along Iteration 2), and is looked up and used \nat line 4 in It\u00aderation 3. When this fact .ows through b = a it becomes a = null . a = null, which simpli.es \nto a = null. There\u00adfore, a .x point is reached. The .nally computed weakest at-once pre-condition at \nthe beginning of method f for the post-condition b = null is b = null . a = null, which, in this case, \nturns out be the precise solution.  3.2 Optimizations Using a precomputed side-effects analysis. During \nthe analysis of a method m if we encounter a method call c with a disjunct f as the post-state, we propagate \nthe dis\u00adjunct f to the entry of c (if the summary for f is not available). However, if the information \nabout the set of all access-paths in f that may be modi.ed by c (after map\u00adping) is available then we \ncan partition the disjunct f into f1 . f2 such that none of the access-paths in f2 (or their aliases) \nare modi.ed by c. By the frame rule of the separation logic [16], wp1(m, f)= wp1(m, f1) . f2. Hence, \nan over\u00adapproximation of wp1(m, f) could be computed by over\u00adapproximating wp1(m, f1) and conjuncting \neach resulting disjunct it with f2. We consider all the access-paths that uses the return value of c \nas modi.ed by c. This approach has two advantages, (a) it reduces the amount of work that has to be done \ninside the method m (and all its transitive callees) (b) it increases the summary hits. In our implementation, \nwe used an inexpensive ModRef (or side-effect) analysis that was available in our program analysis framework, \nWala. Increasing summary hits by reusing summaries for weaker disjuncts. In cases where we do not have \na mapping for a disjunct f1 in the summary table, but have a map\u00adping for a disjunct f2 such that f1 \n. f2 (i.e., f1 im\u00adplies f2), instead of analyzing the method m with f1 as post-condition we simply pick \nup and use S[[m]](f2). Since wp1(m, f1) . wp1(m, f2)), if f1 implies f2 then any over-approximation of \nwp1(m, f2) is a correct over\u00adapproximation of wp1(m, f1). However, in order to mini\u00admize the loss of \nprecision that this may entail, we use this heuristic only if the root predicate in f1 is the same as \nthe root predicate in f2. We found this approach pragmatically effective. 4. Implementation details \n4.1 Analysis framework We have implemented our approach using the Wala [21] program analysis framework. \nWala provides us control\u00ad.ow graphs of methods on demand, as well as points-to (i.e., may-alias) information \n(which it pre-computes). From among the various points-to analysis methods Wala offers, we selected a \n.ow-insensitive, partially context-sensitive method (namely, ReceiverTypeContextSelector). Our approach \nuses Wala s points-to analysis results for three purposes: (a) to construct a call-graph (particularly, \nto re\u00adsolve virtual method calls), (b) to compute Mod-Ref sets (i.e., side-effect information) for methods, \n(c) in the PUT-FIELD rule, to reduce the number of aliasing combinations that have to be considered (see \nFigure 3). Imprecision in Wala s points-to analysis affects the precision of our ap\u00adproach due to points \n(a) and (b) above, and affects the scal\u00adability of our approach due to all three points above. Note that \npoint (c) does not affect our precision, due to our precise modeling of aliasing relations (see Section \n2.1.2). Therefore, our approach would signi.cantly bene.t from a more pre\u00adcise points-to analysis. However, \ndue to scalability limita\u00adtions of Wala s points-to analysis implementations, we chose a points-to analysis \nmethod that is reasonably precise but highly scalable.  4.2 Balancing scalability and precision There \nare several idioms in real-world Java programs that pose severe challenges to any technique that wishes \nto per\u00adform veri.cation scalably and precisely. We explore some of these idioms, and discuss the engineering \ndecisions we have taken wrt giving up precision in certain situations wherein a precise analysis would \nhave turned out to be impractically expensive. Issue 1: Call-backs from the library. In Java, several \nmethods like equals, hashCode, toString de.ned in the ap\u00adplication classes are invoked by library methods; \nsuch calls are generally referred to as call backs. For e.g., Hash\u00adSet::Contains invokes the equals method \non the elements of a hash set, which could be objects of application classes. Given a root dereference \ninside a called-back method, such as equals, it is often very expensive to propagate the dis\u00adjuncts that \nreach the entry of this method back through all its caller chains (via library code) to the entry of \nthe program, as there may be numerous (transitive) call sites for this meth\u00adods inside the library and \nin the application. In fact, going through all paths back from called-back methods would re\u00adsult in many \nspurious paths; e.g., a path back from method A::Equals through HashSet::Contains can only reach points \nin the application where HashSet::Contains is being called on a hash-set that contains objects of type \nA. Rather than an\u00adalyze all these paths, which increases running time without giving much precision gain, \nwe have chosen to always drop (i.e., reduce to true) a disjunct that needs to be propagated back from \nthe entry of a called-back method to a call-site to the same method that is inside a library method1. \nBy called\u00adback method , we mean a method that is not on the context stack when the analysis leaves the \nmethod, which implies that the root dereference is contained in the method or in one of its transitive \ncallees. Issue 2: Unbounded access paths, and arrays. In the pres\u00adence of recursive data-structures the \nlength of an access path (used in a predicate) can become unbounded, leading to non\u00ad 1 with one exception \n when the library method is Thread.start termination. In our analysis, we ensure that the .elds in an \naccess path do not repeat (see Section 2.1.1). Whenever an access-path in a predicate violates this property, \nwe drop the predicate (i.e., reduce it to true). A commonly mentioned alternative is to forcibly bound \nthe lengths of access paths using k-limiting [13, 14]. The problem with this approach is that two syntactically \nidentical k-limited access-paths ap1 and ap2 (at a programs point) need not necessarily point to the \nsame runtime object, and hence are not must-aliased. Therefore, strong updates become dif.cult to perform, \nim\u00adpacting precision. For an illustration of this, consider a put\u00ad.eld statement ap1.f = v, such that \nthe post-condition f af\u00adter this statement involves ap2.f. Even though ap1 = s ap2, we cannot simply \nreplace ap2.f in the f with v. Rather, we would need to blow up f, and produce two disjuncts, f[v/ap2.f].ap1 \n= ap2, and f.ap1 = ap2. The problem is that since ap1 and ap2 are k-limited, neither ap1 = ap2 nor ap1 \n= ap2 will be invalidated during further propagation. This means there is no chance of f getting invalidated, \nim\u00adplying imprecision. Inferring must-alias relationships in the presence of recursively de.ned data \nstructures needs sophis\u00adticated shape analysis [17], which in its current state of evo\u00adlution is unlikely \nto scale to programs of sizes that we are in\u00adterested in. Hence our decision to drop predicates involving \naccess paths with repeated .elds. Note that an advantage of our approach over k-limiting is that we do \nallow access path lengths to grow without any apriori constant length bound, as long as .elds do not \nrepeat. Therefore, for low values of k, our approach is likely to be actually more precise than k-limiting. \nSimilar to predicates with unbounded access-paths, we also drop predicates containing array accesses. \nSince array elements are implicitly initialized to null on creation, un\u00adless strong updates are performed \non array elements, a pred\u00adicate of the form (arrayap = null), where arrayap con\u00adtains an array access, \nwill eventually become true. There do exist techniques, e.g., that of Dillig et al [7], which pre\u00adcisely \nmodel integer arithmetic, and perform strong updates on array operations. However, it is not clear how \nsuch tech\u00adniques can be adapted in a demand-driven setting like ours for analysing real world Java programs. \nIssue 3: Too many AP op null predicates in disjuncts Note that the ASSUME, GETFIELD, and PUTFIELD rules \nconjunct predicates of the form AP op null to disjuncts, where op is = or = . Along long paths the average \nnum\u00adber of these predicates per disjunct increases a lot, without contributing proportionally to improved \nprecision. Our ob\u00adservation was that among the numerous conditional state\u00adments that are typically encountered \nalong a path that a dis\u00adjunct propagates through, the ones that refer to the same object as the object \nreferred to by the root predicate of the disjunct are the ones that usually correlate with whether the \ndisjunct gets validated or invalidated. Therefore, we limit the addition of AP op null predicates to \nthe following situa\u00adtions: In a statement ASSUME b we conjunct b to the dis\u00adjunct f ' only if the access \npath in b is may-aliased with the access path in the root predicate in f at the point after the assume \nstatement. In GETFIELD and PUTFIELD statements we conjunct r = null into f ' only if r is may-aliased \nwith the access path in the root predicate in f at the point after the statement, where r is the variable \nbeing dereferenced in the statement.  Issue 4: Virtual method calls. As mentioned earlier, when encountering \na virtual method call we query Wala s moderate-precision points-to analysis to .nd its possible tar\u00adgets. \nIn the programs that we analyzed there do exist virtual method call sites with tens or hundreds of targets, \nanalysing all of which is practically infeasible. Hence, when we en\u00adcounter a virtual call having more \ntargets than a prede.ned bound (which is 10, in our implementation), we do not an\u00adalyze any of the targets. \nInstead, we drop all predicates that make use of the return value from the method, and also all predicates \nthat contain access paths that are aliased with any of the access paths mutated by any of the targets \nof the call (as indicated by Wala s Mod-Ref information), and continue the analysis above the call. Using \na more precise points-to analysis would mitigate the problem with virtual calls, but it would not scale \nwell. The idea of directed call-graph con\u00adstruction [3] would also yield bene.t in this situation, but \nits scalability in our context is not yet clear. Issue 5: Missing call targets. It is possible that our \nanaly\u00adsis encounters virtual method calls that do not have any tar\u00adgets. This may happen if the target \nof a method call is de\u00ad.ned in a class that is not available to the analysis. In most cases, these method \ncalls are calls to the GUI and JDBC li\u00adbraries, which we do not link, as it affects the scalability of \nWala s points-to analysis. We treat such method calls con\u00adservatively. Predicates that use the return \nvalue of method calls that do not have a target are dropped. We also over\u00adapproximate the side-effects \nof such a method call by as\u00adsuming that it may modify every object reachable from the arguments passed \nto it. We use this over-approximate side\u00adeffects information to drop predicates that may be modi.ed by \nthe method call. Issue 6: Library calls. In our experiments, we found that analyzing every library method \ncall adversely affects the scalability. Hence, we analyze a library method only if the value returned \nby the library method is used in an access\u00adpath. This is the case wherein we feel the effort of the analysis \nis balanced by signi.cant improvement in precision. If a library method mutates (writes to) an access-path \n(or its alias) used in a predicate in the post-state (as per Wala s Mod-Ref analysis), we drop the predicate. \nThis said, we add two features to our approach to add back some of the precision that was given up by \nthe above\u00admentioned heuristic. We manually created a list (which we refer to as the skip-list) of library \nmethods that have no ex- Benchmark Description jlex 1.2.6 Lexical analyzer generator javacup 0.1 Parser \ngenerator ourtunes 1.3.3 ITunes browser jbidwatcher Online Auctioning system 2.1.2 bcel 5.2 Libraries \nfor bytecode manipulation antlr 3.3 Compiler &#38; translator generator sablecc 4.2 OO Frameworks generator \nproguard 4.5 Code optimizer &#38; obfuscator freecol 0.9.5 multi-player game l2j 3.7 Multi-player game \nserver Figure 9. Benchmarks used in the study ternally visible side effect as per their documentation, \nbut that are nonetheless declared by Wala s imprecise Mod-Ref analysis as potentially having side effects; \nduring the anal\u00adysis, for methods in this list, we treat their Mod-Ref sets as empty. Conversely, we \nfound that there are several li\u00adbrary methods that are called primarily for their side-effects, as opposed \nto their return values. We therefore created an analyze-list containing such methods, and always propagate \nto them predicates that might be affected by them (as per the Mod-Ref information). We use these two \nlists (which are dis\u00adjoint) not only with direct calls to library methods, but also when library methods \nare potential targets of virtual calls. Currently our skip-list and analyze-list contain 136 and 84 library \nmethods, respectively. It is noteworthy that manually creating these lists, although time consuming, \nis worthwhile because the lists can be reused during analysis of any pro\u00adgram. To give maximum bene.t, \nhowever, it is likely that a lot more methods could be added to both these lists. 5. Experimental Results \nFigure 9 shows the programs used in our empirical study. Several of these programs are commonly used \nto evaluate the ef.ciency of the program analysis tools for Java. In fact, 8 out of the 10 benchmarks \nused in the study were picked from the Salsa paper [13], which also proposes a null-dereference veri.cation \ntechnique for Java programs. In Wala, to construct a call-graph for a program, one or more entry methods \nfor the program (called entry-points) have to be speci.ed. All the benchmarks reported had at least one \nmain method which was used by our implementation as an entry-point for constructing the call-graph. For \nprograms that had more than one main method we included all of them. However, we ignored the main methods \nin the test-suites that are distributed along with these benchmarks. We analyzed the entire portion of \neach program that is reachable from the entry points, including library methods called from these portions \n(see the discussion in Section 4.2). We carried out all our experiments using Open JDK 1.6 libraries, \non a server machine having 2.27 GHz, 8 core Intel Xeon processor, 16 GB RAM, running CentOS linux operating \nsystem. Our implementation is single threaded.  Benchmarks Lib. bytecode App. bytecode Preprocessing \ntime (s) analysis time (s) # of derefs (excluding derefs of this) Unsafe derefs % deref veri.ed jlex \njavacup bcel jbidwatcher sablecc ourtunes proguard antlr freecol l2j 408 509 3596 16381 5409 7317 2766 \n7224 8909 14816 25056 29180 86483 105043 157169 127167 185594 251010 260785 373661 6 7 13 25 26 16 24 \n24 53 34 13 19 63 390 252 227 207 42294 5291 545 2510 2851 10143 9643 14017 16449 17736 17409 24077 36899 \n93 607 1184 1480 2116 1393 2767 4043 6986 5429 96.3% 78.7% 88.3% 84.7% 84.9% 91.5% 84.4% 76.8% 70.9% \n85.3% Avg = 84.2% Figure 10. Results of analysing the benchmarks shown in Figure 9  Figure 10 gives \nthe overview of our experiments. We run our backward analysis separately on each dereference in the portion \nof the application that was analyzed (exclud\u00ading dereferences inside library methods). The second and \nthird columns, show the total number of bytecodes in li\u00adbrary methods and application methods, respectively, \nthat were ever entered during the analysis of any dereference. The preprocessing time is the time taken \nby Wala s points\u00adto analysis, call-graph construction and Mod-Ref analysis phases, which we perform once \nbefore we analyze any of the dereferences. The analysis time reported is the total time taken for analysing \nall the dereferences in the program. The next two columns show the total number of dereferences that \nwere analyzed, and the number among them that were found potentially unsafe. The last column is the percentage \nof dereferences that were found safe. It is to be noted that other than the skip list and analyze list \n(for standard Java li\u00adbrary methods) that we create manually, there are no other manual inputs to our \ntool. For the benchmarks used in the study the peak memory utilized by the analysis was in the range \n350MB (for jlex) to 2013 MB (for antlr). The results show that on an average the tool classi.es about \n84.2% of the dereferences as safe and the remaining 15.8% of dereferences as unsafe. For seven out of \nten pro\u00adgrams, less than 16% of the dereferences were reported un\u00adsafe, while for the remaining three \nprograms (viz. javacup, freecol, antlr) between 20-30% of the dereferences were de\u00adclared unsafe. In \norder to put our numbers in context, we did a quick search on the bugzillas of the 10 programs we analyzed; \nthis revealed a total of 412 null dereference reports for the program l2j, and a total of 67 reports \nfor the other programs. These numbers indicate that our approach probably has a high false positive rate \n(as do previous approaches). It is possible, however, to mitigate the problem of false positives by .ltering \nand prioritizing bug reports in various ways, e.g., as in Xylem [15] and FindBugs. Also, as discussed \nin detail later, we propose that a dereference be marked as high priority if there is a static, inter-procedurally \nvalid path that connects a null assignment statement to dereference. In fact, we .nd that fewer than \n20% of our bug reports fall in this category. Our search of the bugzillas yields a separate observation, \nthat null-dereference errors are a real problem commonly encountered during program usage, and worthy \nof the consideration of researchers in veri.cation. 5.1 Categorization of error reports To understand \nthe reasons for large number of error reports on some benchmarks, we categorized the unsafe derefer\u00adences \nbased on the reasons for the root predicate becoming true during the analysis (see Section 2.1 for the \nde.nition of root predicate). Figure 11 shows the 6 categories used in the classi.cation. A single dereference \ncan go into multiple categories as several disjuncts may be introduced at various program points during \nthe analysis of a dereference, with each of them becoming true for a different reason. The cate\u00adgories \ncover the most important reasons for imprecision but not every possible reason. In other words, some \ndereferences may not fall into any of the categories. For these reasons, the sum of the percentages across \nany row may be less than or more than 100%. We .rst focus on the category unbounded access-paths shown \nin Figure 11. An unsafe dereference falls in this category if during its analysis some disjunct happened \nto contain a root predicate that was dropped because it had an access path with a repeating .eld or a \nreference to an element of an array. The .gure shows that on an average 40% (max. 76% and min. 11%) of \ndereferences reported as unsafe by our analysis fall under this category, implying that recursive data \nstructures and array accesses are ubiquitous in Java programs. However, handling these in a precise and \nscalable way is quite dif.cult.  Benchmarks call-backs missing-targets virtual-calls library calls unbounded-aps \nnull-assignment jlex 10% 1% 0% 0% 50% 27% javacup 3% 0% 0% 0% 76% 2% bcel 9% 1% 17% 0% 56% 25% jbidwatcher \n36% 19% 27% 1% 17% 21% sablecc 6% 5% 6% 12% 22% 14% ourtunes 3% 1% 0% 5% 68% 8% proguard 7% 0% 38% 0% \n57% 36% antlr 9% 14% 51% 52% 20% 23% freecol 4% 16% 20% 44% 13% 17% l2j 17% 36% 0% 3% 11% 21% Average \n10.4% 9.3% 15.9% 11.7% 40% 19.4% Figure 11. Percentage of unsafe dereferences in the various categories \n The Null-assignment category includes all the derefer\u00adences, in whose analysis, a disjunct containing \na root pred\u00adicate was propagated through a null assignment statement that replaced the access-path in \nthe root predicate by null. For instance, say during the analysis of dereference r at a program point \np, a disjunct <v.f = null > is propagated thor\u00adough the statement S : v.f = null. In this case, v.f would \nbe replaced by a null in the resulting formulae and hence the dereference of r at p would be included \nin the null\u00adassignment category. For the dereferences that fall under this category (which is around \n19%) there exist at least one static path along which a null value .ows to the root dereference. Hence, \nthey are more likely to be true positives (they could still include false-positives because of infeasible \npaths not caught by our limited path-sensitivity). It is an encourag\u00ading sign that this percentage is \nhigh (second highest among the categories) as it implies that for 19% of unsafe deref\u00aderences the analysis \nis able .nd one static inter-procedurally valid path (which is also to some extent path-sensitive) along \nwhich a null value .ows to the point of dereference. We now consider the categories virtual calls and \nmiss\u00ading targets that arise mainly due to the imprecision in the call-graph. A dereference falls into \nthe virtual calls category if during its analysis some disjunct containing a root predi\u00adcate had to be \npropagated though a virtual method call with more than 10 targets, at least one of which modi.es the \nac\u00adcess path in the root predicate. Similarly, a dereference falls into the missing targets category \nif during its analysis some disjunct containing the root predicate had to be propagated though a method \ncall with no targets (which are mostly calls to GUI libraries and JDBC libraries). In these situations \nwe stop the analysis of the current dereference and call it unsafe (see Section 4.2). Figure 11 show \nthat, on an average, a sig\u00adni.cant percentage of unsafe dereferences, namely around 16% and 9%, fall \nunder the two categories, respectively. It is to be noted that in programs freecol and antlr for which \nour analysis generates many error reports, the number of deref\u00aderences that fall under one of the two \ncategories is quite high (around 52% in antlr and 20% in freecol) implying that the large number of error \nreports are more likely the result of an imprecise call-graph. We now discuss the categories call-backs \nand library calls that arise due to the complex interactions between the application and the library \ncode, as explained in Section 4.2. A dereference falls into one of these categories, respectively, if \nduring its analysis some disjunct containing the root pred\u00adicate (a) reached the entry of a method that \nis called-back by a library method or (b) had to be propagated though a li\u00adbrary method that has a side-effect \non the access-path used in the root predicate. Figure 11 indicates that a signi.cant number of error \nreports fall into these two categories in all benchmarks (particularly, in antlr, freecol and jbidwatcher). \nLater, in Section 5.4.2, we present an empirical evaluation of the impact of analysing all library methods \nand call-backs on the scalability the analysis.  5.2 Evaluation of the scalability of the analysis We \nnow present some metrics that highlights the scalability of our analysis. The most important measure \nof the scalabil\u00adity of a demand-driven analysis is the response time, which is the time taken by the \nanalysis to report a dereference as safe or unsafe. The analysis is very ef.cient on the 8 pro\u00adgrams \nother than antlr and freecol. In each of these 8 pro\u00adgrams approximately 98% of dereferences take up \nto 250 ms each, while the rest take up to 14 seconds each. In the case of freecol, around 93% of dereferences \ntake up to 250 ms each, while the rest take upto 51 seconds each. In antlr, 85% of dereferences take \nup to 250 ms each, 14.3% take up to 100 seconds each, while the remaining 0.7% take more than 100s and \nare timed out. We believe that the very low response time of our analysis makes it more suitable for \nuse in a interactive development environment. In the later part of the section, we illustrate that in \nspite of having a very low response time our analysis is able to prove many complicated dereferences \nas safe by traversing long inter-procedural paths.  In our analysis there are three important factors \nthat af\u00adfect the scalability of the analysis wrt both time and memory, viz. summary hits/miss, number \nof disjuncts and the size of disjuncts in the DNF formulae computed after each propa\u00adgation. We now present \na few metrics that provides insight into these three factors. Figure 12 shows how the average disjunct \nsize (averaged over all the disjuncts that arose at all program points) varies across the analyses of \nthe deref\u00aderences for each of the benchmarks. Each point (x,y) in the graph means that in y percentage \nof dereferences the aver\u00adage disjunct size was less than or equal to x. The disjunct sizes can be less \nthan one, because in our implementation the empty disjunct (disjunct with no predicates) models log\u00adical \ntruth. From Figure 12, it can be seen that almost 100% of the dereferences have the average disjunct \nsize less than or equal to 2 (across all benchmarks). It is to be noted that the maximum disjunct size \ncan only be 4 as we bound the dis\u00adjunct sizes. However, Figure 12 illustrates that at least 90% of all \nthe dereferences in each program have average dis\u00adjunct size less than or equal to 1.5. This is because \n(a) We do not include all the branch conditions encountered during a propagation of the disjunct into \nthe disjunct (as discussed in Section 4.2, Issue 3), (b) We drop a predicate from a dis\u00adjunct after it \nhas been propagated through 1000 statements (see Section 2.1.2). Later, in Section 5.4.1, we show that \nin spite of having a very low disjunct size we are able to invali\u00addate many unsatis.able paths compared \nto a path-insensitive analysis. We now discuss the metric average number of disjuncts per propagation, \nwhich refers to the ratio of the total num\u00adber of disjuncts that were generated at all program points \nduring the analysis of a dereference to the total number of propagations performed by the analysis (each \napplication of a transfer function in Figure 3 is regarded as a propa- Figure 13. Summary miss percentages \ngation). It is only when a disjunct is propagated through a PUTFIELD statement that the number of disjuncts \nthat re\u00adsults after the propagation can become more than one. As discussed in Section 2.1.2, after every \nPUTFIELD rule po\u00adtentially 2k disjuncts can be created, where k is total number of (unique) pre.xes of \nthe access-paths in the input disjunct. In our analysis, when a propagation through a statement re\u00adsults \nin a disjunct becoming false we stop its propagation. Hence, whenever a propagation invalidates an incoming \ndis\u00adjunct, the number of disjuncts that results after the propaga\u00adtion is considered zero. For these \nreasons, the average num\u00adber of disjuncts per propagation could be less than 1. We .nd that in all our \nbenchmarks, for about 99% of the deref\u00aderences the average number of disjuncts per propagation is between \n0.8 and 1, which indicates that the blow-up due to the PUTFILED rule is insigni.cant; i.e, most of the \ndisjuncts that are added by the PUTFIELD rule get invalidated after a few propagations. Figure 13 shows \nhow the summary miss percentage varies across the dereference analyses for each of the benchmarks. We \nde.ne summary miss percentage as the percentage ra\u00adtio of the total number of times the summary lookups \nfailed (see Line 18 in Figure 6) to the total number of times the summary lookups were performed during \nan analysis (the condition in Line 15). We do not count the summary lookup that happens the .rst time \na method is encountered during the analysis of a dereference as it would be a compulsory miss. It is \nto be noted that we do not share the summaries across analyses of different dereferences, even within \nthe same program. Figure 13 shows that in each of the bench\u00admarks (except bcel), for about 45% of all \nthe dereferences in the benchmark the summary miss ratio of the analysis is less than or equal to 30%. \n  (a) large programs (b) small programs 5.3 Evaluation of the complexity of the dereferences reported \nas safe In this section we present some metrics that highlights the characteristics of the safe dereferences \nidenti.ed by our analysis. An advantage of our analysis is that it can propagate dis\u00adjuncts over long \ninter-procedural paths. To measure the ex\u00adtent of the inter-procedural propagation our analysis per\u00adformed, \nwe use two metrics (a) propagation count (b) context-depth. We use the term propagation count to refer \nto the maximum length of the path along which a disjunct is propagated during the analysis of a dereference. \nA summary hit is counted as a single propagation. Figure 14 shows for each benchmark, the number of safe \ndereferences belonging to each of the six propagation count ranges shown along the x-axis. We limit the \nheights of the bars to 500, and show the actual height of each truncated bar on top of the bar. There \nare 177 (ourtunes) to 873 (l2j) safe dereferences in all the .ve larger programs that have a propagation \ncount > 50 (see Part (a) of the .gure). l2j and antlr have 269 and 138 safe dereferences with propagation \ncount > 400. Interestingly, in l2j, the maximum propagation count was 5419 indicating that a dereference \nwas proven safe after exploring a path of length 5419. Part (b) of the .gure shows a similar trend for \nall the .ve smaller programs. In these programs (not count\u00ading javacup) there are 109 (bcel) to 287 (sablecc) \nsafe deref\u00aderences with propagation count > 50. The results clearly illustrate that our analysis is able \nto prove signi.cant number of dereferences safe by exploring long paths. It is to be noted that, as mentioned \nin Section 5.2 our analysis has a very low response time in spite of travers\u00ading long paths. We now present \na metric context-depth that captures the maximum length of the call chain (measured from the method containing \nthe dereference) that had to be explored by the analysis to prove a dereference safe. Formally, we de.ne \ncontext depth of a dereference as the sum of the max\u00adimum number of entries in the context stack (at \nany point in the analysis of the dereference) used in the our inter\u00adprocedural analysis algorithm (see \nFigure 6) plus the num\u00adber of methods that were exited through their entry state\u00adments but were not previously \nentered through their exit statements (these are the transitive callers of the method that contains the \nroot dereference). For every dereference the context depth will be at least 1 by the above de.ni\u00adtion. \nFigure 15(a) shows that for 3 out of 5 programs (viz. l2j, antlr and freecol) there are 882 (antlr) to \n1165 (freecol) safe dereferences with context depth = 3. For two programs, namely, l2j and antlr, 110 \nand 150 dereferences respectively are proven safe after exploring inter-procedural paths with context \ndepth > 10, indicating the need for a deep inter\u00adprocedural analysis for these dereferences. In fact, \nit can be seen from the Figure 15(a) that there are dereferences with a context-depth of up to 160. Figure \n15(b) shows a simi\u00adlar trend for the .ve smaller programs. In four programs (viz.sablecc, javacup, jbidwatcher \nand bcel), there are 170 (jlex) to 766 (sablecc) safe dereferences with context depth = 3. In particular \nbcel and sablecc each have more than 700 dereferences with context depth = 3. The dereferences with high \ncontext depth are dif.\u00adcult to discover as safe through manual code analysis. The Figure 16 shows a code \nsnippet taken from the l2j benchmark that has a high context depth. In Fig\u00adure 16 the root dereference \nis the variable t at line number 31. The root predicate before the line num\u00adber 30 would be airship. \nposition = null. It has to be propagated through the constructors of L2AirshipInstance, L2Character, \nL2Object and then through the methods init-Position and setObjectPosition where it is proven to be safe. \n  (a) large programs (b) small programs 1 class L2Object { ObjectPosition _position 2 public L2Object(){ \n3 ... 4 initPosition(); 5} 6 public void initPosition(){ 7 ... 8 setObjectPosition(new CharPosition(this)) \n9} 10 public void setObjectPosition(...) { 11 _position = value 12 } 13 public ObjectPosition getPosition() \n{ 14 return _position; 15 } 16 } 17 class L2Character extends L2Object { 18 public L2Character() { 19 \nsuper() 20 } 21 } 22 class L2AirShipInstance extends L2Character { 23 public L2AirShipInstance() { 24 \nsuper() 25 } 26 } 27 public void L2AirShipInstanceParseLine() { 28 ... 29 airship = new L2AirShipInstance(..); \n30 t = airship.getPosition() 31 t.setHeading(); 32 ... 33 } Figure 16. A real world example with high \ncontext depth In this example the context depth is 6 which is the length of the call chain from L2AirshipInstanceParseLine \nto setO\u00adbjectPosition. Another interesting measure of the complexity involved in proving a dereference \nsafe is the length of the longest access-path encountered during the analysis of the derefer\u00adence. In \nour experiments we found that for four of our bench\u00admarks, namely, bcel, jlex, proguard, and antlr, around \n1% to 10% of the safe dereferences have maximum access-path length greater than 2. In particular, in \njlex some of the safe dereferences discovered by our analysis required access\u00adpaths of length up to 6. \nIt is to be noted that in Salsa [13] the access-path lengths are limited to 2. Our .nding illus\u00adtrates \nthat such a limit can prevent the analysis from proving a signi.cant number of dereferences as safe in \nsome bench\u00admarks.  5.4 Evaluation of optimizations As discussed in Section 4.2, we various optimizations \nand heuristics to make the analysis scale to real world Java pro\u00adgrams. In this section we measure the \nimpact of these op\u00adtimizations on precision/scalability through a series of ex\u00adperiments carried out \non our benchmarks. Due to the high running time and memory overhead in analysing freecol and antlr, we \nexclude them from this series of experiments. 5.4.1 Evaluation of limited path-sensitivity We evaluate \nour default analysis (with all our optimizations, including limited path sensitivity) that we used to \nreport all the results above by comparing it with two variants of our analysis (a) a variant that includes \nall branch conditions in the path (upto a speci.ed bound), referred to as mode(a) (b) a variant that \nincludes no branch conditions at all in the path (i.e. path insensitive), referred to as mode(b). In \nmode(a), which tracks all branch conditions, we enriched the predicate domain to include predicates involving \nboolean variables and integers. However we do not model integer arithmetic and track only <, >, = comparisons \non integers. We also added new simpli.cation rules to the simpli.er so that it takes into account the \nnew predicates (involving boolean variables and integers) during the simpli.cation of a disjunct. In \nfact, the domain and the simpli.ers we used are very similar to those used in Xylem [15]. We found that \ntracking all branches without any limit will not scale to even the smallest of our programs viz. jlex. \nHence, we bounded the disjunct sizes using the parameters k1 and k2 (see Section 2.1.2), which limit \nthe number of propagations of a predicate and the sizes of the disjuncts. We set k1 and k2 to the same \nvalues as in the default setting (which is 1000 and 3 respectively).  Figure 17 shows the number of \nunsafe reports and the time taken (excluding preprocessing time) for running the analysis on all the \ndereferences in each of the 3 different modes. In spite of having the bounds on the disjuncts, the version \nthat tracks all branches, mode(a), did not scale to many of the larger programs (which are not shown \nin Fig\u00adure 17) within reasonable time limits. Figure 17 also shows that the version without path\u00adsensitivity, \nmode(b), is very imprecise in comparison with the limited path-sensitivity mode. In fact, it even takes \nmore time than limited path-sensitivity mode (because the limited path-sensitive mode can avoid propagation \nthrough unsatis\u00ad.able paths thereby, reducing the running time). An interest\u00ading observation is that \nfor jlex, the precision of the mode(a) is actually less than that of the limited path-sensitivity mode. \nThis is because in mode(a) some of the relevant branch pred\u00adicates (conditions that can invalidate a \npath) get crowded out by less relevant predicates (due to bounds on disjunct sizes) before they come \nin useful to invalidate the disjunct.  5.4.2 Evaluation of limited library analysis As discussed in \nSection 4.2, Issue 6, we over-approximate the effect of a library method using its Mod-Ref information, \nexcept in the cases where its return value is used in an access-path. We also provide two manually created \nlists, viz. the skip and analyze lists, to the analysis to mitigate the loss of precision due to the \nabove approximation. Similarly, we also prevent the propagation of a disjunct reaching the entry of an \nanalyzed method through its caller if it happens to be a library method (referred to as library call-back). \nTo evaluate the precision/scalability trade-off in using this approach, we consider two variants of our \nanalysis (viz. mode(a) and mode(b)) as explained below, and compare them with our default analysis (all \noptimizations turned on). In mode(a) we allow the analysis to analyze all library methods that are found \nto have a side-effect on an access\u00adpath by the WALA s Mod-Ref analysis, and also allow a disjunct reaching \nthe entry of a method to be propagated through all its callers irrespective of whether they belong to \nthe library or not. In mode(b) we perform a limited library analysis but do not use the manually created \nskip/analyze lists. Note that the use of the analyze list can increase preci\u00adsion, but with extra cost \nin analysis time. The skip list neither increases or decreases the running time, impacting only pre\u00adcision. \nDefault setting w/o Weaker disjunct Summary reuse Benchmark Unsafe derefs Time(s) Unsafe derefs Time(s) \njlex 93 13 92 18 javacup 607 19 607 18 bcel 1184 63 1184 104 jbidwatcher 1480 390 1480 597 sablecc 2116 \n252 2116 275 ourtunes 1393 227 1393 228 proguard 2767 207 2767 204 l2j 5429 545 5429 550 Figure 19. Results \nof the analysis with and without the weaker disjunct summary reuse optimization  Figure 18 shows the \nresult of analysing the benchmarks in the three different modes. We use 8 in Figure 18 to denote that \nthe analysis of a benchmark did not complete within a reasonable time limit (which is more than twice \nthe analysis time of the limited library analysis mode). Figure 18 shows that mode(a) (in which all the \nlibraries having side-effects are analyzed) does not scale to many of the large benchmarks, clearly indicating \nthat analysing all library methods will not scale to real world applications. In mode(b) (in which analyze \nand skip lists are not used), the analysis time is signi.cantly lower than the limited li\u00adbrary analysis \nmode for some benchmarks like jbidwatcher. This is because mode(b) analyses even fewer library meth\u00adods \nthan the limited library analysis mode. However, it re\u00adsults in some loss of precision in half of the \nprograms shown in Figure 18. In fact, in ourtunes, mode(b) results in almost 5% increase in the number \nof reported unsafe dereferences compared to the limited library analysis mode.  5.4.3 Evaluation of \nweaker disjunct summary reuse optimization As mentioned in Section 3.2, in cases where we do not have \na mapping for a disjunct (say f1) in the summary table, but have a mapping for a weaker disjunct f2, \nwe reuse the summary computed for the weaker disjunct f2 provided the the root predicate (if any) in \nboth the disjuncts matches. Figure 19 shows the results of running the analysis, .rst in the default \nmode (i.e., with all optimizations), and then without the reuse optimization. It can be seen that for \nmost of the programs shown in Figure 19, the analysis time increases in the absence of this optimization \n(particular for bcel and jbidwatcher). Figure 19 also shows that this optimization has no negative impact \non the precision of the analysis (i.e, the number of error reports) in all programs, except jlex in which \nthe number of error reports increased by just 1.  Default setting Mode(a) Mode(b) Limited path sens.: \ntracks a few branches Tracks all branches Tracks no branches Benchmark Unsafe derefs Time(s) Unsafe derefs \nTime(s) Unsafe derefs Time(s) bcel 1184 63 1119 8123 2501 97 javacup 607 19 463 881 1036 29 jlex 93 13 \n105 148 296 33 Figure 17. Result of running the analysis with different path-sensitivity  Default setting \nLimited lib. analysis, with both lists Mode(a) Analyzes all libs Mode(b) limited lib. analysis, w/o the \nlists Benchmark Unsafe derefs Time(s) Unsafe derefs Time(s) Unsafe derefs Time(s) jlex 93 13 93 13 93 \n12 javacup 607 19 607 19 607 18 bcel 1184 63 1184 74 1184 63 jbidwatcher 1480 390 - 8 1483 103 sablecc \n2116 252 - 8 2116 239 ourtunes 1393 227 - 8 1438 222 proguard 2767 207 - 8 2772 183 l2j 5429 545 - 8 \n5446 533 Figure 18. Evaluation of the precision/scalability trade-off in analysing library methods  \n6. Comparison with related work The approaches of Xylem [15], Salsa [13] and Spoto [19] are the most \nclosely related approaches to ours, in the sense that they target null-dereference analysis of real Java \nprograms. We discuss these approaches in detail .rst, and later give an overview of other related techniques. \nXylem. Xylem is a bug .nding technique rather than a ver\u00adi.cation technique, meaning they may miss real \nbugs, and may also report false positives. Our approach, though aim\u00ading at veri.cation, has several attributes \nthat are inspired by Xylem; e.g., a demand-driven backward data.ow analy\u00adsis from each dereference, predicates \nas data.ow facts, cus\u00adtom simpli.cation rules for predicates rather than a theo\u00adrem prover, a context-stack \nduring inter-procedural data.ow propagation for context-sensitivity, abstracting out arith\u00admetic, and \ninter-procedural summary tables for ef.ciency. However, there are several key differences. Xylem aims \nfor very high precision, and hence uses a richer set of predicates that result in a greater extent of \npath-sensitivity. This could have an adverse impact on scalability. In order to ensure reasonable analysis \ntime they enforce various limits on the analysis, like the sizes of disjuncts (which we do, too), the \nnumber of paths analyzed from a dereference (we do not), and even on the analysis time (which we do not). \nWhen the analysis of a dereference gets terminated due to any of these thresholds being crossed, they \nignore the dereference and go on to the next one, implying unsoundness. In our approach, we aim to conservatively \nlabel each dereference as safe or unsafe, with reasonable precision, meaning we need to maximize the \nnumber of paths back from the dereference that we analyze, as well as the depths of these paths. Therefore, \nfor scalability, we use a smaller lattice, and more limited path-sensitivity. Within this setting, in \norder to maximize precision, we do not drop an entire disjunct when its size crosses a threshold, which \nwould im\u00admediately terminate the analysis (with an unsafe answer); instead, we drop individual old predicates \nfrom the dis\u00adjunct in order to reduce its size, and then continue its prop\u00adagation (as discussed in Section \n2.1.2). Also, when we en\u00adcounter a library call that we choose not to analyze (as dis\u00adcussed in Section \n4), we drop individual predicates that may be mutated by the call, and not the entire disjunct. There\u00adfore, \nwe give up precision in a .ne-grained manner. We also address recursion completely, analyzing a recursive \nmethod until a .x-point is reached; Xylem does not compute .x\u00adpoints for recursive methods, instead using \nad-hoc bounds to terminate the analysis. Another distinction is that we per\u00adform strong updates at put-.eld \nstatements for precision, by keeping track of hypotheses on aliasing relations between variables, and \nvalidating (or invalidating) these hypotheses at assignment statements. Having mentioned these distinc\u00adtions \nbetween the two approaches, it is worth reiterating that Xylem s objective is different from ours (i.e., \nto .nd a few, important bugs), which means some of these distinctions are sensible from both perspectives. \n In subsequent work [14] the same authors apply Xylem, with a few modi.cations, to the problem of identifying \nnec\u00adessary conditions on inputs to model-transformation pro\u00adgrams that cause these programs to throw \nvarious excep\u00adtions, including null-dereference exceptions. Salsa. Salsa is an approach that aims at \nsound null\u00addereference veri.cation of Java programs. It is not based on propagating conditions. Rather, \nit is a (non-demand-driven) data.ow analysis that uses a custom designed lattice to track at each program \npoint access paths that are known to be def\u00adinitely non-null at that point. In their approach they perform \nlimited-scope analysis (in terms of depths of call-chains con\u00adsidered) for scalability; we have shown \nscalability without scope limitation; in fact, in our evaluation we have found numerous dereferences \nthat require analysis over deep call chains. Also, the extent to which Salsa can perform strong updates \nis dependent on the precision of a pre-requisite must-alias analysis, whereas we avoid the need for must\u00adalias \nanalysis by keeping track of aliasing relationships at each point in each path that we analyze. In order \nto facilitate a quantitative comparison between our approach and theirs, we have chosen 8 benchmarks \nfrom their list of benchmarks (plus two more from outside their list). While on two of these benchmarks \n(namely, jlex and bcel) the percentage of dereferences we report as unsafe is less than their corresponding \nnumber, they do better on the other six benchmarks. This said, precision comparisons be\u00adtween the two \napproaches are dif.cult, for multiple reasons. For many of the benchmarks they report a benchmark size \nthat is much smaller than our corresponding number; poten\u00adtial reasons for this include differences in \nversion numbers (they do not indicate the version numbers they use), and in the entry points into the \nprogram that were considered. More importantly, they appear to never analyze library methods, and depend \non programmer speci.cations of the behaviors of these libraries. We analyze library methods, in general, \nwith some exceptions. In our experiments, over all bench\u00admarks, we entered and analyzed the bodies of \n625 distinct library methods. It is very dif.cult to write meaningful, pre\u00adcise speci.cations for library \nmethods, especially when they have side effects. The only manual inputs we use are the skip-list and \nthe analyze-list of library methods, which are a very simple form of speci.cation. The approach of Spoto. \nSpoto describes a .ow-and context-sensitive forward, non-demand driven, static anal\u00adysis to conservatively \n.nd null dereferences. They address exceptions, but not multi threaded programs. They use an abstract \nlattice of formulas that is more expressive than ours, wherein they encode formulas using Binary Decision \nDi\u00adagrams (BDDs). Somewhat surprisingly, it is actually not clear that their precision is better than \nours in practice. There are two common programs in our benchmark set and theirs jlex and javacup. They \nreport separate precision numbers for get.eld , put.eld 2, and call dereferences, whereas we report a \nsingle overall precision number over all derefer\u00adences. On jlex our overall precision is 96.3%, whereas \ntheirs works out to 78.7%. In the case of javacup, our overall pre\u00adcision is 78.7%, while theirs works \nout to 85.4%. There a couple noteworthy caveats, though: they do not mention the version of each program \nthey analyze, nor the number of dereferences in each program, nor the exact procedure they use to select \nthe dereferences (we analyze all dereferences in methods reachable from the entry points). Regarding \nrun\u00adning time, our analysis time over all dereferences is approxi\u00admately double theirs, but our analysis \nis demand driven, with no sharing of intermediate results at all between analyses of different root dereferences. \nOther related techniques. There have been several ap\u00adproaches reported in the literature for verifying \nassertions in programs using very precise forms of reasoning. (Note that safety checking of assertions \nis a more general problem, of which dereference veri.cation is an instance.) For instance, Slam [1] uses \npredicate abstraction, and counter-example guided abstraction re.nement, to initially over-approximate \nthe weakest at-least-once pre-condition of a given condition, and then successively strengthen the over-approximation \nun\u00adtil a concrete trace is produced or a timeout is reached. Syn\u00adergy [10] follows a similar approach, \nbut uses concolic, i.e., simultaneous concrete and symbolic execution, to accelerate the strengthening \nof the over-approximation. The approach of Dillig et al [6] over-approximates the weakest at-least\u00adonce \npre-condition directly, without predicate abstraction. These systems have been designed for property-veri.cation \nin C programs, and have been shown to be precise in prac\u00adtice. While in theory they could be used for \nnull-dereference veri.cation for Java, their practical applicability in this set\u00adting is not clear. Large \nJava programs have certain charac\u00adteristics not shared by C programs, such as extensive use of heap references, \nvirtual method dispatch, deeply nested method calls (with small methods), and call backs. More\u00adover, \nthese approaches do not appear ef.cient enough for use by an individual developer as a part of their \ndesktop de\u00advelopment environment to verify dereferences in a large ap\u00adplication. ESC/Java [8] and Spec# \n[2] use an abstraction-free weak\u00adest pre-condition analysis to verify assertions. They rely extensively \non programmer-given annotations, e.g., method pre-post-conditions, and loop invariants, for completeness, \nmodularity, and scalability. There are several approaches, e.g., Snugglebug [3], Java PathFinder [20], \nand DART [9], that use precise symbolic or concolic analysis to search for a concrete execution path \nthat ends at a given program point with a state that satis.es a given condition. In other words, these \napproaches under\u00ad 2 Although it is not fully clear from their paper, we assume that they include arrayload \nand arraylength instructions within their get.eld category, and arraystore instructions within their \nput.eld category.  approximate the weakest at-least-once pre-condition, rather than over-approximate \nit (as we do). These approaches may not always terminate in their search in the presence of loops and \nrecursion. Their approach is applicable when one is trying to con.rm a potential bug, but is not applicable \nto the problem of proving that an assertion is safe. (The approach of Dillig et al, as well as Synergy, \nare capable of under\u00adapproximation also.) It is noteworthy that there are several interesting ideas used \nin the over-approximating as well under-approximating approaches mentioned above, that could potentially \nbe in\u00adcorporated into our own approach to improve its precision, scalability, or suitability for checking \nproperties other than dereference safety. 7. Conclusions and Future Work We have presented a demand-driven \napproach for veri.ca\u00adtion of null-dereferences, based on over-approximation of the weakest at-least-once \npre-condition, using a novel set of design decisions to make the approach practical. We have implemented \nthe approach, and have evaluated it on a set of real Java programs. Our experimental results indicate \nthe our approach is scalable to large programs, has very quick response time per dereference, and has \nreasonable precision. To the best of our knowledge ours is the .rst practical weakest pre-conditions-based \nveri.cation approach to be demonstrated on large, real Java programs. Future work will be guided by objective \nof increasing the precision of the approach, while still retaining its practicality and demand\u00addrivenness. \nIn particular, we would like to investigate more precise techniques to reason about references to arrays \nand recursive data structures, perhaps by inferring and using speci.cations for container classes (which \noften encapsulate arrays and recursive data structures). We would also like to investigate ef.cient approaches \nto deal with dif.cult idioms such as call backs. Finally, we would also like to investigate application \nof our approach to veri.cation problems other than null-dereference analysis. Acknowledgments. We thank \nAmogh Margoor, graduate student, and Ankur Sinha, project intern at IISc, for their valuable and substantial \nhelp with the experimental evalua\u00adtion. We thank our funding partners Infosys, IBM Research India, and \nMicrosoft Research India for their support. References [1] T. Ball and S. Rajamani. Automatically validating \ntemporal safety properties of interfaces. In Model Checking Software, volume 2057 of Lecture Notes in \nComputer Science, pages 102 122. Springer Berlin / Heidelberg, 2001. [2] M. Barnett, K. R. M. Leino, \nand W. Schulte. The spec# pro\u00adgramming system: An overview. In G. Barthe, L. Burdy, M. Huisman, J.-L. \nLanet, and T. Muntean, editors, Construc\u00adtion and Analysis of Safe, Secure, and Interoperable Smart Devices, \nvolume 3362 of Lecture Notes in Computer Science, pages 49 69. Springer Berlin / Heidelberg, 2005. [3] \nS. Chandra, S. J. Fink, and M. Sridharan. Snugglebug: a powerful approach to weakest preconditions. In \nPLDI 09: Proc. ACM SIGPLAN Conference on Programming Language Design and Implementation, pages 363 374, \nNew York, NY, USA, 2009. ACM. [4] P. Cousot and R. Cousot. Abstract interpretation: a uni.ed lattice \nmodel for static analysis of programs by construction or approximation of .xpoints. In Proc. ACM SIGACT-SIGPLAN \nSymposium on Principles of Programming Languages, POPL 77, pages 238 252, New York, NY, USA, 1977. ACM. \n[5] E. W. Dijkstra. A Discipline of Programming. Prentice Hall PTR, Upper Saddle River, NJ, USA, 1997. \n[6] I. Dillig, T. Dillig, and A. Aiken. Sound, complete and scal\u00adable path-sensitive analysis. In PLDI \n08: Proceedings of the ACM SIGPLAN 2001 conference on Programming language design and implementation, \npages 270 280, 2008. [7] I. Dillig, T. Dillig, and A. Aiken. Fluid updates: Beyond strong vs. weak updates. \nIn A. Gordon, editor, Program\u00adming Languages and Systems, volume 6012 of Lecture Notes in Computer Science, \npages 246 266. Springer Berlin / Hei\u00addelberg, 2010. [8] C. Flanagan, K. R. M. Leino, M. Lillibridge, \nG. Nelson, J. B. Saxe, and R. Stata. Extended static checking for java. In PLDI 02: Proc. SIGPLAN Conference \non Programming Language Design and Implementation, pages 234 245, 2002. [9] P. Godefroid, N. Klarlund, \nand K. Sen. Dart: directed auto\u00admated random testing. In PLDI 05: Proc. ACM SIGPLAN Conference on Programming \nLanguage Design and Imple\u00admentation, pages 213 223, 2005. [10] B. Gulavani, T. Henzinger, Y. Kannan, \nA. Nori, and S. Ra\u00adjamani. Synergy: An new algorithm for property checking. In FSE 06: Proc. ACM SIGSOFT \nSymp. on Foundations of Software Engineering, pages 117 127, 2006. [11] D. Hovemeyer, J. Spacco, and \nW. Pugh. Evaluating and tuning a static analysis to null pointer bugs. In PASTE 05: Proc. ACM SIGPLAN-SIGSOFT \nworkshop on Program Analysis for Software Tools and Engineering, pages 13 19, 2005. [12] G. Kildall. \nA uni.ed approach to global program optimiza\u00adtion. In POPL 73: Proc. ACM Symposium on Principles of Programming \nLanguages, pages 194 206, New York, NY, USA, 1973. [13] A. Loginov, E. Yahav, S. Chandra, S. Fink, N. \nRinetzky, and M. Nanda. Verifying dereference safety via expanding-scope analysis. In ISSTA 08: Proc. \nInternational Symposium on Software Testing and Analysis, pages 213 224, New York, NY, USA, 2008. ACM. \n[14] M. G. Nanda, S. Mani, V. S. Sinha, and S. Sinha. Demystify\u00ading model transformations: an approach \nbased on automated rule inference. In OOPSLA 09: Proc. ACM SIGPLAN Con\u00adference on Object Oriented Programming \nSystems Languages and Applications, pages 341 360, 2009. [15] M. G. Nanda and S. Sinha. Accurate interprocedural \nnull\u00addereference analysis for java. In ICSE 09: Proc. International Conference on Software Engineering, \npages 133 143, Wash\u00adington, DC, USA, 2009. IEEE Computer Society.  [16] J. C. Reynolds. Separation logic: \nA logic for shared mutable data structures. Logic in Computer Science, Symposium on, pages 55 74, 2002. \n[17] M. Sagiv, T. Reps, and R. Wilhelm. Parametric shape analysis via 3-valued logic. ACM Trans. Program. \nLang. Syst., 24:217 298, May 2002. [18] M. Sharir and A. Pnueli. Two approaches to interprocedural data \n.ow analysis. In S. S. Muchnick and N. D. Jones, editors, Program Flow Analysis: Theory and Application. \nPrentice Hall Professional Technical Reference, 1981. [19] F. Spoto. Precise null-pointer analysis. Software \nand Systems Modeling, 10:219 252, 2011. 10.1007/s10270-009-0132-5. [20] W. Visser, C. S. P.as.areanu, \nand S. Khurshid. Test input generation with java path.nder. In ISSTA 04: Proc. ACM SIGSOFT International \nSymposium on Software Testing and Analysis, pages 97 107, 2004. [21] T.J. Watson Libraries for Analysis \n(WALA), http://wala.sf.net. A. Formalizing our analysis as an abstract interpretation In this section \nwe sketch a formulation of our weakest at\u00adleast once pre-condition analysis as an abstract interpreta\u00adtion. \nAs is conventional, we .rst formulate the concrete se\u00admantics as a data.ow analysis. The concrete lattice \nis the powerset lattice of the set of states (i.e., concrete stores). The backward concrete transfer \nfunction cst of any statement st is as follows: If st is any statement other than an ASSUME then cst \n= .S.{s1 |.s2 . S : st transforms s1 to s2}. If st is ASSUME b then cst = .S.{s | s . S and s satis.es \nb}. The join operation is union (therefore, = is .). Let p be a pro\u00adgram point, and C be a post-condition \nat p. The initialization of the concrete analysis is as follows: the set of stores satis\u00adfying C at p, \nand the empty set at all other program points. It is easy to show that the join over all paths solution \nat the en\u00adtry of the program according to the above concrete analysis is precisely the set of states \nthat satisfy wp1(p, C). Our abstract lattice and transfer functions were pre\u00adsented in Section 2. The \nconcretization function . is .F . Formula.{s | s satis.es F }, which is monotonic. For each statement \ntype st the abstract transfer function fst for st is the composition of its .ow function in Figure 3 \nand the simpli.er fules in Figure 5 (as was discussed in Sec\u00adtion 2.2). Each abstract transfer function \nfst is monotonic, and also conservatively over-approximates the correspond\u00ading concrete transfer function \ncst ; i.e., for any formula F . Formula, .(fst (F )) . cst (.(F )). The initialization for the abstract \nanalysis is as follows: the given disjunct (i.e., post\u00adcondition) C at the given program point p, and \nthe empty set (i.e., false) at all other points. Therefore, it follows that the pre-condition computed \nat the program s entry by our anal\u00adysis is equal to or weaker than wp1(p, C). It is instructive to contrast \nour analysis with a weakest pre-conditions analysis. Whereas we over-approximate the weakest at-least \nonce pre-condition, it is natural to under\u00adapproximate the weakest pre-condition. In this setting, the \nbackward abstract transfer fst of any statement st, when ' applied to any formula F , ought to return \na formula F such that when st is executed on the set of states S ' that ' satisfy F , every state that \nresults satis.es F . We omit the details of the lattice and transfer functions in this setting, which \nare somewhat different from the ones we use. For instance, for the GETFIELD instruction v = r.f, the \ntransfer function when applied to a post-condition f would return a pre-condition r = null . f[r.f/v], \nrather than what we return, namely r = null . f[r.f/v]; also, in order to bound any access path safely, \nthe transfer function would need to make the predicate that contains the access path false, rather than \ntrue. Furthermore, while the concretization function . would be the same as in our setting, the join \noperation on the abstract lattice would be logical AND (rather than the union we use, which implements \nlogical OR).     \n\t\t\t", "proc_id": "2048066", "abstract": "<p>Null dereferences are a bane of programming in languages such as Java. In this paper we propose a sound, demand-driven, inter-procedurally context-sensitive dataflow analysis technique to verify a given dereference as safe or potentially unsafe. Our analysis uses an abstract lattice of formulas to find a pre-condition at the entry of the program such that a null-dereference can occur only if the initial state of the program satisfies this pre-condition. We use a simplified domain of formulas, abstracting out integer arithmetic, as well as unbounded access paths due to recursive data structures. For the sake of precision we model aliasing relationships explicitly in our abstract lattice, enable strong updates, and use a limited notion of path sensitivity. For the sake of scalability we prune formulas continually as they get propagated, reducing to true conjuncts that are less likely to be useful in validating or invalidating the formula. We have implemented our approach, and present an evaluation of it on a set of ten real Java programs. Our results show that the set of design features we have incorporated enable the analysis to (a) explore long, inter-procedural paths to verify each dereference, with (b) reasonable accuracy, and (c) very quick response time per dereference, making it suitable for use in desktop development environments.</p>", "authors": [{"name": "Ravichandhran Madhavan", "author_profile_id": "81488654219", "affiliation": "Microsoft Research India, Bangalore, India", "person_id": "P2839320", "email_address": "t-rakand@microsoft.com", "orcid_id": ""}, {"name": "Raghavan Komondoor", "author_profile_id": "81100544771", "affiliation": "Indian Institute of Science, Bangalore, India", "person_id": "P2839321", "email_address": "raghavan@csa.iisc.ernet.in", "orcid_id": ""}], "doi_number": "10.1145/2048066.2048144", "year": "2011", "article_id": "2048144", "conference": "OOPSLA", "title": "Null dereference verification via over-approximated weakest pre-conditions analysis", "url": "http://dl.acm.org/citation.cfm?id=2048144"}