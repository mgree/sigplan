{"article_publication_date": "10-22-2011", "fulltext": "\n Ef.ciently Speeding up Sequential Computation through the N-Way Programming Model Romain Cledat Tushar \nKumar Santosh Pande Georgia Institute of Technology Georgia Institute of Technology Georgia Institute \nof Technology romain@gatech.edu tusharkumar@gatech.edu santosh@cc.gatech.edu Abstract With core counts \non the rise, the sequential components of applications are becoming the major bottleneck in perfor\u00admance \nscaling as predicted by Amdahl s law. We are there\u00adfore faced with the simultaneous problems of occupying \nan increasing number of cores and speeding up sequential sec\u00adtions. In this work, we reconcile these \ntwo seemingly incom\u00adpatible problems with a novel programming model called N\u00adway. The core idea behind \nN-way is to bene.t from the algo\u00adrithmic diversity available to express certain key computa\u00adtional steps. \nBy simultaneously launching in parallel multi\u00adple ways to solve a given computation, a runtime can just-in\u00adtime \npick the best (for example the fastest) way and therefore achieve speedup. Previous work has demonstrated \nthe bene.ts of such an approach but has not addressed its inherent waste. In this work, we focus on providing \na mathematically sound learning-based statistical model that can be used by a run\u00adtime to determine the \noptimal balance between resources used and bene.ts obtainable through N-way. We further de\u00adscribe a dynamic \nculling mechanism to further reduce re\u00adsource waste. We present abstractions and a runtime support to \ncleanly encapsulate the computational-options and monitor their progress. We demonstrate a low-overhead \nruntime that achieves signi.cant speedup over a range of widely used kernels. Our results demonstrate \nsuper-linear speedups in certain cases. Categories and Subject Descriptors D.1.3 [Programming Techniques]: \nConcurrent Programming Parallel program\u00adming; D.3.3 [Programming Languages]: Language Con\u00adstructs and \nFeatures Permission to make digital or hard copies of all or part of this work for personal or classroom \nuse is granted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. OOPSLA \n11, October 22 27, 2011, Portland, Oregon, USA. Copyright c &#38;#169; 2011 ACM 978-1-4503-0940-0/11/10. \n. . $10.00 General Terms Algorithms, Performance Keywords Parallel programming model, N-Way, algorith\u00admic \ndiversity, speedup, sequential computations 1. Introduction Moore s law states that the number of transistors \non a chip doubles approximately every two years. Today, these tran\u00adsistors are used to increase the number \nof cores as opposed to scaling the frequency of cores as was previously the case. The Haswell [13] family \nof processors, for example, is ru\u00admored to come with 8 cores standard. Intel also introduced Rock Creek \nwhich provides 24 dual cores on a single chip [14]. GPUs already have hundreds of cores, albeit special\u00adized. \nTherefore, it is clear that the trend for new generations of chips is i) an increase in hardware parallelism \nand ii) a stagnation of clock speeds. While massively parallel parts of applications will bene\u00ad.t from \neach new processor generation due to their inherent parallelism, our work is motivated by the fact that \ni) the se\u00adquential portions will increasingly become a bottleneck and ii) the amount of time cores are \nfully utilized will decrease. Figure 1(a) shows the maximum achievable speedup as a function of the amount \nof parallelism in an application (Am\u00addahl s law) while Figure 1(b) shows the maximum achiev\u00adable utilization \nas a function of the number of cores (as\u00adsuming parallel sections are embarrassingly so). Both .g\u00adures \nclearly show that sequential components not only limit speedup but also cause more cores to idle, thus \nwasting them. Hill s study of Amdahl s law [12] drew similar conclusions. With the stagnation in processor \nfrequency, exploiting parallel hardware resources is the only way to speed up ap\u00adplications, including \ntheir sequential components. Current approaches such as speculative execution and the dynamic extraction \nof parallelism (the Galois model [19] or the CnC model [17]) allow some speedup of sequential algorithms. \nHowever, such approaches remain dif.cult to apply and lim\u00adited. Furthermore, even for code that is considered \nparallel, scaling to an ever increasing number of cores can be chal\u00adlenging. Indeed, current parallelization \napproaches (data/\u00adtask parallelism) rely on breaking-up a computation into  (a) Amdahl s law 1 (b) \nAchievable utilization: U (N)= P +N(1-P ) Figure 1. Motivating trends pieces which can be processed in \nparallel. This process has its limits for all but embarrassingly parallel code as noted by Patterson \nin [24]. It is therefore important to develop new methodologies that can leverage the ever increasing \navail\u00adable parallel hardware to improve both sequential and paral\u00adlel codes. 1.1 The N-way model The \nN-way model relies on the observation that for many im\u00adportant computations, there is a multitude of \nways to solve them which can lead to varying execution times even on the same input. This diversity can \ncome from the use of heuris\u00adtics or randomness in algorithms. Therefore running multi\u00adple ways in parallel \nand picking the fastest will provide an expected speedup. The N-way model was introduced by Cle\u00addat et \nal. [6, 7] as well as Trachsel et al. in [30, 31]. Complex problems such as NP-hard problems are computationally \nin\u00adtractable to solve exactly and therefore have a plethora of approximations and trade-off based algorithms \nthat can be used to solve them. Randomized algorithms, which execute differently for the same input also \nlead to diversity which can be exploited. Section 2 elaborates on the presence of di\u00adversity in many \nimportant problems. The attractiveness of the N-way model resides in the fact that it exploits an algorithmic \nproperty, diversity, which is orthogonal to the properties exploited in the traditional breaking-up parallelism. \nN-way thus enables speedups on hitherto sequential computations as the traditional distinc\u00adtion between \nsequential and parallel computations no longer applies in N-way: the distinction is now between computations \nwhich exhibit diversity and those that do not irrespective of whether or not the computation can be bro\u00adken \nup into independent tasks and/or data chunks. It is important to note that the speedup provided by N\u00adway \ndoes not come from parallelism over multiple cores. The speedup provided comes from improving the expected \ncompletion time of a computation over n cores as opposed to the expected completion time over a single \ncore. Note that we consider the expected completion time of a computation as our approach is a probabilistic \none. Consider for example a computation C implemented as a randomized algorithm R which is run on an \ninput I. Since the execution time of R (I) may vary from run to run (due to its random nature), an expected \ncompletion time for C on input I must be computed: it corresponds to the average execution time of R \n(I) over multiple runs. Similarly, suppose that C is now implemented using various heuristics H1 to Hh. \nSince each heuristic executes differently, the expected completion time for C is again the average of \nthe completion times of H1 to Hh on input I. The potential speedup for N-way is directly related to the \namount of diversity present in the ways to solve the computation. In particular, if the various ways \nto solve the computation C exhibit very little diversity in their execution time, N-way will not be able \nto provide speedup irrespective of the number of parallel resources committed. Conversely, if a large \nspread of execution time exists, N-way will provide a speedup which may be super-linear. Furthermore, \ncontrary to other parallel models, the N\u00adway model is free from complex and error-prone parallel programming \nconstructs such as threads, locks and barriers. As far as the programmer is concerned, each way is a \nsepa\u00adrate sequential program that can be written using sequential techniques. Figure 2 illustrates the \nN-way model where B is a step in the problem that can be solved with multiple ways. The N-way model guarantees \nsemantic equivalence between 2(a) and 2(b). (a) (b) Figure 2. Sequential .ow versus N-way .ow 1.1.1 \nProblem: a potentially wasteful model While previous works have shown that the N-way model has potential \nfor speedup, they have not addressed the wasteful\u00adness in terms of resources that the model entails. \nIndeed, if n ways are launched on n cores, n-1 cores will perform work that will eventually be discarded. \nIncreased waste can lead to an increase in memory and bandwidth contention there\u00adfore limiting or negatively \nimpacting the speedup obtained. For N-way to be practical in a wide range of scenarios, it is thus crucial \nto answer the following question: how many and which ways to launch in order to best balance speedup \nand resource usage? In other words, we wish to determine how the expected speedup scales with resources. \nNote that the notion of scaling is the opposite of what is traditionally considered: we want to determine \nthe minimum n that best balances expected speedup and resources used. The optimal n will be directly \nrelated to the amount of expressible di\u00adversity: in an extreme case, if all ways solve the problem in \nexactly the same execution time and quality, n should be 1 as no bene.t will be gained from launching \nmultiple ways.  Answering the previous question is dif.cult for the pro\u00adgrammer because of i) the dependence \nof kernel execution time on input data and ii) the lack of models linking execu\u00adtion time of the various \nways together. In this paper, we propose a runtime system capable of learning the execution time characteristics \nof the N-way computation (C) based on prior invocations thereby enabling the runtime to estimate the \nbene.ts of launching various sets of ways in subsequent invocations of C. If C is not invoked multiple \ntimes, N-way is still applicable but not the learning model. Even if the optimal set of ways is picked, \nsome ways will make progress towards C s solution much slower than others making them highly unlikely \nto complete .rst. Additional re\u00adsource savings can therefore be had by killing these unpro\u00adductive ways \nearly. For many applications, it is straightfor\u00adward for the programmer to de.ne progress monitors which \ncan be monitored by the N-way runtime in deciding which ways to kill. This technique, which we call culling, \nallows signi.cant additional resource savings when the program\u00admer can de.ne the progress metrics. Therefore, \nthe learning of n and culling are two inde\u00adpendent techniques which can be applied individually or to\u00adgether \nfor even more saving of resources.  1.2 Contributions We make the following contributions: We recognize \nthe utility of diversity in many important computational steps. In particular, we demonstrate that certain \nsequential computations can be sped-up using N\u00adway. Quality of result improvements can also be obtained \nwith the same techniques.  We propose a statistical learning approach to determine the number of ways \nto launch beyond which signi.cantly diminishing returns (in terms of expected speedup for example) occur. \nThis approach frees us from relying on application speci.c knowledge.  We further propose progress monitors \nas a programmer de.ned way to track the progress of ways and a runtime that uses this information to \nterminate ways that are mak\u00ading little progress.  We implement our system with programmer friendly APIs \nthat avoid traditional parallel programming pitfalls. Our model is supported by a low overhead runtime. \n We demonstrate the applicability of our ideas to a range of application domains by using representative \nbench\u00admarks. We show how speedup and QoR improvements can be achieved using both randomness and other \nforms of diversity. Throughout the paper, we use the following terms: N-way computation or computation \nrefers to an algo\u00adrithmic step to which the N-way technique can be ap\u00adplied. In other words, a N-way \ncomputation can be exe\u00adcuted in a variety of ways for a given input and each way will produce a solution \nto the computation.  N-way kernel or kernel refers to a speci.c implemen\u00adtation used to solve a N-way \ncomputation. For example, if multiple heuristics are used to solve a N-way computa\u00adtion, each heuristic \nwill be called a N-way kernel.  Expected speedup refers to the speedup that N-way will provide. We will \nnote it Sn for n cores.  Effective utilization or utilization refers to the effec\u00adtive number of cores \nutilized by a N-way computation. We note this neff . We always have neff = n where n is the number of \ncores allocated to the N-way computation. neff can be smaller than n in the presence of culling for example \nwhen some cores are freed before the comple\u00adtion of the computation.  The remainder of this paper is \norganized as follows. Sec\u00adtion 2 demonstrates the wide presence of diversity in many important problems. \nSection 3 presents the N-way model as well as the statistical learning model to determine the op\u00adtimal \nset of ways to launch while Section 4 describes the culling technique. The API and runtime implementation \nare presented in Section 5. Section 6 presents experimental re\u00adsults. Section 7 covers related work and \nSection 8 concludes the paper with future work. 2. Diversity The N-way model relies on the existence \nof diversity in com\u00admon computations. In this Section, we motivate the presence of such diversity in \nmany applications. Let us consider a N-way computation C. By de.nition, C is a diverse computation that \ncan be solved using a mul\u00adtitude of ways. The different ways may solve C differently, some operating \nfaster than others or with a higher quality of result (QoR). Note that the granularity of C is irrelevant \nin the de.nition. C could be an entire application or a small basic building-block kernel such as a sort \nproblem. Find\u00ading diversity at the granularity of a kernel will allow larger problems that depend on \nthe kernel to also bene.t from the diversity present in the kernel. 2.1 Algorithmic diversity We identify \nthree types of algorithmic diversity in a compu\u00adtation C: i) across distinct algorithms, ii) within an \nalgorithm if it can be parametrized and iii) within an algorithm if it uti\u00adlizes randomness.  Diversity \nacross algorithms For many real-world prob\u00adlems, such as NP-hard problems, .nding an exact solution in \na reasonable amount of time is impossible. Even for prob\u00adlems in P, the large size of the problem may \nmake exactly solving it problematic. For such problems an acceptable solution rather than an optimal \none is sought. For exam\u00adple, when .nding a path between two nodes in a graph, any path is an acceptable \nsolution although some may be preferred over others (shorter paths for example). Accep\u00adtance of a wider \nset of solutions enables the use of a vari\u00adety of algorithms to solve the same problem. Approxima\u00adtion \nalgorithms [33] have been utilized for such a purpose: they give a solution of provable quality within \nprovable run\u00adtime bounds1. Similar to approximation algorithms, heuris\u00adtics based algorithms provide \nthe same notion of diversity without provable quality and/or runtime bounds. Diversity in parametrized \nalgorithms Apart from distinct algorithms, even if only a single algorithm exists to solve C, diversity \nmay still be present if the algorithm can be param\u00adetrized. For example, the CPLEX solver [29] offers \nover 100 parameters to tune its algorithm. Each pair (A, Param) can be considered a distinct algorithm \nwith differing execution characteristics as in the previous case. Diversity due to randomness Randomized \nalgorithms [22, 23] utilize a degree of randomness as part of their logic. A randomized algorithm seeks \nto achieve good performance on average. Due to its random nature however, its running time, its output, \nor both are probabilistically characterized random variables. Randomized algorithms are therefore parametrized \nim\u00adplicitly by the random seed used. The difference with param\u00adetrized algorithms is that the parameter \nspace is innumerable (for all practical purposes). Note also that randomized algo\u00adrithms are particularly \ninteresting for diversity as a single algorithm will provide diversity as opposed to different al\u00adgorithms \nin the other cases.  2.2 Other sources of diversity While we focus on the exploitation of algorithmic \ndiversity, diversity also occurs at the hardware level (due to architec\u00adtural heterogeneity for example) \nand at the compiler level (different optimization options). This latter aspect was ex\u00adplored in more \ndetail in [31]. These sources of diversity only enhance the current ones we identi.ed and described. \n 2.3 Diversity is common Throughout this section, we have shown that different types of algorithms commonly \nused to solve dif.cult problems 1 Note that the bounds are worse-case bounds and not precise enough to \ndetermine which algorithm will perform better on a speci.c input. contain diversity which we can exploit. \nWe give here con\u00adcrete examples to show the pervasiveness of diversity in a few of the 13 dwarfs described \nby Asanovic et al. as form\u00ading the cornerstone of tomorrow s computation [2]. Linear algebra (dense \nor sparse) Parametrized algo\u00adrithms are frequently used. Certain randomized algo\u00adrithms are also used \n[32].  Combinatorial logic Given their high computational complexity, such problems are frequently approximated \n[33]. Randomized algorithms are frequently used and we present results with WalkSAT [28] a randomized \nSAT solver.  Graph traversal Randomness is frequently utilized to solve large-scale graph problems given \ntheir good aver\u00adage case complexity. In particular, many AI algorithms utilize graph traversals to evaluate \npossible moves . We present results for a randomized Hamiltonian cycle builder.  Graphical models Searching \nfor relationships in Baye\u00adsian networks is a NP-hard problem and many differ\u00adent search algorithms can \nbe applied. Non-exhaustive searches frequently make use of heuristic and random ex\u00adploration as part \nof their algorithm. Simulated annealing and genetic algorithms are examples of that.  Backtrack / branch-and-bound \nThe Traveling Sales\u00adman Problem (TSP) is a classical example of a branch\u00adand-bound algorithm. Quickly \nexploring the huge space of possibilities requires the use of heuristics and/or ran\u00addomness. We present \nresults for heuristical TSP solvers.  3. N-way model The N-way model seeks to exploit the diversity \ndescribed in Section 2 to provide both expected speedup and QoR improvements; for clarity, we focus on \nexpected speedup and brie.y describe how QoR can be provided in Section 3.3. 3.1 Base model As shown \nin Figure 2, the N-way model exploits the diver\u00adsity in B to transform the sequential execution .ow shown \nin Figure 2(a) to the parallel one shown in 2(b) in a way that makes both .ows semantically equivalent. \nIf we assume that each way returns a valid solution, semantic equivalence is ensured by the enforcement \nof the following two rules: Parallel ways execute in isolation from one another in a side-effect free \nmanner  One and only one of the parallel ways makes its compu\u00adtation visible to the rest of the program. \nIn Figure 2(b), C will only be affected by one and only one of B1 to B3  Apart from ensuring semantic \nequivalence, isolation (de\u00adscribed in Section 5) also enables the programmer to pro\u00adgram each way as \nif it were sequential code. While B1, B2 and B3 all execute in parallel, C will only be affected by the \nexecution of one and only one.  While the idea behind N-way parallelism is simple and previous work \n[6, 30, 31] has shown that naively launch\u00ading as many ways as possible maximizes expressed diversity \nand therefore expected speedup potential, the real dif.culty lies in determining the best balance between \nresources used and expected speedup potential to make N-way ef.cient, practical and scalable. Launching \ntoo few ways will reduce speedup potential while launching too many will waste re\u00adsources and energy \nas well as possibly degrade the perfor\u00admance of the ways thereby leading to sub-optimal speedups. Determining \nwhich and how many ways to launch is depen\u00addent on i) the inputs because of the data-dependent behav\u00adior \nof many algorithms, and ii) the characteristics of the al\u00adgorithms themselves, for example the execution \ntime distri\u00adbution of the randomized algorithms or the applicability of a particular heuristic to a given \ninput. The information re\u00adquired is thus not easily available to the programmer making it dif.cult for \nhim to a-priori make an informed decision. In this Section, we describe our statistical learning model \nthat can effectively determine the best combination of ways to launch.  3.2 Ef.cient N-way model In \ntraditional parallelism, the number of parallel resources that can be effectively utilized is bounded \nby the amount of divisibility (work or data) in the algorithm. N-way exploits diversity instead of divisibility \nand therefore the number of parallel resources that can be effectively utilized is limited by the amount \nof diversity present in the computation. Intu\u00aditively, for a given input, if the spread of completion \ntimes of the different ways is wide (large diversity), many resources can be effectively used to explore \nthis spread whereas if the completion times fall in a narrow band, allocating more and more resources \nwill only provide a marginal bene.t. As an extreme example, a computation which can only be solved using \na deterministic algorithm exhibits no diversity (a spread of 0) and utilizing more than one resource \nto solve it is wasteful. While models exist to a-priori determine the potential speedup that can be obtained \nthrough the division of work, the amount of diversity in algorithms is dif.cult to charac\u00adterize statically \nas it is heavily data-dependent in a non pre\u00addictable manner. An exact determination of the number of \nresources needed to optimally express diversity in a problem is therefore not possible and we instead \npropose a statisti\u00adcal model which allows a runtime to compute the expected speedup associated with a \nparticular choice of ways. The expected parallel ef.ciency de.ned as PE. (n)= Sn/n, where Sn is the expected \nspeedup on n resources, can be computed and maximized. Note that we compute the ex\u00adpected parallel ef.ciency \nas a function of n. Culling may reduce the utilization of cores from n to neff . 3.2.1 Assumptions We \ndo not rely on the programmer to provide information about the computation C s execution-time distribution. \nWe instead make the following assumptions about how the ap\u00adplication invokes C: Repetition: The N-way \ncomputation C is invoked repeat\u00adedly within an application.  Stability: The underlying (unknown) execution-time \ndis\u00adtribution for C only changes slowly over consecutive in\u00advocations of C.  These assumptions allow \nus to construct a statistical learning scheme that can reliably estimate C s execution time distri\u00adbution \nbased on previously observed behavior over past in\u00adputs. Stability assumption Note that the stability \ncondition does not require that consecutive inputs be similar to one another but rather that the behaviors \nof the various ways observed over previous inputs remain stable. For example, consider a randomized algorithm \nbeing fed two different types of inputs, one large where the randomized algorithm takes a fairly long \ntime to return a result and one small where the algorithm returns quickly. Even if individually the large \nand small inputs are very different, if future inputs are similar to either the large or the small ones, \nthe execution time distribution of the randomized algorithm will not change: it will be the superposition \nof the execution time distribution of the small input and that of the large input. Similarly, consider \nfor example that the execution time distribution is originally a Gaussian distribution N (m, s). If N \nsamples (execution times) are drawn from this distri\u00adbution, they are most de.nitely not equal or even \nsimilar. Collectively, however, they form N (m, s). If the distribu\u00adtion now changes to N (m1,s1) and \nM samples are drawn, the M samples will collectively indicate the N (m1,s1) dis\u00adtribution. Moreover, \na mix of samples (from the .rst N and from the last M) will indicate another distribution but if the \nchange from m to m1 and s to s1 is small, the mix will closely approximate N (m1,s1). The learning scheme \nwill therefore still be able to infer useful information. While our stability condition is therefore \nnot overly con\u00adstraining, if it does not hold, the execution time for the next invocation of C within \nthe application may not be sampled from the distribution observed over the prior few invocations of C. \nIn that case, the choice of n will be less optimal with regards to maximizing expected speedup and minimizing \nre\u00adsource waste. There is often good reason why the stability assumption holds for a variety of applications. \nThe characteristics of the input data signi.cantly determine the execution-time of C. For example, in \nTSP, the size of the graph and the degree of nodes signi.cantly determine the execution time. A given \ninvocation of the application is likely to invoke C using data whose characteristics fall within a relatively \nnarrow range.  Therefore, the behavior of C on the next input will be drawn from the distribution of \nbehaviors on previous inputs which will validate the stability condition. For example, an application \nplanning routes for delivery trucks would repeatedly invoke TSP over the same graph, but with perhaps \ndifferent constraints for each truck. In all of our sample applications, the stability condition held. \n 3.2.2 Formal problem The general problem is, given a set S of algorithms to solve a N-way computation \nC, which combination of ways taken from S has the best parallel ef.ciency. Note that there are two separate \ncomponents to this: i) which algorithms to pick and ii) for algorithms that have inherent diversity (such \nas randomized algorithms) how many instances of that algo\u00adrithm to run. In the following sections, we \nwill study two orthogonal special cases: i) C is solved by a randomized al\u00adgorithm R and ii) C is solved \nby a set of non randomized heuristics (or parametrized algorithms) H1 to Hq. 3.2.3 Randomized algorithms \nIn a randomized algorithm R, a random seed determines the behavior of the algorithm, and in particular \nits execu\u00adtion time. For a given input I, the execution times of the algorithm will be distributed according \nto a probability dis\u00adtribution function (PDF) and while the exact execution time of a run on I is unknown, \nan expected completion time can be computed from the PDF. For a speci.c input I, a PDF can be learned \nby repeatedly invoking the algorithm on I but it will not be the same across inputs. However, under the \nstability assumption, the characteristics of the inputs vary slowly; in other words, we can assume: PDF \n(Ij+1) PDF (I1 ...Ij) Therefore, R s PDF for previous inputs is assumed to apply to the next input. \nOur goal is to pick n that maximizes PE. (n)= Sn/n. To do this, for each n =1, 2,..., the runtime estimates \nSn using the estimated PDF. Let F1 (n) denote the cumulative distribution function (CDF) corresponding \nto the estimated PDF: F1 (t) = Prob {Execution time of P <t} However, we are interested in the CDF for \nthe fastest completing way when n independent ways are launched in parallel: Fn (t) = Prob{Execution \ntime of fastest way < t, when n independent ways are launched} It can be shown that: n Fn (t)=1 - (1 \n- F1 (t)) This is because the probability for each independent way to not complete within time t is 1 \n- F1 (t), and therefore the time for none of the n ways to complete in time t is (1 - F1 (t))n. The expected \ncompletion time En is then the mean over Fn: Fn (En)=0.5. Now, Sn = E1/En. In practice In practice, we \nmaintain the CDFs as step func\u00adtions. Suppose we launch a single instance of R repeat\u00adedly and measure \ntimes t1, t2 and t3 with t1 <t2 <t3, we will update our approximation of F1 with those three points and \nF1 will be such that .t<t1,F1 (t)=0, .t . [t1,t2) ,F1 (t)=1/3, .t . [t2,t3) ,F1 (t)=2/3 and .t = t3,F1 \n(t)=1. Although crude, this allows a quick approximation of the CDF. When we launch n instances of R, \nwe update Fn instead of F1 and since all CDFs can be derived from one another, we can simultaneously \nupdate all of them. To adapt to the slow variations in input characteristics, we weigh each input point, \ndecreasing the importance of older points. With the CDFs, we can compute En for each n from which we \ndeduce Sn. Selecting the optimal number of ways To select the num\u00adbers of ways to launch, we compute \nS1, S2, etc. and deter\u00admine if the improvement in speedup is worth the extra re\u00adsource cost. The speci.c \ncriteria can be set by the program\u00admer. Our approach is greedy in the sense that if Si does not meet \nthe criteria Si+1 does not either and we do not need to compute it.  3.2.4 Distinct algorithms In the \ncase of distinct heuristics, randomness is no longer a source of uncertainty. However, uncertainty about \nthe ex\u00adact input that will be passed to the computation C remains. Since the inputs passed to C vary, \nit may not be possible to predict which heuristic will do better on the next input. However, our assumption \nthat past input characteristics are representative of upcoming inputs (stability assumption) still allows \nthe runtime to learn statistically meaningful informa\u00adtion. Mathematical model Given q heuristics H1 \nto Hq, sup\u00adpose we run each heuristic to completion on each input Il to Im. For each input point, we \nrecord the completion time of each heuristic: eij = Hi (Ij ) The average completion time E [Hi] of Hi \nis given by: m m E [Hi]=1/m eij j=1 Given the stability assumption on inputs, we can assume that the \nexpected completion time of a given heuristic on the next input is the average time over past inputs: \nHi (Im+1) E [Hi]  This allows us to estimate the expected completion time of each individual heuristic \nfor the next input Im+1. If we had to pick only one heuristic to run (i.e., n =1), we would pick the \nHi with the smallest E [Hi]. However, we are interested in determining whether running multiple heuristics \nprovides signi.cantly greater speedup. For this we need to determine an optimal n and the set of n heuristics \nto launch in parallel. Consider a naive approach based on sorting by smallest E [Hi] as follows: {Hi1 \n,Hi2 ,Hi3 ,...} where for n = K Hi1 ,...,HiK get launched. However, as Figure 3 shows, this approach \nis suboptimal. The .gure shows execution times for heuristics on inputs Il to Im, with the sorted heuristics \n{H2,H3,H1}. For n =2, launching {H2,H3}as per the naive approach performs worse than launching {H2,H1}, \nbased on fastest completion times. Even though E [H3] <E [H1], H3 is always slower than either H1 or \nH2, hence selecting it is not useful (even for n =3). Figure 3. Execution times for heuristics H1, H2, \nH3 on inputs Il to Im The above illustrated a need for a general case test (for n> 1) that takes input-dependent \norderings of heuristics into account. Therefore, in selecting the best Hik in addition to Hi1 , we need \nto .nd Hik with the smallest expected completion time: m m E [Hi1 ,Hik ]=1/m min (ei1j ,eikj ) j=1 Further, \nwe can decide n =1 versus n =2 based on: E [Hi1 ,Hik ] /2 <E [Hi1 ] /1 where we pick the best Hik . This \ninductively generalizes to answering n =3, 4,... and picking the best Hi to add at each step. For n =3 \nfor example, pick the best Hiq minimizing: m m EHi1 ,Hik ,Hiq=1/m minei1j ,eikj,eiqj j=1 and also ensure \nthat: EHi1 ,Hik ,Hiq/3 <E [Hi1 ,Hik ] /2 Note that the above strategy is not optimal since we greedily \nadd only one additional heuristic at each step. Ide\u00adally we should test every possible subset of size \n2 against Hi1 , and then every possible subset of size 3 against the best subset of size 2, and so on. \nSuch an approach has an exponential complexity, making it unsuitable in practice. In practice In the \ntheoretical model, we assumed measure\u00adment of the completion time for each heuristic on each input. In \npractice, we obviously do not want to do this as it negates the bene.ts of N-way by forcing the runtime \nto wait for all heuristics to complete whereas we will only wait for the .rst heuristic to complete and \nrecord its completion time T . For all other heuristics, we approximate their completion time on the \ninput in the following way: The completion time of heuristics that were run but did not complete is \ncT where c> 1 is a parameter de.ned in the runtime. Indeed, the completion time of these heuris\u00adtics \nis greater than T and the programmer controlled c pa\u00adrameter indicates how strongly winning heuristics \nshould be favored.  The completion time of heuristics that were not run (for example, if they were not \npicked) is dA where A is the average completion time for that heuristic over past in\u00adputs and d< 1 is \na programmer controlled parame\u00adter controlling how often untried or cast-away heuris\u00adtics should be given \na chance. This allows the learning algorithm to adapt to changing conditions by ensuring that ways that \nwere previously discarded are periodically evaluated.  The two heuristics above allow the runtime to \nestimate the completion time for all heuristics on all inputs and we can then apply the mathematical \nmodel described in the previous section. Similar to the randomized case, we also forget older points \nto allow adaptation, restricting to a window Il to Im of prior invocations of the computation C.  3.2.5 \nGeneralized algorithm We considered ef.cient learning models for the special cases of randomized and \nheuristics-driven C. A more generalized model combining both for the same C is possible (TSP can combine \nrandomization and heuristics), but such a model is the subject of future work given its high computational \ncomplexity in its current form.  3.2.6 Resilience of the learning models The N-way model was designed \nto make use of the idle cores and therefore assumes that each N-way thread is given ex\u00adclusive access \nto a core. However, even if transient loads occur that may perturb this assumption, the learning mod\u00adels \ndescribed are resilient. This is due to the fact that the effects of even a relatively large perturbation \nwill only im\u00adpact a few samples and be mitigated by the other samples. A suf.ciently large and persistent \nperturbation however will clearly compromise our models but only for a short duration after it goes away \nas our models use only recent samples (using a sliding window). Furthermore, even if perturbations do \naffect the estima\u00adtions of expected completion times, the decision process of the N-way model will not \nalways be compromised. Con\u00adsider the choice between n ways versus n +1 ways. Even if the estimations \nof En and En+1 are perturbed, the N-way model will make the same decision as long as the ordering of \nPE. (n) and PE. (n + 1) does not change. Therefore, the model can tolerate signi.cant errors in the estimations \nof the explected completion times as long as the resulting parallel ef.ciencies compare the same way. \n  3.3 Support for Quality-of-Result We described the ability of the N-way model to provide speedup \nwhere the measurable quantity is execution time and is lower bounded by 0 which represents the best pos\u00adsible \ncase (ie: an algorithm that takes no time to execute is the best case). The only assumption that the \nlearning algo\u00adrithm makes is that smaller is better . It can therefore be extended to provide QoR improvements \nif the programmer can provide a measure for quality at the end of the way and a distance that quantitatively \ncompares two qualities. The runtime can then learn the distribution of qualities instead of execution \ntimes. 4. Ef.ciency through culling The learning framework proposed in Section 3.2 tries to a-priori \npick n to maximize the parallel ef.ciency Peff . However, as the various ways execute, more information \nmay be gained about their progress which can enable the dynamic culling of ways which are almost certainly \nnot going to win . This will reduce the overall utilization of processor cores (to neff ) while having \nvery little impact on the expected speedup (as only ways that will lose will be preemptively culled). \nIn this Section, we propose a culling framework. The motivation is to i) relieve system pressure on shared \nresources in particular and ii) improve energy ef.ciency. 4.1 Notion of progress To determine the ways \nto cull, we rely on a programmer\u00adsupplied notion of algorithmic progress; in other words, how far has \nthe algorithm come in solving the computation. This notion is not always easy to characterize and is \nvery computation speci.c but is relevant in a number of cases: Computations that are solved using greedy \nconstructive algorithms have a natural notion of progress as both the amount of work done and to-do are \nreadily available.  Other computations, such as optimization problems, are solved by .nding partial \nsolutions which are subsets of the .nal solution: dynamic programming, incremental al\u00adgorithms (Dijkstra \ns single source/destination), etc. The size of the partial solution is a good measure for pro\u00adgress. \n To indicate progress, the programmer must de.ne a nor\u00admalized metric M that takes values between 0 \nand 1 where 1 indicates completion2.  4.2 Culling mechanism While the progress metric M provided and \nupdated by the programmer gives absolute progress for each way, we are ac\u00adtually interested in progress \nper resource because we want to eliminate the ways making the least progress but consuming the most resources. \nCurrently, we only consider the CPU as a resource and thus measure progress per CPU-time which we denote \nas Ms. However, as part of our future work, we are looking at measuring other resources consumed by each \nway (such as L2 footprint, bus activity, etc.) through the use of performance counters. Assumptions Metric \nmonitoring provides us with informa\u00adtion about past progress. We therefore assume linearity in progress \nto extrapolate future progress. This simplifying as\u00adsumption is constraining but the programmer can update \nM to best approximate this assumption and the culling system can tolerate this approximation. Culling \nrelies on answers to two questions: i) when to look for ways to cull and ii) what ways to cull. When \nto cull? Ideally, we would like to cull inef.cient ways as early as possible but we do not want to constantly \nmonitor progress to limit overheads. Therefore, we attempt to check only when it is likely that ways \ncan be culled. In our current mechanism, each way W reports its pro\u00adgress MW to the runtime which then \ndecides whether or not to attempt culling. It will be attempted if either i) a way has made signi.cant \nprogress, ii) suf.cient time has elapsed, or iii) a suf.cient number of progress reports have occurred \nsince the last culling attempt. The runtime has access to the last reported progress information from \neach way. What to cull? Given the linearity assumption, we can di\u00adrectly compare progress metrics Ms \n. We seek to quickly W determine all under-performing ways that have little chance of catching-up with \nthe best performing ways. To do so, we cluster all ways based on their Ms. The idea is to group together \nways that are making a similar amount of progress. We do not .x the number of clusters a\u00adpriori instead \nrelying on a hierarchical bottom-up clustering approach where we stop the clustering process when the \nmean squared error (MSE) of the merged cluster D of A and B is greater than the weighted sum of the MSEs \nof A and B where: m 2 MSE (D)= Ms - Ms (D) / |D| Wi Wi.D We will cull the worst cluster Dworst if a (1 \n- Ms (Dworst)) > 1 - Ms (Dbest) 2 Note that M may be derived from auxiliary metrics if this is easier \nto express for the programmer  where a is smaller than 1 (we used 10%) and Ms of a cluster is the mean \nmetric of ways in that cluster. This ensures that culling is less aggressive when the best ways have \nmade little progress (and it is still unclear who will win ) and more aggressive when the best ways are \ncloser to .nishing. This also allows the linearity assumption to be relaxed and progress need only be \nmostly linear.  4.3 Compatibility with learning Culling is fully compatible with our learning N-way \nmodel under the assumption that culling only culls ways that would not have won . This is a reasonable \nassumption as the culling algorithm kills off ways that are making little pro\u00adgress and are therefore \nhighly unlikely to win . Under this assumption, culling has no impact on learning as it does not modify \nthe information used by the learning algorithm, namely the completion time of the winning way. Culling \nis optional and does not directly provide addi\u00adtional speedup. However, a speedup can be achieved indi\u00adrectly \nby reducing contention for shared resources. Even if the programmer cannot provide progress monitors, \nthe learn\u00ading approach is still fully applicable. 5. Implementation The N-way model is supported by a \nruntime to enable dy\u00adnamic monitoring and adapt the selection of ways as the characteristics of the inputs \nchanges. The runtime has the following key roles: i) determine the Types of algorithm to launch and how \nmany of each Type (in the case of a random\u00adized algorithm), ii) optionally monitor progress and cull \nany under-performing way and iii) provide isolation among the ways and keep them side-effect free. 5.1 \nAPI All code examples provided here are in pseudo C-like code. The actual implementation makes heavy \nuse of C++ s tem\u00adplating abilities to provide stronger type-checking guaran\u00adtees. The exact API calls \nare therefore not described in this section. However, the interested reader can .nd the complete framework \non Sourceforge at https://sourceforge. net/projects/nway. An example (the WalkSAT bench\u00admark presented \nin Section 6) is also provided which illus\u00adtrates the use of our API. The N-way model requires very little \nfrom the program\u00admer who needs to identify: A diverse computation C and the different N-way kernels \navailable to solve it.  An optimization objective: speedup or QoR improve\u00adments. In the case of QoR \nimprovements, a quality metric must be provided.  To support culling, an optional speci.cation of progress \nfor the ways is required.  The code example in Figure 4 will be used to illustrate the API. 1 s t r \nu c t Node i n p u t t {s t a r t , end ; } myInput ; 6 s t r u c t f i n d A P a t h t {i n t p a t \nh L e n g t h ; b o o l p at hFo und ; } f i n d A P a t h ; / * h e u r 1 f , p a r a m f and r a n \nd f ar e f u n c t i o n p o i n t e r s */ f i n d A P a t h . addAlgo ( h e u r 1 f ) ; 11 f i n d \nA P a t h . addAlgo ( p a r a m f , p a r a m e t e r S p a c e ) ; f i n d A P a t h . addAlgo ( r a \nn d f , nonE numerab le ) ; / * Two example switches */ struct chooseFastest t { 16 StopWhen = (0, \n0); MinimumRequired ( findAPath { return f . pathFound ; }} chooseFastest ; 21 struct chooseShortest \nt {StopWhen = (undef, 1); MinimumRequired ( findAPath Comparator ( findAPath t f1 , { return f1 . pathLength \n- 26 } chooseShortest ; t f) t f) { return f . pathFound ; }findAPath t f2 ) f2 . pathLength ; } findAPath.run(myInput, \nchooseFastest); findAPath.run(myInput, chooseShortest); Figure 4. N-way pseudo-code for a path-.nding \nproblem. Specifying a N-way computation Specifying a N-way computation is simply a matter of specifying \nthe various algorithms (N-way kernels) that are possible. This is shown on Lines 10 to 12 where three \ndifferent function pointers are attached to the same N-way computation findAPath. Heuristics, parametrized \nalgorithms and randomized algo\u00adrithms which can be launched multiple times can all be added. The programmer \nis responsible for identifying the functions that belong to the same N-way computation. Specifying an \noptimization-objective The programmer must specify whether he wishes to obtain speedup or QoR through \nan optimization objective as shown on Lines 15 and 21. Note that in the common case of speedup, the pro\u00adgrammer \nhas nothing to specify as we provide a default speedup objective . The optimization objective becomes \nrelevant when QoR is traded off with execution time. The optimization-objective determines when and how \nthe run\u00adtime chooses the winning way. We provide the following APIs: StopWhen is a tuple (pt,pc) where \npt = 0 and 0 = pc = 1. The idea behind both numbers is to allow a trade-off between the time that the \nruntime waits and the number of ways it waits for. Indeed, to obtain maximum QoR, the programmer would \nideally like to wait for all ways to complete to pick the absolute best but in practice, he might want \nto cap the runtime of the N-way problem thereby trading off quality with execution time. If t1 is the \ncompletion time of the .rst way, and nc (t) is the number of ways that have completed by time t, td the \ntime the runtime will make a decision is de.ned as the minimum  nc(td) t such that either td = (1 + \npt) * t1 or = pc n is true. Two such speci.cations are shown in Lines 16 (maximum speedup) and 22 (maximum \nquality). Comparator qualitatively compares two ways to deter\u00admine which provides the best QoR. The \ncomputation is based on the metrics de.ned for the N-way computation (Line 6) which the programmer must \nupdate before the way completes.  MinimumRequired is also a comparator that determines if a way completed \nwith a valid result. Indeed, it is fre\u00adquent for heuristics to return an answer of the type I have not \nfound a solution . While semantically valid, the pro\u00adgrammer may wish to return a solution if at all \npossible. The programmer can specify what condition a good so\u00adlution must meet. Solutions not meeting \nthis requirement are considered to still be computing (and therefore take in.nite time). Note that if \nno good way is found, the .rst way to return will win .  Again, these functions are only required in \nthe case of QoR objectives and allow great .exibility in specifying a mean\u00adingful quality objective. \nLaunching a N-way computation Lines 28 and 29 both show an invocation of a N-way computation: the .rst \none will run the N-way computation and pick the .rst way to .nish while the second will pick the one \nreturning the short\u00adest path. The programmer would replace the execution of the N-way computation B (in \nFigure 2(a)) by one of these two calls to obtain its N-way execution. During N-way execu\u00adtion, one and \nonly one way executes on each selected core. In other words, if the runtime determines that only one \ncore should be used, only one way (picked by the runtime) will be launched. For n cores, n ways will \nbe launched, one per core.  5.2 Progress monitors The optional culling relies on ways reporting on their \npro\u00adgress. In our implementation, each way is passed an indi\u00advidual copy of a set of Metrics as shown \non Line 6 (the Metrics are, in this case, the two variables pathLength and pathFound . Each way maintains \na copy of these computation-speci.c values that the N-way runtime can read from each of the ways states \nwhenever required. Note that for ef.ciency reasons, the Metrics are implemented as a double buffered \ndata structure which enables the runtime to read a consistent Metrics state while not blocking or other\u00adwise \nimpacting any of the ways. The programmer must also provides a function that takes as input the Metrics \nand re\u00adturns a single progress value (Ms ). W  5.3 Providing isolation A very important aspect of the \nAPI and runtime is to enforce isolation between ways. Our goal is to encapsulate each way so that when \ndestroyed, no trace of its activity remains. Since we chose to implement each individual way as a thread \nrather than as a separate process the wrapping of non-local memory accesses to provide isolation among \nthe threads is required. We justify our choice for this as op\u00adposed to other mechanisms such as fork \non Linux in Sec\u00adtion 5.4.1. We currently do not handle certain system calls (such as writes to a .le \nor device) but these can be wrapped (or intercepted using ptrace for example on Linux) as well if required. \nOur current framework handles two aspects of isolation: Garbage collection of all heap-allocated memory \nin dis\u00adcarded ways.  Wrapping of non-local state which we de.ne as anything visible outside the scope \nof a way (globals or heap vari\u00adables are accessible from outside the scope of the way).  Garbage collection \nis implemented in a distributed manner: each thread is responsible for its own garbage which it clears \nbefore being terminated. 5.3.1 Wrapping non-local state: a lightweight versioning system We rely on \na lightweight versioning of data where each non\u00adlocal variable is implemented as a vector of pointers, \none for each of the executing ways plus a default pointer represent\u00ading the latest of.cial value. Each \nway-pointer points to a lazily created private copy of the variable. Figure 5 shows a variable Foo in \na situation where 4 ways are running (on threads T1 through T4). Ways that do not have a private copy \nwill use the default copy in read-only mode (labeled D in Figure 5). We de.ne three operations on the \nvariables: Figure 5. A non-local variable Foo in our isolation system Read: The private value for the \nthread, or default value if none exists, is returned. The value returned cannot be modi.ed (const in \nC++). This is an O(1) operation.  Write: A write will trigger an automatic copy-on-write if this is \nthe .rst write access. Subsequent writes will modify the private value. The COW approach minimizes memory \noverhead as well as copy overhead. This is an O(1) operation as well (minus the copy itself).  Commit: \nThis operation serves to make public a way s private copies and is implemented as a simple pointer change \n(no copy). The private copies of the other ways as   well as the old default value are destroyed. Destruction \nis O(n) in the number of ways for each variable and can be distributed across the unpicked ways. Figure \n5 shows the state of Foo after T2 and T3 have per\u00adformed read access to and and T1 and T4 have either \nonly read it (and would therefore read the value in V1) or not ac\u00adcessed it at all. To minimally modify \nthe program, we utilize C++ s tem\u00adplating capabilities and de.ne a type, NVShared<T>, which behaves exactly \nlike T except that accesses go through our isolation system. For example, in Figure 5, Foo would be declared \nas NVShared<T> Foo where T is the C++ type of Foo. Access to Foo from thread T2 would be equivalent to \naccessing V2 while access from T1 would equate accessing V1 if read-only or a newly created copy of V1 \notherwise.  5.4 Thread-based implementation The ways are run using a pool of worker threads. At the \nstart of a N-way program, the runtime launches a .xed number of worker threads which will wait until \na speci.c way is assigned to them. We implemented our own pool based on Boost threads but added support \nto cancel a running way without also cancelling the underlying worker thread. This allows the quick reuse \nof the worker threads without the need to respawn them. 5.4.1 Comparison with a fork mechanism On UNIX \nbased systems, the fork mechanism could also be used as it would naturally provide an OS supported iso\u00adlated \naddress space and also be ef.cient given its copy-on\u00adwrite implementation [3]. The main advantage of \nthe fork mechanism is that the original code could run unmodi.ed although our Clang pass mitigates this. \nHowever, the fork mechanism also has downsides: The cost of a fork is much higher than that of assigning \na way to a thread in a pool (our mechanism).  The fork mechanism is not implemented in every OS. Windows \nfor example does not have an equivalent mech\u00adanism. Our framework only relies on the ability to create \nthreads which makes it much more OS independent.  The isolation mechanism we use provides a natural \nway for the runtime to monitor the different ways. This aspect of the runtime was detailed in Section \n4.  The isolation mechanism we provide also allows a much more .ne-grained control of the data that \nwill be copied. Indeed, the fork mechanism s COW will only function at the granularity of a page, which \nmay not be optimal if the data written by each of the ways is very small or very scattered. Our mechanism \nminimizes the amount of data that has to be copied.  Both mechanisms therefore have their advantages. \nThe va\u00adlidity of the N-way model is independent of the mecha\u00adnism used to provide isolation and a different \nimplementa\u00adtion would provide similar bene.ts.  5.5 Debuggability The fact that the programmer does \nnot know a-priori which of the ways will commit may seem like a debugging night\u00admare. However, since \nthe ways are well encapsulated, de\u00adbuggability will not be much harder than for sequential pro\u00adgrams \nand de.nitely simpler than for traditional parallel pro\u00adgrams. If each way is a sequential kernel, debugging \neach one of them individually in the context of the entire pro\u00adgram without N-way parallelism is suf.cient \nto ensure that the resulting N-way program is debugged. None of the hard to debug parallel bugs like \ndeadlock, live-locks and races are introduced. Repeatability of execution can also be obtained by recording \nthe choices made at each commit point.  5.6 Automated compiler transformation of a program for N-way \nTo lessen the burden on the programmer, we developed a compiler based approach relying on the C++ frontend \nClang [5] which allows source-level analysis to identify non-local variables. We perform a source-to-source \ntranslation from the original code to a N-way code where non-local variable de.nitions get wrapped with \nNV Shared. The programmer identi.es the functions to be converted to N-way problems. The transformation \nof a regular sequential program to one that supports N-way consists of: The programmer identi.es the \nvarious functions that be\u00adlong to the same N-way problem.  Optionally the programmer identi.es Metrics \nfor the N-way problem to support culling and/or a QoR objec\u00adtive. He modi.es appropriate functions to \nupdated these Metrics.  Running the source code through our Clang based trans\u00adlator.  Compiling the \nprogram normally. Our runtime is imple\u00admented as a shared library which should be linked against the \nprogram.  The burden on the programmer rests principally in identi\u00adfying and expressing diversity in \nparts of the program. Our Clang translator takes care of the source-level transforma\u00adtions needed to \ncorrectly wrap non-local variables and our runtime takes care of the complex process of determining the \nexact ways to run to maximally bene.t from the N-way model. If no bene.t can be extracted, the N-way \nsystem will approximate a sequential execution. 6. Experimental results With our experiments, we seek \nto validate the following: Brute-force N-way parallelism provides speedup and QoR improvement.  Our \nlearning approach allows the N-way model to pro\u00advide signi.cant speedup while requiring signi.cantly \nfewer resources. In other words, the effective utilization of the machine neff is substantially reduced \nby the learn\u00ading model.  Culling further reduces neff with little impact on the expected speedup obtained \nvia the N-way model.  We also demonstrate the low-overhead and scalability of our runtime. All tests \nwere run on a 64bit Linux Ubuntu System run\u00adning a dual quad-core Xeon E5540 at 2.53 GHz with 12GB or \nRAM. GCC 4.4.3 was used to compile the roughly 10000 lines of runtime code with -O3 . 6.1 Benchmarks \nWe applied N-way to the following benchmarks to demon\u00adstrate the above points. WalkSAT is a randomized \nSAT solver that has shown good performance in SAT competitions. At each step, the solver picks a random \nunsatis.ed clause and uses a heuristic to select the variable in the clause to .ip to satisfy the clause. \nThe SAT inputs are representative inputs taken from DIMACS [9].  The MSL library is a motion planning \nlibrary that uses rapidly-exploring random trees (RRTs) [18] which are designed to ef.ciently search \na high-dimensional non\u00adconvex space. We used the library in a hand crafted sim\u00adulation to demonstrate \nthe limits of the N-way learning model as well as the impressive super-linear speedups that can be obtained. \n TSP-solve is a TSP solver implemented by Chat Hurwitz which contains a variety of heuristics.  GALib \n[34] is a library providing support for genetic al\u00adgorithms (GAs) which we use to again solve the TSP. \nGe\u00adnetic algorithms rely on randomness to create successive generations of solutions. We demonstrate \nhow N-way can be used to improve the quality of the .nal generation.  ListSched: we have implemented \na simple greedy list\u00adscheduler using three different heuristics to pick the node that gets scheduled \nnext (if more than one is avail\u00adable). With this benchmark, although modest in size, we demonstrate the \nusefulness of culling as well as QoR improvements with a very small amount of diversity.  For some of \nthese benchmarks, specialized traditional par\u00adallel implementations exist; however, we aim to show the \npotential of an orthogonal parallelization strategy that relies on diversity. The fact that an orthogonal \nform of parallelism is also possible does not detract from the value of diversity\u00adbased parallelism (N-way) \nand we do not seek to compare the two. The applicability of both approaches will depend on the characteristics \nof the application: sometimes a traditional approach will be more natural (when divisibility of task/data \nis naturally present) and sometimes a N-way approach will be simpler (for irregular algorithms for example \nwhen it is dif.cult to determine dynamic dependencies). The baseline for all speedup results is the unmodi.ed \nsequential code (without any N-way modi.cation)  6.2 Speedup through randomness In the graphs, speedup \nis relative to the original unmodi\u00ad.ed C++ code in which only a single instance of the ran\u00addomize algorithm \nis run. The slowdown shown for the 1\u00adthread case therefore represents the overhead of our system. Speedup \nis shown for a brute force approach where the N\u00adway system is forced to use a .xed number of ways as \nwell as for our learning and our learning+culling approach (when appropriate). In those cases, the number \nof threads corre\u00adsponds to the maximum number allowed to the runtime. All results are averaged over multiple \nruns. 6.2.1 WalkSAT Figure 6 gives WalkSAT results. For very small inputs (f400) a slowdown occurs due \nto the overheads of the N\u00adway system but overheads are negligible for larger inputs. For larger inputs, \nthe bene.t of N-way becomes clear as signi.cant speedups obtained in particular for many threads. Speedup \na function of diversity The speedup obtained de\u00adpends on the variation in execution time of the original \npro\u00adgram for a given input. In general, a broad correlation ex\u00adists between the speedup obtained through \nN-way and the Coef.cient of Variance 3 (CoV) which is representative of the amount of diversity present: \nthe larger the CoV, the more speedup potential there is. For example, the CoV for ssa7552-158 is 0.73 \ncompared to 0.82 for f3200. 6.2.2 MSL motion planning Results for our hand-crafted Car benchmark are \nshown in Figure 7. The benchmark simulates a car that has to .nd its way through a narrow opening and \nshowcases the strengths of the N-way framework as the benchmark is similar to .nding a needle in a haystack \n. The random algorithm may .nd it very quickly (short execution time) or may take a very long time. The \nCoV is thus very high (1.33 for an opening of size 4) and super-linear speedups are therefore possible. \nThe bene.ts tapper down with larger openings as the probability of quickly .nding the opening increases. \n 6.2.3 Effects of learning and culling In Figure 6, bars 7 and 8 show the speedup obtained when using \nour learning approach that tries to launch just enough ways to obtain a good speedup for each input. \nThe runtime is given a maximum number of cores to utilize (8 for bar 7 and 16 for bar 8) and may choose \nto launch anywhere from 1 to that maximum number of ways (at most one per core). Therefore, across all \ninputs, the effective utilization of cores, 3 de.ned as the ratio of the standard deviation by the mean \n  Figure 6. Speedup results for the WalkSAT benchmark Figure 7. Results for the MSL benchmark. The \nbenchmark names represent either a .xed opening size (s.) or a periodic increment in the opening s size \n(f.). neff , is lower (the runtime will not always choose to launch before the N-way computation completes. \nIn both cases, we S 8 or 16 ways). de.ne the parallel ef.ciency as PE. = . neff The last two bars (9 \nand 10) show when our learning approach is compounded with our culling approach. In this WalkSAT benchmark \nFigure 8 illustrates the utility of case neff is further lowered as certain ways will be culled learning. \nWe can see that the speedup obtained with N-way by increasing the number of cores tappers off after 8 \ncores. This means that the additional cores are not being effectively Fixed Learn Learn+Cull Dataset \nS E S E S E f3200 2.88 .37 2.53 .34 2.59 .57 g125.17 4.28 .56 5.58 .79 6.26 .89 ii32b3 2.30 .31 2.95 \n.42 2.79 .39 ii32d3 2.16 .29 2.31 .32 2.07 .28 ssa7552-159 1.74 .24 1.71 .24 3.41 .47 Table 1. Table \nshowing the maximal speedup S obtained and the corresponding parallel ef.ciency for the .xed case for \nWalkSAT. The .xed n this was achieved for is 8 and corresponds to the best case for these benchmarks. \nThe same quantities are shown when learning and learning+culling is applied for 8 threads Figure 8. \nSpeedup versus core utilization for the ii32b3 in\u00adput to WalkSAT. The line shows the evolution of the \nspeedup if a .xed number of cores is used (without learning or culling) and the points. used by N-way \n(the diversity present in the algorithm is suf\u00ad.ciently expressed with fewer than 8 cores). We can see \nthat the runtime correctly chooses the run the benchmark with neff =6.97 obtaining a good speedup while \nnot commit\u00adting unnecessary cores. For the other benchmarks, Table 1 summarizes results for all WalkSAT \ninputs. It is important to note that our learning scheme is de\u00adsigned to optimize for parallel ef.ciency \n(PE. ) and not for maximal speedup. If the learning scheme was designed for maximal speedup, it would \nalways choose to launch as many ways as possible. Our scheme, however, does not try to max\u00adimize speedup \nand therefore the speedups obtained by our learning scheme can be lower than those obtained for the basic \nN-way scheme that does not integrate learning and culling: the speedup may be higher utilizing more ways \nbut the ef.ciency is worse as shown in Table 1. In most cases, ef.ciency increases when learning is used \nand increases even more when culling is also used. This in\u00addicates that, on average, fewer cores were \nused to obtain a similar speedup. For f3200, learning actually lowers ef.\u00adciency; this is due to the \nfact that f3200 is still relatively small and uses few iterations which limits the possibility to learn. \nAs expected, the addition of culling increases ef.\u00adciency while providing very little speedup gains. \nAny gains are most likely due to the freeing up of shared resources (such as the L2 cache) by the culled \nways. This is partic\u00adularly visible in the large ssa7552-159 dataset. Note that although it is hard to \nprovide a good progress measure for a SAT solver, the addition of culling does not negatively im\u00adpact \nperformance and actually performs rather well even in these dif.cult conditions. MSL For the MSL benchmark, \nthe case of the Car bench\u00admark with the size of the opening changing is interesting. We see in Figure \n7 that while the learning algorithm per\u00adforms well for Car (f. 1) where the size of the opening in\u00adcreases \nslowly (by 1 every iteration), it performs very badly when it increases by 10 every time. This is caused \nby the un\u00adderlying distribution of execution times changing too rapidly (since we can rapidly go from \na small opening size where diversity is useful to a large opening size where diversity is not). Therefore, \nthe assumption that the inputs will be sim\u00adilar to one another or evolve slowly is broken. In this case, \nwe see that we cannot perform as optimally as we could have but we do not incur a slowdown. Culling is \nnot applicable to MSL as the notion of progress is hard to de.ne.  6.3 Speedup through heuristics Diversity \nthrough heuristics can also be used to obtain speedup: we use tsp solve, a program containing different \nTSP heuristics, to show this. Note that we used no random\u00adness in the heuristics. We solved 50 randomly \ngenerated TSP problems and measured the total execution time for all 50 problems as well as the average \ntour length. The results are summarized in Ta\u00adble 2. When running a .xed number of threads, the heuristics \nThreads Time (s) Speedup Avg. Tour 1 1191 1.0 8497 2 828 1.44 8463 4 335 3.56 8452 8 298 4.0 8445 16 \n295 4.04 8443 16 (learning) 300 3.97 8444 Table 2. Table showing results for a .xed set of 50 ran\u00addomly \ngenerated TSPs. are chosen at random while with learning, our runtime picks the best heuristic(s). Learning \ndoes not signi.cantly lower speedup and reduces resource use: the learning approach ef\u00adfectively uses \nabout 3 cores as opposed to the 16 used in the brute-force approach. Closely studying the results con.rm \nthat only two or three heuristics (out of a total of 13) fre\u00adquently win with the others providing only \nmarginal ben\u00ade.ts. The runtime is able to correctly select those. Note that all heuristics are useful \nas, for each heuristic, there are in\u00adputs on which it performs better than the other heuristics.  Our \ninput data set favored a few but, as a programmer, it is impossible to know a-priori if this is always \nthe case. The N\u00adway model can dynamically determine the useful heuristics for the programmer.  6.4 QoR \nthrough randomness In GAs, the .tness of a population is a natural quality mea\u00adsurement. In our TSP example, \na .tter individual is one representing a shorter path. Furthermore, the randomness in GAs has very little \neffect on the time each generation takes to be created but signi.cantly impacts the .tness of the popu\u00adlation. \nTherefore, GALib is a good candidate to demonstrate QoR improvements with N-way. To evaluate the QoR \nimpact of N-way, we ran the algo\u00adrithm for 2500 generations and compared the .tness of the best individual \nat the end of the 2500 generations. At each in\u00adtermediate generation, we picked the .ttest generation \nas the winner and it was fed to the next step. Results for various inputs from TSPLIB [25] are shown \nin Table 3. For learn\u00ading and culling, effective resource utilization (in number of cores) is shown in \nparenthesis. N-way clearly improves the # Th. a280 lin318 rat575 Orig. 7.8 139 31.5 1 7.8 139 31.5 2 \n7 134.3 29.3 4 6.8 128.2 28.7 8 6.5 122.3 27.7 16 6.3 113.8 26.7 8 (l) 7.1 (4.6) 126.9 (4.5) 28.7 (4.1) \n8 (l+c) 7.1 (3.5) 132.5 (3.9) 29.1 (3.8) 16 (l) 7 (7.0) 128.9 (7.3) 28.9 (6.6) 16 (l+c) 7.2 (7.0) 132.3 \n(7.2) 29.3 (6.6) Table 3. Fitness of the population after 2500 generations (smaller is better) as well \nas resource utilization for learning (marked l ) and learning+culling (marked l+c ). The .rst column \n(marked # Th. ) lists the number of threads from 1 to 16. Orig. refers to the unmodi.ed benchmark (1 \nthread without N-Way). .nal population s .tness, thereby .nding a shorter, or bet\u00adter path. More importantly, \nwe also note the bene.ts of both our learning and culling algorithms. For example, the dif\u00adference in \n.tness between the case with 8 threads and 8 threads with learning is always less than 8%, thereby still \ngiving the full bene.ts of the N-way model, yet the learning algorithm reduces neff by over 40% (average \nutilization of 4.5 for learning). Culling further slightly reduces quality but reduces resource usage \nby an extra 20%. 6.5 QoR through heuristics We implemented various heuristics to pick the next ready \nnode to schedule in ListSched: pick the one with the longest critical path, biggest fan-out, etc. This \nbenchmark is very small and runtime overheads dwarf possible speedups but N-way allows for QoR improvements. \nWe generated a set of directed acyclic graphs (DAG) to schedule and ran all three original heuristics. \nWe then ran the N-way benchmark which runs all three heuristics in parallel and waits for at least 2 \nof them to complete. The best schedule is then selected. Waiting for only two limits the total execution \ntime while still providing the bene.t of quality improvement. The sum of the schedules found is listed \nin Table 4. Although the Heuristic or N-way Sum FanoutFirst 3129986 LongestDelayFirst 3133401 CriticalPathFirst \n3131065 N-way (3 ways) 3115375 Table 4. Sum of the schedules lengths found over a set of DAGs for various \nheuristics improvements are modest, they come for a low cost to the programmer. No single solution works \nas well as the N\u00adway solution showing that a static choice is sub-optimal across all inputs. A domain \nexpert could potentially add more heuristics and extract even better performance.  6.6 Runtime overhead \nand scalability The speedup results in Figures 6 and 7 show that the absolute overhead of our system \nis low. Indeed, the basic overhead of our N-way system can be viewed as the difference between the .rst \nbar (the unmodi.ed program) and the second bar (n =1 N-way program) as the difference between those bars \ncorresponds to the overhead of the N-way runtime and isolation system. We further analyze this overhead \nto pinpoint the hotspots in our runtime as well as its scalability. We use a home\u00adgrown high-precision \npro.ler which outputs information identical to that of gprof except that it does so in an exact manner \n(no sampling) and only pro.les marked functions. We present results for a run of the WalkSAT benchmark \non the f1600 benchmark with 1, 8, and 16 threads in Ta\u00adble 5. Non-shown runtime functions each represented \nless than 1% overhead. Other benchmarks present similar over\u00adhead characteristics. Not surprisingly, \nthe vast majority of Function % total overhead Call time (ns) 1 8 16 1 8 16 Access (RO) 53.8 48.5 49.7 \n12 13 26 Access (RW) 24.7 31.7 24.1 9 13 21 Th. info 15 13.9 18.5 2 2 5 Th. private 5.9 4.5 6.2 24 24 \n57 Table 5. N-way runtime overheads for 1, 8 and 16 threads. Access (RO) corresponds to a RO access \nto a NVShared, th. info to a call to determine a running thread s character\u00adistics and th. private to \na TLS fetch. the overhead comes from dealing with access to non-local data. Close to 75% of the overhead \nis spent in accessing data and although the overhead of each call is relatively low, it does add up. \nWhile this overhead is non-negligible, it allows for the bene.ts described previously in Section 5.4.1. \n The scalability of our runtime is also very good up to 8 threads: the fact that the time per-call stay \nmostly identical demonstrates this. However, the time per-call doubles when moving to 16 threads and \nwe suspect this is due to the dual\u00adsocket architecture of our machine. In our implementation, threads \nare bound to cores in increasing order and would therefore go off-chip for anything more than 8. This \nincurs additional overhead as the runtime must now communicate with threads located on two separate sockets. \n7. Related work There is strong algorithmic evidence of diversity in algo\u00adrithms [15, 20, 22] and launching \nmultiple instances of the same algorithm to speed up a computation has been used in some very speci.c \ninstances: ManySAT [10, 35] for ex\u00adample uses a notion of portfolios that is very similar to our N-way \nproblem. Other work in the security area [8, 27] seeks to exploit diversity to improve the security guarantees \nof a system. These works demonstrate the wide scope of di\u00adversity in algorithms (although they exploit \nit for a different purpose). Work by Cledat et al. [6, 7] and Trachsel et al. [30, 31] introduced the \nnotion of brute force N-way/CPE. We build on this work by introducing a statistical learning model and \na culling approach that dramatically reduces re\u00adsource consumption while preserving the speedup bene.ts. \nIn addition to speedup, we also address maximizing QoR within the same framework. Our methodologies allow \nthe programmer to focus solely on expressing algorithmic diver\u00adsity without having to worry about how \nto achieve speedup and at what cost: our runtime will dynamically ensure that speedup is maximized while \nresource usage is minimized. Coupled with our simple API, we believe that our work presents a practical \nframework to exploit diversi.ed com\u00adputations to obtain speedup for dif.cult to parallelize codes. Our \napproach is orthogonal to the one taken with auto\u00adtuners. Auto-tuners usually rely on the presence of \nmany tunable parameters for a given algorithm and will try to pick the best set of values for these parameters \nacross a wide range of input data for a speci.c machine. Once the parameters are set, they do not change \nand that version is considered optimal for that machine. In our work, we exploit diversity when it is \nnot clear a-priori which version will be better (ie: no .xed set of parameters is optimal). Auto-tuning \ncould be used in conjunction with our runtime to come up with a reduced set of good parameters which \nour runtime could take as inputs to con.gure the different ways. Note also that recent work on PetaBricks \n[1] similarly tries to trade-off execution time and quality of results but takes a more tuning approach \nas opposed to a competitive execution approach. The model we propose is another parallel model but it \ndifferentiates itself from other parallel models by the type of parallelism it exploits. Dynamic parallelism \nis discovered and exploited in [17, 19]. Our work does not seek to discover or express dynamic parallelism \nbut rather the parallelism present in the diversity of computations. Our work relies on task-parallelism \nbut, unlike other models such as Cilk [16] and Intel s TBB, we do not rely on the breaking-up of a computation. \nWork has also been done to improve the quality of results. In particular, in [21], Misailovic explores \na quality of service pro.ler which aides the programmer in determining whether certain loops can be replaced \nby less expensive and quicker versions. The pro.ler gives both the speedup implications as well as the \nquality of result implications. Such work shows the importance of a balance between speedup and quality \nof results. Our framework is different in the sense that it does not rely on pro.ling but rather on just-in-time \ndecisions. Furthermore, we believe that our framework could complement this work by using the versions \ngenerated by this pro.ler as various ways that could be picked at runtime. Similarly, our framework could \nalso complement another quality of results motivated work: the different execution possibilities generated \nin [26] could be used as the various ways to try at runtime. It would also be interesting to see how \nthe information derived in [26] about the error bound could be integrated in the decision making process \nof the N-Way runtime. The isolation guarantees we provide are similar to those provided by STMs [11] \nbut the mechanism to provide them is much more lightweight. No logging is required and we do not care \nabout the interleaving of interactions. Versioned boxes [4] also use versioning to keep track of the \ndifferent values of a variable but their approach seeks to solve a different problem than ours. As such, \ntheir implementation is much more complex and slower than ours. Furthermore [4] provides isolation in \nthe context of STMs and not in the N-way context. Both approaches use some form of type wrapping to function. \n8. Conclusion and future work In this paper, we presented the N-way programming model as an alternative \nway to exploit parallel resources: instead of relying on a break-up of the computation, we exploit algo\u00adrithmic \ndiversity to obtain speedup or QoR improvements. In this model, a set of ways are run in parallel to \nsolve a problem and the best one, for example the fastest, is picked just-in-time. We justi.ed our approach \nby the demonstrated presence of diversity in algorithms (heuristics, parametrized algorithms and randomized \nalgorithms in particular). We made the following important contributions to the N\u00adway model: i) a statistical \nlearning model to estimate the ex\u00adpected bene.t of sets of ways and therefore allow an ef.cient and practical \nuse of N-way; ii) a culling framework based on the measure of progress in ways to further reclaim unpro\u00adductive \nresources and iii) an API for N-way that simpli.es state encapsulation and iv) a compiler .ow for automated \nconversion of a C++ program to N-way incorporating state isolation.  We implemented our contributions \nusing a low-overhead runtime and demonstrated its applicability using a variety of problems. Our results \nshow that our runtime ef.ciently manages N-way and achieves notable speedup and QoR improvements for \na variety of benchmarks. Future work We are investigating several possible exten\u00adsions of this work. \nFirstly, we are looking into launching more ways than the available hardware parallelism. This would \nenable the runtime to have access to a wider array of possibilities for each way: instead of leaving \nit alive or killing it, the runtime could throttle its priority based on its performance. Secondly, we \nare investigating how N-way could be used in conjunction with traditional parallelism in which case N-way \nwould act as a multiplier effect on tra\u00additional parallelism. Note also that different parallelization \napproaches could also provide another source of diversity. Thirdly, we are investigating how to more \ndirectly mea\u00adsure diversity by identifying similar ways. This would en\u00adable for example an alternate \nculling algorithm where the ob\u00adjective would be to cull ways that are similar thereby keep\u00ading the breadth \nof diversity intact while reducing the num\u00adber of resources required to express it. Characterizing diver\u00adsity \nbetween algorithms could also be used to further guide the runtime algorithm in picking which heuristics \nshould be launched together and which should not: heuristics that are too similar should not be launched \ntogether whereas very different heuristics should as this helps maximize the diver\u00adsity expressed. Finally, \nwe are unifying our statistical learn\u00ading models for randomized algorithms and heuristics. We have developed \na general model but it is currently too com\u00adputationally intensive to be practical. Acknowledgment The \nauthors would like to thank the anonymous reviewers for their comments. The authors also gratefully acknowledge \nthe support of NSF grants CCF-1018544 and CCF-0916962. References [1] J. Ansel, Y. L. Wong, C. Chan, \nM. Olszewski, A. Edelman, and S. Amarasinghe. Language and compiler support for auto-tuning variable-accuracy \nalgorithms. In CGO 11. IEEE Computer Society, 2011. [2] K. Asanovic et al. The landscape of parallel \ncomput\u00ading research: A view from berkeley. Technical Report UCB/EECS-2006-183, EECS Department, University \nof Cal\u00adifornia, Berkeley, Dec 2006. [3] E. D. Berger, T. Yang, T. Liu, and G. Novark. Grace: safe multithreaded \nprogramming for C/C++. In OOPSLA 09, pages 81 96, New York, NY, USA, 2009. ACM. [4] J. Cachopo and A. \nRito-Silva. Versioned boxes as the basis for memory transactions. Sci. Comput. Program., 63(2):172 185, \n2006. [5] CLANG: A C family frontend for LLVM. http://clang. llvm.org/, 2010. [6] R. Cledat, T. Kumar, \nJ. Sreeram, and S. Pande. Opportunistic computing: A new paradigm for scalable realism on many cores. \nIn HotPar 2009: 1st USENIX Workshop on Hot Topics in Parallelism. USENIX, 2009. [7] R. Cledat and S. \nPande. Energy ef.ciency via the n-way model. In PESPMA 2010, in conjunction with ISCA. ACM, 2010. [8] \nB. Cox, D. Evans, A. Filipi, J. Rowanhill, W. Hu, J. Davidson, J. Knight, A. Nguyen-tuong, and J. Hiser. \nN-variant systems: A secretless framework for security through diversity. In In Proceedings of the 15th \nUSENIX Security Symposium, pages 105 120, 2006. [9] Dimacs benchmarks. http://tinyurl.com/myj2m7, 2009. \n[10] Y. Hamadi, S. Jabbour, and L. Sais. Manysat: Solver de\u00adscription. Technical Report MSR-TR-2008-83, \nMicrosoft Re\u00adsearch, May 2008. [11] T. Harris and K. Fraser. Language support for lightweight transactions. \nIn OOPSLA 03: Proceedings of the 18th annual ACM SIGPLAN conference on Object-oriented programing, systems, \nlanguages, and applications, pages 388 402, New York, NY, USA, 2003. ACM Press. [12] M. D. Hill and M. \nR. Marty. Amdahl s law in the multicore era. IEEE COMPUTER, 2008. [13] Intel haswell. http://tinyurl.com/28dxp67, \n2010. [14] Intel shows 48-core datacentre on a chip . http:// tinyurl.com/2fyhejo, 2010. [15] S. K. Iyer, \nJ. Jain, M. R. Prasad, D. Sahoo, and T. Sidle. Error detection using BMC in a parallel environment. In \nCHARME, pages 354 358, 2005. [16] C. F. Joerg. The Cilk System for Parallel Multithreaded Computing. \nPhD thesis, Department of Electrical Engineer\u00ading and Computer Science, Massachusetts Institute of Tech\u00adnology, \nCambridge, Massachusetts, Jan. 1996. Available as MIT Laboratory for Computer Science Technical Report \nMIT/LCS/TR-701. [17] K. Knobe. Ease of use with concurrent collections (CnC). In HotPar 2009: 1st USENIX \nWorkshop on Hot Topics in Parallelism. USENIX, 2009. [18] J. J. Kuffner Jr. and S. M. Lavalle. RRT-connect: \nAn ef.cient approach to single-query path planning. In Proc. IEEE Int. Conf. Robot. Autom. (ICRA), pages \n995 1001, 2000. [19] M. Kulkarni, K. Pingali, B. Walter, G. Ramanarayanan, K. Bala, and L. P. Chew. Optimistic \nparallelism requires ab\u00adstractions. In PLDI 07, pages 211 222, 2007. [20] M. Luby and W. Ertel. Optimal \nparallelization of las vegas algorithms. In STACS 94, pages 463 474. Springer, 1994. [21] S. Misailovic, \nS. Sidiroglou, H. Hoffmann, and M. Rinard. Quality of service pro.ling. In Proceedings of the 32nd ACM/IEEE \nInternational Conference on Software Engineer\u00ading -Volume 1, ICSE 10, pages 25 34, New York, NY, USA, \n2010. ACM.  [22] M. Mitzenmacher and E. Upfal. Probability and Computing. Cambridge University Press, \n2005. [23] R. Motwani and P. Raghavan. Randomized Algorithms. Cam\u00adbridge University Press, 1995. [24] \nD. Patterson. The trouble with multicore. http: //spectrum.ieee.org/computing/software/ the-trouble-with-multicore/, \nJuly 2010. [25] G. Reinelt. TSPLIB -a traveling salesman problem library. In ORSA Journal on Computing, \nvolume 3, pages 376 384, 1991. [26] M. Rinard. Probabilistic accuracy bounds for fault-tolerant computations \nthat discard tasks. In Proceedings of the 20th annual international conference on Supercomputing, ICS \n06, pages 324 334, New York, NY, USA, 2006. ACM. [27] B. Salamat, T. Jackson, A. Gal, and M. Franz. Orchestra: \nintrusion detection using parallel execution and monitoring of program variants in user-space. In EuroSys \n09: Proceedings of the 4th ACM European conference on Computer systems, pages 33 46, New York, NY, USA, \n2009. ACM. [28] B. Selman, H. Kautz, and B. Cohen. Local search strategies for satis.ability testing. \nIn DIMACS Series in Discrete Math\u00adematics and Theoretical Computer Science, pages 521 532, 1995. [29] \nTomLab. CPLEX parameters interface. http://tomopt. com/docs/cplexug/tomlab_cplex014.php, March 2010. \n[30] O. Trachsel and T. Gross. A platform for competitive exe\u00adcution. In PESPMA 2008, in conjunction \nwith ISCA. ACM, 2008. [31] O. Trachsel and T. R. Gross. Variant-based competitive paral\u00adlel execution \nof sequential programs. In CF 10: Proceedings of the 7th ACM international conference on Computing fron\u00adtiers, \npages 197 206, New York, NY, USA, 2010. ACM. [32] M. Tygert. A fast algorithm for computing minimal-norm \nso\u00adlutions to underdetermined systems of linear equations. May 2009. [33] V. Vazirani. Approximation \nAlgorithms. Springer, 2001. [34] M. Wall. GAlib. http://lancet.mit.edu/ga/, 2009. [35] C. M. Wintersteiger, \nY. Hamadi, and L. Moura. A concurrent portfolio approach to smt solving. In CAV 09, pages 715 720, Berlin, \nHeidelberg, 2009. Springer-Verlag.     \n\t\t\t", "proc_id": "2048066", "abstract": "<p>With core counts on the rise, the sequential components of applications are becoming the major bottleneck in performance scaling as predicted by Amdahl's law. We are therefore faced with the simultaneous problems of occupying an increasing number of cores and speeding up sequential sections. In this work, we reconcile these two seemingly incompatible problems with a novel programming model called N-way. The core idea behind N-way is to benefit from the algorithmic diversity available to express certain key computational steps. By simultaneously launching in parallel multiple ways to solve a given computation, a runtime can just-in-time pick the best (for example the fastest) way and therefore achieve speedup.</p> <p>Previous work has demonstrated the benefits of such an approach but has not addressed its inherent waste. In this work, we focus on providing a mathematically sound learning-based statistical model that can be used by a runtime to determine the optimal balance between resources used and benefits obtainable through N-way. We further describe a dynamic culling mechanism to further reduce resource waste.</p> <p>We present abstractions and a runtime support to cleanly encapsulate the computational-options and monitor their progress. We demonstrate a low-overhead runtime that achieves significant speedup over a range of widely used kernels. Our results demonstrate super-linear speedups in certain cases.</p>", "authors": [{"name": "Romain E. Cledat", "author_profile_id": "81336488528", "affiliation": "Georgia Institute of Technology, Atlanta, GA, USA", "person_id": "P2839227", "email_address": "romain@gatech.edu", "orcid_id": ""}, {"name": "Tushar Kumar", "author_profile_id": "81336490600", "affiliation": "Georgia Institute of Technology, Atlanta, GA, USA", "person_id": "P2839228", "email_address": "tusharkumar@gatech.edu", "orcid_id": ""}, {"name": "Santosh Pande", "author_profile_id": "81409594751", "affiliation": "Georgia Institute of Technology, Atlanta, GA, USA", "person_id": "P2839229", "email_address": "santosh@cc.gatech.edu", "orcid_id": ""}], "doi_number": "10.1145/2048066.2048109", "year": "2011", "article_id": "2048109", "conference": "OOPSLA", "title": "Efficiently speeding up sequential computation through the n-way programming model", "url": "http://dl.acm.org/citation.cfm?id=2048109"}