{"article_publication_date": "10-22-2011", "fulltext": "\n Synthesis of First-Order Dynamic Programming Algorithms Yewen Pu Rastislav Bod\u00b4ik Saurabh Srivastava \nUniversity of California, Berkeley Abstract To solve a problem with a dynamic programming algorithm, \none must reformulate the problem such that its solution can be formed from solutions to overlapping subproblems. \nBe\u00adcause overlapping subproblems may not be apparent in the speci.cation, it is desirable to obtain the \nalgorithm directly from the speci.cation. We describe a semi-automatic synthe\u00adsizer of linear-time dynamic \nprogramming algorithms. The programmer supplies a declarative speci.cation of the prob\u00adlem and the operators \nthat might appear in the solution. The synthesizer obtains the algorithm by searching a space of candidate \nalgorithms; internally, the search is implemented with constraint solving. The space of candidate algorithms \nis de.ned with a program template reusable across all linear\u00adtime dynamic programming algorithms, which \nwe charac\u00adterize as .rst-order recurrences. This paper focuses on how to write the template so that the \nconstraint solving process scales to real-world linear-time dynamic programming algo\u00adrithms. We show \nhow to reduce the space with (i) symmetry reduction and (ii) domain knowledge of dynamic program\u00adming \nalgorithms. We have synthesized algorithms for vari\u00adants of maximal substring matching, an assembly-line \nopti\u00admization, and the extended Euclid algorithm. We have also synthesized a problem outside the class \nof .rst-order recur\u00adrences, by composing three instances of the algorithm tem\u00adplate. Categories and Subject \nDescriptors I.2.2 [Automatic Pro\u00adgramming]: Program synthesis; D.3.3 [Language Con\u00adstructs and Features]: \nConstraints General Terms Algorithms, Languages, Veri.cation Keywords Synthesis, Constraint Solving Permission \nto make digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior speci.c permission and/or a fee. OOPSLA 11, October \n22 27, 2011, Portland, Oregon, USA. Copyright c &#38;#169; 2011 ACM 978-1-4503-0940-0/11/10. . . $10.00. \n 1. Introduction Dynamic programming is an algorithmic technique that ex\u00adploits the structure of an optimization \nproblem [6, 13, 25]. While direct execution of a speci.cation will typically enu\u00admerate all legal solutions, \ndynamic programming breaks down the problem into overlapping subproblems and eval\u00aduates the resulting \nidentical subproblems only once. The challenge in obtaining a dynamic programming algorithm is in reformulating \nthe problem into a recurrence that exposes identical subproblems. Even if the speci.cation already is \na recurrence, it is rare that its subproblems are overlapping. Human insight into the hidden structure \nof the shared sub\u00adproblems is typically necessary. We describe a synthesizer of dynamic programming al\u00adgorithms \n(DPAs) that reduces the need for human insight. Our programmer (1) speci.es the problem with a declara\u00adtive \nspeci.cation, which can be implemented as an exhaus\u00adtive, exponential-time search; and (2) provides the \nsynthe\u00adsizer with (a superset of) operators to be used in the recur\u00adrence underlying the DPA. Given the \nspeci.cation and the operators, the synthesizer automatically produces the DPA, if one exists that uses \nonly the operators supplied. The pro\u00adgrammer is asked neither for suitable identical subproblems nor \nfor the problem is solved from the solutions to these sub\u00adproblems. We build on the idea of synthesis \nwith partial programs, where program templates are completed by the synthesizer to yield a program functionally \nidentical to the speci.ca\u00adtion [1, 12, 20, 23]. We express a generic skeleton of a DPA as a template \nwhose holes are parameterized with a suit\u00adable language of candidate expressions. The completion of a \ntemplate can be viewed as a search, in the space of candi\u00addate DPAs syntactically de.ned by the template, \nfor a DPA that implements the speci.cation. The size of the space typ\u00adically prohibits exhaustive search. \nInstead, ef.cient search techniques have been developed using machine learning [1], version space algebra \n[12], inductive synthesis via constraint solving [20], or invariant inference [23]. In this paper, we \nuse solver of the Sketch synthesizer [20, 2], which reduces the problem to a SAT problem, but encoding \ntechniques should carry over to other constraint solvers, such as SMT solvers. Previously, a DPA has \nbeen successfully synthesized with a partial-program synthesizer, using a template tailored to the problem \n[23]. Our goal is to develop a reusable tem\u00ad plate and thus develop a synthesizer for a broad class of \nDPAs. Our approach is to translate the programmer-provided hints into a template which serves are the \npartial program for the DPA synthesis. Our template generator thus consti\u00adtutes a domain theory for dynamic \nprogramming, in that it embeds knowledge needed to synthesize any DPA in our class. While we study a \nclass of dynamic programming, our lessons will hopefully inform creation of templates for other domains. \n We focus on the domain of linear-time dynamic program\u00adming algorithms, speci.cally those that grow the \nsolution by considering one input element at a time. We characterize DPAs for this class as .rst-order \nrecurrences, but our tem\u00adplate automatically expands to k-order recurrences. We can synthesize solutions \nwith an arbitrary (bounded) number of subproblems. We have synthesized algorithms for variants of maximal \nsubstring matching, an assembly-line optimization, and the extended Euclid algorithm. We have also demonstrated \nsyn\u00adthesis of a problem outside the class of .rst-order recur\u00adrences using a template composed of three \ninstances of our single algorithm template. We make the following contributions: We show that partial-program \nsynthesis based on con\u00adstraint solving is able to produce realistic linear-time dy\u00adnamic programming \nalgorithms. We synthesize these al\u00adgorithms from a reusable template, asking the user only for operators \nthat are to be used in the algorithm.  We show how to produce templates that lead to ef.cient synthesis \nby relying on (i) the properties of the domain of dynamic programming and (ii) symmetry reduction.  \nWe show that several instances of our template can be composed to solve problems that are outside the \nclass of .rst-order recurrences. Because we formulate the domain theory as a partial program, the user \ncan easily modify and grow the template as she sees .t, without understand\u00ading the internals of the synthesizer. \n 2. Overview We .rst introduce two classes of problems solvable by dy\u00adnamic programming (Sections 2.1 \n2.4). Next, we describe the interactions of a programmer with our synthesizer (Sec\u00adtion 2.5). Finally, \nwe give an overview of the synthesizer al\u00ad gorithms and outline our encoding of the synthesis problem \n(Sections 2.6 2.8). 2.1 Dynamic Programming for Optimization Problems Dynamic programming is an algorithmic \ntechnique that ex\u00adploits the structure of an optimization problem [6, 13, 3, 25]. A naive approach to \nsolving an optimization problem is to enumerate all legal solutions and selecting one with the optimum \nvalue e.g., enumerating nonconsecutive sub\u00adsequences of an array followed by selecting one with the largest \nsum. Often, exponentially many candidate solutions need to be explored. Dynamic programming avoids enumeration \nby construct\u00ading an optimal solution from the solutions to subproblems. A problem can be solved with \na dynamic programming al\u00adgorithm (DPA) if 1. the problem exhibits an optimal substructure, i.e., an optimal \nsolution can be composed from optimal solutions to subproblems; and 2. the problem can be broken down \ninto subproblems that are overlapping, i.e., they share identical subproblems.  Because solutions to \nidentical subproblems are reused, typ\u00adically only a polynomial number of subproblems need to be evaluated. \n 2.2 Dynamic Programming for Functional Problems Dynamic programming is also applicable to problems that \ncompute the function of the input, instead of searching a space of solutions. One example is computing \nthe Fibonacci sequence. We synthesize DPAs for two such problems, Oth\u00aderSum and Extended Euclid algorithm, \nin Sections 4.1.7 and 4.1.6, respectively. To be amenable to dynamic programming, these func\u00adtional problems \nmust also have a substructure property with overlapping subproblems. We cannot talk about optimal sub\u00adstructure, \nof course, as there is no notion of an optimal solu\u00adtion. We introduce functional problems separately \nfrom the optimization problems of Section 2.1 because optimization problems permit more ef.cient synthesis \n(cf. Section 3.3). 2.3 The Challenge of Designing a DPA The design of a DPA amounts to de.ning a recurrence \nrela\u00adtion that gives rise to overlapping subproblems whose results can be stored in a table and computed \nby traversing the table, usually in an input-independent order. Sometimes, the de.nition of the problem \nreveals the over\u00adlapping subproblems. For example, the n-th Fibonacci num\u00adber, F (n)= F (n - 1) + F (n \n- 2), is computed from two subproblems, F (n - 1) and F (n - 2), which share the sub\u00adproblems F (n - \n2) and F (n - 3). The DPA table stores the solutions to F (n) and F (n - 1) and can be computed in O(n) \ntime. Most problems must be reformulated to expose identical subproblems, a process that requires human \ninsight. In par\u00adticular, a suitable recurrence may involve enriched or even orthogonal subproblems that \nare not immediate from the problem speci.cation but are essential because identical sub\u00adproblems occur \nonly in the modi.ed problem space. As an example of a subproblem that needs to be invented, consider \nthe problem of .nding a sub-string of an integer array that maximizes the sum of its elements, which \ncould be negative. A naive algorithm searches over all O(n2) sub\u00adstrings, performing either linear work \nor if accumulating the sum then constant work for each substring, resulting in a O(n3) or O(n2) algorithm. \nTo obtain a linear-time DPA that scans the array left-to-right, we need two subproblems: 1. the maximal-sum \nsubstring to the left of index i; and 2. the maximal-sum substring to the left of index i that ends \nat index i.  The latter subproblem may seem non-intuitive but we need it to .nd an optimal substring \nthat spans the index i. The design of dynamic programming algorithms from their declarative speci.cation \nrequires insight and is there\u00adfore taught as an art.  2.4 Running Example: Maximum Independent Sum Given \nan array of positive integers A =[a1,...,an], The Maximum Independent Sum (MIS) problem selects a sub\u00adset \nof non-consecutive array elements that maximizes the sum of its elements [15]. Formally, MIS .nds an \nassign\u00adment of boolean values B =[b1,...,bn], where bi =1 iff ai is selected. Array B is legal if it \ncontains no sub\u00adstring of contiguous 1s. If A = [2, 3, 4, 1] and the assign\u00adment to B is [1, 0, 1, 0], \nthe value of applying the assign\u00adment is apply([2, 3, 4, 1], [1, 0, 1, 0]) = 6. The objective is to maximize \nthe value of the assignment. For instance, MIS([4, 1, 2, 3]) = 7 via the assignment [1, 0, 0, 1]. Note \nthat this problem is not given as a recurrence; in\u00adstead, the de.nition gives the legality and optimality \ncondi\u00adtions. Therefore, it does not suggest any obvious overlapping subproblems.  2.5 Synthesizer Input \nand Output The user provides a problem speci.cation and operator hints, and the synthesizer outputs a \nDPA. Next, we illustrate the inputs and outputs of the synthesizer. Speci.cation The user can give the \nspeci.cation in one of two ways. A functional speci.cation computes the solu\u00adtion to a given problem \ninstance; a declarative speci.cation checks whether a value is a correct output for a given in\u00adstance. \nThe speci.cation for the MIS problem is interesting in that it is functional but is written in a declarative \nstyle of universally quantifying over all solutions: def spec( A = [a1, ...,an]) best= 0 for all bitstrings \n{B = [b1, ...,bn]}: if B is not consecutive: best = max(best, apply(A,B)) return best Programmer hints \nThe synthesizer asks the programmer for hints on which operators will form the recurrence. In the case \nof MIS, the programmer supplies two unary operators (the identity and the zero functions); one binary \noperator (addition); and the optimality function (maximum): unary(x) = {x, 0} binary(x,y) = {x+y} opt(x,y) \n= max(x,y)  The good news is that a suf.cient set of operators can often be obtained from the speci.cation. \nIn MIS, the addition and the maximum arise from maximizing the sum of selected ar\u00adray elements; the identity \nand the zero function correspond to the decision of selecting (identity) or not selecting (zero) the \ncurrent input element. We have not attempted to auto\u00admate the extraction of the operator hints from the \nspeci.ca\u00adtion. The programmer speci.es the operators and then itera\u00adtively increases the number of subproblems \nthe synthesizer is to consider. For small values that do not contain any solu\u00adtion the synthesizer informs \nthe user as such. The program\u00admer iterates until a solution is obtained. Synthesized Solution The synthesizer \noutputs a linear\u00adtime DPA that implements the speci.cation. The synthesized algorithm follows the common \ntable-.lling pattern, shown here for the MIS problem: def mis ( A = [a1,...,an]) p1 = array() // solutions \nto subproblem 1 p2 = array() // solutions to subproblem 2 p1[0] = 0 // base case for subproblem 1 p2[0] \n= 0 // base case for subproblem 2 for i from 1 to n: p1[i] = p2[i-1] + A[i] p2[i] = max(p1[i-1], p2[i-1]) \nreturn max(p1[n], p2[n]) The synthesizer created two subproblems, which we can interpret as follows: \nP1(i): solution to MIS(A[0:i]) provided ai was selected; P2(i): solution to MIS(A[0:i]) provided ai was \nnot selected. Notice how the synthesizer invented subproblems whose iterative computation implicitly \nencodes the condition that no two consecutive elements are picked. The current element is added only \nto P2 which holds solutions that exclude the previous element. The table of the DPA is formed by arrays \np1 and p2, which store the values of the solutions to the subproblems P1(i) and P2(i), respectively. \nWhen A = [3, 1, 2, 4, 4, 6], then p1[5]=9, corresponding to the selection [1, 0, 1, 0, 1], and p2[5]=7, \ncorresponding to the selection [1, 0, 0, 1, 0]. The synthesized DPA initializes the base cases of the \ntwo subproblems to 0 and iterates through the arrays left to right, .lling the table. The synthesizer \ndetermines that the solution to the overall problem is the larger of the solutions to the subproblems \nfrom the last iteration. If desired, we can manually translate the synthesized pro\u00adgram to a recurrence \nrelation. We obtain a .rst-order recur\u00adrence, i.e., solutions to subproblems of size i depend solely \non the solutions of size i - 1. All DPAs that we synthesize are of this form; we discuss the expressiveness \nof .rst-order recurrences in Section 3. Notice that the recurrence is com\u00ad posed from the hints provided \nby the programmer. MIS(n)= max(P1(n),P2(n)) P2(n - 1) + A(n) if n> 0 P1(n)=0 if n =0 max(P1(n - 1),P2(n \n- 1)) if n> 0 P2(n)=0 if n =0 The synthesizer does not generate a proof that DPA cor\u00adrectly implements \nthe speci.cation. The synthesizer s ver\u00adi.er checks the correctness of the synthesized DPA via bounded \nmodel checking [20], which ensures that the spec\u00ad i.cation and the DPA agree on all inputs of a small \nsize. In our experiments, we veri.ed the solution on all arrays of sizes 1 to 4. We have found this small-world-assumption \ntest suf.cient.  2.6 Synthesis as Constraint Solving We view synthesis as a search in the space of candidate \nprograms. We de.ne a space of DPAs and the synthesizer is asked to .nd a DPA that agrees with the speci.cation \non a small number of representative inputs I. The space of DPAs is de.ned with a template program, which \nin our system is called a sketch [20]. The template is translated into a constraints system whose solution \nserves as arguments to the template. The instantiated template yields a DPA that agrees with the speci.cation \non all inputs from I. If inputs in I yield a DPA that fails the bounded model checking test, the counterexample \ninput from the test is added to I and constraint solving is repeated. This is the core solving technique \nunderlying the Sketch synthesizer [20]. To make constraints-based synthesis suitable for synthe\u00adsis of \nDPAs, we need to address two interrelated problems: Completeness: How to de.ne the space of DPAs so \nthat it includes all DPAs of interest?  Ef.ciency: How to de.ne the space of DPAs that induces a constraint \nproblem that is solvable in a few minutes.  The next two subsections overview our solution to these \ntwo problems.  2.7 First-Order Dynamic Programming Algorithms It may be tempting to de.ne the space \nof DPAs with a rather general recurrence template, such as this one that breaks the problem P into two \nunspeci.ed subproblems: P (n)= f(P (g(n)),P (h(n)),n) This template asks the synthesizer to .nd functions \ng, h that suitably break the problem into subproblems, and a function f that composes the solution to \nP (n) from the solutions to the two subproblems P (g(n)) and P (h(n)). It is straightforward to de.ne \na complete template in this general manner, but only at the cost of two problems: Allowing the synthesizer \nto select functions g, h from a large space of functions would likely produce a space of DPAs that is \ntoo large for all state-of-the-art constraint\u00adbased synthesizers [2, 24, 10].  The recurrence template \ndoes not insist that the synthe\u00adsized subproblems are overlapping. As a result, not all re\u00adcurrences \nsynthesized from this template could be trans\u00adlated to a DPA. Another challenge, speci.c to the Sketch \nsynthesizer, is that due to unrolling of calls to P , this tem\u00adplate would result in a constraint system \nwhose size is exponential in n, and thus likely too large even for the small inputs on which Sketch performs \nthe synthesis.  To overcome the .rst problem, we restrict the space of DPAs to .rst-order recurrences \n(FORs), which have the form P (n)= f(P (n - 1),n) that is, we hardcode that a problem of size n is decomposed \ninto a subproblem of size n - 1. Our space of DPAs is more general than this recurrence template may \nsuggest: our template allows (i) k-order recurrences and (ii) multiple subproblems (cf. Section 3.2). \nWe believe that this template covers most linear-time DPAs. To overcome the second problem, rather than \nsynthesiz\u00ading a recurrence, we synthesize directly a table-.lling form of a DPA. By storing solutions \nto subproblems in a linear\u00adsize table, a table-.lling template insists that the synthesized recurrence \nwill exhibit overlapping subproblems. We encode a table-.lling FOR algorithms with the following template: \ndef attempt ( a = [a1,...,an] ): // create arrays to hold the values of subproblems p1 = array() p2 = \narray() // initialize p1[0] = init() // some initial value p2[0] = init() // some initial value // update \nthe subproblem values for i from 1 to n: p1[i] = update1(p1[i-1], p2[i-1], a[i]) p2[i] = update2(p1[i-1], \np2[i-1], a[i]) // the terminate function composes the .nal solution return terminate(p1[n], p2[n]) The \ntemplate we illustrate here uses two subproblems; the number of subproblems is adjusted accordingly by \nour synthesizer that auto-generates the template from user-input. The structure of this template is identical \nto that of the so\u00adlution for the MIS problem in Section 2.4; the differences are in the underlined functions, \nwhich the synthesizer se\u00adlects from a space of functions by constraint solving. The initialization function, \nint, returns an integer constant (in our experiments, the constant was restricted to -8, 0, and 8). The \npropagation function, update, and the termination function, terminate, are selected from a space of functions \nthat are compositions of the user-provided operators. For MIS, the search spaces are de.ned by the sets \nbelow. The {| e1 | e2 | ... |} operator asks the synthesizer to select be\u00adtween expressions e1 and e2. \n int() := {| -8 | 0 |8 |} update(x,y,aval) := {| x | aval | y+x | x+max(y,aval) | /*...*/ |} terminate(x,y) \n:= {| x | y+x | max(x,y) | /*...*/ |} Our space of candidate DPAs is thus formed by this FOR template \nwith the underlined functions selected from function spaces described in the next subsection.  2.8 Ef.cient \nConstraint Solving Even after we restricted DPAs to .rst-order recurrences, the search space remains \nexcessively large. For instance, in MIS, which is a relatively simple problem, the space contains 84,934,656 \ncandidate DPAs. To further reduce the space, we apply space and symmetry reductions on the space of update \nand termination functions. In space reduction, we exploit properties of the optimization problems. Speci.cally, \nwe restrict the syntactic forms of our functions, ruling out DPAs we know to be incorrect. In symmetry \nreduction, we prune away functions that are semantically equivalent (symmetric) to other programs despite \nbeing syntactically distinct. Space Reduction Space reduction is made possible by two observations on \nthe nature of the optimization problems (Section 2.1). First, each step of the algorithm makes an optimal \nchoice from several legal alternative solutions, each computed from optimal solutions of smaller size. \nThe update function thus needs to have the optimality function opt in the root of the expression selecting \nthe optimal solution is the last step of its computation. In other words, the syntactic form of the update \nfunction thus must be update(x,y,aval) := opt(f1(x,y,aval),...,fk(x,y,aval)); where functions fi compute \nthe values of legal solutions. For example, in MIS the goal is to return the largest sum, corre\u00adsponding \nto the best assignment of non-contiguous elements. We can insist that update functions have the max operator \nat the root of the expression. Second, we observe that in FOR algorithms, each of the legal solutions \ncan depend on at most one subproblem. This argument becomes clearer if we view an optimization prob\u00adlem \nas returning n optimal decision, one for each element of the input. In the case of MIS, for the legal \nsolution, if the value for P1(i) was constructed from the values to both P1(i - 1) and P2(i - 1), these \ntwo solutions would some\u00adhow need to be combined and the overall problem would return more than n decisions. \nFor instance, in MIS the op\u00adtimal solution corresponds to an array of 1 and 0. This ob\u00adservation allows \nus to syntactically restrict functions fi such that each consumes only one optimal subproblem. Note that \neach optimal subproblem can still be used to compute multi\u00adple alternative solutions. These solutions \nhowever cannot be combined other than in the opt function that selects one of these alternatives. The \nform of the update function is now re\u00adstricted to this form, which reduces the size of the candidate \nDPAs that we need to search: update(x,y,aval) := max(choose subset(f1(x,aval),f2(y,aval))) fi(x,aval) \n:= {| x | aval | max(x,aval) | /*...*/|} Symmetry Reduction We further reduce the number of programs \nby noticing that the operators such as + and max are commutative. For example, the expression x+max(y, \nz) is identical to the expression max(z, y)+ x. We prune away this symmetry by de.ning canonical expression \ntrees and ensuring that only canonical trees are constructed. For MIS, the symmetry reduction, along \nwith space reduction, reduced the number of update functions from 589,824 to 65,536. Symmetry reduction \nbecomes vital if the problem is not an optimization problem, in which case we cannot apply space reduction. \n3. The DPA Synthesizer This section describes the implementation of the DPA syn\u00adthesizer. The synthesizer \nconsists of two parts, a front\u00adend template generator and a back-end synthesis constraint solver. The \nfront-end translates the user-provided problem speci.cation and hints into a template that ef.ciently \nen\u00adcodes the space of DPAs. The backend resolves the template to a desired algorithm. We use the Sketch \n[2] solver, which was outlined in Section 2.6. This section details template generation, which was overviewed \nin Sections 2.7 and 2.8. Recall that the main concern is balancing expressiveness and ef.ciency: the \ntem\u00adplate must de.ne a space of programs that includes all DPAs of interest yet must be small enough \nto give rise to easy-to\u00adsolve constraint systems. We .rst present a space of DPAs that is suf.ciently \nexpressive and then gradually prune it by exploiting domain-speci.c space reduction and symmetry reduction. \n3.1 First-Order Recurrences Here we de.ne the space of general FORs, which contains all DPAs of interest \nto us. Subsequent subsections will nar\u00adrow down the de.nition of the recurrence, tailoring it to dy\u00adnamic \nprogramming. Assume that the speci.cation is a predicate spec(a, o) which holds when o is a solution \nto the problem instance a. Assume that array a =[a1, ..., an], where each am is a scalar or a tuple of \nvalues. We encode our algorithm as a .rst-order recurrence. FOR Algorithm:  F OR(a)= terminate(p1(n), \n..., pk(n)) p1(0) = init1() . . . pk(0) = initk() p1(m)= update1(p1(m - 1), ..., pk(m - 1),a(m)) . . \n. pk(m)= updatek(p1(m - 1), ..., pk(m - 1),a(m)) The correctness condition asserts that the synthesized \nalgo\u00adrithm is correct on all problem instances from a set I of small-size instances: .a . I. spec(a, \nF OR(a)) We de.ne some terminology: A subproblem, denoted pi, is a series of subproblem instances that \nare solved with the same update function.  A sub-problem instance is a particular instance in a sub\u00adproblem, \ndenoted pi(m).  An update is a function of the form:  updatei :(p1,...,pk,a) . pi There is one update \nfunction per subproblem. When we need to compute the value of a subproblem instance pi(m), we invoke \nthe update function updatei(p1(m - 1), ..., pk(m - 1),am). This update follows a .rst-order recurrence \nbecause subproblem instances at step m de\u00adpend only on subproblem instances at step m - 1. pp(m)= update(pp(m \n- 1),a(m)) A termination is a function of the form: terminate :(p1,...,pk) . output The termination function \ncomputes the solution to the original problem from the solutions to subproblems. Since subproblems range \nover a synthesized domain, the termination function maps subproblems back to the do\u00admain of the original \nproblem. The synthesizer determines the minimal number of sub\u00adproblems by attempting to solve the problem \nwith one sub\u00adproblem, k =1, and gradually increasing k as the solver determines that a solution cannot \nbe found for a given value of k. The user can, of course, .x k should she have an intu\u00adition as to how \nmany subproblems are needed. We want to remark that our FOR space includes kth-order recurrences via \nthe following reduction: p(m)= update(p(m - 1), ..., p(m - k)) . p(m)= p1(m) p1(m)= update(p1(m - 1),...,pk(m \n- 1)) p2(m)= p1(m) . . . pk(m)= pk-1(m) We now describe the function spaces of the functions init, update, \nand terminate with context free grammars. init init(a) := -8 | 0 |8 The init function establishes the \ninitial conditions of the re\u00adcurrence. The user may suggest values other than those used in our experiments, \nincluding other constants or a function parameterized by the problem instance a, such as the .rst element \nof the array, a[1]. update updatei(p1, ..., pk,a) := p1 | ... | pk | a | unary?(updatei(p1, ..., pk,a)) \n | binary?(updatei(p1, ..., pk,a), updatei(p1, ..., pk,a)) where unary?(x) := unary1(x) | ... | unaryl(x) \nbinary?(x, y) := binary1(x, y) | ... | binarym(x, y)  The update function computes the solution to a \nsubprob\u00adlem at step m given am and the solutions at step m - 1. The function space includes all expressions \nthat can be con\u00adstructed from the user-provided unary and binary operators. The recursive formulation \nof update does not scale in gen\u00aderal. Since update sends k values to 1 output, if we let m be the total \nnumber of user speci.ed binary operators, the k total number of functions for each updatei is at least \nmas it takes k binary operators to reduce k +1 values to 1 output. In section 3.3 and 3.4 we discuss \nhow to search this space ef.ciently. terminate terminate(p1, ..., pk) := update(p1, ..., pk) The termination \nfunction is a one-step update function that operates on the subproblems from the last, nth step. It is \ndrawn from the language of the update grammar.  3.2 The FOR Template We now implement the FOR algorithm \nas a template pro\u00adgram that executes a bottom-up, table-.lling dynamic pro\u00adgramming algorithm with explicit \nmemoization of overlap\u00adping sub-problems: def algorithm ( a = [a1,...,an] ): // create arrays to hold \nthe values of sub-problems p1 = array() ... pk = array() // initialize p1[0] = init() ... pk[0] = init() \n// update the sub-problem values for i from 1 to n: p1[i] = update1(p1[i-1], pk[i-1], a[i]) ... pk[i] \n= updatek(p1[i-1], pk[i-1], a[i]) // terminate return terminate(p1[n],...,pk[n]) The FOR template uses \nk arrays to hold solutions to k subproblems. The underlined functions are de.ned in their own templates. \nAs an example, consider the templates for the function init, which is a direct translation of the init \ncontext-free grammar from the previous section: def basecase (): case =0 | 1 | 2 case 0: return -8 case \n1: return 0 case 2: return 8  3.3 Update Functions for Optimization Problems In the FOR template, the \nupdate function can be any expres\u00adsion constructed from the user-provided operators. We will now restrict \nthe syntactic form of the update function to re\u00ad.ect the structure of an optimization problem. Our encoding \nis applicable to the problems de.ned in Section 2.1 but not to the functional problems in Section 2.2. \nThe functional prob\u00ad lems can, however, take advantage of the more general (but weaker) optimizations \ndescribed in the following subsection. Recall the two special structural properties of optimiza\u00adtions \nproblems (cf. Section 2.8): 1. Assume that Si(m) is the set of solutions (not all optimal) to the subproblem \ninstance pi(m). The value of pi(m) is the optimal solution from Si(m). The computation of pi(m) thus \nhas the form opt(Si(m)), i.e., the opt function is at the root of the computation tree. During synthesis, \nwe can thus rule out update functions where opt is syntactically in a non-root position. 2. Each solution \nsi,j (m) . Si(m) is computed from exactly  one subproblem pi(m - 1). If si,j (m) were to combine Figure \n1. The update function for optimization problems. solutions to multiple subproblems, it would have to \ncom\u00adbine the histories of optimal decisions from both of these subproblems into a single history of m \ndecisions, forcing the examination of the two histories for the purpose of deciding which decisions to \npreserve. In order to avoid examining histories, an optimal solution in an FOR DPA is constructed by \nextending the history of one optimal so\u00adlution with a single optimal decision, based on the current input \nelement. To capitalize on these restrictions, we synthesize an up\u00addate function by asking the synthesizer \nto make the follow\u00ading decisions: Synthesize functions that compute rk solutions, with r solutions for \neach of the k optimal subproblems pi. These functions extend solutions to subproblems of size m - 1 into \nsolutions for a problem of size m. The solutions are stored in variables extj,l, l = k, j = r. The solutions \nextj,l are computed with a combiner function that con\u00adsumes a solution and the current input element. \nThe tem\u00adplate of the combiner function is de.ned in the next sub\u00adsection. ext1,1 = combiner(p1,a) . . \n. extr,1 = combiner(p1,a) . . . ext1,k = combiner(pk,a) . . . extr,k = combiner(pk,a) Next we ask the \nsynthesizer to decide, for each subprob\u00adlem pi, which of the solutions extj,l solve pi. This de\u00adcision \npopulates the sets Si de.ned above with the solu\u00adtions extj,l. The template selectsynthesizes into a \nfunc\u00ad s tion that selects up to s of its arguments. pi(m)= opt(select(ext1,1, . . . , extr,k)) s Figure \n1 shows the structure of the update function. As an example of how the update function is constructed, \nconsider the MIS problem de.ned in Section 2.4. Let us call its two subproblems pick and no pick. The \nsynthesizer .rst creates the solutions: We can extend the sub-problem no pick into two solutions, by \npicking versus not picking the current array element: ext pick(no pick(m - 1)) = no pick(m - 1) + am \next no pick(no pick(m - 1)) = no pick(m - 1) + 0 In contrast, we can extend the subproblem pick only \ninto one solution, by not picking the current element, becaus we cannot pick contiguous array elements: \next no pick(pick(m - 1)) = pick(m - 1) + 0 Next, to .nd an update function that solves the no pick subproblem, \nthe synthesizer needs to select from the three solutions, by resolving the selecttemplate: s no pick(m)= \nmax(select2(ext pick(no pick(m - 1)) ext no pick(pick(m - 1)), ext no pick(no pick(m - 1)))) The synthesized \nselect function picks two solutions across which the no pick problem optimizes. This gives us the .nal \nupdate function for the no pick subproblem. no pick(m)= max(ext no pick(pick(m - 1)), ext no pick(no \npick(m - 1))) Discussion: In this section, we have restricted the update function syntactically. By \ndoing so, we sought to reduce the function space to be explored during constraint solving. Whenever possible, \nsyntactic restrictions seem preferable over symmetry-breaking predicates [17] used in Section 3.4. This \nis because syntactic restrictions offer the advantage of simultaneously reducing the size of the constraint \nsystem. In contrast, symmetry-breaking predicates prune the space by adding clauses to the constraint \nsystem; these clauses prevent symmetric candidates from arising as solutions but do so at the cost having \nthe solver maintain con.ict clauses over variables in the predicate.  3.4 Encoding of the Combiner Function \nTemplate In the previous subsection, we restricted the form of the up\u00addate function to re.ect the properties \nof optimization prob\u00adlems. Here, we develop a template for the combiner function that is invoked from \nthe update function. The combiner is used by both the optimization problems (Section 2.1) and the functional \nproblems (Section 2.2). The combiner func\u00ad tion reduces k inputs, x1,...,xk, to an output value using \nthe unary and binary operators provided by the programmer. Recall that in an optimization problem, we \nhave k =2, and the combiner computes a solution from a solution to a single subproblem and the current \ninput element. In the functional problems, k = 2 because the combiner is allowed to com\u00adbine the current \ninput element multiple subproblems. For this reason, functional problems in particular bene.t from the \noptimizations of this subsection. The goal of the two optimizations is to reduce the space of combiner \nfunctions by eliminating those functions that are syntactically distinct but semantically equivalent. \nFirst, we normalize the combiners by distributing unary operators to the leaves of the expression. Second, \nwe eliminate com\u00adbiners that are identical up to commutativity of binary oper\u00adators. We adopt the restriction \nthat each input xi is used in the combiner exactly once. This restriction will allow us to work with \ntrees, rather than dags. Should the DPA recurrence require a duplicate use of xi, the synthesizer will \nimplicitly work around this restriction by introducing an additional subproblem, whose value will be \nequal to xi. For the sake of conciseness, we introduce an in.x form of the grammar binary as 8, and abbreviate \nthe grammar unary as u. Distribute Unary Operators to Leaves In the update grammar of Section 3.1, unary \noperators can appear at any position of the expression tree; they can also appear multiple times. For \ninstance, the grammar generates the expression identity(zero(max(x, y)+ z)) as well as the semantically \nequivalent expression max(zero(x), zero(y)) + zero(z). We consider the latter expression canonical and \neliminate the former. To distribute unary operators across binary op\u00aderators, we introduce a combiner \ngrammar where the unary operators appear only in the leafs of the expression tree: combiner(x1,...,xk) \n:= reduce(u(x1),...,u(xk)) Combiners thus .rst apply the unary operators on the inputs, then reduce \nthem exclusively with binary operators. The reduce grammar is introduced in the following section. We \nnow give conditions under which this combiner grammar de.nes the same space of (semantic) functions as \nthe update grammar. In situations when the combiner grammar loses some functions from the update grammar, \nwe show how to recover the lost expressiveness by adding binary operators. As a running example, we use \nthe set of unary operators that we used most frequently in our exper\u00adiments: negate, identity, zero; \nthe set of binary operators are +, -, *, /, max, min, %. The reducer grammar is equivalent to the update \ngram\u00admar if the following conditions hold: 1. The set of unary operators is closed under composition, \ni.e., the grammar u . ... . u and the grammar u generate the same space of (semantic) functions. 2. \nEach unary operator distributes over each binary opera\u00adtor, i.e., the grammar u(x 8 y) generates the \nsame space of semantic functions as the grammar u(x) 8 u(y).  We currently check these properties manually. \nOn our run\u00adning example, the .rst property is easy to show. Any com\u00adposition containing the unary operator \nzero yields the zero operator; any composition without zero is either negate or identity depending on \nthe number of negate s used. For example, negate . identity . zero . negate = zero. The second property, \nu(x 8 y)= u(x) 8 u(y), requires some thought. We need to show that for all instances of 8 and u on the \nleft-hand side there exists an equivalent instance of 8 and u on the right-hand side. We .nd that the \noperator negate does not distribute over the operators max, min, %, while all other unary operators distribute \nover all binary operators. For instance, negate(x/y) = negate(x)/identity(y). In cases when a unary operator \nu does not distribute across a binary operator 8 , such as negate(max(x, y)), we extend the grammar of \n8 with a new binary operator u (x8 y), which restores the expressiveness by hard-coding the combination \nof the two operators. In practice, we found that it was not necessary to keep the combiner grammar equivalent \nto the update grammar by adding these new operators. To see that the combiner grammar is equivalent to \nthe update grammar if the two conditions hold, observe that any expression from the latter grammar can \nbe rewritten into an expression of the former grammar with these transforma\u00adtions: u(xi) . u(xi) // base \ncase u . ... . u(exp) . u(exp) // by property 1 u(exp1 . exp2) . u(exp1) . u(exp2) // by property 2 While \nwe are currently performing the legality check manually, in the future, this step can be automated using \nthe synthesizer, which can automatically .nd the equivalent instance on the right-hand side, if one exists. \nSymmetry reduction for commutative binary operators We now write the template for the function reduce, \nwhich encodes all possible ways of reducing k inputs x1,...,xk, into one output with exclusively binary \noperators. If any operators in reduce are commutative, then the space of re\u00adducers includes symmetric \nexpressions that are identical up to commutativity. In this section, we explain the sym\u00admetry reduction \nfor commutative binary operators, which prunes the search space without losing any expressiveness. For \nthe sake of presentation, we assume all binary operators are commutative; we explain below how to handle \nreduce that may include non-commutative operators. Note that the binary operators may or may not be associative. \nWe do not address symmetry reduction based on associativity because we found symmetries due to associativity \nless harmful in our experiments. The space of functions de.ned by reduce is de.ned with this grammar: \nreduce(A) := reduce(A1) 8 reduce(A2) reduce(x) := x Here, A denotes the set of inputs, A = x1,...,xn, \nand A1, A2 an arbitrary partition of the set A. The reduce function takes in a set of inputs, arbitrarily \nsplits the set into two partitions, recursively reduces each partition, combining the results with an \narbitrary binary operator. Because all operators in 8 are commutative, two expres\u00adsion trees using the \noperators 8 are symmetric if one tree can be transformed into another tree by swapping children at the \n8 nodes. The symmetry is an equivalence relation. For example, these two trees belongs to the same equivalent \nclass: x 8 (y 8 z) and (y 8 z) 8 x. To reduce the sym\u00admetry, we de.ne a canonical tree for each equivalence \nclass and admit only the canonical expression tree while rejecting non-canonical trees. We now de.ne \na predicate that identi.es canonical trees. Consider the power set 2A, where A is the set of inputs. \nEach element B of 2A has a natural correspondence to a bit mask b, where the bit at b[i] marks whether \nB contains element xi. The lexicographic order on bit-masks de.nes a total order over 2A, where B1 <B2 \nif b1 <b2. We extend the total order on 2A to a partial order on T , the set of all expression trees \nwith inputs B . 2A. Let t1 be an expression tree with inputs B1, and t2 with inputs B2, we say t1 <t2 \niff B1 <B2. Notice that this is a partial order because given a subset of inputs B, there are many expression \ntrees over B, which are un-ordered. The canonical predicate c states that an expression tree t is canonical \nif its children are canonical and that its left child is less than its right child: c(leaf)= true c(tree(l, \nr)) = c(l) . c(r) . l<r We now give an outline of a proof that each equivalence class has a unique canonical \nelement: 1. Existence of a canonical element. Any tree can be canonicalized by identifying all nodes \nwhose left child is greater than its right child, and swapping the children. Note that swapping the children \nat a particular node does not affect the lexicographic orders of children at any other sub-trees. Thus, \nthe canonical tree of an equiva\u00adlence class can be obtained by selecting any tree from the class and \ncanonicalizing it. Since the canonicalization is performed by swapping, the canonicalized tree will be \nin the same class, proving every class has a canonical ele\u00adment. Figure 2 illustrates the canonicalization \nprocess. 2. Uniqueness of the canonical element. Suppose an equiv\u00adalence class contains two distinct \ncanonical expression trees t1 and t2. Then there exists a sequence of swaps that transform t1 into t2. \nConsider a particular sub-tree which has its children swapped. Since neither subsequent not previous \nswaps can change the order of the children of this particular sub-tree, either t1 or t2 must have its \nchildren ordered in descending order at this sub-tree, and is thus not canonical.  Up to this point \nwe have assumed that all binary op\u00aderators are commutative. To canonicalize trees with non\u00adcommutative \noperators, we de.ne for every non-commutative Figure 2. Canonicalization of the left tree into the right \ntree. The process swaps the mis-ordered children at two nodes. Each node is labeled with the subset of \ninputs to the subtree rooted at that node. The labels 0001, 0010, 0100, 1000 denote x1,x2,x3,x4, respectively. \n binary operator 8 a companion operator, 8 , de.ned with 8 (x, y)= 8(y, x). With the companion operator, \nwe can swap children of a non-commutative nodes by substituted the operator with its companion. Having \nde.ned the canonical predicate c, we can explain how it is used during synthesis. The synthesizer consults \nthe predicate to rule out trees that are not canonical. The predicate is evaluated as the tree is generated, \nto allow for early pruning of the search compared to the alternative of evaluating the predicate on a \ncomplete tree. We show here the template the de.nes the expression tree and evaluates the predicate: \ndef reduce( A ): if (|A| == 1): return the only element in A else: A1,A2 = split A to two arbitrary subsets \nassert A1,A2 are nonempty, disjoint unions of A assert weight(A1) < weight(A2) return reduce(A1) 8 reduce(A2) \nThe synthesizer always selects a canonical tree. After we have added the companion operators to the grammar \n8, the may use either the original operator or its companion, depending on how inputs are used in the \nexpression tree. In our experiments, we have found that this template did not lead to suf.cient scalability. \nThis was in part because at the time of our experiments, the Sketch synthesizer trans\u00adlated recursive \ntemplates like reduce eagerly into an expo\u00adnentially large formula. Therefore, we precomputed the reduce \ntemplate stati\u00adcally, effectively evaluating the canonical predicate c on all subtrees, and inlined the \nresulting grammar into the com\u00adbiner template. For illustration, here is the resulting encod\u00ading for \na combiner of three elements: combiner(x, y, z) := reduce(u(x),u(y),u(z)) reduce(x, y, z) := (x 8 y) \n8 z | (x 8 z) 8 y | (y 8 z) 8 x  3.5 Constraint Solving In addition to suitably encoding a template \nfor the DPA, we need to assert a correctness condition that guides the synthe\u00adsizer towards a correct \nprogram. The speci.cation is given as a predicate spec(input, output) that the output of the DPA must \nsatisfy. This predicate is usually implemented with a naive, exponential-time algorithm. We assert the \ncorrectness condition over a bounded domain I of inputs. We de.ne I with arrays of length N, where each \narray element contains integer or tuple of integers ranging between [0,M]. We have found that a naive \nalgorithm is too expensive to encode in Sketch because it is translated to exponentially large formulas. \nTherefore, we precompute the speci.cation as a table, by evaluating the naive algorithm of.ine, with \na script. The speci.cation is then supplied to Sketch in a table\u00adlookup form: naive( A ): if (A = [0, \n0, ..., 0, 0]) return out1 . . . if (A =[M, M, ..., M, M]) return outm We have several options in how \nwe de.ne the correctness condition. First, one can assert that the template is equivalent to the naive \nalgorithm, by asserting, at once, that the program must be correct on all inputs from I. This is inef.cient \nbecause it create a large constraint system. Instead, we take advantage of the counterexample-guided \ninductive synthesis (CEGIS) loop in the Sketch synthesizer. This re.nement procedure reduces the function \nspace itera\u00adtively, by gradually asserting that the synthesized program must be correct on one more input. \nThis input is obtained by checking the correctness of the synthesized candidate algo\u00adrithm. When we get \na correct algorithm, we stop. If the al\u00adgorithm is incorrect, we have an input-output pair on which the \ncandidate algorithm and the naive algorithm diverge. We assert this particular input-output pair as an \nadditional con\u00adstraint to the template. This reduces the load in the synthe\u00adsizer signi.cantly by having \nit consider only the inputs that the previous algorithm failed on. Our .nal option is to assert correctness \non all arrays of a small size, then gradually consider bigger input arrays. Finally, we want to note \nthat whenever a naive algorithm is dif.cult to write, the input-output pairs needed for syn\u00adthesis can \nbe generated manually from a declarative speci.\u00adcation. Usually, only a small number of examples is needed \nto synthesize a correct algorithm, and these can be obtained manually. 4. Experiments In this section \nwe evaluate our approach by synthesizing var\u00adious algorithms. We wish to evaluate whether the symmetry \nand space reduction techniques we developed in Sections 3.4 and 3.3, along with the assertion techniques \nin Section 3.5 make the synthesizer ef.cient while retaining completeness of the template. The synthesizer \nis evaluated on the follow\u00ading benchmark problems. The .rst four benchmark problems are from a class \nof multi-marking problems. In these problems, given an ar\u00adray of integers, the objective is to .nd another \narray of in\u00adtegers, an assignment, such that the dot product of the two arrays is maximized. The problems \ndiffer in their require\u00adments for the assignment arrays. The Maximal Independent Sum (MIS) problem takes \nan array of positive integers and .nds a selection array consisting of 0 and 1 with no adjacent 1s. The \nMaximal Segment Sum (MSS) problem takes an ar\u00adray of positive or negative integers and .nds a selection \nof 0 and 1 such that all the 1s are consecutive. The Maximal Alternating Sum (MAS) problem takes an array \nof positive or negative integers and .nds a selection of 0, 1, and -1 such that all the 1s and -1s are \nconsecutive, and that the 1s and -1s must interleave. The Maximal Multi-Marking (MMM) prob\u00adlem takes \nan array of positive or negative integers and .nds a selection of 0, 1, and -1 such that no two 0, 1, \nor -1 are consecutive. Our next benchmark is the Assembly Line (ASSM) prob\u00adlem. Given two assembly lines \nA and B, and the costs for staying on a line (stayi) or switching to a different line (switchi), the \nproblem is to .nd the minimal cost of travers\u00ading the assembly. Our last two benchmarks are the OtherSum \n(OSUM) and the Extended Euclid (EUC) problems, which will be de\u00adscribed below. We will .rst show the \nsynthesized solutions, followed by empirical evaluation on the effects of our encodings. 4.1 Solutions \nto Synthesis Problems 4.1.1 MIS User Hints: unary(x)= x | 0 binary(x, y)= x + y | max(x, y) opt = max \n Synthesized Recurrence Relation: mis(n) = max(pick(n),no pick(n)) pick(n) = max(no pick(n-1) + array(n)) \nno pick(n) = max(pick(n-1),no pick(n-1)) pick(0) = 0 no pick(0) = 0 Here pick(n) is the sub-problem \nof the best legal assignment up to the nth array element where we are forced to pick the th nelement. \nno pick(n) denotes the best legal assignment th up to the nth element and that we are forced to avoid \nthe nelement.  4.1.2 MSS User Hints: unary(x)= x | 0 binary(x, y)= x + y | max(x, y) opt = max  Synthesized \nRecurrence Relation: mss(n) = max(suf.x(n),best(n)) suf.x(n) = max(max(0,suf.x(n-1))+array(n)) best(n) \n= max(suf.x(n-1),best(n-1)) suf.x(0) = 0 best(0) = 0  Here suf.x(n) denotes the best suf.x assignment \nending at th the narray element, and best(n) denotes the best legal assignment up to the nth element. \n 4.1.3 MAS User Hints: unary(x)= x | 0 |-x binary(x, y)= x + y | max(x, y) opt = max  Synthesized Recurrence \nRelation: mas(n) = max(suf.x pos(n),suf.x neg(n),best(n)) suf.x pos(n) = max(0,suf.x2(n-1)) + array(n) \nsuf.x neg(n) = max(0,suf.x1(n-1)) - array(n) best(n) = max(suf.x pos(n-1),suf.x neg(n-1),best(n-1)) suf.x \npos(0) = 0 suf.x neg(0) = 0 best(0) = 0 th Here suf.x1(n) denotes the best legal suf.x up to the nelement \nthat ends in 1 while suf.x2(n) ends in -1. best(n) denotes the best legal assignment up to the nth element. \n 4.1.4 MMM User Hints: unary(x)= x | 0 |-x binary(x, y)= x + y | max(x, y) opt = max  Synthesized Recurrence \nRelation: mmm(n) = max(mark 0(n),mark 1(n),mark -1(n)) mark ig(n) = max(mark -1(n-1),mark 1(n-1)) mark \npi(n) = max(mark 0(n-1)+array(n), mark -1(n)+array(n-1)) mark ne(n) = max(mark 0(n-1)-array(n), mark \n1(n)-array(n-1)) mark ig(0) = 0 mark pi(0) = 0 mark ne(0) = 0  Here mark ig(n),mark pi(n),mark ne(n) \nare the sub-problems th of the best legal assignment up to the narray element where we are forced to \nignore,pick,and negate the nth ele\u00adment, respectively.  4.1.5 ASSEM User Hints: unary(x)= x | 0 binary(x, \ny)= x + y | min(x, y) opt = min Synthesized Recurrence Relation: assem(n) = min(line1(n),line2(n)) line1(n) \n= min(line1(n-1)+stay1(n),line2(n-1)+switch1(n)) line2(n) = min(line2(n-1)+stay2(n),line1(n-1)+switch2(n)) \nline1(n) = 0 line2(n) = 0 Here line1(n),line2(n) denote the optimal cost of n assem\u00adblies that end in \nline1 and line2, respectively.  4.1.6 Extended Euclid Algorithm Here we attempt to synthesize the extended \nEuclid Algo\u00adrithm (EUC): Given two integers x and y with greatest com\u00admon divisor (gcd) of 1, .nd coef.cients \na and b such that a * x + b * y =1. The greatest common devisors of 2 num\u00adbers can be found by the Euclid \nalgorithm as follows: euclid(x,y): p1 = array() p2 = array() p1[0] = x p2[0] = y i=0 while(p2[i] != 0): \ni += 1 p1[i] = p2[i-1] p2[i] = p1[i-1] % p2[i-1] return p1[i] Suppose the user vaguely remembers that \nEUC is performed by traversing the computing histories of Euclid s Algorithm, p1 and p2, backwards. The \nuser .rst reverse the histories: q1= reverse(p1),q2= reverse(p2), and asks the synthe\u00adsizer to formulate \na DPA that computes the coef.cients a and b by expressing the following constraints: on input (x,y): \n(a,b) = DPA(q1,q2) assert(a*x+b*y == 1 OR a*y+b*y == -1) Note that the user was not completely sure if \nthe coef.\u00adcient s parities, hence she expresses a relaxed constraint, and is happy if the coef.cients \ncan compute either positive or negative 1. User Hints: unary(x)= x | 0 binary(x, y)= x + y | x * y | \nx%y | x - y | x/y  Synthesized Recurrence Relation: e euc(n) = (p1(n),p2(n)) p1(n) = p2(n-1) p2(n) = \np1(n-1)+p2(n-1)*(q1(n)/q2(n)) p1(n) = 1 p2(n) = 1  Here there is no obvious concise interpretation for \nthe mean\u00adings of the sub-problems. In short, solution takes advantage of the fact that top(n-1) = bot(n) \nand bot(n-1) = top(n) mod bot(n). 4.1.7 Other Sum In this section we study the composability of the \nFOR tem\u00adplates, and argue that extending the template for a speci.c problem can be done without expertise \nin program synthe\u00adsis. OtherSum problem: given an array of integers a = [a1, ..., an], compute the array \nb =[s - a1,...,s - an] where s =Sin =1ai. That is, b[i] equals the sum of every element in a other than \nthe ith one. The catch is that we can\u00adnot use subtraction and the algorithm must be O(n) time. We use \nthe easier-to-express subtraction algorithm can as speci.cation: def spec (A = [a1,...,an]) total = 0 \nfor i from 1 to n: total += A[i] ret = array() for i from 1 to n: ret[i] = total - A[i] return ret \nNext, we asked the synthesizer to produce a DPA restricted to use only addition. The synthesizer answers \nthat no such algorithm exists, in less than a second. Since the domain theory does not include the desired \nalgorithm, we make some conjectures on the properties of the desired algorithm: 1. In the speci.cation, \nthe answer is computed with multiple loops. Perhaps multiple passes over the array are needed also in \nthe desired algorithm. 2. Since the synthesizer failed to identify a recurrence that works around the \nlacks of subtraction, the key to an ef.cient algorithm seems to be a different traversal order.  We \nthen wrote a more general template that encoded these conjectures. The .rst conjecture is encoded by \ncomposing multiple DPA templates: def sketch(A): temp1 = DPA1(A) temp2 = DPA2(A,temp1) result = DPA3(temp1,temp2) \n return result  Both DPA1 and DPA2 are modi.ed to return the entire sub-problem arrays (as in Extended \nEuclid algorithm, we extract the entire sub-problem p1 and p2) so that it can be consumed by the next \nDPA as inputs, whereas DPA3 is acting in place of the function terminate except it now attempts to summarize \nentire arrays rather than just values of the last iteration. To encode the second conjecture, we relaxed \nthe loop iteration order from the default left-to-right to any arbi\u00adtrary array traversal order, to be \nselected by the synthesizer. We do this by asking the synthesizer to produce an arbi\u00adtrary reordering \nreorder that translates the iteration space [1,...,n - 1] to the space [r(1),...,r(n - 1)]. The map reorder \nis implemented in DPA1 and DPA2 using the array reorder that is initialized by the synthesizer. The synthesizer \n.nds an initialization of reorder that leads to a correct algo\u00adrithm. def DPA1(A): // initialize the \narray with n synthesized constants This may seem like a random traversal. However, a closer inspection \nyields a remarkable observation: The iteration reordering on DPA1 and DPA2 are always the reverse of \none another! For example, if the reorder is [3, 4, 0, 1, 2] in DPA1, then the reorder is [2, 1, 0, 4, \n3] for DPA2. This suggested to us that the traversal in DPA1 could be left-to-right and the traversal \nin FOR2 could be right-to-left. To synthesize a suitable map reorder in DPA2, we replaced reorder[i] \nwith n-i+??, which then synthesized the following .nal program: otherSum(A): temp1 = array() temp1[0] \n= 0 for ifrom 1 to n-1: temp1[i] = temp1[i-1]+A[i-1] temp2 = array() temp2[0] = 0 for ifrom 1 to n-1: \ntemp2[n-i-1] = temp[n-i]+A[n-i] ret = array() for ifrom 0 to n-1:int[n] reorder = [1, ..., 1] | ... | \n[n - 1, ..., n - 1] // all reorders ret[i] = temp1[i] + temp2[i] ret = array() ret[reorder[0]] = 0 for \ni from 1 to n-1: ret[reorder[i]] = combiner2(ret[reorder[i-1]], A[reorder[i-1]]) return ret def DPA2(A,B): \n return ret  4.2 Empirical Studies In this section we show the effects of different encodings of the \nrecurrence and different assertion schemes on the scalabilities of the solver. int[n] reorder = [1, ..., \n1] | ... | [n - 1, ..., n - 1] // all reorders We ran all our experiments on a four CPU, 2GHz ma\u00adret \n= array() chine, with 4GB of memory, since we iterate through the ret[reorder[0]] = 0 for i from 1 to \nn-1: ret[reorder[i]] = combiner3(ret[reorder[i-1]], A[reorder[i-1]],B[reorder[i-1]]) return ret def DPA3(A,B,C): \nret = array() for i from 0 to n-1: ret[i] = combiner3(ret[i-1],A[i-1],B[i-1]) return ret The observant \nreader will notice that we have asked the synthesizer to produce a map reorder that is .xed to a particular \nvalue of n, and will (only) work for a .xed value n. We have a reason to ask the synthesizer for reorder \nbound to a .xed n. The synthesized reorder serves as a demonstration of a particular traversal order \nthat allows us to solve OtherSum in O(n)-time. As such, it reveals the algo\u00adrithm on a given n, in the \nspirit of angelic programming [5]. The demonstration provides hints for the user on how the problem might \nbe solved in the general case for all n. There exist many maps reorder that lead to a correct algorithm, \nand the synthesizer is capable of returning any of such maps. One possible value of reorder is [3, 4, \n0, 1, 2]. numbers of sub-problems, here we shown only the data of the last iteration. Figure 3 compares \nthe representation size and the func\u00ad tion space size of our encodings. The representation size is measured \nin nodes, where each node is an operator in the formula constructed from the template by the Sketch syn\u00adthesizer. \nThe function space size is measured in bits, where each combination of bits correspond to a distinct \ncompletion of the partial program, i.e., a candidate program. We remark that the naive grammar encoding \ndescribed in Section 3.2 creates a constraint system so large that the synthesizer runs out of memory \nwhile trying to construct it. In contrast, the combiner (Section 3.4) and extender (Section 3.3) encod\u00ad \ning signi.cantly reduce the formula size and the space of functions explored. For the harder problems, \nthe extender encodings reduce the representation and function space size dramatically. For instance, \nMAS has a function space of 172 bits with the combiner encoding, but a mere 40 bits for the extender \nencoding, which is a 1040-fold improvement. Figure 4 shows that reducing the problem s representa\u00ad tion \nsize and function space reduce the synthesis time for a particular algorithm. However, it is not true \nacross algo\u00adrithms. For instance, MMM has fewer nodes and function space size than MAS, yet it took signi.cantly \nlonger to syn\u00adthesize. This maybe caused by an easier constraint of the Figure 3. Constraint Size and \nFunction Space. GR denotes the naive grammar encoding of the update functions, while COMB and EXT employ \ncombiners and extenders, respec\u00adtively. Note that the extender encoding is only available for optimization \nproblems. Benchmk GR COMB EXT rep space rep space rep space MIS MSS MAS MMM ASM OSM EUC mem N.A. mem \nN.A. mem N.A. mem N.A. mem N.A. mem N.A. mem N.A. 871 25 876 25 4785 172 4251 71 mem N.A. 1023 23 38837 \n766 453 9 458 9 1101 40 634 19 1905 33 N.A. N.A. N.A. N.A. Prob. CO MB EXT time space time space MIS \n16 355 9 95 MSS 35 784 10 41 MAS N.A. mem 467 193 MMM N.A. mem 5909 790 ASM N.A. mem 101 90 OSM 2 54 \nN.A. N.A. EUC N.A. mem N.A. N.A. Figure 4. Synthesis Time (s) and Memory Usage (MB). Benchmk ALL CEG \nINC time space time space time space MIS 9 355 9 95 10 31 MSS 10 784 10 41 14 48 MAS 689 mem 467 193 \n58 160 MMM N.A. mem 5909 790 626 316 ASM N.A. mem 101 90 30 60 OSM 15 377 2 54 N.A. N.A. EUC N.A. mem \nN.A. mem 316 1410 Figure 5. Solving Time (s) and Memory Usage (MB). ALL denotes we assert all the correctness \nconditions at once, CEG uses the default CEGIS loop, while INC asserts correctness conditions from small \nto large. MAS problem, perhapse multiple candidate algorithms are correct. Also notice that the EUC algorithm \nfailed to synthe\u00adsize with the default CEGIS assertions since the constraint system is too dif.cult. \nNext, we experiment with the different encodings of cor\u00adrectness. Figure 5 shows the effect of different \nencodings of the correctness assertions, as discussed in Section 3.5 on the runtime and memory usage \nof the solver. Notice the dramatic improvement when the incremental assertion is employed, turning the \nEUC from unsynthesizable to synthesizable. 5. Related Work We relate our work to alternative approaches \naddressing the two novel ideas we present here, namely that of semi\u00adautomatic synthesis of DPA, and that \nof using partial pro\u00adgrams as domain theories. 5.1 Derivation/synthesis of DPA Most prior work concerned \nwith the derivation of ef.cient dynamic programming algorithms exploits an optimization theorem, while \nsome prior work has attempted to formu\u00adlate the synthesis of DPA as a constraint-solving synthesis problem. \nWe will .rst discuss the re.nement transformation\u00adbased approaches below, followed by program synthesis\u00adbased \nDPA generation. With optimization theorem-based approaches, it is un\u00adclear how a user may take her algorithmic \ninsight and .nd a suitable transformations that exploits the insight. She would have to link her program \ninsight to the meta-reasoning at the level of transformations. In our approach, the program\u00admer does \nnot need to reason about transformations. She thus reasons directly about programs (if at all the domain \ntem\u00adplate needs tweaking), and not about transformations that produce programs. With prior synthesis-based \napproaches, a constraint-system has to be setup for each DPA instance. Re.nement theorem-based Yao shows \nthat if a problem, formulated as a recurrence relation, satis.es the quadrangle inequality [26], then \nthe problem can be solved in quadratic time, which improves on the asymptotic complexity of a naive cubic-time \nalgorithm that implements the speci.cation directly. The quadrangle inequality allows us to safely re\u00adstrict \nthe range of subproblems that need to be considered to produce an optimal solution. Phrased in terms \nof the mem\u00adoization table computed by a dynamic programming algo\u00adrithm, the inequality prunes the number \nof cells that the algo\u00adrithm needs to visit. The inequality does not seems to appli\u00adcable to linear-time \nproblems that we consider in this paper. Additionally, the problem must be formulated in a form that \nsatis.es the preconditions of the theorem. Derivation of linear-time dynamic programming algo\u00adrithms \nhas been considered by Bird and de Moor [4]. The op\u00ad timization theorem, called the thinning theorem, \ntransforms problems into an ef.cient solution if two suitable preorders are found. To reduce the burden \nof the work needed before the theorem can be applied, Sasano et al restrict their do\u00admain to maximum-weightsum \nproblems, which can be au\u00adtomatically translated to ef.cient algorithms as long as the problem can be \nexpressed as a mutumorphism [16]. As far as we can tell, this process requires the programmer to iden\u00adtify \nthe suitable sub-problems, which we seek to avoid with our synthesis methodology. Existing approaches \nfor derivation of dynamic program\u00adming algorithms rely on instantiating the program from an algorithmic \ntheory [14]. The approach relies on the user to invent or symbolically calculate dominance relations \nfor the algorithmic theory. Program synthesis-based In most partial-program synthe\u00adsizers [20, 23], \neach partial program/template is speci.c to the synthesis problem at hand. In other words, only one functionally-unique \nprogram can be synthesized from a par\u00adtial program. (One exception may be synthesis, i.e., infer\u00adence, \nof program invariants in the context of program anal\u00adysis, where a single template can synthesize into \nmany pos\u00adsible invariants [7, 8, 22, 21].) Srivastava et al synthesized the .rst dynamic programming \nalgorithm from a partial pro\u00adgram. For each problem they have a separate template that may capture multiple \northogonal solutions which veri.ably meet the speci.cation, but which is restricted to that prob\u00adlem. \nHere, we generalize templates enough to be reusable across problems.  5.2 Partial programs as domain \ntheories Our work is motivated by the desire to equip programmers with practical program synthesis tools. \nWe believe that it is dif.cult to achieve this goal if we ask programmers to de\u00advelop optimizing program \ntransformations and/or instanti\u00adate algorithmic theories. Everyday programmers may miss the necessary \nformal background; another open question is teachability of program derivation tools. For that reason, \nour domain theory for dynamic programming is purely syn\u00adtactic, expressed as a program template called \na partial pro\u00adgram. With partial programs, programmers can write skele\u00adtons of desired code rather than \na transformation that pro\u00adduces that code. We develop our partial program-based domain theory in the \nSKETCH language, which has been used to synthe\u00adsize cryptographic algorithms, stencil kernels and concurrent \ndata structures [20, 18, 19]. While previous uses of SKETCH have developed one partial program per desired \nsynthesized program, we have developed a partial program that serves as a domain theory from which an \nentire class of dynamic pro\u00adgramming problems can be synthesized. This is the .rst use of programmer-editable \npartial programs as a domain theory. Informally, we say that a partial program forms a domain theory \nif the partial program can synthesize programs for a range of speci.cations. The .rst synthesizer with \nsuch a partial-program domain theory was SmartEdit, which learnt editor macros from programmer demonstrations \n[12]. The synthesis algorithm was the version space algebra; the partial program encoded a language of \nlearnable macros. While a single partial program could learn a large range of macros, the domain theory \nwas produced by the tool creator and was not modi.able by the user, while we attempt to make the domain \ntheory user-modi.able. Itzhaky et al [11] developed partial-program-based do\u00ad main theories that were \nprogrammable by the synthesizer expert. Their partial programs, called target languages, could produce \nlinear-time graph classi.ers and perform .nite dif\u00adferencing on set operations. It will be interesting \nto explore whether dynamic programming can be expressed in their language. Their partial programs were \nexpressed in a re\u00adstricted language dictated by their synthesis algorithm and are likely accessible only \nto the synthesis expert. 6. Conclusion We have shown that linear-time dynamic programming algo\u00adrithms \ncan be synthesized with rather small guidance from the programmer. Our synthesizer is based on constraints \nsolving, where a program template de.nes the space of pro\u00adgrams explored by the synthesizer. This paper \nfocuses on how to encode the dynamic programming template so that it includes all algorithsm of interest \nwithout making constraint solving prohibitively expensive. Acknowledgments This material is based on \nwork supported by U.S. Depart\u00adment of Energy grant under Grant Number DE SC0005136, by NSF under Grant \nNumber CCF 0916351, by a grant from University of California Discovery program, as well as by contracts \nwith Intel and Microsoft. References [1] D. Andre and S. J. Russell. State abstraction for pro\u00adgrammable \nreinforcement learning agents. In AAAI/IAAI, pages 119 125, 2002. [2] e. a. Armando Solar-Lezama. The \nSketch synthesizer. http://sketch1.csail.mit.edu/demo/.  [3] R. Bellman. The theory of dynamic programming. \nBull. Amer. Math. Soc., 60:503 515, 1954. [4] R. Bird and O. de Moor. The Algebra of Programming. Prentice-Hall, \n1996. [5] R. Bod\u00b4ik, S. Chandra, J. Galenson, D. Kimelman, N. Tung, S. Barman, and C. Rodarmor. Programming \nwith angelic nondeterminism. In Hermenegildo and Palsberg [9], pages 339 352. [6] T. H. Cormen, C. E. \nLeiserson, R. L. Rivest, and C. Stein. Introduction to Algorithms. The MIT Press, New York, 2001. [7] \nS. Gulwani, S. Srivastava, and R. Venkatesan. Program analysis as constraint solving. In PLDI, 2008. \n[8] S. Gulwani, S. Srivastava, and R. Venkatesan. Constraint\u00adbased invariant inference over predicate \nabstraction. In VMCAI 09: Proceedings of the 2009 Conference on Veri.cation Model Checking and Abstract \nInterpretation, 2009. [9] M. V. Hermenegildo and J. Palsberg, editors. Proceedings of the 37th ACM SIGPLAN-SIGACT \nSymposium on Principles of Programming Languages, POPL 2010, Madrid, Spain, January 17-23, 2010. ACM, \n2010. [10] S. Itzhaky, S. Gulwani, N. Immerman, and M. Sagiv. A simple inductive synthesis methodology \nand its applications. In OOPSLA, pages 36 46, 2010. [11] S. Itzhaky, S. Gulwani, N. Immerman, and M. \nSagiv. A simple inductive synthesis methodology and its applications. In W. R. Cook, S. Clarke, and M. \nC. Rinard, editors, OOPSLA, pages 36 46. ACM, 2010.  [12] T. A. Lau, S. A. Wolfman, P. Domingos, and \nD. S. Weld. Programming by demonstration using version space algebra. Machine Learning, 53(1-2):111 156, \n2003. [13] U. Manber. Introduction to Algorithms: A Creative Approach. Addison-Wesley Longman Publishing \nCo., Inc. Boston, MA, USA, 1989. [14] S. Nedunuri and W. R. Cook. Synthesis of fast programs for maximum \nsegment sum problems. In J. G. Siek and B. F. 0002, editors, GPCE, pages 117 126. ACM, 2009. [15] S. \nNedunuri, D. R. Smith, and W. R. Cook. A class of greedy algorithms and its relation to greedoids. In \nA. Cavalcanti, D. D\u00b4eharbe, M.-C. Gaudel, and J. Woodcock, editors, ICTAC, volume 6255 of Lecture Notes \nin Computer Science, pages 352 366. Springer, 2010. [16] I. Sasano, Z. Hu, M. Takeichi, and M. Ogawa. \nMake it practical: A generic linear-time algorithm for solving maximum-weightsum problems. In In Proceedings \nof the 5th ACM SIGPLAN International Conference on Functional Programming (ICFP 00, pages 137 149. ACM \nPress, 2000. [17] I. Shlyakhter. Generating effective symmetry-breaking pred\u00adicates for search problems. \nDiscrete Applied Mathematics, 155(12):1539 1548, 2007. [18] A. Solar-Lezama, G. Arnold, L. Tancau, R. \nBodik, V. Saraswat, and S. Seshia. Sketching stencils. In PLDI, pages 167 178, 2007. [19] A. Solar-Lezama, \nC. G. Jones, and R. Bodik. Sketching concurrent data structures. In PLDI, 2008. [20] A. Solar-Lezama, \nL. Tancau, R. Bodik, S. Seshia, and V. Saraswat. Combinatorial sketching for .nite programs. In ASPLOS-XII: \nProceedings of the 12th international conference on Architectural support for programming languages and \noperating systems, pages 404 415. ACM, 2006. [21] S. Srivastava and S. Gulwani. Program veri.cation using \ntemplates over predicate abstraction. In PLDI, 2009. [22] S. Srivastava, S. Gulwani, and J. S. Foster. \nVS3: SMT solvers for program veri.cation. In CAV, 2009. [23] S. Srivastava, S. Gulwani, and J. S. Foster. \nFrom program veri.cation to program synthesis. In POPL, 2010. [24] S. Srivastava, S. Gulwani, and J. \nS. Foster. From program veri.cation to program synthesis. In Hermenegildo and Palsberg [9], pages 313 \n326. [25] Wikipedia. Dynamic programming, Aug. 2011. [26] F. F. Yao. Ef.cient dynamic programming using \nquadrangle inequalities. In Proceedings of the twelfth annual ACM symposium on Theory of computing, STOC \n80, pages 429 435, New York, NY, USA, 1980. ACM.    \n\t\t\t", "proc_id": "2048066", "abstract": "<p>To solve a problem with a dynamic programming algorithm, one must reformulate the problem such that its solution can be formed from solutions to overlapping subproblems. Because overlapping subproblems may not be apparent in the specification, it is desirable to obtain the algorithm directly from the specification. We describe a semi-automatic synthesizer of linear-time dynamic programming algorithms. The programmer supplies a declarative specification of the problem and the operators that might appear in the solution. The synthesizer obtains the algorithm by searching a space of candidate algorithms; internally, the search is implemented with constraint solving. The space of candidate algorithms is defined with a program template reusable across all linear-time dynamic programming algorithms, which we characterize as first-order recurrences. This paper focuses on how to write the template so that the constraint solving process scales to real-world linear-time dynamic programming algorithms. We show how to reduce the space with (i)~symmetry reduction and (ii)~domain knowledge of dynamic programming algorithms. We have synthesized algorithms for variants of maximal substring matching, an assembly-line optimization, and the extended Euclid algorithm. We have also synthesized a problem outside the class of first-order recurrences, by composing three instances of the algorithm template.</p>", "authors": [{"name": "Yewen Pu", "author_profile_id": "81487647091", "affiliation": "UC Berkeley, Berkeley, CA, USA", "person_id": "P2839135", "email_address": "evanthebouncy@gmail.com", "orcid_id": ""}, {"name": "Rastislav Bodik", "author_profile_id": "81100033082", "affiliation": "UC Berkeley, Berkeley, CA, USA", "person_id": "P2839136", "email_address": "bodik@cs.berkeley.edu", "orcid_id": ""}, {"name": "Saurabh Srivastava", "author_profile_id": "81100062128", "affiliation": "UC Berkeley, Berkeley, CA, USA", "person_id": "P2839137", "email_address": "saurabhs@cs.berkeley.edu", "orcid_id": ""}], "doi_number": "10.1145/2048066.2048076", "year": "2011", "article_id": "2048076", "conference": "OOPSLA", "title": "Synthesis of first-order dynamic programming algorithms", "url": "http://dl.acm.org/citation.cfm?id=2048076"}