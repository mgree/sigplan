{"article_publication_date": "10-22-2011", "fulltext": "\n Da Capo con Scala Design and Analysis of a Scala Benchmark Suite for the Java Virtual Machine Andreas \nSewe Mira Mezini Technische Universit\u00a8at Darmstadt {sewe, mezini}@st.informatik.tu-darmstadt.de Abstract \nOriginally conceived as the target platform for Java alone, the Java Virtual Machine (JVM) has since \nbeen targeted by other languages, one of which is Scala. This trend, however, is not yet re.ected by \nthe benchmark suites commonly used in JVM research. In this paper, we thus present the design and analysis \nof the .rst full-.edged benchmark suite for Scala. We furthermore compare the benchmarks contained therein \nwith those from the well-known DaCapo 9.12 bench\u00admark suite and show where the differences are between \nScala and Java code and where not. Categories and Subject Descriptors C.4 [Performance of Systems]: Performance \nattributes; D.2.8 [Metrics]: Perfor\u00admance measures General Terms Languages, Measurement, Performance \nKeywords Benchmarks, dynamic metrics, Scala, Java 1. Introduction While originally conceived as a target \nplatform of the Java language only, the Java Virtual Machine (JVM) [31] has since been targeted by numerous \nprogramming languages, ranging from Ada to Z-code. Among the most popular of these are Clojure, Groovy, \nJRuby, Jython, and Scala, which have gathered a following in recent years, as they bring expressive constructs \nto a stable and portable platform. However, the benchmark suites so often used in JVM re\u00adsearch do not \nyet re.ect this development. Not only do older benchmark suites like SPECjbb20051 and Java Grande [10] \naltogether ignore languages other than Java, but also are the two modern benchmark suites commonly used \nin research 1 See http://www.spec.org/jbb2005/. Permission to make digital or hard copies of all or \npart of this work for personal or classroom use is granted without fee provided that copies are not made \nor distributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. OOPSLA 11, October 22 27, 2011, Portland, Oregon, USA. Copyright \nc &#38;#169; 2011 ACM 978-1-4503-0940-0/11/10. . . $10.00 Aibek Sarimbekov Walter Binder University of \nLugano {aibek.sarimbekov, walter.binder}@usi.ch still .rmly Java-focused: The SPECjvm2008 suite2 does \nnot contain a single benchmark written in a non-Java language. The DaCapo suite [7] contains only a single \none, namely jython. But while a single such benchmark might serve as an interesting curiosity,3 it does \nnot allow for deeper insights into the execution characteristics of non-Java languages on the JVM. We \nhave therefore set out to complement the exist\u00ading Java benchmark suites with a new suite featuring a \nlarge set of non-Java applications. The language we chose for our endeavour is Scala [34], a statically-typed \nlanguage with roots in both functional and object-oriented programming. The reason for choosing a single \nlanguage rather than several is simple: Any relevant benchmark suite needs to cover a broad selection \nof real\u00adworld applications; choosing a single language leads to a comprehensive yet cohesive suite. The \nreason for preferring Scala over other possible candidates like Clojure, Groovy, JRuby, or Jython is \nmore involved: Of the aforementioned .ve languages, four are dynamically-typed. But this single language \nfeature has signi.cant impact [49] on the perfor\u00admance of the JVM, a machine which, until recently [41], \nhas been speci.cally tailored towards a single, statically\u00adtype language, namely Java. In contrast, the \nexecution char\u00adacteristics of other statically-typed languages like Scala on the JVM are less well understood, \nin part due to the lack of a comprehensive benchmark suite. Scala = Java mod JVM? thus becomes an interesting \nresearch question [45]: Does Scala code differ signi.cantly from Java code4 when viewed at the bytecode \nlevel? In this paper we will therefore extensively analyze our Scala benchmark suite with respect to \nvarious dynamic met\u00adrics, e.g., the benchmarks instruction mixes, the code hot\u00adness at different levels, \nor the use of re.ection and box\u00ading. These metrics are computed for three distinct, but re\u00adlated reasons: \nFirst, to validate that the suite is indeed a valid Scala benchmark suite, i.e., that Scala code contributes \nsigni.cantly to the benchmarks executions. Second, to en\u00ad 2 See http://www.spec.org/jvm2008/. 3 The \njython benchmark, e.g., exhibits very many short-lived objects. 4 For the sake of brevity, we refer to \nJava bytecode that was compiled from Java and Scala sources as Java code and Scala code, respectively. \n sure that the selected Scala benchmarks exhibit a varied set of execution characteristics. Third, to \ninvestigate whether the Scala benchmarks execution characteristics differ from those of an established \nJava benchmark suite. All our metrics are hence chosen to be relevant to common JVMs but inde\u00adpendent \nof any particular implementation. We will therefore focus exclusively on properties of bytecode rather \nthan ma\u00adchine code. Moreover, our analysis is focused on code rather than memory and pointer behaviour; \na detailed study of the latter is beyond the scope of this paper. Our evaluation is concerned with comparing \nthe execu\u00adtion characteristics of Scala and Java code;5 it is explicitly not designed to compare the \nperformance of Scala with the performance of raw Java. Such a study would require two sets of equivalent \nyet idiomatic applications written in both languages. Instead, our Scala benchmark suite, like the Da-Capo \nsuite we compare it against, consists of independently\u00addeveloped, real-world applications. The scienti.c \ncontributions of this paper are two-fold: 1. The design of a new, comprehensive, and open-source Scala \nbenchmark suite that complements the popular Da-Capo benchmark suite [7].  2. The analysis of both suites \nwith respect to various static and dynamic metrics.  This paper is structured as follows: First, Section \n2 gives a brief overview of our evaluation setup. Next, Section 3 describes our benchmark suite and the \ndesign criteria we applied. Then, Section 4 describes the metrics we used to compare the Scala benchmarks \nto each other and to the Java benchmarks of the DaCapo suite as well as the results of this analysis. \nSection 5 discusses related work, before Section 6 concludes with a summary of our analysis. Finally, \nSection 7 suggests directions of future work. Two appendices offer additional notes and observations \non building a benchmark suite and on the collection of dynamic metrics.  2. Evaluation Setup In contrast \nto several other researchers [7, 21], we refrain entirely from using hardware performance counters to \nchar\u00adacterize the benchmark suite s workloads. Instead, we rely exclusively on metrics which are independent \nof both the speci.c JVM and architecture used, as it has been shown that, e.g., the JVM used can have \na large impact for short\u00adrunning benchmarks [21]. Another, pragmatic reason for us\u00ading bytecode-based \nmetrics is that doing so does not obscure the contribution of the source language and its compiler as \nmuch as the JVM and its just-in-time compiler would do. To collect metrics which are independent of the \nspeci.c JVM, we extended the JP2 calling-context pro.ler [42, 43] to collect execution frequencies for \nbasic blocks. The rich 5 Scala also compiles to CIL [20]. However, an analysis of Scala s execution characteristics \non the .NET platform is beyond the scope of this paper. pro.les collected by the call-site aware JP2 \nenable us to derive all code metrics with one exception (see below) from a single, consistent set of \nmeasurements. We use a specially-written callback [42] that ensures that only the benchmark itself is \npro.led by JP2; this includes both the benchmark s setup and its actual iteration, but excludes JVM startup \nand shutdown as well as the benchmark harness. This methodology ensures that the results are not diluted \nby startup and shutdown code [19]. Also, the Scala benchmarks remain undiluted by Java code used only \nin the harness. More information on JP2 can be found in Appendix B. We furthermore extended TamiFlex \n[8] to measure the use of re.ection. All these measurements have been nor\u00admalized with respect to a dummy \nbenchmark, which simply does nothing during its iteration. This normalization step en\u00adsures that re.ection \nused during JVM startup or shutdown or within the harness does not perturb the measurements. All measurements \nwere conducted on a Quad-Core AMD Opteron 8356 processor with 40 GiB of RAM running build 17.1-b03 of \nthe Java HotSpot 64 bit Server VM (build 17.1\u00adb03, JRE build 1.6.0 22-b04) in mixed mode. Externally \nmulti-threaded benchmarks where run with the maximum number of threads. We used version 9.12 of the DaCapo \nbenchmark suite and a pre-release version of our own Scala benchmark suite. All benchmarks safe one (factorie)in \nthe latter suite are compiled for version 2.8.1 of the Scala language. This ensures that any difference \nbetween Scala benchmarks are due to the workload itself rather than due to differences in the compiler. \nThe internal validity of our .ndings is high; both JP2 and TamiFlex have been shown to collect stable \nand complete pro.les [6, 8]. The chosen JRE, however, exerts some in\u00ad.uence on the pro.les, as both Scala \nand Java benchmarks spend a considerable portion of their execution in the JRE s code (cf. Section 3.3). \nThis needs to be kept in mind when interpreting the results.  3. Benchmark Design The Scala benchmark \nsuite is based on the latest release (ver\u00adsion 9.12, nicknamed Bach ) of the DaCapo benchmark suite, \na suite already popular among JVM researchers which speci.cally strives for ease of use [7]. Table 1 \nsummarizes the 12 Scala benchmarks we added to the 14 Java bench\u00admarks;6 thus, our suite is almost on \npar with the current re\u00adlease of the DaCapo benchmark suite and larger than its pre\u00advious one, despite \na more limited set of well-known appli\u00adcations written in Scala to choose from. To allow for easy experimentation \nwith different inputs, several benchmarks come with more than the two to four input sizes (small, de\u00adfault, \nlarge,and huge) typical for the DaCapo benchmarks. This gives rise to 51 unique workloads, i.e., benchmark\u00adinput \ncombinations. The DaCapo benchmark suite offers 44 such workloads. 6 See http://www.scalabench.org/. \n  Benchmark Description Inputs (#) References actors Trading sample with Scala and Akka actors tiny \ngargantuan (6) apparat Framework to optimize ABC, SWC, and SWF .les tiny gargantuan (6) factorie Toolkit \nfor deployable probabilistic modeling tiny gargantuan (6) [32] kiama Library for language processing \nsmall default (2) [47] scalac Compiler for the Scala 2 language small large (3) [44] scaladoc Scala \ndocumentation tool small large (3) scalap Scala class.le decoder small large (3) scalariform Code formatter \nfor Scala tiny huge (5) scalatest Testing toolkit for Scala and Java programmers small huge (4) scalaxb \nXML data-binding tool tiny huge (5) specs Behaviour-driven design framework small large (3) tmt Stanford \nTopic Modeling Toolbox tiny huge (5) [37]  Table 1. The 12 Scala benchmarks selected for inclusion in \nthe benchmark suite.  3.1 Covered Application Domains The external validity of our .ndings hinges on \nthe bench\u00admarks representativeness. We have therefore chosen a large set of applications from a range \nof different domains as the basis of our benchmark suite. In fact, only two categories of application \nare completely absent from the Scala benchmark suite but present in the latest release of the DaCapo \nbench\u00admark suite: client/server applications (tomcat, tradebeans, and tradesoap) and in-memory databases \n(h2). Of these cat\u00adegories, the earlier release (2006-10) of the DaCapo suite also covers in-memory databases \n(hsqldb). The absence of # Methods called 104.5 104      client/server applications from our suite \nis explained by the 103.5 fact that all three such DaCapo benchmarks rely on either a Servlet container \nor an application server, a dependency which a Scala benchmark within this category will have to share. \nIn fact, a benchmark based on the popular Lift web framework was designed [45] but then discarded, as \nits pro\u00ad.le proved to be dominated by the Java-based container. The absence of in-memory databases is \nexplained by the fact that, to the best of our knowledge, no such Scala application yet exists that is \nmore than a thin wrapper around Java code. While the range of domains covered is nevertheless broad, \nseveral benchmarks occupy the same niche. This was a deliberate choice made to avoid bias from prefer\u00adring \none application over another in a domain where Scala is frequently used: automated testing (scalatest, \nspecs), source-code processing (scaladoc, scalariform), or machine\u00adlearning (factorie, tmt). In this \npaper, we will thus show that the inclusion of several applications from the same domain is indeed justi.ed; \nin particular, the respective benchmarks exhibit a distinct instruction mix (cf. Section 4.1).  3.2 \nCode Size Using established source code metrics [12], Blackburn et al. [7] argue that the DaCapo benchmarks \nexhibit much richer code complexity, class structures, and class hierar\u00adchies than their predecessors. \nSource code metrics, how\u00adever, are less useful when assessing our Scala benchmarks  103 104 Figure 1. \nNumber of classes loaded and method called at least once by the DaCapo ( ) and Scala ( ) bench\u00admarks \n(default input size). as there are, aside from various micro-benchmarks,7 no pre\u00addecessors to compare \nthe benchmarks against. Also, source code metrics are dif.cult to compare across language bound\u00adaries. \nAll metrics to compare Scala with Java code are thus based on bytecode rather than source code. The selected \nbenchmarks are of signi.cant code size, comparable to those of the DaCapo benchmarks. As Fig\u00adure 1 shows, \neven comparatively simple Scala programs like scalap consist of thousands of classes, although the number \nof methods actually called per class is, in general, slightly lower than for their Java counterparts. \nThis is due to the translation strategy the Scala compiler employs [44]; a sin\u00adgle Scala class often \nresults in several Java classes with few methods apiece. This fact may have performance rami.ca\u00ad 7 See \nhttp://www.scala-lang.org/node/360.  tions, as class metadata stored by the JVM consumes a sig\u00adni.cant \namount of memory [35]. For the Scala benchmarks, abstract and interface classes on average account for \n13.8 % and 13.2 % of the loaded classes, respectively. For the Java benchmarks, the situation is similar: \n11.3 % and 14.1 %. In case of the Scala bench\u00admarks, though, 48.4 % of the loaded classes are marked \n.nal. This is in stark contrast to the Java benchmarks, where only 13.5 % are marked thusly. This discrepancy \nis in part explained by the Scala compiler s translation strategy for anonymous functions: On average, \n32.8 % of the classes loaded by the Scala benchmarks represent such functions. The methods executed at \nleast once by the Scala bench\u00admarks consist, on average, of just 2.9 basic blocks, which is much smaller \nthan the 5.1 basic blocks found in the Java benchmarks methods. Not only do methods in Scala code generally \nconsist of less basic blocks, they also consist of less instructions, namely 17.3 on average, which is \nagain signi.cantly smaller than the 35.8 instructions per method of the Java benchmarks; on average, \nScala methods are only half as large as Java methods.  3.3 Code Sources For research purposes the selected \nbenchmarks must not only be of signi.cant size and representative of real-world applications, but they \nmust also consist primarily of Scala code. This requirement rules out a large set of Scala pro\u00adgrams \nand libraries as they are merely a thin wrapper around Java code. To assess to what extent our benchmarks \nare com\u00adprised of Java and Scala code, respectively, we thus catego\u00adrize all bytecodes loaded by the \nbenchmarks according to their containing classes package names and source .le at\u00adtributes into one of \n.ve categories: Java Runtime. Packages java, javax, sun, com.sun,and com.oracle; *.java source .les Other \nJava libraries. Other packages; *.java source .les Scala Runtime (Java code). Package scala; *.java Scala \nRuntime (Scala code). Package scala;8 *.scala Scala application and libraries. Other packages, *.scala \nRuntime-generated classes (proxies and mock classes) are categorized like the library that generated \nthe class, even though the generated class typically resides in a different package than the generating \nlibrary. Based on the categorization, the inner circles in Figure 2 show how the loaded bytecodes are \ndistributed among the .ve classes, with the circles areas signifying the relative number of bytecodes \nloaded by the benchmarks. As can be seen, all benchmarks contain signi.cant portions of Scala code, albeit \nfor three of them (actors, factorie,and tmt)the actual application consists only of a rather small Scala \nker\u00ad 8 The package scala.tools was excluded; it contains, e.g., the Scala com\u00adpiler and the ScalaDoc \ntool that are used as benchmarks in their own right. actors apparat factorie  scalac scaladoc kiama \n scalatest scalap scalariform  specs scalaxb tmt  Figure 2. Bytecodes loaded and executed by each \nof the 12 Scala benchmarks (default input size): Java runtime ( ), Java libraries ( ), Scala runtime \nwritten in Java ( )and Scala ( ), and Scala application ( ). nel. Still, in terms of bytecodes executed \nrather than merely loaded, all but two benchmarks (actors, scalatest) spend at least two thirds of their \nexecution within these portions, as is signi.ed by the outer rings. The two exceptional bench\u00admarks nevertheless \nwarrant inclusion in a Scala benchmark suite:Inthe caseofthe actors benchmark, the Java code it primarily \nexecutes is part of the Scala runtime rather than the Java runtime. In the case of the scalatest benchmark, \na vast portion of code loaded is Scala code. Like the scalatest benchmark, the specs benchmark is particularly \nnoteworthy in this respect: While it loads a large number of bytecodes belonging to the Scala application, \nit spends most of its execution elsewhere, namely in parts of the Scala runtime. This behaviour is explained \nby the fact that the workloads of both benchmarks execute a series of tests written using the ScalaTest \nand Specs testing frame\u00adworks, respectively. While the volume of test code is high, each test is only \nexecuted once and then discarded. This behaviour places the emphasis on the JVM s interpreter or baseline \njust-in-time compiler as well as its class meta\u00addata organization. As such, this kind of behaviour is \nnot well-covered by current benchmark suites like DaCapo or SPECjvm2008, but nevertheless of real-world \nimportance since tests play a large role in modern software development.  Native method invocations \nare rare; on average, 0.44 % of all method calls target a native method. The actors bench\u00admark (1.8 %), \nwhich makes heavy use of actor-based concur\u00adrency [29], and the scalatest benchmark (2.1 %), which uses \nthe Java runtime library quite heavily, are the only notable outliers. These values are very similar \nto those obtained for the Java benchmarks; on average 0.49 % of method calls target native methods, with \ntomcat (2.0 %)and trades\u00adoap (1.3 %) being the outliers. The actual execution time spent in native code \ndepends on the used Java runtime and on the concrete execution platform, as none of the benchmarks analyzed \nin this paper contain any native code themselves. Since we focus on dynamic metrics at the bytecode level, \na detailed analysis of the contribution of native code to the overall benchmark execution time is not \nwithin the scope of this paper. An initial study by one of the authors on native code execution, using \nthe SPECjvm989 and SPECjbb2005 benchmarks, can be found elsewhere [5].   4. Benchmark Analysis In the \nfollowing, we analyze the Scala and DaCapo bench\u00admark suites with respect to a variety of metrics all \nof which affect compilation and compiler optimizations. An analysis with respect to properties more relevant \nto a JVM s garbage collector [7, 16] than to its interpreter and compiler is subject to future work. \nAll these metrics are dynamic in nature. If we, e.g., say that a call site targets a single method only \n(cf. Section 4.2) and thus is particularly suitable for inlining, this means that the call site in question \nonly had a single target method during the benchmark s execution; the call site may or may not be monomorphic \nin general. Also, any concrete JVM may be able to infer that a call site is de-facto monomorphic only \nin some of the cases, due to the inevitable limitations of any static analysis. However, we assume an \nidealized JVM for our metrics. The numbers we report are thus upper bounds of what a JVM can infer. \n4.1 Instruction Mix The instruction mix of a benchmark can serve as an indicator whether the application \nin question is, e.g., array intensive or .oating-point intensive; moreover, it may show patterns discriminating \nScala from Java code. We have thus used JP2 to obtain precise frequency counts for all Java bytecode \ninstructions. 9 See http://www.spec.org/jvm98/. Unlike some related work on workload characterization, \nwe neither consider all 156 instructions10 individually [13, 14] nor do we group them manually [14, 19]. \nInstead, we apply principal component analysis (PCA) [36] to discern meaningful groupings of instructions. \nThis approach offers a higher-level view of the instruction mix which is objective rather than being \nin.uenced by one s own intuition of what groupings might be meaningful. The benchmarks are represented \nby vectors X in a 156\u00addimensional space whose components Xi are the relative execution frequencies of \nthe individual instructions. As such high-dimensional vector spaces are hard to comprehend, we apply \nPCA to reduce the data s dimensionality. To this effect, we standardize each component Xi to zero mean, \ni.e., Yi = Xi - Xi. We do not standardize to unit variance, however. In other words, we use PCA with \nthe covariance rather than the correlation matrix; this is justi.ed as using the latter would exaggerate \nrarely-executed instructions whose execution frequency varies little across benchmarks (e.g., nop, multianewarray, \n.oating-point coercions). PCA now yields a new set of vectors, whose uncorrelated components Zi = j aij \nYj are called principal components. We discard those principal components that account for a low variance \nonly, i.e., those which do not discriminate the benchmarks well, and retain 4 components that account \nfor 58.9 %, 15.3 %,6.4 %, and 5.6 % of the variance present in the data. Taken together, these four principal \ncomponents explain more than 86.2 % of the total variance, whereas none of the discarded components accounts \nfor more than 2.7 % of variance. Figure 3 depicts the so-called loadings, i.e., the weights aij . [-1, \n+1], of the four retained principal components. We now proceed to interpret the retained principal com\u00adponents. \nFor the .rst component, several instructions ex\u00adhibit a strong positive correlation (|a1j| > 0.1) with \nthe .rst principal component: reference loads (aload), method calls (invoke...), and several kinds of \nmethod return (areturn, ireturn,and return). Likewise, one group of three instruc\u00adtions exhibits an equally \nstrong negative correlation: integer variable manipulations (iinc, iload,and istore). This sug\u00adgests \nthat the .rst component contrasts interprocedural with intraprocedural control .ow, i.e., method calls \nand their cor\u00adresponding returns with counting loops controlled by inte\u00adger variables. This is further \nsubstantiated by the fact that the if icmpge instruction commonly found in such loops is also negatively \ncorrelated. The second principal component is governed by .oating-point manipulations (.oad, fmul,and \nfstore)and .eld accesses (get.eld, put.eld), all of which are positively correlated. Figure 4 shows to \nwhat degree both the Scala and Java benchmarks are affected by these principal components. As 10 This \nnumber is slightly smaller than the actual number of JVM instruc\u00adtions (201); mere abbreviations like \naload 0 and goto have been treated as aload and goto w, respectively. The wide modi.er is treated similarily. \n  iload iinc get.eld put.eld .oad fmulfstorefadd instanceof ifeq return invokevirtual invokestatic aload \n Figure 3. The top four principal components that account for 86.2 % of variance in the benchmarks instruction \nmixes. 4th Principal Component 2nd Principal Component 0.04  0.02 0    -0.02 -0.04 -0.06    \n    -0.05 -0.08 3rd Principal Component 1st Principal Component -0.06 -0.04 -0.020 0.02 0.04 0.06 \n-0.15 -0.1 -0.050 0.05 0.10.15 Figure 5. The DaCapo ( ) and Scala benchmarks ( ) with Figure 4. The DaCapo \n( ) and Scala benchmarks ( ) with respect to the third and fourth principal component. respect to the \n.rst and second principal component. The third principal component correlates positively with can be \nseen, all Scala benchmarks with the exception of calls that are dynamically dispatched (invokevirtual \nand actors exhibit high values for the .rst principal component, invokeinterface) and negatively with \ncalls that are stati\u00adthe scalatest benchmark with its heavy usage of the Java cally dispatched (invokespecial \nand invokestatic). Moreover, runtime (cf. Section 3.3) being a borderline case. This shows other forms \nof dynamic type checks (checkcast, instanceof) that these Scala benchmarks strongly favor interprocedural \nalso contribute negatively to this component. The fourth over intraprocedural control .ow.Infact,theydosotoa \nprincipal component again correlates (negatively) with vari\u00adlarger degree than most of the considered \nJava benchmarks. ous .oating-point operations, but is otherwise hard to grasp actors actors apparat \napparat factorie factorie kiama kiama scalac scalac scaladoc scaladoc scalap scalap scalariform scalariform \nscalatest scalatest scalaxb scalaxb specs specs tmt tmt  Figure 6a. The relative number of callsites \nusing Figure 7a. The relative number of calls made using invokevirtual ( ), invokeinterface ( ), invokespecial \n( ), invokevirtual ( ), invokeinterface ( ), invokespecial ( ), and invokestatic ( ) instructions for \nthe Scala benchmarks. and invokestatic ( ) instructions for the Scala benchmarks. avrora batik eclipse \nfop h2 jython luindex lusearch pmd sun.ow tomcat tradebeans tradesoap xalan Figure 6b. The relative \nnumber of callsites using invokevirtual ( ), invokeinterface ( ), invokespecial ( ), and invokestatic \n( ) instructions for the Java benchmarks. intuitively. Still, as Figure 5 shows, it has signi.cant dis\u00adcriminatory \npower, in particular with respect to the Scala benchmarks. What is noteworthy in Figure 4 and particularly \nin Fig\u00adure 5 is that benchmarks from the same application domain, e.g., factorie and tmt (machine-learning), \nnevertheless ex\u00adhibit a different instruction mix. This justi.es their joint in\u00adclusion into the suite. \n 4.2 Call Site Polymorphism In object-oriented languages like Java or Scala, polymor\u00adphism plays an \nimportant role. For the JVM, however, call sites that potentially target different methods pose a chal\u00adlenge, \nas dynamic dispatch hinders method inlining, which is an important optimization. At the level of Java \nbyte\u00adcode, such dynamically dispatched calls take the form of avrora batik eclipse fop h2 jython luindex \nlusearch pmd sun.ow tomcat tradebeans tradesoap xalan Figure 7b. The relative number of calls made using \ninvokevirtual ( ), invokeinterface ( ), invokespecial ( ), and invokestatic ( ) instructions for the \nJava benchmarks. invokevirtual or invokeinterface instructions, whereas the invokespecial and invokestatic \ninstructions are statically dis\u00adpatched. Figures 6a and 6b contrast the relative occurrences of these \ninstructions in the Scala and Java benchmarks. Here, only instructions executed at least once have been \ncounted; dormant code is ignored. The numbers are remark\u00adably similar for the Scala and Java benchmarks, \nwith the ver\u00adtical bars denoting the respective arithmetic mean, with only a slight shift from invokevirtual \nto invokeinterface instruc\u00adtions due to the way the Scala compiler unties inheritance and subtyping [44, \nChapter 2] to implement mixins. But the numbers in these .gures provide only a static view. At runtime, \nthe instructions actual execution frequen\u00adcies may differ. Figures 7a and 7b thus show the relative number \nof actual calls made via the corresponding call sites.  actors apparat factorie avrora batik eclipse \n  kiama scalac scaladoc fop h2 jython  scalap scalariform scalatest luindex lusearch pmd  specs \n scalaxb tmt sun.ow tomcat tradebeans # Call Sites 105 103 101 5 1015+ 5 1015+ 5 1015+ tradesoap xalan \n#Targets #Targets #Targets Figure 8a. The number of dynamically-dispatched call sites targeting a given \nnumber of methods for the Scala bench\u00ad marks. Here, the numbers for the Scala and Java benchmarks diverge, \nwith the Scala benchmarks exhibiting a dispro\u00adportionate amount of calls made by invokeinterface and \ninvokestatic instructions. The former is again explained by the use of mixins, the latter by the use \nof singleton objects in general and by companion objects in particular. The di\u00advergence is even more \npronounced, speci.cally with respect to calls made by invokeinterface, when comparing the num\u00adbers for \nScala with results obtained for older Java bench\u00admark suites [22]. In general, Scala code bene.ts more \nthan Java code from techniques for the ef.cient execution of the invokeinterface instruction [1]. Polymorphic \ncalls involve dynamic binding, since the tar\u00adget method may depend on the runtime type of the object \nreceiving the call. In their analysis [19], Dufour et al. there\u00adfore distinguish between the number of \ntarget methods and the number of receiver types for polymorphic call sites, as there are typically more \nof the latter than of the former; not all subclasses override all methods. Both dynamic metrics are relevant \nto different com\u00adpiler optimizations [19]: The number of receiver types for polymorphic call sites is \nrelevant for inline caching [25], an optimization technique commonly used in runtimes for dynamically-typed \nlanguages. Modern JVMs, however, rely on virtual method tables in combination with method inlin\u00ad 5 10 \n15+ 5 10 15+ #Targets #Targets Figure 8b. The number of dynamically-dispatched call sites targeting a \ngiven number of methods for the Java bench\u00admarks. ing. We thus do not investigate the number of receiver \ntypes in this paper,11 but focus on the number of target methods for polymorphic call sites. This information \nis essential for method inlining, one of the most effective compiler opti\u00admizations for the JVM. If a \ncall site has only a single target method, the target method can be (speculatively) inlined. Moreover, \neven if a call site has more than one target, in\u00adlining is still possible with appropriate guards in \nplace [15]. Only if the number of possible targets grows too large, inlin\u00ading becomes infeasible. Figures \n8a and 8b (respectively 9a and 9b) show his\u00adtograms of the polymorphic call sites, presenting the num\u00adber \nof call sites (respectively the number of calls for call sites) with x = 1 target methods. These statistics \ncom\u00adprise all exercised call sites corresponding to invokevirtual or invokeinterface instructions. Call \nsites that are never ex\u00ad 11 The Scala compiler relies on a similar compilation technique using Java re.ection \nand polymorphic inline caches for structural types [18]. However, this technique is not applied within \nthe JVM itself.  actors apparat factorie avrora batik eclipse  kiama scalac scaladoc fop h2 jython \n  scalap scalariform scalatest luindex lusearch pmd  specs scalaxb tmt sun.ow tomcat tradebeans  \n5 1015+ 5 1015+ 5 1015+ tradesoap xalan #Targets #Targets #Targets 1011  Figure 9a. The number of dynamically-dispatched \ncalls # Calls made at call sites with a given number of targets for the Scala 106 benchmarks. ecuted \nby the workload are excluded; also, call sites corre\u00adsponding to invokestatic and invokespecial bytecodes \nare ex\u00adcluded, as they are trivially monomorphic. In Figures 8a and 8b, the actual number of invocations \nat a call site is not taken into account; call sites are merely counted. If a call site S, e.g., targets \ntwo methods M and N, whereby M is invoked mS times and N is invoked nS times, then call site S will be \ncounted once for the bar at x =2; the actual numbers mS and nS will be ignored. In contrast to Figures \n8a and 8b, Figures 9a and 9b do take the actual number of invocations into account. That is, in the previous \nexample, call site S contributes mS +nS to the bar at x =2. Figures 8a and 8b thus correspond to Figures \n6a and 6b, whereas Figures 9a and 9b in turn correspond to Figures 7a and 7b. Whereas the former histograms \nshow the possibility of inlining, the latter show its possible effectiveness. As the analysis shows, \nthere exist marked differences be\u00adtween the individual benchmarks, while the differences be\u00adtween the \nScala and DaCapo benchmark suites are less pro\u00adnounced. Still, dynamic dispatch with many different tar\u00adgets \nper call site used less by the Java benchmarks than by most Scala benchmarks except for factorie. However, \nmega\u00admorphic call sites with 15 or more targets are exercised almost all benchmarks in both the Scala \nand the DaCapo 101 5 10 15+ 5 10 15+ #Targets #Targets Figure 9b. The number of dynamically-dispatched \ncalls made at call sites with a given number of targets for the Java benchmarks. benchmark suites, i.e., \nby 11 out of 12 and 10 ouf of 14 benchmarks, respectively. In general, polymorphism plays a larger role \nfor the Scala benchmarks than for their Java counterparts, but this does not diminish the effectiveness \nof inlining signi.cantly: Call sites with few targets in general and monomorphic call sites in particular \naccount for a large part of potentially polymor\u00adphic method calls: For the Scala benchmarks, on average \n97.1 % of callsites are monomorphic and they account for 89.7 % of the overall method calls. For the \nJava benchmarks on average 97.8 % of callsites are monomorphic and account for 91.5 % of calls. The Scala \nbenchmarks, however, show a higher vari\u00adance (6.0 %) than the Java benchmarks (3.2 %) with respect to \nthe number of monomorphic calls: The portion of such calls ranges from 76.4 % (apparat) to 99.2 % (tmt)rather \nthan from 83.9 % (h2) to 96.0 % (xalan). With respect to this metric, our Scala benchmark suite is therefore \nat least as diverse as the DaCapo benchmark suite.  apparat actors factorie kiama scalac scaladoc \n scalap scalariform scalatest Scala ( ) and Dacapo ( ) benchmarks (default input size). specs scalaxb \ntmt  4.3 Stack Usage and Recursion On the JVM, each method call creates a stack frame which, at least \nconceptually, holds the method s arguments and local variables, although some of them may be placed in \nregisters instead (cf. Section 4.4). As most modern JVMs are unable to re-size a thread s call stack \nat runtime, they have to re\u00adserve a suf.ciently large amount of memory whenever a new thread is created. \nIf the newly-created thread does not require all the reserved space, memory is wasted; if it requires \nmore space than was reserved, a StackOver.owException ensues. As Figure 10 shows, the stack usage of \nthe Scala bench\u00admarks is signi.cantly higher than for the Java benchmarks. However, for both benchmark \nsuites the required stack size varies widely across benchmarks: For the Scala benchmarks, it ranges from \n110 (tmt) to 1,460 frames (kiama), with an average of about 472. For the Java benchmarks, these num\u00adbers \nare more stable and signi.cantly lower, ranging from 45 (lusearch) to 477 frames (pmd), with an average \nof 137. The question is thus what gives rise to this signi.cant in\u00adcrease in stack usage, the prime suspects \nbeing infrastructure methods inserted by the Scala compiler on the one hand and recursion on the other \nhand. To assess to what extent the pro\u00adgrammer s use of the latter contributes to the benchmarks stack \nusage, we make use of the calling context pro.les col\u00adlected by JP2. In the presence of polymorphism, \nhowever, a recursive method call is dynamically dispatched and may or may not target an implementation \nof the method identical to the caller s. This, e.g., often occurs when the Composite pattern is used. \nFigures 11a and 11b, which depict the distribution of stack heights for each of the benchmarks, therefore \nuse an extended de.nition of recursion. In this de.nition we dis\u00adtinguish between true recursive calls \nthat target the same implementation of a method and plain recursive calls, which may also target a different \nimplementation of the same method. Now, to compute the histograms of Figures 11a 1 250 500+ 1 250 500+ \n1 250 500+ Stack Height Stack Height Stack Height Figure 11a. The distribution of stack heights upon \na method call for the Scala benchmarks: all method calls ( ), recursive calls ( ), and recursive calls \nto the same implementation ( ). and 11b, at every method call the current stack height x is noted. All \nmethod calls contribute to the corresponding light grey bar, whereas only recursive calls contribute \nto the dark grey bar, and only true recursive calls contribute to the black one. The scalariform benchmark, \ne.g., makes no true re\u00adcursive calls that target an implementation identical to the caller s at a stack \nheight beyond 287, but still makes plenty of recursive calls targeting a different implementation of \nthe selfsame method. In this case, the phenomenon is explained by the benchmark traversing a deeply-nested \ncomposite data structure. In general, this form of recursion is harder to opti\u00admize, as dynamic dispatch \nhinders, e.g., tail call elimination. For all benchmarks, recursive calls indeed contribute sig\u00adni.cantly \nto the stack s growth, up to its respective max\u00adimum size, although for two Scala (scalap and scalari\u00adform) \nand one Java (pmd) benchmark this is not a direct consequence of true recursive calls, but of dynamically\u00addispatched \nones. For all other benchmarks, however, there exists a stack height x at which all calls are truly recur\u00adsive. \nThis shows that, ultimately, it is the more extensive use of recursion by Scala programmers rather than \nof in\u00adfrastructure methods by the Scala compiler that leads to the observed high stack usage.  avrora \nbatik eclipse batik factorie fop 106 101 fop h2 jython lusearch sun.ow tmt 1011  25 % # Calls # Calls \n# Calls # Calls for the Java benchmarks: all method calls ( ), recursive calls ( ), and recursive calls \nto the same implementation ( ).  4.4 Argument Passing A method s stack frame not only stores a method \ns local vari\u00adables, but it also contains the arguments passed to the method in question. Now both the \nnumber and kind of arguments passed upon a method call can have a performance impact: Large numbers of \narguments lead to spilling on register\u00adscarce architectures; they cannot be passed in registers alone. \nDifferent kinds of arguments may need to be passed differ\u00adently; on many architectures, e.g., .oating \npoint numbers oc\u00adcupy a distinct set of registers. As not all benchmarks in the DaCapo and Scala bench\u00admark \nsuites make much use of .oating-point arithmetic, we .rst focus on the six benchmarks for which at least \n1 % of method calls carries at least one .oating-point ar\u00adgument: the Java benchmarks batik (1.9 %), \nfop (3.4 %), lusearch (2.6 %), and sun.ow (25.1 %) and the Scala bench\u00admarks factorie (5.1 %)and tmt \n(13.5 %). The histograms in Figure 12 depict the number of .oating\u00adpoint arguments passed upon a method \ncall, in relation to 0 % 012345 012345 012345 luindex lusearch pmd # Arguments # Arguments # Arguments \n 1011 106 101 1011 106 101 1011 106 101 Figure 12. Distribution of the number of .oating-point ar\u00adguments \npassed upon a method call: none ( ), 1 ( ), 2 ( ), 3( ),4( ),and5ormore( ). the overall number of arguments. \nThe bars shadings corre\u00adspond to the number of .oating-point arguments; the darker the shade, the more \narguments are of either .oat or double type. As can be seen, not only do sun.ow and tmt most frequently \npass .oating-point arguments to methods, but these two benchmarks are also the only ones where a no\u00adticeable \nportion of calls passes more than one .oating-point argument: In the case of sun.ow, four-argument methods \nare frequently called with three .oating-point arguments, indicated by the dark portion of the respective \nbar. In the case of tmt, three-argument methods occasionally have two .oating-point arguments (1.0 % \nof all calls). The relation of .oating-point and integral arguments is not the only dimension of interest \nwith respect to argument passing: The histograms in Figures 13a and 13b thus depict the number of reference \narguments passed upon an method call, in relation to the overall number of arguments, i.e., of primitive \nand reference arguments alike. Here the bars shad\u00ading corresponds to the number of reference arguments; \nthe darker the shade, the more of the arguments are references rather than primitives. Figures 13a and \n13b distinguish between calls with an implicit receiver (invokevirtual, invokeinterface,and invokespecial) \nand calls without one (invokestatic). Both .gures display the amount of arguments attributed to the former \ngroup above the x-axis, whereas that attributed to the latter is displayed below the x-axis. Taken together, \nthe bars above and below the axis show the distribution of ref\u00aderence arguments for all calls to methods \nwith x arguments. The receiver of a method call (if any) is hereby treated as a reference argument as \nwell. This is in line with the treatment this receives from the virtual machine; it is simply placed \nin local variable 0 [31]. For Scala and Java benchmarks alike, almost all meth\u00adods have at least one \nargument, be it explicit or implicit, viz., this. This is in line with earlier .ndings on Java bench\u00ad \nsun.ow tradesoap tomcat tradebeans  xalan 1 250 500+ 1 250 500+ Stack Height Stack Height Figure 11b. \nThe distribution of stack heights upon a call  actors apparat factorie avrora batik eclipse 50 % 50 \n% 0 % 0 % # Calls # Calls # Calls # Calls # Calls # Calls # Calls # Calls # Calls 50 % 50 % kiama scalac \nscaladoc fop h2 jython 50 % 50 % 0 % 0 % 50 % 50 % scalap scalariform scalatest luindex lusearch pmd \n50 % 50 % 0 % 0 % 50 % 50 % specs scalaxb tmt sun.ow tomcat tradebeans 50 % 50 % 0 % 0 % 50 % 50 % 012345 \n012345 012345 tradesoap # Arguments # Arguments # Arguments xalan 50 % Figure 13a. Distribution of the \nnumber of reference argu\u00ad ments passed upon a method call by the Scala benchmarks: none( ),1( ),2( ),3( \n),4( ),and5ormore( ). 0 % 50 % 012345 012345 # Arguments # Arguments marks [14]. But while the maximum \nnumber of passed argu\u00adments can be as large as 21 for Scala code (specs) and 35 for Java code (tradebeans, \ntradesoap), on average only very few arguments are passed upon a call: 1.04 1.47 for Scala code and 1.69 \n2.43 for Java code. In particular, the vast major\u00adity of methods called by the Scala benchmarks has no \nargu\u00adments other than the receiver; they are simple getters. This has an effect on the economics of method \ninlining: the direct bene.t of inlining, i.e., the removal of the actual call, clearly outweighs the \npossible indirect bene.ts, i.e., the propagation of information about the arguments types and values \nwhich, in turn, facilitates, e.g., constant folding. This marked difference between Scala and Java bench\u00admarks \nis of particular interest, as the Scala language offers special constructs, namely implicit parameters \nand default values, to make methods with many arguments less incon\u00advenient to the programmer. But while \nmethods taking many parameters do exist, they are rarely called.  4.5 Method and Basic Block Hotness \nAny modern JVM with a just-in-time compiler adaptively optimizes application code, focusing its efforts \non those parts that are hot, i.e., executed frequently. Regardless of whether the virtual machine follows \na traditional, region\u00adbased [24, 48], or trace-based [3] approach to compilation, Figure 13b. Distribution \nof the number of reference argu\u00adments passed upon a method call by the Java benchmarks: none( ),1( ),2( \n),3( ),4( ),and5ormore( ). pronounced hotspots are fundamental to the effectiveness of adaptive optimization \nefforts. It is thus of interest to which extent the different benchmarks exhibit such hotspots. In contrast \nto Dufour et al., who report only the num\u00adber of bytecode instructions responsible for 90 % execu\u00adtion \n[19], our metric is continuous. Figures 14a and 14b report to which extent the top 20 % of all static \nbytecode instructions in the code contribute to the overall dynamic bytecode execution. A value of 100 \n% on the x-axis cor\u00adresponds to all instructions contained in methods invoked at least once; dormant \nmethods are excluded. However, ba\u00adsic blocks that are either dead or simply dormant during the benchmark \ns execution in an otherwise live method are taken into account, as they still would need to be compiled \nusing a traditional approach. A value of 100 % on the y-axis simply corresponds to the total number of \nall executed bytecodes. Current JVMs typically optimize at the granularity of methods rather than basic \nblocks and are thus often unable to optimize just the most frequently executed instructions or actors \napparat factorie avrora batik eclipse 100 % 100 % 90 % 90 % # Bytecodes # Bytecodes # Bytecodes # Bytecodes \n# Bytecodes # Bytecodes # Bytecodes # Bytecodes # Bytecodes 80 % 100 % 80 % kiama scalac scaladoc fop \nh2 jython 100 % 90 % 80 % 90 % 80 % 100 % scalap scalariform scalatest luindex lusearch pmd 100 % 90 \n% 80 % 90 % 80 % 100 % specs scalaxb tmt sun.ow tomcat tradebeans 100 % 90 % 80 % 90 % 80 % 10 % 20 \n% 10 % 20 % 10 % 20 % tradesoap xalan # Bytecodes # Bytecodes # Bytecodes 100 % 90 % 80 % 10 % 20 % 10 \n% 20 % # Bytecodes # Bytecodes Figure 14a. Cumulative number of executed bytecodes for the most frequently \nexecuted bytecodes when measured at the granularity of basic blocks ( ) or methods ( ). basic blocks. \nTo re.ect this, Figures 14a and 14b also report the extent to which the hottest methods are responsible \nfor Figure 14b. Cumulative number of executed bytecodes for the execution. The two data series are derived \nas follows: the most frequently executed bytecodes when measured at Basic block hotness. The basic blocks \n(in methods exe\u00adcuted at least once) are sorted in descending order of their execution frequencies. Each \nbasic block bi is then plotted .i .i at x = size(bj ) and y = size(bj ) \u00b7 freq(bj), j=1 j=1 where size(bj) \nis the number of bytecodes in bj and freq(bj) is the number of times bj has been executed. Method hotness. \nThe methods (that are executed at least once) are sorted in descending order of the overall number of \nbytecodes executed in each method. Each .i . method mi is then plotted at x = length(b) j=1 b.Bj .i . \nand y = length(b) \u00b7 freq(b), where Bj are j=1 b.Bj the basic blocks of method mj . For the actors and \nscalap benchmarks, e.g., only 1.4 % and 1.5 % of all bytecode instructions, respectively, are re\u00adsponsible \nfor 90 % of all executed bytecodes. However, be\u00adyond that point, the basic block hotness of the two bench\u00admarks \ndiffers considerably. Moreover, the method hotness of these two benchmarks is also different, the discrepancy \nbetween basic block and method hotness being much larger for scalap than for actors: A method-based compiler \nwould need to compile just 2.7 % of actor s bytecodes to cover the granularity of basic blocks ( ) or \nmethods ( ). 90 % of all executed instructions, whereas 4.7 % of all in\u00adstructions need to be compiled \nfor scalap to achieve the same coverage. In general, the discrepancy between basic block and method hotness \nis quite pronounced. This is the effect of methods that contain both hot and cold basic blocks, i.e., \nsome that are frequently executed and some that are not. Four Java benchmarks (jython, pmd, tradesoap,and \nxalan) with a larger than average number of basic blocks per method suffer most from this problem. The \nremaining Java benchmarks exhibit patterns similar to the Scala bench\u00admarks. Both region-based [24, 48] \nand trace-based com\u00adpilation [3] can be employed to lessen the effect of such temperature drops within \nmethods.  4.6 Re.ection While re.ective features that are purely informational, e.g., runtime-type information, \ndo not pose implementation chal\u00adlenges, other introspective features like re.ective invoca\u00adFigure 15a. \nNumber of methods invoked re.ectively by Figure 16a. Number of objects instantiated re.ectively by the \nScala benchmarks with a single or multiple Method in-the Scala benchmarks with a single or multiple Class \nin\u00adstances per call site, normalized to the number of all method stances per call site, normalized to \nthe number of all object invocations. instantiations (new).  Figure 15b. Number of methods invoked \nre.ectively by the Java benchmarks with a single or multiple Method instances per call site, normalized \nto the number of all method invoca\u00adtions. tions or instantiations are harder to implement ef.ciently \nby a JVM [40]. It is thus of interest to what extent Scala code makes use of such features, in particular \nas Scala s struc\u00adtural types are compiled using a re.ective technique [18]. We have thus extended TamiFlex \n[8] to gather informa\u00adtion about the following three usages of re.ection: method calls (Method.invoke), \nobject allocation (Class.newInstance, Constructor.invoke,and Array.newInstance), and .eld ac\u00adcesses (Field.get, \nField.set,etc.). We .rst consider re.ective invocations. What is of in\u00adterest here is not only how often \nsuch calls are made, but also whether a call site exhibits the same behaviour through\u00adout.Ifacallsitefor \nMethod.invoke, e.g., consistently refers to the same Method instance, partial evaluation [9] might Figure \n16b. Number of objects instantiated re.ectively by the Java benchmarks with a single or multiple Class \nin\u00adstances per call site, normalized to the number of all object instantiations (new). avoid the re.ective \ncall altogether. For the Scala and DaCapo benchmark suites, respectively, Figures 15a and 15b depict \nthe number of re.ective method invocations with a single or multiple Method instances per call site. \nThese numbers have been normalized with respect to the number of over\u00adall method calls (invokevirtual \ninvokeinterface bytecode in\u00adstructions). As can be seen, few benchmarks in either suite perform a signi.cant \nnumber of re.ective method invoca\u00adtions. Even for those benchmarks (scalap, pmd, tomcat, and tradesoap), \nre.ective invocations account for at most 0.03 % of invocations. In the case of scalap, these invoca\u00adtions \nare almost exclusively due to the use of structural types within the benchmark. This also explains why \nonly a single Method instance is involved [18]. Should the use of re.ec\u00adtion to implement structural \ntypes be supplanted by the use 100 of invokedynamic, these metrics remain meaningful, as sim\u00adilar caching \ntechniques can be applied as an optimization. 80  We next consider re.ective object instantiations, \ni.e., calls to Class.newInstance, Constructor.invoke,aswellas Array.newInstance. Again, for reasons explained \nabove, we distinguish between call sites referring to a single meta\u00ad object and call sites referring \nto several.12 Figures 16a and # Calls [%] 60 40 16b depict the number of re.ective instantiations for \nthe 20 Scala and DaCapo benchmark suites, respectively. The num\u00adbers have been normalized with respect \nto the number of 0 overall allocations (new bytecode instructions). The surprisingly large number of \nre.ective instantiations by several Scala benchmarks can be attributed to the creation of arrays via \nscala.re.ect.ClassManifest, where a single call  site instantiates numerous arrays of different type. \nThis is an Figure 17a. Boxed instances requested (valueOf) and cre\u00adartifact of Scala s translation strategy, \nas one cannot express ated (Constructor) by the Scala benchmarks, normalized to the creation of generic \narrays in Java bytecode without re-the number of all object allocations (new). sortingtore.ection. Unlike \nre.ective invocations and instantiations, re.ective 100 .eld accesses are almost absent from both Scala \nand Java benchmarks. The only notable exception is eclipse, which 80 uses re.ection to write to a few \nhundred .elds.  4.7 Boxed Types Boxing of primitive types like int or double may incur signif\u00ad icant \noverhead; not only is an otherwise super.uous object created, but simple operations like addition now \nrequire prior # Calls [%] 60 40 20 unboxing of the boxed value. We have therefore measured to which \ndegree the Java and Scala benchmarks create boxed 0 values. To this end, we distinguish between the mere \nrequest to create a boxed value by using the appropriate valueOf fac\u00adtory method and the actual creation \nof a new instance using the boxed type s constructor; usage of the factory method allows for caching \nbut may impede JIT compiler optimiza- Figure 17b. Boxed instances requested (valueOf) and cre\u00ad tions \n[11]. Figures 17a and 17b show how many boxed values are re\u00adquested and how many are actually created. \nThe counts have been normalized with respect to the number of all object allocations (new bytecode instructions). \nNote that we have not counted boxed values created from strings instead of un\u00adboxed values, as the intent \nhere is rather different, namely to parse a string. While for the Java benchmarks (Figure 17b), boxing \naccounts only for very few object creations, this is not true for many of the Scala benchmarks (Figure \n17a); al\u00admost all of the objects created by, e.g., the tmt benchmark are boxed primitives. In general, \nthe caching performed by the factory methods is effective. Only for factorie and tmt, a signi.cant num\u00adber \nof requests (calls to valueOf) to box a value result in an actual object creation; this is due to the \nfact these two benchmarks operate with .oating-point values, for which 12 We treat the Class instance \npassed to Array.newInstance as the meta\u00adobject here. ated (Constructor) by the Java benchmarks, normalized \nto the number of all object allocations (new). the corresponding valueOf factory methods do not perform \ncaching. That being said, the Scala benchmarks in general both create and request more boxed values than \ntheir Java counterparts. Extensive use of user-directed type specializa\u00adtion [17] may be able to decrease \nthese numbers, though. Another interesting fact about the usage of boxed values is that they are created \nat a few dozen sites only within the program, the fop Java benchmark being the only exception with 4,547 \ncall sites of a boxing constructor.  5. Related Work Blackburn et al. [7] created the DaCapo benchmark \nsuite to improve upon the state-of-the-art of Java benchmark suites at that time. Their selection of \nmetrics puts a strong em\u00adphasis on the benchmarks memory and pointer behaviours, whereas the present \npaper concentrates on the difference between Scala and Java code, leaving a detailed analysis of memory \nand pointer behaviour to future work (cf. Sec\u00adtion 7). While the creators of the DaCapo benchmark suite \nalso report code-related metrics, only the static metrics re\u00adported, e.g., the number of loaded classes \nor the coupling between them, are JVM-independent; the reported dynamic metrics like instruction mix \n(cf. Section 4.1) or method hotness (cf. Section 4.5) have been measured in a JVM\u00addependent fashion. \nLike others [26], Blackburn et al. use principal component analysis to demonstrate the bench\u00admarks diversity. \n Dufour et al. [19] computed a range of VM-independent metrics for a selection of Java workloads. Unlike \nin the present paper, the authors primary objects of study are the metrics, not the benchmarks. While \nDufour et al. were trying to assess the usefulness of the various metrics in distinguish\u00ading the benchmarks, \nwe use metrics to distinguish between the benchmarks themselves, in particular with respect to the Scala \n/ Java code dichotomy. Shiv et al. [46] analyzed the SPECjvm2008 benchmark suite both qualitatively and \nquantitatively. The majority of their quantitative evaluation, however, is based on JVM\u00adand architecture-dependent \nmetrics. The authors further\u00admore offer a brief comparison of the SPECjvm2008 and SPECjvm98 benchmark \nsuites. Daly et al. [14] conducted an analysis of the Java Grande benchmark suite [10] using VM-independent \nmetrics. Their analysis focuses on the benchmark s static and dynamic in\u00adstruction mix and considers \nthe 201 instructions both indi\u00advidually and by manual assignment to one of 22 groups. In contrast to \nDaly et al., we apply principal component anal\u00adysis to automatically extract groupings, which we then \nuse to show that there a differences not only between individ\u00adual benchmarks, but also between the Scala \nand Java bench\u00admarks as a whole. The number of actual invocations of native methods is a dynamic metric \nthat can be obtained by incrementing a counter at runtime. Gregg et al. [22] used an instrumented version \nof the Kaffe virtual machine in order to gather this metric. Thus, their approach is not portable and \nprovides only a very coarse-grained view of where CPU time is actu\u00adally spent. Some researchers provided \na more detailed break\u00addown of where CPU time is spent in Java workloads [23, 30]; however, they likewise \nhad to sacri.ce portability by directly modifying a JVM. Hoste and Eeckhout [26] used principal component \nanal\u00adysis to show that workload characterization is most effec\u00adtively done in a microarchitecture-independent \nfashion; in\u00adstead of relying on a particular microarchitecture, metrics should instead be de.ned with \nrespect to an idealized one. In the present paper, we apply this tenet by using metrics de.ned with respect \nto Java bytecode, it being arguably the natural microarchitecture of JVMs. To assess the performance \nimpact of run-time types, Schinz [44, Chapter 6] performed some experiments with earlier (circa 2005) \nversions of two Scala applications that are part of our benchmark suite: scalac13 and scalap.These experiments, \nhowever, were limited to evaluating different translation strategies for run-time types. Recent work \nby Richards et al. [39] and Ratanaworab\u00adhan et al. [38] applied an approach similar to ours to ana\u00adlyze \nthe dynamic behaviour of JavaScript; the authors use platform-independent metrics to characterize, e.g., \nthe in\u00adstruction mix, call site polymorphism, and method hotness. The key difference, besides the considered \nlanguages, is that our work resulted in a full-.edged benchmark suite re\u00adsearchers can re-use. Only Ratanaworabhan \net al. compare their .ndings with established JavaScript benchmarks suites. Jibaja et al. [27] recently \ncompared the memory be\u00adhaviour of two managed languages, namely PHP and Java, to guide language implementers. \nFrom the signi.cant similari\u00adties found between benchmarks of the PHP and SPECjvm98 benchmark suites, \nthe authors infer that PHP would bene.t from garbage collector designs similar to those successful in \nmodern JVMs.  6. Summary and Conclusions The Java Virtual Machine is no longer targeted by the Java \nlanguage alone but by a variety of programming languages. The benchmark suites used within the JVM research \ncom\u00admunity, however, do not yet re.ect this trend. The bench\u00admark suite presented in this paper addresses \nthis issue for the Scala language. They have been shown to a offer a varied set of workloads from a broad \nrange of application domains. To summarize, our analysis of the Scala benchmark suite led to the following \n.ndings, which may be of interest to both the developers of JVMs and to the developers of the Scala compiler \nand its associated libraries. Instruction Mix. Scala and Java programs differ signi.\u00adcantly in their \ninstruction mix. Call Site Polymorphism. Although polymorphism plays a larger role for Scala than for \nJava code, the overwhelming number of callsites is effectively monomorphic and ac\u00adcounts for the majority \nof calls. Inlining is thus expected to be as effective for Scala code as its is for Java code. Stack \nUsage and Recursion. Scala applications require sig\u00adni.cantly more space on the call stack than their \nJava counterparts. Recursive method calls to varying target implementations contribute signi.cantly to \nthis. Argument Passing. The vast majority of method calls in Scala code target parameter-less getters; \nmethods with more than one arguments are rarely called. This nega\u00adtively affects the economics of method \ninlining, as the 13 At that time, it was still called nsc, the New Scala Compiler.  optimization propagates \nless information into the inlined method. Method and Basic Block Hotness. Hotspots in Scala and Java \ncode are similarly distributed. However, Scala code seems to be slightly easier for method-based compilation \nto cope with. Re.ection. Although the Scala compiler resorts to using re\u00ad.ection to translate structural \ntypes, re.ective invocations are not a signi.cant performance bottleneck in Scala ap\u00adplications. Scala \nis thus much less likely to bene.ts from the invokedynamic instruction than dynamically-typed languages \nlike JRuby or Jython. Boxed Types. While the Scala compiler already tries to avoid boxing, Scala programs \nnevertheless request and create signi.cantly more boxed values than Java pro\u00adgrams. Therefore, canonicalization \nor similar optimiza\u00adtions are crucial. Our evaluation suggests that the execution characteristics of \nScala code do differ from Java code in several ways. However, this does not invalidate optimizations \nperformed by current JVMs; instead, it suggests that .ne-tuning may be suf.ciently pro.table.  7. Future \nWork No benchmark suite stays relevant forever. Like Blackburn et al. have done for the DaCapo benchmark \nsuite [7], we plan to maintain the benchmark suite, incorporate community feedback, and extend the suite \nto cover further application domains once suitable Scala applications emerge. We will also maintain and \nextend the toolchain we used to build the Scala benchmark suite (cf. Appendix A), so that it becomes \neasier for other researchers to build their own benchmarks, possibly for further languages beyond Java \nor Scala. Over the years, many researchers have investigated the memory behaviour of Java programs on \na range of bench\u00admark suites and with a variety of methods [7, 16, 28]. In the present paper, we concentrate \non the structure and behaviour of the code itself. However, we do plan to investigate where the differences \nare between Scala and Java code with respect to memory behaviour in the future. While the Scala library \noffers dedicated support for the actor-model of concurrency, a surprisingly small number of benchmarks \n(actors, apparat, scalac, scaladoc, scalatest, tmt) is multi-threaded. A detailed analysis of the execution \ncharacteristics of concurrent Scala program on the JVM re\u00adquires further research. The actors benchmark \ncontained in our suite, however, with its usage of different actor imple\u00admentations, may prove a good \nstarting point. The Scala distribution also supports the Microsoft .NET platform.14 For practical reasons, \nhowever, we have re\u00adstricted our analysis to a single platform, namely the JVM. 14 See http://www.scala-lang.org/node/168. \n A detailed analysis of Scala s execution characteristics on the .NET platform is beyond the scope of \nthe present paper, but subject to future work. A. Building a Benchmark Suite The entire benchmark suite \nis built using Apache Maven,15 a build management tool whose basic tenet rings particu\u00adlarly true in \nthe context of a research benchmark suite: build reproducibility. The use of a central artifact repository \nmir\u00adrored many times worldwide, in particular, ensures that the benchmark suite can be built reproducibly \nin the future. We have developed a dedicated Maven plugin that not only packages a benchmark according \nto the DaCapo suite s requirements but also performs a series of integration tests on the newly-built \nbenchmark. It furthermore retrieves but keeps separate all transitive dependencies of the benchmark and \nits harness. Just as the benchmark suite, the Maven plugin is Open Source and freely available for download. \n B. Collecting Dynamic Metrics with JP2 The measurements presented in this paper have been ob\u00adtained \nwith our open-source pro.ler JP216 [42, 43], which produces a calling context tree (CCT) [2] representing \nover\u00adall program execution. JP2 is a reimplementation and exten\u00adsion of the pro.ler JP [4, 6, 33]. It \nproduces a complete CCT that distinguishes between different callsites and counts how often each basic \nblock is executed in each context. JP2 has been designed for compatibility with standard JVMs and supports \na variety of output formats for the generated pro.le. In the following text, we brie.y summarize the \nkey proper\u00adties, including the remaining limitations,ofJP2. Completeness of the CCT. We consider the \nCCT to be complete if it represents, after an initial JVM bootstrap\u00adping phase, every method call where \neither the caller or the callee has a bytecode representation. The JVM bootstrap\u00adping phase .nishes before \nthe program s main method is invoked. This de.nition ensures that invocations of native methods by bytecode \nand callbacks from native code into bytecode are also included in the CCT. Furthermore, implicit method \ninvocations, such as for class loading and class ini\u00adtialization, must be properly represented. Distinguishing \nbetween Callsites in the CCT. Some call\u00ading context pro.lers like JP do not distinguish between indi\u00advidual \ncallsites within a method; if two callsites in a method invoke the same target method, these invocations \nwill be in\u00addistinguishable in the CCT. However, if callsites cannot be distinguished, several useful \nmetrics like call site polymor\u00adphism (cf. Section 4.2) cannot be computed from the result\u00ading pro.les. \nJP2 solves this issue by associating each CCT node with two keys, a unique identi.er of the target method \nandanidenti.er of the callsite within the calling method. 15 See http://maven.apache.org/. 16 See http://jp-profiler.origo.ethz.ch/. \n  Counting the Execution of Basic Blocks of Code. While JP only counts the number of executed bytecodes \nfor each calling context, the latest version of JP2 keeps a separate counter for each basic block of \ncode in a method. That is, each CCT node stores an array of counters, one for each ba\u00adsic block in the \ncorresponding method. Together with the CCT itself and the loaded class.les, these counts are suf.\u00adcient \nto compute a variety of dynamic metrics. For the purpose of this paper, only bytecodes that may non-sequentially \nchange the control .ow (e.g., goto, ifeq, return, athrow, etc.) end a basic block. In particular, method \ninvocations as well as instructions which may throw excep\u00adtions implicitly (e.g., aastore) do not end \na basic block. We have shown in previous work that this has little to no impact on the pro.les accuracy \n[6]. Compatibility with Standard JVMs. JP2 is implemented with ASM,17 a light-weight bytecode manipulation \nframe\u00adwork. The pro.ler keeps structural modi.cations of class\u00ad.les to a minimum; it only modi.es method \nbodies and in\u00adserts entries in the class.le constant pool. These transfor\u00admations are neither visible \nthrough the Java re.ection API nor do they interfere with stack introspection. Only a few in\u00adstance .elds \nare inserted into java.lang.Thread to reduce the overhead of thread-local variables used by JP2. JP2 \nrelies on native method pre.xing, a JVMTI feature introduced with Java 6, in order to pro.le the invocation \nof native methods. Native methods are renamed (by adding apre.x) and wrapped with Java methods that will \ninvoke the corresponding pre.xed native methods. Although these transformations change the class.le structure, \nthe Java class library and the JVM have been designed to properly deal with this issue (e.g., in the \ncode for stack introspection). However, in some JVM releases, there are still a few native methods that \ncannot be pre.xed. Output formats. JP2 offers several plugins to serialize the CCT upon program termination. \nThe plugin used for the measurements presented in this paper stores the CCT in an XML format. Together \nwith the stored class.les, converted to XML by ASM, we were able to compute all dynamic met\u00adrics using \nXQuery by cross-referencing between class.les and CCT [43]. Limitations. JP2 currently does not distinguish \nbetween different classloaders. That is, if a polymorphic callsite in\u00advokes two different target methods \nwith the same name and signature that are de.ned in distinct classes bearing the same name but de.ned \nby distinct classloaders, the two targets will be represented by the same CCT node. Such a situa\u00adtion \nmay yield corrupt pro.les, but was not encountered for any of the evaluated benchmarks. Depending on \nthe JVM used, pre.xing may not work with a few native methods (e.g., registerNatives). Invoca\u00adtions of \nthese methods will not be present in the CCT. 17 See http://asm.ow2.org/.   Acknowledgments We are \ndeeply grateful to e.e d3si9n, Joa Ebert, Patrik Nordwall, Daniel Ramage, Bill Venners, Tim Vieira, and \nthe late Sam Roweis for their assistance with the various programs and input data that ultimately made \nit into our benchmark suite. We are also grateful to the DaCapo group for building such an excellent \nfoundation on which we could build our benchmark suite. We would furthermore like to thank both the participants \nof and the reviewers for the Work-on-Progress session at the 8th International Conference on the Principles \nand Prac\u00adtice of Programming in Java for suggestions and encourage\u00adment. Eric Bodden, Michael Eichberg, \nIvan Jibaja, Kathryn McKinley, Nate Nystrom, and Jan Sinschek provided valu\u00adable comments on the contents \nof this paper. This work was supported by the Center for Advanced Security Research Darmstadt (www.cased.de) \nand the Swiss National Science Foundation.  References [1] B. Alpern, A. Cocchi, S. Fink, and D. Grove. \nEf.cient implementation of Java interfaces: invokeinterface considered harmless. In Proceedings of the \n16th Conference on Object-oriented Programming, Systems, Languages, and Applications (OOPSLA), 2001. \n[2] G. Ammons, T. Ball, and J. R. Larus. Exploiting hardware performance counters with .ow and context \nsensitive pro.l\u00ading. In Proceedings of the 10th Conference on Programming Language Design and Implementation \n(PLDI), 1997. [3] M. Bebenita, M. Chang, G. Wagner, A. Gal, C. Wimmer, and M. Franz. Trace-based compilation \nin execution environments without interpreters. In Proceedings of the 8th International Conference on \nthe Principles and Practice of Programming in Java (PPPJ), 2010. [4] W. Binder. A portable and customizable \npro.ling framework for Java based on bytecode instruction counting. In Proceed\u00adings of the 3rd Asian \nSymposium on Programming Languages and Systems (APLAS), 2005. [5] W. Binder, J. Hulaas, and P. Moret. \nA quantitative evaluation of the contribution of native code to Java workloads. In Proceedings of the \nIEEE International Symposium on Workload Characterization (IISWC), 2006. [6] W. Binder, J. Hulaas, P. \nMoret, and A. Villaz\u00b4Platform\u00ad on. independent pro.ling in a virtual execution environment. Software: \nPractice and Experience, 39(1):47 79, 2009. [7] S. M. Blackburn, R. Garner, C. Hoffmann, A. M. Khang, \nK. S. McKinley, R. Bentzur, A. Diwan, D. Feinberg, D. Frampton, S. Z. Guyer, M. Hirzel, A. Hosking, M. \nJump, H. Lee, J. E. B. Moss, B. Moss, A. Phansalkar, D. Stefanovi\u00b4c, T. VanDrunen, D. von Dincklage, \nand B. Wiedermann. The DaCapo benchmarks: Java benchmarking development and analysis. In Proceedings \nof the 21st Conference on Object-oriented Programming, Systems, Languages, and Applications (OOPSLA), \n2006.  [8] E. Bodden, A. Sewe, J. Sinschek, M. Mezini, and H. Oueslati. Taming re.ection: Aiding static \nanalysis in the presence of re.ection and custom class loaders. In Proceedings of the 33rd International \nConference on Software Engineering (ICSE), 2011. [9] M. Braux and J. Noy\u00b4e. Towards partially evaluating \nre.ection in Java. In Proceedings of the Workshop on Partial Evaluation and Semantics-Based Program Manipulation \n(PEPM), 1999. [10] J. M. Bull, L. A. Smith, M. D. Westhead, D. S. Henty, and R. A. Davey. A methodology \nfor benchmarking Java Grande applications. In Proceedings of the ACM 1999 Conference on Java Grande, \n1999. [11] Y. Chiba. Redundant boxing elimination by a dynamic compiler for Java. In Proceedings of the \n5th International Conference on the Principles and Practice of Programming in Java (PPPJ), 2007. [12] \nS. R. Chidamber and C. F. Kemerer. A metrics suite for object oriented design. IEEE Transactions on Software \nEngineering, 20(6):476 493, June 1994. ISSN 0098-5589. [13] C. Collberg, G. Myles, and M. Stepp. An empirical \nstudy of Java bytecode programs. Software: Practice and Experience, 37(6):581 641, May 2007. ISSN 0038-0644. \n[14] C. Daly, J. Horgan, J. Power, and J. Waldron. Platform independent dynamic Java virtual machine \nanalysis: the Java Grande Forum benchmark suite. In Proceedings of the 2001 joint ACM-ISCOPE Conference \non Java Grande, 2001. [15] D. Detlefs and O. Agesen. Inlining of virtual methods. In Proceedings of the \n13th European Conference on Object\u00adoriented Programming (ECOOP), 1999. [16] S. Dieckmann and U. H\u00a8olzle. \nA study of the allocation behav\u00adior of the SPECjvm98 java benchmarks. In Proceedings of the 13th European \nConference on Object-oriented Program\u00adming (ECOOP), 1999. [17] I. Dragos and M. Odersky. Compiling generics \nthrough user-directed type specialization. In Proceedings of the 4th Workshop on the Implementation, \nCompilation, Optimiza\u00adtion of Object-oriented Languages and Programming Sys\u00adtems (ICOOOLPS), 2009. [18] \nG. Dubochet and M. Odersky. Compiling structural types on the JVM: a comparison of re.ective and generative \ntechniques from Scala s perspective. In Proceedings of the 4th Workshop on the Implementation, Compilation, \nOptimization of Object\u00adoriented Languages and Programming Systems (ICOOOLPS), 2009. [19] B. Dufour, K. \nDriesen, L. Hendren, and C. Verbrugge. Dy\u00adnamic metrics for Java. In Proceedings of the 18th Conference \non Object-oriented Programing, Systems, Languages, and Ap\u00adplications (OOPSLA), 2003. [20] Common Language \nInfrastructure (CLI). ECMA Interna\u00adtional, 5th edition, December 2010. [21] L. Eeckhout, A. Georges, \nand K. De Bosschere. How Java programs interact with virtual machines at the microarchi\u00adtectural level. \nIn Proceedings of the 18th Conference on Object-oriented Programing, Systems, Languages, and Appli\u00adcations \n(OOPSLA), 2003. [22] D. Gregg, J. Power, and J. Waldron. A method-level compar\u00adison of the Java Grande \nand SPEC JVM98 benchmark suites. Concurrency and Computation: Practice and Experience, 17: 757 773, 2005. \n[23] N. M. Hanish and W. E. Cohen. Hardware support for pro.ling Java programs. In Proceedings of the \nWorkshop on Hardware Support for Objects and Microarchitectures for Java, 1999. [24] R. E. Hank, W.-M. \nW. Hwu, and B. R. Rau. Region-based compilation: an introduction and motivation. In Proceed\u00adings of the \n28th International Symposium on Microarchitec\u00adture (MICRO), 1995. [25] U. H\u00a8Optimizingolzle, C. Chambers, \nand D. Ungar. dynamically-typed object-oriented languages with polymor\u00adphic inline caches. In Proceedings \nof the 5th European Con\u00adference on Object-oriented Programming (ECOOP), 1991. [26] K. Hoste and L. Eeckhout. \nMicroarchitecture-independent workload characterization. IEEE Micro, 27:63 72, 2007. [27] I. Jibaja, \nS. Blackburn, M. Haghighat, and K. McKinley. Deferred grati.cation: Engineering for high performance \ngarbage collection from the get go. In Proceedings of the Workshop on Memory Systems Performance and \nCorrect\u00adness (MSPC), 2011. [28] R. E. Jones and C. Ryder. A study of Java object demograph\u00adics. In Proceedings \nof the 7th International Symposium on Memory Management (ISMM), 2008. [29] R. K. Karmani, A. Shali, and \nG. Agha. Actor frameworks for the JVM platform: a comparative analysis. In Proceedings of the 7th International \nConference on the Principles and Practice of Programming in Java (PPPJ), 2009. [30] G. Lashari and S. \nSrinivas. Characterizing Java application performance. In Proceedings of the 17th International Parallel \nand Distributed Processing Symposium (IPDPS), 2003. [31] T. Lindholm and F. Yellin. The Java Virtual \nMachine Speci.cation. Addison-Wesley, 2nd edition, 1999. [32] A. McCallum, K. Schultz, and S. Singh. \nFACTORIE: Probabilistic programming via imperatively de.ned factor graphs. Advances on Neural Information \nProcessing Systems, 2009. [33] P. Moret, W. Binder, and A. Villaz\u00b4on. CCCP: Complete calling context \npro.ling in virtual execution environments. In Proceedings of the Workshop on Partial Evaluation and \nProgram Manipulation (PEPM), 2009. [34] M. Odersky, L. Spoon, and B. Venners. Programming in Scala. Artima \nPress, 2nd edition, 2010. [35] K. Ogata, D. Mikurube, K. Kawachiya, S. Trent, and T. On\u00adodera. A study \nof Java s non-Java memory. In Proceedings of the 25th Conference on Object-oriented Programming, Systems, \nLanguages, and Applications (OOPSLA), 2010. [36] K. Pearson. On lines and planes of closest .t to systems \nof points in space. Philosophical Magazine, 2:559 572, 1901. [37] D. Ramage, E. Rosen, J. Chuang, C. \nD. Manning, and D. A. McFarland. Topic modeling for the social sciences. In NIPS  Workshop on Applications \nfor Topic Models: Text and Beyond, 2009. [38] P. Ratanaworabhan, B. Livshits, and B. G. Zorn. JSMeter: \ncomparing the behavior of JavaScript benchmarks with real Web applications. In Proceedings of the 2010 \nUSENIX Conference on Web Application Development, 2010. [39] G. Richards, S. Lebresne, B. Burg, and J. \nVitek. An analysis of the dynamic behavior of JavaScript programs. In Proceedings of the Conference on \nProgramming Language Design and Implementation (PLDI), 2010. [40] I. Rogers, J. Zhao, and I. Watson. \nApproaches to re.ective method invocation. In Proceedings of the 3rd Workshop on Implementation, Compilation, \nOptimization of Object\u00adoriented Languages, Programs and Systems (ICOOOLPS), 2008. [41] J. R. Rose. Bytecodes \nmeet combinators: invokedynamic on the JVM. In Proceedings of the 3rd Workshop on Virtual Machines and \nIntermediate Languages (VMIL), 2009. [42] A. Sarimbekov, P. Moret, W. Binder, A. Sewe, and M. Mezini. \nComplete and platform-independent calling context pro.ling for the Java virtual machine. In Proceedings \nof the 6th Workshop on Bytecode Semantics, Veri.cation, Analysis and Transformation (BYTECODE), 2011. \n[43] A. Sarimbekov, A. Sewe, W. Binder, P. Moret, M. Sch\u00a8oberl, and M. Mezini. Portable and accurate \ncollection of calling\u00adcontext-sensitive bytecode metrics for the java virtual ma\u00adchine. In Proceedings \nof the 9th International Conference on the Principles and Practice of Programming in Java (PPPJ), 2011. \n[44] M. Schinz. Compiling Scala for the Java Virtual Machine. PhD thesis, EPFL, Switzerland, September \n2005. [45] A. Sewe. Scala = ? Java mod JVM.In Proceedings of the Work-in-Progress Session at the 8th \nInternational Conference on the Principles and Practice of Programming in Java (PPPJ), volume 692 of \nCEUR Workshop Proceedings, 2010. [46] K. Shiv, K. Chow, Y. Wang, and D. Petrochenko. SPECjvm2008 performance \ncharacterization. In Proceed\u00adings of the SPEC Benchmark Workshop, 2009. [47] A. M. Sloane. Experiences \nwith domain-speci.c language embedding in Scala. In Proceedings of the 2nd Inter\u00adnational Workshop on \nDomain-Speci.c Program Develop\u00adment (DSPD), 2008. [48] T. Suganuma, T. Yasue, and T. Nakatani. A region-based \ncompilation technique for a Java just-in-time compiler. In Proceedings of the 16th Conference on Programming \nLanguage Design and Implementation (PLDI), 2003. [49] C. Thalinger and J. Rose. Optimizing invokedynamic. \nIn Proceedings of the 8th International Conference on the Principles and Practice of Programming in Java \n(PPPJ), 2010.   \n\t\t\t", "proc_id": "2048066", "abstract": "<p>Originally conceived as the target platform for Java alone, the Java Virtual Machine (JVM) has since been targeted by other languages, one of which is Scala. This trend, however, is not yet reflected by the benchmark suites commonly used in JVM research. In this paper, we thus present the design and analysis of the first full-fledged benchmark suite for Scala. We furthermore compare the benchmarks contained therein with those from the well-known DaCapo 9.12 benchmark suite and show where the differences are between Scala and Java code---and where not.</p>", "authors": [{"name": "Andreas Sewe", "author_profile_id": "81413591960", "affiliation": "Technische Universit&#228;t Darmstadt, Darmstadt, Germany", "person_id": "P2839245", "email_address": "sewe@st.informatik.tu-darmstadt.de", "orcid_id": ""}, {"name": "Mira Mezini", "author_profile_id": "81100583946", "affiliation": "Technische Universit&#228;t Darmstadt, Darmstadt, Germany", "person_id": "P2839246", "email_address": "mezini@st.informatik.tu-darmstadt.de", "orcid_id": ""}, {"name": "Aibek Sarimbekov", "author_profile_id": "81482660049", "affiliation": "University of Lugano, Lugano, Switzerland", "person_id": "P2839247", "email_address": "aibek.sarimbekov@usi.ch", "orcid_id": ""}, {"name": "Walter Binder", "author_profile_id": "81100315534", "affiliation": "University of Lugano, Lugano, Switzerland", "person_id": "P2839248", "email_address": "walter.binder@usi.ch", "orcid_id": ""}], "doi_number": "10.1145/2048066.2048118", "year": "2011", "article_id": "2048118", "conference": "OOPSLA", "title": "Da capo con scala: design and analysis of a scala benchmark suite for the java virtual machine", "url": "http://dl.acm.org/citation.cfm?id=2048118"}