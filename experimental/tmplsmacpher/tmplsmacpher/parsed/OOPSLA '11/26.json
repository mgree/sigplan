{"article_publication_date": "10-22-2011", "fulltext": "\n Two for the Price of One: A Model for Parallel and Incremental Computation Sebastian Burckhardt Caitlin \nSadowski Thomas Ball Daan Leijen Jaeheon Yi Microsoft Research Microsoft Research Univ. of California \nat Santa Cruz tball@microsoft.com {sburckha,daan}@microsoft.com {supertri,jaeheon}@cs.ucsc.edu Abstract \nParallel or incremental versions of an algorithm can signif\u00adicantly outperform their counterparts, but \nare often dif.cult to develop. Programming models that provide appropriate abstractions to decompose \ndata and tasks can simplify par\u00adallelization. We show in this work that the same abstractions can enable \nboth parallel and incremental execution. We present a novel algorithm for parallel self-adjusting computation. \nThis algorithm extends a deterministic parallel programming model (concurrent revisions) with support \nfor recording and repeating computations. On record, we con\u00adstruct a dynamic dependence graph of the \nparallel computa\u00adtion. On repeat, we reexecute only parts whose dependencies have changed. We implement \nand evaluate our idea by studying .ve ex\u00adample programs, including a realistic multi-pass CSS lay\u00adout \nalgorithm. We describe programming techniques that proved particularly useful to improve the performance \nof self-adjustment in practice. Our .nal results show signi.cant speedups on all examples (up to 37x \non an 8-core machine). These speedups are well beyond what can be achieved by parallelization alone, \nwhile requiring a comparable effort by the programmer. Categories and Subject Descriptors D.1.3 [Software]: \nProgramming Techniques Concurrent Programming; D.3.3 [Software]: Programming Languages Language Constructs \nand Features General Terms Languages, Performance Keywords Self-adjusting computation, Incremental mem\u00adoization, \nParallel programming Permission to make digital or hard copies of all or part of this work for personal \nor classroom use is granted without fee provided that copies are not made or distributed for pro.t or \ncommercial advantage and that copies bear this notice and the full citation on the .rst page. To copy \notherwise, to republish, to post on servers or to redistribute to lists, requires prior speci.c permission \nand/or a fee. OOPSLA 11, October 22 27, 2011, Portland, Oregon, USA. Copyright c &#38;#169; 2011 ACM \n978-1-4503-0940-0/11/10. . . $10.00 (a) (b) compute: record { -deterministic compute() ; while (true) \n{ -potentially parallel } compute() ; -no I/O while (true) mutate() ; { } mutate: mutate() ; -nondeterministic \nrepeat ; -may perform I/O } Figure 1. (a) Imperative compute-mutate loop. (b) An equivalent loop using \nour record and repeat commands that can exploit redundancy in the computation. 1. Introduction Many programs \nperform repetitive tasks but are incapable of exploiting the redundancy inherent in the repeated com\u00adputation. \nFor example, in reactive or interactive programs, we often .nd compute-mutate loops as shown in Fig. \n1(a). In such a loop, a deterministic computation, represented by compute, is repeated. Between repetitions, \nthe user or envi\u00adronment may read and write the state in a nondeterminis\u00adtic manner, represented by the \nfunction mutate. The changes performed by mutate may be small or large, yet compute is always fully repeated, \nwhich is potentially wasteful. For example, a web browser recomputes the layout of a page many times, \nwhenever the page content changes based on user actions or the execution of scripts. If the changes are \nsmall, much of the layout computation is redundant; dis\u00adcovering and exploiting such redundancy manually \nor auto\u00admatically is dif.cult in practice. Other examples of compute\u00admutate loops appear in games, compilers, \nspreadsheets, edi\u00adtors, forms, simulations, and so on. Researchers have long recognized the performance \npo\u00adtential of incremental computation. For many algorithms, specialized versions can adjust quickly to \nchanges in the in\u00adput, typically improving the asymptotic complexity. More\u00adover, many general techniques \nto incrementalize a compu\u00adtation automatically or semi-automatically have been pro\u00adposed. For some algorithms, \nstandard techniques like mem\u00adoization or dynamic programming may be suf.cient. For im\u00adperative programs \nwith side-effects, researchers have pro\u00adposed self-adjusting computation, which can track data and control \ndependencies, selectively repeating only the parts of the computation that depend on changed inputs. \n Since the performance gains can be quite dramatic, self\u00adadjusting computation is a viable alternative \nto paralleliza\u00adtion (where it applies). Parallelization has been well studied as a means of improving \nthe performance of applications, but its success often requires serious thought and effort by the programmer. \nOver the years, researchers developed many programming models (embodied by languages, language ex\u00adtensions, \nor libraries) that facilitate parallelization by provid\u00ading suitable abstractions to the programmer. \nSelf-adjusting computation and parallelization exhibit some deep similarities: both aim to improve performance \nand both need to leverage the structure of the computation to do so successfully. This similarity in \npurpose and method is not merely of academic interest. In this paper, we demon\u00adstrate that it is possible \nand sensible to write programs that are both parallel and incremental, and can thus reap the performance \nbene.ts of both. We show that a single, small set of programming prim\u00aditives (record, repeat, fork, join, \nand type declarations) are suf.cient to simultaneously exploit parallelism and perform self-adjusting \ncomputation, at a programming effort compa\u00adrable to traditional parallelization. The crucial insight \nis that once the programmer declares how to divide a task into in\u00addependent parts, both self-adjustment \nand parallelization can bene.t. Overall, we make the following contributions: We present a single, small \nset of primitives that allow pro\u00adgrammers to simultaneously express the potential for par\u00adallelization \nand incrementalization in their applications.  We describe an algorithm for parallel self-adjusting \ncom\u00adputation, i.e. an algorithm that can (1) record dependen\u00adcies of a deterministic parallel computation, \nand (2) re\u00adpeat the computation while only reexecuting parts whose dependencies have changed.  We implement \nour model as an extension to the concur\u00adrent revisions system, deployed in the form of a C# li\u00adbrary, \nand apply it to .ve sample applications written in C#, including a multi-pass CSS layout algorithm. \n We describe three programming techniques (granularity control, simple outputs, and markers) that enable \nself\u00adadjusting computation to perform better in practice.  We demonstrate that for all of the studied \nexamples, self-adjustment can provide signi.cant performance im\u00adprovements compared to sequential or \nparallel execution only, while requiring a programming effort comparable to manual parallelization. Across \nour benchmarks we ob\u00adserve a 12x to 37x speedup compared to the sequential baseline on an 8-core machine. \n Section 2 gives an overview of our programming model. In Section 3 we describe the algorithm and give \na correct\u00adness argument. In Section 4 we discuss the benchmarks, the programming techniques, and the \nperformance results, con\u00adcluding in Section 5 with a discussion of related work. 2. Overview In this \nsection, we establish the parameters for our work. We begin with a simple illustration example that records \na com\u00adputation of a parallel sum, changes one of the summands, and then repeats the computation. We then \ndescribe the es\u00adsentials of the concurrent revisions model and the various isolation types in more detail. \n2.1 Programming Model Our work builds on concurrent revisions, a recently pro\u00adposed deterministic parallel \nprogramming model [14]. In this model, the programmer forks and joins tasks called re\u00advisions that can \nexecute in parallel. The programmer must declare data that is shared by concurrent revisions to have \nan isolation type. The model then guarantees deterministic par\u00adallel execution [16] by copying memory \nlocations that are accessed concurrently, and by resolving con.icts determin\u00adistically. We summarize \nthe essentials of this model in Sec\u00adtion 2.5. To extend the concurrent revisions model with support for \nself-adjusting computation, we add record and repeat commands that let the programmer identify the computation \nthat is to be repeated (Fig. 1 (b)). Moreover, we require that the programmer use isolation types not \nonly for concurrently accessed variables, but also for variables that are accessed by consecutive revisions \nand that thus may introduce data dependencies.  2.2 Parallel Sum Example The pseudo-code in the small \nexample program in Fig. 2 demonstrates how to record and repeat a parallel summa\u00adtion using record and \nrepeat commands. Summation follows a parallel divide-and-conquer structure and switches to se\u00adquential \nsummation below some threshold, a common prac\u00adtice for parallel algorithms. When programming with con\u00adcurrent \nrevisions, all variables that may be accessed by con\u00adcurrent revisions or by consecutive revisions must \nbe de\u00adclared using an isolation type. In main(), we .rst allocate and initialize an array of ver\u00adsioned \nintegers. Then we record the computation, a paral\u00adlel summation using recursive divide-and-conquer and \nfork\u00adjoin parallelism with a user-speci.ed threshold for switching over to sequential summation. In ParallelSum(), \nthe variable sum is declared to have type CumulativeInt. This data type de\u00adclares sum as versioned (and \nisolated in each revision), and on a join operation sum combines any additions that were done in parallel. \nThis is very similar in concept to hyperob\u00adjects in Cilk++ [22]. The .rst computed total is 1000. Next, \n const int threshold := 250 ; int ParallelSum(Versioned(int)[] a, int from, int to) {if (to - from . \nthreshold) return SequentialSum(a, from, to) ; else { CumulativeInt sum := 0 ; Revision r1 := fork { \n sum := sum + ParallelSum(a, from, (from + to)/2) ; }Revision r2 := fork {sum := sum + ParallelSum(a, \n(from + to)/2, to) ; } join r2 ; join r1 ; return sum ; }} int SequentialSum(Versioned(int)[] a, int \nfrom, int to) {int sum := 0; for (int i := from; i < to; i++) { sum := sum + a[i]; }return sum ; } void \nmain() {Versioned(int) a[1000] ; Versioned(int) total := 0; for (int i := 0; i < 1000; i++) { a[i] := \n1; } //initialize record { total := ParallelSum(a, 0, 1000) ; }assert(total = 1000) ; a[333] := a[333] \n+ 1 ; repeat ; assert(total = 1001) ; } Figure 2. Simple example of applying self-adjusting com\u00adputation \nto a parallel program using revisions. we add 1 to element a[333] and do a repeat where the com\u00adputed \ntotal is now 1001.  2.3 Execution In Section 3, we present the algorithm to implement record and repeat \nin detail. This algorithm records the dynamic de\u00adpendence graph of a parallel computation while it executes. \nWhen repeating a computation, the algorithm selectively re\u00adexecutes (and rerecords) only those revisions \nthat have a de\u00adpendency on changed input. Revisions are the unit of memoization in our model. Dur\u00ading \nthe repeat step, revisions whose input dependencies have not changed are not reexecuted but rather replayed \nby con\u00adsulting and applying their memoized effect. The execution of our example program is shown in Fig\u00adure \n3. The left side shows the dynamic unfolding of the re\u00adcursive computation of revisions during the record \nphase (in a so-called revision diagram). Note that memoization takes Figure 3. Revision diagrams illustrating \nthe initial compu\u00adtation recorded by record (left) and the repeated computation performed by repeat. \nOn repeat, the runtime avoids reexecut\u00ading some of the revisions, and simply replays their effects on \njoin. place at the level of the forked-joined revisions, not func\u00adtions; revisions are reexecuted or \nreplayed in their entirety. The right side of Fig. 3 shows the revisions that are reexe\u00adcuted during \nthe repeat phase (solid lines). This reexecution includes the revision that sums the range a[250-499] \nas well as that revision s ancestors. The dotted lines represent revi\u00adsions whose behavior is not affected \nby the update to a[333]. Only two of these revisions inside the computation (r.1 and r.1.2) are actually \nreexecuted during the repeat phase. During recording, we need to track memory accesses. Since the memory \nlocations that are potentially involved in data dependencies have a special isolation type, we can avoid \nthe excessive overhead that would result from tracking every single memory access.  2.4 Performance \nWe give a detailed evaluation of our algorithm and imple\u00admentation in Section 4. In particular, we parallelized \na multi\u00adpass CSS layout algorithm using the concurrent revisions model and then applied the self-adjustment \nalgorithm. We also studied two other interactive sample applications (a ray\u00adtracer and a morph creator) \nand two smaller benchmarks (a webcrawler and a spell checker). Just as it is sometimes dif.cult to write \nparallel pro\u00adgrams that perform well, we found that effective use of self-adjustment requires some thought \nby the programmer. By studying the performance of our benchmarks, we iden\u00adti.ed speci.c programming techniques \nthat enable self\u00adadjustment to perform better in practice: The .rst important observation is that if \nthe granularity of the smallest replayable unit is too small, the overhead of dependence tracking is \ntoo high. This issue is in per\u00adfect sync with the well-known importance of controlling the granularity \nof the smallest schedulable unit to limit the overhead of parallelization. Both of these issues can be \naddressed uniformly in our framework, by limiting the number of revisions forked (because in our model, \nrevisions are both the smallest schedulable unit and the smallest replayable unit). In our example program, \nwe control the granularity using a threshold parameter.  The second observation is that if revisions \naccess large amounts of data (for example, a bitmap representing a picture), tracking access to each \nindividual memory lo\u00adcation incurs too much overhead. To solve this prob\u00adlem, we introduce pseudovariables \ncalled markers. Each marker represents a group of locations, and we ensure that whenever revisions read/write \na memory location represented by a marker, it also reads/writes the cor\u00adresponding marker. Using such \nmarkers allows us to track dependencies more coarsely (trading precision for speed). Using the granularity \nand marker techniques, we were able to achieve excellent performance on all sample appli\u00adcations: The \nrecording overhead was very small and was in all cases more than compensated by the speed gained from \nparallel execution. On a eight-core machine, recording was thus still between 1.8x and 6.4x times faster \nthan the sequential baseline computation.  Repeating the computation after a small change was be\u00adtween \n12x and 37x times faster than the sequential base\u00adline computation (again on a 8-core machine).   2.5 \nConcurrent Revisions This section brie.y explains the concurrent revisions pro\u00adgramming model. Adding \nself-adjusting computation to a concurrent imperative language with low-level synchroniza\u00adtion primitives \n(such as threads and locks) is problematic. Racing side-effects introduce nondeterminism and cross\u00adthread \ndata dependencies that are dif.cult to track. To enable parallel self-adjusting computations, it is much \nbetter to start with a deterministic programming model that is not sensitive to a particular thread schedule. \nOur work is based on a recently proposed deterministic concurrent programming model, concurrent revisions \n[14]. Preliminary results [14] indicate that (1) concurrent revi\u00adsions are relatively easy to use for \nparallelizing reactive or interactive applications, (2) can deliver reasonable parallel performance for \nsuch applications, and (3) work well even for tasks that exhibit con.icts and would be dif.cult to par\u00adallelize \nby other means. The key design principles are: Explicit Join. The programmer forks and joins revisions, \nwhich can execute concurrently. All revisions must be joined explicitly. Declarative Data Sharing. The \nprogrammer uses special isolation types to declare data that may be shared.  Effect Isolation. All changes \nmade to shared data within a revision are only locally visible until that revision is joined.  Conceptually, \nthe runtime copies all shared data when a new revision is forked, marks all locations that are modi.ed \nin the revision, and writes the changed locations back to the joining revision at the time of the join. \nTherefore, the run\u00adtime can schedule concurrent revisions for parallel execution without creating data \nraces. As a result, the computation is determinate, meaning that it is fully speci.ed by the program \nand does not depend on the relative timing or scheduling of tasks [15]. To compare and contrast this \nmodel with classic asyn\u00adchronous tasks, consider the example code in Fig. 4. In Fig. 4(a), we fork/join \na classic asynchronous task; the as\u00adsignment x := 1 races with the assert statement above the join, with \nnondeterministic results. In Fig. 4(b), we fork/join a revision, and the variable x is declared to have \nthe isolation type Versioned(int). Since the writes to x become visible to the main revision at the join \nonly, there is no race, and the the value of x is deterministic. A technical report companion to the \noriginal paper con\u00adtains a thorough discussion and formalization of the seman\u00adtics of concurrent revisions \n[15, 16]. 2.5.1 Revision Diagrams It is often helpful to visualize computations using revision diagrams \n(see Fig. 4(c)). Such diagrams show each revi\u00adsion as a vertically aligned sequence of points, each point \nrepresenting one (dynamic) instance of a statement. We use curved arrows to show precisely where new \nrevisions branch out (on a fork) and where they merge in (on a join). In gen\u00aderal, revisions can be nested \nand the life-time of a child revi\u00adsions can exceed that of its parent. As such, it is more general than \npure fork-join paralellism where the life-time of each task is limited to the lexical scope. Some restrictions \nstill exist, where a revision diagram always forms a semi-lattice which is important when merging isolation \ntypes [16]. Since revisions can not directly communicate and are fully isolated, information .ows along \nedges only. Revision diagrams overapproximate control and data dependencies: two statements not connected \nby a path in the revision dia\u00adgram are always independent.  2.5.2 Cumulative Types In initial applications \nfor concurrent revisions [14], the ver\u00adsioned type Versioned(T) delivered the correct semantics for the \nmajority of data. However, this type is not appropriate for parallel aggregation, since it wipes out \nthe current value of a location when joining a revision that modi.ed that location. For aggregation, \nthe revisions model supports the cu\u00admulative isolation type Cumulative(T,f), where f is a user\u00ad  (a) \n(b) (c) x := 0. Versioned(int) x; int x; x := 0; t x := 0; Revision r := fork. Task t := fork{ fork{ \nx := 1; t \\ assert x := 1; ..x := 1 }(x=0) } assert(x = 0 assert(x = 0) ; t .  . x = 1); join. join \nt; join r; assert t assert(x = 1) ; assert(x = 1) ; . (x=1) Figure 4. (a) A classic asynchronous task \noperating on a standard integer variable. (b) A revision operating on a ver\u00adsioned integer variable. \n(c) A revision diagram that visual\u00adizes how the state of x is branched and merged. high-level pseudo-code \nof our algorithm and the implemen\u00adtation of the main operations of read and write (of versioned locations), \nrecord, repeat, fork, and join. Our presentation contains simplifying assumptions and restrictions that \nare not part of our actual implementation1 since we believe that these details would make it quite chal\u00adlenging \nto follow the logic of the algorithm. We discuss some of the differences and the relationship to the \nconcurrent re\u00advisions algorithm [14] in more detail in the appendix. 3.1 Motivation We present detailed \npseudocode since the algorithm proved dif.cult to get right and we went through multiple iterations. \nMoreover, it is signi.cantly different from all previous im\u00adplementations of self-adjusting computations, \nwhich were designed purely for sequential execution. Some particular dif.culties include: (a) (b) x := \n0. t CumulativeInt x := 0 ; .r := fork { x := x + 2; } t . x := x + 3..x := x + 2 x := x + 3; join r; \n//x = 3 +(2 - 0) t  . assert(x = 5) ; assert t . (x=5) Figure 5. Example illustrating parallel aggregation \nwith the isolation type CumulativeInt. speci.ed merge function f : (T,T,T) . T. This merge func\u00adtion \nis called by the runtime on join. For example, we can de.ne a cumulative integer as CumulativeInt = Cumulative(int,accumulate) \nwhere the function accumulate is de.ned as follows: int accumulate(int current, int joined, int original) \n{ return current + (joined - original) ; } The runtime calls this function during a join with current \nbe\u00ading the current value in the joining revision, joined being the current value in the joined revision, \nand original being the value at the time the revision was originally forked. Fig. 5 shows an example \nof how CumulativeInt performs aggrega\u00adtion. Both Versioned(T) and Cumulative(T,f) are subclasses of a \nVersioned class. Note that the determinacy of the computation does not depend on the merge function being \nassociative or commu\u00adtative; the only requirement is that it be a function in the mathematical sense. \n3. Implementation We now proceed to explain how we add support for self\u00adadjusting computation to concurrent \nrevisions. We provide The decision whether to reexecute or replay a revision must be both fast and precise. \nSimple validation schemes (e.g. recording all inputs and checking whether they are the same before replaying) \nare too slow, while simple in\u00advalidation schemes (e.g. mark a revision for reexecution any time anybody \nwrites to one of its inputs) are too im\u00adprecise. Precision was particularly important for the CSS layout \nexample.  Dependencies must be tracked accurately. In a sequential setting, timestamps are suf.cient \nto determine dependen\u00adcies. In a concurrent setting, we need to track the struc\u00adture of the computation \n(which is in our case represented by the revision diagram). Note that this approach cru\u00adcially relies \non the isolation guarantees.  The data structures must be designed to be thread-safe (both record and \nrepeat contain parallelism), yet use min\u00adimal synchronization to avoid overhead.   3.2 Description \nof Operation At a high level, our implementation handles the example program in Fig. 2 as follows: 1. \nThe call to record performs a computation as shown in the revision diagram in Fig. 3, on the left. As \nrevisions exe\u00adcute, we construct a summary tree, where each summary records details about a revision, \nsuch as (1) a pointer to the entry point of the code, (2) the sequence of child sum\u00admaries, in fork order, \n(2) the writes performed by the summary, and (3) the dependencies of the summary. De\u00adpendencies can be \nexternal (the summary depends on a 1 Speci.cally, we assume that no revisions are used outside the record/repeat \ncomputations, we omit memory management considera\u00adtions such as garbage removal and reference counting, \nwe store writes di\u00adrectly in List data types (assumed to be thread-safe) rather than building truly thread-safe \nsegment trees [14], we do not consider cumulative types as described in Section 2.5.2, and we do not \ndiscuss how we handle the creation of temporary or permanent versioned locations during recording.  \nvalue that was written before the recording began), or in\u00adternal (the summary depends on a value that \nwas written earlier during recording). When .nished recording, the summary tree contains all the needed \ninformation. We assume throughout that all forked revisions are joined be\u00adfore recording ends. 2. Upon \nthe assignment a[333] := 2 the runtime records that this location was written to. 3. When the user calls \nrepeat, we .rst perform external invalidations. This means that for any summary that has an external \ndependency on a value that was modi.ed since the last record, we invalidate that summary and all of its \nparents. In this example, the summary r.1.2 has an external dependency on the modi.ed location a[333], \nso we invalidate summaries for r.1.2 and r.1 and r. 4. We now check if we can replay the computation. \nIf the root summary r were still valid, we could replay the entire computation instantly (by writing \n1000 to total). However, in this case, the top summary r is invalid, so we have to reexecute it. As we \nreexecute, we also construct a new summary. 5. When the re-executing code calls fork, we pull up the \nsummary from the previous execution that has the same location in the summary tree as the summary that \nwe are just about to fork. If a summary is still valid, we need not reexecute it and we can keep the \nold summary object. Otherwise, we reexecute and construct a new summary object. In this example, summaries \nr.1 and r.1.2 are reex\u00adecuted, but not r.1.1 and r.2. 6. When joining a summary that was not reexecuted, \nwe reapply the effects as memoized during record at the join point. For r.1.1, the recorded effect is \nto add 250 to the sum. Similarly, revision r.2 is entirely elided and its effect (adding 500 to the sum) \nis reapplied at its join point. See Figure 16 in the appendix for how the MergeWrites function (from \nFigure 8) works in the presence of user\u00adde.ned merge functions.  Perhaps the most intricate part of \nthe algorithm is the propagation of internal invalidations (in the above example, there are no internal \ndependencies and thus no internal in\u00advalidations). We perform internal invalidations whenever we encounter \na fork while reexecuting a summary; right before executing the fork (and checking whether its summary \nis still valid), we .rst examine what has been written by the cur\u00adrent revision so far, and check if \nany downstream summaries need to be invalidated because of it. A common case is where the reexecuting \nrevision writes a different value than the previously recorded one. Another case is where it writes to \na location that it did not write to before (and any part of the code that depended on an older write \nneeds to be invalidated). Perhaps the most subtle case is where a location that was written to previously \nis not written to at all this time around (and any part of the code that depended on the previous write \nneeds to be invalidated). To handle such cases, we explicitly pass along the previous summary tree while \nbuilding a new one during re-execution. Another subtlety is that we wish to only invalidate sum\u00admaries \nthat are forked after the invalidating write. To handle this, we need to index writes in a summary relative \nto the number of forks and use this index when tracking dependen\u00adcies. 3.2.1 The Summary Class Figure \n6 shows the main data structure in our algorithm; a Summary. A summary captures the information we record \nabout each revision. Summaries store the entry point2 of the recorded code in code (which may be needed \nfor reexecution), and a thread thread that executes the code. The summaries form a tree structure that \ncorresponds to the fork structure of the pro\u00adgram. Each summary has a parent summary (null if it is the \nroot of the tree), and a list of children summaries. The child summaries are ordered by the fork order \nin the summary s thread. A summary s may be uniquely identi.ed in the summary tree by a coordinate (d,i), \nwhere d represents the depth of the summary in the tree (with the root summary having depth 0), and i \nrepresents the index of s in the list s.parent.children (i is -1 if s is the root summary). The execution \nof a code thunk by a thread is broken into segments by fork operations: a thread that forks n children \nhas n +1 segments. For each segment, the summary records in the writes .eld the last write operation \nto each versioned location written during the execution of that segment. Each segment maintains an explicit \nwrite list3; we can then implement reads to versioned locations by searching through a parent chain. \nThis gives the correct semantics for concurrent revisions where each revision only sees its own writes \nand is fully isolated from other revisions. Further\u00admore, we have a static variable globalwrites (also \nde.ned in Figure 6) that contains all the global writes (i.e. outside the recorded computation4). We \nassume that all versioned loca\u00adtions are present in the .rst map in the globalwrites list as if they \nwere all initialized before the program starts. The isValid .ag tracks if the summary is still valid: \nif any dependency changes, this .ag is set to false when dependen\u00adcies are validated. The dependencies \n.eld maintains the de\u00adpendencies of a summary explicitly. It is implemented as a map from versioned locations \nto coordinates in the summary tree. For example, if a location l was read in the summary s thread, there \nwould be an entry for l with coordinates (d,i) if 2 For simplicity, we assume that parameters are passed \nusing versioned variables. 3 Our actual implementation encodes these lists using thread-safe segment \ntrees [14]. 4 Again, our actual implementation uses segment trees which enables us to use concurrent \nrevisions outside the computation as well.  private class Summary { // readonly .elds Thunk code ; Summary \nparent ; // tree parent int depth, index ; // coordinate in tree // all .elds below change during execution \nof code bool isValid ; Thread thread ; List(Summary) children ; // tree children // read before write \ndependencies Map(Versioned, Pair(int,int)) dependencies ; List(Map(Versioned,Value)) writes ; // write \nsegments int forkCount() { return children.Count ; } // constructor Summary(Summary parent, Thunk code) \n{this.parent := parent ; this.code := code ; this.depth := (parent = null) ? 0 : parent.depth+1 ; this.index \n:= (parent = null) ? 0 : parent.forkCount() ; this.isValid := true ; this.children := new List(Summary)() \n; this.dependencies := new Map(Versioned, Pair(int, int))() ; this.writes := new List(Map(Versioned,Value))() \n; } private void Start(Summary prev) { if (thread null) = error( cannot start a summary more than once \n) ; writes.Add(new Map(Versioned,Value)()) ; thread := StartNewThread{ // current and previous are threadlocal \nstate current := this ; previous := prev ; code() ; } } private void Wait() { if (thread null) thread.join() \n; = }} [threadlocal] private static Summary previous ; [threadlocal] private static Summary current \n; private static List(Map(Versioned,Value)) globalwrites ; Figure 6. The de.nition of a Summary, including \nthe (thread local) static variables current, previous, and globalwrites. that location l was last written \nby an ancestor summary iden\u00adti.ed by (d,i); i.e. it was written by a prior summary at depth d in parent \ntree, just before its ith fork point. Dependence on writes which occur at the top level, outside of the \nsummary tree, are captured by the location (0,-1). Finally, we have the methods Start and Wait that start \nthe thread associated with the summary and wait for it to end. The Start method sets two thread local \nvariables, current and previous, before executing the associated code. The current thread local variable \npoints to the summary itself while the previous thread local variable points to the summary con\u00adstructed \nduring a previous execution. On a .rst recording, the previous variable will be null. The previous summary \nis used when a certain summary becomes invalid and is re-executed. By keeping the previous summary, we \ncan compare the writes done in the new ex\u00adecution with the ones done previously. This information is \nnecessary in order to precisely determine if more locations become invalidated when re-executing a summary. \nFor ex\u00adample, if during re-execution a certain location is not writ\u00adten, but it was written during the \nprevious execution, then all summaries that depended on that location need to be re\u00adexecuted too.  3.2.2 \nWriting, Reading, and Dependencies We assume that any writes and reads to versioned locations call the \nread and write operations de.ned in Figure 7. A write operation on a versioned location updates the global \nstore if its execution is outside of a computation being recorded or reexecuted (current=null). Otherwise, \nit writes to the last segment of the current (thread local) summary. In our real implementation, we implement \nsuch write lists using version maps in concurrent revisions, but for simplicity, we represent these lists \nhere explicitly to model the concurrent revision semantics. Similarly, a read operation on a versioned \nlocation reads from the global store if its execution is outside of a computa\u00adtion being recorded or \nreexecuted. Otherwise, the lookup op\u00aderation proceeds up the tree of summaries, starting with the current \nsummary. Note that when reading from each parent, we start looking at s.index in the writes list since \nwe cannot see writes that occurred after our own fork point. The other notable aspect of the read procedure \nis that it records write-to-read dependencies, when the write occurs outside of the summary in which \nthe read took place using the AddDependency method. A dependency records the fact that a read of location \nl by summary s reads the value written by the last write to l within a particular parent segment in the \nsummary tree; this segment is identi.ed by its depth and index. The dependencies are stored in the dependencies \nmap of the currently executing summary (see class Summary). Now that we have described the summary data \nstructure, the execution of reads and writes, and how they establish dependencies, we are in a position \nto describe the record, repeat, fork, and join procedures.  public void write(Versioned loc, Value val) \n{if (current = null) globalwrites.Last()[loc] := val ; else current.writes.Last()[loc] := val ; } public \nValue read(Versioned loc) {if (current = null) return LastWrite(globalwrites, loc) ; // was loc written \nto by this summary? if (FindWrite(current.writes, loc)) return LastWrite(current.writes, loc) ; // otherwise, \nsearch along parent chain Summary s := current ; while (s.parent = null) { for (i := s.index ; i ? 0 \n; i--) if (s.parent.writes[i].ContainsKey(loc)) { // internal dependency AddDependency(current, loc, \ns.parent.depth, i) ; return s.parent.writes[i][loc] ; } s := s.parent ; } // external dependency AddDependency(current, \nloc, -1, 0); return LastWrite(globalwrites, loc) ; } private bool FindWrite(List(Map(Versioned,Value)) \nwritelist, Versioned loc) {for (i := writelist.Count-1; i ? 0 ; i--) if (writelist[i].Contains(loc)) \nreturn true ; return false ; } private Value LastWrite(List(Map(Versioned,Value)) writelist, Versioned \nloc) {for (i := writelist.Count-1; i ? 0 ; i--) if (writelist[i].ContainsKey(loc)) return writelist[i][loc] \n; error( write not found ) ; } private void AddDependency(Summary s, Versioned loc, int depth, int index) \n{if (! s.dependencies.ContainsKey(loc)) {s.dependencies[loc] := (depth, index) ; }} Figure 7. Implementation \nfor writing and reading versioned data, including the FindWrite, LastWrite, and AddDependency helper \nmethods. private static Summary compute ; public void record(Thunk code) {compute := new Summary(null, \ncode) ; compute.Start(null) ; compute.Wait() ; globalwrites.Add(new Map(Versioned,Value)); MergeWrites(compute.writes, \nglobalwrites.Last()) ; } public void repeat() {DoExternalInvalidations() ; if (!compute.isValid) { var \nprev = compute ; compute := new Summary(null, compute.code) compute.Start(prev) compute.Wait() ; } globalwrites.Add(new \nMap(Versioned,Value)); MergeWrites(compute.writes, globalwrites.Last()) ; } private void MergeWrites(List(Map(Versioned,Value)) \njoinee, List(Map(Versioned,Value)) main){foreach(segment in joinee) {foreach ((loc,value) in segment) \n{ // writes in joinee win main.Last()[loc] := value ; }}} private void Invalidate(Summary s) {s.isValid \n:= false ; // invalidate along the parent chain too if (s.parent = null) Invalidate(s.parent) ; } private \nvoid DoExternalInvalidations() {var before := globalwrites.WithoutLastElement() ; foreach((loc,value) \nin globalwrites.Last()) { // skip if value has not changed if (value = LastWrite(before,loc)) continue \n; // invalidate dependent summaries foreach((s, (d, i)) where s.dependencies[loc] = (d, i)) {if (d = \n-1) // depth outside of summary tree Invalidate(s) ; }}} Figure 8. Implementation of record and repeat, \nand the helper methods MergeWrites, Invalidate, and DoExternalInvalidations.  3.2.3 Record and Repeat \nThe record procedure (Figure 8) takes a code thunk as input and creates a summary object that summarizes \nthe execution of that thunk. After the execution of the thunk completes, the procedure merges the writes \nby the thunk into the global write list using MergeWrites. The MergeWrites procedure de\u00adscribed in this \nsection implements only the joinee wins strategy for regular versioned locations. In the appendix we \nalso show an implementation that can handle user de.ned merge functions (Figure 16). The repeat procedure \n.rst performs invalidations via DoExternalInvalidations, which may set the isValid .ag of various summaries \nto false. If the root summary is not valid, then a new summary must be created via (partial) re\u00adexecution, \nusing the same code thunk as initially provided. The new root summary is started with the previous computed \nsummary prev as its argument. The procedure DoExternalInvalidations (see Figure 8) de\u00adtermines which \nsummaries are invalidated by global writes that occurred between the record procedure and beginning of \nthe repeat procedure. By construction, these writes occur in the last map of the list globalwrites. If \nthe value of a loca\u00adtion loc was written but has not changed then no invalidation takes place. Otherwise, \nfor every summary s that is depen\u00addent on the location loc by a write with global scope (depth equal \nto -1), s is invalidated by a call to Invalidate. The procedure Invalidate takes a summary as input and \nsets its isValid bit to false, and recursively invalidates the parent summary, until reaching the root \nof the summary tree.  3.2.4 Fork and Join The fork operation de.ned in Figure 9 takes a code thunk as \ninput. A fork operation may only take place in a thread that has a corresponding Summary (that is, no \nfork is allowed outside of a repeat/record execution). The fork operation .rst performs invalidations \nvia DoInternalValidations. It then .nds the (candidate) summary from the previous execution that corresponds \nto this fork. If this candidate is valid and has the same code thunk as input to the fork, then it can \nbe safely reused/replayed. Otherwise, the forked code must be re\u00adexecuted by creating a new summary. \nFinally, the summary corresponding to the fork is added to the current summary s children and a new write \nmap is appended to accumulate updates in the next segment of the current thread s execution. The procedure \nDoInternalValidations de.ned in Figure 10 determines which summaries must be invalidated due to differences \nin the behavior of the current thread segment (just before the fork) compared to its past behavior. The \nmap recentwrites contains the locations written in the segment, while the map previouswrites contains \nthe location written in the corresponding segment of the previous execution. For each location loc, there \nare three cases that (might) require invalidation: 1. loc is in the domain of recentw, but not previousw; \npublic Summary fork(Thunk code) {if (current = null) error( no fork allowed outside computation ) ; DoInternalInvalidations() \n; // null if unde.ned Summary candidate := null ; if (previous = null . previous.children.Count > current.forkCount()) \ncandidate = previous.children[current.forkCount()] ; if (candidate = null . candidate.isValid . candidate.code \n= code) {candidate.parent := current ; } else { var prev = candidate ; candidate := new Summary(current, \ncode) ; candidate.Start(prev) ; } current.children.Add(candidate) ; current.writes.Add(new Map(Versioned,Value)()) \n; return candidate ; } public void join(Summary s) {if (current = null) error( no join allowed outside \ncomputation ) ; s.Wait() ; MergeWrites(s.writes, current.writes.Last()) ; } Figure 9. Implementation \nof fork and join. 2. loc is in the domain of previousw, but not recentw; 3. loc is in the domain of \nboth maps but has different values in each map;  When any of these cases occur, InvalidateDependentSummaries \nis called with that location as an argument. This procedure iterates over all summaries s that are dependent \non loc. If s is already invalid, there is nothing to do Otherwise, we know that the summary s was previously \nor currently dependent because of a write to location loc in execution segment at coordinate (d,i). The \nwhile loop searches up the parent chain in the sum\u00admary tree to see if the summary current is between \nthe coordinate (d,i) and the given summary s. More precisely, we mean that the (added, modi.ed, or deleted) \nwrite to loca\u00adtion loc at coordinate (current.depth,current.forkCount()) ex\u00adecutes at or after the write \nto loc at coordinate (d,i) and before the read of location loc by summary s at coordinate (s.depth.s.index). \nThe variable dep is initially set to summary s. The while loop iterates as long as dep.depth is positive, \ngreater than d, and greater than current.depth (if any of these conditions become false it is clear that \nthe summary current cannot be between (d,i) and s).  private void DoInternalInvalidations() {if (previous \n= null . previous.writes.Count current.forkCount()) return ; var recentw := current.writes.Last() ; var \npreviousw := previous.writes[current.forkCount()] ; // process recent writes foreach ((loc, value) in \nrecentw) if (!previousw.ContainsKey(loc) . previousw[loc] = value) InvalidateDependentSummaries(loc) \n; // process the old writes that did not happen again foreach (Versioned loc in previousw.keys-recentw.keys) \nInvalidateDependentSummaries(loc) ; } private void InvalidateDependentSummaries(Versioned loc) {foreach \n((s, (d, i)) where s.dependencies[loc] = (d, i)) {if (!s.isValid) continue ; Summary dep := s ; while \n(dep.depth > 0 . dep.depth > d . dep.depth > current.depth) { if (dep.parent = previous) {if (current.forkCount() \n> dep.index) break ; if (current.depth = d . current.forkCount() < i) break ; Invalidate(s) ; break ; \n } dep := dep.parent ; }}} Figure 10. Implementation of the helper methods DoInternalInvalidations and \nInvalidateDependentSummaries. The loop body .rst checks if dep.parent is equal to previous. This check \ndeserves some explanation. Certain summaries in the dependencies list of location loc may have been inserted \nduring the current execution of repeat. There\u00adfore, it is important to verify which tree we are in (the \none from the past, before the execution of repeat, or the one cur\u00adrently being constructed) as we traverse \nup the parent chain. The check ensures that dep.parent is from the previous sum\u00admary tree. Now, if current.forkCount() \nis greater than dep.index then its execution order is after the summary s under considera\u00adtion, and so \nit cannot occur between (d,i) and s. In this case, s will not be invalidated. Otherwise, the current.forkCount() \nis less than or equal to dep.index, and since we have that dep.depth > current.depth, we also know that \nthe segment (current.depth, current.forkCount()) executes before s. If current.depth is equal to d and \ncurrent.forkCount() is less than i, the segment (current.depth, current.forkCount()) executes before \nthe segment (d,i), which means that the write to loc in this segment, if repeated, would block the write \nfrom current reaching the read of loc in s. Otherwise, one of three cases hold: d is less than current.depth, \nin which case the summary current executes after segment (d,x) for any x.  d equals current.depth and \ni is less than current.forkCount(), in which case the summary current also executes after (d,i);  d \nequals current.depth and i equals current.forkCount(). In this case, we must assume that the write to \nlocation loc in segment (current.depth,current.forkCount()) comes after the write to loc in segment (d,i). \n In all three of the case above, invalidation of summary s takes place. The algorithm as presented is \nquite intricate and it would be useful to have correctness proof. We do not have a com\u00adplete proof at \nthis point, but we have worked through the most interesting parts of the reasoning with some care, and \nincluded it in the appendix (Section A.1). 4. Experiments and Results In this section, we describe our \nexperiences applying self\u00adadjusting concurrent revisions to a selection of small appli\u00adcations. We give \nquantitative performance results and de\u00adscribe the programming techniques that were particularly ef\u00adfective \nat improving the performance of self-adjusting com\u00adputation. Our library prototype is an extension of \nthe original con\u00adcurrent revisions C# library, which uses an advanced work\u00adstealing scheduler. We added \nprimitives for record and repeat as described. To pass code arguments to the fork and repeat operations, \nwe use delegates, the C# version of closures. Since our prototype is implemented entirely as a C# library, \nwe were able to take existing sample programs and conve\u00adniently parallelize/incrementalize them using \nour new prim\u00aditives. 4.1 Studied Examples We studied two simple benchmarks (which we wrote our\u00adselves), \ntwo interactive parallel sample applications (which we took from samples delivered with .NET 4.0 [30]), \nand a sequential CSS layout algorithm (which we obtained from a research group investigating ef.cient \nweb browser designs). Each example contains a compute-mutate loop, for which we chose some representative \nsmall mutation. Raytracer This interactive application (an extension of a sample from [30]) repeatedly \nrenders a classic raytracer scene, showing polished balls on a tiled plane. The com\u00adputation traces, \nfor each pixel, an independent ray that touches various objects as it bounces around. The small mutation \nbetween repetitions is to alter the color of one of the balls, which changes the color of all pixels \nwhose rays have touched that ball.  StringDistance This simple example is inspired by a spell\u00adchecker \nthat provides correction suggestions. The com\u00adputation .nds for each of 20 given words the 3 clos\u00adest \nmatches in a dictionary of 12,000 words, using the Levenshtein distance metric (also known as edit dis\u00adtance) \n[31]. The small mutation is to add one word to the dictionary. WebCrawler This example is inspired by \na webcrawler, and (unlike all other examples) is I/O-bound rather than CPU\u00adbound. The computation performs \nseveral tasks for each of 30 given URLs, such as measuring latency, download\u00ading and validating the HTML, \nand counting the number of words. The small mutation is to modify one of the URLs. For this benchmark, \nwe assume the content at the URLs is static. CSS Layout This example models a CSS (cascading style sheet) \nlayout algorithm, as employed by HTML browsers. While simpler than a real browser, it contains important \ncomplexities such as the presence of .oating boxes. The original sample code we obtained was sequential; \nwe parallelized it and added self-adjustment. The computa\u00adtion lays out a randomly generated tree containing \nabout 400,000 boxes using three passes performed in sequence: 1. The .rst pass propagates down temporary \nwidth and height, and propagates up minimum and preferred width. 2. The second pass computes the width \non the way down, and the height on the way up (laying out the children boxes). It also sets the relative \nposition of all boxes. 3. The third pass computes the absolute position of all  boxes. The small mutation \nis to change the contents of one of the (leaf) text boxes, making it wider in the process. Depending \non the data, this could in principle change the position of every single box, but in practice most boxes \nstay in the same position. Morph This interactive application (an extension of a sam\u00adple from [30]) computes \na morph, that is, an animation that interpolates between two bitmaps guided by a list of vector pairs \nsupplied by the user. The small mutation is to alter a small portion of each picture (in our case, by \noverlaying some text in each picture). All examples with the exception of the layout algorithm exhibited \nplenty of parallelism and were easy to parallelize (by words, URLs, or picture tiles). In the layout \nexam\u00adple, divide-and-conquer naturally .ts the tree structure, and can be applied in each of the three \npasses. The achievable speedup is modest, however, because the tree is not bal\u00adanced. In principle, one \ncould spend extra development effort and manually derive incremental versions of all of these al\u00adgorithms. \nHowever, we believe that for any realistically de\u00adtailed application, the complexity of manual incrementaliza\u00adtion \ncan quickly become overwhelming. In the layout algo\u00adrithm, in particular, understanding dependencies \nis challeng\u00ading as they show up in many places (vertically for each pass, going down or up, and also \nlaterally from one pass to the next).  4.2 Performance Results We show the main performance results \nin Fig. 11. For each benchmark, the baseline column shows the time taken by the original code for the \ncomputation (without versioning, paral\u00adlelism, or any form of self-adjusting computation). We then show \nexecution times and speedups for recording the com\u00adputation the .rst time around, and repeating it after \nmak\u00ading a small change to the inputs. All speedups are relative to the baseline, and all measurements \nare taken on a 8\u00adcore machine (a 2.33 GHz Intel Xeon(R) E5345). On the right, we show statistics about \nthe computation: the number of summaries that were affected by the small mutation and needed to be reexecuted \n(out of the total number of sum\u00admaries recorded), and the number of read (R) and write (W) accesses to \nversioned locations (V) and markers (M) that the computation performed during recording. The numbers \nshow that the recording overhead is small enough to be compensated by the gains from parallel execu\u00adtion. \nSpeci.cally, recording is still faster than the sequen\u00adtial baseline, between 1.8x and 6.4x times. When \nrepeat\u00ading the computation after a small change, the speedup is much beyond the reach of optimal parallelization \n(8x), rang\u00ading from 12x to 37x. These numbers con.rm our claim that self-adjusting computation is not \njust an alternative to paral\u00adlelization, but indeed a lucrative extension. To identify in more detail \nhow the speedups are com\u00adposed, we provide a more detailed breakdown and visual representation in Fig. \n12. Again, we compare normalized execution times relative to the sequential baseline. For each benchmark \nwe measure record and repeat, and also compute (which uses the vanilla concurrent revisions model, without \nrecording dependencies). Moreover, for each of those sce\u00adnarios, we test how much of the effect is due \nto parallel ex\u00adecution by imposing variable parallelism bounds on the un\u00adderlying task scheduler: a parallelism \nbound of n means that at most n revisions can be simultaneously executing at any point of time. We observe \na few interesting details in Fig. 12. First, we can see that the use of the concurrent revisions model \nadds overhead: compute with parallelism bound 1 is up to Figure 12. Graphical representation of normalized \nexecution times for compute, record and repeat, relative to the sequential baseline. Lower is better. \nWe show execution times for various parallelism bounds (for example, pb=4 instructs the revision scheduler \nto schedule at most 4 revisions in parallel).  Benchmark lines baseline parallel record parallel repeat \nreexecuted RV WV RM WM Raytracer 1409 2.140 s 751 ms 2.8x 116 ms 18x 19 of 145 3.5M 0 0 0 StringDistance \n618 268 ms 122 ms 2.2x 22.5 ms 12x 2 of 13 236k 236k 0 0 WebCrawler 525 16.3 s 3.42 s 4.8x 1.03 s 16x \n3 of 109 135 135 0 0 CSS Layout 2674 185 ms 101 ms 1.8x 6.88 ms 27x 11 of 143 1674 377 2605 1296 Morph \n2238 143 s 22.2 s 6.4x 3.86 s 37x 121 of 1729 0 0 5.7M 0 Figure 11. Main performance results and statistics. \nWe show benchmark characteristics, average execution times and speedups relative to the sequential baseline, \nand statistics about the summaries and accesses to versioned locations and markers.  Benchmark Memory \nin VM Working Set Size self-adj. baseline self-adj. baseline Raytracer 7.55 2.59 91.7 90.3 Morph 2.62 \n1.13 127.2 131.6 Css0 169 126 368 281 Figure 13. Measured memory consumption for the three largest benchmarks, \nin Megabytes.  30% slower than the sequential baseline. However, the dif\u00adference between compute and \nrecord is marginal, meaning that adding self-adjustment to concurrent revisions does not worsen this \noverhead signi.cantly. We can also see that while parallelism is useful during repeat for two of the \n.ve examples (Raytracer, Morph) it does not help the other three, because they exhibit no signi.cant \nparallelism within the parts of the computation that is reexecuted during repeat. Memory Consumption. \nRunning a self-adjusting computa\u00adtion does of course require more memory than the original baseline computation, \nsince it records the computation and stores multiple versions of shared variables. To get a better izontal \naxis shows (on a logarithmic scale) the number of revisions in the computation (which increases exponentially \nwhen we decrease granularity). The vertical axis shows the speedup (or slowdown, if less than 1) relative \nto the baseline, also on a logarithmic scale.  understanding of the practical impact of this fact, \nwe mea\u00adsured the following quantities for the three largest bench\u00admarks (Fig. 13): Size of the Recorded \nComputation. We measure the mem\u00adory allocated by the VM right after recording the compu\u00adtation, and compare \nit to the memory allocated by the baseline (which does not use versions or record compu\u00adtations) at the \ncorresponding program point.  Comparison of Working Set Size. We compare the work\u00ading set size (as reported \nby the OS) of the self-adjusting computation and the baseline.  Not surprisingly, the size of the recorded \ncomputation varies with the benchmark. For the Raytracer and Morph bench\u00admarks, it consumes a large proportion \nof the managed mem\u00adory, but does not signi.cantly affect the working set size. For the layout examples, \nthe extra memory required is about 30%. Overall, we consider the extra memory consumption to be well \nwithin reason for many applications, considering the gains in speed.  4.3 Programming Techniques Just \nlike it is dif.cult to write parallel programs that per\u00adform well, we found that effective use of self-adjustment \nre\u00adquires some thought on behalf of the programmer. By study\u00ading the performance of our benchmarks, we \nwere able to identify speci.c programming techniques that enable self\u00adadjustment to perform better in \npractice. Revision Granularity. Since the creation and synchroniza\u00adtion of tasks incurs a non-negligible \noverhead, parallel per\u00adformance suffers if the task granularity is too .ne. We ad\u00addressed this issue \nby using parameters that control the num\u00adber of revisions directly, or indirectly, e.g. by setting some \nthreshold for recursive divide-and-conquer (such as shown in Fig. 3). Varying these parameters can affect \nexecution times dramatically; we show this effect in Fig. 14. As can be seen, the speedups are best when \nusing relatively few (less than 500) revisions only. This is true even for the repeat phase, where we \nmay expect smaller granularities to pay off. Simple Outputs. We found that for some locations, we can \navoid versioning and improve performance (due to less indirection and copying). We call a location l \na simple output if l is created before the compute/mutate loop.  l is never read within compute(), \nand never written within mutate().  l is race-free, i.e. not concurrently written to.  Simple outputs \ndo not require versioning because (1) no computation in Compute depends on them, and (2) their value \npersists across iterations. Markers. We found that we can improve performance con\u00adsiderably by tracking \nan entire group of locations as a whole, rather than each individual location separately. The idea is \nthat The library provides a special marker class, with methods MarkRead() and MarkWritten().  For each \ngroup of locations that the client code would like to track, it creates a single marker object.  The \nclient code ensures that whenever it reads from or writes to a location in the group, it calls MarkRead() \nor MarkWritten(), respectively.  The client code ensures that all locations in the tracked group are \nrace-free (i.e. they may not be concurrently accessed by two revisions, with at least one access being \na write) and feedback-free (i.e. they may not be both an input to and an output of compute()).  If properly \nused, markers ensure suf.cient invalidation of revisions to maintain the correct semantics of self-adjusting \ncomputation. We used markers in the layout algorithm, to track entire subtrees of the box trees as a \nsingle entity. Clearly, it is desirable to reduce the reliance on such man\u00adual techniques as much as \npossible. We hope to automate more of this process in the future. For example, it is conceiv\u00adable that \nlessons learnt in the context of parallelization, such as automated parameter tuning [8], can apply similarly \nfor self-adjustment. Even so, we feel that the study of manual techniques is an important .rst step that \ncan not be skipped. 5. Related Work Our work spans the two research areas of (1) deterministic parallel \nprogramming, and (2) self-adjusting computation, and contributes to both. To the former, it adds the \ncapability of incremental execution. To the latter, it adds parallelism, but also a shift in perspective \nthat puts more control into the hand of the programmer and less burden on the compiler. It thus re.ects \na similar shift of perspective that has driven the parallel programming community towards parallel program\u00adming \nmodels and away from parallelizing compilers. Fig. 15 clari.es how to position our model with respect \nto previous work: the research presented in this paper spans the two left quadrants. We now give some \nmore detailed background on these research areas. 5.1 Incremental and Self-Adjusting Computation An \nincremental algorithm ef.ciently computes its output with respect to the previous output and the changes \nin input. Compared to conventional (static) algorithms, an incremen\u00adtal algorithm has the potential for \nasymptotic improvements in speed for certain classes of changes to input. Tradition\u00adally, incremental \ncomputation has been explored in the con\u00adtext of functional languages. The primary techniques used to \nachieve incremental computation are dependence graphs, memoization, and partial evaluation [5]. Dependence \ngraphs track what computations must be updated upon changes to input [18, 26, 37]. Memoization (caching \nthe result of func\u00adtion calls) [1, 6, 25, 29, 33] and partial evaluation (specializ\u00ading a function with \nrespect to some .xed input) [20, 36] can be used to achieve or enhance incremental computation. A comprehensive \nbibliography on incremental computation is provided by Ramalingam and Reps [34]. Incremental com\u00adputation \nis increasingly important as the size of data sets in\u00adcreases and the computation over that data becomes \nmore in\u00advolved. For example, Guo et al. implemented memoization and dependence tracking in a Python interpreter \nto help sci\u00adentists do faster prototyping of compute-intensive data pro\u00adcessing scripts [23].  Computation \nDeclared by Inferred by Structure programmer compiler Deterministic Parallel Parallel Programming Models \nParallelizing Compilers Programming -Cilk -Fortran -DPJ -SUIF -NESL -Polaris -Concurrent Revisions -Paradigm \nSelf-Adjusting Self-Adjusting Self-Adjusting Computation Programming Models Compilers -This paper -Delta \nML -Early CEAL versions Figure 15. Categorization of Related Work.  Our work builds on self-adjusting \ncomputation, a method for automatically obtaining an incremental algorithm from a batch algorithm [3]. \nThe bene.t of self-adjusting computa\u00adtion comes from not having to design an incremental algo\u00adrithm, \nwhich is known to be quite tricky for even relatively simple problems. In the classical approach, data \ndependen\u00adcies are tracked with modi.able references, and change propagation is ef.ciently implemented \nwith dynamic de\u00adpendence graphs and memoization. An early self-adjusting library, written in ML [2], \nwas also reimplemented in a monadic style in Haskell [17]. In recent work, dependence tracking has been \naddressed at the data structure level, in\u00adstead of individual memory cells, enabling further perfor\u00admance \nimprovements [4].  5.2 Parallel Self-Adjusting Computation Several recent papers have proposed parallel \nself-adjusting algorithms for speci.c problems [7, 11, 12, 35]. Unlike our work presented here, they \ndo not investigate how to provide a general programming model. We are aware of only one paper addressing \nthis same question [24]. In that work, the authors consider a tiny language that in\u00adcludes a letpar primitive \nfor expressing the parallel evalua\u00adtion of multiple expressions. They impose the restriction that locations \nbe written at most once and are never read before written. While this restriction does indeed enforce \ndetermin\u00adism, it is more akin to data.ow models than to standard im\u00adperative parallel programming models \n(we discuss those in Section 5.3 below). The programming model we consider, concurrent revisions, is \nstrictly more expressive, allowing multiple (and even concurrent) writes to shared locations. Our work \nshares the idea from the Hammer et al. pro\u00adposal [24] of tracking the sequential and parallel control\u00ad.ow \nof a computation as a tree. In their work, they track dependences from writes to reads so as to later \ndetermine which reads are affected (read from a location whose con\u00adtents have changed) by a change propagation \nalgorithm. Our invalidation and re-execution approach over the tree of re\u00adgions is similar to that of \nHammer et al. However, our ap\u00adproach memoizes computations at the level of the concurrent revision (task), \nwhile the approach of Hammer et al. mem\u00adoizes at the level of individual reads of locations. Thus, our \napproach can be much more coarse-grained and under the control of programmer (based on the programmer \ns choice of parallel decomposition using concurrent revisions). The semantics of classical self-adjusting \ncomputation dif\u00adfers slightly from our loop characterization in cases where outputs of compute are overwritten \nby mutate, or where compute exhibits feedback (i.e. outputs are also inputs). For more detail, see Section \nA.2 in the appendix. We believe the loop characterization is more easily understood by program\u00admers, \nand lends itself better to apply to existing compute\u00admutate loops.  5.3 Models for Deterministic Concurrency \nand Parallelism Concurrent revisions [14] are a good .t for self-adjusting computation due to the determinism \nand well-de.ned se\u00admantics [15]. In recent years, a variety of related program\u00adming models for deterministic \nconcurrency have been pro\u00adposed that restrict tasks from producing con.icts. Some of these models leverage \nhardware support [9, 19], but do not guarantee isolation. Others support a restricted fork-join concurrency \nmodel [10, 13], or involve the implementation of ef.cient deterministic scheduling [32]. We believe that \nour approach to integrating self-adjusting computations with parallel programming could be realized in \nmany of these frameworks. Another interesting connection between par\u00adallel execution and self-adjusting \ncomputation is that both appear to bene.t from raising the abstraction level of data types [4, 27, 28]. \n6. Conclusion and Future Work We have shown that a single, small set of primitives can enable programmers \nto write applications that can both (1) exploit parallelism, and (2) react incrementally to changes. \nWe have presented the .rst known algorithm to perform such parallel self-adjusting computation. We have \nexperimentally evaluated this idea by applying it to .ve example programs, and observe performance gains \nthat are well beyond what can be achieved by parallelization alone.  Many questions remain to be answered \nby future work. Our current library implementation fully trusts the pro\u00adgrammer to eliminate data races \nand invisible dependen\u00adcies. More stringent checking may prove useful to .nd bugs, using a static approach \nas in DPJ [13] or a dynamic approach as in precise data race detection [21].  Given the numerous programming \ntechniques known for parallel programming, we believe the three we have pre\u00adsented for incremental programming \nbarely scratch the surface. Knowing more about these techniques, and .nd\u00ading ways to apply them automatically, \nmay help to fur\u00adther promote self-adjusting computation as an alternative to (or even better, an extension \nof) parallel computation.  Similar techniques that we used to implement self\u00adadjustment may also work \nto support speculative par\u00adallelism and an embedding of optimistic concurrency in the concurrent revisions \nmodel.  We are considering user studies to evaluate our program\u00adming model and provide further evidence \nof its useful\u00adness.  Acknowledgments We thank Dan Grossman and Manuel F\u00a8ahndrich for helpful comments \nand discussions. References [1] M. Abadi, B. W. Lampson, and J.-J. L\u00b4evy. Analysis and caching of dependencies. \nIn International Conference on Functional Programming (ICFP), 1996. [2] U. Acar, G. Blelloch, M. Blume, \nR. Harper, and K. Tang\u00adwongsan. A library for self-adjusting computation. Electronic Notes in Theoretical \nComputer Science, 148:127 154, 2006. [3] U. A. Acar. Self-adjusting computation (an overview). In Workshop \non Partial Evaluation and Program Manipulation (PEPM), 2009. [4] U. A. Acar, G. Blelloch, R. Ley-Wild, \nK. Tangwongsan, and D. T\u00a8urko.glu. Traceable data types for self-adjusting computa\u00adtion. In Programming \nLanguage Design and Implementation (PLDI), 2010. [5] U. A. Acar, G. E. Blelloch, M. Blume, R. Harper, \nand K. Tang\u00adwongsan. An experimental analysis of self-adjusting compu\u00adtation. Transactions on Programming \nLanguages and Systems (TOPLAS), 32:3:1 3:53, November 2009. [6] U. A. Acar, G. E. Blelloch, and R. Harper. \nSelective mem\u00adoization. In Principles of Programming Languages (POPL), 2003. [7] U. A. Acar, A. Cotter, \nB. Hudson, and D. T\u00a8urko.glu. Paral\u00adlelism in dynamic well-spaced point sets. In Proceedings of the 23rd \nACM Symposium on Parallelism in Algorithms and Architectures, 2011. Symposium on Parallelism in Algorithms \nand Architectures. [8] J. Ansel, C. Chan, Y. Wong, M. Olszewski, Q. Zhao, A. Edel\u00adman, and S. Amarasinghe. \nPetaBricks: A language and com\u00adpiler for algorithmic choice. In Programming Language De\u00adsign and Implementation \n(PLDI), 2009. [9] T. Bergan, O. Anderson, J. Devietti, L. Ceze, and D. Gross\u00adman. CoreDet: A compiler \nand runtime system for determin\u00adistic multithreaded execution. In Architectural Support for Programming \nLanguages and Operating Systems (ASPLOS), 2010. [10] E. Berger, T. Yang, T. Liu, and G. Novark. Grace: \nSafe mul\u00adtithreaded programming for C/C++. In Object-Oriented Pro\u00adgramming, Systems, Languages, and Applications \n(OOPSLA), 2009. [11] P. Bhatotia, A. Wieder, I. E. Akkus, R. Rodrigues, and U. A. Acar. Large-scale incremental \ndata processing with change propagation. In USENIX Workshop on Hot Topics in Cloud Computing (HotCloud), \n2011. [12] P. Bhatotia, A. Wieder, R. Rodrigues, U. A. Acar, and R. Pasquini. Incoop: Mapreduce for incremental \ncomputa\u00adtions. In ACM Symposium on Cloud Computing, 2011. [13] R. Bocchino, V. Adve, D. Dig., S. Adve, \nS. Heumann, R. Ko\u00admuravelli, J. Overbey, P. Simmons, H. Sung, and M. Vakilian. A type and effect system \nfor Deterministic Parallel Java. In Object-Oriented Programming, Systems, Languages, and Ap\u00adplications \n(OOPSLA), 2009. [14] S. Burckhardt, A. Baldassin, and D. Leijen. Concurrent pro\u00adgramming with revisions \nand isolation types. In Object-Oriented Programming, Systems, Languages, and Applica\u00adtions (OOPSLA), \n2010. [15] S. Burckhardt and D. Leijen. Semantics of concurrent revi\u00adsions (full version). Technical \nReport MSR-TR-2010-94, Mi\u00adcrosoft, 2010. [16] S. Burckhardt and D. Leijen. Semantics of concurrent re\u00advisions. \nIn European Symposium on Programming (ESOP), 2011. [17] M. Carlsson. Monads for incremental computing. \nIn Interna\u00adtional Conference on Functional Programming (ICFP), 2002. [18] A. Demers, T. Reps, and T. \nTeitelbaum. Incremental evalua\u00adtion for attribute grammars with application to syntax-directed editors. \nIn Principles of Programming Languages (POPL), 1981. [19] J. Devietti, B. Lucia, L. Ceze, and M. Oskin. \nDMP: Determin\u00adistic shared-memory multiprocessing. Micro, IEEE, 30(1):40 49, 2010. [20] J. Field and \nT. Teitelbaum. Incremental reduction in the lambda calculus. In Conference on LISP and Functional Programming, \n1990. [21] C. Flanagan and S. Freund. Ef.cient and precise dynamic race detection. In Programming Language \nDesign and Impl. (PLDI), 2009. [22] M. Frigo, P. Halpern, C. E. Leiserson, and S. Lewin-Berlin. Reducers \nand other Cilk++ hyperobjects. In Symposium on Parallel Algorithms and Architectures (SPAA), 2009. [23] \nP. J. Guo and D. Engler. Towards practical incremental re\u00adcomputation for scientists: An implementation \nfor the Python  language. In Workshop on the Theory and Practice of Prove\u00adnance (TAPP), 2010. [24] M. \nHammer, U. A. Acar, M. Rajagopalan, and A. Ghuloum. A proposal for parallel self-adjusting computation. \nIn Workshop on Declarative Aspects of Multicore Programming (DAMP), 2007. [25] A. Heydon, R. Levin, and \nY. Yu. Caching function calls using precise dependencies. In Programming Language Design and Implementation \n(PLDI), 2000. [26] R. Hoover. Incremental graph evaluation. PhD thesis, Cornell University, 1987. [27] \nE. Koskinen, M. Parkinson, and M. Herlihy. Coarse-grained transactions. In Principles of Programming \nLanguages (POPL), 2010. [28] M. Kulkarni, K. Pingali, B. Walter, G. Ramanarayanan, K. Bala, and L. Chew. \nOptimistic parallelism requires abstrac\u00adtions. In Programming Language Design and Implementation (PLDI), \n2007. [29] Y. A. Liu, S. D. Stoller, and T. Teitelbaum. Static caching for incremental computation. Transactions \non Programming Languages and Systems (TOPLAS), 20:546 585, 1998. [30] Microsoft. Parallel programming \nsamples, .NET framework 4. http://code.msdn.microsoft.com/ParExtSamples, May 2010. [31] G. Navarro. A \nguided tour to approximate string matching. ACM Computing Surveys, 33:31 88, 2001. [32] M. Olszewski, \nJ. Ansel, and S. Amarasinghe. Kendo: ef.cient deterministic multithreading in software. SIGPLAN Notices, \n44:97 108, 2009. [33] W. Pugh and T. Teitelbaum. Incremental computation via function caching. In Principles \nof Programming Languages (POPL), 1989. [34] G. Ramalingam and T. Reps. A categorized bibliography on \nincremental computation. In Principles of Programming Languages (POPL), 1993. [35] O. Sumer, U. A. Acar, \nA. Ihler, and R. Mettu. Fast parallel and adaptive updates for dual-decomposition solvers. In Con\u00adference \non Arti.cial Intelligence (AAAI), 2011. [36] R. S. Sundaresh and P. Hudak. Incremental computation via \npartial evaluation. In Principles of Programming Languages (POPL), 1991. [37] D. M. Yellin and R. E. \nStrom. INC: a language for incremental computations. In Transactions on Programming Languages and Systems \n(TOPLAS), volume 13, pages 211 236, 1991. A. Appendix: Additional Material In this appendix we include \nsome additional material that discusses potentially interesting details that are however not crucial \nto the main contributions of the paper.    A.1 Correctness The algorithm as presented is quite intricate \nand it would be useful to have correctness proof. Clearly, a full formalization of the algorithm and \nexecution semantics is beyond the scope of this paper. However, we can give a precise correctness argument \nfor our algorithm where we focus on the essential details. Our notion of correctness centers around the \nloop characterization described in the introduction. In particular, we leverage the key assumptions that \nthe code executed by a summary is deterministic and that all threads are joined before the end of the \noutermost summary. What we would like to show, is that after the initial call to record(code), calling \nrecord(code) subsequent times should have the same effect as repeat. This correctness argument relies \non two key lemmas. We say that a summary is replayed if we do not have to re\u00adexecute any of the code \nfor that summary. Otherwise, it is re-executed . Note that summaries may be replayed in either the repeat \nor fork methods. We say that two summaries are deeply equal if they have the same .elds and the same \ntree structure. With these de.nitions, we can state the key lemmas: LEMMA 1 (Record is neutral). If we \ninitialize isValid to false for every summary (so that every summary is re\u00adexecuted), the behaviour is \nthe same as if we just execute all the code associated with each summary. In other words, recording does \nnot effect the computation. LEMMA 2 (Replay is sound). If isValid for a summary is set to true at the \njoin point for that summary, replay or re-execution produce summaries which are deeply equal. In other \nwords, in Figure 9, af\u00adter executing candidate.parent := current, then candidate is deeply equal to: \ncandidate := new Summary(current, code) ; candidate.Start(prev) ;. The .rst lemma is easy to validate \nby checking that each repeat and fork re-executes all code in each case and rebuilds each summary. The \ncorrectness argument for the second key lemma is more intricate though. We argue its correctness as follows: \n Assume there is an execution where the re-executed sum\u00admary would produce a different value than the \nreplayed one. So, there is an execution where the valid bit was set but re-execution got a different \nvalue.  The only way the program can diverge from a previous execution is if a read returns a different \nvalue. So we need to show that if isValid is true, than all the reads are in fact returning the same \nvalues.  Assume that there is a read which returns a different value. Consider the .rst such divergent \nread in in the partial order of segments enclosed by summaries. This partial order is with respect to \nforks and joins (a segment calling fork is before the fork, segment calling join is after the segment \nbeing joined). We perform a case analysis based on where the write initially came from:  If the write \ncame from the same summary, then it could not have diverged by our determinism assump\u00adtion; all prior \nwrites are deterministic. In the case where the value was written by a join, we leverage the fact that \nthis is the .rst divergent read in the partial or\u00adder. The summary being joined must not contain any \ndivergent reads, and so, by induction, has determinis\u00adtic behaviour.  If the write came from a previous \nsummary, the cur\u00adrent summary must have been created with a call to fork. However, the call to DoInternalInvalidations \nin\u00adside of fork would have invalidated the current sum\u00admary if either the write was from the same segment \nwith a different value, from a different segment, or did not occur. If the write came from outside the \nsummary tree, then it has either been replaced with a more recent write inside the summary tree or else \na different value was written outside of the summary tree. In the .rst case, by the determinism assumption, \nthe write must have come from a previous summary. However, DoInternalInvalidations would invalidate the \ncurrent summary if a new write occurs. For the sec\u00adond case, the only way to reenter a summary tree after \nexiting is via the repeat method. In this case, DoExternalInvalidations would have set the isValid bit \nfor the current summary if a different value had been written outside. D  A.2 Compute-Mutate Loop Semantics \nOur loop characterization can help to answer subtle semantic questions about self-adjusting computation \nin an imperative setting. It is not necessarily exactly equivalent to traditional notions of self-adjusting \ncomputation, in situations where either (1) outputs of the computation are overwritten by mutate, or \n(2) there is feedback (locations that are both inputs and outputs of the computation). Speci.cally, our \nloop characterization implies that in both of the following programs, the assertions do not fail: intx=0; \nintx=0; record { x := 1; } record { x := x + 1; } x = 2; repeat ; repeat ; repeat ; assert(x = 1) ; assert(x \n= 3) ; Not all interpretations of self-adjusting computation in prior work behave in this way (the assertions \nmay fail). There is often an implicit assumption that the programmer is responsible for identifying inputs \nand outputs, and must manually copy outputs back to inputs when feedback is desired. A.3 Extending the \nImplementation for User-De.ned Merges In Section 3 we simpli.ed discussion by eliding the case with user-de.ned \nmerge functions. In this case, the merge function must be run when the summary is replaying or reexecuting. \nHowever, the MergeWrites function as presented does not have a way to calculate the three values needed \nto run a merge function. Figure 16 details an alternate version of MergeWrites which handles user de.ned \nmerges. In this case, MergeWrites takes as input the main sum\u00admary which we are merging writes into and \nalso the joinee summary we are merging writes from. Note that this version of MergeWrites is suitable \nfor use in the join function; in the case of record and repeat, we extend this function to handle the \nlack of a current summary. We need to work backward through the writes from the joined summary, only \ncalling the merge function for the last write. To do this, we keep track of which locations we have already \nmerged in the mergedLocations set. In order to .nd the original and current values for the location loc, \nwe use the special methods GetValue and GetParentValue. GetValue cycles back through the summary parent \nchain to .nd the .rst location in which loc was written. This method is very similar to the read method, \nbut does not add dependencies. The GetParentValue method starts immediately by looking for the value \nin the parent chain. It turns out that as a conse\u00adquence of the concurrent revision model where revision \ndia\u00adgrams always from a semi-lattice, the original ancestor value of the main and joinee is always the \ndirect parent value of the joinee [16].  A.4 Pseudocode vs. Actual Implementation We now discuss some \nof the differences between the pre\u00adsented pseudocode algorithm and our actual implementation in more \ndetail. Our pseudocode uses List(Map(Versioned,Value)) to store write sets. In the actual implementation, \nwe use segment trees [14] (trees that have immutable nodes and mutable leaves) to encode the structure \nof the revision diagram. Each segment has a unique id. We then store inside each Versioned) object a \nmap from int to Value that stores the var\u00adious versions. This map is encoded as an array of arrays of \nkey-value pairs, for the sake of minimizing the synchroniza\u00adtion requirements under concurrent access. \nAs described in [14], Segment objects (redundantly) maintain a list of versioned objects that contain \nversions for them; this allows us to remove those versions when deal\u00adlocating segments. We deallocate \nsegments when their ref\u00aderence count reaches zero. To store the list of writes in a Summary, we keep \na list List(Segment) of segments, with an extra .rst entry repre\u00adsenting the root from which the revision \nwas forked (if we have n segments, this list has thus size n +1). We can then compute the write lists \nas needed by the algorithm by walk\u00ad  private void MergeWrites(Summary joinee, Summary main) {Set(Versioned) \nmergedLocations = new Set(Versioned)() ; Map(Versioned, Value) target = current.writes.Last() ; for (int \ni := joinee.writes.Count-1; i ? 0 ; i--) foreach ((loc,value) in joinee.writes[i]) if (!mergedLocations.Contains(loc)) \n// only merge last write { mergedLocations.Add(loc) ; Value mainValue := GetValue(main, loc) ; Value \norigValue := GetParentValue(joinee, loc) ; if (origValue = mainValue) main[loc] := value ; // no con.ict \nelse main[loc] := loc.merge(mainValue, value, origValue) ; }} private Value GetValue(Summary s, Versioned \nloc){if (FindIndex(s.writes, loc) = -1) return LastWrite(s.writes, loc) ; else return GetParentValue(s,loc) \n; } private Value GetParentValue(Summary s, Versioned loc) {while (s.parent = null) {for (i := s.index \n; i ? 0 ; i--) if (s.parent.writes[i].ContainsKey(loc)) {return s.parent.writes[i][loc] ; } s := s.parent \n; } return LastWrite(globalwrites, loc) ; } Figure 16. Merging writes with merge functions. ing through \nthe segment chain. Note that it is possible that several segments in this list are the same; this helps \nus to avoid excessive creation of empty write lists, which turns out to have a big performance impact \n(since the searching for writes tends to be expensive, and depends heavily on how many segments need \nto be traversed). Finally, note that the foreach loop in DoExternalInvalidations, namely: foreach((s, \n(d, i)) where s.dependencies[loc] = (d, i)) is potentially expensive since it visits all summaries in \nor\u00adder .nd all dependencies. To keep the cost under control, we store the dependencies (which we presented \nas a .eld Map(Versioned,(int x int)) inside summaries in the pseu\u00addocode) in the versioned object, as \na Map(Summary,(int x int)). This helps to iterate the loop above. Similarly to the way we back-reference \nversioned objects that contain ids of Segment objects inside a list in the Seg\u00adment objects, we redundantly \nstore a list of Versioned objects in the summary objects, containing all the Versioned objects that contain \nthat summary as a key in their dependency map. Again this helps to remove those dependency entries when \na summary is deallocated.  \n\t\t\t", "proc_id": "2048066", "abstract": "<p>Parallel or incremental versions of an algorithm can significantly outperform their counterparts, but are often difficult to develop. Programming models that provide appropriate abstractions to decompose data and tasks can simplify parallelization. We show in this work that the same abstractions can enable both parallel and incremental execution. We present a novel algorithm for parallel self-adjusting computation. This algorithm extends a deterministic parallel programming model (concurrent revisions) with support for recording and repeating computations. On record, we construct a dynamic dependence graph of the parallel computation. On repeat, we reexecute only parts whose dependencies have changed.</p> <p>We implement and evaluate our idea by studying five example programs, including a realistic multi-pass CSS layout algorithm. We describe programming techniques that proved particularly useful to improve the performance of self-adjustment in practice. Our final results show significant speedups on all examples (up to 37x on an 8-core machine). These speedups are well beyond what can be achieved by parallelization alone, while requiring a comparable effort by the programmer.</p>", "authors": [{"name": "Sebastian Burckhardt", "author_profile_id": "81350574118", "affiliation": "Microsoft Research, Redmond, WA, USA", "person_id": "P2839207", "email_address": "sburckha@microsoft.com", "orcid_id": ""}, {"name": "Daan Leijen", "author_profile_id": "81100572466", "affiliation": "Microsoft Research, Redmond, WA, USA", "person_id": "P2839208", "email_address": "daan@microsoft.com", "orcid_id": ""}, {"name": "Caitlin Sadowski", "author_profile_id": "81418599954", "affiliation": "University of California at Santa Cruz, Santa Cruz, CA, USA", "person_id": "P2839209", "email_address": "supertri@cs.ucsc.edu", "orcid_id": ""}, {"name": "Jaeheon Yi", "author_profile_id": "81351599511", "affiliation": "University of California at Santa Cruz, Santa Cruz, CA, USA", "person_id": "P2839210", "email_address": "jaeheon@cs.ucsc.edu", "orcid_id": ""}, {"name": "Thomas Ball", "author_profile_id": "81100472343", "affiliation": "Microsoft Research, Redmond, WA, USA", "person_id": "P2839211", "email_address": "tball@microsoft.com", "orcid_id": ""}], "doi_number": "10.1145/2048066.2048101", "year": "2011", "article_id": "2048101", "conference": "OOPSLA", "title": "Two for the price of one: a model for parallel and incremental computation", "url": "http://dl.acm.org/citation.cfm?id=2048101"}