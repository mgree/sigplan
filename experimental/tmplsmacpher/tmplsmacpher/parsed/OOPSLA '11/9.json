{"article_publication_date": "10-22-2011", "fulltext": "\n Integrated Language De.nition Testing Enabling Test-Driven Language Development Lennart C. L. Kats Rob \nVermaas Eelco Visser Delft University of Technology LogicBlox Delft University of Technology l.c.l.kats@tudelft.nl \nrob.vermaas@logicblox.com visser@acm.org Abstract The reliability of compilers, interpreters, and development \nenvironments for programming languages is essential for ef\u00adfective software development and maintenance. \nThey are of\u00adten tested only as an afterthought. Languages with a smaller scope, such as domain-speci.c \nlanguages, often remain untested. General-purpose testing techniques and test case generation methods \nfall short in providing a low-threshold solution for test-driven language development. In this paper \nwe introduce the notion of a language-parametric testing language (LPTL) that provides a reusable, generic \nbasis for declaratively specifying language de.nition tests. We inte\u00adgrate the syntax, semantics, and \neditor services of a language under test into the LPTL for writing test inputs. This paper describes \nthe design of an LPTL and the tool support pro\u00advided for it, shows use cases using examples, and describes \nour implementation in the form of the Spoofax testing lan\u00adguage. Categories and Subject Descriptors D.2.5 \n[Software En\u00adgineering]: Testing and Debugging Testing Tools; D.2.3 [Software Engineering]: Coding Tools \nand Techniques; D.2.6 [Software Engineering]: Interactive Environments General Terms Languages, Reliability \nKeywords Testing, Test-Driven Development, Lan\u00adguage Engineering, Grammarware, Language Workbench, Domain-Speci.c \nLanguage, Language Embedding, Com\u00adpilers, Parsers Permission to make digital or hard copies of all or \npart of this work for personal or classroom use is granted without fee provided that copies are not made \nor distributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. OOPSLA 11, October 22 27, 2011, Portland, Oregon, USA. Copyright \nc &#38;#169; 2011 ACM 978-1-4503-0940-0/11/10. . . $10.00 1. Introduction Software languages provide \nlinguistic abstractions for a do\u00admain of computation. Tool support provided by compil\u00aders, interpreters, \nand integrated development environments (IDEs), allows developers to reason at a certain level of abstraction, \nreducing the accidental complexity involved in software development (e.g., machine-speci.c calling conventions \nand explicit memory management). Domain\u00adspeci.c languages (DSLs) further increase expressivity by restricting \nthe scope to a particular application domain. They increase developer productivity by providing domain\u00adspeci.c \nnotation, analysis, veri.cation, and optimization. With their key role in software development, the correct \nimplementation of languages is fundamental to the reliability of software developed with a language. \nErrors in compilers, interpreters, and IDEs for a language can lead to incorrect execution of correct \nprograms, error messages about correct programs, or a lack of error messages for incorrect programs. \nErroneous or incomplete language implementations can also hinder understanding and maintenance of software. \nTesting is one of the most important tools for software quality control and inspires con.dence in software \n[1]. Tests can be used as a basis for an agile, iterative development pro\u00adcess by applying test-driven \ndevelopment (TDD) [1], they unambiguously communicate requirements, and they avoid regressions that may \noccur when new features are introduced or as an application is refactored [2, 31]. Scripts for automated \ntesting and general-purpose testing tools such as the xUnit family of frameworks [19] have been successfully \napplied to implementations of general-purpose languages [16, 38] and DSLs [18, 33]. With the successes \nand challenges of creating such test suites by hand, there has been considerable research into automatic \ngeneration of compiler test suites [3, 27]. These techniques provide an effective solution for thorough \nblack-box testing of complete compilers, by using annotated grammars to generate input programs. Despite \nextensive practical and research experience in testing and test generation for languages, rather less \natten\u00adtion has been paid to supporting language engineers in writ\u00ading tests, and to applying TDD with \ntools speci.c to the do\u00admain of language engineering. General-purpose testing tech\u00adniques, as supported \nwith xUnit and testing scripts, require signi.cant investment in infrastructure to cover test cases re\u00adlated \nto syntax, static semantics, and editor services, spe\u00adci.c for the tested language. They also use isolated \ntest pro\u00adgrams to test particular language aspects, requiring a certain amount of boilerplate code with \neach test program (e.g., im\u00adport headers), and require users to manually code how to execute the test \n(parse/compile/run/etc.) and how to evaluate the result and compare it to an expectation. Tool support \nfor writing test cases and specifying test conditions is lacking, particularly for negative test cases \nwhere errors are expected. Test generation techniques are an effective complementary technique for stress \ntesting complete compiler implementa\u00adtions, but are less effective during the development of a new language \nde.nition. In this paper, we present a novel approach to language de.nition testing by introducing the \nnotion of a language\u00adparametric testing language (LPTL). This language provides a reusable, generic basis \nfor declaratively specifying lan\u00adguage de.nition tests. It can be instantiated for a speci.c language \nunder test through language embedding: we inte\u00adgrate the syntax, semantics, and editor services of a \nlanguage under test into the LPTL for writing test inputs. For the generic basis of the LPTL, we provide \ngeneral constructs to con.gure test modules and to declaratively specify test conditions. Based on the \nobservable behavior of languages implementations, we selected an open-ended set of test condition speci.cation \nconstructs. These form the heart of the testing language, and support writing tests for language syntax, \nstatic semantics, editor services, generated code, and dynamic semantics. To support language engineers \nin writing and understand\u00ading tests, we show how full language-speci.c IDE support can be provided for \nwriting test cases. The instantiated LPTL provides editor services such as syntax highlighting, syntax \nand semantic error marking, and content completion, based on the de.nition of the language under test. \nUsing an LPTL signi.cantly reduces the threshold for language testing, which is important because such \na thresh\u00adold is often a reason for developers to forgo testing [15]. First, by providing a reusable infrastructure \nfor language test speci.cations that facilitates test execution, analysis, maintenance, and understanding. \nSecond, by providing full language-speci.c IDE support for writing tests. The contributions of this paper \nare as follows. The design of a generic, declarative test speci.cation language for language de.nition \ntesting.  A fully language-agnostic approach to language embed\u00adding that incorporates syntactic, semantic, \nand editor ser\u00advice aspects of a language under test.  The implementation of such a testing language \nas the Spoofax testing language1 and a description of its imple\u00admentation architecture.  Outline We \nbegin this paper with background on lan\u00adguage de.nitions. Next, we discuss the design of a language\u00adparametric \ntesting language from three angles: .rst from a purely linguistic perspective in Section 3, then from \na tool support perspective in Section 4, and .nally by illustrating use cases with examples in Section \n5. Our implementation architecture is described in Section 6. We conclude with re\u00ad lated work on language \ntesting approaches and directions for future work. 2. Background: Language De.nitions The development \nof a compiler for a DSL for a domain com\u00adprises many tasks, ranging from construction of a parser to \na semantic analyzer and code generator. In addition to a com\u00adpiler, the construction of an integrated \ndevelopment environ\u00adment (IDE) is essential, as developers increasingly rely on IDEs to be productive. \nTraditionally, a lot of effort was re\u00adquired for each of these tasks. Parsers, data structures for abstract \nsyntax trees, traversals, transformations, and so on would be coded by hand for each language. The implemen\u00adtation \nof editor services expected from modern IDEs, such as syntax highlighting, an outline view, reference \nresolving for code navigation, content completion, and refactoring, added to this already heavy burden. \nThis meant that a signi.cant investment in time and effort was required for the develop\u00adment of a new \nlanguage. Language engineering tools Nowadays there are many tools that support the various aspects of \nlanguage engineer\u00ading, allowing language engineers to write high-level lan\u00adguage de.nitions rather than \nhandwrite every compiler, in\u00adterpreter and IDE component. Particularly successful are parser generators, \nwhich can generate ef.cient parsers from declarative syntax de.nitions. For semantic aspects of lan\u00adguages, \nthere are numerous meta-programming languages and frameworks. For the development of IDE support there \nare also various tools and frameworks that signi.cantly de\u00adcrease the implementation effort. Language \nworkbenches are a new breed of language de\u00advelopment tools [13] that integrate tools for most aspects \nof language engineering into a single environment. Language workbenches make the development of new languages \nand their IDEs much more ef.cient, by a) providing full IDE support for language development tasks and \nb) integrating the development of the language compiler/interpreter and its IDE. Examples of language \nworkbenches include MPS [37], MontiCore [28], Xtext [11], and our own Spoofax [25]. Language workbenches \nallow for an agile development model, as they allow developers to use an IDE and to play with the language \nwhile it is still under development. Fig\u00adure 1 shows a screenshot illustrating how they can combine the \ndevelopment of a language with the use of generated ed\u00ad 1Distributed as part of the Spoofax language \nworkbench [25], available with nightly builds of version 0.6.1 via http://www.spoofax.org/.  Figure \n1. Language workbenches can combine language de.nition (left) and language use (right). itors for that \nlanguage. Once a syntax de.nition has been de\u00adveloped (at the left), they can generate an editor with \nbasic editor services such as syntax highlighting and syntax error marking. From there, language engineers \ncan incrementally and iteratively implement new language components and ed\u00aditor services. Examples and \ntests Many new languages start with sketches. Sketches of example programs or code snippets that solve \na particular problem in the domain of the lan\u00adguage. With language workbenches, it is common practice \nto maintain a scratch pad with some example program that focuses on new features that are under development. \nLan\u00adguage engineers can interact with it in various ways. For ex\u00adample, they can introduce type errors \n( does the type checker catch this? ), control-click on an identi.er ( does this hyper\u00adlink point to \nthe right place? ) or generate and run code for the example. Example programs quickly grow unwieldy, \nand, as com\u00admonly seen with interactive systems [31], they are often thrown away once they show satisfactory \nresults. This is a major problem as these test cases are a valuable investment that simply disappears \nafter testing is completed. The prob\u00adlem results from the high threshold of setting up tests for languages \nand their IDE, running tests in an automated fash\u00adion, ensuring that the observed behavior complies with \nthe expectations, and so on. The effort does not compare to the ease of testing it interactively. Without \nbetter tool support, proper testing remains an afterthought, even in an integrated language engineering \nenvironment such as a language work\u00adbench. 3. Test Speci.cation Language Design In this section we describe \nthe design of a language\u00adparametric testing language and show how it can be used to test different aspects \nof language de.nitions. The design of the language is highly intertwined with the tool support that is \nprovided for it and how users can interact with it. We discuss those aspects of the language in the next \nsection. The central goal set out for design of an LPTL is to pro\u00advide a low-threshold test speci.cation \nlanguage that forms the basis for a reusable infrastructure for testing different languages. The design \nprinciples of this language are as fol\u00adlows: P1 Provide a language-agnostic framework. The language \nshould provide a generic, language-agnostic basis that caters for a wide spectrum of different types \nof tests. P2 Maintain implementation independence. The language should emphasize black-box testing [31], \nallowing tests to be written early in the design process, and abstracting over implementation speci.cs. \nP3 Support language-speci.c instantiation. It should be possible to instantiate the language for a speci.c \nlan\u00adguage under test, thereby integrating the two languages and the tool support provided for the two. \nP4 Facilitate series of tests with test .xtures. The language should support test .xtures to specify \nseries of tests with common boilerplate code such as import headers. In the remainder of this section \nwe show how these prin\u00adciples can be realized, and show the design of the Spoofax testing language, our \nimplementation of an LPTL. A language-agnostic framework (P1) Key to providing a reusable, language agnostic \nframework is providing a generic language that can quote test fragments and can spec\u00adify conditions to \nvalidate for those tests. We realize this using the following syntax to specify tests: test description \n[[ fragment ]] condition*  where description is a string that describes the current test case in human \nreadable form and fragment is an embedded program or program fragment in the language under test. The \ncondition* elements specify the expectations of the test case, and control what test is performed for \nthe input frag\u00adment. Figure 2 shows an example test case where we test the mobl [20] language, a domain-speci.c \nlanguage for mobile applications. In this example we declare a local variable s of type String and assign \nan integer literal value to it. This is a negative test case: a value of type String would be expected \nhere. The conditions clause of this test case indicates that exactly one error was expected here, which \nmeans that the test case passes. To ensure the test speci.cation syntax is language agnos\u00adtic, it cannot \nhave any speci.c elements for a particular lan\u00adguage. The set of different possible tests that can be \nspeci.ed must be based on a generic interface for observable behavior commonly supported by languages. \nFurthermore, the quo\u00adtation mechanism cannot be limited to only a single, .xed sequence of characters \nsuch as the double square brackets above, since those may also be in use by the language under test. \nIn the Spoofax testing language we address this issue by supporting additional quotation markers such \nas [[[, [[[[, and variations with series of \"\" quotes. test Cannot assign an integer to a string [[ \nmodule Example function f() { var s : String = 1; } ]] 1 error Figure 2. A basic mobl test case. test \nbasic completion [[ module Example function get(argument : String) : String { return [[ arg ]] ; } ]] \ncomplete to \"argument\" Figure 3. A test to verify that the selected identi.er arg completes to argument. \nImplementation independence via black-box testing (P2) Black-box tests [31] test the functionality rather \nthan the in\u00ad ternal workings of a software artifact. They are independent of the internal workings of \na tested artifact or unit and focus only on its observable behavior (output) given some input. The example \nof Figure 2 illustrates this principle, as it re\u00ad veals nor assumes anything about the implementation \nof the language under test. As inputs of a black-box language test we use 1) the quoted fragment of code, \n2) the conditions clause, and 3) se\u00adlections of code within the quoted fragment. The .rst two were illustrated \nin the example of Figure 2. The test input fragment indicates the input to feed to the language imple\u00admentation, \nand the conditions clause indicates what action to trigger and what check to perform. In our example, \nthe 1 error clause indicates that the implementation should perform semantic checking and that only one \nerror is ex\u00adpected. Other clauses can specify other actions and checks such as syntactic checks, name \nresolution, refactoring, and execution. In some cases, they specify user input such as a name for a rename \nrefactoring or command-line arguments for an execution test. We give an overview of these facilities \nat the end of this section. For many forms of tests it is useful to specify some form of selection in \nthe input fragment. For instance, consider Fig\u00adure 3, which shows a content completion test. The double \nbrackets inside the quotation indicate a selected part of the program where content completion would \nbe applied. Selec\u00adtions can indicate identi.ers or larger program fragments for tests of features such \nas reference resolving, content com\u00adpletion, and refactorings. Some tests use multiple selections, in \nparticular for reference resolving, where both a name ref\u00aderence and its declaration can be selected. \nLanguage-speci.c instantiation (P3) Instantiation of the testing language for a speci.c language under \ntest requires that the test suite speci.es which language to use for its test cases. Optionally tests \nsuites can also specify which language mobl setup [[ module Example function f() { var s : String = \n\"\"; [[ ... ]] }  ]] test Cannot assign an integer to a string [[ s = 1; ]] 1 error test Can assign a \nstring to a string [[ s = \"a string\"; ]] Figure 4. A testing module with a shared setup block. syntactic \nstart symbol they use, e.g. a module, statement, or expression. Based on this information, it becomes \npossible to evaluate the test cases by invoking or interpreting the language implementation. To fully \nrealize language-speci.c instantiation, the IDE that supports the testing language can also be adapted \nto incorporate the syntax and semantics of the tested language, as we illustrate in the next section. \nWe organize suites into one or more modules (i.e., .les), where each module has a series of test cases \nand its own con.guration. For each module we use headers that indicate their name, what language to use, \nand what start symbol to use: module test-assignments language Mobl start symbol Expression  Of these \nheaders, only the language header is compulsory. Once the language under test is speci.ed, the LPTL and \nthe language under test are composed together, and quoted test fragments are no longer treated as mere \nstrings but as structured part of test speci.cations. Test .xtures (P4) A common technique in testing \nframe\u00adworks such as the xUnit [19] family of frameworks, is to use setup() and tearDown() methods2 to \ncreate test .x\u00adtures. These methods respectively initialize and de-initialize common state for a series \nof tests. For language testing, the same principle can be applied. We use setup blocks to spec\u00adify common \nelements for tests. Consider again our basic example of Figure 2. Suppose we want to write multiple, \nsimilar tests for assignments in the same context. This would require writing the same boilerplate in \nfor each test case: a module, function, and variable declaration. These commonalities can be factored \nout using setup blocks, as shown in Figure 4. Instead of writing the same boilerplate for every test \ncase, it suf.ces to write it only once using the shared setup block. The contents 2Note that more recent \nimplementations such as JUnit 4 often use annotations for this functionality. Con.guration module name \nlanguage language start symbol symbol Module name name. Use language as the language under test. Use \nsyntactic start symbol symbol. Test cases test description f c* test description expression setup description \nf A test case where f must satisfy conditions c*. A white-box test case where freeform test condition \nexpression must be satis.ed. A setup block for test cases in this module. Can use [[...]]3 in f for placeholders. \nTested fragments (f) [[ ( code | [[code]] )* ]]3 [[[ ( code | [[[code]]] )* ]]] [[[[ ( code | [[[[code]]]] \n)* ]]]] Partial code fragments in the language under test. Alternative fragment quoting style with three \nbrackets. Alternative fragment quoting style with four brackets. Test conditions (c) succeeds Fragment \nsucceeds parsing and has no semantic errors/warnings (default cond.). fails Fragment has semantic errors \nor warnings. parse pattern Fragment parses according to pattern. n error | n errors n warning | n warnings \n/regex/ Fragment has exactly n semantic error(s). Fragment has exactly n semantic warning(s). Fragment \nhas an error or warning matching regular expression regex. resolve ( #n )? Successfully resolves the \nidenti.er at the (nth) selection. resolve #n to #m Resolves the identi.er in the nth selection to a declaration \nat the mth selection. complete ( #n )? to x Content completion proposals for the (nth) selection include \na name x. refactor ( #n )? r ( (arg) )? p Applies refactoring r with argument string arg according to \npattern p. build builder ( (arg) )? p Builds the fragment using builder builder with argument arg according \nto p. run runner ( (arg) )? p Executes the fragment using runner runner with argument arg according to \np. e Freeform expression e, speci.ed in the language de.nition language, is satis.ed. Patterns in test \nconditions (p) succeeds fails to term to fragment to file file Empty pattern: same as succeeds. Operation \n(i.e., refactoring, builder, execution) is expected to be successful. Operation is expected to fail. \nThe result should match a term pattern such as PropAccess(\"a\",\"b\"). The result should match a code fragment \nfragment. The result should match the contents of .le .le. Figure 5. Summary of the test speci.cation \nsyntax. of the setup block serves as a template for the test cases, where the [[...]] placeholder is \n.lled in with the contents of each test block.4 The placeholder is optional: if none is speci.ed, we \nassume it occurs at the end of the setup block. Setup blocks are essentially a purely syntactic, language\u00adagnostic \nfeature, but they are highly .exible. They can be used to factor out boilerplate code from individual \ntests, such as module and import declarations. They can also be used to declare types, functions and \nvalues used in test cases. Much like with the setup() and tearDown() methods of xUnit, they can also \nbe used to perform tasks such as database initialization for test cases that execute tested programs. \n3Alternatively, [[[... ]]]or [[[[... ]]]]can be used. 4Note that we overload the quotation brackets to \nspecify anti\u00adquotations for selections and for placeholders in setup blocks. This design ensures minimal \nsyntactic interference with the language under test, as lan\u00adguage engineers can pick which quotation \nmarkers to use (e.g., [[, [[[, and so on). Overview We conclude this section with an overview of our \ntest speci.cation syntax, shown in Figure 5. So far we already discussed test con.guration, test cases, \nand tested fragments. The table also shows the possible condition clauses for syntactic, static semantic, \nand dynamic semantics tests, and the patterns that can be used with some condition clauses. We further \nillustrate those elements with a series of examples in Section 5. 4. Test Speci.cation Interaction Design \nTool support is an important factor for productivity with pro\u00adgramming languages. For the domain of testing \nin particular, good tool support is important to lower the threshold of test\u00ading [15]. In this section \nwe show how tool support in the form of IDE components can be applied to lower the thresh\u00adold to language \nde.nition testing and to increase the produc\u00adtivity of language engineers when editing and running tests. \n We propose a combination of four forms of tool support for language de.nition testing. For editing tests, \nwe propose to aid language engineers by providing editor services for 1) the generic test speci.cation \nlanguage, and 2) editor ser\u00advices of the language under test in test fragments. For run\u00adning tests, we \npropose a combination of 3) live evaluation of test cases as they are edited, and 4) a batch test runner \nfor testing larger test suites. In the remainder of this section we show how these forms of tool support \nare realized in the Spoofax testing language and how they can be used. The un\u00adderlying implementation \naspects are discussed in Section 6.  4.1 Editor Services for Test Speci.cation Integrated development \nenvironments are a crucial factor in programmer productivity [32]. Modern IDEs incorporate many different \nkinds of editor services, assisting developers in code understanding and navigation, directing developers \nto inconsistent or incomplete areas of code, and even helping them with editing code by providing automatic \nindentation, bracket insertion, and content completion. Most editor services provided in modern IDEs \nare lan\u00adguage speci.c, and can be de.ned as part of the language de.nition. The challenge in providing \neffective IDE sup\u00adport for language de.nition testing is in providing language\u00adspeci.c support for both \nthe testing language and for the em\u00adbedded language under test. Editor services for the generic testing \nlanguage Editor services for the generic testing host language are the meat and potatoes for making language \nengineers more produc\u00adtive with testing. Our implementation provides the full range of syntactic and \nsemantic editor services for working with the testing language, ranging from syntax highlighting to er\u00adror \nmarking and content completion for all elements of Fig\u00adure 5. Editor services for language under test \nRather than treat tested fragments as an opaque input string, we use editor ser\u00advices of the language \nunder test to support them as .rst-class parts of a test speci.cation. Our implementation provides services \nsuch as syntax highlighting, syntax error marking, semantic error marking, and content completion, as \nshown in the screenshots of Figure 6 (a) and (b). Note that error mark\u00ad ers are only shown for failing \ntest cases, not for negative test cases where errors are expected (Figure 6 (c)).  4.2 Running Language \nDe.nition Tests Live evaluation of test cases Live evaluation of test cases as they are edited ensures \nthat language engineers get the same rapid feedback and editing experience as they get with throwaway \nprograms used to test language de.nitions. To achieve this effect, our implementation evaluates tests \nin the background and shows which tests fail through error and warning markers in the editor. With this \nfeedback, devel\u00adopers can quickly determine the status of tests in a testing module. Since some operations \nmay be long-running, we ex\u00ad  (b) Online evaluation of tests and error markers.  (c) A passing test \ncase specifying negative test condition. Figure 6. IDE support for test speci.cations. clude test cases \nthat depend on building or executing the test from background execution, and instead focus on tests of \nthe syntax, static semantics, and transformations de.ned for a language. Batch execution To support long-running \ntest cases and larger test suites, we also provide a batch test runner as de\u00adscribed in [15]. Such a \ntest runner is particularly important as a language project evolves and the number of tests grows substantially \nand tests are divided across multiple test mod\u00adules. Figure 7 shows a screenshot of our graphical test \nrun\u00ad ner. The test runner gives a quick overview of passing and failing tests in different modules and \nallows developers to navigate to tests in a language project. Tests can also be eval\u00aduated outside the \nIDE, for example as part of a continuous integration setup. 4.3 Using Integrated Language De.nition \nTesting Rather than designing a complete new language on paper, before its implementation, it is good \npractice to incremen\u00adtally introduce new features and abstractions through a pro\u00adcess of evolutionary, \ninductive design [13, 36]. The LPTL approach makes it possible to start a language design with examples \nthat (eventually) form test cases.  Figure 7. The batch test runner. Testing from the point of inception \nof a language requires that the tested language implementation artifact is in such a state that it can \nproduce some form of output for a given input program. Language workbenches such as Spoofax can generate \nan executable and thus testable language plu\u00adgin from only a (partial) syntax de.nition [25]. Additional \nfeatures, in the form of editor services and static and dy\u00adnamic semantics, can then be iteratively and \nincrementally added. With an LPTL, each new feature can be tested at at any stage of the development \nprocess. This makes it possible to develop languages in a test-driven fashion, following the rhythm described \nin [1]: 1. Write a test case. 2. Watch it fail. 3. Implement the tested feature. 4. Watch all tests \nsucceed. 5. Refactor when necessary and repeat.  Our approach facilitates this process for language \nengineer\u00ading by providing a specialized language testing infrastruc\u00adture that gives direct feedback at \nany stage in this develop\u00adment cycle. 5. Language De.nition Testing by Example In this section we show \nhow different language aspects can be tested through examples using the mobl language. Mobl Mobl is a \nstatically typed language and compiles to a combination of HTML, Javascript, and CSS. Mobl integrates \nsub-languages for user interface design, data modeling and querying, scripting, and web services into \na single language. In this paper we focus on the data modeling language. An excerpt of the syntax of \nmobl is shown in Figure 8. In mobl, most .les starts with a module header, followed by a list of entity \ntype de.nitions, functions, and possibly state\u00adments. An example of a mobl module that de.nes a single \nentity type is shown in Figure 9. Entities are persistent data types that are stored in a database and \ncan be retrieved using Start ::= \"module\" QId Def* Def ::= \"entity\" ID \"{\" EBD* \"}\" | Function | Stm \nEBD ::= ID \":\" Type | Function Function ::= \"function\" ID \"(\" (FArg (\",\" FArg)*)? \")\" \":\" Type \"{\" Stm* \n\"}\" Stm ::= \"var\" ID \":\" Type \"=\" Exp \";\" | \"var\" ID \"=\" Exp \";\" | Exp \"=\" Exp \";\" | Exp \";\" | \"{\" Stm* \n\"}\" | \"if\" \"(\" Exp \")\" Stm (\"else\" Stm)? | \"foreach\" \"(\" ID \"in\" Exp \")\" Stm | \"foreach\" \"(\" ID \":\" Type \n\"in\" Exp \")\" Stm Exp ::= STRING | NUMBER | \"null\" | \"this\" | Exp \".\" ID | ID \"(\" (NameExp (\",\" NameExp)*)? \n\")\" NameExp ::= ID \"=\" Exp | Exp FArg ::= ID \":\" Type Type ::= ID | \"Collection\" \"<\" Type \">\" QId ::= \nID | QId \"::\" ID Figure 8. A subset of mobl s syntax, from [20]. module tasks::datamodel entity Task \n{ name : String date : DateTime } Figure 9. A mobl de.nition of a Task data type. mobl s querying API. \nWe import the tasks::datamodel example module in tests throughout this section. 5.1 Syntax The syntax \nde.nition forms the heart of the de.nition of any textual language. It incorporates the concrete syntax \n(keywords etc.) and the abstract syntax (data structure for analysis and transformations) of a language, \nand is generally the .rst artifact developed with a new language de.nition. The syntax can be considered \nseparately from the remainder of a language de.nition, and can be used to generate a parser and editor \nwith basic syntactic services. It can also be tested separately from any semantic aspects of the language. \nSyntax tests can be used to test newly added language constructs. They can include various non-trivial \ntests such as language mobl start symbol Stm test Named parameters [[ var e = Task(name=\"Buy milk\"); \n]] parse succeeds test true is a reserved keyword [[ var true = 1; ]] parse fails test Test dangling \nelse [[ if (true) if (true) {} else {}  ]] parse to IfNoElse(True, If(True, _, _)) test Nested property \naccess [[ v = a.b.c; ]] parse to Assign(\"v\", FieldAccess(FieldAccess(\"a\", \"b\"), \"c\")) Figure 10. Syntactic \ntests. tests for operator precedence, reserved keywords, language embeddings, or complex lexical syntax \nsuch as the quotation construct of Figure 5. We distinguish two forms of syntax tests. First, there are \npure black-box tests, which test if a code fragment can be parsed yes or no. The .rst two examples of \nFigure 10 show positive and negative black-box tests. Next, we also support syntactic tests that use \ntree patterns to match against the abstract syntax produced by the parser for a given fragment. The third \nand fourth tests in the .gure show examples of such tests.5 These tests are not pure black-box tests \nas they expose something about the implementation of the parser. They may not rely directly on the internals \nof the parser, but they still depend on the technology used. Many parser generators rely on restricted \ngrammar classes [26], placing restrictions on the syntax de.nition, making it dif.cult to produce certain \ntrees such as the left-recursive trees for .eld access in Figure 10. In Spoofax, these restrictions are \nnot an issue since we use a generalized-LR parser.  5.2 Static semantic checks Static semantic checks \nin languages play an important role in the reliability of programs written in that language. With DSLs \nsuch as mobl, these checks are often speci.c to the domain of the language, and not supported by the \ntarget platform. In mobl s case, a dynamic language is targeted that performs no static checks at all. \nWith tests we can have better con.dence in the static checks de.ned for a language. Examples are shown \nin Fig\u00ad 5We use pre.x constructor terms to match against tree patterns, match\u00ading against the name of \na tree node and its children. Wildcards are indicated with an underscore. language mobl setup [[ module \ntasks import tasks::datamodel var todo = Task(name=\"Create task list\"); ]] test Entity types have an \nall() built-in [[ var all : Collection<Task> = Task.all(); ]] succeeds test Assigning a property to a \nNum [[ var name : Num = todo.name; ]] 1 error /type/ test Local variable shadowing [[ function f() { \nvar a : A; {  var a :A; } } ]] 1 error /already defined/ Figure 11. Tests for static semantic checks. \n ure 11. We use a setup block6 in this .gure to import the tasks::datamodel module, and to initialize \na single Task for testing. The .rst test case is a positive test, checking that the built-in all() accessor \nreturns a collection of tasks. The other tests are negative tests. For such test cases, it is gener\u00adally \nwise to test for a speci.c error message. We use regular expressions such as /type/ to catch speci.c \nerror messages that are expected. 5.3 Navigation Modern IDEs provide editor services for navigation \nand code understanding, such as reference resolving and content completion. These services are a manifestation \nof the name analysis that is highly important for the dependability of a language implementation. Tests \nfor reference resolving and content completion test not only the user experience in an editor, but also \nthe underlying name analysis and any other analyses it may depend on. Figure 12 shows examples of tests \nfor reference resolv\u00ad ing and content completion. Note how reference resolving tests can use multiple \nselected areas. Our .rst test case tests variable shadowing, while the second one tests reference re\u00adsolving \nfor function calls. For content completion we test completion for normal local variables, and for built-ins \nsuch as the all() accessor. 5.4 Transformations and Refactorings Transformations can be used to create \nviews and for com\u00adpilation. To test transformations, we use the notion of a 6Recall that the [[...]] \nplaceholder notation is optional: by default, we assume that the placeholder is at the end. This default \nelegantly allows most mobl tests to avoid explicit placeholders. language mobl setup [[ module navigation \nimport tasks::datamodel var example = \"Magellan\"; ]] test Resolve a shadowing variable [[ function getExample() \n: String { var [[ example ]] = \"Columbus\"; return [[ example ]] ; } ]] resolve #2 to #1 test Resolve \na function call [[ function [[ loop ]] (count : Num) { [[ loop ]] (count + 1); } ]] resolve #2 to #1 \ntest Content completion for globals [[ var example2 = [[ e ]] ; ]] complete to \"example\" test Content \ncompletion for queries [[ var example2 = Task. [[ a ]] ; ]] complete to \"all()\" Figure 12. Reference \nresolving and content comple\u00adtion tests. builder. Builders are transformations that can be triggered \nby the user, displaying a view or generating code [25]. Mobl implements several, for use by both end-programmers \nand meta-programmers. The .rst test case in Figure 13 shows an example of a test for the desugar builder, \none of the builders used by the designers of mobl to inspect the desug\u00adared version of a module. Refactorings \nare a form of transformations that rely on pre-conditions and post-conditions to perform behavior\u00adpreserving \ntransformations [12]. With tests, language engi\u00ad neers can gain more con.dence about the transformation \nperformed for a refactoring and its pre-and post-conditions. The second test case in Figure 13 is an \nexample of a refac\u00ad toring test. The example tests the rename refactoring with the input string \"y\" which \ndetermines the new name of the selected identi.er. The test condition compares the output to the expectation, \nwhere behavior is only preserved if just the local variable x is replaced and not the other x s.  5.5 \nCode Generation and Execution The ultimate goal of most language de.nitions is to generate or interpret \ncode for execution. Sometimes, languages also generate artifacts for inspection by the user, such as \na graph\u00adical view of all entities in a mobl application. Testing can be used to con.rm that exactly the \nright output is generated for a particular input, but those tests are often rather fragile: one small \nchange in a compiler can break a test case even if the program still compiles correctly. It is more practical \nto use an external oracle for those tests, such as a compiler language mobl setup [[ module Example \nimport tasks::datamodel ]] test Desugaring adds a type to foreach [[ foreach (t in Task.all()) { // ... \n} ]] build desugar to [[ foreach (t : Task in Task.all()) { // ... } ]] test Rename refactoring [[ var \nx= 1; function x(x : Num) : Num { return [[ x ]] ; }  ]] refactor rename(\"y\") to [[ var x= 1; function \nx(y : Num) : Num { return y; } ]] Figure 13. Tests for transformations and refactorings. or lint-type \nchecker. Another strategy is to ensure that the program is executable and to simply run it: execution \ntests can indirectly serve as tests of generated code correctness. For execution tests we use the notion \nof a runner. Similar to a builder, runners are operations that execute code, through interpretation or \nby running a generated program. Figure 14 shows tests for code generation and execution. We use a setup \nblock to initialize the database by adding new Task instances that can be used in the tests. In the .rst \ntest case we have a test that only triggers code gener\u00adation, using mobl s generate-artifacts builder. \nIn this case the builder is only required to succeed, we do not ex\u00adplicitly check its output. The other \ntest cases use a runner run-test, which invokes the getResult() function and returns the result as a \nstring for comparison. 5.6 Testing for end-programmers So far we have considered testing for meta-programmers. \nEnd-programmers that use a language are generally not in\u00adterested in testing the syntax or static semantics \nof a lan\u00adguage. They are, however, interested in the dynamic se\u00admantics; writing unit tests for programs \nwritten in the lan\u00adguage. An LPTL can be used as a basis for maintaining and running such tests. End-programmers \nthen get the same language-speci.c feedback and tooling for writing tests as meta-programmers, and can \nuse the same testing language for testing multiple DSLs that may be employed in a project. The LPTL as \nwe designed it is aimed at meta\u00adprogrammers, and provides a general platform for testing. For end programmers \nit can be specialized for one partic\u00ad language mobl setup [[ module runtimetests import tasks::datamodel \nforeach (n in range(0, 10)) { add(Task(name=\"Task \"+n)); }  function getResult() : Num { return [[ ... \n]] ; } ]] test Compile to HTML/JavaScript/CSS [[ 1+1 ]] build generate-artifacts test String API [[ \"string\".indexOf(\"r\") \n]] run run-test(\"getResult\") to \"2\" test Counting query [[ Task.all().count() ]] run run-test(\"getResult\") \nto \"10\" Figure 14. Code generation and execution tests. ular language (eliminating the language header) \nand for the purpose of execution tests (simplifying the run clause). Providing specialized instances \nof the test speci.cation lan\u00adguage is considered future work.  5.7 Freeform tests The test speci.cation \nlanguage is open-ended: if there are aspects of a language de.nition that need testing but are not covered \nby the .xed conditions in the table of Figure 5, freeform test expressions can be used. In the Spoofax \ntesting language, we use the Stratego language [5] to specify them, as it is also the language used to \nde.ne semantic aspects of language de.nitions in Spoofax [25]. Freeform expressions can directly interact \nwith the language implementation to express white-box test cases. For example, they can test whether \nan internal function that retrieves all the ancestors in the inheritance chain of a class works, or they \ncan test that generate-artifacts correctly writes a .js .le to disk.  5.8 Self application An interesting \ncapability of the testing language is that it can be applied to itself. In our implementation of the \nSpoofax testing language, it can be applied to any language designed in the language workbench, including \ninstantiations of the testing language. Figure 15 shows an example. Note how we use the triple-bracket \nquotation form (i.e., [[[ ... ]]]) in this example, as the testing language itself uses the normal double \nbrackets. For the outer test speci.cation, any selec\u00adtions or setup placeholders should then also be \nspeci.ed us\u00ading triple brackets. The inner test speci.cation is free to use double brackets. language \nSpoofax-Testing test Testing a mobl test specification [[[ module test-mobl language mobl test Testing \nmobl [[  module erroneous // ... ]] 1 error ]]] succeeds Figure 15. Testing the test speci.cation language. \n 6. Implementation In this section we describe our implementation of an LPTL and the infrastructure that \nmakes its implementation pos\u00adsible. We implemented the Spoofax testing language as a language de.nition \nplugin for the Spoofax language work\u00adbench [25]. Spoofax itself is, in turn, implemented as a col\u00ad lection \nof plugins for the extensible Eclipse IDE platform. Most Spoofax language de.nitions consist of a combination \nof a declarative SDF [35] syntax de.nition and Stratego [5] transformation rules for the semantic aspects \nof languages, but for this language we also wrote parts of the testing in\u00adfrastructure in Java. 6.1 Infrastructure \nThe Spoofax language workbench provides an environment for developing and using language de.nitions [25]. \nIt pro\u00ad vides a number of key features that are essential for the im\u00adplementation of an LPTL. A central \nlanguage registry Spoofax is implemented as an extension of the IDE Meta-tooling Platform (IMP) [7] which \nprovides the notions of languages and a language registry. The language registry is a component that \nmaintains a list of all languages that exist in the environment. It also allows for runtime re.ection \nover the services they provide and any meta-data that is available for each language, and can be used \nto instantiate editor services for them. Dynamic loading of editor services Spoofax supports dy\u00adnamic, \nheadless loading of separate language and editor ser\u00advices of the language under test. This is required \nfor instanti\u00adation of these services in the same program instance (Eclipse environment) but without opening \nan actual editor for them. Functional interfaces for editor services Instantiated edi\u00adtor services have \na functional interface. This decouples them from APIs that control an editor, and allows the LPTL to \nin\u00adspect editor service results and .lter the list of syntactic and semantic error markers shown for \nnegative test cases. Support for a customized parsing stage Most Spoofax plugins use a generated parser \nfrom an SDF de.nition, but it is also possible to customize the parser used. This allows the LPTL to \ndynamically embed a language under test.7 7Note that even though Spoofax supports generalized parsing \nand syn\u00adtax embedding techniques, a different approach is required in this case as These features are \nnot trivially supported in language work\u00adbenches, but there are other workbenches that support a sub\u00adset. \nFor instance, where many language workbench imple\u00admentations generate relatively opaque, autonomous Eclipse \nplugins, MPS [37] is an example of a workbench with .rst\u00ad class languages and a language registry. Many \nworkbenches support some level of dynamic loading of services, although their implementation may be tied \nto IDE interfaces that may make it hard to instantiate them in a headless fashion. Func\u00adtional interfaces \nfor editor services are rare, but could be im\u00adplemented for workbenches that generate the service imple\u00admentations. \nMontiCore [28] is an example of a workbench that applies similar techniques with regard to combining \nhost and embedded language parsers.  6.2 Syntax and Parsing Language engineers can instantiate the testing \nlanguage for any Spoofax language that is loaded in the Eclipse environ\u00adment, either as an Eclipse plugin, \nor as a language project in source form. Once the developer speci.es which language to test, the syntax \nof the testing language is instantly spe\u00adcialized by integrating the syntax of the language under test. \nThis makes it possible to provide syntactic editor services such as syntax highlighting, and to parse \nthe .le to a sin\u00adgle abstract syntax tree that is used in the implementation of semantic editor services \nand tests evaluation. In previous work, we have shown how generalized parsers can be used to syntactically \ncompose languages [6]. Generalized parsers support the full class of context-free grammars, which is \nclosed under embedding. That is, any two context-free grammars can be composed to form again a context-free \ngrammar. This makes it possible to support modular syntax de.nitions and allow for language composi\u00adtion \nscenarios such as embedding and extension. Unfortunately, the embedding as we have de.ned it for the \nLPTL is not context-free. First, because test fragments can only be parsed when their setup block (context) \nis taken into consideration. Second, because test fragments are al\u00adlowed to contain syntax errors, such \nas spurious closing brackets. Even when considering syntax error recovery tech\u00adniques, which use a local \nor global search space or token skipping techniques [10], the test fragments must be consid\u00ad ered in \nisolation to ensure correct parsing of the test speci.\u00adcation. As we cannot compose the LPTL parser with \nthat of the language under test at the level of the syntax de.nition, we have chosen for a more ad hoc \napproach, parsing the LPTL and the language under test in separate stages. Figure 16 il\u00ad lustrates these \nstages. First, we parse the LPTL using a skele\u00adtal test speci.cation syntax, where every setup and test \nfrag\u00adment is parsed as a lexical string. Second, we parse each setup and test fragment again using the \nparser of the lan\u00ad the embedding cannot be expressed as a context-free grammar, as we dis\u00adcuss in Section \n6.2. guage under test. For this, the setup fragment is instantiated for each test, and also parsed separately. \nAs a third step, we merge the abstract syntax trees and token streams8 of the skeletal LPTL and of the \ntest cases. The merged tree and to\u00adken stream are then used for editor services and to evaluate the tests \nin the module. Our approach makes it possible to directly instantiate the language without generating \na new parser for the instantiated LPTL. Our Java-based scannerless generalized-LR (JSGLR) parser is relatively \nef.cient, ensuring good runtime perfor\u00admance and allowing for interactive use of the Spoofax testing \nlanguage. With support for error recovery techniques [9, 23], JSGLR also ensures that a valid abstract \nsyntax tree is pro\u00adduced for providing editor services in case a test module is syntax incorrect (as \nseen in Figure 6 (a)). There are still opportunities for performance optimizations; e.g. the three stages \ncould be more tightly integrated and caching could be added for parsing the test fragments, but so far \nwe have not found the need for this. 6.3 Tool Support The language registry provided by Spoofax and \nIMP main\u00adtains a collection of all languages supported in the environ\u00adment, and provides access to factory \nclasses to instantiate language-speci.c editor services (e.g. a syntax highlighter, content completion \nservice, or code generator). Using the language registry, and the dynamic editor service loading fa\u00adcilities \nof Spoofax, it is possible to access the parser, syn\u00adtactic start symbols, and a list of editor services \nthat can be instantiated for a language, given its name. We use the reg\u00adistry to instantiate these services \nfor editor support in the language under tests and for evaluating tests. Editor service support in test \nfragments is provided by delegation to services of the language under test. An exam\u00adple is content completion \nsupport to help write test cases. The editor services for the testing language simply detect that they \nare invoked inside a test fragment, and then dele\u00adgate the work to a service of the language under test. \nThe only special cases are the syntax error marking and seman\u00adtic error marking service. These produce \nlists of errors that must be .ltered according to the test expectations (e.g., if an error is expected, \nthe IDE should not add a red marker for it). The parser and editor services are transparently loaded \non demand once they are used for a test case. As a result, the editor for a test module is instantly \nspecialized by simply specifying the name of the language under test. Further con\u00ad.guration is not required. \nTest cases are also automatically re-evaluated in the editor if a language de.nition is changed. Test \nevaluation Tests are evaluated by instantiating the appropriate editor services for the language under \ntest and 8Note that Spoofax uses scannerless parsing, but still constructs a token stream after parsing \nbased on the lexical structure of parsed sentences, used for editor services such as syntax highlighting. \n applying them to the abstract syntax tree that corresponds to the test input fragment. For example, \nconsider a reference resolving test such as that of Figure 12. To evaluate such a test, the language \nregistry is used to instantiate services for semantic analysis and reference resolving. Like all editor \nservices, the reference resolver has a functional interface, which essentially gets an analyzed program \nand an abstract syntax tree node of a reference as its input, and returns the declaration abstract syntax \ntree node. To test it, we give it the analyzed program, obtained from the analysis service, and the tree \nnode that corresponds to the reference selected in the test fragment. The result is then compared to \nthe expected result of the test case. Execution tests often depend on some external executable that runs \noutside the IDE and that may even be deployed on another machine. Our implementation is not speci.c for \na particular runtime system or compiler backend. Instead, language engineers can de.ne a custom runner \nfunction that controls how to execute a program in the language. For a language such as mobl, a JavaScript \nengine such as Rhino or a browser emulator such as WebDriver can be used. At the time of writing, we \nhave not yet completed that binding for mobl yet. Test evaluation performance Editor services in Spoofax \nare cached with instantiation, and run in a background thread, ensuring low overhead and near-instant \nresponsive\u00adness for live test evaluation. Most editor services are very fast, but long-running tests \nsuch as builds or runs are bet\u00adter executed in a non-interactive fashion. We only run those through the \nbatch test runner of Section 4.2 and display in\u00ad formation markers in the editor if the test was changed \nafter it last ran. 7. Discussion and Related Work Related work on testing of language implementations \ncan be divided into a number of categories: testing with general\u00adpurpose tools, testing with homogeneous \nand heterogeneous language embeddings, and test case generation. Testing with general-purpose tools \nConsiderable experi\u00adence exists in the use of general-purpose testing tools and scripts for tests of \nlanguage de.nitions [14, 16, 18, 22, 30]. Sometimes, they take the form of a simple shell script that \nbuilds all .les in a directory. In other cases they use JUnit or a related xUnit-family [19] testing \nframework. The use of these tools introduces a number of challenges when applied to language de.nitions. \nFirst, a major issue is that a signi.cant investment in language-speci.c testing infrastructure must \nbe made to support tests for different as\u00adpects of languages, ranging from syntax to semantics and editor \nservices. We support a generic, declarative language for testing these aspects. A second issue is that \nto make sure tests are considered in isolation, each test case is gener\u00adally put in a separate .le. Using \nseparate .les for test cases introduces boilerplate code such as import headers. It also makes it harder \nto organize tests, requiring conventions for .le and directory names. Using test .les speci.ed purely \nin the tested language also separates test conditions and expec\u00adtations from the test program. With an \nadditional investment in effort, some of these issues can be solved, but only on a per-language basis. \nWe provide a language-generic solution. Another limitation of these general testing tools is that they \ndo not provide specialized IDE support for writing tests. Standard IDEs are only effective for writing \nvalid programs and report spurious errors for negative test cases where errors are expected. Batch test \nrunners such as JUnit also do not have the capability to directly direct users to the failing line of \ncode in a test input in case a test fails. Testing with homogeneous embeddings Language em\u00adbedding is \na language composition technique where separate languages are integrated. An example is the embedding \nof a database querying language into a general-purpose host lan\u00adguage. These embeddings can be heterogeneous, \nwhere an embedded language can be developed in a different language than the host language, or homogeneous, \nwhere the host lan\u00adguage is used to de.ne the embedded language [21, 34]. Ho\u00ad mogeneous embeddings of \nDSLs are sometimes also called internal DSLs. The embedding technique applied in this pa\u00adper is a heterogeneous \nembedding. Homogeneously embedded languages must always target the same host language. This can be a \nweakness when exe\u00adcution on a different platform is desired (e.g., JavaScript in the case of mobile development \nwith mobl [20]). It can also be a strength in terms of tool support. As embedded lan\u00adguages all target \nthe same platform, they can be tested in the same way. General-purpose testing frameworks can then be \napplied more effectively, since they can directly use the em\u00adbedded language. In MPS [37], tests based \non JUnit can be evaluated as they are typed, much like with our testing lan\u00adguage. A restriction of the \napproach used in MPS is that it can only be used test the dynamic semantics of a language, and only allow \nfor positive test cases. With MPS, a projec\u00adtional editor is used that even restricts the tests so they \ncan only follow the existing syntax of the language, ruling out sketches of new syntax and test-driven \ndevelopment of new syntactic constructs. Our test speci.cation language is more .exible, supporting a \nmuch wider spectrum of test condi\u00adtions (Figure 5), including negative test cases for syntax or static \nsemantics and refactoring tests. Testing with heterogeneous embeddings In previous work, we developed \nparse-unit, a grammar testing tool de\u00adveloped as part of the Stratego/XT program transformation system \nand tool [5]. This tool formed a precursor to the present work. Parse-unit would embed quoted program \nfrag\u00adments in a test module, much like in the Spoofax testing language, but only supported syntactic \ntest cases. There was also no IDE support for parse-unit, and all quoted program fragments were treated \nas strings rather than forming a .rst\u00adclass part of the language. Other testing tools that support embedding \nhave similar limitations as parse-unit, supporting only syntactic tests, and lacking IDE support. A notable \nexample is gUnit [17], a testing language for ANTLR grammars. Facilities as those provided by gUnit have \nbeen lacking in current language workbenches and other interactive tools for building and debugging parsers \nsuch as ANTLRWorks [4]. Test case generation techniques There is a long history of research on test case \ngeneration techniques for language implementations. An overview is given in surveys by Bou\u00adjarwah and \nSaleh [3], and Kossatchev and Posypkin [27]. These techniques use grammars to generate test programs. \nTo control the theoretically in.nite set of programs that can be generated using most grammars, they \nuse annotations in grammars, external control mechanisms, or even imperative generators [8], to constrain \nthis set. In some cases, sophis\u00ad ticated, hand-tailored program generators are used, such as Csmith [40], \na successful 40,000-line C++ generator pro\u00ad gram for randomly generating C programs. The set of test \nprograms generated with these approaches can be used to stress-test compiler implementations. For ex\u00adample, \nthey can be used to compare a compiler to its refer\u00adence implementation, or to check for validity of \ngenerated code for the subset of programs that type-check. As such, they provide an excellent complementary \napproach to our test speci.cations, possibly catching corner cases that a lan\u00adguage engineer did not \nthink of. However, as they only test complete compilation chains and rely on a test oracle such as a \nreference compiler, they are less effective for testing lan\u00adguages while they are still under development. \nIn contrast, our approach can be used from the point of inception of a language and even in the design \nprocess. By applying test\u00addriven development, test speci.cations can be used to guide the development \nprocess. Our test speci.cations also pro\u00advide a more varied array of tests by providing an extensive, \nopen-ended set of test condition speci.cation constructs for observable behavior of language implementations. \n Unit tests for domain-speci.c languages As a side-effect of providing a language for testing language \nimplementa\u00adtions, our test speci.cation language can also be used to test programs written in that language \n(see Section 5.6). This makes it particularly useful in the domain of testing DSL programs, where testing \ntools and frameworks are scarce. Traditionally, language engineers would have to build such tools and \nframeworks by hand, but recently Wu et al. [39] provided a reusable framework for language testing. They \nrequire language engineers to extend their language with scripting constructs that generate JUnit test \ncases. The com\u00adbined scripting and DSL language can then be used to spec\u00adify tests. Their framework ensures \nthat the mapping between the DSL test line numbers and the generated JUnit tests is maintained for reporting \nfailing tests. They also provide a graphical batch test runner. While our approach does not provide the \nsame .exibility and requires tests to be speci\u00ad.ed with a quotation marks and language and run clauses, \nit is interesting to note how our approach relieves language engineers from much of the heavy lifting \nrequired for imple\u00admenting a DSL testing solution. We only require language engineers to specify a binding \nto an execution engine (Sec\u00adtion 6.3), and we provide a generic test speci.cation host language that \nis combined with the DSL to test. 8. Concluding Remarks In this paper we proposed an approach to language \ndef\u00adinition testing by introducing the notion of a language\u00adparametric testing language. The LPTL provides \na zero\u00adthreshold, domain-speci.c testing infrastructure based on a declarative test speci.cation language \nand extensive tool support for writing and executing tests. Our implementation in the form of the Spoofax \ntesting language shows the prac\u00adtical feasibility of the approach. Tests inspire con.dence in language \nimplementations, and can be used to guide an agile, test-driven language de\u00advelopment process. Unfortunately, \nin current software lan\u00adguage engineering practice, tests are still too often an af\u00adterthought. Especially \nDSLs often remain untested, as they are developed in a short timespan with limited resources. We believe \nthat declarative language test suites should become standard components of language de.nitions, just \nas BNF\u00adstyle grammars are. Supported by an LPTL, tests are con\u00adcise, implementation-independent, and \nrequire little to no ef\u00adfort to setup. Future work In this paper we emphasized testing of ob\u00adservable \nbehavior of languages, such as reported errors and name analysis as manifested by reference resolving \nin an IDE. Other analyses such as type or .ow analysis are not manifested that way, but it can be useful \nto write test cases for them. Right now, these aspects are either indirectly tested, or tested using \nthe generic builders interface for custom transformations. Direct support for testing such lan\u00adguage \nde.nition aspects could be a worthwhile addition. Al\u00adternatively, rather than seeking to support all \npossible com\u00adpiler and IDE aspects in a testing language, perhaps a bet\u00adter test abstraction mechanism \nis needed to specify multiple tests that interact with a language de.nition in the same way. Similarly, \nan abstraction mechanism for setup blocks could be introduced for improved modularization of test suites, \ne.g. by allowing of setup blocks to be put in libraries, to support multiple arguments, and to support \ncomposition. For the interaction design of the LPTL, previous work on interactive disambiguation [24] \ncould be applied for hand\u00ad ling ambiguities of quoted test programs. Test understand\u00adability can also \nbe improved using further visual aids, for example to emphasize differences between test inputs and outputs \nfor refactoring tests. The declarative basis provided by the LPTL can be used to integrate generally \napplicable supportive techniques for testing, such as test case prioritization, coverage metrics, coverage \nvisualization, mutation testing, and mutation-based analyses for untested code. In particular, the integration \nof such techniques specialized for the domain of parser and compiler testing [3, 27, 29] is an important \narea of future work. Acknowledgements This research was supported by NWO project 612.063.512, TFA: Transformations \nfor Abstractions and the NIRICT LaQuSo Build Farm project. We would like to thank Martin Bravenboer for \nhis work on the parse\u00adunit project, which provided a basis for the Spoofax testing language; Danny Groenewegen \nfor his inspiring tests scripts and test collection for WebDSL; Zef Hemel for his work on mobl that was \nused in examples in this paper; and Maartje de Jonge, Sander Vermolen, and the anonymous referees for \nsuggestions for this paper. References [1] K. Beck. Test-driven development: by example. Addison-Wesley \nProfessional, 2003. [2] B. Beizer. Software testing techniques. Dreamtech Press, 2002. [3] A. Boujarwah \nand K. Saleh. Compiler test case generation methods: a survey and assessment. Information and Software \nTechnology, 39(9):617 625, 1997. [4] J. Bovet and T. Parr. ANTLRWorks: an ANTLR grammar de\u00advelopment \nenvironment. Software: Practice and Experience, 38(12):1305 1332, 2008. [5] M. Bravenboer, K. T. Kalleberg, \nR. Vermaas, and E. Visser. Stratego/XT 0.17. A language and toolset for program trans\u00adformation. Science \nof Computer Programming, 72(1-2):52 70, 2008. [6] M. Bravenboer and E. Visser. Concrete syntax for objects: \ndomain-speci.c language embedding and assimilation with\u00adout restrictions. In J. M. Vlissides and D. C. \nSchmidt, editors, Object-Oriented Programming, Systems, Languages, and Ap\u00adplications, OOPSLA 2004, pages \n365 383. ACM, 2004. [7] P. Charles, R. M. Fuhrer, S. M. Sutton, Jr., E. Duester\u00adwald, and J. Vinju. Accelerating \nthe creation of customized, language-speci.c IDEs in Eclipse. In Proceeding of the 24th ACM SIGPLAN conference \non Object oriented programming systems languages and applications, OOPSLA 09, pages 191 206. ACM, 2009. \n[8] B. Daniel, D. Dig, K. Garcia, and D. Marinov. Auto\u00admated testing of refactoring engines. In I. Crnkovic \nand A. Bertolino, editors, 6th joint meeting of the European Soft\u00adware Engineering Conference and the \nInt. Symposium on Foundations of Software Engineering, pages 185 194. ACM, 2007. [9] M. de Jonge, E. \nNilsson-Nyman, L. C. L. Kats, and E. Visser. Natural and .exible error recovery for generated parsers. \nIn M. van den Brand, D. Gasevic, and J. Gray, editors, Software Language Engineering, SLE 2009, volume \n5969 of LNCS, pages 204 223. Springer, 2009. [10] P. Degano and C. Priami. Comparison of syntactic error \nhandling in LR parsers. Software Practice and Experience, 25(6):657 679, 1995. [11] S. Efftinge and \nM. V\u00f6lter. oAW xText -a framework for textual DSLs. In Modeling Symposium, Eclipse Summit, 2006. [12] \nM. Fowler. Refactoring: Improving the design of existing code. In D. Wells and L. A. Williams, editors, \nExtreme Programming and Agile Methods -XP/Agile Universe 2002, Second XP Universe and First Agile Universe \nConference, volume 2418 of LNCS, page 256. Springer, 2002. [13] M. Fowler. Language workbenches: The \nkiller-app for domain speci.c languages? http://martinfowler. com/articles/languageWorkbench.html, 2005. \n[14] J. D. Frens and A. Meneely. Fifteen compilers in .fteen days. In D. Baldwin, P. T. Tymann, S. M. \nHaller, and I. Russell, editors, 39th Technical Symposium on Computer Science Ed\u00aducation, 2006, pages \n92 96. ACM, 2006. [15] E. Gamma and K. Beck. Test infected: Programmers love writing tests. Java Report, \n3(7):37 50, 1998. [16] J. B. Goodenough. The Ada compiler validation capability. In Proceedings of the \nACM-SIGPLAN symposium on Ada programming language, SIGPLAN 80, pages 1 8. ACM, 1980.  [17] gUnit -grammar \nunit testing. http://www.antlr. org/wiki/display/ANTLR3/gUnit+-+Grammar+ Unit+Testing. [18] R. G\u00f3mez, \nJ. C. Augusto, and A. Galton. Testing an event speci.cation language. In Software Engineering &#38; Knowl\u00adedge \nEngineering, SEKE 2001, pages 341 345, 2001. [19] P. Hamill. Unit Test Frameworks, chapter. Chapter 3: \nThe xUnit Family of Unit Test Frameworks. O Reilly, 2004. [20] Z. Hemel and E. Visser. Declaratively \nprogramming the mo\u00adbile web with mobl. In Proceedings of the 26th Annual ACM SIGPLAN Conference on Object-Oriented \nProgram\u00adming, Systems, Languages, and Applications, OOPSLA 2011, Portland, Oregon, USA, 2011. ACM. [21] \nP. Hudak. Modular domain speci.c languages and tools. In Software Reuse, ICSR 98. Computer Society, jun \n1998. [22] Jacks (Jacks is an Automated Compiler Killing Suite). http://sources.redhat.com/mauve/jacks. \nhtml. [23] L. C. L. Kats, M. de Jonge, E. Nilsson-Nyman, and E. Visser. Providing rapid feedback in generated \nmodular language en\u00advironments: adding error recovery to scannerless generalized-LR parsing. In S. Arora \nand G. T. Leavens, editors, Object-Oriented Programming, Systems, Languages, and Applica\u00adtions, OOPSLA \n2009, pages 445 464. ACM, 2009. [24] L. C. L. Kats, K. T. Kalleberg, and E. Visser. Interactive dis\u00adambiguation \nof meta programs with concrete object syntax. In M. van den Brand, B. Malloy, and S. Staab, editors, \nSoftware Language Engineering, SLE 2010, LNCS. Springer, 2011. [25] L. C. L. Kats and E. Visser. The \nSpoofax language work\u00adbench: rules for declarative speci.cation of languages and IDEs. In W. R. Cook, \nS. Clarke, and M. C. Rinard, editors, Object-Oriented Programming, Systems, Languages, and Ap\u00adplications, \nOOPSLA 2010, pages 444 463. ACM, 2010. [26] L. C. L. Kats, E. Visser, and G. Wachsmuth. Pure and declar\u00adative \nsyntax de.nition: paradise lost and regained. In W. R. Cook, S. Clarke, and M. C. Rinard, editors, Object-Oriented \nProgramming, Systems, Languages, and Applications, OOP-SLA 2010, pages 918 932. ACM, 2010. [27] A. Kossatchev \nand M. Posypkin. Survey of compiler testing methods. Programming and Computer Software, 31(1):10 19, \n2005. [28] H. Krahn, B. Rumpe, and S. V\u00f6lkel. Monticore: Modular development of textual domain speci.c \nlanguages. In R. F. Paige and B. Meyer, editors, Objects, Components, Models and Patterns, TOOLS EUROPE \n2008, volume 11 of Lecture Notes in Business Information Processing, pages 297 315. Springer, 2008. [29] \nR. L\u00e4mmel. Grammar testing. In H. Hu\u00dfmann, editor, Fun\u00addamental Approaches to Software Engineering, FASE \n2001, volume 2029 of LNCS, pages 201 216. Springer, 2001. [30] B. A. Malloy, J. F. Power, and J. T. Waldron. \nApplying soft\u00adware engineering techniques to parser design: the develop\u00adment of a C# parser. In SAICSIT \n02: 2002 research con\u00adference of the South African institute of computer scientists and information technologists \non Enablement through tech\u00ad nology. South African Institute for Computer Scientists and Information Technologists, \n2002. [31] G. Myers. The art of software testing, 2nd edition. Wiley-India, 2008. [32] R. W. Selby, editor. \nSoftware Engineering: Barry W. Boehm s Lifetime Contributions to Software Development, Manage\u00adment, and \nResearch. Wiley-Computer Society Press, 2007. [33] M. Strembeck and U. Zdun. An approach for the systematic \ndevelopment of domain-speci.c languages. Software: Prac\u00adtice and Experience, 39(15):1253 1292, 2009. \n[34] L. Tratt. Domain speci.c language implementation via compile-time meta-programming. Trans. Program. \nLang. Syst., 30(6), 2008. [35] E. Visser. Syntax De.nition for Language Prototyping. PhD thesis, University \nof Amsterdam, September 1997. [36] E. Visser. WebDSL: A case study in domain-speci.c lan\u00adguage engineering. \nIn R. L\u00e4mmel, J. Visser, and J. Saraiva, editors, Generative and Transformational Techniques in Soft\u00adware \nEngineering II, Int. Summer School, GTTSE 2007, vol\u00adume 5235 of LNCS, pages 291 373. Springer, 2007. \n[37] M. Voelter and K. Solomatov. Language modularization and composition with projectional language \nworkbenches illus\u00adtrated with MPS. In M. van den Brand, B. Malloy, and S. Staab, editors, Software Language \nEngineering, SLE 2010, LNCS. Springer, 2010. [38] B. Wichmann and Z. Ciechanowicz. Pascal compiler valida\u00adtion. \nJohn Wiley &#38; Sons, Inc. New York, NY, USA, 1983. [39] H. Wu, J. Gray, and M. Mernik. Grammar-driven \ngeneration of domain-speci.c language debuggers. Software: Practice and Experience, 38(10):1073 1103, \n2008. [40] X. Yang, Y. Chen, E. Eide, and J. Regehr. Finding and under\u00adstanding bugs in C compilers. \nIn Conference on Programming Language Design and Implementation, PLDI 2011. Press, June 2011.    \n\t\t\t", "proc_id": "2048066", "abstract": "<p>The reliability of compilers, interpreters, and development environments for programming languages is essential for effective software development and maintenance. They are often tested only as an afterthought. Languages with a smaller scope, such as domain-specific languages, often remain untested. General-purpose testing techniques and test case generation methods fall short in providing a low-threshold solution for test-driven language development. In this paper we introduce the notion of a language-parametric testing language (LPTL) that provides a reusable, generic basis for declaratively specifying language definition tests. We integrate the syntax, semantics, and editor services of a language under test into the LPTL for writing test inputs. This paper describes the design of an LPTL and the tool support provided for it, shows use cases using examples, and describes our implementation in the form of the Spoofax testing language.</p>", "authors": [{"name": "Lennart C.L. Kats", "author_profile_id": "81381609357", "affiliation": "Delft University of Technology, Delft, Netherlands", "person_id": "P2839146", "email_address": "l.c.l.kats@tudelft.nl", "orcid_id": ""}, {"name": "Rob Vermaas", "author_profile_id": "81375612565", "affiliation": "LogicBlox, Atlanta, GA, USA", "person_id": "P2839147", "email_address": "rob.vermaas@logicblox.com", "orcid_id": ""}, {"name": "Eelco Visser", "author_profile_id": "81100561215", "affiliation": "Delft University of Technology, Delft, Netherlands", "person_id": "P2839148", "email_address": "visser@acm.org", "orcid_id": ""}], "doi_number": "10.1145/2048066.2048080", "year": "2011", "article_id": "2048080", "conference": "OOPSLA", "title": "Integrated language definition testing: enabling test-driven language development", "url": "http://dl.acm.org/citation.cfm?id=2048080"}