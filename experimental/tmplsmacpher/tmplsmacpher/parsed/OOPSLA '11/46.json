{"article_publication_date": "10-22-2011", "fulltext": "\n Reducing Trace Selection Footprint for Large-scale Java Applications without Performance Loss Peng \nWu Hiroshige Hayashizaki Hiroshi Inoue Toshio Nakatani IBM Research pengwu@us.ibm.com,{hayashiz,inouehrs,nakatani}@jp.ibm.com \nAbstract When optimizing large-scale applications, striking the bal\u00adance between steady-state performance, \nstart-up time, and code size has always been a grand challenge. While recent advances in trace compilation \nhave signi.cantly improved the steady-state performance of trace JITs for large-scale Java applications, \nthe size control aspect of a trace compi\u00adlation system remains largely overlooked. For instance, us\u00ading \nthe DaCapo 9.12 benchmarks, we observe that 40% of traces selected by a state-of-the-art trace selection \nalgorithm are short-lived and, on average, each selected basic block is replicated 13 times in the trace \ncache. This paper studies the size control problem for a class of commonly used trace selection algorithms \nand proposes six techniques to reduce the footprint of trace selection without incurring any performance \nloss. The crux of our approach is to target redundancies in trace selection in the form of either short-lived \ntraces or unnecessary trace duplication. Using one of the best performing selection algorithms as the \nbaseline, we demonstrate that, on the DaCapo 9.12 benchmarks and DayTrader 2.0 on WebSphere Application \nServer 7.0, our techniques reduce the code size and com\u00adpilation time by 69% and the start-up time by \n43% while retaining the steady-state performance. On DayTrader 2.0, an example of large-scale application, \nour techniques also improve the steady-state performance by 10%. Categories and Subject Descriptors D.3.4 \n[Processors]: Compilers, Optimization, Run-time environments General Terms Algorithms, Experimentation, \nPerformance Keywords Trace selection and compilation, pro.ling, Java Permission to make digital or hard \ncopies of all or part of this work for personal or classroom use is granted without fee provided that \ncopies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. OOPSLA 11, October 22 27, 2011, Portland, Oregon, \nUSA. Copyright c &#38;#169; 2011 ACM 978-1-4503-0940-0/11/10. . . $10.00 1. Introduction 1.1 Trace compilation \nfor large applications How to effectively optimize large-scale applications has al\u00adways posed a great \nchallenge to compilers. Although method inlining expands the compilation scope of a method JIT, its effectiveness \ncan be limited when the compilation target lacks hot-spots and has numerous calling contexts and deep \ncall chains, all of which are common in large-scale Java ap\u00adplications. While trace-based compilation \nwas traditionally explored where mature JITs are absent, such as binary translators [1, 6, 7], easy-to-develop \nJITs [10, 21], and scripting lan\u00adguages [2, 4, 5, 8, 17], we explore trace compilation for Java, focusing \non large-scale applications. To our problem domain, the promise of trace-based compilation lies in its \npotential to construct better compilation scopes than method inlining does. In a trace compiler, a trace \nis a single-entry multiple-exit region formed out of instructions following a real execution path. The \nmost appealing trait of traces is its ability to span many layers of method boundaries, naturally achieving \nthe effect of partial inlining [20], especially in deep calling contexts. The challenges of trace compilation \nfor Java are also aplenty. As a .rst step, recent work in [13, 16] has signif\u00adicantly bridged the performance \ngap between trace compi\u00adlation and the state-of-the-art method compilation for Java, where a trace JIT \nis able to reach 96% of the steady-state performance of a mature product JIT on a suite of large-scale \napplications. This is achieved primarily by aggressively de\u00adsigning the trace selection algorithm to \ncreate larger traces. 1.2 Space Ef.ciency of Trace Selection A trace selection design needs to optimize \nall aspects of sys\u00adtem performance including steady-state performance, start\u00adup and compilation time \nand binary code size. While opti\u00admizing for steady-state performance often leads to selection algorithms \nthat maximize trace scope, such a design often in\u00adcreases start-up and compilation time, and binary code \nsize. The latter three all relate to one trace selection metrics as de.ned below. De.nition 1 Selection \nfootprint is de.ned as the cumulative size of all traces formed by a selection algorithm. We use space \nef.ciency to refer to a trace selection algo\u00adrithm s ability to maximize steady-state performance with \nminimal selection footprint. Space ef.ciency is especially important for large-scale applications where \nmemory sub\u00adsystem and compilation resources can be stressed. For ex\u00adample, code size bloat can degrade \nthe steady-state perfor\u00admance due to bad instruction cache performance. While space ef.ciency of trace \nselection was not ex\u00adtensively studied before as most trace JITs target small or medium size workloads, \nspace considerations have been in\u00adcorporated into existing selection algorithms. The common approaches \nfall into the following categories: Selecting from hot regions. Several trace JITs [2, 8, 10] select \ntraces only out of hot code regions, such as loops. This approach achieves superb space ef.ciency when \ndealing with codes with obvious hot spots, but not for large-scale applications, which often exhibit \nlarge, .at execution pro.le.  Limiting trace size. This approach limits individual trace size using \nheuristics expressed as trace termination con\u00additions, such as terminating trace recording at loop head\u00aders, \nat existing trace heads (known as stop-at-existing\u00adhead), or when exceeding buffer length. These heuris\u00adtics, \nhowever, sometimes can signi.cantly degrade the performance. For instance, we observe up to 2.8 times \nslower performance after applying the stop-at-existing\u00adhead heuristic. The space and performance impact \nof ex\u00adisting trace termination heuristics are summarized in Sec\u00adtion 6.  Trace grouping. This approach \ngroups linear traces so that common paths across linear traces can be merged [2, 8, 10, 14]. Existing \ntrace grouping algorithms focus solely on loop regions. However, they have yet to demon\u00adstrate the ability \nto reach the required level of selection coverage for large-scale non loop-intensive workloads.  When \ndealing with large-scale applications, existing ap\u00adproaches are either ineffective or insuf.cient in \nreducing se\u00adlection footprint, or otherwise degrade steady-state perfor\u00admance. In this paper, we focus \non improving the space ef\u00ad.ciency of trace selection by reducing selection footprint without degrading \nthe steady-state performance for large\u00adscale applications. 1.3 Key Observations We focus on a class \nof commonly used trace selection al\u00adgorithms, pioneered by Dynamo [1] and subsequently used in [5, 12, \n14, 17, 21] as well as the Java trace JIT mentioned earlier. In the paper, the speci.c selection algorithm \nused is derived from [16] and is referred to as the baseline algo\u00adrithm throughout the paper. 100% 90% \n80% 70% 60% 50% 40% 30% 20% 10% 0% DayTraderavrorabatikeclipsefoph2jythonluindexlusearchpmdsunflowtomcattradebeansxalangeomean \n Figure 1. Percentage of traces selected by the baseline algo\u00adrithm with less than 500 execution count \nduring steady-state execution. While the baseline algorithm is one of the best perform\u00ading of its kind, \nit exhibits serious space ef.ciency issues. Fig\u00adure 2 shows the traces selected by the baseline algorithm \nfor a simple example of 5 basic blocks. In total, the baseline al\u00adgorithm creates four traces (A-D) with \na selection footprint of 18 basic blocks and a duplication factor of 3.6. We identify two sources of \nspace inef.ciency in the base\u00adline algorithm that we will brie.y describe below. Formation of short-lived \ntraces refers to a phenomenon where some traces are formed but seldom executed. To quantify this effect, \nwe measured the execution count of traces formed by the baseline algorithm. Figure 1 shows that 38% traces \nformed for the DaCapo 9.12 benchmarks and the DayTrader benchmark have less than 500 execu\u00adtion counts \nduring steady-state runs.1 Intuitively, a trace becomes dead when its entry point is completely covered \nby traces formed later but whose entry points are topologically earlier. At that point, the original \ntrace is no longer executed. This is analogous to rendering a method dead after inlining the method to \nall its call-sites. Non-pro.table trace duplication refers to the duplication of codes within or across \ntraces that do not improve performance. While previous work focuses primarily on traces that are created \nunnecessarily long, we identi.ed another cause of the problem, that is, duplication due to convergence \nof a selection algorithm. In this context, convergence refers to the state where a working set is covered \ncompletely by existing traces so that no more new traces are created. 1 For cyclic traces, execution \ncounts include the number of times the trace cycles back to itself. Figure 2. A working example: trace \nformation by the baseline algorithm.  Figure 3. An example of low convergence due to form\u00ading max-length \ntraces: when the trace buffer size (3BB) is smaller than loop body size (4BB), the algorithm takes 4 \ntraces to converge. The problem stems from the fact that a trace can start and end at arbitrary program \npoints, which in the presence of tracing through cyclic paths could lead to pathological convergence. \nFigure 3 gives such an example: when the critical path of the loop body is too large to .t into a single \ntrace, many largely overlapping traces are formed, all of which start and end at slightly different program \npoints.  1.4 Evaluation and Contribution In this paper, we proposed six techniques to collectively ad\u00address \nthe problems of short-lived trace formation (in Sec\u00adtion 3) and trace duplication (in Section 4). We \nhave implemented the proposed techniques in a Java trace JIT based on IBM J9 JVM and JIT compiler. The \ntechniques are applied to the baseline algorithm from [16], which has been heavily optimized for steady-state \nperfor\u00admance. After applying our techniques, we are able to re\u00adduce the code size and compilation time \nby 69%, and the start-up time by 43%, and with no degradation to the steady\u00adstate performance of the \nDaCapo 9.12 benchmarks and a real-world large-scale Java application, DayTrader on top of IBM s Websphere \napplication server. On DayTrader, our techniques improved the steady-state performance by 10% due to \nbetter cache performance. The paper makes the following contributions: Deepen the understanding of the \nspace ef.ciency prob\u00adlem for a class of commonly used selection algorithms and expanded the solution \nspace.  Identify the problem of short-lived trace formation and proposed techniques to reduce short-lived \ntraces.  Identify new sources of trace duplication problem and proposed trace truncation to reduce non-pro.table \ndupli\u00adcation.  Demonstrate the effectiveness of our techniques on a set of large-scale Java applications. \n   2. The Baseline Trace Selection Algorithm The baseline algorithm is a one-pass selection algorithm \nderived from the one used in [16]. One-pass selection al\u00adgorithms form traces that are either straight-line \nor cyclic and contain no inner join or split points. One-pass trace se\u00adlection algorithms are the most \ncommon type of selection algorithms, which include NET [1], LEI [14], YETI [21], PyPy [5], and LuaJIT \n[17]. It has been demonstrated that one-pass trace selection can scale to large applications and achieve \nthe kind of coverage and performance comparable to those of a mature method JIT [13, 16]. A typical one-pass \ntrace selection algorithm consists of two steps. First, it identi.es a set of program points called potential \ntrace head, and monitors their execution counts. Once the execution count of a potential trace head exceeds \na threshold, trace recording is triggered, where it simply records every instruction following the execution \nof the trace head into a buffer called the recording buffer. Trace record\u00ading continues until a trace \ntermination condition is satis.ed, at which point, a new trace for the selected trace head is formed \nout of the recording buffer. The rest of the section describes these two components in detail. The rest \nof the paper uses a working example to illustrate various aspects of trace selection. Figure 2 shows \na code fragment that sums the number of characters in an array of strings (countChar) and the traces \nformed by the baseline selection algorithm. 2.1 Trace Head Selection The main driver of the baseline \nalgorithm is shown in Al\u00adgorithm 1. TraceSelection(e) is invoked every time the in\u00adterpreter dispatches \na control-.ow event, which can be 1) control-.ow bytecodes such as invoke, branch and return, or 2) when \nan exit is taken from a trace. The baseline algorithm identi.es two types of potential trace heads (lines \n17-19 in Algorithm 1). The .rst type is the target of a backward branch, such as bb2 on trace A in Figure \n2. This heuristic approximates loop headers without constructing a control-.ow graph. The second type \nis the target of a trace-exit (exit-head), such as bb4 on trace B in Figure 2.2 This allows traces to \nbe formed out of side branches of a trace. The algorithm pro.les the execution counts of potential trace \nheads such that those whose execution counts exceed a threshold, Th, trigger a new trace recording (lines \n21-27 in Algorithm 1). The algorithm assumes that counters and traces are kept in a PC-indexed global \nmap that is manip\u00adulated by getCounter and getTrace that return the counter and trace associated with \na PC, respectively, and by getAd\u00ad 2 Prior to enter the loop in Figure 2, trace 0 is already formed from \nthe entry point of String.length and includes codes following the return of String.length from a different \ncalling context. Therefore, When the execution enters trace 0 from the loop, a trace exit is taken at \nthe end of bb0. Algorithm 1 TraceSelection(e), baseline Input: Let e be a control-.ow event, PC be target \nPC of e, and buf be the recording buffer 1: /* trace recording and termination */ 2: if mode = record \nthen 3: if ShouldEndTrace(e) then 4: mode . default 5: create trace from buf then submit to compilation \n6: else 7: append(buf,e) 8: end if 9: return 10: end if 11: /* trace dispatch */ 12: if getTrace(PC)= \nnull then 13: dispatch to trace execution 14: return 15: end if 16: /* identify potential trace head \n*/ 17: if (isBackwardBranch(e) or isTraceExit(e)) then 18: ctr .getAddCounter(PC) 19: end if 20: /* trace \nhead selection and start recording */ 21: if (ctr . getCounter(PC))= null then 22: ctr ++ 23: if ctr \n> Th then 24: mode . record, buf .\u00d8 25: append(buf,e) 26: end if 27: end if 28: return dCounter that, \nin addition, allocates a new counter if none exists for a PC. The rest of the algorithm covers trace \ndispatch (lines 12\u00ad15 in Algorithm 1) and trace recording (lines 2-10 in Algo\u00adrithm 1), where its principle \ncomponent, trace termination conditions, are described in Section 2.2 and Algorithm 2. 2.2 Trace Termination \nconditions Trace termination conditions are the primary mechanism in a selection algorithm to control \nthe formation of a trace for a given trace head. Algorithm 2, ShouldEndTrace(e), describes the termina\u00adtion \nconditions used in the baseline algorithm. This particu\u00adlar choice of termination conditions is intended \nto maximize the number of cyclic traces and the length of linear traces, both of which imply large scope \nfor compilation and less trace transition overhead [13, 22]. Like any other selection algorithm, the \nbaseline algorithm must address the follow\u00ading key aspects of when to end a trace recording. Repetition \ndetection ends the recording when a cyclic rep\u00adetition path is detected in the recorded trace. Repetition \nAlgorithm 2 ShouldEndTrace(e), baseline Input: Let e be a control-.ow event, PC be target PC of e Output: \ntrue if the trace should be terminated at e,or false if trace recording should continue. 1: if PC is \nalready recorded on the trace and not a false\u00adloop then 2: if PC is not the trace head then 3: set trace \nlength be the index of the repeating PC 4: end if 5: return true 6: else if recording buffer over.ows \nthen 7: return true 8: else if e is an JNI call that cannot be included on trace then 9: return true \n 10: else if e is an irregular event (e.g., exception) then 11: return true 12: else 13: return false \n14: end if detection is necessary for the convergence of the selec\u00ad tion algorithm as well as the formation \nof cyclic traces. The baseline algorithm (lines 1-5 in Algorithm 2) detects repetition when current program \ncounter (PC) is already recorded in the buffer (stop-at-repeating-pc) [14] and when the cycle is not \na false loop [13]. When a repeating PC appears at the beginning of the recording buffer, such as traces \nA, B,and D in Figure 2, a cyclic trace is formed. Sometimes the repeating PC appears in the middle of \nthe recording buffer (rejoined), then the recording buffer is backtracked to the rejoined point to avoid \nintroducing inner join to the trace, such as trace C in Figure 2. Buffer over.ow ends the recording \nwhen the recording buffer reaches its size limit (lines 6-7 in Algorithm 2). Tracing out-of-scope ends \nthe recording when an event outside the tracing scope has occurred, such as invok\u00ading a native call. \nTracing scope is a property of the trace system and may not be violated as it may result in incorrect \ntraces. In our system, tracing beyond an exception throw or a JNI call is not allowed (lines 8-11 in \nAlgorithm 2).  2.3 Characteristics of the Baseline Algorithm The baseline algorithm is designed to maximize \nthe steady\u00ad state performance and reuses many existing heuristics in other systems. Table 1 summarizes \nthe basic characteristics of traces selected by the baseline algorithm for the DaCapo 9.12 and the DayTrader \n2.0 benchmarks (setup details in Section 5). Benchmark Description coverage # traces bbs/trace dup factor \ncall/trace avrora simulates programs running on AVR microcontrollers 100.0% 1853 43 11 27 batik produces \nSVG images based on unit tests in Apache Batik 99.7% 4817 43 11 26 eclipse executes the jdt performance \ntests for Eclipse 99.9% 27862 40 15 21 fop parse and format an XSL-FO .le and generate a PDF .le 99.7% \n6096 62 19 41 h2 a JDBCbench-like in-memory benchmark 100.0% 4124 57 17 34 jython interprets the pybench \nPython benchmark 99.7% 10109 77 20 50 luindex uses lucene to indexes a set of documents 99.9% 1376 31 \n7 17 lusearch uses lucene to do a text search of keywords 100.0% 1447 39 9 23 pmd analyzes Java classes \nfor a set of source code problems 98.6% 7029 56 25 33 sun.ow renders a set of images using ray tracing \n100.0% 1624 44 11 29 tomcat query against Tomcat to retrieve/verify webpages 99.4% 17155 49 13 28 tradebeans \nDayTrader via Java Beans on top of GERONIMO and h2 100.0% 8621 47 9 28 xalan transforms XML documents \ninto HTML 99.5% 3119 59 15 35 DayTrader DayTrader 2.0 on IBM WebSphere Application Server 7.0 100% 19809 \n69 15 40 geomean 99.7% 3869 49 13 30 Table 1. Characteristics of traces formed by the baseline algorithm \nfor DaCapo 9.12 and Daytrader benchmarks. Coverage is the percentage of total bytecodes executed from \ntraces; # traces is the number of compiled traces; bbs/trace is average number of basic blocks per trace; \ndup factor is the ratio between the number of bytecodes on traces and that of bytecodes in distinct basic \nblocks on traces; and call/trace is the number of invoke or return bytecodes per trace. One important \ncharacteristic is the coverage of a trace se\u00adlection. High coverage is a particular requirement for a \nbyte\u00adcode trace JIT like ours where more than ten-fold perfor\u00admance gap exists between being covered \n(compiled) and not covered (interpreted) by the trace JIT. As shown in Ta\u00adble 1, the algorithm achieves \nclose to 100% coverage at the steady-state, similar to that of the method JIT. Table 1 also shows other \nstatic characteristics of the trace selection. The number of traces selected ranges from 1400 to 27K, \nindicating the algorithm s ability to support large working sets. It is also observed that traces formed \nby the baseline algorithm are quite long with an average 49 basic blocks per trace and can span many \nlayers of method bound\u00adaries, where, on average, 30 invoke or return bytecodes are included per trace. \n 3. Reducing Short-lived Traces In this section, we propose a set of techniques that reduce selection \nfootprint without degrading the performance by targeting traces that are short lived. 3.1 Short-lived \nTrace Formation Short-lived trace formation refers to the phenomenon that the selection algorithm creates \ntraces that become dead short after their creation. For instance, the baseline algorithm .rst creates \ntrace B in Figure 2, and shortly after trace A is created.3 Since the head of trace B (bb3) is dominated \nby that of trace A (bb1) and is included in trace A, the creation of trace A renders trace B dead. 3 \nTrace B is created before A because the baseline algorithm identi.es bb3 as a potential trace head .rst, \nbefore the .rst backward branch targeting bb1 is taken. Intuitively, a trace becomes dead when the head \nof the trace is completely covered by later formed traces such that the trace is no longer dispatched. \nA formal de.nition of dead traces is given below. De.nition 2 An instruction x is an infeasible dispatch \npoint at time t,ifafter t, there is no invocation of TraceSelection(e) where the target of e is x. De.nition \n3 A trace A starting from x becomes dead at time t if, after t, x becomes an infeasible dispatch point. \nShort-lived trace formation is an inherent property of a trace selection algorithm that satis.es the \nfollowing two conditions. Condition 1 Two traces may be dispatched in the reverse order of how their \ncorresponding trace heads are selected. Condition 2 The head of an earlier trace can be part of a later \ntrace. The baseline algorithm satis.es both conditions. Condi\u00adtion 1 is a property of trace head selection. \nMost selection al\u00adgorithms satis.es this condition because there is no guaran\u00adteed ordering on how a \ntrace head is selected. Potential trace heads may accumulate counter values at different speed. For instance, \nbasic blocks at control-.ow join are executed more often than their predecessors. Condition 2, on the \nother hand, is a property of trace termination conditions. Certain termination conditions, such as stop-at-existing-head, \ncan prevent the condition to be satis.ed. Algorithm 3 TraceSelectionWithSizeControl(e), optimized The \nnew algorithm creates only 2 traces for the working Input: Let e be a control-.ow or an exact-bb event, \nPC be target PC of e, Ph and Th be the pro.ling and trace-head threshold, and buf be the recording buffer. \n1: /* trace recording */ 2: if mode = record then 3: if ShouldEndTrace(e) then 4: StructureTruncation(buf, \ne) 5: clear counters (if any) of bbs in buf 6: create trace from buf, mode . default 7: else 8: append(buf,e) \n9: end if 10: return 11: end if 12: /* trace path pro.ling */ 13: if curr tr = null then 14: if PC = \ngetStartPC(curr tr, curr pos) then 15: curr pos ++ 16: return 17: else 18: incExitFreq(curr tr,curr pos) \n19: curr tr . null 20: end if 21: end if 22: /* trace head selection and dispatch */ 23: if isBackwardBranch(e) \nor isTraceExit(e) then 24: if (tr . getTrace(PC)) =null then 25: if isCompiled(tr) then 26: dispatch \nto binary address of tr 27: else 28: /* enter trace path pro.ling mode */ 29: tr.entryFreq++ 30: if tr.entryFreq \n>Ph then 31: pro.leTruncation(tr) 32: submit tr to compilation queue 33: end if 34: curr pos ++, curr \ntr . tr 35: end if 36: return 37: end if 38: ctr . getAddCounter(PC) 39: if (+ + ctr) >Th then 40: mode \n. record, buf .\u00d8, append(buf, e) 41: end if 42: end if 43: return      3.2 Short-lived Trace Elimination \nexample, trace A and a trace that contains bytecode 0-4, with a selection footprint of 5 basic blocks \nand a duplication factor of 1. This is in contrast to the selection footprint of 18 basic blocks by the \nbaseline algorithm as shown in Figure 2. The rest of the section describes the new algorithm in detail. \n3.2.1 Constructing Precise Basic Blocks In the baseline algorithm, TraceSelection(e) is invoked every \ntime the interpreter executes a control-.ow bytecode. The bytecode sequence between consecutive control-.ow \nbyte\u00adcodes is called a dynamic instruction block. Dynamic in\u00adstruction blocks can be partially overlapping, \nsuch as bb1+2 and bb2 of trace C in Figure 2. We refer to such dynamic in\u00adstruction blocks as imprecise \nbasic blocks. Dynamic instruction blocks can trigger the formation of short-lived traces. Consider the \nformation of trace C and D in Figure 2. A trace recording starts from bytecode 0 and continues through \ntwo iterations of the loop. The recording is terminated when a repeating PC, bytecode 10, is detected \nin the middle of the recording buffer. Trace C is formed by backtracking the recorded trace to bytecode \n10,and then trace D is created from the target of the end-exit from trace C. Once trace C and D are formed, \ntrace A and B become dead because their respective entry points, bb2 and bb4 be\u00adcome infeasible dispatch \npoints. Such short-lived traces are caused by the termination condition that detects repetition by checking \nrepeating PCs (as line 1 of Algorithm 2) at control-.ow bytecodes. This termination condition works .ne \nonly when two distinct basic blocks are disjoint. Because of imprecise basic blocks, the baseline algorithm \ndetects bytecode 10 as the repeating PC in the recording buffer, whereas bytecode 4 is the actual .rst \nrepeating PC. To address this problem, we identify boundaries of pre\u00adcise basic blocks and call TraceSelection(e)at \nthe end of each precise basic block. The new algorithm correctly de\u00adtects that bytecode 4 is the .rst \nrepeating PC in the recording buffer. 3.2.2 Trace-head Selection Optimization The baseline algorithm \nperforms two lookups for each in\u00advocation of TraceSelection(e): 1) look up and dispatch the trace for \nevent e (lines 12-15 of Algorithm 1), and 2) look up and update the counter associated with e (lines \n21-27 of Algorithm 1). While this design dispatches traces and ac\u00adcumulate frequency counts as fast as \npossible, it can cause pathological formation of short-lived traces. Consider the formation of trace \nA and B. Despite the loop body having only a single execution path, the baseline In this section, we \nidentify the causes of short-lived trace for-algorithm identi.ed two potential trace heads, bb1 (the \ntarget mation in the baseline algorithm and propose a more space of a backward branch) and bb3 (the target \nof a side-exit from ef.cient algorithm, TraceSelectionWithSizeControl(e), as trace 0). Selecting multiple \npotential trace heads along one shown in Algorithm 3. critical path can result in short-lived traces \nbecause only the one that dominates the others along the same path can survive as the head of a long-lasting \ntrace. To address this problem, we propose to dispatch traces and update counters only when a trace \nexit or a backward branch is taken, shown as lines 24-41 of Algorithm 3. The algorithm has two nice properties: \n A counter is always available at the time of counter and trace lookup because if one is not available, \na new counter is allocated (as line 38 of Algorithm 3). This property bounds the number of failed counter \nand trace lookups per PC to Th, thus reduces lookup related runtime over\u00adhead. In a trace runtime, failed \nlookups can be expensive as well as pervasive when trace selection coverage is low such as during start-up \ntime.  It imposes a partial order on how potential trace heads are selected. For instance, the restriction \nof dispatching traces only at trace-exit and backward branch events pre\u00advents trace 0 from being dispatched \nin the loop body. As a result, bb3 is not marked as a potential trace head.  3.2.3 Clearing Counters \nalong Recorded Trace The third technique we propose is a simple heuristic: when a new trace is formed, \nwe clear the counter value of any basic block on the trace ( if any) when the basic block is topologically \nafter the head of the recorded trace (line 5 of Algorithm 3). One cause of short-lived traces is that \ntrace head selection is based on counter values combining execution counts of a given PC along all paths. \nBy clearing counter values of any potential trace head on a newly formed trace, execution counts contributed \nby the newly formed trace are excluded. The rationale of using topological ordering to decide whether \nto clear a counter is to prevent a short-lived trace from clearing the counter of a long-lived trace. \nWe use a simple heuristic to approximate topological ordering: a ba\u00adsic block x is considered topologically \nearlier than a basic block y,if x and y belong to the same method and if the bytecode index of x is less \nthan that of y. 3.2.4 Trace Path Pro.ling While the techniques proposed before all reduce the forma\u00adtion \nof short-lived traces, the next technique, trace path pro\u00ad.ling, prevents short-lived traces from being \ncompiled after they become dead. The algorithm for trace path pro.ling is shown as lines 13-21 and 29-34 \nin Algorithm 3. It works as follows. 1. A newly formed trace is not immediately submitted to compilation, \ninstead it is kept in a nursery and inter\u00adpreted for a short time. 2. When the trace is being interpreted, \nthe interpreter records the entry and exit counts for each basic block on the trace, but does not update \ncounters associated with any basic block on the trace and does not dispatch to other traces.  3. A \nnursery trace is compiled only if its entry count ex\u00adceeds a prede.ned threshold, at which point, the \npro.l\u00ading mode ends. Traces that never leave the nursery are dead traces. Intuitively, trace path pro.ling \ncan identify dead traces because it mimics the execution of a compiled trace, there\u00adfore short-lived \ntraces that manifest in a binary trace exe\u00adcution also manifests in trace path pro.ling. The more fun\u00addamental \nexplanation of this effect has to do with more ac\u00adcurate accounting by excluding execution frequencies \nfrom infeasible dispatch points. In trace path pro.ling, the imple\u00admentation detail of not updating counters \nassociated with any basic blocks on the trace is crucial to dead trace elim\u00adination. It has the effect \nof preventing execution counts from other paths to be counted towards that of a potential trace head \nor the entry count of a nursery trace. A second key aspect of trace path pro.ling is that while traces \nmay be formed out of sync with respect to the topolog\u00adical ordering of trace heads, program execution \nalways fol\u00adlows topological orders. As such, during trace path pro.ling, traces that start from a topologically \nearlier program point are dispatched .rst, thus render those starting from topolog\u00adically later program \npoints dead.  4. Reducing Trace Duplication In this section, we study the problem of code duplication \nacross traces and propose techniques to reduce unnecessary duplication. 4.1 The problem of Trace Duplication \nTrace duplication refers to the phenomenon that a program point is included in many traces in a given \ntrace formation. The degree of duplication by the baseline algorithm is quite signi.cant. As shown in \nTable 1, in our benchmarks, on average, each distinct PC is duplicated 13 times across all traces. Trace \nduplication is an inherent property of trace selec\u00adtion and often a desirable feature as it re.ects a \nselection algorithm s innate ability to specialize. Fundamentally, du\u00adplication happens when tracing \nthrough the merge points of a .ow graph, such as loop headers and the entry and return points of methods \nwith multiple call-sites. The inlining ef\u00adfect of trace selection, for instance, is the result of tracing \nthrough the entry point of a method. However, not every form of trace duplication is bene.\u00adcial. In addition \nto short-lived traces, we identify three other forms of trace duplication that are likely not bene.cial: \nDuplication due to slow convergence refers to duplication caused by convergence issues of a selection \nalgorithm. One particular form of convergence problem is triggered by max-length traces as shown in the \nexample in Fig\u00adure 3. Max-length traces have the property that those that start from different program \npoints often end at differ\u00adAlgorithm 4 StructureTruncation(buf,bb)  Input: Let buf be the trace recording \nbuffer with n bbs, ML be the maximal trace length, and bb be the bb executed after buf[n - 1] Output: \nreturns the length of the truncated trace. 1: if buf is cyclic or n =1 then 2: return n 3: end if 4: \nfor i . 1 to n - 1 do 5: if isLoopHeader(buf[i]) then 6: let L be the loop whose header is buf[i] 7: \nif isloopExited(L, i, buf)= false then 8: if trunc-at-entry-edge and isEntryEdge(buf[i - 1],buf[i]) then \n9: return i 10: end if 11: if trunc-at-backedge and isBackEdge(buf[i - 1],buf[i]) then 12: return i 13: \nend if 14: if trunc-at-loop-header then 15: return i 16: end if 17: end if 18: end if 19: end for 20: \nif n = MLandisTraceHead(bb)= false then 21: for i . n - 1 to 1 do 22: if isTraceHead(buf[i]) then 23: \nLet tr be the trace whose head is buf[i] 24: if match(buf[i : n),tr[0:n - i]) and isMethod\u00adReturned(i, \nbuf)=false then 25: return i 26: end if 27: end if 28: end for 29: end if 30: return n ent program points. \nWhen combined with tracing along cyclic paths, this property can cause slow convergence of a selection \nalgorithm. Loop-related duplication refers to duplication as the result of tracing through a common \ntype of control-.ow join, loop headers. One form of unnecessary duplication happens when trac\u00ading through \nthe entry-edge of a loop. This is analogous to always peeling the .rst iteration of a loop into a trace. \nAnother form of duplication happens when tracing through the backedge or exit-edge of a loop (also known \nas tail duplication). Trace segment with low utilization refers to the case where the execution often \ntakes a side-exit before reaching the tail segment of a trace. The most common scenario of this problem \nmanifests when a mega-morphic control-.ow bytecode, such as the return bytecode from a method with many \ncalling con\u00adtexts, appears in the middle of the trace. For example, trace A in Figure 2 contains a return \nbytecode from String.length that has many different calling con\u00adtexts. As a result, the return bytecode \non trace A is a hot side-exit and a good candidate for truncation.  4.2 Trace Truncation We propose \ntrace truncation that uses structure or pro.ling information to determine the most pro.table end point \nof a trace. We propose two types of trace truncation. One is structure-based that applies truncation \nbased on static prop\u00aderties of a recorded trace (shown as StructureTruncation in Algorithm 4). The other \nis pro.le-based that truncates based on trace path pro.ling information (lines 31 in Algorithm 3). Traditionally, \na selection algorithm controls duplication by imposing additional termination conditions. Compared to \nthis approach, trace truncation has the advantage of being able to look ahead and use the knowledge on \nthe path beyond to decide the most pro.table trace end-point. Since trace truncation may shorten lengths \nof active traces, care must be taken to minimize degradation to per\u00adformance. For this consideration, \nwe de.ne the following guidelines of where not to apply truncation: Do not truncate cyclic traces. Cyclic \ntraces can capture large scopes of computation that are disproportional to its size, therefore the performance \ncost of a bad truncation may outweigh the bene.t of size reduction.  Do not truncate between a matching \npair of method entry and return. The rule preserves the amount of partial in\u00adlining in a trace, which \nis a key indicator of trace quality in our system.  Do not truncate at non trace-heads. This rule prevents \ntruncation from introducing new potential trace heads (thus new traces) and worsening the convergence \nof trace selection.  4.2.1 Structure-based Truncation Structure-based truncation is applied immediately \nafter a trace recording and ends before the trace is created. We pro\u00adpose the following heuristics for \nstructure-based truncation. The .rst three exploit loop structures for truncation. The last one is speci.cally \ndesigned for max-length traces with no loop-based edges. trunc-at-loop-entry-edge that truncates at the \nentry-edge to the .rst loop on the trace with more than one iteration. This is based on the consideration \nthat peeling the .rst iteration of a loop is likely not pro.table.  trunc-at-loop-backedge that truncates \nat the backedge to the .rst or last loop on the trace with more than one iteration. This is based on \nthe consideration that the backedge is a critical edge that forms cycles. Therefore, truncation at backedge \nmay improve the convergence of the algorithm. This heuristic allows cyclic traces to be formed on loop \nheaders, but not on other program points in the loop.  trunc-at-loop-header that truncates at the header \nof the .rst/last loop on the trace with more than one iteration. This is a combination of the previous \ntwo heuristics.  trunc-at-last-trace-head that truncates at the last location on the trace, where 1) \nit is the head of an existing trace, 2) the existing trace matches the portion of the trace to be truncated, \n3) it is not in between a matching pair of method enter and return.  The structure-based truncation \nalgorithm is given in Al\u00adgorithm 4, where isMethodReturned checks whether a po\u00adtential truncation point \nis between the entry and return of a method on the trace; isLoopExited(L,i,buf) assumes that the ith \nbasic block in buf is the header of loop L and checks if the remaining portion of the trace exits from \nthe body of L4,and isEntryEdge (isBackEdge) checks whether an edge is the loop entry-edge (backedge). \n 4.2.2 Pro.le-based Truncation Pro.le-based truncation uses the pro.ling information col\u00adlected by trace \npath pro.ling to truncate traces at hot side\u00adexits (as line 31 in Algorithm 3). For a given trace, trace \npath pro.ling collects the entry count to a trace as well as trace exit count of each basic block on \nthe trace, i.e., the number of times execution leaves a trace via this basic block. From trace exit counts, \none can compute the execution count of each basic block on the trace. Pro.le\u00adbased trace truncation uses \na simple heuristic: for a given basic block x on a trace, if the entry count of x on the trace is smaller \nthan a prede.ned fraction of the entry count of the trace, we truncate the trace at x. In our implementation, \nwe use a truncation threshold of 5%.  5. Evaluation 5.1 Our Trace JIT System Overview Figure 4 shows \na component view of our trace JIT, which is built on top of IBM J9 JVM and JIT compiler [11]. Traces \nare formed out of Java bytecodes and compiled by the J9 JIT, which is extended to compile traces. Our \ntrace JIT supports both trace execution and interpretation, as well as all major functionality of the \nmethod JIT. Compilation is done by a dedicated thread, similar to the method JIT. 4 In our implementation, \nwe check if the remaining portion of the trace includes codes from the same method but outside the loop \nbody or whether the owning method of the loop header has returned. Both indicate that loop L has been \nexited. Figure 4. Overview of our trace JIT architecture Trace head threshold 500 (BB exec freq) Trace \nbuffer length 128 (BBs) Structure trunc. (rejoined) 1st loop-header Structure trunc. (max-length) 1st \nloop-header or last trace-head Trace pro.ling window 128 (trace entry freq) Pro.le trunc. threshold 5% \n Table 2. Trace selection algorithm parameters. The trace compiler enables a subset of warm level opti\u00admizations \nof the baseline method JIT such as various (par\u00adtial) redundancy elimination optimizations, (global) \nregis\u00adter allocation, and value propagation. Our system is aggres\u00adsively optimized to reduce runtime \noverhead due to trace ex\u00adecution and trace monitoring (including trace linking opti\u00admizations). The current \nimplementation also has limitations compared to the method JIT. For example, the trace JIT does not support \nrecompilation. It does not support escape anal\u00adysis and enables only a subset of loop optimizers in the \nJ9 JIT. Detailed design of the trace JIT is described in [16]. Table 2 summarizes some of the key parameters \nof the selection algorithm for the evaluation. 5.2 Experiment Setup Experiments are conducted on a 4-core, \n4GHz POWER6 processors with 2 SMT threads per core. The system has 16 GB of system memory and runs AIX \n6.1. For the JVM, we use 1 GB for Java heap size with 16MB large pages and the generational garbage collector. \nWe used two benchmarks: DaCapo 9.12 benchmark [3] running with the default data size. We did not include \nthe tradesoap benchmark because the baseline system with the method-JIT some\u00adtimes failed for this benchmark. \nDayTrader 2.0 [19] running on IBM WebSphere Applica\u00adtion Server version 7.0.0.13 [15]. This is an example \nof large-scale Java applications. For DayTrader, the DB2 database server and the client emulator ran \non separate machines. In this paper, we use the following metrics to evaluate our techniques. For each \nresult, we report the average of 16 runs along with the 95% con.dence interval. Selection footprint: \nthe total number of bytecodes in com\u00adpiled traces. Compiled binary code size: the total binary code size. \nSteady-state execution time: For DaCapo 9.12, we exe\u00adcuted 10 iterations for eclipse and 25 iterations \nfor the other benchmarks, and reported the average execu\u00adtion time of the last 5 iterations. For DayTrader, \nwe ran the application for 420 seconds that includes 180-second client ramp-up but excludes setup and \ninitialization, and used the average execution time per request during the last 60 seconds. Start-up \ntime: the execution time of the .rst iteration for DaCapo 9.12, and the time spent before the WebSphere \nApplication Server becomes ready to serve for Day-Trader. Compilation time: the total compilation time. \n 5.3 Reduction in Selection Footprint We evaluated the six techniques proposed in this paper as summarized \nin Table 3. First, we measured the impact of each individual technique on selection footprint. Figure \n5 shows the normalized selection footprint when we apply each technique to the baseline. We observe that \neach technique is effective in reducing selection footprint, with the average reduction ranging from \n12% (exact-bb) to 40% (head-opt). The only exception is when applying head-opt to jython, where selection \nfoot\u00adprint increases by 2%. Second, we measured the combined effects of the tech\u00adniques in reducing selection \nfootprint, as shown in Figure 6. In this and following .gures, the techniques are combined according \nto the natural order (left to right) by which they are applied during the selection. For example, the \nbar +struct\u00adtrunc stands for the case where we apply exact-bb, head-opt, and struct-trunc to the baseline. \nWith all techniques applied, the average selection foot\u00adprint is reduced to 30% of the baseline s. We \nalso observe that each technique is able to further reduce selection foot\u00adprint over the ones applied \nbefore it. Figure 8 shows a detailed breakdown on where the reduc\u00adtion in selection footprint comes from. \n The bottom bar our algo w/ all-opt is the selection foot\u00adprint of our algorithm relative to the baseline \ns.  Short-lived traces eliminated represent the total bytecode size of short-lived traces eliminated \nby our optimizations.  Structure truncated BCs and pro.le truncated BCs ac\u00adcount for bytecodes eliminated \nby structure-based and pro.le-based truncation, respectively.  100% 90% 80% 70% 60% 50% 40% 30% 20% \n10% 0%  others eliminated  profile-trunc BCs structure-trunc BCs short-lived traces eliminated  \nour algo w/ all-opts DayTraderavrorabatikeclipsefoph2jythonluindexlusearchpmdsunflowtomcattradebeansxalangeomean \n Figure 8. Breakdown of selection footprint reduction nor\u00admalized to that of the baseline. Others represent \nthe rest of the reduction, which is likely due to improved convergence of the baseline algorithm that \ngenerates fewer new traces.  5.4 Impact on System Performance Figure 7 shows the combined impact of \nour techniques on compiled binary code size. With all our techniques com\u00adbined, the compiled code size \nis reduced to 30% of the base\u00adline s, which is consistent with the degree of reduction on selection footprint. \nFigure 9 shows the combined impact of our techniques on steady-state execution time. The steady-state \nperformance was unchanged on average after all techniques are applied, with a maximal degradation of \n4% for luindex. It is also notable that the steady-state performance of DayTrader is improved by 10%. \nThis is because L2 cache misses were reduced and thus clock per instruction was improved, due to reduced \ncode size. This shows that code size control is not only important for memory reduction itself but also \nimportant for the steady-state performance in large\u00adscale applications. Figure 10 and Figure 11 show \nthe normalized start-up time and compilation time when the techniques are applied in combination, respectively. \nUsing our techniques, compi\u00adlation time and start-up time was reduced to 32% and 57% of the baseline \ns, respectively. 5.5 Discussions Our results show that the reduction in selection footprint is linear \nto that of compilation time and binary code size. Start-up time is closely related to but not linear \nto selec\u00adtion footprint because it is in.uenced by other factors such as the ratio of interpretation \noverhead to native execution and how fast bytecodes are captured as traces and com\u00adpiled to binaries. \nOnly very large-scale applications, such as DayTrader and eclipse, experience an improvement in Figure \n6. Selection footprint (normalized to the baseline) after combining techniques over the baseline (shorter \nis better). Normalized Selection Footprint Normalized Selection Footprint DayTrader avrora batik eclipse \nfop h2 jython luindex lusearch pmd sunflow tomcat tradebeans xalan geomean Figure 5. Selection footprint \n(normalized to the baseline) when applying each technique (shorter is better). 2.0 baseline +exact-bb \n+head-opt +clear-counter +struct-trunc +prof +prof w/ trunc 1.5 1.0 0.5 0.0 DayTrader avrora batik eclipse \nfop h2 jython luindex lusearch pmd sunflow tomcat tradebeans xalan baseline exact-bb head-opt clear-counter \nstruct-trunc prof prof w/ trunc 1.5 1.0 0.5 0.0 geomean Name Description Described in Main effect exact-bb \nexact basic block construction Section 3.2.1 Reduced short-lived traces &#38; duplication head-opt counter/trace \nlookup at backward branch and exit-heads Section 3.2.2 Reduced short-lived traces struct-trunc structure-based \ntruncation Section 4.2.1 Reduced short-lived traces &#38; duplication clear-counter clearing counters \nof potential trace heads on a recorded trace Section 3.2.3 Reduced short-lived traces pro.le trace pro.ling \nSection 3.2.4 Reduced short-lived traces prof-trunc trace pro.ling with pro.le-based truncation Section \n4.2.2 Reduced duplication Table 3. Summary of evaluated techniques 2.0 better). steady-state performance \nas the result of selection footprint reduction. Eliminating short-lived traces have the biggest impact \nin footprint reduction. Of all the techniques that eliminate short-lived traces, ensuring proper ordering \nby which to se\u00adlect trace heads (head-opt) addresses the root cause of short lived traces. We would also \nlike to point out that some of the pro\u00adposed techniques may potentially degrade start-up perfor\u00admance \nbecause they either reduce the scope of individual traces (e.g., truncation) or prolongs the time before \na byte\u00adcode is executed from a binary trace (e.g., pro.ling). But our results show empirically that footprint \nreduction in gen\u00aderal improves start-up performance because, for large-scale workloads, compilation speed \nis likely a more critical bot\u00adtleneck to the start-up time than other factors. However, the cost-bene.t \neffects may change depending on the compila\u00adtion resource of the trace compiler, the coverage require\u00adment \nof the selection algorithm, and the characteristics of the workload.  6. Comparing with Other Selection \nAlgorithms While our techniques are described in the context of the baseline algorithm, many design choices \nare also common in other trace selection algorithms. Table 4 summarizes im\u00adportant aspects of trace selection \ndiscussed in the paper for all existing trace selection algorithms. Figure 7. Binary code size (normalized \nto the baseline) after combining techniques over the baseline (shorter is better). Figure 9. Steady-state \nexecution time (normalized to the baseline) after combining techniques over the baseline (shorter is \nNormalized Steady-state Execution TimeNormalized Generated Binary Code Size DayTrader avrora batik eclipse \nfop h2 jython luindex lusearch pmd sunflow tomcat tradebeans xalan geomean 2.0 baseline +exact-bb +head-opt \n +clear-counter +struct-trunc +prof +prof w/ trunc methodJIT 1.5 1.0 0.5 0.0 DayTrader avrora batik eclipse \nfop h2 jython luindex lusearch pmd sunflow tomcat tradebeans xalan geomean 0.0 0.5 1.0 1.5 2.0 baseline \n+exact-bb +head-opt +clear-counter +struct-trunc +prof +prof w/ trunc methodJIT 6.1 One-pass selection \nalgorithms We .rst compare our techniques with size control heuristics used in existing one-pass selection \nalgorithms, all of which are based on termination conditions. Stop-at-existing-head terminates a trace \nrecording when the next instruction to be recorded is the head of an exist\u00ading trace. This heuristic \nwas .rst introduced by NET [1]. It is most effective size-control heuristic because it min\u00adimizes duplication \nacross traces. It does not generate short-lived traces either because Condition 2 of short\u00adlived trace \nformation is no longer satis.ed. However, stop-at-existing-head can have signi.cant impact of per\u00adformance \ndue to reduced trace scope. Figure 12 shows the relative execution time and code size of stop-at-existing-head \nnormalized to that of our algorithm. It shows that stop-at-existing-head excels at space ef.ciency, but \ndegrades the performance by up to 2.8 times. A main source of the degradation comes from the re\u00adduction \nin the inlining effect of the trace selection. In particular, once a trace is formed at the entry point \nof a method, stop-at-existing-head prevents the method to be inlined into any later traces that invoke \nthe method. Stop-at-loop-boundary terminates a trace recording at loop boundaries, such as loop headers \nor loop exit-edges. Vari\u00ad ations of this heuristic are used in PyPy, TraceMonkey, HotpathVM, SPUR, and \nYETI. We compared stop-at-loop-head heuristic, which is one type of stop-at-loop-boundary and terminates \na trace at loop headers (.gure not included in the paper). On the Figure 10. Start-up time (normalized \nto the baseline) after combining techniques over the baseline (shorter is better). Figure 11. Compilation \ntime (normalized to the baseline) after combining techniques over the baseline (shorter is better). DayTrader \navrora batik eclipse fop h2 jython luindex lusearch pmd sunflow tomcat tradebeans xalan geomean  DayTrader \navrora batik eclipse fop h2 jython luindex lusearch pmd sunflow tomcat tradebeans xalan geomean  System \nour baseline SPUR HotpathVM LuaJIT TraceMonkey PyPy YETI NET Merrill+ LEI [13, 16] [2] [10] and [9] [17] \n[8] [5] [21] [1] [18] [14] Target Program Java CIL Java Lua JavaScript Python Java Binary Java Binary \nMulti-pass or one-pass selection one multi multi N/A multi one one one one one Trace loophead Y YY Y \nY Y YYY Y Head exithead Y YY Y Y N/A YY Y Y Condition method entry Y Y Trace stop-at-any-backward-branch \nY Y Termination stop-at-repeating-pc Y Y Y Y Condition Stop-at-loop-back-to-head Y Y Y Y stop-at-existing-trace-heads \nY N/A call N/A Y Y Y stop-when-leaving-static-scope loop method loop loop N/A N/A method stop-when-exceeding-max-size \nY Y Y Y Y Y Y Y Y Y stop-at-native-method-call Y N/A N/A N/A N/A N/A N/A N/A Imprecise BB problem Y N/A \nY Y Y Table 4. Comparison of Trace Selection Algorithms 3.0 2.5 2.0 1.5 1.0 0.5 0.0 DayTraderavrorabatikeclipsefoph2jythonluindexlusearchpmdsunflowtomcattradebeansxalangeomean \nExecution Time Binary Code Size Figure 12. Execution time and code size of stop-at-exitsing\u00adhead relative \nto our algorithm. DaCapo 9.12 benchmark and DayTrader, stop-at-loop\u00adhead is 3% slower than ours in the \nsteady-state perfor\u00admance, and its binary code size is 2.45 times of ours. Stop-at-return-from-method-of-trace-head \nterminates a recording when the execution leaves the stack frame of the trace-head. This heuristic is \nused in PyPy, Hot\u00adpathVM and Merrill et al. This heuristic (.gure not included in the paper) is on average \n6% slower in steady-state performance compared to ours. The binary code size is 1.6 times of ours, ranging \nfrom 1.22 times (h2) to 2.4 times (sunflow).  6.2 Multi-pass selection algorithms Multi-pass trace selection \nalgorithms form traces after mul\u00adtiple recordings and combine individual traces into groups. Traces formed \nby such algorithms can allow split paths or inner-join within a trace (group). While direct comparison \nwith multi-pass selection algorithms is beyond the scope of this work, multi-pass selection algorithms \nconceptually have more compact footprint than one-pass selection algorithms because paths can be joint \nin a trace group. On the other hand, existing multi-pass selection algo\u00adrithms are designed primarily \nwith loops in mind. SPUR, HotpathVM, and TraceMonkey [2, 8, 10] are three such sys\u00adtems, all of which \nbuild trees of traces anchored at loop head\u00aders. For non-loop intensive workloads, some computation may \nhappen outside any loops, some may occur in loops whose bodies are too large to be captured into one \ntrace tree. It is an open question whether existing multi-pass se\u00adlection algorithms can achieve high \ncoverage on large-scale non loop-intensive applications.  7. Conclusion Designing a balanced trace selection \nalgorithm largely boils down to meeting the competing needs of creating larger traces to maximize performance \nand reducing the selection footprint to minimize resource consumption. This paper fo\u00adcuses on the latter \nproblem of controlling the footprint of trace selection. In this work, we discovered some of the most \nintriguing aspects of trace selection algorithms. Our .rst insight comes from the observation of trace \nchurning , where a signi.\u00adcant amount of traces, shortly after being created, are no longer executed. \nA careful study of the baseline algorithm reveals pitfalls of several common-sense trace selection de\u00adsign \nchoices that could lead to pathological formation of short-lived traces. Our second insight comes from \nstudying the cause of excessive duplication in traces that are not necessarily short\u00adlived. While trace \nduplication has been studied before in the context of tail duplication, we identi.ed new sources of unnecessary \nduplication due to poor convergence property of a trace selection algorithm. By addressing these sources \nof footprint inef.ciency in common trace selection algorithms, our techniques are able to reduce the \nselection footprint and the binary code size to one third of the baseline and the startup time to slightly \nover half of the baseline with no performance loss. In one large-scale enterprise workload based on a \nproduction web server, our techniques improve the steady-state performance by 10%, due to improved instruction \ncache performance.  References [1] BALA,V., DUESTERWALD,E., AND BANERJIA,S. Dy\u00adnamo: A Transparent Runtime \nOptimization System. In Proceedings of Conference on Programming Language Design and Implementation (PLDI) \n(June 2000). [2] BEBENITA,M., BRANDNER,F., FAHNDRICH,M., LO-GOZZO,F., SCHULTE,W., TILLMANN,N., AND VENTER, \nH. SPUR: a trace-based JIT compiler for CIL. In Proceedings of International Conference on Object Oriented \nProgramming Systems Languages and Applications (OOPSLA) (2010), pp. 708 725. [3] BLACKBURN,GARNER,HOFFMANN,KHANG,MCKINLEY, \nBENTZUR,DIWAN,FEINBERG,FRAMPTON,GUYER, AND HOSKING. The DaCapo benchmarks: java benchmarking development \nand analysis. In Proceedings of Conference on Object Oriented Programming Systems Languages and Applications \n(OOPSLA) (Oct. 2006). [4] BOLZ,C., CUNI,A.,FIJALKOWSKI,M., LEUSCHEL,M., PEDRONI,S., AND RIGO, A. Allocation \nremoval by partial evaluation in a tracing JIT. In Proceedings of Workshop on Partial Evaluation and \nProgram Manipulation (2011). [5] BOLZ,C., CUNI,A.,FIJALKOWSKI,M., AND RIGO,A. Tracing the Meta-Level: \nPyPy s Tracing JIT Compiler. In Workshop on Implementation, Compilation, Optimization of Object-Oriented \nLanguages and Programming Systems (2009). [6] BRUENING,D., AND AMARASINGHE, S. Maintaining Con\u00adsistency \nand Bounding Capacity of Software Code Caches. In Proceedings of International Symposium on Code Generation \nand Optimization (CGO) (Mar. 2005). [7] BRUENING,D., GARNETT,T., AND AMARASINGHE,S. An Infrastructure \nfor Adaptive Dynamic Compilation. In Proceedings of International Symposium on Code Generation and Optimization \n(CGO) (Mar. 2003). [8] GAL,A., EICH,B., SHAVER,M., ANDERSON,D., MAN-DELIN,D., HAGHIGHAT,M. R., KAPLAN,B.,HOARE,G., \nZBARSKY,B., ORENDORFF,J., RUDERMAN,J.,SMITH, E. W., REITMAIER,R., BEBENITA,M., CHANG,M., AND FRANZ, M. \nTrace-based just-in-time type specialization for dynamic languages. In Proceedings of Conference on Programming \nLanguage Design and Implementation (PLDI) (2009), pp. 465 478.         [9] GAL,A., AND FRANZ, \nM. Incremental dynamic code gen\u00aderation with trace trees. Tech. rep., University of California Irvine, \nNovember 2006. [10] GAL,A., PROBST,C., AND FRANZ,M. HotPathVM: An Effective JIT Compiler for Resource-constrained \nDevices. In Proceedings of International Conference on Virtual Execution Environments (VEE) (June 2006). \n[11] GRCEVSKI,N., KIELSTRA,A., STOODLEY,K., STOOD-LEY,M., AND SUNDARESAN, V. Java just-in-time compiler \nand virtual machine improvements for server and middleware applications. In Proceedings of International \nConference on Virtual Execution Environments (VEE) (June 2004). [12] GUO,S., AND PALSBERG, J. The essence \nof compiling with traces. In Proceedings of International Symposium on Principles of Programming Languages \n(POPL) (2011), pp. 563 574. [13] HAYASHIZAKI,H., WU,P.,INOUE,H., SERRANO,M. J., AND NAKATANI, T. Improving \nthe performance of trace\u00adbased systems by false loop .ltering. In Proceedings of International Conference \non Architectural Support for Programming Languages and Operating Systems (ASPLOS) (March 2011). [14] \nHINIKER,D., HAZELWOOD,K., AND SMITH,M. D. Improving region selection in dynamic optimization sys\u00adtems. \nIn Proceedings of 38th International Symposium on Microarchitecture (MICRO) (Dec. 2005). [15] IBM CORPORATION. \nWebSphere Application Server. http://www-01.ibm.com/software/ webservers/appserv/was/. [16] INOUE,H.,HAYASHIZAKI,H., \nWU,P., AND NAKATANI,T. A Trace-based Java JIT Compiler Retro.tted from a Method\u00adbased Compiler. In Proceedings \nof International Symposium on Code Generation and Optimization (CGO) (April 2011). [17] LuaJIT design \nnotes in lua-l mailing list. http: //lua-users.org/lists/lua-l/2008-02/ msg00051.html. [18] MERRILL,D., \nAND HAZELWOOD, K. Trace fragment selection within method-based jvms. In Proceedings of International \nConference on Virtual Execution Environments (VEE) (June 2008). [19] THE APACHE SOFTWARE FOUNDATION. \nDayTrader. http://cwiki.apache.org/GMOxDOC20/ daytrader.html. [20] WHALEY, J. Partial Method Compilation \nusing Dy\u00adnamic Pro.le Information. In Proceeding of Conference on Object-Oriented Programming Systems, \nLanguages and Applications (OOPSLA) (Oct. 2001), pp. 166 179. [21] ZALESKI,M., DEMKE-BROWN,A., AND STOODLEY, \nK. YETI: a graduallY Extensible Trace Interpreter. In Proceedings of International Conference on Virtual \nExecution Environments (VEE) (2007), pp. 83 93. [22] ZHAO,C., WU,Y., STEFFAN,J., AND AMZA, C. Length\u00adening \nTraces to Improve Opportunities for Dynamic optimiza\u00adtion. In 12th Workshop on Interaction between Compilers \nand Computer Architectures (Feb 2008).            \n\t\t\t", "proc_id": "2048066", "abstract": "<p>When optimizing large-scale applications, striking the balance between steady-state performance, start-up time, and code size has always been a grand challenge. While recent advances in trace compilation have significantly improved the steady-state performance of trace JITs for large-scale Java applications, the size control aspect of a trace compilation system remains largely overlooked. For instance, using the DaCapo 9.12 benchmarks, we observe that 40% of traces selected by a state-of-the-art trace selection algorithm are short-lived and, on average, each selected basic block is replicated 13 times in the trace cache.</p> <p>This paper studies the size control problem for a class of commonly used trace selection algorithms and proposes six techniques to reduce the footprint of trace selection without incurring any performance loss. The crux of our approach is to target redundancies in trace selection in the form of either short-lived traces or unnecessary trace duplication.</p> <p>Using one of the best performing selection algorithms as the baseline, we demonstrate that, on the DaCapo 9.12 benchmarks and DayTrader 2.0 on WebSphere Application Server 7.0, our techniques reduce the code size and compilation time by 69% and the start-up time by 43% while retaining the steady-state performance. On DayTrader 2.0, an example of large-scale application, our techniques also improve the steady-state performance by 10%.</p>", "authors": [{"name": "Peng Wu", "author_profile_id": "81408599112", "affiliation": "IBM Research, Yorktown Heights, NY, USA", "person_id": "P2839267", "email_address": "pengwu@us.ibm.com", "orcid_id": ""}, {"name": "Hiroshige Hayashizaki", "author_profile_id": "81381610007", "affiliation": "IBM Research, Tokyo, Japan", "person_id": "P2839268", "email_address": "hayashiz@jp.ibm.com", "orcid_id": ""}, {"name": "Hiroshi Inoue", "author_profile_id": "81100619710", "affiliation": "IBM Research, Tokyo, Japan", "person_id": "P2839269", "email_address": "inouehrs@jp.ibm.com", "orcid_id": ""}, {"name": "Toshio Nakatani", "author_profile_id": "81100311827", "affiliation": "IBM Research, Tokyo, Japan", "person_id": "P2839270", "email_address": "nakatani@jp.ibm.com", "orcid_id": ""}], "doi_number": "10.1145/2048066.2048127", "year": "2011", "article_id": "2048127", "conference": "OOPSLA", "title": "Reducing trace selection footprint for large-scale Java applications without performance loss", "url": "http://dl.acm.org/citation.cfm?id=2048127"}