{"article_publication_date": "10-22-2011", "fulltext": "\n Exploiting Coarse-Grain Speculative Parallelism Hari K. Pyla, Calvin Ribbens, Srinidhi Varadarajan Center \nfor High-End Computing Systems Department of Computer Science Virginia Tech {harip, ribbens, srinidhi}@cs.vt.edu \nAbstract Speculative execution at coarse granularities (e.g., code\u00adblocks, methods, algorithms) offers \na promising program\u00adming model for exploiting parallelism on modern archi\u00adtectures. In this paper we \npresent Anumita, a framework that includes programming constructs and a supporting run\u00adtime system to \nenable the use of coarse-grain speculation to improve program performance, without burdening the pro\u00adgrammer \nwith the complexity of creating, managing and re\u00adtiring speculations. Speculations may be composed by \nspec\u00adifying surrogate code blocks at any arbitrary granularity, which are then executed concurrently, \nwith a single winner ultimately modifying program state. Anumita provides ex\u00adpressive semantics for winner \nselection that go beyond time to solution to include user-de.ned notions of quality of so\u00adlution. Anumita \ncan be used to improve the performance of hard to parallelize algorithms whose performance is highly \ndependent on input data. Anumita is implemented as a user\u00adlevel runtime with programming interfaces to \nC, C++, For\u00adtran and as an OpenMP extension. Performance results from several applications show the ef.cacy \nof using coarse-grain speculation to achieve (a) robustness when surrogates fail and (b) signi.cant speedup \nover static algorithm choices. Categories and Subject Descriptors D.1.3 [Programming Techniques]: Concurrent \nProgramming Parallel program\u00adming; D.3.3 [Programming Languages]: Language Con\u00adstructs and Features Concurrent \nprogramming structures; D.3.4 [Programming Languages]: Processors Run-time environments General Terms \nAlgorithms, Design, Languages, Measure\u00adment and Performance Permission to make digital or hard copies \nof all or part of this work for personal or classroom use is granted without fee provided that copies \nare not made or distributed for pro.t or commercial advantage and that copies bear this notice and the \nfull citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. OOPSLA 11, October 22 27, 2011, Portland, Oregon, \nUSA. Copyright c &#38;#169; 2011 ACM 978-1-4503-0940-0/11/10. . . $10.00 Keywords Speculative Parallelism, \nCoarse-grain Specula\u00adtion, Concurrent Programming and Runtime Systems 1. Introduction As processor architectures \nevolve from fast single core de\u00adsigns to multi/many core designs using multiple simpler cores (lower \nclock frequency, shorter pipelines), there is in\u00adcreasing pressure on programmers to use application \nlevel threading to improve performance. While some applications are amenable to simple parallelization \ntechniques, a large body of algorithms and applications are inherently hard to parallelize due to execution \norder constraints in.icted by data and control dependencies. Furthermore, for a signi.cant number of \napplications, performance (a) is highly sensitive to input data and (b) does not scale well to 100 s \nof cores. Our objective is to provide programmers with a simple tool for exploiting parallelism in such \napplications. In the arsenal of concurrent programming techniques, speculative execution is used in a \nvariety of contexts to improve per\u00adformance. Low level .ne-grain speculation employed by the hardware \nand compiler (e.g., branch prediction, prefetch\u00ading) is a proven technique. Software transaction systems \nare premised on speculative execution of potentially coarse\u00adgrain code blocks. More generally, we believe \nspeculative execution relying on optimistic concurrency at coarse gran\u00adularities (e.g., code-blocks, \nmethods, algorithms) offers a promising programming model for exploiting parallelism for many hard-to-parallelize \napplications on multi and many core architectures. In this paper we focus on coarse-grain speculation \nas a means to achieve parallelism. We provide a simple program\u00adming model to express at any arbitrary \ngranularity, the parts of an application that may be executed speculatively. Writing correct shared memory \nparallel programs is a challenging task in itself [23], and detecting concurrency bugs (e.g., data races, \ndeadlocks, order violations, atomicity violations) is an extremely dif.cult problem [41]. Hence, we do \nnot want to burden the programmer with the additional responsibilities of using low level threading primitives \nto create speculative control .ows, manage rollbacks and perform recovery ac\u00adtions in the event of mis-speculations. \n We present Anumita (guess in Sanskrit), a simple specu\u00adlative programming framework where multiple \ncoarse-grain speculative code blocks execute concurrently, with the re\u00adsults from a single speculation \nultimately modifying the pro\u00adgram state. Our goal is to make speculation a .rst class parallelization \nmethod for hard-to-parallelize and input de\u00adpendent code blocks. Anumita consists of a shared library, \nwhich implements the framework API for common type\u00adunsafe languages including C, C++ and Fortran, and \na user\u00adlevel runtime system that transparently (a) creates, instan\u00adtiates, and destroys speculative control \n.ows, (b) performs name-space isolation, (c) tracks data accesses for each spec\u00adulation, (d) commits \nthe memory updates of successful spec\u00adulations, and (e) recovers from memory side-effects of any mis-predictions. \nIn the context of high-performance com\u00adputing applications, where the OpenMP threading model is prevalent, \nAnumita also provides a new OpenMP pragma to naturally extend speculation into an OpenMP context. Anumita \nworks by associating the memory accesses made by each speculation .ow (e.g., an instance of a code block \nor a function) in a speculation composition (loosely, a collection of possible code blocks that execute \nconcur\u00adrently). Anumita localizes these memory updates and pro\u00advides isolation among speculation .ows \nthrough privatiza\u00adtion of address space. Ultimately, a single speculation .ow within a composition is \nallowed to modify the program state. Anumita simpli.es the notion of speculative parallelism and relieves \nthe programmer from the subtleties of concurrent programming. The framework is designed to support a \nbroad category of applications by providing expressive evaluation criteria for speculative execution \nthat go beyond time to so\u00adlution to include arbitrary quality of solution criteria. An\u00adumita supports \nmultithreaded applications and sequential applications alike. Anumita is implemented as a language independent \nruntime system and its use requires minimal modi.cations to application source code. We evaluate Anumita \nusing microbenchmarks and real applications from several domains. Our experimental results indicate that \nAnumita is capable of signi.cantly improving the performance of applications by leveraging speculative \nparallelism. Programmable speculation is susceptible to lim\u00aditations of speculative execution and may \ncome at an ex\u00adpense. Speculation requires additional resources (pipelines, cores, memory) to handle speculative \n.ows and which may consume more energy and power. In this paper we show that speculative execution of \nseveral alternative (or surrogate ) code blocks incurs at worst a modest overhead in terms of energy \nconsumption, and can frequently yield improvements in energy consumption, when compared to the use of \na single statically chosen surrogate. The rest of the paper is organized as follows. Section 2 outlines \nthe motivation for this work. Section 3 presents the programming model and constructs used to express \ncoarse\u00adgrain speculative execution. Section 4 presents how these constructs can be implemented ef.ciently \nwithout sacri.c\u00ading performance, portability and usability. Section 5 presents our experimental evaluation. \nSection 6 surveys the related work, Section 7 describes future directions and Section 8 presents our \nconclusions. 2. Motivating Problems Coarse-grain speculative parallelism is most useful for ap\u00adplications \nwith two common characteristics: (1) there exist multiple possible surrogates (e.g., code blocks, methods, \nal\u00adgorithms, algorithmic variations) for a particular computa\u00adtion, and (2) the performance (or even \nsuccess) of these sur\u00adrogates is problem dependent, i.e., relative performance can vary widely from problem \nto problem, and is not known a priori. Whether or not there exist ef.cient parallel imple\u00admentations \nof each surrogate is an orthogonal issue to the use of coarse-grain speculation. If only sequential imple\u00admentations \nexist, speculation provides a degree of useful parallelism that is not otherwise available. If parallel \nsur\u00adrogate implementations do exist, speculation still provides resilience to hard-to-predict performance \nproblems or fail\u00adures, while also providing an additional level of parallelism to take advantage of growing \ncore counts, e.g., by assigning a subset of cores to each surrogate rather than trying to scale a single \nsurrogate across all cores. We discuss two motivating examples in detail. (Perfor\u00admance results for these \nproblems are given in Section 5). In graph theory, vertex coloring is the problem of .nding the smallest \nset of colors needed to color a graph G = (V, E) such that no two vertices vi,vj . V with the same color \nshare an edge e. Graph coloring problems arise in several domains including job scheduling, bandwidth \nallo\u00adcation, pattern matching and compiler optimization (regis\u00adter allocation). Several state-of-the-art \napproaches that solve this problem employ probabilistic and meta-heuristic tech\u00adniques, e.g., simulated \nannealing, tabu search and variable neighborhood search. Typically, such algorithms initialize the graph \nwith a random set of colors and then employ a heuristic algorithm to attempt to color the graph using \nthe speci.ed number of colors. Depending on the input graph, the performance of these techniques varies \nwidely. Obvi\u00adously, there will be cases where no coloring can be found (when the speci.ed number of colors \nis too small) by some or all methods. In addition to this sensitivity to the input, algorithms for the \ngraph coloring problem are hard to par\u00adallelize due to inherent data dependancies. Parallel imple\u00admentations \nthat exist employ a divide and conquer strategy by dividing the graph into subgraphs and applying color\u00ading \ntechniques on the subgraphs in parallel. During reduc\u00adtion, con.icting subgraphs are recolored. Despite \nsuch ef\u00adforts, the challenge still persists to develop ef.cient parallel algorithms for vertex coloring. \nAs a second example, consider the numerical solution of partial differential equations (PDEs). This is \none of the most common computations in high performance computing and is a dominant component of large \nscale simulations arising in computational science and engineering applications such as .uid dynamics, \nweather and climate modeling, structural analysis, and computational geosciences.  The large, sparse \nlinear systems of algebraic equations that result from PDE discretizations are usually solved using preconditioned \niterative methods such as Krylov solvers [33]. Choosing the right combination of Krylov solver and preconditioner, \nand setting the parameter values that de.ne the details of those preconditioned solvers, is a challenge. \nThe theoretical convergence behavior of precon\u00additioned Krylov solvers on model problems is well under\u00adstood. \nHowever, for general problems the choice of Krylov solver, preconditioner, and parameter settings is \noften made in an ad hoc manner. Consequently, iterative solver perfor\u00admance can vary widely from problem \nto problem, even for a sequence of problems that may be related in some way, e.g., problems corresponding \nto discrete time steps in a time\u00addependent simulation. In the worst case, a particular iterative solver \nmay fail to converge, in which case another method must be tried. The most conservative choice is to \nabandon iterative methods completely and simply use a direct fac\u00adtorization, i.e., some variant of Gaussian \nElimination (GE). Suitably implemented, GE is essentially guaranteed to work, but in most cases it takes \nconsiderably longer than the best preconditioned iterative method. The problem is that the best iterative \nmethod is not known a priori. One could list many other examples that are good candi\u00addates for coarse-grain \nspeculation. Even for a simple prob\u00adlem such as sorting, where theoretical algorithmic bounds are well \nknown, in practice the runtime of an algorithm de\u00adpends on a variety of factors including the amount \nof input data (algorithmic bounds assume asymptotic behavior), the sortedness of the input data, and \ncache locality of the imple\u00admentation [3]. 3. Speculation Programming Model For a coarse-grain speculation \nmodel to be successful, it should satisfy several usability and deployability constraints. First, the \nmodel should be easy to use, with primarily se\u00adquential semantics, i.e., the programmer should not have \nto worry about the complexities and subtleties of concurrent programming. Speculation is not supported \nby widely used languages or runtime systems today. Hence, in order to ex\u00adpress speculation, the programmer \nis burdened with creat\u00ading and managing speculation .ows using low-level thread primitives [28]. Second, \nthe speculation model should en\u00adable existing applications (both sequential and parallel) to be easily \nextended to exploit speculation. This includes sup\u00adport for existing imperative languages, including \npopular type-unsafe languages such as C and C++. Third, the model should be expressive enough to capture \na wide variety of speculation scenarios. Finally, to ensure portability across platforms, the speculation \nmodel should not require changes to the operating system. Furthermore, we need to accom\u00adplish these objectives \nwithout negatively impacting the per\u00adformance of applications that exploit speculation. A general use \ncase for Anumita is illustrated in Fig\u00adure 1. The example shows an application with three threads, two \nof which enter a speculative region. (The simplest case would involve a single-threaded code that enters \na single speculative region.) Each sequential thread begins execution non-speculatively until a speculative \nregion is encountered, at which time n speculative control .ows are instantiated, where n is programmer-speci.ed. \nEach .ow executes a dif\u00adferent surrogate code block. We refer to this construct as a concurrent continuation, \nwhere one control .ow enters a speculative region through an API call and n speculative control .ows \nemerge from the call. Anumita achieves par\u00adallelism by executing the n speculative .ows concurrently. \nIn Figure 1, the concurrent continuation out of thread 0 is a composition of three surrogates, while \nthe continuation out of thread 2 has two surrogates. Note that individual surro\u00adgates may themselves \nbe multithreaded, e.g., surrogate esti\u00admation in the continuation .owing out of thread 2. Although not \nshown in the .gure, Anumita also supports nested specu\u00adlation, where a speculative .ow in turn creates \na speculative composition. To mitigate the impact of introducing speculation into the already complex \nworld of concurrent programming, no additional explicit locking is introduced by the speculation model. \nIn other words, a programmer using the Anumita API to add speculation to a single-threaded application \ndoes not have to worry about locking or synchronization of any kind. Of course, if the original application \nwas already multi\u00adthreaded, then locking mechanisms may already be in place, e.g., to synchronize among \nthe three threads in Figure 1 in non-speculative regions. Each speculative .ow operates in a context \nthat is iso\u00adlated from all other speculations, thereby ensuring the safety of concurrent write operations. \nAnumita presents a shared memory model, where each speculative .ow is exactly iden\u00adtical to its parent \n.ow in that it shares the same view (albeit write-isolated) of memory, i.e., global variables, heap and \nmore importantly, the stack. The Anumita programming model provides a .exible mechanism for identifying \nthe winner and committing the results of a speculation. The .rst .ow to successfully commit its results \nis referred to as the winning speculation. However, the decision to commit can be made in a variety of \nways. The model easily supports the simplest case, where the .rst .ow to achieve some programmer-de.ned \ngoal cancels the re\u00admaining speculative .ows and committs its updates to the parent .ow, which resumes \nexecution at the point of com\u00admit. Surrogate estimation illustrates this case in Figure 1. Alternately, \nspeculative .ows may choose to abort them\u00adselves if they internally detect a failure mode of some kind, \nFigure 1. A typical use case scenario for composing coarse-grain speculations. Anumita supports both \nsequential and multi\u00adthreaded applications.  e.g., surrogate adaptive in the .gure, when area != 42. \nMore generally, each surrogate may de.ne success in terms of an arbitrary user-de.ned evaluation function, \npassed to an evaluation interface supplied by the parent .ow (labeled evaluation context in Figure 1). \nThe evaluation context safely maintains state that it can use to steer the composi\u00adtion, deciding which \nsurrogates should continue and which should terminate. In our example, surrogates conservative and extrapolation \nuse the evaluation interface to communi\u00adcate with their parent .ow. 3.1 Program Correctness Any concurrent \nprogramming model needs well-de.ned se\u00admantics for propagation of memory updates. Anumita sup\u00adports concurrency \nat three levels: (1) between surrogates in a speculative composition, (2) between threads in a single \nmultithreaded surrogate, and (3) between threads in non\u00adspeculative regions of an existing multithreaded \napplication. We consider each in turn. Unlike the traditional threads model, where any con.ict\u00ading accesses \nto shared memory must be properly synchro\u00adnized, Anumita avoids synchronization and its associated complexity \nby providing isolation among speculative .ows through privatization of the shared address space (global \ndata and heap). Furthermore, a copy of the stack frame of the parent .ow is passed to each speculative \n.ow. Since up\u00addates are isolated, con.icting accesses do not require syn\u00adchronization. Anumita s commit \nconstruct implements a rel\u00adatively straightforward propagation rule: for a given com\u00adposition, only the \nupdates of a single winning speculative .ow are made visible to its parent .ow at the completion of a \ncomposition. Furthermore, compositions within a single control .ow are serialized, in that a control \n.ow cannot start a speculative composition without completing prior compo\u00adsitions in program order. Cumulatively, \nthese two proper\u00adties are suf.cient to ensure program correctness in sequen\u00adtial applications (a single \ncontrol .ow) even in the presence of nested speculations. We do not present a formal proof of Figure \n2. Pseudo code for composing speculations using the programming constructs exposed by Anumita. In the \nabsence of an evaluation function, the fastest surrogate (by time to solution) wins.  correctness here; \nhowever, the rationale behind the proof is that since the updates of exactly one of the valid outcomes \nis committed and since each speculation was isolated while arriving at this result, relaxing the requirements \nof explicit synchronization does not affect program correctness. Surrogates in Anumita may themselves \nbe multithreaded, requiring lock based concurrency control between threads, e.g., surrogate estimation \nin Figure 1. Since surrogates are replacements for each other, we would not expect a surrogate to have \nsynchronization dependencies with one of its sibling surrogate, e.g., estimation with monte-carlo in \nthe .gure. Hence the correctness of multithreaded surrogates reduces to the standard case of threaded \nshared-memory concurrent programming. While the above properties ensure program correctness for concurrent \ncontinuations .owing out of a single control .ow, we also need to de.ne how speculative .ows can be composed \nin a multithreaded environment. Anumita allows multiple speculative and non-speculative regions to execute \nconcurrently, e.g., the regions associated with threads 0 and 2, along with thread 1 in Figure 1. However, \nthe model does not support synchronization between speculative .ows from different speculative regions, \nor between speculative .ows and other non-specualtive application threads. (We note that this restriction \nis also true for value speculation systems such as [28].) Hence, correctness of such codes stems from \nthe Figure 3. Pseudo code for evaluating speculations in Anu\u00admita. correctness of the original multithreaded \ncode, since each speculative region exhibits transaction-like semantics with respect to other threads, \ni.e., no memory updates from a given speculative region are visible to other threads until the commit \nprocess .nishes, at which point all the updates are complete, and control resumes in a non-speculative \nregion. Externally visible I/O actions are not handled by Anumita in its current form. We are working \non extending Anumita to support disk I/O. However, Anumita performs speculation aware memory management \nand garbage collection from failed speculations. This mechanism correctly hides the side\u00adeffects of system \ncalls such as sbrk etc.  3.2 Syntax and Semantics Figures 2 and 3 show pseudocode corresponding to \nthe sce\u00adnario illustrated in Figure 1 for composing speculations us\u00ading Anumita. Table 1 de.nes the Anumita \nAPI. A specula\u00adtive composition is initialized bya call to init speculation, which returns a speculation \ncontext. A composition is in\u00adstantiated by a call to begin speculation, which imple\u00adments a concurrent \ncontinuation of the parent .ow. Each speculative .ow in the concurrent continuation is exactly identical \nto its parent .ow in that it shares the same view of memory, but is isolated from other concurrent speculative \n.ows. In order to distinguish speculative .ows from each other, we associate each speculative .ow with \na unique rank. This notion of rank is identical to ranks in MPI and thread number in OpenMP. A speculation \nmay query its rank (0 to n-1) in order to map a particular unit of work to itself. The parent .ow then \nenters an evaluation context, where it waits (descheduled) for evaluation requests from its speculative \n.ows. To implement an interface for evaluation, the call to begin speculation takes an argument that \nspeci.es the size of a memory region that is used for communication between speculative .ows and the \nparent evaluation con\u00adtext. Each speculative .ow receives a distinct memory re\u00adgion of the speci.ed size; \nthis region is shared between a speculative .ow and the parent evaluation context. Periodi\u00adcally, a speculative \n.ow can request an evaluation using the evaluate speculation call, passing the parent interme\u00addiate results \nusing the shared memory region. This call syn\u00adchronously transfers control to the evaluation context \n(i.e., the idled parent .ow), which executes the evaluation func\u00adtion and returns a status indicating \nwhether the speculation calling the evaluation should continue or abort execution. The evaluation context \nmay also use the intermediate re\u00adsults to cancel other speculations based on the results of the current \nevaluation, for instance, when the progress of one surrogate is signi.cantly better than another within \nthe same composition. In essence, the evaluation mechanism enables pruning of surrogates based on a user-de.ned \nnotion of re\u00adsult quality. On completing execution, a surrogate terminates the speculative region by \ncalling commit speculation. The .rst call to commit speculation succeeds, canceling all other speculations \nin the composition and propagating its execution context to the parent .ow, which then resumes execution \nat the point of commit. Selecting by time to solu\u00adtion (fastest surrogate wins) is trivially implemented \nby not specifying an evaluation function, as shown in Figure 2. In this case the .rst surrogate to commit \nwould succeed and cancel its siblings. For completeness, the API also supports an abort speculation call \nthat can be used by a surro\u00adgate to terminate itself if it detects that it is not making progress or \nhas reached some failure mode. We also pro\u00advide a cancel speculation call that can be used by any Figure \n4. Composing speculations in OpenMP using the OpenMP extensions built on top of the programming con\u00adstructs \nexposed by Anumita. Anumita s source-source trans\u00adlator expands the speculate pragma to begin-commit \ncon\u00adstructs. surrogate to terminate any other surrogate. This can be use\u00adful, for example, in a case \nwhere a subset of surrogates can be pruned from the composition when one member of that subset meets \nsome condition. Many scienti.c applications use OpenMP directives for shared memory programming rather \nthan the underlying POSIX threads interface. To support such applications, we provide extensions to OpenMP \nin the form of a new OpenMP pragma that provides a natural interface to spec\u00adulation. Figure 4 illustrates \nthe OpenMP syntax for cre\u00adating a composition. The speculate pragma is scoped be\u00adtween an open and close \nbrace ({ and }), with an implicit commit speculation at the end of the speculate pragma. In traditional \nOpenMP programming, name space isolation is achieved through explicit variable scoping (e.g., private, \nshared, etc.). To simplify programming, the Anumita run\u00adtime automatically isolates speculative .ows \nwithout requir\u00ading explicit private scoping.  3.3 Overhead Anumita achieves low runtime overhead since \nspeculative .ows are isolated and mispredictions cause the memory updates of the failed speculation to \nbe discarded as opposed to rollback recovery. The memory overhead is proportional Table 1. Programming \nconstructs exposed by Anumita for leveraging speculation. For brevity, C++ and Fortran interfaces are \nomitted.  Programming Constructs Description speculation t* init speculation(void) int begin speculation(speculation \nt *spec context, int num spec, size t mem) int get rank(speculation t *spec context) int get size(speculation \nt *spec context) int commit speculation(speculation t *spec context) int abort speculation(speculation \nt *spec context) int cancel speculation(speculation t *spec context, int rank) int evaluate speculation(speculation \nt *spec context, evaluate t *evaluation fn, void *ptr) void * get ir memory(speculation t *spec context) \nint (*evaluation fn)(speculation t *spec context, void *ptr) Initialize a speculative composition. Begins \na speculative composition. Arguments are composi\u00adtion context, number of speculations and size of memory \nto allocate for storing intermediate results used in evaluating speculations. Gets the rank of the calling \nspeculative .ow. Gets the number of speculative .ows in a composition. Attempts to commit the state of \nthe calling speculative .ow. Aborts the calling speculative .ow. Cancels (terminates) the speculation \n.ow with a rank of rank . Used to invoke an evaluation of the intermediate results of the calling speculative \n.ow. Intermediate results are passed through ptr . Returns a pointer to the calling speculative .ow s \nmemory region used to store intermediate results for evaluation. Signature of the user de.ned evaluation \nfunction. to the write-set of all the speculative .ows, which is typically much smaller than the read \nset. Given N speculative .ows with the write-set of each .ow being W pages, the memory overhead is O(NW). \n4. Implementation The Anumita implementation consists of a shared library that exposes our API and a \nruntime system. The OpenMP interfaces are implemented using source-to-source transla\u00adtion. To ensure \nease of deployment, Anumita is implemented completely in user-space with no modi.cations to the oper\u00adating \nsystem. The rest of this section describes the Anumita runtime in detail. 4.1 Shared Address Space In \nthe POSIX threads model, each thread has a distinct stack and threads of a process share their address \nspace. In contrast, distinct processes are fully isolated from each other and execute in separate virtual \naddress spaces. Neither of these models satis.es the isolation and selective state sharing requirements \nimposed by Anumita. Intuitively, we need an execution model that provides the ability to selectively \nshare state between execution contexts. To create the notion of a shared address space among pro\u00adcesses, \nwe implemented the cords abstraction .rst proposed in [29]. The constructor in our runtime (a shared \nlibrary) tra\u00adverses through the link map of the application (ELF binary) at runtime and identi.es the \nglobal data (.bss and .data) sec\u00adtions, i.e., the zero initialized and uninitialized data and non\u00adzero \ninitialized data, respectively. The runtime then unmaps these sections from the loaded binary image in \nmemory, maps them from a SYSV memory mapped shared memory .le and reinitializes these sections to the \noriginal values. This mapping to a shared memory .le is done by the main process before its execution \nbegins at main. Specu\u00adlative .ows are then instantiated as processes (we use the clone() system call \nin Linux to ensure that .le mappings are shared as well) and a copy of the address space of the par\u00adent \nis created for each instantiation of a speculation. Conse\u00adquently, the speculations inherit the shared \nglobal data map\u00adping. Hence any modi.cations made by a process to global data are immediately visible \nto all processes. Such a tech\u00adnique guarantees that all the processes have the same view of global data, \nsimilar to a threads model. In essence, this tech\u00adnique creates a set of processes that are semantically \nidenti\u00adcal to threads, but operate in distinct virtual address spaces. By controlling the binding to \nthe shared memory mapping, data can be selectively isolated or shared based on the re\u00adquirements of the \nspeculation model. To implement a shared heap, we modi.ed Doug Lea s dlmalloc [15] allocator to operate \nover shared memory map\u00adpings so that the allocated memory is visible to all processes. Our runtime system \nprovides global heap allocation by shar\u00ading memory management metadata among processes using the same \nshared memory backing mechanism used for .data and .bss sections. Hence any process can allocate memory \nthat is visible and usable by other processes. If a process ac\u00adcesses memory that is not mapped in its \naddress space, it results in a segmentation violation (a map error). Our run\u00adtime system handles this \nsegmentation violation by consult\u00ading memory management metadata to check if the reference is to a valid \nmemory address allocated by a different process. If so, it maps the shared memory .le associated with \nthe memory, thereby making it available. Note that such an ac\u00adcess fault only occurs on the .rst access \nto a memory region allocated by a different process, and is conceptually similar to lazy memory allocation \nwithin an operating system.  To ensure program correctness, speculative .ows within the same composition \nshould appear as a concurrent contin\u00aduation of the parent execution context. To achieve this, our runtime \nsystem ensures that the base address of the stack in a speculative .ow is identical to that of the parent \nspecula\u00adtion. The default size of a stack is 8MB. When composing a speculation, the runtime saves only \nthe stack frame of the parent speculation (not the entire 8MB) and each speculation within a composition \nuses a copy of this stack frame for ex\u00adecution. Each speculative .ow is now identical to its parent .ow, \nthereby creating a concurrent continuation. Since each speculative .ow is implemented as a process, it \nis important to note that in UNIX process semantics, each process is created with its own copy of the \ndata segment of the shared libraries. Consequently, by default the runtime is not shared among speculation \n.ows. To circumvent this problem and to maintain a shared and consistent view of the runtime, each newly \ncreated process automatically executes an initialization routine that maps the shared state of the Anumita \nruntime.  4.2 Speculative Composition To mitigate the costs of creating and terminating specula\u00adtive \n.ows, the runtime instantiates a con.gurable pool of speculative .ows in the Anumita library constructor \nbefore the main process begins its execution. Additional specula\u00adtive .ows are created as necessary. \nThis pool of speculative .ows is initially idle (blocked), waiting for work from the main process. On \nthe termination of a speculative .ow, it is returned back to the pool. In principle this is similar to \na worker thread pool used to mitigate the performance impact of thread creation. To instantiate a speculation, \nthe parent .ow .rst saves its current stack frame and execution context (setjmp) before waking the speci.ed \nset of speculative .ows from the pool. Upon waking, each speculation adjusts its execution context (longjmp), \nrestores its stack to that of the parent .ow and isolates its shared virtual memory address (VMA) before \nstarting execution. The speculations begin their execution as a concurrent continuation of the begin \nspeculation construct. The parent .ow then enters an evaluation context and waits for messages from the \nmembers of the speculative composition. The parent .ow may be woken up under three scenarios. First, \nif a speculative .ow completes its assigned task it executes a commit speculation. A call to commit speculation \nis mutually exclusive to prevent race conditions on commits from multiple speculations. The .rst speculation \nto invoke commit is designated the winner. The winning speculation saves its current execution context \nand its stack frame so as to allow its parent to continue from the commit point. Additionally, the winning \nspeculation at\u00adtaches (ptrace) itself to the remaining sibling speculations and alters their instruction \npointer to point to a cleanup rou\u00adtine. In the cleanup routine, it performs an inclusion (prop\u00adagation \nof privatized updates) of the shared virtual memory address (VMA) and frees any dynamically allocated \nmem\u00adory it allocated before returning to the pool. The winning speculation then commits its changes, \nwakes up its parent with a winning speculation message and joins the worker pool. Upon waking up, the \nparent .ow adjusts its execution context and stack and returns from commit speculation to continue its \nexecution. Second, if a speculative .ow requests an evaluation, the parent .ow executes the user de.ned \nevaluation function and returns a boolean value to indicate either a success or a cancellation. The speculative \n.ow then either continues or aborts its execution based on the boolean value. Ad\u00additionally, the parent \n.ow can steer the computations of speculative .ows. Anumita also implements a .exible ap\u00adproach to allow \nthe parent .ow to store the intermediate results of speculative .ows for evaluation. In order to ac\u00adcomplish \nthis a speculative .ow may access memory using get ir memory. This region of memory is shared between \nthe parent .ow and the speculative .ow and it is unique to each speculative .ow. This obviates the need \nfor any syn\u00adchronization among speculative .ows to update their inter\u00admediate results. Finally, in the \nevent that all the speculations in a compo\u00adsition abort, the last speculation to abort (in program order \nwithin for a composition) signals the parent .ow to termi\u00adnate the program, since no surrogate satis.ed \nthe expected quality criterion.  4.3 Nested Speculative Compositions Implementing support for nested \nspeculations presents ad\u00additional challenges. Recall that in order to contain updates within a speculative \n.ow, the pages modi.ed in a specula\u00adtive .ow are privatized. Hence, if a speculative .ow in turn creates \na new composition, then it should propagate all its privatized updates to the speculative .ows in the \nnew com\u00adposition. This has to be achieved without committing the up\u00addates, since the parent of the nested \nspeculation may not be the winner in its composition. Conversely, the updates by the speculations in \na nested composition should be propagated only to its parent .ow to ensure program correctness. To resolve \nthis, in the case of nested speculations, the run\u00adtime creates new speculative .ows during the call to \nbegin speculation instead of using a worker pool entry. This creates a current copy of the parent .ow \nand includes privatized updates. Since the parent is blocked upon compos\u00ading a speculation, the lazy \ncopy-on-write semantics provided  Figure 5. (left) Illustrates the virtual memory address (VMA) layout \nof each process. The runtime provides memory isolation by using shared memory objects and memory mapped \n.les to share global state among processes. (right) Illustrates how the VMA is manipulated by the runtime \nwith a simple example. by the operating system ef.ciently creates isolated private address spaces for \nthe nested speculations.  4.4 Containment In order to determine the write-set and and contain (priva\u00adtize) \nthe updates of a speculation, Anumita employs page level protection and privatization of the shared VMA. \nEach speculative .ow initially write-protects (PROT READ) its shared VMA. Read accesses to shared data \ndo not produce any faults and execute normally. However, if a specula\u00adtion attempts to write to a shared \npage (global data, heap), the runtime handles the access fault (SEGV ACCERR) by remapping the faulting \npage from the shared memory in pri\u00advate (MAP PRIVATE) mode. The runtime maintains a list of pages that \nwere modi.ed by each speculation within a composition. The permissions of the page are then reset to \nread-write so that the speculation can continue its execution. This privatization provides containment \nof updates by a speculation. Such a lazy privatization scheme that defers pri\u00advatization until the instant \nmemory update happens results in weak atomicity [8]. Weak atomicity is suf.cient to en\u00adsure program correctness \nin our speculation model. We do not present a formal proof of correctness, however, the in\u00adtuition behind \nthe proof is that once a winning speculation commits, none of the remaining speculations will be allowed \nto commit, thus precluding Write-after-Read or Write-after-Write hazards. Hence, we chose the lazy privatization \nap\u00adproach over a conservative approach, which privatizes the entire VMA. The runtime does not track accesses \nof local variables on the stack. Since a copy of the parent stack frame is passed to each speculation, \nthe stacks do not need to be write\u00adprotected. Instead the parent s stack frame is updated with contents \nof the stack frame from the winning speculation. Such a strategy works for programs that contain pointers \nto stack-allocated data.  4.5 Inclusion When a speculation composition culminates with a winning surrogate, \nthe updates of the winning speculation (contained in the privatized data) must be propagated and made \nvisible to the parent .ow and any other non speculative .ows in the program.  In order to perform this \ninclusion of updates, we imple\u00adment the shadow addressing technique similar to [29] that leverages the \nlarge virtual memory address (VMA) provided by 64-bit operating systems. Recall that the runtime system \nmaps globals and the heap (described in Section 4.1) using shared memory objects and memory mapped .les. \nUsing the same shared memory objects and memory mapped .le, the runtime creates an identical secondary \nshadow mapping of the global data sections and heap at a high address (45th bit set) in the VMA of each \nspeculation .ow. The program is unaware of this mapping, and performs its accesses (read\u00ads/writes) at \nthe original low address space. The high address space shadow is always shared among speculative .ows \nand is never privatized. Hence, any updates to it are propagated across all .ows. In effect, the high \naddress mapping cre\u00adates a shadow address space for all shared program data and modi.cations (unless \nprivatized) are visible in both address spaces (shown in Figure 5). To perform this inclusion the runtime \nemploys two dis\u00adtinct strategies depending on the depth of the speculations (nested or otherwise). In \na single level speculation, where a .ow creates a composition, updates from the winning spec\u00adulation \nmust be made visible to the parent .ow. To achieve this, the runtime copies all the pages in the write \nset of the winning speculation to the high address shadow region of the VMA (shown in Figure 5), which \nautomatically propagates the updates to the parent .ow, due to the shared memory bindings. The runtime \nthen reverts all privatized mappings within the speculative .ows (cleanup) and returns them to the pool. \nIn a nested speculation, the runtime creates a new shared memory mapping equal to the write set of the \nwinning spec\u00adulation and it copies the write set of the winning speculation to the newly created mapping. \nThe parent .ow then copies the write-set from this new mapping into its address space to perform inclusion. \n 4.6 Example We present a simple example to illustrate how the runtime manipulates the VMA of each speculation \n.ow while pro\u00adviding isolation, privatization and inclusion. Consider the scenario as shown in Figure \n5 where a control .ow creates a composition involving three speculations. Initially, all the shared pages \n(1, 2, 3, 4) of the speculative .ows are write\u00adprotected. When the .ow with rank 0 attempts to write \nto pages (1, 2), the pages are privatized (lazy privatization). Similarly, the runtime system privatizes \nthe updates of spec\u00adulations with ranks 1 and 2, which write to pages (2, 3) and (2, 4) respectively. \nIf the speculation with rank 2 wins the composition, the runtime commits the write-set to the shared \nhigh-address space to propagate the updates. Additionally, in Figure 5 we illustrate how the speculative \n.ows may request evaluation of their progress. At the begin\u00adning of a speculative composition, a program \nmay choose to request space for storing partial results. In the above ex\u00adample, pages (5, 6, 7) are allocated \nby the runtime system to store the partial results. Each speculation may use the get ir memory() to obtain \nthe address of the region used to store its partial results. The speculative .ow at rank 0 writes its \npartial results to page 5, before requesting evaluation. Fol\u00adlowing the same procedure, speculative .ow \n1 writes to page 6 before requesting evaluation from its parent .ow. The par\u00adent .ow can access these \nmemory locations and execute the evaluation function to determine the relative quality and/or progress \nof the speculative .ows. Recall, that while Anumita supports multi-threaded ap\u00adplications, caution should \nbe exercised in leveraging specu\u00adlative parallelism in a multi-threaded environment. As dis\u00adcussed in \nSection 3.1, if a surrogate requires concurrency with other non-speculative threads or other concurrently \nex\u00adecuting speculative threads then it is not a candidate for spec\u00adulation, since such a surrogate is \nbased on concurrency rather than speculation. Hence, in the presence of data dependen\u00adcies we expect \nexplicit serialization in composing specula\u00adtions in order to ensure program correctness. A complication \narises when two distinct threads of a pro\u00adcess independently instantiate speculative compositions that \nexecute concurrently. This is depicted in Figure 1, where threads 0 and 2 enter distinct compositions. \nWhile the com\u00adpositions may be distinct, the granularity of our protection mechanism is at the level \nof an operating system page, which can cause false-sharing if updates to distinct bytes from dis\u00adtinct \ncompositions reside on the same operating system page. The artifact of this problem is that in the case \nof concurrent speculations, the contents on a page subject to false shar\u00ading will re.ect the updates \nfrom the last speculation in pro\u00adgram order to successfully commit without re.ecting any of the updates \nfrom concurrent commits. For example in Fig\u00adure 1, if the monte-carlo method updated page 1, which was \nalso updated (albeit at different locations) by the adaptive method, the .nal contents of page 1 will \nre.ect the updates from adaptive and none of the updates from monte-carlo. In effect, updates to pages \nsubject to false sharing are mutually exclusive, which is clearly incorrect. To solve this problem, we \nneed an ef.cient mechanism to propagate updates from a winning speculative .ow to all concurrent speculations. \nThis is achieved by computing and propagating XOR differences. To see how this works, con\u00adsider the example \nshown in Figure 1. When monte-carlo wins the speculative composition in thread 2, prior to per\u00adforming \ninclusion the runtime determines if there are con\u00adcurrent speculative compositions in other threads. \nIf so, for each page in the write-set of the .rst winning .ow (monte\u00adcarlo), the runtime computes an \nXOR of the privatized page with its counterpart in the high address space. Recall that prior to inclusion, \nthe page in the write-set of monte-carlo is privatized and includes updates from the .ow, whereas its \ncounterpart in the shadow address space contains the origi\u00adnal contents of the page. The XOR difference \nthus yields the exact set of bits that were updated by monte-carlo method.  The runtime then pushes \nthis XOR difference to all con\u00adcurrently executing .ows (adaptive, conservative, extrapo\u00adlation) and \nperforms inclusion of updates from monte-carlo as before. The concurrent .ows apply the differences by \ncomputing the XOR of the difference they received with their privatized copy of the page (if one exists \ndue to false sharing). Intuitively this mechanism updates the privatized contents of a concurrent speculation \nwith the latest updates from a winning speculation in another composition. In the example above, when \nadaptive .nally wins its composition, its write set already contains the updates from monte-carlo and \nhence the .nal resulting update of a falsely shared page from adaptive correctly contains the cumulative \nupdates from winning speculations. To minimize time and space overhead, the XOR difference is only computed \non the pages in the write set that are subject to false sharing (typically small), which is determined \nby computing the intersection of the write sets of the winning .ow (monte-carlo) and all other concurrently \nexecuting .ows(adaptive, conservative, extrapolation).  4.7 Support for OpenMP To support OpenMP, we \nprovide a simple source to source translator that expands the #pragma speculate (...){....} di\u00adrective \nto begin and commit constructs. Our translator parses only the speculate pragma leaving the rest of the \nOpenMP code intact. This approach does not require any modi.ca\u00adtions to existing OpenMP compilers and/or \nOpenMP run\u00adtime libraries. Our runtime system overrides mutual exclusion locks, barriers and condition \nvariables of the POSIX thread in\u00adterface and a few OpenMP library routines in order to provide a clean \ninterface to OpenMP. We overload the omp get thread num call in OpenMP to return the spec\u00adulation rank \nfrom get rank. The Anumita runtime auto\u00admatically detects if an OpenMP program is in a specula\u00adtive context \nand selectively overloads OpenMP calls, which fall back to their original OpenMP runtime when execution \nis outside a speculative composition. Finally, our OpenMP subsystem implements a simple static analyzer \nto perform lexical scoping of a speculative composition. This can be used to check for logical errors \nsuch as a call to commit before beginning a speculation. 5. Experimental Evaluation We evaluated the \nperformance of the Anumati runtime over three applications: a multi-algorithmic PDE solving frame\u00adwork \n[32], a graph (vertex) coloring problem [25] and a suite of sorting algorithms [36]. We ran each benchmark \nunder two scenarios. The .rst scenario uses Anumita to speculatively execute multiple algorithms concurrently. \nThis was done by modifying ap\u00adproximately 8-10 lines of source code in the above bench\u00admarks. Since Anumita \nguarantees isolation, these modi.ca\u00adtions were short and required little to no understanding of the algorithms \nthemselves. In the other scenario we ran the vanilla benchmark executing each algorithm individually. \nAll experiments were performed on a 16 core shared mem\u00adory machine (NUMA) running Linux 2.6.31-14 with \n64GB of RAM. The system contains four 2 GHz Quad-core AMD Opteron processors. 5.1 PDE solver One approach \nfor dealing with the unpredictable input\u00addependent performance of PDE solvers is a poly-algorithmic strategy, \nwhere multiple algorithms are tried in parallel, with the one .nishing .rst declared the winner, e.g., \n[4, 6]. This approach is robust, essentially guaranteeing a solution, and is easily implemented using \nour framework. We consider the scalar linear elliptic equation aa - 2 u + ux + uy = f(x, y), (\u00df + x + \ny)2 (\u00df + x + y)2 with Dirichlet boundary conditions on the unit square, where \u00df> 0. Discretized with \ncentered .nite differences, the resulting linear system of algebraic equations is increasingly ill-conditioned \nfor large a and small \u00df. Krylov linear solvers have dif.culty as this problem approaches the singular \ncase, i.e., as a/\u00df2 grows. What is not so clear is how quickly the performance degrades, and how much \npreconditioning can help. To simplify the case study, we .x \u00df at 0.01 and vary a. Discretizing the problem \nusing a uniform grid with spac\u00ading h =1/300 results in a linear system of dimension 89401. We consider \nthree iterative methods and one direct method for solving this system of equations: 1. GMRES(kdim=20) \nwith ILUTP(droptol=.001) 2. GMRES(kdim=50) with ILUTP(droptol=.0001) 3. GMRES(kdim=100) with ILUTP(droptol=.00001) \n 4. Band Gaussian Elimination  Here kdim is the GMRES restart parameter, ILUTP is the incomplete LU \nwith threshold pivoting precondi\u00adtioner [33, Chap. 10], and droptol controls the number of nonzeros kept \nin the ILU preconditioner. Increasing kdim or decreasing droptol increases the computational cost per \niteration of the iterative method, but should also increase the residual reduction per iteration. Hence, \none can think of methods one to four as being ordered form fast but brittle to slow but sure. Our PDE-solving \nframework for these experiments is ELLPACK [32], with the GMRES imple\u00admentation from SPARSKIT [34]. Figure \n6 shows the performance of the four methods and speculation for varying a. For small a the results are \nconsis\u00adFigure 6. Time to solution for individual PDE solvers and speculation based version using Anumita. \nCases that fail to converge in 1000 iterations are not shown. The results show that Anumita has relatively \nsmall overhead, allowing the speculation based program to consistently achieve performance comparable \nto the fastest individual method for each problem.  Method Fail Speedup Min Max Median 1 51 0.84 2.47 \n0.94 2 27 0.94 2.89 1.18 3 23 0.94 3.62 1.52 4 0 0.95 36.19 5.01 Table 2. Number of failing cases (out \nof 125) for each PDE solver, and speedup of speculative approach relative to each method. tent, with \nMethod 1 consistently fastest. As a grows, how\u00adever, the performance of the iterative methods vary dramati\u00adcally, \nwith each method taking turns being the most ef.cient. In many cases the GMRES iteration fails to converge \n(i.e., it\u00aderations exceeding 1000 are not shown in the .gure). Even\u00adtually, for large enough a, Band \nGE is the only method that succeeds. The write set of the PDE solver is 157156 pages ( 614MB) of data. \nThe overhead of speculation shrinks steadily as the problem dif.culty grows, with overheads of no more \nthan 5% for large a. This is to be expected since the time to solve sparse linear systems grows faster \nas a function of problem dimension than the data set size, which largely de\u00adtermines the overhead. However, \nin cases with small (< 10 sec) runtime, the overhead due to speculation is noticeable (up to 16%). This \nis due to initial thread creation and start up costs which are otherwise amortized over the runtime of \na larger run. The results show that speculative execution provides clear bene.ts over any single static \nselection of PDE solver. Ta\u00adble 2 summarizes the performance of the four methods rela\u00adtive to the speculatively \nexecuted case. Statically choosing any one of the GMRES methods (Methods 1-3) causes a serious robustness \nproblem, as many of the problems fail completely. Even for the cases where GMRES succeeds, we see that \nthe speculative approach yields noticeable im\u00adprovements. For the problems where method 1 succeeds, it \nis faster than speculation more than half the time (median speedup = 0.94). Compared to methods 2-4 speculation \nis signi.cantly faster in the majority of cases. In essence, spec\u00adulation dynamically chooses the best \nalgorithm for a given problem, with minimal overhead. It must be pointed out that the speculative code \nuses four computational cores, while the standalone cases each use only one core. In the case where we \nonly have sequential implementations of a given surrogate, speculation gives us a convenient way to do \nuseful work on multiple cores, mov\u00ading more quickly on average to a solution. However, given parallel \nimplementations of each of the four methods, an al\u00adternative to speculation is to choose one method to \nrun (in parallel) on the four cores. However, this strategy still suf\u00adfers from the risk of a method \nfailing, in which case one or more additional methods would have to be tried. In addition, it is well-known \nthat sparse linear solvers do not exhibit ideal strong scaling, i.e., parallel performance for a .xed \nprob\u00adlem does not scale well to high core counts. By contrast, running each surrogate on a core is embarrassingly \nparal\u00adlel; each core is doing completely independent work. Given hundreds of cores, the optimal strategy \nis likely to be to use  (a) LE 450 15c, seed=1 (b) LE 450 15c, seed=12  (c) LE 450 15c, seed=1234 \n(d) LE 450 15d, seed=1234 Figure 7. The performance of Graphcol benchmark using two DIMACS data sets \nLE 450 15c (sub.gures (a) through (c)) and LE 450 15d (sub.gure (d)). speculation at the highest level, \nwith each surrogate running in parallel on some subset of the cores. Choosing the num\u00adber of cores to \nassign to each surrogate should depend on the problem and the scalability of each method on that problem, \nand is beyond the scope of this paper.  5.2 Graph Coloring Problem In graph theory, vertex coloring \nis the problem of .nding the smallest set of colors needed to color a graph G =(V, E) such that no two \nvertices vi,vj . V with the same color share an edge e. The graphcol [25] benchmark implements three \nsurrogate heuristics for coloring the vertices of a graph: simulated annealing, tabu search and variable \nneighborhood search. The benchmark initializes the graph by randomly coloring the vertices with a speci.ed \nset of colors and each heuristic algorithm iteratively recolors the graph within the coloring constraints. \nWe used the DIMACS [13] data sets for the graph coloring benchmark, which are widely used in evaluating \nalgorithms and serve as the testbed for DIMACS implementation challenges. Each data set (graph) has a \n.xed number of colors that it can use to color a graph. We experi\u00admented with over 80 DIMACS data sets \nusing different seeds (for initial colors) and show the results from representative runs. In Figure 7 \nwe present the results of the graph coloring benchmark using two DIMACS data sets. The results show several \ninteresting characteristics. First, certain heuristics do not converge and cannot guarantee a solution. \nFor instance, simulated annealing (sa) cannot color the graph beyond a certain number of colors. Second, \nthe choice of the input seed, which decides the initial random coloring, creates sig\u00adni.cant performance \nvariations among the heuristics (Fig\u00adures 7 (a) through (c)) even when the graph is identical. Third, \nwhen the seed is constant, there is performance varia\u00adtion among the data sets, which represent different \ngraphs as shown in Figures 7 (c) and (d). In the presence of such strong input dependence across multiple \ninput parameters, it is dif\u00ad.cult even for a domain expert to predict the best algorithm a priori.  \n Figure 8. Performance of Anumita over a suite of sorting algorithms. Using Anumita it is possible to \nobtain the best solution among multiple heuristics. We found that in some cases where sa failed to arrive \nat a solution (unable to color the graph using speci.ed number of colors), the use of specula\u00adtion guaranteed \nnot only a solution but also one that is nearly as fast the fastest alternative. Since the write set \nis relatively small at around 50-100 pages, the overhead of speculation is negligible. Anumita s speedup, \nacross all the data sets (in Figure 7), ranges from 0.954 (vns with 26 colors in in Fig\u00adure 7 b) in the \nworst case, when the static selection is the best surrogate to 7.326 (vns with 21 colors in in Figure \n7 d), when the static selection is the worst surrogate. We omit the results from the sa method in calculating \nspeedup since sa consistently performs worse than the other algorithms on these data sets.  5.3 Sorting \nAlgorithms Since the overhead of speculation in our runtime is propor\u00adtional to the write set of an application, \nwe chose sort as our third benchmark since it can be con.gured to have an arbitrarily large memory footprint. \nSort is relatively easy to understand, and yet there are wide variety of sorting algo\u00adrithms with varying \nperformance characteristics depending on the size of the data, sortedness and cache behavior [3]. Our \nsuite of sorting algorithms includes C implementations of quick sort, heap sort, shell sort, insertion \nsort, merge sort and bubble sort. The time to completion of the sorting algorithms is based on several \ncardinal properties including the input size, their values (sorted or unsorted) and algorithmic complexity. \nIn this set of experiments we .xed the input size and used two sets of input data completely sorted \nand completely ran\u00addom, each of size 8 GB. Each sorting algorithm is imple\u00admented as a separate routine. \nThe input data is generated using a random number generator. After sorting the data the benchmark veri.es \nthat the data is properly sorted. We mea\u00adsured the runtime of each sorting algorithm and excluded the \ninitialization and veri.cation phases. Using Anumita, we speculatively executed all six sorting algorithms \nconcur\u00adrently. In Figure 8 we present the results of the sort benchmark. Results for insertion sort and \nbubble sort for random data were omitted since their runtime exceeds 24 hours. The results show that \ninsertion sort is the fastest for sorted data and quick sort performs the best on completely random data, \nwhich is expected. Despite the large write set of 8 GB per speculation, a total of 6x8GB for the entire \nspeculative composition, Anumita is at least the second fastest of all the alternatives considered and \nis nearly as fast as the fastest alternative. The worst case overhead of speculation on sorted data relative \nto the best algorithm (insertion sort) is 15.78% (3.2 sec), which stems from the map faults handled by \nthe runtime system. The worst case overhead of speculation compared to the fastest algorithm on the random \ndata is 8.72% (50.34 sec over 616 secs). This overhead stems from privatization, isolation and inclusion \nof the large 8 GB data set. Anumita achieves a speedup ranging from 0.84 (quick sort/random data) to \n62.95 (heap sort/sorted data).  5.4 Energy Overhead The primary focus of Anumita is to improve run time \nperfor\u00admance. Reducing energy consumption runs counter to this goal. However, in this section, we demonstrate \nthat adopt\u00ading coarse-grain speculation to exploit parallelism on multi\u00adcore systems does not come with \na large energy consumption penalty, and in fact can reduce total energy consumption in many cases. Energy \nconsumption in modern multi-core processors is not proportional to CPU utilization. An idle core consumes \nnearly 50% of the energy of a fully loaded core [27]. There is a signi.cant body of research in the architectures \ncommunity on making the energy consumption proportional to offered  Figure 9. Energy consumption of \nPDE solver using surrogates in Anumita. The results show that Anumita has relatively low energy overhead. \nload, which is motivated by energy consumption of large data centers that run at an average utilization \nof 7 - 10%. To measure the energy overhead of coarse-grain specula\u00adtive execution using Anumita, we connected \na Wattsup Pro wattmeter to the AC input of the 16 core system running the benchmark. This device measures \nthe total input power to the entire system. We performed the power measurement using the SPEC Power daemon \n(ptd), which samples the in\u00adput power averaged over 1 sec intervals for the entire run time of the application. \nWe calculated energy consumption as the product of the total runtime and the average power. We measured \nenergy consumption under two scenarios: a) each algorithm run individually and b) speculatively execute \nmultiple algorithms using Anumita. Figure 9 presents the energy consumption of the PDE solver. We report \nthe results for alpha values greater than 1700 for the PDE solver, since they have a runtime of at least \na few seconds (required to make any meaningful power measurements). Comparing the most energy ef.cient \nalgo\u00adrithm at each alpha with the corresponding speculative exe\u00adcution, we found that the overall energy \noverhead of specula\u00adtion ranged between 7.72% and 19.21%. It is comforting to see that, even in the presence \nof running four surrogates con\u00adcurrently, Anumita incurred a maximum energy overhead of 19.21% compared \nto the most energy ef.cient algorithm. More importantly, since the most energy ef.cient algo\u00adrithm for \na given problem in not known a priori, we again see a large robustness advantage for speculation this \ntime with respect to energy consumption. With a static choice of algorithm there is substantial risk \nthat a method will fail (ne\u00adcessitating the use of another method) or take much longer than the best \nmethod, all of which consume more energy than the speculatively executed approach. Figure 10 shows the \nenergy consumption for two vertex coloring algorithmic surrogates (tabu, vns) and speculation using Anumita. \nIn this case, Anumita speculates over three algorithms (sa, tabu and vns) even though one of them con\u00adsistently \nfails to color the graph. Comparing the most en\u00adergy ef.cient algorithm at each color with the corresponding \nspeculation, we found that the overall energy overhead due to speculation ranged between 6.08% and 16.04%. \n The total energy consumed to color the graph is actually lower for speculation when compared to a static \nchoice of either algorithmic surrogate. This is because energy is the product of power and time, and \nsince neither algorithm is consistently better (strong input dependence), speculation results in lower \ntime to completion of an entire test case which translates to lower energy consumption. Speculation using \nAnumita takes 252 seconds for the problem set with a total energy consumption of 107904 joules. Tabu \ntakes a total of 321 seconds and consumes a total energy of 128531 joules for the problem set. In contrast \nthe best static choice of the surrogate (vns) runs in 314 seconds and consumes 125194 joules. Speculation \nhere is 24.6% faster in time and consumes 16.02% less energy, a result that is positive in both aspects. \n  5.5 Summary Anumita provides resilience to failure of optimistic algo\u00adrithmic surrogates. In both \ngraph coloring as well as PDE solvers, not all algorithmic surrogates successfully run to completion. \nIn the absence of a system such as Anumita, the alternative is to run the best known algorithmic surrogate \nand if it fails, retry with a fail-safe algorithm that is known to succeed. While this works for PDE \nsolving example with Band Gaussian Elimination being the fail-safe, there is no clear equivalent for \ngraph coloring, with each surrogate fail\u00ading at different combinations of graph geometry and initial \ncoloring. With modest energy overhead and sometimes sav\u00adings, Anumita can signi.cantly improve the performance \nof otherwise hard to parallelize applications. 6. Related Work We categorize existing software based \nspeculative execu\u00adtion models [11, 14, 18 21, 28, 30, 31, 35, 37] into two categories depending on the \ngranularity at which they per\u00adform speculation loops or user de.ned regions of code. Loop level models \n[11, 19 21, 30, 31, 35, 37] achieve paral\u00adlelism in sequential programs by employing speculative ex\u00adecution \nwithin loops. While such models transparently par\u00adallelize sequential applications without requiring \nany effort from the programmer, their scope is limited to loops. In contrast, the second category of \nspeculative execution mod\u00adels [5, 14, 18, 28, 38] allow the programmer to specify re\u00adgions of code to \nbe evaluated speculatively. We restrict our discussion to these models throughout the rest of this sec\u00adtion. \nBerger et al. [5] proposed the Grace framework to specu\u00adlatively execute fork-join based multithreaded \napplications. Grace uses processes for state separation with virtual mem\u00adory protection and employs page-level \nversioning to detect mis-speculations. Grace focuses on eliminating concurrency bugs through sequential \ncomposition of threads. Ding et al. [14] proposed behavior oriented paralleliza\u00adtion (BOP). BOP aims \nto leverage input dependent course grained parallelism by allowing the programmer to anno\u00adtate regions \nof code, denoted by possibly parallel regions (PPR). BOP uses a lead process to execute the program non\u00adspeculatively \nand uses processes to execute the possibly par\u00adallel regions. When the lead process reaches a PPR, it \nforks a speculation and continues the execution until it reaches the end of the PPR. The forked process \nthen jumps to the end of the PPR region and in turn acts as lead process and con\u00adtinues to fork speculations. \nThis process is repeated until all the PPRs in the program are covered. BOP s PPR execution model is \nidentical to pipelining. The lead process at the start of the pipeline waits for the speculation it forked \nto com\u00adplete and then checks for con.icts before committing the results of the speculation. This process \nis recursively per\u00adformed by all the speculation processes which assumed the role of the lead process. \nBOP employes page-based protec\u00adtion of shared data by allocating each shared variable in a separate page \nand uses a value-based checking algorithm to validate speculations. In another study, Kelsey et al. [18] \nproposed the Fast Track execution model, which allows unsafe optimization of sequential code. It executes \nsequential (normal tracks) and speculative variants (fast tracks) of the code in paral\u00adlel and compares \nthe results of both these tracks to validate speculations. Their model achieves speedup by overlapping \nthe normal tracks and by starting the next normal track in program order as soon as the previous fast \ntrack is com\u00adpleted. Fast Track performs source transformation to convert all global variables to use \ndynamic memory allocation so its runtime can track accesses to global variables. Additionally, Fast Track \nemployes a memory-safety checking tool to insert memory checks while instrumenting the program. Finally, \nFast Track provides the programmer with con.gurations that tradeoff program correctness against performance \ngains. In contrast, Anumita provides transparent name space isolation and it does not require any annotations \nto the variables in a program. Additionally, Anumita does not rely on program instrumentation. Prabhu \net al. [28] proposed a programming language for speculative execution. Their model uses value speculation \nto predict the values of data dependancies between coupled interactions based on a user speci.ed predictor. \nTheir work de.nes a safety condition called rollback freedom and is combined with static analysis techniques \nto determine the safety of speculations. They implemented their constructs as a C# library. The domains \nwhere value speculation is applicable are orthogonal to our work. Trachsel and Gross [38, 39] present \nan approach called competitive parallel execution (CPE) to leverage multi-core systems for sequential \nprograms. In their approach they exe\u00adcute different variants of a single threaded program compet\u00aditively \nin parallel on a multicore system. The variants are ei\u00adther hand generated surrogates or automatically \ngenerated by selecting different optimization strategies during compila\u00adtion. The program s execution \nis divided into phases and the variants compete with each other in a phase. The variant that .nishes \n.rst (temporal order) determines the execution time of that phase, thereby reducing the overall execution \ntime. In contrast, Anumita is capable of supporting both sequential and parallel applications and provides \nexpressive evaluation criterion (temporal and qualitative) to evaluate speculations. Praun et al. [40] \npropose a programming model called implicit parallelism with ordered transactions (IPOT) for ex\u00adploiting \nspeculative parallelism in sequential or explicitly parallel programming models. The authors implement \nan emulator using the PIN instrumentation tool to collect mem\u00adory traces and emulate their proposed speculation \nmodel. In their work, they propose and de.ne various attributes to vari\u00adables to enable privatization \nat compile time and avoid con\u00ad.icts among speculations. In contrast, as mentioned previ\u00adously, Anumita \ndoes not require annotations to variables or rely on binary instrumentation. Instead, Anumita provides \nisolation to shared data at runtime.  In another study, Cledat et al. [12] proposed opportunistic computing, \na technique to increase the performance of ap\u00adplications depending on responsiveness constraints. In \ntheir model multiple instances of a single program are generated by varying input parameters to the program, \nwhich then compete with each other. In contrast, Anumita is designed to support speculation at arbitrary \ngranularity as opposed to the entire program. Ansel et. al. [3] proposed the PetaBricks programming language \nand compiler infrastructure. PetaBricks provides language constructs to specify multiple implementations \nof algorithms in solving a problem. The PetaBricks compiler automatically tunes the program based on \npro.ling and gen\u00aderates an optimized hybrid as a part of the compile process. In contrast, our approach \nperforms coarse-grain speculation at runtime and is hence better suited for scenarios where per\u00adformance \nis highly input data dependent. Additionally, certain compiler directed approaches [7, 16, 17, 22, 24, \n26] provide support for speculative execution and operate at the granularity of loops. Such approaches \nrely on program instrumentation [17], use hardware counters for pro.ling [17] or binary instrumentation \nto collect traces [24, 26] in order to optimize loops. In contrast to such systems, Anumita is implemented \nas a language independent runtime system. The main goal of Anumita is to simplify the notion of speculative \nexecution. Finally, a nondeterministic programming languages (e.g., Prolog, Lisp) allows the programmer \nto specify various al\u00adternatives for program .ow. The choice among the alterna\u00adtives is not directly \nspeci.ed by the programmer, however the program at runtime decides to choose between the al\u00adternatives \n[1]. Several techniques such as backtracking and reinforcement learning are commonly employed in choosing \na particular alternative. It is unclear if it is the responsibility of the programmer to ensure and correct \nthe side-effects of the alternatives. Anumita represents a concurrent implemen\u00adtation of the non-deterministic \nchoice operator. The contri\u00adbution here is to introduce this notion and an ef.cient imple\u00admentation to \nimperative programming. 7. Future Work We are continuing to improve Anumita. We are presently working \non extending support for disk IO among specula\u00adtive surrogates. While Anumita simpli.es the subtleties \nof coarse-grain speculative parallelism by providing simple se\u00adquential semantics, the programmer must \nidentify the scope for speculation. We plan to automate this aspect of our sys\u00adtem. Currently there is \nan ongoing effort [2, 9, 10] to extend C++ to include threading models. We propose that specu\u00adlation \nshould also be a natural extension of the imperative languages and the speculation model should be a \nnatural extension to threading models. We plan to to investigate ex\u00adtending language support for speculation. \n8. Conclusions In this paper we presented Anumita, a language indepen\u00addent runtime system to achieve \ncoarse-grain speculative par\u00adallelism in hard to parallelize and/or highly input depen\u00addent applications. \nWe proposed and implemented program\u00adming constructs and extensions to the OpenMP program\u00adming model to \nachieve speedup in such applications with\u00adout sacri.cing performance, portability and usability. Exper\u00adimental \nresults from a performance evaluation of Anumita show that it is (a) robust in the presence of performance \nvari\u00adations or failure and (b) achieves signi.cant speedup over statically chosen alternatives with modest \noverhead. The im\u00adplementation of Anumita and the benchmarks used in this study will be made available \nfor public download. References [1] H. Abelson and G. J. Sussman. Structure and Interpretation of Computer \nPrograms. MIT Press, Cambridge, MA, USA, 2nd edition, 1996. ISBN 0262011530. [2] S. V. Adve and H.-J. \nBoehm. Memory Models: A Case for Rethinking Parallel Languages and Hardware. Communica\u00adtions of the ACM, \n53:90 101, August 2010. ISSN 0001-0782. URL http://doi.acm.org/10.1145/1787234.1787255. [3] J. Ansel, \nC. Chan, Y. L. Wong, M. Olszewski, Q. Zhao, A. Edelman, and S. Amarasinghe. PetaBricks: A Language and \nCompiler for Algorithmic Choice. In Proceedings of the 2009 ACM SIGPLAN conference on Programming Language \nDesign and Implementation, PLDI 09, pages 38 49, New York, NY, USA, 2009. ACM. ISBN 978-1-60558-392-1. \n[4] R. Barrett, M. Berry, J. Dongarra, V. Eijkhout, and C. Romine. Algorithmic bombardment for the iterative \nsolution of linear systems: A poly-iterative approach. Jnl. of Computational &#38; Appl. Math., 74:91 \n110, 1996. [5] E. D. Berger, T. Yang, T. Liu, and G. Novark. Grace: safe multithreaded programming for \nC/C++. In OOPSLA 09: Proceeding of the 24th ACM SIGPLAN conference on Object Oriented Programming Systems \nLanguages and Applications, pages 81 96. ACM, 2009. ISBN 978-1-60558-766-0. [6] S. Bhowmick, L. C. McInnes, \nB. Norris, and P. Raghavan. The role of multi-method linear solvers in pde-based simulations. In ICCSA \n(1), pages 828 839, 2003. [7] A. Bhowmik and M. Franklin. A general compiler framework for speculative \nmultithreading. In SPAA 02: Proceedings of the fourteenth annual ACM symposium on Parallel algorithms \nand architectures, pages 99 108, New York, NY, USA, 2002. ACM. ISBN 1-58113-529-7. [8] C. Blundell, E. \nLewis, and M. Martin. Subtleties of transactional memory atomicity semantics. IEEE Computer Architecture \nLetters, 5(2):17, 2006. ISSN 1556-6056. [9] H.-J. Boehm. Threads Cannot be Implemented As a Library. \nIn Proceedings of the 2005 ACM SIGPLAN conference on Programming Language Design and Implementation, \nPLDI  05, pages 261 268, New York, NY, USA, 2005. ACM. ISBN 1-59593-056-6. URL http://doi.acm.org/10.1145/ \n1065010.1065042. [10] Boehm, Hans-J. and Adve, Sarita V. Foundations of the C++ Concurrency Memory Model. \nIn Proceedings of the 2008 ACM SIGPLAN conference on Programming Language Design and Implementation, \nPLDI 2008, pages 68 78, New York, NY, USA, 2008. ACM. ISBN 978-1-59593-860-2. URL http://doi.acm.org/10.1145/1375581.1375591. \n[11] T. Chen, M. Feng, and R. Gupta. Supporting speculative parallelization in the presence of dynamic \ndata structures. In PLDI 10: Proceedings of ACM SIGPLAN 2010 conference on Programming Language Design \nand Implementation, volume 45, pages 62 73, New York, NY, USA, 2010. ACM. [12] R. Cledat, T. Kumar, J. \nSreeram, and S. Pande. Opportunistic Computing: A New Paradigm for Scalable Realism on Many-Cores. In \nProceedings of the First USENIX conference on Hot topics in parallelism, HotPar 09, pages 5 5, Berkeley, \nCA, USA, 2009. USENIX Association. URL http://portal. acm.org/citation.cfm?id=1855591.1855596. [13] DIMACS. \nDiscrete Mathematics and Theoretical Computer Science, A National Science Foundation Science and Technol\u00adogy \nCenter. http://dimacs.rutgers.edu/, April 2011. [14] C. Ding, X. Shen, K. Kelsey, C. Tice, R. Huang, \nand C. Zhang. Software behavior oriented parallelization. In PLDI 07: Proceedings of ACM SIGPLAN 2007 \nconference on Programming Language Design and Implementation, volume 42, pages 223 234, New York, NY, \nUSA, 2007. ACM. [15] Doug Lea. A memory allocator. http://g.oswego.edu/ dl/html/malloc.html, April 2011. \n[16] T. A. Johnson, R. Eigenmann, and T. N. Vijaykumar. Min\u00adcut program decomposition for thread-level \nspeculation. In PLDI 04: Proceedings of ACM SIGPLAN 2004 conference on Programming Language Design and \nImplementation, volume 39, pages 59 70, New York, NY, USA, 2004. ACM. [17] T. A. Johnson, R. Eigenmann, \nand T. N. Vijaykumar. Specu\u00adlative thread decomposition through empirical optimization. In PPoPP 07: \nProceedings of the 12th ACM SIGPLAN sym\u00adposium on Principles and Practice of Parallel Programming, pages \n205 214, New York, NY, USA, 2007. ACM. ISBN 978-1-59593-602-8. [18] K. Kelsey, T. Bai, C. Ding, and C. \nZhang. Fast Track: A Software System for Speculative Program Optimization. In CGO 09: Proceedings of \nthe 7th annual IEEE/ACM Inter\u00adnational Symposium on Code Generation and Optimization, pages 157 168, \nWashington, DC, USA, 2009. IEEE Com\u00adputer Society. ISBN 978-0-7695-3576-0. [19] M. Kulkarni, K. Pingali, \nB. Walter, G. Ramanarayanan, K. Bala, and L. P. Chew. Optimistic Parallelism Requires Abstractions. In \nPLDI 07: Proceedings of the 2007 ACM SIGPLAN conference on Programming Language Design and Implementation, \npages 211 222, New York, NY, USA, 2007. ACM. ISBN 978-1-59593-633-2. [20] M. Kulkarni, K. Pingali, G. \nRamanarayanan, B. Walter, K. Bala, and L. P. Chew. Optimistic Parallelism Bene.ts from Data Partitioning. \nIn ASPLOS XIII: Proceedings of the 13th International conference on Architectural Support for Programming \nLanguages and Operating Systems, volume 36, pages 233 243, New York, NY, USA, 2008. ACM. [21] M. Kulkarni, \nM. Burtscher, R. Inkulu, K. Pingali, and C. Casc\u00b8aval. How much Parallelism is There in Irregular Applications? \nIn PPoPP 09: Proceedings of the 14th ACM SIGPLAN symposium on Principles and Practice of Parallel Programming, \npages 3 14, New York, NY, USA, 2009. ACM. ISBN 978-1-60558-397-6. [22] W. Liu, J. Tuck, L. Ceze, W. Ahn, \nK. Strauss, J. Renau, and J. Torrellas. POSH: a TLS compiler that exploits program structure. In PPoPP \n06: Proceedings of the eleventh ACM SIGPLAN symposium on Principles and Practice of Parallel Programming, \npages 158 167, New York, NY, USA, 2006. ACM. ISBN 1-59593-189-9. [23] S. Lu, S. Park, E. Seo, and Y. \nZhou. Learning from mistakes: a comprehensive study on real world concurrency bug characteristics. In \nASPLOS XIII: Proceedings of the 13th International conference on Architectural Support for Programming \nLanguages and Operating Systems, pages 329 339. ACM, 2008. ISBN 978-1-59593-958-6. [24] Y. Luo, V. Packirisamy, \nW.-C. Hsu, A. Zhai, N. Mungre, and A. Tarkas. Dynamic performance tuning for speculative threads. In \nISCA 09: Proceedings of the 22nd annual Inter\u00adnational Symposium on Computer Architecture, volume 37, \npages 462 473, New York, NY, USA, 2009. ACM. [25] Marco Pagliari. Graphcol: Graph Coloring Heuristic \nTool. http://www.cs.sunysb.edu/~algorith/implement/ graphcol/implement.shtml, April 2011. [26] P. Marcuello \nand A. Gonz\u00b4alez. Thread-Spawning Schemes for Speculative Multithreading. In HPCA 02: Proceedings of \nthe 8th International Symposium on High-Performance Computer Architecture, page 55, Washington, DC, USA, \n2002. IEEE Computer Society. [27] Patterson, David A. and Hennessy, John L. Computer Organization and \nDesign, Fourth Edition, Fourth Edition: The Hardware/Software Interface (The Morgan Kaufmann Series in \nComputer Architecture and Design). Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 4th edition, \n2008. ISBN 0123744938, 9780123744937. [28] P. Prabhu, G. Ramalingam, and K. Vaswani. Safe Pro\u00adgrammable \nSpeculative Parallelism. In PLDI 10: Proceed\u00adings of ACM SIGPLAN 2010 conference on Programming Language \nDesign and Implementation, volume 45, pages 50 61, New York, NY, USA, 2010. ACM. [29] H. K. Pyla and \nS. Varadarajan. Avoiding Deadlock Avoidance. In PACT 2010: Proceedings of the 19th International Conference \non Parallel Architectures and Compilation Techniques, 2010. [30] A. Raman, H. Kim, T. R. Mason, T. B. \nJablin, and D. I. August. Speculative parallelization using software multi\u00adthreaded transactions. In \nASPLOS XV: Proceedings of the 15th International conference on Architectural Support for Programming \nLanguages and Operating Systems, volume 38, pages 65 76, New York, NY, USA, 2010. ACM.  [31] L. Rauchwerger \nand D. A. Padua. The LRPD Test: Specu\u00adlative Run-Time Parallelization of Loops with Privatization and \nReduction Parallelization. IEEE Transactions on Parallel Distributed Systems, 10(2):160 180, 1999. ISSN \n1045-9219. [32] J. R. Rice and R. F. Boisvert. Solving Elliptic Problems Using ELLPACK. Springer Verlag, \n1985. [33] Y. Saad. Iterative Methods for Sparse Linear Systems. PWS Publishing, Boston, 1996. [34] Y. \nSaad. SPARSKIT: A basic tool kit for sparse matrix computations. Technical Report 90-20, Research Institute \nfor Advanced Computer Science, NASA Ames Research Center, Moffet Field, CA, 1990. [35] J. G. Steffan, \nC. Colohan, A. Zhai, and T. C. Mowry. The STAMPede approach to thread-level speculation. ACM Transactions \non Computer Systems, 23(3):253 300, 2005. ISSN 0734-2071. [36] Thomas Wang. Sorting Algorithm Examples. \nhttp: //www.concentric.net/~ttwang/sort/sort.htm, April 2011. [37] C. Tian, M. Feng, N. Vijay, and G. \nRajiv. Copy or Discard execution model for speculative parallelization on multicores. In MICRO 41: Proceedings \nof the 41st annual IEEE/ACM International Symposium on Microarchitecture, pages 330 341, Washington, \nDC, USA, 2008. IEEE Computer Society. ISBN 978-1-4244-2836-6. [38] O. Trachsel and T. R. Gross. Variant-based \ncompetitive Parallel Execution of Sequential Programs. In Proceedings of the 7th ACM international conference \non Computing frontiers, CF 10, pages 197 206, New York, NY, USA, 2010. ACM. ISBN 978-1-4503-0044-5. [39] \nO. Trachsel and T. R. Gross. Supporting Application-Speci.c Speculation with Competitive Parallel Execution. \nIn 3rd ISCA Workshop on Parallel Execution of Sequential Programs on Multi-core Architectures, PESPMA \n10, 2010. [40] C. von Praun, L. Ceze, and C. Cas\u00b8caval. Implicit Parallelism with Ordered Transactions. \nIn Proceedings of the 12th ACM SIGPLAN symposium on Principles and Practice of Parallel Programming, \nPPoPP 2007, pages 79 89, New York, NY, USA, 2007. ACM. ISBN 978-1-59593-602-8. URL http://doi.acm.org/10.1145/1229428.1229443. \n[41] W. Zhang, C. Sun, and S. Lu. Conmem: detecting severe concurrency bugs through an effect-oriented \napproach. In ASPLOS XV:Proceedings of the 15th International conference on Architectural Support for \nProgramming Languages and Operating Systems, pages 179 192, New York, NY, USA, 2010. ACM. ISBN 978-1-60558-839-1. \n   \n\t\t\t", "proc_id": "2048066", "abstract": "<p>Speculative execution at coarse granularities (e.g., code-blocks, methods, algorithms) offers a promising programming model for exploiting parallelism on modern architectures. In this paper we present Anumita, a framework that includes programming constructs and a supporting runtime system to enable the use of coarse-grain speculation to improve program performance, without burdening the programmer with the complexity of creating, managing and retiring speculations. Speculations may be composed by specifying surrogate code blocks at any arbitrary granularity, which are then executed concurrently, with a single winner ultimately modifying program state. Anumita provides expressive semantics for winner selection that go beyond time to solution to include user-defined notions of quality of solution. Anumita can be used to improve the performance of hard to parallelize algorithms whose performance is highly dependent on input data. Anumita is implemented as a user-level runtime with programming interfaces to C, C++, Fortran and as an OpenMP extension. Performance results from several applications show the efficacy of using coarse-grain speculation to achieve (a) robustness when surrogates fail and (b) significant speedup over static algorithm choices.</p>", "authors": [{"name": "Hari K. Pyla", "author_profile_id": "81339523171", "affiliation": "Virginia Tech, Blacksburg, VA, USA", "person_id": "P2839230", "email_address": "harip@vt.edu", "orcid_id": ""}, {"name": "Calvin Ribbens", "author_profile_id": "81451595351", "affiliation": "Virginia Tech, Blacksburg, VA, USA", "person_id": "P2839231", "email_address": "ribbens@vt.edu", "orcid_id": ""}, {"name": "Srinidhi Varadarajan", "author_profile_id": "81100297692", "affiliation": "Virginia Tech, Blacksburg, VA, USA", "person_id": "P2839232", "email_address": "srinidhi@vt.edu", "orcid_id": ""}], "doi_number": "10.1145/2048066.2048110", "year": "2011", "article_id": "2048110", "conference": "OOPSLA", "title": "Exploiting coarse-grain speculative parallelism", "url": "http://dl.acm.org/citation.cfm?id=2048110"}