{"article_publication_date": "10-22-2011", "fulltext": "\n Data-Driven Synthesis for Object-Oriented Frameworks * Kuat Yessenov Zhilei Xu Armando Solar-Lezama \nMassachusetts Institute of Technology {kuat,timxu,asolar}@csail.mit.edu Abstract Software construction \ntoday often involves the use of large frameworks. The challenge in this type of programming is that object-oriented \nframeworks tend to grow exceedingly intricate; they spread functionality among numerous classes, and \nany use of the framework requires knowledge of many interacting components. We present a system named \nMatchMaker that from a simple query synthesizes code that interacts with the framework. The query consists \nof names of two frame\u00adwork classes, and our system produces code enabling interaction between them. MatchMaker \nrelies on a database of dynamic program traces called DeLight that uses novel abstraction-based indexing \ntechniques to answer queries about the evolution of heap connectivity in a matter of seconds. The paper \nevaluates the performance and e.ectiveness of MatchMaker on a number of benchmarks from the Eclipse framework. \nThe paper also presents the results of a user study that showed a 49% average productivity improvement \nfrom the use of our tool. Categories and Subject Descriptors D.2.2 [Design Tools and Techniques]: Computer-aided \nsoftware engi\u00adneering; I.2.2 [Automatic Programming]: Program syn\u00adthesis General Terms Human Factors \nKeywords Program Synthesis, Dynamic Instrumenta\u00adtion, Thin Slicing, Software Engineering 1. Introduction \nModern programming relies heavily on extensible frame\u00adworks that pack large amounts of functionality. \nThese * All authors are primary authors, listed in reverse alphabetical order. Permission to make digital \nor hard copies of all or part of this work forpersonal or classroom use is granted without fee provided \nthat copiesare not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. Tocopy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. OOPSLA 11, October 22 27, 2011, Portland, Oregon, \nUSA. Copyright c &#38;#169; 2011 ACM 978-1-4503-0940-0/11/10...$10.00 frameworks make it possible to \nwrite rich applications by piecing together pre-existing components, but the productivity bene.ts come \nat a price: a signi.cant learn\u00ading curve as programmers master a complex framework with thousands of \ncomponents. Synthesis [14, 16, 18] can alleviate this problem by leveraging the novice program\u00admer s \nlimited understanding of the system to generate code that uses the framework. Speci.cally, this paper \ndemonstrates the potential impact of synthesis through a new tool, MatchMaker, that addresses a concrete \npro\u00adgramming challenge: establishing an interaction between two framework classes. The problem arises \nfrom the way object-oriented frame\u00adworks factor functionality into a multitude of compo\u00adnents. This factoring \nmakes the framework .exible, but it also implies that interactions that look simple at a high level require \ncollaboration of a number of auxil\u00adiary objects. As a consequence, the user must write glue code whose \nsole purpose is to coordinate these auxiliary objects. To illustrate the problem, consider the following \nrun\u00adning example: extending an Eclipse [3] editor with syntax highlighting. This is done by de.ning a \nsub\u00adclass of RuleBasedScanner with the code to identify and color tokens in a .le. However, the editor \ndoes not use the scanner directly. Instead, the interaction is mediated by .ve additional classes. First, \nthe editor in\u00adteracts with a component called SourceViewer (see Fig. 1), which manages its add-ons. SourceViewer \nin turn uses PresentationReconciler to maintain a representation of the document in the presence of changes. \nPresentationReconciler uses an IPresentationDamager to identify changes to a doc\u00adument, and an IPresentationRepairer \nto incrementally scan those changes, and it is these two classes that interact directly with the scanner. \nTo glue these classes together, the programmer must extend SourceViewerCon.guration and override the \ngetPresentationReconciler method to return an instance of the PresentationReconciler class. This instance \nmust have its IPresentationDamager and IPresentationRepairer reference the new scanner. Finally, the \nnew SourceViewerCon.guration must be registered with the editor by calling setSourceViewerCon.guration \nin the constructor of the editor (see Fig. 2).  class AbstractTextEditor { SourceViewerCon.guration \nfCon.guration; ISourceViewer fSourceViewer; createPartControl() {fSourceViewer = createSourceViewer(); \nfSourceViewer.con.gure(fCon.guration); } setSourceViewerCon.guration(con.g) { fCon.guration = con.g; \n} } class SourceViewer { IPresentationReconciler fPresentationReconciler; con.gure(SourceViewerCon.guration \ncon.g) { fPresentationReconciler = con.g.getPresentationReconciler(); } } Figure 1. Eclipse code in AbstractTextEditor \n(2950 LOC) and SourceViewer (537 LOC) relevant to interaction with a scanner. class UserCon.guration \nextends SourceViewerCon.guration { IPresentationReconciler getPresentationReconciler() { PresentationReconciler \nreconciler = new PresentationReconciler(); RuleBasedScanner userScanner = new UserScanner(); DefaultDamagerRepairer \ndr = new DefaultDamagerRepairer(userScanner);reconciler.setRepairer(dr, DEFAULT_CONTENT_TYPE); reconciler.setDamager(dr, \nDEFAULT_CONTENT_TYPE); return reconciler; } } class UserEditor extends AbstractTextEditor { UserEditor() \n{ userCon.guration = new UserCon.guration(); setSourceViewerCon.guration(userCon.guration); } } class \nUserScanner extends RuleBasedScanner {...} Figure 2. User code required to let an editor use a scanner. \nMatchMaker automatically synthesizes the text in black. MatchMaker automatically synthesizes the glue \ncode in Fig. 2 from a simple query of the form: How do I get AbstractTextEditor and RuleBasedScanner \nto interact with each other? . More generally, given two types A and B, MatchMaker identi.es what the \nuser of the framework has to write in order for two objects of these types to work together. In order \nfor MatchMaker to do this, the .rst problem that has to be addressed is to give semantic meaning to the \nquery; i.e. what does it mean for two objects to interact with each other ? We assign semantic meaning \nby exploiting a new hypothesis about the design of object-oriented frameworks. MatchMaker hypothesis: \nIn order for two objects to interact with each other, there must be a chain of references linking them \ntogether. Therefore, the set of actions that led to the creation of the chain is the set of actions that \nneed to take place to enable the interaction. The MatchMaker hypothesis does not always hold; sometimes, \nfor example, two objects can interact with each other by modifying the state of some globally shared \nobject, without having necessarily a chain of references that connects them. Nevertheless, we have experimental \nevidence to suggest that the hypothesis is true for a number of pairs of classes in Eclipse (see the \ngenerality evaluation in Sec. 6.3) and that a tool based on this hypothesis can have a real impact on \nprogrammer productivity (see the user study in Sec. 6.4). The hypothesis is useful because it suggests \na relatively simple algorithm to answer the programmer s query. In the case of the running example, the \nalgorithm works like this: Take a set of editors written on top of Eclipse that implement syntax highlighting. \n Identify code in the implementation of these editors that contributes to the creation of a chain of \nreferences from an editor to a scanner.  Generalize this code to remove arbitrariness speci.c to individual \nimplementations.  This is the basic algorithm behind our synthesis tool; the rest of the section will \nelaborate more on the speci.c challenges that arise for each of the high-level steps outlined above. \n 1.1 Finding Critical Chains with DeLight The biggest challenge presented by the algorithm is that it \nrequires reasoning about the evolution of the heap with a level of precision that is hard to achieve \nby static analy\u00adsis given the scale of the frameworks and their aggressive use of dynamic dispatch and \nre.ection. Rather than an\u00adalyzing the program statically, we collect its execution traces and organize \nthem o.-line for e.cient processing of queries about the exercised program behavior. The algorithm outlined \nabove requires us to identify code in the implementation of existing editors that leads to the creation \nof a chain of references from one object, call it the source, to another object we call the target. The \n.rst step in this process is to .nd events in each concrete execution trace where the source and target \nobjects become linked by a chain of references. In the speci.c case of the editor and the scanner, we \nare looking for events in the execution such that before this event, the scanner cannot be reached from \nthe editor, but after the event it can. We call the reference created by this event the critical link \nbetween the editor and the scanner, Figure 3. Event o3.f . t establishes the critical link between s \nand t and creates two critical chains.  and any chain of references from the source to the target object \nthat was created as a consequence of adding the critical link is called a critical chain. Note that while \nthe critical link is unique, there can be many critical chains as illustrated in Fig. 3. Conceptually, \nwe can .nd a critical link by performing a depth .rst search from the source object after every memory \nwrite operation. Unfortunately, this na\u00efve strat\u00adegy is as ine.cient as it sounds; the cost of running \nsearch over the entire heap after every memory update is prohibitive for all but the shortest of traces. \nInstead, our data management engine DeLight uses abstraction to organize and index execution data in \na way that makes critical chain computations e.cient. The .rst insight exploited by DeLight is that it \nis possible to represent the sequence of heaps generated by the program after each memory update with \na single graph we call a heap series graph,where nodes correspond to objects, and every edge corresponds \nto a reference from one object to the other, labeled with the time interval when this reference existed. \nGiven a path in this graph, we can .nd the set of time steps where this path was present by taking the \nintersection of the time intervals for every edge in the path. The heap series graph provides a concise \nrepresentation of the overall evolution of the heap, but it is too big to explore e.ciently. This leads \nto the second insight exploited by DeLight: the heap series graph can be abstracted into a coarser graph \nwhere each node or edge represents a set of objects or references. The abstraction is conservative; any \npath in the original heap series graph has a corresponding path in the abstract graph, but the abstract \ngraph also contains spurious paths. Thus, if a critical chain is found in the abstraction, then it needs \nto be checked against the complete graph to ensure that this critical chain is real. Sec. 4 describes \nthis algorithm in full detail. Thanks to the use of abstraction, our algorithm answers the critical chain \nquery in under .ve seconds for 100 GB of trace data (see Sec. 6.2).  1.2 Synthesizing Code from Critical \nChains The critical chain computation produces a set of events that creates a chain of references from \nthe source to the target object, but these events may belong to the framework code and cannot be invoked \ndirectly by the user. What the synthesizer is looking for is the set of actions including API calls, \n.eld updates, class instantiations, etc. that the user code needs to e.ect in order for those events \ninside the framework to take place. Our algorithm is described in Sec. 5.1 and is based on a dynamic \nform of thin slicing [15]. The slice is the set of events relevant to the creation of the critical chain. \nBy separating events that took place in the user code from those that took place inside the framework, \nthe algorithm identi.es the subset of the events in the slice at the boundary between them: relevant \nframework events from the point of view of the user, and methods in the user code that are called by \nthe framework. The slices that result from each trace contain informa\u00adtion about how the interaction \nwas established in that particular trace. Some of the information in the slices, however, may be too \nspeci.c to a particular use. Our system copes with this by projecting the slices to remove extraneous \ndetails about the structure of the di.erent user implementations. This exposes a large degree of sim\u00adilarity \nbetween them, and allows us to identify distinct patterns of interaction. We explain the formal model \nof the program trace data in Sec. 2 and its implementation in Sec. 3. The critical chain and synthesis \nalgorithms are described in Sec. 4 and Sec. 5. We conclude the paper with an experimental evaluation \nin Sec. 6.  2. DeLight Data Model DeLight relies on three complementary views of execu\u00adtion data. The \n.rst is the call tree presentation, which directly models the sequence of instructions executed by each \nthread and the nesting of method calls (Sec. 2.1). While it provides detailed information focused around \nany particular point in the execution, it makes it di.cult to answer global queries without looking at \nthe entire trace. To support heap connectivity queries, DeLight provides a complementary graph-based \npresentation that provides a global view of the evolution of the heap. We call this presentation a heap \nseries (Sec. 2.2). The two presentations are connected via time stamps that are assigned to every program \ninstruction. In order to make queries on the heap series graph more tractable, we introduce heap abstractions \nthat capture the essential domain information and approximate the heap series to reduce its size (Sec. \n2.3).  2.1 Call Tree Presentation This presentation is essentially a sequence of events. An event is \ntriggered for every state update and every transition across method boundaries: Type Description a . \nb.f a . f a . b[i] Read of value a from .eld f of object b. Read from a static .eld f. Read of value \na from array b. b.f . a f . a b[i] . a Write of value a into .eld f of object b. Write to a static .eld \nf. Write of value a into array b. call m(p) return a throw e Method enter. Normal exit of a method. Exceptional \nexit of a method. The sequence p in method enter events is the sequence of parameters to the method \ncall, starting with this for non-static methods. Values V in our model consist of object instances, the \nspecial value null, and primitive values (integer, void, etc.) type(a) denotes Java type of value a. \nEach event is assigned a unique timestamp (or as we call it later, its time), which among other things \nallows us to assign a total order to events executed by di.erent threads. Conceptually, method enter \nand exit events for a single thread form a call tree where the leaf nodes are the state reads and writes. \nThis presentation provides a pre-order traversal of the call tree, allowing us to query information in \nthe dynamic call scope of any given event as used, for example, in computing slices. 2.2 Heap Series \nPresentation The call tree presentation is useful for analyzing call patterns, but when reasoning about \nthe evolution of heap connectivity, a graph-based view is more desirable. A heap H in our model is a \ndirected multi-graph on the set of values V and edges labeled by .elds F. An edge f a -. b denotes the \nfact that the value of non-static .eld f of an object instance a is b. We use set algebra on the set \nof edges to describe updates to a multi-graph. The e.ect of an event on the heap is the addition and \nremoval of edges. Starting from the empty heap H0, we build a sequence of heaps {Ht} by applying t-th \nevent to Ht-1. The rule for the .eld write event is: ff [ b.f . a] H =H\\ (b -. V). (b -. a) (all f edges \ncoming out from b are removed and an f edge from b to a is added.) We do not store static .eld writes \nin the heap model, and the remaining events have no heap side-e.ects. The sequence of heaps {Ht} is compacted \ninto a heap series presentation H , which is a directed multi-graph on the set of values V and edges \nlabelled by pairs of .elds F and non-empty time intervals: F\u00d7 2Z. Here an edge records all indexes t \nfor which Ht has an edge labelled by f between the nodes: (f,T ) a ---. b .HH whenever there was an edge \nin one of the heaps: f T = {t | a -. b .Ht}.|T | > 0 The heap series model and the call tree model are \nconnected by the times of the events. This allows us to quickly jump from a time on some edge in H to \nthe call stack for the corresponding event and vice versa. Time intervals are represented either as decision \ntrees or as unions of disjoint segments. The motivation for using time intervals instead of multiple \nheaps and the real gain in compacting heap series come from a simple observation: most .elds are not \nupdated frequently. Abstract .elds Containers are pervasive in Java code, and their simple interfaces \nencapsulate complex heap representations. DeLight approximates the internal be\u00adhavior of container objects \nvia abstract .elds. For exam\u00adple, we model lists with the binary relation content .F, content where a \n----. b if a is an instance of java.util.ArrayList and b is an element of the list a. Similarly, the \nabstract values .eld values .F matches a map to its values: a ---. b if a is an instance of java.util.HashMap \nand b is a value for some key in the map a. Finally, the abstract .eld array relates an array object \nto its elements. Figure 4 describes how DeLight computes heap series H by observing only the method enter \nand exit events for collection classes. For example, List.remove takes a position as an input and returns \nthe removed list element at that position, allowing us to deduce the e.ect on the heap without knowing \nthe list content ahead of time. Note that unlike concrete .elds, abstract .elds may have multiple edges \nbetween two instances (e.g. if a list contains duplicates) or many outgoing edges from an instance (since \ncontainers usually have many elements.) Using only method events does not let us build an absolutely \nprecise model; DeLight does not, for example, handle element position in a list, removal via an iterator, \nor null value in a map.  2.3 Heap Abstractions The heap series graph H is quite large: the number of \nnodes and edges easily reaches millions. To support e.cient computation on H , DeLight provides several \nheap abstractions aimed to further reduce the size of the graph using semantic domain knowledge while \npreserving properties of interest. We use heap abstractions to derive approximate answers to queries \nwhich we then re.ne by selectively querying H . The critical property is the Figure 4. A selection of \nthe heap update rules for abstract .elds: here l is an instance of java.util.ArrayList; k, v, and o are \narbitrary values; m is an instance of java.util.HashMap.  [ call List.add(l, o)]]H = H . (l content----. \no) [ call List.add(l, i, o)]]H = H . (l content----. o) where i . Z [ call List.clear(l)]]H = H \\ (l \ncontent----. V) [ call List.remove(l, o)]]H = H \\ (l content----. o) [ call List.remove(l, i); return \no] H = = H \\ (l content----. o) where i . Z [ call Map.put(m, k, v); return o] H = = H \\ (m values---. \no) . (m values---. v) [ call Map.clear(m)]]H = H \\ (m values---. V) [ call Map.remove(m, k); return o] \nH = H \\ (m values---. o) heap connectivity: if two values are connected by a path in Ht for some t, \nthen they are connected in the heap abstraction A(HH) of HH. In the heap series HH, we call a simple \npath (f1,T1)(fk,Tk) a0 ----. ... - ---. ak viable if the edge intervals share a common time. The lifetime \nof the path is the intersection niTi of these time intervals. Viable paths have non-empty lifetimes. \nOur abstraction techniques are all based on the idea of summarizing groups of objects and groups of .eld \nconnections akin to type-based .eld-aware static anal\u00adysis [2]. The research question is what makes a \ngood group of objects and how to merge edges in HH. We at\u00adtempted to answer this question by empirically \nanalyzing heap series graphs while keeping the end-to-end perspec\u00adtive of how DeLight is used by MatchMaker. \nOur requirement for the abstraction is somewhat di.erent from static analysis since our goal is to optimize \nqueries rather than prove properties. We use terminology of graph homomorphisms to describe our technique. \nGraph homomorphisms are natural graph transforma\u00adtions that preserve connectedness, and they are charac\u00adterized \nby two functions: cluster that maps nodes of HHto nodes of A(HH) and relabel that maps edge labels of \nHHto edge labels of A(HH). The soundness of these abstractions rests on the following property: for any \nviable path ai in HHthere exists a viable path bi in A(HH) such that both paths have the same length \nand bi = cluster(ai). The reverse is not always true: there are false candidate abstract paths that have \nno concrete counterparts. We call this process of taking a candidate viable path in the abstraction and \nattempting to .nd a concrete path concretization. Since viability of paths is determined by the time \nintervals on the edges, relabel must respect these intervals. A simple way to ensure that is to only \nallow over approximation where a time interval is mapped to a larger set of times. If a group of edges \nis mapped to the same edge in the abstraction, then the abstract edge time interval must at the very \nleast include the union of the concrete time intervals. Our graph homomorphisms are composed in the order \nthey are described. The composition respects the prop\u00aderty as long as each individual transformation \ndoes. The quality of abstraction is the measure of how many false candidate paths are introduced. We \nfound the follow\u00ading techniques e.ective in reducing the size of the heap abstraction, our main goal, \nbut having high quality of abstraction. Type-based abstraction Type-based clustering is ap\u00adpealing since \na typical heap connectivity query is con\u00adcerned with the types of the end-points rather than concrete \ninstances. Intuitively, objects of the same type play the same role in the way they interact with the \nrest of the heap. This abstraction introduces many false paths for contain\u00aders, for which the type carries \nvery little information. For example, if all maps are deemed equivalent, any object pointing to a map \nreaches any value of any map. There\u00adfore, we parametrize our abstraction by exception types, for which \ninstances are not merged. Collection types are the only exception types in our implementation. To summarize, \ntype-based abstraction is de.ned as: o for exception types cluster(o)= type(o) otherwise Field abstraction \nEdge labels in the heap series are pairs of a time interval and a .eld. Field abstraction strips the \n.eld from the pair, leaving only the time interval: relabel((f, T )) = T The e.ect is that viable paths \nremain viable but they lack information about .elds between every two consecutive clusters. The job of \nthe concretizer is to select .elds that would connect objects belonging to the clusters of the candidate \npath. Multiple such .elds are possible if the .elds are declared in the same class and have the same \nvalue type. Time abstraction The edges in the resulting abstrac\u00adtion are labelled with just time intervals. \nConceptually, this abstraction graph is now simple since the time inter\u00advals can be merged together with \nthe set union operation. Time abstraction amounts to expanding the merged time intervals to larger sets: \nrelabel(T )= T I, where T . T I.  Figure 5. E.ect of the time abstraction: Y -axis is the number of \nedges with N or less contiguous ranges in their time intervals. This operation increases lifetimes of \nall paths, and so it preserves viable paths. The gain comes from the smaller representation of the time \nintervals. We represent them with a list of disjoint half-open ranges like [l, h). A frequently updated \n.eld has an irregular time interval with many such ranges. Determining viability of a path requires taking \nset inter\u00adsection of edge time intervals along the path. When a complex time interval occurs on the path, \nit adversely a.ects the computation time. Moreover, since we are dealing with large data structures, \nmemory representa\u00adtion becomes important. Complex time intervals cannot be simply represented in memory \nand so require more space. Time abstraction applies a simple expansion to ai exn lh every range [l, h) \nin the interval:\u00b7 r, \u00b7 r (where rr r is a tunable parameter). When the distance between two consecutive \nranges is smaller than r, they collapse into a single range. Figure 5 (red) shows the number of such \nranges in time intervals in a real heap series built from trace e_3_25 (see Sec. 6.1). After expanding \nedges with more than 10 ranges with r = 128* 1024, the edges with irregular time intervals disappear \nfrom the abstraction (blue).  3. DeLight Collection and Storage The basic architecture of DeLight system \nis shown in Fig. 6. The components of our system are: the trace collector that extracts traces from executions \nof programs, the query engine that organizes traces and exposes the high-level interface, and the lower-level \n Figure 6. System architecture Figure 7. Collection agent storage databases. MatchMaker is the target \nclient of DeLight. The work .ow of building a database of program behav\u00adiors consists of four steps: \n1. Executing the subject Java program with a dynami\u00adcally instrumenting Java agent (Sec. 3.1). 2. Interacting \nwith the program while the agent records the raw log. 3. O.-line processing of the raw log and storing \nthe data in databases (Sec. 3.2). 4. Registering the new trace with the query engine under a single \nquery interface (Sec. 3.3).  3.1 Instrumentation Instrumentation is performed dynamically using ASM \nbytecode instrumentation framework [1]. Every applica\u00adtion class is modi.ed upon loading to record its \nevents in a raw log .le. Once all the desired functionality of the program is exercised, the collection \nagent processes the log .le and bulk inserts the data into a relational database (see Fig. 7). Filtering \nevents Signi.cant portion of events are safely ignored without degrading the quality of the syn\u00adthesized \ncode. Statements are instrumented selectively emphasizing object interactions in the application code \nas opposed to internals of the language library. First, the trace collector does not distinguish between \nprimitive values and treats them all as a single abstract value. In a high-level language like Java, \nmuch of the program logic is encoded in the interaction between objects rather than in the manipulation \nof primitive values.  Second, the recording framework ignores most of the events related to classes \nin the Java standard library. It does not record any .eld reads or writes to instances of java.* and \njavax.* classes. Among JDK classes, it only records public method boundary events for commonly used collection \nclasses such as java.util.ArrayList and java .util.HashMap. The rest of the program (e.g. application \npackages such as org.eclipse.*) is fully recorded. Since the collector instruments the methods themselves, \ncallbacks from the standard library to the application are still recorded. Concurrency DeLight handles \nconcurrent programs by serializing events from multiple threads with a lock to the single raw log. Thus, \nMatchMaker produces code that may potentially be executed concurrently since it uses events from DeLight \ndatabase. Currently, De-Light does not log synchronization statements. We have not yet found it to be \nproblematic since frameworks like Eclipse manage thread coordination themselves, reliev\u00ading user code \nfrom manual concurrency control. With\u00adout thread synchronization information in the database, MatchMaker \ncannot generate correct synchronization statements in the cases where user thread coordination is required. \n 3.2 Data Representation The two data model presentations call for di.erent kinds of representations. \nCall tree view closely matches the relational table form. All events are stored in a single nested-set \ntable that has columns for the timestamp, thread, stack depth, successor (matching method exit event), \nand event-speci.c details. Parameters to method calls and object to type mapping are stored in separate \ntables. The underlying RDBMS engine is MySQL 5.1. The tables are augmented with specialized database \nindexes to optimize slicing queries. For graph-based heap series, we use the graph database Neo4J 1.2 \n[12]. This database is optimized for graph traversals which is the main use-case in DeLight. The heap \nseries is built by replaying the entire event sequence. Heap abstractions are represented using in-memory \ngraph data structures and are built from the heap series. 3.3 Bene.ts of the Data-Driven Approach The \nadvantage of a data-driven engine is that it scales well by partitioning the trace data and running the \nqueries on each partition in parallel. In addition to trace-level parallelism, each individual trace \ncan be split into several smaller traces in order to balance the lengths of traces across partitions. \nThe more traces are put into DeLight, the more queries it can potentially answer since more of the program \nbehavior is exercised.  4. Critical Chain Algorithm MatchMaker algorithm performs three di.erent kinds \nof analysis on the data in order to generate code. First, it .nds critical chains connecting objects \nof the two given types as we describe in this section. Then it computes a slice based on the events in \neach critical chain, and .nally, it synthesizes the code from the slices (Sec. 5). A chain is a simple \npath in the heap connecting objects via directed edges labeled with concrete or abstract .elds. As the \nheap evolves over time, chains form and disappear, but with the entire sequence of heaps at hand, we \nare able to determine the earliest moment when the two given objects get connected by a chain. We call \nthe event corresponding to this moment the critical event, and any chain between the two objects created \nby the critical event is called a critical chain (recall from Fig. 3 that there can be more than one). \nFormally, a critical event occurs at the minimal time t for which there is a chain in heap Ht connecting \nthe two objects of interest. Any chain in Ht connecting the two objects will be a critical chain. We \nformulate the critical event query as a data .ow equation on HH. Let us denote the time interval during \nwhich objects a and b are connected via some viable chain as viable(a, b). It is the union of lifetimes \nof all viable paths between a and b, as described by the following inductive de.nition:  viable(a, b)= \n(viable(a, c) n T ) (f,T ) c---.b.H (union is taken over incoming edges of b). The right hand side is \nmonotonic in viable(a, \u00b7) and, thus, could be used for the least .xed point computation with initial \nvalues viable(a, b)= . for a b = and viable(a, a)= T. The minimal time in viable(a, b) is the critical \ntime for a and b. However, applying the equation directly to HHis intractable due to the size of the \ngraph (millions of objects), the required number of iterations (long paths in the heap), and complex \ntime intervals (millions of .eld writes). Thus, we have developed an algorithm for computing critical \nchains that makes use of heap abstraction A(HH) to drive an exhaustive graph search on heap series HH. \nBy framing the problem as graph search, we can also do more than just .nding the very .rst critical chain. \nWe give the user the ability to search for the subsequent chains, providing an iterator-like interface \nto  Input: chain {b0,...,bk} in A(H ), graph view G of H for i =0 to k do choose ai such that cluster(ai)= \nbi and {a0,...,ai} is a simple, viable chain in G return {ai} or the shortest unsatis.able pre.x of {bi} \nFigure 8. Algorithm Concretize. Input: clusters s, t .A(H ), graph view G of H AG . restriction of A(H \n) to duration time interval of G and subgraph of nodes reachable from s and to t Q . ({s})while Q is \nnon-empty do b = {b0,...,bk}. Q if bk = t then match Concretize(b,G): case a in G . return a case pre.x \nc r b . remove all p . Q such that c r p else for all bk . o .AG do if b = {b0,...,bk,o} is simple and \nviable then Q . Q . b Figure 9. Algorithm Search query chains between two objects. The algorithm consists \nof two parts: Search that performs a traversal of A(HH) to .nd candidate paths and Concretize that checks \nwhether the candidate paths have concrete counterparts in HH. 4.1 Concretization Algorithm Concretize \n(Fig. 8) is the necessary valida\u00adtion routine that takes a candidate chain in A(HH) and attempts to .nd \na corresponding chain in HH. In addition to a candidate chain, it takes as input a graph view of HHthat \n(1) excludes certain set of edges, (2) restricts edge time intervals to a certain duration time interval. \nThe algorithm iteratively expands nodes in HHthat are in the view and match the candidate chain in A(HH). \nIf it fails to .nd the full concrete chain, it reports the shortest unsatis.able pre.x of the candidate \nchain. 4.2 Graph Search Algorithm Search (Fig. 9) is an all-paths breadth-.rst search algorithm on A(HH). \nIt takes source and target clusters in the abstraction graph, and the graph view of HH. It builds a queue \nof candidate chains in A(HH) in a breadth-.rst fashion, calling Concretize when the end node of the chain \nmatches the target cluster. If concretization succeeds, the algorithm stops; otherwise, it eliminates \nall chains in the queue that start from the same unsatis.able pre.x. 4.3 Query Interface Combination \nof graph search and concretization allows us to de.ne the following arsenal of high-level queries for \nenumerating meaningful chains in program traces. Find a chain between two types The .rst time the user \ntries to search for a critical chain, he/she may only have the types of the end points in mind. Type-based \nclustering in the heap abstraction is particularly suitable to this kind of query. To answer such query, \nwe simply execute Search on HHwith the two input types. Find a critical chain between two objects Once \nwe have one chain between two objects, we can minimize the critical event time by iterative execution \nof Search on a graph view of HHwith the time duration [0,t), where t is the critical time of the previous \nchain. The view also restricts the source and end objects of the chain to the ones in the previous chain. \nThe algorithm converges when no new chain is found. Find the next chain Sometimes,the .rst chain might \nnot be the one that is desirable. Therefore, one may need to .nd a subsequent chain by specifying an \nedge to drop from the current chain in HH. This is done by passing a graph view to Search that skips \ncertain edges in HHand restricts time intervals to [t, 8), where t is the critical event time of the \n.rst chain. 4.4 Bene.ts of the Heap Abstraction Apart from signi.cantly reducing the size of the graphs, \nheap abstractions improve the search algorithm. First, the queue of abstract paths is a more compact \nrepre\u00adsentation than a queue of concrete paths: they share pre.xes, have simpler time intervals, and \ndo not have .elds. Second, concretization limits expansion to only a subset of concrete nodes improving \nperformance of the graph database (since heap series can only be partially loaded into memory, it must \nreside on disk). Finally, non-viable abstract paths are eagerly eliminated during search. For example, \nthe query used in the user study (Sec. 6.4) eliminates around 10% of abstract paths each accounting for \nall concrete paths having it as a pre.x.  5. MatchMaker Synthesis Algorithm The goal of the synthesis \nalgorithm is to produce user code that when called by the framework causes a critical chain to form. \nIts starting point are the critical chains computed in Sec. 4. Each critical chain identi.es the set \nof events that created each of its links, but these events are usually deep inside the framework and \ninvisible to the user. In order to derive user code from these events, the algorithm follows a two-step \nprocess: (1) .rst, it identi.es for each critical chain the actions of the user code that led to its \ncreation, and (2) then it generalizes from those speci.c instances to produce the essential code needed \nto create the chain. The .rst step is achieved through a form of slicing, while the second step is achieved \nthrough a new projection algorithm that eliminates trace-speci.c details from the slice. The sections \nthat follow elaborate on each of these steps.  5.1 Dynamic Thin Slicing The synthesis algorithm relies \non a dynamic form of thin slicing, an approach to slicing pioneered by Srid\u00adharan et al. that has been \nshown to be very e.ective when computing slices for the purpose of program un\u00adderstanding [15, 19]. The \nbasic observation behind thin slicing is that traditional slices contain too much in\u00adformation; for example, \na slice for an operation that retrieves an element from a data-structure will contain not only the event \nthat inserted the element into the data-structure but also many other events that modi\u00ad.ed the data-structure, \nincluding many that added and removed other unrelated elements. Thin slicing avoids many of these irrelevant \nevents by ignoring value .ows to base pointers of heap accesses; for example, for a .eld write b.f . \na, thin slicing only follows value .ows to a, whereas a traditional slicing algorithm would also follow \nvalue .ows to b. Thin slicing also does not follow control dependencies. In some cases, this may lose \nimportant information, so in the original application of thin slicing, the programmer was given the ability \nto explore some of these value .ows to base pointers or follow some of the control dependence to get \na better understanding of the program behavior (they call these expansions of thin slicing). In our case, \nthe algorithm cannot rely on the programmer to decide when it might be useful to follow value .ows to \nbase pointers, so instead it applies a set of simple heuristics to do automatic expansion based on the \nobservation that the synthesis algorithm cares primarily about user code and not so much about what happens \nin framework code: When slicing on value a the assignments b.f . a or a . b.f causes the algorithm to \nfollow the base pointer b if and only if: (a) the statement is within a framework method, but the value \nof b is ofa user-de.ned class, or (b) the statement is within a user method, and the producer statement \nand the consumer statement of the value are not in the same method.  When examining a statement e in \nuser code, walk through the call stack of e and .nd the nearest call oI.f() where f is de.ned in user \ncode and the caller is part of the framework. Add the dependency between  I the call event and e through \nobject oto the slice. The rationale for including this dependency in the slice is that object oI must \nbe correctly set in order for the framework to call the right method f, so e has a control dependency \non oI to the call event. We use the symbol S to refer to the dependency relation oI computed by the slicing \nalgorithm. A tuple e -e . belongs to S whenever the event e is a producer of the object o which is then \ndirectly consumed by the class WidgetViewer { // Framework class void main() { init(); // e1 initViews(); \n// e6 } void init(...) { Widget u = ... // e2 u.init(); // e3 } void initViews(...) { Widget u = ... \n// e7 u.regViews(); // e8 } static void addView(View x) { ... } } class MyWidget extends Widget { // \nUser class private MyView f; @Override void init() { MyView x = new MyView(); // e4 this.f = x; // e5 \n} @Override void regViews() { MyView x = getView(); // e9 WidgetViewer.addView(x); // e12 } private MyView \ngetView() { MyView x = this.f; // e10 return x; // e11 } } Figure 10. Example code. I event e.1 Figure \n11 shows a fragment of the slice for the statement WidgetViewer.addView(x) in Fig. 10. In this simple \nexample, we assume that MyWidget and MyView are user classes while WidgetViewer, Widget, and View are \nframework classes. The approach de.ned above raises an important ques\u00adtion: how does the system know \nwhere to draw the boundary between user and framework code? Match-Maker draws the boundary by relying \non the Java package mechanism. In Eclipse, each plug-in corresponds to a bundle of packages. For example, \nthe Eclipse Java Development Tools plug-in corresponds to a bundle con\u00adtaining all packages with the \nname org.eclipse.jdt.*, and the TeXlipse plug-in corresponds to a bundle containing all packages with \nthe name org.texlipse.*. For each plug-in in the DeLight program behavior database, we man\u00adually specify \nits corresponding bundle. The classes in the bundles are potential user classes, and classes not in any \nbundle are considered framework classes. When MatchMaker .nds a critical chain from object A to object \nB, if the class of A is in some bundle X, then MatchMaker treats all classes in X as user classes. 1 \nWe use the notation from dependence analysis of drawing arrows from a producer to a consumer, rather \nthan the arguably less intuitive convention, sometimes used in the slicing literature, of drawing arrows \nbackwards.  (x) in Fig. 10. Shaded events are framework events. MatchMaker does the same thing to B, \nand all other classes are treated as framework class. By this means, MatchMaker gets the framework-user \nseparation and uses it to guide slicing and projection.  5.2 Projection to User Code The slice created \nby MatchMaker contains all the statements that were necessary to create the critical chain in a particular \nheap. However, a lot of the code in the slice belongs to the framework, and is therefore of no interest \nto the user. Additionally, the slice contains many details that are too speci.c to a particular example, \nsuch as transitive copies of objects through internal .elds, or calls to functions internal to the user \ncode. MatchMaker addresses this problem by computing a projected slice that eliminates framework code \nas well as super.uous details from the original slice. The projection process also has the e.ect of normalizing \nthe slices, making it easier to compare the results from many di.erent traces. Speci.cally, projection \ncauses many slices to become identical, and those that remain di.erent usually correspond to di.erent \nways of using the framework. The essence of the interaction between user and frame\u00adwork code is captured \nby calls that cross the framework\u00aduser boundary. Calls from the framework to the user are called fu-calls, \nand calls from the user code to the frame\u00adwork are called uf-calls. In the example from Fig. 10, e3 and \ne8 are fu-calls, while e12 is a uf-call; the rest of the calls, like e9 are termed l-calls, because they \nstay local to either the framework or the user code (see Fig. 12). fu-calls and uf-calls describe the \ninteraction between framework and user: fu-calls tell us which classes to extend and which method to \noverride, while uf-calls tell us which APIs to call. The projected slice must Figure 12. User code and \nframework interaction. preserve information about fu-calls and uf-calls as well as relevant user code, \nbut simplify away the complexity and arbitrariness of the speci.c sample code from which the database \nwas built. For the running example, the projected slice will indicate that init() must write to f and \nregViews() must pass the value of f to addView(), but the fact that this involves a call to getView() \nis only a detail of this particular example and not relevant in the projected slice. We .rst introduce \nseveral de.nitions to talk about the dynamic structure of calls. For any user event e, the cover of e \nis the latest enclosing fu-call of e. In other words, all calls on the stack from cover(e) to e are user \nl-calls except for cover(e) itself which must reside in the framework code but invoke a user method. \nFor example, in the sample program e3 = cover(e5) and e8 = cover(e10). Dually, we de.ne the cover of \na framework event to be its latest enclosing uf-call. We say that an event e is a framework event (written \nas F(e)) if it is generated by a statement in the framework code; otherwise, the event is a user event \n(or U(e)). The local producer of a user event e and an object o, consumed by e, is the earliest event \nthat produced object o and has the same cover as e. The local producer is always a user event. For example, \nthe local producer of the object x in e12 is e10 since they have the same cover e8. The projection algorithm \ntakes the set of events E establishing the critical chain as a starting point. It uses the slicing relation \nS and the function cover, both provided by DeLight, to compute the projected slice E consisting only \nof user events. The algorithm consists of two steps: the base step computes the initial set E from E \nand the iteration step expands E to reach the least .xed point. Base step First, every user event from \nE is included: {e | e . E .U(e)}. Next, from the set E* = S*(E) of all events in the thin slice, the \nalgorithm adds the following groups of user events:  covers of the framework events: {cover(e) | e . \nE * .F(e)} return events of the covers of the user events: {return event of the call cover(e) | e . \nE * .U(e)}  user events that produce data consumed by the framework events:  I o {e I | e . E * .F(e) \n.U(e I) ..o : e -}. e .S Iteration At every step, the algorithm picks an event e . E and an object o \nthat it consumes. If there is no I o local producer, the producer eI (i.e. e. e .S) must lie -outside \nof the call cover(e). In that case, the algorithm adds eI to E only if eI is a user event. If there is \na local producer for e and o, then the algorithm adds it only if that local producer got the value from \nthe heap or from a constructor. In the example, this is relevant when the algorithm picks event e10 and \nobject this; in that case, the local producer of this is event e9, but e9 gets the value from the parameter \nlist of the call e8, so e9 is not added to E . Using the above rules, MatchMaker computes the least .xed \npoint of E . It is easy to synthesize code from the projected slice E : the covers tell us which methods \nto override and the events in E tell us what instructions to put in these overridden method, while the \ndependency relation S glues the instructions together by data dependency and gives a partial order on \nthe instructions. In this sense, the projected slice is as expressive as user source code, but it elides \nlow-level details such as reordering of independent instructions. Just before synthesizing the code, \nMatchMaker havs two more things to do. First, MatchMaker .nds for each user class A the most generic \nframework class to extend from. For each fu-call method A.f, let A_f indicate the original declaring \nclass of f, then A must be a subclass of A_f; for each uf-call g(x0, x1, ..., xk) where x0 is the receiver \nand the rest xi s are actual parameters, if the original type of the j-th formal parameter (including \n0-th, the receiver) is A_j and xj is an instance of A, then A must be a subclass of A_j. MatchMaker extracts \nfrom fu-calls and uf-calls all these subclass constraints, and computes the join of these constraint \nto get a lower bound of class A, which is the most generic framework class for A to extend from. In fact, \na similar process is employed to determine for each used framework class B, the most generic class to \nuse in place of B. Finally, MatchMaker gives each user class A a pretty name: if A is determined to extend \nfrom C, then A is called MyC. MatchMaker names each variable of type X by making the .rst letter of X \nlower case and appending a unique number su.x. MatchMaker also renames each .eld X.f of type Y to X.fY \n(again add numbers to avoid con.ict .eld names). Unresolved variables are named ??, the holes in the \ncode returned to the user.  6. Evaluation We have implemented MatchMaker and performed a user study \n(see Sec. 6.4) that demonstrates the positive e.ect of our tool on developer productivity on a challenge \nproblem inspired by our motivating example. To assess the extent of validity of MatchMaker assumption, \nwe have also performed a generality case study (see Sec. 6.3). In the case study, we analyzed the quality \nof the code synthesized by the projection algorithm for a number of pairs of classes from Eclipse. We \nopen this section with a description of the experi\u00admental setup: the amount of trace data we collected \nand DeLight collection performance. Sec. 6.2 focuses on the critical chain algorithm performance and \ndemonstrates that DeLight .nds chains in just a few seconds on 100 GB of trace data, implying that DeLight \ncan be a foundation for an interactive tool like MatchMaker. 6.1 Experimental Setup We have collected \naround 100 GB of data from nine executions of Eclipse 3.6.1 with several plug-ins installed (see Fig. \n13). All executions except one took less than 5 minutes. We performed the following actions in Eclipse: \n1. clicking on items in the toolbar and menu; 2. editing Java and .properties .les; 3. editing BibTEX \nand TEX .les using TeXlipse; 4. editing a Python .le using PyDev; 5. editing a grammar using ANTLR \nIDE; 6. invoking a shortcut (e.g. to save a document); 7. navigating using outline viewer; 8. using \na spell checker to correct a word; 9. invoking auto completion while typing in an editor; 10. browsing \na CVS repository and its commit history; 11. running a sample RCP application; 12. running a sample \nEclipse editor plug-in application.  These actions were selected as a representative sample of what \na user may do in Eclipse, not as a sample of MatchMaker queries we expected to run. We did strive for \ncompleteness, however, in that we tried to exercise similar functionality (such as auto-complete) in \nall editors. The e.ect of instrumentation on the subject application is noticeable in the early phase \nof instrumentation when a large number of classes are loaded, instrumented, and veri.ed by the class \nloader. Once dynamic instrumen\u00adtation completes, the application runs slower but still at an interactive \nspeed, allowing us to use features like auto-completion without causing internal Eclipse time\u00adouts. The \nnumbers in Fig. 13 suggest that 1 minute of execution produces roughly 1 GB of data and requires 10 to \n20 minutes for o.-line processing.  Database # of Execution Processing SQL DB Heap series Heap abstrac- \nBuild Graph DB events time time size graph size tion size time size e_2_28 68 M <5 min 61 min 6.9 GB \n0.887 M / 1.675 M 50 K / 75 K 199 s 334 MB e_3_3 573 M 60 min 1076 min 57.3 GB 4.143 M / 9.098 M 225 \nK / 292 K 35 m 1.7 GB e_3_25 37 M <5 min 28 min 3.6 GB 0.495 M / 1.206 M 30 K / 48 K 159 s 223 MB e_3_26 \n81 M <5 min 76 min 7.9 GB 1.333 M / 2.747 M 67 K / 93 K 6 m 531 MB e_3_27 52 M <5 min 47 min 5.1 GB 0.599 \nM / 1.403 M 36 K / 57 K 200 s 262 MB c_3_28 24 M <5 min 19 min 2.3 GB 0.365 M / 0.840 M 22 K / 35 K 98 \ns 158 MB e_3_31 35 M <5 min 30 min 3.6 GB 0.686 M / 1.272 M 39 K / 56 K 181 s 255 MB t_4_3 29 M <5 min \n24 min 2.9 GB 0.416 M / 1.004 M 26 K / 41 K 121 s 186 MB h_4_4 3 M <5 min 3 min 395 MB 0.100 M / 0.230 \nM 4 K / 7 K 12 s 44 MB Figure 13. Collected execution data. For graph sizes, n/e means a graph with n \nnodes and e edges. Execution time is the running time of the subject application. Processing time is \nthe time to process and store a trace in MySQL. Heap build time is the aggregate time to construct heap \nseries and abstraction and store graphs Neo4J.  We used a quad-core machine with 7 GB RAM for DeLight \ncollection and MatchMaker queries. Both tools are implemented in Scala. We have allocated 5 GB to Java \nHotSpot 64-bit JVM 1.6.0. We ran Neo4J 1.2 graph database in the same JVM instance and MySQL 5.1 on the \nsame machine.  6.2 DeLight Query Performance To evaluate performance of DeLight critical chain queries, \nwe took a sample of pairs of types (see Fig. 14). For every such pair, we asked DeLight to .nd a chain \nfor every pair of subtypes on every database (nine of them) in succession. We report the total number \nof chains found and the average time per chain. If the chain was not found, it could be because of two \nreasons: Search ran out of the imposed execution time bound, or the chain was genuinely missing from \nthe collected data. We report the total number of time-outs (and also as a percentage of all failed queries). \nFor the remaining missing chains, we indicate the average time. We manually inspected the pairs that \ntime-out and concluded that the chains, indeed, are unlikely to be present (often, since the two end \ntypes belong to two distinct plug-ins.) For every chain, we executed the critical chain query. We show \nthe number of queries that time-out in Search and the average time for the remaining successful queries. \nNote that a timed-out critical chain query does not necessarily result in a wrong answer. It simply indicates \nthat Search was unable to prove non-existence of a path in a graph view. Even if the answer is not exact, \nthe current candidate chain returned by the critical chain algorithm is likely to be good enough for \nMatchMaker.  6.3 MatchMaker Generality Case Study We have performed the following experiment to evaluate \nthe generality of MatchMaker in the context of Eclipse. During the process of developing MatchMaker we \nhave learned four class pairs for which the tool generates adequate source code: Editor to Scanner, Editor \nto ICompletionProposal (an auto-completion choice in Eclipse), Menu to Action, and Toolbar to Action. \nThe synthesized code is close to the code that would be written by a human expert, except for two kinds \nof imperfections: Some method call parameters may not resolve and are shown as ?? (holes). The human \nneeds to consult the documentation and tutorials to .nd out the exact values for these holes.  Some \nstatements cannot be captured by the critical chain, such as the call to reconciler.setDamager() in the \nEditor-Scanner example (see Fig. 2). The critical chain chosen by MatchMaker passes through the link \nfrom the reconciler to the repairer, but not through the damager, so MatchMaker is unable to .nd the \ncall to setDamager. The human needs to manually add the missing statements.  We think that the additional \njob is relatively easy for the human, because she now has the knowledge of all the necessary classes \nand majority of the API calls, and by using them as keywords to search the tutorials and documents, she \ncan soon learn about the missing calls and .ll in the holes. In Figure 15, we show for each class pair \nthe extra changes the human needs to make. In addition to the above mentioned four class pairs, we tried \na dozen more pairs from Eclipse. We selected these pairs as follows: we took eclipse.jdt.internal.ui \nplug\u00adin, treated everything in eclipse.jdt.* as the user code, and extracted all framework classes that \nare extended and used inside this plug-in. There are more than two hundred of these classes. We examined \nthese classes manually and picked pairs of classes that seemed relevant to each other judging from their \nnames. We were able to pick 16 pairs of classes that we thought related with each other. Then we ran \nMatchMaker on each of these pairs, and examined the generated code to see whether it is acceptable glue \ncode to establish interaction between the pair of classes, and measure how far it is from the functionally \ncorrect version. For 12 out of these 16 pairs MatchMaker generates reasonable code; and for the remaining \n4 pairs MatchMaker generates empty code, but we have yet to .nd whether it is because of incompleteness \nof our program behavior database or because of limitations of MatchMaker hypothesis.  Source type Target \ntype Find a chain Find a critical chain (# of subtypes) (# of subtypes) found not found t.o. avg # avg \n(max) time t.o. (%) avg time time AbstractTextEditor (17) RuleBasedScanner (40) 46 1.3 s (4.6 s) 14 (0.2 \n%) 1 ms 10 1.6 s AbstractTextEditor (17) ICompletionProposal (44) 37 2.8 s (11.6 s) 20 (0.3 %) 1 ms 20 \n0.9 s WorkbenchWindow (1) PartEventAction (14) 74 1.4 s (20.6 s) 6 (11.5 %) 1 ms 66 147 ms Figure 14. \nCritical chain computation performance. Search execution was bounded by 30 seconds ( t.o. indicates the \nnumber of queries in which the algorithm ran out of time.)  Source type Target type # holes # miss Editor \nEditor Menu Toolbar Scanner ICompletionProposal Action Action 1 1 1 1 1 0 0 0 ITextEditor IContentOutlinePage \n0 1 MonoReconciler IReconcilingStrategy 1 0 ITextEditor QuickAssistAssistant 0 1 QuickAssistAssistant \nIQuickAssistProcessor 0 0 ITextEditor ITextHover 0 0 ISpellCheckEngine ISpellChecker 1 1 ITextEditor \nSelectionHistory 0 0 ITextEditor SemanticHighlighting 1 1 IContentAssist- ContentAssist\u00ad -Processor -InvocationContext \n1 0 ITextEditor IAutoEditStrategy 0 0 ITextEditor ContextBased\u00ad -FormattingStrategy 1 2 ITextEditor TextFile\u00ad \n-DocumentProvider 0 0 Figure 15. Generality evaluation. First column shows the names of the class pairs. \n\"# holes\" and \"# miss\" list the number of holes and missing statements in the generated code.  From \nFigure 15 we can see that for the pairs for which MatchMaker generates solutions, the resulting code \nis quite close to the correct answer. This suggests that the MatchMaker approach can be generally applied \nto a wider range and is not restricted to the elaborated example.  6.4 User Study We have conducted \na user study to measure the e.ect of MatchMaker on programmer productivity. The results of the user study \nshow that for the programming task we tested, MatchMaker improved programmer productivity by 49 percent \non average, and the improve\u00adment is statistically signi.cant. Task Description For our user study, subjects \nwere asked to implement Syntax Highlighting in an editor for a new language the Sketch language developed \nby our group. Speci.cally, the subjects were asked to implement syntax highlighting for two keywords \nin the language: implements and the operator ??. As a starting point, we provided them with an incomplete \nclass that extends and overrides RuleBasedScanner. The class implemented highlighting only for the keyword \nimplements, but they had to complete it and connect it to the editor by writing glue code like the one \nin Fig. 2. As can be seen from Sec. 6.3, the code generated by MatchMaker might not be perfect: it may \ncontain holes or miss statements. To observe how users deal with imperfect synthesized code, we intentionally \nchose a task for which MatchMaker generates code with a hole and a missing statement, so the MatchMaker \nuser had to look up the documentation and tutorials on the web to .ll the hole and add the missing statement. \nWe observed that adding the missing statement is the most time-consuming work item for MatchMaker users. \nWe could have chosen another task like matching Toolbar and Action, for which MatchMaker synthesizes \nnearly perfect code, and the MatchMaker users just need to copy and paste the generated code and do simple \nediting, and might perform even better, but that would not give us a comprehensive example of using MatchMaker. \nMethodology We recruited participants through mass advertising around the campuses of MIT and Harvard \nwith the promise of two free movie tickets for any participant who attends the study. When participants \narrived, they were randomly assigned to one of two groups: those in the control group were simply given \na description of the task and were told they could use any information available on the Internet to help \nthem complete the study; those in the experimental group were given an additional 10 minutes to review \na tutorial on MatchMaker. The tutorial showed how to use MatchMaker on an unrelated matching problem. \nThe subjects in the experimental group were advised to consult both MatchMaker and the tutorials and \ndocumentation on the web because the result given by MatchMaker may contain holes or miss important statements. \nSubjects in the control group did not know they were in the control group, or even of the existence of \nMatchMaker; they were led to believe that the purpose of the study is simply to analyze programmer s \nuse of the Eclipse framework.  The work environment was a virtual machine created with VirtualBox. The \nvirtual machine was set up with Eclipse IDE, Google Chrome browser, and other fre\u00adquently used applications \nto allow users to read various documentation formats. The virtual machine was set up to do screen-captures \nat 1 frame per second; this, together with the local history feature of Eclipse IDE and the subject s \nbrowsing history gave us a very com\u00adplete picture of the programmer s actions during the user study. \nSubjects A total of nine subjects completed the user study. Four of them were randomly assigned to the \nexperimental group (we will call them MatchMaker users M1, M2, M3, and M4), four others (we will call \nthem C1, C2, C3, and C4) were assigned to the control group, and the other one was an expert (E0) in \nthe Eclipse framework. All subjects in both experimental and control groups had competent Java programming \nexperience, but none of them had ever written any plug-in for Eclipse or similar object-oriented frameworks. \nOn the other hand, E0 had .ve years of experience writing Eclipse plug-ins it is safe to say that he \nknows signi.cantly more about the Eclipse framework than we do. He was .rstly assigned to the experimental \ngroup and given the MatchMaker tool, but he never used it because he was extremely comfortable writing \nEclipse code without the aid of the tool. After he .nished the task he told us in the questionnaire that \nhe was an expert. Therefore we put him into a separate category. Aside from the nine subjects who .nished \nthe task, there were another three subjects (X1, X2, X3) who did only part of the task and were excluded \nfrom the result: X1 was assigned to the control group. He spent around 40 minutes but still did not .nish \nthe glue code, and then he was distracted by a phone call and had to quit the study. X2 was assigned \nto the experimental group. He spent about 20 minutes and successfully established partial connection \n(see Phase 2 of Sec. 6.4.2) between editor and scanner, and then he also quit because of a phone call. \nX3 was assigned to the control group. He spent around half an hour but did not .nish the glue code, and \ncomplained to us that he was totally clueless and felt that he would never .nish the task , and then \nhe quit. 6.4.1 Result Figure 16 shows time spent on the task by each subject; the total time consumption \nis split into four work items: Glue Coding is the time spent writing and debugging glue code to match \nthe Editor class with the Scanner class. By glue code we mean the source code that pieces components \nin the framework together, such as the code shown in Fig. 2. Web Browsing for Glue Code is the time spent \nbrowsing the web (including searching and reading the documentation, tutorials, etc.) to .nd out how \nto write glue code. Task Coding is the time spent writing task code to complete the Scanner. Task code \nis the source code in the subclass of RuleBasedScanner that identi.es the two keywords. Web Browsing \nfor Task Code is the time spent browsing the web to .nd out how to write task code. It is hard to completely \nseparate the web pages that are related to glue code from those related to task code, so we use a simple \ncriterion: we can easily tell whether the programmer was writing glue code or task code, so any web browsing \nimmediately before writing glue code is considered Web Browsing for Glue Code, and any web browsing immediately \nbefore writing task code is considered Web Browsing for Task Code. Figure 16 also shows two aggregate \nresults: Match-Maker Users Average and Control Subjects Average, which were computed by taking arithmetic \naverage of the experimental group and the control group, respec\u00adtively. Web Browsing for Glue Code and \nGlue Coding together form the time spent on the matching problem (connecting two classes, Editor and \nScanner, together). From the .gure we can see that MatchMaker improves productivity by reducing the time \nspent on the match\u00ading problem. On average, control subjects spent 82.25 minutes writing the glue code \nto match Editor with Scanner and 98 minutes to .nish the whole task, while MatchMaker reduces these times \nto 35.75 minutes (56% improvement) and 50.25 minutes (49% improve\u00adment), respectively. The Wilcoxon Rank-Sum \ntest shows that the di.erence between the times spent on the whole task by the control group and the \nexperimental group is statistically signi.cant (p-value=0.03), and the di.erence between the times spent \non the matching problem by the control group and the experimental group is also statistically signi.cant \n(p-value=0.03). Even if we add 10 minutes on reviewing the MatchMaker tutorial to the total time spent \nby each MatchMaker user, the experimental group still performs faster than the control group, and the \np-value of the Wilcoxon Rank-Sum test remains 0.03,  indicating a statistically signi.cant di.erence \nbetween the two groups.  6.4.2 Observations We pick a representative MatchMaker user M3 to describe \nin detail the process of using MatchMaker to solve the task. The process is naturally split into four \nphases: Phase 1 M3 started by entering a query TextEditor RuleBasedScanner into MatchMaker, and got the \nsynthesized code after less than one minute. Then he copied and pasted the code into his project, and \nsplit it into several class .les. At this point he had to .ll the hole in the statement reconciler.setRepairer(dr, \n??), and he did not know that there is a missing statement reconciler. setDamager. Phase 2 M3 then spent \naround .ve minutes browsing the web to .nd out the value for the hole, and then he was able to connect \nthe editor with the scanner and make the program run. Because setDamager was missing, this was only a \npartial connection: with only repairer but no damager set, the editor implements a static form of syntax \nhighlighting; that is, it will highlight the keywords when it loads the document, but it will not change \ntheir color when you edit the document. Phase 3 M3 quickly realized that his editor can only do static \nsyntax highlighting when he was testing his program, and he spent a little more than 25 minutes browsing \nthe web to .nd out why. After he found that this was because of the missing setDamager, he .xed it immediately \nand got the correct glue code. Phase 4 M3 spent another 23 minutes developing the task code, including \nbrowsing the web and writing and debugging the code. This is a very typical usage of MatchMaker. The \nother three users all showed a similar pattern. The control subjects, on the other hand, struggled with \nglue coding. For example, C1 and C2 both browsed the web for more than 25 minutes before writing the \n.rst line of glue code, and both spent a total of one hour to produce correct glue code. The other two \ncontrol subjects C3 and C4 spent more than 80 minutes browsing the web to .nd the answer to the matching \nproblem, and more than 100 minutes to solve it. It is worth pointing out that there are a number of tuto\u00adrials \non the web that describe exactly how to implement syntax highlighting, several of which were visited \nby all four control subjects. However, these tutorials are either poorly written or contain too much \ninformation, so it took them a signi.cant amount of time to extract the rel\u00adevant facts from these tutorials. \nAs an example, Subject C3 came to a tutorial with all the details of implement\u00ading syntax highlighting \nwithin his .rst three searches, but this tutorial is very verbose and contains informa\u00adtion about many \ncomponents related to text editors in Eclipse and long descriptions explaining the internal re\u00adlationship \namong those components, so C3 spent about 7 minutes reading the tutorial without extracting anything \nessential. Instead, he turned to the o.cial API reference of Eclipse, a JavaDoc style document with little \ninforma\u00adtion about interactions among classes. After spending a lot of time on the API reference, C3 \ndiscovered an\u00adother tutorial on the o.cial Eclipse web site, which is the most authoritative tutorial \non this topic. Although this tutorial is more concise than the previous one, it is still verbose and \nhard to read, so C3 quickly closed it and turned to the API reference again. During Web Browsing for \nGlue Code, C3 visited the tutorial on the Eclipse web site a total of four times, but for the .rst three \ntimes only read it for less than 2 minutes. At last he realized that it had the information he needed, \nread it carefully for 5 minutes, and got the answer, but he had already spent more than an hour on web \nbrowsing. The other control subjects faced similar di.culties in de\u00adciding which documents were valuable \nand in extracting useful information from the long tutorials. The MatchMaker users also browsed the web \nfor help, but they did it in a di.erent way. They had speci.c goals when browsing: to .ll the holes in \nthe statements generated by MatchMaker, or to .nd out the missing statement. In both cases they used \nthe class names or the method names as keywords to quickly skim the tutorial or even search around the \ntutorial, so they were more e.cient in deciding which tutorials were valuable and locating the essential \ncode snippet. MatchMaker helped them by improving their e.ectiveness in using the online resources. This \nis supported by the data: MatchMaker users spent 24.25 minutes on Web Browsing for Glue Code, compared \nto 64.5 minutes on average of control subjects. The Wilcoxon test shows that the di.erence is statistically \nsigni.cant (with p-value=0.04).  The framework expert was di.erent from subjects in both control and \nexperimental groups: he knew exactly what he was looking for on the web and found it very quickly, so \nhe could .nish glue code in 16 minutes. Overall, our observations from the subjects allow us to draw \nsome preliminary conclusions. The matching problem is indeed a signi.cant problem when writing code \non top of complex frameworks. This is particularly true for people who are new to the framework, and \nless so for experts.  Tutorials and documentation available online are not enough to close the gap between \nnovices and experts. The class-by-class documentation available through JavaDoc is particularly unhelpful \nbecause it fails to describe multi-object interactions. Tutorials, in turn, can be unreliable because \nof errors and omissions, but the most important problem is the sheer amount of data that a novice has \nto read before beginning to understand the framework.  MatchMaker has a statistically signi.cant impact \non programmer productivity by showing programmers the object interactions that are necessary to achieve \na task. This is true even when the tool fails to give a complete solution to the task in question.  \n  7. Related work The idea of using large corpus of data for program understanding has seen many incarnations \nin the past few years. Prospector [9], XSnippet [13], MAPO [20], PARSEWeb [17], and Strathcona [7] mine \nsource code repositories and assist programmers in common tasks: .nding call sequences to derive an object \nof one type from an object of another type, complex initialization patterns, and frequent API usage patterns. \nThey do so by computing relevant code snippets as determined by the static program context and then applying \nheuristics to rank them. Since they primarily utilize static analysis, the context lacks heap connectivity \ninformation. These tools are geared towards code assistance and do not produce full templates of the \nprogram that may span multiple classes. Whyline [8] combines source code analysis with dynamic analysis \nto assist debugging by tracing input/output events together with the program, and suggesting ques\u00adtions \nthat relate external observations to internal method calls. DeLight could potentially serve as a common \nframework for tools like Whyline and MatchMaker that need to query program executions. MatchMaker does \nnot use external observations (GUI or input events) to locate points of interest in the trace, instead \nit uses internal heap con.guration to identify important events in the trace. PQL [10] proposes a query \nlanguage for analyzing pro\u00adgram execution statically or dynamically. It is aimed at .nding design defects \nin the code and as such requires detailed knowledge of the code. It is not suitable for program understanding \ntasks. PTQL [4] uses its own relational query language to instrument a program and dynamically query \nlive executions. FUDA [6] is closely related in its goal of producing program templates from example \ntraces. Like our system, this tool also lever\u00adages the distinction between user and framework code to \nproject slices. However, the API trace slicing used in FUDA only uses shared objects in argument lists \nof calls to detect dependencies in the heap. FUDA does not keep track of the heap updates. All of the \nabove tools rely on light-weight dynamic analysis and as such, require manual e.ort in formulating queries \nin a specialized lan\u00adguage or instrumenting programs speci.cally for these queries. MatchMaker attempts \nto reuse databases for answering all queries, while keeping the query language very simple. BugNet [11] \nand similar tools record the full program trace for deterministic replay debugging. MatchMaker does not \ncollect entire execution data. It uses just enough information from the trace to be able to answer program \nsynthesis-related queries e.ectively. Program synthesis systems such as SKETCH [14] can produce program \ntext from a slower version of the same program or its speci.cation via a combinatorial search over ASTs. \nThe level of deep static reasoning about program that is needed by SKETCH has not been achieved for the \nlarge scale software like Eclipse. Moreover, the dynamic features that are prevalent in Eclipse and its \nscale make it very hard to employ any static reasoning except for the very light-weight. The projection \nalgorithm in Sec. 5.2 is an instance of dynamic amorphous program slicing [5]. Like traditional program \nslicing, amorphous slicing simpli.es a program while preserving a projection of its semantics. Unlike \ntraditional program slicing that only allows picking a subset of the program, amorphous slicing may use \nany simplifying transformation. The projection algorithm used by MatchMaker is amorphous because it links \nconsumer events directly to their local producers.  8. Conclusion We have presented a new approach to \nsynthesis based on the analysis of very large amounts of program ex\u00adecution data. The approach is made \npossible by the DeLight system, which allows for the e.cient collec\u00adtion, management and analysis of \nthis data. DeLight uses abstraction to support detailed queries about how the heap evolves as the program \nexecutes, which are necessary to support our synthesis algorithm.  Our synthesis algorithm focuses on \nthe problem of generating the glue code necessary for two classes to interact with each other. This glue \ncode often involves instantiating new classes, making API calls, and even overriding methods in speci.c \nclasses, and our tool MatchMaker can support all of these actions. Our empirical evaluation shows that \nwriting this glue code is especially time consuming for novice program\u00admers, and that MatchMaker can \nsigni.cantly improve their productivity. It also shows that MatchMaker is general enough to handle many \ninteresting queries, and produces code that can be used by programmers with very few changes. Acknowledgement \nWe would like to give special thanks to Professor Greg Morrisett for helping us con\u00adduct part of the \nuser study at Harvard University. We would also like to thank the participants in our user study. This \nresearch was supported by the National Science Foundation grant CCF-1049406 and by MIT s Computer Science \nand Arti.cial Intelligence Lab (CSAIL).  References [1] E. Bruneton, R. Lenglet, and T. Coupaye. ASM: \na code manipulation tool to implement adaptable systems. Adaptable and extensible component systems, \n2002. [2] A. Diwan, K. S. McKinley, and J. E. B. Moss. Type-based alias analysis. PLDI 98, pages 106 \n117, New York, NY, USA, 1998. ACM. [3] Eclipse. Helios release notes, 2010. [4] S. F. Goldsmith, R. O \nCallahan, and A. Aiken. Relational queries over program traces. OOPSLA 05, pages 385 402, New York, NY, \nUSA, 2005. ACM. [5] M. Harman, D. Binkley, and S. Danicic. Amorphous program slicing. J. Syst. Softw., \n68:45 64, October 2003. [6] A. Heydarnoori, K. Czarnecki, and T. T. Bartolomei. Supporting framework \nuse via automatically extracted concept-implementation templates. In Proceedings of the 23rd European \nConference on ECOOP 2009 Object-Oriented Programming, Genoa, pages 344 368, Berlin, Heidelberg, 2009. \nSpringer-Verlag. [7] R. Holmes and G. C. Murphy. Using structural context to recommend source code examples. \nICSE 05, pages 117 125, New York, NY, USA, 2005. ACM. [8] A. J. Ko and B. A. Myers. Debugging reinvented: \nasking and answering why and why not questions about program behavior. ICSE 08, pages 301 310, New York, \nNY, USA, 2008. ACM. [9] D. Mandelin, L. Xu, R. Bod\u00edk, and D. Kimelman. Jungloid mining: helping to navigate \nthe api jungle. PLDI 05, pages 48 61, New York, NY, USA, 2005. ACM. [10] M. Martin, B. Livshits, and \nM. S. Lam. Finding application errors and security .aws using pql: a program query language. OOPSLA 05, \npages 365 383, New York, NY, USA, 2005. ACM. [11] S. Narayanasamy, G. Pokam, and B. Calder. Bugnet: Continuously \nrecording program execution for determinis\u00adtic replay debugging. ISCA 05, pages 284 295, Washington, \nDC, USA, 2005. IEEE Computer Society. [12] Neo4J. Home page, 2011. [13] N. Sahavechaphan and K. Claypool. \nXsnippet: mining for sample code. OOPSLA 06, pages 413 430, New York, NY, USA, 2006. ACM. [14] A. Solar-Lezama, \nL. Tancau, R. Bodik, S. Seshia, and V. Saraswat. Combinatorial sketching for .nite programs. ASPLOS-XII, \npages 404 415, New York, NY, USA, 2006. ACM.  [15] M. Sridharan, S. J. Fink, and R. Bodik. Thin slicing. \nPLDI 07, pages 112 122, New York, NY, USA, 2007. ACM. [16] S. Srivastava, S. Gulwani, and J. S. Foster. \nFrom program veri.cation to program synthesis. POPL 10, pages 313 326, New York, NY, USA, 2010. ACM. \n[17] S. Thummalapenta and T. Xie. Parseweb: a programmer assistant for reusing open source code on the \nweb. ASE 07, pages 204 213, New York, NY, USA, 2007. ACM. [18] M. Vechev and E. Yahav. Deriving linearizable \n.ne\u00adgrained concurrent objects. PLDI 08, pages 125 135, New York, NY, USA, 2008. ACM. [19] G. Xu, N. \nMitchell, M. Arnold, A. Rountev, E. Schonberg, and G. Sevitsky. Finding low-utility data structures. \nPLDI 10, pages 174 186, New York, NY, USA, 2010. ACM. [20] H. Zhong, T. Xie, L. Zhang, J. Pei, and H. \nMei. MAPO: Mining and recommending API usage patterns. ECOOP 2009 Object-Oriented Programming, pages \n318 343, 2009.  \n\t\t\t", "proc_id": "2048066", "abstract": "<p>Software construction today often involves the use of large frameworks. The challenge in this type of programming is that object-oriented frameworks tend to grow exceedingly intricate; they spread functionality among numerous classes, and any use of the framework requires knowledge of many interacting components. We present a system named MATCHMAKER that from a simple query synthesizes code that interacts with the framework. The query consists of names of two framework classes, and our system produces code enabling interaction between them. MATCHMAKER relies on a database of dynamic program traces called DELIGHT that uses novel abstraction-based indexing techniques to answer queries about the evolution of heap connectivity in a matter of seconds.</p> <p>The paper evaluates the performance and effectiveness of MATCHMAKER on a number of benchmarks from the Eclipse framework. The paper also presents the results of a user study that showed a 49% average productivity improvement from the use of our tool.</p>", "authors": [{"name": "Kuat Yessenov", "author_profile_id": "81384615851", "affiliation": "Massachusetts Institute of Technology , Cambridge, MA, USA", "person_id": "P2839132", "email_address": "kuat@csail.mit.edu", "orcid_id": ""}, {"name": "Zhilei Xu", "author_profile_id": "81351591505", "affiliation": "Massachusetts Institute of Technology , Cambridge, MA, USA", "person_id": "P2839133", "email_address": "timxu@csail.mit.edu", "orcid_id": ""}, {"name": "Armando Solar-Lezama", "author_profile_id": "81100173160", "affiliation": "Massachusetts Institute of Technology , Cambridge, MA, USA", "person_id": "P2839134", "email_address": "asolar@csail.mit.edu", "orcid_id": ""}], "doi_number": "10.1145/2048066.2048075", "year": "2011", "article_id": "2048075", "conference": "OOPSLA", "title": "Data-driven synthesis for object-oriented frameworks", "url": "http://dl.acm.org/citation.cfm?id=2048075"}