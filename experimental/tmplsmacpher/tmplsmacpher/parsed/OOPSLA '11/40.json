{"article_publication_date": "10-22-2011", "fulltext": "\n Automated Construction of JavaScript Benchmarks Gregor Richards, Andreas Gal , Brendan Eich , Jan Vitek \nS3Lab, Computer Science Dept., Purdue University Mozilla Foundation Abstract JavaScript is a highly \ndynamic language for web-based ap\u00adplications. Many innovative implementation techniques for improving \nits speed and responsiveness have been devel\u00adoped in recent years. Industry benchmarks such as WebKit \nSunSpider are often cited as a measure of the ef.cacy of these techniques. However, recent studies have \nshown that these benchmarks fail to accurately represent the dynamic nature of modern JavaScript applications, \nand thus may be poor predictors of real-world performance. Worse, they may lead to the development of \noptimizations which are unhelp\u00adful for real applications. Our goal in this work is to develop techniques \nto automate the creation of realistic and represen\u00adtative benchmarks from existing web applications. \nWe pro\u00adpose a record-and-replay approach to capture JavaScript ses\u00adsions which has suf.cient .delity \nto accurately recreate key characteristics of the original application, and at the same time is suf.ciently \n.exible that a recording produced on one platform can be replayed on a different one. We describe JS-BENCH, \na .exible tool for workload capture and benchmark generation, and demonstrate its use in creating eight \nbench\u00admarks based on popular sites. Using a variety of runtime metrics collected with instrumented versions \nof Firefox, In\u00adternet Explorer, and Safari, we show that workloads created by JSBENCH match the behavior \nof web applications. Categories and Subject Descriptors C.4 [Performance of systems]: Design studies; \nMeasurement techniques; D.2.8 [Metrics]: Performance measures General Terms Languages, Measurement, Performance \nKeywords Reproduction, Repetition, Benchmarks Permission to make digital or hard copies of all or part \nof this work for personal or classroom use is granted without fee provided that copies are not made or \ndistributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. OOPSLA 11, October 22 27, 2011, Portland, Oregon, USA. Copyright \nc &#38;#169; 2011 ACM 978-1-4503-0940-0/11/10. . . $10.00 1. Introduction JavaScript s popularity has \ngrown with the success of the web. Over time, the scripts embedded in web pages have become increasingly \ncomplex. Use of technologies such as AJAX has transformed static web pages into responsive ap\u00adplications \nhosted in a browser and delivered through the web. These applications require no installation, will run \non any machine, and can provide access to any information stored in the cloud. JavaScript is the language \nof choice for writing web applications. Popular sites such as Amazon, GMail and Facebook exercise large \namounts of JavaScript code. The complexity of these applications has spurred browser devel\u00adopers to increase \nperformance in a number of dimensions, including JavaScript throughput [6]. Because browser performance \ncan signi.cantly affect a user s experience with a web application, there is commer\u00adcial pressure for \nbrowser vendors to demonstrate perfor\u00admance improvements. As a result, browser performance re\u00adsults from \na few well-known JavaScript benchmark suites are widely used in evaluating and marketing browser im\u00adplementations. \nThe two most commonly cited JavaScript benchmark suites are WebKit s SunSpider1 and Google s suite associated \nwith their V8 JavaScript engine2. The benchmarks in both of these suites, unlike real web ap\u00adplications, \nare small; V8 benchmarks range from approxi\u00admately 600 to 5,000 lines of code, most SunSpider bench\u00admarks \nare even smaller. Unrepresentative benchmarks may mislead language implementers by encouraging optimiza\u00adtions \nthat are not important in practice and by missing op\u00adportunities for optimization that are present in \nthe real web applications but not in the benchmarks. Weak benchmarks have had a negative impact on language \nimplementations in the past. For example, the SPECjvm98 benchmark suite was widely used to evaluate Java \n[4] even though there was agreement in the community it was not representative of real applications. \nDissatisfaction with SPECjvm98 led to the cre\u00adation of the DaCapo benchmark suite, which includes realis\u00adtic \nprograms [1]. 1 www2.webkit.org/perf/sunspider/sunspider.html 2 v8.googlecode.com/svn/data/benchmarks/v6 \n In previous work we measured the behavior of real-world JavaScript applications [13] and compared those \nresults to similar measurements for the industry standard benchmarks. Figure 1 shows one visually striking \nexample of the differ\u00adence between the SunSpider benchmark suite and a real web application, in this \ncase the Google search engine. The .g\u00adures show the accesses performed on objects over their lifes\u00adpan. \nTime is measured in events that have the object as a tar\u00adget for a read, add, delete and update operation. \nObject lifes\u00adpans are normalized to construction time (shown as a vertical line in the graph). SunSpider \nis completely unrepresentative of Google s behavior. First, objects stay live for the duration of the \nbenchmark. Second, the vast majority of operations are reads and adds. Third, once constructed, the shape \nof objects, i.e. their .elds and methods, stay unchanged. Lastly, no .elds or methods are deleted. None \nof these hold in the Google code. The benchmark completely misses out on the use of dynamic features \nof JavaScript such as object proto\u00adcol changes and the extensive use of eval which we docu\u00admented in \n[12]. Our claim in [13] was that industry bench\u00admarks should not be used to draw conclusions about the \nper\u00adformance of JavaScript engines on real-world web applica\u00adtions. Contemporary work came to the same \nconclusion [11]. At the time we did not provide an experimental validation of this claim. We do this \nin the present paper (see Figure 19). The goal of this work is to provide developers with the means to \ncreate JavaScript benchmarks that are representa\u00adtive of the behavior of real-world web applications, \nso that these benchmarks may enable evaluating the performance of JavaScript implementations. In particular, \nthese bench\u00admarks should contain instruction mixes that retain the dy\u00adnamism we have observed in web \napplications. There are multiple challenges that make the task dif.cult. Amongst them, many web applications \nare constructed dynamically by servers. Thus the same web page can have different behaviors at different \ntimes. JavaScript code can come in many forms: as embedded <script> tags in a HTML doc\u00adument, .les that \nare downloaded by the browser, or dy\u00adnamically constructed strings of text that are executed by eval. \nThe code executed for any given web page is often browser-speci.c, with different features activated \ndepend\u00ading on the browser capabilities or special algorithms to cir\u00adcumvent performance issues in some \nJavaScript implemen\u00adtations. Moreover while JavaScript is a single-threaded lan\u00adguage, JavaScript programs \nare exposed to a number of sources of non-determinism. Web applications are event\u00adbased applications \nin which mouse position and timing of user actions can affect the behavior of the program, timed events \n.re at different rates on different systems. All of these features render the task of creating repeatable \nperformance benchmarks challenging. In addition to the technical challenges, another feature of real-world \nJavaScript applications is their .uidity. The tech\u00adnology used to design web applications and the expectations \nFigure 1. Object timelines. Comparing the operations per\u00adformed on objects in industry standard benchmarks \nand web applications. Above, SunSpider. Below, Google.  of the users are evolving rapidly, towards richer \nand more behavior-intensive applications. This means that it is unclear whether one .xed benchmark suite \ncan remain relevant for very long. Instead we argue that it is desirable to develop tools that will allow \nany web developer to create a set of benchmarks that capture the behaviors that are relevant to that \nparticular developer at the time. Until now, no strategy for generating stand-alone, push-button replayable \nbench\u00admarks has been proposed. Our tool, JSBENCH, .lls this gap by automatically generating replayable \npackaged bench\u00admarks from large JavaScript-based web applications. We achieve this goal by building a \nbrowser-neutral record-replay system. While such a system can be used for other purposes such as producing \ntest cases or capturing crash-producing JavaScript executions in the .eld, in this paper we focus on \nusing record-replay-based techniques to produce repre\u00adsentative benchmarks for evaluating JavaScript \nimplementa\u00adtions. Following industry practice, we focus on throughput\u00adoriented performance measurements, \nleaving an assessment of responsiveness to future work. Furthermore, we designed JSBENCH to isolate the \nperformance of JavaScript engines from other compute-intensive browser tasks such as layout, painting, \nand CSS processing. This allows for a head-to-head comparison of JavaScript implementations across different \nbrowsers. We aim to produce benchmarks that ful.ll four re\u00adquirements: 1. Deterministic replay: multiple \nruns of a benchmark should display the same behavior.  2. Browser-independence: a benchmark s behavior \nshould not be affected by browser-speci.c features and should execute on all browsers. 3. Fidelity: \nbenchmarks should correctly mimic the behav\u00adior of live interactions with a web application. As there \nis no result in a web page, we focus on the execution of events and changes to the web page. 4. Accuracy: \nbenchmarks should be representative of the performance and non-functional characteristics of the original \nweb applications.  Previous projects have either focused on recording client\u00adside user behavior (e.g. \n[9]) or proposed browser-based in\u00adstrumentation and recording techniques [11, 13]. Client-side recording \ncan fail to intercept some of the JavaScript code and requiring a proxy to be present at the time of \nreplay. Browser-based approaches do not help with the goal of syn\u00adthesizing browser-independent benchmarks. \nIn summary, this paper makes the following contribu\u00adtions: We propose a browser-independent record system, \ncalled JSBENCH, for reliably capturing complex user-driven applications based solely on JavaScript source-level \nin\u00adstrumentation and generating benchmarks to faithfully reproduce the applications behavior.  We describe \nhow the record-replay approach can be used for producing deterministic replays of non-trivial pro\u00adgrams \nand propose trace post-processing steps designed to achieve high .delity.  Through demonstration, we \nshow that complex web ap\u00adplications can be successfully captured using JSBENCH with little effort.  \nWe demonstrate results for eight real, large JavaScript ap\u00adplications. Our evaluation includes a variety \nof runtime metrics pertaining to the JavaScript engine behavior as well as JavaScript-browser interactions, \nsuch as memory usage, GC time, event loop behavior, DOM layout and re\u00adpaint events, etc. These were obtained \nwith instrumented versions of Internet Explorer, WebKit and Firefox.  We emphasize that the benchmark \ngeneration strategy we enable is most suitable for comparing and tuning the per\u00adformance of JavaScript \ninterpreters and just-in-time compil\u00aders. The immediate impact of this work is to evaluate the impact \nof different implementation techniques for the lan\u00adguage. Longer term, we expect these benchmarks, coupled \nwith the data of [12, 13], to help language designers evolve the language. We explicitly did not aim \nto evaluate perfor\u00admance of web browsers. While our tool allows to capture some of that behavior, we \nleave the task of dealing with as\u00adpects such as responsiveness and rendering to future work. JSBENCH \nis open source and available from: http://sss.cs.purdue.edu/projects/dynjs 2. Related Work At the core \nof JSBENCH, we have devised a technique for capturing a program, including its dynamically generated \ncomponents, and replaying it in the absence of the sur\u00adrounding execution environment (browser, .le system, \nnet\u00adwork connections, etc.) and to do so deterministically. The approach can be generalized to other \nlanguages and sys\u00adtems and has applications, including mock object genera\u00adtion for unit testing [3]. \nReplay techniques have been investi\u00adgated in the past, mostly for debugging purposes. Cornelis et al. \n[2] and Dionne et al. [5] have surveyed and categorized replay-based debuggers. In Dionne s taxonomy, \nJSBENCH is a data-based automatic replay system as it records data exchanged between the program and \nits environment and re\u00adquires no human-written changes to the source of the mon\u00aditored program. Failure \nof a replay occurs if the program s interactions with its environment deviates from the recorded trace. \nWhile unlikely, this can happen due to implemen\u00adtation difference between browsers. A self-checking mode \nfor each benchmark may be used to catch departures from pre-recorded traces. Related systems have been \nproposed. Mugshot [9] is a record-replay tool for JavaScript which aims for the highest possible .delity, \nand as such recreates events exactly as they appeared in the original recording. Al\u00adthough suitable for \ndebugging, this mechanism is too fragile for our goal of general-purpose replay, as it prevents the user \nfrom dispatching events and requires specialized functions to recreate every possible event that the \nbrowser could dis\u00adpatch. The API s for dispatching browser events are incon\u00adsistent between browsers, \nand furthermore require careful tuning of the event infrastructure to assure that only the de\u00adsired handlers \nare called. This creates browser-inconsistency and unpredictable overhead, both of which are unacceptable \nfor benchmarking. Our system does not depend on recreat\u00ading and dispatching true browser events, and \nas such relies only on the JavaScript language itself. This is simpler and less obtrusive. Another difference \nis that Mugshot aims to be an always-on system and thus must be extremely low\u00adoverhead and deal with \nprivacy issues. JSBENCH can afford higher recording costs and needs not worry about con.den\u00adtiality of \nuser data. Mugshot also requires a proxy at the time of replay, which JSBENCH does not need. Ripley [14] \nre\u00adplays JavaScript events in a server-side replica of a client program. DoDOM [10] captures user interaction \nsequences with web applications to allow fault-injection testing for detecting unreliable software components. \nWhile DoDOM captures sequences of external interactions with a web ap\u00adplication for later replay, it \nassumes that later interactions will be with the original web site. Additionally, there have been several \nsystems to instrument JavaScript at the source level. JSBENCH in particular is based on the same frame\u00adwork \nas AjaxScope [7] and Doloto [8]. However, our goal of having minimum impact on the behavior of the original \ncode is quite different from these systems. 3. Record/Replay Principles and Requirements A JavaScript \nprogram running in a browser executes in a single-threaded, event-driven fashion. The browser .res events \nin response to end-user interactions such as cursor movements and clicks, timer events, networks replies, \nand other pre-de.ned situations. Each event may trigger the ex\u00adecution of an event handler, which is \na JavaScript function. When that function returns, the JavaScript event loop han\u00addles the next event. \nThe timing and order of events is depen\u00addent on the particular browser, system, and other environ\u00admental \nfactors. Thus, there is non-determinism due to the order in which events are processed by the browser. \nThe same program will yield different results on different pro\u00adcessors (the rate of processing events \nis different) and differ\u00adent browsers (the internal event schedule may be different). The JavaScript \ncode interacts with the browser, the network and even indirectly the .le system through a set of native \noperations that have browser-speci.c semantics. Figure 2 il\u00adlustrates common sources of non-deterministic \nbehavior in JavaScript-based web applications. Since every browser im\u00adplements its own proprietary API \ns, and frequently these API s are undocumented, Figure 2 is necessarily incomplete, but from inspection \nand experience is a representative list of sources of non-determinism which are portable between browsers. \nRepeatability is further complicated by the fact that all web applications are interacting with one or \nmore remote servers. These servers provide inputs to the program and code of the application. Presentation \nDOM objects event handlers Network XMLHttpRequest objects event handlers File System DOM objects (cookies) \nTime Date, setTimeout User input DOM objects event handlers Nondeterministic functions Math.random Environment \nqueries document.location, navigator Figure 2. Sources of non-determinism in JavaScript\u00adbased web applications. \nDOM objects mirror the layout of the web page, Listeners are used to associate callbacks to events, XHR \nobjects provide asynchronous HTTP requests, setTimeout associates a callback to timer events. In order \nto create benchmarks that are reproducible, these sources of non-determinism must be isolated. Thus, \nwe are looking to produce benchmarks that somehow approximate, in a deterministic and browser-neutral \nfashion, these non\u00addeterministic programs. Ideally we would want a determin\u00adistic program that faithfully \nand accurately reproduces the original program. Another challenge is that there is no clear notion of \noutput of a web based application, no single result that it is intended to produce. To capture a web \nsite and turn it into a replayable benchmark, we propose a record/replay approach with the following \nsteps: 1. A client-side proxy instruments the original web site s code to emit a trace of JavaScript \noperations performed by the program. 2. The trace is .ltered to get rid of unnecessary information; \n 3. A replayable JavaScript program, with all non\u00addeterminism replaced, is generated from the trace; \n 4. The program is recombined with HTML from the origi\u00adnal web site.  While it is an attractive cross-browser \napproach, benchmark generation through JavaScript instrumentation has certain disadvantages. By introducing \nnew JavaScript code into the replay (the code that removes non-determinism at the very least must be \nadded to the original program), there is un\u00adavoidable perturbation of the original behavior. If we fail \nto fully instrument the code (such as would be the case when eval is called) on code that was not observed \nby the instru\u00admentation proxy, certain parts of the program may not be recorded. We will now describe \nthe properties and require\u00adments for replayable programs. De.nition 1. The execution state of a web application \ncon\u00adsists of the state of a JavaScript engine P and an environ\u00adment E. A step of execution is captured \nby a transition re\u00ad a lation P |E -.t P '|E' where a is a label and t is a time stamp. As shown in Figure \n3, the set of labels is split into labels representing actions initiated by the environment, aE (ei\u00adther \nevents or returns from calls to native browser functions), and actions performed by the JavaScript engine, \naP , which include function calls and returns, property reads and writes, object allocation, calls to \nnative functions, etc. (These ac\u00adtions are modeled on TracingSafari [13], JSBENCH only captures the subset \nof events needed for creating replays.) aE Browser interactions EVT f, v REP v External event handled \nby function f Return value v from an external call aP Trace events APP f, v RET v GET v, p SET v, p, \nv NEW f, v INV f, v Call function f with arguments v Return value v from a call Read member p from object \nv Set member p from object v to v Create an object with constructor function f Invoke external operation \nf Figure 3. Trace events. Operations performed by a Java-Script program and inputs from the environment. \n De.nition 2. A trace T is a sequence a1,t1,...,an,tn a1an corresponding to an execution P |E -.t1 ... \n-.tn P ' |E ' . We write P |E f T when execution of a con.guration P |E yields trace T . For any given \nprogram P , the same sequence of end-user ac\u00adtions (e.g. mouse clicks, key presses, etc.) can result \nin a dif\u00adferent E due to timing and network latency issues. Different browsers will de.nitely lead to \ndifferent environment behav\u00adior. Thus, requiring traces to be exactly equal is overly strin\u00adgent. As \nthere is no single output, we consider two traces to be equivalent if the display elements shown by the \nbrowser are identical. This notion of equality captures the end-user observable behavior of the program. \nWe write T |p to denote the sub-trace of T matching predicate p. Thus, T |DOM de\u00adnotes a sub-trace of \nT composed only of SET actions on objects that belong to the DOM.3 De.nition 3. Two traces are DOM-equivalent, \nT ~', if =D T ' SET v, p, v,t . T |DOM =. SET v ' , p, v,t . T ' |DOM . ' SET v ' , p, v,t . T ' |DOM \n=. SET v, p, v,t . T |DOM DOM-equivalence is not order preserving, because the order of event handlers \nexecuted by P may be different. A more complex equivalence relation could try to capture the or\u00addering \nwithin event handlers and dependencies across event handlers, but a simple equivalence suf.ces for our \npurposes. We are striving for determinism. This can be de.ned as yielding DOM-equivalent traces for any \nenvironment E. In\u00adtuitively, this is the case when the program is independent of external factors. Of \ncourse, deterministic programs run\u00adning on different JavaScript engines may have slightly dif\u00adferent \ntraces due to browser optimizations, but they should be DOM-equivalent. De.nition 4. A program P is DOM-deterministic \nif ~ .E1,E2 : P |E1 f T1 . P |E2 f T2 =. T1 =D T2 In order to create a replayable benchmark for some \npro\u00adgram P , the program must be run in an environment able to record its actions. We denote the recording \nR(P ) and write R T for the trace obtained during recording. It worth to point R out that R(P )|E and \nP |E will yield traces T and T which are not identical (but are DOM-equivalent). This is explained by \nthe presence of additional JavaScript operations needed by the recording infrastructure appearing in \nthe trace. De.nition 5. Recording program P in environment E, writ- R ten R(P )|E f T , results in replayable \nprogram PR. The replayable program, PR, is constructed so as to ex\u00adhibit the following properties. First, \nthe replayable program 3 This test can be implemented as v instanceof Node, but since the replay programs \ncontain mock objects, we use (v instanceof Node ||v.isJSBProxy()). is fully deterministic, given the \nsame environment it al\u00adways yields the same trace, and secondly even in different browsers the traces \nare DOM-equivalent. We have not spec\u00adi.ed how to construct PR, the details of this are an imple\u00admentation \nchoice. Property 1. [Determinism] PR|E always yields the same ' trace T and for any environment E ' \n, PR|E ' yields a trace T that is DOM-equivalent to T . One technique that can be used to construct replayable \npro\u00adgrams is to avoid non-determinism by proxying. For any ob\u00adject v that performs a non-deterministic \noperations (e.g. a native call) in one of its methods, replace that object with a proxy, v, and memoize \nall of results returned by its meth\u00adods during the original run, then use the memoized values in replay \nruns. Thus PR could eschew non-determinism by al\u00adways returning values obtained at record time for non \ndeter\u00administic calls. To abstract from the behavior of the browser, one can choose to record events issued \nby the browser and replay them in a deterministic and browser-agnostic order. Of course, determinism \nis not a suf.cient requirement as one could pick the empty program for PR and get a really deterministic \n(and short) trace. The second required property of replays is that they preserve the behavior of the \nrecorded trace. R Property 2. [Fidelity] If R(P )|E f T , and PR|E f T R ~ then T =D T . Fidelity is \na property of recorded and replay traces stating the replay will issue the same DOM calls as the recorded \ntrace. Fidelity is really a minimal requirement. It gives a sense that replay is correct with respect \nto the observed behavior of the recorded trace. But in order to get a bench\u00admark that is a good predictor \nof the performance for orig\u00adinal program we need to make sure that when shedding non-determinism, PR \nhas not been turned into a trivial se\u00adquence of memoized calls. What is needed is to make sure that replays \nare faithful to the computation performed in the original trace and not only it s DOM I/O behavior. Given \na suitable de.nition of distance between traces, d, the replay should be within the range of possible \ntraces generated by the original program. Property 3. [Accuracy] If P |E f T and PR|E f T , ' there \nis some environment E ' such that P |E ' f T and the distance between the traces d(T, T ) <d(T,T ' ). \nAccuracy says that a trace generated from a replay is close to a trace that could have been generated \nfrom the original program under some environment. We deliberately leave the de.nition of distance open. \nAnother desirable property is to make sure that the com\u00adputation performed by the replayable program \nPR be com\u00adparable in running time to the original program execution. The execution time of trace T , \nof length n, written t(T ), is tn - t0. Naively one would like that for a program P |E f T , and its \nreplay PR|E f T , the execution times of the traces be comparable t(T ) ~t(T ). This is unfor\u00ad = tunately \nimpossible to achieve while retaining determinism. Instead, t(T ) > t(T ) is more likely. The replay \ntrace suffers from time compression due to two main reasons. First, the original trace contains slack \ntime between the last instruc\u00adtion of the previous event and the .ring of the next event, t((a, t), (EVT \nv,t ' )). Second the compute time of native op\u00aderations t((INV f,v,t), (REP v,t ' )) can be substantial. \nThe replay program does not preserve either. Slack time depen\u00addents inherently on the user and on the \nspeed of processing the previous event. JavaScript does not have means to accu\u00adrately control timing \nof event dispatches. Second, the exe\u00adcution time of native operations is highly dependent on the quality \nof the implementation of the browser. In Section 5, instead of simply comparing execution times, we argue \nthat the replay has comparable dynamic attributes. 4. Creating Benchmarks with JSBench To create a benchmark, \nusers of JSBENCH need only point their browser to a web application hosted on some remote server. JSBENCH \nacts as a proxy between the user s browser and the server. Any JavaScript code loaded from the server \nwill be intercepted by JSBENCH and rewritten in-place to an instrumented program with similar behavior. \nThe instru\u00admentation introduced by JSBENCH records a trace of the op\u00aderations performed by the web application. \nOnce the execu\u00adtion is complete, a post-processing pass is applied to turn that trace into a stand-alone \nJavaScript program consisting of the original HTML and a rewritten version of the source code in which \ndependencies to the origin server are removed and all non-deterministic operations are replaced by calls \nto mock objects returning memoized results. Figure 4 summa\u00adrizes this architecture and the rest of this \nsection discusses it in more detail. 4.1 Recording JavaScript executions Before a piece of JavaScript \ncode is handed to the browser for execution, JSBENCH instruments it, rewriting all func\u00adtion entries \nand exits as well as .eld access and related oper\u00adations, as detailed below.4 The purpose of the instrumenta\u00adtion \nis to create, as a side effect of executing the application, a trace of the executed operations. There \nare many potential mechanisms for observing the behavior of JavaScript pro\u00adgrams. We have based JSBENCH \non source-level instrumen\u00adtation for two reasons: Source-level instrumentation is portable across differ\u00adent \nbrowsers and different versions of the same browser, users will thus be able to create benchmarks on \nany browser. 4 Code in this section is simpli.ed for readability and to spare the reader the gruesome \ndetails of assuring that JSBENCH s variables are never shadowed, and other nuances of JavaScript that \nare not directly relevant.  Figure 4. System architecture. JSBENCH acts as a proxy between the browser \nand the server, rewriting JavaScript code on the .y to add instrumentation and .nally creating an executable \nstand-alone application. DEC o Declare object or function o exists. GET o,f, v Field o.f has value v \nSET o,f, v Field o.f is updated to v APP o, [o1 ...], v Calling function o with return value v. NEW o, \n[o1 ...], o Creating object o with constructor o. EVT g, [o1 ...] Function g .red as an event with arguments \n[o1 ...] . g may either be a static function, or the result of a CTX event. CTX o, g, [f1 ...], The function \no refers to the original static function [v1 ...] g with variables [f1 ...] in its closed context referring \nto values [v1 ...] . Figure 5. Record-time trace statements. The JavaScript language evolves at a slower \nrate than its implementations. JSBENCH uses proxy-based instrumentation of JavaScript code to record \nan execution trace that contains all the in\u00adformation needed to create an executable replay of that trace \n[7]. The trace generated by JSBENCH consists of a sequence of labeled trace statements, as detailed in \nFig\u00adure 5. DEC declares that a pre-existing JavaScript object has been encountered, GET and SET indicate \nreads and writes of properties, APP indicates a function call, NEW marks the construction of a new object, \nEVT denotes the .ring of an event, and CTX denotes the construction of a closure. Not all operations \nneed to be logged. Indeed, only operations that introduce non-determinism must be recorded. For those, \nJS\u00adBENCH records enough information to reproduce their be\u00adhavior deterministically. JSBENCH will record \ncalls with their arguments as well as their results, and replace any ob\u00adject that has non-deterministic \noperations with a memoiza\u00adtion wrapper. A memoization wrapper is a small JavaScript object which contains \na reference to the original object and a unique identi.er. The purpose of the memoization wrap\u00adper is \nto record all operations performed over the object and any objects or functions it refers to. We avoid \ncreation of multiple memoization wrappers for the same original object leveraging the dynamism of JavaScript. \nJSBENCH extends wrapped objects by adding one extra .eld that holds a back pointer to the wrapper. Some \nnative objects can not be ex\u00adtended. In this case, a reverse reference is not stored, a warn\u00ading is produced, \nand each access to the object will create a new wrapper. Every time a memoization wrapper is cre\u00adated, \na DEC event is logged in the trace. The behavior of certain functions must be memoized as well. A memoizing \nfunction is similar in principle to a memoization wrapper. It stands between the code and the real function, \nand its behav\u00adior when called is identical to that of the function it memo\u00adizes, but it also records \nan APP event. The reproduction of memoized functions at replay time must be capable of pro\u00adducing the \nsame argument-to-value mapping as was seen by APP events. Constructors are just a special case of functions. \nWhen a memoizing function is called with new (and thus, as a constructor), a NEW event is generated, \nbut otherwise the behavior is the same. JSBENCH makes sure that before any JavaScript code is allowed \nto run, a JSBENCH-generated function is run which replaces all of the objects and func\u00adtions that we \nhave identi.ed as introducing non-determinism with memoization objects and memoizing functions. This \nfunction also produces DEC and GET trace events for each replaced object. 4.1.1 Instrumenting .eld accesses \nJSBENCH needs to instrument .eld accesses. This is for two reasons: reads of DOM and other wrapped objects \nwill be memoized for replay, and writes to these objects may be checked at replay time to ensure .delity. \nReads are denoted by GET events in the trace, writes by SET events. JavaScript s highly-dynamic nature \nmakes record\u00adtime memoization a straightforward task: all object mem\u00adber accesses are rewritten to use \na logging function, which returns an object containing the appropriate member. The JavaScript expression \nx[exp], which uses associative syntax to access the .eld named by the value of the expression exp of \nobject x, is rewritten into: ( memoRead( x, t = (exp)))[ t ] where t is a unique temporary variable, \nused to assure that side effects of exp are not repeated. The more conventional syntax for reading .elds, \nx.y, is syntactic sugar for x[ y ], and is rewritten appropriately. The memoRead() has a somewhat baroque \nde.nition as a direct consequence of the semantics of JavaScript. In Java-Script, functions take an extra \nargument to a context object. Within the body of the function this argument is the im\u00adplicit this reference. \nA call of the form x.f () will execute function f with this bound to x. A call of the form x[f ]() must \nalso bind this to x in f. But, surprisingly, the follow\u00ading z=x[f ];z() will not. Our translation preserves \nthis seman\u00adtics so, memoRead does not return the value of the property rather it returns an object that \nhas the desired property. Con\u00adsider the following access to x[ f ]: x[ f ] . memoRead(x,t= f )[t] . {f:wrap(x[ \nf ])}[ f ]  This design also allows unwrapped objects to be returned directly, while still preserving \ncall semantics: x[ f ] . memoRead(x, t= f )[t] . x[ f ] The translation replaces the original expression \nwith a call to memoRead() and, assuming x is wrapped, memoRead() returns a newly created object with \na property f referring to a wrapped value. More precisely the memoRead() function behaves as follows. \nIt checks if x is a memoization wrapper object. If not, it simply returns x. If it is, then the wrapper \nhas a reference to the original object, the operation is forwarded to that object and a GET event is \nlogged. The return value depends on the type of the value referenced by the requested property: A primitive: \nThe wrapped object is returned without fur\u00adther processing.  An object: if the object has not previously \nbeen wrapped in a memoization wrapper, a memoization wrapper is cre\u00adated for it and the associated DEC \nevent is logged. A new object containing a mapping of the expected prop\u00aderty name to the wrapper is created \nand returned.  A function: a memoizing function is created which wraps the original function. A new \nobject containing a mapping of the expected property name to the memoizing function is created and returned. \nThe memoizing function is capa\u00adble of determining when it is called with the returned ob\u00adject as the \ncontext (value of this); in this case, the original function is called with this as x. Otherwise, the \noriginal this is passed through.  Assignments are implemented similarly, with a memoWrite() function \nwhich logs a SET event for memoization wrapper objects, and performs the associated write. It can also \ndeal with operators such as += by an optional argument which describes which operation is used to update \nthe argument. This case produces both a GET event of the original read, and a SET event of the new value. \nOther related operations worthy of note include opera\u00adtor in, statement for(...in...), equality-related \noperators and instanceof. They are replaced by memoization-wrapper\u00ad 1 function onloadHandler() { 2 myvalue \n= document 3 .getElementById( input ) 4 .value; 5 } (a) Source code 1 EVT onloadHandler, [window] 2 \nDEC o1 3 GET window, document, o1 4 DEC f1 5 GET o1, getElementById, f1 6 DEC o2 7 APP f1, [o1, input \n], o2 8 GET o2, value, Hello!  (b) Recorded Trace Figure 6. Example. Source code to the trace and browser \nand JavaScript interactions. aware versions. This is a much simpler task, as none of these return objects, \nso no new wrappers need be generated. yin x is replaced by memoIn(y, x), and for (y in x) is replaced \nby for (y in unwrap(x)). Both of these functions simply per\u00adform the action of the operation they replace \non the wrapped object. Equality operators are replaced by unwrapping ver\u00adsions, such as unwrap(x)==unwrap(y). \n 4.1.2 Instrumenting functions JSBENCH instruments every function entry and exit point. In the browser \nmodel, a program execution consists of se\u00adquence of single-threaded event handler calls. As any func\u00adtion \ncan be used to handle events, we need to identify which functions are being invoked as event handlers \nand memoize the arguments of these functions for replay. JSBENCH mod\u00adi.es every function in the source \ncode to check if its caller is another JavaScript function. If this is not the case, then JSBENCH deduces \nthat the function has been invoked by function f (...) { if (! callerJS) { // event handler callerJS \n= true; var ret ;  try { var hid = logContext(f,{/* closure context */ }); var wthis = memoizationWrap(this); \nvar args = arguments.map(memoizationWrap); logEvent(hid, wthis, args); ret = f.apply(wthis, args); } \ncatch (ex) { callerJS = false; throw ex; }callerJS = false; return ret ; } else { /* original function \nbody*/ }} Figure 7. Example of function instrumentation. the browser in reponse to an event happening. \nEvery func\u00adtion in the source is instrumented as shown in Figure 7. The logEvent() adds an EVT event \ncorresponding to the current function invocation to the trace. In addition to memoizing the event handler \narguments, if the event handler is a clo\u00adsure, then JSBENCH must also memoize the environment of that \nclosure. This is necessary because at replay time the closure may try to access variables in its environment. \nAs it impossible to name the particular closure called as a han\u00addler, the replay instrumentation wraps \nthe function such that it can recreate part of its context. The logContext() function generates a CTX \nevent, as well as a unique closure ID for this particular instance of this function. The closure con\u00adtext \nalluded to in the above code is an object literal which maps the names of all variables which are closed \nover (de\u00adterminable statically since JavaScript is lexically scoped) to their current values. For instance, \nif f closed over variables x and y, the closure context object literal would be {x:x, y:y}. This strategy \nworks can lead to over-memoization. The post\u00adprocessing used to alleviate this is discussed next.  4.1.3 \nExample To illustrate in the inner workings of JSBENCH, we provide an extended example. Figure 6 shows \nthe browser operations on the left hand side and JavaScript engine operations on the right hand side. \nTo describe the interaction, we start with a piece of JavaScript source code in Figure 6(a). Figure 6(b) \nshows the trace that JSBENCH logs as part of recording the execution of this code. 1. onload: onLoadHandle \nis set up as a listener such that it will be called by the browser when the load event occurs. The .rst \nstep of the recording process signi.ed by the EVT label records the presence of an external event that \ninvokes the onLoadHandler handler.  2. Calling getElementById: The next step involves two references \nto objects not de.ned in the program. The trace system creates temporaries for them: o1 (document object \nprovided by the browser) and o2 (the DOM el\u00adement corresponding to input). Additionally, the native function \ndocument.getElementById is referred to as f1. Two GET labels in the trace re.ect the relevant state of \nthe program heap. 3. Returning the DOM element: Next, we create a trace object o2 for the result of \ngetElementById call and record a APP label identifying this fact. 4. value property lookup: The last \nstep records the text string which corresponds to property value of o2. This is recorded with the last \nGET label.  Once the trace has been generated, converting to the replay script shown in Figure 8 is \na syntax-directed translation process. Lines 2 and 6 initialize the two objects used for replay. Line \n4 does the same for f1. Bodies of all such functions use memoization tables to map inputs to outputs, \nas illustrated by the cases setup in line 8. GETs in the trace result in heap assignments in lines 3, \n6, and 9. Finally, each event in the trace corresponds to a function call such as shown in line 10. function \nreplay() { var o1 = {}; window.document = o1; var f1 = function() { return lookupCase(f1, this, arguments); \n} o1.getElementById = f1; var o2 = {}; f1.cases[2][o1][ input ] = o2; o2.value = Hello! ; onloadHandler.call(window); \n} Figure 8. Example replay function.  4.2 Replaying recorded traces The instrumentation of existing \ncode for replay is far lighter than record instrumentation. There are only two instrumen\u00adtation steps. \nIn some cases it is not possible to override variables in JavaScript, e.g. certain browsers do not allow \ndocument to be overwritten, and in these cases references to such variables must be renamed in the original \ncode. This is accomplished simply by replacing all instances of the identi.er with an automatically-generated \nunique iden\u00adti.er. Not all static functions have a global name. But for the replay to be able to call \nthem, there must be a refer\u00adence to them which the replay code can access. Those func\u00adtions which are \nreferred to by EVT events are instrumented to store a reference to the function in an object owned by \nJSBENCH. For instance, if a function func is de.ned, and is used in an EVT event with ID f1, then a statement \nsuch as window.handlers.f1 =func is added to the program to retain the reference. 4.2.1 Generating the \nreplay code At replay time, the lightly-instrumented original code is combined in .le called replay. \njs, which is generated from the trace. All of the objects which were memoized at record time will be \nreplaced with mock objects at replay time which expose the same API as was used in the original source. \nMemoized objects are replaced with mock objects which have all of the recorded members (all members seen \nby GET events) with their original values, and memoizing functions are replaced by mock functions, which \nmap all of the argu\u00adments seen in APP events to the return values seen. To gen\u00aderate replay. js, all \nevents are grouped with their preceding EVT event, the event handler that caused the relevant action. \nEvents which are grouped with an EVT generate code which precedes the code generated for EVT itself, \nto ensure that the created mock objects expose the API which the handler uses. Figure 9 shows the JavaScript \ncode generated for each trace statement. DEC, GET, APP and NEW events are used to construct mock objects \nand functions. DEC events are re\u00adplaced by creating an object named by their unique ID. Non\u00adredundant \nGET events are replaced by an assignment to the relevant mock object. Each mock function generated con\u00adtains \nan associated mapping between arguments and return values. APP and NEW events are replaced by adding \nnew en\u00adtries to this mapping. SET events do not need to be replayed since they represent a behavior performed \nby the replayed JavaScript code, and not a source of non-deterministic be\u00adhavior, but may optionally \nbe replayed as assertions to verify that the .eld sets actually occur in replay. DEC o (for objects) \nvar o = {}; DEC o (for functions) var o = function() {return callCase(o,this,arguments);} GET o,f, v \no.f = v; APP o, [o1...on], v o.cases[n][o1 ]...[ on] = v; NEW o, [o1...on], o o.newCases[n][o1]...[on] \n= o ; EVT g, [o1 ... on] g. call (o1, ..., on); CTX o,g, [f1...fn], o = g(v1, ..., vn); [v1 ... vn] Figure \n9. Replay statements.  4.2.2 Trace Post-processing Our tracing framework captures a large amount of \ninforma\u00adtion, and much of it is easily determined to be unnecessary during replay. Although a trivial \nconversion of the trace into a replay function is feasible and can produce semantically correct results, \nthe overhead of such na\u00a8ive conversion is too high. To preserve accuracy of the captured trace, the trace \nis processed and .ltered in various ways before producing the .nal replay code. The most important post-processing \ntask is trimming. The record-time instrumentation memo\u00adizes some objects which ultimately do not need \nto be gen\u00aderated as mock objects in the replay. In particular, since we memoize all variables in the \ncontext of each closure used as an event handler, recreating all the relevant mock objects would be both \nexpensive and unnecessary. To restore true interaction with a given object in a trace, it is possible \nto follow the variable and its dependencies through the trace, determine all objects which are ultimately \nattained by refer\u00adences from the variable to be removed, and remove them. All EVT events, when logged \nduring recording, have an associ\u00adated CTX event describing the event s closure context. The replay implementation \nwraps functions which are referred to by at least one CTX event such that their closure context may be \nreproduced. For instance, a function f with ID f1 which closes over x and y but modi.es neither is wrapped \nas follows. var f = (window.handlers.f1 = function(x, y) {return function() { /* original body */ }})(x, \ny); However, quite frequently the closure context never changes, making this wrapping an unnecessary \nexpense. As such, all CTX events which correspond to the same static function are compared, and those \nvariables which never change are re\u00admoved. Any resulting CTX events which refer to no variables at all \nare removed, and associated EVT events are then free to refer to the static functions directly. Such \nEVT events do not generate wrapped functions to regenerate closure con\u00adtext, and as such have much lower \noverhead. In our experi\u00adence, the vast majority of CTX events are removed from the trace.  4.3 Practical \nChallenges While the principles of memoization are outlined above, many other issues had to be addressed \nbefore JSBENCH was able to record and replay large real sites. Missed and Uninstrumented Code. There \nare two primary means by which uninstrumented code can be run: The proxy may have failed to recognize \nand instrument it, or it may have been generated entirely by JavaScript. The solution to the former case \nis simply to .x the instrumentation engine in the proxy, but the latter case is more dif.cult to contend \nwith; JavaScript s semantics make it impossible to override the eval function, so there is in general \nno way to reliably ensure that generated code is instrumented. We have no solution to this problem, but \nhave observed that it does not substantially reduce the correctness of the technique because: Most complicated \neval code is loaded through XHR re\u00adquests, and so will be instrumented.  The remaining eval code tends \nnot to access memoized objects.  Neither of these properties are intrinsic to the language how\u00adever, \nand so it is possible for real benchmarks to function improperly due to uninstrumented code. Failure \nPruning. Our system is imperfect, and on some sites it fails to create a workable replay. Although ideally \nthis would be solved by .xing whatever bug led to the replay failure, it is also possible to identify \nwhich events fail (by catching exceptions in the generated replay code) and pruning them from the log, \nthereby generating a working replay that captures less of the original behavior. Closures. Although JavaScript \ns event dispatch semantics generally passes only simple and trivially-memoizable ob\u00adjects as arguments \nto event handler functions, the func\u00adtions themselves can be closures, and as such may have im\u00adplicit \ndependencies on arbitrarily-complex data in their clo\u00adsure contexts. Furthermore, it is impossible purely \nwithin JavaScript code to get a reference to a particular instance of a closure without having bound \nit to a name. Binding ev\u00adery closure to a name is impractical due to the high runtime cost of the mechanism. \nFurthermore, such a solution would be fragile to any inconsistencies between record and replay. Instead, \nwe use our memoization techniques to capture all closed variables, but regenerate only those that change \nat runtime. Mirroring. Making redistributable and replayable record\u00adings requires mirroring the web page, \nas otherwise running the benchmarks would necessitates setting up the browser s HTTP proxy, an unacceptable \nrequirement for performance testing. Mirroring dynamic web pages is a research problem in and of itself, \nbut is not within the scope of this paper. We used three techniques to mirror web sites, each of which \nworked on particular sites: Static mirroring. Download tools such as wget have the ability to mirror \nweb pages, but as they are not browsers and therefore cannot mirror dynamic content.  Dynamic mirroring. \nBrowsers such as Firefox and Chrome have the ability to save whole web pages by saving everything from \nthe browser s internal state, rather than the original .les. This has the disadvantage that dy\u00adnamic \nchanges made to the web page by JavaScript code will be saved, which can con.ict with the replay code \nwhich will redo their effect.  Cache mirroring. The proxy used for instrumentation is also capable of \ncaching. The cached .les can be ex\u00adtracted from the proxy for mirroring purposes. This has the advantage \nof including every referenced .le, but the disadvantage of requiring manual remapping of the .les to \ntheir new locations on disk.  Harness. To simplify running benchmarks generated by JSBENCH, the mirrored \nsite is combined with replay. js and is placed in a separate iframe. The replay function is then called \nand the execution time is timed using the JavaScript Date functions. Our harness uses a push-button approach \nto run all the recorded sites, similar to the setup of SunSpider benchmarks.  4.4 Replay Modes While \nwe are focusing on JavaScript throughput, one of the strengths of the JSBENCH approach is that the level \nof non-determinism in the replay code can be dialed up to evaluate other browser features. We have implemented \nthree dimensions of customizability. Event loop generation. In JavaScript, execution unfolds by calling \na series of event handlers in a single thread. When replaying the event handlers, the challenge is to \ninvoke them in a manner similar to the original in-browser execution. JS-BENCH supports several options: \n All handlers may be collapsed into one, resulting essen\u00adtially in one very long event. This approach \ndoes not exer\u00adcise the browser s event loop, deviating from the original execution.  The original events \nmay be recreated by createEvent, then .red by dispatchEvent. However, this mechanism does not integrate \nwith our memoization system, and has poor inter-browser compatibility. These methods are uncommon in \nreal code, so being sensitive to their timing may produce unrealistic benchmarks.  Event handlers may \nbe invoked using the setTimeout function, passing the handler closure as the callback argu\u00adment and 0 \nas the delay. Ideally this would simply yield to the browser and .re as soon as possible, but in fact \nmost browsers cannot time out for less than a particular OS-and browser-dependent amount of time, so \nmost of the execution time would be spent idle.  The postMessage mechanism, although intended for inter-domain \nand inter-frame communication, also yields to the browser s event loop to .re the message handler. On \nmost browsers, there is no idle time between plac\u00ading a message with postMessage and the handler .ring, \nbut other browser jobs can be performed. postMessage, however, is extremely uncommon in real code, and \non some browsers the implementation is slow. In spite of these limitations, it is currently the most \nreliable mech\u00adanism available which yields to the browser event loop.  DOM Removal. If we wish to create \na JavaScript bench\u00admark that can be run outside the context of a browser or that exercise only the JavaScript \nengine, we need to remove ac\u00adcesses to DOM objects. As such, our record-time instrumen\u00adtation memoizes \nDOM objects. Objects generally considered to be part of the DOM are those accessible by traversing members \nof the document object. The replays created will normally have no interaction with the DOM, as mock ob\u00adjects \nwill be put in its place. To restore DOM interaction, we need only to trim document and its dependencies \nfrom the trace. Mock Object Collection. Because every access to every memoized object is traced, the \nexact lifespan that each of these objects needs is known. As such, a simple processing procedure guarantees \nthat mock objects are created as late as possible, and all references to them held by the replay code \nare removed as early as possible. This optimization trades time for space: without it, all mock objects \nwould be alive for the entire duration of the replay, taking a lot of unnecessary space, but the process \nof creating and destroying them would not be part of the replay proper, removing that time from the recorded \ntime of the benchmark. 5. Empirical Evaluation This section provides a detailed empirical evaluation \nof JS-BENCH as well as evidence supporting our claims about the quality of the benchmarks created with \nour tool. The empiri\u00adcal evaluation relies on two different research infrastructures which we extended \nfor this paper. 1. TracingSafari is an instrumented version of the Safari 5 browser (WebKit Rev. 49500) \nbased on [13] which is able to record low-level, compact JavaScript execution traces. We use it to compare \nthe behavior of replays to live traces. Even though TracingSafari only runs in interpreter mode, the \ninstrumentation is lightweight enough to be barely noticeable on most sites. 2. FirefoxMem is a heavily \ninstrumented build of the Fire\u00adfox 4.0b2 browser that produces exhaustive information about memory and \nobject lifetimes. The cost of memory tracing is rather prohibitive, but FirefoxMem provides a very detailed \npicture of memory usage.  We used an unmodi.ed version of Internet Explorer with ETW (low-overhead \nevent tracing) enabled. We also used several different versions of popular browsers, listed in Fig\u00adure \n10, in order to highlight certain properties of the bench\u00admarks generated by JSBENCH. Unless indicated \notherwise, to obtain the experimental results presented in this section were obtained on an HP xw4300 \nWorkstation (Intel Pen\u00adtium 4 3.6 GHz machine with 2 Gigabytes of memory) run\u00adning Windows 7 Enterprise \n64-bit operating system. Short name Browser Browser version IE8 MS Internet Explorer 8 (8.0.7600.16385) \nIE9 MS Internet Explorer 9 Preview 4 (9.0.7916.6000) FF3 Mozilla Firefox 3.6.8 FF4 Mozilla Firefox 4.0b2 \nChrome5 Google Chrome 5.0.375.99 Chrome6 Google Chromium 6.0.492.0 (Rev. 55729) Opera10 Opera 10.61.3484 \nSafari5 Apple Safari 5.0.1 (build 7533.17.8) Figure 10. Browsers used for measurements.  5.1 Web Applications \nJSBENCH is capable of creating replayable workloads from most websites, but not all of these will be \nuseful in practice. We have identi.ed four properties for good candidates for benchmarks. Reasonable \nexecution time: A short-running benchmark is subject to more perturbation due to system conditions than \na long-running benchmark. Ideally the inter\u00adaction with the web page should be long enough that the gen\u00aderated \nbenchmark produces consistent results. Meaningful interaction: One can easily produce a long-running \nbench\u00admark on many sites by simply waiting several minutes while timer events .re, then measuring those \ntimer events. How\u00adever, this is not a realistic interaction with the site. To be comparable to the real \nsite s expected behavior, the interac\u00adtion should be representative of a typical end-user s experi\u00adence. \nLimited overhead: The amount of overhead introduced by JSBENCH depends on the number of proxies that \nhave to be introduced. On a site for which the replay introduces many mock objects, the time taken to \ncreate skew the results. Many browsers come with pro.lers that can be used to es\u00adtimate this overhead; \nthe benchmarks we have presented all had an overhead of under 10%. Generality: The site chosen for recording \nshould have behavior which is representative, so that its results generalize a signi.cant segment of \nweb ap\u00adplications. We have constructed eight benchmarks for this paper, listed in Figure 11. The .rst \ntwo are small programs that are self-contained JavaScript applications. sibeli.us is an arcade-style \ngame and JSMIPS is a MIPS emulator.5 Both are small single-.le JavaScript programs. The latter six rep\u00adresent \na large class of widely-used web applications, accord\u00ading to alexa.com. We do not claim that this is \na de.nitive set of benchmarks, selecting such a set will require further ex\u00adperimentation, community \ninvolvement and, in order to dis\u00adtribute the suite, consent from web site s owner. The power of JSBENCH \ncomes from the fact that anyone can take their own benchmark and package it. Benchmark Bytes Files LOC \nsibeli.us 19K 1 650 JSMIPS 108K 1 4,036 amazon.com 1,065K 10 23,912 microsoft.com 791K 21 17,241 bing.com \n39K 4 1,872 maps.google.com 572K 8 5,318 economist.com 239K 5 7,129 msnbc.msn.com 621K 11 4,273 total \n3,4564K 61 60,585 Figure 11. Summary. Information about original sites.  5 http://codu.org/jsmips For \nweb applications we have captured user interactions of length around 1 minute. The sites that were selected, \namazon, microsoft, bing google, economist, msnbc, are all among the top 100 US web site and according \nto [13] exhibit representative dynamic behavior. The sizes and complex\u00adity of these sites varies between \n1,8 KLOC and 23 KLOC, as measured in lines of JavaScript code. As a point of comparison, the entire SunSpider \nand V8 benchmark suites have 13,963 and 11,208 LOC, respectively, being composed of small, well-behaved \napplications [11, 13]. 5.2 Determinism We have evaluated determinism of our benchmarks both within the \nsame browser and across browsers and found no difference in the behavior of each benchmark. They run \nin a completely deterministic fashion and are DOM-equivalent. Determinism of the re\u00adplays across browsers \nhinges Replay mode on hiding browser-speci.c behavior from the replay Browser Real Poison code by the \nmeans of mock IE8 0 11 objects. We experimented with the impact of removing Firefox 3 0 0 the memoization \naround Chrome 5 0 2 the navigator object which Figure 12. Browser is used by JavaScript code differences. \n to detect the browser. This (economist.com) will result in each browser behaving slightly differently \nif the application contains browser snif.ng code code specialized to a particular browser or version. \nFigure 12 shows the result of a replay instrumented to output the call path of the program. In this run, \nthere are 10,773 function calls. Column 2 shows that if we memoize the navigator ob\u00adject, the replay \nis fully deterministic, as demonstrated by an identical call trace. Column 3 of the table shows the number \nof differences in the call trace obtained when we record with Firefox and replay with a different browser. \nNote that the difference numbers are all quite small, representing only a tiny percentage of the overall \ntrace. 5.3 Fidelity One measure of .delity is to verify that any DOM call observed during trace recording \nis present, with identical arguments, at replay time. This is true by construction of our trace and we \nhave validated it experimentally. Another measure of .delity is to line up events .red in the recording \nand in the replay. JavaScript programs usu\u00adally have several load-time events .ring, many XHR events \n.ring, then a sequence of user interaction events .ring. We compare the event .ring behavior. To do so, \nwe collect event .ring traces and post-processed them to correlate the events and their order of dispatch. \nWe observed perfect .delity, with all events appearing in record and replay traces, but some difference \nin ordering due to timing issues. Since the traces  (a) Load-time (b) Time-based events (c) Quiescence \nare about 2,000 events each, we cannot show them fully. In\u00adstead, we compare three representative segments \nof the trace separately. The result of this experiment is shown in Fig\u00adure 13. Each oval represents an \nindividual event, with real events on the left and replay events on the right. (a) Load-time. The initial \nportion of the two traces match up quite well, except for the fact that they are offset by two stray \nXHR events happening in the replay that happen later in the real trace. This is an example of browser \nscheduling non-determinism. (b) Time-based events. This segment is taken from the middle of the trace, \nwhen various timers that run as part of standard Amazon.com execution kick in. Unsur\u00adprisingly, with \ntimer-based, XHR and onload events be\u00ading .red by the browser s scheduler, the real and re\u00adplay events \ncan be scheduled in a very different order, as shown in the .gure. (c) Quiescence. This segment corresponds \nto the end of the trace and a state of quiescence for this site. The traces match up perfectly.   5.4 \nAccuracy Comparing the behavior of the replay with the original pro\u00adgram is a bit more tricky. A replay \nPR has been obtained by running an instrumented program, thus it is conceivable that the behavior observed \nat recording, R(P ) is signi.cantly different from an un-instrumented run of the original pro\u00adgram P \n. While, ideally one could compare traces, d(T, T ), our infrastructure can not give us a trace of the \noriginal pro\u00adgram without substantially perturbing the very characteris\u00adtics we want to observe. So instead \nof measuring the distance between traces, we will argue for accuracy by observing a number of properties \nof original and replay executions and argue that they have suf.cient similarities so that replays can \nFigure 14. Write accuracy. Each point on x-axis repre\u00adsents one thousand bytecodes executed by the JavaScript \nen\u00adgine. The y-axis gives the absolute number of object prop\u00aderty writes performed in each 1K window. \nThe maximum deviation observed over multiple run was 10.4%. (msnbc; TracingSafari). As a .rst approximation \nof replay accuracy, we provide a high-level view of the updates performed by the bench\u00admark on non-DOM \nobjects. While .delity ensures that all DOM updates performed in the recording will also happen at replay, \nit makes no guarantees about other writes. Fig\u00adure 14 plots the number of writes that are performed in \nin a window of one thousand bytecodes. We compare an original (non-instrumented) run of msnbc with a \nrun of the replay program. The data is obtained using TracingSafari as it has a non-intrusive (browser-speci.c) \nrecording mechanism. Vi\u00adsually, it is clear that original and the replay line up, but are not identical. \nThis is expected as any non-instrumented run will have different numbers of timer events, different order \nof events, and the replay has mock objects. We measure the difference of between the original and the \nreplay trace using normalized root-mean-square deviation (NRMSD).6 For .ve real and .ve replay runs, \nthe maximum NRMSD is 10.4% which suggests that the replay are generally close to origi\u00adnal runs in terms \nof the update operations they perform. The NRMSD between replay runs is always 0% (attesting to their \ndeterminism). To get another reading on replay accuracy, we measured the internal operations performed \nthe JavaScript engine dur\u00ading execution of a replay and compared it with an origi\u00adnal run. For this measurement \nwe used the ETW, a low\u00adoverhead tracing framework supported by Internet Explorer. ETW let us measure \nthe number of invocations of the Java-Script parser, the bytecode compiler, the native code gener\u00adator, \nother calls to the engine, and calls to the DOM. Fig\u00ad 6 NRMSD is a common statistical measure of the \ndeviation between func\u00adtions; however, it is not ideal as it has no ability to contend with repeated \nor re-ordered events. Behavior Real Replay Mean diff. Mean Std. dev. Mean Std. dev. Parsing 20 0 20 \n0 0 Bytecode Gen 20 0 20 0 0 Native Code Gen 32.4 2.6 32.4 3.8 0 Calls to JS engine 2,540.6 36.86 2,525 \n0 15.6 Calls to DOM 62,032 566.35 61,753 0 279 Figure 15. Service tasks. Calls to the JavaScript engine \nover .ve runs of the real application and its replay. (amazon; IE9).  ure 15 summarizes the results \nof .ve different run of the original amazon site and .ve runs of the replay. As can be readily observed, \ndifferent real runs have make slightly dif\u00adferent number of service requests on the JavaScript engine. \nThe replay on the other hand is deterministic. The last col\u00adumn gives the absolute difference in the \nmeans. This differ\u00adence is rather small and within the standard deviation. The number of times parsing \nand bytecode generation is invoked is exactly identical in both original and replay. ETW also allows \nus to measure the time spent garbage collecting. It is important to make sure that our replay mech\u00adanism \ndoes not fundamentally affect the memory behavior of the program. For this we report in Figure 16 the \nnumber of calls to the garbage collector in each run of the real and replay program and the time spent \nin GC. The number of GC cycles is slightly smaller in the replay runs but the amount of time actually \nspent in GC ranges from similar to slightly higher, which is easily explained by the introduction of \nmock objects. (a) GC Cycles (b) GC Time (ms) To get a better understanding of the memory usage and impact \nof mock objects we used FirefoxMem to record the JavaScript heap growth over time. Figure 17 shows the \nsize of heap over time. It compare .ve real runs to .ve replay runs. The overall behavior is similar \nalthough one can ob-serve time compression as the replays complete faster than the real runs. The replay \nruns with heap size 6.8% smaller than the original program, a reduction due to the fact that mock objects \nare smaller than the objects they are replacing. Though, this need not be the case.  We observed that \nreplay experience time compression as slack time is eliminated and native operations are memoized. Figure \n18 shows the CPU utilization for microsoft.com over time for original and replay runs. While total CPU \nalign nicely, the real site takes considerably longer than the re\u00adplay, 710 ms compared to 265 ms. One \npotential threat due to time compression is that the lack of slack time removes op\u00adportunities for the \nJavaScript engine to perform service tasks such as code JITing, garbage collection, and code prefetch. \nThis may be an important consideration in browser engineer\u00ading and, as such, illustrates the inherent \nchallenges in creat\u00ading effective browser benchmarks.  5.5 JSBench vs. SunSpider A representative benchmark \nshould serve as a predictor of performance of a system on real sites and a guide for imple\u00admenters. We \nhave argued that industry standard benchmarks are ill suited to this task. We provide one sample experiment \nto back up this claim. Figure 19 gives the relative throughput improvement, over Firefox 1.5, obtained \nby subsequent ver\u00adsions when running the SunSpider industry standard bench\u00admark and a benchmark constructed \nby JSBENCH from an in\u00adteraction with the amazon website. The graph clearly shows that, according to SunSpider, \nthe performance of Firefox im\u00adproved over 13\u00d7 between version 1.5 and version 3.6. Yet when we look at \nthe performance improvements on amazon they are a more modest 3\u00d7. And even more interestingly, in the \nlast two years, gains on amazon have .attened. Suggest\u00ading that some of the optimizations that work well \non Sun-Spider do little for amazon. Note that as we have previously demonstrated [11, 13], popular sites \nbehave rather similarly, so we anticipate the results for other large popular sites to be similar to \nwhat we are observing for amazon.  5.6 Browser-speci.c Replays In this paper, our primary focus is \non comparing the perfor\u00admance of JavaScript engines by running them on JavaScript\u00adonly versions of our \nbenchmarks. However, JSBENCH does support generation of traces with some browser-speci.c op\u00aderations \nleft in. In these partially-deterministic modes JS-BENCH does not guarantee that the program will run \nidenti\u00adcally, or at all, in a different browser (because the browser may perform DOM accesses that were \nnot encountered at recording time), but when replays can run in multi\u00adple browsers it is possible to \ncompare the impact of other browser features on performance. We start by looking at the performance impact \nof DOM operations. For this we measure the performance of a replay without mock objects for DOM reads/writes. \nThis means  Figure 21. Cross-browser comparison. Impact of events on throughput. (number normalized \nto the replay without event processing; average over .ve runs; lower is better) that throughput measure \nwill include the time spent in the browser s DOM implementation. Figure 20 illustrates the relative performance \nof browsers with DOM turned on in the amazon and bing benchmarks. We can see that the impact of DOM operations \nis negligible for bing and substantial for amazon. We see that Safari5 and IE9 stand out in the case \nof amazon, which may be because of a slower DOM implementation or a comparatively fast JavaScript engine. \nFigure 21 shows the relative cost of enabling event pro\u00adcessing in replay as the ratio of the running \ntimes. In many browsers, the cost of event processing for the bing bench\u00admark is relatively high, and \nas high as 77x in the case of Safari5. This may be because our chosen method of event dispatch through \npostMessage is on a particularly unopti\u00admized code path in the case of that browser. Next, we look at \nhow stable our execution time results are across the dif\u00ad  (a) Layout (b) Paint (c) CSS Calc. Benchmark \nIE8 IE9 FF3 FF4 Chrome5 Chrome6 Opera10 Safari5 Sibelius 684 117 315 285 82 82 89 74 Amazon 342 63 104 \n85 110 111 147 62 Microsoft 42 4 14 11 12 10 58 5 Bing 87 11 44 51 30 27 9 9 Economist 81 103 124 48 \n39 40. 57 MSNBC 32 172 85 32 31 43 48 JSMIPS 7,773 4,935 3,480 12,596 Figure 23. Cross-browser running \ntime comparison.  Times are in milliseconds. An empty cell indicates that the benchmark could not produce \nresults, due either to insuf.\u00adcient feature support or taking too long to execute. ferent JavaScript \nengines by running each .ve times and looking at the standard deviation between the running times. Most \nbrowsers reliably produce consistent results. We eval\u00aduate consistency by computing the standard deviation \nover .ve runs and then normalizing it by the mean running time. For IE8, the maximum across all applications \nis 0.04; for FF3 it is 0.13. It is encouraging that these ratios are quite small. The browser that stands \nout in terms of inconsis\u00adtency is Opera. For some of the benchmarks, this ratio is as high 2.1, which \nimplies that either Opera s speed is very in\u00adconsistent, or its JavaScript time mechanisms are incorrect. \nIn fact, the bing.com benchmark sometimes yields negative time on Opera, indicating that its Date s do \nnot monotoni\u00adcally increase! Figure 22 demonstrates that for the number of layout, paint, and CSS calculation \nevents performed by the browser, the replay trace is actually more deterministic than the real trace. \nWe can think of our replay mechanism as removing some of the inherent browser uncertainty. Despite the \nfact our goal is to enable JavaScript engine comparisons, we acknowledge that our benchmarks will be \nused to compare browsers. We provide a snapshot of the performance of our benchmarks on browsers available \nat the time of writing in Figure 23. 6. Conclusions Previous work has shown that relying on industry-standard \nbenchmark suite leads to optimizations that do not improve the throughput of JavaScript engines running \ncomplex web applications. This paper has presented a methodology for constructing realistic benchmarks \nas replays of event-driven, interactive web applications. The JSBENCH tool can do so by encapsulating \nall the non-determinism in the execu\u00adtion environment, including user input, network IO, timed events, \nand browser-speci.c features. The result is a re\u00adplayable program that can be deterministically executed \nin a JavaScript engine with a high degree of .delity compared to the recorded trace, and high accuracy \nwhen compared to runs of the original web site. As a caveat, while we believe that we have demonstrated \nthat creating representative benchmarks with a high degree of .delity is possible with this approach, \nwe do not claim that the speci.c benchmarks used in this paper are in fact the correct set to be used \nin ultimately comparing the performance of JavaScript engines. With JS-BENCH, however, one can easily \ncapture benchmarks that matter. As the use of client-heavy web applications evolves, approaches such \nas JSBENCH will enable browser manu\u00adfacturers and web site builders to tailor their efforts to the ever-changing \napplication landscape. Acknowledgments The authors thank Ben Livshits and Ben Zorn at Microsoft Research \nfor their input, discussions and feedback during the development of this project as well as Steve Fink \nat Mozilla for providing a custom instrumented build of Mozilla Fire\u00adfox for our evaluation. This material \nis based upon work sup\u00adported by the National Science Foundation under Grant No. 1047962 and 0811631, \nby a SEIF grant from Microsoft Re\u00adsearch and PhD fellowship from the Mozilla Foundation. References [1] \nS. M. Blackburn et al. The DaCapo benchmarks: Java bench\u00admarking development and analysis. In Conference \non Object-Oriented Programming Systems Languages and Applications (OOPSLA), pages 169 190, 2006. [2] \nF. Cornelis, A. Georges, M. Christiaens, M. Ronsse, T. Gh\u00adesquiere, and K. D. Bosschere. A taxonomy of \nexecution re\u00adplay systems. In Conference on Advances in Infrastructure for Electronic Business, Education, \nScience, Medicine, and Mo\u00adbile Technologies on the Internet, 2003. [3] J. de Halleux and N. Tillmann. \nMoles: Tool-assisted environ\u00adment isolation with closures. In International Conference on  Objects, \nModels, Components, Patterns (TOOLS), pages 253 270, 2010. [4] S. Dieckmann and U. H\u00a8olzle. A study of \nthe allocation be\u00adhaviour of the SPECjvm98 Java benchmarks. In European Conference on Object Oriented \nProgramming, (ECOOP), pages 92 115, 1999. [5] C. Dionne, M. Feeley, and J. Desbien. A taxonomy of dis\u00adtributed \ndebuggers based on execution replay. pages 203 214, 1996. [6] A. Gal et al. Trace-based just-in-time \ntype specialization for dynamic languages. In Conference on Programming Lan\u00adguage Design and Implementation \n(PLDI), pages 465 478, 2009. [7] E. Kiciman and B. Livshits. Ajaxscope: a platform for re\u00admotely monitoring \nthe client-side behavior of Web 2.0 ap\u00adplications. In Symposium on Operating Systems Principles (SOSP), \npages 17 30, 2007. [8] B. Livshits and E. Kiciman. Doloto: code splitting for network-bound Web 2.0 applications. \nIn Conference on Foun\u00addations of Sofware Engineering (FSE), pages 350 360, 2008. [9] J. Mickens, J. Elson, \nand J. Howell. Mugshot: Deterministic capture and replay for JavaScript applications. In Symposium on \nNetworked Systems Design and Implementation (NSDI), 2010. [10] K. Pattabiraman and B. Zorn. DoDOM: Leveraging \nDOM invariants for Web 2.0 applications robustness testing. In International Symposium on Software Reliability \nEngineering (ISRE), 2010. [11] P. Ratanaworabhan, B. Livshits, and B. Zorn. JSMeter: Com\u00adparing the behavior \nof JavaScript benchmarks with real Web applications. In Conference on Web Application Development, 2010. \n[12] G. Richards, C. Hammer, B. Burg, and J. Vitek. The eval that men do: A large-scale study of the \nuse of eval in JavaScript applications. In European Conference on Object-Oriented Programming (ECOOP), \n2011. [13] G. Richards, S. Lebresne, B. Burg, and J. Vitek. An analysis of the dynamic behavior of JavaScript \nprograms. In Confer\u00adence on Programming Language Design and Implementation (PLDI), pages 1 12, 2010. \n[14] K. Vikram, A. Prateek, and B. Livshits. Ripley: automatically securing Web 2.0 applications through \nreplicated execution. In Conference on Computer and Communications Security (CCS), pages 173 186, 2009. \n  \n\t\t\t", "proc_id": "2048066", "abstract": "<p>JavaScript is a highly dynamic language for web-based applications. Innovative implementation techniques for improving its speed and responsiveness have been developed in recent years. Industry benchmarks such as WebKit SunSpider are often cited as a measure of the efficacy of these techniques. However, recent studies have shown that these benchmarks fail to accurately represent the dynamic nature of modern JavaScript applications, and so may be poor predictors of real-world performance. Worse, they may guide the development of optimizations which are unhelpful for real applications. Our goal is to develop a tool and techniques to automate the creation of realistic and representative benchmarks from existing web applications. We propose a record-and-replay approach to capture JavaScript sessions which has sufficient fidelity to accurately recreate key characteristics of the original application, and at the same time is sufficiently flexible that a recording produced on one platform can be replayed on a different one. We describe JSBench, a flexible tool for workload capture and benchmark generation, and demonstrate its use in creating eight benchmarks based on popular sites. Using a variety of runtime metrics collected with instrumented versions of Firefox, Internet Explorer, and Safari, we show that workloads created by JSBench match the behavior of the original web applications.</p>", "authors": [{"name": "Gregor Richards", "author_profile_id": "81438595000", "affiliation": "Purdue University, West Lafayette, IN, USA", "person_id": "P2839249", "email_address": "gkrichar@purdue.edu", "orcid_id": ""}, {"name": "Andreas Gal", "author_profile_id": "81100568135", "affiliation": "Mozilla Foundation, Mountain View, CA, USA", "person_id": "P2839250", "email_address": "gal@mozilla.com", "orcid_id": ""}, {"name": "Brendan Eich", "author_profile_id": "81100620870", "affiliation": "Mozilla Foundation, Mountain View, CA, USA", "person_id": "P2839251", "email_address": "brendan@mozilla.org", "orcid_id": ""}, {"name": "Jan Vitek", "author_profile_id": "81100018102", "affiliation": "Purdue University, West Lafayette, IN, USA", "person_id": "P2839252", "email_address": "jvitek@purdue.edu", "orcid_id": ""}], "doi_number": "10.1145/2048066.2048119", "year": "2011", "article_id": "2048119", "conference": "OOPSLA", "title": "Automated construction of JavaScript benchmarks", "url": "http://dl.acm.org/citation.cfm?id=2048119"}