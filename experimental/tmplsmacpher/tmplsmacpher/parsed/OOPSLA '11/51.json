{"article_publication_date": "10-22-2011", "fulltext": "\n Delegated Isolation Roberto Lublinerman Jisheng Zhao Zoran Budimli\u00b4c Pennsylvania State University \nRice University Rice University rluble@psu.edu jisheng.zhao@rice.edu zoran@rice.edu Swarat Chaudhuri \nVivek Sarkar Rice University Rice University swarat@rice.edu vsarkar@rice.edu Abstract Isolation the \nproperty that a task can access shared data without interference from other tasks is one of the most \nbasic concerns in parallel programming. In this paper, we present Aida, a new model of isolated execution \nfor par\u00adallel programs that perform frequent, irregular accesses to pointer-based shared data structures. \nThe three primary ben\u00ade.ts of Aida are dynamism, safety and liveness guarantees, and programmability. \nFirst, Aida allows tasks to dynam\u00adically select and modify, in an isolated manner, arbitrary .ne-grained \nregions in shared data structures, all the while maintaining a high level of concurrency. Consequently, \nthe model can achieve scalable parallelization of regular as well as irregular shared-memory applications. \nSecond, the model offers freedom from data races, deadlocks, and livelocks. Third, no extra burden is \nimposed on programmers, who ac\u00adcess the model via a simple, declarative isolation construct that is similar \nto that for transactional memory. The key new insight in Aida is a notion of delegation among concurrent \nisolated tasks (known in Aida as assem\u00adblies). Each assembly A is equipped with a region in the shared \nheap that it owns the only objects accessed by A are those it owns, guaranteeing race-freedom. The region \nowned by A can grow or shrink .exibly however, when A needs to own a datum owned by B, A delegates itself, \nas well as its owned region, to B. From now on, B has the responsibility of re-executing the task A set \nout to complete. Delegation as above is the only inter-assembly communication primi\u00adtive in Aida. In \naddition to reducing contention in a local, Permission to make digital or hard copies of all or part \nof this work for personal or classroom use is granted without fee provided that copies are not made or \ndistributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. OOPSLA 11, October 22 27, 2011, Portland, Oregon, USA. Copyright \nc &#38;#169; 2011 ACM 978-1-4503-0940-0/11/10. . . $10.00 data-driven manner, it guarantees freedom from \ndeadlocks and livelocks. We offer an implementation of Aida on top of the Ha\u00adbanero Java parallel programming \nlanguage. The implemen\u00adtation employs several novel ideas, including the use of a union-.nd data structure \nto represent tasks and the regions that they own. A thorough evaluation using several irreg\u00adular data-parallel \nbenchmarks demonstrates the low over\u00adhead and excellent scalability of Aida, as well as its bene.ts over \nexisting approaches to declarative isolation. Our results show that Aida performs on par with the state-of-the-art \ncus\u00adtomized implementations of irregular applications and much better than coarse-grained locking and \ntransactional memory approaches. Categories and Subject Descriptors D.1.3 [Programming Techniques]: Concurrent \nProgramming; D.3.2 [Program\u00adming Languages]: Language Classi.cations Concurrent, distributed, and parallel \nlanguages General Terms Languages, Design Keywords Isolation, Programming abstractions, Irregular parallelism, \nContention 1. Introduction Isolation1 the property that a task can access shared data without interference \nfrom other tasks has always been a fundamental concern in shared-memory parallel program\u00adming. It is \nalso the topic of much recent research [19, 22, 35]. Much of this research is driven by the observation \nthat the traditional approach to enforcing isolation lock-based synchronization has some serious shortcomings, \nand falls short of the demands made by everyday programmers in an era of multicore computers. On one \nhand, low-level, .ne\u00adgrained locking is too complex and error-prone, and requires 1 In the parallel programming \nliterature, the terms isolation and atomic\u00adity are often con.ated. As advocated in [19], we use isolated \nto indicate a statement that executes without interference from other isolated statements, as opposed \nto atomic which implies strong atomicity.  an effort on the part of the programmer that does not scale \nwell with software size. On the other hand, coarse-grained locking leads to scalability bottlenecks. \nConsequently, nu\u00admerous research groups have embarked on quests for declar\u00adative and scalable approaches \nto isolation. At the language level, a declarative approach to isola\u00adtion lets the programmer demarcate \nblocks of code that are to be executed in an isolated manner the complexity of maintaining isolation \nis thereby pushed into the compiler and the runtime system. As might be expected, proper de\u00adsign and \nimplementation of such compilers and runtimes is highly nontrivial. A primary concern here is scalable \nper\u00adformance on challenging, real-world applications. A second goal is programmability: the programming \ninterface for the construct should be as simple, as general, and as modular as possible. A third goal \nis the ease of reasoning about pro\u00adgrams that the model provides and the availability of high\u00adlevel correctness \nguarantees, such as freedom from dead\u00adlocks and livelocks. Meeting the above goals is especially daunting \nin settings where the goal is to parallelize applications that make fre\u00adquent dynamic accesses to large, \npotentially sparse shared\u00admemory data structures [25]. Such sparse graph applica\u00adtions appear in a wide \nrange of domains including data min\u00ading, scienti.c computing, compilers, graphics, network sci\u00adence, \nand system modeling [32]: concrete examples include classic graph algorithms like shortest-paths or minimum \nspanning trees, algorithms for ray-tracing, Delaunay trian\u00adgulation, mesh re.nement [10], epidemic simulations \n[4], N\u00adbody simulations, and solutions of partial differential equa\u00adtions. These applications operate \non large shared data struc\u00adtures, are resource-intensive and contain large amounts of in\u00adtrinsic parallelism. \nHowever, building scalable runtime sys\u00adtems guaranteeing isolation is especially challenging in this \ncontext, as the available parallelism is highly data-dependent and prone to changes at runtime, with \nsome pathological in\u00adstances exhibiting no parallelism at all. Not only does such irregular parallelism \nconfound static parallelization, it baf\u00ad.es dynamic approaches based on transactional memory as well \n[26]. In this paper, we present Aida, a new model of isolated concurrent execution targeted at this dif.cult \nproblem space. Rather than adapting an existing model for isolated execu\u00adtion (such as software transactions), \nwe aimed in this effort for a clean operational semantics informed by a foundational perspective on irregular \nparallelism. The outcome Aida is a minimalistic execution model containing a single prim\u00aditive for data \ncommunication that lets a task A delegate its work to a different task B. Pleasantly, the model satis.es \neach of our design objectives: Our implementation of Aida (on top of the Habanero-Java parallel programming \nlanguage [7, 18]) performs or\u00adders of magnitude better than the Java-based transaction library DSTM2 \n[12] on irregularly parallel benchmarks from the Lonestar benchmark suite [24]. Its scalability is competitive \nwith Galois [25], a custom library and pro\u00adgramming model for parallelizing loops with irregular data \naccesses. Like transactions and unlike Galois, Aida offers a highly general and minimalistic programming \nmodel. The pro\u00adgrammer uses a single keyword to declare certain asyn\u00adchronous tasks to be isolated, and \nno further annotation is demanded. Aida can support any computation in an isolated task, and is not restricted \nto operations on pre\u00adselected data structures.  Aida satis.es strong safety and liveness properties, \nguar\u00adanteeing, among others, deadlock-and livelock-freedom.  In the rest of this paper, we elaborate \non the main ideas in Aida (Section 2), our implementation of the model (Sec\u00adtion 3), and experimental \nresults (Section 4). Related work is discussed in Section 5; we conclude with some discussion in Section \n6. 2. The Aida Execution Model The .rst basic insight behind Aida is the following: In a problem domain \nwhere the extent of available parallelism is data-dependent and prone to dynamic changes, granularity \nof tasks, communication be\u00adtween tasks, and management of contention should be dynamic as well. In most \ntraditional models of concurrency, the logic for orga\u00adnizing an application into parallel tasks, as well \nas the com\u00admunication between these tasks, is decided statically. This is a de.ciency in the context \nof irregular applications, where the optimal granularity of parallelism depends intimately on the problem \ninstance. For example, it is known [25, 28, 30] that while typical irregular applications require global \nmemory access in the worst case, the effects of their imperative updates are often restricted to small, \nlocal regions in shared data structures. A natural strategy for parallelization, then, would be to let \neach of these regions be processed by a distinct concurrent, isolated task. However, because the regions \nin question are highly instance-dependent, it is impossible to know statically the number and schedule \nof tasks that would lead to optimal parallelism, as well as interferences among tasks. A more dynamic \napproach is needed. To see why the principled design of such a dynamic ap\u00adproach is not obvious, consider \na scenario that is unavoidable in any dynamic approach to isolation. Here, tasks A and B, executing isolated \ncode and each owning parts of the heap, con.ict over an object u in the shared heap. (See Figure 1; here \nnodes represent shared objects, edges represent point\u00aders between objects, the shaded ovals represent \nthe regions of objects owned by the two tasks.) Clearly, only one of the two tasks say A should be allowed \nto access u. But what happens to B? Letting B block while maintaining owner\u00adship of some data does not \nseem to be a good idea: not only will tasks seeking B s data now have to wait, in the worst case, we \nwill have deadlocks. Another option is to let the task manager retry B. If we retry B too soon, we will \nhave unnecessary contention that will degrade performance; in the worst case, we will have livelocks. \nIf we do not retry B soon enough, do we release all objects held by B? If so, what if a task C acquires \nsome of this data before B is retried?  We note that the right strategy for concurrent execution here \ndepends to a large extent on which thread owns which objects, and it is imperative in these settings \nthat task and ownership management are somehow coupled. Unfortu\u00adnately, this is not the case in many \ntraditional dynamic ap\u00adproaches to isolation. Rather than using an operational se\u00admantics that relates \ntasks with ownership, they often resolve con.icts using heuristics like exponential back-off [34] that \ndo not offer solid guarantees, are divorced from high-level programming models, and depend on ad hoc \nparameters. In contrast, the Aida execution model is based on a single principle that ties together management \nof parallel, isolated tasks and management of ownership. The fundamental ques\u00adtion in task management \nis When should two isolated tasks be run in parallel? The answer provided here is: Two isolated tasks \nA and B should be run in parallel when A and B are not trying to establish ownership over the same object \nin the shared heap. Indeed, if two tasks are trying to establish ownership over (i.e., make isolated \naccesses to) the same objects in heap, then concurrency among them is not serving any useful purpose. \nWe would therefore be better off if A and B were merged into a single task. To make such a mechanism \npossible, we let each task in Aida have explicit ownership of a local region in the shared heap, and \nonly access objects in that region. This task owner\u00adship abstraction is borrowed from the Chorus programming \nmodel [28] for irregular applications. As in Chorus [28], such a task is called an (object) assembly. \nBy de.nition, an assembly accesses the heap in an isolated manner. However, as discussed in Section 5, \nthe Chorus model is more restric\u00adtive than Aida because it is only applicable to parallelization of cautious \napplications [30] and it lacks Aida s notion of delegated isolation. In case of con.icts among concurrently \nexecuting assem\u00adblies, the key new notion of delegation comes into play. In particular, consider the \nscenario in Figure 1: A and B are assemblies, and the shaded ovals depict their owned regions. Now suppose \nB attempts to access the object u via a pointer. In Aida, this con.ict scenario is handled by letting \nB dele\u00adgate itself and its owned region to A. In essence, A and B are now merged, and from now on, A: \n(1) owns the region that B passed to A; and (2) has the responsibility of completing the task that B \nset out to do. Figure 1. An example of con.icting tasks, A and B, due to accesses to object u. (In the \n.gure, nodes correspond to objects, arrows to pointers/references, and shaded ovals to regions.) Delegation \nas above is the only inter-task communication primitive in Aida. The delegation mechanism is accessed \nby the programmer using a simple programming construct similar to that used in software transactions. \nThe core pro\u00adgramming model contains only two parallel constructs: a two-word keyphrase async isolated \nthat forks an asyn\u00adchronous task that is to be executed in an isolated manner, and a keyword finish for \nsynchronization. This frame\u00adwork for scoped fork-join parallelism is derived from the X10 [9] and Habanero \nJava [7] languages, but applied to async isolated tasks in Aida. While the finish con\u00adstruct (akin to \nsync in Cilk [3]) is a form of task synchro\u00adnization, there is no data communication involved in it and \nit does not introduce issues such as deadlocks or livelocks. Se\u00admantically, any two tasks that join at \nthe same program point and operate on different parts of the shared data structure are causally independent, \nand can be executed in any order. We observe that the above programming model for del\u00adegated isolation \nis quite general and can be integrated with any imperative parallel language; in particular, we have \nim\u00adplemented it on top of the Habanero Java [7, 18] language. Note that the underlying programming language \nmay have other primitives for task creation that offer no guarantees of isolation for example, in Habanero \nJava, there is a keyword async for creating such tasks without the isolated qual\u00adi.er. Such tasks do \nnot fall within the ambit of Aida, and are excluded from discussion in the rest of the paper. Provided \naccess to shared memory only happens from within blocks declared async isolated, isolation and race-freedom \nare guaranteed in Aida. This is because the only objects accessed by an assembly A are those it owns. \nDeadlocks and livelocks are ruled out because when two assemblies con.ict, one of them always delegates \nto the other, and the merged assembly continues to progress. Extended example: Delaunay mesh re.nement \nTo see how the Aida programming and execution model work, let us consider a classic irregular application: \n2D Delaunay mesh re.nement [10, 26]. Given a set of points in the 2D Euclidean space, a Delaunay triangulation \npartitions their convex hull into a set of triangles such that no point lies in any triangle s circumcircle. \nIn many applications [10], there \n\t\t\t", "proc_id": "2048066", "abstract": "<p>Isolation---the property that a task can access shared data without interference from other tasks---is one of the most basic concerns in parallel programming. In this paper, we present Aida, a new model of isolated execution for parallel programs that perform frequent, irregular accesses to pointer-based shared data structures. The three primary benefits of Aida are dynamism, safety and liveness guarantees, and programmability. First, Aida allows tasks to dynamically select and modify, in an isolated manner, arbitrary fine-grained regions in shared data structures, all the while maintaining a high level of concurrency. Consequently, the model can achieve scalable parallelization of regular as well as irregular shared-memory applications. Second, the model offers freedom from data races, deadlocks, and livelocks. Third, no extra burden is imposed on programmers, who access the model via a simple, declarative isolation construct that is similar to that for transactional memory. The key new insight in Aida is a notion of delegation among concurrent isolated tasks (known in Aida as assemblies). Each assembly A is equipped with a region in the shared heap that it owns---the only objects accessed by A are those it owns, guaranteeing race-freedom. The region owned by A can grow or shrink flexibly---however, when A needs to own a datum owned by B, A delegates itself, as well as its owned region, to B. From now on, B has the responsibility of re-executing the task A set out to complete. Delegation as above is the only inter-assembly communication primitive in Aida. In addition to reducing contention in a local, data-driven manner, it guarantees freedom from deadlocks and livelocks.</p> <p>We offer an implementation of Aida on top of the Habanero Java parallel programming language. The implementation employs several novel ideas, including the use of a union-find data structure to represent tasks and the regions that they own. A thorough evaluation using several irregular data-parallel benchmarks demonstrates the low overhead and excellent scalability of Aida, as well as its benefits over existing approaches to declarative isolation. Our results show that Aida performs on par with the state-of-the-art customized implementations of irregular applications and much better than coarse-grained locking and transactional memory approaches.</p>", "authors": [{"name": "Roberto Lublinerman", "author_profile_id": "81317497568", "affiliation": "The Pennsylvania State University, University Park, PA, USA", "person_id": "P2839289", "email_address": "rluble@psu.edu", "orcid_id": ""}, {"name": "Jisheng Zhao", "author_profile_id": "81361600781", "affiliation": "Rice University, Houston, TX, USA", "person_id": "P2839290", "email_address": "jisheng.zhao@rice.edu", "orcid_id": ""}, {"name": "Zoran Budimli&#263;", "author_profile_id": "81100227584", "affiliation": "Rice University, Houston, TX, USA", "person_id": "P2839291", "email_address": "zoran@rice.edu", "orcid_id": ""}, {"name": "Swarat Chaudhuri", "author_profile_id": "81309496839", "affiliation": "Rice University, Houston, TX, USA", "person_id": "P2839292", "email_address": "swarat@rice.edu", "orcid_id": ""}, {"name": "Vivek Sarkar", "author_profile_id": "81100597290", "affiliation": "Rice University, Houston, TX, USA", "person_id": "P2839293", "email_address": "vsarkar@rice.edu", "orcid_id": ""}], "doi_number": "10.1145/2048066.2048133", "year": "2011", "article_id": "2048133", "conference": "OOPSLA", "title": "Delegated isolation", "url": "http://dl.acm.org/citation.cfm?id=2048133"}