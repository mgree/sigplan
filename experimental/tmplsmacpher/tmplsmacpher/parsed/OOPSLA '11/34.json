{"article_publication_date": "10-22-2011", "fulltext": "\n Scalable Join Patterns Aaron Turon Northeastern University turon@ccs.neu.edu Abstract Coordination \ncan destroy scalability in parallel program\u00adming. A comprehensive library of scalable synchroniza\u00adtion \nprimitives is therefore an essential tool for exploiting parallelism. Unfortunately, such primitives \ndo not easily combine to yield solutions to more complex problems. We demonstrate that a concurrency \nlibrary based on Fournet and Gonthier s join calculus can provide declarative and scalable coordination. \nBy declarative, we mean that the programmer needs only to write down the constraints of a coordination \nproblem, and the library will automatically derive a correct solution. By scalable, we mean that the \nderived solutions deliver robust performance both as the number of proces\u00adsors increases, and as the \ncomplexity of the coordination problem grows. We validate our claims empirically on seven coordination \nproblems, comparing our generic solution to specialized algorithms from the literature. Categories and \nSubject Descriptors D.3.3 [Programming Languages]: Language Constructs and Features Concurrent programming \nstructures General Terms Algorithms, Languages, Performance Keywords message passing, concurrency, parallelism \n1. Introduction Parallel programming is the art of keeping many processors busy with real work. But except \nfor embarrassingly-parallel cases, solving a problem in parallel requires coordination be\u00adtween threads, \nwhich entails waiting. When coordination is unavoidable, it must be carried out in a way that minimizes \nboth waiting time and interprocessor communication. Effec\u00adtive implementation strategies vary widely, \ndepending on the coordination problem. Asking an application programmer to grapple with these concerns \nwithout succumbing to con\u00adcurrency bugs is a tall order. Permission to make digital or hard copies of \nall or part of this work for personal or classroom use is granted without fee provided that copies are \nnot made or distributed for pro.t or commercial advantage and that copies bear this notice and the full \ncitation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to \nlists, requires prior speci.c permission and/or a fee. OOPSLA 11, October 22 27, 2011, Portland, Oregon, \nUSA. Copyright &#38;#169; 2011 ACM 978-1-4503-0940-0/11/10. . . $10.00 Claudio V. Russo Microsoft Research \ncrusso@microsoft.com  var j = Join.Create(); Synchronous.Channel[] hungry; Asynchronous.Channel[] chopstick; \nj.Init(out hungry, n); j.Init(out chopstick, n); for (int i = 0; i < n; i++) { var left = chopstick[i]; \nvar right = chopstick[(i+1) % n]; j.When(hungry[i]).And(left).And(right).Do(() => { eat(); left(); right(); \n// replace chopsticks }); } Figure 1. Dining Philosophers, declaratively The proliferation of specialized \nsynchronization primi\u00ad tives is therefore no surprise. For example, java.util.con\u00ad current [13] contains \na rich collection of carefully engi\u00ad neered classes, including various kinds of locks, barriers, semaphores, \ncount-down latches, condition variables, ex\u00ad changers and futures, together with nonblocking collections. \nSeveral of these classes led to research publications [14, 15, 26, 28]. A JAVA programmer faced with \na coordination prob\u00ad lem covered by the library is therefore in great shape. Inevitably, though, programmers \nare faced with new problems not directly addressed by the primitives. The prim\u00ad itives must be composed \ninto a solution. Doing so correctly and scalably can be as dif.cult as designing a new primitive. Take \nthe classic Dining Philosophers problem [3], in which philosophers sitting around a table must coordinate \ntheir use of the chopstick sitting between each one; such competition over limited resources appears \nin many guises in real systems. The problem has been thoroughly studied, and there are solutions using \nprimitives like semaphores that perform reasonably well. There are also many natural so\u00ad lutions that \ndo not perform well or do not perform at all. Naive solutions may suffer from deadlock, if for example \neach philosopher picks up the chopstick to their left, and then .nds the one to their right taken. Correct \nsolutions may scale poorly with the number of philosophers (threads). For example, using a single global \nlock to coordinate philoso\u00ad phers is correct, but will force non-adjacent philosophers to take turns \nthrough the lock, adding unnecessarily sequential\u00ad ization. Avoiding these pitfalls takes experience \nand care. In this paper, we demonstrate that Fournet and Gonthier s join calculus [4, 5] provides the \nbasis for a declarative and scalable coordination library. By declarative, we mean that the programmer \nneeds only to write down the constraints for a coordination problem, and the library will automatically \nderive a correct solution. By scalable, we mean that the derived solutions deliver robust, competitive \nperformance both as the number of processors or cores increase, and as the complexity of the coordination \nproblem grows. Figure 1 shows a solution to Dining Philosophers using our library, which is a drop-in \nreplacement for Russo s C. JOINS library [24]. The library uses the message-passing paradigmn of the \njoin calculus. For Dining Philosophers, we use two arrays of channels (hungry and chopstick) to carry \nvalue-less messages; being empty, these messages represent unadorned events. The declarative aspect of \nthis example is the join pattern starting with j.When. The declaration says that when events are available \non the channels hungry[i], left, and right, they may be simultaneously and atomically consumed. When \nthe pattern .res, the philosopher, having obtained exclusive access to two chopsticks, eats and then \nreturns the chopsticks. In neither the join pattern nor its body is the order of the chopsticks important. \nThe remaining details are explained in \u00a72.1 Most implementations of join patterns, including Russo s, \nuse coarse-grained locks to achieve atomicity, resulting in poor scalability (as we show experimentally \nin \u00a74). Our con\u00adtribution is a new implementation of the join calculus that uses ideas from .ne-grained \nconcurrency to achieve scala\u00adbility on par with custom-built synchronization primitives: We .rst recall \nhow join patterns can be used to solve a wide range of coordination problems (\u00a72), as is well\u00adestablished \nin the literature [2, 4, 5]. The examples pro\u00advide basic implementations of some of the java.util. concurrent \nclasses mentioned above.2 In each case, the JOINS-based solution is as straightforward to write as the \none for dining philosophers.  To maximize scalability, we must allow concurrent, in\u00addependent processing \nof messages, avoid centralized contention, and spin or block intelligently. This must be done, of course, \nwhile guaranteeing atomicity and progress for join patterns.  We meet these challenges by (1) using \nlock-free [9] data structures to store messages, (2) treating messages as resources that threads can \nrace to take possession of, (3) avoiding enqueuing messages when possible ( lazy queuing and specialized \nchannel representations), and (4) allowing barging ( message stealing ). 1 By default, our implementation \nprovides only probabilistic fairness (and hence probabilistic starvation freedom); see \u00a73.6 for more \ndetail. 2 Our versions lack some features of the real library, such as timeouts and cancellation, but \nthese should be straightforward to add (\u00a73.6). We present our algorithm incrementally, both through diagrams \nand through working C. code (\u00a73). We validate our scalability claims experimentally on seven different \ncoordination problems (\u00a74). For each co\u00adordination problem we evaluate a joins-based implemen\u00adtation \nrunning in both Russo s lock-based library and our new .ne-grained library. We compare these results \nto the performance of direct, custom-built solutions. In all cases, our new library scales signi.cantly \nbetter than Russo s. We scale competitively with sometimes better than the custom-built solutions, though \nwe suffer from higher constant-time overheads in some cases.  We state and sketch proofs for the key \nsafety and liveness properties characterizing our algorithm (\u00a75).  Our goal is not to replace libraries \nlike java.util.con\u00adcurrent, but rather to complement them, by exposing some of their insights in a way \neasily and safely useable by ap\u00adplication programmers. We discuss this point further, along with related \nwork in general, in the .nal section. 2. The JOINS library Russo s Joins [24] is a concurrency library \nderived from Fournet and Gonthier s join calculus [4, 5], a process algebra similar to Milner s p-calculus \nbut conceived for ef.cient implementation. In this section, we give an overview of the library API and \nillustrate it using examples drawn from the join calculus literature [2, 4, 5]. The join calculus takes \na message-passing approach to concurrency where messages are sent over channels and channels are themselves \n.rst-class values that can be sent as messages. What makes the calculus interesting is the way messages \nare received. Programs do not actively request to receive messages from a channel. Instead, they employ \njoin patterns (also called chords [2]) to declaratively specify reactions to message arrivals. The power \nof join patterns lies in their ability to respond atomically to messages arriving simultaneously on several \ndifferent channels. Suppose, for example, that we have two channels Put and Get, used by producers and \nconsumers of data. When a pro\u00adducer and a consumer message are available, we would like to receive both \nsimultaneously, and transfer the produced value to the consumer. Using Russo s Joins API, we write: class \nBuffer<T> { public readonly Asynchronous.Channel<T> Put; public readonly Synchronous<T>.Channel Get; \npublic Buffer() { Join j = Join.Create(); // allocate a Join object j.Init(out Put); // bind its channels \nj.Init(out Get); j.When(Get).And(Put).Do // register chord (t => { return t; }); } } This simple example \nintroduces many aspects of the API. First, we are using two different kinds of channels: Put is an asynchronous \nchannel that carries messages of type T, while Get is a synchronous channel that returns T replies but \ntakes no argument. A sender never blocks on an asyn\u00adchronous channel, even if the message cannot immediately \nbe received through a join pattern. For the Buffer class, that means that a single producer may send \nmany Put messages, even if none of them are immediately consumed. Because Get is a synchronous channel, \non the other hand, senders will block until or unless a pattern involving it is enabled. Syn\u00adchronous \nchannels also return a reply to message senders; the reply is given as the return value of join patterns. \nJoin patterns are declared using When. The single join pat\u00adtern in Buffer stipulates that when one Get \nrequest and one Put message are available, they should both be consumed. After specifying the involved \nchannels through When and And, the Domethod is used to give the body of the join pattern. The body is \na piece of code to be executed whenever the pattern is matched and relevant messages consumed. It is \ngiven as a delegate (C s .rst-class functions), taking as arguments the contents of the messages. In \nBuffer, the two channels Get and Put yield only one argument, because Get messages take no argument. \nThe body of the pattern simply returns the ar\u00adgument t (from Put), which then becomes the reply to the \nGet message. Altogether, each time the pattern is matched, one Get and one Put message are consumed, \nand the argu\u00adment is transferred from Put to the sender of Get. Channels are represented as delegates, \nso that messages are sent by simply invoking a channel as a function. From a client s point of view, \nPut and Get look just like methods of Buffer. If buf is an an instance of Buffer, a producer thread can \npost a value t by calling buf.Put(t), and a consumer thread can request a value by calling buf.Get(). \nFinally, each channel must be associated with an instance of the Join class.3 Such instances are created \nusing the static factory method Join.Create, which optionally takes the maximum number of required channels. \nChannels are bound using the Init method of the Join class, which ini\u00adtializes them using an out-parameter. \nThese details are not important for the overall design, and are elided from subse\u00adquent examples. The \nfull API including the determination of types for join pattern bodies is given in Appendix A. As we have \nseen, when a single pattern mentions several channels, it forces synchronization: Asynchronous.Channel<A> \nFst; Asynchronous.Channel<B> Snd; Synchronous<Pair<A,B>>.Channel Both; // create j and init channels \n(elided) j.When(Both).And(Fst).And(Snd).Do((a,b) => new Pair<A,B>(a,b)); 3This requirement retains compatibility \nwith Russo s original JOINS li\u00adbrary; we also use it for the stack allocation optimization described \nin \u00a73.6. The pattern will consume messages Fst(a), Snd(b) and Send() atomically, when all three are available. \nOnly the .rst two messages carry arguments, so the body of the pat\u00adtern takes two arguments. Its return \nvalue, a pair, becomes the return value of the call to Both(). On the other hand, several patterns may \nmention the same channel, expressing choice: Asynchronous.Channel<A> Fst; Asynchronous.Channel<B> Snd; \nSynchronous<Sum<A,B>>.Channel Either; // create j and init channels (elided) j.When(Either).And(Fst).Do(a \n=> new Left<A,B>(a)); j.When(Either).And(Snd).Do(b => new Right<A,B>(b)); Each pattern can ful.ll a request \non Either(), by consuming a message Fst(a) or a message Snd(b), and wrapping the value in a variant of \na disjoint sum. Using what we have seen, we can build a simple (non\u00adrecursive) Lock. As in the dining \nphilosophers example, we use void-argument, void-returning channels as signals. The Release messages \nare tokens that indicate that the lock is free to be acquired; it is initially free. Clients must follow \nthe protocol of calling Acquire() followed by Release() to obtain and relinquish the lock. Protocol violations \nwill not be detected by this simple implementation. However, when clients follow the protocol, the code \nwill maintain the invariant that at most one Release() token is pending on the queues and thus at most \none client can acquire the lock. class Lock { public readonly Synchronous.Channel Acquire; public readonly \nAsynchronous.Channel Release; public Lock() { // create j and init channels (elided) j.When(Acquire).And(Release).Do(() \n=> { }); Release(); // initially free } } With a slight generalization, we can obtain a semaphore: class \nSemaphore { public readonly Synchronous.Channel Acquire; public readonly Asynchronous.Channel Release; \npublic Semaphore(int n) { // create j and init channels (elided) j.When(Acquire).And(Release).Do(() \n=> { }); for (; n > 0; n--) Release(); // initially n free } } A semaphore allows at most n clients to \nAcquire the resource and proceed; further acquisitions must wait until another client calls Release(). \nWe arrange this by priming the basic Lock implementation with n initial Release() tokens. We can also \ngeneralize Buffer to a synchronous channel that exchanges data between threads: class Exchanger<A, B> \n{ readonly Synchronous<Pair<A, B>>.Channel<A> left; readonly Synchronous<Pair<A, B>>.Channel<B> right; \npublic B Left(A a) { return left(a).Snd; } public A Right(B b) { return right(b).Fst; } public Exchanger() \n{ // create j and init channels (elided) j.When(left).And(right).Do((a,b) => new Pair<A,B>(a,b)); } \n} Dropping message values, we can also implement an n-way barrier that causes n threads to wait until \nall have arrived: class SymmetricBarrier { public readonly Synchronous.Channel Arrive; public SymmetricBarrier(int \nn) { // create j and init channels (elided) var pat = j.When(Arrive); for (int i = 1; i < n; i++) pat \n= pat.And(Arrive); pat.Do(() => { });  } } This example is unusual in that its sole join pattern mentions \na single channel n times: the pattern is nonlinear. This rep\u00adetition means that the pattern will not \nbe enabled until the requisite n threads have arrived at the barrier, and our use of a single channel \nmeans that the threads need not distinguish themselves by invoking distinct channels (hence symmet\u00adric \n). On the other hand, if the coordination problem did call for separating threads into groups (eg. gender \nis useful in a parallel genetic algorithm [26]), it is easy to do so. We can construct a barrier requiring \nn threads of one kind and m threads of another, simply by using two channels. We can also implement a \ntree-structured variant of an asymmetric barrier, which breaks a single potentially large n-way coordination \nproblem into O(n) two-way problems: class TreeBarrier { public readonly Synchronous.Channel[] Arrive; \nprivate readonly Join j; // create j, init chans ... public TreeBarrier(int n) {Wire(0, n-1, () => {});} \nprivate void Wire(int low, int high, Action Done) { if (low == high) j.When(Arrive[low]).Do(Done); else \nif (low + 1 == high) j.When(Arrive[low]).And(Arrive[high]).Do(Done); else { // low + 1 < high Synchronous.Channel \nLeft, Right; // init chans j.When(Left).And(Right).Do(Done); int mid = (low + high) / 2; Wire(low, mid, \n() => Left()); Wire(mid + 1, high, () => Right()); } } } Such tree-structured barriers (or more generally, \ncombin\u00aders) have been studied in the literature (see [9] for a survey); the point here is just that adding \ntree-structured coordina\u00adtion is straightforward using join patterns. As we show in \u00a74, the tree-structured \nvariant performs substantially better than the .at barrier, although both variants easily outperform \nthe .NET Barrier class (a standard sense-reversing barrier). Finally, we can implement a simple reader-writer \nlock [2], using private asynchronous channels (idle and shared) to track the internal state of a synchronization \nprimitive: class ReaderWriterLock { private readonly Asynchronous.Channel idle; private readonly Asynchronous.Channel<int> \nshared; public readonly Synchronous.Channel AcqR, AcqW, RelR, RelW; public ReaderWriterLock() { // create \nj and init channels (elided) j.When(AcqR).And(idle).Do(() => shared(1)); j.When(AcqR).And(shared).Do(n \n=> shared(n+1)); j.When(RelR).And(shared).Do(n => { if (n == 1) idle(); else shared(n-1); }); j.When(AcqW).And(idle).Do(() \n=> { }); j.When(RelW).Do(() => idle()); idle(); // initially free } } While we have focused on the simplest \nsynchronization primitives as a way of illustrating JOINS, join patterns can be used to declaratively \nimplement more complex concurrency patterns, from Larus and Parks-style cohort-scheduling [2], to Erlang-style \nagents or active objects [2], to stencil compu\u00adtations with systolic synchronization [25], as well as \nclassic synchronization puzzles [1]. 3. Scalable join patterns We have seen, through a range of examples, \nhow the join calculus allows programmers to solve coordination problems by merely writing down the relevant \nconstraints. Now we turn to the primary concern of this paper: an implementation that solves these constraints \nin a scalable way. The chief challenge in implementing the join calculus is providing atomicity when \n.ring patterns: messages must be noticed and withdrawn from multiple collections simulta\u00adneously. A simple \nway to ensure atomicity, of course, is to use a lock, and this is what most implementations do (\u00a76).4 \nFor example, Russo s original library associates a single lock with each Join object. Each sender must \nacquire the lock and, while holding it, enqueue their message and de\u00adtermine whether any patterns are \nthereby enabled. Russo puts signi.cant effort into shortening the critical section: he uses bitmasks \nsummarizing message availability to accel\u00aderate pattern matching [12], represents void asynchronous 4 \nSome implementations use STM [29], which we discuss in \u00a76. channels as counters, and permits message \nstealing to in\u00adcrease throughput all the tricks from Benton et al. [2]. Unfortunately, even with relatively \nshort critical sections, such coarse-grained locking inevitably limits scalability. The scalability problems \nwith locks are well-documented [19], and they are especially pronounced if we use the JOINS li\u00adbrary \nto implement low-level synchronization primitives. At a high level, the problem with coarse-grained locking \nis that it serializes the process of matching and .ring chords: at most one thread can be performing \nthat work at a time. In cases like the exchanger and Dining Philosophers, a much greater degree of concurrency \nis both possible and desir\u00adable. At a lower level, coarse-grained locking can drastically increase interprocessor \ncommunication, because all threads processing related messages are reading and writing to a sin\u00adgle shared \nlocation the lock status. Since memory band\u00adwidth tends to be quite limited, this extra traf.c inhibits \nscal\u00adability, especially for low-level coordination (\u00a74). In summary, we want an implementation of JOINS \nthat matches and .res chords concurrently while minimizing costly interprocessor communication. 3.1 Overview \nWe need to permit highly-concurrent access to the col\u00adlection of messages available on a given channel. \nTherefore, we use lock-free [9] bags to represent channels which allow truly concurrent examination and \nalteration of data. The re\u00adsult is that, for example, two threads can be simultaneously adding separate \nmessages to the same channel bag, while a third examines a message already stored in the bag without \nany of the threads waiting on any other, and in many cases without any memory bus traf.c. In choosing \na bag rather than, say, a queue, we sacri.ce message ordering guarantees to achieve greater concurrency: \nFIFO ordering imposes a se\u00adquential bottleneck on queues. The original join-calculus did not provide \nany ordering guarantees, and relaxed ordering is typical in implementations [2, 4, 24]. This choice is \nnot fun\u00addamental, and ordered channels are easily provided (\u00a73.6). None of our examples rely on ordering. \nLock-free bags allow messages to be added and inspected concurrently, but they do not solve the central \nproblem of atomically consuming a pattern s worth of messages. To provide atomic matching, we equip messages \nwith a Status .eld of the following type: enum Stat { PENDING, CLAIMED, CONSUMED }; Statuses are determined \naccording to an optimistic protocol: Each message is PENDING to begin with, meaning that it is available \nfor matching and .ring.  Matching consists of .nding suf.ciently many PENDING messages, then using CAS \n(see below) to try to change them one by one to from PENDING to CLAIMED.  If matching is successful, \neach message can be marked CONSUMED without issuing a memory fence. If it is un-  Example When(A).And(B).And(C).Do(...), \nsend on C . . Lozenges are messages (with last read status) Legend Double border: about to CAS to CLAIMED \n. Shaded interior: won CAS race; CLAIMED by us Snapshot 1 Found PENDINGmessages in each relevant bag, \nincluding our own message. About to CAS, trying to claim Snapshot 2 Failed to claim the message (CLAIMED \nby an\u00adother thread), but succeeded in claiming another message on A and one on B. About to CAS the .nal \nmessage:  Snapshot 3 Failed to claim the message (CLAIMED by an\u00adother thread). Rollback our CLAIMED \nmessages to PENDING:  Takeaway When running the protocol, the only things we know for sure are: our \nCLAIMED messages (the shaded lozenges) will remain CLAIMED, and any CONSUMEDmessages will remain CONSUMED. \nNew messages may appear any time. Figure 2. Example: failing to .re successful, each CLAIMED message \nis reverted to PENDING, again without a fence. We use compare-and-swap (CAS) to ensure correct mutual \nexclusion: CAS atomically updates a memory location when its current value matches some expected value. \nThus, the status .eld acts as a kind of lock, but one tied to individual messages rather than an entire \ninstance of Join. Further, if we fail to acquire a message, we do not immediately spin or block. Instead, \nwe can continue looking through the relevant bag of messages for another message to claim or, more generally, \nfor another join pattern to match (\u00a73.3). Figure 2 walks through an example run of the protocol.  // \nMsg implements: Chan Chan { get; }; Stat Status { get; set; }; bool TryClaim(); // CAS from PENDING to \nCLAIMED Signal Signal { get; }; Match ShouldFire { get; set; }; object Result { get; set; }; // Chan<A> \nimplements: Chord[] Chords { get; }; bool IsSync { get; }; Msg AddPending(A a); Msg FindPending(out bool \nsawClaims); // Match implements: Chord Chord { get; }; Msg[] Claims { get; }; Figure 3. The interfaces \nto our key data structures There are three reasons the above is just an overview and not the full algorithm: \n Knowing when to terminate the above protocol with the result of no pattern matched turns out to be subtle: \nbecause of the concurrent nature of the message bags, new potential matches can occur at any time. Terminat\u00ading \nthe protocol is important for returning control to an asynchronous sender, or deciding to block a synchronous \nsender. But terminating too soon can result in dropped (undetected, but enabled) matches, which can lead \nto deadlock. The full protocol, including its termination condition, is given in \u00a73.3.  Patterns involving \nsynchronous channels add further complexity: if an asynchronous message causes such a pattern to be .red, \nit must alert a synchronous waiter, which must in turn execute the pattern body. Likewise, if there are \nmultiple synchronous senders in a given pat\u00adtern, they must be coordinated so that only one executes \nthe body and communicates the results to the others. We cover these details in \u00a73.4.  Two optimizations \nof the protocol turn out to be cru\u00adcial for achieving scalability: lazy queueing and message stealing. \nThe details of these optimizations are spelled out in \u00a73.5, while their rami.cations on scalability are \nexamined empirically in \u00a74.  3.2 Representation Next we give our implementation using a simpli.ed version \nof its C code. We begin the interfaces for key data struc\u00adtures, in Figure 3. The get and set keywords \nare used to specify getters and setters for properties in .NET. The Msg class, in addition to carrying \na message payload, includes a Chan .eld tying it to a particular channel, and the Status .eld discussed \nabove. The Chan .eld is just a conve\u00adnience for the presentation in this paper. The remaining Msg .elds \n(Signal, ShouldFire and Result) are used for blocking on synchronous channels (\u00a73.4). The Chan<A> class \nimplements a lock-free bag of mes\u00adsages of type A. The key operations on a channel are: AddPending, \nwhich takes a message payload and atomi\u00adcally adds a Msg with PENDING status to the bag.  FindPending, \nwhich returns (but does not remove) some message in the bag observed to have a PENDING status; of course, \nby the time control is returned to the caller, the status may have been altered by a concurrent thread. \nIf no PENDING messages were observed at some linearization point,5 null is returned, and the out-parameter \nsawClaims re.ects whether any CLAIMED messages were observed at that linearization point.  Notice that \nChan<A> does not provide any direct means of removing messages; in this respect, it is not a traditional \nbag. Any message with status CONSUMED is considered logi\u00adcally removed from the bag, and will be physically \nremoved from the data structure when convenient. Our JOINS imple\u00admentation obeys the invariant that once \na message is marked CONSUMED, its status is never changed again. We do not de\u00adtail our bag implementation \nhere; it is similar to a lock-free queue [18], except of course that FindPending can traverse the whole \nbag. A more aggressive bag implementation could increase concurrency further. Match is a simple, immutable \nclass used to communicate data making up a matched pattern: a chord, and an array of CLAIMED messages \nsuf.cient to .re it. 3.3 The core algorithm: resolving a message We have already discussed the key safety \nproperty for a JOINS implementation: pattern matching should be atomic. In addition, an implementation \nshould ensure at least the following liveness property (assuming a fair scheduler): If a chord can .re, \neventually some chord is .red.6 Our strategy is to drive the .ring of chords by the concurrent arrival \nof each message: each sender must resolve its own message. We consider a message resolved if: 1. It is \nmarked CLAIMED by the sending thread, along with suf.ciently many other messages to .re a chord; or \n2. It is marked CONSUMED by another thread, and hence was used to .re a chord; or 3. No pattern can \nbe matched using only the message and messages that arrived prior to it.  Ensuring that each message \nis eventually resolved is tricky, because message bags and statuses are constantly, concur\u00ad 5A linearization \npoint [10] is the moment at which an operation that is observably atomic, but not actually atomic, is \nconsidered to take place. 6 Notice that this property does not guarantee fairness; see \u00a75. 1 Match Resolve(Msg \nmsg) { 2 var backoff = new Backoff(); 3 while (true) { 4 bool retry = false; 5 foreach (var chord in \nmsg.Chan.Chords) { 6 Msg[] claims = chord.TryClaim(msg, ref retry); 7 if (claims != null) 8 return new \nMatch(chord, claims); 9 } 10 if (!retry || msg.Status == Stat.CONSUMED) 11 return null; 12 backoff.Once(); \n13 } 14 } Figure 4. Resolving a message rently in .ux. In particular, just as one thread determines \nthat its message msg does not enable any chord, another message in another thread may arrive that enables \na chord involving msg. The key is that each sender need only take responsibility for the messages that \ncame before its own; if a later sender enables a chord, that later sender is responsible for it. Given \nthe highly concurrent nature of message bags, what does it mean for one message to arrive before another? \nThere is no need to provide a direct way of asking this question. Instead, we rely on the linearizability \n[10] of the bag implementation. Linearizability means that we can think of calls to AddPending and FindPending \n(along with CASes to Status) as being executed atomically, in some global sequential order. The result \nis that all messages even those added to distinct bags are implicitly ordered by the time of their insertion. \nThe bag interface does not provide a way to directly observe this ordering, but FindPending respects \nit: if one thread executes Msg m1 = bag1.AddPending(x); bool sawClaims; Msg m2 = bag2.FindPending(out \nsawClaims);  then m2 == null only if no message in bag2 prior to m1 is PENDING. This is the only guarantee \nwe need in order to safely stop looking for matches. Figure 4 gives our implementation of resolution. \nThe Resolve method takes a message msg that has already been added to the appropriate channel s bag and \nloops until the message has been resolved. We .rst attempt to claim a chord involving msg, successively \ntrying each chord in which msg s channel is involved (lines 5 9). The Chord class s TryClaim method either \nreturns an array of messages (which includes msg) that have all been CLAIMED by the current thread, or \nnull if claiming failed. In the latter case, the retry by-ref parameter is set to true if any of the \ninvolved message bags contained a message CLAIMED by another thread. Cumulatively, the retry.ag records \nwhether an externally-CLAIMED message was seen in any failing chord. We must 1 2 3 4 5 6 7 8 9 10 11 \n12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 partial class Chord { Chan[] Chans; // \nthe channels making up this chord Msg[] TryClaim(Msg msg, ref bool retry) { var msgs = new Msg[Chans.length]; \n// cached // locate enough pending messages to fire chord for (int i = 0; i < Chans.Length; i++) { if \n(Chans[i] == msg.Chan) { msgs[i] = msg; } else { bool sawClaims; msgs[i] = Chans[i].FindPending(out \nsawClaims); retry = retry || sawClaims; if (msgs[i] == null) return null; } } // try to claim the messages \nwe found for (int i = 0; i < Chans.Length; i++) { if (!msgs[i].TryClaim()) { // another thread won the \nrace; revert for (int j = 0; j < i; j++) msgs[j].Status = Stat.PENDING; retry = true; return null; } \n}  return msgs; // success: each message CLAIMED } } Figure 5. Racing to claim a chord involving msg \n track such CLAIMED messages because they are unstable, in the sense that they may be reverted to PENDING, \npossibly en\u00adabling a chord for which the sender is still responsible. The .rst way a message can be resolved \nby claim\u00ading it and enough other messages to make up a chord corresponds to the return on line 8. The \nsecond two ways correspond to the return on line 11. If none of the three conditions hold, we must try \nagain. We perform exponential backoff (line 12) in this case, because repeated retrying can only be caused \nby high contention over messages. Resolu\u00adtion may fail to terminate, but only if the system as a whole \nis making progress (according to our liveness property above); see \u00a75 for a proof sketch. Figure 5 gives \nthe code for TryClaim, which works in two phases. In the .rst phase (lines 8 17), we .rst try to locate \nsuf.ciently many PENDING messages to .re the chord. We are required to claim msg in particular. If we \nare unable to .nd enough messages, we exit (line 15) without having written to memory. Otherwise, we \nenter the second phase (lines 20 28), wherein we race to claim each message. The message\u00adlevel TryClaim \nmethod performs a CAS on the Status .eld, 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 void \nAsyncSend<A>(Chan<A> chan, A a){ Msg myMsg = chan.AddPending(a); Match m = Resolve(myMsg); if (m == null) \nreturn; // no chord to fire ConsumeAll(m.Claims); if (m.Chord.IsAsync) { // asynchronous chord: fire \nin a new thread new Thread(m.Fire).Start(); } else { // synchronous chord: wake a synchronous caller \nfor (int i = 0; i < m.Chord.Chans.Length; i++) { // pick the first synchronous caller if (m.Chord.Chans[i].IsSync) \n{ m.Claims[i].ShouldFire = m; m.Claims[i].Signal.Set(); // assumed fence return; } } } } 23 24 25 26 \n27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 R SyncSend<R, A>(Chan<A> chan, A a) { Msg myMsg \n= chan.AddPending(a); Match m = Resolve(myMsg); if (m == null) { // myMsg CONSUMED, or no match myMsg.Signal.Block(); \nm = myMsg.ShouldFire; if (m == null) return myMsg.Result; // rendezvous } else { ConsumeAll(m.Claims); \n} var r = m.Fire(); for (int i = 0; i < m.Chord.Chans.Length; i++) { if (m.Chord.Chans[i].IsSync &#38;&#38; \n// rendezvous m.Claims[i] != myMsg) { m.Claims[i].Result = r; // transfer result m.Claims[i].Signal.Set(); \n// assumed fence } } return (R)r; } Figure 6. Sending a message ensuring that only one thread will succeed \nin claiming a given message. If at any point we fail to claim a message, we roll back all of the messages \nclaimed so far (lines 23 24). The implementation ensures that the Chans arrays for each chord are ordered \nconsistently, so that in any race at least one thread entering the second phase will complete the phase \nsuccessfully (\u00a75). Note that the code in Figure 5 is a simpli.ed version of our implementation that does \nnot handle patterns with repeated channels, and does not stack-allocate or recycle message arrays. These \ndifferences are discussed in \u00a73.6. 3.4 Firing, blocking and rendezvous Message resolution does not depend \non the (a)synchrony of a channel, but the rest of the message-sending process does. The code for sending \nmessages is shown in Figure 6, with separate entry points AsyncSend and SyncSend. The actions taken while \nsending depend, in part, on the result of message resolution: Send We CLAIMED They CONSUMED No match \nAsync (AC) Spawn (10) (SC) Wake (13 20) Return (5) Return (5) Sync Fire (35) Wait for result (28) Block \n(28)  where AC and SC, respectively, stand for asynchronous chord (no synchronous channels) and synchronous \nchord (at least one synchronous channel). First we follow the path of an asynchronous message, which \nbegins by adding and resolving the message (lines 2 3). If either the message was CONSUMED by another \nthread (in which case that thread is responsible for .ring the chord) or no pattern is matchable, we \nimmediately exit (line 5). In both of these cases, we are assured that any chords involving the message \nwill be (or has been) dealt with elsewhere, and since the message is itself asynchronous, we need not \nwait for this to occur. On the other hand, if we resolved the message by claim\u00ading it and enough other \nmessages to .re a chord, we pro\u00adceed by consuming all involved messages (which does not require a memory \nfence). If the chord s pattern involved only asynchronous channels (line 10) we spawn a new thread to \nexecute the chord body asynchronously. Otherwise we must communicate with a synchronous sender, telling \nit to execute the body. A key difference from asynchronous senders is that syn\u00adchronous sending should \nnot return until a relevant chord has .red. Moreover, synchronous chord bodies should be executed by \na synchronous caller, rather than by a newly\u00adspawned thread. Since multiple synchronous callers can be \ncombined in a single chord, exactly one of them should be chosen to execute the chord, and then share \nthe result with (and wake up) all the others (we call this rendezvous ). Each synchronous message has \na Signal associated with it. Signals provide methods Block and Set, allowing syn\u00adchronous senders to \nblock7 and be woken. Calls to Set atom\u00adically trigger the signal. If a thread has already called Block, \nit is awoken and the signal is reset. Otherwise, the next call 7 It spins a bit .rst; see \u00a73.6 to Block \nwill immediately return, again resetting the signal. We ensure that Block and Set are each called by \nat most one thread; the implementation ensures that waking only occurs as a result of triggering the \nsignal (no spurious wakeups ). If an asynchronous sender consumed a synchronous sender s message (line \n6), the synchronous sender will be blocked waiting to be informed of this event (line 28). The asynchronous \nsender chooses one such synchronous sender to wake (lines 15 18), telling it which chord and messages \nto .re (line 16). Now we consider synchronous senders. Just as before, we .rst add and resolve the message \n(lines 24 25). If the mes\u00adsage was resolved by claiming enough additional messages to .re a chord, all \nrelevant messages are immediately con\u00adsumed (line 32). Otherwise, the sender must block (line 28). There \nare two ways a blocked, synchronous sender can be woken: by an asynchronous sender or by another syn\u00adchronous \nsender (rendezvous). In the former case, the (ini\u00adtially null) ShouldFire .eld will contain a match that \nthe synchronous caller is responsible for .ring. In the latter case, ShouldFire remains null, but the \nResult .eld will con\u00adtain the result of a chord body as executed by another syn\u00adchronous sender, which \nis immediately returned (line 30). If a synchronous sender does execute a chord body (line 35), it must \nthen wake up any other synchronous senders involved in the chord and inform them of the result (lines \n36 42). For simplicity, we ignore the possibility that the chord body raises an exception, but proper \nhandling is easy to add and is addressed by the benchmarked implementation. 3.5 Lazy queuing, counter \nchannels, and message stealing To achieve competitive scalability, we make three enhance\u00adments to the \nabove implementation. The importance of these enhancements is shown empirically in \u00a74. First, we do not \nalways need to allocate or enqueue a message when sending. For example, in the Lock class when sending \nan Acquire message, we could .rst check to see whether a corresponding Releasemessage is available, and \nif so, immediately claim and consume it without ever touching the Acquire bag. This shortcut saves both \non allocation and potentially on interprocessor communication. More generally, we provide an optimistic \nfast-path that attempts to immediately match the remaining messages to .re a chord. We call this lazy \nqueuing. Implementing lazy queuing is relatively straightforward, so we do not provide the code here. \nThe second enhancement applies to void, asynchronous channels (e.g. Lock.Release). Sophisticated lock-based \nim\u00adplementations of join patterns typically optimize the rep\u00adresentation of such channels to a simple \ncounter, neatly avoiding the cost of allocation for messages used just as signals [2]. We have implemented \na similar optimization, adapted to suit our protocol. The main challenge in employing the counter representa\u00adtion \nis that, in our .ne-grained protocol, it must be possible to tentatively decrement the counter (the analog \nof claim\u00ading a message), in such a way that other threads do not in\u00adcorrectly assume the message has \nactually been consumed. Our approach is to represent void, asynchronous message bags as a word sized \npair of half-words, separately count\u00ading claimed and pending messages. Implementations of, for example, \nChan.AddPending and Msg.TryClaim are specialized to atomically update the shared-state word by CASing \nin a classic read-modify-try-update loop. For example, we claim a message as follows: bool TryClaim() \n{ uint startState = chan.state; // shared state uint curState = startState; while (true) { startState \n= curState; ushort claimed; ushort pending = Decode(startState, out claimed); if (pending > 0) { var \nnextState = Encode(++claimed, --pending); curState = CAS(ref chan.state, comparand: startState, value: \nnextState); if (curState == startState) return true; } else return false; } } More importantly, Chan.FindPending \nno longer needs to tra\u00adverse a data structure but can merely atomically read the bag s encoded state \nonce, setting sawClaimed if the claimed count is non-zero. While the counter representation avoids allocation, \nit does lead to more contention over the same shared state (compared with a proper bag). It also introduces \nthe possi\u00adbility of over.ow, which we ignored here. Nevertheless, we have found it to be bene.cial in \npractice (\u00a74), especially for non-singleton resources like Semaphore.Release messages. The .nal enhancement \nis only relevant to synchronous channels. When an asynchronous sender matches a syn\u00adchronous chord, it \nconsumes all the relevant messages, and then wakes up one of the synchronous senders. If the syn\u00adchronous \ncaller is actually blocked so that waking requires a context switch signi.cant time may lapse before \nthe chord is actually .red. Since we do not provide a fairness guarantee, we can in\u00adstead permit stealing \n: we can wake up one synchronous caller, but roll back the rest of the messages to PENDING status, putting \nthem back up for grabs by currently-active threads including the thread that just sent the asynchronous \nmessage. In low-traf.c cases, messages are unlikely to be stolen; in high-traf.c cases, stealing is likely \nto lead to bet\u00adter throughput. This strategy is similar to the one taken in Figure 7. Waking a synchronous \nsender while allowing stealing; replaces lines 12 20 of AsyncSend 1 bool foundSleeper = false; 2 for \n(int i = 0; i < m.Chord.Chans.Length; i++) { 3 if (m.Chord.Chans[i].IsSync &#38;&#38; !foundSleeper) \n{ 4 foundSleeper = true; 5 m.Claims[i].AsyncWaker = myMsg; // hand over msg 6 m.Claims[i].Status = Stat.WOKEN; \n7 m.Claims[i].Signal.Set(); // assumed fence 8 } else { 9 m.Claims[i].Status = Stat.PENDING; // relinquish \n10 } 11 } Polyphonic C [2], as well as the barging allowed by the java.util.concurrent synchronizer \nframework [15]. Some care must be taken to ensure our key liveness prop\u00aderty still holds: when an asynchronous \nmessage wakes a synchronous sender, it moves from a safely resolved state (CLAIMED as part of a chord) \nto an unresolved state (PENDING). There is no guarantee that the woken synchronous sender will be able \nto .re a chord involving the original asyn\u00adchronous message (see [2] for an example). Yet AsyncSend simply \nreturns to its caller. We must somehow ensure that the original asynchronous message is successfully \nresolved. Thus, when a synchronous sender is woken, we record the asynchronous message that woke it, \ntransferring responsi\u00adbility for the message. The new code for an asynchronous sender notifying a synchronous \nwaiter is shown in Figure 7; it replaces lines 12 20 of AsyncSend. We introduce a new status, WOKEN.A \nsynchronous message is marked WOKEN if an asynchronous sender is transferring responsibility, and CONSUMED \nif a syn\u00adchronous sender is going to .re a chord involving it. In both cases, the signal is set after \nthe status is changed; in the latter case, it is set after the chord is actually .red and the return \nvalue is available. Resolve is revised to return null at any point that the message is seen at status \nWOKEN or CONSUMED. The WOKEN status ensures that a blocked synchronous caller is woken only once, which \nis important both for our use of Signal, and to ensure that the synchronous sender will only be responsible \nfor one waking asynchronous message. The message .eld AsyncWaker replaces ShouldFire, and is used to \ninform the woken thread which asynchronous mes\u00adsage woke it up. Figure 8 gives the revised code for sending \na synchronous message in the presence of stealing; it is meant to replace lines 27 33 in the original \nimplementation. A synchronous sender loops until its message is resolved by claiming a chord (exit on \nline 6), or by another thread consuming it (exit on line 13). In each iteration of the loop, the sender \nblocks (line 10); even if its message has already been CONSUMED, it must wait for the signal to get its \nreturn value. In the case 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 Msg waker = null; \nvar backoff = new Backoff(); while (true) { m = Resolve(myMsg); if (m != null) break; // claimed a chord; \nexit if (waker != null) RetryAsync(waker); // retry last async waker myMsg.Signal.Block(); if (myMsg.Status \n== Stat.CONSUMED) { return myMsg.Result; // rendezvous } else { // status is Stat.WOKEN waker = myMsg.AsyncWaker; \n// will retry later myMsg.Status = Stat.PENDING; backoff.Once();  } } ConsumeAll(claims); // retry last \nasync waker, *after* consuming myMsg if (waker != null) RetryAsync(waker); Figure 8. Sending a synchronous \nmessage while coping with stealing; replaces lines 27 23 of SyncSend that the synchronous sender is woken \nby an asynchronous message (lines 15 17), it records the waking message and ultimately tries once more \nto resolve its own message. We perform exponential backoff every time this happens, since continually \nbeing awoken only to .nd messages stolen indi\u00adcates high traf.c. After every resolution of the synchronous \nsender s mes\u00adsage (myMsg), the sender retries sending the last asynchronous message that woke it, if \nany (lines 7 8, 22). Doing so ful.lls the liveness requirements outlined above: the synchronous sender \nhas become responsible for the message that woke it. We use RetryAsync, which is similar to AsyncSend \nbut uses an already-added message rather than adding a new one. We are careful to avoid calling RetryAsync \nwhile myMsg is CLAIMED by the calling thread; doing so could result in the retry code forever waiting \nfor us to revert or consume the claim. On the other hand, it is .ne to retry the message even if it has \nalready been successfully consumed as part of a chord; RetryAsync will simply exit in this case. 3.6 \nPragmatics and extensions There are a few smaller differences between the presented code and the actual \nimplementation: We avoid boxing (allocation) and downcasts whenever possible: on .NET, additional polymorphism \n(beyond what the code showed) can help avoid uses of object.  We do not allocate a fresh message array \nevery time TryClaim is called; in fact, we do not heap-allocate mes\u00ad   sage arrays at all. Instead, \nwe stack-allocate an array8 in SyncSend and AsyncSend, and reuse this array for every call to TryClaim. \nWe handle nonlinear patterns, in which a single channel appears multiple times. The code is straightforward. \n An important pragmatic point is that the Signal class .rst performs some spinwaiting before blocking. \nSpinning is per\u00adformed on a memory location associated with the signal, so each spinning thread will \nwait on a distinct memory location whose value will only be changed when the thread should be woken, \nan implementation strategy long known to per\u00adform well on cache-coherent architectures [17]. The amount \nof spinning performed is determined adaptively on a per\u00adthread, per-channel basis. We expect it to be \nstraightforward to add timeouts and nonblocking attempts for synchronous sends to our imple\u00admentation, \nbecause we can always use CAS to consume a message we have added to a bag to abort a send (which will, \nof course, also discover if the send has already succeeded). Finally, to add channels with ordering constraints \none needs only use a queue or stack rather than a bag. Switch\u00ading from bags to fair queues and disabling \nmessage stealing yields per-channel fairness for joins. In Dining Philosophers, for example, queues will \nguarantee that requests from wait\u00ading philosophers are ful.lled before those of philosophers that have \njust eaten. Such guarantees come at the cost of sig\u00adni.cantly decreased parallelism, since they entail \nsequential matching of join patterns. At an extreme, programmers can enforce a round-robin scheme for \nmatching patterns using an additional internal channel. 4. Performance In this section we provide empirical \nsupport for the claims that our implementation is (1) scalable and (2) competitive with purpose-built \nsolutions. 4.1 Methodology To evaluate our implementation, we constructed a series of microbenchmarks \nfor seven classic coordination prob\u00adlems: dining philosophers, producers/consumers, mutual ex\u00adclusion, \nsemaphores, barriers, rendezvous, and reader-writer locking. Our solutions for these problems are fully \ndescribed in the .rst two sections of the paper. They cover a spectrum of shapes and sizes of join patterns. \nIn some cases (producer/\u00adconsumer, locks, semaphores, rendezvous) the size and num\u00adber of join patterns \nstays .xed as we increase the number of processors, while in others a single pattern grows in size (barriers) \nor there are an increasing number of .xed-size pat\u00adterns (philosophers). 8 Stack-allocated arrays are \nnot directly provided by .NET, so we use a custom value type built by polymorphic recursion. Each benchmark \nfollows standard practice for evaluating synchronization primitives: we repeatedly synchronize, for a \ntotal of k synchronizations between n threads [17]. We use k = 100,000 and average over three trials \nfor all bench\u00admarks. To test interaction with thread scheduling preemp\u00adtion, we let n range up to 96 \ntwice the 48 cores in our benchmarking machine. Each benchmark has two variants for measuring different \naspects of synchronization: PARALLEL SPEEDUP In the .rst variant, we simulate doing a small amount of \nwork between synchronization episodes (and during the critical section, when appropri\u00adate). By performing \nsome work, we can gauge to what extent a synchronization primitive inhibits or allows par\u00adallel speedup. \nBy keeping the amount of work small, we gauge in particular speedup for .ne-grained parallelism, which \npresents the most challenging case for scalable co\u00adordination. PURE SYNCHRONIZATION In the second variant, \nwe synchronize in a tight loop, yielding the cost of synchro\u00adnization in the limiting case where the \nactual work is neg\u00adligible. In addition to providing some data on constant\u00adtime overheads, this variant \nserves as a counterpoint to the previous one: it ensures that scalability problems were not hidden by \ndoing too much work between syn\u00adchronization episodes. Rather than looking for speedup, we are checking \nfor slowdown. To simulate work, we use .NET s Thread.SpinWait method, which spins in a tight loop for \na speci.ed number of times (and ensures that the spinning will not be optimized away). To make the workload \na bit more realistic and to avoid lucky schedules we randomize the number of spins be\u00adtween each synchronization, \nwhich over 100,000 iterations will yield a normal distribution of total work with very small standard \ndeviation. We ensure that the same random seeds are provided across trials and compared algorithms, so \nwe always compare the same amount of total work. The mean spin counts are determined per-benchmark and \ngiven in the next section. For each problem we compare performance between: a join-based solution using \nour fully-optimized imple\u00admentation (S-Join, for scalable joins ),  a join-based solution using Russo \ns library (L-Join, for lock-based joins ),  at least one purpose-built solution from the literature \nor .NET libraries (label varies), and  when relevant, our implementation with some or all op\u00adtimizations \nremoved to demonstrate the effect of the op\u00adtimization (e.g., S-J w/o S,C for dropping the Stealing and \nCounter optimizations; S-J w/o L for dropping the Lazy queuing optimization).  We detail the purpose-built \nsolutions below. Figure 9. Speedup on simulated .ne-grained workloads: throughput versus threads Threads \n(on 48-core machine) Figure 10. Pure synchronization performance: throughput versus threads Threads \n(on 48-core machine) Two benchmarks (rendezvous and barriers) required ex\u00adtending Russo s library to \nsupport multiple synchronous channels in a pattern; in these cases, and only in these cases, we use a \nmodi.ed version of the library. Our benchmarking machine has four AMD Magny-Cours Opteron 6100 Series \n1.7GHz processors, with 12 cores each (for a total of 48 cores), 32GB RAM, and runs Windows Server 2008 \nR2 Datacenter. All benchmarks were run under the 64-bit CLR. 4.2 The benchmarks The results for all \nbenchmarks appear in Figure 9 (for paral\u00adlel speedup) and Figure 10 (for pure synchronization). The axes \nare consistent across all graphs: the x-axis measures the number of threads, and the y-axis measures \nthroughput (as iterations performed every 10\u00b5s). Larger y values re.ect bet\u00adter performance. For measuring \nparallel speedup, we used the following mean spin counts for simulated work: Benchmark In crit. section \nOut of crit. section Philosophers 25 5,000 Prod/Cons N/A producer 5,000 consumer 500 Lock 50 200 Semaphore \n25 100 Rendezvous N/A 5,000 Barrier N/A 10,000 RWLock 50 200  With too little simulated work, there \nis no hope of speedup; with too much, the parallelism becomes coarse-grained and thus insensitive to \nthe performance of synchronization. These counts were chosen to be high enough that at least one implementation \nshowed speedup, and low enough to yield signi.cant performance differences. The particulars of the benchmarks \nare as follows, where n is the number of threads and k the total number of iterations (so each thread \nperforms k/n iterations): Philosophers Each of the n threads is a philosopher; the threads are arranged \naround a table. An iteration consists of acquiring and then releasing the appropriate chopsticks. We \ncompare against Dijkstra s original solution, using a lock per chopstick, acquiring these locks in a \n.xed order, and releasing them in the reverse order. Producer/consumer We let n/2 threads be producers \nand n/2 be consumers. Producers repeatedly generate trivial output and need not wait for consumers, while \nconsumers repeatedly take and throw away that output. We compare against the .NET 4 BlockignCollection \nclass, which trans\u00adforms a nonblocking collection into one that blocks when attempting to extract an \nelement from an empty collec\u00adtion. We wrap the BlockingCollection around the .NET 4 ConcurrentQueue class \n(a variant of Michael and Scott s classic lock-free queue [18]) and ConcurrentBag. Lock An iteration \nconsists of acquiring and then releasing a single, global lock. We compare against both the built-in \n.NET lock (which is a highly-optimized part of the CLR im\u00adplementation itself) and System.Threading.SpinLock \n(which is implemented in .NET). Semaphore We let the initial semaphore count be n/2; An iteration consists \nof acquiring and then releasing the semaphore. We compare to two .NET semaphores: the Semaphore class, \nwhich wraps kernel semaphores, and Sema\u00adphoreSlim, a faster, pure .NET implementation of semaphores. \nRendezvous The n threads perform a total of k syn\u00adchronous exchanges as quickly as possible. Unfortunately, \n.NET 4 does not provide a built-in library for rendezvous, so we ported Scherer et al. s exchanger [26] \nfrom JAVA; this is the exchanger included in java.util.concurrent. Barriers An iteration consists of \npassing through the bar\u00adrier. We show results for both the tree and the .at versions of the join-based \nbarrier. We compare against the .NET 4 Barrier class, a standard sense-reversing barrier. RWLock An iteration \nconsists of (1) choosing at random whether to be a reader or writer and (2) acquiring, and then releasing, \nthe appropriate lock. We give results for 50-50 and 75-25 splits between reader and writers. We compare \nagainst two .NET implementations: the ReaderWriterLock class, which wraps the kernel RWLocks, and the \nReaderWriterLock-Slim class, which is a pure .NET implementation. 4.3 Analysis The results of Figure \n9 demonstrate that our scalable join patterns are competitive with and can often beat state of the art \ncustom libraries. Application-programmers can solve coordination problems in the simple, declarative \nstyle we have presented here, and expect excellent scalability, even for .ne-grained parallelism. In \nevaluating benchmark performance, we are most in\u00adterested in the slope of the throughput graph, which \nmea\u00adsures scalability with the number of cores (up to 48) and then scheduler robustness (from 48 to 96). \nIn the parallel speedup benchmarks, both in terms of scalability (high slope) and ab\u00adsolute throughput, \nwe see the following breakdown: S-Join clear winner Producer/consumer, Semaphore, Barrier S-Join competitive \nPhilosophers, Lock, Rendezvous, RWLock 50/50 .NET clear winner RWLock 75/25  The .NET concurrency library \ncould bene.t from replacing some of its primitives with ones based on the joins imple\u00admentation we have \nshown the main exception being locks. With further optimization, it may be feasible to build an en\u00adtire \nscalable synchronization library around joins. The performance of our implementation is mostly robust \nas we oversubscribe the machine. The Barrier benchmark is a notable exception, but this is due to the \nstructure of the problem: every involved thread must pass through the barrier at every iteration, so \nat n> 48 threads, a context switch is required for every iteration. Context switches are very expensive \nin comparison to the small amount of work we are simulating. Not all is rosy, of course: the pure synchronization \nbench\u00admarks show that scalable join patterns suffer from constant\u00adtime overheads in some cases, especially \nfor locks. The table below approximates the overhead of pure synchronization in our implementation compared \nto the best .NET solution, by dividing the scalable join pure synchronization time by the best .NET pure \nsynchronization time: Overhead compared to best custom .NET solution n Phil Pr/Co Lock Sema Rend Barr \nRWL 6 5.2 0.7 6.5 2.9 0.7 1.5 4.2 12 5.2 0.9 7.4 4.0 1.7 0.3 3.9 24 1.9 0.9 6.6 3.0 1.1 0.2 1.8 48 1.6 \n1.2 7.4 2.3 1.0 0.2 1.4  (n threads; smaller is better) Overheads are most pronounced for benchmarks \nthat use .NET s built-in locks (Philosophers, Lock). This is not sur\u00adprising: .NET locks are mature and \nhighly engineered, and are not themselves implemented as .NET code. Notice, too, that in Figure 10 the \noverhead of the spinlock (which is im\u00adplemented within .NET) is much closer to that of scalable join \npatterns. In the philosophers benchmark, we are able to compensate for our higher constant factors by \nachieving bet\u00adter parallel speedup, even in the pure-synchronization ver\u00adsion of the benchmark. One way \nto decrease overhead, we conjecture, would be to provide compiler support for join patterns. Our library\u00adbased \nimplementation spends some of its time travers\u00ading data structures representing user-written patterns. \nIn a compiler-based implementation, these runtime traversals could be unrolled, eliminating a number \nof memory ac\u00adcesses and conditional control .ow. Removing that overhead could put scalable joins within \nstriking distance of the ab\u00adsolute performance of .NET locks. On the other hand, such an implementation \nwould probably not allow the dynamic construction of patterns that we use to implement barriers. In the \nend, constant overhead is trumped by scalabil\u00adity: for those benchmarks where the constant overhead is \nhigh, our implementation nevertheless shows strong parallel speedup when simulating work. The constant \noverheads are dwarfed by even the small amount of work we simulate. Fi\u00adnally, even for the pure synchronization \nbenchmarks, our im\u00adplementation provides competitive scalability, in some cases extracting speedup despite \nthe lack of simulated work. Effect of optimizations Each of the three optimizations discussed in \u00a73.5 \nis important for achieving competitive throughput. Stealing tends to be most helpful for those prob\u00adlems \nwhere threads compete for limited resources (Philoso\u00adphers, Locks), because it minimizes the time between \nre\u00adsource release and acquisition, favoring threads that are in the right place at the right time.9 Lazy \nqueuing improves constant factors across the board, in some cases (Produc\u00ader/Consumer, Rendezvous) also \naiding scalability. Finally, the counter representation provides a considerable boost for benchmarks, \nlike Semaphore, in which the relevant channel often has multiple pending messages. Performance of lock-based \njoins Russo s lock-based im\u00adplementation of joins is consistently often dramatically the poorest performer \nfor both parallel speedup and pure synchronization. On the one hand, this result is not surpris\u00ading: \nthe overserialization induced by coarse-grained locking is a well-known problem. On the other hand, Russo \ns im\u00adplementation is fairly sophisticated in the effort it makes to shorten critical sections. The implementation \nincludes all the optimizations proposed for POLYPHONIC C [2], including a form of stealing, a counter \nrepresentation for void-async channels (simpler than ours, since it is lock-protected), and bitmasks \napproximating the state of messages queues for fast matching. Despite this sophistication, it is clear \nthat lock\u00adbased joins do not scale. We consider STM-based join implementations in \u00a76.2. 5. Correctness \nThere are many ways to characterize correctness of concur\u00adrent algorithms. The most appropriate speci.cation \nfor our algorithm is something like the process-algebraic formula\u00adtion of the join calculus [4]. In that \nspeci.cation, multiple messages are consumed and a chord is .red in a single step. A full proof that \nour implementation satis.es this spec\u00adi.cation is too much to present here, and we have not yet carried \nout such a rigorous proof. We have, however, identi\u00ad.ed what we believe are the key lemmas for performing \nsuch a proof. We take for granted that our bag implementation is linearizable [10] and lock-free [9]; \nroughly, this means that operations on bags are observably atomic and dead/live\u00adlock free even under \nunfair scheduling. There are a pair of key properties one safety, one liveness characterizing the Resolve \nmethod: Lemma 1 (Resolution Safety). Assume that msg has been inserted into a channel. If a subsequent \ncall to Resolve(msg) returns, msg is in a resolved state; moreover, the return value correctly re.ects \nhow the message was resolved. Lemma 2 (Resolution Liveness). Assume that threads are scheduled fairly. \nIf a sender is attempting to resolve a mes\u00adsage, eventually some message is resolved by its sender. Recall \nthat there are three ways a message can be re\u00adsolved: it and a pattern s worth of messages can be marked \nCLAIMED by the calling thread; it can be marked CONSUMED by another thread; and it can be in an arbitrary \nstatus when it is 9 This is essentially the same observation Doug Lea made about barging for abstract \nsynchronizers [15]. determined that there are not enough messages prior to it to .re a chord. Safety \nfor the .rst two cases is fairly easy to show: we can assume that CAS works properly, and can see that \n Once a message is CLAIMED by a thread, the next change to its status is by that thread.  Once a message \nis CONSUMED, its status never changes.  These facts mean that interference cannot unresolve a message \nthat has been resolved in those three ways. The other fact we need to show is that the retry .ag is only \nfalse if, indeed, no pattern is matched using only the message and messages that arrived before it. Here \nwe use the linearizabil\u00adity assumptions about bags, together with the facts about the status .ags just \ngiven. Now we turn to the liveness property. Notice that a call to Resolve fails to return only if retry \nis repeatedly true. This can only happen as a result of messages being CLAIMED. We can prove, using the \nconsistent ordering of channels during the claiming process, that if any thread reaches the claiming \nprocess (lines 33 42), some thread succeeds in claiming a pattern s worth of messages. The argument goes: \nclaiming by one thread can fail only if claiming/consuming by another thread has succeeded, which means \nthat the other thread has managed to claim a message on a higher-ranked channel. Since there are only \n.nitely-many channels, some thread must have succeeded in claiming the last message it needed to match \na pattern. Using both the safety and liveness property for Resolve, we expect the following overall liveness \nproperty to hold: Conjecture 1. Assume that threads are scheduled fairly. If a chord can be .red, eventually \nsome chord is .red. The key point here is that if a chord can be .red, then in particular some message, \ntogether with its predecessors, does match a pattern, which rules out the possibility that the message \nis resolved with no pattern matchable. 6. Related work 6.1 Join calculus Fournet and Gonthier originally \nproposed the join calculus as an asynchronous process algebra designed for ef.cient implementation in \na distributed setting [4, 5]. It was posi\u00adtioned as a more practical alternative to Milner s p-calculus. \nThe calculus has been implemented many times, and in many contexts. The earliest implementations include \nFour\u00adnet et al. s JOCAML [7] and Odersky s FUNNEL [21] (the precursor to SCALA), which are both functional \nlanguages supporting declarative join patterns. JOCAML s runtime is single-threaded so the constructs \nwere promoted for concur\u00adrency control, not parallelism. Though it is possible to run several communicating \nJOCAML processes in parallel, pat\u00adtern matching will always be sequential. FUNNEL targeted the JAVA VM, \nwhich can exploit parallelism, but we could .nd no evaluation of its performance on parallel hardware. \nBenton, Cardelli and Fournet proposed an object-oriented version of join patterns for C called POLYPHONIC \nC [2]; around the same time, von Itzstein and Kearney indepen\u00addentlydescribed JOINJAVA [11],asimilarextensionof \nJAVA. The advent of generics in C 2.0 led Russo to encapsu\u00adlate join pattern constructs in the JOINS \nlibrary [24], which served as the basis for our library. There are also implemen\u00adtations for ERLANG [22], \nC++ [16], and VB [25]. All of the above implementations use coarse-grained locking to achieve the atomicity \npresent in the join calcu\u00adlus semantics. In some cases (e.g. POLYPHONIC C , Russo s library) signi.cant \neffort is made to minimize the critical section, but as we have shown (\u00a74) coarse-grained locking remains \nan impediment to scalability. By implementing joins as a library we forgo some expres\u00adsivity, not just \nstatic checking. JOCAML, for example, sup\u00adports restricted polymorphic sends: the type of a channel can \nbe generalized in those type variables that do not appear in the types of other, conjoined channels [6]. \nSince our chan\u00adnels are monomorphic C delegates, we are, unfortunately, unable to capture that polymorphism. \nNevertheless, one can still express a wide range of useful generic abstractions (e.g. Buffer<T>, Exchanger<A,B>). \nAnother difference is that our rendezvous patterns are more restrictive than JOCAML s. Our implementation \nonly allows us to return a single value to all synchronous channels, instead of returning separately \ntyped values to each synchronous channel. In effect, we strikeacompromisebetweenthepowerof JOCAML andlim\u00aditations \nof POLYPHONIC C (which allowed at most one syn\u00adchronous channel per pattern). As a consequence, our coding \nof exchange channels is clumsier than JOCAML s, requir\u00ading wrapper methods to extract the relevant half \nof the com\u00admon return value. JOCAML instead supports (the equivalent of) selective return statements, \nallowing one to write, e.g. return b to Left; return a to Right; within the same chord. The static semantics \nof selective returns are dif.cult to capture in a library, so we have avoided them. Note that forcing \nall channels to wait on a single return statement, as we do, also sacri.ces some concurrency. 6.2 STM \nWe are aware of two join-calculus implementations that do not employ a coarse-grained locking strategy, \ninstead us\u00ading HASKELL s software transactional memory (STM [8]). Singh s implementation builds directly \non the STM library, using transacted channels and atomic blocks to provide atomicity [30]; the goal is \nto provide a simple implemen\u00adtation, and no performance results are given. In unpublished work, Sulzmann \nand Lam suggest a hybrid approach, saying that an entirely STM-based implementation suffers from poor \nperformance [31]. Their hybrid approach uses a non\u00adblocking collection to store messages, and then relies \non STM for the analog to our message resolution process. In addition to basic join patterns, Sulzmann \nand Lam allow guards and propagated clauses in patterns, and to handle these features they spawn a thread \nper message; HASKELL threads are lightweight enough to make such an approach viable. The manuscript provides \nsome performance data, but only on a four core machine, and does not provide compar\u00adisons against direct \nsolutions to the problems they consider.  The simplest but perhaps most important advantage of our implementation \nover STM-based implementations is that we do not require STM, making our approach more portable. STM \nis an active area of research, and state-of-the\u00adart implementations require signi.cant effort. The other \nadvantage over STM is the speci.city of our algorithm. An STM implementation must provide a general mechanism \nfor declarative atomicity, con.ict-resolution and contention management. Since we are attacking a more \ncon\u00adstrained problem, we can employ a more specialized (and likely more ef.cient and scalable) solution. \nFor example, in our implementation one thread can be traversing a bag looking for PENDING messages, determine \nthat none are avail\u00adable, and exit message resolution all while another thread is adding a new PENDING \nmessage to the bag. Or worse: two threads might both add messages to the same bag. It is not clear how \nto achieve the same degree of concurrency with STM: depending on the implementation, such transactions \nwould probably be considered con.icting, and one aborted and retried. While such spurious retries might \nbe avoidable by making STM aware of the semantics of bags, or by care\u00adfully designing the data structures \nto play well with the STM implementation, the effort involved is likely to exceed that of the relatively \nsimple algorithm we have presented. To test our suspicions about the STM-based join imple\u00admentations, \nwe replicated the pure synchronization bench\u00admarks for producer/consumer and locks, on top of both Singh \nand Sulzmann s implementations. Figure 11 gives the results on the same 48-core benchmarking machine \nwe used for \u00a74. Note that, to avoid losing too much detail, we plot the throughput in these graphs using \na log scale. The comparison is, of course, a loose one, as we are comparing across two very different \nlanguages and runtime systems. However, it seems clear that the STM implementations suffer from both \ndrastically increased constant overheads, as well as much poorer scalability. Surprisingly, of the two \nSTM implemen\u00adtations, Singh s much simpler implementation was the better performer. Given these results, \nand the earlier results for lock-based implementations, our JOINS implementation is the only one we know \nto scale when used for .ne-grained parallelism. While STM can be used to implement joins, it is also \npos\u00adsible to see STM and joins as two disparate points in the spectrum of declarative concurrency: STM \nallows arbitrary shared-state computation to be declared atomic, while joins only permits highly-structured \natomic blocks in the form of join patterns. From this standpoint, our library is a bit like k-compare \nsingle swap [20] in attempting to provide scal\u00adable atomicity somewhere between CAS and STM. There is \na clear tradeoff: by reducing expressiveness relative to a framework like STM, our joins library admits \na relatively simple implementation with robust performance and scala\u00adbility. 6.3 Coordination in java.util.concurrent \nThe java.util.concurrent library contains a class called Ab\u00adstractQueuedSynchronizer that provides basic \nfunctionality for queue-based, blocking synchronizers [15]. Internally, it represents the state of the \nsynchronizer as a single 32-bit in\u00adteger, and requires subclasses to implement tryAcquire and tryRelease \nmethods in terms of atomic operations on that integer. It is used as the base class for at least six \nsynchroniz\u00aders in the java.util.concurrent package, thereby avoiding substantial code duplication. In \na sense, our JOINS library is a generalization of the abstract synchronizer framework: we support arbitrary \ninternal state (represented by asynchronous messages), n-way rendezvous, and the exchange of mes\u00adsages \nat the time of synchronization. Another interesting aspect of java.util.concurrent is its use of dual \ndata structures [27], in which blocking calls to a data structure (such as Pop on an empty stack) insert \na reser\u00advation in a nonblocking manner; they can then spinwait to see whether that reservation is quickly \nful.lled, and other\u00adwise block. Reservations provide an analog to the conditions used in monitors, but \napply to nonblocking data structures. JOINS provide another perspective on reservations: for us, Pop \ncan be used as a method, but it is really a message\u00adsend on a synchronous channel. Reservations, spinwaiting \nand blocking thus fall out as a natural consequence. With JOINS, it is also easy to allow such synchronous \nmessages to be involved in several different patterns, allowing the re\u00adquests to be ful.lled in multiple \nways, each of which can in\u00advolve complex synchronization; the correct protocol for han\u00addling reservations \nis then automatically provided. Our benchmarking results indicate that it may be reason\u00adable to build \nsome parts of a library like java.util.concur\u00adrent around JOINS; certainly, some of .NET s concurrency \nlibrary could be pro.tably replaced with a JOINS-based im\u00adplementation. While this is a nice result, \nour goal is not to replace such carefully-engineered libraries. Rather, we want to push forward the frontier, \ntaking the insights that made java.util.concurrent successful and putting them in the hands of application \nprogrammers, without exposing their intricacies. 6.4 Parallel CML Our algorithm draws some inspiration \nfrom Reppy, Russo and Xiao s PARALLEL CML, a combinator library for .rst\u00adclass synchronous events [23]. \nThe dif.culty in implement\u00ading CML is, in a sense, dual to that of the join calculus: dis\u00adjunctions of \nevents (rather than conjunctions of messages) must be atomically resolved. PARALLEL CML implements disjunction \n(choice) by adding a single event to the queue of each involved channel. Events have a state similar \nto our message statuses; event resolution is performed by an opti\u00admistic protocol that uses CAS to claim \nevents. The PARALLEL CML protocol is, however, much sim\u00adpler than the protocol we have presented: events \nare resolved while holding a channel lock. In particular, if an event is of\u00adfering a choice between sending \non channel A and receiving on channel B, the resolution code will .rst lock channel A while looking for \npartners, then (if no partners are found) unlock A and lock channel B. These channel locks prevent concurrent \nchanges to channel queues, allowing the imple\u00admentation to avoid subtle questions about when it is safe \nto stop running the protocol exactly the questions we address in \u00a73.3. The tradeoff for this simplicity \nis reduced scalability under high contention due to reduced concurrency, as in our experimental results \nfor lock-based joins. 7. Conclusion We have demonstrated that it is possible to place many of the insights \nbehind scalable synchronization algorithms directly into the hands of library users without requiring \nthose users to understand those insights, to use a .xed set of primitives, or to give up scalability. \nThere is still much work to be done, both in evaluation (does declarative coordination help in real applications?) \nand scope (can we expand beyond the join calculus?), but we believe the algorithms presented in this \npaper provide a promising foundation for further exploration. Acknowledgements We are indebted to both \nSam Tobin-Hochstadt and Vincent St-Amour for careful reading and many helpful discussions, Jesse Tov, \nDaniel Brown, and Stephen Chang for feedback on drafts of this paper, Stephen Toub and Microsoft s PCP \nteam for providing access to our test machine, Luc Maranget for motivating our counter optimization, \nand Samin Ishtiaq and Matthew Parkinson for general support. References [1] N. Benton. Jingle bells: \nSolving the Santa Claus problem in Polyphonic C.. Unpublished manuscript, Mar. 2003. URL http://research.microsoft.com/ \nnick/santa.pdf. [2] N. Benton, L. Cardelli, and C. Fournet. Modern concurrency abstractions for C.. ACM \nTransactions on Programming Lan\u00adguages and Systems, 26(5), Sept. 2004. [3] E. W. Dijkstra. Hierarchical \nordering of sequential processes. Acta Informatica, 1(2):115 138, 1971. [4] C. Fournet and G. Gonthier. \nThe re.exive chemical abstract machine and the join-calculus. In ACM SIGPLAN Symposium on Principles \nof Programming Languages (POPL), 1996. [5] C. Fournet and G. Gonthier. The join calculus: a language \nfor distributed mobile programming. In APPSEM Summer School, Caminha, Portugal, Sept. 2000, 2002. [6] \nC. Fournet, C. Laneve, L. Maranget, and D. R\u00e9my. Implicit typing \u00e0 la ML for the join-calculus. In International \nConfer\u00adence on Concurrency Theory (CONCUR), 1997. [7] C. Fournet, F. Le Fessant, L. Maranget, and A. \nSchmitt. Jo-Caml: a language for concurrent distributed and mobile pro\u00adgramming. In Advanced Functional \nProgramming, 4th Inter\u00adnational School, Oxford, Aug. 2002, 2003. [8] T. Harris, S. Marlow, S. Peyton-Jones, \nand M. Herlihy. Com\u00adposable memory transactions. In ACM SIGPLAN Symposium on Principles and Practice \nof Parallel Programming (PPoPP), 2005. [9] M. Herlihy and N. Shavit. The Art of Multiprocessor Pro\u00adgramming. \nMorgan Kaufmann, 2008. [10] M. P. Herlihy and J. M. Wing. Linearizability: a correct\u00adness condition \nfor concurrent objects. ACM Transactions on Programming Languages and Systems, 12(3):463 492, July 1990. \n[11] G. S. Itzstein and D. Kearney. Join Java: An alternative concurrency semantics for Java. Technical \nReport ACRC-01\u00ad001, University of South Australia, 2001. [12] F. Le Fessant and L. Maranget. Compiling \njoin-patterns. In In\u00adternational Workshop on High-Level Concurrent Languages (HLCL), Sept. 1998. [13] \nD. Lea. URL http://gee.cs.oswego.edu/dl/concurrency-interest/. [14] D. Lea. A java fork/join framework. \nIn ACM conference on Java, 2000. [15] D. Lea. The java.util.concurrent synchronizer framework. Science \nof Computer Programming, 58(3):293 309, 2005. [16] Y. Liu, 2009. URL http://channel.sourceforge.net/. \n[17] J. M. Mellor-Crummey and M. L. Scott. Algorithms for scalable synchronization on shared-memory multiprocessors. \nACM Transactions on Computer Systems (TOCS), 9(1):21 65, February 1991. [18] M. M. Michael and M. L. \nScott. Simple, fast, and practi\u00adcal non-blocking and blocking concurrent queue algorithms. In ACM Symposium \non Principles of Distributed Computing (PODC), 1996. [19] M. M. Michael and M. L. Scott. Nonblocking \nalgorithms and preemption-safe locking on multiprogrammed shared mem\u00adory multiprocessors. Journal of \nParallel and Distributed Com\u00adputing, 51(1):1 26, May 1998. [20] M. Moir, V. Luchangco, and N. Shavit. \nNon-blocking k\u00adcompare single swap. In ACM Symposium on Parallelism in Algorithms and Architectures (SPAA), \n2003. [21] M. Odersky. An overview of functional nets. In APPSEM Summer School, Caminha, Portugal, Sept. \n2000, 2002. [22] H. Plociniczak and S. Eisenbach. JErlang: Erlang with Joins. In Coordination Models \nand Languages, 2010. [23] J. Reppy, C. V. Russo, and Y. Xiao. Parallel concurrent ml. In ACM SIGPLAN \nInternational Conference on Functional Programming (ICFP), 2009. [24] C. Russo. The Joins Concurrency \nLibrary. In International Symposium on Practical Aspects of Declarative Languages (PADL), 2007. [25] \nC. Russo. Join Patterns for Visual Basic. In ACM SIGPLAN International Conference on Object-Oriented \nProgramming, Systems, Languages and Applications (OOPSLA), 2008. [26] W. Scherer, III, D. Lea, and M. \nL. Scott. A scalable elimination-based exchange channel. In Synchronization and Concurrency in Object-Oriented \nLanguages (SCOOL), 2005. [27] W. N. Scherer, III and M. L. Scott. Nonblocking concurrent objects with \ncondition synchronization. In International Sym\u00adposium on Distributed Computing (DISC), 2004. [28] W. \nN. Scherer, III, D. Lea, and M. L. Scott. Scalable syn\u00adchronous queues. In ACM SIGPLAN Symposium on Princi\u00adples \nand Practice of Parallel Programming (PPoPP), 2006. [29] N. Shavit and D. Touitou. Software transactional \nmemory. In ACM Symposium on Principles of Distributed Computing (PODC), 1995. [30] S. Singh. Higher-order \ncombinators for join patterns using STM. ACM SIGPLAN Workshop on Transactional Comput\u00ading (TRANSACT), \nJune 2006. [31] M. Sulzmann and E. S. L. Lam. Parallel join patterns with guards and propagation. Unpublished \nmanuscript, 2008. A. Library Reference A new Join instance j is allocated by calling an overload of factory \nmethod Join.Create: Join j = Join.Create(); or Join j = Join.Create(size);  The optional integer size \nis used to explicitly bound the num\u00adber of channels supported by Join instance j. An omitted size argument \ndefaults to 32; size initializes the constant, read-only property j.Size. A Join object notionally owns \na set channels, each ob\u00adtained by calling an overload of method Init, passing the location, channel(s), \nof a channel or array of channels using an out argument: j.Init(out channel); j.Init(out channels, length); \n The second form takes a length argument to initialize loca\u00adtion channels with an array of length distinct \nchannels. Channels are instances of delegate types. In all, the library provides six channel .avors: \n// void-returning asynchronous channels delegate void Asynchronous.Channel(); delegate void Asynchronous.Channel<A>(A \na); // void-returning synchronous channels delegate void Synchronous.Channel(); delegate void Synchronous.Channel<A>(A \na); // value-returning synchronous channels delegate R Synchronous<R>.Channel(); delegate R Synchronous<R>.Channel<A>(A \na); The outer class of a channel Asynchronous, Synchronous or Synchronous<R> should be read as a modi.er \nthat speci\u00ad.es its blocking behaviour and optional return type. When a synchronous channel is invoked, \nthe caller must wait until the delegate returns (void or some value). When an asynchronous channel is \ninvoked, there is no result and the caller proceeds immediately without waiting. Waiting may, but need \nnot, involve blocking. Apart from its channels, a Join object notionally owns a set of join patterns. \nEach pattern is de.ned by invoking an overload of the instance method When followed by zero or more invocations \nof instance method And followed by a .nal invocation of instance method Do. Thus a pattern de.nition \ntypically takes the form: j.When(c1).And(c2)\u00b7\u00b7\u00b7.And(cn).Do(d) Each argument c to When(c) or And(c) can \nbe a single channel or an array of channels. All synchronous channels that appear in a pattern must agree \non their return type. The argument d to Do(d) is a continuation delegate that de.nes the body of the \npattern. Although it varies with the pattern, the type of the continuation is always an instance of one \nof the following delegate types: delegate R Func<P1,...,Pm,R>(P1 p1,...,Pm pm); delegate void Action<P1,...,Pm>(P1 \np1,...,Pm pm);  The precise type of the continuation d, including its num\u00adber of arguments, is determined \nby the sequence of channels guarding it. If the .rst channel, c1, in the pattern is a syn\u00adchronous channel \nwith return type R, then the continuation s return type is R; otherwise the return type is void. The \ncontinuation receives the arguments of channel in\u00advocations as delegate parameters P1 p1,...,Pm pm, for \nm = n. The presence and types of any additional parameters P1 p1,...,Pm pm is dictated by the type of \neach channel ci: If ci is of non-generic type Channel or Channel[] then When(ci)/And(ci) adds no parameter \nto delegate d.  If ci is of generic type Channel<P >, for some type P then When(ci)/And(ci) adds one \nparameter pj of type Pj = P to delegate d.  If ci is an array of type Channel<P >[] for some type P \nthen When(ci)/And(ci) adds one parameter pj of array type Pj = P [] to delegate d.  Parameters are \nadded to d from left to right, in increasing order of i. In the current implementation, a continuation \ncan receive at most m = 16 parameters. A join pattern associates a set of channels with a body d. A body \ncan execute only once all the channels guarding it have been invoked. Invoking a channel may enable zero, \none or more patterns: If no pattern is enabled then the channel invocation is queued up. If the channel \nis asynchronous, then the ar\u00adgument is added to an internal bag. If the channel is syn\u00adchronous, then \nthe calling thread is blocked, joining a no\u00adtional bag of threads waiting on this channel.  If there \nis a single enabled join pattern, then the argu\u00adments of the invocations involved in the match are con\u00adsumed, \nany blocked thread involved in the match is awak\u00adened, and the body of the pattern is executed in that \nthread. Its result -some value or exception -is broad\u00adcast to all other waiting threads, awakening them. \nIf the pattern contains no synchronous channels, then its body runs in a new thread.  If there are \nseveral enabled patterns, then an unspeci.ed one is chosen to run.  Similarly, if there are multiple \ninvocations of a particu\u00adlar channel pending, which invocation will be consumed when there is a match \nis unspeci.ed.  The current number of channels initialized on j is avail\u00adable as read-only property \nj.Count; its value is bounded by j.Size. Any invocation of j.Init that would cause j.Count to exceed \nj.Size throws JoinException. Join patterns must be well-formed, both individually and collectively. Executing \nDo(d) to complete a join pattern will throw JoinException if d is null, the pattern repeats a chan\u00adnel \n(and the implementation requires linear patterns), a chan\u00adnel is null or foreign to this pattern s Join \ninstance, or the join pattern is empty. A channel is foreign to a Join instance j if it was not allocated \nby some call to j.Init. A pattern is empty when its set of channels is empty (this can only arise through \narray arguments). Though not discussed in the body of this paper, array patterns are useful for de.ning \ndynamically sized joins, e.g. an n-way exchanger: class NWayExchanger<T> { public Synchronous<T[]>.Channel<T>[] \nValues; public NWayExchanger(int n) { var j = Join.Create(n); j.Init(out Values, n); j.When(Values).Do(vs \n=> vs); } }   \n\t\t\t", "proc_id": "2048066", "abstract": "<p>Coordination can destroy scalability in parallel programming. A comprehensive library of scalable synchronization primitives is therefore an essential tool for exploiting parallelism. Unfortunately, such primitives do not easily combine to yield solutions to more complex problems. We demonstrate that a concurrency library based on Fournet and Gonthier's join calculus can provide declarative and scalable coordination. By declarative, we mean that the programmer needs only to write down the constraints of a coordination problem, and the library will automatically derive a correct solution. By scalable, we mean that the derived solutions deliver robust performance both as the number of processors increases, and as the complexity of the coordination problem grows. We validate our claims empirically on seven coordination problems, comparing our generic solution to specialized algorithms from the literature.</p>", "authors": [{"name": "Aaron J. Turon", "author_profile_id": "81418594363", "affiliation": "Northeastern University, Boston, MA, USA", "person_id": "P2839233", "email_address": "turon@ccs.neu.edu", "orcid_id": ""}, {"name": "Claudio V. Russo", "author_profile_id": "81100638789", "affiliation": "Microsoft, Cambridge, United Kingdom", "person_id": "P2839234", "email_address": "crusso@microsoft.com", "orcid_id": ""}], "doi_number": "10.1145/2048066.2048111", "year": "2011", "article_id": "2048111", "conference": "OOPSLA", "title": "Scalable join patterns", "url": "http://dl.acm.org/citation.cfm?id=2048111"}