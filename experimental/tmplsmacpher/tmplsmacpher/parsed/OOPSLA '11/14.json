{"article_publication_date": "10-22-2011", "fulltext": "\n Automatic Fine-Grain Locking using Shape Properties Guy Golan-Gueta Nathan Bronson Alex Aiken Tel Aviv \nUniversity Stanford University Stanford University ggolan@tau.ac.il nbronson@cs.stanford.edu aiken@cs.stanford.edu \nG. Ramalingam Mooly Sagiv Eran Yahav * Microsoft Research Tel Aviv University Technion grama@microsoft.com \nmsagiv@tau.ac.il yahave@cs.technion.ac.il Abstract We present a technique for automatically adding .ne-grain \nlocking to an abstract data type that is implemented using a dynamic forest i.e., the data structures \nmay be mutated, even to the point of violating forestness temporarily dur\u00ading the execution of a method \nof the ADT. Our automatic technique is based on Domination Locking, a novel lock\u00ading protocol. Domination \nlocking is designed speci.cally for software concurrency control, and in particular is de\u00adsigned for \nobject-oriented software with destructive pointer updates. Domination locking is a strict generalization \nof ex\u00adisting locking protocols for dynamically changing graphs. We show our technique can successfully \nadd .ne-grain locking to libraries where manually performing locking is extremely challenging. We show \nthat automatic .ne-grain locking is more ef.cient than coarse-grain locking, and ob\u00adtains similar performance \nto hand-crafted .ne-grain locking. Categories and Subject Descriptors D.1.3 [Programming Techniques]: \nConcurrent Programming; F.3.2 [Logics and Meanings of Programs]: Semantics of Programming Lan\u00adguages; \nE.1 [Data Structures]: Trees General Terms Theory, Algorithms, Languages, Perfor\u00admance Keywords Concurrency, \nLocking Protocol, Synthesis, Se\u00adrializability, Atomicity, Reduction * Deloro Fellow Permission to make \ndigital or hard copies of all or part of this work for personal or classroom use is granted without fee \nprovided that copies are not made or distributed for pro.t or commercial advantage and that copies bear \nthis notice and the full citation on the .rst page. To copy otherwise, to republish, to post on servers \nor to redistribute to lists, requires prior speci.c permission and/or a fee. OOPSLA 11, October 22 27, \n2011, Portland, Oregon, USA. Copyright c &#38;#169; 2011 ACM 978-1-4503-0940-0/11/10. . . $10.00 1. Introduction \nThe proliferation of multi-core processors and diminishing returns in single-threaded performance have \nincreased the need for scalable multi-threading. Concurrent data structures are a key ingredient of parallel \nprograms: these implement common data structures, such as search trees, but permit multiple threads to \nconcurrently perform operations on a shared data structure instance. A commonly used correct\u00adness criterion \nfor such data structures is that each operation should appear to execute atomically, which greatly simpli\u00ad.es \nreasoning about programs that use these data structures. In this paper, we consider the problem of turning \na se\u00adquential data structure into a concurrent data structure. One of the main challenges in this problem \nis to guarantee atom\u00adicity of concurrent operations in a scalable way, restricting parallelism only where \nnecessary. Furthermore, we are in\u00adterested in systematic techniques that are broadly applicable, rather \nthan techniques speci.c to a single data structure. Existing approaches to this problem are limited. \nOpti\u00admistic transactional memory [19, 22] provides limited ben\u00ade.t due to high overhead and its limited \nability to handle irreversible operations (e.g., I/O). Automatic lock inference techniques (e.g., [11, \n12, 15, 17, 24, 29]) are traditionally based on the two-phased locking protocol [16] that does not permit \nearly lock release and therefore limits parallelism. Fine-Grain Locking One way to achieve scalable multi\u00adthreading \nis to use .ne-grain locking. In .ne-grain locking, one associates, e.g., each object with its own lock, \npermitting multiple operations to simultaneously operate on different parts of a data-structure. Reasoning \nabout .ne-grain lock\u00ading is challenging and error-prone. As a result, programmers often resort to coarse-grain \nlocking, leading to limited scal\u00adability. The Problem We address the problem of automatically adding \n.ne-grain locking to a module. A module encapsu\u00adlates shared data with a set of procedures, which may \nbe invoked by concurrently executing threads. Given the code of a module, our goal is to add correct \nlocking that permits a high degree of parallelism. Speci.cally, we are interested in locking in which \neach shared object has its own lock, and locks may be released before the end of the computation.  Our \nmain insight is that we can use the restricted topology of pointer data structures to simplify reasoning \nabout .ne\u00adgrain locking and automatically infer ef.cient and correct .ne-grain locking. Domination Locking \nWe de.ne a new locking protocol called Domination Locking (DL). Domination Locking is a set of conditions \nthat guarantee atomicity and deadlock\u00adfreedom. Domination Locking is designed to handle dynam\u00adically \nmanipulated recursive data-structures by leveraging natural domination properties of dynamic data structures. \nDomination locking is a strict generalization of several re\u00adlated .ne-grain locking protocols such as \nhand-over-hand locking [4, 6], and dynamic DAG locking [4, 10]. Automatic Fine-Grain Locking We present \nan automatic technique to enforce the conditions of Domination Locking. The technique is applicable to \nmodules where the shape of the shared memory is a forest. The technique allows the shape of the heap \nto change dynamically as long as the shape is a forest between invocations of module operations. In contrast \nto existing lock inference techniques, which are based on two-phased locking, our technique is able to \nrelease locks at early points of the computation. Finally, as we demonstrate in Section 4 and Section \n5, our technique adds effective .ne-grain locking in several practical data-structures where it is extremely \nhard to manu\u00adally produce similar locking. Our examples include balanced search-trees [3, 18], a self-adjusting \nheap [36] and special\u00adized data-structure implementations [5, 31]. 1.1 Motivating Example Consider a \nmodule that implements the Treap data struc\u00adture [3]. A Treap is a search tree that is simultaneously \na binary search tree (on the key .eld) and a heap (on the priority .eld). If priorities are assigned \nrandomly the re\u00adsulting structure is equivalent to a random binary search tree, providing good asymptotic \nbounds for all operations. The Treap implementation consists of three procedures: insert, remove and \nlookup. Manually adding .ne-grain locking to the Treap s code, is challenging since it requires considering \nmany subtle details in the context of concurrency. For example, consider the Treap s remove operation \nshown in Fig. 1. To achieve concurrent execution of its operations, we must release the lock on the root, \nwhile an operation is still in progress, once it is safe to do so. Either of the loops (starting at Lines \n4 or 12) can move the cur\u00adrent context to a subtree, after which the root (and, similarly, other nodes) \nshould be unlocked. Several parts of this proce\u00addure implement tree rotations that change the order between \nthe Treap s nodes, complicating any correctness reasoning that depends on the order between nodes. Fig. \n2 shows an 1 boolean remove(Node par, int key) { 2 Node n = null; 3 n = par.right; // right child has \nroot 4 while (n != null &#38;&#38; key != n.key) { 5 par = n; 6 n = (key < n.key) ? n.left : n.right; \n7 } 8 if (n == null) 9 return false; // search failed, no change 10 Node nL = n.left; 11 Node nR = n.right; \n12 while (true) { // n is the node to be removed 13 Node bestChild = (nL == null || 14 (nR != null &#38;&#38; \nnR.prio > nL.prio)) ? nR : nL; 15 if (n == par.left) 16 par.left = bestChild; 17 else 18 par.right = \nbestChild; 19 if (bestChild == null) 20 break; //nwas aleaf 21 if (bestChild == nL) { 22 n.left = nL.right; \n// rotate nL to n s spot 23 nL.right = n; 24 nL = n.left; 25 } else { 26 n.right = nR.left; // rotate \nnR to n s spot 27 nR.left = n; 28 nR = n.right; 29 } 30 par = bestChild; 31 } 32 return true; 33 } Figure \n1. Removing an element from a treap by locating it and then rotating it into a leaf position. example \nof manual .ne-grain locking of the Treap remove operation. Manually adding .ne-grain locking to the code \ntook an expert several hours and was an extremely error\u00adprone process. In several cases, the expert locking \nreleased a lock too early, resulting in an incorrect concurrent algorithm (e.g., the release operation \nin Line 28). Our technique is able to automatically produce .ne-grain concurrency in the Treap s code, \nby relying on its tree shape. This is in contrast to existing alternatives, such as manually enforcing \nhand-over-hand locking, that require deep under\u00adstanding of code details.  1.2 Overview of Our Approach \nIn this section, we present an informal brief description of our approach. Domination Locking We de.ne \na new locking protocol, called Domination Locking (DL). DL is a set of conditions that are designed to \nguarantee atomicity and deadlock free\u00addom for operations of a well-encapsulated module. DL differentiates \nbetween a module s exposed and hid\u00adden objects: exposed objects (e.g., the Treap s root) act as the intermediary \nbetween the module and its clients, with pointers to such objects being passed back and forth be\u00adtween \nthe module and its clients, while the clients are com\u00adpletely unaware of hidden objects (e.g., the Treap \ns interme\u00addiate nodes). The protocol exploits the fact that all operations must begin with one or more \nexposed objects and traverse the heap-graph to reach hidden objects. In essence, exposed  1 boolean \nremove(Node par, int key) { 2 Node n = null; 3 acquire(par); 4 n = par.right; 5 if(n != null) acquire(n); \n6 while (n != null &#38;&#38; key != n.key) { 7 release(par); 8 par = n; 9 n = (key < n.key) ? n.left \n: n.right; 10 if(n != null) acquire(n); 11 } 12 if (n == null){ release(par); return false; } 13 Node \nnL = n.left; if(nL != null) acquire(nL); 14 Node nR = n.right; if(nR != null) acquire(nR); 15 while (true) \n{ 16 Node bestChild = (nL == null || 17 (nR != null &#38;&#38; nR.prio > nL.prio)) ? nR : nL; 18 if (n \n== par.left) 19 par.left = bestChild; 20 else 21 par.right = bestChild; 22 release(par); 23 if (bestChild \n== null) 24 break; 25 if (bestChild == nL) { 26 n.left = nL.right; 27 nL.right = n; 28 // release(nL); \n// an erroneous release statment 29 nL = n.left; 30 if(nL != null) acquire(nL); 31 } else { 32 n.right \n= nR.left; 33 nR.left = n; 34 nR = n.right; 35 if(nR != null) acquire(nR); 36 } 37 par = bestChild; 38 \n} 39 return true; 40 } Figure 2. Treap s remove code with manual .ne-grain locking. objects own hidden \nobjects. This implements an ownership scheme which permits ownership transfer. The protocol requires \nthe exposed objects passed as pa\u00adrameters to an operation to be locked in a fashion similar to two-phase-locking. \nHowever, hidden objects are handled differently. A thread is allowed to acquire a lock on a hidden object \nif the locks it holds dominate the hidden object. (A set S of objects is said to dominate an object u \nif all paths (in the heap-graph) from an exposed object to u contains some object in S.) In particular, \nhidden objects can be locked even after other locks have been released, thus enabling early re\u00adlease \nof other locked objects (hidden as well as exposed). This simple protocol generalizes several .ne-grain \nlock\u00ading protocols de.ned for dynamically changing graphs [4, 6, 10] and is applicable in more cases \n(i.e., the conditions of DL are weaker). We use the DL s conditions as the basis for our automatic technique. \nAutomatic Locking of Forest-Based Modules Our tech\u00adnique is able to automatically enforce DL, in a way \nthat re\u00adleases locks at early points of the computation. Speci.cally, the technique is applicable for \nmodules whose heap-graphs form a forest at the end of any complete sequential execution (of any sequence \nof operations). Note that existing shape analyses, for sequential pro\u00adgrams, can be used to automatically \nverify if a module satis\u00ad.es this precondition (e.g., [34, 40] ). In particular, we avoid the need for \nexplicitly reasoning on concurrent executions. For example, the Treap is a tree at the end of any of \nits operations, when executed sequentially. Note that, during some of its operation (insert and remove) \nits tree shape is violated by a node with multiple predecessors (caused by the tree rotations). Our technique \nuses the following locking scheme: a pro\u00adcedure invocation maintains a lock on the set of objects di\u00adrectly \npointed to by its local variables (called the immediate scope). When an object goes out of the immediate \nscope of the invocation (i.e., when the last variable pointing to that object is assigned some other \nvalue), the object is unlocked if it has (at most) one predecessor in the heap graph (i.e., if it does \nnot violate the forest shape). If a locked object has mul\u00adtiple predecessors when it goes out of the \nimmediate scope of the invocation, then it is unlocked eventually when the ob\u00adject has at most one predecessor. \nThe forest-condition guar\u00adantees that every lock is eventually released. To realize this scheme, we use \na pair of reference counts to track incoming references from the heap and local vari\u00adables of the current \nprocedure. All the updates to the ref\u00aderence count can be done easily by instrumenting every as\u00adsignment \nstatement, allowing a relatively simple compile\u00adtime transformation. While we defer the details of the \ntrans\u00adformation to Section 4, Fig. 3 shows the transformed im\u00adplementation of remove (from Fig. 1). ASNL \nand ASNF are macros that perform assignment to a local variable and a .eld, respectively, update reference \ncounts, and condition\u00adally acquire or release locks according to the above locking scheme.  1.3 Contributions \nThe contributions of this paper can be summarized as fol\u00adlows: We introduce a new locking protocol entitled \nDomination Locking. We show that domination locking can be en\u00adforced and veri.ed by considering only \nsequential execu\u00adtions [4]: if domination locking is satis.ed by all sequen\u00adtial executions, then atomicity \nand deadlock freedom are guaranteed in all executions, including non-sequential ones.  We present an \nautomatic technique to generate .ne-grain locking by enforcing the domination locking protocol for modules \nwhere the heap graph is guaranteed to be a forest in between operations. Our technique can handle any \ntemporary violation of the forest shape constraint, including temporary cycles.  We present an initial \nperformance evaluation of our tech\u00adnique on several examples, including balanced search\u00adtrees [3, 18], \na self-adjusting heap [36] and specialized   1 boolean remove(Node par, int key) { 2 Node n = null; \n3 Take(par); 4 ASNL(n, par.right); 5 while (n != null &#38;&#38; key != n.key) { 6 ASNL(par, n); 7 ASNL(n, \n(key < n.key) ? n.left : n.right); 8 } 9 if (n == null) { 10 ASNL(par, null); 11 ASNL(n, null); 12 return \nfalse; 13 } 14 Node nL = null; ASNL(nL, n.left); 15 Node nR = null; ASNL(nR, n.right); 16 while (true) \n{ 17 Node bestCh = null; ASNL(bestCh, (nL == null || 18 (nR != null &#38;&#38; nR.prio > nL.prio)) ? \nnR : nL); 19 if (n == par.left) 20 ASNF(par.left, bestCh); 21 else 22 ASNF(par.right, bestCh); 23 if \n(bestCh == null) { 24 ASNL(bestCh, null); 25 break; 26 } 27 if (bestCh == nL) { 28 ASNF(n.left, nL.right); \n29 ASNF(nL.right, n); 30 ASNL(nL, n.left); 31 } else { 32 ASNF(n.right, nR.left); 33 ASNF(nR.left, n); \n34 ASNL(nR, n.right); 35 } 36 ASNL(par, bestCh); 37 ASNL(bestCh, null); 38 } 39 ASNL(par, null); ASNL(n, \nnull); ASNL(nL, null); 40 ASNL(nR, null); 41 return true; 42 } Figure 3. Augmenting remove with macros \nto dynamically enforce domination locking. data-structure implementations [5, 31]. The evaluation shows \nthat our automatic locking provides good scalabil\u00adity and performance comparable to hand crafted locking \n(for the examples where hand crafted locking solutions were available). We discuss extensions and additional \napplications of our suggestions. 2. Preliminaries Our goal is to augment a module with concurrency control \nthat guarantees strict con.ict-serializability [33]. In this sec\u00adtion we formally de.ne what a module \nis and the notion of strict con.ict-serializability for modules. Syntax and Informal Semantics A module \nde.nes a set of types and a set of procedures that may be invoked by clients of the module, potentially \nconcurrently. A type con\u00adsists of a set of .elds of type boolean, integer, or pointer to a user-de.ned \ntype. The types are private to the module: an object of a type T de.ned by a module M can be al\u00adlocated \nor dereferenced only by procedures of module M. However, pointers to objects of type T can be passed \nback stms = skip | x = e(y1,...,yk) | assume(b) |x = new R() |x = y.f | x.f = y | acquire(x) | release(x) \n| return(x) Figure 4. Primitive instructions, b stands for a local boolean variable, e(y1,...,yk) stands \nfor an expression over local vari\u00adables. and forth between the clients of module M and the proce\u00addures \nof module M. Dually, types de.ned by clients are pri\u00advate to the client. Pointers to client-de.ned types \nmay be passed back and forth between the clients and the module, but the module cannot dereference such \npointers (or allocate objects of such type). Procedures have parameters and local variables, which are \nprivate to the invocation of the proce\u00addures. (Thus, these are thread-local variables.) There are no \nstatic or global variables shared by different invocations of procedures. (However, our results can be \ngeneralized to sup\u00adport them.) We assume that body of a procedure is represented by a control-.ow graph. \nWe refer to the vertices of a control\u00ad.ow graph as program points. The edges of a control-.ow graph are \nannotated with primitive instructions, shown in Fig. 4. Conditionals are encoded by annotating control-.ow \nedges with assume statements. Without loss of generality, we assume that a heap object can be dereferenced \nonly in aload ( x = y.f ) or store ( x.f = y ) instruction. Operations to acquire or release a lock refer \nto a thread\u00adlocal variable (that points to the heap object to be locked or unlocked). The other primitive \ninstructions reference only thread-local variables. We present a semantics for a module independent of \nany speci.c client. We de.ne a notion of execution that covers all possible executions of the module \nthat can arise with any possible client, but restricting attention to the part of the program state owned \nby the module. (In effect, our semantics models what is usually referred to as a most\u00adgeneral-client \nof the module.) For simplicity, we assume that each procedure invocation is executed by a different thread, \nwhich allows us to identify procedure invocations using a thread-id. We refer to each invocation of a \nprocedure as a transaction. We model a procedure invocation as a creation of a new thread with an appropriate \nthread-local state. We describe the behavior of a module by the relation -.. A transition s -. s' represents \nthe fact that a state s can be transformed into a state s' by executing a single instruction. Transactions \nshare a heap consisting of an (unbounded) set of heap objects. Any object allocated during the execu\u00adtion \nof a module procedure is said to be a module (owned) object. In fact, our semantics models only module \nowned ob\u00adjects. Any module object that is returned by a module pro\u00adcedure is said to be an exposed object. \nOther module objects are hidden objects. Note that an exposed object remains ex\u00adposed forever. A key \nidea encoded in the semantics is that at any point during execution a new procedure invocation may occur. \nThe only assumption made is that any module object passed as a procedure argument is exposed; i.e., the \nobject was returned by some earlier procedure invocation.  Each heap allocated object serves as a lock \nfor itself. Locks are exclusive (i.e., a lock can be held by at most one transaction at a time). The \nexecution of a transaction trying to acquire a lock (by an acquire statement) which is held by another \ntransaction is blocked until a time when the lock is available (i.e., is not held by any transaction). \nLocks are reentrant; an acquire statement has no impact when it refers to a lock that is already held \nby the current transaction. A transaction cannot release a lock that it does not hold. Whenever a new \nobject is allocated, its boolean .elds are initialized to false, its integer .elds are initialized to \n0, and pointer .elds are initialized to null. Local variables are initialized in the same manner. A formal \nsemantics for the language appears in Ap\u00adpendix A. Running Transactions Each control-.ow graph of a pro\u00adcedure \nhas two distinguished control points: an entry site from which the transaction starts, and an exit site \nin which the transaction ends (if a CFG edge is annotated with a return statement, then this edge points \nto the exit site of the procedure). We say that a transaction t is running in a state s, if t is not \nin its entry site or exit site. An idle state, is a state in which no transaction is running. Executions \nThe initial state sI has an empty heap and no transactions. A sequence of states p = s0,...,sk is an \nexecution if the following hold: (i) s0 is the initial state, (ii) for 0 = i<k, si -. si+1. An execution \np = s0,...,sk is a complete execution, if sk is idle. An execution p = s0,...,sk is a sequential execution, \nif for each 0 = i = k at most one transaction in si is running. An execution is non-interleaved if transitions \nof differ\u00adent transactions are not interleaved (i.e., for every pair of transactions ti tj either all \nthe transitions executed by ti = come before any transition executed by tj , or vice versa). Note that, \na sequential execution is a special case of a non\u00adinterleaved execution. In a sequential execution a \nnew trans\u00adaction starts executing only after all previous transactions have completed execution. In a \nnon-interleaved execution, a new transaction can start executing before a previous trans\u00adaction completes \nexecution, but the execution is not permit\u00adted to include transitions by the previous transaction once \nthe new transaction starts executing. We say that a sequential execution is completeable if itis a pre.x \nof a complete sequential execution. Schedules The schedule of an execution p = s0,...,sk is a sequence \n(t0,e0),..., (tk-1,ek-1) such that for every 0 = i<k, si can be transformed into si via transaction ti \nexecuting the instruction annotating control-.ow edge ei. Graph-Representation The heap (shared memory) \nof a state identi.es a edge-labelled multidigraph (a directed graph in which multiple edges are allowed \nbetween the same pair of vertices), which we call the heap graph. Each heap\u00adallocated object is represented \nby a vertex in the graph. A pointer .eld f in an object u that points to an object v is represented by \nan edge (u, v) labelled f. (Note that the heap graph represents only objects owned by the module. Objects \nowned by the client are not represented in the heap graph.) . Strict Con.ict-Serializability Given an \nexecution, we say that two transitions con.ict if: (i) they are executed by two different transactions, \n(ii) they access some common object (i.e., read or write .elds of the same object). Executions p and \np ' are said to be con.ict-equivalent if they consist of the same set of transactions, and the schedule \nof every transaction t is the same in both executions, the ex\u00adecutions agree on the order between con.icting \ntransitions (i.e., the ith transition of a transaction t precedes and con\u00ad.icts with the jth transition \nof a transaction t ' in p, iff the former precedes and con.icts with the latter in p '). Con.ict\u00adequivalent \nexecutions produce the same state [38]. An exe\u00adcution is con.ict-serializable if it is con.ict-equivalent \nwith a non-interleaved execution. We say that an execution p is strict con.ict-serializable if it is \ncon.ict-equivalent to a non-interleaved execution p ' where a transaction t1 completes execution before \na transac\u00adtion t2 in p ' if t1 completes execution before a transaction t2 in p. Assume that all sequential \nexecutions of a module sat\u00adisfy a given speci.cation F. In this case, a strict con.ict\u00adserializable execution \nis also linearizable [23] with respect to speci.cation F. Thus, correctness in sequential executions \ncombined with strict con.ict-serializability is suf.cient to ensure linearizability. 3. Domination Locking \nIn this section we present the Domination Locking Protocol (abbreviated DL). We show that if every sequential \nexecution of a module satis.es DL and is completable, then every con\u00adcurrent execution of the module \nis strict con.ict-serializable and completeable (i.e., atomicity and deadlock-freedom are guaranteed). \nThe locking protocol is parameterized by a total order = on all heap objects, which remains .xed over \nthe whole execution. DEFINITION 3.1. Let = be a total order of heap objects. We say that an execution \nsatis.es the Domination Locking protocol, with respect to =, if it satis.es the following con\u00additions: \n 1. A transaction t can access a .eld of an object u, only if u is currently locked by t. 2. A transaction \nt can acquire an exposed object u, only if t has never acquired an exposed object v such that u = v. \n 3. A transaction t can acquire an exposed object, only if t has never released a lock. 4. A transaction \nt can acquire a hidden object u, only if every path between an exposed object to u includes an object \nwhich is locked by t.  Intuitively, the protocol works as follows. Requirement (1) prevents race conditions \nwhere two transactions try to update an object neither has locked. Conditions (2) and (3) deal with exposed \nobjects. Very little can be assumed about an object that has been exposed; references to it may reside \nanywhere and be used at any time by other transactions that know nothing about the invariants t is maintaining. \nThus, as is standard, requirements (2) and (3) ensure all transac\u00adtions acquire locks on exposed objects \nin a consistent order, preventing deadlocks. The situation with hidden objects is different, and we know \nmore: other threads can only gain access to t s hidden objects through some chain of refer\u00adences starting \nat an exposed object, and so it suf.ces for t to guard each such potential access path with a lock. An\u00adother \nway of understanding the protocol is that previous pro\u00adposals (e.g., [10, 25, 26, 35]) treat all objects \nas exposed, whereas domination locking also takes advantage of the in\u00adformation hiding of abstract data \ntypes to impose a different, and weaker, requirement on encapsulated data. In particu\u00adlar, no explicit \norder is imposed on the acquisition or release of locks on hidden objects, provided condition (4) is \nmain\u00adtained. THEOREM 3.1. Let = be a total order of heap objects. If every sequential execution of the \nmodule is completeable and satis.es Domination Locking with respect to =, then every execution of the \nmodule is strict con.ict-serializable, and is a pre.x of a complete-execution. This theorem implies \nthat a concurrent execution can\u00adnot deadlock, since it is guaranteed to be the pre.x of a complete-execution. \nThe proof can be found in Appendix B. Domination Locking generalizes previously proposed protocols such \nas Dynamic Tree Locking (DTL) protocol and Dynamic Dag Locking (DDL) protocol [4], which them\u00adselves \nsubsume idioms such as hand-over-hand locking. The DTL and DDL protocols were inspired by database protocols \nfor trees and DAGs ([10, 25, 26, 35]), but customized for use in programs where shape invariants may \nbe temporarily violated. In particular, any execution that satis.es DTL or DDL can be shown to satisfy \nDL. In comparing these protocols, it should be noted that DTL and DDL were described in a re\u00adstricted \nsetting where the exposed objects took the form of a statically .xed set of global variables. DL generalizes \nthis by permitting a dynamic set of exposed objects (which can grow over time). More importantly, DL \nis a strict generaliza\u00adtion of DTL and DDL: executions that satisfy DL might not satisfy either DTL or \nDDL. Among other things, DL does not require the heap graph to satisfy any shape invariants. Thus, the \nabove theorem generalizes a similar theorem established for DDL and DTL in [4]. The above theorem, like \nthose in [4], is important because it permits the use of sequential rea\u00adsoning, e.g., to verify if a \nmodule guarantees strict con.ict\u00adserializability via DL. More interestingly, this reduction the\u00adorem \nalso simpli.es the job of automatically guaranteeing strict con.ict-serializability via DL, as we illustrate \nin this paper. One interesting aspect of the DL protocol is the follow\u00ading. Even if every sequential \nexecution of a module is com\u00adpletable and satis.es DL, a concurrent execution of the mod\u00adule might not \nsatisfy DL! This is in contrast to protocols such as DTL and DDL. This fact complicates the proof of \nthe above theorem. The requirement for a total order of exposed objects, does not restrict its applicability \nsince in any conventional pro\u00adgramming environment such order can be obtained (e.g., by using memory \naddress of objects, or by using a simple mechanism that assigns unique identi.ers to objects). Fur\u00adthermore, \nno order is needed when each transaction accesses a single exposed object. 4. Enforcing DL in Forest-Based \nModules In this section, we describe our technique for automatically adding .ne-grain locking to a module \nwhen the module op\u00aderates on heaps of restricted shape. Speci.cally, the tech\u00adnique is applicable to \nmodules that manipulate data struc\u00adtures with a forest shape, even with intra-transaction viola\u00adtions \nof forestness. For example, the Treap of Section 1 has a tree shape which is temporarily violated by \ntree-rotations (during tree-rotations a node may have two parents). Our technique has no limit on the \nnumber of violations or their effect on the data structures shape, as long as they are elimi\u00adnated before \nthe end of the transaction. In Section 4.1, we describe the shape restrictions required by our technique, \nand present dynamic conditions that are enforced by our source transformation. We refer to these conditions \nas the Eager Forest-Locking protocol (EFL), and show that it ensures domination locking. In Section 4.2, \nwe show how to automatically enforce EFL by a source-to-source transformation of the original module \ncode. 4.1 Eager Forest-Locking When the shape of the heap manipulated by the module is known to be a \nforest (possibly with temporary violations), we can enforce domination locking by dynamically enforc\u00ading \nthe conditions outlined below. First, we de.ne what it means for a module to be forest\u00adbased.  Forestness \nCondition We say that a hidden object u is consistent in a state s, if u has at most one incoming edge \nin s.1 We say that an exposed object u is consistent in a state s, if it does not have any incoming edges \nin s. DEFINITION 4.1. A module M is a forest-based module, if in every sequential execution, all objects \nin idle states are consistent. For a forest-based module, we de.ne the following Eager Forest-Locking \nconditions, and show that they guarantee that the module satis.es the domination locking conditions. \nEager Forest-Locking Requirements Given a transaction t, we de.ne t s immediate scope as the set of objects \nwhich are directly pointed to by local variables of t. Intuitively, ea\u00adger forest-locking is a simple \nprotocol: a transaction should acquire a lock on an object whenever it enters its immediate scope and \nit should release a lock on an object whenever the object is out of its immediate scope and is consistent. \nThe protocol description below is a bit complicated because the abovementioned invariant will be temporarily \nviolated while an object is being locked or unlocked. (In particular, condi\u00adtions 1, 2, and 4 restrict \nthe extent to which the invariant can be violated.) DEFINITION 4.2. Let = be a total order of heap objects. \nWe say that an execution satis.es the Eager Forest-Locking (EFL) with respect to =, if it satis.es the \nfollowing condi\u00adtions: 1. A transaction t can access a .eld of an object, only if all objects in t s \nimmediate scope are locked by t. 2. A transaction t can release an object, only if all objects in t \ns immediate scope are locked by t. 3. A transaction t can release a lock of an object u, only if u is \nconsistent. 4. Immediately after a transaction t releases a lock of an object u, t removes u from its \nimmediate scope (i.e., the next instruction of t removes u from immediate scope) 5. A transaction t \ncan acquire an exposed object u, only if t has never acquired an exposed object v such that u = v.  \nIn contrast to the DL conditions, the EFL conditions can directly be enforced by instrumenting the code \nof a given module because all its dynamic conditions can be seen as conditions on its immediate scope \nand local memory. Such code instrumentation is allowed to only consider sequential executions, as stated \nby the following theorem and conclu\u00adsion: THEOREM 4.1. Let = be a total order of heap objects. Let p \nbe a sequential execution of a forest-based module. If p satis.es EFL with respect to =, then p satis.es \nDL with respect to =. 1 In the graph representation of the heap. Recall that the heap-graph contains \nonly module owned objects. In particular, this de.nition does not consider pointers to exposed objects \nthat may be stored in client objects From Theorem 3.1 and Theorem 4.1 we conclude the following. CONCLUSION \n4.1. Let = be a total order of heap objects. If every sequential execution of a forest-based module is \ncompleteable and satis.es EFL with respect to =, then every execution of this module is strict con.ict-serializable, \nand is a pre.x of a complete-execution.  4.2 Enforcing EFL In this section, we present a source-to-source \ntransformation that enforces EFL in a forest-based module. The idea is to in\u00adstrument the module such \nthat it counts stack and heap refer\u00adences to objects, and use these reference counts to determine when \nto acquire and release locks. Since the EFL conditions are de.ned over sequential executions, reasoning \nabout the required instrumentation is fairly simple. Run-Time Information The instrumented module tracks \nobjects in the immediate scope of the current transaction2 by using stack-reference counters; the stack-reference \ncounter of an object u, tracks the number of references from local variables to u; hence u is in the \nimmediate scope of current transaction whenever its stack-reference counter is greater than 0. To determine \nconsistency of objects, it uses a heap\u00adreference counter; the heap-reference counter of an object u, \ntracks the number of references in heap objects that point to u; a hidden object is consistent, whenever \nits heap-counter equals to 0 or 1; and an exposed object is consistent, when\u00adever its heap-counter equals \nto 0. To determine whether an object has been exposed, it uses a boolean .eld; whenever an object is \nexposed (returned) by the module, this .eld is set to true (in that object). Locking Strategy The instrumented \ncode uses a strategy that follows EFL conditions. At the beginning of the proce\u00addure, the instrumented \nmodule acquires all objects that are pointed to by parameters (and are thus exposed objects). The order \nin which these objects are locked is determined by using a special function, unique that returns a unique \nidenti.er for each object3. After locking all exposed objects, the instrumented module acts as follows: \n(i) it acquires ob\u00adject u whenever its stack-reference-counter becomes 1; (ii) it releases object u whenever \nu is consistent, and its stack\u00adreference-counter becomes 0. This strategy releases all locks before completion \nof a transaction (since every object becomes consistent before that point), so it cannot create incompleteable \nsequential executions. Source-to-Source Transformation Our transformation in\u00adstruments each object with \nthree additional .elds: stackRef 2 Note that we consider sequential executions, so we can assume a single \ncurrent transaction. 3 Note that only exposed objects are pointed by the procedure parameters. And according \nto De.nition 4.1 these are the only exposed objects the transaction will see.  Operation Code Take(x) \nif(x!=null) { acquire(x); x.stackRef++; } Drop(x) if(x!=null) { x.stackRef-\u00ad; if(x.stackRef==0 &#38;&#38; \nIsConsistent(x)) release(x); } IsConsistent(x) if(x.isExposed) return (x.heapRef == 0); else return (x.heapRef \n<= 1); MarkExposed(x) if(x!=null) x.isExposed=true; Table 1. Primitive operations used in the EFL transformation. \n1 TakeArgs2(x,y) { 1 void AddValues(Node x, Node y) { 2 if(unique(x) < unique(y)) 2 while(x!=null &#38;&#38; \ny!=null) { 3 { Take(x); Take(y); } 3 x.value+=y.value; 4 else 4 x=x.next; 5 { Take(y); Take(x); } 5 y=y.next; \n6 } 6 }} Figure 5. Acquiring two procedure arguments in a unique locking order. ASNL(x,ptrExp) { temp=ptrExp; \nTake(temp); x = ptrExp Drop(x); x=temp; } ASNF(x.f,ptrExp) { temp=x.f; Take(temp); if(temp!=null) temp.heapRef--; \nDrop(temp); x.f = ptrExp temp=ptrExp; Take(temp); if(temp!=null) temp.heapRef++; Drop(temp); x.f = temp; \n} Table 2. The macros ASNL and ASNF for pointer assign\u00adments enforcing EFL. and heapRef to maintain the \nstack and heap reference counts (respectively), and isExposed to indicate whether the object has been \nexposed. The transformation is based on the primitive operations of Table 1. The procedures Take and \nDrop maintain stack reference counters and perform the actual locking. Take(x) locks the object referenced \nby x and increments the value of its stack reference counter. Drop(x) decreases the stack reference count \nof the object referenced by x, and releases its lock if it is safe to do so according to the EFL protocol, \ni.e., if the ref\u00aderence from x was the only reference to the object, and the object is consistent. Drop \nuses the function IsConsistent which indicates whether an object is consistent or not (ac\u00adcording to \nits heap-counter and the isExposed .eld). Figure 6. Example procedure adding the values from one linked-list \ninto another. For each procedure, of the module, our transformation is performed as follows: 1. At the \nbeginning of the procedure, add code that acquires all objects pointed to by arguments according to a \n.xed order; in a case of a single pointer argument l, this can be done by adding Take(l) (as in line \n3 of Fig. 3); the code of Fig. 5 demonstrates the case of 2 pointer arguments; in the general case objects \nare sorted to obtain the proper order. 2. Replace every assignment of a pointer expression with the \ncorresponding code macros in Table 2. The macro ASNL(x,ptrExp) replaces an assignment of a pointer expression \nptrExp to a local pointer x, this macro per\u00adforms this assignment, while maintaining stack-counters and \nfollowing the required locking strategy. The macro ASNF(x.f,ptrExp) replaces an assignment of a pointer \nexpression to a .eld of an object, this macro maintains the heap-counters in objects (its implementation \nfollows the required locking strategy). 3. Whenever a local variable l reaches the end of its scope, \nadd ASNL(l,null); this releases the object pointed by l. If this is the end of the procedure, and l is \nabout to be returned (i.e., by the statement return(l)), then instead of adding ASNL(l,null) add the \nblock {MarkExposed(l);Drop(l);}.  Example The procedure of Fig. 6 takes a pair of pointers to singly-linked \nlists, and adds values of one list to the values of the other. Fig. 7 shows the code transformed to enforce \nEFL. The transformed procedure starts with an invocation of TakeArgs2 (shown in Fig. 5) to lock exposed \nobjects in  1 void AddValues(Node x, Node y) { 2 TakeArgs2(x,y); 3 while(x!=null &#38;&#38; y!=null) \n{ 4 x.value+=y.value; 5 ASNL(x,x.next); 6 ASNL(y,y.next); 7 } 8 ASNL(x,null); ASNL(y,null); 9 } Figure \n7. Transformed code enforcing EFL for the proce\u00addure AddValues of Fig. 6. a .xed order. In the body of \nAddValues, the assignment x=x.next is replaced by the macro ASNL(x,x.next), which assigns x.next to x \nwhile maintaining EFL require\u00adments. The assignment y=y.next is handled in a similar way. At the end \nof AddValues, local variables go out of scope and locks are released by adding ASNL(x,null) and ASNL(y,null). \nPractical Consideration In some cases, some of our in\u00adstrumentation code can be avoided. For example, \ninstead of replacing x=null with ASNL(x,null), we could just add Drop(x) before the assignment. Or whenever \nit is known that a variable will not have a null value, we could avoid the if statements in Take and \nDrop. In modules where the forestness condition is not violated even temporarily, the heap reference \ncounter is not needed (since all objects remain consistent during a sequential exe\u00adcution of this transaction). \nIn many cases, exposed objects can be identi.ed by the types of objects (e.g., List is a type of exposed \nobjects, and Node is a type of hidden object); in such cases type information can be used instead of \nusing the isExposed .eld. Using Static Analysis The shown instrumented code can be optimized by using \nvarious static techniques. It is suf.\u00adcient for such static techniques to consider only sequential executions \nof the module. A live-variables analysis [2] can detect local pointers with unused values. Assigning \nnull to such pointers will eliminate unused pointers, and as a result will release locks earlier. Some \nstatic tools (e.g. [27]) can help avoid some of the instrumentation code. For example, if a tool can \ndetect that a local variable l is always null at some point of the CFG, our instrumentation code can \navoid calling Take(l) in this case.  4.3 Example for Dynamically Changing Forest As an example for a \ndynamically changing forest, consider the procedure shown in Fig. 8. This procedure operates on two Skew-Heaps \n[36] (a self-adjusting minimum-heap im\u00adplemented as a binary tree). The procedure moves the con\u00adtent \nof one Skew Heap (pointed by src) to another one (pointed by dest), by simultaneously traversing the \nheaps; 1 void move(SkewHeap src, SkewHeap dest) { 2 Node t1, t3, t2; 3 t1=dest.root; 4 t2=src.root; 5 \nif(t1.key > t2.key) { // assume both heaps are not empty 6 t3=t1; t1=t2; t2=t3; 7 } 8 dest.root=t1; 9 \nsrc.root=null; 10 t3=t1.right; 11 while(t3 != null &#38;&#38; t2 != null) { 12 t1.right=t1.left; 13 \nif(t3.key < t2.key) { 14 t1.left=t3; t1=t3; t3=t3.right; 15 } 16 else { 17 t1.left=t2; t1=t2; t2=t2.right; \n18 } 19 } 20 if(t3 == null) t1.right=t2; 21 else t1.right=t3; 22 } Figure 8. Moving the content of one \nSkew-Heap to another Skew-Heap. 1 void move(SkewHeap src, SkewHeap dest) { 2 Node t1, t3, t2; 3 TakeArgs2(src,dest); \n4 ASNL(t1, dest.root); 5 ASNL(t2, src.root); 6 if(t1.key > t2.key) { 7 ASNL(t3,t1); ASNL(t1,t2); ASNL(t2,t3); \n8 } 9 ASNF(dest.root, t1); 10 ASNL(dest, null); // dest becomes dead 11 ASNF(src.root, null); 12 ASNL(src, \nnull); // src becomes dead 13 ASNL(t3, t1.right); 14 while(t3 != null &#38;&#38; t2 != null) { 15 ASNF(t1.right, \nt1.left); 16 if(t3.key < t2.key) { 17 ASNF(t1.left, t3); ASNL(t1, t3); ASNL(t3, t3.right); 18 } 19 else \n{ 20 ASNF(t1.left, t2); ASNL(t1, t2); ASNL(t2, t2.right); 21 } 22 } 23 if(t3 == null) ASNF(t1.right, \nt2); 24 else ASNF(t1.right, t3); 25 ASNL(t1, null); ASNL(t2, null); ASNL(t3, null); 26 } Figure 9. Moving \nSkew Heaps with automatic .ne-grain locking. during its operation, nodes are dynamically moved from one \ndata-structure to another one. Fig. 9 show its code, after the source transformation. 5. Performance \nEvaluation We evaluate the performance of our technique on several benchmarks. For each benchmark, we \ncompare the perfor\u00admance of the benchmark using .ne-grain locking automat\u00adically generated using our \ntechnique to the performance of the benchmark using a single coarse-grain lock. We also compare some \nof the benchmarks to versions with hand\u00adcrafted .ne-grain locking. For some benchmarks, manually adding \n.ne-grain locking turned out to be too dif.cult even for concurrency experts.  In our experiments, we \nconsider 5 different benchmarks: two balanced search-tree data structures, a self-adjusting heap data \nstructure, and two specialized tree-structures (which are tailored to their application). The experiments \nwere run on a machine with 8 hardware threads. Speci.cally, we used an Intel Core2 i7 processor with \n4 cores that each multiplex 2 hardware threads. 5.1 General Purpose Data-Structures 5.1.1 Balanced Search-Trees \nWe consider two Java implementations of balanced search trees: a Treap [3], and a Red-Black Tree with \na top-down balancing [8, 18]. For both balanced trees, we consider the common operations of insert, remove \nand lookup. Methodology We follow the evaluation methodology of Herlihy et al. [20], and consider the \ndata structures under a workload of 20% inserts, 10% removes, and 70% lookups. The keys are generated \nfrom a random uniform distribution between 1 and 2 \u00d7 106. To ensure consistent and accurate results, \neach experiment consists of .ve passes; the .rst pass warms up the VM and the four other passes are timed. \nEach experiment was run four times and the arithmetic average of the throughput is reported as the .nal \nresult. Every pass of the test program consists of each thread performing one million randomly chosen \noperations on a shared data-structure; a new data-structure is used for each pass. Evaluation For both \nsearch trees, we compare the results of our automatic locking to a coarse-grain global lock. For the \nTreap, we also consider a version with manual hand\u00adover-hand locking. Enforcing hand-over-hand locking \nfor the Treap is challenging because after a rotation, the next thread to traverse a path will acquire \na different sequence of locks. Assuring the absence of deadlock under different acquisition orders is \nchallenging. For the Red-Black Tree, the task of manually adding .ne\u00adgrain locks proved to be too challenging \nand error prone. Rotations and deletions are much more complicated than in a Treap. Previous work on \n.ne-grain locking for these trees alters the tree invariants and algorithm, as in [32]. Even after spending \na whole day, we were unable to .nd or develop a correct manual locking strategy for true Red-Black Trees. \nFig. 10 shows results for the Treap. Our locking scales as well as manual hand-over-hand locking. They \nboth outper\u00adform the single-lock as the number of threads is increased. Fig. 11 shows results for the \nRed-Black Tree. Starting from 2 threads, our locking is faster than the single-lock. 5.1.2 Self-Adjusting \nHeap We consider a Java implementation of a Skew Heap [8, 36], which is a self-adjusting heap data-structure. \nWe consider the operations of insert and removeMin.  We use the same evaluation methodology we used \nfor the search trees. Here we consider a workload of 50% inserts and 50% removes on a heap initialized \nwith one million elements. We compare the results of our automatic locking to a coarse-grain global lock. \nThe results are shown in Fig. 12.  5.2 Specialized Implementations To illustrate the applicability \nof our technique to specialized data-structures (which are tailored to their application), we consider \nJava implementation of Barnes-Hut algorithm [5], and a C++ implementation of the Apriori Data-Mining \nalgo\u00adrithm [1] from [31].   5.2.1 Apriori In this application, a number of threads concurrently build \na Hash-Tree data-structure (a tree data-structure in which each node is either a linked-list or a hash-table). \nThe original ap\u00adplication uses customized hand-over-hand locking tailored to this application. We evaluate \nthe performance of our lock\u00ading relative to this specialized manual locking and to a single global lock. \nWe show that our locking performs as well as the specialized manual locking scheme in the original appli\u00adcation. \nIn the experiments, we measured the time required for the threads to build the Hash-Tree. Fig. 13 shows \nthe speedup of the original hand-crafted locking, and our locking over a single lock. For 2 and 4 threads, \nthe speedup of our locking is almost as good as the original manual locking. In the case of 8 threads \nit performs better than the original locking (around 30% faster). Both have a small overhead in the case \nof a single thread (around 4% slower).  5.2.2 Barnes-Hut The Barnes-Hut algorithm simulates the interaction \nof a system of bodies (such as galaxies or particles) and is built from several phases. Its main data-structure \nis an OCT-Tree. We parallelized the Construction-Phase in which the OCT-Tree is built, and used our technique \nfor synchronization. We measured the bene.t gained by our locking. In the experiments, we measured the \ntime required for threads to build the OCT-tree (i.e., the Construction-Phase). Fig. 14 shows the results. \nOur locking and manual hand\u00adover-hand locking show high overhead for 1 and 2 threads (150% for our locking, \nand 70% for manual hand-over\u00adhand). For a larger number of threads our locking and man\u00adual hand-over-hand \nare comparable, and they are both faster than the sequential version. 6. Discussion In this section, \nwe discuss extensions and additional poten\u00adtial applications of our approach. Other Ways to Enforce Domination \nLocking We have presented a way to enforce domination locking on forest\u00adbased modules. Still, domination \nlocking can be enforced in several other cases (both manually and automatically). In particular, it can \nbe enforced in cases in which the shared heap is not a forest. As an illustrative example, consider a \nnon forest-based module M with a single exposed object e in which hidden objects never point to e. Assume \nthat M uses the instrumen\u00adtation presented in Section 4, and that each transaction re\u00adleases all its \nlocks before its completion (can be realized, for example, by adding a ReleaseAll statement at the end \nof transactions; this statement releases all locks that are owned by the transaction). It is easy to \nsee that M follows domina\u00adtion locking in sequential executions. Hence, if M s sequen\u00adtial executions \nare completable, then its concurrent execu\u00adtions are strict con.ict-serializable and completeable. We \nbelieve that it is interesting to explore different ways for realizing the domination locking protocol, \ntogether with their practical implications. Using Optimistic Synchronization for Read-Only Opera\u00adtions \nDomination locking can be combined with optimistic synchronization to improve scalability for read-only \ntransac\u00adtions by adding version information to objects. Read-write transactions would synchronize between \nthemselves using DL, with no chance of rollback, while read-only transactions would use the version numbers \nto ensure consistent reads. Version numbers could either be managed locally, by incre\u00admenting them on \neach commit (e.g., as in [9]), or globally us\u00ading a timestamp scheme (e.g., as in [14]). The local scheme \nwould provide better scalability for writers, while the global scheme admits very ef.cient read-only \ntransactions. Con\u00adtention management in our system would be easier than in purely optimistic schemes \nsuch as STM, because read-only transactions can fall back to DL locking after experiencing too many rollbacks. \nVeri.cation Domination Locking protocol can provide a basis for veri.cation. For example, [4] describes \na veri.ca\u00adtion technique based on special cases of domination lock\u00ading (dynamic DAG and Tree locking). \nBy using domination locking, their analysis can be simpli.ed and extended be\u00adcause of the weaker conditions \nof domination locking.  7. Related Work Locking Protocols Locking protocols are used in database and \nother software systems to guarantee correctness of con\u00adcurrently executing transactions. A widely used \nprotocol is the two-phase-locking (2PL) protocol [16] which guarantees con.ict-serializability of transactions, \nbut does not guaran\u00adtee deadlock-freedom. In the 2PL protocol, locking is done in two phases, in the \n.rst phase locks are only allowed to be acquired (releasing locks is forbidden); in the second phase \nlocks are only allowed to be released. These restrictions require that locks are held until the .nal \nlock is obtained, thus preventing early release of a lock even when locking it is no longer required. \nThis limits parallelism, especially in the presence of long transactions. (e.g., a tree traversal must \nhold the lock on the root until the .nal node is reached.) Other locking protocols (non-2PL protocols) \nrely on the shape of the heap. Most of these protocols (e.g.[25, 35]) were designed for databases in \nwhich the shape of shared ob\u00adjects does not change during a transaction, and thus are not suitable for \nmore general cases with dynamically changing heaps. [4, 10, 26] show protocols that can handle dynami\u00adcally \nchanging heap shapes. Attiya et al. [4] present a dynamic tree-locking protocol and show that if it is \nsatis.ed by all sequential executions then it is also satis.ed by all concurrent executions. In con\u00adtrast, \nDL does not enforce any requirement on the heap. In fact, it is possible for a program that follows DL \nfor all se\u00adquential executions to violate the DL conditions during a concurrent execution. Still, we \nshow that if DL is satis.ed by all sequential executions then all concurrent executions are correct, \ni.e., guarantee atomicity and deadlock freedom. This result simpli.es the task of reasoning about programs \nusing DL, since it allows both programmers and program analysis tools to ignore interleaved states. Wang \net al. [37] describe a static analysis and accompa\u00adnying runtime instrumentation that eliminates the \npossibil\u00adity of deadlock from multi-threaded programs using locks. Their tool adds additional locks that \ndominate any potential locking cycle, but it requires as a starting point a program that already has \nthe locks necessary for atomicity. Boyapati et al. [7] describe an ownership type system that guarantees \ndata race freedom and deadlock freedom, but still not atomicity. Their approach can prevent deadlocks \nby relying on partial-order of objects, and also permit to dynamically change this partial-order. Interestingly, \nDL also relies on the intuition of dynamic ownership where exposed objects dominate hidden objects. Lock \ninference There has been a lot of work on inferring locks for implementing atomic sections. Most of the \nalgo\u00adrithms in the literature infer locks for following the 2PL locking protocol [11, 12, 15, 17, 24, \n29]. The algorithms in [15, 24, 29] employ a 2PL variant in which all locks are re\u00adleased at the end \nof a transaction. In these algorithms, dead\u00adlock is prevented by statically ordering locks and rejecting \ncertain programs. The algorithms in [11, 17] use a 2PL vari\u00adant in which all locks are acquired at the \nbeginning of trans\u00adactions and released at the end of transactions. In these algo\u00adrithms, deadlock is \nprevented by using a customized locking protocol at the beginning of atomic sections. As described above, \n2PL limits parallelism as all locks must be held until the .nal lock is acquired. Transactional Memory \nTransactional memory approaches (TMs) dynamically resolve deadlocks by rolling back par\u00adtially completed \natomic regions.4 The TM programming model can be implemented as an extension to the cache coherence protocol \n[22] or as a code transformation [21]. Preserving the ability to roll back requires that transactions \nbe isolated from the rest of the system, which prohibits them from performing I/O. Software transactions \nare also prohib\u00adited from calling modules that have not been transformed by the TM. Ad-hoc proposals \nfor speci.c forms of I/O are present in many TMs [30], but in the general case at most one transaction \nat a time can safely perform an irrevoca\u00adble action [39]. Pessimistic automatic concurrency control schemes \nsuch as our technique, in contrast, do not limit con\u00adcurrent I/O or calls to foreign modules. GC Algorithms \nIt is interesting to note that our locking using reference counters bears similarities with garbage col\u00adlection \nalgorithms based on reference counts (e.g., [13, 28]). In particular, it is bene.cial to maintain separate \nreference counts from the stack and the heap. Acknowledgments We thank the anonymous referees for their \nuseful comments. This research was partially supported by The Israeli Science Foundation (grant no. 965/10), \nthe National Science Foun\u00addation (grant NSF CCF-0702681), and a gift from IBM. References [1] AGRAWAL, \nR., MANNILA, H., SRIKANT, R., TOIVONEN, H., AND VERKAMO, I. Advances in knowledge discovery and data \nmining. American Association for Arti.cial Intel\u00adligence, Menlo Park, CA, USA, 1996, ch. Fast discovery \nof association rules, pp. 307 328. [2] AHO, A. V., LAM, M. S., SETHI, R., AND ULLMAN, J. D. Compilers: \nPrinciples, Techniques, and Tools (2nd Edition). Addison Wesley, 2006. [3] ARAGON, C., AND SEIDEL, R. \nRandomized search trees. Foundations of Computer Science, Annual IEEE Symposium on 0 (1989), 540 545. \n[4] ATTIYA, H., RAMALINGAM, G., AND RINETZKY, N. Se\u00adquential veri.cation of serializability. In POPL \n10: Pro\u00adceedings of the 37th annual ACM SIGPLAN-SIGACT sym\u00adposium on Principles of programming languages \n(New York, NY, USA, 2010), ACM, pp. 31 42. 4 Many TMs also roll back transactions in the case of incorrect \nspeculation.  Instruction Transition Side Condition skip x = e(y1, . . . , yk) assume(b) s .t,e -. (h, \nr, e[t . (k ' , ., L)]) s .t,e -. (h, r, e[t . (k ' , .[x . [ e]](.(y1), . . . , .(yk))], L)]) s .t,e \n-. (h, r, e[t . (k ' , ., L)]) .(b) = true x = newR() x = y.f x.f = y s .t,e -. (h[a . o], r, e[t . (k \n' , .[x . a], L)]) s .t,e -. (h, r, e[t . (k ' , .[x . h(.(y))(f)], L)]) s .t,e -. (h[.(x) . (h(.(x))[f \n. .(y)])], r, e[t . (k ' , ., L)]) a . dom(h) . .(R)() = o .(y) . dom(h) .(x) . dom(h) acquire(x) release(x) \ns .t,e -. (h, r, e[t . (k ' , ., L . {.(x)})]) s .t,e -. (h, r, e[t . (k ' , ., L \\ {.(x)})]) .(x) . \nL . .(k '' , . ' , L ' ) . range(e) : .(x) . L ' .(x) . L return(x) return(x) s .t,e -. (h, r[.(x) . \ntrue], e[t . (k ' , ., L)]) s .t,e -. (h, r, e[t . (k ' , ., L)]) .(x) . dom(h) .(x) . dom(h) Table \n3. The semantics of primitive instructions. For brevity, we use the shorthands: s = (h,r, e) and e(t)= \n(k, ., L), and omit (k, k ' )= e . CFGt from all side conditions. [5] BARNES, J., AND HUT, P. A hierarchical \nO(N log N) force\u00adcalculation algorithm. Nature 324 (Dec. 1986), 446 449. [6] BAYER, R., AND SCHKOLNICK, \nM. Concurrency of opera\u00adtions on B-Trees. Acta Informatica 9 (1977), 1 21. [7] BOYAPATI, C., LEE, R., \nAND RINARD, M. Ownership types for safe programming: preventing data races and deadlocks. In Proceedings \nof the 17th ACM SIGPLAN conference on Object-oriented programming, systems, languages, and appli\u00adcations \n(New York, NY, USA, 2002), OOPSLA 02, ACM, pp. 211 230. [8] BRASS, P. Advanced Data Structures. Cambridge \nUniversity Press, New York, NY, USA, 2008. [9] BRONSON, N. G., CASPER, J., CHAFI, H., AND OLUKO-TUN, \nK. A practical concurrent binary search tree. In PPOPP (2010), pp. 257 268. [10] CHAUDHRI, V. K., AND \nHADZILACOS, V. Safe locking policies for dynamic databases. In PODS 95: Proceedings of the fourteenth \nACM SIGACT-SIGMOD-SIGART symposium on Principles of database systems (New York, NY, USA, 1995), ACM, \npp. 233 244. [11] CHEREM, S., CHILIMBI, T., AND GULWANI, S. Inferring locks for atomic sections. In PLDI \n(2008), pp. 304 315. [12] CUNNINGHAM, D., GUDKA, K., AND EISENBACH, S. Keep off the grass: Locking the \nright path for atomicity. In Com\u00adpiler Construction, vol. 4959 of Lecture Notes in Computer Science. \nSpringer Berlin / Heidelberg, 2008, pp. 276 290. [13] DEUTSCH, L. P., AND BOBROW, D. G. An ef.cient, \nincre\u00admental, automatic garbage collector. Commun. ACM 19,9 (1976), 522 526. [14] DICE, D., SHALEV, O., \nAND SHAVIT, N. Transactional locking ii. In DISC (2006), pp. 194 208. [15] EMMI, M., FISCHER, J. S., \nJHALA, R., AND MAJUMDAR, R. Lock allocation. In POPL (2007), pp. 291 296. [16] ESWARAN, K. P., GRAY, \nJ. N., LORIE, R. A., AND TRAIGER, I. L. The notions of consistency and predicate locks in a database \nsystem. Commun. ACM 19 (November 1976), 624 633. [17] GUDKA, K., EISENBACH, S., AND HARRIS, T. Lock Infer\u00adence \nin the Presence of Large Libraries. Tech. rep., November 2010. [18] GUIBAS, L. J., AND SEDGEWICK, R. \nA dichromatic frame\u00adwork for balanced trees. In Proceedings of the 19th Annual Symposium on Foundations \nof Computer Science (Washing\u00adton, DC, USA, 1978), IEEE Computer Society, pp. 8 21. [19] HARRIS, T., LARUS, \nJ., AND RAJWAR, R. Transactional memory, 2nd edition. Synthesis Lectures on Computer Archi\u00adtecture 5, \n1 (2010), 1 263. [20] HERLIHY, M., LEV, Y., LUCHANGCO, V., AND SHAVIT, N. A provably correct scalable \nconcurrent skip list. In OPODIS 06: Proceedings of the 10th International Conference On Principles Of \nDistributed Systems (December 2006). [21] HERLIHY, M., LUCHANGCO, V., MOIR, M., AND III, W. N. S. Software \ntransactional memory for dynamic-sized data structures. In PODC (2003), pp. 92 101. [22] HERLIHY, M., \nAND MOSS, J. E. B. Transactional memory: Architectural support for lock-free data structures. In ISCA \n(1993), pp. 289 300. [23] HERLIHY, M. P., AND WING, J. M. Linearizability: a cor\u00adrectness condition for \nconcurrent objects. Proc. of ACM TOPLAS 12, 3 (1990), 463 492. [24] HICKS, M., FOSTER, J. S., AND PRATTIKAKIS, \nP. Lock in\u00adference for atomic sections. In Proceedings of the First ACM SIGPLAN Workshop on Languages, \nCompilers, and Hard\u00adware Support for Transactional Computing (June 2006). [25] KEDEM, Z. M., AND SILBERSCHATZ, \nA. A characterization of database graphs admitting a simple locking protocol. Acta Inf. 16 (1981), 1 \n13.  [26] LANIN, V., AND SHASHA, D. Tree locking on changing trees. Tech. rep., 1990. [27] LEV-AMI, \nT., AND SAGIV, M. TVLA: A framework for Kleene based static analysis. In Saskatchewan (2000), vol. 1824 \nof Lecture Notes in Computer Science, Springer-Verlag, pp. 280 301. [28] LEVANONI, Y., AND PETRANK, E. \nAn on-the-.y reference\u00adcounting garbage collector for Java. ACM Trans. Program. Lang. Syst. 28, 1 (2006), \n1 69. [29] MCCLOSKEY, B., ZHOU, F., GAY, D., AND BREWER, E. Autolocker: synchronization inference for \natomic sections. In POPL 06: Conference record of the 33rd ACM SIGPLAN-SIGACT symposium on Principles \nof programming languages (New York, NY, USA, 2006), ACM, pp. 346 358. [30] MOSS, J. E. B. Open Nested \nTransactions: Semantics and Support. In Poster at the 4th Workshop on Memory Perfor\u00admance Issues (WMPI-2006). \nFebruary 2006. [31] NARAYANAN, R., \u00d6ZIS. IKYILMAZ, B., ZAMBRENO, J., MEMIK, G., AND CHOUDHARY, A. Minebench: \nA bench\u00admark suite for data mining workloads. In 2006 IEEE Inter\u00adnational Symposium on Workload Characterization \n(2006), pp. 182 188. [32] NURMI, O., AND SOISALON-SOININEN, E. Uncoupling updating and rebalancing in \nchromatic binary search trees. In Proceedings of the tenth ACM SIGACT-SIGMOD-SIGART symposium on Principles \nof database systems (New York, NY, USA, 1991), PODS 91, ACM, pp. 192 198. [33] PAPADIMITRIOU, C. H. The \nserializability of concurrent database updates. J. ACM 26, 4 (1979), 631 653. [34] SAGIV, M., REPS, T., \nAND WILHELM, R. Parametric Shape Analysis via 3-valued Logic. ACM Trans. on Prog. Lang. and Systems (TOPLAS) \n24, 3 (2002), 217 298. [35] SILBERSCHATZ, A., AND KEDAM, Z. A family of locking protocols for database \nsystems that are modeled by directed graphs. Software Engineering, IEEE Transactions on SE-8,6 (Nov. \n1982), 558 562. [36] SLEATOR, D. D., AND TARJAN, R. E. Self adjusting heaps. SIAM J. Comput. 15 (February \n1986), 52 69. [37] WANG, Y., LAFORTUNE, S., KELLY, T., KUDLUR, M., AND MAHLKE, S. A. The theory of deadlock \navoidance via dis\u00adcrete control. In POPL (2009), pp. 252 263. [38] WEIKUM, G., AND VOSSEN, G. Transactional \ninformation systems: theory, algorithms, and the practice of concurrency control and recovery. Morgan \nKaufmann Publishers Inc., San Francisco, CA, USA, 2001. [39] WELC, A., SAHA, B., AND ADL-TABATABAI, A.-R. \nIrre\u00advocable transactions and their applications. In SPAA (2008), pp. 285 296. [40] YANG, H., LEE, O., \nBERDINE, J., CALCAGNO, C., COOK, B., AND DISTEFANO, D. Scalable shape analysis for systems code. In In \nCAV (2008). v . V al = Loc I ZI {true, false, null}. .E = . V al V yh .H = Loc y.F y . V al l .L = Loc \ns .S = K\u00d7E\u00d7 2L s . S= H\u00d7 (Loc y.{true, false}) \u00d7 (T y.S) Figure 15. Semantic domains A. Semantics Fig. \n15 de.nes the semantic domains of a state of a module, and meta-variables ranging over them. Let t .T \nbe the domain of transaction identi.ers. A state s = (h, r, e). S of a module is a triple: h assigns \nvalues to .elds of dynamically allocated objects. A value v . Val can be either a location, an integer, \na boolean value,or null. r maps exposed objects to true, and hidden objects to false. Finally, e associates \na transaction t with its transaction local state e(t).A transaction-local state s = (k, ., L). S is: \nk is the value of the transaction s program counter, . records the values of its local variables, and \nL is the transaction s lock set which records the locks that the transaction holds. The behavior of a \nmodule is described by the relations -. and .. The relation -. is a subset of S \u00d7 (T\u00d7 (K\u00d7 K)) \u00d7 S, and \nis de.ned in Table 3.5 t,e A transition s -. s ' represents the fact that s can be transformed into s \n' via transaction t executing the instruc\u00adtion annotating control-.ow edge e. Invocation of a new transaction \nis modeled by the relation .. S \u00d7T \u00d7 S; t we say that (h, r, e). s ' if s ' = (h, r, e[t . s]) where \nt . dom(e) and s is any valid initial local state: i.e., s = (entry, ., {}), where entry is the entry \nvertex, and . maps local variables and parameters to appropriate initial values (based on their type). \nIn particular, . must map any pointer parameter of a type de.ned by the module to an exposed ob\u00adject \n(i.e., an object u in h such that r(u)= true). We write t s -. s ', if there exists t such that s . s \n' or there exists t,e (t, e) such that s -. s ' . The schedule of an execution p = s0,...,sk is a se\u00adquence \n(t0,e0),..., (tk-1,ek-1) such that for 0 = i<k: ti,eiti si -. si+1, or si . si+1 and ei = einit (where \neinit is disjoint with all edges in the CFG). We say that a sequence . = (t0,e0),..., (tk-1,ek-1) is \na feasible schedule, if . is a schedule of an execution. The schedule of a transaction t in an execution \nis the (possibly 5 For simplicity of presentation, we use an idempotent variant of acquire (i.e., acquire \nhas no impact when the lock has already owned by the current transaction). We note that this variant \nis permitted by the Lock interface from the java.util.concurrent.locks package, and can easily be implemented \nin languages such as Java and C++.  non-contiguous) subsequence of the execution s schedule consisting \nonly of t s transitions. We de.ne the allocation id of an object in an execution to be the pair (t, i) \nif the object was allocated by the i-th transition executed by a transaction t. An object o1 in an execution \np1 corresponds to an object o2 in an execution p2 iff their allocation ids are the same. In the sequel \nwe will compare states and objects belonging to different executions modulo this correspondence relation. \nB. Proofs DEFINITION 1. An execution is said to be well-locked if every transaction in the execution \naccesses a .eld of an object, only when it holds a lock on that object. DEFINITION 2. We say that a set \nS of objects dominates an object u (in a given state) if every path from an exposed object to u contains \nsome objects from S. We say that a transaction t blocks an object u (in a given state) if the set of \nobjects locked by t dominates u. DEFINITION 3. We say that a transaction t is in phase-1 if it is still \nrunning and has never released a lock. Otherwise, we say that t is in phase-2 (i.e., t is in phase-2 \nif it has already completed, or it has released at least one lock). LEMMA 1. Let . = .p.t.s be any feasible \nwell-locked schedule, where .t is the schedule of a transaction t. If t is in phase-1 (after .), then \nthere is no con.ict between .t and .s (in .). Proof Immediate from the de.nition of phase-1. DEFINITION \n4. We say that an ni-execution (non-interleaved execution) is phase-ordered if all phase-2 transactions \npre\u00adcede phase-1 transactions. LEMMA 2. Any feasible well-locked ni-schedule .1.2 \u00b7\u00b7\u00b7 .n is con.ict-equivalent \nto a well-locked phase-ordered ni\u00adschedule .i1 \u00b7\u00b7\u00b7 .in . Proof Because of Lemma 1, moving all phase-1 \ntransac\u00adtions to the end of the schedule does not affect any of the con.ict-dependences. In the following \nwe assume that =h is a total order of all heap objects. We assume that =h has a minimal value . (i.e., \nif u is an object then .=h u). We say that u<h v, if u = v and u =h v. DEFINITION 5. We say that max(s, \nt)= u, if u is the maximal exposed object that is locked by transaction t in state s (i.e., u is locked \nby t in s, and every exposed object v that is locked by t in s satis.es v =h u). If no exposed object \nis locked by t in s, then max(s, t)= .. DEFINITION 6. Let p = a1 \u00b7\u00b7\u00b7 ak be a phase-ordered exe\u00adcution. \nLet s be the last state of p. We say that p is fully\u00adordered, if for every ai and aj that are in phase-1 \nthe fol\u00adlowing holds: if i<j then max(s, tj) =h max(s, ti). LEMMA 3. Any feasible well-locked ni-schedule \n.1.2 \u00b7\u00b7\u00b7 .n is con.ict-equivalent to a well-locked fully-ordered ni\u00adschedule .i1 \u00b7\u00b7\u00b7 .in . Proof Similar \nto Lemma 2. Here we also reorder the phase\u00ad1 transactions according to =h. LEMMA 4. Consider any sequential-execution \np1 . s . p2 that follows domination locking. Assume that t is in phase 2 at the end of p1 . s. For any \nobject o in state s, t can access o during the (remaining) execution s . p2 only if t blocks o in s. \nProof Let u be an object in s which is not blocked by t in s. Hence s contains a path P from an exposed \nobject to u, such that none of the objects in P are locked by t. We can inductively show that none of \nthe objects in P are locked, and hence not accessed or modi.ed during the rest of the execution. DEFINITION \n7. Let p1 and p2 be two executions such that for every transaction t the schedule of t in p1 is a pre.x \nof the schedule of t in p2. p2 is said to be a con.ict-equivalent ex\u00adtension of p1 if every step (t, \ne) in p1 has the same con.ict\u00adpredecessors as the corresponding step in p2. p2 is said to be an equivalent \ncompletion of p1 if it is a complete execution and is a con.ict-equivalent extension of p1. Note that \nif an execution a1\u00df1 \u00b7\u00b7\u00b7 an\u00dfn is a con.ict\u00adequivalent extension of p = a1 \u00b7\u00b7\u00b7 an, then the execution \na1 \u00b7\u00b7\u00b7 an\u00df1 \u00b7\u00b7\u00b7 \u00dfn is also a con.ict-equivalent extension of p. LEMMA 5. Let pni be a well-locked ni-execution \nwith a schedule a1 \u00b7\u00b7\u00b7 ak. Let pe be a con.ict-equivalent exten\u00adsion of pni with a schedule a1\u00df1 \u00b7\u00b7\u00b7 \nak\u00dfk. Assume that ti blocks an object u at the end of ai in pe. Then, the execution of ai+1 \u00b7\u00b7\u00b7 ak in \npni does not access u 6 . Proof Let s denote the state at the end of ai in pni. For any object x in s \naccessed by the execution of ai+1 \u00b7\u00b7\u00b7 ak in pni we de.ne the path Px inductively as follows. If x is \nan exposed object in s, then Px is de.ned to be the se\u00adquence [x]. If x is a hidden object in s, then \nthe execution of ai+1 \u00b7\u00b7\u00b7 ak must have dereferenced some .eld of some object that pointed to x. Consider \nthe .rst .eld y.f derefer\u00adenced by ai+1 \u00b7\u00b7\u00b7 ak that pointed to x, where y represents an object. We de.ne \nPx to consist of the sequence Py followed by x. 6 Note that in this case ti might not actually block \nobject u at the end of ai in pni.  Assume that u is accessed during the execution of ai+1 \u00b7\u00b7\u00b7 ak in \npni. Hence Pu exists at the end of ai in pni. By the de.nition of a con.ict-equivalent extension, Pu \nalso exists at the end of execution of ai in pe. (in particular, for 1 = j = i the execution of \u00dfj in \npe does not access any object in Pu). Hence, ti must hold a lock on some object y in this path (at the \nend of ai in both pni as well as pe). Since pni is well-locked, the execution of ai+1 \u00b7\u00b7\u00b7 ak in pni could \nnot have locked y which is a contradiction. Hence u is not accessed during the execution of ai+1 \u00b7\u00b7\u00b7 \nak in pni LEMMA 6. Let p = a1 \u00b7\u00b7\u00b7 an be a well-locked fully\u00adordered execution with at least one incomplete \ntransac\u00adtion. Let tk be the .rst incomplete transaction in p (i.e., k is the minimal number such that \ntk is incomplete). If ev\u00adery sequential-execution of a module follows domination locking and is completable, \nthen p has an equivalent ex\u00adtension a1 \u00b7\u00b7\u00b7 ak\u00dfkak+1 \u00b7\u00b7\u00b7 an in which transaction tk is completed. Proof \nSince a1 \u00b7\u00b7\u00b7 ak represents a sequential-execution, it has a completion a1 \u00b7\u00b7\u00b7 ak\u00dfk that follows domination \nlock\u00ading. We consider the following cases. Case 1: After p transaction tk is in phase-2. Let s represent \nthe state produced by the execution of a1 \u00b7\u00b7\u00b7 ak. From Lemma 4, all objects in s accessed during the \nexecu\u00adtion of \u00dfk (in a1 \u00b7\u00b7\u00b7 ak\u00dfk) must be blocked by tk in s. From Lemma 5 the execution of ak+1 \u00b7\u00b7\u00b7 \nan (in a1 \u00b7\u00b7\u00b7 an) cannot access any object blocked by tk in s. Hence the schedule a1 \u00b7\u00b7\u00b7 ak\u00dfkak+1 \u00b7\u00b7\u00b7 \nan is feasible and is a con.ict-equivalent extension of a1 \u00b7\u00b7\u00b7 an. Case 2: k = n Here a1 \u00b7\u00b7\u00b7 ak\u00dfk is \nthe equivalent extension. Case 3: k<n, and after p transaction tk is in phase-1 Let s represent the \nstate produced by the execution of a1 \u00b7\u00b7\u00b7 ak-1. Let k<m = n. Because of Lemma 1, no con.ict-dependence \ncan exist between the running transactions (because they are all in phase-1), hence a1 \u00b7\u00b7\u00b7 ak-1am represents \na feasible se\u00adquential execution that follows domination locking. Let u be an exposed object in s that \nis accessed by tk in a1 \u00b7\u00b7\u00b7 ak-1ak\u00dfk, we will show that u is not accessed by tm in a1 \u00b7\u00b7\u00b7 ak-1am. If \nu is accessed or locked by ak in a1 \u00b7\u00b7\u00b7 ak-1ak\u00dfk, then u is not accessed or locked by am in a1 \u00b7\u00b7\u00b7 ak-1am \n(because tk and tm have no con.ict in p). Otherwise, u is locked by \u00dfk in a1 \u00b7\u00b7\u00b7 ak-1ak\u00dfk. Let s ' denote \nthe state produced by the execution of p. max(s ' ,tm) <h max(s ' ,tk) (because after the fully\u00adordered \nexecution p, tk and tm are in phase-1 and tk pre\u00adcedes tm ). max(s ' ,tk) <h u (because of condition \n2). Hence, max(s ' ,tm) <h u Hence, u is not accessed or locked by tm in a1 \u00b7\u00b7\u00b7 ak-1am. Let v be a hidden \nobject in s that is accessed by tk in a1 \u00b7\u00b7\u00b7 ak-1ak\u00dfk. we will show that v is not accessed by tm in a1 \n\u00b7\u00b7\u00b7 ak-1am. v is necessarily reachable from exposed objects in s, hence there exists a path P (in s) \nfrom an exposed object w to v, such that w is the only exposed object in P . tk accesses w in a1 \u00b7\u00b7\u00b7 \nak-1ak\u00dfk (because of conditions 4 and 1). Assume that v is accessed by tm in a1 \u00b7\u00b7\u00b7 ak-1am, then tm accesses \nw in a1 \u00b7\u00b7\u00b7 ak-1am (conditions 4 and 1). But we have showed that this is not possible for exposed objects. \nTherefore v is not accessed by tm in a1 \u00b7\u00b7\u00b7 ak-1am. We have showed, for every k<m = n, tk does not ac\u00adcess \n(in a1 \u00b7\u00b7\u00b7 ak-1ak\u00dfk) any object that is accessed by tm (in a1 \u00b7\u00b7\u00b7 ak-1am). Hence, a1 \u00b7\u00b7\u00b7 ak\u00dfkak+1 \u00b7\u00b7\u00b7 \nan is an equivalent extension of a1 \u00b7\u00b7\u00b7 an. LEMMA 7. Let p = a1 \u00b7\u00b7\u00b7 an be a well-locked fully-ordered \nexecution. If every sequential-execution of a module follows domination locking and is completable, then \np has an equiv\u00adalent completion a1\u00df1 \u00b7\u00b7\u00b7 an\u00dfn. Proof If p is not a complete execution, we can construct \nan equivalent completion a1\u00df1 \u00b7\u00b7\u00b7 an\u00dfn by repeatedly apply\u00ading Lemma 6. LEMMA 8. Let . = .p.t.s be any \nfeasible well-locked ni\u00adschedule, where .t is the schedule of a transaction t. If . \u00b7 (t, e) is feasible, \nthen .p.t \u00b7 (t, e) is also feasible. Proof Assume that . \u00b7 (t, e) is feasible. We show that .p.t \u00b7 (t, \ne) is feasible. The only sources of infeasibility are when the step (t, e) involves a conditional branch \n(i.e., an assume statement) or an attempt to acquire a lock. We make the simplifying assumption that \nan assume statement refers to only thread-local variables. (Note that there is no loss of generality \nhere since any statement assume e can be rewritten as x = e; assume x where x is a thread-local variable.) \nAs a result, .p.t \u00b7 (t, e) must be feasible if (t, e) involves a conditional branch. Now, consider the \ncase where (t, e) involves an acquire x instruction where x is a thread-local variable. If the object \nx points to is unlocked at the end of .p.t.s, it must be unlocked at the end of .p.t as well. Hence, \nfeasibility follows in this case as well.  LEMMA 9. If every sequential-execution of a module fol\u00adlows \ndomination locking and is completable, then every ni\u00adexecution is well-locked. Proof We prove by induction \non the length of the execu\u00adtions. Let . be a schedule of a well-locked ni-execution. We will prove that \nif . \u00b7 (t, e) is feasible, then it is a schedule of a well-locked execution. Assume that after ., the \nstep (t, e) accesses an object u. From Lemma 3, . is con.ict-equivalent to a fully-ordered ni-schedule \n. ' = a1 \u00b7\u00b7\u00b7 an. We consider the following cases. Case 1: there exists i such that t = ti and 1 = i<n. \nFrom Lemma 8, a1 \u00b7\u00b7\u00b7 ai \u00b7(ti,e) is a feasible schedule. From the induction hypothesis, ti holds a lock \non u after a1 \u00b7\u00b7\u00b7 ai. Hence, ti holds a lock on u after . ' = a1 \u00b7\u00b7\u00b7 an. Hence, ti holds a lock on u \nafter .. Case 2: t = tn. From Lemma 7, . ' has an equivalent completion with the schedule a1\u00df1 \u00b7\u00b7\u00b7 an\u00dfn. \nWe de.ne . '' = a1\u00df1 \u00b7\u00b7\u00b7 an-1\u00dfn-1an (this is a pre.x of a1\u00df1 \u00b7\u00b7\u00b7 an\u00dfn). The step (tn,e) accesses u after \n. '' (because tn has the same local state after . ' and . ''). Since . '' \u00b7 (tn,e) represents a sequential \nexecution, u is locked by tn after . ''. Hence, tn holds a lock on u after a1 \u00b7\u00b7\u00b7 an. Hence, tn holds \na lock on u after .. Case 3: t does not appear in . According the de.nition of a schedule, the .rst \nstep of a transaction does not access an object. LEMMA 10. If every sequential-execution of a module \nfol\u00adlows domination locking and is completable, then every ex\u00adecution p is con.ict-equivalent to a fully-ordered \nexecution p ' such that a transaction t completes before a transaction t ' begins in p ' if t completes \nbefore t ' begins in p. Proof We prove this by induction on the length of the ex\u00adecution. Consider any \nexecution with a schedule . \u00b7 (ti,e). By the inductive hypothesis, the execution of . is con.ict\u00adequivalent \nto a fully-ordered execution with the schedule . ' = a1 \u00b7\u00b7\u00b7 ak 7 such that a transaction t completes \nbefore a transaction t ' begins in . ' if t completes before t ' begins in .. From Lemma 9, . ' is well-locked. \nWe consider the following cases: Case 1: After a1 \u00b7\u00b7\u00b7 ai, transaction ti is in phase 2, and (ti,e) does \nnot access an heap object. In this case, a1 \u00b7\u00b7\u00b7 ak \u00b7 (ti,e) is con.ict equivalent to a1 \u00b7\u00b7\u00b7 ai \u00b7 (ti,e) \n\u00b7 ai+1 \u00b7\u00b7\u00b7 ak 7 Note that 1 = i = k and ai may be empty Case 2: After a1 \u00b7\u00b7\u00b7 ai, transaction ti is in \nphase 2, and (ti,e) accesses an heap object u. According to Lemma 7, . ' = a1 \u00b7\u00b7\u00b7 ak has an equivalent \ncompletion . '' = a1\u00df1 \u00b7\u00b7\u00b7 ak\u00dfk. ai-1\u00dfi-1ai (. ''' Let . ''' = a1\u00df1 \u00b7\u00b7\u00b7 is a pre.x of . ''). ti has \nthe same local state after a1 \u00b7\u00b7\u00b7 ai and . ''' (according the de.nition of con.ict-equivalent extension). \nAccording to Lemma 8, a1 \u00b7\u00b7\u00b7 ai \u00b7 (ti,e) is a feasible sched\u00adule, so . ''' \u00b7 (ti,e) is also a feasible \nschedule. Also . ''' \u00b7 (ti,e) represents a sequential execution (which follows domination locking). Hence \naccording to Lemma 4, ti blocks u after . ''' . Hence, ti blocks u after ai in . '' . Hence, according \nto Lemma 5, ai+1 \u00b7\u00b7\u00b7 ak does not access u in . ' = a1 \u00b7\u00b7\u00b7 ak. Therefore, a1 \u00b7\u00b7\u00b7 ak \u00b7 (ti,e) is con.ict \nequivalent to a1 \u00b7\u00b7\u00b7 ai \u00b7 (ti,e) \u00b7 ai+1 \u00b7\u00b7\u00b7 ak Case 3: Transaction ti is in phase 1 after a1 \u00b7\u00b7\u00b7 ai. \nBecause of Lemma 1, we can reorder all phase-1 transac\u00adtions and ti (even if ti is in phase-2 after a1 \n\u00b7\u00b7\u00b7 ak \u00b7 (ti,e)). If ti is in phase-2 after a1 \u00b7\u00b7\u00b7 ak \u00b7 (ti,e), then we can con\u00adstruct the fully-ordered \nequivalent execution by moving ai \u00b7 (ti,e) just before all the phase-1 transactions. Otherwise (ti is \nstill in phase-1 after a1 \u00b7\u00b7\u00b7 ak \u00b7 (ti,e)), we can construct the fully-ordered equivalent execution by \nmov\u00ading ai \u00b7 (ti,e) to the right place according to the max values (between the phase-1 transactions). \n THEOREM B.1. If every sequential-execution of a module follows domination locking and is completable, \nthen every execution of the module is strict con.ict-serializable. Proof Immediate from Lemma 10. THEOREM \nB.2. If every sequential-execution of a module follows domination locking and is completable, then every \nexecution of the module is a pre.x of a complete-execution. Proof Consider any execution p. According \nto Lemma 10, there exists a fully-ordered execution p ' = a1 \u00b7\u00b7\u00b7 an which is con.ict equivalent to p. \nAccording to Lemma 7, p ' has an equivalent completion a1\u00df1 \u00b7\u00b7\u00b7 an\u00dfn. According the de.ni\u00adtion of con.ict-equivalent \nextension, there exists execution a1 \u00b7\u00b7\u00b7 an\u00df1 \u00b7\u00b7\u00b7 \u00dfn. Hence, p ' is a pre.x of a complete exe\u00adcution. \nSince p and p ' end with the same state, p is also a pre.x of a complete execution.    \n\t\t\t", "proc_id": "2048066", "abstract": "<p>We present a technique for automatically adding fine-grain locking to an abstract data type that is implemented using a dynamic forest -i.e., the data structures may be mutated, even to the point of violating forestness temporarily during the execution of a method of the ADT. Our automatic technique is based on Domination Locking, a novel locking protocol. Domination locking is designed specifically for software concurrency control, and in particular is designed for object-oriented software with destructive pointer updates. Domination locking is a strict generalization of existing locking protocols for dynamically changing graphs. We show our technique can successfully add fine-grain locking to libraries where manually performing locking is extremely challenging. We show that automatic fine-grain locking is more efficient than coarse-grain locking, and obtains similar performance to hand-crafted fine-grain locking.</p>", "authors": [{"name": "Guy Golan-Gueta", "author_profile_id": "81490696196", "affiliation": "Tel Aviv University, Tel Aviv, Israel", "person_id": "P2839164", "email_address": "ggolan@tau.ac.il", "orcid_id": ""}, {"name": "Nathan Bronson", "author_profile_id": "81361595088", "affiliation": "Stanford University, Palo Alto, USA", "person_id": "P2839165", "email_address": "nbronson@cs.stanford.edu", "orcid_id": ""}, {"name": "Alex Aiken", "author_profile_id": "81100399954", "affiliation": "Stanford University, Palo Alto, USA", "person_id": "P2839166", "email_address": "aiken@cs.stanford.edu", "orcid_id": ""}, {"name": "G. Ramalingam", "author_profile_id": "81479640532", "affiliation": "Microsoft Research, Bangalore, India", "person_id": "P2839167", "email_address": "grama@microsoft.com", "orcid_id": ""}, {"name": "Mooly Sagiv", "author_profile_id": "81100150928", "affiliation": "Tel Aviv University, Tel Aviv, Israel", "person_id": "P2839168", "email_address": "msagiv@tau.ac.il", "orcid_id": ""}, {"name": "Eran Yahav", "author_profile_id": "81100285431", "affiliation": "Technion, Haifa, Israel", "person_id": "P2839169", "email_address": "yahave@cs.technion.ac.il", "orcid_id": ""}], "doi_number": "10.1145/2048066.2048086", "year": "2011", "article_id": "2048086", "conference": "OOPSLA", "title": "Automatic fine-grain locking using shape properties", "url": "http://dl.acm.org/citation.cfm?id=2048086"}