{"article_publication_date": "10-22-2011", "fulltext": "\n Sprint: Speculative Prefetching of Remote Data Arun Raman Greta Yorsh Martin Vechev Princeton University, \nUSA ARM, UK * ETH Zurich and IBM Research rarun@princeton.edu greta.yorsh@arm.com martin.vechev@gmail.com \nEran Yahav Technion, Israel * yahave@cs.technion.ac.il Abstract Remote data access latency is a signi.cant \nperformance bot\u00adtleneck in many modern programs that use remote databases and web services. We present \nSprint a run-time system for optimizing such programs by prefetching and caching data from remote sources \nin parallel to the execution of the original program. Sprint separates the concerns of exposing potentially-independent \ndata accesses from the mechanism for executing them ef.ciently in parallel or in a batch. In contrast \nto prior work, Sprint can ef.ciently prefetch data in the presence of irregular or input-dependent access \npatterns, while preserving the semantics of the original program. We used Sprint to automatically improve \nthe perfor\u00admance of several real-world Java programs that access re\u00admote databases (MySQL, DB2) and web \nservices (Face\u00adbook, IBM s Yellow Pages). Sprint achieves speedups rang\u00ading 2.4\u00d7 to 15.8\u00d7 over sequential \nexecution, which are comparable to those achieved by manually modifying the program for asynchronous \nand batch execution of data ac\u00adcesses. Sprint provides a simple interface that allows a pro\u00adgrammer to \nplug in support for additional data sources with\u00adout modifying the client program. Categories and Subject \nDescriptors D.3.4 [Programming Languages]: Processors Run-time environments General Terms Design, Languages, \nPerformance Keywords remote data, automatic, prefetching, paralleliza\u00adtion, speculation, batching, caching, \ncompiler, run-time, tool * This work was done while the author was at IBM Research. Permission to make \ndigital or hard copies of all or part of this work for personal or classroom use is granted without fee \nprovided that copies are not made or distributed for pro.t or commercial advantage and that copies bear \nthis notice and the full citation on the .rst page. To copy otherwise, to republish, to post on servers \nor to redistribute to lists, requires prior speci.c permission and/or a fee. OOPSLA 11, October 22 27, \n2011, Portland, Oregon, USA. 1. Introduction Web, business, and scienti.c programs have increasingly \nbe\u00adcome data bound. They issue large numbers of long latency data access requests long latency because \nthe data is often served by remote web services or databases. Owing to the disparity between CPU speeds \nand network latencies and bandwidths, these programs spend a signi.cant fraction of their execution time \nwaiting for the data access requests to be serviced. To improve the performance of such programs, program\u00admers \nexpend a lot of time and effort scheduling the requests in a way that minimizes the overall execution \ntime using schemes such as asynchronous execution, batching, and par\u00adallelization. It usually requires \nsigni.cant code rewriting, thereby obscuring the functional logic of the program, and often results in \nnon-portable performance gains. Ideally, the programmer should only be concerned with expressing the \nfunctional logic of the program, and allow the compiler and run-time to orchestrate the remote data requests \nef.ciently. 1.1 AddressingLatencyIssuesviaPrefetching A conventional way to overcome the problem of \ndata access latency is data prefetching [23, 31]. The idea is to issue asyn\u00adchronous data requests before \nthe data is really needed so that the data may be available locally when accessed by the program. The \ndesired characteristics of a prefetching mecha\u00adnism are (1) accuracy: make a good prediction about what \nre\u00admote data will be accessed by the program, (2) effectiveness: make the remote data available locally \nby the time the pro\u00adgram needs it, and (3) correctness: guarantee that prefetching does not affect the \nprogram s behavior. Prefetching has been widely studied in the microarchitecture community to hide the \nlatency between the processing core and the memory subsystem [6, 16, 21, 35]. Prefetching has also been \nused to hide the latency of a local .lesystem [4, 18]. The ratio of la\u00adtency of data access and latency \nof computation is relatively low in these domains. Consequently, prefetchers in these do- Copyright \n&#38;#169; 2011 ACM 978-1-4503-0940-0/11/10. . . $10.00 mains need run only slightly ahead of the main \nthread of computation, and overlapping just a single data access with computation often suf.ces. By \ncontrast, prefetchers for re\u00admote data must be able to overlap several remote data ac\u00adcesses since the \nratio of latency of data access and latency of computation is very high. Most prefetchers are history-based: \nthey analyze data ac\u00adcess patterns performed in the past, predict that future data accesses will follow \nsimilar patterns, and prefetch the corre\u00adsponding data [8, 10, 17, 18, 25]. While this approach works \nfor programs with regular data access patterns, such as array\u00adbased scienti.c programs, it is not effective \nfor programs whose data accesses depend on the input, are not structured in easily predicted patterns, \nor do not contain recurrences (that is, frequent reuse of the same remote data). In a departure from \nhistory-based prefetching, Chang and Gibson use speculative execution to allow programs to dy\u00adnamically \ndiscover future read accesses to disk [4]. In the presence of dependencies between accesses, their approach \noften causes misspeculation of future disk accesses, and spu\u00adrious disk accesses. Speculative parallelization \nschemes [9, 19, 33] provide a mechanism for detecting violations of con\u00adtrol and data dependencies and \nuse re-execution to guaran\u00adtee that all dependencies are respected. These schemes of\u00adfer hope for issuing \nmultiple remote data requests in paral\u00adlel, but have some disadvantages in our setting. In particu\u00adlar, \nviolation of dependencies that do not contribute to gen\u00aderating remote data requests may cause re-execution, \nthus re-executing some expensive remote access and hindering progress towards exposing other requests \nfor remote data. It is therefore important to distinguish between dependencies that matter for identifying \nremote data accesses, and those that matter for computation that uses the data returned by the accesses. \n 1.2 PrefetchingfromRemoteDataSources We propose an approach, implemented in a tool called Sprint, that \nseparates the concerns of exposing potentially\u00adindependent remote data accesses from the mechanism by \nwhich the overall completion time of the data accesses is re\u00adduced. To expose remote data requests, our \napproach relies on a prefetcher, automatically constructed from the origi\u00adnal program. To schedule these \nrequests ef.ciently, our ap\u00adproach relies on an execution engine (see Figure 1). Prefetcher The prefetcher \nis essentially a copy of the orig\u00adinal program, that executes concurrently with the original. The prefetcher \nand the original program issue remote data requests and cache the returned data locally in a shared data\u00adstructure. \nThe prefetcher executes faster than the original program, because it is executed by multiple threads \nin a speculatively-parallel manner, whenever resources are avail\u00adable. In the common case, the original \nprogram .nds that the data it requires is already available in the cache. By restricting the effects \nof the prefetcher to the cache, the behavior of the original program is preserved. By con\u00ad Well-defined \nAPI input output  (a) Original program interacts with a remote data source through a well-de.ned long-latency \nAPI (b) Original program interacts with a prefetcher via a data cache, managed by an execution engine, \nwhich executes remote data accesses in parallel or in a batch   Figure1. Conceptual model of execution \n(a) without Sprint (b) with Sprint structing the prefetcher from the original program, all depen\u00addencies \nare respected unless an explicit decision to speculate is made. The prefetcher is designed to respect \ndependen\u00adcies between remote data accesses, but may violate other de\u00adpendencies to expose potentially-independent \ndata accesses. This approach works well even for programs with irregular and input-dependent data accesses. \nExecution engine The execution engine manages the cache and automatically decides when to batch up accesses \nand when to issue them in parallel. Additionally, the execu\u00adtion engine is responsible for monitoring \nthe effectiveness of the speculative prefetcher and restarting the prefetcher if the speculation goes \nastray (because some remote data ac\u00adcesses were incorrectly dubbed as independent). Fortunately, monitoring \ncache misses in the execution of the original pro\u00adgram provides a low-overhead way to identify violations \nof dependencies between remote data accesses. If the remote data source may be modi.ed by other enti\u00adties \n(not the program executed under Sprint), then the exe\u00adcution engine keeps the cache in sync with the \ndata source using the mechanism of triggers and callbacks. If this mech\u00adanism is not supported by the \nAPI of the remote data source (databases usually support it, but remote services may not), then the programmer \nis responsible for guaranteeing that the program does not rely on any data consistency constraints to \nbe maintained by the remote data source.  Interaction with the programmer Our goal is to reduce the \nburden on the programmer by helping to avoid code rewriting that obstructs the functional logic of the \nprogram and results in non-portable performance gains. Towards this end, we designed a practical system \nthat can automatically perform speculative prefetching and optimization of remote data accesses, implemented \nusing certain standard APIs, such as JDBC and HTTP. Our system uses pro.ling (on rep\u00adresentative inputs \nsupplied by the programmer) and run-time monitoring methods (Section 5) to automatically identify re\u00admote \ndata accesses that are likely to be independent, and to decide when to start and stop speculative prefetching. \n 1.3 Contributions The main contribution of this paper is the design, imple\u00admentation, and evaluation \nof an integrated tool called Sprint. Sprint is the .rst automatic tool to reduce the latency of pro\u00adgrams \nthat perform multiple accesses to data from sources such as remote web services and databases, while \npreserv\u00ading the semantics of such programs. Sprint consists of the following components: Pro.ler that \nidenti.es potentially-independent remote data accesses to improve accuracy of speculation  Bytecode \ntransformer that uses the pro.ling infor\u00admation to automatically generate a program-speci.c prefetcher \nthat preserves the behavior of the original pro\u00adgram  Execution enginethat optimizes remote data accesses \nby executing them in parallel and in a batch, monitors the effectiveness of the prefetcher, and maintains \ncorrectness by keeping the local cache in sync with the remote data source  Plug-in supportfor newAPIs \nthat allows programmers to apply Sprint to new data sources without modifying the client programs  We \nused Sprint to automatically improve the performance of several Java programs that access remote databases \n(MySQL, DB2) and web services (Facebook, IBM s Yel\u00adlow Pages), and achieved speedups ranging from 2.4\u00d7 \nto 15.8\u00d7. Our approach is not a silver bullet for all latency issues with remote data sources. It is \ndesigned for programs that can be characterized by mostly read-only accesses, irregu\u00adlar or input-dependent \ndata access patterns, and very low computation latency to data access latency ratios. Our ex\u00adperimental \nevaluation in Section 6 shows that for programs in the target domain, our approach automatically provides \nperformance that is comparable to the performance achieved by manually modifying the program for asynchronous \nand batch execution of remote data accesses.  2. MotivatingExample Consider the example code shown in \nListing 1 (ignoring @Launch and @Speculate annotations on lines 16 and 26 for now). When executed, this \nprogram displays the manage\u00adment hierarchy rooted at the employee whose email address is the program \ninput. In line 27, the method buildTree is called to fetch the corresponding hierarchy subtree from the \nremote data source into a local data-structure of Node objects.  1 class Node { 2 staticint numNodes \n= 0; 3 Node tree; 4 5 Node buildTree (String email) { 6 Employee emp; 7 try { 8 emp = getEmployee(email); \n//Expensive remotedata access 9 } catch (EmployeeNotInDatabaseException e) { 10 System.err.println( \nEmployee + email + not found! ); 11 return null; 12 } 13 Node root = new Node(emp); 14 numNodes++; \n15 for (String reportee email : root.getReporteesEmail()) { 16 17 if (child != null) { 18 root.add(child); \n 19 child.setParent(root); 20 } 21 } 22 return root; 23 } 24 25 void main(String[] args) { 26 @Launch \nOptimist(buildTree); 27 tree = new Node().buildTree(args[1]); 28 display(tree); 29 } 30 } Listing1. \nExample of building a managerial tree The program uses a high-level API to access the remote data source. \nFrom the viewpoint of the program, the remote data source is just a mapping from keys to values. A remote \ndata access is a lookup of the value stored in the remote data source for a given key. In this example, \na key is the email address of an employee, and a value is the record of that employee, including the \nlist of email addresses of employees who directly report to the employee. In buildTree, the call to getEmployee \nin line 8 is expensive, because this method accesses the remote data source to fetch the details of the \nemployee (im\u00adplementation not shown). If the employee is not found, buildTree prints an error message \nand returns. Other\u00adwise, buildTree constructs a node that will be the root of the subtree that corresponds \nto the employee (line 13), in\u00adcrements the count of nodes in the tree (line 14), and iterates over the \ndirect reportees of the employee (line 15). Every it\u00aderation recursively builds the subtree that corresponds \nto the reportee (line 16) and updates the tree by linking the em\u00adployee and the reportee nodes (line \n18 and line 19). Figure 2 shows the hierarchy computed by a sample ex\u00adecution of this program. In this \nexecution, the program per\u00adformed a sequence of 9 remote data accesses, which corre\u00adsponds to the depth-.rst \ntraversal of the tree in Figure 2.  The total execution time of this program is dominated by the latency \nof the remote data accesses. Our goal is to reduce the total execution time by overlapping the round \ntrip times of remote data accesses whenever possible, without requiring the programmer to modify code. \nExisting APIs support parallel and batch access to remote data sources and thus provide a way to overlap \nround trips. The challenge is to identify as early as possible which remote data is accessed by the program, \nin the presence of dependencies among remote data accesses. We say that a remote data access t2 depends \non remote data access t1 if the key used by t2 is com\u00adputed from the value returned by t1. For example, \nthe remote access getEmployee(jacob) de\u00adpends on getEmployee(david), because the key jacob is computed \nusing the value returned by getEmployee(david). It is easy to see that the de\u00adpendencies in this example \nare structured as a tree that mimics the hierarchy shown in Figure 2. The longest chain of dependent \nremote accesses is of length 3, indicating potential for improvement upon the sequential execution. Note \nthat there are no dependencies between remote data accesses in different iterations of the loop in line \n15, but there are memory dependencies between the loop iterations due to updates of numNodes in line \n14 and the Node data\u00adstructure in line 18. In other words, there are two kinds of dependencies those \nthat are required to determine the key for the next remote data access, and those that are not. Existing \nmethods are ineffective in this setting because they do not distinguish between these two kinds of depen\u00addencies. \nFor example, parallelization of the loop in line 15 using Safe Futures [33] would end up executing all \nre\u00admote data accesses sequentially, because Safe Futures re\u00adspect all dependencies, including memory \ndependencies in lines 14 and 18. The speculative execution method proposed by Chang and Gibson [4] would \nspeculate the return value of the .rst call to getEmployee in line 8, leading to misspec\u00adulation of the \nsubsequent remote data accesses that depend on this value. Other methods (e.g., transactional memory \nwith abstract locking [24], Galois [19], batching [2, 14, 15]) would require the programmer to modify \nthe code or to spec\u00adify which dependencies are safe to ignore.  3. Sprint Architecture In this section, \nwe describe the conceptual architecture of Sprint in a platform-independent manner and highlight the \ndesign decisions that matter the most for effective prefetch\u00ading. Section 5 provides more details about \nour implementa\u00adtion of Sprint for Java, and shows an effective transformation of the code example in \nthe previous section. Sprint automatically transforms a program with multiple remote data accesses into \na well-performing program that Figure2. Dependencies between remote data accesses per\u00adformed during an \nexecution of the example program shown in Listing 1. Assuming suf.cient resources, Sprint can re\u00adduce \nthe execution latency to the height of the longest depen\u00addence chain of remote data accesses multiplied \nby a round trip latency. combines the bene.ts of parallel and batch execution of re\u00admote requests. Figure \n1 illustrates Sprint s system architec\u00adture. Figure 1(a) shows a system where a program interacts with \na remote data source through a well-de.ned API (the notion of well-de.ned is explained later), without \nSprint. Figure 1(b) shows the changes with Sprint. The Sprint bytecode transformer makes two versions \nof the original pro\u00adgram, the Optimist (O) and the Pessimist (P). P is nearly identical to the original \nprogram, except that at a certain point in its execution, it spawns O and communicates all live-in values \nfor O s execution (indicated by the arrow from Pessimist to Optimist in Figure 1(b)). The idea is for \nO to serve as a prefetcher for P and to issue remote data accesses as early as possible. Thus, it is \nimportant that O runs faster and stays ahead of P. For this, Sprint creates O by specula\u00adtively parallelizing \none or more loops or recursive methods in the original program. Listing 1 shows annotations at the program \npoints where O is launched (line 26) and O will be speculatively parallelized (line 16). For parallelized \nexecu\u00adtion of O, Sprint s execution engine includes an intelligent thread pool and task queue (Section \n3.1). O and P communicate via a data cache (inside the exe\u00adcution engine) that contains key-value pairs \n(Section 3.2). The key corresponds to the URL of some remote data, and the value corresponds to the remote \ndata. By virtue of O s runahead execution, P is likely to .nd that the remote data is available locally. \nSpeculative stores to memory (other than the cache) by O are dynamically privatized at run-time by Sprint, \nand the data cache maintains consistency with respect to the data source; this ensures the correctness \nof P s execu\u00adtion (Section 4). To execute multiple data accesses ef.ciently, the execu\u00adtion engine uses \nthe logic shown in Figure 4. It dispatches data accesses in parallel, or batches some accesses together \nin case the remote data source supports batch execution. In the expected case, O makes data available \nto P ahead of time through the data cache. The executions of O and P are overlapped in a pipelined fashion \ncausing the over\u00adall speedup in program execution time to be fundamentally limited only by the larger \nof (i) the length of the longest de\u00adpendence chain of remote accesses multiplied by a round trip latency \nand (ii) the time to execute the original code when all required data is available locally. In Figure \n2, assuming suf.\u00adcient resources, the overall execution latency of the program will be reduced from 9 \nround-trips to 3 round-trips to the re\u00admote data source. This is because O would have executed the subtrees \nrooted at james and joseph while P is executing the subtree rooted at jacob, thereby completely hiding \nthe latency of six out of the nine remote data accesses.  3.1 OptimistThreadPoolandTaskQueue The Optimist \nis executed by multiple threads in a thread pool (initially containing just one thread) that has the \nfollowing property: If there is no thread available to execute a task being enqueued, a new thread is \ncreated (without exceeding the maximum thread pool size that is speci.ed). A task is a unit of parallel \nwork that transitively results in a remote data access. For example, buildTree on line 16 in Listing \n1 is a task. In some applications, if all tasks in the task queue of the thread pool are treated equally, \nit might happen that O s threads spend most of their time executing data requests that are logically \nmuch later in the sequential execution. Consider the task graph shown in Figure 3 which may be generated \nby a search over tree-structured data. If O s threads execute subtrees rooted at T 4 and T 6, P misses \nin the cache frequently since it is executing logically earlier data requests. And if P terminates early \n(while O did not because of misspeculation), a large fraction of the prefetches turns out to be useless. \nTo increase the likelihood of O fetching at least those data items that will be used by P, the task queue \nis implemented as a priority queue which assigns higher priority to logically earlier tasks. For example, \na task created on iteration 1 of a loop is assigned higher priority than a task created on iter\u00adation \n2. In case tasks spawn more tasks in a nested fashion, a child task inherits the priority of the parent \ntask, with the priorities of siblings being decided in the logical program or\u00adder. Using the priority \ntask queue, tasks in the subtree rooted at T 1 will be executed before tasks in other subtrees, allow\u00ading \nT 3 to be overlapped with T 2. This results in a reduction in the total time to .nd the item. 3.2 DataCache \nThe data cache is the sole means of communication between O and P. Remote data accesses by bothO and \nP are recorded in the data cache. An entry in the cache is a pair of key and value. key corresponds to \nthe URL of some remote data. value corresponds to the remote data that is fetched from the URL. value \nhas metadata that indicates its state: absent, issued, or present. Upon a cache lookup, if the state \nis absent, then a remote data access is issued. If the state is issued, the caller is blocked until the \ndata is returned by the remote data source. If the state is present, data is returned from the cache. \nNote that both O and P interact with the cache in an identical Figure 3. Depending on timing of task \n(unit of parallel work) execution, several useless data prefetches may be is\u00adsued in place of useful \nones. A priority task queue prioritizes tasks that come earlier in the original program order thereby \nimproving the number of useful prefetches. Tasks are num\u00adbered according to their order in the original \nprogram. fashion. Consequently, either can fetch data for the other. In the uncommon case that O falls \nbehind P, the data fetched by P serves to accelerate O. Batch Execution The execution engine is also \nresponsi\u00adble for batching remote data accesses. Since there are many threads executing different parts \nof the program (in both O and P), many entries will be created in the cache for different remote data \naccesses. Adding another state called batched to the metadata of value allows the engine to aggregate \nall queries in the cache in the batched state, issue them all at once, and return the values appropriately \nthereby releasing the callers that were blocked. This capability of the exe\u00adcution engine frees the programmer \nfrom the onerous task of identifying queries to batch and writing code to match the return values of \neach query with the appropriate point in the original program. To avoid deadlock when the batch limit \nhas not yet been reached and the application will not issue any more queries, a batch .ush operation \nis inserted at the end of the transformed code region. This is described in more detail in Section 5.2.4. \nFigure 4 describes how a data request is processed. Prefetch Throttling O executes the program speculatively \nand may go down execution paths that differ from the origi\u00adnal program s execution, or prefetch remote \ndata that is never used. This may have negative effects such as contention for bandwidth to the remote \ndata source between P, O, and other entities that access the remote data source. To minimize such effects, \nthe execution engine can throttle the speculation by using information available in the data cache as \na proxy for the degree of misspeculation. Speci.cally, if the cache miss rate exceeds a threshold, then \nthe prefetcher is deemed to be unhelpful, and is shut down.  4. CorrectnessofSprintExecution In this \nsection, we de.ne what it means for a program to execute correctly under Sprint, and show how the Sprint \narchitecture described in Section 3 ensures correctness.  Figure 4. Data request processing algorithm: \nA data access request is serviced by the cache if data has been prefetched. Otherwise, a data request \nis issued if batch size equals one, else the request is queued up in a batch and the requesting thread \nwaits. When the batch becomes full, a batch request is issued; when it returns, waiting threads are noti.ed \nand data is cached. A batch is .ushed at the end of a transformed code region. 4.1 SprintGuarantees The \nSprint bytecode transformer provides the following two guarantees (Section 5.2 describes how): 1. Non-interference \nthrough memory: The Optimist and Pessimist execute in the same process, hence the same virtual memory \nspace. The Sprint bytecode transforma\u00adtion ensures that O s actions cannot affect P s memory state: (i) \nP interacts solely with objects created by itself, and (ii) O only updates the objects that it creates, \nbut may also read objects created by P (indicated by arrow from P to O in Figure 1(b)). 2. Suppression \nof externally-visible behavior: The trans\u00adformed program must generate the same sequence of externally-visible \nbehavior as the original program would have generated. The Sprint bytecode transformer ensures this by \neliding all side effecting operations from O. In practice, this means catching all exceptions, removing \nstatements such as System.out.print, and API op\u00aderations that may modify the remote data source. To iden\u00adtify \nsuch API operations, Sprint requires that the API be well-de.ned . For every method whose execution has \nany observable effects on the remote data, Sprint must know what are the keys corresponding to the modi.ed \nvalues.  4.2 Preliminaries A remote data source is a mapping M : K . V from keys to values. Let R be \na (possibly concurrent) program that accesses a remote data source M via a well-de.ned API. The API of \nthe remote data source supports operations for blocking lookup and update operations that can be described \nby key-value pairs r[k, v] and w[k, v], respectively. An execution of R is a sequence of transitions, \neach of which corresponds to an execution of a basis statement of R or a key-value pair that corresponds \nto a lookup or up\u00addate operation on M called by R. A transition (a, b, st, tid) consists of source state, \ntarget state, statement of R, and thread id. A state (h, s) of the program R contains a heap h and a \nmapping from thread id to the corresponding stack. We assume that M guarantees sequential consistency, \nand therefore, all the operations on M performed by some exe\u00adcution of R can be ordered. The operations \non M performed by an execution t of R form a sequence of key-value pairs def S(t, M)= (k1 ,v1 ), (k2 \n,v2 ),.... For any sequence S (of pairs or transitions), and for 0 = i< |S|, let S[i] denote the i-th \nvalue in the sequence S, and |S| the length of S. Let R ' be the result of transforming the original \nprogram R using the Sprint bytecode transformer. Let P and O denote the Pessimist and Optimist parts \nof R ' , respectively. For an execution t ' of R ' , we use tP and tO to denote the projection of t ' \nonto the statements in the P and O parts of R ' , respectively. Let t '' be a restriction of all states \nof tP to (hp,sp), where hp contains objects allocated by P (excluding the cache) and sp contains threads \nexecuting in P . We say that t '' is the Pessimist execution of t ' .  4.3 ProofofCorrectness THEOREM \n4.1 (Correctness of Sprint Execution). For every execution t ' of R ' , there exists an execution t of \nR such that t '' = t. In other words, a Sprint-ed execution t ' of a program R ' is correct when it can \nbe partitioned into non-interfering Pessimist and Optimist executions and the Pessimist of t ' is equivalent \nto some execution of the originalprogram R. Proof: A transition of t ' is an operation on M. t ' can \nbe partitioned into tP and tO precisely when the following conditions hold: 1. Every state (h, s) of \nt ' can be partitioned into (i) objects allocated by each of P and O, h = hp . ho . c, where c encapsulates \nan implementation of the abstract data type cache: K . V , and (ii) threads executing in each of P and \nO, s = sp . so 2. No transition (a, b, st, tid) of t ' such that st is in P accesses (ho,so) (in state \na)   3. All transitions (a, b, st, tid) of t ' such that st is in O preserve (hp,sp) 4. No transition \nof tO performs externally-visible opera\u00adtions (such as an update operation on M, an uncaught exception, \netc.) Guarantee 1 (non-interference throughput memory) pro\u00ad  vided by the Sprint bytecode transformer \nensures that Con\u00additions 1, 2, and 3 hold, while Guarantee 2 (suppression of externally-visible behavior) \nensures that Condition 4 holds. Thus, the execution t ' can be correctly partitioned into tP and tO . \nNote that this does not impose any constraints on the cache. t '' is the restriction of tP to (hp,sp). \nBy construction, P consists of the same statements as R. From this, and from the correctness of partitioning, \nwe can state that starting from the same original state, for each transition induced by a basis statement \n(a statement which does not interact with M) of R ' , there exists a trace t of R in which the same basis \nstatement induces the same transition. It remains to show that this holds for non-basis statements as \nwell. We .rst de.ne when values in the cache are in sync with the remote data source, assuming there \nare no updates to the remote data source by entities other than the Sprint-ed program. For a state a, \nM(a) and cache(a) denote the remote data source and the cache mapping in state a. DEFINITION 4.2 (Consistency \nof Cache and Remote Data Source). Let t ' be an execution of R ' with Sprint, as before. For all 0 = \ni =|t ' |, if t ' [i] is a lookup operation on M and a is the source state of the transition t ' [i], \nthen for all k . cache(a), if the metadata of k in cache(a) is present and the value of k in cache(a) \nis v, then the value of k in M(a) is v. For all 0 = i =|t ' |, if t ' [i] is an update operation on M, \nthen the metadata of k in cache(a) is set to absent and M(a) is updated, with both operations happeningatomically. \nA non-basis statement that updates M can only appear in tP since the update operations in O are elided. \nFurthermore, the statement does not change (hp,sp). For a statement that looks up M, the consistency \nof the cache and remote data source ensures that the value returned by the cache is the same as what \nwould have been returned by the remote data source; consequently the statement induces the same tran\u00adsition \nin both R ' and R. Thus, for the transitions induced by the non-basis statements of R ' , the same transitions \nare induced by the same statements in trace t of R. D  4.4 AccuracyofPrefetching Theorem 4.1 implies \nthat for every key-value sequence that may be generated by the Pessimist in some execution t ' of the \ntransformed program, there exists an execution t of the original program on the same input that generates \nthe same key-value sequence: S(t, M)= S(tP ,M). Accurate prefetching can now be formally de.ned. DEFINITION \n4.3 (Accuracy of Prefetching). Let t ' be an execution of R ' with Sprint. The speculation is accurate \nif and onlyif for every 0 = i< |t ' |, if t ' [i] is a transition of the Opti\u00admist anditperforms alookup \noperation on M described by thekey-valuepair [k, v],  then there exists a j, where 0 = i<j< |t ' |, \nsuch that t ' [j] is a transition ofthePessimist anditperforms a lookupoperation on M with k,  and there \nis no update operation on M with the key k in t ' between i and j.  Note that according to this de.nition, \nthe prefetch is considered accurate even if the cache was invalidated be\u00adtween i and j by other entities. \nThe sequence observed by an accurate Optimist consists of all the key-value pairs in S(tP ,M), albeit \npossibly in a different order, because the Optimist is a parallelized version of the original program. \nHowever, the Optimist is a speculatively parallelized ver\u00adsion of the original program. Misspeculation \nof control and data dependencies could result in a sequence S(tO ,M) that consists of key-value pairs \nthat are different from those in S(tP ,M). The expectation is that misspeculation is rela\u00adtively uncommon \n(as we show in the evaluation section) and S(tO ,M)= S(tP ,M).  4.5 CorrectnessintheFaceofRemoteUpdates \nSuppose that a program that uses the remote data source is executing with Sprint, while the data source \nis concurrently modi.ed by other entities. In this situation, a reordering of reads in O s execution \nmight observe an inconsistent state of the remote data and violate some invariant in P s execution. Suppose \nthat the invariant of P relies on some integrity property of the remote data, and that this integrity \nproperty is (atomically) guaranteed by all other entities that may modify the remote data source. Consider \nan execution in which the remote data is modi.ed by another entity between two out-of-order reads performed \nby O. It is possible that O observes a state of remote data that does not satisfy the integrity property, \nand is not observable in any execution of the original program. EXAMPLE 4.4. Consider a data source M \nwith initial state M = {a . 1,b . 2}. The data integrity that is to be maintained by all entities that \ninteract with the remote data source is M[b] >M [a]. This is a typical invariant, for example b could \nbe a summary of elements such as a. The following two programs execute concurrently using M: P1() { x=read(M,a); \ny=read(M,b); assert (y > x); } P2() { atomic{ write(M,a,2); write(M,b,3) } } In a concurrent execution \nof P1 and P2, the set of all possible key-value sequences that can be generated by P1 is: S = {((a, 1), \n(b, 2)), ((a, 1), (b, 3)), ((a, 2), (b, 3))} Note that the assertion holds in P1 in all three cases. \nSuppose that P1 is transformed and executed by Sprint while P2 also executes concurrently using M. If \nthe reads in the Optimist of P1 execute in parallel and happen to be served out of order by the data \nsource, the following sequence of events may occur at the data source:  read(b) // by Optimist of P1 \nwrite(a,2),write(b,3) // by P2 read(a) // by Optimist of P1 read(a),read(b) // by Pessimist of P1 The \nfollowing key-value sequence is generated by the ' Optimist of P1 in this execution: S =((a, 2), (b, \n2)). The data source invariant has been violated! If the remote data source supports trigger capabilities, \nSprint can solve the consistency problem by installing callbacks in the data source for certain operations \nthat update the data source. D Enforcing Consistency via Triggers and Callbacks Dur\u00ading the execution \nof a program under Sprint, whenever the Sprint execution engine performs a remote data access with some \nkey, it installs a callback in the remote data source that states notify me when the value that corresponds \nto this key is updated . Any write operation will cause the callback to be triggered and the remote data \nsource will notify the Sprint execution engine. Upon receiving noti.cation, the Sprint ex\u00adecution engine \nwill invalidate the appropriate entry in the cache. EXAMPLE 4.5. In Example 4.4, with the operation read(b) \nperformed by the Optimist of P1, the Sprint engine installs a callback on key b. The operation write(b,3) \nperformed by P2 triggers the callback on the key b. The execution engine then invalidates the entry for \nb in the cache. Consequently, read(b) by the Pessimist of P1 will miss in the cache, and the request \nwill be reissued. The sequence of key-value pairs observed by the Pessimist of P1 is ([a, 2], [b, 3]) \nand the assertion in P1 holds. D Data sources such as the MySQL database provide trig\u00adgers with the above \nsemantics that could be leveraged by Sprint. In the absence of trigger APIs, Sprint could ask the programmer \nwhether the consistency semantics arising from the read-read and read-write order relaxation is acceptable. \nOur current implementation focuses on programs with read\u00adonly accesses; the invalidation scheme described \nabove for data sources with trigger APIs will be integrated in future work.  5. Sprint Implementation \nIn this section, we present implementation details of the Sprint pro.ler that determines candidate methods \nto op\u00adtimize (Section 5.1), the Sprint bytecode transformer that transforms the program at run-time based \non the pro.ling re\u00adsults (Section 5.2), and the Sprint interface that a program\u00admer can implement to \nuse Sprint for optimizing programs that interact with data sources other than those that are cur\u00adrently \nsupported (Section 5.3). 5.1 Pro.ler Sprint uses pro.ling to determine suitable program sites to launch \nthe Optimist (Listing 1, line 26) and the program sites at which to speculate (Listing 1, line 16). Without \nany mod\u00adi.cations, the user executes the program of interest on a rep\u00adresentative input with the Sprint \npro.ler turned on. The pro\u00ad.ler records the calling contexts leading to remote data ac\u00adcess method invocations \n(such as the JDBC execute state\u00adment for executing SQL queries). The pro.ler also main\u00adtains loop-(or \nrecursion-) sensitive metadata. Speci.cally, it records whether a loop (or recursive method) transitively \ninvokes remote data access methods. Such statements within loops (or recursive method callsites) are \nmarked as candi\u00addates . At this point, there are two modes of operation: Interactive the user may prune \nthe candidate set  Automatic the pro.ler directly feeds candidate infor\u00admation to the bytecode rewriter \nIn the interactive mode, the user is presented with a list  of candidates. The user puts the @Speculate \nannotation inside a candidate that is expected to not have dependencies between remote data accesses \nemanating from it (e.g., List\u00ading 1, line 16). If the user annotates incorrectly there are dependencies \nbetween data accesses at run-time Sprint en\u00adsures correct program execution. In the fully automatic mode, \nSprint can infer the @Speculate annotation in one of two ways. Sprint can use a dynamic data.ow tracking \ntool called Pepe [29] that tracks the .ow of data through the remote data access methods to build a remote \ndata access dependency graph. The depen\u00addency information is maintained in the context of the candi\u00addates. \nReferring to the candidate loop between lines 15 21 in Listing 1, the pro.ler records the number of dependen\u00adcies \nbetween remote data accesses that are carried around the loop s back-edge. The frequency of dependencies \nis used to determine the pro.tability of transforming the loop. Pepe works for JDBC method invocations \nonly. To transform pro\u00adgrams that interact with other data sources, Sprint can trans\u00adform each and every \ncandidate (independently) and then observe the cache statistics on training runs to determine whether \nit is worthwhile to retain the transformed candidate. Candidates with high cache hit rates would be transformed \nwhile candidates with high miss rates would be ignored. Pepe is not integrated into Sprint as yet. To \nobtain the results in Section 6, the Pro.ler output a list of candidates for each program. The top candidate \nin each program was marked by the user with the @Speculate annotation; Sprint automatically transformed \nthe programs with that single an\u00adnotation.  5.2 BytecodeRewriter We describe in detail below the code \nmodi.cations for initi\u00adating O in P, constructing O, and preserving the semantics of the original program. \nThe Sprint bytecode rewriter uses the ASM class transformation library [3] to augment the classes that \nare loaded at run-time. The bytecode rewriter is writ\u00adten entirely in Java, with no modi.cations to the \nunderlying virtual machine. The code modi.cations are also illustrated on the running example in the \nform of high level Java state\u00adments for ease of understanding; in practice, the changes are done to Java \nbytecode.  5.2.1 InitiatingtheOptimist Algorithm1:Initiating the Optimist Input: Program : original \nprogram IR Input: CandidateSet : set of annotated statements Output: Program with Optimist initiation \nforeachcandidate . CandidateSet do defMethod . getDefMethod(Program, candidate) callsites . getCallSites(Program, \ndefMethod) foreachcallsite . callsites do argsCopy . cloneArgs(callsite) initiatePrefetcher(defMethod, \nargsCopy) P is constructed from the original Program by modify\u00ading Program to initiate O at each callsite \nof the method containing the candidate loop statement or recursive method invocation (see Algorithm 1). \nReferring to Listing 1, the immediate predecessor of buildTree is main. Listing 2 shows the change to \nmain. In practice, O is executed by a thread pool (Section 3.1), and initiating O means submitting a \ntask for execution by the thread pool (line 18 in Listing 2). The bytecode rewriter inserts code in P \nto encapsulate the method and its arguments as a task and submit the task to the thread pool. 1 class \nNode { 2 static int numNodes = 0; 3 Node tree; 4 5 Node buildTree (String email) { 6 Employee emp; 7 \n... 8 for (String reportee email : root.getReporteesEmail()) { 9 10 ... 11 } 12 return root; 13 } 14 \n15 void main(String[] args) { 16 Nodet= new Node(); 17 18 19 20 tree = t.buildTree(args[1]); 21 display(tree); \n22 } 23 }  Listing2. Initiating the Optimist  5.2.2 ConstructingtheOptimist O is constructed out of \nthe original program. Sprint performs two code transformations to address the issues of interfer\u00ad Algorithm2:Memory \nProtection Input: ClassSet : Set of classes loaded by program Output: Pessimist protected from Optimist \nforeachclass . ClassSet do addPrivateBoolean(class, createdByOptimist ) foreachmethod . class do copy \n. cloneMethod(method) entryBlock . getEntryBlock(method) addConditionalRedirectTo(entryBlock, copy) foreachinst \n. copy do ifinst is store to instance .eld then guard . createGuard (createdByOptimist, true) replace(inst, \nguarded(inst, guard)) ifinst is store to static .eld then delete(inst) ifinst is store to array then \nbb . getBasicBlock(inst) callinst . createCallInst(lookupMap, arrayOwnershipMap) addBefore(bb, inst, \ncallinst) guard . createGuard (callinstReturnVal, true) replace(inst, guarded(inst, guard))  ence of \nO and P through client program memory, and se\u00adquence of side effects that are visible to the external \nworld. The general approach is to create two versions of every method in a class, one for use by O and \nthe other by P. The design choice of method duplication is motivated by a space\u00adtime tradeoff, namely \nthat it allows P to be almost as fast as the original sequential program because P s code remains nearly \nidentical to the original program, at the cost of having multiple copies of each method. Memory Protection \n(MP) Transformation All writes to class members are protected by guards in the O version of each method. \nThe details of the MP transformation vary de\u00adpending on whether the type is an array (see Algorithm 2). \nNon-array types are discussed .rst. Listing 3 shows the Node class from earlier listings after the MP \ntransformation. A new .eld createdByOptimist (line 4) is added to the class to indicate whether the current \ninstance was allocated by O. This .eld is set during object allocation (lines 7 8). All writes by O to \nthe instance .elds of a class are guarded by ownership checks (for example, the write of tree by O in \nmain Optimistic on lines 42 44). O is allowed to write only if the method is invoked on a class instance \nallocated by O(createdByOptimist is true). Writes to static class variables are suppressed in the O versions \nof methods (line 25). Sprint uses a different strategy for array elements since the array type cannot \nbe extended to incorporate ownership information. For each array allocated by the original pro\u00adgram, \nSprint allocates a variable that maintains ownership metadata that is updated at the time of array creation. \nSprint maintains a map from array to ownership metadata. This map is used to lookup ownership information \nwhen an ar\u00adray is being updated. Sprint uses an optimized multi-level lookup table to reduce the overhead \nof this operation [12].  1 class Node { 2 static int numNodes = 0; 3 Node tree; 4 5 6 Node() { 7 \n 8 9 }  10 11 Node buildTree (String email) { 12 if (Thread.group.equals(SPRINT TGRP)) 13 return main \nOptimistic(args); 14 ... 15 } 16 Node buildTree Optimistic (String email) { 17 Employee emp; 18 try { \n19 emp = getEmployee(email); //Expensiveremotedata access 20 } catch (EmployeeNotInDatabaseException \ne) { 21 System.err.println( Employee + email + not found! ); 22 return null; 23 } 24 Node root = new \nNode(emp); 25 26 for(String reportee email : root.getReporteesEmail()) {  27 Node child = buildTree(reportee \nemail); 28 if (child != null) { 29 root.add(child); 30 child.setParent(root); 31 } 32 } 33 return root; \n34 } 35 36 void main(String[] args) { 37 if (Thread.group.equals(SPRINT TGRP)) 38 return main Optimistic(args); \n39 ... 40 } 41 void main Optimistic(String[] args) { 42 43 44 45 display(tree); 46 } 47 } Listing3. \nProtecting shared memory Externally-Visible Side Effect Protection (SEP) Transfor\u00admation To prevent O \nfrom performing operations that re\u00adsult in externally-visible side effects, all such operations (Listing \n3, line 21) are elided from the O version of each method. Sprint maintains a database of methods to be \nelided. Another component of side effect protection is exception trapping. Exceptions may be thrown during \nthe course of O s execution that may not have occurred during the execution of the original program. \nTo ensure that such exceptions do not escape to the user, the prefetch initiation method invocation is \nwrapped in a try-catch block (Listing 2, lines 17 19). In a future implementation, wrapping can be performed \nat .ner granularities in the control .ow graph in order to allow O to make useful progress beyond local \nexceptions. Input Operation Transformation Reading from .les and other input operations are typically \nouter loop activities, whereas Sprint typically optimizes inner loop nests/recur\u00adsions that use the input \ndata. Presently, all input opera\u00adtions are elided from O similar to the SEP transformation above. Only \nP performs such operations. In case there is an input operation in the optimized loop, a dependence on \nwhich leads O astray, in the current implementation O will be restarted by the execution engine after \nobserving several misses in the cache. If dependencies on input operations are expected to be frequent \nin the optimized loops/recursions, an alternative implementation could synchronize O and P on input operations. \nRecent work proposes a system that spec\u00adulates on user input in the context of web prefetching [22] this \nis discussed in Section 7.  5.2.3 OptimizingtheOptimist 1 class Node { 2 staticint numNodes = 0; 3 \nNode tree; 4 private boolean createdByOptimist; 5 6 Node() { 7 if (Thread.group.equals(SPRINT TGRP)) \n8 createdByOptimist = true; 9 } 10 11 Node buildTree (String email) { 12 if (Thread.group.equals(SPRINT \nTGRP)) 13 return main Optimistic(args); 14 ... 15 } 16 Node buildTree Optimistic (String email) { 17 \nEmployee emp; 18 try { 19 emp = getEmployee(email); //Expensive remotedata access 20 } catch (EmployeeNotInDatabaseException \ne) { 21 return null; 22 } 23 Node root = new Node(emp); 24 for (String reportee email : root.getReporteesEmail()) \n{ 25 26 27 if (child != null) { 28 root.add(child); 29 child.setParent(root); 30 } 31 } 32 return root; \n33 } 34 35 void main(String[] args) { 36 if (Thread.group.equals(SPRINT TGRP)) 37 return main Optimistic(args); \n38 ... 39 } 40 void main Optimistic(String[] args) { 41 Node temp = new Node().buildTree(args[1]); 42 \nif (createdByOptimist) 43 tree = temp; 44 display(tree); 45 } 46 } Listing4. Optimizing the Optimist \n(Prefetcher)  For O to execute faster than P, Sprint spawns multiple invocations of a Sprint annotation \nsite optimistically in par\u00adallel (Listing 4, lines 25 26). Details of OptimistTpool (thread pool) creation \nare left out with the note that it hap\u00adpens when the Java agent is loaded. The thread executing the continuation \nof the spawned fu\u00adture does not block on the future returned by the submit call (Listing 4, lines 25 \n26). Instead, Sprint speculates a re\u00adturn value. In the current implementation, the speculated val\u00adues \nare the equivalent of the null value for different types. A future implementation could use the results \nof pro.ling or memoize values from prior invocations for more advanced speculation.  5.2.4 AvoidingDeadlockDuetoBatching \nBatch execution and the interaction of the Optimist and the Pessimist via the cache introduce the possibility \nof deadlock: The execution engine builds a batch of queries as the pro\u00adgram issues the queries; however, \nthat batch may remain in\u00adcomplete once the program issues all the queries that it will ever issue. In \nthe absence of an appropriate mechanism, the program will wait for the queries to return while the execu\u00adtion \nengine will wait for the program to issue more queries to .ll the batch, thus resulting in deadlock. \nTo address this, Sprint inserts a .ush batch operation at the top of the im\u00admediate postdominator of \nthe transformed loop or recursive method in P (the Pessimist). This ensures that the last, po\u00adtentially \nincomplete, batch is forced to execute even though the batch may not be full, thus allowing P (and hence \nthe Sprint-ed program) to make progress.  5.3 ExtendingSprint Sprint currently supports data sources \nthat are accessed via the Java Database Connectivity (JDBC) API and the Java URLConnection API. The implementation \nof the data re\u00adquest processing algorithm in Figure 4 is fully parameterized with respect to the key \nand value types. This allows arbitrary remote data access APIs to be used with the data request processing \nalgorithm in a straightforward manner using the Cache and Batcher interfaces (see Listing 5). To extend \nSprint to support other data sources, the pro\u00adgrammer must implement the Batch interface. This is be\u00adcause \ndifferent data sources differ in the types of queries that can be batched, the means to prepare a batch, \nand the means to execute a batch. The Batch interface abstracts these de\u00adtails away from the batching \nlogic, allowing the programmer to just supply the data source speci.c batch preparation and execution \ncode. For example, a JDBCBatch implementa\u00adtion of the execute method of the Batch interface for a DB2 \ndatabase involves preparing a batch statement via conn.prepareStatement and executing the batch via stmt.executeDB2QueryBatch. \nThe Batch interface is never used by the programmer; it is used internally by the data request processing \nalgorithm. Note that the programmer must implement the Batch interface only once for each new data source \naccess API; the implementation can be reused across all programs that use the same API. The Cache and \nBatcher interfaces are shown only to illustrate their pa\u00adrameterization; the programmer remains blissfully \nunaware of their existence. 1 public interface Cache <K extends Object, V extends Object> { 2 /**Returnthe \ncached value corresponding tothekey*/ 3 V get(.nal K key); 4 /**Insert avalueto cachecorresponding tokey*/ \n5 V put(.nal K key, .nal V value); 6 /**Remove cached value corresponding to key*/ 7 void remove(.nal \nK key); 8 /**Flush the cache*/ 9 void clear(); 10 /**If cached entry corresponding tokey already exists,then \n11 * return entry; else cache value*/ 12 V putIfAbsent(K key, V value); 13 } 14 15 public interface Batcher<T \nextends Batchable<R>,R> { 16 /**Add offered element to batch*/ 17 R add(T obj); 18 } 19 20 public interface \nBatch<T extends Batchable<R>,R> { 21 /**Execute batch*/ 22 void execute(); 23 /**Returntrueifitwaspossibletoadd \nthe offered elementtothebatch, elsefalse*/ 24 boolean offer(T obj); 25 /**Returntrueifbatchisfull andisthe \n.rstcaller,else 26 *false.Mustbeinvoked only after 'offer'returnsfalse.*/ 27 boolean isFull(); 28 /**Returntrueifbatchiscurrently \nexecuting*/ 29 boolean isExecuting(); 30 } Listing5. Sprint interfaces for extensibility   6. Evaluation \nSprint is implemented in Java, and is evaluated on several client programs and data sources. Table 1 \nlists the different data sources (with abbreviated names we use in the rest of the paper), the APIs used \nto access them, and the network (local vs remote) between the client machine and the data source. Table \n2 lists the client programs (with abbreviated names we use in the rest of the paper), brief descriptions \nof each, their core algorithms, the data sources with which they interact, and their input sizes. The \nprograms that interact with YP and DB2 make use of the standard Java URLCon\u00adnection and JDBC APIs respectively. \nThe Facebook client programs use the RestFB API [1] that itself is a Java client of the Facebook Graph \nand REST APIs and Java URLCon\u00adnection. All client programs are written in Java (1.6) running on Linux \n2.6.32. The client machine is equipped with a dual core Intel Core 2 Duo processor running at 2.1 GHz \nwith 32KB (I) and 32KB (D) private L1 caches and a 2MB shared L2 cache, and 4GB of DDR2 800MHz RAM. Reported \nnumbers are averages over .ve runs. For the client programs and data sources studied, the round trip \nlatency to a data source and back dominated overall latency when compared to the la\u00adtency of processing \nrequests at the data source.    Supports Data Source (Abbreviation) API Network Batching   IBM \ns Yellow Pages Java URLConnection \u00d7 LAN Web Service (YP) Java Database DB2 Database (DB2) LAN Connectivity \n(JDBC) Facebook Web Service (FB) Facebook Graph API WAN Table 1. Data sources with which the Sprint-ed \nprograms timist goes to waste (see Table 5). Prioritized task execution prioritizes data requests that \ncome earlier in the original pro\u00adgram order and increases the likelihood that useful data is prefetched. \nPrioritized task execution improves Sprint s per\u00adformance by 25.6% in the case of ES (see Figure 5). \n20x FC interact 6.1 Speedup Figure 5 shows the speedup obtained using Sprint. The base\u00ad line for all \nspeedup numbers is the wall-clock execution time of the original unmodi.ed sequential program. The execu\u00adtion \ntime of the optimized versions includes the overhead Speedup over original execution 15x Original Sprint \nSprint-Batch 10x Sprint-Priority Manual Opti. 5x due to run-time bytecode rewriting. The Manual Opti. \nbar represents the speedup obtained by manually rewriting the original sequential program for asynchronous \nexecution (via Java Futures) and batch execution. Sprint achieves the same performance improvements without \nneeding any code rewrite. Finally, because the programs are not CPU bound, the speedups obtained (from \n2.4\u00d7 to 15.8\u00d7) are not limited by the number of cores (two) on the evaluation platform. Speci.cally, \nthe number of threads in the Optimist s thread pool grows according to whether a thread is available \nto exe\u00adcute a task being enqueued. This allows many (> 2) remote data accesses (contained in the tasks) \nto be concurrently in .ight. Batching Optimization For data sources that support batching, data requests \ncould be accelerated beyond the number of simultaneous connections to the data source, while also enabling \nthe data source s query optimizer to plan a better execution of the queries. Particularly in the case \nof FC (Facebook), the large reduction in the number of round trips via batching yields a huge bene.t. \nThe batch size was arbitrarily set to 100. The interaction of batch execution with parallel execution \nresults in a complex performance model and merits investigation [28]. Prioritized Task Execution In the \ncase of Employee Search (ES), the target loop is responsible for searching a tree and the loop terminates \nas soon as the item be\u00ading searched is found. Sprint s control speculation mecha\u00adnism speculates that \nthe loop will continue executing; con\u00adsequently the Optimist traverses parts of the search space that \nare not accessed in the Pessimist s search for the given input. Hence, a signi.cant fraction of prefetching \nby the Op\u00ad 0x Figure 5. Speedup of automatically Sprint-ed execution over original. Bene.ts of batching \nand prioritized execution are also shown. Sprint s performance gains are comparable to the best manual \noptimizations which include both asyn\u00adchronous and batch query execution but avoid Sprint s du\u00adplicate \ncomputation overhead. 6.2 ProgramCharacteristics Two fundamental program characteristics limit speedup: \n1. Ratio of remote data access latency and computation la\u00adtency: The higher the amount of time a program \nspends in remote data accesses compared to time spent in comput\u00ading the addresses of the accesses and \nother operations, the more scope Sprint has to improve program execution. 2. Length of remote data access \ndependency chains: The length of the longest dependency chain times a single remote data access latency \nis the lower bound on the time to execute all the remote data accesses in a program. Table 3 shows the \nlength of the longest chain for each program. Assuming in.nite resources and perfect scalability of the \n remote data source, Table 3 shows the theoretical upper bound on achievable speedup.  6.3 CacheBehavior \nTable 4 shows the number of cache hits, misses, and waits (requests that had already been issued by O \ncausing P to wait). Sprint converts a lot of remote accesses by the original program into local accesses \nto the cache in the Sprint-ed program. Benchmark (Abbreviation) Description Algorithm Data Source Input \nSize (in number of remote accesses) Management Hierarchy (MH) Builds manager-employee relationship of \norganization Tree Building YP 766 Employee Search (ES) Finds employee meeting certain search parameters \nTree Search YP 293 Citations Count (CC) Aggregates citations of a research group Tree Traversal DB2 502 \nBibliography Aggregation (BA) Aggregates bibliography under each manager in research organization Tree \nBuilding, Traversal YP, DB2 1268 Friend Connectivity (FC) Displays connectivity of Facebook friends circle \nTransitive Closure of Graph FB 401 Table2. Benchmark details Benchmark Remote Access Time (Percentage \nof Total) Longest Dependence Chain Length Upper bound on Speedup MH 97.42% 4 31.36x ES 97.17% 7 19.41x \nCC 99.92% 2 209.16x BA 98.27% 4 49.02x FC 98.45% 2 48.99x Table 3. Fraction of total execution time \nspent in remote data accesses and length of longest dependency chain limit the speedup achievable by \nSprint.  Cache Benchmark Accesses Hits Waits Misses Miss %    MH 766 747 10 9 1.12% ES 293 197 \n48 48 16.38% CC 502 202 168 132 26.29% BA 1268 949 178 141 11.12% FC 401 394 0 7 1.75% Table4. Cache \nstatistics: Waits denote accesses by the Pes\u00adsimist that had already been issued by the Optimist but \nhad not yet been serviced by the data source, causing the Pes\u00adsimist to wait for the data. Table 5 shows \nthe distribution of useful and useless data prefetched by O. The ratio of useful prefetches to the total \nnumber of accesses shows that Sprint successfully prefetched a signi.cant fraction of the used data. \nThe cause of useless prefetches in Employee Search has already been discussed in Section 6.1 under Prioritized \nTaskExecution.  Prefetches Benchmark Total Useful Useless Useful %    MH 757 757 0 100.00% ES 714 \n245 469 34.31% CC 370 370 0 100.00% BA 1127 1127 0 100.00% FC 394 394 0 100.00% Table 5. Cache statistics: \nSplit of prefetches according to use by Pessimist. Sprint successfully prefetches a signi.cant fraction \nof the used data.  6.4 ComparisonwithSequentialPrefetching To understand the importance of parallel \nprefetching by many Optimist threads as opposed to sequential prefetching by a single thread, we implemented \na sequential prefetcher and studied its performance using the FC (Facebook) ap\u00adplication. Sequential \nprefetching yields 2% speedup over original program execution. By contrast, parallel prefetch\u00ading unlocks \nan order of magnitude (15\u00d7) speedup. The fun\u00addamental reason for the performance difference is: Sequen\u00adtial \nprefetching is limited by the time to process all nodes accessed in a data structure (work). Therefore, \nsequential prefetching can reduce overall latency by at most a constant factor of the total work. Parallel \nprefetching is limited by the time to process the nodes in the longest dependency chain (depth), which \ncan be signi.cantly smaller than the total work. Parallel prefetching exploits the parallelism inherent \nin the processing of different parts of a data structure.  7. RelatedWork Batching The idea of batching \nis to convert several round trips into one, and thereby amortize the round trip cost over more data. \nRelated remote data access calls are not per\u00adformed at the point the client requests them, but are instead \ndeferred until the client actually needs the value of a result. By that time, a number of deferred calls \nmay have accumu\u00adlated and the calls are sent all at once, in a batch [2, 14, 15]. The major disadvantage \nof these batching proposals is that they require the programmer to rewrite the code (both client\u00adside \nand server-side) in non-trivial ways that typically ob\u00adscure the program logic. Parallelization Parallelization \nexposes independent re\u00admote data accesses and overlaps their round trip latencies. Manual parallelization \nusing locks is error prone. Transac\u00adtional memories with abstract locking could be used to simplify the \ntask of parallelization [24]. However, both ap\u00adproaches often require the relaxation of the original \nseman\u00adtics of the program. For example, assume that the for-loop on lines 15 21 in Listing 1 is parallelized. \nSynchronized up\u00addates to the root node on line 18 do not guarantee that the original program order is \npreserved. Unordered iterators in Galois have the same problem [19]. Ordered iterators could be used \nbut that would serialize execution. Further, Galois still requires the programmer to implement synchronized \nac\u00adcess to data structures. Speculative parallelization systems such as Safe Futures [33] and Spice [27] \nare fully automatic; unfortunately, the memory dependencies between iterations cause iterations to be \nre-executed thereby resulting in the re\u00adexecution of the expensive remote data accesses. In contrast \nto thread-level speculation schemes, Sprint does not moni\u00adtor all memory reads and writes and does not \nabort on all con.icts. Finally, none of these techniques incorporate batch execution where possible. \nProgram Slicing Conceptually, program slicing could be used to determine the slice of the program that \nis required to execute remote data accesses [13, 30]; this slice could be executed ahead of the remainder \nof the program thereby overlapping remote data communication with computation. Furthermore, automatic \nparallelization tools could be used to optimize the prefetcher slice . However, automatic slicing and \nparallelization tools rely on analyses (such as pointer analysis) being interprocedural. These analyses \nare typically imprecise for complex code. Furthermore, these analyses may have to be applied at the bytecode \nor binary level when recompilation of source code is not an option. Advances in the program slicing front \ncould reduce the amount of duplicate computation performed by a Sprint-ed program. Memory Prefetching \nTo mask the latency of servicing memory operations that miss in the cache, prefetching via pre-execution \nhas been proposed. This approach uses com\u00adpiler analyses to generate a backward slice ( p-slice ) from \nthe address of each target memory operation. All stores in the backward slice are elided. A p-slice is \nscheduled for exe\u00adcution by a helper thread that acts as a prefetcher for a delin\u00adquent load [32, 35]. \nLike the prefetching approach proposed by Chang and Gibson [4], the elimination of .ow depen\u00addencies \nfrom stores to loads causes divergence between the main thread and the helper thread, requiring frequent \nsyn\u00adchronization. In contrast to p-slices, Sprint generates Opti\u00admist threads that are long running and \nwithout store elisions thereby respecting most dependencies and avoiding resyn\u00adchronization costs.  \nLee et al. propose a helper thread construction algorithm that privatizes only array locations that are \nwritten in the backward slices from the addresses of the target memory operations, and constructs a single \nhelper thread [20]. Sprint handles both arrays and records, and constructs multiple helper threads. Additionally, \nthe technique proposed by Lee et al. is restricted to regions with a single live-in; Sprint does not \nhave such a restriction. Cooksey et al. propose content-based prefetching, a tech\u00adnique that scans data \nin cache lines to identify addresses and issues prefetch requests for those addresses [7]. By con\u00adtrast, \nSprint pre-computes addresses that will most likely be accessed this approach is more suitable for the \ncase when the ratio of remote access latency and computation latency is high (as in the domain of interest). \nWhereas content-based prefetching traverses recursive data structures sequentially, Sprint s unique code \ntransformation enables the prefetcher to traverse data structures in parallel. This gives Sprint a sig\u00adni.cant \nperformance advantage. Sprint s execution engine is unique to the application do\u00admain. Most memory prefetching \ntechniques assume hard\u00adware support for fast pre-execution thread initialization, cache line scanning, \netc., whereas Sprint runs programs on commodity systems. Another way of viewing memory prefetching is \nthat it can serve to reduce the request process\u00ading latency at the data source by reducing last level \ncache misses, whereas Sprint reduces the total round trip network latency by overlapping the latencies \nof multiple data requests in the client. As mentioned in Section 6, the round trip la\u00adtency (and not \nthe processing time at the data source) was the dominant factor in the overall application latency for \nthe ap\u00adplication and data source combinations studied in this paper. To cover the space of diverse workloads, \nSprint and memory prefetching can be combined in a complementary fashion. I/O Prefetching To mask the \nlatency of .lesystem access, prefetching via history-based prediction [8, 18] and via pre\u00adexecution [4, \n5, 34] has been used. As discussed in Section 1, history-based prefetchers work only for programs with \nreg\u00adular data access patterns. When accesses lack regularity, as in the domain of interest, history-based \nprefetchers cannot help. SpecHint speculates future I/O accesses [4]. In the presence of dependencies \nbetween accesses, SpecHint of\u00adten causes misspeculation of future disk accesses and spu\u00adrious disk accesses. \nBy contrast, Sprint generates Optimist threads that are long running without store elisions thereby respecting \nmost dependencies and avoiding resynchroniza\u00adtion costs. Other pre-execution approaches [5, 34] rely \non slicing techniques similar to the one used to construct mem\u00adory prefetchers, and thus suffer from \nthe problems discussed earlier. Sprint uses speculative parallelization to avoid these shortcomings. \nIn those pre-execution approaches, program slicing is used to construct a single prefetcher thread per \nmain thread. Sprint s code transformation creates several useful parallel prefetcher threads per main \nthread that can yield signi.cantly better performance. Patterson and Gib\u00adson modify programs to insert \nprefetch hints to the .lesys\u00adtem [26]. Koller and Rangaswami propose I/O deduplica\u00adtion that reduces \nduplication of data on disk by introducing a content addressable cache that is indexed on write oper\u00adations \nto avoid writing duplicate data and to improve write latency; read operations are cached based on history \n[17]. By contrast, Sprint is designed to reduce the latency of read op\u00aderations that are not amenable \nto history-based caching and prefetching. Web Prefetching Prefetching based on cumulative usage statistics \nat the client and server of pages linked via hy\u00adperlinks in a webpage has been proposed [10, 25]. The \nshortcomings of history-based approaches have already been discussed (Section 1). Mickens et al. proposed \nCrom, a JavaScript speculation engine to accelerate rich web appli\u00adcations [22]. Sprint and Crom perform \ncomplementary spec\u00adulation. Crom speculates on user activity rather than results of data accesses [22]. \nSprint speculates on the results of data accesses. Referring back to the discussion on input op\u00aderations \nin Section 5.2.2, Crom can speculate input events while Sprint can accelerate processing of each input \nevent handler resulting in an expected multiplicative performance improvement. Eden et al. propose to \nmodify the browser to enable a user to indicate future webpage accesses via a click [11]. The browser \nthen prefetches the page so that the page is available by the time the user navigates to the page. Sprint \nis entirely transparent to the user.  8. Conclusion Many modern programs spend a signi.cant portion \nof their execution time waiting for data from remote data sources. Sprint automatically reduces the total \nlatency of such pro\u00adgrams while preserving their semantics. Sprint provides speedups between 2.4\u00d7 to \n15.8\u00d7 on a set of applications that access different data sources. Sprint combines both parallel and \nbatch execution of remote data accesses. Sprint extends the state-of-the-art in prefetching for irregular \nand input\u00addependent data access patterns. Indeed, the techniques pre\u00adsented here are applicable in other \ncontexts such as prefetch\u00ading from disk. Since the system is speci.ed in terms of a data access API, \nporting Sprint to other applicable contexts is just a matter of specifying the relevant API in that context. \n  Acknowledgments We thank Mark Wegman and Nick Mitchell for their inputs during various stages of \nthis work. We also thank the anony\u00admous reviewers for their insightful feedback.  References [1] M. \nAllen. RestFB: Facebook Java api. http://restfb.com. [2] P. Bogle and B. Liskov. Reducing cross domain \ncall overhead using batched futures. In Proc.ofOOPSLA 94. [3] E. Bruneton, R. Lenglet, and T. Coupaye. \nASM: A code manipulation tool to implement adaptable systems. In Adaptable and ExtensibleComponent Systems, \n2002. [4] F. Chang and G. A. Gibson. Automatic I/O hint generation through speculative execution. In \nProc.ofOSDI 99. [5] Y. Chen, S. Byna, X.-H. Sun, R. Thakur, and W. Gropp. Hiding I/O latency with pre-execution \nprefetching for parallel applications. In Proc.ofSC 08. [6] J. D. Collins, D. M. Tullsen, H. Wang, and \nJ. P. Shen. Dynamic speculative precomputation. In Proc.ofMICRO 01. [7] R. Cooksey, S. Jourdan, and D. \nGrunwald. A stateless, content-directed data prefetching mechanism. In Proc. of ASPLOS 02. [8] K. M. \nCurewitz, P. Krishnan, and J. S. Vitter. Practical prefetching via data compression. In Proc.ofSIGMOD \n93. [9] C. Ding, X. Shen, K. Kelsey, C. Tice, R. Huang, and C. Zhang. Software behavior oriented parallelization. \nIn Proc. of PLDI 07. [10] D. Duchamp. Prefetching hyperlinks. In Proc. of USENIX 99. [11] A. N. Eden, \nB. W. Joh, and T. Mudge. Web latency reduction via client-side prefetching. In Proc.ofISPASS 00. [12] \nC. Flanagan and S. N. Freund. The RoadRunner dynamic analysis framework for concurrent programs. In Proc. \nof PASTE 10. [13] S. Horwitz, T. Reps, and D. Binkley. Interprocedural slicing using dependence graphs. \nIn Proc.ofPLDI 88. [14] A. Ibrahim, M. F. II, W. R. Cook, and E. Tilevich. Remote batch invocation for \nweb services: Document-oriented web services with object-oriented interfaces. In Proc. of ECOWS 09,. \n[15] A. Ibrahim, Y. Jiao, E. Tilevich, and W. R. Cook. Remote batch invocation for compositional object \nservices. In Proc. ofECOOP 06,. [16] D. Kim and D. Yeung. Design and evaluation of compiler algorithms \nfor pre-execution. In Proc.ofASPLOS 02. [17] R. Koller and R. Rangaswami. I/O deduplication: Utilizing \ncontent similarity to improve I/O performance. ACM Transactions on Storage(TOS), 2010. [18] D. Kotz and \nC. S. Ellis. Practical prefetching techniques for parallel .le systems. In Proc.ofPDIS 91. [19] M. Kulkarni, \nK. Pingali, B. Walter, G. Ramanarayanan, K. Bala, and L. P. Chew. Optimistic parallelism requires abstractions. \nIn Proc.ofPLDI 07. [20] J. Lee, C. Jung, D. Lim, and Y. Solihin. Prefetching with helper threads for \nloosely coupled multiprocessor systems. IEEETransactions onParallelandDistributedSystems, 2009. [21] \nC.-K. Luk. Tolerating memory latency through software\u00adcontrolled pre-execution in simultaneous multithreading \nprocessors. SIGARCHComputer ArchitectureNews, 2001. [22] J. Mickens, J. Elson, J. Howell, and J. Lorch. \nCrom: Faster web browsing using speculative execution. In Proc. of NSDI 10. [23] T. C. Mowry, A. K. Demke, \nand O. Krieger. Automatic compiler-inserted I/O prefetching for out-of-core applica\u00adtions. In Proc.ofOSDI \n96. [24] Y. Ni, V. S. Menon, A.-R. Adl-Tabatabai, A. L. Hosking, R. L. Hudson, J. E. B. Moss, B. Saha, \nand T. Shpeisman. Open nesting in software transactional memory. In Proc. of PPoPP 07. [25] V. N. Padmanabhan \nand J. C. Mogul. Using predictive prefetching to improve world wide web latency. ACM SIGCOMMComputer \nCommunication Review, 1996. [26] R. H. Patterson and G. A. Gibson. Exposing I/O concurrency with informed \nprefetching. In Proc.ofPDIS 94. [27] E. Raman, N. Vachharajani, R. Rangan, and D. I. August. Spice: speculative \nparallel iteration chunk execution. In Proc. ofCGO 08. [28] U. Srivastava, K. Munagala, J. Widom, and \nR. Motwani. Query optimization over web services. In Proc.ofVLDB 06. [29] J. M. Tamayo. Pepe JDBC dependency \ntracker. http://github.com/jtamayo/pepe. [30] F. Tip. A survey of program slicing techniques. Technical \nreport, CWI, Amsterdam, The Netherlands, The Netherlands, 1994. [31] K. S. Trivedi. On the paging performance \nof array algorithms. IEEETransactions onComputers, 1977. [32] P. H. Wang, J. D. Collins, H. Wang, D. \nKim, B. Greene, K. ming Chan, A. B. Yunus, T. Sych, S. F. Moore, and J. P. Shen. Helper threads via virtual \nmultithreading on an experimental Itanium 2 processor-based platform. In Proc. of ASPLOS 04. [33] A. \nWelc, S. Jagannathan, and A. Hosking. Safe Futures for Java. In Proc.ofOOPSLA 05. [34] C.-K. Yang, T. \nMitra, and T.-c. Chiueh. A decoupled architecture for application-speci.c .le prefetching. In Proc. ofFREENIXTrack:USENIX \n02. [35] W. Zhang, D. M. Tullsen, and B. Calder. Accelerating and adapting precomputation threads for \neffcient prefetching. In Proc.ofHPCA 07.  \n\t\t\t", "proc_id": "2048066", "abstract": "<p>Remote data access latency is a significant performance bottleneck in many modern programs that use remote databases and web services. We present Sprint - a run-time system for optimizing such programs by prefetching and caching data from remote sources in parallel to the execution of the original program. Sprint separates the concerns of exposing potentially-independent data accesses from the mechanism for executing them efficiently in parallel or in a batch. In contrast to prior work, Sprint can efficiently prefetch data in the presence of irregular or input-dependent access patterns, while preserving the semantics of the original program.</p> <p>We used Sprint to automatically improve the performance of several real-world Java programs that access remote databases (MySQL, DB2) and web services (Facebook, IBM's Yellow Pages). Sprint achieves speedups ranging 2.4x to 15.8x over sequential execution, which are comparable to those achieved by manually modifying the program for asynchronous and batch execution of data accesses. Sprint provides a simple interface that allows a programmer to plug in support for additional data sources without modifying the client program.</p>", "authors": [{"name": "Arun Raman", "author_profile_id": "81100001392", "affiliation": "Princeton University, Princeton, NJ, USA", "person_id": "P2839176", "email_address": "rarun@princeton.edu", "orcid_id": ""}, {"name": "Greta Yorsh", "author_profile_id": "81315492721", "affiliation": "ARM, Cambridge, United Kingdom", "person_id": "P2839177", "email_address": "greta.yorsh@arm.com", "orcid_id": ""}, {"name": "Martin Vechev", "author_profile_id": "81100269652", "affiliation": "ETH Zurich and IBM Research, Zurich, Switzerland", "person_id": "P2839178", "email_address": "martin.vechev@gmail.com", "orcid_id": ""}, {"name": "Eran Yahav", "author_profile_id": "81100285431", "affiliation": "Technion, Haifa, Israel", "person_id": "P2839179", "email_address": "yahave@cs.technion.ac.il", "orcid_id": ""}], "doi_number": "10.1145/2048066.2048088", "year": "2011", "article_id": "2048088", "conference": "OOPSLA", "title": "Sprint: speculative prefetching of remote data", "url": "http://dl.acm.org/citation.cfm?id=2048088"}