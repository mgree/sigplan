{"article_publication_date": "06-15-2009", "fulltext": "\n Chameleon: Adaptive Selection of Collections Ohad Shacham Martin Vechev Eran Yahav Tel Aviv University \nIBM Research IBM Research Abstract Languages such as Java and C#, as well as scripting languages like \nPython, and Ruby, make extensive use of Collection classes. A collection implementation represents a \n.xed choice in the dimensions of operation time, space utilization, and synchroniza\u00adtion. Using the collection \nin a manner not consistent with this .xed choice can cause signi.cant performance degradation. In this \npaper, we present CHAMELEON, a low-overhead auto\u00admatic tool that assists the programmer in choosing the \nappropri\u00adate collection implementation for her application. During program execution, CHAMELEON computes \nelaborate trace and heap-based metrics on collection behavior. These metrics are consumed on-the\u00ad.y by \na rules engine which outputs a list of suggested collection adaptation strategies. The tool can apply \nthese corrective strategies automatically or present them to the programmer. We have implemented CHAMELEON \non top of a IBM s J9 pro\u00adduction JVM, and evaluated it over a small set of benchmarks. We show that for \nsome applications, using CHAMELEON leads to a sig\u00adni.cant improvement of the memory footprint of the \napplication. Categories and Subject Descriptors D.2.5 [Testing and Debug\u00adging]; D.3.3 [Language Constructs \nand Features] General Terms Performance, Languages Keywords bloat, collections, java, semantic pro.ler \n 1. Introduction Programming languages such as Java, C#, Python and Ruby include a collection framework \nas part of the language runtime. Collection frameworks provide the programmer with abstract data types \nfor handling groups of data (e.g, Lists, Sets, Maps), and hide the details of the underlying data-structure \nimplementation. Modern programs written in these languages rely heavily on collections, and choosing \nthe appropriate collection implementa\u00adtion (and parameters) for every usage point in a program may be \ncritical to its performance. Real-world applications may be allocating collections in thou\u00adsands of program \nlocations, making any attempt to manually select and tune collection implementations into a time-consuming \nand of\u00adten infeasible task. It is therefore not surprising that recent studies [22] have shown that in \nsome production systems, the utilization of collections might be as low as 10%, that is, 90% of the space \nconsumed by collections in the program is overhead. Permission to make digital or hard copies of all \nor part of this work for personal or classroom use is granted without fee provided that copies are not \nmade or distributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. PLDI 09, June 15 20, 2009, Dublin, Ireland. Copyright c . 2009 \nACM 978-1-60558-392-1/09/06. . . $5.00. Existing pro.lers ignore collection semantics and memory lay\u00adout, \nand aggregate information based on types. Of.ine approaches using heap-snapshots (e.g., [21, 22]) lack \ninformation about access patterns, and cannot correlate heap information back to the relevant program \nsite. In this paper, we present the .rst practical tool that automati\u00adcally selects the appropriate collection \nimplementations for a given application. Our tool uses what we call semantic pro.ling together with a \nset of collection selection rules to make an informed choice. This approach is markedly different from \nexisting pro.ling tools where the user is forced to manually .lter massive amounts of ir\u00adrelevant data, \ntypically of.ine, in order to make an educated guess. Semantic Collections Pro.ling The semantic pro.ler \nconsists of a tightly integrated collections-aware production virtual ma\u00adchine and a runtime library. \nDuring program execution, these two components collect a myriad of complementary context-speci.c collection-usage \nstatistics such as continuous space utilization and access patterns for each object. This information \nis obtained online and transparently to the programmer, without any need for an of\u00ad.ine analysis of a \ngeneral (non-targeted) heap dump. The ability of CHAMELEON to map all statistics back to the particular \nallocation context in the program is extremely useful as it enables the devel\u00adoper to focus on collections \nwith maximum bene.t. We have also pre-equipped our tool with a set of collection selection rules which \nare evaluated on the dynamic statistics. The output of the tool is a set of suggestions on how to improve \nthe collections allocated at a particular allocation context. We are not aware of any other tool that \ncan automatically produce such effective information in a low\u00adoverhead manner. Selection from Multiple \nImplementations In this work, we as\u00adsume that we are given a set of interchangeable implementations for \nevery collection type. The requirement is that the different im\u00adplementations have the same logical behavior. \nFor example, a Set may be implemented using an underlying array, or a linked-list, but all implementations \nhave to maintain the functional behavior of a set (e.g., have no duplicates). We focus on optimizing \nthe choice of collection implementation, and do not address the orthogonal prob\u00adlem of showing that two \ncollection implementations are logically equivalent (as done, e.g., in [20]). Our library provides a \nnumber of alternative implementations, and we allow the user to add her own implementations, and imple\u00admentations \nobtained from other sources (e.g., [1, 2, 3, 4]). 1.1 Main Contributions The main contributions of this \npaper can be summarized as follows: A semantic pro.ler which tracks useful collection usage pat\u00adterns \nacross space and time. The pro.ler aggregates and sorts data for each collection allocation-context. \n A collection-aware garbage collector which continuously gath\u00aders statistics for a collection ADT rather \nthan individual objects.   Figure 1. Tool overview This is very useful as collection ADTs usually \nconsists of sev\u00aderal objects (that can be described by maps). The collector is parametric on the semantic \nADT maps, and can be reused for any (including user-speci.c) collection implementation. A .exible rule \nengine that selects the appropriate collection im\u00adplementation based on the pro.ling information. Our \nrule en\u00adgine allows the programmer to write implementation selection rules over the collected pro.le \ninformation using a simple, but expressive implementation selection language.  A complete implementation \nof our tool over a production JVM.  Evaluation of our tool on a small set of benchmarks where we show \nthat following CHAMELEON recommendations can lead to signi.cant improvement in program space requirements, \nas well as running time.   2. Overview Inthissectionweprovideahigh-leveloverviewof CHAMELEON by demonstrating \nhow it is applied to an example, and brie.y discuss some of the tradeoffs of collection implementations. \n 2.1 Motivating Example TVLA [18] is a .exible static analysis framework from Tel-Aviv University. The \nframework performs abstract interpretation with parametric abstractions, and computes a set of abstract \nstates that over-approximate the set of all possible concrete program states. TVLA is a memory-intensive \napplication, and its ability to tackle key veri.cation challenges such as concurrent algorithms (which \nhave large state spaces) is mostly limited by memory consumption. The TVLA framework makes extensive \nuse of collections. Our goal in this example is to optimize the collections usage in TVLA. The .rst step \ntowards that goal is to check the potential for collection optimizations in the application. Fig. 1 shows \nan overview of CHAMELEON. The tool works in two automated phases: (i) semantic collection pro.ling gathering \na wide range of collection statistics during a program run; (ii) auto\u00admatic selection using a rule engine \nusing a set of selection rules evaluated over the collected statistics to make implementation se\u00adlection \ndecisions. The tool is parametric on the semantic maps used for pro.ling (Sec. 3.2), and on the set of \nselection rules (Sec. 3.3). Fig. 2 shows the percentage of live-data that is consumed by collections \nin TVLA running on a particular analysis problem \u00adshowing that Lindstrom s binary-search-tree traversal \npreserves the shape of the underlying tree. This .gure is the actual output as produced by the semantic \npro.ler in CHAMELEON. The .gure shows three measures as percentage of the live data: (i) total live data \nconsumed by collections (live); (ii) total size of the used parts of collections (used); (iii) lower-bound \nspace consumption of the actual collection content (core). The gap between live and core is the best-case \npotential space saving for collections (see more details on the nature of this space Figure 3. Combined \nresults for top 4 allocation contexts in TVLA. overhead in Section 2.2). Of course, some collection implementa\u00adtions, \nsuch as hash tables, introduce additional space in order to facilitate ef.cient operations, so comparison \nto core is non realis\u00adtic. We provide the core measure as a lower-bound for the space requirement, and \nto see how changes to the volume of the actual stored data affect the space consumed by collections. \nThe gap between live and used corresponds to the total space allocated by the collection implementation \nthat is not used to store application entries. In the .gure, we see that collections constitute up to \n70% of the live data, and the part used to store collection elements is only up to 40% of the live data. \nAt this stage, it seems that there is a realistic potential for space saving, but the question is how \ndo we realize this potential? In particular, how do we relate this information to the program? What can \nbe done in order to avoid this space overhead? What points in the program should be modi.ed? Using several \nheap-snapshots taken during program execution may reveal the types that are responsible for most of the \nspace con\u00adsumption. However, a heap snapshot does not correlate the heap objects to the point in the \nprogram in which they are allocated. Therefore, .nding the program points that need to be modi.ed re\u00adquires \nsigni.cant effort, even for programmers familiar with the code. Moreover, once the point of collection \nallocation is found, it is not clear how to choose an alternative collection implementa\u00adtion. In particular, \nchoosing an alternative collection implementa\u00adtion with lower space overhead is not always desirable. \nSome struc\u00adtures, such as hash-tables, have inherent space overhead to facili\u00adtate more time-ef.cient \noperations. In order to pick an appropriate implementation, some information about the usage pattern \nof the collection in the particular application is required.  CHAMELEON is the .rst tool to integrate \nheap-information with information about the usage-pattern of collections. The semantic pro.ler in CHAMELEON \nproduces a ranked list of allocation con\u00adtexts in which there is a potential for space saving. For each \nsuch allocation context, the pro.ler CHAMELEON provides comprehen\u00adsive information such as the distribution \nof operations performed on collections allocated at the context, the distribution of collection sizes, \netc. Fig. 3 shows an example for such a summary. It shows the top 4 allocation contexts in TVLA, with \ntheir corresponding space saving potential. For example, for context 1, there is a space poten\u00adtial of \nroughly 10 percent of total live heap. Additionally, for each context, the tool provides the distribution \nof operations (represented as circles in the .gure). For brevity, we don t show the names of the operations. \nFor contexts 1, 3 and 4, the operation distribution is en\u00adtirely dominated by get operations, while for \ncontext 2 there is also a small portion of add and remove operations. In addition to pro.ling information \nfor each context, CHAMELEON produces suggestions on which collection implementations to use. For this \nexample, we get the following succinct messages (for brevity we only show suggestions for contexts 1 \nand 4): 1: HashMap:tvla.util.HashMapFactory:31;tvla.core.base.BaseTVS:50 replace with ArrayMap 4: ArrayList:BaseHashTVSSet:112; \ntvla.core.base.BaseHashTVSSet:60 set initial capacity To produce this report, CHAMELEON combines information \non how the collections are used, with information on the poten\u00adtial saving in each context. The combined \ninformation is used by CHAMELEON s rule-engine, to yield collection tuning decisions that are presented \nto the user. The .nal report usually includes the precisely tracked context, which in our case consists \nof the call\u00adstack when allocation occurred (usually of depth 2 or 3). This is often required when the \napplication uses factories for creation of collections (as is done sometimes in TVLA). Next,weapplythecollectiondecisions \nCHAMELEON advocated in 5 top allocation sites in TVLA, and re-run the application. The overall effect \non the total space required by the application is dra\u00admatic. In particular, the minimal heap-size required \nto run the ap\u00adplication has been reduced by 50%. The effect on the overall run\u00adning time is also signi.cant. \nThe total time required to complete the veri.cation by using the modi.ed version based on the collection \nimplementations advocated by CHAMELEON is more than 2.5 times faster than the original one (from 49 to \n19 minutes). Further sav\u00ading of time and space is possible by modifying additional program points. In \ncontrast to standard pro.ling tools which require heavy manual involvement, by using the semantically \nfocused, context\u00adspeci.c suggestions provided by CHAMELEON, we were able to achieve dramatic performance \nimprovements quickly and with lit\u00adtle manual effort.  2.2 Tradeoffs in Collection Implementations Selecting \nan appropriate collection implementation is more com\u00adplicated than it seems at .rst sight. Time It is \npossible to base the selection on asymptotic time com\u00adplexity of collection operations. However, the \nasymptotic time complexity of collection operations is not a good measure of their behavior when the \ncollections contain a small number of items. In the realm of small sizes, constants matter. Furthermore, \nin prac\u00adtice, the actual performance of a collection is affected by different aspects, such as the locality \nof the selected structure, the cost of computing a hash function, cost of resizing the structure etc. \nSpace Collections vary in how much space overhead is consumed for storing a speci.c amount of data. They \ntypically have different .xed overhead per element in the collection. For example, every el\u00adement stored \nin the LinkedList implementation has an Entry object associated with it, where the Entry object stores \na reference to the actual element, and two references to the next and previous entries in the list. At \neach allocation site in the program, we de.ne the utilization of a data structure as the ratio between \nthe size of the data that it represents and the total amount of memory that this instance currently uses. \nSimilar utilization metrics are used in the context of memory health measures [22]. As utilization varies \nduring the execution, we consider both the utilization along points of program execution, and the overall \naverage utilization of the collection. There are several causes of low utilization: (i) the initial capacity \nof the collection is not suited to the average size of data stored in it; (ii) the collection is not \ncompacted when elements are removed from it; (iii) high overhead per item in the collection. For example, \nan ArrayList expands its capacity whenever it runs out of available space. The capacity grows by the \nfunc\u00adtion newCapacity =(oldCapacity * 3)/2+1. Consider an ArrayList that has an initial capacity of 100 \nand contains 100 elements. Adding another element increases the size of the allo\u00adcated array to 151 while \nonly containing 101 elements. Space/Time Tradeoffs It is key to note the tradeoff between time and utilization \n(space). We can improve utilization by taking more time to perform operations. For example, given an \nArrayList implementation, we can resize the array on every operation exactly to the number of elements \nit contains. This would incur a signi.\u00adcant time penalty, but would keep the utilization at close to \n100% (accounting for the meta-data in the collection object header etc.). Conversely, if we don t care \nabout utilization, we can pre-allocate the array at the maximal number of elements, which would yield \na very low utilization, but would avoid the need for resizing the ar\u00adray. Similarly, choosing an array \nover a linked-list would improve utilization, but would make update operations more costly.  2.3 Possible \nSolutions for Low Utilization There are several seemingly reasonable solutions that can be used to tackle \nthe poor utilization of data structures. First, we can set the initial size of all allocated collections \nto one and then resize the collection size whenever an insertion or removal operation takes place. Second, \nwe can use a hybrid collection mechanism. Initially the structure is implemented as an array. Then, whenever, \nthe size of the collection increases beyond a certain bound, we can convert the array structure to the \noriginal implementation. The advantage of both of these solutions is that they operate based only on \nlocal knowledge. That is, decisions for the collec\u00adtions implementation and size are determined within \nthe speci.c collection object and are not based on any kind of global informa\u00adtion such as allocation \ncontext. Unfortunately, we were unable to reduce the memory footprint using these solutions with a reasonable \ntime penalty. Using small initial sizes does not reduce the memory footprint due to the fact that in \nHash-based ADT, such as HashMap, each hash entry is represented by a new object containing three pointer \n.elds. The .rst is a next pointer referencing the next entry. The second is a prev pointer referencing \nthe previous entry. The third is a pointer to the data itself. The entry object alone on a 32-bit architecture \nconsumes 24 bytes (object header and three pointers). Therefore, even when starting with a small initial \nsize, signi.cant memory not related to actual data is consumed, in this case, due to the large entry \nsize.  The second (hybrid) solution can be effective in reducing foot\u00adprint; however, choosing the size \nwhen the conversion from an ar\u00adray based implementation should take place is very tricky with\u00adout causing \nsigni.cant runtime degradation. In TVLA for example, making the conversion of ArrayMap to HashMap at \nsize 16 pro\u00advides a relatively low footprint with 8% performance degradation. However, increasing the \nconversion size to a larger number than 16 does not provide a smaller footprint and leads to performance \ndegradation. Moreover, reducing the conversion size to 13 provides the same footprint as the original \nimplementation does.  3. Automated Collection Selection In this section, we discuss our solution for \nautomatically selecting the appropriate collections for a given user program. First, in Sec\u00adtion 3.1, \nwe de.ne the problem. Then, we show how we address this problem with a combination of semantic collection \npro.ling (Section 3.2), and a rule engine (Section 3.3). 3.1 Optimal Selection of Collection Implementations \nGiven a program that uses collections, our goal is to .nd an as\u00adsignment of collection implementations \nthat is optimal for that pro\u00adgram. An optimal choice of collection implementations tries to bal\u00adance \ntwo dimensions: minimizing the time required to perform op\u00aderations while also minimizing the space required \nto represent ap\u00adplication data. The problem of optimal collection selection can be viewed as a search \nproblem: for every point in a program allocating a collection, for each possible collection implementation, \nrun the program, and compare the results in terms of space consumption and overall run\u00adning time. However, \nthis approach is not likely to scale for anything but the smallest programs. Furthermore, comparing results \nacross executions is a daunting task in the presence of non-determinism and concurrency. An alternative \napproach is to select collection implementations based on collection usage statistics extracted from \nthe client pro\u00adgram. Since there is no a priori bound on the number of collection objects in a program, \nand there is no a priori bound on the sequence of operations applied on a collection object, it is not \npractical to represent all operation sequences directly, and an abstraction of the usage patterns is \nrequired. In principle, an abstraction of the collection usage pattern in a program can be obtained either \nstatically or dynamically. However, static approaches to this problem typically abstract away the oper\u00adation \ncounts, which are a crucial component of usage patterns, and are not likely to scale to realistic applications. \nSeeking a scalable approach, we focus our attention on selection based on dynamic in\u00adformation. A dynamic \napproach would have to track, in a scalable manner, enough information on the usage of collections to \nenable the choice of appropriate implementations.  3.2 Semantic Collections Pro.ling In this section \nwe describe the information collected by the seman\u00adtic collections pro.ler of CHAMELEON and how this \ninformation is used in order to make collection selection decisions. 3.2.1 Allocation Context Our work \nis based on the hypothesis that the usage patterns of col\u00adlection objects allocated at the same allocation \ncontext are similar. More precisely, we de.ne the allocation context of an object o to be the allocation \nsite in which o was allocated, and the call stack at the point when the allocation occurred. Name Description \nOverall live data Total/Max size of all reachable objects Collection live data Total/Max size of collection \nobjects Collection used data Total/Max used part of collection objects Collection core data Total/Max \ncore part of collection objects Collection object number Total/Max # of live collection objects Number \nof operations Total number of operations performed Avg/Var operation count Average # of times an operation \nwas per\u00adformed, and its standard deviation. Avg/Var of maximal Size Average maximal size of collections \nal\u00adlocated, and its standard deviation. Table 1. Heap and trace statistics gathered by CHAMELEON for \neach execution. Information is aggregated per allocation context. For allocation contexts in which we \nobserve similarity between usage patterns to hold within reasonable statistical con.dence, we determine \nthe type of collections that should be allocated in the context based on the average usage pattern. DEFINITION \n3.1 (Stability). We de.ne the stability of a metric in a partial allocation context c as the standard \ndeviation of that metric in the usage pro.le of collections allocated in c. Examples of metrics are: \nthe number of times a certain oper\u00adation is performed on a collection instance and the maximal size of \nthe collection during its lifetime. For every metric we de.ne a threshold that determines the limit under \nwhich the metric is con\u00adsidered stable. Practically, the full allocation context is rarely needed, and \nmaintaining it is often too expensive. Therefore, we use a partial allocation context, containing only \na call stack of depth two or three. The observation that (a small) allocation context is crucial to object \nbehavior is inline with the recent study of Jones et. al [15]. In that study, the authors observed that \nobjects allocated in the same context tend to behave similarly and a garbage collection strategy should \nbe made aware of this correlation. In our work, we exploit this insight for optimizing collection usage. \n 3.2.2 Collection Statistics CHAMELEON records a wide range of statistics indicating how col\u00adlections \nin the program are used. Much of the information recorded by the tool is per allocation context, and \nis an aggregation of the information collected for objects allocated at that context. Dynamically Tracked \nData The tracked data is shown in Table 1. The collected information is a combination of information \nabout the heap (e.g., the maximal heap size occupied by collection objects during execution), and information \nabout the usage pattern (e.g., the total number of times contains was invoked on collections in the context). \nHeap Information The heap information provides a comprehen\u00adsive summary of the space behavior of collections \nduring program execution. This information is collected on every garbage collec\u00adtion (GC) cycle. The \nGC computes the total and maximal live data of the program where the total live data is the sum of all \nlive data s accumulated over all of the GC cycles and the maximal live data is the largest live data \nseen in any GC cycle. The GC has been augmented with semantic maps and routines to compute various context-speci.c \ncollection information (discussed further in Sec\u00adtion 4). First, it computes the total and maximal space \nconsumed by reachable collection objects across all GC cycles. Second, it com\u00adputes the total and maximal \nspace actually used by these collection objects (collection used data). This is important for knowing \nhow much of the collection object is really utilized. Thirdly, it computes the total and maximal collection \ncore size, which would be the ideal  rule := srcT ype cond implT ype| srcT ype cond implT ype(capacity) \nsrcT ype := Collection | ArrayList | LinkedList | ... implT ype := ArrayList | ArrayMap | HashSet | ... \ncond := comparison | cond . cond | ... comparison := expr = constant | expr == constant | ... expr := \nopCount | opV ar | data | expr + expr | ... opCount := #add | #get(int) | #get(Object) | ... opV ar := \n@add | @remove | ... data := tracedata | heapdata tracedata := size | maxSize | initialCapacity heapdata \n:= maxLive | totLive | maxUsed | totUsed ... capacity := INT | maxSize Figure 4. Simple language for \nimplementation selection rules. space that would be required to store the core elements of the col\u00adlection \nobject in an array. This statistic is useful to provide a lower bound on the space requirement for the \ncontent of the collection (hence indicating the limit of any optimization). Finally, the total and maximum \nnumber of live collection objects are computed. Trace Information As mentioned earlier, recording the \nfull se\u00adquence of operations applied to a collection object has a prohibitive cost. Instead, our trace \ninformation records the distribution of op\u00aderations, as well as the maximal size observed for collections \nat the given context. The average operation counts provide a count of all possible collection operations. \nFor brevity, we do not list all of them here. For some operations, those that involve interactions be\u00adtween \ncollections, we introduce additional counters that count both sides of the interaction. For example, \nwhen adding the contents of one collection into another using the c1.addAll(c2) operation, we record \nthe fact that addAll was invoked on c1, but also the fact that c2 was used as an argument for addAll. \nSimilarly, we record when a collection was used in a copy constructor. These counters are par\u00adticulary \nimportant for identifying temporary collection objects that are never operated upon directly, other then \ncopying their content. Using Pro.ling Information The statistics from the tool can be used in several \nways. For example, as the program runs, the user can request the tool to output the current top allocation \ncontexts, sorted by maximum bene.t. In the case where the user wants to make manual changes, she can \nfocus on the most bene.cial contexts instantly. Alternatively, she can use the recommendations automatically \ncomputed by the tool, which are based on a set of selection rules. To allow .exibility in querying the \ninformation collected by the tool, and select appropriate implementations based on it, we let the user \nwrite rules in a simple language. We describe those next.  3.3 Rule Engine A Simple Rule Language We \nallow the user to write replacement rules, using the language of Fig. 4. The language is pretty standard, \nand in the .gure we abbreviate rules that contain standard combi\u00adnations of operations, such as boolean \ncombinations for cond and arithmetic operators for expr. The language allows to write condi\u00adtional expressions \nunder which a replacement takes place. The con\u00additional expressions use the metrics of Table 1 as the \nbasic vocabu\u00adlary. The language allows to write conditional expressions compar\u00ading the ratios between \noperation counts (e.g., the ratio of contains operations #contains/#allOps), the operation count itself \n(e.g., #remove == 0) etc. It also allows the user to check the variance of counts (e.g, @add). The language \nalso allows the user to query the live-data occupied by collections at the context, and the used\u00addata \noccupied by collections at the context. These are typically used to .gure out whether the potential saving \nin this allocation context (totLive - totUsed) is greater than some threshold. 3.3.1 Chameleon Collection \nSelection Table 2 shows several examples of selection rules that are built into CHAMELEON. The constants \nused in the rules are not shown, as they may be tuned per speci.c environment. For example, the rule \nArrayList :#contains>X.maxSize>Y.LinkedHashSet speci.es that if the type allocated at this context is \nan ArrayList, and the average number of contains operations performed on collections in this context \nis greater than some threshold X, and the average maximal size of the collection is greater than some \nthreshold Y , then the selected type should be a LinkedHashSet. This rule corresponds to the fact that \nperforming a large number of contains operations on large-sized collections is better handled when the \ncollection is a LinkedHashSet. Of course, the rule can be re.ned to take other operations into account. \nThe user can write various expressions in this language that dictate which im\u00adplementation to select. \nFor example, when the potential space sav\u00ading is high, one may want to apply a different collection selection \neven if it results in a potential slowdown. For instance, the space bene.t of the rule selecting an ArraySet \ninstead of HashSet may outweigh the time slowdown when the potential space sav\u00ading (totLive-totUsed) \nis greater than some threshold. Conversely, we can avoid any space-optimizing replacement when the potential \nspace savings seems negligible. Section 5 shows that using CHAMELEON recommendations based on rules such \nas those of Table 2 can yield signi.cant perfor\u00admance improvements. Stability If stability is not speci.ed \nexplicitly in the rule, it is as\u00adsumed that any metric has to have its standard deviation less than a \n.xed constant (in the current implementation, size values are re\u00adquired to be tight, while operation \ncounts are not restricted). Gen\u00aderally, different metrics may require different measures of variance \nbased on their expected distribution. For example, while the opera\u00adtion counters usually distribute normally, \nmaximal collection sizes are often biased around a single value (e.g., 1), with a long tail. Our current \nimplementation uses standard-deviation as the stabil\u00adity measure, but in the future we plan to evaluate \nthe suitability of other measures of variance for different metrics.  3.3.2 Towards Complete Automation \nThe current operation mode in which CHAMELEON is used is to evaluate all selection rules at the end of \nprogram execution, when complete information has been obtained for all collections allo\u00adcated at a given \ncontext. The suggested implementations can then be applied by the programmer (or by the tool) and the \nprogram can be executed again (with or without pro.ling). An interesting challenge is whether the act \nof replacement can be applied while the program is running. Such an online solution may be bene.cial \nfor several reason: Lack of Stability: It is possible that collection objects from a given allocation \ncontext exhibit wide variation in behavior, for example due to different program inputs, phasing or non\u00addeterminism. \nHence, detecting these cases and allocating the ap\u00adpropriate collection object may be more advantageous \nthan sticking to a single implementation for all cases. Optimization of Underlying Framework: Most real-world \nsoft\u00adware makes use of framework code. The framework code itself may make extensive use of collection. \nOnline selection can special\u00adize the collection-usage in underlying frameworks, that is typically outside \nthe scope of programmer s manual modi.cations. In gen\u00ad  Type Condition Category: Message Suggested Fix \nArrayList #contains > X . maxSize > Y Time: Inef.cient use of an ArrayList: large volume of contains \nLinkedHashSet operations on a large sized list LinkedList #get(int) > X Time: Inef.cient use of a LinkedList: \nlarge volume of random ArrayList accesses using get(i) LinkedList (#add(int, Object)+ #addAll(int, Collection)+ \n#remove(int) + #removeF irst) < X Space: LinkedList overhead not justi.ed when adding/removing elements \nfrom the middle/head of the list is hardly performed ArrayList Collection maxSize == 0 Space: Redundant \ncollection allocation LazyArrayList HashSet maxSize < X Space: ArraySet more ef.cient than an HashSet. \nTime: opera- ArraySet tions on a small array might be faster than on an HashSet Collection #allOps = \n0 Space/Time: redundant collection avoid allocation Collection #allOps == #copied Space/Time: redundant \ncopying of collections eliminate temporaries Collection maxSize > initialCapacity Space/Time: incremental \nresizing set initial capacity Iterator collection.size == 0 Space: Redundant iterator remove Table 2. \nExample of built-in CHAMELEON rules. eral, this follows a theme of specializing the library for a particular \nclient, as part of the client s execution in the runtime environment. No Programmer Effort: Manual replacement \nmay require non\u00adtrivial code-modi.cations to deal with factories and deep allocation contexts. Dynamic \nselection is performed as part of the runtime en\u00advironment and requires no manual modi.cations to the \nsource code. Dealing with completely automatic replacement is challenging because decisions may have \nto be based on partial information: at what point of the execution can we decide to select one col\u00adlection \nimplementation over another? For example, if the tool re\u00adplaces the type allocated at a given context \nfrom a HashMap to an ArrayMap on the premise that objects allocated at that con\u00adtext have small maximal \nsizes, even a single collection with large size may considerably degrade program performance. Additionally, \nsuch a tool must run with suf.ciently low overhead to be enabled during production deployment. Therefore, \nit is crucial to reduce overhead costs and in particular, it is vital to be able to obtain allo\u00adcation \ncontext cheaply. Towards the vision of fully automatic management of collec\u00adtions at runtime, we performed \npreliminary experiments where we used CHAMELEON inamodewhereallreplacementsaredonecom\u00adpletely automatically \nat runtime, without any user involvement. We describe our results in Section 5.   4. Implementation \nIn this section we present the design and implementation of our tool. The tool consist of two complementary \ncomponents: the li\u00adbrary and the virtual machine, which are integrated in a manner that is transparent \nto the end user. The design of these components is such that they can be used separately by switching \non and off each component on demand. However, for maximal bene.t we typ\u00adically use them together. By \nselectively instrumenting the library, we are able to record various useful statistics such as frequency \nof operations and distributions of operations for a given collection size. While this information is \nuseful, it still does not provide us with a relative view of how collections behave with respect to the \nwhole system. However, such global information can be extracted from the virtual machine and in particular \nfrom the garbage collec\u00adtor (GC). By instrumenting the GC to gather semantic information about collections, \nwe are able to answer questions such as the total live data occupied by collections at a speci.c point \nin time. Such information, while cheap to obtain from the GC, is very costly to obtain at the library \nlevel. Next, we describe each component sep\u00adarately as well as how they interact with each other. 4.1 \nDesign Choices One of the core principles that we followed in our approach is to avoid as much as possible \nany changes to the original program. A key place where a dilemma between portability and slightly better \nef.ciency occurs is during allocation of a collection ob\u00adject. For example, if the user program requests \nan allocation of a HashMap object and the system determines that for this context, it is best to implement \nthat HashMap object with an ArrayMap, we are faced with two possible implementation choices. First, we \ncan make ArrayMap a subtype of HashMap and then return ArrayMap. The problem is, that ArrayMap would \nthen inherit all .elds from HashMap. Further, any program expressions that depend on the precise type \nbeing HashMap may work incorrectly. Another so\u00adlution is to have ArrayMap and HashMap as sibling types, \nbut to return an object of type ArrayMap. In that case, we need to make sure that all type declarations \nin the program match ArrayMap (that were HashMap before) and that all semantic behavior depending on \na speci.c type is preserved. This is the approach taken by Sutter et. al for details [25]. However, statically \nre-writing the type decla\u00adrations of the program is intrusive, challenging, can lead to subtle errors \ndue to language features such as dynamic typing, and is gen\u00aderally dif.cult to scale on large programs. \nOur solution in that case has been true to Lampson s statement that all problems in com\u00adputer science \ncan be solved by another level of indirection. Hence, we add another level of indirection between the \nprogram and the collection implementation. That is, each allocation of a collection object requires a \nwrapper. In our example, whenever HashMap is allocated, it will be a small wrapper object. Then, internally, \nthe wrapper object can point to any implementation of HashMap. We believe that a small delta in inef.ciency \nis worth the software re\u00adliability gains. Further, we believe that with VM support we can reduce this \ninef.ciency further (e.g. via object inlining)  4.2 Library Architecture Fig. 5 shows the architecture \nof the CHAMELEON libraries. Our wrappers delegate collection operations to the underly\u00ading selected collection \nimplementation (similar to the Forwarding types in Google s Collections [2]). The only information kept \nin the wrapper object is a reference to the particular implementation. In our solution, the actual backing \nimplementation can be determined statically by the programmer (by explicitly providing the construc\u00adtor \nwith an appropriate constant), left as the default choice that the programmer indicated, or determined \ndynamically by the system. As the wrapper allocates the backing implementation object, it also obtains \nthe call stack (context) for this allocation site and constructs a VMContextKey object that records it \n(via the loca\u00adtionId .elds inside it). This object is then used to look up the cor\u00adFigure 5. CHAMELEON \nlibrary architecture. Shaded .elds are up\u00addated by the VM.  responding ContextInfoobject, which records \naggregate infor\u00admation for this context. In order to collect information on the col\u00adlection usage pattern \nfor this context, the backing implementation may allocate an ObjectContextInfo. This object is used to \nstore the various operation counters, collection maximal size, etc. When the collection implementation \nobject dies, the contents of its object information object are aggregated into the corresponding ContextInfoobject \n(via .nalizers as described later). Obtaining Allocation Context CHAMELEON tracks information at the \nlevel of an allocation context. This requires that an allocation context be obtained whenever a collection \nobject is allocated. We have implemented two methods for obtaining the allocation con\u00adtext: (i) a language-level \nmethod based on walking the stack frames of a Throwableobject; (ii) a method using JVMTI. The JVMTI-based \nimplementation is signi.cantly faster than the Throwable-based implementation which requires the expen\u00adsive \nallocation of a Throwable object, and the manipulation of method signatures as strings (our native implementation \nworks di\u00adrectly with unique identi.ers, without constructing intermediate objects to represent the sequence \nof methods in the context). We are currently working on a third implementation using a modi.cation of \nthe JVM to obtain bounded context information in a lightweight manner. In addition, there are many approaches \nthat target the problem of obtaining context [7, 9, 10, 28], we intend to explore some of these in future \nwork. Sampling of Allocation Context: To further mitigate the cost of obtaining the allocation context, \nCHAMELEON can employ sam\u00adpling of the allocation contexts. Moreover, when the potential space saving \nfor a certain type is observed to be low, CHAMELEON can completely turn off tracking of allocation context \nfor that type. (Technically, sampling is controlled at the level of a speci.c con\u00adstructor.) Available \nImplementations Our goal in this work is to study the problem of collection implementation selection, \nand not to improve the default collection implementations. There are many alterna\u00adtive open-source collection \nimplementations [1, 2, 3, 4], varying in terms of robustness, compatibility, and performance. In princi\u00adple, \nthese implementations can be swapped-in as additional possi\u00adble implementations for the collection interfaces, \nwith appropriate selection rules on when they should be used. In our experiments, however, we used our \nown alternative im\u00adplementations for collections, for example: List: ArrayList -resizable array implementation. \nLinkedList -a doubly-linked list implementation. LazyArrayList -allocate internal array on .rst update. \nIntArray -array of ints. (Similar for other primitives) Set (and similarly for Map): HashSet (default) \n-backed up by a HashMap LazySet -allocates internal array on .rst update ArraySet -backed up by an array \nSizeAdaptingSet -dynamically switch underlying imple\u00ad mentation from array to HashMap based on size. \nFurther performance improvements can be achieved by swap\u00adping additional implementations under the appropriate \nconditions. However, some of these conditions are subtle. For example, select\u00ading an open-addressing \nimplementation of a HashMap (e.g., from the Trove collections) requires some guarantees on the quality \nof the hash function being used to avoid disastrous performance im\u00adplications. This is hard to determine \nin Java, where the programmer can (and does) provide her own hashCode()implementation. Context Information \nAs mentioned previously, the ObjectContextInfo object collects the usage pattern for collection instances. \nThis information is aggregated into the ContextInfo maintained for the corresponding allocation con\u00adtext. \nAs we will see later, with VM support, the context information can also contain information about the \nheap usage of collections allocated at the given allocation context. As we mentioned already, our design \nallows us to bene.t from VM support, but can also be used when such VM support is absent.  4.3 VM Support \nWhile gathering information at the library level is useful, it is often very dif.cult to obtain any kind \nof global view of how collections .t into the whole behavior of the program. For example, even though \na particular context allocates memory at a high rate, it is still not clear whether there is much bene.t \nglobally in tracking collection usage, for it may be the case that it is a small percent of total memory. \nAlso, it may often be useful to monitor the application with very low overhead, without tracking any \nlibrary usage, in order to determine whether there is any potential whatsoever in changing the implementation \nof collections. One place where much of this global information can be ac\u00adcessed is during the GC cycle. \nBy examining the program heap during a GC cycle, we can calculate various collection parameters such \nas distribution of live data and collection utilization. More\u00adover, with careful techniques, this valuable \ninformation can be ob\u00adtained with virtually no additional cost to the program execution time, and as \npart of normal operation of the collector. To that end, we extended the GC to gather valuable semantic \ninformation per\u00adtaining to collections. At the end of each cycle, the collector aggre\u00adgates this information \nin the ContextInfo object (which also contains trace-based information). The library can then inspect \nthe combination of trace and heap information at the same time. 4.3.1 Context-Sensitive Collection Data \nNote that simply examining the heap is often not enough, espe\u00adcially in large applications with thousands \nof program sites allo\u00adcating collections. In particular, we would like to focus on speci.c allocation \nsites in the program which have the highest potential for gain. To that end, if the library maintains \ncontext information, the collector will automatically take advantage of this and record vari\u00adous context-speci.c \ninformation into the ContextInfoobject.  4.3.2 Collector Modi.cations In our implementation, we used \nthe base parallel mark and sweep garbage collector, which works in the standard way. First, the roots \nof the program are marked (thread stacks, .nalizer buffers,  Name Description Live Data The size of \nall reachable objects Collection Live Data Total occupied size of collection objects Collection Used \nData Total used size of collection objects Collection Core Data Total core size of collection objects \nCollection Object Number Total number of live collection objects Type Distribution Live size breakdown \nfor each type Table 3. Statistics gathered on every garbage collection cycle for each allocation context \nstatic class members, etc). Then, several parallel collector threads perform the tracing phase and compute \ntransitive closure from these roots, marking all objects in that transitive closure. Finally, during \nthe sweeping phase, all objects which are not marked are freed. In our system, the number of parallel \nthreads is the same as the number of cores available in hardware. We note that our choice of this speci.c \ncollector can possibly lead to different results than if we had used for example a generational collector. \nHowever, the improvements in collection usage are orthogonal to the speci.c GC. We have instrumented \nthe base collector to compute various semantic metrics during its marking phase. The set of metrics computed \nby collector is shown in Table 3. From these metrics, we can compute aggregate per-context metrics over \nall GC cycles as described in Section 3.2.2 and shown in Table 1. Semantic ADT Maps Typically, a collection \nobject may contain several internal objects that implement the required functionality. For example, an \nArrayList object may contain an internal array of type java.lang.Object[] to store the required data. \nThis means that if we blindly iterate over the heap, we will not be able to differentiate object arrays \nthat are logically part of ArrayList and those object ar\u00adrays that have nothing to do with collections \n(e.g. allocated outside of ArrayList methods). This lack of semantic correlation between objects is a \ncommon limitation of standard pro.lers. Therefore, to ef.ciently obtain accurate statistics (such as \nsize) about collections, we use what we call semantic maps. In brief, every collection type is augmented \nwith a semantic map which describes the offsets that the collector use to .nd information such as the \nsize of the ob\u00adject (which may involve looking up the size of the underlying ar\u00adray), the actual allocated \nsize and its underlying allocation context pointer. Semantic maps are pre-computed for all collection \ntypes on VM startup. Using semantic maps allows us to obtain accurate information by avoiding expensive \nclass and .eld name lookups during collection operation. Further, because the whole process is parametric \non the semantic maps, we can run the system on any collection implementation (including custom implementations). \nOperation Every time the collector visits a non-marked object, it checks whether it is an object of interest \n(a collection object). In that case, it consults the semantic map of its type and quickly gathers the \nnecessary statistics such as the live data occupied by the object (and its internal objects), the used \ndata and the core data (the ideal space if we had only used a pointer array to represent the application \ndata). Further, if the object tracks context information, using the semantic map, the collector .nds \nthe ContextInfo object and records the necessary information for that allocation context (as described \nin Table 3).  4.4 Discussion By augmenting the GC with semantic ADT maps, we were able to automatically \nand continuously compute various useful context\u00adsensitive utilization metrics speci.c to the semantics \nof collections. Moreover, because the statistics are gathered during normal collec\u00adtion operation, no \nadditional performance overhead is incurred. The information obtained from the collector can be used \nin various ways. In our case, we propagate the information back to the ContextInfo object in the library \nin order to allow the tool to make a more informed decision by combining this with the library trace-based \ninformation. In addition, we also record the results for each cycle separately (it is up to the user \nto specify what they want to sort the results by as well as how many contexts to show) for further analysis. \nThis information can be readily used by the programmer to quickly focus on contexts which have the most \npotential for further improvement. Finalizer Usage In our early versions of this tool, we extended all \ncollection implementation types with .nalizer methods. How\u00adever, we found that .nalizers noticeably slowed \nthe system down. One of the main reasons for this is that .nalizer objects live for an additional collector \ncycle and hence all objects transitively reach\u00adable from the .nalizable object will also live for an \nadditional cycle (even if they are never referred by the finalize()method). We still rely on selective \nusage of .nalizers and we use them only for ObjectContextInfo objects. These objects are usually very \nsmall (few words) and do not have other objects in their transitive closure. Moreover, in the online \nversion, ObjectContextInfo objects are not always allocated, further mitigating any costs asso\u00adciated \nwith .nalizers. Note that for our purposes, we can also easily compute these statistics in the sweeping \nphase of the garbage col\u00adlection cycle (and not rely on .nalizers).  5. Experimental Results 5.1 Benchmarks \nBecause our tool runs on top of a production virtual machine and requires no changes to the application \nprogram, we were able to quickly run CHAMELEON on various applications. In our results, we focus on space-critical \napplications such as SOOT [26], TVLA [18] and FINDBUGS [14]. We also ran CHAMELEON on all of the Da\u00adcapo \nbenchmarks [8]. Most of the Dacapo benchmarks do not make intensive use of collections, and hence our \ntool showed little poten\u00adtial saving for those. However, it did show that there is potential on the benchmarks \nBLOAT, FOP, and PMD. Hence, we concentrated our efforts on the results for these benchmarks and we present \nthose later in this section. The inputs we used for our benchmarks are an internal Dacapo version for \nSOOT, TVLA source code for FIND-BUGS, the large inputs for Dacapo benchmarks, and an analysis problem \n-showing that Lindstrom s binary-search-tree traversal preserves the shape of the underlying tree for \nTVLA. Also, in our experiments we did not track the potential in benchmarks such as HSQLDB which use \ntheir own collection classes. However, with very little manual effort in the library, we can also pro.le \nsuch appli\u00adcations. The collection-aware GC can pro.le them already as it is parametric in the semantic \nmaps that describe the custom collection classes.  5.2 Methodology For each benchmark, we took the following \nsteps towards optimiz\u00ading collection usage: 1. Run CHAMELEON on the application. Based on the results, \nevaluate whether there is any saving potential. If there is no potential, move on to the next application, \notherwise, proceed to the next step. 2. For benchmarks with potential, CHAMELEON reports the allo\u00adcation \ncontexts in sorted order with the appropriate suggestions. 3. Modify the top allocation contexts using \nthe tool suggestions. This is a replacement step and hence can be easily automated. 4. Repeat steps \n1-3 on the modi.ed version.   Figure 6. Improvement of minimal heap size required to run the benchmark, \nshown as percentage of the original minimal heap size. Figure 7. Improvement of running times of the \nbenchmarks after applying .xes suggested by CHAMELEON, shown as percentage of the original running time. \nRunning times were obtained by running each benchmark with its corresponding original minimal-heap size. \n5. Compare the gains for the top allocation contexts in the before and after versions. 6. Evaluate the \neffect of collection optimizations in terms of the minimal-heap size required to run the program, and \nthe execu\u00adtion time when running with the original minimal-heap size.   5.3 Results Fig. 6 shows the \nimprovement of minimal space required to run the benchmark after applying .xes suggested by CHAMELEON. \nFig. 7 shows the improvement of running times of the benchmarks after applying .xes suggested by CHAMELEON. \nThe running times were obtained by running each benchmark with its corresponding origi\u00adnal minimal-heap \nsize requirement. Our experiments were obtained on an Intel Xeon 3.8Ghz dual hyper threaded CPUs, 5GB \nRAM platform running a 64 bit Linux. Next, we discuss each application we considered in more details. \nbloat The potential for bloat is shown in Fig. 8. The x-axis is the number of the GC cycle, while the \ny-axis is the percentage of the total live data computed at the end of the GC. This output is ob\u00adtained \ndirectly from the collection-aware GC. The .gure shows that bloat s footprint is dominated by a spike \nof collections (at GC#656 in the .gure), where the true required space for the collections is signi.cantly \nlower. The top allocation context reported by CHAMELEON for BLOAT corresponds to this spike of collections, \nand had a potential  Figure 8. Percentage of collections in original version of bloat that dominated \nthe potential of all other contexts. Furthermore, CHAMELEON reported that most of the LinkedLists allocated \nat that context remained empty and were never used. Around 25% of the heap at that point of execution \nwas consumed by LinkedList$Entry objects that are allocated as the head of an empty linked list. More \nthan 20% of space can be saved by making the lists into LazyArrayLists, but a simple manual modi.cation \nin the code can make the allocation itself lazy, which reduces the minimal-heap size required to run \nthe program by 56%. FOP In FOP (v0.95), based on the tool recommendations, some HashMaps were replaced \nwith ArrayMaps and ini\u00adtial sizes of other collections were tuned. There was also one context that allocated \ncollections that were never used (in InlineStackingLayoutManager). The result is a 7.69% re\u00adduction in \nthe minimal-heap size required to run the program. Findbugs Based on CHAMELEON suggestions, we replaced \nsome HashMaps by ArrayMaps, HashSets by ArraySets, and the initial sizes of other collections were tuned. \nWe also performed lazy allocation for HashMaps in contexts where large percentage of the collections \nremain empty. The overall result is a reduction of 13.79% in the minimal-heap size required to run the \nprogram. PMD PMD was already manually optimized to a correct collec\u00adtion usage. EMPTY LIST was assigned \nto List pointers when needed and the initial size of many ArrayLists was manu\u00adally set. CHAMELEON discovered \nmany empty and small sized ArrayLists that were mistakenly initialized to a high number. We manually \nperformed lazy allocation for these ArrayLists which reduced more than 20 million ArrayList allocations. \nIn addition, we set the tuned size of lists and replaced ArrayList allocation by SingletonList. And also \nreplaced some HashSets by SizeAdaptingSet (similarly for maps). Unfortunately, all these changes did \nnot reduce the minimal heap size required to run PMD. There are two main reasons for this. The .rst is \nthat most of the reduced collections are short lived. The second is that most of the long-lived collection \ndata in PMD is large and stable HashSets as well as large ArrayLists. However, even though our modi.\u00adcations \ndid not reduce the minimal heap size. The number of GCs reduced by 16% which led to a runtime improvement \nof 8.33%. Soot SOOT s heap consists of many small objects that are long\u00adlived. It s intermediate representation \nof program entities makes intensive use of Collection classes. For the most part, SOOT uses ArrayLists \nfor .exibility. However, the initial capacity of the lists is rarely provided, and the overall utilization \nof the lists is rather low (overall, around 25%). For cases in which lists are known to be singletons, \nSOOT sometimes uses a designated type SingletonListto reduce space overhead.  The collection choices \nwe applied in SOOT were simple. Using our tool, we .rst observed that in the few top contexts in which \nArrayLists were used to store singletons (by construction), the constructed collections are never modi.ed, \nand replaced them with immutable SingletonList (e.g., in JIfStmt). We note that the SOOT team has made \na similar selection for other commonly used types. The second suggestion CHAMELEON pointed out is the \nlarge potential saving for ArrayLists created in useBoxes methods. The idiom there is one of aggregation \nof used values up a tree. Every node creates an ArrayList of its uses, and aggregates uses from its children. \nThe result is the creation of many ArrayLists that are being rolled into other ArrayList using addAll. \nAvoiding all temporaries requires a major rewrite of the code, but even without rewriting the code, we \nselected proper initial sizes for these lists. The overall result for SOOT was a saving of 6% in space, \nand 11% improvement in the running time. TVLA Most of the heap in TVLA is dedicated to storing the ab\u00adstract \nprogram states that arise during abstract interpretation. The abstract program states use collections \nto store the state informa\u00adtion. Most of the collection data is stored in HashMapsfrom seven contexts. \nCHAMELEON points this collections as ones that can be replaced by ArrayMaps. Replacing these collections \nprovides a minimal-heap reduction of 53.95%. CHAMELEON also pointed an initial size setting for several \ncontexts and LinkedList that can be replaced by an ArrayList.  5.4 Discussion Experience with Fully \nAutomatic Replacement Our tool can run in fully-automatic mode in which replacement of collections is \nperformed during program execution. Due to the high cost of obtaining allocation contexts, we expected \nthe tool to incur a high time overhead, and only evaluated its effectiveness in terms of space reduction. \nWe ran the tool in the fully automatic mode for all of our benchmarks to evaluate the quality of its \nreplacement decisions. Much to our surprise, for most benchmarks, the overall slowdown was noticeable, \nbut not prohibitive. For TVLA, the space saving achieved was identical to the one we got with the manual \nmodi.cation. However, the impact on running time was signi.cant, due to the cost of obtaining allocation \ncontexts. Overall, TVLA suffered a slowdown of 35%. For space\u00adcriticalapplicationssuchas TVLA thismaybeanacceptabletradeoff \nin practice. The only benchmark for which the slowdown was prohibitive (6x slowdown) was PMD, which performs \nmassive rapid allocation of short-lived collections, which ampli.ed the cost of obtaining allocation \ncontexts. Our experiments indicate that the performance bottleneck stand\u00ading in the way to fully automatic \nreplacement is the task of obtain\u00ading an allocation context. We believe that with better VM support for \nthis functionality, fully online replacement is within reach. Iterators In many of our benchmarks, we \nhave observed the (somewhat expected) massive creation of iterator objects. Quite often, the iterators \nwere created over empty collections. For some of the collection interfaces (e.g., Set), the creation \nof a new iterator object can be avoided in this case in favor of returning a .xed static empty iterator. \nHowever, some collection interfaces allow addition of new items through an iterator, and therefore require \nthat a new iterator object will be created even when the collection is empty. Specialized Partial Interfaces \nThe Java collection interfaces are rather rich, and pose signi.cant restrictions on the underlying im\u00adplementations. \nMore ef.cient implementations could be introduced if collection interfaces are minimized, or at least \nseparated. For ex\u00adample, the Listinterface currently supports a list iterator that can traverse the list \nboth forward and backward. For practical purposes, such interface precludes an underlying implementation \nof using a singly-linked list. While we can leverage static analysis to show place, it seems more desirable \nto modify the library interfaces to permit additional implementations.  6. Related Work Recent work \nby Jones and Ryder [15] suggests that allocation context is indicative of object behavior and argues \nthat GC should take advantage of this (rather than relying on .xed heuristics). We use a similar observation \nto gather semantic-oriented object metrics and perform corrective actions accordingly. The challenge \nof freeing the user from managing and choosing the right data structure for their application is not \na new one. For example, in the context of the high-level language SETL, Dewar et. al [12] suggest the \nusage of a special sublanguage to declaratively specify the type of a data structure that a set or a \nmap of a SETL program should use. A compiler then takes as input the SETL program and the data structure \nspeci.cation in the sublanguage and outputs an ef.cient implementation. More work on this subject by \nSchonberg et. al [23] focuses on eliminating the need for manually specifying the structures in a sub-language. \nIt proposes an analysis that takes as input a pure SETL program and automatically infers suitable data \nstructures for it. A similar line of work based on static analysis is presented by Low [19]. In contrast, \nour work is done in the context of a lower-level language (Java) where the operations on the data-structure \n(collection) are explicit. Further, our work is centered around dynamic (rather than static) analysis. \nThe mere size of current programs combined with modern language features make it challenging to statically \noptimize collection usage. Active Harmony [11] is a system for automatic tuning of pro\u00adgrams. The system \ncontains a layer for automatic tuning of parame\u00adters as well as a library speci.cation layer that helps \nthe application select the right library to execute. Active Harmony requires each library to provide \na performance-evaluation function, and a cost\u00adestimate function. The functions are used by the tuning \nalgorithm to evaluate the performance of the library, or estimate its possible behavior. In contrast, \nour work targets general purpose object ori\u00adented programs where the program is executed in a runtime \nenvi\u00adronment, and optimizes this environment to collect valuable infor\u00admation. In addition, we use allocation \ncontexts to share historical information between objects as well as gain some metric of stability of \ncollection behavior. Moreover, our work combines the GC and VM information per context to decide which \ncontexts are worthy to optimize. More recent work dealing with the challenges of using custom collections \nin Java is that of Sutter et. al [25]. They apply static analysis to determine when a replacement of \nan existing collec\u00adtion type with a custom type is possible without violation of type constraints. Further, \nthey pro.le several applications to determine where a replacement may be possible. Subject to the type \ncon\u00adstraints, their analysis automatically replaces existing types with custom types. Their replacement \nis based on allocation site (rather than context as is in our case). Their pro.ling information does \nnot include heap information (as we do via VM support). We see our work as complementary. We can provide \na more detailed pro.ling information and then use a static analysis to determine when it is safe to replace \none type with another. However, because our sys\u00adtem supports wrappers, we are able to always make a replacement \nas the type safety cannot be violated. Recently, there has been work on application-speci.c selection \nof GCs, see Soman and Krintz [24] for details. The challenges they face are broadly similar to ours: \nwhen should one switch from one GC to another and what application characteristics should switch\u00ading \ntake into account. For example, the authors describe a scenario of switching to a GC that is tuned for \nresource-constrained en\u00advironments when the memory becomes scarce. GC switching oc\u00adcurs at pre-de.ned \npoints when all application threads are stopped. Switching GCs is complex as it may involve on stack \nreplacement to adjust methods to the speci.c GC (e.g. use write barriers for gen\u00aderational GC). In our \ncase, switching is localized as it occurs when a collection object is allocated which does not require \nus to stop ap\u00adplication threads. An interesting item of future work is looking into GC strategies that \nhave semantic knowledge of collection objects. For example, the GC may allocate ArrayList and its internal \nobject array together for locality purposes.  Recently, there has been work on semantically modifying \nthe GC to detect various correctness properties, see [5, 6]. In our case, we extended the GC to gather \ncontext-speci.c collection informa\u00adtion. We believe that exploring conceptually small but highly ben\u00ade.cial \nsemantic extensions to the VM is a fruitful area of research. Additional research were done in the .eld \nof automatic tuning. A works for automatically tuning linear algebra was done by Wha\u00adley and Dongarra \n[27] in the ATLAS project. Their work automat\u00adically generates ef.cient linear algebra routines for a \ngiven micro\u00adprocessor. They show an automatic generation of matrix multipli\u00adcation routines for different \nhardware architectures. A work for au\u00adtomatically choosing a decision heuristic for a SAT solver was \ndone by Lagoudakis and Littman [17]. In their work a decision heuris\u00adtic is chosen according to a value \nfunction, which is calculated on the current state of the search. The value function is created before\u00adhand, \nusing a training set. There are also works on dynamic pre\u00adtenuring in GC [13, 16]. These works use the \nsame notion as ours of automatic tuning, however, these works do not try to select and manage data structures \nand tackle different problems in a different domain.  Acknowledgments We thank Matthew Arnold, Nick \nMitchell, Mooly Sagiv, and Gary Sevitsky.  References [1] Apache collections. http://commons.apache.org/collections/. \n[2] Google collections. http://code.google.com/p/google-collections/. [3] Javolution collections. http://javolution.org/. \n[4] Trove collections. http://trove4j.sourceforge.net/. [5] AFTANDILIAN, E., AND GUYER, S. Z. GC assertions: \nUsing the garbage collector to check heap properties. In MSPC (2008), ACM. [6] ARNOLD, M., VECHEV, M., \nAND YAHAV, E. QVM: an ef.cient runtime for detecting defects in deployed systems. In OOPSLA 08: Proceedings \nof conference on Object oriented programming systems languages and applications (2008), pp. 143 162. \n[7] BARRETT, D. A., AND ZORN, B. G. Using lifetime predictors to improve memory allocation performance. \nIn PLDI 93: Proceedings of the ACM SIGPLAN 1993 conference on Programming language design and implementation \n(New York, NY, USA, 1993), ACM, pp. 187 196. [8] BLACKBURN, S. M., GARNER, R., HOFFMAN, C., KHAN, A. \nM., MCKINLEY, K. S., BENTZUR, R., DIWAN, A., FEINBERG, D., FRAMPTON, D., GUYER, S. Z., HIRZEL, M., HOSKING, \nA., JUMP, M., LEE, H., MOSS, J. E. B., PHANSALKAR, A., STEFANOVI C\u00b4, D., VANDRUNEN, T., VON DINCKLAGE, \nD., AND WIEDERMANN, B. The DaCapo benchmarks: Java benchmarking development and analysis. In OOPSLA 06: \nconf. on Object-Oriented Programing, Systems, Languages, and Applications (2006), pp. 169 190. [9] BLACKBURN, \nS. M., SINGHAI, S., HERTZ, M., MCKINELY, K. S., AND MOSS, J. E. B. Pretenuring for java. In OOPSLA 01: \nProceedings of the 16th ACM SIGPLAN conference on Object\u00adoriented programming, systems, languages, and \napplications (New York, NY, USA, 2001), ACM, pp. 342 352. [10] BOND, M. D., AND MCKINLEY, K. S. Probabilistic \ncalling context. SIGPLAN Not. 42, 10 (2007), 97 112. [11] CHUNG, I.-H., AND HOLLINGSWORTH, J. K. Runtime \nselection among different api implementations. Parallel Processing Letters 13, 2 (2003), 123 134. [12] \nDEWAR, R. K., ARTHUR, LIU, S.-C., SCHWARTZ, J. T., AND SCHONBERG, E. Programming by re.nement, as exempli.ed \nby the setl representation sublanguage. ACM Trans. Program. Lang. Syst. 1, 1 (1979), 27 49. [13] HARRIS, \nT. L. Dynamic adaptive pre-tenuring. In ISMM 00: Proceedings of the 2nd international symposium on Memory \nmanagement (New York, NY, USA, 2000), ACM, pp. 127 136. [14] HOVEMEYER, D., AND PUGH, W. Finding bugs \nis easy. In OOPSLA 04: Companion to the conference on Object-oriented programming systems, languages, \nand applications (2004), pp. 132 136. [15] JONES, R. E., AND RYDER, C. A study of java object demographics. \nIn ISMM 08: Proceedings of the 7th international symposium on Memory management (2008), pp. 121 130. \n[16] JUMP, M., BLACKBURN, S. M., AND MCKINLEY, K. S. Dynamic object sampling for pretenuring. In ISMM \n04: Proceedings of the 4th international symposium on Memory management (New York, NY, USA, 2004), ACM, \npp. 152 162. [17] LAGOUDAKIS, M. G., AND LITTMAN, M. L. Learning to select branching rules in the dpll \nprocedure for satis.ability. In SAT (2001). [18] LEV-AMI, T., AND SAGIV, M. TVLA: A framework for Kleene \nbased static analysis. In Saskatchewan (2000), vol. 1824 of Lecture Notes in Computer Science, Springer-Verlag, \npp. 280 301. [19] LOW, J. R. Automatic data structure selection: an example and overview. Commun. ACM \n21, 5 (1978), 376 385. [20] MITCHELL, J. C. Representation independence and data abstraction. In POPL \n86: Proceedings of the 13th ACM SIGACT-SIGPLAN symposium on Principles of programming languages (New \nYork, NY, USA, 1986), ACM, pp. 263 276. [21] MITCHELL, N., AND SEVITSKY, G. Leakbot: An automated and \nlightweight tool for diagnosing memory leaks in large java applications. In ECOOP 2003 -Object-Oriented \nProgramming, 17th European Conference (2003), vol. 2743 of Lecture Notes in Computer Science, pp. 351 \n377. [22] MITCHELL, N., AND SEVITSKY, G. The causes of bloat, the limits of health. In OOPSLA 07: Proceedings \nof the 22nd annual ACM SIGPLAN conference on Object oriented programming systems and applications (New \nYork, NY, USA, 2007), ACM, pp. 245 260. [23] SCHONBERG, E., SCHWARTZ, J. T., AND SHARIR, M. Automatic \ndata structure selection in setl. In POPL 79: Proceedings of the 6th ACM SIGACT-SIGPLAN symposium on \nPrinciples of programming languages (New York, NY, USA, 1979), ACM, pp. 197 210. [24] SOMAN, S., AND \nKRINTZ, C. Application-speci.c garbage collection. J. Syst. Softw. 80, 7 (2007), 1037 1056. [25] SUTTER, \nB. D., TIP, F., AND DOLBY, J. Customization of java library classes using type constraints and pro.le \ninformation. In ECOOP 2004 -Object-Oriented Programming, 18th European Conference, Oslo, Norway, June \n14-18, 2004 (2004), vol. 3086 of Lecture Notes in Computer Science, pp. 585 610. [26] VALL \u00b4EE-RAI, R., \nCO, P., GAGNON, E., HENDREN, L., LAM, P., AND SUNDARESAN, V. Soot -a java bytecode optimization framework. \nIn CASCON 99: Proceedings of the 1999 conference of the Centre for Advanced Studies on Collaborative \nresearch (1999), IBM Press, p. 13. [27] WHALEY, C. R., AND DONGARRA, J. J. Automatically tuned linear \nalgebra software. In Supercomputing (1998). [28] ZHUANG, X., SERRANO, M. J., CAIN, H. W., AND CHOI, J. \nD. Accurate, ef.cient, and adaptive calling context pro.ling. In PLDI 06 (2006), pp. 263 271.  \n\t\t\t", "proc_id": "1542476", "abstract": "<p>Languages such as Java and C#, as well as scripting languages like Python, and Ruby, make extensive use of Collection classes. A collection implementation represents a fixed choice in the dimensions of operation time, space utilization, and synchronization. Using the collection in a manner not consistent with this fixed choice can cause significant performance degradation. In this paper, we present CHAMELEON, a low-overhead automatic tool that assists the programmer in choosing the appropriate collection implementation for her application. During program execution, CHAMELEON computes elaborate trace and heap-based metrics on collection behavior. These metrics are consumed on-thefly by a rules engine which outputs a list of suggested collection adaptation strategies. The tool can apply these corrective strategies automatically or present them to the programmer. We have implemented CHAMELEON on top of a IBM's J9 production JVM, and evaluated it over a small set of benchmarks. We show that for some applications, using CHAMELEON leads to a significant improvement of the memory footprint of the application.</p>", "authors": [{"name": "Ohad Shacham", "author_profile_id": "81100234221", "affiliation": "Tel Aviv University, Tel Aviv, Israel", "person_id": "P1464332", "email_address": "", "orcid_id": ""}, {"name": "Martin Vechev", "author_profile_id": "81100269652", "affiliation": "IBM Research, Hawthorne, NY, USA", "person_id": "P1464333", "email_address": "", "orcid_id": ""}, {"name": "Eran Yahav", "author_profile_id": "81100285431", "affiliation": "IBM Research, Hawthorne, NY, USA", "person_id": "P1464334", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1542476.1542522", "year": "2009", "article_id": "1542522", "conference": "PLDI", "title": "Chameleon: adaptive selection of collections", "url": "http://dl.acm.org/citation.cfm?id=1542522"}