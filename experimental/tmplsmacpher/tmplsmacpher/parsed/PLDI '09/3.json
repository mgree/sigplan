{"article_publication_date": "06-15-2009", "fulltext": "\n PetaBricks: A Language and Compiler for Algorithmic Choice Jason Ansel Cy Chan Yee Lok Wong Marek Olszewski \nQin Zhao Alan Edelman Saman Amarasinghe Computer Science and Arti.cial Intelligence Laboratory Massachusetts \nInstitute of Technology Cambridge, MA, USA {jansel, cychan, ylwong, mareko, qin zhao, edelman, saman}@csail.mit.edu \nAbstract It is often impossible to obtain a one-size-.ts-all solution for high performance algorithms \nwhen considering different choices for data distributions, parallelism, transformations, and blocking. \nThe best solution to these choices is often tightly coupled to different architectures, problem sizes, \ndata, and available system resources. In some cases, completely different algorithms may provide the \nbest performance. Current compiler and programming language techniques are able to change some of these \nparameters, but today there is no simple way for the programmer to express or the compiler to choose \ndifferent algorithms to handle different parts of the data. Existing solutions normally can handle only \ncoarse\u00adgrained, library level selections or hand coded cutoffs between base cases and recursive cases. \nWe present PetaBricks, a new implicitly parallel language and compiler where having multiple implementations \nof multiple algorithms to solve a problem is the natural way of programming. We make algorithmic choice \na .rst class construct of the language. Choices are provided in a way that also allows our compiler to \ntune at a .ner granularity. The PetaBricks compiler autotunes programs by making both .ne-grained as \nwell as algorithmic choices. Choices also include different automatic parallelization techniques, data \ndistributions, algorithmic parameters, transformations, and blocking. Additionally, we introduce novel \ntechniques to autotune algo\u00adrithms for different convergence criteria. When choosing between various \ndirect and iterative methods, the PetaBricks compiler is able to tune a program in such a way that delivers \nnear-optimal ef.ciency for any desired level of accuracy. The compiler has the .exibility of utilizing \ndifferent convergence criteria for the various components within a single algorithm, providing the user \nwith accuracy choice alongside algorithmic choice. Categories and Subject Descriptors D.3.2 [Programming \nLan\u00adguages]: Language Classi.cations Concurrent, distributed, and parallel languages; D.3.4 [Programming \nLanguages]: Processors Compilers General Terms Algorithms, Languages, Performance Permission to make \ndigital or hard copies of all or part of this work for personal or classroom use is granted without fee \nprovided that copies are not made or distributed for pro.t or commercial advantage and that copies bear \nthis notice and the full citation on the .rst page. To copy otherwise, to republish, to post on servers \nor to redistribute to lists, requires prior speci.c permission and/or a fee. PLDI 09, June 15 20, 2009, \nDublin, Ireland. Copyright c &#38;#169; 2009 ACM 978-1-60558-392-1/09/06. . . $5.00 1. Introduction While \ntraditional compiler optimizations can be successful at optimizing a single algorithm, when an algorithmic \nchange is required to boost performance, the burden is put on the programmer to incorporate the new algorithm. \nIf a composition of multiple algorithms is needed for the best performance, the programmer must write \nboth algorithms, the glue code to connect them together, and .gure out the best switch over points. Today \ns compilers are unable to change the nature of this composition because it is constructed with traditional \ncontrol logic such as loops and switches. In this work, we propose new language constructs that allow \nthe programmer to specify a menu of algorithmic choices and new compiler techniques to exploit these \nchoices to generate high performance yet portable code. Hand-coded algorithmic compositions are commonplace. \nA typical example of such a composition can be found in the C++ Standard Template Library (STL)1 routine \nstd::sort, which uses merge sort until the list is smaller than 15 elements and then switches to insertion \nsort. Our tests have shown that higher cutoffs (around 60-150) perform much better on current architectures. \nHowever, because the optimal cutoff is dependent on architecture, cost of the comparison routine, element \nsize, and parallelism, no single hard-coded value will suf.ce. This problem has been addressed for certain \nspeci.c algorithms by autotuning software, such as ATLAS (Whaley and Dongarra 1998) and FFTW (Frigo and \nJohnson 1998, 2005), which have training phases where optimal algorithms and cutoffs are automatically \nselected. Unfortunately, systems like this only work on the few algorithms provided by the library designer. \nIn these systems, algorithmic choice is made by the application without the help of the compiler. In \nthis work, we present PetaBricks, a new implicitly parallel programming language for high performance \ncomputing. Programs written in PetaBricks can naturally describe multiple algorithms for solving a problem \nand how they can be .t together. This information is used by the PetaBricks compiler and runtime to create \nand autotune an optimized hybrid algorithm. The PetaBricks system also optimizes and autotunes parameters \nrelating to data distribution, parallelization, iteration, and accuracy. The knowledge of algorithmic \nchoice allows the PetaBricks compiler to automatically parallelize programs using the algorithms with \nthe most parallelism. We have also developed a benchmark suite of PetaBricks programs. These benchmarks \ndemonstrate the importance of making algorithmic choices available to the compiler. In all cases, hybrid \nalgorithms, consisting of a non-trivial composition of user\u00ad 1 From the version of the libstdc++ included \nwith GCC 4.3.  provided algorithms, perform signi.cantly better than any one algorithm alone. In one \nof our benchmark programs, a multigrid solver for the Poisson equation, we demonstrate how to incorporate \nalgorithms with variable convergence criteria in the autotuning process. This capability is vital when \ncomposing direct (exact) and iterative (approximate) methods in a recursive structure in such a way that \nguarantees a speci.ed target accuracy for the output while ensuring near-optimal ef.ciency. 1.1 Motivating \nExample As a motivation example, consider the problem of sorting. There are a huge number of ways to \nsort a list. For example: insertion sort, quick sort, merge sort, bubble sort, heap sort, radix sort, \nand bucket sort. Most of these sorting algorithms are recursive, thus, one can switch between algorithms \nat any recursive level. This leads to an exponential number of possible algorithmic compositions that \nmake use of more than one primitive sorting algorithm. Since sorting is a well known problem, most readers \nwill have some intuition about the optimal algorithm: for very small inputs, insertion sort is faster; \nfor medium sized inputs, quick sort is faster (in the average case); and for very large inputs radix \nsort becomes fastest. Thus, the optimal algorithm might be a composition of the three, using quick sort \nand radix sort to recursively decompose the problem until the subproblem is small enough for insertion \nsort to take over. Once parallelism is introduced, the optimal algorithm might get more complicated. \nIt often makes sense to use merge sort at large sizes because it contains more parallelism than quick \nsort (the merging performed at each recursive level can also be parallelized). Even with this detailed \nintuition (which one may not have for other algorithms), the problem of writing an optimized sorting \nalgorithm is nontrivial. Using popular languages today, the programmer would still need to .nd the right \ncutoffs between algorithms. This has to be done through manually tuning or using existing autotuning \ntechniques that would require additional code to integrate. If the programmer puts too much control .ow \nin the inner loop for choosing between a wide set of choices, the cost of control .ow may become prohibitive. \nThe original simple code for sorting will be completely obscured by this glue, thus making the code hard \nto comprehend, extend, debug, port and maintain. PetaBricks solves this problem by automating both algorithm \nselection and autotuning in the compiler. The programmer speci.es the different sorting algorithms in \nPetaBricks and how they .t together, but does not specify when each one should be used. The compiler \nand autotuner will experimentally determine the best composition of algorithms to use and the respective \ncutoffs between algorithms. This has added bene.ts in portability. On a different architecture, the optimal \ncutoffs and algorithms may change. The PetaBricks program can adapt to this by merely retuning. 1.2 \nOutline Section 2 describes the PetaBricks language. Section 3 describes the implementation of the compiler \nand autotuning system. Section 4 describes our benchmark suite. Section 5 presents experimental results. \nSection 6 covers related work. Finally, Sections 7 and 8 describe future work and conclusions.  1.3 \nContributions We make the following contributions: We present the PetaBricks programming language, which, \nto best of our knowledge, is the .rst language that enables programmers to express algorithmic choice \nat the language level. While autotuners have exploited coarse-grained algorithmic choice at a programmatic \nlevel, to best of our knowledge this is the .rst compiler that incorporates .ne-grained algorithmic choices \nin program optimization.  We show how our compiler utilizes .ne-grained algorithmic choice to get signi.cant \nspeedup over conventional algorithms.  We show that PetaBricks programs adapt algorithmically to different \narchitectures to create truly portable programs. We demonstrate that a PetaBricks program autotuned locally \non an 8-way x86 64 performs 2.35x faster when compared to a con.guration trained on a 8-way Sun Niagara \n1 processor.  We show that PetaBricks programs are scalable because they can adapt to expose increasing \nparallelism as the number of cores increases. We demonstrate that a con.guration autotuned on 8 cores \nperforms 2.14x faster than a con.guration tuned on a single core, but executed on 8 cores.  We present \na suite of benchmarks to illustrate algorithmic choice in important scienti.c kernels, which appear in \napplica\u00adtions such as computational .uid dynamics, electrodynamics, heat diffusion, and quantum physics. \n We present a compiler that can autotune programs with complex trade-offs such that we ensure the best \nperformance for all required levels of accuracy.  2. PetaBricks Language In designing the language we \nhad the following major goals: Expose algorithmic choices to the compiler  Allow choices to specify \ndifferent granularities and corner cases  Expose all valid execution orders, to allow parallel execution \n Automate consistency checks between different choices  Provide .exible data structures, including \nn-dimensional ar\u00adrays, trees, and sparse representations  The language is built around two major constructs, \ntransforms and rules. The transform, analogous to a function, de.nes an algorithm that can be called \nfrom other transforms, code written in other languages, or invoked from the command line. The header \nfor a transform de.nes to, from, and through arguments, which represent inputs, outputs, and intermediate \ndata used within the transform. The size in each dimension of these arguments is expressed symbolically \nin terms of free variables, the values of which must be determined by the PetaBricks runtime. The user \nencodes choice by de.ning multiple rules in each transform. Each rule de.nes how to compute a region \nof data in order to make progress towards a .nal goal state. Rules have explicit dependencies parametrized \nby free variables set by the compiler. Rules can have different granularities and intermediate state. \nThe compiler is required to .nd a sequence of rule applications that will compute all outputs of the \nprogram. The explicit rule dependencies allow automatic parallelization and automatic detection and handling \nof corner cases by the compiler. The rule header references to and from regions which are the inputs \nand outputs for the rule. The compiler may apply rules repeatedly, with different bindings to free variables, \nin order to compute larger data regions. Additionally, the header of a rule can specify a where clause \nto limit where a rule can be applied. The body of a rule consists of C++-like code to perform the actual \nwork. PetaBricks does not contain an outer sequential control .ow. The user speci.es which transform \nto apply, but not how to apply it. The decision of when and which rules to apply is left up the compiler \nand runtime system to determine. This has the dual  1 transform MatrixMultiply 2 from A[c,h], B[w,c] \n3 to AB[w, h] 4 { 5 // Base case , compute a single element 6 to (AB. cell (x,y) out) 7 from (A. row \n(y) a, B. column (x) b) { 8 out = dot(a,b); 9 } 10 11 // Recursively decompose in c 12 to (AB ab) 13 \nfrom (A. region (0, 0, c/2, h )a1, 14 A. region (c/2, 0, c, h )a2, 15 B. region (0, 0, w, c/2) b1, 16 \nB. region (0, c/2, w, c )b2) { 17 ab = MatrixAdd(MatrixMultiply(a1, b1), 18 MatrixMultiply(a2, b2)); \n19 } 20 21 // Recursively decompose in w 22 to (AB. region (0, 0, w/2, h ) ab1, 23 AB. region (w/2, 0, \nw, h ) ab2) 24 from ( Aa, 25 B. region (0, 0, w/2, c ) b1, 26 B. region (w/2, 0, w, c ) b2) { 27 ab1 \n= MatrixMultiply(a, b1); 28 ab2 = MatrixMultiply(a, b2); 29 } 30 31 // Recursively decompose in h 32 \nto (AB. region (0, 0, w, h/2) ab1, 33 AB. region (0, h/2, w, h ) ab2) 34 from (A. region (0, 0, c, h/2) \na1, 35 A. region (0, h/2, c, h )a2, 36 Bb) { 37 ab1=MatrixMultiply(a1, b); 38 ab2=MatrixMultiply(a2, \nb); 39 } 40 } Figure 1. PetaBricks source code for MatrixMultiply advantages of both exposing algorithmic \nchoices to the compiler and enabling automatic parallelization. It also gives the compiler a large degree \nof freedom to autotune iteration order and storage. Figure 1 shows an example PetaBricks transform, that \nperforms a matrix multiplication. The transform header is on lines 1 to 3. The .rst rule (line 6 to 9) \nis the straightforward way of computing a single matrix element. With the .rst rule alone the transform \nwould be correct, the remaining rules add choices. Rules two, three, and four (line 12 to 39) represent \nthree ways of recursively decomposing matrix multiply into smaller matrix multiplies. The compiler must \npick when to apply these recursive decompositions. The last two rules are actually not needed because \nthey are automatically inferred by the compiler as it explores blocking strategies for iteration. The \nautotuner discovers that the last two rules provide no advantage over the compiler s intrinsic strategies \nand correctly chooses not to use them. In addition to choices between different algorithms, many al\u00adgorithms \nhave con.gurable parameters that change their behavior. A common example of this is the branching factor \nin recursively algorithms such as merge sort or radix sort. To support this PetaBricks has a tunable \nkeyword that allows the user to export custom parameters to the autotuner. PetaBricks analyzes where \nthese tunable values are used, and autotunes them at an appropriate time in the learning process. PetaBricks \ncontains the following additional language features that will not be discussed here in detail: %{ ... \n}% escapes used to embed raw C++ in the output .le. This is primarily used for calling external libraries. \nExternal libraries must be thread safe.  A generator keyword for speci.ng a transform to be used to \nsupply input data during training.  Matrix versions, with a A<0..n> syntax, useful when de.ning iterative \nalgorithms. This construct is syntactic sugar for adding an extra dimension to the matrix, which may \nthen be collapsed by analysis.  Rule priorities and where clauses are used to handle corner cases gracefully. \n Template transforms, similar to templates in C++, where each template instance is autotuned separately. \n 3. Implementation The PetaBricks implementation consists of three components: a source-to-source compiler \nfrom the PetaBricks language to C++;  an autotuning system and choice framework to .nd optimal choices \nand set parameters; and  a runtime library used by the generated code.  The relationship between these \ncomponents is depicted in Figure 2. First, the source-to-source compiler executes and performs static \nanalysis. The compiler encodes choices and tunable parameters in the output code so that autotuning can \nbe performed. When autotuning is performed (either at compile time or at installation time), it outputs \nan application con.guration .le that controls when different choices are made. This con.guration .le \ncan be tweaked by hand to force speci.c choices. Optionally, this con.guration .le can be fed back into \nthe compiler and applied statically to eliminate unused choices and allow additional optimizations. \n3.1 PetaBricks Compiler To help illustrate the compilation process we will use the example transform \nRollingSum, shown in Figure 3. RollingSum computes an incremental (sometimes known as a cumulative) sum \nof an input list. It includes two rules: rule 0 computes an output directly, by iterating all input elements \nto the left; and rule 1 computes a value using a previously computed value to the left. An algorithm \nusing only rule 0 is slower (T(n 2) operations), but can be executed in a data parallel way. An algorithm \nusing only rule 1 is faster (T(n) operations), but has no parallelism and must be run sequentially. Compilation \nconsists of the following main phases. The intermediate representation is built up as the phases proceed. \nIt starts as an abstract syntax tree and ends as a dependency graph. All compilation is done on symbolic \nregions of an unknown size and is general to any number of dimensions. The compilation steps are as follows: \nParsing and normalization. First, the input language is parsed into an abstract syntax tree. Rule dependencies \nare normalized by converting all dependencies into region syntax, assigning each rule a symbolic center, \nand rewriting all dependencies to be relative to this center. (This is done using the Maxima symbolic \nalgebra Figure 2. Interactions between the compiler and output binaries. First, in Steps 1 and 2, the \ncompiler reads the source code and generates an autotuning binary. Next, in Step 3, autotuning is run \nto generate a choice con.guration .le. Finally, either the autotuning binary is used with the con.guration \n.le (Step 4a), or the con.guration .le is fed back into a new run of the compiler to generate a statically \nchosen binary (Step 4b).   1 transform RollingSum 2 from A[n] 3 to B[n] 4 { 5 // rule0 : sum all 6 \nto (B. cell (i) b) 7 b=sum(in); 8 } 9 10 // rule1 : use the 11 to (B. cell (i) b) 12 13 b=a+leftSum \n; 14 } 15 } elements to the left from (A. region (0, i) in) { previously computed value from (A. cell \n(i) a, B. cell (i -1) leftSum ) { Figure 3. PetaBricks source code for RollingSum. A simple example \nused to demonstrate the compilation process. The output element Bx is the sum of the input elements A0..Ax. \nlibrary (Rand 1984).) In our RollingSum example, the center of both rules is equal to i, and the dependency \nnormalization does not do anything other than replace variable names. For other inputs, this transformation \nwould simplify the dependencies. For example, if 1 were added to every coordinate containing i in the \ninput to rule 0 (leaving the meaning of the rule unchanged), the compiler would then assign the center \nto be i +1 and the dependencies would be been automatically rewritten to remove the added 1. Applicable \nregions. Next, the region where each rule can legally be applied, called an applicable, is calculated. \nThese are .rst calculated for each dependency and then propagated upwards with intersections (this is \nagain done by the linear equations solver and inference system). In rule 0 of our RollingSum example, \nboth b and in (and thus the entire rule) have an applicable region of [0,n). In rule 1 a and b have applicable \nregions of [0,n) and leftSum has an applicable region of [1,n) because it would read off the array for \ni =0. These applicable regions are intersected to get an applicable region for rule 1 of [1,n). Applicable \nregions can also be constrained with user de.ned where clauses, which are handled similarly. Choice grid \nanalysis. Next, we construct a choice grid for each matrix type. The choice grid divides each matrix \ninto rectilinear regions where a uniform set of rules are applicable. It does this using an inference \nsystem to sort the applicable regions and divide them into smaller, simpli.ed regions. In our RollingSum \nexample, the choice grid for B is: [0, 1) = {rule 0}[1,n)= {rule 0, rule 1} and A is not assigned a choice \ngrid because it is an input. For anal\u00adysis and scheduling these two regions are treated independently. \nIt is in the choice grid phase that rule priorities are applied. In each region, all rules of non-minimal \npriority are removed. This feature is not used in our example code, but if the user had only provided \nrule 1, he could have added special handler for [0, 1) by specifying a secondary rule. This mechanism \nbecomes especially useful in higher dimensions where there are more corner cases. Non-rectilinear regions \ncan also be created using where clauses on rules. In applicable regions and choice grids the bounding \nbox for these regions is computed and used. An analysis pass of the choice grid handles these regions. \nFor each rectilinear region in the choice grid, if some rules are restricted by where clauses, these \nrestricted rules are replaced by meta-rules that are unrestricted. These meta-rules are constructed by \n.nding sets of rules that cover the entire region, and packaging them up into a single meta-rule. Multiple \nmeta-rules are added to encode any choice. Figure 4. Choice dependency graph for RollingSum (in Figure \n3). Arrows point the opposite direction of dependency (the direction data .ows). Edges are annotated \nwith rules and directions, offsets of 0 are not shown. Choice dependency graph analysis. A data dependency \ngraph is constructed using the simpli.ed regions from the choice grid. The data dependency graph consists \nof edges between these symbolic regions. Each edge is annotated with the set of choices that require \nthat edge, a direction of the data dependency, and an offset between rule centers for that dependency. \nThe direction and offset information are especially useful for parallel scheduling; in many cases, they \neliminate the need for a barrier before beginning the computation of a dependant matrix. Figure 4 shows \nthe choice dependency graph for our example RollingSum. The three nodes correspond to the input matrix \nand the two regions in the choice grid. Each edge is annotated with the rules that require it along with \nthe associated directions and offsets. These annotations allow matrices to be computed in parallel when \nthe rules chosen allow. This high level coarse graph is passed to the dynamic scheduler to execute in \nparallel at runtime. The dependency edges tell the scheduler when it can split regions to compute them \nin parallel. The cost of the dynamic scheduler is negligible because scheduling is done from the top \ndown on large regions of the matrix.  The graph for RollingSum does not require simpli.cation, however \nif the graph were more complicated analysis would be required to simplify it. This simpli.cation process \nis primarily focused around removing cycles. The input graph can contain cycles (provided union of the \ndirections along the cycle points in towards a single hyper-quadrant), but the output schedule must be \na topologically sorted directed acyclic graph. Cycles are eliminated by merging strongly connected components, \ninto meta-nodes. The scheduler then .nds an axis and direction for iterating this larger node where the \ncycle is gone, it then recursively schedules the components making up this larger node using the remaining \nedges. The choice dependency graph is encoded in the output program for use by the autotuner and parallel \nruntime. It contains all information needed to explore choices and execute the program in parallel. These \nprocesses are explained in further detail in Sections 3.3 and 3.4. Code generation. Code generation has \ntwo modes. In the default mode choices and information for autotuning are embedded in the output code. \nThis binary can be dynamically tuned, which generates a con.guration .le, and later run using this con.guration \n.le. In the second mode for code generation, a previously tuned con.guration .le is applied statically \nduring code generation. The second mode is included since the C++ compiler can make the .nal code incrementally \nmore ef.cient when the choices are eliminated.  3.2 Parallelism in Output Code The PetaBricks runtime \nincludes a parallel work stealing dynamic scheduler. The scheduler works on tasks with a known interface. \nThe generated output code will recursively create these tasks and feed them to the dynamic scheduler \nto be executed. Dependency edges between tasks are detected at compile time and encoded in the tasks \nas they are created. A task may not be executed until all the tasks that it depends on have completed. \nThese dependency edges expose all available parallelism to the dynamic scheduler and allow it to change \nits behavior based on autotuned parameters. To expose parallelism and to help the dynamic scheduler schedule \ntasks in a depth-.rst search manner (see Section 3.4), the generated code is constructed such that functions \nsuspended due to a call to a spawned task, can be migrated and executed on a different processor. This \nis dif.cult to achieve as the function s stack frame and registers need to be migrated. We support this \nby generating continuation points, points at which a partially executed function may be converted back \ninto a task so that it can be rescheduled to a different processor. The continuation points are inserted \nafter any code that spawns a task. This is implemented by storing all needed state to the heap. The code \ngenerated for dynamic scheduling incurs some overhead, despite being heavily optimized. In order to amortize \nthis overhead, the output code that makes use of dynamic scheduling is not used at the leaves of the \nexecution tree where most work is done. The PetaBricks compiler generates two versions of every output \nfunction. The .rst version is the dynamically scheduled task\u00adbased code described above, while the second \nversion is entirely sequential and does not use the dynamic scheduler. Each output transform includes \na tunable parameter (set during autotuning) to decide when to switch from the dynamically scheduled to \nthe sequential version of the code. 3.3 Autotuning System and Choice Framework Autotuning is performed \non the target system so that optimal choices and cutoffs can be found for that architecture. We have \nfound that the best solution varies both by architecture and number of processors, these results are \ndiscussed in Section 5. The autotuning library is embedded in the output program whenever choices are \nnot statically compiled in. Autotuning outputs an application con.guration .le containing choices. This \n.le can either be used to run the application, or it can be used by the compiler to build a binary with \nhard-coded choices. The autotuner uses the choice dependency graph encoded in the compiled application. \nThis choice dependency graph is also used by the parallel scheduler discussed in Section 3.4. This choice \ndependency graph contains the choices for computing each region and also encodes the implications of \ndifferent choices on dependencies. The intuition of the autotuning algorithm is that we take a bottom-up \napproach to tuning. To simplify autotuning, we assume that the optimal solution to smaller sub-problems \nis independent of the larger problem. In this way we build algorithms incrementally, starting on small \ninputs and working up to larger inputs. The autotuner builds a multi-level algorithm. Each level consists \nof a range of input sizes and a corresponding algorithm and set of parameters. Rules that recursively \ninvoke themselves result in algorithmic compositions. In the spirit of a genetic tuner, a population \nof candidate algorithms is maintained. This population is seeded with all single-algorithm implementations. \nThe autotuner starts with a small training input and on each iteration doubles the size of the input. \nAt each step, each algorithm in the population is tested. New algorithm candidates are generated by adding \nlevels to the fastest members of the population. Finally, slower candidates in the population are dropped \nuntil the population is below a maximum size threshold. Since the best algorithms from the previous input \nsize are used to generate candidates for the next input size, optimal algorithms are iteratively built \nfrom the bottom up. In addition to tuning algorithm selection, PetaBricks uses an n-ary search tuning \nalgorithm to optimize additional parameters such as parallel-sequential cutoff points for individual \nalgorithms, iteration orders, block sizes (for data data parallel rules), data layout, as well as user \nspeci.ed tunable parameters. All choices are represented in a .at con.guration space. Dependencies between \nthese con.gurable parameters are exported to the autotuner so that the autotuner can choose a sensible \norder to tune different parameters. The autotuner starts by tuning the leaves of the graph and works \nits way up. In the case of cycles, it tunes all parameters in the cycle in parallel, with progressively \nlarger input sizes. Finally, it repeats the entire training process, using the previous iteration as \na starting point, a small number of times to better optimize the result.  3.4 Runtime Library The runtime \nlibrary is primarily responsible for managing paral\u00adlelism, data, and con.guration. It includes a runtime \nscheduler as well as code responsible for reading, writing, and managing inputs, outputs, and con.gurations. \nThe runtime scheduler dynamically schedules tasks (that have their input dependencies satis.ed) across \nprocessors to distribute work. When tasks reach a certain tunable cutoff size, they stop calling the \nscheduler and continue executing sequentially. Conversely, large data parallel tasks are divided up into \nsmaller tasks, to increase the amount of parallelism available to the scheduler. The scheduler attempts \nto maximize locality using a greedy algorithm that schedules tasks in a depth-.rst search order. Following \nthe approach taken by Cilk (Frigo et al. 1998), we distribute work with thread-private deques and a task \nstealing protocol. A thread operates on the top of its deque as if it were a stack, pushing tasks as \ntheir inputs become ready and popping them when a thread needs more work. When a thread runs out of work, \nit randomly selects a victim and steals a task from the bottom of the victim s deque. This strategy allows \na thread to steal another thread s most nested continuation, which preserves locality in the recursive \nalgorithms we observed. We use Cilk s THE protocol to allow the victim to pop items of work from its \ndeque without needing to acquire a lock in the common case.  3.5 Automated Consistency Checking A side \nbene.t of having multiple implementations of algorithms for solving the same problem is that the compiler \ncan check these algorithms against each other to make sure they produce consistent results. This helps \nthe user to automatically detect bugs and increase con.dence in code correctness. This automated checking \nmakes it advisable to include a slow reference implementation as a choice so that faster choices can \nbe checked against it. This consistency checking happens during autotuning when a special .ag is set. \nThe autotuner, by design, is already exploring the space of possible algorithms to .nd one that performs \nthe best. The consistency checking merely uses a .xed input during each autotuning round and ensures \nthat the same output is produced by every candidate algorithm. While not provably correct, this technique \nprovides good testing coverage. Notably, this technique also focuses more testing on the candidate algorithms \nthat are actually used as the autotuner hones in on an optimal choice. Some of our benchmarks use iterative \napproaches that do not produce exact answers. To support such code, our automated checker takes a threshold \nargument where differences below that threshold are ignored.  3.6 Deadlocks and Race Conditions Another \ntypical problem in hand written parallel code is deadlocks. Deadlocks cannot occur in PetaBricks because \nthe program s dependency graph is fully analyzed at compile time. Potential deadlocks manifest themselves \nas a cycle in the graph, and the PetaBricks compiler detects this cycle and reports an error to user. \nThis deadlock freedom guarantee, when using only the core PetaBricks language, is a great advantage. \nWhen external code, written in other languages, is called from PetaBricks, it is the programmers responsibility \nto ensure that the program executes without deadlocks. Similar to deadlocks, race conditions cannot exist \nin PetaBricks, except when caused by externally called code written in other languages. Since PetaBricks \nis implicitly parallel, the programmer cannot manually specify that two operations should run in parallel. \nInstead, analysis is performed by the compiler and tasks that do not depend on each other are automatically \nparallelized. If a race condition were to exist, then the compiler would see that dependency edge and \nnot run the two tasks in parallel. 4. Benchmarks In this section, we describe a set of benchmarks we \nimplemented to illustrate the capabilities of the PetaBricks compiler. The benchmarks were chosen to \nbe relevant, widely applicable scienti.c and computing kernels: solving Poisson s equation, the symmetric \ntridiagonal eigenvalue problem, sorting, and dense matrix multiply. 4.1 Poisson s Equation Poisson s \nequation is a partial differential equation that describes many processes in physics, electrostatics, \n.uid dynamics, and various other engineering disciplines. The continuous and discrete versions are \"2f \n= f and Tx = b, (1) where T , x, and b are the .nite difference discretizations of the Laplace operator, \nf, and f , respectively. (a) Dependencies for the red cells (b) Dependencies for the black cells Figure \n5. Checkerboard dependency pattern for Red-Black SOR. Black cells are shown in white for clarity. To \nsolve Poisson s equation on a 2D grid, we explore the use of four methods: one direct (band Cholesky \nfactorization through LAPACK s DPBSV routine) and three iterative (Jacobi Iteration, Red-Black Successive \nOver Relaxation (SOR), and Multigrid). From top to bottom, each of the iterative methods has a larger \noverhead, but yields a better asymptotic serial complexity (Demmel 1997). The table below lists the complexity \nof each algorithm, n is the number of cells in the grid. Algorithm Direct Jacobi SOR Multigrid Complexity \nn 2 n 2 n 1.5 n 4.1.1 Dependencies for SOR There are different implementations of data dependencies \nfor SOR, and we implement Red-Black ordering. Figure 5 shows the classi.cation of cells into red and \nblack (shown in white for clarity) depending on whether they are updated using neighboring values from \nthe previous or current iteration. Each cell depends on its neighbors, as indicated by the arrows in \nthe .gure. During the .rst half of an iteration, the red cells are updated using the black cells values \nfrom the previous iteration. During the second half of the iteration, the black cells are updated using \nthe red cells values from the current iteration. PetaBricks supports this complex dependency pattern \nby splitting the matrix into two temporary matrices each half the size of the input. One temporary matrix \ncontains only red cells, the other only black cells. Each iteration of SOR then involves updating each \nmatrix in turn. Arranging the data in such a manner leads to better cache behavior since memory is accessed \nin a dense fashion.  4.1.2 Multigrid Multigrid is a recursive and iterative algorithm that uses the \nsolution to a coarser grid resolution (by a factor of two) as part of the algorithm. For simplicity, \nwe assume all inputs are of size N =2k +1 for some positive integer k. Let x be the initial state of \nthe grid, and b be the right hand side of Equation (1). The full multigrid algorithm requires the use \nof a sequence of k V-cycles of increasing re.nement run in succession. In this section, we will focus \non tuning a single V-cycle; the methods employed can be extended to tune a full multigrid algorithm. \nThe pseudo code for this is shown in Figure 7. At the recursive call on line 6, the PetaBricks compiler \ncan make a choice of whether to continue making recursive calls to multigrid (shown as the solid diagonal \narrows) or take a shortcut by using the direct solver or one of the iterative solvers at the current \nresolution (shown as the dotted horizontal arrows). Figure 6 shows these possible paths of the multigrid \nalgorithm.  Figure 6. Choices in the multigrid algorithm. The diagonal arrows represent the recursive \ncase, while the dotted horizontal arrows represent the shortcut case where a direct or iterative solution \nmay be substituted. Depending on the desired level of accuracy a different choice may be optimal at each \ndecision point MULTIGRID-SIMPLE(x, b) 1: if N =3 then 2: Solve directly 3: else 4: Iterate using some \niterative method 5: Compute the residual and restrict to half resolution 6: Recursively call MULTIGRID-SIMPLE \non coarser grid 7: Interpolate result and add correction term to current solution 8: Iterate using some \niterative method 9: end if Figure 7. Pseudo code for MULTIGRID-SIMPLE. The idea of choice can be implemented \nby de.ning a top level function POISSON, which makes calls to either the direct, iterative, or recursive \nsolution, and having MULTIGRID call POISSON. The pseudo code for this is shown in Figure 8. POISSON(x, \nb) 1: either 2: Solve directly 3: Use an iterative method 4: Call MULTIGRID for some number of iterations \n5: end either MULTIGRID(x, b) 1: if N =3 then 2: Solve directly 3: else 4: Iterate using some iterative \nmethod 5: Compute the residual and restrict to half resolution 6: On the coarser grid, call POISSON 7: \nInterpolate result and add correction term to current solution 8: Iterate using some iterative method \n9: end if Figure 8. General pseudo code for choices in POISSON and MULTIGRID. Making the choice in line \n1 of POISSON has two implications. First, the time to complete the algorithm is choice dependent. Second, \nthe accuracy of the result is also dependent on choice since the various methods have different abilities \nto reduce error (depending on parameters such as number of iterations or weights). To make a fair comparison \nbetween the choices, we must take the accuracy of each choice into account. In the other algorithms we \nhave examined thus far, the compiler determines which choices to make based solely on some parameters \nof the input (such as the input size). In autotuning our Poisson solver, we also use the desired accuracy \nlevel to make that (a) (b) Figure 9. (a) Possible algorithmic choices with optimal set designated by \nsquares (both hollow and solid). The choices designated by solid squares are the ones remembered by the \nPetaBricks compiler, being the fastest algorithms better than each accuracy cutoff line. (b) Choices \nacross different accuracies in multigrid. At each level, the autotuner picks the best algorithm one level \ndown to make a recursive call. The path highlighted in red is an example of a possible path for accuracy \nlevel p2 determination. To that end, the autotuner keeps track of not just a single optimal algorithm \nat every recursion level, but a set of such optimal algorithms for varying levels of desired accuracy. \nIn the following sections, we assume we have access to representative training data so that the accuracy \nof our algorithms during tuning closely re.ects their accuracy during use.  4.1.3 Full Dynamic Programming \nSolution We will .rst describe a full dynamic programming solution to handling variable accuracy, then \nrestrict it to a discrete set of accuracies. We de.ne an algorithm s accuracy to be the ratio between \nthe RMS error of its input versus the RMS error of the output compared to optimal. Thus, a higher accuracy \nalgorithm is better. Let level k refer to an input size of N =2k +1. Suppose that for level k - 1, we \nhave solved for some set Ak-1 of optimal algorithms, where optimality is de.ned such that no optimal \nalgorithm is dominated by any other algorithm in both accuracy and compute time. In order to construct \nthe optimal set Ak, we try substituting all algorithms in Ak-1 for step 6 of MULTIGRID. We also try varying \nthe parameters in the other steps of the algorithm (e.g. the choice of iterative method and the number \nof iterations in steps 3 and 4 of POISSON and steps 4 and 8 of MULTIGRID). Trying all of these possibilities \nwill yield many algorithms that can be plotted as in Figure 9(a) according to their accuracy and compute \ntime. The optimal algorithms we add to Ak are the dominant ones designated by square markers. The reason \nto remember algorithms of multiple accuracies for use in step 6 of MULTIGRID is that it may be better \nto use a less accurate, fast algorithm and then iterate multiple times, rather than use a more accurate, \nslow algorithm. Note that even if we use a direct solver in step 6, the interpolation in step 7 will \ninvariably introduce error at the higher resolution.  4.1.4 The PetaBricks Solution The PetaBricks compiler \noffers an approximate version of the above solution. Instead of remembering the full optimal set Ak, \nthe compiler remembers the fastest algorithm yielding an accuracy of at least pi for each pi in some \nset {p1,p2,...,pm}. The vertical lines in Figure 9(a) indicate the discrete accuracy levels pi, and the \noptimal algorithms (designated by solid squares) are the ones remembered by PetaBricks. Each highlighted \nalgorithm is associated with a function POISSONi, which achieves accuracy pi on all input sizes.  To \nfurther narrow the search space, we only use SOR as the iteration function since it performs much better \nthan Jacobi for similar computation cost per iteration. In POISSONi, we .x the weight parameter of SOR \nto .opt, the optimal value for the 2D discrete Poisson s equation with .xed boundaries (Demmel 1997). \nIn MULTIGRIDi, we .x SOR s weight parameter to 1.15 (chosen by experimentation to be a good parameter \nwhen used in multigrid). We also .x the number of iterations of SOR in steps 4 and 8 in MULTIGRIDi to \none. The resulting accuracy-aware Poisson solver is a family of functions, where i is the accuracy parameter. \nThis family of functions is described in the pseudo code in Figure 10 POISSONi(x, b) 1: either 2: Solve \ndirectly 3: Iterate using SOR.opt until accuracy pi is achieved 4: For some j, iterate with MULTIGRIDj \nuntil accuracy pi is achieved 5: end either MULTIGRIDi(x, b) 1: if N =3 then 2: Solve directly 3: else \n4: Compute one iteration of SOR1.15 5: Compute the residual and restrict to half resolution 6: On the \ncoarser grid, call POISSONi 7: Interpolate result and add correction term to current solution 8: Compute \none iteration of SOR1.15 9: end if Figure 10. Pseudo code for family of functions POISSONi and MULTIGRIDi \nwhere i is the required accuracy, as used in the benchmark. The autotuning process must now determine \nwhat choices to make in POISSONi for each i and for each size input. Since the optimal choice for any \nsingle accuracy for an input of size 2k +1 depends on the optimal algorithms for all accuracies for inputs \nof size 2k-1 +1, the PetaBricks autotuner tunes all accuracies at a given level before moving to a higher \nlevel.  4.1.5 Performance The .nal set of multigrid algorithms produced by the autotuner can be visualized \nas in Figure 9(b). Each of the versions can call any of the other versions during its recursive calls \nto the lower level, and the optimal path may switch many times between accuracies as we recurse down \ntowards either the base case or a shortcut case. Figure 11 shows the performance of our autotuned multigrid \nalgorithm for accuracy 109. The autotuned algorithm uses accuracy levels of {10, 103 , 105 , 107 , 109} \nduring its recursive calls. The .gure compares the autotuned algorithm with the direct solver and iterated \ncalls to Jacobi, SOR, and MULTIGRID-SIMPLE (labeled Multigrid). Each of the iterative methods is run \nuntil an accuracy of at least 109 is achieved. The autotuned algorithm shown calls the direct algorithm \nfor small cases up to size N = 129, at which point it starts making recursive calls to MULTIGRID. The \nnumber of iterations computed at each level of recursion is determined by the autotuner to be optimal \ngiven the desired level of accuracy.  4.2 Symmetric Eigenproblem The symmetric eigenproblem is another \nproblem with broad applications in areas such as mechanics, quantum physics and structural engineering. \nGiven a symmetric n \u00d7 n matrix, we want to .nd its eigenvalues and/or eigenvectors. Deciding on which \nalgorithms to use depends on how many eigenvalues to .nd and whether eigenvectors are needed. Here we \nstudy the problem in which all the eigenvalues and eigenvectors are computed. 4.2.1 Algorithms and Choices \nTo .nd all the eigenvalues and eigenvectors of a symmetric matrix, we examine the use of three primary \nalgorithms, QR iteration, Bisection and inverse iteration, and Divide-and-conquer. The input matrix A \nis .rst reduced to A = QT QT , where Q is orthogonal and T is symmetric tridiagonal. All the eigenvalues \nand eigenvectors of T are then computed by the algorithm chosen. The eigenvalues of A and T are equal. \nThe eigenvectors of A are obtained by multiplying Q by the eigenvectors of T . The total work needed \nis O(n 3) for reduction of the input matrix and transforming the eigenvectors, and the cost associated \nwith each algorithm (Demmel 1997). The QR iteration applies the QR decomposition iteratively until T \nconverges to a diagonal matrix. It computes all the eigenvalues and eigenvectors in O(n 3) operations. \nBisection, followed by inverse iteration, .nds k eigenvalues and the corresponding eigenvectors in O(nk2) \noperations, resulting in a complexity of O(n 3) for .nding all eigenvalues and eigenvectors. This algorithm \nis based on a simple formula to count the number of eigenvalues less than a given value. Each eigenvalue \nand eigenvector thus can be computed independently, making the algorithm embarrassingly parallel . The \neigenproblem of tridiagonal T can also be solved by a divide-and-conquer approach. The eigenvalues and \neigenvectors of T can be computed using the eigenvalues and eigenvectors of two smaller tridiagonal matrices, \nand this can be done recursively. Divide-and-conquer requires O(n 3) work in the worst case. The PetaBricks \ntransforms for these three primary algorithms are implemented using LAPACK routines, as is MATLAB polyal\u00adgorithm \neig. Our optimized hybrid PetaBricks algorithm computes the eigenvalues . and eigenvectors X by automating \nchoices of these three basic algorithms. The pseudo code for this is shown in  EIG(T ) 1: either 2: \nUse QR to .nd . and X 3: Use BISECTION to .nd . and X 4: Recursively call EIG on submatrices T1 and T2 \nto get .1, X1, .2 and X2. Use results to compute . and X. 5: end either Figure 13. Pseudo code for eigenvector \nsolve. Figure 13. There are three algorithmic choices, two non-recursive and one recursive. The two non-recursive \nchoices are QR iterations, or bisection followed by inverse iteration. Alternatively, recursive calls \ncan be made. At the recursive call, the PetaBricks compiler will decide the next choices, i.e. whether \nto continue making recursive calls or switch to one of the non-recursive algorithms. Thus the PetaBricks \ncompiler chooses the optimal cutoff for the base case if the recursive choice is made. After autotuning, \nthe best algorithm choice was found to be divide-and-conquer for matrices larger than 48, and switching \nto QR iterations when the size of matrix n = 48.  4.2.2 Performance We implemented and compared the \nperformance of .ve algorithms in PetaBricks: QR iterations, bisection and inverse iteration, divide\u00adand-conquer \nwith base case n =1, divide-and-conquer algorithm with hard-coded cutoff at n = 25, and our autotuned \nhybrid algorithm. In .gure 12, these are labelled QR, Bisection, DC, Cutoff 25 and Autotuned respectively. \nThe input matrices tested were random symmetric tridiagonal. Our autotuned algorithm runs faster than \nany of the three primary algorithms alone (QR, Bisection and DC). It is also faster than the divide-and-conquer \nstrategy which switches to QR iteration for n = 25, which is the underlying algorithm of the LAPACK routine \ndstevd (Anderson et al. 1999).  4.3 Sort For the problem of sorting, we implemented the following algorithms \nin PetaBricks: insertion sort; quick sort; n-way merge sort (when n equals 2, merge sort employs a recursive \nmerge routine that can also be parallelized), where the compiler can select n; and a 16 bucket radix \nsort (a MSD variant that can recursively call any sorting algorithm). The concepts behind the choices \nin sort are discussed in Section 1.1. All of the algorithms are recursive Figure 14. Performance for \nsort on 8 cores. except for insertion sort. . Each of these algorithms recursively calls a generalized \nsort transform, which allows the compiler to switch algorithms at any level. Figure 14 shows the performance \nfor sort on 8 cores. Our au\u00adtotuner was able to achieve signi.cant performance improvements over any \nsingle algorithm. Surprisingly, the autotuned composite algorithm did not utilize radix sort, despite \nit being the second fastest algorithm. Instead, it built a hybrid algorithm using .rst 2-way merge sort, \nfollowed by quicksort, followed by a call to insertion sort for smaller inputs. The sharp bend in performance \nfor merge sort occurs at 1024 where the binary tree of merges grows from 10 to 11 levels. If the graph \nis extended to larger inputs, merge sort s performance forms a step ladder. When merge sort is used in \na autotuned hybrid algorithm this step ladder performance pattern disappears.  4.4 Matrix Multiply The \nfull PetaBricks code for the basic version of matrix multiply can be found in the introduction (Figure \n1). In addition to that example code we also implemented Strassen algorithm (fast matrix multiply). This \nresults in four recursive decompositions and one base case, for a total of .ve algorithmic choices. The \ncompiler also considers non-algorithmic choices including: transposing each of the inputs; various blocking \nstrategies; and various parallelization strategies. For matrix multiply, these non algorithmic choices \nmake a huge impact.  Figure 15 shows performance for various versions of matrix multiply. Since the \nnon-algorithmic optimizations (blocking and transposing) made a large difference performance of those \nopti\u00admizations are also shown. The series labeled Recursive is the recursive decomposition in the c dimension \nshown in Figure 1. The other two recursive decompositions are equivalent to blocking and thus are not \nshown. The autotuned algorithm uses a mixture of blocking, transpose, and the recursive decomposition. \n5. Results Figures 11, 12, 14, and 15 compare the performance of our autotuned algorithms to implementation \nthat only utilize a single algorithmic choice. In all cases the autotuned algorithm has signi.cant speedup. \nThese results were gathered on a 8-way (dual socket, quad core) Intel Xeon E7340 system running at 2.4 \nGHz. The system was running 64 bit CSAIL Debian 4.0 with Linux kernel 2.6.18 and GCC 4.1.2. 5.1 Autotuning \nParallel Performance and Scalability A great advantage of PetaBricks is that it allows a single program \nto be optimized for both sequential performance and parallel performance. We have observed our autotuner \nmake different choices when training in parallel. As a general trend we noticed much lower cutoffs to \nbases cases in sequential programs. In many cases entirely different algorithms are chosen. Of particular \nnote is the fact that algorithms tuned on 8 cores scale much better than algorithms tuned on 1 core. \nAs an example, when tuning sort on 1 core our autotuner picks radix sort with a cutoff of 98 where it \nswitches to 4-way merge sort after which it .nishes with insertion sort at a cutoff of 75. When tuned \nusing 8 cores the autotuner decides to use the 2-way-merge sort (with a parallelizable recursive merge) \nfunction until the input is smaller than 1420, after which it switches to quick sort. Finally, at inputs \nsmaller than 600, it switches to insertion sort. When both algorithms are run using 8 cores, the algorithm \ntuned on 8 cores performs 2.14x faster than the algorithms tuned on 1 core (as seen in Table 1).  5.2 \nEffect of Architecture on Autotuning Multicore architectures have drastically increased the processor \ndesign space resulting in a large variety of processor designs currently on the market. Such variance \nsigni.cantly hinders porting efforts of performance critical code. In this section, we present the results \nof PetaBricks autotuner when optimizing our sort benchmark on three parallel architectures designed for \na variety of purposes: Intel Core 2 Due mobile processor, Intel Xeon E7340 server processor, and the \nSun Fire T200 Niagara low power high throughput server processor. Table 1 illustrates the necessity of \ntuning your program for the architecture that you plan to run on. When autotuning our sort benchmark, \nwe found that con.gurations trained on a different setup than they are run on exhibit signi.cant slowdowns. \nFor example, even though they have the same number of cores, the autotuned con.guration .le from the \nNiagara machine results in a 2.35x loss of performance when used on the Xeon processor. On average we \nobserved a slowdown of 1.68x across all of the systems we tested. Table 2 displays the optimal con.gurations \nfor the sort benchmark after running the same autotuning process on the three architectures. It is interesting \nto note the dramatic differences between the choice of algorithms, composition switching points, and \nscalability. The Intel architectures (with larger computation to communication ratios) appear to perform \nbetter when PetaBricks produces code with less parallelism, suggesting that the cost of communication \noften outweighs any bene.ts from running code containing .ne-grained parallelism. On the other hand, \nthe Sun Niagara processor performs best when executing code with lots of parallelism as shown by the \nexclusive use of recursive algorithms. 6. Related Work A number of empirical autotuning frameworks have \nbeen devel\u00adoped for building ef.cient, portable libraries in speci.c domains. PHiPAC (Bilmes et al. 1997) \nis an autotuning system for dense matrix multiply, generating portable C code and search scripts to tune \nfor speci.c systems. ATLAS (Whaley and Dongarra 1998; Whaley and Petitet 2005) utilizes empirical autotuning \nto produce a cache-contained matrix multiply, which is then used in larger matrix computations in BLAS \nand LAPACK. FFTW (Frigo and Johnson 1998, 2005) uses empirical autotuning to combine solvers for FFTs. \nOther autotuning systems include SPARSITY (Im and Yelick 2001) for sparse matrix computations, SPIRAL \n(Puschel et al. 2005) for digital signal processing, UHFFT (Ali et al. 2007) for FFT on multicore systems, \nOSKI (Vuduc et al. 2005) for sparse matrix kernels, and autotuning frameworks for optimizing sequential \n(Li et al. 2004, 2005) and parallel (Olszewski and Voss 2004) sorting algorithms. In addition to these \nsystems, various performance models and tuning techniques (Williams et al. 2008; Vuduc et al. 2004; Brewer \n1995; Yotov et al. 2003; Lagoudakis and Littman 2000; Yu et al. 2004) have been proposed to evaluate \nand guide automatic performance tuning. There are a number of systems that provide high-level abstractions \nto ease the burden of programming adaptive appli\u00adcations. STAPL (Thomas et al. 2005) is an C++ template \nlibrary that support adaptive algorithms and autotuning. Paluska et al. propose a programming framework \n(Paluska et al. 2008) that allows programmers to specify goals of application behavior and techniques \nto satisfy those goals. The application hierarchically decomposes different situations and adapts to \nthem dynamically. ADAPT (Voss and Eigenmann 2000, 2001) augments compile\u00adtime optimizations with run-time \noptimizations based on dynamic information about architecture, inputs, and performance. It does not Table \n1. Slowdown when trained on a setup different than the one run on. Benchmark is sort on an input size \nof 100,000. Slowdowns are relative to training natively. Descriptions of abbreviated system names can \nbe found in Table 2.  Trained on Mobile Xeon 1-way Xeon 8-way Niagara Run on Mobile - 1.09x 1.67x 1.47x \nXeon 1-way 1.61x - 2.08x 2.50x Xeon 8-way 1.59x 2.14x - 2.35x Niagara 1.12x 1.51x 1.08x - Abbreviation \nSystem Frequency Cores used Scalability Algorithm Choices (w/ switching points) Mobile Core 2 Duo Mobile \n1.6 GHz 2 of 2 1.92 IS(150) 8MS(600) 4MS(1295) 2MS(38400) QS(8) Xeon 1-way Xeon E7340 (2 x 4 core) 2.4 \nGHz 1 of 8 - IS(75) 4MS(98) RS(8) Xeon 8-way Xeon E7340 (2 x 4 core) 2.4 GHz 8 of 8 5.69 IS(600) QS(1420) \n2MS(8) Niagara Sun Fire T200 Niagara 1.2 GHz 8 of 8 7.79 16MS(75) 8MS(1461) 4MS(2400) 2MS(8) Table 2. \nAutomatically tuned con.guration settings for the sort benchmark on various architectures. We use the \nfollowing abbreviations for algorithm choices: IS = insertion sort; QS = quick sort; RS = radix sort; \n16MS = 16-way merge sort; 8MS = 8-way merge sort; 4MS = 4-way merge sort; and 2MS = 2-way merge sort, \nwith recursive merge that can be parallelized. support making algorithmic changes, but instead focuses \non lower level compiler optimizations. FLAME (Gunnels et al. 2001) is a domain-speci.c tuning system, \nproviding a formal approach to the design of linear algebra methods. The system produces C and Fortran \nimplementations from high-level speci.cations via code generation. Yi and Whaley proposed a framework \n(Yi and Whaley 2007) to automate the production of optimized general-purpose library kernels. An embedded \nscripting language, POET, is used to describe custom optimizations for an algorithm. Speci.cation .les \nwritten in POET are fed into a transformation engine, which then generates and tunes different implementations. \nThe POET system requires programmers to describe speci.c algorithmic optimizations, rather than allowing \nthe compiler to explore choices automatically. SPL (Xiong et al. 2001) is a domain-speci.c language and \ncompiler system targeted to digital signal processing. The compiler takes signal processing transforms \nrepresented by SPL formulas and explores different transformations and optimizations to produce ef.cient \nC and Fortran code. However, the SPL system was designed only for tuning sequential machines. 7. Future \nWork We are continuing to improve the PetaBricks language, expand our benchmark suite, and improve performance. \nAn interesting additional future direction is adding a distributed memory backend to our compiler so \nthat we can run unmodi.ed PetaBricks programs on clusters. Moving to clusters will add even more choices \nfor the compiler to analyze, as it must decide both what algorithm to use and where to run it. A key \nchallenge in this area is autotuning the management of data. Since distributed systems are often heterogeneous, \nautotuning can offer greater bene.ts since the trade offs become more complex. Finally, we are also exploring \ncompiler backends for less traditional architectures such as graphics cards and embedded systems. 8. \nConclusions Getting consistent, scalable, and portable performance is dif.cult. The compiler has the \ndaunting task of selecting an effective optimization con.guration from possibilities with drastically \ndif\u00adferent impacts on the performance. No single choice of parameters can yield the best possible result \nas different algorithms may be required under different circumstances. The high performance computing \ncommunity has always known that in many problem domains, the best sequential algorithm is different from \nthe best parallel algorithm. Varying problem size and data sets will also require different algorithms. \nCurrently there is no viable way for incorporating all these algorithmic choices into a single program \nto produce portable programs with consistently high performance. In this paper we introduced the .rst \nlanguage that allows programmers to naturally express algorithmic choice explicitly so as to empower \nthe compiler to perform deeper optimization. We have created a compiler and an autotuner that is not \nonly able to compose a complex program using .ne-grained algorithmic choices but also .nd the right choice \nfor many other parameters. We have shown the ef.cacy of this system by developing a non\u00adtrivial suite \nof benchmark applications. One of these benchmarks also exposes the accuracy of different choices to \nthe compiler. Our results show that the autotuned hybrid programs are always better than any of the individual \nalgorithms. Trends show us that programs have a lifetime running into decades while architectures are \nmuch shorter lived. With the advent of multicore processors, architectures are experiencing drastic changes \nat an even faster rate. Under these circumstances, it is a daunting task to write a program that will \nperform well not only on today s architectures but also those of the future. We believe that PetaBricks \ncan give programs the portable performance needed to increase their effective lifetimes. Acknowledgments \nThis work is partially supported by NSF Award CCF-0832997 and an award from the Gigascale Systems Research \nCenter. We would also like to thank the anonymous reviewers for their constructive feedback. References \nAyaz Ali, Lennart Johnsson, and Jaspal Subhlok. Scheduling FFT computation on SMP and multicore systems. \nIn Proceedings of the ACM/IEEE Conference on Supercomputing, pages 293 301, New York, NY, USA, 2007. \nACM. ISBN 978-1-59593-768-1. Ed Anderson, Zhaojun Bai, Christian Bischof, Susan Blackford, James Demmel, \nJack Dongarra, Jeremy Du Croz, Anne Greenbaum, Sven Hammarling, A. McKenney, and Danny Sorensen. LAPACK \nUsers Guide. Society for Industrial and Applied Mathematics, Philadelphia, PA, third edition, 1999. ISBN \n0-89871-447-8.  Jeff Bilmes, Krste Asanovic, Chee-Whye Chin, and Jim Demmel. Opti\u00admizing matrix multiply \nusing PHiPAC: a portable, high-performance, ANSI C coding methodology. In Proceedings of the ACM/IEEE \nConference on Supercomputing, pages 340 347, New York, NY, USA, 1997. ACM. ISBN 0-89791-902-5. Eric A. \nBrewer. High-level optimization via automated statistical modeling. In Proceedings of the ACM SIGPLAN \nSymposium on Principles and Practice of Parallel Programming, pages 80 91, New York, NY, USA, 1995. ACM. \nISBN 0-89791-701-6. James W. Demmel. Applied Numerical Linear Algebra. SIAM, August 1997. Matteo Frigo \nand Steven G. Johnson. The design and implementation of FFTW3. Proceedings of the IEEE, 93(2):216 231, \nFebruary 2005. Invited paper, special issue on Program Generation, Optimization, and Platform Adaptation \n. Matteo Frigo and Steven G. Johnson. FFTW: An adaptive software architecture for the FFT. In Proceedings \nof the IEEE International Conference on Acoustics Speech and Signal Processing, volume 3, pages 1381 \n1384. IEEE, 1998. Matteo Frigo, Charles E. Leiserson, and Keith H. Randall. The implementation of the \nCilk-5 multithreaded language. In Proceedings of the ACM SIGPLAN Conference on Programming Language Design \nand Implementation, pages 212 223, Montreal, Quebec, Canada, Jun 1998. Proceedings published ACM SIGPLAN \nNotices, Vol. 33, No. 5, May, 1998. John A. Gunnels, Fred G. Gustavson, Greg M. Henry, and Robert A. \nvan de Geijn. FLAME: Formal Linear Algebra Methods Environment. ACM Transactions on Mathematical Software, \n27(4):422 455, December 2001. ISSN 0098-3500. Eun-jin Im and Katherine Yelick. Optimizing sparse matrix \ncomputations for register reuse in SPARSITY. In Proceedings of the International Conference on Computational \nScience, pages 127 136. Springer, 2001. Michail G. Lagoudakis and Michael L. Littman. Algorithm selection \nusing reinforcement learning. In Proceedings of the International Conference On Machine Learning, pages \n511 518. Morgan Kaufmann, 2000. Xiaoming Li, Maria Jesus Garzaran, and David Padua. A dynamically tuned \nsorting library. In Proceedings of the International Symposium on Code Generation and Optimization, pages \n111 122, March 2004. Xiaoming Li, Mara Jess Garzarn, and David Padua. Optimizing sorting with genetic \nalgorithms. In Proceedings of the International Symposium on Code Generation and Optimization, pages \n99 110. IEEE Computer Society, 2005. Marek Olszewski and Michael Voss. Install-time system for automatic \ngeneration of optimized parallel sorting algorithms. In Proceedings of the International Conference on \nParallel and Distributed Processing Techniques and Applications, pages 17 23, 2004. Justin Mazzola Paluska, \nHubert Pham, Umar Saif, Grace Chau, Chris Terman, and Steve Ward. Structured decomposition of adaptive \napplications. In Proceedings of the Annual IEEE International Conference on Pervasive Computing and Communications, \npages 1 10, Washington, DC, USA, 2008. IEEE Computer Society. ISBN 978-0\u00ad7695-3113-7. Markus Puschel, \nJose M. F. Moura, Jeremy R. Johnson, David Padua, Manuela M. Veloso, Bryan W. Singer, Jianxin Xiong, \nAca Gacic Franz Franchetti, Robbert W. Johnson Yevgen Voronenko, Kang Chen, and Nicholas Rizzolo. SPIRAL: \nCode generation for DSP transforms. In Proceedings of the IEEE, volume 93, pages 232 275. IEEE, Feb 2005. \nRichard H. Rand. Computer algebra in applied mathematics: an introduction to MACSYMA. Number 94 in Research \nnotes in mathematics. 1984. ISBN 0-273-08632-4. Nathan Thomas, Gabriel Tanase, Olga Tkachyshyn, Jack \nPerdue, Nancy M. Amato, and Lawrence Rauchwerger. A framework for adaptive algorithm selection in STAPL. \nIn Proceedings of the ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, pages \n277 288, New York, NY, USA, 2005. ACM. ISBN 1-59593-080-9. Michael Voss and Rudolf Eigenmann. Adapt: \nAutomated de-coupled adaptive program transformation. In Proceedings of the International Conference \non Parallel Processing, pages 163 170, 2000. Michael Voss and Rudolf Eigenmann. High-level adaptive program \noptimization with adapt. ACM SIGPLAN Notices, 36(7):93 102, 2001. ISSN 0362-1340. Richard Vuduc, James \nW. Demmel, and Jeff A. Bilmes. Statistical models for empirical search-based performance tuning. International \nJournal of High Performance Computing Applications, 18(1):65 94, 2004. ISSN 1094-3420. Richard Vuduc, \nJames W. Demmel, and Katherine A. Yelick. OSKI: A library of automatically tuned sparse matrix kernels. \nIn Proceedings of the Scienti.c Discovery through Advanced Computing Conference, Journal of Physics: \nConference Series, San Francisco, CA, USA, June 2005. Institute of Physics Publishing. Richard Clint \nWhaley and Jack J. Dongarra. Automatically tuned linear algebra software. In Proceedings of the ACM/IEEE \nConference on Supercomputing, pages 1 27, Washington, DC, USA, 1998. IEEE Computer Society. ISBN 0-89791-984-X. \nRichard Clint Whaley and Antoine Petitet. Minimizing development and maintenance costs in supporting \npersistently optimized BLAS. Software: Practice and Experience, 35(2):101 121, February 2005. Samuel \nWebb Williams, Andrew Waterman, and David A. Patterson. Roo.ine: An insightful visual performance model \nfor .oating-point programs and multicore architectures. Technical Report UCB/EECS\u00ad2008-134, EECS Department, \nUniversity of California, Berkeley, Oct 2008. Jianxin Xiong, Jeremy Johnson, Robert Johnson, and David \nPadua. SPL: a language and compiler for DSP algorithms. In Proceedings of the ACM SIGPLAN Conference \non Programming Language Design and Implementation, pages 298 308, New York, NY, USA, 2001. ACM. ISBN \n1-58113-414-2. Qing Yi and Richard Clint Whaley. Automated transformation for performance-critical kernels. \nIn Proceedings of the ACM SIGPLAN Symposium on Library-Centric Software Design, Oct. 2007. Kamen Yotov, \nXiaoming Li, Gang Ren, Michael Cibulskis, Gerald DeJong, Maria Garzaran, David Padua, Keshav Pingali, \nPaul Stodghill, and Peng Wu. A comparison of empirical and model-driven optimization. In Proceedings \nof the ACM SIGPLAN Conference on Programming Language Design and Implementation, pages 63 76, New York, \nNY, USA, 2003. ACM. ISBN 1-58113-662-5. Hao Yu, Dongmin Zhang, and Lawrence Rauchwerger. An adaptive \nalgorithm selection framework. In Proceedings of the International Conference on Parallel Architectures \nand Compilation Techniques, pages 278 289, Washington, DC, USA, 2004. IEEE Computer Society. ISBN 0-7695-2229-7. \n   \n\t\t\t", "proc_id": "1542476", "abstract": "<p>It is often impossible to obtain a one-size-fits-all solution for high performance algorithms when considering different choices for data distributions, parallelism, transformations, and blocking. The best solution to these choices is often tightly coupled to different architectures, problem sizes, data, and available system resources. In some cases, completely different algorithms may provide the best performance. Current compiler and programming language techniques are able to change some of these parameters, but today there is no simple way for the programmer to express or the compiler to choose different algorithms to handle different parts of the data. Existing solutions normally can handle only coarse-grained, library level selections or hand coded cutoffs between base cases and recursive cases.</p> <p>We present PetaBricks, a new implicitly parallel language and compiler where having multiple implementations of multiple algorithms to solve a problem is the natural way of programming. We make algorithmic choice a first class construct of the language. Choices are provided in a way that also allows our compiler to tune at a finer granularity. The PetaBricks compiler autotunes programs by making both fine-grained as well as algorithmic choices. Choices also include different automatic parallelization techniques, data distributions, algorithmic parameters, transformations, and blocking.</p> <p>Additionally, we introduce novel techniques to autotune algorithms for different convergence criteria. When choosing between various direct and iterative methods, the PetaBricks compiler is able to tune a program in such a way that delivers near-optimal efficiency for any desired level of accuracy. The compiler has the flexibility of utilizing different convergence criteria for the various components within a single algorithm, providing the user with accuracy choice alongside algorithmic choice.</p>", "authors": [{"name": "Jason Ansel", "author_profile_id": "81314480768", "affiliation": "Massachusetts Institute of Technology, Cambridge, MA, USA", "person_id": "P1464384", "email_address": "", "orcid_id": ""}, {"name": "Cy Chan", "author_profile_id": "81435598566", "affiliation": "Massachusetts Institute of Technology , Cambridge, MA, USA", "person_id": "P1464385", "email_address": "", "orcid_id": ""}, {"name": "Yee Lok Wong", "author_profile_id": "81435594173", "affiliation": "Massachusetts Institute of Technology , Cambridge, MA, USA", "person_id": "P1464386", "email_address": "", "orcid_id": ""}, {"name": "Marek Olszewski", "author_profile_id": "81332519582", "affiliation": "Massachusetts Institute of Technology , Cambridge, MA, USA", "person_id": "P1464387", "email_address": "", "orcid_id": ""}, {"name": "Qin Zhao", "author_profile_id": "81318492570", "affiliation": "Massachusetts Institute of Technology , Cambridge, MA, USA", "person_id": "P1464388", "email_address": "", "orcid_id": ""}, {"name": "Alan Edelman", "author_profile_id": "81100308159", "affiliation": "Massachusetts Institute of Technology , Cambridge, MA, USA", "person_id": "P1464389", "email_address": "", "orcid_id": ""}, {"name": "Saman Amarasinghe", "author_profile_id": "81100533031", "affiliation": "Massachusetts Institute of Technology , Cambridge, MA, USA", "person_id": "P1464390", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1542476.1542481", "year": "2009", "article_id": "1542481", "conference": "PLDI", "title": "PetaBricks: a language and compiler for algorithmic choice", "url": "http://dl.acm.org/citation.cfm?id=1542481"}