{"article_publication_date": "06-15-2009", "fulltext": "\n Safe and Timely Dynamic Updates for Multi-threaded Programs Iulian Neamtiu University of California, \nRiverside Riverside, CA 92521, USA neamtiu@cs.ucr.edu Abstract Many dynamic updating systems have been \ndeveloped that enable a program to be patched while it runs, to .x bugs or add new features. This paper \nexplores techniques for supporting dynamic updates to multi-threaded programs, focusing on the problem \nof applying an update in a timely fashion while still producing correct behavior. Past work has shown \nthat this tension of safety versus timeliness can be balanced for single-threaded programs. For multi-threaded \nprograms, the task is more dif.cult because myriad thread inter\u00adactions complicate understanding the \npossible program states to which a patch could be applied. Our approach allows the program\u00admer to specify \na few program points (e.g., one per thread) at which a patch may be applied, which simpli.es reasoning \nabout safety. To improve timeliness, a combination of static analysis and run\u00adtime support automatically \nexpands these few points to many more that produce behavior equivalent to the originals. Experiments \nwith thirteen realistic updates to three multi-threaded servers show that we can safely perform a dynamic \nupdate within milliseconds when more straightforward alternatives would delay some updates indef\u00adinitely. \nCategories and Subject Descriptors F.3.2 [Semantics of Pro\u00adgramming Languages]: Program analysis; D.1.3 \n[Concurrent Pro\u00adgramming]: Parallel programming; D.3.4 [Processors]: Compil\u00aders; C.4 [Performance of \nSystems]: Reliability, availability, and serviceability General Terms Languages, Performance, Reliability \nKeywords dynamic software updating, update safety, update timeliness, multi-threading 1. Introduction \nContinuous operation is a requirement of many of today s computer systems. Nonetheless, such systems \nmust be updated to .x bugs and add new features. To permit on-line updates, many researchers have proposed \nvariations of an approach called dynamic software updat\u00ading (DSU). In this approach, a running program \nis patched with new code and data on the .y, while it runs. DSU is appealing be\u00adcause of its generality: \nin principle any program can be updated in a .ne-grained way, without need for redundant hardware or \nspecial\u00adpurpose software architectures. Application state is naturally pre\u00adserved between updated versions, \nso that current processing is not Permission to make digital or hard copies of all or part of this work \nfor personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior \nspeci.c permission and/or a fee. PLDI 09, June 15 20, 2009, Dublin, Ireland. Copyright c &#38;#169; 2009 \nACM 978-1-60558-392-1/09/06. . . $5.00 Michael Hicks University of Maryland College Park, MD 20742, USA \n mwh@cs.umd.edu compromised or interrupted. General-purpose DSU systems have been shown to successfully \nsupport long strings of updates derived from actual releases of server applications (Neamtiu et al. 2006; \nChen et al. 2007) and operating systems (Baumann et al. 2007), while more specialized systems support \nsmaller bug .xes or secu\u00adrity patches (Makris and Ryu 2007; Altekar et al. 2005; Sidiroglou et al. 2007). \nThe primary challenge in building a DSU system is balancing .exibility and safety: the system should \nsupport as many kinds of dynamic changes as possible, and it must provide means to en\u00adsure that an program \nupdate is well-timed, to avoid incorrect be\u00adhavior (Gupta et al. 1996). To see why timing is important, \ncon\u00adsider that many programs change functions type signatures as they evolve (Neamtiu et al. 2005, 2006). \nIf a patch changes function f s type signature, applying the patch just before the program is about to \ncall f would result in a type error since the caller still presumes f s old signature. To avoid such \nproblems, some DSU systems im\u00adpose some automatic timing restrictions. For example, a system may require \nthat updated functions not be running when a patch is applied (Soules et al. 2003; Baumann et al. 2007; \nAltekar et al. 2005; Arnold and Kaashoek 2009). Since changing a function s type signature necessitates \nchanging its callers, we can be sure, us\u00ading this constraint, that no code will be running that presumes \nthe old signature. Despite the reliance of some systems on purely au\u00adtomatic checks (Soules et al. 2003; \nBaumann et al. 2007; Altekar et al. 2005; Arnold and Kaashoek 2009), these are in general in\u00adsuf.cient \nto ensure safety (Gupta et al. 1996), so manual assistance may be needed to further constrain legal update \ntimes (Hicks and Nettles 2005; Stoyle et al. 2007). Here lies a tension: timing constraints are needed \nto ensure that an update will be applied safely, but if the constraints are too strict, they may prevent \nthe update from taking place at all. For exam\u00adple, if a patch may only be applied when changed functions \nare not active, a change to a function running an in.nite loop will be delayed inde.nitely. For single-threaded \nprograms, the safety/time\u00adliness trade-off can be managed with some care, e.g., by extracting in.nite \nloops into separate functions that occasionally become in\u00adactive (Neamtiu et al. 2006). However, many \nlong-running systems that could bene.t from DSU are multi-threaded, and for them the situation is far \nworse. As explained further in Section 2, myriad in\u00adteractions among threads complicates reasoning about \nsafety, and may render approaches to control timing ineffective. For example, while a single thread may \nbe sure to eventually exit an extracted loop body, there is no guarantee that all threads will eventually \nexit it at once to allow it to be updated (Makris and Ryu 2007). This paper explores new ideas for balancing \nthe tension between safety and timeliness when dynamically updating multi-threaded programs. We start \nwith the idea that the programmer will specify S, a handful of safe update points, perhaps one or two \nper thread, which are program locations at which an update may take place. In a multi-threaded setting, \na dynamic patch p may be applied when all threads become quiescent, meaning they have each reached an \nupdate point e . S and that all automatic timing restrictions, such as activeness, have been satis.ed. \nBy keeping |S| small, it is relatively straightforward for the programmer to reason that p will be applied \ncorrectly. We could implement this semantics by barrier-synchronizing all threads at update points e, \nbut doing so could degrade performance (while threads block, or even deadlock) and fail to achieve quiescence \nquickly because fewer update points implies greater delays between the times they are reached.  Therefore, \nto improve timeliness while retaining the same level of safety, we propose two novel concepts, described \nin Section 3: induced update points and relaxed synchronization. An induced up\u00addate point for a given \npatch p is a program point ep . S such that applying p at ep is equivalent to having applied p at some \ne . S. Thus, induced update points expand the points at which an update may take place without increasing \nthe burden of reasoning about safety. However, expanding the number of possible update points reduces, \nbut does not eliminate, the drawbacks of barrier synchronization. Therefore, we have also developed a \ntechnique we call relaxed synchronization that eliminates the need for threads to block at update points. \nThe basic insight is the following. Suppose that a consecutive range of program statements s1, ..., sn \nqualify as induced update points for S and p. Instead of potentially syn\u00adchronizing at each si, a thread \nt checks in when it reaches s1 to indicate that, until told otherwise, its execution is safe with re\u00adspect \nto p. Thread t may then immediately proceed with executing s2, ...sn, checking out after executing sn \nto indicate that apply\u00ading p would no longer be safe. Once all threads have checked in, p can be applied \nsafely. Check-in points often create more oppor\u00adtunities for updating (whole blocks of code, rather than \nparticular statements) and allow threads to continue to execute until quies\u00adcence is reached, avoiding \nperformance degradation and deadlock. We have implemented these techniques as an extension to Gin\u00adseng, \na freely available DSU framework for C programs (Sec\u00adtion 4). We call our extended system STUMP (for \nSafe and Timely Updates to Multi-threaded Programs). We have used STUMP to dynamically update three open-source, \nmulti-threaded server programs the Icecast streaming server, the Memcached caching server, and the Space \nTyrant game server with updates that cor\u00adrespond to thirteen actual releases that span one year (Section \n5). Experimental results in Section 6 show that induced update points and relaxed synchronization effectively \nbalance safety and time\u00adliness for these programs: with only a few update points (1 or 2 per thread) \nand a few other annotations, we could reach quiescence very quickly, typically in less than ten milliseconds. \nWithout these mechanisms some patches failed to take effect at all, and others took hundreds of milliseconds \nor several seconds to apply. We also measured the impact of STUMP s update support on application performance \nand found that the slowdown for application-speci.c benchmarks is modest, less than 7% for all applications. \nIn summary, the main contributions of this paper are as follows: We introduce two techniques induced \nupdate points and re\u00adlaxed synchronization for balancing safety and timeliness when dymamically updating \nmulti-threaded programs.  We implement these ideas in STUMP, a framework for dynam\u00adically updating C \nprograms, and .nd that STUMP is effective in practice: all updates we considered can be successfully \nap\u00adplied in short order, while overhead on application performance is small.  2. Balancing safety and \ntimeliness Given run-time support for on-line program updates, we must un\u00adderstand how to use this support \nsafely, and yet in such a man\u00adner that an update can be applied in a timely fashion. This section 1 \n2 3 4 5 6 7 8 9 10 11 12 13 14 typedef struct event { int e id ; ... } evt ; void process(evt* ev) {switch \n(ev. e id) { case X: m(ev); ... case Y: n(ev );... log(ev. e id); }void clean(evt* ev) { ... }void handle \nthread() {while (1) {/* de.nite update point */ evt* ev = get event(); /* candidate induced update point \n*/ process(ev); clean(ev); } } (a) Original version }; 1 2 3 4 5 6 7 typedef struct event { char* name; \nint e id ; ... } evt ; void process(evt* ev) {switch (ev. e id) { case X: m(ev); ... case Y: n(ev );... \n/* no log() call */ }void clean(evt* ev) { /* added: */ log(ev. e id); ... }/* handle thread() as before \n*/ }; (b) Changes in new version Figure 1. Example program and update explains why balancing these tensions \nis dif.cult, particularly for multi-threaded programs. 2.1 Example Figure 1(a) depicts a simple server \nin which n threads execute the handle thread function to process events extracted from a global queue \n(via the potentially blocking call get event, code not shown). Events are values of the type evt, which \ncontains an e id .eld (among others not shown) for identifying the event s type. The process function \nis called to actually handle each event, dispatch\u00ading to other functions based on the e id .eld, and \nlogging the re\u00adsult when it completes (via the log function). The clean function performs further post-processing. \n(The comments in handle thread will become clear later.) Figure 1(b) shows a sample update to this program, \nwhich changes the de.nition of the evt type to add a new .eld and changes the implementations of the \nprocess and clean functions. Notice that the log function has been moved from the end of the process \nfunction in the old version to the beginning of the clean function in the new version. Several existing \nDSU systems could support this update or one like it, including POLUS (Chen et al. 2007), Ginseng (Neamtiu \net al. 2006), DLpop (Hicks and Nettles 2005), Jvolve (Subramanian et al. 2009), K42 (Baumann et al. 2007; \nK42), and UpStare (Makris and Bazzi 2009). While the mechanisms differ between systems, essentially the \nde.nitions in Figure 1(b) would be gathered into a dynamic patch along with a type transformer function \nto convert values whose type de.nition has changed.1 In our example, the dynamic patch would contain \na type transformer for struct event. This function might initialize the new name .eld to none while preserving \nthe contents of the other (unchanged) .elds.  2.2 Safe patch application Given a dynamic patch, the \nnext question is: when during the original program s execution can it be applied safely? Most DSU 1 A \ntype transformer s application could occur when the update is ap\u00adplied (Hicks and Nettles 2005; Subramanian \net al. 2009) or as needed during execution (Neamtiu et al. 2006; Chen et al. 2007; Baumann et al. 2007). \n systems apply a patch according to a combination of manually speci.ed and automatically determined \ntiming constraints. A typical automatically determined constraint is that functions changed by a patch \nmay not be running when the patch is ap\u00adplied (Soules et al. 2003; Baumann et al. 2007; Altekar et al. \n2005; Arnold and Kaashoek 2009; Subramanian et al. 2009). To see why such a constraint might be needed, \nsuppose our update was to be applied just before the call to log in the process function on line 4 of \nthe old program (when an active function is updated, the old version continues to run and the new version \ntakes effect on the next call (Chen et al. 2007; Hicks and Nettles 2005; Neamtiu et al. 2006)). Therefore, \nin the old process code there is a subtle prob\u00adlem: this function has been compiled to assume that e \nid is the .rst .eld of evt, but now that evt has been updated, it is the second .eld. Therefore, this \nold code will mistakenly access char *name as if it were the int e id, a type error. The activeness constraint \nwould prevent this problem, and indeed ensures type safety so long as the new program version, compiled \nfrom scratch, is itself type\u00adcorrect (Walton 2001; Stoyle et al. 2007). We may be tempted to believe \nthat activeness checking is suf.cient to ensure an update will be applied correctly, and indeed this \npresumption is made by some existing systems (Arnold and Kaashoek 2009; Baumann et al. 2007). Unfortunately, \nGupta (Gupta et al. 1996) has shown that automatically determining a valid time at which to apply a dynamic \npatch is in general undecidable. In\u00addeed, we can see why activeness is insuf.cient in our example. Suppose \nthat the update were to take effect just after the call to process(ev) on line 12 of handle thread. If \nthere is only a single thread running handle thread, then no changed code is active. The next call is \nto the new clean function, whose .rst action is to call log. But this is probably not what we wanted: \njust prior to the up\u00addate, the old process function would have called log (on line 4) for this same event; \nso this update timing has precipitated a redundant log entry. Given that automatic timing constraints \nare insuf.cient, many DSU systems allow the programmer to assist in controlling up\u00addate timing. There \nare two basic approaches: identify a whitelist of program locations, termed update points, that are valid \nfor an update (Hicks and Nettles 2005; Neamtiu et al. 2006; Makris and Bazzi 2009); or specify a blacklist \nof functions that must be inactive prior to updating (Lee 1983; Gupta et al. 1996; Chen et al. 2007). \nWith such manual controls in hand, prior work has explored applying updates at quiescent points in a \nprogram s execu\u00adtion (Soules et al. 2003; Baumann et al. 2007; Neamtiu et al. 2006). For example, a quiescent \npoint could be just prior to a complete it\u00aderation of an event processing loop. Intuitively, writing \ncorrect state transformation code to take effect at quiescent points is relatively easy because the program \nis not in the middle of some high-level activity, and invariants concerning global data structures are \nclear. In our example, we could imagine requiring the update to take place at the start of handle thread \ns loop on line 9, since this is when it is about to begin its basic high-level activities.  2.3 Timely \npatch application Unfortunately, while timing restrictions are clearly necessary for ensuring safety, \nthey can signi.cantly delay when a patch is actu\u00adally applied. Indeed, the activeness restriction precludes \npatch ap\u00adplication inde.nitely if a changed function is always active, as is the handle thread function \nin our example. We can solve this par\u00adticular problem by extracting the bodies of in.nite loops into \nsep\u00adarate functions which become inactive on each iteration (Neamtiu et al. 2006). With such support \n(or support for more .ne-grained updates to active code (Makris and Bazzi 2009)), a single quies\u00adcent \npoint often suf.ces to assure timeliness for single-threaded programs, e.g., because events are processed \nrelatively quickly. Unfortunately, for multi-threaded programs, the task of choos\u00ading suitable update \npoints is much more dif.cult because the state of all threads must be considered when choosing appropriate \ntimes. One approach would be to identify a small number of safe update points for each thread (e.g., \njust one or two) and apply an update only when all threads have reached safe update points. In our ex\u00adample, \nwe would apply the update only when all threads running handle thread have completed a loop iteration. \nThis approach eases reasoning about patch correctness because only a few system states need to be considered. \nOn the other hand, this approach could se\u00adriously compromise timeliness, since it may be very dif.cult \nor unlikely for all threads to reach update points at once. We could improve the chances by expanding \nthe number of update points per thread, but doing so complicates the programmer s reasoning that a patch \nis correct, and may lead to problems such as those we have described. Overall, the number of system states \nthat must be con\u00adsidered will be n m where m is the number of threads, and n is the number of update \npoints per thread. Thus we must .nd some way to balance the need to easily reason about a patch s correctness \nwhile not unduly delaying the time at which it can be applied. 3. Safe and timely dynamic updates This \nsection describes how we can balance safety and timeliness when applying a dynamic patch. We use the \nfollowing program\u00adming model. To update a program P , the programmer provides an update speci.cation \nU, which is a pair (p, S) where p is a dynamic patch and S is a setof update points. The patch p is a \nmap from variables to new or updated de.nitions (or the type transformer, for changed types). We write \nchanges(p) to denote the names of func\u00adtions, variables and type de.nitions changed by the patch. Each \nupdate point in S is simply a location (i.e., line number) e in the program. The intended semantics is \nthat p should be applied when the program counter of each thread t in P has reached an update point e \n. S, and any automatic safety checks (e.g., activeness) are satis.ed. Our goal is to apply the patch \nas quickly as possible. 3.1 Simple approach: barrier synchronization Once U becomes available, a simple \napproach to applying it safely is to treat each update point e . S as a barrier and block any thread \nthat reaches it. Once all threads have synchronized, the patch is applied, and the threads are resumed. \nWhile pleasingly simple, the barrier approach has two main drawbacks. First, there is the possibility \nof deadlock. For exam\u00adple, if thread t blocks at some point e while holding a lock, then any other thread \nattempting to acquire the lock will never make progress toward reaching some e . S. The second problem \nis re\u00adlated: even if all threads eventually synchronize, application perfor\u00admance may suffer in the meantime. \nFor example, suppose thread t1 is responsible for accepting new connections while t2 performs per\u00adconnection \nevent processing. If t1 reaches point e fairly quickly, but t2 takes much longer to reach point e', say \nbecause it must complete a lengthy transaction .rst, then the entire system will be prevented from accepting \nnew connections, and performance will suffer. In the limit, the barrier approach may needlessly block \nthreads that are completely unaffected by the changes in a patch p. Our way forward is to observe that \nit does not matter if the update takes place when all threads are actually at points in S. Rather, we \nuse induced update points (described in Section 3.2) and relaxed synchronization (Section 3.3) to apply \nthe patch p if the effect of the update will be semantically equivalent to having applied the patch when \nthe PC of each thread is at one of the update points e . S. This expands the number of program points \nthat permit a patch to take effect, improving timeliness, without increasing the reasoning burden on \nthe programmer.  Lines Trace 1 Trace 2 Trace 3 9 (def-upd) 10 g {1} 11 update g 12 p {1, 2}3 e, m {1, \n2}4 l, e {1, 2}13 c {1, 2}6 9 (def-upd) . . g {1, 2}update p,c p {2}e, m {1, 2} c {2}l, e {1, 2} \u00d7 g \n{1}update g,p p {2}e, m {1, 2} c {1, 2} \u00d7 (a) Roll-forward (b) Rollback (c) Disallowed Figure 2. Examples \nof legal and illegal induced updates.  3.2 Induced update points Given a speci.cation U =(p, S), an \ninduced update point is a program point e ' such that if p is applied when a thread reaches e ', the \nprogram will behave as if the patch had been applied at some update point e . S. Given a set of candidate \nupdate points ., our task is to determine, via static analysis, which of those e . . that meet this criterion. \nIn our Ginseng implementation (Section 4), we require programmers to specify ., but here we make no assumptions \nabout how . is generated. 3.2.1 Version-consistent traces The key concept we will use is the notion of \na version-consistent ex\u00adecution trace. This idea is illustrated in Figure 2. Each of the right three \ncolumns represents an execution trace of the handle thread function from Figure 1(a), starting at line \n9 and iterating once around the loop. Each element of the trace consists of an identi\u00ad.er followed by \na set of versions. Identi.ers can be functions (indi\u00adcating the function was called), global variables \n(indicating a read or write), and type names (indicating a variable with that type was read or written). \nWe abbreviate the longer identi.ers used in Fig\u00adure 1(a) with just their .rst letter in the trace, e.g., \ng stands for a call to get event, and e stands for a read/write from an evt value. The version set indicates \nthe version of an identi.er at the point where that identi.er is used. Accesses to variables changed \nby p will always have a version set containing a single element, {1} be\u00adfore the patch is applied, and \n{2} afterward. When a variable is not changed by the patch its de.nition is the same in both program \nversions. Hence its version set is {1, 2}. A trace also contains update events update changes(p) to in\u00addicate \npatch p has been dynamically applied. All the speci.cations (p, S) we consider have S= {9}; that is, \nthe patch p may only appear to take effect at line 9. We refer to this as the de.nite update point, since \nit de.nes what the programmer believes will result in correct behavior (line 9 is annotated in the .rst \ncolumn for refer\u00adence). If given . = {11}, then we must determine whether apply\u00ading p at line 11 would \nhave the same effect as updating at line 9. Figure 2(a) considers the case when changes(p)= {g}; that \nis, it contains a changed version of function get event. In this case, 11 is indeed an induced update \npoint: the execution of the program is equivalent to having performed the update at line 9, at the end \nof the trace marked with a .. To see why, consider the version set of each function that was called. \nIn the trace in Figure 2(a), we can see that all version sets include version 1, the old version, and \nthus the program will behave just as if we rolled forward the program to line 14 before performing the \nupdate. Figure 2(b) considers changes(p)= {p, c}. In this case, 11 is still an induced update point, \nbut now the effect of the program is equivalent to having performed the update at line 9 at the start \nof the trace. This is because all accesses can be attributed to version 2, the new version, just as they \nwould be if we had rolled back the program to perform the update at line 9. Figure 2(c) considers changes(p)= \n{g, p}. In this case there is no program version that all accesses share, so 11 is not valid. 3.2.2 \nVersion consistency via static analysis To predict whether a patch p applied at some e . . will induce \na version-consistent trace, we can statically analyze the program to approximate all relevant behavior \nthat could occur in an execution including e. In particular, we wish to .nd a static approximation of \nthe execution behavior from any preceding de.nite update point to e, which we shall call the prior effect \nae, and an approximation of the execution from e to the .rst occurrence of a de.nite update point, which \nwe call the future effect .e (Neamtiu et al. 2008). For determining version consistency, we are interested \nin those events we considered in our traces above: calls to functions, reads/writes from/to global variables, \nand accesses to values of named type. We are not interested in the number or order of these events, but \nonly in the names of the functions, variables, or types involved. Thus a prior/future effect at e can \nbe characterized as the set of all de.nition names involved in events that could occur before/after e. \nFor e = 11 in our example program we have ae = {g}and .e = {p, e, m, n, l, c}. The reason for the former \nis easy to see: before reaching line 11 on an execution starting from line 9, the only interesting event \nis the call to get event. For the latter, we can see that before again reaching line 9, the program will \nexecute process and clean, and these functions may themselves access values of evt type and call functions \nlog, m, or n. On the other hand g is not included because get event will not be called before line 9 \nis reached. We will discuss how these effects are computed in the next subsection. We can determine whether \napplying a patch p at some e . . will result in a version-consistent trace using contextual effects. \nFor each e, we compute its prior and future effects ae and .e, respectively. If changes(p) n .e = \u00d8 we \nknow that all de.nitions possibly accessed after e up to the next de.nite update point can be attributed \nto the old version since they are unaffected by p. Thus e can be considered a roll-forward induced update \npoint. This condition holds for changes(p)= {g} in our example, as shown in Figure 2(a). On the other \nhand, if changes(p) n ae = \u00d8, then the patch p has not modi.ed any variables possibly accessed since \nthe last de.nite update point. In this case, these accesses can also be attributed to the new version, \nand thus e can be considered a rollback induced update point. This condition holds for changes(p)= {p, \nc} in our example, as shown in Figure 2(b). Neither condition holds for changes(p)= {g, p}, so for this \nparticular patch e is not valid, as shown in Figure 2(c).  3.2.3 Contextual effects Now we explain how \nto compute prior and future effects for the purpose of determining induced update points. Our approach \nis to use a generalization of effect inference (Talpin and Jouvelot 1992) called contextual effect inference \n(Neamtiu et al. 2008). In a traditional effect system, the effect e of some program ex\u00adpression e characterizes \nan aspect of e s non-functional behavior, for example the names of locks e allocates (Pratikakis et al. \n2006), or the abstract names of memory locations e dereferences (Talpin and Jouvelot 1992). For enforcing \nversion consistency our notion of effect is as we just described: a set containing the names of func\u00adtions \nthat are called, global variables that are read or written (and likewise, static names for dereferenced \npointers, acquired from a points-to analysis), and types whose instances whether local or global are \nread or written.  The contextual effect F of an expression e consists of a triple [a; e; .], where e \nis the normal effect of e; a is the prior effect, which characterizes the computation since the last \nde.nite update point up to (but not including) e; and . is the future effect, which characterizes the \ncomputation following e, up until the next de.nite update point. We compute contextual effects using \na constraint\u00adbased analysis that we express as a series of inference rules. In our prior paper (Neamtiu \net al. 2008), we proved that, for single\u00adthreaded programs, valid induced update points produce version\u00adconsistent \nexecutions when prior and future effects are computed with our contextual effects analysis. This basic \nproof is extended to multi-threaded programs in the .rst author s dissertation (Neamtiu 2008). We give \na .avor of the analysis here, and refer the interested reader to our prior work for details (Neamtiu \net al. 2008). Suppose we have two statements in sequence s1; s2 which have contextual effects F1 and \nF2, respectively. Then the contextual effect F of the two statements in sequence is according to the \njudgment F1 C F2 '. F de.ned as follows: F1 =[a1; e1;(e2 . .2)] F2 = [(e1 . a1); e2; .2] F=[a1;(e1 . \ne2); .2] XFlow-Ctxt F1 C F2 '. F There are three key elements of this rule. First, if e1 is the normal \neffect of s1, then because s1 precedes s2, e1 must be included in the prior effect of F2. Conversely, \nif e2 is the normal effect of s2, then because s2 follows s1, e2 must be included in the future effect \nof F1. Finally, the contextual effect of the sequence s1; s2 has the prior effect of F1, the future effect \nof F2, and its normal effect is the union of the normal effects of the two statements. The X.ow-Ctxt \nrule is essentially constraining the form of prior and future effects according to the normal effects \nof each statement. Thus we can use standard techniques to generate per-statement effects e and generate \nadditional constraints for the prior and future effects a and . (Neamtiu et al. 2008). The effects a \nand . are also constrained by the placement of de.nite update points. In the general case, there could \nbe many de.nite update points reachable from a given statement s. Like\u00adwise, several de.nite update points \ncould reach s, and these could be in the same function as s or in its function s callers or callees. \nWhile implementing contextual effects for this general case is pos\u00adsible, doing so is unnecessarily complicated \nand computationally expensive. Therefore, we employ a simpler model, described next. As an alternative \nto S, a set of de.nite update points, we de.ne S to be a set of pairs of de.nite update points. When \ncomputing the prior effect a at e, instead of .nding all possible update points in S that could reach \ne, we only consider a single point e1 such that (e1,e2) . S and (e1,e2) de.nes a lexical scope that encloses \ne. By enclose, we mean that e could literally occur in between e1 and e2 in the program text, or it could \nbe in a function called, directly or transitively, by a statement between e1 and e2. The future effect \n. of e is analogously computed with respect to e2. Since pairs (e1,e2) de.ne a lexical scope, we call \nthem update scopes. To be sound, there can be at most one update scope that encloses a given e, meaning \nthat update scopes may neither nest nor overlap. Moreover, only candidate update points e . . enclosed \nwithin scopes (e1,e2) . S are permitted. With these restrictions, we compute contextual effects for the \nwhole program as follows: For each block of statements e1 : s1; ...; e2 : s2 where (e1,e2) . S we compute \nthe contextual effects of each statement in that scope, constraining the prior effect a1 of s1 and the \nfuture effect .n of sn to be the empty set, thus delimiting the extent of prior and future effects computed \nwithin the scope. We will also compute the contextual effects of functions called by s1...sn. 1 void \nprocess(evt* ev) {2 switch (ev. e id) {3 case X: m(ev); ... 4 case Y: n(ev); ... 5 }; log(ev. e id); \n6 }7 void handle thread() {8 while (1) {9 /* \u00a31 */ 10 evt* ev = get event (); 11 /* ind. upd. pt. */ \n12 process(ev); 13 clean(ev ); 14 /* \u00a32 */ }15 } a ; e ; . {g}{e}{e, m, n, l, c}{g, e}{m, e}{e, l, c}{g, \ne}{n, e}{e, l, c}{g, e, m, n}{l, e}{c} {} {g}{p, e, m, n, l, c} {g}{} {p, e, m, n, l, c} {g}{p, e, m, \nn, l}{c}{g, p, e, m, n, l}{c} {} Figure 3. Contextual effects for example in Fig. 1(a). We cannot allow \ninduced update points in functions, such as those in libraries, that could be called from both within \nand out\u00adside an update scope. Therefore we add the additional constraint that the prior and future effects \nof each thread function (i.e., main or functions passed to pthread create ) include the set of all possible \nevents, written T. Thus any candidate update point in a function called from outside an update scope \nwill have prior and future effect T, effectively precluding an induced update point. On the other hand, \nfunctions called only from within update scopes will not be so restricted, since the effects of scopes \nare computed independently of the functions they reside in. Note there is no problem with a function \ncalled from within multiple (non-nested) update scopes the prior and future effects in this function \nwill naturally approximate the limits imposed by all enclosing scopes.  Finally, we rule out nested \nupdate scopes during inference by not bounding the prior and future effects of update scopes with the \nempty set, as stated above, but rather with a marker set {K}. Then we check that the statements immediately \noutside each update scope do not have K in their prior or future effects.  Figure 3 shows the contextual \neffects computed for process and handle thread from our example program (Figure 1(a)), modi.ed to use \nan update scope (e1,e2) shown in comments. This scope is essentially the same as the single de.nite update \npoint we used be\u00adfore, and the prior and future effects of the candidate update point at line 11 are \nas indicated in the previous subsection. There are a few other things to notice. First, notice that line \n12 s normal ef\u00adfect includes the call to process and the effect of executing its body. Second, notice \nthat the prior and future effects within process prop\u00aderly include the events that take place in its \ncaller, handle thread. Finally, notice the handling of the switch statement in process: the prior/future \neffects of the code in each of the cases assumes that the other branches are not executed (e.g., n is \nnot in the prior/fu\u00adture effect at line 3, and m is not in the prior/future effect at line 4), but the \nprior/future effect of the code that precedes or follows the switch as a whole conservatively presumes \nthat all branches were executed (and hence both m and n are mentioned in the effects).  3.3 Relaxed \nsynchronization While induced update points increase a thread s opportunities for applying a dynamic \nupdate, they still force the thread to block, and thus do not eliminate the potential for degraded performance \nand deadlock. Therefore we have developed a second technique we call relaxed synchronization that avoids \nthe need to block at update points while still greatly improving the chances that an update will be applied \nquickly.  We now present the idea behind relaxed synchronization. Sup\u00adpose we have a sequence of statements \ns1...sn at locations e1...en, respectively, that execute within some update scope. Further sup\u00adpose each \nof the ei in e1...en is a valid induced update point for patch p, which implies that p could be safely \napplied while a thread is executing any of s1...sn. As an optimization, then, there is no need to block \nwhen a thread .rst reaches e1. Instead, the thread can check in with the run-time system to indicate \nit has reached a point at which it is safe to apply p. Then it may continue its ex\u00adecution. When the \nthread reaches en it must inform the run-time system it is no longer safe to perform the update; thus \nit checks out, and again resumes execution. When all threads have checked in, the update may commence. \nIf the update is still taking place when a thread reaches a check-out point, it simply waits for the \nupdate to complete. We can implement this idea as follows. For each e . . enclosed in some update scope \n(e1,e2) . S , we will determine whether e is a legal check-in point (otherwise, it is a check-out point). \nLet L denote the set of all e ' . . reachable from an execution starting at e. Let LS denote the set \nof locations ei of all statements si that, starting from e, could be executed prior to reaching some \ne ' . L .{e2}. Then e is a valid check-in point if all ei . LS are valid induced update points. Otherwise, \ne must be considered a check-out point. In the prior subsection we argued that it would be dif.cult to \ncompute contextual effects if we had to determine all de.nite update points that could be reached after \nexecuting a given de.nite update point. Similarly, it would be dif.cult to determine the sets L and LS, \nabove. We simplify the problem in the same way: rather than specify individual candidate update points \ne . ., we specify candidate update scopes (e(,e)) . . , with the interpretation that e( is a potential \ncheck-in point, and e) is a check-out point. Since the two form a lexical scope, the statements s1; ...; \nsn between them are readily apparent. Moreover, to check that each of these statements is a valid induced \nupdate point, if e is the normal effect of the block s1; ...; sn (which we already compute as a matter \nof S course), then it suf.ces to compute ai = a1 . e and 1=i=n S .i = .n . e = .1. We call the former \nthe check-in prior 1=i=n effect of e( and the latter the check-in future effect of e(. For each (e(,e)) \npair for which our validity check using prior and future check-in effects is satis.ed, at run-time the \nthread will check in when reaching e( and check out at e). Those pairs for which this check is not satis.ed \nwill have no run-time effect. Returning to our example (Figure 3), consider (11,14) as a candidate scope. \nThe normal effect e of the statements process(ev); clean(ev) in this scope is {p, e, m, n, l, c}. Thus \nthe prior check\u00adin effect is {g}.{p, e, m, n, l, c} = {g, p, e, m, n, l, c} and the future check-in effect \nis {} .{p, e, m, n, l, c} = {p, e, m, n, l, c}. So 11 would be a valid check-in point for p where changes(p)= \n{g} since this does not con.ict with the future check-in effect, but would not be considered valid for \np where changes(p)= {p}since p appears in both the prior and future check-in effects. Note the clear \ntradeoff here. The larger the check-in scope, the larger the check-in effects and the less likely a check-in \npoint will be valid. On the other hand, the smaller the check-in scope, the less likely that all threads \nwill be simultaneously executing in valid scopes when an update is available. As we show with our experimental \nresults in Section 6, we .nd a few well-chosen check-ins and relaxed synchronization to be largely bene.cial. \nA more complicated variation of relaxed synchronization is proved sound in the .rst author s dissertation \n(Neamtiu 2008); we believe it would be straightforward to extend the argument there to the present system. \n4. Implementation We have implemented induced update points and relaxed synchro\u00adnization as extensions \nto Ginseng v1.2.2,2 a DSU compiler for single-threaded C programs (Neamtiu et al. 2006). We call our \nextended version STUMP (for Safe and Timely Updates to Multi\u00adthreaded Programs). We believe our techniques \ncould be imple\u00admented for other DSU systems as well. 4.1 Background: DSU in Ginseng In Ginseng, a dynamic \npatch p consists of de.nitions functions, types, or global variables that have been added or changed \nsince the last (deployed) program version. The dynamic patch is com\u00adpiled into a shared object .le and \nthen loaded into the specially compiled running program to take effect. In an updatable program, all \ndirect function calls are compiled to be indirected through func\u00adtion pointers, and after the patch is \nloaded, the targets of the func\u00adtion pointers are redirected to the newly loaded versions. To support \nchanges to type de.nitions, Ginseng compiles all accesses to typed values to be through concretization \nfunctions. For example, if p has type struct T*, then Ginseng will compile the source-program expression \np. x to be con struct T (p). x instead. This function will examine the contents of its argument to see \nwhether it has been updated to the new version of type struct T, and if so, it executes a user-de.ned \ntype transformer function to bring it up to date. Ginseng uses a simple compilation strategy to make \nthis check, and the transformation, possible: all values of a named type that are updatable are given \nan extra version .eld and padded to permit future growth. The programmer may also de.ne a state transformer \nfunction ST. Function ST() is called at the time the update is applied and contains code needed to set \nup the state of the new program, e.g., to initialize the contents of newly created global variables, \nor to make system calls that normally occur in the new version of main when the program is started from \nscratch. Controlling update timing. To specify the time at which a dy\u00adnamic patch is applied, Ginseng \nrequires the programmer to insert explicit calls to the function DSU update at those program points at \nwhich a dynamic patch p may take effect. Thus, these calls in\u00addicate de.nite update points S, though \nGinseng in effect forces the programmer to de.ne S at deployment time, before the next patch to P is \nknown. We further discuss this requirement in Section 4.3. When the program calls DSU update and a patch \nis available, some safety checks are performed before the patch is applied. Prior to deployment, a static \nupdatability analysis (Stoyle et al. 2007) of the program determines, for each DSU update call, a set \n. of variables and type names that may not be changed by a future p if applied at that point, to ensure \ntype-safety. Then the DSU update call is compiled to pass a representation of . to the run-time system, \nwhich will enforce . n changes(p)= \u00d8 before applying the update.3 This check essentially takes the place \nof the activeness check described in Section 2.2, but admits some updates to active code. Code extraction. \nGinseng s compilation strategy ensures that an updated function will be used the next time it is called. \nThis cre\u00adates a problem for functions that run inde.nitely, as described in Section 2.3. To cope with \nthis, Ginseng provides a code extraction mechanism that excises a programmer-indicated code block into \nits own function. The boundaries of extracted functions in the old version effectively designate points \nat which an update could take place within a function, and the boundaries of the same extracted 2 Available \nat http://www.cs.umd.edu/projects/PL/dsu/. 3 The implementation uses set ids as arguments, rather than \nsets, to keep this operation fast regardless of set size.  functions indicate the corresponding code \nto execute (i.e., to where to map the PC) in the new version.  4.2 STUMP extensions We made several \nextensions to Ginseng to support induced update points and relaxed synchronization, and also extended \nparts of its run-time system and compiler to ensure thread-safety. As with de.\u00adnite update points in \nGinseng, update scopes (S ) and candidate up\u00addate points/scopes (./. ) are speci.ed when a program is \ndeployed, before a particular patch p is known. De.nite update points and update scopes. To designate \nthe se\u00adquence s1; ...; sn as an update scope (e1,en) . S , the program\u00admer explicitly labels the sequence \nas UPDATE SCOPE:{s1; ...; sn}. Thus, update scopes are speci.ed at deployment time, rather than patch \ntime, replacing analogous calls to DSU update in standard Ginseng. The compiler uses such scopes when \ncomputing contex\u00adtual effects, and then inserts calls to DSU update at the beginning and end of the scope. \nAs usual, these calls are passed the set . generated by the standard updatability analysis. Induced update \npoints and barrier synchronization. The pro\u00adgrammer speci.es candidate induced update points . by includ\u00ading \nexplicit calls to function DSU induced update in the deployed program. A contextual effects analysis \ninfers a and ., and the up\u00addatability analysis infers ., for these program points; the compiler modi.es \nthe calls to pass a representation of these sets. Once a dy\u00adnamic patch p is available and thread i calls \nDSU induced update the run-time system will check whether (changes(p) n ai = \u00d8. changes(p) n .i = \u00d8) \n. (changes(p) n .i = \u00d8). (We discuss changes below.) If this check succeeds, the current update point \nis compatible with the update and the thread is blocked. Calls to DSU update will block so long as changes(p) \nn .i = \u00d8. Once all threads have blocked, the update may proceed. We de.ne changes(p)= changes(p) . writes(ST), \nwhere writes(ST) is the set of locations (determined by points-to analy\u00adsis) that could be read or written \nby executing state transformer ST. This effect must be included in the safety check to ensure version \nconsistency. For example, if not accounted for, the code prior to an induced update point could read \nglobal variable g, function ST could write to it, and then subsequently executed code in the update scope \ncould read g again, thus seeing the new value, violating ver\u00adsion consistency. A similar problem could \noccur with the execution of ST if code in the update scope prior to and following ST s exe\u00adcution could \nwrite to a variable read by ST, unmasking the illusion that ST is only executed at the start or end of \nan update scope. While we properly account for such changes to the heap, we do not account for interactions \nwith the environment outside the process. For example, if ST writes to the .le system and code in an \nupdate scope could read from the same .le location before and after an induced update point, then an \nupdate at that point could violate version consistency. While we could imagine tracking I/O effects to \navoid this situation, in practice we have never needed to write a state transformer that changes the \nexternal environment in a manner visible to existing code. Check-ins and relaxed synchronization. When \nusing relaxed synchronization, we designate check-in scopes (e(,e)) by label\u00ading a code block as CHECKIN:{s1; \n...; sn}. The compiler inserts a call to the function DSU checkin at the beginning of a check\u00adin block, \nand the computed check-in effect representations (and a similarly adjusted-for-checkin . set) are passed \nas arguments. In particular, if the normal effect of s1; ...; sn is e, and con\u00adtextual/updatability effects \nfor s1 and sn are (a1,.1, .1) and (an,.n, .n), respectively, the call inserted by the compiler will be \nDSU checkin(a1 . e, .n . e, .1). 1 thread restr restriction []; // per-thread check-ins 2 rwlock restriction \nmutex ; 3 4 volatile bool update requested=0; // patch available 5 set changes; // elements changed by \nthe patch 6 set changes; // changes . writes(ST) 7 mutex update mutex; // synchronizes patch application \n8 9 void DSU checkin(a, ., .) 10 11 { read lock ( restriction mutex ); 12 13 restriction [ thread self \n()] = {a, ., .}; unlock( restriction mutex ); 14 15 16 17 if (update requested) {if ( trylock (update \nmutex) == OK) {write lock ( restriction mutex ); 18 19 if (! con.icts ( restriction apply update (); \n,changes, changes)) { 20 update requested = 0; 21 22 } // else constraint unsatis.ed ; unlock( restriction \nmutex ); defer 23 unlock(update mutex); 24 25 }} } Figure 4. Check-in based relaxed synchronization \nprotocol. Generally speaking we should insert a call to check out at the end of a check-in scope, but \nwe avoid doing this in our experiments by making check-in blocks back-to-back and non-nested, so that \nthe code range covered by one is immediately followed by another. Otherwise we would need a stack of \ntriples per thread, where the topmost element represents the thread s current restriction, a check\u00adin \npushes the given triple on the stack, and a check-out pops it off; the stack is initialized to (T, T, \nT). With relaxed synchronization, inserted DSU update calls no longer have run-time effect rather than \nmodify them to include check-in effects we simply ignore them and in practice place a CHECKIN annotation \non the .rst block within the update scope. The pseudocode for DSU checkin is shown in Figure 4. The restriction \narray (line 1) is indexed by a (normalized) thread identi.er, and contains the arguments passed to the \nmost recent DSU checkin call, i.e., a triple of set IDs for the prior and future check-in effects a and \n., and the capability .. When no update is in progress, threads may change restriction in parallel because \nrestriction mutex is acquired in reader mode. This is safe because each thread will only write to its \nown portion of the array. When a patch p becomes available, the .ag update requested is set to 1, and \nthe sets changes(p) and changes(p) are popu\u00adlated. The next thread that checks in and acquires the update \nmutex will attempt to apply the update (lines 15 16). This updating thread will then acquire the restriction \nmutex in writer mode (line 17), and thus other threads will block at their next check\u00adin points (line \n11). Next, the updating thread checks whether the update contents con.ict with the current per-thread \nrestrictions; the call to con.icts checks that for each thread i, (changes(p) n ai = \u00d8. changes(p) n \n.i = \u00d8) . (changes(p) n .i = \u00d8), where restriction [ i ] = {ai,.i, .i}. If this check suc\u00adceeds, apply \nupdate proceeds with the update, redirecting func\u00adtion pointers, installing type transformers, and calling \nST, as described above. Either way, the updating thread releases the restriction mutex to unblock any \nchecked-in threads, and releases the update mutex to enable future updates (or retries).  Supporting \nconcurrency. Ginseng s type wrapping changes the representation of updatable named types, so we must \nbe careful not to introduce races. Since con functions can potentially call the type transformer to update \na value to the current version, a race-free read read access can become a racing write write access. \nTo avoid this problem, we changed con functions to use per-type locks to en\u00adsure atomic type transformation, \nand used double-checked locking to speed up the version check. However, introducing locks creates the \npotential for deadlock; e.g., a type transformer could call a func\u00adtion that tries to acquire an application \nlock while another thread holding that lock invokes the same type transformer. The problem can be avoided \nby writing type transformers that never call func\u00adtions (including other type transformers) that could \nacquire locks; adhering to this restriction was easy for all our test applications.  4.3 Discussion \nGinseng requires the programmer to choose update points S at de\u00adployment time, and STUMP likewise requires \na deployment-time choice of update scopes S and check-in scopes . .4 In our experi\u00adence, the choice of \nS (for single-threaded applications (Neamtiu et al. 2006)) or S (for multi-threaded applications) is \nrelatively clear and applies for all versions. As such, the deployment-time re\u00adstriction imposes no practical \nlimitation. On the other hand, delay\u00ading the choice of . until the patch p is known could reduce update\u00adrelated \ndelays. In particular, we could maximize the extent and number of check-in scopes in . by computing contextual \neffects for every statement in the program, and designating update scopes for each sequence of statements \nthat are safe with respect to p. The problem is that an update-time choice of . is dif.cult to imple\u00adment \nef.ciently. Compiling the program to insert check-in/check\u00adout stubs, where some subset of them is enabled \nwhen the patch is known, would add signi.cant bloat. Shepherding each thread s execution, e.g., using \nptrace, also seems fairly heavyweight. On the other hand, deploying with just de.nite update scopes and \na few check-in blocks opens the possibility of deploying more check-ins later, if needed. The programmer \ncould compute the max\u00adimal set . of check-in scopes relative to the given patch, adjust the original \nprogram source to use this set, and then update the de\u00adployed program with the new check-ins. Since the \nonly difference in the two programs is the presence of check-ins, which have no impact on the normal \nsemantics of the program, we could apply this patch piecemeal (e.g., one function at a time) to reduce \npossi\u00adble con.icts. Once the check-in patch is deployed, the actual patch p can be applied with maximum \neffectiveness. A similar trick can be played to accommodate further code extractions, if necessary. 5. \nExperience We used STUMP to dynamically update three open-source multi\u00adthreaded programs: the Icecast \nstreaming media server, Mem\u00adcached, a high-performance, distributed-memory object caching system, and \nthe Space Tyrant multi-player gaming server. We chose these programs because they are long-running, maintain \nsoft state that could be usefully preserved across patches, employ a variety of multi-threaded programming \npatterns, and spawn a non-trivial number of threads. In the remainder of this section, we brie.y present \neach program and its threading model, then we describe the evolution of these programs during the period \nwe considered, and .nally discuss changes we made to prepare the programs for compilation with STUMP. \n4 We do not speci.cally consider induced update points . in this discussion; they are analogous to check-in \nscopes. 5.1 Application overview Icecast is a streaming media server a popular solution for build\u00ading \nInternet radio stations. Updating Icecast on the .y would en\u00adable media content providers to keep their \nstreams live 24/7, yet be protected with the latest security .xes, or supporting the newest features. \nIcecast employs an event-based server model with a .xed number of threads, each performing separate duties: \naccepting a connection, handling incoming connections, reading from a media source, keeping statistics, \netc. Memcached is a high-performance, distributed-memory object caching system used on high-traf.c sites \nsuch as YouTube, Face\u00adbook, and Wikipedia to store and deliver pre-rendered Web content, avoiding slow \nper-client database accesses and on-demand render\u00ading. Updating Memcached on the .y would help maintain \nhigh web server throughput; taking Memcached down to install the next version would discard the in-memory \ncache and cause degraded operation while the restarted program s cache re.lls. Memcached uses a homogeneous \nthreading model, where all application threads (a user-con.gurable number) perform the same .xed task. \nSpace Tyrant is a multi-threaded gaming server. It uses a mixed threading model: three .xed threads (for \nmanaging the game state, accepting new connections and performing backups) and two threads for each client, \none dealing with user input, one deal\u00ading with output from the server to the client. On-the-.y updates \nto Space Tyrant would enable continuous game server operation, without having to disconnect clients for \neach update. Evolution history. Table 1 summarizes the release information and shows some of the ways \nthe programs changed over time. The .rst two groups of columns describe the .rst and last release we \nconsidered for each program. The last three groups of columns contain the cumulative number of changes \nthat occurred to the software over that span. Type changes refers to structs, unions and typedefs. We \ncan see that programs have changed signi.cantly during the period we considered. For example, Icecast \nadded nearly 4,000 lines of code; there were 25 changes to types, 10 changes to function prototypes, \nand 292 changes to function bodies.  5.2 Source code changes When building updatable applications with \nSTUMP, the program\u00admer may need to intervene at two phases: when preparing the source code for compilation \nwith STUMP, and when creating dy\u00adnamic patches. We present details on the strategy we followed, and programmer \neffort (annotations or lines of code) for each of these phases, in turn, for our three test programs. \nStage extraction. As mentioned in Section 4.1, Ginseng supports code extraction as a solution for updating \nlong-running code. In single-threaded Ginseng programs, long-running loop bodies are often extracted \ninto separate functions where a DSU update call is placed just after the extracted call. This approach \nensures the next loop iteration will be to the new version, and reduces the . re\u00adstrictions (types that \nare not allowed to change) at the update point, since the extracted function is inactive there. However, \njust extract\u00ading the loop body turns out to be insuf.cient in some cases for our multi-threaded programs, \nbecause many threads may be running the same loop, and blocking synchronization may prevent them from \nall exiting the loop at once. For example, producer/consumer\u00adstyle threads may result in one thread blocked \nin the .rst part of a loop while the second thread does work in the second part (Makris and Ryu 2007). \nTo address this problem, we must further extract logical stages of some loops into separate functions \nthat can be updated separately.  Program Updates First release Last release Function changes Type changes \nGlobal var. changes Ver. Date Size Ver. Date Size (LOC) (LOC) Proto Body Init Type Icecast 4 2.2.0 12/2004 \n25,349 2.3.1 11/2005 29,079 10 292 25 0 1 Memcached 3 1.2.2 05/2007 5,743 1.2.5 03/2008 6,345 14 118 \n6 1 5 Space Tyrant 6 0.307 10/2006 18,738 0.351 10/2007 20,223 0 107 11 2 3 Table 1. Application update \ninformation (all versions). Program Update scopes Check-ins Extractions loop stage Icecast 11 17 11 17 \nMemcached 1 1 0 0 Space Tyrant 5 16 7 16 Table 2. Source code annotations. Multi-threading annotations. \nTable 2 presents the number of an\u00adnotations we added to our test programs to prepare them for compi\u00adlation \nwith STUMP. Identifying long-running loops, update scopes, and stages was relatively straightforward, \nas we explain below. The second column shows the number of update scopes. The multi\u00adthreaded servers \nwe have considered perform a few high-level op\u00aderations whose boundaries suggest natural de.nite update \npoints. Examples of such operations are processing one event, accepting and dispatching a client connection, \netc. We enclosed each thread loop body for Icecast and Space Tyrant into an update scope. In the case \nof Memcached, the update scope delimits the processing of one event. The third column shows the number \nof blocks marked as check-ins; we placed check-in annotations around stages. To use the same source code \nfor both barrier and relaxed approaches, we directed STUMP to compile the beginning of check-in blocks \nto DSU induced update for the barrier approach, and to DSU checkin for the relaxed approach (see Section \n4.2). The last two columns show the number of times we used loop or stage extraction. Identifying long-running \nloops was easy, as each long-running thread essentially executes a loop. We identi.ed 11 loops in Icecast \nand 7 in Space Tyrant; these numbers are higher than the number of update scopes because some loops are \nnested. Loop extraction was not necessary for Memcached because looping occurs in an separate event-handling \nlibrary. Finally, the last column shows the number of stages designated for extraction. These often coincide \nwith the check-in blocks, but for Memcached no stage extraction was necessary. Other changes to source \ncode. In addition to designating update scopes, check-ins and stages, we had to make several small changes \nto application source code to cope with Ginseng s conservative safety analysis (Neamtiu et al. 2006). \nThese changes amounted to 42 lines for Icecast, 23 lines for Memcached and 19 for Space Tyrant (details \ncan be found in our technical report (Neamtiu and Hicks 2009)). Adjusting auto-generated patches. Ginseng \nautomatically gen\u00aderates candidate type transformer functions for type de.nitions that have changed. \nWe inspected (and completed, where necessary) the generated type transformers, and wrote state transformers \nwhen needed; across all patches, we had to write 80 lines of code for Icecast, 12 for Memcached and 81 \nlines for Space Tyrant. 6. Experiments We performed two sets of experiments. First, we measured how quickly \nan update can take effect in STUMP as compared to various alternatives (Section 6.1). Second, we measured \nthe overhead that STUMP s update support imposes on application performance com\u00adpared to stock versions \nof the programs, and the original Ginseng without our added support (Section 6.2). We conducted our experiments \nusing a client-server setup, where the updatable applications ran on a quad-core Xeon 2.66GHz server \nwith 4GB of RAM running Red Hat Enterprise Linux AS release 4, kernel version 2.6.9. The clients ran \non a two-way SMP Xeon 2.8GHz machine with 3.6GB of RAM running Red Hat En\u00adterprise Linux WS release 3, \nkernel version 2.4.21. The client and server systems were connected by a 100Mbps network. All C code \n(generated by STUMP or otherwise), was compiled with gcc 3.4.6 at optimization level -O2. 6.1 Update \ntimeliness To measure the effectiveness of induced update points and relaxed synchronization in STUMP, \nwe measure update timeliness, for all 13 patches that we developed, using gradual re.nements (called \nupdate protocols) to our approach for reaching safe update points. We start with a straightforward extension \nto Ginseng, and show that this performs poorly in practice. We then add induced update points, and .nd \nthat they improve update timeliness, though some\u00adtimes we fail to reach safe update points. Finally, \nwe add relaxed synchronization and show that it is very effective at reaching safe update points, fast. \nWe .rst describe the experimental setup, then proceed to describing the protocols and the results. We \nran experiments for each update and measured the time it took the system from the moment the update was \nsignaled to the moment it could safely be applied; results (in milliseconds) are pre\u00adsented for each \nprotocol in Table 3. The .rst column shows the pro\u00adgram, while the second column shows the update sequence \nnumber.5 The subsequent columns show the results for each protocol. For each protocol, we tested two \ncon.gurations, 4 and 16 con\u00adcurrent clients. The number of server-side threads varied, depend\u00ading on \nthe application and number of clients. Icecast has a .xed number of threads; in our con.guration this \nnumber was 6 in Ice\u00adcast 2.2.0, and 7 in later versions. Memcached has a thread pool with a con.gurable \nnumber of handler threads, independent of the number of clients. We present results for two con.gurations, \none with four server threads (Memc-4), and one with 16 server threads (Memc-16). Space Tyrant uses two \nthreads per connected client, plus three .xed threads that perform housekeeping, so the number of Space \nTyrant server-side threads were 11 (8 client handlers + 3 .xed) for the 4-client con.guration and 35 \n(32 client handlers + 3 .xed) for the 16-client con.guration. We are interested in measuring update timeliness \nwhile the server is under load (since thread activity is likely to obstruct up\u00addates from taking effect), \nand most importantly whether an update may take place at all. The methodology for each program was to \nstart the server, connect 4 (or 16) clients that are constantly asking 5 For Icecast, we considered versions \n2.2.0, 2.3.0rc1, 2.3.0rc2, 2.3.0.rc3, and 2.3.1; hence entry 0 corresponds to the update 2.2.0 . 2.3.0rc1. \nFor Memcached, we considered versions 1.2.2, 1.2.3, 1.2.4, and 1.2.5. For Space Tyrant, we considered \nversions 0.307, 0.316, 0.319, 0.331, 0.335, 0.347, and 0.351.  Table 3. Time to reach a safe update \npoint using various update protocols (in milliseconds). for data, and send an update request while the \nserver is perform\u00ading work. We then measured the time from the moment an update was requested to the \nmoment it could be safely applied, or timed out after 15 minutes. We performed each experiment 11 times; \nwe report the median time to reach a safe update point for terminating runs. An Xentry means that for \nthat speci.c con.guration, none of the 11 runs could reach such a point within 15 minutes. We also measured \nupdate loading times, i.e., time to load a dynamic patch after reaching a safe point. Loading times are \nproportional to patch size, and in all cases were less than 3 ms; we omit details due to space constraints. \nWe now present our protocols and .ndings. P-OPNOIND models a straightforward extension to Ginseng, and \ndoes not use induced update points (DSU induced update calls are treated as no-ops) or relaxed synchronization. \nIt na\u00a8ively tries to coax all threads to quiesce by yielding (via sched yield ) when a de.nite update \npoint is reached. If the last thread discovers that all threads are at de.nite update points, and that \nall these points are compatible with the update, the update is applied. The results are in columns 3 \nand 4 of Table 3. We can see that this protocol fails to reach a safe update point for almost all scenarios, \nand when it does, it can take several minutes to apply an update. P-BARRIERNOIND attempts to improve \nP-OPNOIND. When a thread reaches a de.nite update point, and the effects at that point do not con.ict \nwith the update, it blocks. When the last thread discovers that all threads are blocked, the update is \napplied. We can see that this strategy (columns 5 and 6) is more successful than P-OPNOIND, but we still \ncannot reach a safe update point for the Icecast updates and some Memcached updates. We discuss the reasons \nbelow. P-OPWAIT follows the optimistic approach of P-OPNOIND, but calls sched yield at induced update \npoints as well. We can see (columns 7 and 8) that the extra points help P-OPWAIT reaches safe update \npoints more quickly than P-OPNOIND, and in more cases.Likewise, P-BARRIER isthesameas P-BARRIERNOIND \nbut synchronizes at induced update points, and again (columns 9 and 10) the result is improved performance \nin both update successes and update times. P-RELAXED employs STUMP s support for check-ins and relaxed \nsynchronization. As shown in columns 11 and 12, P-RELAXED is the only protocol able to reach a safe point \nfor all updates, and it does so quickly, sometimes orders of magnitude faster than the other protocols. \nThe only update for which P-RELAXED is not the fastestis #0for Icecast (shown in the.rstrow). P-RELAXED \ntakes 1.75 seconds with 4 clients and 1.06 seconds with 16 clients to reach a safe update point, compared \nto 1.06 and 0.94 seconds, respectively, for P-BARRIER. The added slowdown is due to the more conservative \nsafety check for P-RELAXED. As explained in Section 4.2, in the relaxed approach, a check-in scope s1; \n...; sn with normal effect e calls DSU checkin(a1 . e, .1, .1) (recall .1 = .n . e). This effectively \nprevents anything in e from being updated while s1; ...; sn is being executed. By contrast, a DSU induced \nupdate call just prior to s1 would pass in a1,.1, .1, reducing the chance of a con.ict with the prior \neffect. The Icecast update #0 contains a particularly large number of changes, and the safety check fails \nfor many check-in scopes where it succeeds for induced update points at the same positions. On the other \nhand, the barrier-based protocols P-BARRIER and P-BARRIERNOIND must wait for all threads to reach an \n(induced or de.nite) update point before applying the update, which results in some updates failing to \ntake place. For P-RELAXED, the update is applied as soon as it becomes available if allowed by the current \nrestriction. In all the failing cases for Icecast and Memcached one or more threads are suspended due \nto a blocking call (on I/O or a condition variable) before reaching an update point, and will not proceed \nbefore new clients connect. For example, in the Memc-16 scenario, we have 16 server threads, and activity \nfrom only 4 clients prevents each of the 16 threads from being scheduled within the 15 minute time-out \nwindow. While the barrier protocols could reach safe points eventually under different workloads (e.g., \nwithout a .xed set of clients), a more robust solution might be to treat blocking calls as a kind of \ninduced update point, wrapping them with code to register a, ., . with the run-time system just prior \nto the actual call, and adding code to synchronize the thread upon returning from the call if an update \nhas since become available. An update may be applied once currently active threads reach safe points \nas long as regis\u00adtered threads are safe. While this solution should work for these programs, we feel \nbarrier synchronization is still generally undesir\u00adable for two reasons: (1) the server s performance \nwill be degraded because the existing client threads will block at update points and  Table 4. Benchmark \ncompletion times (elapsed times, in seconds, and in % relative to the stock server). not perform any \nwork until the patch is eventually applied, and (2) there is still the possibility of deadlock (despite \nnot observing it in our example applications), and this possibility must be addressed. The .nal protocol \nshown in Table 3 is P-POSTRELAXED, which is like P-RELAXED except that check-ins are treated as no-ops \nun\u00adtil an update has been requested once all threads have checked in their restrictions at least once, \nthe update protocol is the same as P-RELAXED. This protocol potentially reduces overhead dur\u00ading normal \noperation at the cost of slower patch application times. The performance here (columns 13 and 14) is \nroughly similar to P-BARRIER. The failure cases are for the same reason: when the patch becomes available, \nsome threads are blocked, and will only check in once awakened. The successful cases have similar times, \nthough Icecast update #0 is slower due to the more conservative safety check, as described above. Wrapping \nblocking calls as de\u00adscribed above should also help P-POSTRELAXED. However, our performance experiments \nin Section 6.2 show that, in practice, the cost of always doing check-ins is modest, so P-RELAXED provides \na good balance between overhead and timeliness.  6.2 Performance overhead We evaluated the impact of \ndynamic update support on applica\u00adtion performance by running application-speci.c benchmarks, and measuring \nmemory footprint. For each application, we measured the performance of its most recent version under \nthree con.gurations. The stock con.guration forms our base for benchmarking, and consists of the applica\u00adtion \ncompiled normally, without support for updating and with\u00adout involving STUMP. The Ginseng con.guration \nis the applica\u00adtion compiled with a normal Ginseng, which generates type trans\u00adformer code assuming single-threading, \nand treats check-ins as no\u00adops. The STUMP con.guration is the application compiled with STUMP, which \nassumes multi-threading and implements check-ins calls as registering check-in effects as described earlier. \nComparing the Ginseng and STUMP con.gurations shows the additional over\u00adhead STUMP imposes on applications, \ni.e., double-checked lock\u00ading when calling concretization functions, and effect registration at check-ins. \nFor each application and con.guration, we ran a speci.c bench\u00admark and measured the completion time and \nmemory footprint (at the completion of the benchmark). We ran each benchmark in two setups. The .rst \nsetup, remote, shows the results of running the clients and server on separate machines, a scenario that \nmodels how the updatable servers would be used in practice. The second setup, local, shows the results \nof running the clients and server on the same machine (we used the quad-core machine mentioned above). \nThe local con.guration factors out network latency and bandwidth issues (while reducing parallelism of \nthe server). Similar to the up\u00addate protocol experiments in Section 6.1, we report .gures for 4 and 16 \nclients, respectively. Application performance. For Icecast, we measured the time it took the streaming \nserver to serve eight mp3 .les to a wget client. Each .le has size 1, 2, 3, ... 8 MB. To eliminate jitter \ndue to disk I/O, we directed wget to send both its output and the downloaded .le to /dev/null. For Memcached, \nwe ran a slap test that ships with the server. The test program spawns 4 or 16 clients in parallel (the \nsame number of clients as the number of server threads), each client inserting key/value pairs into Memcached \ns hash table. We measured the time it took the test program to complete insertion of 50,000 key/value \npairs. For Space Tyrant, we created a scenario .le that directs a client to perform 500 random moves \nacross the universe, and spawned concurrent clients running this scenario. We measured the time it took \nthe server to process all the clients. In Table 4 we report the median benchmark completion time across \n11 runs. In the remote, more realistic, setup, for Icecast and Space Tyrant, the completion time is similar \nto the stock server. Memcached is however slower in the 16-thread con.guration, with the multi-threaded \nupdatable version 1.6% slower. In the local setup, impact of update support on completion time is higher \nthan in the remote setting; this is because, as expected, update support (e.g., check-ins, function and \ntype indirection) slows down the ap\u00adplication and the slow-down cannot be masked by network latency. \nHowever, even in this scenario the slowdown is small, less than 7% in all cases. Finally, by comparing \nthe Ginseng and STUMP columns we can quantify the additional cost of supporting multi\u00adthreading on top \nof a DSU compiler. For example, by looking at the second-to-last row, we can see that for Memcached, \nthe cost of multi-threading is an extra 0.4% (4.09% vs. 4.49%) in the 4-client con.guration, and 1.34% \n(2.42% vs. 3.76%) in the 16-client con\u00ad.guration, respectively. Memory footprint. Detailed .gures on \nmemory overhead can be found in our technical report (Neamtiu and Hicks 2009). In sum\u00admary, we found \nthe memory footprint increase for the updatable con.gurations to be negligible for Icecast and Memcached: \nless than 1% and 4%, respectively. For Space Tyrant, the increase was up to 46%, due to Ginseng s type \nwrapping scheme. In the technical report we explain how to reduce this overhead to 13% by perform\u00ading \na small refactoring. 7. Related work Several existing systems can dynamically update multi-threaded programs, \nbut to our knowledge our work is the .rst to identify the safety/timeliness trade-off explicitly, and \nto consider solutions for it in any depth. Existing systems do provide restrictions on update timing, \nboth manual and automatic, but fully automatic approaches are insuf.cient to solve the problem while \nlittle or no guidance is given in how to use manual mechanisms effectively. Many systems require that \nupdates not be performed on cur\u00adrently running functions, with some relying on this activeness check \nas the sole means for ensuring safety; examples include the K42 operating system (Soules et al. 2003; \nBaumann et al. 2007), OPUS (Altekar et al. 2005), and Ksplice (Arnold and Kaashoek 2009). Relying on \nthe activeness check alone negatively affects safety and timeliness. As shown in Sections 2.2 and 2.3, \nactiveness does not preclude some invalid update times, and may inde.nitely delay an update if it changes \nfunctions that contain in.nite loops. Some systems (Arnold and Kaashoek 2009; Altekar et al. 2005) attempt \nto avoid safety problems by limiting the form of updates to code only, and not state. However, even this \nrestriction is not suf.cient to avoid all problems. Notice that if our example update in Figure 1 only \nconsisted in moving the call to log from process to clean then it could still exhibit the problematic \nexecution while changing only code.  Several systems including LUCOS (Chen et al. 2006), PO-LUS (Chen \net al. 2007), and UpStare (Makris and Bazzi 2009) provide .ne-grained control over when to apply a patch \nso that it can be applied quickly. LUCOS and POLUS permit updates to ac\u00adtive code where active functions \ncontinue to execute at the old ver\u00adsion; by default, subsequent calls target the most recent version, \nbut the programmer can override a particular call to be .xed at one ver\u00adsion. UpStare uses stack reconstruction \nto allow an actively running function to transition to a corresponding point in the new version of the \nsame function when an update is applied. This technique has the same effect as Ginseng s code extraction, \nbut is more .exible, as transition points can be speci.ed at patch time, not deployment time. (The need \nfor this support was motivated by earlier experi\u00adence writing small updates to long-running functions \nin the Linux kernel (Makris and Ryu 2007).) While these mechanisms are use\u00adful, they do not mitigate \nthe problem of reasoning about the effects of a patch in the large state space of a multi-threaded program. \nBy contrast, our approach reduces the programmer s burden of rea\u00adsoning about safety to a few de.nite \nupdate points, while induced update points and relaxed synchronization ensure timeliness. We imagine \nour techniques could be applied to these existing systems. Our previous work (Neamtiu et al. 2008) introduced \nthe idea of version-consistency and the use of contextual effects to enforce it. In that work we considered \nsingle-threaded programs whereas for the present work we have a full implementation for multi-threading, \nSTUMP, which we have evaluated on several realistic programs. STUMP incorporates the new ideas of check-in \neffects and relaxed synchronization. In our prior work, we proposed enforcing version\u00adconsistency within \nprogrammer-speci.ed update transactions. We initially expected this idea to transfer directly to multi-threaded \nprograms, with programmers using update transactions frequently and at a .ne granularity just as they \nmight use modern software transactions (a.k.a. atomic blocks) (Harris et al. 2005). However, we found \nit much easier to reason about dynamic updates occurring at a small number of de.nite update points/scopes \nat the top-level, as advocated by the present work, rather anywhere within a web of nested transactions. \n8. Conclusion In this paper, we presented an approach for updating multi-threaded programs while they \nrun, and show how we have implemented this approach in STUMP. Updating multi-threaded programs is more \ndif.cult than updating single-threaded programs because myriad thread interactions complicate reasoning \nhow an update will inter\u00adact with the many states of the system, while timing restrictions on an update \ns application that would reduce this burden may un\u00adduly delay the update from taking effect. We address \nthis tension using the novel concepts of induced update points and relaxed syn\u00adchronization, which can \nbe used to ensure updates are performed promptly while easing the programmer burden of reasoning about \npatch application correctness. We evaluated our approach on three realistic multi-threaded servers. We \nfound that programmer effort for building updatable versions of these applications was modest, and experiments \nshow that update support does not signi.cantly impact application performance. Acknowledgments This research \nwas supported in part by NSF grants CCF-0541036 and CNS-0346989, and the partnership between UMIACS and \nthe Laboratory for Telecommunication Sciences. We thank Ray (Otis) Eargin for providing us earlier versions \nof Space Tyrant. We also thank Gavin Bierman, Gianfranco Ciardo, Jeff Foster, Eric Hardisty, Chris Hay\u00adden, \nScott Owens, Polyvios Pratikakis, Peter Sewell, Nikhil Swamy, and the anonymous referees for their helpful \ncomments on drafts of this paper. References Gautam Altekar, Ilya Bagrak, Paul Burstein, and Andrew Schultz. \nOpus: online patches and updates for security. In USENIX Security, 2005. Jeff Arnold and Frans Kaashoek. \nKsplice: Automatic rebootless kernel updates. In EuroSys, 2009. Andrew Baumann, Jonathan Appavoo, Robert \nW. Wisniewski, Dilma Da Silva, et al. Reboots are for hardware: challenges and solutions to updating \nan operating system on the .y. In USENIX ATC, 2007. Haibo Chen, Rong Chen, et al. Live updating operating \nsystems using virtualization. In VEE, 2006. Haibo Chen, Jie Yu, Rong Chen, Binyu Zang, and Pen-Chung \nYew. PO-LUS: A powerful live updating system. In ICSE, 2007. Deepak Gupta, Pankaj Jalote, and Gautam \nBarua. A formal framework for on-line software version change. IEEE TSE, 22(2), 1996. Tim Harris, Simon \nMarlow, Simon L. Peyton Jones, and Maurice Herlihy. Composable memory transactions. In PPOPP, 2005. Michael \nHicks and Scott Nettles. Dynamic software updating. ACM Trans. Program. Lang. Syst., 27(6), 2005. K42. \nThe K42 Project. http://www.research.ibm.com/K42/. Insup Lee. DYMOS: A Dynamic Modi.cation System. PhD \nthesis, Dept. of Computer Science, University of Wisconsin, Madison, April 1983. Kristis Makris and Rida \nBazzi. Multi-threaded dynamic software updates using stack reconstruction. In USENIX ATC, 2009. Kristis \nMakris and Kyung Dong Ryu. Dynamic and adaptive updates of non-quiescent subsystems in commodity operating \nsystem kernels. In EuroSys, 2007. Iulian Neamtiu. Practical Dynamic Software Updating. PhD thesis, Uni\u00adversity \nof Maryland, College Park, August 2008. Iulian Neamtiu and Michael Hicks. Safe and timely updates to \nmulti\u00adthreaded programs. Technical report, UC Riverside, June 2009. Iulian Neamtiu, Jeffrey S. Foster, \nand Michael Hicks. Understanding Source Code Evolution Using Abstract Syntax Tree Matching. In MSR, 2005. \nIulian Neamtiu, Michael Hicks, Gareth Stoyle, and Manuel Oriol. Practical dynamic software updating for \nC. In PLDI, 2006. Iulian Neamtiu, Michael Hicks, Jeffrey S. Foster, and Polyvios Pratikakis. Contextual \neffects for version-consistent dynamic software updating and safe concurrent programming. In POPL, January \n2008. Polyvios Pratikakis, Jeffrey S. Foster, and Michael Hicks. Context-sensitive correlation analysis \nfor detecting races. In PLDI, 2006. Stelios Sidiroglou, Sotiris Ioannidis, and Angelos D. Keromytis. \nBand-aid patching. In HotDep, 2007. C. Soules, J. Appavoo, K. Hui, et al. System support for online recon.gu\u00adration. \nIn USENIX ATC, 2003. Gareth Stoyle, Michael Hicks, Gavin Bierman, Peter Sewell, and Iulian Neamtiu. Mutatis \nMutandis: Safe and .exible dynamic software up\u00addating. TOPLAS, 29(4), August 2007. Suriya Subramanian, \nMichael Hicks, and Kathryn S. McKinley. Dynamic software updates for Java: A VM-centric approach. In \nPLDI, June 2009. Jean-Pierre Talpin and Pierre Jouvelot. Polymorphic type, region and effect inference. \nJFP, 2, 1992. Chris Walton. Abstract Machines for Dynamic Computation. PhD thesis, University of Edinburgh, \n2001. ECS-LFCS-01-425.    \n\t\t\t", "proc_id": "1542476", "abstract": "<p>Many dynamic updating systems have been developed that enable a program to be patched while it runs, to fix bugs or add new features. This paper explores techniques for supporting dynamic updates to multi-threaded programs, focusing on the problem of applying an update in a timely fashion while still producing correct behavior. Past work has shown that this tension of <i>safety</i> versus timeliness can be balanced for single-threaded programs. For multi-threaded programs, the task is more difficult because myriad thread interactions complicate understanding the possible program states to which a patch could be applied. Our approach allows the programmer to specify a few program points (e.g., one per thread) at which a patch may be applied, which simplifies reasoning about safety. To improve timeliness, a combination of static analysis and run-time support automatically expands these few points to many more that produce behavior equivalent to the originals. Experiments with thirteen realistic updates to three multi-threaded servers show that we can safely perform a dynamic update within milliseconds when more straightforward alternatives would delay some updates indefinitely.</p>", "authors": [{"name": "Iulian Neamtiu", "author_profile_id": "81100589658", "affiliation": "University of California, Riverside, Riverside, CA, USA", "person_id": "P1464379", "email_address": "", "orcid_id": ""}, {"name": "Michael Hicks", "author_profile_id": "81100060959", "affiliation": "University of Maryland, College Park, College Park, MD, USA", "person_id": "P1464380", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1542476.1542479", "year": "2009", "article_id": "1542479", "conference": "PLDI", "title": "Safe and timely updates to multi-threaded programs", "url": "http://dl.acm.org/citation.cfm?id=1542479"}