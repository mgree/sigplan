{"article_publication_date": "06-15-2009", "fulltext": "\n Proving Optimizations Correct using Parameterized Program Equivalence * Sudipta Kundu Zachary Tatlock \nSorin Lerner University of California, San Diego {skundu,ztatlock,lerner}@cs.ucsd.edu Abstract Translation \nvalidation is a technique for checking that, after an op\u00adtimization has run, the input and output of \nthe optimization are equivalent. Traditionally, translation validation has been used to prove concrete, \nfully speci.ed programs equivalent. In this paper we present Parameterized Equivalence Checking (PEC), \na gener\u00adalization of translation validation that can prove the equivalence of parameterized programs. \nA parameterized program is a partially speci.ed program that can represent multiple concrete programs. \nFor example, a parameterized program may contain a section of code whose only known property is that \nit does not modify certain variables. By proving parameterized programs equivalent, PEC can prove the \ncorrectness of transformation rules that represent com\u00adplex optimizations once and for all, before they \nare ever run. We im\u00adplemented our PEC technique in a tool that can establish the equiv\u00adalence of two \nparameterized programs. To highlight the power of PEC, we designed a language for implementing complex \noptimiza\u00adtions using many-to-many rewrite rules, and used this language to implement a variety of optimizations \nincluding software pipelin\u00ading, loop unrolling, loop unswitching, loop interchange, and loop fusion. \nFinally, to demonstrate the effectiveness of PEC, we used our PEC implementation to verify that all the \noptimizations we im\u00adplemented in our language preserve program behavior. Categories and Subject Descriptors \nD.2.4 [Software Engineer\u00ading]: Software/Program Veri.cation correctness proofs, relia\u00adbility, validation; \nD.3.4 [Programming Languages]: Processors compilers, optimization General Terms Reliability, Languages, \nVeri.cation. Keywords Compiler Optimization, Correctness, Translation Vali\u00addation. 1. Introduction Compilers \nare a fundamental component of the toolset program\u00admers rely on every day. As a result, compiler correctness \nis crucially important. A bug in the compiler can systematically introduce er\u00adrors in each generated \nexecutable. Furthermore, compiler bugs can * Supported in part by NSF grants CCF-0644306 and CCF-0811512. \nPermission to make digital or hard copies of all or part of this work for personal or classroom use is \ngranted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. PLDI \n09, June 15 20, 2009, Dublin, Ireland. Copyright c &#38;#169; 2009 ACM 978-1-60558-392-1/09/06. . . $5.00 \ninvalidate strong guarantees that were established on the original source program. As an example, various \nanalysis tools can prove the absence of certain kinds errors at the source level (e.g. dou\u00adble locking \nor null pointer exceptions). However, if the compiler is not guaranteed to be correct, then no source-level \nguarantees can be safely transferred to the generated code. Finally, compiler cor\u00adrectness is all the \nmore important in high-assurance domains like avionics and medical equipment, where the cost of incorrect \ncom\u00adpilation can be extremely high. Unfortunately, building reliable compilers is dif.cult, error\u00adprone, \nand requires signi.cant manual effort. Indeed, it takes a long time to develop a compiler that is stable \nenough for broad adoption (often up to a decade), which in turn hinders the development of new languages \nand architectures. One of the most error prone parts of a compiler is its optimiza\u00adtion phase. Many optimizations \nrequire an intricate sequence of complex transformations. Often these transformations interact in unexpected \nways, leading to a combinatorial explosion in the num\u00adber of cases that must be considered to ensure \nthat the optimization phase is correct. In fact, even mature compilers have optimization bugs and as \na result professional developers sometimes go as far as disabling optimizations in critical modules to \nlower the likeli\u00adhood of incorrectly generated code. Unfortunately, it is becoming less and less feasible \nto increase the reliability of a compiler by simply disabling most of its optimizations: with the widespread \nadoption of systems whose good performance depends heavily on compiler optimizations for example just-in-time \ncompilers and higher-level languages turning off the optimizer is no longer an option. Furthermore, \nthe dif.culties of developing correct op\u00adtimizations also prevent end-user programmers (non-compiler \nex\u00adperts) from extending the compiler with simple custom optimiza\u00adtions, making most compilers closed \nblack boxes, rather than open\u00adended extensible frameworks. Previous techniques for providing correctness \nguarantees for optimizations can be divided into two categories. In the .rst cat\u00adegory optimizations \nare proved correct once and for all [7, 31, 8, 2, 14, 15, 16]. In this setting, to prove that an optimization \nis correct, one must prove that for any input program the optimization pro\u00adduces a semantically equivalent \nprogram. The second category con\u00adsists of proving correctness each time an optimization is run. Here, \neach time the compiler runs an optimization, an automated tool tries to prove that the original program \nand the corresponding optimized program are equivalent. This technique, which is called translation validation, \nhas been successfully applied in a variety of settings, including mature optimizing compilers [20, 19, \n22, 6, 32, 27, 28], re.nement checking of CSP programs [11], and high-level synthe\u00adsis validation [12]. \nThe primary advantage of once-and-for-all techniques is that they provide a very strong guarantee: optimizations \nare known to be correct when the compiler is built, before they are run even once.  In contrast, translation \nvalidation provides a weaker correctness guarantee. This is because translation validation guarantees \nthat only a particular run of the optimization is correct. Compilers that include translation validation \nmay still contain bugs and it is unclear what a programmer should do when the compiler is unable to correctly \ncompile a program. On the other hand, translation validation has a clear advantage over once-and-for-all \ntechniques in terms of automation. Most of the techniques that provide once-and-for-all guarantees require \nuser interaction. Those that are fully automated, for example Cobalt [14] and Rhodium [15], work by having \nprogrammers implement opti\u00admizations in a domain-speci.c language using .ow functions and single-statement \nrewrite rules. Unfortunately, the set of optimiza\u00adtions that these techniques can prove correct has lagged \nbehind translation validation. In particular, translation validation can al\u00adready handle complex loop \noptimizations like skewing, splitting and interchange, which have thus far eluded automated once-and\u00adfor-all \napproaches. A common intuition is that once-and-for-all proofs are harder to achieve because they must \nshow that any appli\u00adcation of the optimization is correct, as opposed to a single instance. In this paper, \nwe present a new technique for proving optimiza\u00adtions correct called Parameterized Equivalence Checking \n(PEC) that bridges the gap between translation validation and once-and\u00adfor-all techniques. PEC generalizes \ntranslation validation to han\u00addle parameterized programs, which are partially speci.ed programs that \ncan represent multiple concrete programs. For example, a pa\u00adrameterized program may contain a section \nof code whose only known property is that it does not de.ne or use a particular vari\u00adable. The key insight \nof PEC is that existing translation validation techniques can be adapted to work in the broader setting \nof param\u00adeterized programs. This allows translation validation techniques, which have traditionally been \nused to prove concrete programs equivalent, to prove parameterized programs equivalent. Most im\u00adportantly, \nbecause optimizations can be expressed as nothing more than parameterized transformation rules, using \nbefore and after pa\u00adrameterized code patterns, PEC can prove once and for all that such optimizations \npreserve semantics. To highlight the power and generality of PEC, we designed a new language for writing \noptimizations, and implemented a checker based on PEC that can automatically check the correctness of \noptimizations written in this language. Our language for imple\u00admenting and proving optimizations correct \nis much more expres\u00adsive than previous such optimization languages, like Cobalt [14] and Rhodium [15]: \nwhereas Cobalt and Rhodium only supported local rewrites of a single statement to another, our language \nsup\u00adports many-to-many rewrite rules. Such rules are able to replace an entire set of statements, even \nentire loops and branches, with a completely different set of statements. Using these rules, we can express \nmany more optimizations than in Cobalt and Rhodium, and we can also prove them all correct using our \nPEC algorithm. In summary, our contributions are: We developed and implemented a technique for performing \nPa\u00adrameterized Equivalence Checking. PEC adapts two approaches from traditional translation validation \nto the setting of once\u00adand-for-all correctness proofs, namely the relational approach of Necula [19], \nand the permute approach of Zuck et al. [32].  We developed a new language for implementing optimizations. \nOur language is more expressive than previous languages that can be checked for correctness automatically: \nit has explicit support for expressing many-to-many transformations, meaning that a set of statements \ncan be transformed to another set of statements in a single rewrite.  a[0] += 1; b[0] += a[0]; i:= 0 \na[1] += 1; while (i < n) { i:=0 a[i] += 1; while (i < n -2) { b[i] += a[i]; a[i+2] += 1; c[i] += b[i]; \nb[i+1] += a[i+1]; i++; c[i] += b[i]; } i++ } c[i] += b[i]; b[i+1] += a[i+1]; c[i+1] += b[i+1]; (a) (b) \n Figure 1. Software pipelining: (a) shows the original code, and (b) shows the optimized code. We implemented \nand proved correct a variety of complex op\u00adtimizations in our system. Some of these optimizations, for \nexample partial redundancy elimination, could have been ex\u00adpressed and proved correct in Rhodium [15], \nbut they are eas\u00adier to express in our language because of the built-in sup\u00adport for many-to-many rewrite \nrules. Furthermore, many of the optimizations we implemented and proved correct could not be proved correct \nor even expressed in previous systems like Rhodium. This includes: software pipelining, loop unswitch\u00ading, \nloop unrolling, loop peeling, loop splitting, loop alignment, loop interchange, loop skewing, loop reversal, \nloop fusion and loop distribution. The rest of the paper is organized as follows. Section 2 presents \nan overview of our approach through an example. Section 3 de\u00adscribes our PEC system at an architectural \nlevel, identifying its three main modules. The following three sections (Sections 4, 5 and 6) present \neach of the three modules in more detail. Finally, Section 7 presents our experimental results.  2. \nOverview We illustrate the main ideas of our approach through an example: software pipelining. Software \npipelining is an optimization that tries to break dependencies within a loop body by spreading in\u00adstructions \nfrom one iteration in the original program across multi\u00adple iterations of the transformed program. Software \npipelining can break dependencies inside a loop body, and thus provides more .exibility to the scheduler, \nbut it does so without increasing the code size of the loop body (as opposed to loop unrolling, which \nalso provides more .exibility to the scheduler, but may have ad\u00adverse effects on the cache by increasing \nthe loop body size). As an example, consider the code in Figure 1(a). The loop updates three arrays iteratively, \nbut because each update depends on the previous one, each instruction in the loop must wait until the \ninstruction immediately before it .nishes. Figure 1(b) shows the result of applying software pipelining \non this loop. The key insight is that, in the steady state, the transformed loop still runs the same \nthree instructions from the original loop, but now each of the three instructions is from a different \niteration of the original loop. The a[i+2] instruction runs two iterations ahead; the b[i+1] runs one \niteration ahead; and the c[i] instruction runs on the current iteration. In order to get into this steady \nstate, one has to add a prologue at the beginning of the transformed loop in order to setup the pipelining \neffect. There is also an epilogue after the loop to execute the remaining instructions.  2 3 I := 0 \n2 3 6 S0 7 I := 0 6 7 6 S1 7 6 7 L1 : S0 67 6 7 while (I < E-1) { 6 7 6 L2 : while (I < E) { 7 6 7 6 \n7 S2 6 7 6 L3 : S1 7 =. 6 7 6 7 I++ L4 : S2 6 7 6 7 6 S1 7 4 5 L5 : I++ 6 7 6 } 7 } 45 S2 I++  where \nDoesNotModify(S0, I)@L1. DoesNotModify(S1, I)@L3 . DoesNotModify(S2, I)@L4. StrictlyPositive(E)@L2 . \nDoesNotModify(S1, E)@L3. DoesNotModify(S2, E)@L4 . DoesNotModify(I++, E)@L5 Figure 2. First part of software \npipelining 32 32 L1 : S2 S1[ I+1 ] 4 I++ 54 5 =. S2 S1[ I ] I++ where DoesNotModify(S2, I)@L1. Commute(S2, \nS1[ I+1 ])@L1 Figure 3. Second part of software pipelining fact StrictlyPositive(E) has meaning eval(s, \nE) > 0 fact DoesNotModify(S, E) has meaning eval(s, E)= eval(step(s, S), E) fact Commute(S1, S2) has \nmeaning step(step(s, S1), S2)= step(step(s, S2), S1) Figure 4. Meanings of some facts that we use in \nour system Software pipelining is a non-trivial optimization, with many subtle corner cases that need \nto be correctly implemented. The pro\u00adlogue and the epilogue must execute a precisely crafted sequence \nof instructions to setup and unwind the steady state; the instructions in the loop must be correctly \nre-indexed; and the entire optimization must be applied only if no dependencies in the original program \nwould be broken by software pipelining. 2.1 Expressing Software Pipelining We implement software pipelining \nin our language as the repeated application of two simple optimizations. In Figure 2 we show the .rst \none, which simply moves some instructions (namely S1) from the current iteration to the next iteration. \nOptimizations in our lan\u00adguage are written as parameterized rewrite rules with side condi\u00adtions: P1 =. \nP2 where f,where P1 and P2 are parameterized programs, and f is a side condition that states when the \nrewrite rule can safely be .red. An optimization P1 =. P2 where f states that when a concrete program \nis found that matches the parameterized program P1, it should be transformed to P2 if the side condition \nf holds. Parameterized programs. A parameterized program is a partially speci.ed program that can represent \nmultiple concrete programs. For example, in the original and transformed programs from Fig\u00adure 2, S0 \nranges over concrete statements (including branches, 2 3 I := 0 2 3 6 S0 7 I := 0 6 7 6 S1[ I ] 7 6 7 \nS0 67 6 7 while (I < E-1) { 6 7 6 while (I < E) { 7 6 7 6 7 S1[ I+1 ] 6 7 6 S1[ I ] 7 =. 6 7 6 7 S2 6 \n7 6 S2 7 6 I++ 7 4 5 I++ 6 7 6 } 7 } 45 S2 I++ Figure 5. Software pipelining as one rewrite rule loops, \nand sequences of statements) that are single-entry-single exit; I ranges over concrete program variables; \nand E ranges over concrete expressions. Because variables like S0, I and E range over the syntax of concrete \nprograms, we call such variables meta\u00advariables. To simplify exposition, rather than provide explicit \ntypes for all meta-variables, we instead use the following naming con\u00adventions: meta-variables starting \nwith S range over statements, meta-variables starting with E range over expressions, and meta\u00advariables \nstarting with I range over variables. Side Conditions. The side conditions are boolean combinations of \nfacts that must hold at certain points in the original program. For example the side condition DoesNotModify(S0, \nI)@L1 in Figure 2 states that at location L1 in the original program S0 should not modify I. In general, \nside conditions are .rst-order logic formulas with facts like DoesNotModify(S0, I)@L1 as atomic predicates. \nEach fact used in the side condition must have a semantic meaning, which is a predicate over program \nstates. Figure 4 gives the semantic meanings for the three primary facts that we use in our system. In \ngeneral, meanings can be .rst-order logic formulas with a few special function symbols: (1) s is a term \nthat represents the program state at the point where the fact holds. (2) eval evaluates an expression \nin a program state and returns its value; (3) step executes a statement in a program state and returns \nthe resulting program state. The semantic meanings are used by the PEC algorithm to de\u00adtermine the semantic \ninformation that can be inferred from the side conditions when proving correctness. Although optimization \nwrit\u00aders must provide these meanings, in our experience we have found that there is a small number of \ncommon facts used across many dif\u00adferent optimizations (for example DoesNotModify), and since these meanings \nonly need to be written once, the effort in writing mean\u00adings is not onerous. Executing optimizations. \nOptimizations written in our language are meant to be executed by an execution engine. When running an \noptimization P1 =. P2 where f, the execution engine must .nd concrete program fragments that match P1.Furthermore,it \nmust perform some program analysis to determine if the facts in the side condition f hold. One option \nfor implementing these pro\u00adgram analyses is to use a general purpose programming language. Although this \nprovides the most .exibility, it does not guarantee that the facts in the side condition are computed \ncorrectly. Alter\u00adnatively, if one wants stronger correctness guarantees, the facts in the side conditions \ncan be computed in a way that guarantees that their semantic meanings hold, for example using the Rhodium \nsys\u00adtem of Lerner et al. [15], or using Leroy s Compcert system [16]. Although all the side conditions \nwe used in our system pertain to the original program, it is also possible to express side conditions \nover the transformed program. In such cases, the execution engine would check the side conditions by \nbuilding the transformed pro\u00adgram before knowing that all the side conditions hold, and then  23 i:= \n0 6 a[i] += 1; 7 2 3 67 i:= 0 a[i+1] += 1; 67 67 6 a[i] += 1 7 b[i] += a[i]; 67 2 3 6 7 i:=0 while(i<n-1){ \n6 while (i< n-1-1){ 7 67 . 67 67 6 while (i< n) { 7 a[i+1] += 1; 6 a[i+1+1] += 1; 7 67 67 . S1[ I ] \n67 67 6 a[i] += 1; . S1[ I ] 7 b[i] += a[i]; 6 b[i+1] += a[i+1]; 7 67 . 67 67 67 7 6 b[i] += a[i]; \n7 =. c[i] += b[i]; . S2 =. 6 c[i] += b[i]; 67 6 . S2 7 67 6 c[i] += b[i]; 76 i++; 76 i++ 7 67 67 45 \ni++; 6 } 76 } 7 67 67 } 6 b[i] += a[i]; 7 c[i] += b[i]; 67 45 6 i++ 7c[i] += b[i]; 67 i++ 6 b[i] \n+= a[i]; 7 45 c[i] += b[i]; i++ Figure 6. Two successive applications of the rewrite rule from Figure \n5 running analyses on the transformed program to determine if the transformation can safely be performed. \nThe second part of software pipelining, shown in Figure 3, simply reorders statements. This optimization \nuses a new kind of meta-variable, S1[ I ]. In this case S1 is a program fragment with a hole in it, and \nI .lls in the hole. The S1[ I ] pattern is interpreted as follows: S1 matches any single-entry-single-exit \npiece of code that contains direct uses of a variable I, but no modi.cations of I. In such cases S1 gets \nmatched to the code, but with holes wherever I occurs, so that S1[ I+1 ] represents the original statement \nwith I replaced by I+1. The fact that I is not modi.ed in S1 and that S1 captures all uses of I allows \nus to treat such statements as function calls from a veri.cation point of view. The side condition in \nFigure 3 uses a new fact called Commute, which holds when two statements can be re-ordered. There are \na variety of ways of implementing such a predicate when the compiler runs, for example the Omega test \n[21], or more generally dependence analysis [18]. We also show in Section 6 how we can express a version \nof Commute that can be written more easily in Rhodium, and thus proved correct automatically. To ease \npresentation of how our software pipelining optimiza\u00adtion operates, Figure 5 uses a single rewrite rule \nto summarize the effect of running the transformation from Figure 2 followed by the one from Figure 3. \nWe show in Figure 6 how two applications of this single rewrite rule performs software pipelining on \nour exam\u00adple. At each step, we show what S1 and S2 are instantiated with. If there were more statements \nin the loop, the transformation from Figure 5 could be applied more times, with S1 ranging over one additional \nstatement each time.  2.2 Proving Correctness of Software Pipelining Our goal is to show that the software \npipelining optimization writ\u00adten in our language is correct, once and for all, before it is even run \nonce. To do this, we must show that each of the rewrite rules from Figures 2 and 3 satisfy the following \nproperty: given the side conditions, the original parameterized program and the transformed parameterized \nprogram have the same behavior. To illustrate the salient features of our approach, we show how we can \nprove the .rst and more complicated part of software pipelining, namely the rewrite rule from Figure \n2. Parameterized Equivalence Checking. Translation validation (TV) is a technique that has been used \nto prove equivalence of pro\u00adgram fragments. Traditionally, TV is applied while the compiler is running, \nand so TV proves concrete, fully speci.ed programs equivalent. In our setting, we are attempting to prove \nparameter\u00adized programs equivalent. To achieve this, we developed a tech\u00adnique called Parameterized Equivalence \nChecking (PEC) that gen\u00aderalizes traditional TV techniques to the setting of parameterized programs. \nThere are two simple observations that intuitively explain why techniques from translation validation \ncan be generalized to param\u00adeterized programs. The .rst observation is that if a program frag\u00adment S \nin the original program executes in a program state s,and the same program fragment S executes in the \ntransformed program in the same state s, then we know that the two resulting states are equal. This shows \nthat we can reason about state equality even if we don t know what the program fragments are. The second \nobser\u00advation is that when proving equivalence, we are usually interested in some key invariants that \njustify the optimization. The insight is that the semantic meaning of the side condition captures precisely \nwhen these key invariants can be propagated throughout statements that are not fully speci.ed. For example, \nif the correctness of an optimization really depends on I not being modi.ed in a region of code, the \nside condition will allow us to know this fact, and thus reason about I across such unknown statements. \nBisimulation relations. PEC proves equivalence using bisimula\u00adtion relations, which are de.ned in terms \nof the more basic concept of correlation relations. A correlation relation is a set of entries, where \neach entry relates a program point in the original program with a corresponding program point in the \ntransformed program. Each correlation relation entry also has a predicate that indicates how the state \nin the original program is related to the state of the transformed program at that point. A bisimulation \nrelation is sim\u00adply a correlation relation that satis.es the property that the predi\u00adcate on any entry \nin the relation implies the predicate on all entries reachable from it. The PEC approach works in two \nsteps. In the .rst step we generate a correlation relation. In the second step, we check if the generated \ncorrelation relation has all the properties required to be a bisimulation relation, and if not, we iteratively \nstrengthen it until it does. Figure 7 shows the control .ow graph (CFG) of the original and the transformed \nprograms in our example, along with the correla\u00adtion relation that our approach generates. The entries \nin the relation are labeled A through G, and each entry has a predicate associated with it. These predicates \noperate over the program states s1 and s2 of the original and transformed program. To make the notation \ncleaner, we use some shorthand notation. For example, E1 means eval(s1, E). Using this notation, the \npredicate at edge D states that (1) the two programs states s1 and s2 are equal, (2) I < E holds  A \n(2) the original program executes [S0; assume(I < E)] and (3) the transformed program executes [S0],then \ns1 = s2 . eval (s1, I) < eval(s1, E) will hold after the two statements have executed. In this case, \nthe implication follows immediately from the assume and the fact that S0 produces the same program state \nif started in the same program state. If for some path pair X to Y, the implication does not hold (this \nis not the case in Figure 7), our algorithm would strengthen the condition at X with the weakest precondition \nof the condition at Y. Using such iterative strengthening, our algorithm tries to convert the original \ncorrelation relation into a bisimulation relation. Our algorithm must also prune infeasible paths when \nit performs the checks. For example, when starting at F, it is impossible for the original program to \nstay in the loop it must exit to G. Our checker can determine this from the predicate at F. In particular, \nlet i and e be the original values of I and E at F (in either s1 or s2 since they are equal). The value \ne does not change through the loop as stated by the side conditions. If the original program chooses \nto stay in the loop, the assume(I < E) would give us i +1 <e (where the +1 comes from the increment of \nI and the fact that S2 does not modify I). This would be inconsistent with the assumption from F stating \nthat i = e - 1, and thus the path is pruned. Figure 7. CFGs of running example with the correlation relation \nin s1 and (3) I < E - 1 holds in s2. In this example, our approach would determine that the generated \ncorrelation relation is in fact a bisimulation relation and as a result does not need to be strength\u00adened. \nGenerating a correlation relation. To generate the correlation re\u00adlation, our PEC algorithm .rst adds \nedge A with predicate s1 = s2; this indicates that we can assume the program states are equal at the \ntop of the code. It also adds edge G with predicate s1 = s2; this indicates that we must establish that \nthe program states are equal after the two programs execute. To generate the points in be\u00adtween, our \nalgorithm traverses both programs in parallel from the top entry. Each time a statement is reached like \nS0, S1,and S2, the algorithm .nds the corresponding point in the other program, and adds a relation entry \nbetween the two statements with the predi\u00adcate s1 = s2 (since this is the only mechanism we have to preserve \nequivalence of arbitrary statements). Finally, our algorithm for gen\u00aderating a correlation relation strengthens \nthe predicate along each side of a branch with the branch condition, which leads to the var\u00adious conditions \nrelating I and E in Figure 7. This allows entries in the bisimulation relation to encode information \nabout what branch conditions they are under. Strengthening the correlation relation. Once a correlation \nrela\u00adtion is generated, our PEC algorithm checks whether or not it is a bisimulation relation, and if \nnot, iteratively strengthens it. In par\u00adticular, we must show that the predicate at a given entry is \nstrong enough to imply the predicates at all entries reachable from it. To achieve this, our approach \ntraverses the two programs in parallel starting at each correlation relation entry to .nd the next reachable \nentries. Then, for each discovered path between a correlation rela\u00adtion entry X and another entry Y , \nour algorithm asks a theorem prover to show that, if the two programs start executing at X in states \nwhere X s predicate holds, and Y is reached, then Y s pred\u00adicate holds. In our example from Figure 7, \nthe paths that our algorithm would discover between correlation relation entries are as follows: A toB, \nB to C, C toF, C to D, D toE, E to D, E to F, and F to G. As an example, for the B-C path, our tool would \nask a theorem prover to show that, for any s1 and s2:if(1) s1 = s2 holds and  2.3 Bene.ts or our approach \nOur PEC approach has several bene.ts over previous techniques for proving optimizations correct in a \nfully automated way. First, PEC can prove the correctness of complex rewrite rules once and for all, \nsomething that previous systems like Rhodium were not able to do. This allows us to implement and prove \nmany more optimizations correct, as discussed in Section 7, thus improving the state of the art of the \nset of optimizations that can be proved automatically once and for all. Second, because parameterized \nprograms can contain concrete statements, PEC can in fact prove fully concrete programs equiv\u00adalent (with \nno side conditions provided). In this setting, our PEC technique subsumes many previous approaches to \ntranslation val\u00adidation, for example the relation approach of Necula [19] and the permute approach of \nZuck et al. [6]. Finally, PEC enables a new staged paradigm for optimization veri.cation: we can use \nPEC to check as many of the optimizations as possible before they are run. For those optimizations that \nwe can t prove correct once and for all, we can use PEC again when the compiler is running to perform \ntranslation validation on the concrete input/output pairs.  3. Parameterized Equivalence Checking We \nnow describe our approach in more detail. Our goal is to show that two parameterized programs P1 and \nP2 are equivalent under side conditions f. We represent each program P as a Control Flow Graph (CFG), \nwhich we denote by p. In particular, we assume that p1 is the CFG of the original program, and p2 is \nthe CFG of the transformed program. Each node in a CFG is a program location l, and edges between program \nlocations are labeled with statements. We use .1 and .2 to denote the entry locations of p1, p2,and E1, \nE2 to represent the exit locations of p1, p2. Given a program state s, we use the notation p(s) to represent \nthe program state after executing p starting in state s. DEFINITION 1 (Equivalence). Given two programs \np1 and p2,we de.ne p1 to be equivalent to p2 if for any program state s, we have p1(s)= p2(s). The above \nde.nition of equivalence allows us to use our optimiza\u00adtions anywhere inside in a program: by establishing \nprogram state equivalence, we guarantee that the remainder of the program, after the optimization, runs \nthe same way in the original and the trans\u00adformed programs. We can model observable events such as IO \nus\u00ading heap updates. For example, a call to printf can just append its arguments to a linked list on \nthe heap. In this setting, our approach guarantees that the order of IO events is preserved.  The statements \nin our CFGs are taken from a concrete program\u00adming language of statements, extended with meta-variables. \nThe main components of our approach do not depend on the choice of the concrete language of statements \nthat we start with: this lan\u00adguage can for example include pointers, arrays, and function calls. We do \nhowever make one exception to this rule: we assume the ex\u00adistence of assume statements. In particular, \nwe model conditionals using assume statements on the edges that .ow away from a branch location (as shown \nfor example in Figure 7). We also use assume statements to insert the information from side conditions \ninto the original or transformed program as needed, so that our tool can reason about the side conditions. \nThe choice of concrete language only affects the semantics of statements, which is entirely modu\u00adlarized \nin a function called step (which we have already seen). The only part of the system that knows about \nstep is the theorem prover, which is given background axioms about the semantics of instruc\u00adtions (so \nthat it knows for example how I++ updates the store). All other parts of the system treat step as a black \nbox. Bisimulation relations. Our approach is based on using a bisim\u00adulation relation to relate the execution \nof the original program and the transformed program. Before de.ning what a bisimulation rela\u00adtion is, \nwe .rst de.ne a more basic concept, which is a correlation relation R. A correlation relation R is a \nset of triples of the form (l1,l2,.),where l1 is a location in p1, l2 is a location in p2,and . is a \nformula relating the program states at l1 and l2. In partic\u00adular, . is a formula over s1 and s2, the \nstates in the original and transformed program respectively. Since . is a predicate with free variables \ns1 and s2, we can use . as a function from two program states to booleans. We use the notation l .R to \nmean (l, , ) .R or ( ,l, ) .R,where is a wildcard that pattern matches anything. If l .R, we say that \nl occurs in R. We de.ne a relation -.R which is the successor relation on CFGs, except that it skips \nlocations not in R.Inorder to de.ne just one relation -.R for both CFGs p1 and p2,we assume without loss \nof generality that the two CFGs p1 and p2 have disjoint locations. With this assumption, -.R is de.ned \nas: p l -.R l' holds iff l .R and l' .R and there is path p in the CFG of p1 or p2 from l to l' such \nthat none of the locations on the path, except end points, occur in R. Simulation and bisimulation relations \nare correlation relations with some additional properties. Below we de.ne these relations by adapting \nprevious work [17] to the setting of CFGs. DEFINITION 2 (Simulation Relation). A correlation relation \nR is a simulation relation for p1,p2 iff it satis.es the following proper\u00adties: 1. (.1,.2,s1 = s2) .R \nand (E1,E2,s1 = s2) .R p1 2. for any l1,l1',l2,p1,.,if (l1,l2,.) .R and l1 -.R l1'then p2 there exists \nl2',.',p2 such that (l1',l2',.') .R and l2 -.R l2'and .s1,s2 ..(s1,s2) . .'(step(s1,p1), step(s2,p2)). \nDEFINITION 3 (Bisimulation Relation). A correlation relation R is a bisimulation relation for p1,p2 iff \nR is a simulation relation for p1,p2 and R-1 is a simulation relation for p2,p1,where R-1 is de.ned by \n(l1,l2,.) .R iff (l2,l1,.) .R-1 . THEOREM 1 (Bisimulation Equivalence). If there exists a bisimu\u00adlation \nrelation between p1 and p2 then p1 and p2 are equivalent. function PEC(p1,p2,f) let (p1',p2'):= Permute(p1,p2,f) \nlet R := Correlate(p1',p2') return Check(R,p1',p2',f) Figure 8. Parameterized Equivalence Checking The \nconditions from De.nition 2 are the base case and the in\u00adductive cases of a proof by induction showing \nthat p1 is equivalent to p2. Thus, a bisimulation relation is a witness that two CFGs are equivalent. \nOur approach is based on the above theorem. In partic\u00adular, our general approach is to try to infer a \nbisimulation relation to show that p1 and p2 are equivalent. Architectural overview. Figure 8 shows the \npseudo-code of our PEC approach. There are three steps: the Permute module, the Cor\u00adrelate module and \nthe Checker module. The Permute module runs as a pre-processor before we use our main bisimulation-based \nap\u00adproach. The Permute module applies a general form of the Permute theorem that has been used in translation \nvalidation of loop opti\u00admizations [6], but it does so on parameterized programs. After the Permute module \nhas run, the Correlate and Checker module imple\u00adment our bisimulation approach. In particular, the Correlate \nmod\u00adule .rst generates a correlation relation R from the two CFGs p1 and p2. The Checker module then \nmakes sure that the properties from De.nitions 2 and 3 hold, possibly strengthening the relation in order \nto guarantee property 2. The next three sections of the pa\u00adper describe each of the modules in our system. \nWe .rst describe the Correlate and Checker modules, which are at the heart of our approach, and then \nmove on the Permute module, which acts as a preprocessing step.  4. Correlate module To prove that two \nparameterized programs are equivalent our ap\u00adproach attempts to discover a bisimulation relation between \nthem. To do this, the Correlate module computes a correlation relation, which will then be strengthened \nto a bisimulation relation by the Checker module. Two kinds of locations in p1 and p2 are particularly \nimportant while constructing the correlation relation R: locations that imme\u00addiately precede a statement \nmeta-variable, the set of which we de\u00adnote LS , and locations that immediately precede an assume,the \nset of which we denote LA.We de.ne the .S and .A relations to be the successor relation in the CFG, but \nskipping over nodes that are not in LS or LA, respectively. We assume that the two CFGs p1 and p2 have \ndisjoint locations, which means we can use a sin\u00adgle version of LS that applies to both CFGs (and similarly \nfor LA). More precisely: (1) l .S l' holds iff l' . LS and there exists a path from l to l' in p1 or \np2 with no intermediate locations in LS , p and (2) l .A l' holds iff p is a path from l to l' in p1 \nor p2 that has no intermediate locations in LA and l . LA .{.1,.2}. Using these de.nitions, the correlation \nrelation that we compute is the smallest relation R such that: R(.1,.2,s1 = s2) .R(E1,E2,s1 = s2) (1) \n.l1,l2,l1' ,l2 ' . 01 R(l1,l2, ). @ l1 .S l1 ' . l2 .S l2' . A .R(l1' ,l2' , Cond(l1' ,l2' )) (l1' ,l2' \n)=( E1,E2) (2) where Cond(l1,l2)= Post(l1) . Post(l2) . s1 = s2 W and Post(l)= {l pSP(p, true) .Al} \n 1. function Check(R,p1,p2,f) 2. let R := R.{(.1,.2,s1 = s2), (E1,E2,s1 = s2)}  3. let (p1' ,p2' ):= \nInsertAssumes(p1,p2,f) 4. let P := ComputePaths(R,p1' ,p2' ) 5. if P = Fail then return Fail 6. let \nC := GenerateConstraints(P) 7. return SolveConstraints(C, R) 8. function ComputePaths(R,p1,p2) 9. \nlet P := \u00d8 10. for each (l1,l2,.) .R  p1 11. for each p1,l1 ' such that l1 -.R l1 ' do p2 12. for each \np2,l2 ' such that l2 -.R l2 ' do 13. if \u00acInfeasible(p1,p2,.) 14. if (l1' ,l2' ,  ) .R then return \nFail 15. P := P.{(l1,l2,p1,p2,l1' ,l2' )} 16. return P 17. function Infeasible(p1,p2,.) 18. return ATP(\u00ac(SP(p1,.) \n. SP(p2,.))) = Valid 19. function GenerateConstraints(P) 20. let C := \u00d8 21. for each (l1,l2,p1,p2,l1' \n,l2' ) .P do 22. C := C.{X(l1,l2) . PWP(p1||p2, X(l1,l2))}  23. return C 24. function SolveConstraints(C, \nR) 25. let soln := map from constraint vars to formulas 26. for each (l, l ' ,.) .R do soln(X(l,l )):= \n. 27. let worklist := C 28. while worklist not empty do 29. let [Xx . PWP(p1||p2, Xy)] := worklist.remove \n 30. let F := [soln(Xx) . PWP(p1||p2, soln(Xy))] 31. if ATP(F )= Valid then 32. if x =(.1,.2) then \nreturn Fail 33. soln(Xx):= soln(Xx) . PWP(p1||p2, soln(Xy))  34. worklist := worklist . . PWP( , Xx)]} \n35. {c .C| c =[ 36. return Success  Figure 9. Pseudo-code for the Checker module Here Cond(l1,l2) computes \nthe formula over s1 and s2 that should hold when p1 and p2 are at locations l1 and l2 respec\u00adtively. \nWithin Cond, the predicate Post(l) is the disjunction of the strongest post conditions with respect to \ntrue over paths p for p which there exists some l ' such that l ' .A l. The Correlate function from Figure \n8, which is the core of the Correlate module, computes the correlation relation using the above de.nition. \nIn particular, it starts with an empty relation, and .rst applies Formula (1) to correlate the entry \nand exit nodes. Then it iteratively applies Formula (2) until no more entries can be added. 5. Checker \nmodule The pseudo-code for the Checker module is shown in Figure 9. The checker performs the following \n.ve steps: .rst, it makes sure that the entry and exit locations are related with full state equal\u00adity \n(line 2); then it inserts assume statements into the original and transformed programs corresponding \nto the side conditions that are given in the rewrite rule (line 3); then it computes the paths be\u00adtween \nentries in the correlation relation, doing path pruning any\u00adwhere possible (line 4); using the computed \npaths it generates a set of constraints that the .nal correlation relation must satisfy to be a bisimulation \nrelation (line 6); .nally it solves the generated con\u00adstraints using a .xed point computation (line 7). \nWe describe each of these steps in more detail. InsertAssumes. The InsertAssumes function inserts the \nside con\u00addition assumptions into the original and transformed programs in the form of assume statements. \nAn assume statement takes as ar\u00adgument a predicate over the program state s that occurs at the point \nwhere the assume holds. To ease presentation, we make the simpli\u00adfying assumption that f = f1@L1 . ... \n. fn@Ln (our implemen\u00adtation handles the general case). For each side condition fi,we de.ne [fi] to beapredicateover \ns that directly encodes the side condition s meaning provided by the optimization writer. Then for each \nfi@Li, we .nd the location Li in either the original or the transformed program, and insert assume([fi]) \nat that location. ComputePaths. The ComputePaths function computes the set of paths P between entries \nof the correlation relation R. The function starts by initializing the set of paths to the empty set \n(line 9). Then, for each correlation relation entry (l1,l2,.) .R, it .nds the reachable program points \nl1 ' and l2 ' in each program that are in the pl ' correlation relation (lines 10-12). It does so using \nthe l -.R relation introduced in Section 3, which states that l occurs in R, l ' occurs in R, and there \nis a CFG path p from l to l ' where none of the locations in the path, except for the end points, occur \nin R. For each pair of paths p1, p2 that are found, ComputePaths checks if the paths are infeasible by \ncalling the Infeasible function. Infeasible .rst computes the strongest postcondition of p1 and p2. If \nan automated theorem prover (ATP) can show that the two post-conditions are inconsistent, then the combination \nof those two paths is infeasible, and can be pruned. The Infeasible function performs the pruning that \nwas intuitively described for the software pipelining example in Section 2.2. If the paths are feasible \nand an entry (l1' ,l2' , ) exists in the correlation relation, then the two paths are added to P, along \nwith the beginning and end points (line 15). GenerateConstraints. Once the set of paths in the correlation \nrelation have been collected, the GenerateConstraints function computes the set of constraints C that \nour correlation relation must satisfy to be a bisimulation relation. For each (l, l ' , ) .R, we de.ne \na constraint variable X(l,l ) that represents the formula in the correlation relation relating l and \nl ' . Then, for each path between two entries in the correlation relation (line 21), we add a constraint \nstating that the predicate at the beginning of the path must imply the predicate at the end of the path \n(line 22). We express this condition using the weakest precondition computation PWP, which is a parallel \nversion of the regular weakest precondition. The main challenge in expressing this weakest precondition \nis that the traditional formulation of weakest precondition depends on the structure of the statements \nbeing processed. As a result, it is dif.cult to use this de.nition for statements like S0 and S1 in our \nparameterized programs, because the precise structure of these statements is not known. To address this \nchallenge, we use an alter\u00adnate yet equivalent de.nition of weakest precondition. In particu\u00adlar, consider \nthe traditional weakest precondition computation, and assume that the predicate we are computing is a \nfunction from pro\u00adgram states to booleans. Then the traditional weakest precondition WP can be expressed \nas: WP(S,.)(s)= .(step(s, S)) If we assume that the program state s is simply a free variable in the \npredicate .,then WP can be expressed as: WP(S,.)= .[s . step(s, S)]   Generalizing this to two parallels \npaths in two different pro\u00adgrams, the predicates now have free variables s1 and s2,and we can express \nPWP as follows: PWP(p1||p2,.)= .[s1 . step(s1,p1),s2 . step(s2,p2)] SolveConstraints. Once the set of \nconstraints have been generated, the SolveConstraints function tries to solve these constraints iter\u00adatively \nby starting with the correlation relation R, and iteratively strengthening the conditions in the relation \nuntil all the constraints are satis.ed. In particular, SolveConstraints maintains a map soln that maps \neach constraint variable to the formula we currently as\u00adsociate the variable with. The soln map is initialized \nwith the predi\u00adcates from the correlation relation (line 26). SolveConstraints also maintains a worklist \nof constraints to be processed, which is ini\u00adtialized with all the constraints (line 27). While the worklist \nis not empty, SolveConstraints removes a constraint from the worklist (line 29), and if the constraint \nis not satis.ed (line 31), it strength\u00adens the left-hand side of the implication in the currently stored \nsolu\u00adtion (line 33), and adds to the worklist all the constraints that need to be checked again because \nof the strengthening (line 35). One subtlety is that we cannot strengthen the relation at the entry points \n.1, .2. If we ever try to do this, we indicate a failure (line 32). Be\u00adcause SolveConstraints is trying \nto compute a .xedpoint over the very .exible but in.nite domain of boolean formulas, it may not terminate. \nHowever, as our experiments show in Section 7, in prac\u00adtice SolveConstraints quickly .nds a .xed point. \n6. Permute module Our main technique for PEC relies on a bisimulation approach to prove equivalence. \nHowever, the bisimulation approach has some known limitations. In particular, bisimulation relations \nare not well suited for proving the correctness of non-structure preserving trans\u00adformations, which are \ntransformations that change the execution order of code across loop iterations. Previous work on translation \nvalidation has devised a technique called Permute [32] for han\u00addling such transformations on concrete \nprograms. We have adapted this technique to the setting of parameterized programs. Our version of Permute \nruns as a pre-pass to our bisimulation relation approach: Permute looks for loops in the original and \ntransformed programs that it can prove equivalent, and for the ones it can, it replaces them with a new \nfresh variable S, which will then allow the bisimulation relation part of our PEC approach to see that \nthey are equivalent. Our Permute algorithm tries to .nd a general nested loop of the following form, \nwhere we use .L to denote a total order on L for i1 . I1 by .I1 do . . . for in . In by .In do B(i1, \n\u00b7\u00b7\u00b7 ,in); where Ij is the domain of the index variable ij The relation .Ij represents the order in which \nthe index variable ij is traversed. The above general nested loop can be represented more compactly as \nfollows: for ii . Iiby .I[do B(ii); where Ii= I1 \u00d7\u00b7\u00b7\u00b7\u00d7 In and n _ ii .I[ij .. (i1, \u00b7\u00b7\u00b7 ,ik-1)=(j1, \u00b7\u00b7\u00b7 \n,jk-1) . ik .Ik jk k=1 The relation .I[above is the lexicographic order on Ii. Our algorithm tries to \n.nd a loop structure as above in the orig\u00adinal program and in the transformed program, and for each such \npair, it tries to show that the following loop reordering transforma\u00adtion is correct: for ii1 . Ii1 by \n.I[1 do B(ii1) . (3) for ii2 . Ii2 by .I[2 do B(F (ii2)) The above transformation may change the order \nof the index variables by changing the domain Ii1 to Ii2 and the relation .I[1 to .I[2 and also possibly \nchanging the loop s body by applying a linear transformation from B(ii1) to B(F (ii2)). To show that \nthe above transformation is correct, we need to ensure that the transformed loop executes the same instances \nof the loop body in an order that preserves the body s behavior. In order to de.ne the conditions under \nwhich this happens, we .rst de.ne when two program fragments commute. DEFINITION 4 (Commute). We say \ntwo program fragments S1 and S2 commute, written S1 S2, if starting from an arbitrary initial state, \nthe resultant state of executing S1 and then S2 is the same as executing S2 and then S1. We can now guarantee \nthat the original and transformed loops are equivalent by requiring the following properties to hold: \n1. There exists a 1-1 correspondence between Ii1 and Ii2. 2. For every ii1, ii2 . Ii1,if B(ii1) executes \nbefore B(ii2) in the original program and B(ii2) executes before B(ii1) in the trans\u00adformed program then \nB(ii1) and B(ii2) commute, i.e. B(ii1) B(ii2)  The .rst property above can be established by showing \nthat the ii linear function F : I2 -. I1 is a bijective function, i.e. F is one-to-one and onto. This \nin turn can be guaranteed by de.ning an inverse function F -1 : Ii1 -. Ii2. The above observations are \nsummarized in the following Permute Theorem. THEOREM 2(Permute). A loop reording transformation of the \nform shown in Formula (3) preserves semantics if the following hold: 1. .ii2 . Ii2.F (ii2) . Ii1 2. \n.ii1 . iF -1(ii1) . i I1.I2 3. .ii2 . Ii2. ii2 = F -1(F (ii2)) i1 . ii1 4. .iI1.i= F (F -1(ii1)) \n 5. .ii1, ii1 ' . Ii1. ii1 .[ii1 ' . F -1(ii1' ) .[F -1(ii1)  I1 I2 =. B(ii1) B(ii1' ) Theorem 2 was \nintroduced and proved in previous work [32, 21, 23]. The Permute module tries to apply Theorem 2 by asking \nan automated theorem prover to discharge the preconditions of the theorem assuming the side conditions \ngiven in the transformation. As an example, consider the simple loop interchange optimization shown in \nFigure 10. For clarity and ease of explanation, the ex\u00adample is simpli.ed here to have constant bounds \n(L1,U1,L2,U2) instead of arbitrary expressions. However, our tool checks the more general version of \nthis example.  2 3 for (I := L1; I = U1; I++ ) { 6 for (J := L2; J = U2; J++) { 7 67 6 L1 : S[ I, J \n] 7 45 } } . 2 3 for (J := L2; J = U2; J++) { 6 for (I := L1; I = U1; I++) { 7 6 7 6 S[ I, J ] 7 45 \n} } where . K, L . (K = I . L = J) . 01 DoesNotModify(S[ I, J ], S[ K, L ])@L1 . @A DoesNotModify(S[ \nK, L ], S[ I, J ], )@L1 . DoesNotInterfere(S[ I, J ], S[ K, L ])@L1 fact DoesNotModify(S1, S2) has meaning \nstep(s, S2)|s = step(step(s, S1), S2)|s S2 S2 fact DoesNotInterfere(S1, S2) has meaning step(s, S2)|s \n= step(step(s, S2), S1)|s S2 S2 Figure 10. Loop Interchange The Permute module .rst transforms the original \nand trans\u00adformed programs into our canonical representations of loops. In particular, the original program \nis summarized as i I1 = {(i, j) | i . [L1, U1],j . [L2, U2]} and B((i, j)) = S[ i, j ] and .I[1 is the \nlexicographic order on Ii1 and the transformed program is represented as i I2 = {(i, j) | i . [L2, U2],j \n. [L1, U1]} and B((i, j)) = S[ j, i ] and .I[2 is the lexicographic order on Ii2 Since there is one loop \nin the original program and one in the transformed program, Permute tries to prove them equivalent. In \norder to apply the Permute Theorem, our tool needs to infer the two mapping functions F and F -1, and \nprove properties 1 through 4 of Theorem 2. Permute infers these functions automatically using a simple \nheuristic that runs a range analysis over the original and transformed programs, and uses the results \nof the upper and lower bounds on index variables to infer F and F -1 . For our loop interchange optimization, \nour tool automatically infers that the two functions are: F ((i, j)) = (j, i),and F -1((i, j)) = (j, \ni). Our heuristic infers the appropriate mapping functions in all the optimizations that we have tried \n(see Section 7). However, we also provide the ability for the programmer to provide F and F -1 in the \ncase where our heuristic cannot .nd appropriate functions. The purpose of the side conditions of loop \ninterchange is to al\u00adlow the theorem prover to show property 5 of Theorem 2. One op\u00adtion for expressing \nthe side condition is to use the Commute fact from Figure 4, which gives us a predicate very close to \nproperty 5 directly, and then use a heavyweight analysis when the com\u00adpiler runs to establish Commute \n(for example a theorem prover, the Omega test [21], or more generally dependence analysis [18]). An\u00adother \noption, which we use in Figure 10 to illustrate the .exibility of our approach, is to use a more syntactic \nde.nition of commu\u00adtativity, using two new facts: DoesNotModify, which holds when a statement does not \nmodify the variables or heap locations that an-other may read, and DoesNotInterfere, which holds when \na state\u00adment does not modify the variables or heap locations that another may write to. The notation \ns1|s2 represents the state s1 projected Optimizations Uses permute Time (secs) #ATP calls Category 1 \nCopy propagation No 1 3 Constant propagation No 1 3 Common sub-expression elim No 1 3 Partial redundancy \nelimination No 3 13 Category 2 Loop invariant code hoisting No 8 25 Conditional speculation No 2 14 Speculation \nNo 3 12 Category 3 Software pipelining No 5 19 Loop unswitching No 16 94 Loop unrolling No 10 45 Loop \npeeling No 6 40 Loop splitting No 15 64 Loop alignment Yes 1 5 Loop interchange Yes 1 5 Loop reversal \nYes 1 5 Loop skewing Yes 2 5 Loop fusion Yes 4 10 Loop distribution Yes 4 10 Figure 11. Optimizations \nproven correct using PEC. Category 1: expressible and provable in Rhodium; Category 2: provable in Rhodium, \nbut our version is more general and easier to express; Category 3: not expressible or provable in Rhodium. \n S onto the variables and heap locations that S modi.es if it exe\u00adcutes starting in state s2. The bene.t \nof using the more syntactic DoesNotModify and DoesNotInterfere facts is that they can more easily be \nimplemented using simple Rhodium data.ow functions, which in turn can be proved correct automatically. \nIn this way we will know that the computed facts when the compiler runs imply the semantic meanings that \nour PEC technique assumed when proving the correctness of loop interchange once and for all. 7. Evaluation \nWe implemented PEC in 2,408 lines of OCaml using the Simplify theorem prover [5] to realize the ATP module \nfrom Figure 9. Figure 11 lists a selection of optimizations that we proved cor\u00adrect using our implementation. \nFor each optimization we list the time it took to carry out PEC and the number of queries to the the\u00adorem \nprover. To be clear about our contribution compared to the Rhodium system for automatically proving optimizations \ncorrect, Figure 11 partitions the optimizations into three categories. Category 1 : Optimizations that \nwere also expressed and proved correct in Rhodium, and whose PEC formulation is equivalent to the Rhodium \nformulation. Category 2 : Optimizations that could have been expressed and proved correct in Rhodium, \nbut our versions are much more gen\u00aderal than the Rhodium version, and also much easier to express. For \nexample, in the case of loop invariant code hoisting, PEC can prove the correctness of hoisting loop-invariant \nbranches or even entire loops, while the Rhodium version could only hoist loop-invariant assignments. \nFurthermore, these optimizations are much easier to express in our PEC formulation because of our explicit \nsupport for many-to-many rewrites. In contrast, implementing these opti\u00admizations in Rhodium would require \nan expert to carefully craft sequences of local statement rewrites that achieves the intended ef\u00adfect. \nFor example, moving a statement in Rhodium requires insert\u00ading a duplicate copy of the statement at the \ntarget location, and then removing the original statement in a separate pass.  Category 3 : Optimizations \nthat cannot be proved correct, or even expressed, in Rhodium. Our support for many-to-many rewrite rules \nmakes it easy to express these optimizations, and PEC tech\u00adnique is general enough to handle their correctness \nproofs. The trusted computing base for our system includes: (1) the PEC checker, comprising 2,408 lines \nof OCaml code (2) the Sim\u00adplify automated theorem prover, a widely used and well tested the\u00adorem prover, \nand (3) the execution engine that will run the opti\u00admizations. Within the execution engine, the trust \ncan be further subdivided into two components. The .rst component of the ex\u00adecution engine must perform \nthe syntactic pattern matching for rewrite rules, and apply rewrite rules when they .re. This part is \nalways trusted. The second component of the execution engine must perform program analyses to check each \noptimization s side\u00adconditions in a way that guarantees their semantic meaning. Here our system offers \na choice. These analyses can either be trusted and thus implemented inside the compiler using arbitrarily \ncom\u00adplex analyses, or untrusted and implemented using a provably safe analysis system like Rhodium. 8. \nExecution Engine We implemented a prototype execution engine in 383 lines of OCaml code that runs optimizations \nchecked by PEC. Although PEC can be applied to any intermediate representation for which we can compute \nweakest preconditions, our prototype execution engine transforms programs written in a C-like intermediate \nlan\u00adguage including arrays and function calls. Using this prototype, we were able to run all the optimizations \ndescribed in previous sec\u00adtions. Even though our execution engine is a prototype, it demon\u00adstrates how \nour optimizations can be incorporated into a compiler, and also shows that the optimizations we checked \nexecute as ex\u00adpected. Our execution engine is embodied in a function called Apply, which takes as input \na program p, a transformation rule [P1 . P2 where f], and a pro.tability heuristic ., and returns a trans\u00adformed \nprogram. The Apply function .rst uses pattern matching to .nd all locations in the program p where the \npattern P1 occurs. Then for each match that is found, Apply evaluates the side condi\u00adtion f to make sure \nthat the match is valid. Our current prototype checks side conditions conservatively using read/write \nsets. For ex\u00adample, to guarantee that a statement s1 does not modify another statement s2, we check that \nWriteSet(s1) n ReadSet(s2)= \u00d8. For each match that is found where the side condition holds, Apply builds \na substitution . that records information about the match: . maps the free variables in P1 to concrete \nfragments of p, and it also records the location where the match occurred in p. Apply collects the resulting \nsubstitutions . into a set T,and then it calls the pro.tability heuristic . with T as a parameter. The \nrole of the pro.tability heuristic . is to select from the set T of all substitutions that have been \nfound (representing all the possi\u00adble applications of the transformation rule) those substitutions that \nit wants to apply. Because all the substitutions in T represent cor\u00adrect transformations, it does not \nmatter which subset the pro.tabil\u00adity heuristic chooses, and so the pro.tability heuristic can perform \narbitrary computation without being trusted. The above approach to pro.tability heuristic uses the generate-and-test \napproach pre\u00adsented in the Cobalt system [14]. Alternatively, an execution en\u00adgine could also employ \nthe more demand-driven approach used in function SwPipe(p):= let p ' := Apply(p, t1,.sw) '' ' if (p = \np) then p else SwPipe(Apply(p , t2,.x.x)) Figure 12. Implementation of Software Pipelining using Apply. \nthe Rhodium system [15], where side conditions directly refer to pro.tability facts, thus constraining \nwhich matches are explored. Once the pro.tability heuristic has selected the set of substi\u00adtutions it \nwants to apply, the Apply function performs the corre\u00adsponding transformations. If the pro.tability heuristic \nreturns sub\u00adstitutions that overlap in the program fragments they match, then the Apply function picks \nan order to apply the substitutions in, and only applies a substitution . if no previously applied substitution \nhas transformed elements mentioned in .. As an example, Figure 12 shows a function SwPipe that uses Apply \nto perform software pipelining. We use t1 to represent the .rst part of software pipelining (the transformation \nfrom Figure 2), and t2 to represent the second part (the transformation from Fig\u00adure 3). The SwPipe function \nuses Apply to repeatedly apply t1 and t2. The software pipelining pro.tability heuristic .sw is applied \naf\u00adter t1 has run. In our prototype, we have chosen to implement .sw by selecting matches that reduce \nthe number of dependencies be\u00adtween instructions in loop bodies. Once t1 has .red, we need to apply t2 \non the result before doing another iteration of software pipelining. As a result, the pro.tability heuristic \nfor t2 is the iden\u00adtity function, which simply selects all the matches. 9. Related work Translation Validation. \nOur approach is heavily inspired by the work that has been done on translation validation [20, 19, 22, \n6, 11, 12]. However, unlike previous translation validation approaches, our equivalence checking algorithm \naddresses the challenge of rea\u00adsoning about statements that are not fully speci.ed. As a result, our \napproach is a generalization of previous translation validation tech\u00adniques that allows optimizations \nto be proved correct once and for all. Furthermore, because our PEC approach can handle concrete statements \nas well as parameterized statements, it subsumes many of the previous approaches to translation validation, \nfor example the relation approach of Necula [19] and the permute approach of Zuck et al. [6, 32]. Proving \nloop optimizations correct. Our approach to reasoning about loop reordering transformations by having \na single canonical representation for all these transformations is similar to the trans\u00adlation validation \nwork of Zuck et al. [6] and the legality check ap\u00adproach of Kelly et al. [9]. However, both these approaches \nperform runtime validation of concrete programs instead of once and for all reasoning about parameterized \nprograms. Automated correctness checking of optimizations. As with our PEC algorithm, the Cobalt [14] \nand Rhodium [15] systems are able to check the correctness of optimizations once and for all. However, \nCobalt and Rhodium only support rewrite rules that transform a sin\u00adgle statement to another statement, \nthus limiting the kinds of opti\u00admizations they can express and prove correct. Our PEC approach can handle \ncomplex many-to-many rewrite rules explicitly, allow\u00ading it to prove many more optimizations correct. \nHuman-assisted correctness checking of optimizations. Asig\u00adni.cant amount of work has been done on manually \nproving opti\u00admizations correct, including abstract interpretation [3, 4], the work on the VLISP compiler \n[7], Kleene algebra with tests [10], man\u00adual proofs of correctness for optimizations expressed in temporal \nlogic [25, 13], and manual proofs of correctness based on par\u00adtial equivalence relations [1]. Analyses \nand transformations have also been proven correct mechanically, but not automatically: the soundness \nproof is performed with an interactive theorem prover that requires guidance from the user. For example, \nYoung [31] has proven a code generator correct using the Boyer-Moore theorem prover enhanced with an \ninteractive interface [8]. As another ex\u00adample, Cachera et al. [2] show how to specify static analyses \nand prove them correct in constructive logic using the Coq proof as\u00adsistant. Via the Curry-Howard isomorphism, \nan implementation of the static analysis algorithm can then be extracted from the proof of correctness. \nLeroy s Comcert project [16] has also used a similar technique to manually develop a semantics preserving, \noptimizing compiler for a large subset of C. The Comcert compiler provides an end-to-end correctness \nguarantee, and does not just focus on optimizations, as we do in our approach. Tristan et al. has also \nproved that certain translation validators are correct once and for all, but here again by implementing \nthe proof manually [27, 28]. In all these cases, however, the proof requires help from the user. In contrast \nto these approaches, our proof strategy is fully automated but trusts that the side conditions are computed \ncorrectly when the compiler executes.  Languages for expressing optimizations. The idea of analyz\u00ading \noptimizations written in a specialized language was intro\u00adduced by Whit.eld and Soffa with the Gospel \nlanguage [29]. Many other frameworks and languages have been proposed for specify\u00ading data.ow analyses \nand transformations, including Sharlit [26], System-Z [30], languages based on regular path queries [24], \nand temporal logic [25, 13]. None of these approaches addresses auto\u00admated correctness checking of the \nspeci.ed optimizations. 10. Conclusion We developed and implemented Parameterized Equivalence Checking \n(PEC), a technique for automatically proving optimiza\u00adtions correct once and for all. PEC works by proving \ntransforma\u00adtions correct on parameterized programs, thus generalizing previ\u00adous translation validation \ntechniques and adapting them to provide once and for all correctness proofs. Furthermore, our use of \nex\u00adpressive many-to-many rewrite rules and a robust proof technique enables PEC to automatically prove \ncorrect optimizations that have been dif.cult or impossible to prove in previous systems. References \n[1] Nick Benton. Simple relational correctness proofs for static analyses and and program transformations. \nIn POPL, 2004. [2] David Cachera, Thomas Jensen, David Pichardie, and Vlad Rusu. Extracting a data .ow \nanalyser in constructive logic. In ESOP, 2004. [3] Patrick Cousot and Radhia Cousot. Abstract interpretation: \nA uni.ed lattice model for static analysis of programs by construction or approximation of .xpoints. \nIn POPL, 1977. [4] Patrick Cousot and Radhia Cousot. Systematic design of program transformation frameworks \nby abstract interpretation. In POPL, 2002. [5] D. Detlefs, G. Nelson, and J. Saxe. Simplify: A theorem \nprover for program checking. Journal of the Association for Computing Machinery, 52(3):365 473, May 2005. \n[6] Benjamin Goldberg, Lenore Zuck, and Clark Barrett. Into the loops: Practical issues in translation \nvalidation for optimizing compilers. Electronic Notes in Theoretical Computer Science, 132(1):53 71, \nMay 2005. [7] J. Guttman, J. Ramsdell, and M. Wand. VLISP: a veri.ed implementation of Scheme. Lisp and \nSymbolic Compucation,8(1\u00ad2):33 110, 1995. [8] M. Kauffmann and R.S. Boyer. The Boyer-Moore theorem prover \nand its interactive enhancement. Computers and Mathematics with Applications, 29(2):27 62, 1995. [9] \nWayne Kelly and William Pugh. Finding legal reordering transfor\u00admations using mappings. In Languages \nand Compilers for Parallel Computing, 1994. [10] Dexter Kozen. Kleene algebra with tests. ACM Transactions \non Programming Langauges and Systems, 19(3):427 443, 1997. [11] Sudipta Kundu, Sorin Lerner, and Rajesh \nGupta. Automated re.nement checking of concurrent systems. In ICCAD, 2007. [12] Sudipta Kundu, Sorin \nLerner, and Rajesh Gupta. Validating high-level synthesis. In Computer Aided Ve..cation (CAV), 2008. \n[13] David Lacey, Neil D. Jones, Eric Van Wyk, and Carl Christian Frederiksen. Proving correctness of \ncompiler optimizations by temporal logic. In POPL, 2002. [14] Sorin Lerner, Todd Millstein, and Craig \nChambers. Automatically proving the correctness of compiler optimizations. In PLDI, 2003. [15] Sorin \nLerner, Todd Millstein, Erika Rice, and Craig Chambers. Au\u00adtomated soundness proofs for data.ow analyses \nand transformations via local rules. In POPL, 2005. [16] Xavier Leroy. Formal certi.cation of a compiler \nback-end or: programming a compiler with a proof assistant. In POPL, 2006. [17] R. Milner. Communication \nand concurrency. Prentice-Hall, Inc., Upper Saddle River, NJ, USA, 1989. [18] S. Muchnick. Advanced Compiler \nDesign And Implementation. Morgan Kaufmann Publishers, 1997. [19] G. Necula. Translation validation for \nan optimizing compiler. In PLDI, June 2000. [20] A. Pnueli, M. Siegel, and E. Singerman. Translation \nvalidation. In TACAS, 1998. [21] William Pugh. The omega test: a fast and practical integer programming \nalgorithm for dependence analysis. Communications of the ACM, 8:4 13, 1992. [22] Martin Rinard and Darko \nMarinov. Credible compilation. In Proceedings of the FLoC Workshop Run-Time Result Veri.cation, July \n1999. [23] Martin C. Rinard and Pedro C. Diniz. Commutativity analysis: a new analysis framework for \nparallelizing compilers. In PLDI, 1996. [24] Ganesh Sittampalam, Oege de Moor, and Ken Friis Larsen. \nIncre\u00admental execution of transformation speci.cations. In POPL, 2004. [25] Bernhard Steffen. Data .ow \nanalysis as model checking. In Theoretical Aspects of Computer Science, volume 526 of Lecture Notes in \nComputer Science, pages 346 364. Springer-Verlag, September 1991. [26] Steven W. K. Tjiang and John L. \nHennessy. Sharlit a tool for building optimizers. In PLDI, 1992. [27] Jean-Baptiste Tristan and Xavier \nLeroy. Veri.ed validation of lazy code motion. In POPL, 2008. [28] Jean-Baptiste Tristan and Xavier Leroy. \nFormal veri.cation of translation validators: a case study on instruction scheduling optimizations. In \nPLDI, 2009. [29] Deborah L. Whit.eld and Mary Lou Soffa. An approach for exploring code improving transformations. \nACM Transactions on Programming Languages and Systems, 19(6):1053 1084, November 1997. [30] Kwangkeun \nYi and Williams Ludwell Harrison III. Automatic generation and management of interprocedural program \nanalyses. In POPL, 1993. [31] William D. Young. A mechanically veri.ed code generator. Journal of Automated \nReasoning, 5(4):493 518, December 1989. [32] Lenore Zuck, Amir Pnueli, Benjamin Goldberg, Clark Barrett, \nYi Fang, and Ying Hu. Translation and run-time validation of loop transformations. Form. Methods Syst. \nDes., 27(3):335 360, 2005.   \n\t\t\t", "proc_id": "1542476", "abstract": "<p>Translation validation is a technique for checking that, after an optimization has run, the input and output of the optimization are equivalent. Traditionally, translation validation has been used to prove concrete, fully specified programs equivalent. In this paper we present Parameterized Equivalence Checking (PEC), a generalization of translation validation that can prove the equivalence of parameterized programs. A parameterized program is a partially specified program that can represent multiple concrete programs. For example, a parameterized program may contain a section of code whose only known property is that it does not modify certain variables. By proving parameterized programs equivalent, PEC can prove the correctness of transformation rules that represent complex optimizations once and for all, before they are ever run. We implemented our PEC technique in a tool that can establish the equivalence of two parameterized programs. To highlight the power of PEC, we designed a language for implementing complex optimizations using many-to-many rewrite rules, and used this language to implement a variety of optimizations including software pipelining, loop unrolling, loop unswitching, loop interchange, and loop fusion. Finally, to demonstrate the effectiveness of PEC, we used our PEC implementation to verify that all the optimizations we implemented in our language preserve program behavior.</p>", "authors": [{"name": "Sudipta Kundu", "author_profile_id": "81342501288", "affiliation": "University of California, San Diego, San Diego, CA, USA", "person_id": "P1464308", "email_address": "", "orcid_id": ""}, {"name": "Zachary Tatlock", "author_profile_id": "81392605383", "affiliation": "University of California, San Diego, San Diego, CA, USA", "person_id": "P1464309", "email_address": "", "orcid_id": ""}, {"name": "Sorin Lerner", "author_profile_id": "81100399150", "affiliation": "University of California, San Diego, San Diego, CA, USA", "person_id": "P1464310", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1542476.1542513", "year": "2009", "article_id": "1542513", "conference": "PLDI", "title": "Proving optimizations correct using parameterized program equivalence", "url": "http://dl.acm.org/citation.cfm?id=1542513"}