{"article_publication_date": "06-15-2009", "fulltext": "\n Towards a Holistic Approach to Auto-Parallelization Integrating Pro.le-Driven Parallelism Detection \nand Machine-Learning Based Mapping Georgios Tournavitis Zheng Wang Bj\u00a8orn Franke Michael F.P. O Boyle \nInstitute for Computing Systems Architecture (ICSA) School of Informatics University of Edinburgh Scotland, \nUnited Kingdom gtournav@inf.ed.ac.uk,jason.wangz@ed.ac.uk,{bfranke,mob}@inf.ed.ac.uk Abstract Compiler-based \nauto-parallelization is a much studied area, yet has still not found wide-spread application. This is \nlargely due to the poor exploitation of application parallelism, subsequently result\u00ading in performance \nlevels far below those which a skilled expert programmer could achieve. We have identi.ed two weaknesses \nin traditional parallelizing compilers and propose a novel, integrated approach, resulting in signi.cant \nperformance improvements of the generated parallel code. Using pro.le-driven parallelism detection we \novercome the limitations of static analysis, enabling us to iden\u00adtify more application parallelism and \nonly rely on the user for .\u00adnal approval. In addition, we replace the traditional target-speci.c and \nin.exible mapping heuristics with a machine-learning based prediction mechanism, resulting in better \nmapping decisions while providing more scope for adaptation to different target architec\u00adtures. We have \nevaluated our parallelization strategy against the NAS and SPEC OMP benchmarks and two different multi-core \nplatforms (dual quad-core Intel Xeon SMP and dual-socket QS20 Cell blade). We demonstrate that our approach \nnot only yields sig\u00adni.cant improvements when compared with state-of-the-art par\u00adallelizing compilers, \nbut comes close to and sometimes exceeds the performance of manually parallelized codes. On average, \nour methodology achieves 96% of the performance of the hand-tuned OpenMP NAS and SPEC parallel benchmarks \non the Intel Xeon platform and gains a signi.cant speedup for the IBM Cell platform, demonstrating the \npotential of pro.le-guided and machine-learning based parallelization for complex multi-core platforms. \n Categories and Subject Descriptors D.3.4 [Programming Lan\u00adguages]: Processors Compilers; D.1.3 [Programming \nTechniques]: Concurrent Programming Parallel Programming General Terms Experimentation, Languages, Measurement, \nPer\u00adformance Keywords Auto-Parallelization, Pro.le-Driven Parallelism De\u00adtection, Machine-Learning Based \nParallelism Mapping, OpenMP Permission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies are not made or distributed for \npro.t or commercial advantage and that copies bear this notice and the full citation on the .rst page. \nTo copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci.c \npermission and/or a fee. PLDI 09, June 15 20, 2009, Dublin, Ireland. Copyright c . 2009 ACM 978-1-60558-392-1/09/06. \n. . $5.00 1. Introduction Multi-core computing systems are widely seen as the most viable means of delivering \nperformance with increasing transistor densi\u00adties (1). However, this potential cannot be realized unless \nthe ap\u00adplication has been well parallelized. Unfortunately, ef.cient par\u00adallelization of a sequential \nprogram is a challenging and error\u00adprone task. It is generally agreed that manual code paralleliza\u00adtion \nby expert programmers results in the most streamlined parallel implementation, but at the same time this \nis the most costly and time-consuming approach. Parallelizing compiler technology, on the other hand, \nhas the potential to greatly reduce cost and time-to\u00admarket while ensuring formal correctness of the \nresulting parallel code. Automatic parallelism extraction is certainly not a new research area (2). Progress \nwas achieved in 1980s to 1990s on restricted DOALL and DOACROSS loops (3; 4; 5). In fact, this research \nhas resulted in a whole range of parallelizing research compil\u00aders, e.g. Polaris (6), SUIF-1 (7) and, \nmore recently, Open64 (8). Complementary to the on-going work in auto-parallelization many high-level \nparallel programming languages such as Cilk-5 (9), OpenMP, StreamIt (10), UPC (11) and X10 (12) and \nprogram\u00adming models such as Galois (14), STAPL (15) and HTA (16) have been proposed. Interactive parallelization \ntools (17; 18; 19; 20) provide a way to actively involve the programmer in the detec\u00adtion and mapping \nof application parallelism, but still demand great effort from the user. While these approaches make \nparallelism ex\u00adpression easier than in the past, the effort involved in discovering and mapping parallelism \nis still far greater than that of writing an equivalent sequential program. This paper argues that the \nlack of success in auto-parallelization has occurred for two reasons. First, traditional static parallelism \nde\u00adtection techniques are not effective in .nding parallelism due to lack of information in the static \nsource code. Second, no existing integrated approach has successfully brought together automatic parallelism \ndiscovery and portable mapping. Given that the num\u00adber and type of processors of a parallel system is \nlikely to change from one generation to the next, .nding the right mapping for an application may have \nto be repeated many times throughout an ap\u00adplication s lifetime, hence, making automatic approaches attractive. \nApproach. Our approach integrates pro.le-driven parallelism de\u00adtection and machine-learning based mapping \nin a single frame\u00adwork. We use pro.ling data to extract actual control and data de\u00adpendences and enhance \nthe corresponding static analyses with dy\u00adnamic information. Subsequently, we apply a previously trained \n for (i=0; i < nodes ; i ++) {Anext = Aindex[i]; Alast = Aindex [ i + 1]; sum0 = A[Anext][0][0] * v[i][0] \n+ A[Anext ][0][1] * v[i][1] + A[Anext ][0][2] * v[i ][2]; sum1 = ... Anext ++; while ( Anext < Alast \n) { col = Acol[Anext]; sum0 += A[Anext][0][0] * v[col][0] + A[Anext ][0][1] * v[col][1] + A[Anext ][0][2] \n* v[col ][2]; sum1 += ... w[col][0] += A[Anext][0][0] * v[i][0] + A[Anext ][1][0] * v[i][1] + A[Anext \n][2][0] * v[i ][2]; w[col][1] += ... Anext ++; } w[i][0] += sum0; w[i][1] += ... } Figure 1. Static analysis \nis challenged by sparse array reduction operations and the inner while loop in the SPEC equake benchmark. \nmachine-learning based prediction mechanism to each parallel loop candidate and decide if and how the \nparallel mapping should be per\u00adformed. Finally, we generate parallel code using standard OpenMP annotations. \nOur approach is semi-automated, i.e. we only expect the user to .nally approve those loops where parallelization \nis likely to be bene.cial, but correctness cannot be proven conclu\u00adsively. Results. We have evaluated \nour parallelization strategy against the NAS and SPEC OMP benchmarks and two different multi\u00adcore platforms \n(dual quad-core Intel Xeon SMP and dual-socket QS20 Cell blade). We demonstrate that our approach not \nonly yields signi.cant improvements when compared with state-of-the\u00adart parallelizing compilers, but \ncomes close to and sometimes ex\u00adceeds the performance of manually parallelized codes. We show that pro.ling-driven \nanalyses can detect more parallel loops than static techniques. A surprising result is that all loops \nclassi.ed as parallel by our technique are correctly identi.ed as such, despite the fact that only a \nsingle, small data input is considered for par\u00adallelism detection. Furthermore, we show that parallelism \ndetec\u00adtion in isolation is not suf.cient to achieve high performance, and neither are conventional mapping \nheuristics. Our machine-learning based mapping approach provides the adaptivity across platforms that \nis required for a genuinely portable parallelization strategy. On average, our methodology achieves 96% \nof the performance of the hand-tuned OpenMP NAS and SPEC parallel benchmarks on the Intel Xeon platform, \nand a signi.cant speedup for the Cell platform, demonstrating the potential of pro.le-guided machine\u00adlearning \nbased auto-parallelization for complex multi-core plat\u00adforms. Overview. The remainder of this paper is \nstructured as follows. We motivate our work based on simple examples in section 2. This is followed by \na presentation of our parallelization framework in section 3. Our experimental methodology and results \nare discussed in sections 4 and 5, respectively. We establish a wider context of #pragma omp for reduction(+:sum) \nprivate(d) for (j= 1; j <=lastcol -firstcol -1; j++) {d= x[j] - r[j ]; sum =sum + d * d; } Figure 2. \nDespite its simplicity mapping of this parallel loop taken from the NAS cg benchmark is non-trivial and \nthe best-performing scheme varies across platforms. related work in section 6 before we summarize and \nconclude in section 7.  2. Motivation Parallelism Detection. Figure 1 shows a short excerpt of the smvp \nfunction from the SPEC equake seismic wave propagation bench\u00admark. This function implements a general-purpose \nsparse matrix\u00advector product and takes up more than 60% of the total execution time of the equake application. \nWhile conservative, static analysis fails to parallelize both loops due to sparse matrix operations with \nindirect array indices and the inner while loop, pro.ling-based de\u00adpendence analysis provides us with \nthe additional information that no actual data dependence inhibits parallelization for a given sam\u00adple \ninput. While we still cannot prove absence of data dependences for every possible input we can classify \nboth loops as candidates for parallelization (reduction) and, if pro.tably parallelizable, present it \nto the user for approval. In this example, the user would provide the additional knowledge (and guarantee) \nthat every col index in the inner loop is unique and, hence, accesses to w[col][0] and w[col][1], respectively, \ndo not result in cross-iteration dependen\u00adcies. This example demonstrates that static analysis is overly \ncon\u00adservative. Pro.ling based analysis, on the other hand, can provide accurate dependence information \nfor a speci.c input. When com\u00adbined we can select candidates for parallelization based on empir\u00adical \nevidence and, hence, can eventually extract more application parallelism than purely static approaches. \nMapping. In .gure 2 a parallel reduction loop originating from the parallel NAS conjugate-gradient cg \nbenchmark is shown. De\u00adspite the simplicity of the code, mapping decisions are non-trivial. For example, \nparallel execution of this loop is not pro.table for the Cell BE platform due to high communication costs \nbetween pro\u00adcessing elements. In fact, parallel execution results in a massive slowdown over the sequential \nversion for the Cell for any number of threads. On the Intel Xeon platform, however, parallelization \ncan be pro.table, but this depends strongly on the speci.c OpenMP scheduling policy. The best scheme \n(STATIC) results in a speedup of 2.3 over the sequential code and performs 115 times better than the \nworst scheme (DYNAMIC) that slows the program down to 2% of its original, sequential performance. This \nexample illustrates that selecting the correct mapping scheme has a signi.cant impact on performance. \nHowever, the mapping scheme varies not only from program to program, but also from architecture to architecture. \nTherefore, we need an auto\u00admatic and portable solution for parallelism mapping.  3. Parallelization \nFramework In this section we provide an overview and technical details of our parallelization framework. \nAs shown in .gure 3, a sequential C program is initially ex\u00adtended with plain OpenMP annotations for \nparallel loops and re\u00adductions as a result of our pro.ling-based dependence analysis. In   Analysis \nBased Mapping Figure 3. A two-staged parallelization approach combining pro.ling-driven parallelism detection \nand machine-learning based mapping to generate OpenMP annotated parallel programs. addition, data scoping \nfor shared and private data also takes place at this stage. In a second step we add further OpenMP work \nallocation clauses to the code if the loop is predicted to bene.t from par\u00adallelization, or otherwise \nremove the parallel annotations. This also happens for loop candidates where correctness cannot be proven \nconclusively (based on static analysis) and the user disapproves of the suggested parallelization decision. \nFinally, the parallel code is compiled with a native OpenMP compiler for the target platform. A complete \noverview of our tool\u00adchain is shown in .gure 4. 3.1 Pro.le-Driven Parallelism Detection We propose a \npro.le-driven approach to parallelism detection where the traditional static compiler analyses are not \nreplaced, but enhanced with dynamic information. To achieve this we have devised a novel instrumentation \nscheme operating at the interme\u00addiate representation (IR) level of the compiler. Unlike e.g. (21) we \ndo not need to deal with low-level artifacts of any particular in\u00adstruction set, but obtain dynamic control \nand data .ow information relating to IR nodes immediately. This allows us to back-annotate the original \nIR with the pro.ling information and resume com\u00adpilation/parallelization. The three stages involved in \nparallelism detection are: 1. IR instrumentation, C code generation and pro.ling 2. CDFG construction \nand dependence analysis 3. Parallel code generation  3.1.1 Instrumentation and Pro.le Generation Our \nprimary objective is to enhance the static analysis of a tradi\u00adtional parallelizing compiler using precise, \ndynamic information. The main obstacle here is correlating the low-level information gathered during \nprogram execution such as speci.c memory ac\u00adcesses and branch operations to the high-level data and \ncontrol .ow information. Debug information embedded in the executable is usually not detailed enough \nto enable this reconstruction. To bridge this information gap we perform instrumentation at the IR level \nof the compiler (CoSy). For each variable access, ad\u00additional code is inserted that emits the associated \nsymbol table ref\u00aderence as well as the actual memory address of the data item. All data items including \narrays, structures, unions and pointers are cov\u00adered in the instrumentation. This information is later \nused to disam\u00adbiguate memory accesses that static analysis fails to analyze. Simi\u00adlarly, we instrument \nevery control .ow instruction with the IR node identi.er and code to record the actual, dynamic control \n.ow. Even\u00adtually, a plain C representation close to the original program, but with additional instrumentation \ncode inserted, is recovered using an IR-to-C translation pass and compiled with a native x86 com\u00adpiler. \nThe program resulting from this process is still sequential and functionally equivalent to the original \ncode, but emits an additional trace of data access and control .ow items. 3.1.2 CDFG Construction and \nDependence Analysis The subsequent analysis stage consumes one trace item at a time and incrementally \nconstructs a global control and data .ow graph (CDFG) on which the parallelism detection is performed. \nHence, it is not necessary to store the entire trace if the tools are chained up appropriately. Each \ntrace item is processed by algorithm 1. It distinguishes between control and data .ow items and maintains \nvarious data structures supporting dependence analysis. The control .ow sec\u00adtion constructs a global \ncontrol .ow graph of the application in\u00adcluding call stacks, loop nest trees, and normalized loop iteration \nvectors. The data-.ow section is responsible for mapping memory addresses to speci.c high-level data \n.ow information. For this we keep a hash table where data items are traced at byte-level granu\u00adlarity. \nData dependences are recorded as data edges in the CDFG. These edges are further annotated with the speci.c \ndata sections (e.g. array indices) that cause the dependence. For loop-carried data dependences an additional \nbit vector relating the dependence to the surrounding loop nest is maintained.  Data Items \u00b7 CDF G(V, \nEC ,ED): graph with control (EC ) and data-.ow (ED) edges \u00b7 bite[]: bit.eld in each e . ED \u00b7 sete: address \nset in each e . ED \u00b7 ita[]: iteration vector of address a \u00b7 M[A, {V, it}]: hash table: mem. addr. .{V, \nita}\u00b7 it0[]: current normalized iteration vector \u00b7 u . V : current node Procedure instruction handler \nI . next instruction if I is a memory instruction then a . address accessed by instruction if I is a \nDEF then update last writer in M endif else if USE then .nd matching DEF from M if DEF.USE edge e/. \nCDFG then add e in ED endif sete . sete .{a} foreach i : ita[i] . = it0[i] do bite[i] . true ita . \nit0 endif endif else if I is a control instruction then v . node referenced by instruction if edge (u, \nv) ./EC then add (u, v) in CDF G endif u . v  endif Algorithm 1: Algorithm for CDFG construction. As \nsoon as the complete trace has been processed the con\u00adstructed CDFG with all its associated annotations \nis imported back into the CoSy compiler and added to the internal, statically derived data and control \n.ow structures. This is only possible because the dynamic pro.le contains references to IR symbols and \nnodes in ad\u00addition to actual memory addresses. The pro.ling-based CDFG is the basis for the further detection \nof parallelism. However, there is the possibility of con.icting de\u00adpendence information, for example, \nif a may data dependence Figure 4. Our parallelization framework comprises IR-level instrumentation and \npro.ling stages, followed by static and dynamic dependence analyses driving loop-level parallelization \nand a machine-learning based mapping stage where the user may be asked for .nal approval before parallel \nOpenMP code is generated. Platform-speci.c code generation is performed by the native OpenMP enabled \nC compiler.   has not materialized in the pro.ling run. In this case, we treat such a loop as potentially \nparallelizable, but present it to the user for .nal approval if parallelization is predicted to be pro.table. \n3.1.3 Parallel Code Generation We use OpenMP for parallel code generation due to the low complexity of \ngenerating the required code annotations and the widespread availability of native OpenMP compilers. \nCurrently, we only target parallel FOR loops and translate these into corre\u00adsponding OpenMP annotations. \nPrivatization. We maintain a complete list of true-, anti-and output-dependencies as these are required \nfor parallelization. Rather than recording all the readers of each memory location we keep a map of the \nnormalized iteration index of each memory loca\u00adtion that is read/written at each level of a loop-nest. \nThis allows us to ef.ciently track all memory locations that cause a loop-carried anti-or output-dependence. \nA scalar x is privatizable within a loop if and only if every path from the beginning of the loop body \nto a use of x passes from a de.nition of x before the use. Hence, we can determine the privatizable variables \nby inspecting the incom\u00ading and outgoing data-dependence edges of the loop. An analogous approach applies \nto privatizable arrays. Reduction Operations. Reduction recognition for scalar vari\u00adables is based on \nthe algorithm presented in (22), but unlike the original publication we use a simpli.ed code generation \nstage where it is suf.cient to emit an OpenMP reduction annotation for each recognized reduction loop. \nWe validate statically detected reduction candidates using pro.ling information and use an ad\u00additional \nreduction template library to enable reductions on array locations such as that shown in .gure 1. Synchronization. \nThe default behavior of parallel OpenMP loops is to synchronize threads at the end of the work sharing \nconstruct by means of a barrier. Due to the high cost of this form of synchro\u00adnization it is important \nfor good performance that redundant syn\u00adchronization is avoided. Synchronization also increases idle \ntime, due to load imbalance, and can sequentialize sections of a pro\u00adgram. Based on the CDFG we compute \ninter-loop dependencies and apply a compile time barrier synchronization minimization al\u00adgorithm (23), \nresulting in a minimal number of barriers. For those loops where the default synchronization can be eliminated \nwe ex\u00adtend the annotations with the OpenMP nowait clause. Limitations. At present, our approach to code \ngeneration is rela\u00adtively simple and, essentially, relies on OpenMP code annotations alongside minor \ncode transformations. We do not yet perform high\u00adlevel code restructuring which might help expose or \nexploit more parallelism or improve data locality. While OpenMP is a compiler\u00adfriendly target for code \ngeneration it imposes a number of limita\u00adtions. For example, we do not yet exploit coarse-grain parallelism, \ne.g. pipelines, and wavefront parallelism even though we can also extract this form of parallelism. \n 3.2 Machine Learning Based Parallelism Mapping The responsibilities of the parallelism mapping stage \nare to de\u00adcide if a parallel loop candidate is pro.table to parallelize and, if so, to select a scheduling \npolicy from the four options offered by OpenMP: CYCLIC, DYNAMIC, GUIDED,and STATIC.As the ex\u00adample in \n.gure 2 demonstrates, this is a non-trivial task and the op\u00adtimal solution depends on both the particular \nproperties of the loop under consideration and the target platform. To provide a portable, but automated \nmapping approach we use a machine learning tech\u00adnique to construct a predictor that, after some initial \ntraining, will replace the highly platform-speci.c and often in.exible mapping heuristics of traditional \nparallelization frameworks. 3.2.1 Predictive Modeling Separating pro.tably parallelizable loops from \nthose that are not is a challenging task. Incorrect classi.cation will result in missed opportunities \nfor pro.table parallel execution or even in a slow\u00addown due to an excessive synchronization overhead. \nTraditional parallelizing compilers such as SUIF-1 employ simple heuristics based on the iteration count \nand the number of operations in the loop body to decide on whether or not a particular parallel loop \ncandidate should be executed in parallel. Our data as shown in .gure 5 suggests that such a na\u00a8ive \nscheme is likely to fail and that misclassi.cation occurs frequently.  Number of Instructions Should \nbe parallelized Should NOT be parallelized 1. Baseline SVM for classi.cation 1000 (a) Training data: \nD = {(xi,ci)|xi . Rp,ci .{-1, 1}}n i=1 (b) Maximum-margin hyperplane formulation: ci(w \u00b7 xi - b) = 1,for \nall 1 = i = n. 100 (c) Determine parameters by minimization of ||w|| (in w, b) subject to 1.(b). 2. \nExtensions for non-linear multiclass classi.cation (a) Non-linear classi.cation: Replace dot product \nin 1.(b) by a kernel function, e.g. the following radial basis function: 10 1 10 100 1000 10000 100000 \n1000000 1E7 1E8 Number of Iterations Figure 5. This diagrams shows the optimal classi.cation (sequen\u00adtial/parallel \nexecution) of all parallel loop candidates considered in our experiments for the Intel Xeon machine. \nLinear models and static features such as the iteration count and size of the loop body in terms of IR \nstatements are not suitable for separating pro.tably parallelizable loops from those that are not. A \nsimple work based scheme would attempt to separate the prof\u00aditably parallelizable loops by a diagonal \nline as indicated in the diagram in .gure 5. Independent of where exactly the line is drawn there will \nalways be loops misclassi.ed and, hence, potential per\u00adformance bene.ts wasted. What is needed is a scheme \nthat (a) takes into account a richer set of possibly dynamic loop features, (b) is capable of non-linear \nclassi.cation, and (c) can be easily adapted to a new platform. In this paper we propose a predictive \nmodeling approach based on machine-learning classi.cation. In particular, we use Support Vector Machines \n(SVM) (24) to decide (a) whether or not to paral\u00adlelize a loop candidate and (b) how it should be scheduled. \nThe SVM classi.er is used to construct hyper-planes in the multi\u00addimensional space of program features \n as discussed in the fol\u00adlowing paragraph to identify pro.tably parallelizable loops. The classi.er \nimplements a multi-class SVM model with a radial basis function (RBF) kernel capable of handling both \nlinear and non\u00adlinear classi.cation problems (24). The details of our SVM classi\u00ad.er are provided in \n.gure 6.  3.2.2 Program Features We extract characteristic program features that suf.ciently describe \nthe relevant aspects of a program and present it to the SVM clas\u00adsi.er. An overview of these features \nis given in table 1. The static features are derived from CoSy s internal code representation. Es\u00adsentially, \nthese features characterize the amount of work carried out in the parallel loop similar to e.g. (25). \nThe dynamic features capture the dynamic data access and control .ow patterns of the IR Instruction Count \nIR Load/Store Count Static features IR Branch Count Loop Iteration Count Data Access Count Dynamic features \nInstruction Count Branch Count Table 1. Features characterizing each parallelizable loop. k(x, x .)= \nexp(-.||x - x .||2),for .> 0. (b) Multiclass SVM: Reduce single multiclass problem into multiple binary \nprob\u00adlems. Each classi.er distinguishes between one of the labels and the rest. Figure 6. Support vector \nmachines for non-linear classi.cation. sequential program and are obtained from the same pro.ling exe\u00adcution \nthat has been used for parallelism detection. 3.2.3 Training Summary We use an off-line supervised learning \nscheme whereby we present the machine learning component with pairs of program features and desired mapping \ndecisions. These are generated from a library of known parallelizable loops through repeated, timed execution \nof the sequential and parallel code with the different available scheduling options and recording the \nactual performance on the target platform. Once the prediction model has been built using all the available \ntraining data, no further learning takes place. 3.2.4 Deployment For a new, previously unseen application \nwith parallel annotations the following steps need to be carried out: 1. Feature extraction. This involves \ncollecting the features shown in table 1 from the sequential version of the program and is accomplished \nin the pro.ling stage already used for parallelism detection. 2. Prediction. For each parallel loop \ncandidate the corresponding feature set is presented to the SVM predictor and it returns a classi.cation \nindicating if parallel execution is pro.table and which scheduling policy to choose. For a loop nest \nwe start with the outermost loop ensuring that we settle for the most coarse\u00adgrained piece of work. \n3. User Interaction. If parallelization appears to possible (accord\u00ading to the initial pro.ling) and \npro.table (according to the previ\u00adous prediction step), but correctness cannot be proven by static analysis, \nwe ask the user for his/her .nal approval. 4. Code Generation. In this step, we extend the existing \nOpenMP annotation with the appropriate scheduling clause, or delete the annotation if parallelization \ndoes not promise any performance improvement or has been rejected by the user.   3.3 Safety and Scalability \nIssues Safety. Unlike static analysis, pro.le-guided parallelization can\u00adnot conclusively guarantee the \nabsence of control and data depen\u00addences for every possible input. One simple approach regarding the \nselection of the representative inputs is based on control-.ow coverage analysis. This is driven by the \nempirical observation that for the vast majority of the cases the pro.le-driven approach might have a \nfalse positive ( there is a .ow-dependence but the tool sug\u00adgests the contrary ) is due to a control-.ow \npath that the data input set did not cover. This also gives a fast way to select representa\u00adtive workloads \n(in terms of data-dependencies) just by executing the applications natively and recording the resulting \ncode coverage. Of course, there are many counter-examples where an input depen\u00addent data-dependence appears \nwith no difference in the control\u00ad.ow. The latter can be veri.ed by the user. For this current work, \nwe have chosen a worst-case scenario and used the smallest data set associated with each benchmark for \npro.ling, but evaluated against the largest of the available data sets. Surprisingly, we have found that \nthis naive scheme has detected almost all parallelizable loops in the NAS and SPEC OMP benchmarks while \nnot misclassifying any loop as parallelizable when it is not. Furthermore, with the help of our tools \nwe have been able to identify three incorrectly shared variables in the original NAS benchmarks that \nshould in fact be privatized. This illustrates that manual parallelization is prone to errors and that \nautomating this process contributes to program correctness. Scalability. As we process data dependence \ninformation at byte\u00adlevel granularity and effectively build a whole program CDFG we may need to maintain \ndata structures growing potentially as large as the entire address space of the target platform. In practice, \nhow\u00adever, we have not observed any cases where more than 1GB of heap memory was needed to maintain the \ndynamic data dependence structures, even for the largest applications encountered in our ex\u00adperimental \nevaluation. In comparison, static compilers that perform whole program analyses need to maintain similar \ndata structures of about the same size. While the dynamic traces can potentially be\u00adcome very large as \nevery single data access and control .ow path is recorded, they can be processed online, thus eliminating \nthe need for large traces to be stored. As our approach operates at the IR level of the compiler we do \nnot need to consider detailed architecture state, hence pro.ling can be accomplished at speeds close \nto native, sequential speed. For de\u00adpendence analysis we only need to keep track of memory and con\u00adtrol \n.ow operations and make incremental updates to hash tables and graph structures. In fact, dependence \nanalysis on dynamically constructed CDFGs has the same complexity as static analysis be\u00adcause we use \nthe same representations and algorithms as the static counterparts.  4. Experimental Methodology In \nthis section we summarize our experimental methodology and provide details of the multi-core platforms \nand benchmarks used throughout the evaluation. 4.1 Platforms We target both a shared memory (dual quad-core \nIntel Xeon) and distributed memory multi-core system (dual-socket QS20 Cell blade). A brief overview \nof both platforms is given in table 3. 4.2 Benchmarks For our evaluation we have selected benchmarks \n(NAS and SPEC OMP) where both sequential and manually parallelized OpenMP versions are available. This \nhas enabled us to directly compare our parallelization strategy against parallel implementations from \nindependent expert programmers. More speci.cally, we have used the NAS NPB (sequential v.2.3) and NPB \n(OpenMP v.2.3) codes (26) alongside the SPEC CPU2000 benchmarks and their corresponding SPEC OMP2001 \nProgram Suite Data Sets/Xeon Data Sets/Cell BT NPB2.3-OMP-C S, W, A, B NA CG NPB2.3-OMP-C S, W, A, B \nS, W, A EP NPB2.3-OMP-C S, W, A, B S, W, A FT NPB2.3-OMP-C S, W, A, B S, W, A IS NPB2.3-OMP-C S, W, A, \nB S, W, A MG NPB2.3-OMP-C S, W, A, B S, W, A SP NPB2.3-OMP-C S, W, A, B S, W, A LU NPB2.3-OMP-C S, W, \nA, B S, W, A art SPEC CFP2000 test, train, ref test,train, ref ammp SPEC CFP2000 test, train, ref test,train, \nref equake SPEC CFP2000 test, train, ref test,train, ref Table 2. Benchmark applications and data sets. \n Cell Blade Server  Hardware Dual Socket, QS20 Cell Blade 2 \u00d7 3.2 GHz IBM Cell processors 512KB L2 \ncache per chip 1GB XDRAM O.S Fedora Core 7 with Linux kernel 2.6.22 SMP Compiler IBM XLC single source \ncompiler for Cell v0.9 -O5 -qstrict -qarch=cell -qipa=partition=minute (-qipa=overlay) Cell SDK 3.0 \n  Intel Xeon Server  Hardware Dual Socket, Intel Xeon X5450 @ 3.00GHz 2 Quad-cores, 8 cores in total \n6MB L2-cache shared/2 cores (12MB/chip) 16GB DDR2 SDRAM O.S 64-bit Scienti.c Linux with kernel 2.6.9-55 \nx86 64 Compiler Intel ICC 10.1 (Build 20070913) -O2 -xT -axT -ipo Table 3. Hardware and software con.guration \ndetails of the two evaluation platforms. counterparts. However, it should be noted that the sequential \nand parallel SPEC codes are not immediately comparable due to some amount of restructuring of the of.cial \nparallel codes, resulting in a performance advantage of the SPEC OMP codes over the sequen\u00adtial ones, \neven on a single processor system. Each program has been executed using multiple different input data \nsets (shown in table 2), however, for parallelism detection and mapping we have only used the smallest \nof theavailabledatasets1. The resulting parallel programs have then been evaluated against the larger \ninputs to investigate the impact of worst-case input on the safety of our parallelization scheme.  \n4.3 Methodology We have evaluated three different parallelization approaches: man\u00adual, auto-parallelization \nusing the Intel ICC compiler (just for the Intel platform), and our pro.le-driven approach. For native \ncode generation all programs (both sequential and parallel OpenMP) have been compiled using the Intel \nICC and IBM XLC compilers for the Intel Xeon and IBM Cell platforms, respectively. Furthermore, we use \nleave-one-out cross-validation to eval\u00aduate our machine-learning based mapping technique. This means \nthat for K programs, we remove one, train a model on the remain\u00ading K - 1 programs and predict the Kth \nprogram with the previ\u00adously trained model. We repeat this procedure for each program in turn. For the \nCell platform we report parallel speedup over sequential code running on the general-purpose PPE rather \nthan a single SPE. In all cases the sequential performance of the PPE exceeds that of 1 Some of the larger \ndata sets could not be evaluated on the Cell due to memory constraints.  a single SPE, ensuring we report \nimprovements over the strongest baseline available.  5. Experimental Evaluation In this section we \npresent and discuss our results. 5.1 Overall Results Figures 7(a) and 7(b) summarize our performance \nresults for both the Intel Xeon and IBM Cell platforms. Intel Xeon. The most striking result is that \nthe Intel auto\u00adparallelizing compiler fails to exploit any usable levels of paral\u00adlelism across the whole \nrange of benchmarks and data set sizes. In fact, auto-parallelization results in a slow-down of the BT \nand LU benchmarks for the smallest and for most data set sizes, re\u00adspectively. ICC gains a modest speedup \nonly for the larger data sets of the IS and SP benchmarks. The reason for this disappoint\u00ading performance \nof the Intel ICC compiler is that it is typically parallelizing at inner-most loop level where signi.cant \nfork/join overhead negates the potential bene.t from parallelization. The manually parallelized OpenMP \nprograms achieve an aver\u00adage speedup of 3.5 across the benchmarks and data sizes. In the case of EP, \na speedup of 8 was achieved for large data sizes. This is not surprising since this is an embarrassingly \nparallel program. More surprisingly, LU was able to achieve super-linear speedup (9\u00d7) due to improved \ncaching (27). Some programs (BT, MG and CG)ex\u00adhibit lower speedups with larger data sets (A and B in \ncomparison to W) on the Intel machine. This is a well-known and documented scalability issue of these \nspeci.c benchmarks (28; 27). For most NAS benchmarks our pro.le-driven parallelization achieves performance \nlevels close to those of the manually par\u00adallelized versions, and sometimes outperforms them (EP, IS \nand MG). This surprising performance gain can be attributed to three important factors. Firstly, our \napproach parallelizes outer loops whereas the manually parallelized codes have parallel inner loops. \nSecondly, our approach exploits reduction operations on array loca\u00adtions and, .nally, the machine learning \nbased mapping is more ac\u00adcurate in eliminating non-pro.table loops from parallelization and selecting \nthe best scheduling policy. The situation is slightly different for the SPEC benchmarks. While pro.le-driven \nparallelization still outperforms the static auto-parallelizer we do not reach the performance level \nof the manually parallelized codes. Investigations into the causes of this behavior have revealed that \nthe SPEC OMP codes are not equiv\u00adalent to the sequential SPEC programs, but have been manually restructured \n(29). For example, data structures have been altered (e.g. from list to vector) and standard memory allocation \n(exces\u00adsive use of malloc) has been replaced with a more ef.cient scheme. Obviously, these changes are \nbeyond what an auto-parallelizer is capable of performing. In fact, we were able to con.rm that the sequential \nperformance of the SPEC OpenMP codes is on average about 2 times (and up to 3.34 for art) above that \nof their original SPEC counterparts. We have veri.ed that our approach parallelizes the same critical \nloops for both equake and art as SPEC OMP. For art we achieve a speedup of 4, whereas the SPEC OMP version \nis 6 times faster than the sequential SPEC FP version, of which more than 50% is due to sequential code \noptimizations. We also measured the performance of the pro.le-driven parallelized equake version using \nthe same code modi.cations and achieved a compa\u00adrable speedup of 5.95. Overall, the results demonstrate \nthat our pro.le-driven paral\u00adlelization scheme signi.cantly improves on the state-of-the-art In\u00adtel auto-parallelizing \ncompiler. In fact, our approach delivers per\u00adformance levels close to or exceeding those of manually \nparal\u00adlelized codes and, on average, we achieve 96% of the performance of hand-tuned parallel OpenMP \ncodes, resulting in an average speedup of 3.34 across all benchmarks. IBM Cell. Figure 7(b) shows the \nperformance resulting from manual and pro.le-driven parallelization for the dual-Cell plat\u00adform. Unlike \nthe Intel platform, the Cell platform does not deliver a high performance on the manually parallelized \nOpenMP programs. On average, these codes result in an overall slowdown. For some programs such as CG \nand EP small performance gains could be ob\u00adserved, however, for most other programs the performance degra\u00addation \nis disappointing. Given that these are hand-parallelized pro\u00adgrams this is perhaps surprising and there \nare essentially two rea\u00adsons why the Cell s performance potential could not be exploited. Firstly, it \nis clear that the OpenMP codes have not been developed speci.cally for the Cell. The programmer have \nnot considered the communication costs for a distributed memory machine. Secondly, in absence of speci.c \nscheduling directives the OpenMP runtime library resorts to its default behavior, which leads to poor \noverall performance. Given that the manually parallelized programs de\u00adliver high performance levels on \nthe Xeon platform, the results for the Cell demonstrate that parallelism detection in isolation is not \nsuf.cient, but mapping must be regarded as equally important. In contrast to the default manual parallelization \nscheme, our integrated parallelization strategy is able to successfully exploit signi.cant levels of \nparallelism, resulting in average speedup of 2.0 over the sequential code and up to 6.2 for individual \nprograms (EP). This success can largely be attributed to the improved mapping of parallelism resulting \nfrom our machine-learning based approach.  5.2 Parallelism Detection and Safety Our approach relies \non dynamic pro.ling information to discover parallelism. This has the obvious drawback that it may classify \na loop as potentially parallel when there exists another data set which would highlight a dependence \npreventing correct parallelization. This is a fundamental limit of dynamic analysis and the reason for \nrequesting the user to con.rm uncertain parallelization decisions. It is worthwhile, therefore, to examine \nto what extent our approach suffers from false positives ( loop is incorrectly classi.ed as paral\u00adlelizable \n). Clearly, an approach that suffers from high numbers of such false positives will be of limited use \nto programmers. Column 2 in table 5.2 shows the number of loops our approach detects as potentially parallel. \nThe column labeled FP ( false pos\u00aditive ) shows how many of these were in fact sequential. The sur\u00adprising \nresult is that none of the loops we considered potentially parallel turned out to be genuinely sequential. \nCertainly, this re\u00adsults does not prove that dynamic analysis is always correct. Still, it indicates \nthat pro.le-based dependence analysis may be more ac\u00adcurate than generally considered, even for pro.les \ngenerated from small data sets. Clearly, this encouraging result will need further validation on more \ncomplex programs before we can draw any .\u00adnal conclusions. Column 3 in table 5.2 lists the number of \nloops parallelizable by ICC. In some applications, the ICC compiler is able to detect a considerable \nnumber of parallel loops. In addition, if we examine the coverage (shown in parentheses) we see that \nin many cases this covers a considerable part of the program. Therefore we conclude that it is less a \nmatter of the parallelism detection that causes ICC to perform so poorly, but rather how it exploits \nand maps the detected parallelism (see section 5.3). The .nal column in table 5.2 eventually shows the \nnumber of loops parallelized in the hand-coded applications. As before, the percentage of sequential \ncoverage is shown in parentheses. Far fewer loops than theoretically possible are actually parallelized \nbe\u00adcause the programmer have obviously decided only to parallelize those loops they considered hot and \npro.table . These loops Table 4. Number of parallelized loops and their respective cover\u00adage of the sequential \nexecution time.  Pro.le driven ICC no threshold Manual Application #loops(%cov) FP FN #loops(%cov) #loops(%cov) \nbt 205 (99.9%) 0 0 72 (18.6%) 54 (99.9%) cg 28 (93.1%) 0 0 16 (1.1%) 22 (93.1%) ep 8 (99.9%) 0 0 6(<1%) \n1 (99.9%) ft 37 (88.2%) 0 0 3(<1%) 6 (88.2%) is 9 (28.5%) 0 0 8 (29.4%) 1 (27.3%) lu 154 (99.7%) 0 0 \n88 (65.9%) 29 (81.5%) mg 48 (77.7%) 0 3 9 (4.7%) 12 (77.7%) sp 287 (99.6%) 0 0 178 (88.0%) 70 (61.8%) \nequake SEQ 69 (98.1%) 0 0 29 (23.8%) 11 (98.0%) art SEQ 31 (85.6%) 0 0 16 (30.0%) 5 (65.0%) ammp SEQ \n21 (1.4%) 0 1 43 (<1%) 7 (84.4%) cover a signi.cant part of the sequential time and effective par\u00adallelization \nleads to good performance as can be seen for the Xeon platform. In total there are four false negatives \n(column FN in table 5.2) , i.e. loops not identi.ed as parallel although safely parallelizable. Three \nfalse negatives are contained in the MG benchmark, and two of these are due to loops which have zero \niteration counts for all data sets and, therefore, are never pro.led. The third one is a MAX reduction, \nwhich is contained inside a loop that our machine\u00adlearning classi.er has decided not to parallelize. \n 5.3 Parallelism Mapping In this section we examine the effectiveness of three mapping schemes (manual, \nheuristic with static features, and machine\u00adlearning using pro.ling information) across the two platforms. \nCG.S CG.W CG.A EP.S EP.W EP.A FT.S FT.W FT.A IS.S IS.W IS.A LU.S LU.W LU.A MG.S MG.W MG.A SP.S SP.W \nSP.A art.test art.train art.ref ammp.test ammp.train ammp.ref equake.test equake.train equake.ref AVERAGE \n BT.S BT.W BT.A BT.B CG.S CG.W CG.A CG.B EP.S EP.W EP.A EP.B FT.S FT.W FT.A FT.B IS.S IS.W  IS.A \nIS.B LU.S LU.W LU.A LU.B MG.S MG.W MG.A MG.B SP.S SP.W SP.A SP.B ammp.test ammp.train ammp.refart.testart.train \nart.refequake.testequake.train equake.ref AVERAGE Speedup Intel Xeon. Figure 8(a) compares the performance \nof ICC and our approach to that of the hand-parallelized OpenMP programs. In the case of ICC we show \nthe performance of two different mapping approaches. By default, ICC employs a compile-time pro.tability \ncheck while the second approach performs a runtime check using a dynamic pro.tability threshold. For \nsome cases (BT.B and SP.B) the runtime checks provide a marginal improvement over the static mapping \nscheme while the static scheme is better for IS.B. Overall, both schemes are equally poor and deliver \nless than half of the speedup levels of the hand-parallelized benchmarks. The disappointing performance \nappears to be largely due to non-optimal mapping decisions, i.e. to parallelize inner loops rather than \nouter ones. In the same .gure we compare our machine-learning based mapping approach against a scheme \nwhich uses the same pro.ling information, but employs a .xed, work-based heuristic similar to the one \nimplemented in the SUIF-1 parallelizing compiler (see also .gure 5). This heuristic considers the product \nof the iteration count and the number of instructions contained in the loop body and decides against \na static threshold. While our machine-learning approach delivers nearly the performance of the hand-parallelized \ncodes and, in some cases, is able to outperform them, the static heuristic performs poorly and is unable \nto obtain more than 85% of the performance of the hand-parallelized code. This translates into an average \nspeedup of 2.5 rather than 3.7 for the NAS benchmarks. The main reason for this performance loss is that \nthe default scheme using only static code features and a linear work model is unable to accurately determine \nwhether a loop should be parallelized or not. In .gure 9 we compare the performance resulting from the \ndifferent automated mapping approaches to that of the hand\u00adparallelized SPEC OMP codes. Again, our machine-learning \nbased approach outperforms ICC and the .xed heuristic. On average, our ICC Manual Parallelization Prof-driven \nParallelization 9 8 7 6 5 4 3 2 1 0  (a) Speedup over sequential codes achieved by ICC auto-parallelization, \nmanual parallelization and pro.le-driven parallelization for the Xeon platform. (b) Speedup over sequential \ncode achieved by manual parallelization and pro.le-driven parallelization for the dual Cell platform. \nFigure 7. Speedups due to different parallelization schemes. Figure 9. Impact of different mapping approaches \nfor the SPEC benchmarks (100% = manually parallelized OpenMP code). approach delivers 88% of the performance \nof the hand-parallelized code, while ICC and the .xed heuristic approach achieve perfor\u00admance levels \nof 45% and 65%, respectively. The lower performance gains for the SPEC benchmarks are mainly due to a \nbetter starting point of the hand-parallelized SPEC OMP benchmarks (see section 5.1). IBM Cell. The diagram \nin .gure 8(b) shows the speedup of our machine-learning based mapping approach over the hand\u00adparallelized \ncode on the Cell platform. As before, we compare our approach against a scheme which uses the pro.ling \ninformation, but employs a .xed mapping heuristic. The manually parallelized OpenMP programs are not \nspeci.\u00adcally tuned for the Cell platform and perform poorly. As a con\u00adsequence, the pro.le-based mapping \napproaches show high perfor\u00admance gains over this baseline, in particular, for the small input data sets. \nStill, the combination of pro.ling and machine-learning outperforms the .xed heuristic counterpart by \nfar and, on average,  BT.S CG.S BT.W CG.W BT.A CG.A (b) NAS and SPEC FP benchmarks on the IBM Cell \nplatform. Figure 8. Impact of different mapping approaches (100% = manually parallelized OpenMP code). \nresults in a speedup of 9.7 over the hand-parallelized OpenMP pro\u00adgrams across all data sets. Summary \nThe combined pro.ling and machine-learning ap\u00adproach to mapping comes within reach of the performance \nof hand\u00adparallelized code on the Intel Xeon platform and in some cases outperforms it. Fixed heuristics \nare not strong enough to separate pro.tably parallelizable loops from those that are no and perform poorly. \nTypically, static mapping heuristics result in performance levels of less than 60% of the machine learning \napproach. This is because the default scheme is unable to accurately determine whether a loop should \nbe parallelized or not. The situation is ex\u00adacerbated on the Intel Cell platform where accurate mapping \nde\u00adcisions are key enablers to high performance. Existing ( generic ) manually parallelized OpenMP codes \nfail to deliver any reasonable performance and heuristics, even if based on pro.ling data, are unable \nto match the performance of our machine-learning based scheme.  5.4 Scalability For the Xeon platform \nthe LU and EP benchmarks scale well with the number of processors (see .gure 10). In fact, a super-linear \nspeedup due to more cache memory in total can be observed for the LU application. For other benchmarks \nscalability is more limited and often saturation effects occur for four or more processors. This scalability \nissue of the NAS benchmarks is well-known and in line with other research publications (27). Figure 11 \nshows a performance drop for the step from one to two processors on the Cell platform. This is due to \nthe fact that we use the generally more powerful PPE to measure single processor performance, but then \nuse the multiple SPEs for parallel performance measurements. The diagram reveals that in the best case \nit takes about three SPEs to achieve the original performance of the PPE. Some of the more scalable benchmarks \nsuch as EP and MG follow a linear trend as BT.B EP.S CG.S EP.W CG.WCG.ACG.B EP.A FT.S FT.W EP.S EP.W \nFT.AEP.A IS.SEP.B IS.W (a) NAS benchmarks on the Intel Xeon platform. IS.A LU.S LU.W LU.A MG.S MG.W \nMG.A SP.S SP.W SP.Aart.testart.trainart.refammp.testammp.trainammp.refequake.testequake.trainequake.refAVERAGE \n FT.B IS.S IS.W IS.A IS.B LU.S LU.W LU.A LU.B MG.S MG.WMG.AMG.BSP.SSP.WSP.ASP.BAVERAGE  Number of \nProcessors Number of Processors the number of processors increases, however, most of the remaining benchmarks \nsaturate at a low level.  6. Related Work Parallel Programming Languages. Many approaches have been \nproposed for changing or extending to existing programming lan\u00adguages to enable easier exploiting of \nparallelism (9; 10; 12). Unfor\u00adtunately, these approaches do not alleviate all problems of porting legacy \nsequential programs. Automatic Parallelization. Static automatic parallelism extrac\u00adtion has been achieved \non restricted DOALL and DOACROSS loops (3; 4; 5). Unfortunately, many parallelization opportunities could \nstill not be discovered by a static analysis approach due to lack of information at the source code level. \nSpeculative Parallelization. There are existing automatic paral\u00adlelization techniques that exploit parallelism \nin a speculatively ex\u00adecution manner (30; 31; 32), but these approaches typically re\u00adquire hardware support. \nMatthew et al. (33) have manually par\u00adallelized the SPECINT-2000 benchmarks with thread level specu\u00adlation. \nTheir approach relies upon the programmer to discover par\u00adallelism as well as runtime support for parallel \nexecution. Dynamic Parallelization Rus et al. (34) applied sensitivity analy\u00adsis to automatic parallelize \nprograms whose behaviors may be sen\u00adsitive to input data sets. In contrast to their static analysis and \nrun\u00adtime checking approach, our pro.ling-driven approach discovers more parallel opportunities as well \nas selecting parallel candidates and scheduling policies. Dynamic dependence analysis (35; 36) and hybrid \ndata dependence analysis (37) make use of dynamic depen\u00addence information, but delay much of the parallelization \nwork to the runtime of the program. In contrast, we employ a separate pro\u00ad.ling stage and incorporate \nthe dynamic information in the usual compiler based parallelization without causing any runtime over\u00adhead. \nInteractive Parallelization. Interactive parallelization tools (13; 17; 18; 19; 20) provide a way to \nactively involve the programmer in the detection and mapping of application parallelism. For example, \nSUIF Explorer (13) helps the programmer to identify those loops that are likely to be parallelizable \nand assists the user in checking for correctness. Similarly, the Software Behavior-Oriented Paral\u00adlelization \n(38) system allows the programmer to specify intended parallelism. In (39), programmers mark potential \nparallel regions of the program, and the tool uses dynamic pro.ling information to .nd a good mapping \nof parallel candidates. Unlike our approach, these frameworks require the programmer to mark parallel \nregions, in\u00adstead of discovering parallelism automatically. Moreover, the prob\u00adlem of mapping parallelism \nacross architectures is not well ad\u00addressed in these approaches. Parallelism Mapping. Prior research \nin parallelism mapping has mainly focused on building heuristics and analytical models (40; 41), runtime \nadaptation (42; 43) approaches, and on mapping or migrating tasks on a speci.c platform. Instead of proposing \na new scheduling or mapping technique for a particular platform, we aim to develop a compiler-based, \nautomatic, and portable approach that learns how to take advantage of existing compilers and runtime \nsys\u00adtem for ef.ciently mapping parallelism. Ramanujam and Sadayap\u00adpan (40) used heuristics to solve the \ntask mapping problems in dis\u00adtributed memory machines. Their model requires low-level detail of the hardware \nplatform, such as the communication cost, which have to be re-tuned once the underlying architecture \nchanges. Static analytical models have been proposed for predicting the program s behaviors. For example, \nthe OpenUH compiler uses a cost model for evaluating the cost for parallelizing OpenMP programs (41). \nThere are also some models that predict the parallel performance such as LogP (44). However, these models \nrequire help from their users and are not portable. Corbalan et al. (42) measure perfor\u00admance and allocate \nprocessors during runtime. The adaptive loop scheduler (43) selects both the number of threads and the \nschedul\u00ading policy for a parallel region in SMPs through runtime decisions. In contrast to this runtime \napproach, this paper presents a static pro\u00adcessor allocation scheme which is performed at compilation \ntime. Adaptive Compilation. Machine learning and statistical methods have already been used in single \ncore program transformation. For example, Cooper et al. (45) develop a technique to .nd good compiler \noptimization sequences for code size reduction. In contrast to prior research, we built a model that \nlearns how to effectively map parallelism to multi-core platforms with existing compilers and runtime \nsystems. The model is automatically con\u00adstructed and trained off-line, and the parallelism decisions \nare made at the compilation time.  7. Conclusion and Future Work In this paper we have developed a platform-agnostic, \npro.ling\u00adbased parallelism detection method that enhances static data de\u00adpendence analyses with dynamic \ninformation, resulting in larger amounts of parallelism uncovered from sequential applications. We have \nalso shown that parallelism detection in isolation is not suf.\u00adcient to achieve high performance, but \nrequires close interaction with an adaptive mapping scheme to unfold the full potential of parallel execution \nacross programs and architectures.  Results obtained on two complex multi-core platforms (Intel Xeon \nand IBM Cell) and two sets of benchmarks (NAS and SPEC) con.rm that our method is more aggressive in \nparallelization and more portable than existing static auto-parallelization and achieves performance \nlevels close to manually parallelized codes. Future work will focus on further improvements of the pro.ling\u00adbased \ndata dependence analysis with the ultimate goal of eliminat\u00ading the need for the user s approval for \nparallelization decisions that cannot be proven conclusively. Furthermore, we will integrate support \nfor restructuring transformations into our framework and target parallelism beyond the loop level.  \n References [1] H. P. Hofstee. Future microprocessors and off-chip SOP interconnect. IEEE Trans. on Advanced \nPackaging, 27(2), May 2004. [2] L. Lamport. The parallel execution of DO loops. Communications of ACM, \n17(2), 1974. [3] M. Burke and R. Cytron. Interprocedural dependence analysis and parallelization. PLDI \n, 1986. [4] R. Allen and K. Kennedy. Optimizing Compilers for Modern Archi\u00adtectures: A Dependence-Based \nApproach. Morgan Kaufmann, 2002. [5] A. W. Lim and M. S. Lam. Maximizing parallelism and minimizing synchronization \nwith af.ne transforms. Parallel Computing,ACM, 1997. [6] D. A. Padua, R. Eigenmann, et al. Polaris: A \nnew-generation paral\u00adlelizing compiler for MPPs. Technical report, In CSRD No. 1306. UIUC, 1993. [7] \nM. W. Hall, J. M. Anderson, et al. Maximizing multiprocessor perfor\u00admance with the SUIF compiler. Computer, \n29(12), 1996. [8] Open64. http://www.open64.net. [9] F. Matteo, C. Leiserson, and K. Randall. The implementation \nof the Cilk-5 multithreaded language. PLDI, 1998. [10] M. Gordon, W. Thies, M. Karczmarek, et al. A stream \ncompiler for communication-exposed architectures. ASPLOS, 2002. [11] P. Husbands Parry, C. Iancu, and \nK. Yelick. A performance analysis of the Berkeley UPC compiler. SC, 2003. [12] V. A. Saraswat, V. Sarkar, \nand C von. Praun. X10: Concurrent pro\u00adgramming for modern architectures. PPoPP, 2007. [13] L. Shih-Wei, \nD. Amer, et al. SUIF Explorer: An interactive and interprocedural parallelizer. SIGPLAN Not., 34(8), \n1999. [14] M. Kulkarni, K. Pingali, B. Walter, et al. Optimistic parallelism requires abstractions. PLDI \n07, 2007. [15] L. Rauchwerger, F. Arzu, and K. Ouchi. Standard Templates Adaptive Parallel Library. Inter. \nWorkshop LCR, 1998. [16] Jia Guo, Ganesh Bikshandi, et al. Hierarchically tiled arrays for parallelism \nand locality. IPDPS, 2006. [17] F. Irigoin, P. Jouvelot, and R. Triolet. Semantical interprocedural parallelization: \nan overview of the PIPS project. ICS 1991 [18] K. Kennedy, K. S. McKinley, and C. W. Tseng. Interactive \nparallel programming using the Parascope editor. IEEE TPDS, 2(3), 1991. [19] T. Brandes, S. Chaumette, \nM. C. Counilh et al. HPFIT: a set of inte\u00adgrated tools for the parallelization of applications using \nhigh perfor\u00admance Fortran. part I: HPFIT and the Transtool environment. Parallel Comput., 23(1-2), 1997. \n[20] M. Ishihara, H. Honda, and M. Sato. Development and implemen\u00adtation of an interactive parallelization \nassistance tool for OpenMP: iPat/OMP. IEICE Trans. Inf. Syst., E89-D(2), 2006. [21] S. Rul, H. Vandierendonck, \nand K. De Bosschere. A dynamic anal\u00adysis tool for .nding coarse-grain parallelism. In HiPEAC Industrial \nWorkshop, 2008. [22] W. M. Pottenger. Induction variable substitution and reduction recog\u00adnition in the \nPolaris parallelizing compiler. Technical Report, UIUC, 1994. [23] M. O Boyle and E. St\u00a8ohr. Compile \ntime barrier synchronization minimization. IEEE TPDS, 13(6), 2002. [24] E. B. Bernhard, M. G. Isabelle, \nand N. V. Vladimir. A training algorithm for optimal margin classi.ers. Workshop on Computational Learning \nTheory, 1992. [25] H. Ziegler and M. Hall. Evaluating heuristics in automatically map\u00adping multi-loop \napplications to FPGAs. FPGA, 2005. [26] D. H. Bailey, E. Barszcz, et al. The NAS parallel benchmarks. \nThe International Journal of Supercomputer Applications, 5(3), 1991. [27] R. E. Grant and A. Afsahi. \nA Comprehensive Analysis of OpenMP Applications on Dual-Core Intel Xeon SMPs. IPDPS, 2007. [28] NAS Parallel \nBenchmarks 2.3, OpenMP C version. http://phase.hpcc.jp/Omni/benchmarks/NPB/index.html. [29] V. Aslot, \nM. Domeika, et al. SPEComp: A New Benchmark Suite for Measuring Parallel Computer Performance. LNCS, \n2001. [30] S. Wallace, B. Calder, and D. M. Tullsen. Threaded multiple path execution. ISCA, 1998. [31] \nJ. Dou and M. Cintra. Compiler estimation of load imbalance overhead in speculative parallelization. \nPACT, 2004. [32] R. Ramaseshan and F. Mueller. Toward thread-level speculation for coarse-grained parallelism \nof regular access patterns. MULTIPROG, 2008. [33] M. Bridges, N. Vachharajani, et al. Revisiting the \nsequential program\u00adming model for multi-core. MICRO, 2007. [34] S. Rus, M. Pennings, and L. Rauchwerger. \nSensitivity analysis for automatic parallelization on multi-cores, 2007. ICS, 2007 [35] P. Peterson and \nD. Padua. Dynamic dependence analysis: A novel method for data dependence evaluation. LCPC, 1992. [36] \nM. Chen and K. Olukotun. The JRPM system for dynamically paral\u00adlelizing Java programs. ISCA, 2003. [37] \nS. Rus and L. Rauchwerger. Hybrid dependence analysis for automatic parallelization. Technical Report, \nDept. of CS, Texas A&#38;M U., 2005. [38] C. Ding, X. Shen, et al. Software behavior oriented parallelization. \nPLDI, 2007. [39] W. Thies, V. Chandrasekhar, and S. Amarasinghe. A practical ap\u00adproach to exploiting \ncoarse-grained pipeline parallelism in C pro\u00adgrams. MICRO, 2007. [40] J. Ramanujam and P. Sadayappan. \nA methodology for parallelizing programs for multicomputers and complex memory multiprocessors. SC, 1989. \n[41] C. Liao and B. Chapman. A compile-time cost model for OpenMP. IPDPS, 2007. [42] J. Corbalan, X. \nMartorell, and J. Labarta. Performance-driven proces\u00adsor allocation. IEEE TPDS, 16(7), 2005. [43] Y. \nZhang and M. Voss. Runtime empirical selection of loop schedulers on Hyperthreaded SMPs. IPDPS, 2005. \n[44] L. G. Valiant. A bridging model for parallel computation. Communi\u00adcations of the ACM, 33(8), 1990. \n[45] K. Cooper, P. Schielke, and D. Subramanian. Optimizing for reduced code space using genetic algorithms. \nLCTES, 1999. [46] A. Monsifrot, F. Bodin, and R. Quiniou. A machine learning approach to automatic production \nof compiler heuristics. Arti.cial Intelligence: Methodology, Systems, Applications, 2002. [47] L.N. Pouchet, \nC. Bastoul, A. Cohen, and J. Cavazos. Iterative op\u00adtimization in the polyhedral model: part II, multidimensional \ntime. PLDI, 2008.   \n\t\t\t", "proc_id": "1542476", "abstract": "<p>Compiler-based auto-parallelization is a much studied area, yet has still not found wide-spread application. This is largely due to the poor exploitation of application parallelism, subsequently resulting in performance levels far below those which a skilled expert programmer could achieve. We have identified two weaknesses in traditional parallelizing compilers and propose a novel, integrated approach, resulting in significant performance improvements of the generated parallel code. Using profile-driven parallelism detection we overcome the limitations of static analysis, enabling us to identify more application parallelism and only rely on the user for final approval. In addition, we replace the traditional target-specific and inflexible mapping heuristics with a machine-learning based prediction mechanism, resulting in better mapping decisions while providing more scope for adaptation to different target architectures. We have evaluated our parallelization strategy against the NAS and SPEC OMP benchmarks and two different multi-core platforms (dual quad-core Intel Xeon SMP and dual-socket QS20 Cell blade). We demonstrate that our approach not only yields significant improvements when compared with state-of-the-art parallelizing compilers, but comes close to and sometimes exceeds the performance of manually parallelized codes. On average, our methodology achieves 96% of the performance of the hand-tuned OpenMP NAS and SPEC parallel benchmarks on the Intel Xeon platform and gains a significant speedup for the IBM Cell platform, demonstrating the potential of profile-guided and machine-learning based parallelization for complex multi-core platforms.</p>", "authors": [{"name": "Georgios Tournavitis", "author_profile_id": "81435609533", "affiliation": "University of Edinburgh, Edinburgh, United Kingdom", "person_id": "P1464269", "email_address": "", "orcid_id": ""}, {"name": "Zheng Wang", "author_profile_id": "81435596430", "affiliation": "University of Edinburgh, Edinburgh, United Kingdom", "person_id": "P1464270", "email_address": "", "orcid_id": ""}, {"name": "Bj&#246;rn Franke", "author_profile_id": "81100183298", "affiliation": "University of Edinburgh, Edinburgh, United Kingdom", "person_id": "P1464271", "email_address": "", "orcid_id": ""}, {"name": "Michael F.P. O'Boyle", "author_profile_id": "81452607145", "affiliation": "University of Edinburgh, Edinburgh, United Kingdom", "person_id": "P1464272", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1542476.1542496", "year": "2009", "article_id": "1542496", "conference": "PLDI", "title": "Towards a holistic approach to auto-parallelization: integrating profile-driven parallelism detection and machine-learning based mapping", "url": "http://dl.acm.org/citation.cfm?id=1542496"}