{"article_publication_date": "06-15-2009", "fulltext": "\n LiteRace: Effective Sampling for Lightweight Data-Race Detection Daniel Marino Madanlal Musuvathi Satish \nNarayanasamy University of California, Los Angeles Microsoft Research, Redmond University of Michigan, \nAnn Arbor dlmarino@cs.ucla.edu madanm@microsoft.com nsatish@umich.edu Abstract Data races are one of \nthe most common and subtle causes of perni\u00adcious concurrency bugs. Static techniques for preventing data \nraces are overly conservative and do not scale well to large programs. Past research has produced several \ndynamic data race detectors that can be applied to large programs. They are precise in the sense that \nthey only report actual data races. However, dynamic data race de\u00adtectors incur a high performance overhead, \nslowing down a pro\u00adgram s execution by an order of magnitude. In this paper we present LiteRace, a very \nlightweight data race detector that samples and analyzes only selected portions of a pro\u00adgram s execution. \nWe show that it is possible to sample a multi\u00adthreaded program at a low frequency, and yet, .nd infrequently \noccurring data races. We implemented LiteRace using Microsoft s Phoenix compiler. Our experiments with \nseveral Microsoft pro\u00adgrams, Apache, and Firefox show that LiteRace is able to .nd more than 70% of data \nraces by sampling less than 2% of memory ac\u00adcesses in a given program execution. Categories and Subject \nDescriptors D. Software [D.2 Software Engineering]: D.2.5 Testing and Debugging Debugging aids General \nTerms Algorithms, Experimentation, Reliability, Veri.\u00adcation Keywords Sampling, Dynamic Data Race Detection, \nConcur\u00adrency Bugs 1. Introduction Multi-threaded programs are notoriously dif.cult to get right, largely \ndue to the non-deterministic way in which threads in the program interleave during execution. As a result, \neven well-tested concurrent programs contain subtle bugs that may not be discov\u00adered until long after \ndeployment. Data races [28] are one of the common sources of bugs in shared-memory, multi-threaded pro\u00adgrams. \nA data race happens when multiple threads perform con\u00ad.icting data accesses without proper synchronization. \nThe effects of a data race range from subtle memory corruption issues to un\u00adexpected memory model effects \nof the underlying compiler [23, 6] and hardware [1]. Over the last couple of decades, several static \nand dynamic techniques have been developed to automatically .nd data races Permission to make digital \nor hard copies of all or part of this work for personal or classroom use is granted without fee provided \nthat copies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. PLDI 09, June 15 20, 2009, Dublin, Ireland. \nCopyright c &#38;#169; 2009 ACM 978-1-60558-392-1/09/06. . . $5.00 in a multi-threaded program. Static \ntechniques [7, 17, 33, 37, 19, 34, 16, 40, 27] provide maximum coverage by reasoning about data races \non all execution paths. However, they tend to make conservative assumptions that lead to a large number \nof false data races. On the other hand, dynamic techniques [38, 30, 43, 15] are more precise than static \ntools, but their coverage is limited to the paths and thread interleavings explored at runtime. In practice, \nthe coverage of dynamic tools can be increased by running more tests. A severe limitation of dynamic \ndata races detectors is their run\u00adtime overhead. Data race detectors like RaceTrack [43] that are implemented \nas part of a managed runtime system, incur about 2x to 3x slowdown. Data race detectors for unmanaged \nprograms such as Intel s Thread Checker [36], incur performance overhead on the order of 200x. Such a \nlarge performance overhead prevents the wide-scale adoption of dynamic data-race detectors in practice. \nFirst, such a severe overhead dramatically reduces the amount of testing that can be done for a given \namount of resources. More im\u00adportantly, programmers and testers shy away from using intrusive tools that \ndo not allow them to test realistic program executions. The main reason for this very large performance \noverhead is that dynamic data-race detection requires analyzing every memory operation executed by the \nprogram. In this paper, we propose to use sampling to address this issue. By processing only a small \npercentage of memory accesses, a sampling-based approach can signi.cantly reduce the runtime overhead \nof data-race detection. At the outset, a sampling-based data-race detector may seem un\u00adlikely to succeed. \nMost memory accesses do not participate in data races. Sampling approaches, in general, have dif.culty \ncapturing such rare events. To make matters worse, a data race results from two con.icting accesses. \nThus, the sampler has to capture both of the accesses in order to detect the data race. Due to the multiplica\u00adtive \neffect of sampling probabilities, a naive sampling algorithm will fail to detect most of the data races. \nWe present a sampling algorithm for effective data-race detec\u00adtion. The sampling algorithm is based on \nthe cold-region hypoth\u00adesis that data races are likely to occur when a thread is executing a cold (infrequently \naccessed) region in the program. Data races that occur in hot regions of well-tested programs either \nhave al\u00adready been found and .xed, or are likely to be benign. Our adaptive sampler starts off by sampling \nall the code regions at 100% sam\u00adpling rate. But every time a code region is sampled, its sampling rate \nis progressively reduced until it reaches a lower bound. Thus, cold regions are sampled at a very high \nrate, while the sampling rate for hot regions is adaptively reduced to a very small value. In this way, \nthe adaptive sampler avoids slowing down the performance\u00adcritical hot regions of a program. This paper \ndescribes an implementation of the proposed sam\u00adpling algorithm in a tool called LiteRace, and demonstrates \nits ef\u00adfectiveness on a wide range of programs. LiteRace is implemented using the Phoenix compiler [24] \nto statically rewrite (x86) pro\u00adgram binaries. For every function, LiteRace produces an instru\u00admented \ncopy of the function that logs all memory accesses and synchronization operations. In addition, LiteRace \nadds a check be\u00adfore every function entry. This dynamic check switches the execu\u00adtion between the uninstrumented \nand instrumented functions based on sampling information which is maintained for each thread. Our sampling \ntechnique is an extension of the adaptive pro.ling tech\u00adnique used in SWAT [18] for detecting memory \nleaks. The key dif\u00adference is that our sampler needs to be thread-aware . We want to avoid the situation \nwhere a code region that becomes hot due to repeated execution by a certain thread is not sampled when \nan\u00adother concurrent thread executes it for the .rst time. To accom\u00adplish this, LiteRace maintains separate \npro.ling information for each thread.To our knowledge, LiteRace is the .rst data-race de\u00adtection tool \nthat uses sampling to reduce the runtime performance cost.  This paper describes many of the challenges \nand trade-offs in\u00advolved in building a tool like LiteRace. One of our key require\u00adments for LiteRace \nis that it never report a false data race. Data races, like many concurrency bugs, are very hard to debug. \nWe deemed it unacceptable for the users of the tool to spend lots of time triaging false error reports. \nThus, although LiteRace samples memory accesses, it still captures all the synchronizations in the program. \nThis is necessary to ensure that there are no false posi\u00adtives, as is explained in Section 3.2. By sampling \nonly a portion of the memory accesses, LiteR\u00adace is able to reduce the cost of logging meta-data about \nmem\u00adory accesses, which can easily become a performance bottleneck in dynamic race detectors. The log \nof the sampled memory accesses and all of the synchronization operations can be consumed either by an \nonline data race detector executing concurrently on a spare processor-core in a many-core processor, \nor by an of.ine data race detector. In this paper, we focus on the latter. The of.ine data race detector \ncould be either a happens-before based [21] or a lockset based detector [38]. We chose to use happens-before \nbased detec\u00adtion since it avoids reporting false races. This papers makes the following contributions: \n We demonstrate that the technique of sampling can be used to signi.cantly reduce the runtime overhead \nof a data race detector without introducing any additional false positives. LiteRace is the .rst data-race \ndetection tool that uses sampling to reduce the runtime performance cost. As LiteRace permits users to \nadjust the sampling rate to provide a bound on the performance overhead, we expect that such a sampling-based \napproach will encourage users to enable data race detection even during beta\u00adtesting of industrial applications. \n We discuss several sampling strategies. We show that a naive random sampler is inadequate for maintaining \na high detection rate while using a low sampling rate. We propose a more effec\u00adtive adaptive sampler \nthat heavily samples the .rst few execu\u00adtions of a function in each thread.  We implemented LiteRace \nusing the Phoenix compiler, and used it to analyze Microsoft programs such as ConcRT and Dryad, open-source \napplications such as Apache and Firefox, and two synchronization-heavy micro-benchmarks. The results \nshow that by logging less than 2% of memory operations, we can detect more than 70% of data races in \na particular execution.  The rest of this paper is organized as follows. In Section 2 we review happens-before \ndata race detection, and the reasons for its high runtime overhead. Section 3 presents an overview of \nour sampling based approach to reduce the runtime cost of a data race detector. Section 4 details the \nimplementation of our race detector. We present our experimental results in Section 5. In Section 6 we \nTime Thread 1 Thread 2 Thread 1 Thread 2  data race on X! write X lock L write X unlock L unlock L \n Figure 1. Examples of properly and improperly synchronized ac\u00adcesses to a memory location X. Edges \nbetween nodes represent a happens-before relationship. There is no data race for the example on the left, \nbecause there is a happens-before relation (due to un\u00adlock and lock operations) between the two writes \nto the location X. However, for the example on the right, there is no happens-before relation between \nthe two writes. Thus, it has a data race. describe related work and position our contributions. We brie.y \ndiscuss future work in Section 7 and conclude in Section 8. 2. Background Dynamic data race detectors \n[38, 43] incur a high runtime overhead and we seek to address this problem in this paper. Dynamic data \nrace detectors can be classi.ed into two major categories: happens\u00adbefore based and lockset based. Happens-before \ndata race detec\u00adtors [21, 11] .nd only the data races that manifest in a given pro\u00adgram execution. Lockset \nbased techniques [38] can predict data races that have not manifested in a given program execution, but \ncan report false positives. In this work, we focus on happens-before based data race detectors as they \ndo not report any false positives. However, our approach to sampling could equally well be applied to \na lockset-based algorithm. In this section, we review how happens-before race detection works and the \nreasons for the runtime overhead of a happens-before data race detector. 2.1 Happens-Before Race Detection \nWe provide a brief review of detecting data races by using the happens-before relation on program events. \nThe happens-before re\u00adlation, -., is a partial order on the events of a particular execution of a multi-threaded \nprogram. It can be de.ned by the following rules: (HB1) a-.b if a and b are events from the same sequential \nthread of execution and a executed before b. (HB2) a-.b if a and b are synchronization operations from \ndif\u00ad ferent threads such that the semantics of the synchronization dictates that a precedes b. (HB3) \nThe relation is transitive, so if a-.b and b-.c,then a-.c. We can then de.ne a data race as a pair of \naccesses to the same memory location, where at least one of the accesses is a write, and neither one \nhappens-before the other. In addition to being precise, another advantage that happens-before race detection \nhas over the lockset-based approach is that it supports a wide range of synchronization paradigms, not \njust mutual exclusion locks. For instance, our formulation of the second rule for de.ning happens\u00adbefore \nallows us to introduce a happens-before ordering between a call to fork in a parent thread and the .rst \nevent in the forked child thread. Figure 1 shows how the happens-before relationship is used to .nd data \nraces. The edges between instructions indicate a happens\u00adbefore relationship derived using rule HB1 or \nHB2. Transitively, by HB3, if there is a path between any two nodes, then there is a happens-before relationship \nbetween the two nodes. The example on the left in Figure 1 shows two properly synchronized accesses to \na shared memory location. Since the two writes have a path between them, they do not race with each other. \nIn the example shown on the right in Figure 1, thread 2 accesses a shared memory location without proper \nsynchronization. Because there is no path between the two writes, the two writes are involved in a data \nrace.  2.2 Sources of Runtime Overhead There are two primary sources of overhead for a happens-before \ndynamic data race detector. One, it needs to instrument all the mem\u00adory operations and all the synchronizations \noperations executed by the application. This results in a high performance cost due to the increase in \nthe number of additional instructions executed at run\u00adtime. Two, it needs to maintain meta-data for each \nmemory location accessed by the application. Most of the happens-before based al\u00adgorithms [21, 28, 2, \n9, 10, 12, 11, 39, 31, 35, 26] use vector clocks to keep track of the times of all the memory operations \nalong with the address of the locations they accessed. Maintaining such meta\u00addata further slows down \nthe program execution due to increased memory cost. 3. LiteRace Overview This section presents a high-level \noverview of LiteRace. The im\u00adplementation details together with various design trade-offs are dis\u00adcussed \nin Section 4. LiteRace has two key goals. First, LiteRace should not add too much runtime overhead during \ndynamic data-race detection. Our eventual goal is to run LiteRace during beta-testing of industrial applications. \nProhibitive slowdown of existing detectors limits the amount of testing that can be done for a given \namount of resources. Also, users shy away from intrusive tools that do not allow them to test realistic \nprogram executions. Second, LiteRace should never report a false data race. Data races are very dif.cult \nto debug and triage. False positives severely limit the usability of a tool from a developer s perspective. \nThis second goal has in.uenced many of our design decisions in LiteRace. 3.1 Case for Sampling The key \npremise behind LiteRace is that sampling techniques can be effective for data-race detection. While a \nsampling approach has the advantage of reducing the runtime overhead, the main trade-off is that it can \nmiss data races. We argue that this trade-off is accept\u00adable for the following reasons. First, dynamic \ntechniques cannot .nd all data races in the program anyway. They can only .nd data races on thread interleavings \nand paths explored at runtime. Fur\u00adthermore, a sampling-based detector, with its low overhead, would \nencourage users to widely deploy it on many more executions of the program, possibly achieving better \ncoverage. Another key advantage is that sampling techniques provide a useful knob that allow users to \ntrade runtime overhead for coverage. For instance, users can increase the sampling rate for interactive \napplications that spend most of their time waiting for user inputs. In such cases, the overhead of data-race \ndetection is likely to be masked by the I/O latency of the application. 3.2 Events to Sample Data-race \ndetection requires logging the following events at run\u00adtime. Synchronization operations along with a \nlogical timestamp that re.ects the happens-before relation between these operations. time thread 1 thread \n2 lock L false data race reported on X! unlock L Not Logged unlock L   Figure 2. Failing to log a \nsynchronization operation results in loss of happens-before edges. As a result, a false data race on \nX would be reported. Reads and writes to memory are logged in the program order, logically happening \nat the timestamp of the preceding synchro\u00adnization operation of the same thread. These logs can then \nbe analyzed of.ine or during program exe\u00adcution (\u00a74.4). The above information allows a data-race detector \nto construct the happens-before ordering between synchronization op\u00aderations and the memory operations \nexecuted in different threads. A data race is detected if there is no synchronization ordering be\u00adtween \ntwo accesses to the same memory location, and at least one of them is a write. Clearly instrumenting \ncode to log every memory access would impose a signi.cant overhead. By sampling only a fraction of these \nevents we can reduce the overhead in two ways. First, the execution of the program is much faster because \nof the reduced instrumentation. Second, the of.ine data-race detection algorithm needs to process fewer \nevents making it faster as well. While we seek to reduce the runtime overhead using sam\u00adpling, we must \nbe careful in choosing which events to log and which events not to log. In particular, we have to log \nall the syn\u00adchronization events in order to avoid reporting false data races. Figure 2 shows why this \nis the case. Synchronization operations induce happens-before orderings between program events. Any missed \nsynchronization operation can result in missing edges in the happens-before graph. The data-race detection \nalgorithm will therefore incorrectly report false races on accesses that are oth\u00aderwise ordered by the \nunlogged synchronization operations. To avoid such false positives, it is necessary to log all synchroniza\u00adtion \noperations. However, for most applications, the number of synchronization operations is small compared \nto the number of in\u00adstructions executed in a program. Thus, logging all synchronization operations does \nnot cause signi.cant performance overhead. We can, however, selectively sample the memory accesses. If \nwe choose not to log a particular memory access, we risk missing a data race involving that access (a \nfalse negative). As we discussed in Section 3.1, this is an acceptable trade-off. But, a good strategy \nfor selecting which memory accesses to log is essential in order not to miss too many races. A data race \ninvolves two accesses and a sampler needs to successfully log both of them to detect the race. We describe \na sampler that accomplishes this below.  3.3 Sampler Granularity In this paper, we treat every function \nas a code region. Our static instrumentation tool creates two copies for each function as shown in Figure \n3. The instrumented function logs all the memory op\u00aderations (their addresses and program counter values) \nand synchro\u00adnization operations (memory addresses of the synchronization vari\u00adables along with their \ntimestamps) executed in the function. The un-instrumented copy of the function logs only the synchronization \noperations. Before entering a function, the sampler (represented as dispatch check in Figure 3) is executed. \nBased on the decision of the sampler, either the instrumented copy or the un-instrumented copy of the \nfunction is executed. As the dispatch check happens once per function call, we have to ensure that the \ndispatch code is as ef.cient as possible.  3.4 Thread Local Adaptive Bursty Sampler There are two requirements \nfor a sampling strategy. Ideally, a sampling strategy should maintain a high data-race detection rate \neven with a low sampling rate. Also, it should enable an ef.cient implementation of the dispatch check \nthat determines if a function should be sampled or not. A naive random sampler does not meet these requirements \nas we show in Section 5. Our sampler is an extension of the adaptive bursty sampler [18], previously \nshown to be successful for detecting memory leaks. An adaptive bursty sampler starts off by analyzing \na code region at a 100% sampling rate, which means that the sampler always runs the instrumented copy \nof a code region the .rst time it is executed. Since the sampler is bursty, when it chooses to run the \ninstrumented copy of a region, it does so for several consecutive executions. The sampler is adaptive \nin that after each bursty sample, a code region s sampling rate is decreased until it reaches a lower \nbound. To make the adaptive bursty sampler effective for data-race de\u00adtection, we extend the above algorithm \nby making it thread local . The rationale is that, at least in reasonably well-tested programs, data \nraces occur when a thread executes a cold region. Data-races between two hot paths are unlikely either \nsuch a data race is al\u00adready found during testing and .xed, or it is likely to be a benign or intentional \ndata race. In a global adaptive bursty sampler [18], a particular code region can be considered hot even \nwhen a thread executes it for the .rst time. This happens when other threads have already executed the \nregion many times. We avoid this in LiteR\u00adace by maintaining separate sampling information for each thread, \neffectively creating a thread local adaptive bursty sampler. Our experiments (\u00a75) show that this extension \nsigni.cantly improves the effectiveness of LiteRace. Note that a thread-local adaptive sampler can also \n.nd some data races that occur between two hot regions or between a hot and a cold region. The reason \nis that our adaptive sampler initially assumes that all the regions are cold, and the initial sampling \nrate for every region is set to 100%. Also, the sampling rate for a region is never reduced below a lower \nbound. As a result, our sampler, even while operating at a lower sampling rate, might still be able to \ngather enough samples for a frequently executed hot region. Because of these two aspects of our adaptive \nsampler we .nd some, but not all, data races between hot-hot regions and hot-cold regions in a program. \n4. LiteRace Implementation This section describes the implementation details of LiteRace. 4.1 Instrumenting \nthe Code LiteRace is based on static instrumentation of x86 binaries and does not require the source \ncode of the program. We use the Phoenix [24] compiler and analysis framework to parse the x86 executables \nand perform the transformation depicted in Figure 3. LiteRace creates two versions for each function: \nan instrumented version that logs all the memory operations and an uninstrumented version that does not \nlog any memory operation. As explained in Section 3, avoiding false positives requires instrumenting \nboth the instrumented and the uninstrumented versions to log synchronization operations. Then, LiteRace \ninserts a dispatch check at every function entry. This   When cold Figure 3. LiteRace Instrumentation. \nSynchronization Op SyncVar Add l Sync? Lock / Unlock Lock Object Address No Wait / Notify Event Handle \nNo Fork / Join Child Thread Id No Atomic Machine Ops Target Memory Addr. Yes Table 1. Logging synchronization \noperations. check decides which of the two versions to invoke for a particular call of the function at \nruntime. In contrast to prior adaptive sampling techniques [18], LiteR\u00adace maintains pro.ling information \nper thread. For each thread, LiteRace maintains a buffer in the thread-local storage that is al\u00adlocated \nwhen the thread is created. This buffer contains two coun\u00adters for each instrumented function: the frequency \ncounter and the sampling counter. The frequency counter keeps track of the num\u00adber of times the thread \nhas executed a function and determines the sampling rate to be used for the function (a frequently executed \nfunction will be sampled at a lower sampling rate). The sampling counter is used to determine when to \nsample the function next. On function entry, the dispatch check decrements the sampling counter corresponding \nto that function. If the sampling counter s value is non-zero, which is the common case, the dispatch \ncheck invokes the uninstrumented version of the function. When the sampling counter reaches zero, the \ndispatch check invokes the instrumented version of the function, and sets the sampling counter to a new \nvalue based on the current sampling rate for the function as determined by the frequency counter. As \nthe dispatch check is executed on every function entry, it is important to keep the overhead of this \ncheck low. To avoid the overhead of calling standard APIs for accessing thread-local storage, LiteRace \nimplements an inlined version using the Thread Execution Block [25] structure maintained by the Windows \nOS for each thread. Also, the dispatch check uses a single register edx for its computation. The instrumentation \ntool analyzes the original binary for the function to check if this register and the eflags register \nare live at function entry, and injects code to save and restore these registers only when necessary. \nIn the common case, our dispatch check involves 8 instructions with 3 memory references and 1 branch \n(that is mostly not taken). We measure the runtime overhead of the dispatch check in Section 5.  4.2 \nTracking Happens-Before As mentioned earlier, avoiding false positives requires accurate happens-before \ndata. Ensuring that we correctly record the happens\u00adbefore relation for events of the same thread is \ntrivial since the log\u00adging code executes on the same thread as the events being recorded. Correctly capturing \nthe happens-before data induced by the syn\u00adchronization operations between threads in a particular program \nexecution requires more work. For each synchronization operation, LiteRace logs a SyncVar that uniquely \nidenti.es the synchronization object and a logical timestamp that identi.es the order in which threads \nperform op\u00aderations on that object. Table 1 shows how LiteRace determines the SyncVar for various synchronization \noperations. For instance, Lit\u00adeRace uses the address of the lock object as a SyncVar for lock and unlock \noperations. The logical timestamp in the log should ensure that if a and b are two operations on the \nsame SyncVar and a-.b then a has a smaller timestamp than b. The simplest way to implement the timestamp \nis to maintain a global counter that is atomically incremented at every synchronization operation. How\u00adever, \nthe contention introduced by this global counter can dramat\u00adically slowdown the performance of LiteRace-instrumented \npro\u00adgrams on multi-processors. To alleviate this problem, we use one of 128 counters uniquely determined \nby a hash of the SyncVar for the logical timestamp.  To ensure the accuracy of the happens-before relation, \nit is im\u00adportant that LiteRace computes and logs the logical timestamp atomically with the synchronization \noperation performed. For some kinds of synchronization, we are able to leverage the semantics of the \noperation to guarantee this. For instance, by logging and in\u00adcrementing the timestamp after a lock instruction \nand before an unlock instruction, we guarantee that an unlock operation on a particular mutex will have \na smaller timestamp than a subse\u00adquent lock operation on that same mutex in another thread. For wait/notify \noperations, LiteRace increments and logs the timestamp before the notify operation and after the wait \noperation to guaran\u00adtee consistent ordering. A similar technique is used for fork/join operations. For \nsome synchronization operations, however, LiteRace is forced to add additional synchronization to guarantee \natomic times\u00adtamping. For example, consider a target program that uses atomic compare-and-exchange instructions \nto implement its own locking. Since we don t know if a particular compare-and-exchange is act\u00ading as \na lock or as an unlock , we introduce a critical section to ensure that the compare-and-exchange and \nthe logging and in\u00adcrementing of the timestamp are all executed atomically. Without this, LiteRace can \ngenerate timestamps for these operations that are inconsistent with the actual order. Our experience \nshows that this additional effort is absolutely essential in practice and otherwise results in hundreds \nof false data races.  4.3 Handling Dynamic Allocation Another subtle issue is that a dynamic data-race \ndetector should ac\u00adcount for the reallocation of the same memory to a different thread. A naive detector \nmight report a data-race between accesses to the reallocated memory with accesses performed during a \nprior alloca\u00adtion. To avoid such false positives, LiteRace additionally monitors all memory allocation \nroutines and treats them as additional syn\u00adchronization performed on the memory page containing the allo\u00adcated \nor deleted memory.  4.4 Analyzing the Logs The LiteRace pro.ler generates a stream of logged events \nduring program execution. In our current implementation, we write these events to the disk and process \nthem of.ine for data races. Our main motivation for this design decision was to minimize perturbation \nof the runtime execution of the program. We are also currently investigating an online detector that \ncan avoid runtime slowdown by using an idle core in a many-core processor. The logged events are processed \nusing a standard implementa\u00adtion [36] of the happens-before based data-race detector described in Section \n2.1. We did not use a lock-set based data-race detection algorithm to avoid false positives. However, \nthe proposed sampling algorithms could be useful for lock-set based data-race detectors as well. Benchmarks \nDescription #Fns Bin. Size Dryad Library for distributed data\u00adparallel apps 4788 2.7 MB ConcRT .NET Concurrency \nruntime framework 1889 0.5 MB Apache 2.2.11 Web server 2178 0.6 MB Firefox 3.6a1pre Web browser 8192 \n1.3 MB Table 2. Benchmarks used. The number of functions and the bi\u00adnary size includes executable and \nany instrumented library .les. 5. Results In this section we present our experimental results. We begin \nby de\u00adscribing our benchmarks (\u00a75.1) and the samplers that we evaluate (\u00a75.2). In Section 5.3 we compare \nthe effectiveness of various sam\u00adplers in detecting data races. We show that our thread-local adaptive \nsampler achieves a high data-race detection rate, while maintaining a low sampling rate. Section 5.4 \ndiscusses the performance and log size overhead of thread-local adaptive sampler implemented in Lit\u00adeRace, \nand compares it to an implementation that logs all the mem\u00adory operations. All experiments were run on \na Windows Server 2003 system with two dual-core AMD Opteron processors and 4 GB of RAM. 5.1 Benchmarks \nWe selected the four industrial-scale concurrent programs listed in Table 2 as our benchmarks. Dryad \nis a distributed execution engine, which allows programmers to use a computing cluster or a data center \nfor running coarse-grained data-parallel applications [20]. The test harness we used for Dryad was provided \nby its lead de\u00adveloper. The test exercises the shared-memory channel library used for communication between \nthe computing nodes in Dryad. We ex\u00adperimented with two versions of Dryad, one with the standard C library \nstatically linked in (referred to as Dryad-stdlib), and the other without. For the former, LiteRace instruments \nall the stan\u00addard library functions called by Dryad. Our second benchmark, ConcRT, is a concurrent run-time \nlibrary that provides lightweight tasks and synchronization primitives for developing data-parallel applications. \nIt is part of the parallel extensions to the .NET frame\u00adwork [14]. We used two different test inputs \nfor ConcRT: Messag\u00ading, and Explicit Scheduling. These are part of the ConcRT concur\u00adrency test suite. \nWe use Apache, an open-source HTTP web server, as our third benchmark. We evaluate the overhead and effective\u00adness \nof LiteRace over two different inputs for Apache (referred to as Apache-1 and Apache-2). The .rst consists \nof a mixed work\u00adload of 3000 requests for a small static web page, 3000 requests for a larger web page, \nand 1000 CGI requests. The second con\u00adsists solely of 10,000 requests for a small static web page. For \nboth workloads, up to 30 concurrent client connections are generated by Apache s benchmarking tool. Our \n.nal benchmark is Firefox, the popular open-source web browser. We measure the overhead and sampler effectiveness \nfor the initial browser start-up (Firefox-Start) and for rendering an html page consisting of 2500 positioned \nDIVs (Firefox-Render).  5.2 Evaluated Samplers The samplers that we evaluate are listed in Table 3. \nThe Short Name column shows the abbreviation we will use for the samplers in the .gures throughout the \nrest of this section. The table also shows the effective sampling rate for each sampler. The effective \nsampling rate is the percentage of memory operations that are logged by a sampler. Two averages for effective \nsampling rate are Table 3. Samplers evaluated along with their short names used in .gures, short descriptions, \nand effective sampling rates averaged over the benchmarks studied. The weighted average uses the number \nof memory accesses in each benchmark application as a weight.  Sampling Strategy Short Name Description \nWeighted Average ESR Average ESR Thread-local Adaptive TL-Ad Adaptive back-off per function / per thread \n(100%,10%,1%,0.1%); bursty 1.8% 8.2% Thread-local Fixed 5% TL-Fx Fixed 5% per function / per thread; \nbursty 5.2% 11.5% Global Adaptive G-Ad Adaptive back-off per function globally (100%, 50%, 25%, ... , \n0.1%); bursty 1.3% 2.9% Global Fixed G-Fx Fixed 10% per function globally; bursty 10.0% 10.3% Random \n10% Rnd10 Random 10% of dynamic calls chosen for sampling 9.9% 9.6% Random 25% Rnd25 Random 25% of dynamic \ncalls chosen for sampling 24.8% 24.0% Un-Cold Region UCP First 10 calls per function / per thread are \nNOT sam\u00adpled, all remaining calls are sampled 98.9% 92.3% TL-Ad TL-Fx G-Ad G-Fx Rnd10 Rnd25 UCP  Dryad \nChannel Dryad channel ConcRT ConcRT Apache 1 Apache 2 Firefox Start Firefox Render Average Weighted Avg \n+ stdlib Messaging Explicit Eff Sampling Scheduling Rate Figure 4. Proportion of static data races found \nby various samplers. The .gure also shows the weighted average effective sampling rate for each sampler, \nwhich is the percentage of memory operations logged (averaged over all the benchmarks). TL-Ad TL-Fx G-Ad \nG-Fx Rnd10 Rnd25 UCP TL-Ad TL-Fx G-Ad G-Fx Rnd10 Rnd25 UCP 100% Rare Data Race Detection Rate 90% 80% \n70% 60% 50% 40% 30% 20% 10% Frequent Data Race Detection Rate 100% 90% 80% 70% 60% 50% 40% 30% 20% 10% \n0% 0% Dryad Dryad Apache 1 Apache 2 Firefox Firefox Average Dryad Dryad Apache 1 Apache 2 Firefox Firefox \nAverage Channel + channel Start Render Channel + channel Start Render stdlib stdlib Figure 5. Various \nsamplers detection rate for rare (on the left) and frequent (on the right) static data races. shown. \nOne is just the average of the effective sampling rates over the nine benchmark-input pairs described \nin Section 5.1. The other is the weighted average, where the weight for a benchmark-input pair is based \non the number of memory operations executed at runtime. LiteRace s thread-local adaptive sampler is the \n.rst one listed in the table. For each thread and for each function, this sampler starts with a 100% \nsampling rate and then progressively reduces the sampling rate until it reaches a base sampling rate \nof 0.1%. To understand the utility of this adaptive back-off, we evaluate a thread-local .xed sampler, \nwhich uses a .xed 5% sampling rate per function per thread. The next two samplers are global versions \nof the two samplers that we just described. The adaptive back-off for the global sampler is based on \nthe number of executions of a function, irrespective of the calling thread. This global adaptive sampler \nis similar to the one used in SWAT [18], except that we use a higher sampling rate. Even with a higher \nrate, our experiments show that the global samplers are not as effective as the thread-local samplers \nin .nding data races. The four samplers mentioned thus far are bursty . That is, when they decide to \nsample a function, they do so for ten consecutive executions of that function. The next two samplers \nare based on random sampling and are not bursty. Each function call is randomly sampled based on the \nchosen sampling rate (10% and 25%). The .nal sampler evaluates our cold\u00adregion hypothesis by logging \nonly the uncold regions. That is, it logs all but the .rst ten calls of a function per thread.  5.3 \nEffectiveness of Samplers Comparison In this section, we compare different samplers and show that the \nthread-local adaptive sampler is the most effective of all the sam\u00adplers we evaluated. For our evaluation, \nwe group each data race detected by our tool based on the pair of instructions (identi.ed by the value \nof the program counter) that participate in the data race. We call each group a static data-race. From \nthe user s perspective, a static data-race roughly corresponds to a possible synchronization error in \nthe program. Table 4 shows the number of static data-races that LiteRace .nds for each benchmark-input \npair. The table also distinguishes between rare and frequent static data-races, based on the number of \ntimes a particular static data-race manifests at run\u00adtime.  To have a fair comparison, different samplers \nneed to be eval\u00aduated on the same thread interleaving of a program. However, two different executions \nof a multi-threaded program are not guaran\u00adteed to yield the same interleaving even if the input is the \nsame. To compare the effectiveness of the various samplers in detecting data races accurately, we created \na modi.ed version of LiteRace that performs full logging, where all synchronization and all mem\u00adory operations \nare logged. In addition to full logging, upon function entry, we execute the dispatch check logic for \neach of the sam\u00adplers we wish to compare. We then mark in the log whether or not each of the samplers \nwould have logged a particular memory oper\u00adation. By performing data-race detection on the complete log, \nwe .nd all the data races that happened during the program s execution. We can then perform data-race \ndetection on the subset of the mem\u00adory operations that a particular sampler would have logged. Then, \nby comparing the results with those from the complete log, we are able to calculate the detection rate, \nwhich is the proportion of data races detected by each of the samplers. When we analyze the per\u00adformance \nand space overhead in Section 5.4, however, we experi\u00adment with only LiteRace s thread-local adaptive \nsampler turned on. Each application was instrumented using our modi.ed version of LiteRace described \nabove. We ran the instrumented application three times for each benchmark. The detection rate we report \nfor each benchmark is the average of the three runs. The results for overall data-race detection rate \nare shown in Figure 4. The results are grouped by benchmarks with a bar for each sampler within each \ngroup. The weighted average effective sampling rate for each of the samplers (discussed in Section 5.2) \nis also shown as the last group. A sampler is effective if it has a very low effective sampling rate \nalong with a high data-race detection rate. Notice that the proposed LiteRace sampler (TL-Ad) achieves \nthis, as it detects about 70% of all data-races by sampling only 1.8% of all memory operations. The non-adaptive \n.xed rate thread-local sampler also detects about 72% of data-races, but its effective sampling rate \nis 5.2% (more than 2.5x higher than the TL-Ad sampler). Clearly, among the thread-local samplers, the \nadaptive sampler is better than the .xed rate sampler. The two thread-local samplers outperform the two \nglobal sam\u00adplers. Though the global adaptive sampler logs only 1.3% of mem\u00adory operations (comparable \nto our thread-local adaptive sampler), it detects only about 22.7% of all data-races (about 3x worse \nthan TL-Ad). The global .xed rate sampler logs 10% of memory opera\u00adtions, and still detects only 48% \nof all data-races. All the four samplers based on cold-region hypothesis are bet\u00adter than the two random \nsamplers. For instance, a random sampler .nds only 24% of data-races, but logs 9.9% of all memory opera\u00adtions. \nAnother notable result from the .gure is that of the Un-Cold Region sampler, which logs all the memory \noperations except those executed in the cold-regions (\u00a75.2). It detects only 32% of all data-races, but \nlogs nearly 99% of all memory operations. This result validates our cold-region hypothesis. 5.3.1 Rare \nVersus Frequent Data Race Detection We have so far demonstrated that a thread-local adaptive sampler \n.nds about 70% of all static data-races. If a static data-race occurs frequently during an execution, \nthen it is likely that many sampling strategies would .nd it. It is more challenging to .nd data races \nthat occur rarely at run-time. To quantify this, we classi.ed all of the Table 4. Number of static data-races \nfound for each benchmark\u00adinput pair (median over three dynamic executions), while logging all the memory \noperations. These static data-races are classi.ed into rare and frequent categories. A static data-race \nis rare, if it is detected less than 3 times per million non-stack memory instruc\u00adtions during any execution \nof the program. Benchmarks # races found #Rare #Freq Dryad Channel + stdlib 19 17 2 Dryad Channel 8 3 \n5 Apache-1 17 8 9 Apache-2 16 9 7 Firefox Start 12 5 7 Firefox Render 16 10 6 static data races that \nwere detected (using the full, unsampled log) based on the number of times that a static data-race occurs \nin an execution. We classi.ed as rare those racing instruction pairs that occurred fewer than 3 times \nfor each million non-stack memory in\u00adstructions executed. The rest are considered frequent. The number \nof rare and frequent data races for each benchmark-input pair is shown in Table 4 (some of the data races \nfound could be benign). The various samplers data-race detection rates for these two cate\u00adgories are \nshown in Figure 5. Most of the samplers perform well for the frequent data races. But, for infrequently \noccurring data races, the thread-local samplers are the clear winners. Note that the random sampler .nds \nvery few rare data races.  5.4 Analysis of Overhead In Section 5.3 we presented results showing that \nthe thread-local adaptive sampler performs well in detecting data-races for a low sampling rate. Here \nwe present the performance and log size over\u00adhead of thread-local adaptive sampler implemented in LiteRace. \nWe show that, on average, it incurs about 28% performance over\u00adhead for our benchmarks when compared \nto no logging, and is up to 25 times faster than an implementation that logs all the memory operations. \nApart from the benchmarks used in Section 5.3, we used two ad\u00additional compute and synchronization intensive \nmicro-benchmarks for our performance study. LKRHash is an ef.cient hash table im\u00adplementation that uses \na combination of lock-free techniques and high-level synchronizations. LFList is an implementation of \na lock\u00adfree linked list available from [22]. LKRHash and LFList exe\u00adcute synchronization operations more \nfrequently than the other real world benchmarks we studied. These micro-benchmarks are in\u00adtended to test \nLiteRace s performance in the adverse circumstance of having to log many synchronization operations. \nTo measure the performance overhead, we ran each of the benchmarks ten times for each of four different \ncon.gurations. The .rst con.guration is the baseline, uninstrumented application. Each of the remaining \nthree con.gurations adds a different portion of Lit\u00adeRace s instrumentation overhead: the .rst adds just \nthe dispatch check, the second adds the logging of synchronization operations, and the .nal con.guration \nis the complete LiteRace instrumenta\u00adtion including the logging of the sampled memory operations. By \nrunning the benchmarks in all of these con.gurations we were able to measure the overhead of the different \ncomponents in LiteRace. Figure 6 shows the cost of using LiteRace on the various bench\u00admarks and micro-benchmarks. \nThe bottom portion of each verti\u00adcal bar in Figure 6 represents the time it takes to run the unin\u00adstrumented \napplication (baseline). The overhead incurred by the various components of LiteRace are stacked on top \nof that. As expected, the synchronization intensive micro-benchmarks exhibit Table 5. Performance overhead \nof LiteRace s thread-local adaptive sampler and full logging implementation when compared to the execution \ntime of the uninstrumented application. Log size overhead in terms of MB/s is also shown.  Benchmarks \nBaseline Exec Time LiteRace Slowdown Full Logging Slowdown LiteRace Log Size (MB/s) Full Logging Log \nSize (MB/s) LKRHash 3.3s 2.4x 14.7x 154.5 1936.3 LFList 1.7s 2.1x 16.1x 92.5 751.7 Dryad+stdlib 6.7s \n1x 1.8x 1.2 12.8 Dryad 6.6s 1x 1.14x 1.1 2.6 ConcRT Messaging 9.3s 1.03x 1.08x 0.7 10.6 ConcRT Explicit \nScheduling 11.5s 2.4x 9.1x 4.6 109.7 Apache-1 17.0s 1.02x 1.4x 1.2 41.9 Apache-2 3.0s 1.04x 3.2x 4.0 \n260.7 Firefox Start 1.8s 1.44x 8.89x 7.4 107.0 Firefox Render 0.61s 1.3x 33.5x 19.8 731.1 Average 6.15s \n1.47x 9.09x 28.6 396.5 Average (w/o Microbench) 7.06s 1.28x 7.51x 5.0 159.6 Performance Overhead 3 2.5 \n2 1.5 1 0.5 0  Figure 6. LiteRace slowdown over the uninstrumented applica\u00adtion. the highest overhead, \nbetween 2x and 2.5x, since we must log all synchronization operations to avoid false positives. The ConcRT \nScheduling test also has a high proportion of synchronization oper\u00adations and exhibits overhead similar \nto the micro-benchmarks. The more realistic application benchmarks show modest performance overhead of \n0% for Dryad, 2%to 4% for Apache, and 30%to 44% for Firefox. In order to evaluate the importance of sampling \nmemory oper\u00adations in order to achieve low overhead, we measured the perfor\u00admance of logging all the \nsynchronization and memory accesses. Unlike the LiteRace implementation, this full-logging implemen\u00adtation \ndid not have the overhead for any dispatch checks or cloned code. Table 5 compares the slowdown caused \nby LiteRace to the slowdown caused by full logging. The sizes of the log .les gen\u00aderated for these two \nimplementations are also shown in terms of MB/s. LiteRace performs better than full logging in all cases. \nThe performance overhead over baseline when averaged over realistic benchmarks is 28% for the LiteRace \nimplementation, while the full logging implementation incurs about 7.5x performance overhead. The generated \nlogs, as expected, are also much smaller in Lit\u00adeRace. On average, LiteRace generated logs at the rate \nof 5.0 MB/s, whereas a full logging implementation generated about 159.6 MB/s. 6. Related Work In this \nsection we discuss prior work in two areas related to this paper: data race detectors and samplers. \n6.1 Data Race Detection Prior data race detection can be broadly classi.ed into static and dynamic techniques. \nStatic techniques include those that use type\u00adbased analysis [7, 17, 33, 37] or data-.ow analysis [40, \n16, 27, 42] to ensure that all data accesses are consistently protected by locks. Many of these techniques \nare scalable and most are complete in that they .nd all data races in a program. The downside is that \nstatic techniques are inherently imprecise and typically report a large number of false data races that \nplace a tremendous burden on the user of the tool. More importantly, these techniques are not able to \nhandle synchronizations other than locks, such as events, semaphores, and I/O completion ports common \nin many systems programs. Thus, data accesses that are synchronized through these mechanisms will be \nfalsely reported as potential data races. Model checking techniques [19, 34] are capable of handling \nsuch syn\u00adchronizations, but are not scalable due to the complexity of their analysis. A dynamic tool, \nsuch as LiteRace, does not suffer these problems. Dynamic analysis techniques are either lockset based \n[38, 41, 29] or happens-before based [21, 28, 2, 9, 10, 12, 11, 39, 31, 35, 26] or a hybrid of the two \n[13, 43, 30, 32, 15]. Dynamic techniques are scalable to applications with large code bases and are also \nmore precise than static tools as they analyze an actual execution of a program. The downside is that \nthey have much less coverage of data races (false negatives), as they only examine the dynamic path of \none execution the program. However, the number of false negatives can be reduced by increasing the number \nof tests. One of the main limitations of a dynamic data race detection tool is its high run-time overhead, \nwhich perturbs the execution behavior of the application. Apart from consuming users time, a heavy-weight \ndata race detector is not useful for .nding bugs that would manifest in a realistic execution of an application. \nThere have been attempts to ameliorate the performance cost of dynamic analysis using static optimizations \nfor programs written in strongly typed languages [8]. Dynamic data race detectors for managed code [43] \nalso have the advantage that the runtime system already incurs the cost of maintaining meta-data for \nthe objects, which they make use of. For unmanaged code like C and C++, however, the runtime performance \noverhead of data race detection remains high. Intel s ThreadChecker [36], for example, incurs about 200x \noverhead to .nd data races. In this paper, we propose an ef.cient sampling mechanism that pays the cost \nfor logging only a small fraction of the program execution, but is effective in detecting a majority \nof the data races. Unlike existing data race detectors, it also gives the user an ability to tradeoff \nperformance cost with coverage (number of false negatives).  6.2 Sampling Techniques for Dynamic Analysis \nArnold et al. [4] proposed sampling techniques to reduce the over\u00adhead of instrumentation code in collecting \npro.les for feedback di\u00adrected optimizations. Chilimbi and Hauswirth proposed an adaptive sampler for \n.nding memory leaks [18]. We extend their solution to the sampling of multi-threaded programs, and show \nthat samplers can be effectively used to .nd data races as well. QVM [3] is an extension to Java Virtual \nMachine that provides an interface to en\u00adable dynamic checking such as heap properties, local assertions, \nand typestate properties. It uses sampling to tradeoff accuracy with runtime overhead. The sampling technique \nused in QVM is object\u00adcentric, in that, all the events to a sampled object s instance are pro\u00ad.led. In \ncontrast, our samplers are based on cold-region hypothesis. 7. Future Work Our current LiteRace implementation \nsamples code regions at the granularity of functions (Section 3.3). While this approach works very well \nfor server applications like Apache, web browsers like Firefox, and highly concurrent programs like Dryad \nand ConcRT, it may not be the best possible implementation for computationally intensive scienti.c applications \nlike Parsec [5]. These application often have loops with high trip count. Therefore sampling at a loop\u00adlevel \ngranularity might help improve the ef.ciency of LiteRace for these applications. Of.ine pro.ling can \nbe used to identify loops with high trip count, which can then be instrumented to adaptively reduce the \nsampling rate of the loop within a single function exe\u00adcution. 8. Conclusions Multi-threaded programs \nare hard to understand and debug. Dy\u00adnamic data race detectors can automatically .nd concurrency bugs \nwith a very high accuracy, which would be of immense help to pro\u00adgrammers. However, a signi.cant impediment \nto their adoption is their runtime overhead. Programmers shy away from heavy-weight dynamic tools that \nprevent them from testing realistic executions of their application. Moreover, the high overhead of such \ntools dra\u00admatically reduces the amount of testing possible for a given amount of computing and time resources. \nThis paper argues for sampling-based techniques to ameliorate the runtime performance overhead of dynamic \ndata race detectors. We demonstrate that intelligent sampling can be effective in .nding data races with \nacceptable runtime overhead. Our best sampler, the thread local adaptive sampler, logs less than 2%ofmemory \naccesses but can detect more than 70% of data races. Another key advantage of a sampling-based technique \nis that it provides a knob in the form of sampling rate, which the program\u00admer can use to trade-off performance \nfor data-race coverage. Many testing tools never .nd acceptance in development teams because of their \nhigh runtime overhead. With such a knob, programmers would be able to specify the performance penalty \nthat they are will\u00ading to pay, and they would get coverage that is commensurate with this penalty. Acknowledgments \nWe would like to thank the anonymous reviewers for providing valuable feedback on this paper. We would \nalso like to thank Tr\u00adishul Chilimbi for helpful discussions. References [1] S. Adve and K. Gharachorloo. \nShared memory consistency models: A tutorial. Computer, 29(12):66 76, 1996. [2] S. V. Adve, M. D. Hill, \nB. P. Miller, and R. H. B. Netzer. Detecting data races on weak memory systems. In ISCA 91: Proceedings \nof the 18th Annual International Symposium on Computer architecture, 1991. [3] M. Arnold, M. Vechev, \nand E. Yahav. QVM: An ef.cient runtime for detecting defects in deployed systems. In OOPSLA 08: Proceedings \nof the 23rd ACM SIGPLAN conference on Object\u00adoriented programming systems languages and applications, \n2008. [4] Matthew Arnold and Barbara G. Ryder. A framework for reducing the cost of instrumented code. \nIn PLDI 01: Proceedings of the ACM SIGPLAN 2001 conference on Programming language design and implementation, \npages 168 179, 2001. [5] C. Bienia, S. Kumar, J. P. Singh, and K. Li. The PARSEC benchmark suite: Characterization \nand architectural implications. In Proceedings of the 17th International Conference on Parallel Architectures \nand Compilation Techniques, 2008. [6] Hans-Juergen Boehm and Sarita V. Adve. Foundations of the C++ concurrency \nmemory model. In PLDI 08: Proceedings of the 2008 ACM SIGPLAN conference on Programming language design \nand implementation, pages 68 78, 2008. [7] C. Boyapati, R. Lee, and M. Rinard. Ownership types for safe \nprogramming: Preventing data races and deadlocks. In OOPSLA 02: Proceedings of the 17th ACM SIGPLAN conference \non Object\u00adoriented programming, systems, languages, and applications, pages 211 230, 2002. [8] J. D. \nChoi, K. Lee, A. Loginov, R. O Callahan, V. Sarkar, and M. Srid\u00adharan. Ef.cient and precise datarace \ndetection for multithreaded object-oriented programs. In PLDI 02: Proceedings of the ACM SIGPLAN 2002 \nConference on Programming language design and implementation, pages 258 269, 2002. [9] J. D. Choi, B. \nP. Miller, and R. H. B. Netzer. Techniques for debugging parallel programs with .owback analysis. ACM \nTransactions on Programming Languages and Systems, 13(4):491 530, 1991. [10] M. Christiaens and K. De \nBosschere. TRaDe, a topological approach to on-the-.y race detection in java programs. In JVM 01: Proceedings \nof the Java Virtual Machine Rsearch and Technology Symposium, 2001. [11] J. M. Crummey. On-the-.y detection \nof data races for programs with nested fork-join parallelism. In Supercomputing 91: Proceedings of the \n1991 ACM/IEEE conference on Supercomputing, pages 24 33, 1991. [12] A. Dinning and E. Schonberg. An empirical \ncomparison of monitoring algorithms for access anomaly detection. In PPOPP 90: Proceedings of the second \nACM SIGPLAN symposium on Principles &#38; practice of parallel programming, pages 1 10, 1990. [13] A. \nDinning and E. Schonberg. Detecting access anomalies in programs with critical sections. In PADD 91: \nProceedings of the 1991 ACM/ONR workshop on Parallel and distributed debugging, pages 85 96, 1991. [14] \nJoe Duffy. A query language for data parallel programming: invited talk. In DAMP, page 50, 2007. [15] \nT. Elmas, S. Qadeer, and S. Tasiran. Goldilocks: A race and transaction-aware java runtime. In PLDI 07: \nProceedings of the 2007 ACM SIGPLAN conference on Programming language design and implementation, pages \n245 255, New York, NY, USA, 2007. ACM. [16] D. Engler and K. Ashcraft. RacerX: Effective, static detection \nof race conditions and deadlocks. In SOSP 03: Proceedings of the nineteenth ACM symposium on Operating \nsystems principles, pages 237 252, 2003. [17] C. Flanagan and S. N. Freund. Type-based race detection \nfor Java. In PLDI 00: Proceedings of the ACM SIGPLAN 2000 conference on  Programming language design \nand implementation, pages 219 232, 2000. [18] M. Hauswirth and T. M. Chilimbi. Low-overhead memory leak \ndetection using adaptive statistical pro.ling. In ASPLOS-XI: Proceedings of the 11th international conference \non Architectural support for programming languages and operating systems, pages 156 164, New York, NY, \nUSA, 2004. ACM. [19] T. A. Henzinger, R. Jhala, and R. Majumdar. Race checking by context inference. \nIn PLDI 04: Proceedings of the ACM SIGPLAN 2004 conference on Programming language design and implementation, \npages 1 13, 2004. [20] M. Isard, M. Budiu, Y. Yu, A. Birrell, and D. Fetterly. Dryad: Distributed data-parallel \nprograms from sequential building blocks. In Proceedings of the EuroSys Conference, pages 59 72, 2007. \n[21] L. Lamport. Time, clocks, and the ordering of events in a distributed system. Communications of \nthe ACM, 21(7):558 565, 1978. [22] Generic concurrent lock-free linked list http://www.cs.rpi.edu/ bushl2/project \nweb/page5.html. [23] J. Manson, W. Pugh, and S. Adve. The Java memory model. In Principles of Programming \nLanguages (POPL), 2005. [24] Microsoft. Phoenix compiler. http://research.microsoft.com/Phoenix/. [25] \nMicrosoft. Thread execution blocks. http://msdn.microsoft.com/en\u00adus/library/ms686708.aspx. [26] S. L. \nMin and J.-D. Choi. An ef.cient cache-based access anomaly detection scheme. In Proceedings of the 4th \nInternational Conference on Architectural Support for Programming Languages and Operating System (ASPLOS), \npages 235 244, 1991. [27] M. Naik, A. Aiken, and J. Whaley. Effective static race detection for Java. \nIn PLDI 06: Proceedings of the 2006 ACM SIGPLAN conference on Programming language design and implementation, \npages 308 319, 2006. [28] R. H. B. Netzer. Optimal tracing and replay for debugging shared\u00admemory parallel \nprograms. In Proceedings of the ACM/ONR Workshop on Parallel and Distributed Debugging, pages 1 11, 1993. \n[29] H. Nishiyama. Detecting data races using dynamic escape analysis based on read barrier. Third Virtual \nMachine Research &#38; Technology Symposium, pages 127 138, May 2004. [30] R. O Callahan and J. D. Choi. \nHybrid dynamic data race detection. In PPoPP 03: Proceedings of the ninth ACM SIGPLAN symposium on Principles \nand practice of parallel programming, pages 167 178, 2003. [31] D. Perkovic and P. J. Keleher. Online \ndata-race detection via coherency guarantees. In OSDI 96: Operating System Design and Implementation, \npages 47 57, 1996. [32] E. Pozniansky and A. Schuster. Ef.cient on-the-.y data race detection in multithreaded \nC++ programs. In PPoPP 03: Proceedings of the ninth ACM SIGPLAN symposium on Principles and practice \nof parallel programming, pages 179 190, 2003. [33] P. Pratikakis, J. S. Foster, and M. Hicks. LOCKSMITH: \nContext\u00adsensitive correlation analysis for race detection. In PLDI 06: Proceedings of the 2006 ACM SIGPLAN \nconference on Programming language design and implementation, pages 320 331, 2006. [34] S. Qadeer and \nD. Wu. KISS: Keep it simple and sequential. In PLDI 04: Proceedings of the ACM SIGPLAN 2004 Conference \non Programming Language Design and Implementation, pages 14 24, 2004. [35] M. Ronsse and K. de Bosschere. \nNon-intrusive on-the-.y data race detection using execution replay. In Proceedings of Automated and Algorithmic \nDebugging, Nov 2000. [36] P. Sack, B. E. Bliss, Z. Ma, P. Petersen, and J. Torrellas. Accurate and ef.cient \n.ltering for the Intel Thread Checker race detector. In ASID 06: Proceedings of the 1st workshop on Architectural \nand system support for improving software dependability, pages 34 41, 2006. [37] A. Sasturkar, R. Agarwal, \nL. Wang, and S. D. Stoller. Automated type-based analysis of data races and atomicity. In PPoPP 05: Proceedings \nof the tenth ACM SIGPLAN symposium on Principles and practice of parallel programming, pages 83 94, 2005. \n[38] S. Savage, M. Burrows, G. Nelson, P. Sobalvarro, and T. Anderson. Eraser: A dynamic data race detector \nfor multithreaded programs. ACM Transactions on Computer Systems, 15(4):391 411, 1997. [39] E. Schonberg. \nOn-the-.y detection of access anomalies. In Proceedings of the ACM SIGPLAN 89 Conference on Programming \nLanguage Design and Implementation (PLDI), 1989. [40] N. Sterling. WARLOCK -a static data race analysis \ntool. In Proceedings of the USENIX Winter Technical Conference, pages 97 106, 1993. [41] C. von Praun \nand T. R. Gross. Object race detection. In OOPSLA 01: Proceedings of the 16th ACM SIGPLAN conference \non Object oriented programming, systems, languages, and applications, pages 70 82, 2001. [42] J. W. Voung, \nR. Jhala, and S. Lerner. RELAY: Static race detection on millions of lines of code. In ESEC-FSE 07: Proceedings \nof the the 6th joint meeting of the European software engineering conference and the ACM SIGSOFT symposium \non The foundations of software engineering, pages 205 214, 2007. [43] Y. Yu, T. Rodeheffer, and W. Chen. \nRacetrack: ef.cient detection of data race conditions via adaptive tracking. In SOSP 05: Proceedings \nof the twentieth ACM symposium on Operating systems principles, pages 221 234, 2005.    \n\t\t\t", "proc_id": "1542476", "abstract": "<p>Data races are one of the most common and subtle causes of pernicious concurrency bugs. Static techniques for preventing data races are overly conservative and do not scale well to large programs. Past research has produced several dynamic data race detectors that can be applied to large programs. They are precise in the sense that they only report actual data races. However, dynamic data race detectors incur a high performance overhead, slowing down a program's execution by an order of magnitude.</p> <p>In this paper we present LiteRace, a very lightweight data race detector that samples and analyzes only selected portions of a program's execution. We show that it is possible to sample a multithreaded program at a low frequency, and yet, find infrequently occurring data races. We implemented LiteRace using Microsoft's Phoenix compiler. Our experiments with several Microsoft programs, Apache, and Firefox show that LiteRace is able to find more than 70% of data races by sampling less than 2% of memory accesses in a given program execution.</p>", "authors": [{"name": "Daniel Marino", "author_profile_id": "81410595225", "affiliation": "University of California, Los Angeles, Los Angeles, CA, USA", "person_id": "P1464255", "email_address": "", "orcid_id": ""}, {"name": "Madanlal Musuvathi", "author_profile_id": "81100333862", "affiliation": "Microsoft Research, Redmond, Redmond, WA, USA", "person_id": "P1464256", "email_address": "", "orcid_id": ""}, {"name": "Satish Narayanasamy", "author_profile_id": "81100556410", "affiliation": "University of Michigan, Ann Arbor, Ann Arbor, MI, USA", "person_id": "P1464257", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1542476.1542491", "year": "2009", "article_id": "1542491", "conference": "PLDI", "title": "LiteRace: effective sampling for lightweight data-race detection", "url": "http://dl.acm.org/citation.cfm?id=1542491"}