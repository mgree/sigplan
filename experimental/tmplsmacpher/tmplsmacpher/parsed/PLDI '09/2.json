{"article_publication_date": "06-15-2009", "fulltext": "\n CEAL: A C-Based Language for Self-Adjusting Computation Matthew A. Hammer Umut A. Acar * Yan Chen Toyota \nTechnological Institute at Chicago {hammer,umut,chenyan}@tti-c.org Abstract Self-adjusting computation \noffers a language-centric approach to writing programs that can automatically respond to modi.cations \nto their data (e.g., inputs). Except for several domain-speci.c implementations, however, all previous \nimplementations of self\u00adadjusting computation assume mostly functional, higher-order lan\u00adguages such \nas Standard ML. Prior to this work, it was not known if self-adjusting computation can be made to work \nwith low-level, imperative languages such as C without placing undue burden on the programmer. We describe \nthe design and implementation of CEAL: a C-based language for self-adjusting computation. The language \nis fully gen\u00aderal and extends C with a small number of primitives to enable writing self-adjusting programs \nin a style similar to conventional C programs. We present ef.cient compilation techniques for trans\u00adlating \nCEAL programs into C that can be compiled with existing C compilers using primitives supplied by a run-time \nlibrary for self-adjusting computation. We implement the proposed compiler and evaluate its effectiveness. \nOur experiments show that CEAL is effective in practice: compiled self-adjusting programs respond to \nsmall modi.cations to their data by orders of magnitude faster than recomputing from scratch while slowing \ndown a from-scratch run by a moderate constant factor. Compared to previous work, we measure signi.cant \nspace and time improvements. Categories and Subject Descriptors D.3.0 [Programming Lan\u00adguages]: General; \nD.3.3 [Programming Languages]: Language Constructs and Features General Terms Languages, Performance, \nAlgorithms. Keywords Self-adjusting computation, compilation, control and data .ow, dominators, tail \ncalls, trampolines, performance. 1. Introduction Researchers have long observed that in many applications, \napplica\u00adtion data evolves slowly or incrementally over time, often requiring only small modi.cations \nto the output. This creates the potential for applications to adapt to changing data signi.cantly faster \nthan re\u00adcomputing from scratch. To realize this potential, researchers in the * Acar is partially supported \nby a gift from Intel. Permission to make digital or hard copies of all or part of this work for personal \nor classroom use is granted without fee provided that copies are not made or distributed for pro.t or \ncommercial advantage and that copies bear this notice and the full citation on the .rst page. To copy \notherwise, to republish, to post on servers or to redistribute to lists, requires prior speci.c permission \nand/or a fee. PLDI 09, June 15 20, 2009, Dublin, Ireland. Copyright c &#38;#169; 2009 ACM 978-1-60558-392-1/09/06. \n. . $5.00 algorithms community develop so called dynamic or kinetic algo\u00adrithms or data structures that \ntake advantage of the particular prop\u00aderties of the considered problem to update computations quickly. \nSuch algorithms have been studied extensively over a range of hun\u00addreds of papers (e.g. [13, 17] for \nsurveys). These advances show that computations can often respond to small modi.cations to their data \nnearly a linear factor faster than recomputing from scratch, in practice delivering speedups of orders \nof magnitude. As a frame of comparison, note that asymptotic improvements in performance far surpasses \nthe goal of parallelism, where speedups are bound by the number of available processors. Designing, analyzing, \nand im\u00adplementing dynamic/kinetic algorithms, however, can be complex even for problems that are relatively \nsimple in the conventional set\u00adting, e.g., the problem of incremental planar convex hulls, whose conventional \nversion is straightforward, has been studied over two decades (e.g., [32, 11]). Due to their complexity, \nimplementing these algorithms is an error-prone task that is further complicated by their lack of composability. \nSelf-adjusting computation (e.g., [4, 3]) offers a language\u00adcentric approach to realizing the potential \nspeedups offered by incremental modi.cations. The approach aims to make writing self-adjusting programs, \nwhich can automatically respond to mod\u00adi.cations to their data, nearly as simple as writing conventional \nprograms that operate on unchanging data, while delivering ef.\u00adcient performance by providing an automatic \nupdate mechanism. In self-adjusting computation, programs are strati.ed into two com\u00adponents: a meta-level \nmutator and a core. The mutator interacts with the user or the outside world and interprets and re.ects \nthe modi.cations in the data to the core. The core, written like a con\u00adventional program, takes some \ninput and produces an output. The core is self-adjusting: it can respond to modi.cations to its data \nby employing a general-purpose, built-in change propagation mech\u00adanism. The mutator can execute the core \nwith some input from scratch, which we call a from-scratch or an initial run, modify the data of the \ncore, including the inputs and other computation data, and update the core by invoking change propagation. \nA typical mu\u00adtator starts by performing a from-scratch run of the program (hence the name initial run), \nand then repeatedly modi.es the data and updates the core via change propagation. At a high level, change \npropagation updates the computation by re-executing the parts that are affected by the modi.cations, \nwhile leaving the unaffected parts intact. Change propagation is guaran\u00adteed to update the computation \ncorrectly: the output obtained via change propagation is the same as the output of a from-scratch exe\u00adcution \nwith the modi.ed data. Even in the worst case, change prop\u00adagation falls back to a from-scratch execution \nasymptotically, it is never slower (in an amortized sense) but it is often signi.cantly faster than re-computing \nfrom-scratch. Previous research developed language techniques for self\u00adadjusting computation and applied \nit to a number of application domains (e.g., for a brief overview [3]). The applications show that from-scratch \nexecutions of self-adjusting programs incur a moderate overhead compared to conventional programs but \ncan respond to small modi.cations orders-of-magnitude faster than re\u00adcomputing from scratch. The experimental \nevaluations show that in some cases self-adjusting programs can be nearly as ef.cient as the hand-designed \nand optimized dynamic/kinetic algorithms (e.g., [6]). Recent results also show that the approach can \nhelp develop ef.cient solutions to challenging problems such as some three-dimensional motion simulation \nproblems that have resisted algorithmic approaches [5].  Existing general-purpose implementations of \nself-adjusting computation, however, are all in high-level, mostly functional lan\u00adguages such as Standard \nML (SML) or Haskell [27, 12]. Several exist in lower-level languages such as C [6] and Java [35] but \nthey are domain-speci.c. In Shankar and Bodik s implementation [35], which targets invariant-checking \napplications, core programs must be purely functional and functions cannot return arbitrary values or \nuse values returned by other functions in an unrestricted way. Acar et al s C implementation [6] targets \na domain of tree applications. Neither approach offers a general-purpose programming model. The most \ngeneral implementation is Hammer et al s C library [21], whose primary purpose is to support ef.cient \nmemory manage\u00adment for self-adjusting computation. The C library requires core programs to be written \nin a style that makes dependencies between program data and functions explicit, limiting its effectiveness \nas a source-level language. That there is no general-purpose support for self-adjusting computation in \nlow level, imperative languages such as C is not accidental: self-adjusting computation critically relies \non higher\u00adorder features of high-level languages. To perform updates ef.\u00adciently, change propagation \nmust be able to re-execute a previously\u00adexecuted piece of code in the same state (modulo the modi.ca\u00adtions), \nand skip over parts of the computation that are unaffected by the modi.cations. Self-adjusting computation \nachieves this by rep\u00adresenting the dependencies between the data and the program code as a trace that \nrecords speci.c components of the program code and their run-time environments. Since higher-order languages \ncan natively represent closed functions, or closures, consisting of a function and its free variables, \nthey are naturally suitable for im\u00adplementing traces. Given a modi.cation, change propagation .nds the \nclosures in the trace that depend on the modi.ed data, and re\u00adexecutes them to update the computation \nand the output. Change propagation utilizes recorded control dependencies between clo\u00adsures to identify \nthe parts of the computation that need to be purged and uses memoization to recover the parts that remain \nthe same. To ensure ef.cient change propagation, the trace is represented in the form of a dynamic dependence \ngraph that supports fast random access to the parts of the computation to be updated. In this paper, \nwe describe the design, implementation, and eval\u00aduation of CEAL: a C-based language for self-adjusting \ncompu\u00adtation. The language extends C with several primitives for self\u00adadjusting computation (Section \n2). Re.ecting the structure of self\u00adadjusting programs, CEAL consists of a meta language for writing \nmutators and a core language for writing core programs. The key linguistic notion in both the meta and \nthe core languages is that of the modi.able reference or modi.able for short. A modi.able is a location \nin memory whose contents may be read and updated. CEAL offers primitives to create, read (access), and \nwrite (update) modi.ables just like conventional pointers. The crucial difference is that CEAL programs \ncan respond to modi.cations to modi.ables automatically. Intuitively, modi.ables mark the computation \ndata that can change over time, making it possible to track dependen\u00adcies selectively. At a high level, \nCEAL can be viewed as a dialect of C that replaces conventional pointers with modi.ables. By designing \nthe CEAL language to be close to C, we make it possible to use familiar C syntax to write self-adjusting \nprograms. This poses a compilation challenge: compiling CEAL programs to self-adjusting programs requires \nidentifying the dependence infor\u00admation needed for change propagation. To address this challenge, we \ndescribe a two-phase compilation technique (Sections 5 and 6). The .rst phase normalizes the CEAL program \nto make the depen\u00addencies between data and parts of the program code explicit. The second phase translates \nthe normalized CEAL code to C by using primitives supplied by a run-time-system (RTS) in place of CEAL \ns primitives. This requires creating closures for representing depen\u00addencies and ef.ciently supporting \ntail calls. We prove that the size of the compiled C code is no more than a multiplicative factor larger \nthan the source CEAL program, where the multiplicative factor is determined by the maximum number of \nlive variables over all pro\u00adgram points. The time for compilation is bounded by the size of the compiled \nC code and the time for live variable analysis. Section 3.2 gives an overview of the compilation phases \nvia an example. We implement the proposed compilation technique and evaluate its effectiveness. Our compiler, \ncealc, provides an implementation of the two-level compilation strategy and relies on the RTS for sup\u00adplying \nthe self-adjusting-computation primitives. Our implementa\u00adtion of the RTS employs the recently-proposed \nmemory manage\u00adment techniques [21], and uses asymptotically optimal algorithms and data structures to \nsupport traces and change propagation. For practical ef.ciency, the compiler uses intra-procedural compilation \ntechniques that make it possible to use simpler, practically ef.cient algorithms. Our compiler cealc \nis between a factor of 3 8 slower and generates binaries that are 2 5 times larger than gcc. We perform \nan experimental evaluation by considering a range of benchmarks, including several primitives on lists \n(e.g., map, .l\u00adter), several sorting algorithms, and computational geometry al\u00adgorithms for computing \nconvex hulls, the distance between con\u00advex objects, and the diameter of a point set. As a more complex \nbenchmark, we implement a self-adjusting version of the Miller-Reif tree-contraction algorithm, which \nis a general-purpose tech\u00adnique for computing various properties of trees (e.g., [28, 29]). Our timing \nmeasurements show that CEAL programs are about 6 19 times slower than the corresponding conventional \nC program when executed from scratch, but can respond to small changes to their data orders-of-magnitude \nfaster than recomputing from scratch. Compared to the state-of-the-art implementation of self\u00adadjusting \ncomputation in SML [27], CEAL uses about 3 5 times less memory. In terms of run time, CEAL performs signi.cantly \nfaster than the SML-based implementation. In particular, when the SML benchmarks are given signi.cantly \nmore memory than they need, we measure that they are about a factor of 9 slower. More\u00adover, this slowdown \nincreases (without bound) as memory becomes more limited. We also compared our implementation to a hand\u00adoptimized \nimplementation of self-adjusting computation for tree contraction [6]. Our experiments show that we are \nabout 3 4 times slower. In this paper, we present a C-based general-purpose language for self-adjusting \ncomputation. Our contributions include the lan\u00adguage, the compiler, and the experimental evaluation. \nAn extended version of this paper, including the proofs and more detailed exper\u00adiments, can be found \nin the accompanying technical report [22]. 2. The CEAL language We present an overview of the CEAL language, \nwhose core is for\u00admalized in Section 4. The key notion in CEAL is that of a modi.\u00adable reference (or \nmodi.able, for short). A modi.able is a location in memory whose content may be read and updated. From \nan op\u00aderational perspective, a modi.able is just like an ordinary pointer in memory. The major difference \nis that CEAL programs are sen\u00adsitive to modi.cations of the contents of modi.ables performed by the mutator, \ni.e., if the contents are modi.ed, then the computation can respond to that change by updating its output \nautomatically via change propagation.  Re.ecting the two-level (core and meta) structure of the model, \nthe CEAL language consists of two sub-languages: meta and core. The meta language offers primitives for \nperforming an ini\u00adtial run, modifying computation data, and performing change propagation the mutator \nis written in the meta language. CEAL s core language offers primitives for writing core programs. A \nCEAL program consists of a set of functions, divided into core and meta functions: the core functions \n(written in the core language), are marked with the keyword ceal, meta functions (written in the meta \nlanguage) use conventional C syntax. We refer to the part of a program consisting of the core (meta) \nfunctions simply as the core (meta or mutator). To provide scalable ef.ciency and improve usability, \nCEAL provides its own memory manager. The memory manager performs automatic garbage collection of allocations \nperformed in the core (via CEAL s allocator) but not in the mutator. The language does not require the \nprogrammer to use the provided memory manager, the programmer can manage memory explicitly if so desired. \nThe Core Language. The core language extends C with modi\u00ad.ables, which are objects that consist of word-sized \nvalues (their contents) and supports the following primitive operations, which essentially allow the \nprogrammer to treat modi.ables like ordinary pointers in memory. modref t* modref(): creates a(n) (empty) \nmodi.able void write(modref t *m, void *p): writes p into m void* read(modref t *m): returns the contents \nof m In addition to operations on modi.ables, CEAL provides the alloc primitive for memory allocation. \nFor correctness of change propagation, core functions must modify the heap only through modi.ables, i.e., \nmemory accessed within the core, excluding mod\u00adi.ables and local variables, must be write-once. Also \nby de.ni\u00adtion, core functions return ceal (nothing). Since modi.ables can be written arbitrarily, these \nrestrictions cause no loss of generality or undue burden on the programmer. The Meta Language. The meta \nlanguage also provides primitives for operating on modi.ables; modref allocates and returns a mod\u00adi.able, \nand the following primitives can be used to access and up\u00addate modi.ables: void* deref(modref t *m): \nreturns the contents of m. void modify(modref t *m, void *p): modi.es the con\u00ad tents of the modi.able \nm to contain p. As in the core, the alloc primitive can be used to allocate memory. The memory allocated \nat the meta level, however, needs to be explicitly freed. The CEAL language provides the kill primitive \nfor this purpose. In addition to these, the meta language offers primitives for starting a self-adjusting \ncomputation, run core, and updating it via change propagation, propagate. The run core primitive takes \na pointer to a core function f and the arguments a, and runs f with a. The propagate primitive updates \nthe core computation created by run core to match the modi.cations performed by the mutator via modify.1 \nExcept for modi.ables, the mutator may not modify memory accessed by the core program. The meta language \nmakes no other restrictions: it is a strict superset of C. 1 The actual language offers a richer set \nof operations for creating multiple self-adjusting cores simultaneously. Since the meta language is not \nour focus here we restrict ourselves to this simpler interface. typedef enum { NODE, LEAF} kind t; typedef \nstruct {kind t kind; enum { PLUS, MINUS } op; modref t *left, *right; } node t; typedef struct {kind \nt kind; int num; } leaf t; Figure 1. Data type de.nitions for expression trees. 1 ceal eval (modref t \n*root, modref t *res) { 2 node t *t = read (root); 3 if (t->kind == LEAF) { 4 write (res,(void*)((leaf \nt*) t)->num); 5 } else { 6 modref t *m a = modref (); 7 modref t *m b = modref (); 8 eval (t->left, m \na); 9 eval (t->right, m b); 10 int a = (int) read (m a); 11 int b = (int) read (m b); 12 if (t->op == \nPLUS) { 13 write (res, (void*)(a + b)); 14 } else { 15 write (res, (void*)(a -b)); 16 } 17 } 18 return; \n19 } Figure 2. The eval function written in CEAL (core). exp= \"(3d +c 4e) -b (1g -f 2h)+a (5j -i 6k)\"; \ntree = buildTree (exp); result = modref (); run core (eval, tree, result); subtree = buildTree (\"6m +l \n7n\"); t = find (\"k\",subtree); modify (t,subtree); propagate (); Figure 3. The mutator written in CEAL \n(meta). 3. Example and Overview We give an example CEAL program and give an overview of the compilation \nprocess (Sections 5 and 6) via this example. 3.1 An Example: Expression Trees We present an example \nCEAL program for evaluating and updat\u00ading expression trees. Figure 1 shows the data type de.nitions for \nexpressions trees. A tree consists of leaves and nodes each repre\u00adsented as a record with a kind .eld \nindicating their type. A node additionally has an operation .eld, and left &#38; right children placed \nin modi.ables. A leaf holds an integer as data. For illustrative pur\u00adposes, we only consider plus and \nminus operations. 2 This represen\u00adtation differs from the conventional one only in that the children \nare stored in modi.ables instead of pointers. By storing the children in modi.ables, we enable the mutator \nto modify the expression and update the result by performing change propagation. Figure 2 shows the code \nfor eval function written in core CEAL. The function takes as arguments the root of a tree (in a modi.able) \nand a result modi.able where it writes the result of evaluating the tree. It starts by reading the root. \nIf the root is a leaf, then its value is written into the result modi.able. If the 2 In their most general \nform, expression trees can be used to compute a range of properties of trees and graphs. We discuss such \na general model in our experimental evaluation.  Figure 4. Example expression trees. root is an internal \nnode, then it creates two result modi.ables (ma and mb) and evaluates the left and the right subexpressions. \nThe function then reads the resulting values of the subexpressions, combines them with the operation \nspeci.ed by the root, and writes the value into the result. This approach to evaluating a tree is standard: \nreplacing modi.ables with pointers, reads with pointer dereference, and writes with assignment yields \nthe conventional approach to evaluating trees. Unlike the conventional program, the CEAL program can \nrespond to updates quickly. Figure 3 shows the pseudo-code for a simple mutator example illustrated in \nFigure 4. The mutator starts by creating an expres\u00adsion tree from an expression where each subexpression \nis labeled with a unique key, which becomes the label of each node in the ex\u00adpression tree. It then creates \na result modi.able and evaluates the tree with eval, which writes the value 7 into the result. The muta\u00adtor \nthen modi.es the expression by substituting the subexpression (6m +l 7n) in place of the modi.able holding \nthe leaf k and up\u00addates the computation via change propagation. Change propagation updates the result \nto 0, the new value of the expression. By repre\u00adsenting the data and control dependences in the computation \naccu\u00adrately, change propagation updates the computation in time propor\u00adtional to the length of the path \nfrom k (changed leaf) to the root, instead of the total number of nodes as would be conventionally required. \nWe evaluate a variation of this program (which uses .oat\u00ading point numbers in place of integers) in Section \n8 and show that change propagation updates these trees ef.ciently.  3.2 Overview of Compilation Our \ncompilation technique translates CEAL programs into C pro\u00adgrams that rely on a run-time-system RTS (Section \n6.1) to provide self-adjusting-computation primitives. Compilation treats the mu\u00adtator and the core separately. \nTo compile the mutator, we simply replace meta-CEAL calls with the corresponding RTS calls no major code \nrestructuring is needed. In this paper, we therefore do not discuss compilation of the meta language \nin detail. Compiling core CEAL programs is more challenging. At a high level, the primary dif.culty is \ndetermining the code dependence for the modi.able being read, i.e., the piece of code that depends on \nthe modi.able. More speci.cally, when we translate a read of modi.able to an RTS call, we need to supply \na closure that encapsulates all the code that uses the value of the modi.able being read. Since CEAL \ntreats references just like conventional pointers, it does not make explicit what that closure should \nbe. In the context of functional languages such as SML, Ley-Wild et al used a continuation-passing-style \ntransformation to solve this problem [27]. The idea is to use the continuation of the read as a conservative \napproximation of the code dependence. Supporting continuations in stack-based languages such as C is \nexpensive and cumbersome. Another approach is to use the source function that contains the read as an \napproximation to the code dependence. This not only slows down change propagation by executing code unnecessarily \nbut also can cause code to be executed multiple times, e.g., when a function (caller) reads a modi.ed \nvalue from a callee, the caller has to be executed, causing the callee to be executed again. To address \nthis problem, we use a technique that we call normal\u00adization (Section 5). Normalization restructures \nthe program such 1 ceal eval (modref t *root, modref t *res) {2 node t *t = read (root); tail read r \n(t,res); 19 } a ceal 3 if (t->kind == LEAF) { 4 write (res,(void*)((leaf t*) t)->num);  tail eval final \n(); 5 } else {6 modref t *m a = modref (); 7 modref t *m b = modref (); 8 eval (t->left, m a); 9 eval \n(t->right, m b); 10 int a = (int) read (m a); tail read a (res,a,m b); 17 }} b ceal 11 int b = (int) \nread (m b); tail read b (res,a,b); } c ceal 12 if (t->op == PLUS) { 13 write (res, (void*)(a + b)); \n tail eval final (); 14 } else {15 write (res, (void*)(a -b)); tail eval final (); 16 }} d ceal eval \nfinal () {18 return; } Figure 5. The normalized expression-tree evaluator. that each read operation is \nfollowed by a tail call to a function that marks the start of the code that depends on the modi.able \nbeing read. The dynamic scope of the function tail call ends at the same point as that of the function \nthat the read is contained in. To nor\u00admalize a CEAL program, we .rst construct a specialized rooted control-.ow \ngraph that treats certain nodes function nodes and nodes that immediately follow read operations as entry \nnodes by connecting them to the root. The algorithm then computes the dom\u00adinator tree for this graph \nto identify what we call units, and turns them into functions, making the necessary transformations for \nthese functions to be tail-called. We prove that normalization runs ef.\u00adciently and does not increase \nthe program by more than a factor of two (in terms of its graph nodes). As an example, Figure 5 shows \nthe code for the normalized expression evaluator. The numbered lines are taken directly from the original \nprogram in Figure 2. The highlighted lines correspond to the new function nodes and the tail calls to \nthese functions. Normalization creates the new functions, read r, read a, read b and tail-calls them \nafter the reads lines 2, 10, and 11 respectively. Intuitively, these functions mark the start of the \ncode that depend on the root, and the result of the left and the right subtrees (ma and mb respectively). \nThe normalization algorithm creates a trivial function, eval final for the return statement to ensure \nthat the read a and read b branch out of the conditional otherwise the correspondence to the source program \nmay be lost.3 We .nalize compilation by translating the normalized program into C. To this end, we present \na basic translation that creates clo\u00adsures as required by the RTS and employs trampolines4 to sup\u00adport \ntail calls without growing the stack (Section 6.2). This basic 3 In practice we eliminate such trivial \ncalls by inlining the return. 4 A trampoline is a dispatch loop that iteratively runs a sequence of closures. \n Types t ::= int | modref t | t* Values v ::= R | n Prim. op s o ::= .|e| ... Expressions e ::= v | \no(x) | x[y] Commands c ::= nop | x := e | x[y] := e | x := modref() | x := read y | write xy | x := \nalloc yf z | call f (x) Jumps j ::= goto l | tail f (x) Basic Blocks b ::= {l : done}|{l : cond xj1 j2}|{l \n: c ; j}Fun. Defs F ::= f (t1 x) {t2 y; b}Programs P ::= F Figure 6. The syntax of CL. translation is \ntheoretically satisfactory but it is practically expen\u00adsive, both because it requires run-time type information \nand be\u00adcause trampolining requires creating a closure for each tail call. We therefore present two major \nre.nements that apply trampolining selectively and that monomorphize the code by statically generat\u00ading \ntype-specialized instances of certain functions to eliminate the need for run-time type information (Section \n6.3). It is this re.ned translation that we implement (Section 7). We note that the quality of the self-adjusting \nprogram generated by our compilation strategy depends on the source code. In partic\u00adular, if the programmer \ndoes not perform the reads close to where the values being read are used, then the generated code may \nnot be effective. In many cases, it is easy to detect and statically elimi\u00adnate such poor code by moving \nreads appropriately our compiler performs a few such optimizations. Since such optimizations are orthogonal \nto our compilation strategy (they can be applied inde\u00adpendently), we do not discuss them further in this \npaper. 4. The Core Language We formalize core-CEAL as a simpli.ed variant of C called CL (Core Language) \nand describe how CL programs are executed. 4.1 Abstract Syntax Figure 6 shows the abstract syntax for \nCL. The meta variables x and y (and variants) range over an unspeci.ed set of variables and the meta \nvariables e (and variants) range over a separate, unspeci.ed set of (memory) locations. For simplicity, \nwe only include integer, modi.able and pointers types; the meta variable t (and variants) range over \nthese types. The type system of CL mirrors that of C, and thus, it offers no strong typing guarantees. \nSince the typing rules are standard we do not discuss them here. The language distinguishes between values, \nexpressions, com\u00admands, jumps, and basic blocks. A value v is either a memory location e or an integer \nn. Expressions include values, primitive operations (e.g. plus, minus) applied to a sequence of variables, \nand array dereferences x[y]. Commands include no-op, assignment into a local variable or memory location, \nmodi.able creation, read from a modi.able, write into a modi.able, memory allocation, and function calls. \nJumps include goto jumps and tail jumps. Programs consist of a set of functions de.ned by a name f, a \nsequence of formal arguments t1 x, local variable declarations t2 y and a body of basic blocks. Each \nbasic block, labeled uniquely, takes one of three forms: a done block, {l : done},a conditional block, \n{l : cond xj1 j2}, and a command-and-jump block, {l : c ; j}. When referring to command-and-jump blocks, \nwe sometimes use the type of the command, e.g., a read block, a write block, regard\u00adless of the jump. \nSymmetrically, we sometimes use the type of the jump, e.g., a goto block, a tail-call block, regardless \nof the com\u00admand. Note that since there are no return instructions, a function cannot return a value (since \nthey can write to modi.ables arbitrar\u00adily, this causes no loss of generality). 4.2 Execution (Operational \nSemantics) Execution of a CL program begins when the mutator uses run core to invoke one of its functions \n(e.g, as in Figure 3). Most of the op\u00aderational (dynamic) semantics of CL should be clear to the reader \nif s/he is familiar with C or similar languages. The interesting aspects include tail jumps, operations \non modi.ables and mem\u00adory allocation. A tail jump executes like a conventional function call, except \nthat it never returns. The modref command allocates a modi.able and returns its location. Given a modi.able \nlocation, the read command retrieves its contents, and the write command destructively updates its contents \nwith a new value. A done block, {k : done}, completes execution of the current function and re\u00adturns. \nA conditional block, {k : cond xj1 j2}, checks the value of local variable x and performs jump j1 if \nthe condition is true (non-zero) and jump j2 otherwise. A command block, {l : c ; j}, executes the command \nc followed by jump j. The alloc command allocates an array at some location e with a speci.ed size in \nbytes (y) and uses the provided function f to initialize it by calling f with e and the additional arguments \n(z). After initialization, it returns e. By requiring this stylized interface for allocation, CL makes \nit eas\u00adier to check and conform to the correct-usage restrictions (de.ned below) by localizing initialization \neffects (the side effects used to initialize the array) to the provided initialization function. To accurately \ntrack data-dependencies during execution, we re\u00adquire, but do not enforce that the programs conform to \nthe follow\u00ading correct-usage restrictions: 1) each array is side-effected only during the initialization \nstep and 2) that the initialization step does not read or write modi.ables.  4.3 From CEAL to CL We \ncan translate CEAL programs into CL by 1) replacing sequences of statements with corresponding command \nblocks connected via goto jumps, 2) replacing so-called structured control-.ow (e.g., if, do, while, \netc.) with corresponding (conditional) blocks and goto jumps, and 3) replacing return with done. 5. Normalizing \nCL Programs We say that a program is in normal form if and only if every read command is in a tail-jump \nblock, i.e., followed immediately by a tail jump. In this section, we describe an algorithm for normaliza\u00adtion \nthat represents programs with control .ow graphs and uses dominator trees to restructure them. Section \n5.4 illustrates the tech\u00adniques described in this section applied to our running example. 5.1 Program \nGraphs We represent CL programs with a particular form of rooted control .ow graphs, which we shortly \nrefer to as a program graph or simply as a graph when it is clear from the context. The graph for a program \nP consists of nodes and edges, where each node represents a function de.nition, a (basic) block, or a \ndistinguished root node (Section 5.4 shows an example). We tag each non-root node with the label of the \nblock or the name of the function that it represents. Additionally, we tag each node representing a block \nwith the code for that block and each node representing a function, called a function node, with the \nprototype (name and arguments) of the function and the declaration of local variables. As a matter of \nnotational convenience, we name the nodes with the label of the corresponding basic block or the name \nof the function, e.g., ul or uf . The edges of the graph represent control transfers. For each goto jump \nbelonging to a block {k : c ; goto l}, we have an edge from node uk to node ul tagged with goto l. For \neach function node uf whose .rst block is ul, we have an edge from uf to ul la\u00adbeled with goto l. For \neach tail-jump block {k : c ; tail f(x)}, we have an edge from uk to uf tagged with tail f(x). If a node \nuk represents a call-instruction belonging to a block {k : call f(x); j}, then we insert an edge from \nuk to uf and tag it with call f(x). For each conditional block {k : cond xj1 j2}where j1 and j2 are the \njumps, we insert edges from k to targets of j1 and j2, tagged with true and false, respectively.  We \ncall a node a read-entry node if it is the target of an edge whose source is a read node. More speci.cally, \nconsider the nodes uk belonging to a block of the form {k : x := read y ; j} and ul which is the target \nof the edge representing the jump j; the node ul is a read entry. We call a node an entry node if it \nis a read-entry or a function node. For each entry node ul, we insert an edge from the root to ul into \nthe graph. There is a (ef.ciently) computable isomorphism between a pro\u00adgram and its graph that enables \nus to treat programs and graphs as a single object. In particular, by changing the graph of a program, \nour normalization algorithm effectively restructures the program itself. Property 1 (Programs and Graphs) \nThe program graph of a CL program with n blocks can be con\u00adstructed in expected O(n) time. Conversely, \ngiven a program graph with m nodes, we can construct its program in O(m) time.  5.2 Dominator Trees \nand Units Let G =(V,E) be a rooted program graph with root node ur. Let uk,ul . V be two nodes of G. \nWe say that uk dominates ul if every path from ur to ul in G passes through uk. By def\u00adinition, every \nnode dominates itself. We say that uk is an imme\u00addiate dominator of ul if uk = ul and uk is a dominator \nof ul such that every other dominator of ul also dominates uk. It is easy to show that each node except \nfor the root has a unique im\u00admediate dominator (e.g., [26]). The immediate-dominator relation de.nes \na tree, called a dominator tree T =(V,EI ) where by EI = {(uk,ul) | uk is an immediate dominator of ul}. \nLet T be a dominator tree of a rooted program graph G = (V, E) with root ur. Note that the root of G \nand T are both the same. Let ul be a child of ur in T . We de.ne the unit of ul as the vertex set consisting \nof ul and all the descendants of ul in T ; we call ul the de.ning node of the unit. Normalization is \nmade possible by an interesting property of units and cross-unit edges. Lemma 2 (Cross-Unit Edges) Let \nG =(V, E) be a rooted program graph and T be its dominator tree. Let Uk and Um be two distinct units \nof T de.ned by vertices uk and um respectively . Let ul . Uk and un . Um be any two vertices from Uk \nand Um. If (ul,un) . E, i.e., a cross-unit edge in the graph, then un = um. Proof: Let ur be the root \nof both T and G. For a contradiction, suppose that (ul,un) . E and un um = . Since (ul,un) . E there \nexists a path p = ur r uk r ul . un in G. Since um is a dominator of un, this means that um is in p, \nand since um it = un must be the case that either uk proceeds um in p or vice versa. We consider the \ntwo cases separately and show that they each lead to a contradiction. If uk proceeds um in p then p \n= ur r uk r um r ul . un. But this means that ul can be reached from ur without going through uk (since \nur . um . G). This contradicts the assumption that uk dominates ul.  If um proceeds uk in p then p = \nur r um r uk r ul . un. But this means that un can be reached from ur without going through um (since \nur . uk . G). This contradicts the assumption that um dominates un.  1 NORMALIZE (P )= 2 Let G =(V, \nE) be the graph of P rooted at ur 3 Let T be the dominator tree of G (rooted at ur) 4 G' =(V ',E'), \nwhere V ' = V and E' = \u00d8 5 for each unit U of T do 6 ul . defining node of U 7 if ul is a function node \nthen 8 (* ul is not critical *) 9 EU .{(ul,u) . E | (u . U)} 10 else 11 (* ul is critical *) 12 pick \na fresh function f, i.e., f . funs(P ) 13 let x be the live variables at l in P 14 let y be the free \nvariables in U 15 z = y \\ x 16 V ' . V ' .{uf } 17 tag(uf )= f(x){z} 18 EU .{(ur,uf ), (uf ,ul)}. 19 \n{(u1,u2) . E | u1,u2 . U . u2 = ul}20 for each critical edge (uk,ul) . E do 21 if uk . U then 22 (* \ncross-unit edge *) 23 EU . EU .{(uk,uf )}24 tag((u, uf )) = tail f(x) 25 else 26 (* intra-unit edge *) \n27 if uk is a read node then 28 EU . EU .{(uk,uf )}29 tag((u, uf )) = tail f(x) 30 else 31 EU . EU .{(uk,ul)}32 \nE' . E' . EU Figure 7. Pseudo-code for the normalization algorithm.  5.3 An Algorithm for Normalization \nFigure 7 gives the pseudo-code for our normalization algorithm, NORMALIZE. At a high level, the algorithm \nrestructures the original program by creating a function from each unit and by changing control transfers \ninto these units to tail jumps as necessary. Given a program P , we start by computing the graph G of \nP and the dominator tree T of G. Let ur be the root of the dominator tree and the graph. The algorithm \ncomputes the normalized program graph G' =(V ',E') by starting with a vertex set equal to that of the \ngraph V ' = V and an empty edge set E' = \u00d8. It proceeds by considering each unit U of T . Let U be a \nunit of T and let the node ul of T be the de.ning node of the unit. If ul is a not a function node, then \nwe call it and all the edges that target it critical. We consider two cases to construct a set of edges \nEU that we insert into G' for U. If ul is not critical, then EU consists of all the edges whose target \nis in U. If ul is critical, then we de.ne a function for it by giving it a fresh function name f and \ncomputing the set of live variables x at block l in P , and free variables y in all the blocks represented \nby U. The set x becomes the formal arguments to the function f and the set z of variables de.ned as the \nremaining free variables, i.e., y \\ x, become the locals. We then insert a new node uf into the normalized \ngraph and insert the edges (ur,uf ) and (uf ,ul) into EU as well as all the edges internal to U that \ndo not target ul. This creates the new function f (x) with locals z whose body is de.ned by the basic \nblocks of the unit U. Next, we consider critical edges of the form (uk,ul). If the edge is a cross-unit \nedge, i.e., uk . U, then we replace it with an edge into uf by inserting (uk,uf ) into EU and tagging \nthe edge with a tail jump to the de.ned function f representing U. If the critical edge is an intra-unit \nedge, i.e., uk . U, then we have two cases to consider: If uk is a read node, then we insert (uk,uf ) \ninto EU and tag it with a tail jump to function f with the appropriate arguments, effectively redirecting \nthe edge to uf . If uk is not a read node, then we insert the edge into EU , effectively leaving it intact. \nAlthough the algorithm only Figure 9. The dominator tree for the graph in Figure 8.  redirects critical \nedges Lemma 2 shows this is suf.cient: all other edges in the graph are contained within a single unit \nand hence do not need to be redirected.  5.4 Example Figure 8 shows the rooted graph for the expression-tree \nevaluator shown in Figure 2 after translating it into CL (Section 4.3). The nodes are labeled with the \nline numbers that they correspond to; goto edges and control-branches are represented as straight edges, \nand (tail) calls are represented with curved edges. For example, node 0 is the root of the graph; node \n1 is the entry node for the function eval; node 3 is the conditional on line 3; the nodes 8 and 9 are \nthe recursive calls to eval. The nodes 3, 11, 12 are read-entry nodes. Figure 9 shows the dominator \ntree for the graph (Figure 8). For illustrative purposes the edges of the graph that are not tree edges \nare shown as dotted edges.5 The units are de.ned by the vertices 1, 3, 11, 12, 18. The cross\u00adunit edges, \nwhich are non-tree edges, are (2,3), (4,18), (10,11), (11,12), (13,18), (15,18). Note that as implied \nby Lemma 2, all these edges tar\u00adget the de.ning nodes of the units. Figure 10 shows the rooted program \ngraph for the normalized program created by algo\u00adrithm NORMALIZE. By inspection of Figures 9 and 2, we \nsee that the critical nodes are 3, 11, 12, 18, because they de.ne units but they are not function nodes. \nThe algorithm creates the fresh nodes a, b, c, and d for these units and redi\u00adrects all the cross-unit \nedges into the new func\u00ad tion nodes and tagged with tail jumps (tags not Figure 8. shown). In this example, \nwe have no intra-unit critical edges. Figure 5 shows the code for the normalized graph (described in \nSection 3.2).  5.5 Properties of Normalization We state and prove some properties about the normalization \nalgo\u00adrithm and the (normal) programs it produces. The normalization al\u00adgorithm uses a live variable analysis \nto determine the formal and ac\u00adtual arguments for each fresh function (see line 13 in Figure 7). We let \nTL(P ) denote the time required for live variable analysis of a CL program P . The output of this analysis \nfor program P is a func\u00adtion live(\u00b7), where live(l) is the set variables which are live at (the start \nof) block l . P . We let ML(P ) denote the maximum number of live variables for any block in program \nP , i.e., maxl.P |live(l)|. We assume that each variable, function name, and block label re\u00ad 5 Note that \nthe dominator tree can have edges that are not in the graph. quire one word to represent. The following \ntheorems relate the size of a CL program before and after normalization (Theorem 3) and bound the time \nrequired to perform normalization (Theorem 4). Theorem 3 (Size of Output Program) If CL program P has \nn blocks and P ' = NORMALIZE(P ), then P ' also has n blocks and at most n additional function de.nitions. \nFurthermore if it takes m words to represent P , then it takes O(m + n \u00b7 ML(P )) words to represent P \n' . Proof: Observe that the normalization algorithm creates no new blocks just new function nodes. Furthermore, \nsince at most one function is created for each critical node, which is a block, the algorithm creates \nat most one new function for each block of P . Thus, the .rst bound follows. For the second bound, note \nthat since we create at most one new function for each block, we can name each function using the block \nlabel followed by a marker (stored in a word), thus re\u00adquiring no more than 2n words. Since each fresh \nfunction has at most ML(P ) arguments, representing each function signature re\u00adquires O(ML(P )) additional \nwords (note that we create no new variable names). Similarly, each call to a new function requires O(ML(P \n)) words to represent. Since the number of new functions and new calls is bounded by n, the total number \nof additional words needed for the new function signatures and the new function calls is bounded by O(m \n+ n \u00b7 ML(P )). Theorem 4 (Time for Normalization) If CL program P has n blocks then running NORMALIZE(P \n) takes O(n + n \u00b7 ML(P )+ TL(P )) time. Proof: Computing the dominator tree takes linear time [19]. By \nde.nition, computing the set of live variables for each node takes TL(P ) time. We show that the remaining \nwork can be done in O(n + n \u00b7 ML(P )) time. To process each unit, we check if its de.ning node is a function \nnode. If so, we copy each incoming edge from the original program. If not, we create a fresh function \nnode, copy non-critical edges, and process each incoming critical edge. Since each node has a constant \nout degree (at most two), the total number of edges considered per node is constant. Since each de.ning \nnode has at most ML(P ) live variables, it takes O(ML(P )) time to create a fresh function. Replacing \na critical edge with a tail jump edge requires creating a function call with at most ML(P ) arguments, \nrequiring O(ML(P )) time. Thus, it takes O(n + n \u00b7 ML(P )) time to process all the units. 6. Translation \nto C The translation phase translates normalized CL code into C code that relies on a run-time system \nproviding self-adjusting com\u00adputation primitives. To support tail jumps in C without growing the stack \nwe adapt a well-known technique called trampolining (e.g., [24, 36]). At a high level, a trampoline is \na loop that runs a sequence of closures that, when executed, either return another closure or NULL, which \ncauses the trampoline to terminate. 6.1 The Run-Time System Figure 11 shows the interface to the run-time \nsystem (RTS). The RTS provides functions for creating and running closures. The closure make function \ncreates a closure given a function and a complete set of matching arguments. The closure run function \nsets up a trampoline for running the given closure (and the clo\u00adsures it returns, if any) iteratively. \nThe RTS provides functions for creating, reading, and writing modi.ables (modref t). The  typedef struct \n{...} closure t; closure t* closure make(closure t* (*f)(tx), tx); void closure run(closure t* c); typedef \nstruct {...} modref t; void modref init(modref t *m); void modref write(modref t *m, void *v); closure \nt* modref read(modref t *m, closure t *c); void* allocate(size t n, closure t *c); Figure 11. The interface \nfor the run-time system. Expressions: [ e] = e Commands: [ nop] = ; [ x := e] = x := [ e]]; [ x[y] := \ne] = x[y] := [ e] ; [ call f(x)]] = closure run(f(x)); [ x := alloc yf z] = closure t *c; c := closure \nmake(f,NULL::z); x := allocate(y,c); [ x := modref()]] = [ x := alloc (sizeof(modref t)) modref init \n()] [ write xy] = modref write(x, y); Jumps: [ goto l] = goto l; [ tail f(x)]] = return (closure make(f,x)); \nBasic Blocks: [ {l : done}] = {l: return NULL;} [ {l : cond xj1 j2}] = {l:if (x) {[ j1] } else {[ j2] \n}} [ {l : c ; j}] = {l: [ c] ; [ j] } [ {l : x := read y ;= {l: closure t *c; tail f(x, z)}] c := closure \nmake(f,NULL::z); return (modref read(y,c));} Functions: [ f (tx x) {ty y ; b}] = closure t* f(tx x){ty \ny; [ b] } Figure 12. Translation of CL into C. modref init function initializes memory for a modi.able; \nto\u00adgether with allocate this function can be used to allocate modi.\u00adables. The modref write function \nupdates the contents of a mod\u00adi.able with the given value. The modref read function reads a modi.able, \nsubstitutes its contents as the .rst argument of the given closure, and returns the updated closure. \nThe allocate function allocates a memory block with the speci.ed size (in bytes), runs the given closure \nafter substituting the address of the block for the .rst argument, and returns the address of the block. \nThe closure acts as the initializer for the allocated memory.  6.2 The Basic Translation Figure 12 illustrates \nthe rules for translating normalized CL pro\u00adgrams into C. For clarity, we deviate from C syntax slightly \nby using := to denote the C assignment operator. The most interesting cases concern function calls, tail \njumps, and modi.ables. To support trampolining, we translate functions to return closures. A function \ncall is translated into a direct call whose return value (a closure) is passed to a trampoline, closure \nrun. A tail jump simply creates a corresponding closure and returns it to the active trampoline. Since \neach tail jump takes place in the context of a function, there is always an active trampoline. The translation \nof alloc .rst creates a closure from the given initialization function and arguments prepended with NULL, \nwhich acts as a place-holder for the allocated location. It then supplies this closure and the speci.ed \nsize to allocate. We translate modref as a special case of allocation by supplying the size of a modi.able \nand using modref init as the initialization function. Translation of write commands is straightforward. \nWe translate a command-and-jump block by separately translating the command and the jump. We translate \nreads to create a closure for the tail jump following the read command and call modref read with the \nclosure &#38; the modi.able being read. As with tail jumps, the translated code returns the resulting \nclosure to the active tram\u00adpoline. When creating the closure, we assume, without loss of gen\u00aderality, \nthat the result of the read appears as the .rst argument to the tail jump and use NULL as a placeholder \nfor the value read. Note that, since translation is applied after normalization, all read commands are \nfollowed by a tail jump, as we assume here. 6.3 Re.nements for Performance One disadvantage of the basic \ntranslation described above is that it creates closures to support tail jumps; this is known to be slow \n(e.g., [24]). To address this problem, we trampoline only the tail jumps that follow reads, which we \ncall read trampolining, by re\u00ad.ning the translation as follows: [ tail f (x)]] = return f(x). This re.nement \ntreats all tail calls other than those that follow a read as conventional function calls. Since the translation \nof a read already creates a closure, this eliminates the need to create extra closures. Since in practice \nself-adjusting programs perform reads periodically (popping the stack frames down to the active trampo\u00adline), \nwe observe that the stack grows only temporarily. Another disadvantage is that the translated code uses \nthe RTS function closure make with different function and argument types, i.e., polymorphically. Implementing \nthis requires run-time type information. To address this problem, we apply monomor\u00adphisation [37] to \ngenerate a set of closure make functions for each distinctly-typed use in the translated program. Our \nuse of monomorphisation is similar to that of the MLton compiler, which uses it for compiling SML, where \npolymorphism is abundant [1].  6.4 Bounds for Translation and Compilation By inspecting Figure 12, we \nsee that the basic translation requires traversing the program once. Since the translation is purely \nsyn\u00adtactic and in each case it expands code by a constant factor, it takes linear time in the size of \nthe normalized program. As for the re.nements, note that read trampolining does not affect the bound \nsince it only performs a simple syntactic code replacement. Monomorphisation requires generating specialized \ninstances of the closure make function. Each instance with k arguments can be represented with O(k) words \nand can be generated in O(k) time. Since k is bounded by the number of arguments of the tail jump (or \nalloc command) being translated, monomorphisation requires linear time in the size of the normalized \ncode. Thus, we conclude that we can translate a normalized program in linear time while generating C \ncode whose size is within a constant factor of the nor\u00admalized program. Putting together this bound and \nthe bounds from normalization (Theorems 3 and 4), we can bound the time for com\u00adpilation and the size \nof the compiled programs. Theorem 5 (Compilation) Let P be a CL program with n blocks that requires m \nwords to represent. Let ML(P ) be the maximum number of live variables over all blocks of P and let TL(P \n) be the time for live-variable analysis. It takes O(n + n \u00b7 ML(P )+ TL(P )) time to compile the program \ninto C. The generated C code requires O(m+n\u00b7ML(P )) words to represent.  7. Implementation The implementation \nof our compiler, cealc, consists of a front\u00adend and a runtime library. The front-end transforms CEAL \ncode into C code. We use an off-the-shelf compiler (gcc) to compile the translated code and link it with \nthe runtime library. Our front-end uses an intra-procedural variant of our normal\u00adization algorithm that \nprocesses each core function independently from the rest of the program, rather than processing the core \npro\u00adgram as a whole (as presented in Section 5). This works because inter-procedural edges (i.e., tail \njump and call edges) in a rooted graph don t impact its dominator tree the immediate dominator of each \nfunction node is always the root. Hence, the subgraph for each function can be independently analyzed \nand transformed. Since each function s subgraph is often small compared to the to\u00adtal program size, we \nuse a simple, iterative algorithm for comput\u00ading dominators [30, 14] that is ef.cient on smaller graphs, \nrather than an asymptotically optimal algorithm with larger constant fac\u00adtors [19]. During normalization, \nwe create new functions by com\u00adputing their formal arguments as the live variables that .ow into their \nentry nodes in the original program graph. We use a stan\u00addard (iterative) liveness analysis for this \nstep, run on a per-function basis (e.g., [30, 7]). Since control .ow can be arbitrary (i.e., non\u00adreducible) \nand the number of local variables can only be bounded by the number of nodes in the function s subgraph, \nn, this iterative algorithm has O(n 3) worst case time for pathological cases. How\u00adever, since n is often \nsmall compared to the total program size and since functions are usually not pathological, we observe \nthat this approach works suf.ciently well in practice. Our front-end is implemented as an extension to \nCIL (C Inter\u00admediate Language), a library of tools used to parse, analyze and transform C code [31]. \nWe provide the CEAL core and mutator primitives as ordinary C function prototypes. We implement the ceal \nkeyword as a typedef for void. Since these syntactic ex\u00adtensions are each embedded within the syntax \nof C, we de.ne them with a conventional C header .le and do not modify CIL s C parser. To perform normalization, \nwe identify the core functions (which are marked by the ceal return type) and apply the intra-procedural \nvariant of the normalization algorithm. The translation from CL to C directly follows the discussions \nin Section 6. We translate muta\u00adtor primitives using simple (local) code substitution. Our runtime library \nprovides an implementation of the interface discussed in Section 6. The implementation is built on previous \nwork which focused on ef.ciently supporting automatic memory management for self-adjusting programs. \nWe extend the previous implementation with support for tail jumps via trampolining, and support for imperative \n(multiple-write) modi.able references [4]. Our front-end extends CIL with about 5,000 lines of additional \nObjective Caml (OCaml) code, and our runtime library consists of about 5,000 lines of C code. Our implementation \nis available online at http://ttic.uchicago.edu/~ceal. 8. Experimental Results We present an empirical \nevaluation of our implementation. 8.1 Experimental Setup and Measurements We run our experiments on a \n2Ghz Intel Xeon machine with 32 gigabytes of memory running Linux (kernel version 2.6). All our programs \nare sequential (no parallelism). We use gcc version 4.2.3 with -O3 to compile the translated C code. \nAll reported times are wall-clock times measured in milliseconds or seconds, averaged over .ve independent \nruns. For each benchmark, we consider a self-adjusting and a con\u00adventional version. Both versions are \nderived from a single CEAL program. We generate the conventional version by replacing mod\u00adi.able references \nwith conventional references, represented as a word-sized location in memory that can be read and written. \nUnlike modi.able references, the operations on conventional references are not traced and thus conventional \nprograms are not normalized. The resulting conventional versions are essentially the same as the static \nC code that a programmer would write for that benchmark. For each self-adjusting benchmark we measure \nthe time re\u00adquired for propagating a small modi.cation by using a special test mutator. Invoked after \nan initial run of the self-adjusting version, the test mutator performs two modi.cations for each element \nof the input: it deletes the element and performs change propagation, it in\u00adserts the element back and \nperforms change propagation. We report the average time for a modi.cation as the total running time of \nthe test mutator divided by the number of updates performed (usually two times the input size). For each \nbenchmark we measure the from-scratch running time of the conventional and the self-adjusting versions; \nwe de.ne the overhead as the ratio of the latter to the former. The overhead mea\u00adsures the slowdown caused \nby the dependence tracking techniques employed by self-adjusting computation. We measure the speedup \nfor a benchmark as the ratio of the from-scratch running time of the conventional version divided by \nthe average modi.cation time computed by running the test mutator.  8.2 Benchmark Suite Our benchmarks \ninclude list primitives, and more sophisticated algorithms for computational geometry and tree computations. \nList Primitives. Our list benchmarks include the list primitives filter, map, and reverse, minimum, and \nsum, which we expect to be self-explanatory, and the sorting algorithms quicksort and mergesort. Our \nmap benchmark maps each number x of the in\u00adput list to f(x)= Lx/3J + Lx/7J + Lx/9J in the output list. \nOur filter benchmark .lters out an input element x if and only if f(x) is odd. Our reverse benchmark \nreverses the input list. The list re\u00adductions minimum and sum .nd the minimum and the maximum elements \nin the given lists. We generate lists of n (uniformly) ran\u00addom integers as input for the list primitives. \nFor sorting algorithms, we generate lists of n (uniformly) random, 32-character strings. Computational \nGeometry. Our computational-geometry bench\u00admarks are quickhull, diameter, and distance; quickhull computes \nthe convex-hull of a point set using the classic algorithm with the same name; diameter computes the \ndiameter (the max\u00adimum distance between any two points) of a point set; distance computes the minimum \ndistance between two sets of points. The implementations of diameter and distance use quickhull as a \nsubroutine. For quickhull and distance, input points are drawn from a uniform distribution over the unit \nsquare in R2. For distance, two non-overlapping unit squares in R2 are used, and from each square we \ndraw half the input points. Tree-based Computations. The exptrees benchmark is a varia\u00adtion of our simple \nrunning example, and it uses .oating-point num\u00adbers in place of integers. We generate random, balanced \nexpression trees as input and perform modi.cations by changing their leaves. The tcon benchmark is an \nimplementation of the tree-contraction technique by Miller and Reif [28]. This this a technique rather \nthan a single benchmark because it can be customized to compute vari\u00adous properties of trees, e.g., [28, \n29, 6]. For our experiments, we generate binary input trees randomly and perform a generalized contraction \nwith no application-speci.c data or information. We measure the average time for an insertion/deletion \nof edge by iter\u00adating over all the edges as with other applications.  From-Scratch Propagation Application \nn Cnv. Self. O.H. Ave. Update Speedup Max Live filter 10.0M 0.5 7.4 14.2 2.1 \u00d7 10-6 2.4 \u00d7 105 3017.2M \nmap 10.0M 0.7 11.9 17.2 1.6 \u00d7 10-6 4.2 \u00d7 105 3494.6M reverse 10.0M 0.6 11.9 18.8 1.6 \u00d7 10-6 3.9 \u00d7 105 \n3494.6M minimum 10.0M 0.8 10.9 13.8 4.8 \u00d7 10-6 1.6 \u00d7 105 3819.4M sum 10.0M 0.8 10.9 13.9 7.0 \u00d7 10-5 1.1 \n\u00d7 104 3819.8M quicksort 1.0M 3.5 22.4 6.4 2.4 \u00d7 10-4 1.4 \u00d7 104 8956.7M quickhull 1.0M 1.1 12.3 11.5 2.3 \n\u00d7 10-4 4.6 \u00d7 103 6622.9M diameter 1.0M 1.0 12.1 12.0 1.2 \u00d7 10-4 8.3 \u00d7 103 6426.9M exptrees 10.0M 1.0 \n7.2 7.2 1.4 \u00d7 10-6 7.1 \u00d7 105 4821.1M mergesort 1.0M 6.1 37.6 6.1 1.2 \u00d7 10-4 5.1 \u00d7 104 15876.3M distance \n1.0M 1.0 11.0 11.0 1.3 \u00d7 10-3 7.5 \u00d7 102 5043.6M rctree-opt 1.0M 2.6 20.6 7.9 1.0 \u00d7 10-4 2.5 \u00d7 104 5843.7M \n Table 1. Summary of measurements with CEAL; all times in seconds and space in bytes. From-Scratch (Self.) \nPropagation Time Propagation Max Live Application n CEAL SaSML SaSML CEAL CEAL SaSML SaSML CEAL CEAL \nSaSML SaSML CEAL filter 1.0M 0.7 6.9 9.3 1.4 \u00d7 10-6 8.7 \u00d7 10-6 6.2 306.5M 1333.5M 4.4 map 1.0M 0.8 7.8 \n9.3 1.6 \u00d7 10-6 1.1 \u00d7 10-5 7.1 344.7M 1519.1M 4.4 reverse 1.0M 0.8 6.7 8.0 1.6 \u00d7 10-6 9.2 \u00d7 10-6 5.8 344.7M \n1446.7M 4.2 minimum 1.0M 1.1 5.1 4.6 3.4 \u00d7 10-6 3.0 \u00d7 10-5 8.8 388.4M 1113.8M 2.9 sum 1.0M 1.1 5.1 4.6 \n4.8 \u00d7 10-5 1.7 \u00d7 10-4 3.5 388.5M 1132.4M 2.9 quicksort 100.0K 1.6 43.8 26.9 1.6 \u00d7 10-4 2.6 \u00d7 10-3 15.6 \n775.4M 3719.9M 4.8 quickhull 100.0K 1.0 5.1 5.1 1.0 \u00d7 10-4 3.3 \u00d7 10-4 3.3 657.9M 737.7M 1.1 diameter \n100.0K 0.9 5.2 5.8 8.6 \u00d7 10-5 3.7 \u00d7 10-4 4.3 609.0M 899.5M 1.5 Table 2. Times and space for CEAL versus \nSaSML for a common set of benchmarks. 8.3 Results For brevity, we illustrate detailed results for one \nof our benchmarks, tree contraction (tcon), and summarize the others. Figure 13 shows the results with \ntcon. The leftmost .gure compares times for a from-scratch run of the conventional and self-adjusting \nversions; the middle graph shows the time for an average update; and the rightmost graph shows the speedup. \nThe results show that self\u00adadjusting version is slower by a constant factor (of about 8) than the conventional \nversion. Change propagation time increases slowly (logarithmically) with the input size. This linear-time \ngap between recomputing from scratch and change propagation yields speedups that exceed four orders of \nmagnitude even for moderately sized inputs. We also compare our implementation to that of an hand\u00adoptimized \nimplementation, which is between 3 4 times faster. We interpret this is as a very encouraging results \nfor the effectiveness of our compiler, which does not perform signi.cant optimizations. The accompanying \ntechnical report gives more details on this com\u00adparison [22]. Table 1 summarizes our results for CEAL \nbenchmarks at .xed input sizes of 1 million (written 1.0M ) and 10 million (writ\u00adten 10.0M ). From left \nto right, for each benchmark, we report the input size considered, the time for conventional and self-adjusting \nruns, the overhead, the average time for an update, the speedup, and the maximum live memory required \nfor running the experiments (both from-scratch and test mutator runs). The individual graphs for these \nbenchmarks resemble that of the tree contraction (Fig\u00adure 13). For the benchmarks run with input size \n10M, the average overhead is 14.2 and the average speedup is 1.4 \u00d7 105; for those run with input size \n1M, the average overhead is 9.2 and the average speedup is 3.6 \u00d7104. We note that for all benchmarks \nthe speedups are scalable and continue to increase with larger input sizes.  8.4 Comparison to SaSML \nTo measure the effectiveness of the CEAL approach to previously proposed approaches, we compare our implementation \nto SaSML, the state-of-art implementation of self-adjusting computation in SML [27]. Table 2 shows the \nrunning times for the common bench\u00admarks, taken on the same computer as the CEAL measurements, with inputs \ngenerated in the same way. For both CEAL and SaSML we report the from-scratch run time, the average update \ntime and the maximum live memory required for the experiment. A column  cealc gcc Program Lines Time \nSize Time Size Expression trees List primitives Mergesort Quicksort Quickhull Tree contraction Test Driver \n422 553 621 622 988 1918 4229 0.84 1.87 2.25 2.22 3.81 8.16 13.69 74K 109K 123K 123K 176K 338K 493K 0.34 \n0.49 0.54 0.54 0.72 1.03 2.61 58K 61K 62K 62K 66K 76K 110K SaSML labeled CEAL follows each pair of CEAL \nand SaSML measure\u00adments and it reports the ratio of the latter to the former. Since the SaSML benchmarks \ndo not scale well to the input sizes considered in Table 1, we make this comparison at smaller input \nsizes 1 mil\u00adlion and 100 thousand (written 100.0K ). Comparing the CEAL and SaSML .gures shows that CEAL \nis about 5 27 times faster for from-scratch runs (about 9 times faster on average) and 3 16 times faster \nfor change propagation (about 7 times faster on average). In addition, CEAL consumes up to 5 times less \nspace (about 3 times less space on average). An important problem with the SaSML implementation is that \nit relies on traditional tracing garbage collectors (i.e., the collec\u00adtors used by most SML runtime systems, \nincluding the runtime used by SaSML) which previous work has shown to be inherently incompatible with \nself-adjusting computation, preventing it from scaling to larger inputs [21]. Indeed, we observe that, \nwhile our implementation scales to larger inputs, SaSML benchmarks don t. To illustrate this, we limit \nthe heap size and measure the change\u00adpropagation slowdown computed as the time for a small modi.ca\u00adtion \n(measured by the test mutator) with SaSML divided by that with CEAL for the quicksort benchmark (Figure \n14). Each line ends roughly when the heap size is insuf.cient to hold the live memory required for that \ninput size. As the .gure shows, the slow\u00addown is not constant and increases super linearly with the input \nsize to quickly exceed an order of magnitude (can be as high as 75).  8.5 Performance of the Compiler \nWe evaluate the performance of our compiler, cealc, using test programs from our benchmark suite. Each \nof the programs we consider consists of a core, which includes all the core-CEAL code needed to run the \nbenchmark, and a corresponding test mutator for testing this core. We also consider a test driver program \nwhich consists of all the test mutators (one for each benchmark) and their corresponding core components. \nWe compile each program with cealc and record both the com\u00adpilation time and the size of the output binary. \nFor comparison pur\u00adposes, we also compile each program directly with gcc (i.e., with\u00adout cealc) by treating \nCEAL primitives as ordinary functions with external de.nitions. Table 3 shows the results of the comparison. \nAs can be seen, cealc is 3 8 times slower than gcc and creates binaries that are 2 5 times larger. In \npractice, we observe that the size of core functions can be bounded by a moderate constant. Thus the \nmaximum number of live variables, which is an intra-procedural property, is also bounded by a constant. \nBased on Theorem 5, we therefore expect the compiled binaries to be no more than a constant factor larger \nthan the source programs. Our experiments show that this constant to be between 2 and 5 for the considered \nprograms. Theorem 5 implies that the compilation time can bounded by the size of the program plus the \ntime for live-variable analysis. Since Table 3. Compilation times (in seconds) and binary sizes (in bytes) \nfor some CEAL programs. All compiled with -O0. our implementation performs live variables analysis and \nconstructs dominator trees on a per-function basis (Section 7), and since the sizes of core functions \nare typically bounded by a moderate constant, these require linear time in the size of the program. We \ntherefore expect to see the compilation times to be linear in the size of the generated code. Indeed \nFigure 15 shows that the cealc compilation times increase nearly linearly with size of the compiled binaries. \n9. Related Work We discuss the most closely related work in the rest of the paper. In this section, we \nmention some other work that is related more peripherally. Incremental and Self-Adjusting Computation. \nThe problem of developing techniques to enable computations respond to in\u00adcremental changes to their \noutput have been studied since the early 80 s. We refer the reader to the survey by Ramalingam and Reps \n[34] and a recent paper [27] for a more detailed set of ref\u00aderences. Effective early approaches to incremental \ncomputation either use dependence graphs [16] or memoization (e.g., [33, 2]). Self-adjusting computation \ngeneralizes dependence graphs tech\u00adniques by introducing dynamic dependence graphs, which enables a change \npropagation algorithm update the structure of the com\u00adputation based on data modi.cations, and combining \nthem with a form of computation memoization that permits imperative updates to memory [3]. Dominators. \nThe dominator relation has common use in com\u00adpilers that perform program analysis and optimization (e.g., \n[7, 15, 18]). There are a number of asymptotically ef.cient algo\u00adrithms for computing dominators (e.g., \n[26, 19]). In practice simple but asymptotically inef.cient algorithms also perform reasonably well [14]. \nOur implementation uses the simple algorithm described in many compiler books, e.g., [30]. Tail Calls. \nWe use a selective trampolining to support tail calls ef.ciently (Section 6). Several other proposals \nto supporting tail calls in C exists (e.g., [36, 23, 9, 20]). Peyton Jones summarizes some of these techniques \n[24] and discusses the tradeoffs. The primary advantage of trampolining is that it is fully portable; \nthe disadvantage is its cost, which our compiler reduces by piggy\u00adbacking closure creation with those \nof the reads.  10. Discussions We discuss some limitations of our approach and propose future research \ndirections. Syntax and Types for Modi.ables. The modi.able primitives read and write assume that the \ntype of a modi.able s contents is void*. As a result, their uses sometimes require explicit type coercions \n(e.g., as in Figure 2). Furthermore, the primitives have a function-like syntax, rather than the familiar \nC-like syntax for dereferencing (reading) and assignment (writing). It may be possible to support more \nconventional C syntax and avoid type coercions by generalizing the notion of a modi.able ref\u00aderence to \nthat of a modi.able .eld . In this approach, the program\u00admer would annotate the .elds of structs that \nare subject to change across core updates with a new keyword (e.g., mod) that indicates that the .eld \nis modi.able. Accessing (reading) and assigning to (writing) these .elds would use the same syntax as \nconventional C structs, which would make the use of modi.able primitives im\u00adplicit. Just as conventional \n.elds carry type information, each mod\u00adi.able .eld would also carry a type that could be used to ensure \nthat its uses are well-typed. This approach would generalize modi.able references since they could be \neasily encoded as struct with a single modi.able .eld. Automatic Minimization of Read Bodies. Conceptually, \neach read operation in CEAL has an implicit read body which consists of the code that uses the read value. \nOur normalization algorithm approximates each read body conservatively by assuming that it extends to \nthe end of the function containing the associated read operation. In general, however, a read value may \nbe used in only a small portion of the body found by normalization. In these cases it s often advantageous \n(though not always necessary) to refactor the core program so that each read body identi.ed by normalization \nis minimal, i.e., it contains no code that is independent from the read value. In the proposed approach \nthe programmer can perform this refactoring by hand, but we expect that a compiler mechanism could handle \nsome cases for refactoring automatically by employ\u00ading further data-and control-.ow analysis. Support \nfor Return Values. Currently, core functions in CEAL cannot return values as functions can in C, but \ninstead use destina\u00adtion-passing style (DPS) to communicate results through modi.\u00adables. In DPS, the \ncaller provides one or more destinations (mod\u00adi.ables) to the callee who writes these destinations with \nits results before returning control to the caller; to access the results, the caller reads the value \nof each destination (e.g., in Figure 2). By restrict\u00ading core programs to DPS, we ensure that the caller/callee \ndata\u00addependencies can be correctly tracked via modi.able references and their associated operations. \nAlthough DPS simpli.es tracking data dependencies in CEAL, it also forces the programmer to essentially \nperform a DPS con\u00adversion by hand. Moreover, the DPS restriction can be viewed as overly conservative: \nif the return value of a core function is not af\u00adfected by changes to modi.ables then the use of this \nvalue need not be tracked via modi.able references, i.e., it can be returned di\u00adrectly to the caller. \nWe expect that future work on CEAL can add support conventional C-style return values by adding an automatic \nDPS conversion to the CEAL compiler that acts selectively: when a return value is subject to change across \nmodi.able updates the compiler should automatically DPS convert this function (and its call sites), otherwise, \nthe function can return a value as usual, with\u00adout any special treatment. Optimizations. Although the \nproposed approach is faster and re\u00adquires less memory than previous approaches, we expect that future \noptimizations will offer additional time and space savings. As an example, it may be possible to reduce \nthe overhead of tracing the core primitives (e.g., read) when they are used in consecutive sequences \n(e.g., Figure 2 contains a pair of consecutive reads). The compiler could detect such uses and allow \nthe runtime to trace them as a group rather than individually, thereby reducing their individual costs. \nIn particular, since consecutive reads usually store similar closures in the trace (i.e., closures with \none or more common values), tracing such reads using a single closure would offer a signi.cant time and \nspace savings. In addition to reducing the tracing overhead, future work can explore more ef.cient ways \nof supporting tail calls in normalized programs. The proposed approach uses trampolines since they are \nsimple to implement in a portal manner. However, each bounce on a trampoline requires at least a function-return \n(passing control to the trampoline) and an indirect call (to the function contained in the trampolined \nclosure). By contrast, a real tail call is little more than an unconditional jump [36, 24]. Intermediate \nRepresentation. Our core language CL can be thought of as a simpli.ed source language for self-adjusting \ncom\u00adputation. As we show, this language is suf.cient to implement and reason about the proposed compilation \ntechniques. However, we expect that both implementing and reasoning about future anal\u00adyses, transformations \nand optimizations (e.g., those proposed in this section) will be simpli.ed by translating into an intermediate \nlanguage that is either based on static single assignment (SSA) form [15], or a suitable alternative \n[10]. Furthermore, since SSA form shares a close relationship to functional programming [8, 25], we expect \nthat using it as an intermediate language for CEAL will allow future work on CEAL to be more directly \napplicable to the self-adjusting languages that extend existing functional languages (e.g., SaSML [27]). \n11. Conclusion We describe a C-based language for writing self-adjusting pro\u00adgrams in a style similar \nto conventional C programs, present com\u00adpilation strategies for the language, and describe &#38; evaluate \nour im\u00adplementation. This is the .rst result in making self-adjusting com\u00adputation work in its full generality \nwith a (low-level) language that does not support higher-order features or offer automatic memory management. \nOur experiments show that the proposed approach is ef.cient in practice and signi.cantly improves performance \nover previous approaches. Acknowledgements We thank Matthew Fluet for many helpful discussions and sugges\u00adtions, \nand the anonymous reviewers for their valuable feedback. References [1] MLton. http://mlton.org/. [2] \nMart\u00b4in Abadi, Butler W. Lampson, and Jean-Jacques L\u00b4evy. Analysis and Caching of Dependencies. In Proceedings \nof the International Conference on Functional Programming, pages 83 91, 1996. [3] Umut A. Acar. Self-adjusting \ncomputation (an overview). In Proceedings of ACM SIGPLAN Symposium on Partial Evaluation and Semantics-Based \nProgram Manipulation, 2009. [4] Umut A. Acar, Amal Ahmed, and Matthias Blume. Imperative self\u00adadjusting \ncomputation. In Proceedings of the 25th Annual ACM Symposium on Principles of Programming Languages, \n2008.  [5] Umut A. Acar, Guy E. Blelloch, Kanat Tangwongsan, and Duru T\u00a8urko.glu. Robust Kinetic Convex \nHulls in 3D. In Proceedings of the 16th Annual European Symposium on Algorithms, September 2008. [6] \nUmut A. Acar, Guy E. Blelloch, and Jorge L. Vittes. An experimental analysis of change propagation in \ndynamic trees. In Workshop on Algorithm Engineering and Experimentation, 2005. [7] Alfred V. Aho, Ravi \nSethi, and Jeffrey D. Ullman. Compilers: principles, techniques, and tools. Addison-Wesley Longman Publishing \nCo., Inc., Boston, MA, USA, 1986. [8] Andrew W. Appel. SSA is functional programming. SIGPLAN Not., \n33(4):17 20, 1998. [9] Henry G. Baker. Cons should not cons its arguments, part II: Cheney on the MTA. \nSIGPLAN Not., 30(9):17 20, 1995. [10] J. A. Bergstra, T. B. Dinesh, and J. Heering. A complete transfor\u00admational \ntoolkit for compilers. ACM Transactions on Programming Languages and Systems, 19:639 684, 1996. [11] \nGerth Stolting Brodal and Riko Jacob. Dynamic planar convex hull. In Proceedings of the 43rd Annual IEEE \nSymposium on Foundations of Computer Science, pages 617 626, 2002. [12] Magnus Carlsson. Monads for Incremental \nComputing. In Proceedings of the 7th ACM SIGPLAN International Conference on Functional programming, \npages 26 35. ACM Press, 2002. [13] Y.-J. Chiang and R. Tamassia. Dynamic algorithms in computational \ngeometry. Proceedings of the IEEE, 80(9):1412 1434, 1992. [14] Keith D. Cooper, Timothy J. Harvey, and \nKen Kennedy. A simple, fast dominance algorithm. [15] Ron Cytron, Jeanne Ferrante, Barry K. Rosen, Mark \nN. Wegman, and F. Kenneth Zadeck. Ef.ciently computing static single assignment form and the control \ndependence graph. ACM Transactions on Programming Languages and Systems, 13(4):451 490, 1991. [16] Alan \nDemers, Thomas Reps, and Tim Teitelbaum. Incremental Evaluation of Attribute Grammars with Application \nto Syntax-directed Editors. In Proceedings of the 8th Annual ACM Symposium on Principles of Programming \nLanguages, pages 105 116, 1981. [17] David Eppstein, Zvi Galil, and Giuseppe F. Italiano. Dynamic graph \nalgorithms. In Mikhail J. Atallah, editor, Algorithms and Theory of Computation Handbook, chapter 8. \nCRC Press, 1999. [18] Matthew Fluet and Stephen Weeks. Conti.cation using dominators. In Proceedings \nof the International Conference on Functional Programming, pages 2 13, 2001. [19] Loukas Georgiadis and \nRobert E. Tarjan. Finding dominators revisited: extended abstract. In Proceedings of the .fteenth annual \nACM-SIAM symposium on Discrete algorithms, pages 869 878, 2004. [20] Jr. Guy L. Steele. Rabbit: A compiler \nfor scheme. Technical report, Cambridge, MA, USA, 1978. [21] Matthew A. Hammer and Umut A. Acar. Memory \nmanagement for self-adjusting computation. In ISMM 08: Proceedings of the 7th international symposium \non Memory management, pages 51 60, 2008. [22] Matthew A. Hammer, Umut A. Acar, and Yan Chen. CEAL: A \nC-based language for self-adjusting computation. Technical Report TTIC-TR-2009-2, Toyota Technological \nInstitute, 2009. [23] Simon L Peyton Jones. Implementing lazy functional languages on stock hardware: \nThe spineless tagless g-machine. Journal of Functional Programming, 2:127 202, 1992. [24] Simon Peyton \nJones. C--: A portable assembly language. In Proceedings of the 1997 Workshop on Implementing Functional \nLanguages. Springer Verlag, 1998. [25] Richard A. Kelsey. A correspondence between continuation passing \nstyle and static single assignment form. In Papers from the 1995 ACM SIGPLAN workshop on Intermediate \nrepresentations, pages 13 22, New York, NY, USA, 1995. ACM. [26] Thomas Lengauer and Robert Endre Tarjan. \nA fast algorithm for .nding dominators in a .owgraph. ACM Transactions on Programming Languages and Systems, \n1(1):121 141, 1979. [27] Ruy Ley-Wild, Matthew Fluet, and Umut A. Acar. Compiling self-adjusting programs \nwith continuations. In Proceedings of the International Conference on Functional Programming, 2008. [28] \nGary L. Miller and John H. Reif. Parallel tree contraction, part I: Fundamentals. Advances in Computing \nResearch, 5:47 72, 1989. [29] Gary L. Miller and John H. Reif. Parallel tree contraction, part 2: Further \napplications. SIAM Journal on Computing, 20(6):1128 1147, 1991. [30] Steven S. Muchnick. Advanced compiler \ndesign and implementation. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 1997. [31] George \nC. Necula, Scott Mcpeak, S. P. Rahul, and Westley Weimer. Cil: Intermediate language and tools for analysis \nand transformation of C programs. In In International Conference on Compiler Construction, pages 213 \n228, 2002. [32] Mark H. Overmars and Jan van Leeuwen. Maintenance of con.g\u00adurations in the plane. Journal \nof Computer and System Sciences, 23:166 204, 1981. [33] William Pugh and Tim Teitelbaum. Incremental \ncomputation via function caching. In Proceedings of the 16th Annual ACM Symposium on Principles of Programming \nLanguages, pages 315 328, 1989. [34] G. Ramalingam and T. Reps. A Categorized Bibliography on Incremental \nComputation. In Proceedings of the 20th Annual ACM Symposium on Principles of Programming Languages, \npages 502 510, 1993. [35] Ajeet Shankar and Rastislav Bodik. DITTO: Automatic Incremental\u00adization of \nData Structure Invariant Checks (in Java). In Proceedings of the ACM SIGPLAN 2007 Conference on Programming \nlanguage Design and Implementation, 2007. [36] David Tarditi, Peter Lee, and Anurag Acharya. No assembly \nrequired: compiling standard ML to C. ACM Letters on Programming Languages and Systems, 1(2):161 177, \n1992. [37] Andrew Tolmach and Dino P. Oliva. From ML to Ada: Strongly-typed language interoperability \nvia source translation. Journal of Functional Programming, 8(4):367 412, 1998.    \n\t\t\t", "proc_id": "1542476", "abstract": "<p>Self-adjusting computation offers a language-centric approach to writing programs that can automatically respond to modifications to their data (e.g., inputs). Except for several domain-specific implementations, however, all previous implementations of self-adjusting computation assume mostly functional, higher-order languages such as Standard ML. Prior to this work, it was not known if self-adjusting computation can be made to work with low-level, imperative languages such as C without placing undue burden on the programmer.</p> <p>We describe the design and implementation of CEAL: a C-based language for self-adjusting computation. The language is fully general and extends C with a small number of primitives to enable writing self-adjusting programs in a style similar to conventional C programs. We present efficient compilation techniques for translating CEAL programs into C that can be compiled with existing C compilers using primitives supplied by a run-time library for self-adjusting computation. We implement the proposed compiler and evaluate its effectiveness. Our experiments show that CEAL is effective in practice: compiled self-adjusting programs respond to small modifications to their data by orders of magnitude faster than recomputing from scratch while slowing down a from-scratch run by a moderate constant factor. Compared to previous work, we measure significant space and time improvements.</p>", "authors": [{"name": "Matthew A. Hammer", "author_profile_id": "81330491901", "affiliation": "Toyota Technological Institute at Chicago, Chicago, IL, USA", "person_id": "P1464381", "email_address": "", "orcid_id": ""}, {"name": "Umut A. Acar", "author_profile_id": "81100077236", "affiliation": "Toyota Technological Institute at Chicago, Chicago, IL, USA", "person_id": "P1464382", "email_address": "", "orcid_id": ""}, {"name": "Yan Chen", "author_profile_id": "81361601201", "affiliation": "Toyota Technological Institute at Chicago, Chicago, IL, USA", "person_id": "P1464383", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1542476.1542480", "year": "2009", "article_id": "1542480", "conference": "PLDI", "title": "CEAL: a C-based language for self-adjusting computation", "url": "http://dl.acm.org/citation.cfm?id=1542480"}