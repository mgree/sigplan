{"article_publication_date": "06-15-2009", "fulltext": "\n Snugglebug:APowerfulApproachToWeakest Preconditions Satish Chandra Stephen J. Fink Manu Sridharan IBMT.J.Watson \nResearch Center {satishchandra,sj.nk,msridhar}@us.ibm.com Abstract Symbolic analysis shows promise asa \nfoundation forbug-.nding, speci.cation inference, veri.cation, and test generation. This pa\u00adper addresses \ndemand-driven symbolic analysis for object-oriented programs and frameworks. Manysuch codes comprise \nlarge, partial programs with highly dynamic behaviors polymorphism, re.ec\u00adtion, and so on posing signi.cant \nscalability challenges for any static analysis. We present an approach based on interprocedural backwards \npropagation of weakest preconditions. We present several novel techniques to improve the ef.ciency of \nsuch analysis. First, we present directed call graph construction,where call graph construc\u00adtion and \nsymbolic analysis are interleaved. With this technique, call graph construction is guided by constraints \ndiscovered during symbolic analysis, obviating the need for exhaustively exploring a large, conservative \ncall graph. Second, we describe generalization, a technique that greatly increases the reusability of \nprocedure sum\u00admaries computed during interprocedural analysis. Instead of tabu\u00adlating how a procedure \ntransforms a symbolic state in its entirety, our technique tabulates how the procedure transforms only \nthe per\u00adtinent portion of the symbolic state. Additionally, we show how integrating an inexpensive, custom \nlogic simpli.er with weakest precondition computation dramatically improves performance. We have implemented \nthe analysis in a tool called SNUGGLEBUG and evaluated it as a bug-report feasibility checker. Our results \nshow that the algorithmic techniques were critical for successfully analyzing large Java applications. \nCategories and Subject Descriptors D.2.4 [Software/Program Veri.cation]:Validation; D.2.5[Testing and \nDebugging]: Sym\u00adbolic execution GeneralTerms Algorithms, Languages,Veri.cation Keywords Interprocedural \nanalysis, symbolic analysis, weakest preconditions 1. Introduction We consider the problem of .nding \na preconditionf that necessar\u00adily drives a program from a particular entrypoint m to a particular goal \nstate g.Ageneral solution to this problem would have numer\u00adous applications in tools for software engineering, \nsuch as: Permission to make digital or hard copies of all or part of this work for personal or classroom \nuse is granted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page.To copyotherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. PLDI \n09, June 15 20, 2009, Dublin, Ireland. Copyright c &#38;#169; 2009ACM 978-1-60558-392-1/09/06... $5.00. \n Speci.cation discovery and API hardening Here, g could represent some behavior of a library, and a discovered \nprecon\u00addition would illustrate how to make such behavior occur. For example, g might represent that the \nlibrary throws a particular exception at a particular line of code. Presented with a precon\u00addition for \nan exceptional exit, a library developer might either change the code to avoid the exception, or add \nthe precondition to the documentation.  Bug validation Such analysis could reduce the impactoffalse \npositives fromabug-.nding tool,by treating eachbug report as a goal state and searching for suf.cient \npreconditions. When a tool .ndsa suf.cient precondition forabug report, said report would be considered \nvalidated , and hence deserve higher priority.  Test case generationGiven a precondition f for method \nfoo, one may wish to construct a test case that executes foo in a state satisfying f (e.g., as a debugging \naid). Suppose we con\u00adstruct a universal driver program that can execute candidate sequences of method \ncalls, which embody a space of possible tests. If we can force the universal driver to the goal state \nof f at the entry of foo, a tool could output the corresponding method sequence as the desired test. \n In this paper,we desirea sound solution to the goal-reachability problem, one that models the exact \nsemantics of all statements, in\u00adcluding interprocedural .ow and exceptional conditions. When the analysis \n.nds a precondition f for g, we insist that the analysis guarantee that any state which satis.es f must \nnecessarily drive program execution to g. No other exceptions will be thrown be\u00adfore reaching g.Asound \nanalysis must necessarily perform full in\u00adterprocedural analysis, since the analysis cannot optimistically \nas\u00adsume that certain method calls have no side effects or even return normally. We propose a solution \nto this goal-reachabilityproblem based on backwardsymbolic analysis. In principle, such an analysis com\u00adputes \nweakest preconditions [12]over each control-.owpath, going backwards from the goal statement to the entrypoint. \nIf the com\u00adputed precondition f for anypath r is satis.able, then a satisfying assignment for f gives \ninputs thatwould forceexecution along r to the goal. Backward symbolic analysis suits our problem for \na number of reasons. The analysis is sound and models concrete semantics (i.e., no abstraction), and \nhence it outputs no false positives. It is also demand driven; by design, it explores only states that \nare relevant to reaching the goal, unlike symbolic execution [23] and most testing-based techniques(e.g., \n[17]). Finally, in contrast to testing, backward symbolic analysis does not require a complete program;it \ncan analyze libraries withoutanexecution environment and client code. Backward symbolic analysis with \nconcrete semantics is neces\u00adsarily incomplete.Itmayfailto .nda suitable precondition,even if one exists, \nin the presence of loops and recursion. This is not an immediate disquali.cation of its utility: our \nexperiments show that for goals arisingin realistic programs,a systematic searchof executions even through \nprograms containing loops and recursive procedures often succeeds in producing a suitable precondition. \n Challenges Real-world programs present many challenges for weakest precondition(wp)analysis. The .rst \nproblem arises from the sheer scale of large programs. Even in loop-free programs, symbolic analysisfacesanexponentialexplosionduetothe \nnumber of distinct paths through the program. In straight-line code alone, handling language features \nlike aliasing and type tests can require disjunctions, another source of state explosion. Procedure calls \nboth exacerbate these dif.culties and introduce entirely new challenges, especially for large object-oriented \nli\u00adbraries and frameworks.For object-oriented programs, performing interprocedural analysis requires \ndetermining the possible targets of virtual method calls. Unfortunately, standard call graph con\u00adstruction \nalgorithms [18] face myriad dif.culties disambiguating virtual calls in real-world libraries, due to \nthe scale of the pro\u00adgrams, unknown aliasing that clients might establish, and dynamic language features \nlike re.ection. Analyzing all possible virtual call targets as computedby simpler techniques(e.g., using \nthe type sys\u00adtem) dramatically reduces analysis scalability. Evenifcall graph dif.culties were resolved, \nthe standard chal\u00adlenges of interprocedural analysis remain. The best-known previ\u00adous work in this area, \nESC/Java [15], performed only intraproce\u00addural analysis, requiring programmers to provide appropriate \nspec\u00adi.cations for called methods. However, the concomitant annotation burdenofthisapproachcanbeheavyevenwithtool \nassistance[14]. The simplest approach to automatic interprocedural analysis is to inline callees. However, \ninliningfails to terminate with recursion, and furthermore often leads to an exponential explosion in \npro\u00adgram size; moreover, it sacri.ces anypossibility of reuse of anal\u00adysis of a procedure. An alternative \nsolution, long investigated for interprocedural data.ow analysis [29],worksby computing proce\u00addure summaries \nautomatically and reusing them to reduce redun\u00addant computation. The challenge in this approach, when \napplied to symbolic analysis, is to ef.ciently compute summaries that are general enough to reuse frequently. \nContributions We present an approach to interprocedural weak\u00adest precondition computation based on Sharir \nand Pnueli s func\u00adtional interprocedural analysis framework [29]. Ourkeycontribu\u00adtions lie in the following \ntechniques, which enable analysis of real\u00adworld object-oriented programs: 1. Directed Call Graph Construction \nWe show an iterative algo\u00adrithm that interleaves symbolic analysis with call graph con\u00adstruction. The \ncall graph grows in stages, driven by feedback from symbolic analysis, in an attempt to explore only \ncallees consistent with the goal. Furthermore, the technique requires no whole-program analysis beyond \nclass hierarchyconstruction. 2. Generalization We describe a technique to enhance summary reuse via \ncomputation of generalized procedure summaries, in the context of the Reps-Horwitz-Sagiv (RHS) tabulation-based \nanalysis framework [26]. 3. We show how integrating an inexpensive, custom logic sim\u00adpli.er with weakest \nprecondition computation dramatically im\u00adproves performance.  Wehaveimplemented this approachinatool \ncalledSNUGGLEBUG and evaluated it on challenging bug-validation tasks from large framework-oriented Java \napplications, including the open-source Tomcat web server and the Eclipse IDE.SNUGGLEBUG succeeded in \nestablishing concrete preconditions for 29 out of 38 feasible goals it considered, each within a half-hour \nlimit. To our knowl\u00adedge, this paper is the .rst to successfully apply demand-driven interprocedural \nsymbolic analysis to programs of this scale. Our evaluation showed that the techniques listed above were \ncritical for successful analysis of these applications. Precomputed call graphs (without directed call \ngraph construction) often encom\u00adpassed thousands of methods, can overwhelm symbolic analysis, whereas \nthe directed call graph construction needed at most 93 methods. Generalization improved the frequencyof \nsummary-edge reuse to 87% from 5%, resulting in a factor of two bottom-line speedup. Finally, the integrated \ncustom simpli.er improved perfor\u00admanceby roughlyafactorof ten. The remainderofthispaperproceedsasfollows.Section2gives \nan overview of our techniques, and Section 3 describes our core analysis architecture. Section 4 presents \ndirected call graph con\u00adstruction, and Section5 shows generalization. Section6 presents other details \nof our system relevant to performance. Section 7 showsexperimental results, Section8discusses relatedwork, \nand Section9concludes. 2. Overview Here, we give an overview of the key techniques employed in SNUGGLEBUG, \nusing the examples in Figures 1 and 2. For Figure 1, we wish to discover a precondition for the public \nentrypoint method that will force program execution to line 38, whichthrowsanexception.This preconditioncouldbeusefuleither \nto.ndbugsoras documentation.Inthiscase,weshalldiscoverthat line 38 is reached if entrypoint is invoked \nwith a NewCarList containing a Car whose year is not 2009. Firstwegivean informal tasteoftheweakest precondition(wp) \npropagation[12]atthe coreofSNUGGLEBUG. Starting backwards from the goal at line 38 of Figure 1, wp computes \nthe following symbolic states, all preconditions for reaching the error: 1. Before line 38, true 2. \nBefore line 37, y  = 2009 3. Before line 36, x.year = 2009. 4. Before line 35, x.year = 2009 . newCarsOnly \nand so on. At each step, we apply the statement s wp transformer to a postcondition to arrive at a precondition. \nThekeychallenge in analyzing Figure1is handling its virtual method calls. Continuing the above wp computation \nbackwards in checkValid() requires analyzing calls to Iterator.next(), Iterator.hasNext(), and Collection.iterator(). \nThese calls can havemanypossible targets in the application or libraries had this code relied on the \nEclipse UI subsystem, there would be at least 86 concrete implementations of iterator(), 135 implemen\u00adtations \nof hasNext(), and 157 implementations of next(). Which of these methods should the analysisexplore? Answering \nthis ques\u00adtion accurately is a requirement for ef.cient analysis of this exam\u00adple. In this case, when \nline 38 is reached from entrypoint, a path condition constrains the receivers of these calls to the particular \ntypes NewCarList and NewCarList$Itr. 2.1 Directed Call Graph Construction Directed call graph constructionworksby \niteratively skipping anal\u00adysis of certain virtual calls and then choosing call targets based on feedback \nfrom symbolic analysis. In the .rst pass of backward anal\u00adysis of Figure 1, we skip the calls to iterator(), \nhasNext(), and next() in checkValid(), reaching entry of entrypoint with a formula that symbolically \nrepresents their return values and possible effects. On any path from line 38 to the entry of entrypoint, \nwe have the constraint that c must be a subtype of NewCarList (due to line 28). With this constraint, \nthe analysis next decides to expand the iterator() call at line 32 to include  1 public class Car { \n2 int year; 3 void setYear(int y) { this.year = y; } 4 int getYear() { return year; } 5 } 6 public class \nNewCarList implements List<Car> { 7 private Car[] elems = new Car[10]; 8 public final Iterator iterator() \n{ 9 return new Itr(); 10 } 11 public Car set(int i, Car c) { 12 Car old = elems[i]; 13 elems[i] = c; \n14 return old; 15 } 16 private class Itr implements Iterator { 17 int cursor = 0; 18 public boolean hasNext() \n{ 19 return cursor != elems.length; 20 } 21 public Object next() { 22 return elems[cursor++]; 23 } 24 \n} 25 // other List methods... 26 } 27 public static void entrypoint(Collection<Car> c) 28 checkValid(c, \nc instanceof NewCarList); 29 } 30 private static void checkValid(Collection<Car> c, 31 boolean newCarsOnly) \nthrows MyException { 32 Iterator<Car> it = c.iterator(); 33 while (it.hasNext()) { 34 Car x = it.next(); \n35 if (newCarsOnly) { 36 int y = x.getYear(); 37 if (y != 2009) { 38 throw new MyException(); // GOAL \n39 } 40 } 41 // ... other checks. 42 } 43 } Figure 1. A(contrived) motivating example NewCarList.iterator() \nas a target for analysis. In this man\u00adner, symbolic analysisgives crucial feedback to the interprocedural \npropagation. Newly-added callees can in.uence which methods are added to the call graphin later analysis \nstages.For ourexample, after adding NewCarList.iterator() to the call graph, the analysis discovers that \nit returns an object of type NewCarList$Itr (line 9). This fact constrains the call graph expansion for \nthe calls at lines 33 and line 34, forcing analysis of methods in NewCarList$Itr for subsequent passes. \nNotice how the call target selection is guided by the latest known symbolic constraints, in this case \nthe concrete type of the return value from NewCarList.iterator(). Directed call graph construction improves \nscalability because in practice, the analysis needs to only explore a small portion of an overapproximate(worst \ncase) call graph.An upfront static analysis would have dif.culty determining the right part of the call \ngraph to explore it is the interleaving of interprocedural symbolic analysis and call graph construction \nthat makes this strategy work.  2.2 Generalization Our interprocedural analysis algorithm embodiesafunctional \ninter\u00adprocedural data.ow analysis [29] using an adaptation of the RHS tabulation algorithm [26]. In our \ncase, the domain of this data.ow problem consists of symbolic formulae, where each formula rep\u00adresents \na set of concrete program states. The tabulation algorithm builds partial procedure summaries on-the-.y, \nas it discovers pairs 1 public static void test(NewCarList l) throws MyException { 2 Car c1 = new Car(); \n3 l.set(0,c1); 4 c1.setYear(2008); // a bad car 5 Car c2 = new Car(); 6 l.set(1,c2); 7 c2.setYear(2009); \n// a good car 8 entrypoint(l); 9 } Figure 2. Motivating example for modular reuse. of input-outputfacts \nfor each procedure. The algorithm maintains a table of input-output summaries for each procedure, and \nreuses a computed summary whenit propagates an inputfact that yieldsa hit in the summary table. With \nsymbolic formulae as data.owfacts, our interprocedural analysiswill hit inthe summarytableonlywhen presentedwitha \nsyntactically identical formula. However, even when using canoni\u00adcal formsto represent symbolic formulae, \nsummaryreuse based on syntactic matching oftenfails.We present generalization, a tech\u00adnique to compute \nmore general symbolic summaries. Generaliza\u00adtion lifts summariesover individual symbolicfactsinto summaries \nover more general classesoffacts. We now show informally how generalization applies to the example code \nin Figure 2. Here, the test() method invokes entrypoint() (from Figure1)witha NewCarList that causes \nthe previously discussed exception at line 38 in checkValid(). Note that test() contains two calls to \neach of NewCarList.set() and Car.setYear(); we shall show how generalization enables analyzing each of \nthese methods once and re-using the result. Consider the symbolic states that arise during wp analysis \nim\u00admediately after the call sites to setYear() and set() in Figure2 (certain irrelevant conjuncts elided \nfor brevity): set(), 6 l.elems[0] = c2 . l.elems[0].year = 2009 set(), 3 read(update(year, c1, 2008), \nl.elems[0]) = 2009 The formula notation will be explained further in Section 3; for now, just notice \nthat the two formulae for setYear() are syntac\u00adtically different; likewise for set(). Since these formulae \nare the inputkeys in the summary tables for setYear() and set(),1 the tabulation-based analysis cannot \nachieve anyreuse for the repeated calls. Generalization enables computation of more general symbolic \nsummaries [10] within a tabulation-based interprocedural analysis by (1) extracting references to concrete \nlocations in input formu\u00adlae for procedures and (2) using the frame rule from separation logic [27] to \nanalyze procedures only with relevant, genericized conjuncts. Generalizationofthe postconditionatline7yields: \nl0 = 2009 . l0 = read(year,l1) . l1 = l.elems[0] . l.elems.length > 0 Compared to the corresponding formula \nin the table above, the reference to the year .eld has been extracted into a generic conjunct l0 = read(year,l1) \n(l0 and l1 are fresh variables; read is discussed in Section 3). The analysis then reasons via the frame \nrule [27] that since setYear() may modify only year, only this generic conjunct is relevant when analyzing \nthe method. Hence, the algorithm propagates only the generic conjunct l0 = read(year,l1), to the callee, \nyielding the following entry in 1In the actual tables, the formulae are translated into the callee namespace. \n setYear() s input-output table: setYear() l0 = read(year,l1) ------. l0 = read(update(year, this,y),l1) \nThe above representsa symbolic summary that appliesto an entire classoffacts, namely those that reference \nthe year .eld. General\u00adization of the post-condition for the setYear() callat line4again yields the input \nfact l0 = read(year,l1), enabling reuse of the above summary via tabulation. Similar reuse is achieved \nfor the calls to set(), as we shall show in Section 5. Generalized summaries promote reuse because methods \nfre\u00adquently embody context-independent behaviors. SNUGGLEBUG computes generalized summaries on demand \nwithin the interproce\u00addural wp computation, consistent with the overall demand-driven analysis approach. \nSection5gives further detailsonhowto inte\u00adgrate generalized summary computation intoa standard tabulation\u00adbased \nanalysis [26].  2.3 IntegratedTabulation and Simpli.cation Even with directed call graph construction \nand generalization, straightforward propagation of weakest preconditions would not scale to the size \nof programs we handle. In particular, we found that an approach of constructing weakest preconditions \ncomposi\u00adtionally and sending the results to an SMT solver to check decid\u00adability (as was done in previous \nsystems [15]) was not practical for our target programs. Afurtherkey to scalability lies in close co-operation \nbetween wp propagation and formula handling. At each step of wp prop\u00adagation, SNUGGLEBUG invokes an inexpensive \ncustom procedure to simplify formulae using language-speci.c theories, and it drops formulae from further \npropagation once proven unsatis.able. This technique also increases the effectiveness of directed call \ngraph construction and generalization. Section 6 describes our formula handling and other optimizations. \n3. Analysis Basics In this section, we .rst describe an intraprocedural wp calculation for Java (Section \n3.1) and then extend the analysis to handle proce\u00addure calls (Section 3.2). SNUGGLEBUG extends this core \nanalysis with directed call graph construction (Section4)and enhancements for modular reuse (Section \n5). 3.1 Intraprocedural Computation Our intraprocedural analysis operates on a SSA register-transfer \nlanguage representation with semantics close to Javabytecode. The .rst column ofTable1 shows some of \nthe statements in the lan\u00adguage; their semantics follow those of Java. The assume statement is a no-op \nif its condition is true and hangs otherwise. Follow\u00ading a conditional branch with condition c, the taken \nbranch jumps to assume(c) and the not taken branch to assume(!c). Note that many statements can throw \nimplicit exceptions corresponding to built-in Java safety conditions. We abbreviate null pointer, class \ncast, and array index out of bounds exceptions as NPE, CCE, and OOB respectively. The analysis operates \nona control-.ow graph (CFG)builtover this representation, where each basic block has at most one state\u00adment.2 \nEach CFG has a unique Entry and unique Exit node. Each block has distinct outgoing edges corresponding \nto normal execu\u00adtionanddifferent casesofexceptionalexecution. Exceptionaledges from a potentially excepting \nstatement go to either catch blocks or the exit node. SNUGGLEBUG represents symbolic states (the domain \nof the analysis) as quanti.er-free formulae in .rst-order logic with equal\u00adity.Table2informally presents \nsomeofthevocabularyofthislogic 2We elide some straightforward details involving SSAf statements. Functions \nand Constants Ti Mi Fi sigi read(f, v) update(f, v, w) aread(a, v, i) aupdate(a, w, i, v) typeOf(v) dispatch(t, \nsig) subType(t1, t2) type constants (one per concrete type) method constants (one per concrete method) \n.eld constants (one per declared .eld) constant corresponding to a method signature f(v), where f is \nVal . Val (a relational model of some declared .eld) functional update of f, i.e. f[v . w] a(v, i), where \na is Val \u00d7 Index . Val functional update of a, i.e. a[(w, i) . v] the type of object to which v points \nmethod to which signature sig will dispatch to on receiver of type t true ifftype t1 is subtype of type \nt2 in Java Axioms this = null .f..v..w.read(update(f, v, w), v) = w .f..v..w..u.u = v . read(update(f, \nv, w), u) = read(f, u) subType(T1, T2) . T1 is a subtype of T2 in Java dispatch(T1, sigA.m) = MB.m . \n.x.(x.class = T1 . x.m() dispatches to target method B.m()) .x.read(length, x) = 0 Table 2. Some representative \nsymbols and axioms in our theory for Java programs. and shows some representative axioms.3 In addition \nto primitive values and pointers, the vocabulary expresses relations between types, methods, .elds, and \nmethod signatures. The logic models Java .elds as relations manipulated with read and update from the \ntheory of arrays [25]. Java arrays (which are heap-allocated) are modeled with two-dimensional arrays \n(from the theory), indexed bya base pointer and an array index. The bottom halfofTable2 shows some representative \naxioms, such as the standard axioms that de.ne the theory of arrays. Additionally, the table shows ax\u00adioms \nbased on the type hierarchyof the program. The last axiom ensures that the length .eld of arrays is non-negative. \nTable1de.nes the weakest precondition transformers for sev\u00aderal statements. The notation f[t2/t1] means \nf with all syntactic occurrences of t1 replaced by t2.Technically, the wp transformer occurs on an outgoing \nedge from a basic block (recall that each basic block has at most one statement).For some statements, \nwp must take into account whether the CFG edge represents normal or exceptional control .ow, as indicated \nin the second column ofTa\u00adble 1. Note that the wp transformerfor callsinTable1only handles intraprocedural \nsemantics(i.e., reasoning about non-nullness of re\u00adceivers and virtual dispatch); Section 3.2 discusses \ninterprocedural analysis. Figure 3 gives pseudocode for an iterative computation of in\u00adtraprocedural \nweakest pre-conditions. The analysis computes a set (D)of symbolic states at each program point. After \ncomputing wp(s, fpost ) for a statement s and formula fpost , the algorithm (i)invokes a simpli.er on \nthe result and(ii)merges the simpli\u00ad.ed result with thefacts already present before s (line 8).We use \na lightweightbut highly effective simpli.er for(i), described in Section 6.For(ii), we perform ad hoc \nchecks to optimize away certain patterns of redundancy: for example, merge(f . a, f.!a) simpli.es to \nf;it is only necessary to propagatef further. In the presenceofloops,I NTRAWPmaynot terminate,butin practice,we \nonly need to run it until a suitable D(entry) is obtained, even if it is not the weakest such condition. \n(Section6contains more details on handling of loops.) Example Consider the method setYear in Figure 1. \nSuppose we wish to .nd the precondition for the predicate x.year == 2008 3Though the axioms include quanti.ers, \nthey never arise in the symbolic state representation.  statement edge condition wp(statement, f) v \n= w f[w/v] v = v1 op v2 f[(v1 op v2)/v] v = w.f normal successor NPE successor (w = null) . f[read(f, \nw)/v] (w = null) . f[fresh(NPE)/exc] v.f = w normal successor NPE successor (v = null) . f[update(f, \nv, w)/f] (v = null) . f[fresh(NPE)/exc] v = w[i] normal successor NPE successor OOB successor (w = null) \n. (i < read(length, w) . i = 0) . f[aread(a, w, i)/v] (w = null) . f[fresh(NPE)/exc] (w = null) . (i \n< 0 . i = read(length, w)) . f[fresh(OOB)/exc] w[i] = v normal successor NPE successor OOB successor \n(w = null) . (i < read(length, w) . i = 0) . f[aupdate(a, w, i, v)/a] (w = null) . f[fresh(NPE)/exc] \n(w = null) . (i < 0 . i = read(length, w)) . f[fresh(OOB)/exc] v = new T f[fresh(T )/v] assume c f . \nc v1 = (T ) v2 CCE successor normal successor v2 = null . \u00ac(subType(typeOf(v2), T )) . f[fresh(CCE)/exc] \n(v2 = null . subType(typeOf(v2), T )) . f[v2/v1] return v f[null/ exc][v/ ret] w = v.m() normal successor, \ncallee meth NPE successor meth = dispatch(typeOf(v), m()) . v = null (v = null) . f[fresh(NPE)/exc] \nTable 1. Speci.cation of wp for representative Java statements. v, w and c variables represent symbolic \nregisters in the input language, and corresponding free variables in the logic; theycan hold values of \neither primitive types (integers, reals) or pointers. exc is a special variable thatholdsapointertoanexceptionthathasbeen \nraisedbutnotcaught,and ret represents the return value. fresh(T ) returns a fresh value v from the domain \nof pointers, such that typeOf(v)= T . INTRAWP(CFG, postcondition) 1 var D:Statement .{Formula}2 var worklist:stack \nof(Statement, Formula) 3 .s . Statement , D(s) .\u00d8 4 worklist .{(Exit, postcondition)}5 while worklist \nis not empty 6 do (s',fpost ) . get from worklist 7 foreach s such that (s, s') is an edge in CFG 8 do \nfpre . merge(D(s), simplify(wp(s, fpost ))) 9 if fpre = FALSE 10 then D(s) . D(s) . fpre 11 add (s, fpre \n) to worklist 12 return D(Entry) Figure 3. Computation of intraprocedural weakest pre-conditions. at \nthe normal exit of setYear, where x is some global variable. The symbolic state representing this predicate \nand normal execu\u00adtion of setYear is exc = null . read(year,x) = 2008 at the exit of setYear. Traversing \nsetYear backwards, the analy\u00adsis .rst encounters a return statement (not shown in code). Apply\u00ading the \nwp transformer for the return statement substitutes null for exc, giving read(year,x) = 2008. Applying \nthe wp trans\u00adformer for the put.eld statement at line 3, we get this = null . read(update(year, this,y),x) \n= 2008. Before propagating this formula, the simpli.er uses the axiom this = null to obtain the simpler \nformula read(update(year, this,y),x) = 2008. If later in the analysis, some path condition ensures that \nthis = x, then the simpli.er will further simplify, based on the theory of arrays, resulting in y = 2008. \n 3.2 Interprocedural Computation We next describe interprocedural analysis assuming some oracle has \nprovided a call graph; Section 4 discusses our directed call graph construction. Our analysis handles \nprocedure callsinacontext-sensitiveman\u00adner, i.e., we only consider realizable interprocedural paths. \nCon\u00adtext sensitivity is accomplished through a functional approach [29] based on the Reps-Horwitz-Sagiv \n(RHS) tabulation algorithm [26], enhanced to handle merge functions and combination of local and non-local \n.ows at return sites. The analysis operatesover an interprocedural control .owgraph (ICFG), consistingof \nCFGs linked via edges from call sitesto and from the Entry and Exit nodesin corresponding callee CFGs.Asin\u00adgleworklist \nholdsthe pendingwork (symbolic statesto propagate), i.e., the algorithm does not completely analyze a \ncallee before con\u00adtinuing work in the caller. The global worklist effectively manages instancesofINTRAWPas \nco-routines. Analogouslytothe problem with loops in INTRAWP, the procedure may not terminate in the presence \nof recursion. Propagationofa formulatoandfromacalleeworksasfollows. Supposeaformula f reachesacall site \nw = v.m(),and assume for thereexists one possible callee A.m.(For multiple possible callees, the procedure \nis simply iterated over each callee.) The analysis .rst projects f into A.m s namespace substituting \nformals for actuals, and so on and propagates the result fpost to A.m s Exit. This symbolic state then \npropagates through A.m via INTRAWP, processing any further calls recursively. Whenever as a result of \nthis propagation, a formula fpre reaches A.m s Entry, the solver A.m() records a summary edge fpost - \n--. fpre , indicating that fpre is a suf.cient precondition to ensure reaching fpost at the Exit. Note \nthat forasingle fpost ,the solver may discover manysuf.cient preconditions as it explores more paths. \nFinally, the solver applies the summary at the call site by projecting fpre to the caller s namespace \nand conjoining it with wp(w = v.m()), described in Table 1. Note that the analysis caches summary edges \nandreuses them when identical formulae propagate to an Exit node [26]. Section5describes techniquestoincreasetheeffectivenessofthis \nreuse. 4. Directed Call Graph Construction The previous section addressed interprocedural analysis,but \nit ne\u00adglected to address the central challenge discussedin Section 2.1: What call graph should we use, \nwhen faced with high degrees of polymorphism? This section presentsa solution called directed call graph \nconstruction, which uses feedback from symbolic analysis to choose the call graph.  INTERWPDEMAND(ICFG, \npostcondition) 1 var F:{Formula} 2 F . INTERWP(ICFG, postcondition) ' 3 F .{f . F | f has no skolems \n.f is satis.able } ' 4 if F = \u00d8 ' 5 thenreturn F 6 else foreach t . F 7 do choose smethod from t ' 8 \nchoose m . targets(smethod , ICFG) s.t. t . smethod = m ' satis.able ' 9 if no such m 10 then continue \n' 11 ICFGnew . ICFG with m as possible target at site(smethod ) 12 return INTERWPDEMAND(ICFGnew , postcondition) \n[ Failed to expand call graph 13 return \u00d8 Figure 4. Pseudocode for directed call graph construction. \nsite(s) denotes the call site represented by a skolem constant, targets(s, ICF G) gives the set of methods \nin the current ICFG for site(s). Directed construction requires analyzing the program in stages. The \n.rst stage performs symbolic analysis while skipping over all method calls. If this analysis .nds a satis.able \nprecondition throughapaththatdoesnothaveanymethodcalls,the computation terminates, having found a call-free \nfeasible path that reaches the goal. Otherwise, the algorithm expands the call graph, adding a callee \nat some call site. The key insight is that constraints from symbolic analysis guide the choice of call \nsite and target. We skip method calls by modifying the intraprocedural wp computationforcallsfromTable1.Weintroduce \nskolem constants essentially existentially quanti.ed variables to represent the con\u00adstraints inducedbya \nskipped method.Fora call w = v.m(), we use four types of fresh skolem constants: 1. sret represents the \nundetermined return value assigned to w. 2. smethod represents the undetermined target of the method \ndis\u00adpatch; constraints on this variable guide selection of a target when expanding the call. 3. For \neach .eld f referenced in the post-condition of the call, sf captures the undetermined side-effects of \nthe method on f. We introduce an uninterpreted functionmod where mod(f, sf ) is an array term (in the \ntheory of arrays; see Section 3.1) that represents the updates to the relation f performedbythe callee. \nJava arrays are handled similarly (details elided). 4. sexc represents the undetermined exception value \ngenerated during the execution of the method.  The modi.ed wp for skipped calls follows; we show only \nthe non\u00adexceptional case: wp(w = v.m(),f)= smethod = dispatch(typeOf(v),m()) . v = null . f[sret /w][sexc/exc][mod(f, \nsf )/f](*) ((*) for each .eld and array) Given the modi.ed wp, Figure4gives pseudocode for directed call \ngraph construction. We assume a procedure INTERWP that computes the interprocedural symbolic analysis \ndescribed in Sec-tion3.2:givenanICFGand post-condition,it returnsasetof satis.\u00adable preconditions at \nentry. The forloop from lines6to12 attempts to expand the call graph. Note the satis.ability check at \nline 8, showinghow symbolic constraintsin.uencethe choiceofcalltar\u00adgets. Finally, note that the analysis \nallows for expanding multiple targets at a call site. This functionality is needed not only for calls \nwith multiple possibletargets,but also for cases whena calleeis feasible accordingto constraintsoverskolem \nconstants,buthasbe\u00adhavior incompatible with the post-condition(e.g.,if we needanon\u00adnull return value \nand the expanded callee always returns null). As in Figure 4, SNUGGLEBUG currently computes wp from scratchineach \nanalysisphase.Reuseofworkfrompreviousphases could yielda large performance bene.t,but bounded analysis \nand skolem constants make such reuse non-trivial.We plan to investi\u00adgate reuse across phases further \nin future work. 4.1 Directed Call Graph Construction Example Here we illustrate in detail how directed \ncall graph construction runs on the example of Figure 1, as was discussed at a high level in Section2.1. \nRecallthatthegoalisto.ndaconcreteexecutionfrom the beginning of entrypoint to line 38.We focus on the \nloop-free backwards path p going through lines 37, 36, 35, 34, 33, 32, and 28. The starting formula that \nwe propagate backwards from line 38 is simply true, i.e., the line was executed. Phase 1 In the .rst \nphase, the ICFG contains methods entrypoint(), checkValid() and Car.getYear().4 During wp propagation \nalong path p, the .ow functions introduce skolem constants for the method calls iterator(), hasNext(), \nand next(),which do not appear in the initial ICFG and so are skipped: Fornext, smethod,n , sexc,n , \nsret,n , and syear,n For hasNext, smethod,h , sexc,h , sret,h , and syear,h  For iterator, smethod,i \n, sexc,i , sret,i , and syear,i  We omitexc and the sexc variables from our discussion, as theyare not \nrelevant in this example. The formula that reaches the entry of entrypoint follows, applying the appropriate \n.ow functions from Table 1 for each statement in the path: smethod,n = dispatch(typeOf(sret,i ), next()) \n. smethod,h = dispatch(typeOf(sret,i ), hasNext()) . smethod,i = dispatch(typeOf(c), iterator()) . read(mod(mod(mod(year,syear,i \n),syear,h ),syear,n ),sret,n ) = 2009 . sret,h = true . c = null . sret,n = null . sret,i = null . subType(typeOf(sret,n \n), Car) . subType(typeOf(sret,i ), Iterator) . subType(typeOf(c), NewCarList) The read term arises from \nanalyzing Car.getYear(), and the nested mod terms compositionally indicate the possible side effects \nof skipped methods on contents of the year .eld. The preceding formula is satis.able. However, it contains \nskolem constants, which indicate that the path which generates this formula skipped over some calls. \nHence, INTERWPDEMAND must expand the call graph, trying to .nd a path with no skipped calls. Supposeit \nselectstoexpandthecallto iterator (line 32) next. The type constraint subType(typeOf(c), NewCarList) \nindicates that c must be of type NewCarList. (The constraint arose from the instanceof check at line \n28.) Hence, INTERWPDEMAND concludes that smethod,i = NewCarList.iterator(),expands the call graph accordingly, \nand recurses. Phase 2 INTERWPDEMAND performs symbolic analysis over the expanded call graph. This time \nthe following symbolic state 4For expository purposes, we assume the expansion of the monomorphic call \nto Car.getYear has already occurred.  reaches entry, indicating two skipped method calls on the path: \nsmethod,n = dispatch(NewCarList$Itr, next()) . smethod,h = dispatch(NewCarList$Itr, hasNext()) . read(mod(mod(year,syear,h \n),syear,n ),sret,n ) = 2009 . sret,h = true . c = null . sret,n = null . subType(typeOf(sret,n ), Car) \n. subType(typeOf(c), NewCarList) Note that since we analyze NewCarList.iterator, the con\u00adcrete type NewCarList$Itr \nreturned by the method now appears in the dispatch constraints. Continuing, we successively add targets \nnext() and hasNext() in the ICFG, both drawn from NewCarList$Itr. Phases3and4 After the next twophases, \nthe following symbolic state reaches entry: c = null . c.elems = null . c.elems.length > 0 . c.elems[0] \n= null . c.elems[0].year = 2009 . subType(typeOf(c), NewCarList) (For clarity, we write x.foo for read(foo,x), \nwhere foo cannot be an update or mod term.) Since this formula contains no skolem constants,it representsa \npath with no skipped calls.Areader may verify that this pre-condition at entrypoint would indeed lead \nthe execution to goal. Note that the order in which calls are expanded can affect performance signi.cantly.Forexample,if \nour algorithm insistedon expanding the next() callat line34of Figure1.rst,itmayhave tried many possibilities \nbefore .nding the method corresponding to NewCarList. Our implementation employs simple heuristics to \ndetermine a pro.table order to expand calls, which works well in ourexperience. Furthermore, our implementation \nmay heuristically expand more than one in a stage, especially calls to small methods and single-dispatch \ncalls. 5. Enhancing Summary Reuse As discussed in Section 2.2, a na\u00a8ive approach to tabulation-based \ninterprocedural analysis yields poor reuse of procedure summaries. Here we describe generalization, a \ntechnique that enables compu\u00adtation of generalized summaries entirely within a tabulation-based analysis \n[26]. For generalization, we adapt two ideas to our interprocedural wp framework.The.rstideaistocompute \nunderapproximatesym\u00adbolic summaries [10]. Whereas usual functional IPA creates ta\u00adbles of how individual \ninputfactsmapto outputfacts,a symbolic summary applies to general classes of inputfacts.5 Symbolic sum\u00admaries \ncangive better reuse because theyare more generallyappli\u00adcable. The second idea uses the frame rule [27] \nto analyze callees with smaller formulae. Supposea formula fpost propagatedtoa call site of m could be \nre-written as fI . fD , in a way that no subterm post post of fI is written by m. fI is the method independent \npart, and post post fD post is the method dependent part. Then, using the frame rule, for a call to m \nwe have wp(m, fI . fD )= fI . wp(m, fD ). post post post post The advantage of this decomposition is \nthat a summary can be created and looked up based only on fD , and thus applied more post generally. \nAlgorithm Our generalization algorithm rewrites a formula to a form in which the above two ideas easily \napply, by generalizing terms referencing locations. Some conditional rewrite rules for generalization \nare in Figure 5. The li variables are fresh generic variables introduced to achieve a symbolic summary, \nand vret is 5Note the distinction between tabulation-based summaries in a domain of symbolic formulae \nand symbolic summaries in this general sense as described in [10]. if vret occurs in f : f . f[li/vret \n] . li = vret if read(f,e) occurs in f : f . f[li/read(f,e)] . lj = e . li = read(f, lj ) if aread(a, \ne1,e2) occurs in f : f . f[li/aread(a, e1,e2)] . lj = e1 . lk = e2 . li = aread(a, lj ,lk) Figure 5. \nConditional rewrite rules for generalization. the variable assigned the return value of the call, if \nany.6 After the rules are applied for all subterms possibly modi.ed by the callee, the method-dependent \nconjuncts for fD are the li = t post terms where the callee may modify t.7 Now, when the callee is analyzed \nusing standard tabulation the functional IPAcreates a generalized summary based only on fD post . For \nexample, consider id(x) { return x; } with call site q = id(p) and postcondition q> 5. The call modi.es \nonly q, so generalization via the rewrite rules in Figure 5 yields l0 > 5 . l0 = q. fD is l0 = q, or \nl0 = ret in the callee namespace, post and fI is l0 > 5. Tabulation of fD through id builds the post \npost id() summary l0 = ret --. l0 = x (a fully general summary for id). Propagating back to the caller \nand applying the frame rule, we get fpre is l0 = p . l0 > 5, which simpli.es via elimination to p> 5, \nas expected. Note that generalization entails a trade-off: fully general sum\u00admaries are easierto reuse,but \npotentially moredif.cultto compute, because the generalization erases information that could be used \nto prune infeasible paths while analyzing the callee. Our system inten\u00adtionally computes less general \nsummaries than possible at times: it does not generalize for possibly modi.ed locations not seen in any \ncall site formula, and it does not generalize for post-conditions con\u00adstraining the return value to be \ntrue, false, null, or non-null. 5.1 Generalization Example Let us consider how generalization affects \nanalysis of test in Fig\u00adure2;thegoalagainisto .nda precondition leadingtoanexcep\u00adtionat line38of Figure1.Forbrevity,we \nelidevarious irrelevant conjuncts throughout this discussion. As shown in Section 4, the analysis discovers \na pre-condition for entrypoint with the con\u00adjunct c.elems[0].year = 2009, which before line8 of Figure2 \nbecomes l.elems[0].year = 2009 (1) For thesetYear call at line 7, since the callee may modify year, generalization \nyields l0 = 2009 . l0 = l1.year . l1 = l.elems[0] (2) We propagatethe only possibly-modi.ed conjunct,l0 \n= l1.year, to the callee. Analysis of setYear yields the summary setYear() l0 = l1.year -----. l0 = read(update(year, \nthis, y),l1) (3) which is fully general for setYear. Applying this summary edge via the frame rule at \nline7(substituting actuals for formals) yields l0 = 2009 . l0 = read(update(year,c2, 2009),l1) . l1 = \nl.elems[0] (4) which, after elimination of generic variables, further simpli.es to c2 = l.elems[0] . \nl.elems[0].year = 2009 (5) 6We elide similar rules and handling ofupdate andexceptionvariables for brevity. \n7Our implementation currently uses a simple type-based analysis to reason about possibly modi.ed locations. \nSharper analysisis possible,butitwould be moreexpensiveto compute.Wewillexaminethe tradeoffin futurework. \n  (if c2 = l.elems[0], the formula becomes false). Now the analysis reaches the set call at line 6. \nSince set modi.es the contents of an array(this.elems), generalization of (5) yields l0 = l1[l2] . l1 \n= l.elems . l2 =0 . c2 = l0 . l0.year = 2009 (6) Analyzing set with the possibly modi.ed conjunct l0 \n= l1[l2] yields the summary edge set() l0 = l1[l2] - -. (7) l0 = aread(aupdate(a, this .elems, i, c),l1,l2) \nWhen applied at line6with i =1 and l2 =0 (different indices), the aread term simpli.es to l.elems[0]. \nHence, the formula before line6is(5), unmodi.edbythe call. Analyzing line5shows c2 = l.elems[0], yielding \nformula (1) again before line 5. We shall now see reuse of the above summary edges. Gener\u00adalization of \n(1) for the setYear call at line4again yields (2), so clearly summary edge (3) can be reused for the \ncall. Applying the edge yields read(update(year,c1, 2008), l.elems[0]) = 2009 (8) before line 4. Generalizing \n(8) for the set call at line3yields l0 = l1[l2] . l1 = l.elems . l2 =0 (9) . read(update(year,c1, 2008),l0) \n= 2009 Since the .rst conjunct is identical to that of (6), we can reuse summary edge (7) for this call. \nIn this case, we have i = l1 =1 at the caller, so the aread term from (7) simpli.es to c1. Hence, we \nhave read(update(year,c1, 2008),c1) = 2009 which simpli.es to true. This means test always causes an \nex\u00adception at line 38 of Figure 1, as expected. 6. The Rest of the Story As with any non-trivial system, \nmany design decisions strongly impact the real-world performance of SNUGGLEBUG. Here, we brie.y discuss \nother important aspects of the system. Disjunctiveformula propagation The cornerstone of our design is \nto propagate minterms the disjuncts of a formula in disjunctive normal form (DNF) independently during \nwp computation. This separate propagation is possible due to the distributivity property of wp, wp(s, \nfa . fb)= wp(s, fa) . wp(s, fb). This design gives rise to the set of symbolic states D(s) at each program \npoint in Figure 3; in the implementation, each formula in the set is a minterm. Propagating minterms \nindependently has several advantages. First, it exposes redundant states that need not be explored sepa\u00adrately.Forexample, \nif D(s) contains distinct disjuncts A, B, and C, we need only propagate A, B, and C independently, not \ndis\u00adjunctive combinations like A . B.For similar reasons, propagating minterms improves reuse of summary \nedges during interprocedu\u00adral propagation. Additionally, independent minterms tend to stay small even \nas there can be more of them which makes sim\u00adpli.cation less costly. Instead of repeatedly performing \nexpensive conversions to DNF, the implementation of each wp transformer outputs a set of minterms. On-the-.y \nsimpli.cation Many formulae develop internal con\u00adtradictions and become unsatis.able during wp computation; \nsuch formulae must be dropped from propagation early for good per\u00adformance. One way to detect contradictions \nis to use a full SMT solverateach propagationstep,butwe foundthistobetooexpen\u00adsive. SNUGGLEBUG, therefore, \nincorporates a custom, lightweight (x = y) . (x = y) . x<y (x<y) . (x = y) . x<y 0 = read(length,x) . \ntrue (typeOf(a)= T ) . (a = null) . false subType(a, c1) . subType(a, c2) . subType(c1,c2) . subType(a, \nc1) (typeOf(x) subtype T ) . (isFinal(T )) . typeOf(x)= T read(f, fresh(T )) . d(T ) Figure 6. Sample \nrewrite rules in our on-the-.y simpli.er. isFinal is a predicate which identi.es final Java classes. \nd(T ) denotes the default value for a type in a newly allocated .eld. simpli.er, serving the simplify() \nfunction in Figure 3. Our sim\u00adpli.er relies on a difference constraints solver, a term rewriting en\u00adgine, \nand standard elimination techniques to compute solved forms of constraint systems. Figure 6 shows some \nof SNUGGLEBUG s rewrite rules. Addi\u00adtional rules (not shown) simplify terms involving method dispatch, \narithmetic, theory of arrays, re.ection, etc. Additionally,we canon\u00adicalize formulae via hash consing \nand fold constants during term construction. Loops, Recursion, and Search Heuristic The algorithm pre\u00adsented \nin Section 3.2 does not enforce .xed upper bounds on the number of times loops and recursion are analyzed; \nwithout such bounds, the algorithm may not terminate. SNUGGLEBUG instead works witha generousbut .xedbudget \non the number wp steps for each goal, aborting thereafter. For typical SNUGGLEBUG applications, we just \nneed one sat\u00adisfactory precondition to reach the program entry. The selection of which item in the worklist \nto process next i.e. the search strategy hasabig impacton whetherthebudgetis used well.In fact, it can \nbe shown that in the presence of common forms of loops, random selection of items from the worklist can \nlead to pathologi\u00adcalbehavior,squanderingawaythebudget.As observedinworkon symbolic execution [31, 9] \nand well-understood for graph search algorithms in general an informed search heuristic iskeyto per\u00adformance \nfor such an approach. Our current search heuristic prioritizes paths with less looping or call depth, \nsearching for a feasible path in a quasi breadth\u00ad.rst manner.Asecondary heuristic prioritizes program \npoints that fewerfacts have reached, aiming to spread the search effort more equitably across the program.Arigorous \nstudyof search heuristics inSNUGGLEBUG isatopicfor futurework. Constraining Searchwith Overapproximation \nAs noted in other work [19], abstract interpretation can aid symbolic analysis by cheaply computingoverapproximate \ninformation.Weuse an ad hoc pre-pass of abstract interpretation to determine certain invariants such \nas constants, known array lengths, and non-nullness. The symbolic analysis uses these invariants to simplify \nformulae and prune infeasible paths. More importantly,overapproximate analysis willbeakeytool necessary \nto synthesize loop invariants, enabling better handling of loops; this is a topic of future work for \nus. Other details We implemented our algorithm using theT.J.Wat\u00adson Libraries for Analysis(WALA) [32]. \nThe interprocedural anal\u00adysis isbuilt onWALA s tuned tabulation solver [26], aiding scal\u00adability. The \nimplementation handles all features of (sequential) JVM bytecode semantics, including intra-and inter-procedural \nex\u00adceptional control .ow, string constants, and re.ection via class ob\u00adjects manipulated with ldc. We \nadditionally handle many native Table 3. Information about our benchmarks, popular open-source Java programs. \neclipse.ui consists of the plugins from Eclipse in the org.eclipse.ui.* namespace.  Benchmark Version \nSource kLOC ant 1.7.0 88 antlr 2.7.2 38 batik 1.6 157 tomcat 6.0.16 163 eclipse.ui 3.3.1 305 Con.guration \nValidated Not Avg. Time Validated Per Goal (s) Production 29 0 93 NoGeneralization 27 2 193 NoSimpli.cation \n11 18 1122 NoFeedback-CHA 12 17 1057 NoFeedback-Andersen 8 21 1331 methods from the standard libraries \nwith models, including many features of re.ection. Our analysis does not reason about concurrency. The \nimplemen\u00adtation does not model exactly the semantics of some bitwise opera\u00adtors, .oating point arithmetic, \nand integer over.ow issues. For any native method m in a client program, we assume that (1) m cannot \nthrow an exception, (2) m can return an arbitrary value, and (3) m does not modify the heap. When SNUGGLEBUG \npushes a symbolic state to an entrypoint, it usestheSMTsolverCVC3[6]asa .nal checkof satis.ability.In \npractice, most of the satis.ability queries are decidable. However, our logic can encode nonlinear constraints \nover integers, which are undecidable. The solver may return unknown in such cases; the tool interprets \nthis result as unsat and continues searching. 7. Evaluation We evaluated our algorithm by using it to \nvalidate null dereference warnings output by FindBugs [21]. As noted in Section 1, bug validationisjustone \npossibleuseofSNUGGLEBUG;wechosethis client for the evaluation to obtain an unbiased source of goals for \nour analysis. Also, note that SNUGGLEBUG-style sound analysis may not present the best tradeoffs forbugvalidation;a \nreal-world tool may choose to trade off some soundness(e.g., by assuming some methods do not throw exceptions) \nfor better performance. We considera reportofa possible null-pointerexception (NPE) at program point \np validated if our analysis discovers a precondi\u00adtion f for the closest public method m such that if \nm is invoked with parameters satisfying f, p must throw an NPE. The emphasis on public methods is deliberate: \na potential problem in a private methodmayprove infeasibleifall callersoftheprivate method es\u00adtablish \nan appropriate invariant. Hence we try to establish interpro\u00adcedural feasibility from a public method, \nwhich makes for a more useful client,but also for much more challenging analysis. Note, that our validation \nprocedure does not verify that a state satisfying the precondition f can actually be constructed via \nthe publicAPIsofthe corresponding classes.Otherworkhas addressed test generation respecting such invariants \n[11],but such issuesfall outside the scope of this paper. We ran FindBugs v.1.3.4 on a number of open-source \nJava programs, and selected for this experiment those in which Find-Bugs reported potential null pointer \nbugs, shown in Table 3. To\u00adgether,thesebenchmarks comprisemore than 750,000 lines of non\u00adcomment non-whitespace \ncode, with millions of lines of dependent libraries. To measure the effectiveness of our techniques, \nwe ran the following .ve analysis con.gurations: Production All described techniques enabled. NoGeneralization \nGeneralization (Sec. 5) disabled. NoSimpli.cation On-the-.y simpli.cation (Sec. 6) disabled. NoFeedback-CHA \nAnalysis over a pre-computed call graph based on class hierarchyanalysis instead of directed call graph \nconstruction (Sec. 4) Table 4. Comparison of results across .ve con.gurations. NoFeedback-Andersen Analysis \nover a pre-computed call graph based on Andersen s analysis [2] instead of directed call graph construction \n(Sec. 4) All experiments ran on a dual-processor IBM ThinkCentre run-ningWindowsXP,withtwo3GHz Pentium4processorsand2GB \nof RAM. Our analysis infrastructure (described in Section 6) ran on the Sun JDK 1.6 with 1GB of max heap \nspace. 7.1 Results Does the analysisworkfor large programs? From the 750,000 lines of source code considered, \nFindBugs reported .ndings for 56 possibleNPEbugsinthe considered categories.Weexaminedeach .nding by \nhand and with SNUGGLEBUG. SNUGGLEBUG success\u00adfully validated 29 of the 56 .ndings (52%). Of the remaining \n27 .ndings, we concluded based on manual inspection that 18 are infeasible, i.e., there is no feasible \npath from a public entrypoint that results in the NPE. Infeasibility in all of these cases was due to \nsome invariant enforced interprocedurally. The remaining 9 .ndings represent cases that we believe to \nbe feasible,butSNUGGLEBUG could not .ndavalid precondition withinthe allottedtime(30 minutes).Overall,SNUGGLEBUG \nsuc\u00adceeded in .nding a precondition for 29 of 38 feasible cases (76%). SNUGGLEBUGfailedtovalidateafewcasesdueto \nnon-linear arith\u00admetic beyond the reach of the SMT solver,inability to reason about type conversions \nbetween integers and unsigned ints and .oats, and an incomplete model of native methods related to the \nJava security model. The remaining cases presented a bigger search space than SNUGGLEBUGcould handle \nin the time limit, often requiring anal\u00adysis of complex XML parsing libraries. The remainder of this \nsection evaluates particular techniques presentedinthispaper.Werestrictour attentiontothe29validated \n.ndingsforthe remainderofthissection,sincetheother casestime out on all con.gurations. Table4compares \nresults acrossthe.vecon.gurations.The sec\u00adond columnshowsthe numberofbugsvalidatedby each con.gu\u00adration; \nthe remaining tasks timed out with the 30 minute limit. The last column reports the arithmetic mean of \nrunning time for each task. This time represents the end-to-end wall-clock time, includ\u00ading call graph \nconstruction, re.nement, and calls to the SMT solver. Whentaskstimeout,weassignatimeof30 minutes;sowhentime\u00adouts \noccur, the reported time is a lower bound. Figure7 presents more details on the distribution of times \nfor the 29 tasks considered. The .gure shows for each con.guration, the percentageof tasks completedasa \nfunctionof time.The results showthatoverhalfthetasksare easy ,inthat Productionvalidates the goal in \none minute or less. The Production con.guration vali\u00addated each of the 29 goals in 13 minutes or less. \nNoSimpli.cation, NoFeedback-CHA and NoFeedback-Andersen seem effective only for some easy tasks; these \ncon.gurations discover no precondi\u00adtions after three minutes of analysis. How effective is directed call \ngraph construction? The last two rows of Table 4 consider results with pre-computed call graphs instead \nof directed call graph construction. The NoFeedback-CHA  Figure 7. Comparison of running times across \ncon.gurations. con.gurationbuildsa call graph with class hierarchyanalysis; this call graphis conservativebut \nimprecise. The NoFeedback-Andersen con.guration builds a call graph on-the-.y with context-insensitive \nAndersen s pointer analysis [2], as implemented inWALA [32]. TheWALA implementation han\u00addles manydif.cult \nlanguage features, including re.ection patterns and models of many native methods. We initialize points-to \nsets for parameters to entrypoints with objects of all possible parame\u00adter subtypes non-deterministically, \nwhich may still be incomplete due to missing subtypes for parameter .elds. With WALA s rel\u00adatively conservative \ntreatment of re.ection, call graph construc\u00adtion for large programs on the Java 1.6 libraries would exceed \nthe 30 minute timeout; we limited the call graph construction to 25K nodes, which ranto completionforall \ncasesinacoupleof minutes. Table4shows that both con.gurations with pre-computed call graphs areineffective,failingtovalidate \nmostofthe tasks within thetimelimitanddegrading performancebyatleastafactoroften. As just discussed, \nour best-effort Andersen call graph is potentially unsound due to dif.culties with pointer analysis and \nlibrary entry\u00adpoints; this accounts for the4 cases where NoFeedback-Andersen failedtovalidateabug foundby \nNoFeedback-CHA. Figure8presentsdata illustratingthesizeofthe program subset explored whilevalidating \nthe call graph. The .gure shows that with directedcallgraph construction, roughly50%ofthe tasksexplored \na call graph of 10 nodes or less. The largest expanded call graph contained 93 nodes. For the pre-computed \ncall graphs, we de.ne an Effective Call Graph as follows. For each task, let d be the maximum depth of \ntheexpanded call graphbuiltby the Production con.guration. Given a pre-computed call graph G and an entrypoint \ne, we de.ne the Effective Call Graph to consist of those nodes in G which are reachable from e via breadth-.rst-search \n(BFS) up to depth d. Assuming an oracle provided the necessary depth d, the Effective Call Graph is the \nsmallest known to be suf.cient for the task. Figure8 shows that the effective call graph using Andersen \ns analysis is typically a factor of ten larger than the one built by directed construction, and the class-hierarchycall \ngraph is at least a factor of ten larger still. We conclude that directed call graph construction is \ncrucial. How effective is our summary-based reuse? We measured sum\u00admary edge reuse as follows: each timeafact \npropagates interpro\u00adcedurally to a callee, we record whether or not a summary edge already exists; we \ncall the percentage of times a summary edge alreadyexists the reuse factor. Across all runs, the Production \ncon-Figure 8. Effective call graph sizes. 1 public void setPrefix(String prefix) throws DOMException \n{ 2 if (this.isReadonly()) { 3 throw createDOMException(DOMException.NO_MOD_ALLOWED_ERR, 4 \"readonly.node\", \n5 ...); 6 } 7 String uri = this.getNamespaceURI(); 8 if (uri == null) { 9 throw createDOMException(DOMException.NAMESPACE_ERR, \n10 \"namespace\", 11 ...); 12 } 13 String name = this.getLocalName(); 14 if (prefix == null) { 15 this.setNodeName(name); \n16 } 17 if (!prefix.equals(\"\") &#38;&#38; 18 !DOMUtilities.isValidName(prefix)) { Figure 9. Excerpt from \nbatik-0.6, AbstractNode.java .guration sawa reusefactorof 87%, while the NoGeneralization con.gurationsawa \nreusefactorof5.3%.Table4showsthatgener\u00adalizationimproves performanceby morethanafactoroftwo(93s vs. 193s, \nonaverage).We conclude that tabulation-based modular analysis withgeneralization is effective for interprocedural \npropa\u00adgation of weakest preconditions. Note that although our reusefactoris already 87%,itis possible \nthat increasing reuse further by a small amount could have a large impact on bottom-line performance, \ne.g., if repeated analysis of large methods were avoided. Also, there could be further large performance \nbene.ts from reusing work across phases of directed call graph construction, as discussed in Section \n4. What is the impact of on-the-.y simpli.cation? Table4reports that without on-the-.y simpli.cation, \nperformance degrades by at least a factor of 10, and 18 of the 29 tasks fail to .nish within 30 minutes. \nAs with pre-computed call graphs, Figure7suggests that NoSimpli.cation is effective only for easy tasks, \nmaking no further progress after the .rst two minutes of analysis.  7.2 Examples from Experiments In \nthis section, we offer selected short code excerpts from the ex\u00adperimental study, in order to illustrate \nhow the concepts discussed in this paper arise in real-world examples. Example from Apache Batik Figure \n9 shows an excerpt from batik-0.6, an instance method from class AbstractNode. The  1 public String[] \nfindManagedBeans(String group) { 2 ArrayList results = new ArrayList(); 3 Iterator items = descriptors.values().iterator(); \n4 while (items.hasNext()) { 5 ManagedBean item = (ManagedBean) items.next(); 6 if ((group == null) &#38;&#38; \n(item.getGroup() == null)) { 7 results.add(item.getName()); 8 } else if (group.equals(item.getGroup())) \n{ 9 results.add(item.getName()); 10 } 11 } 12 String values[] = new String[results.size()]; 13 return \n((String[]) results.toArray(values)); 14 } Figure 10. Excerpt from tomcat-6.0.16, Registry.java goal \nis to reach line 17 with prefix equalto null. It is easy to see that prefix must be null at entry to \nreach this goal. However, a sound analysis must .nd a precondition that also satis.es the following conditions: \n1. The virtual call to isReadonly() (line2) must returnfalseto proceed beyond the .rst conditional. \n2. The virtual call to getNamespaceURI() (line 7) must return a non-null value to continue past the second \nconditional. 3. The virtual calls to getLocalName() (line 13) and setNodeName() (line 15) must then \nreturn without throwing an exception to continue to the goal. 4. The type of this must be such that \nthe concrete methods to which the above calls dispatch together exhibit the required behavior.  The \nbatik source code includes 85 concrete subtypes of AbstractNode, with dozens of implementations of the \nvirtual methods just described. Only a few subtypes can be instantiated as this and satisfy these criteria. \nIt is dif.cult for a human to ex\u00adamine all 85 types and reason about whether anysuch type satis.es these \ncriteria. 8 Our analysis with pre-built call graphs failed to .nd a satis.\u00adable precondition, since the \nconservatively computed call graphs expose a search space that is too big. However, the directed call \ngraph construction succeeded in narrowing the search for the ap\u00adpropriate subtype of AbstractNode, relying \non type and dispatch constraints handled by the theorem prover to rule out types asso\u00adciated with (dead-end) \nmethod implementations explored in early iterations. Example fromApacheTomcat Figure10showsanexample \nfrom Apache Tomcat, similar in spirit to the motivating example of Figure 1. The goal is to .nd a precondition \nsuch that execution reaches line8with group == null. Clearly group must be null at method entry to reach \nthe goal. However, other conditions also must be satis.ed: 1. The descriptors .eld (of declared type \nHashMap)ofthis must be non-null, and descriptors.values().iterator() must return a non-null, non-empty \nIterator. 2. The items.next() call must return a non-null item, and item.getGroup() cannot return null. \n Note that it is impossible for a human to determine whether this condition is feasible from inspecting \njust this code snippet; one must also check that the conditions on callees (such as getGroup) are satis.able. \n8In fact, when .rst looking at this example, we hypothesized that the condition was infeasible. Analysis \nof this example requires the same type of reasoning as previously discussed for Figure 1. Through several \niterations, the analysis determines appropriate Set and Iterator implementa\u00adtions that are consistent \nwith these conditions, and veri.es that that iterator can return a ManagedBean such that item.getGroup() \n!= null. 8. RelatedWork Backward symbolic analysis ESC/Java [15] pioneered practi\u00adcal weakest-precondition \nanalysis forJava.SNUGGLEBUG follows ESC in performing abstraction-free, underapproximate backwards symbolic \nreasoning, checking satis.ability with a theorem prover. ESC generatedveri.cation conditionsofworst-case \nquadratic size, pushingexponential searchfactorsintothe theoremprover.In con\u00adtrast, SNUGGLEBUG manages \nthe exponential search space di\u00adrectly, allowing for path pruning via inexpensive on-the-.y sim\u00adpli.cation \n(Sec. 6). ESC employed only intraprocedural analysis, relying on user annotations and speci.cations to \nreason across procedure calls. To reduce the annotation burden, the ESC/Java team developed Houdini [14], \na tool to infer speci.cations for unannotated pro\u00adgrams. Houdini generated a large set of candidate speci.cations \nfor each procedure and checked them using ESC/Java. In contrast, SNUGGLEBUG follows a more direct approach \nto generating pro\u00adcedure summaries driven by functional-style IPA[29]. Boogie [5] is a veri.er for Spec# \nin the tradition of ESC. The Boogie program representation includes modi.es speci.cations for procedures, \nwhichSNUGGLEBUGcouldexploitto improve separa\u00adtion. Boogie also employs abstract interpretation to synthesize \nloop invariants, also potentially bene.cial toSNUGGLEBUG. Like SNUGGLEBUG, the PSE tool [24] performed \nbackwards interprocedural symbolic analysis using functional-style IPA, tar\u00adgetingbugvalidation. UnlikeSNUGGLEBUG, \nPSE did not provide a sound underapproximate analysis, since the tool did not repre\u00adsent the entire path \ncondition and sometimes fell back to abstract representations of the heap. Preconditions generatedbysymbolic \nanalysistoa library entry\u00adpoint may still be infeasible, ruled out by other program invari\u00adants. Addressing \nthis problem, the DSD-Crasher [11] tool com\u00adbines ESC-Java with a tool to generate concrete tests as \ncounterex\u00adamples. Moreover,it usesa dynamicinvariant detectorto constrain inputs to obey likely invariants, \nruling out some spuriousbug re\u00adports. Handling this invariant issue in SNUGGLEBUG remains for future \nwork. ProgramTesting via Symbolic Execution Program testing gen\u00aderally targets high coverage, and not \nspeci.c goal states. Never\u00adtheless, much work that enhances program testing using symbolic execution \nhas technical connections toSNUGGLEBUG. PRE.x [8] was among the .rst systems to show fully au\u00adtomatic \ninterprocedural symbolic execution on real-world pro\u00adgrams. PRE.x relied on a pre-built call graph and \nprocessed pro\u00adcedures bottom-up, building underapproximate procedure sum\u00admaries. These summaries may \nnot represent all possible input environments to a function, so PRE.x falls back to conservative estimates \nof behavior as needed. In contrast, S NUGGLEBUG com\u00adputes partial summaries as needed on demand. Saturn \n[33] performs an interprocedural bit-precise symbolic execution for C programs, and it has been demonstrated \nto .nd bugs in large systems programs. Saturn employs property-speci.c function summaries for modular \ninterprocedural analysis,but the summaries do not generally support full interprocedural path sen\u00adsitive \nanalysis. In contrast, SNUGGLEBUG builds fully automatic path-sensitive symbolic summaries on demand, \nexploiting general\u00adization to enhance reuse.  Calysto [3] introduced structural abstraction, a staged \nsym\u00adbolicexecution that initially skipsover manycalls and inlines them lateriftheyappear on feasible \npaths. Thiswork represents the clos\u00adest precursor to our directed call graph construction. Calysto relied \nonapre-computedcallgraphforhandling indirectcalls;in contrast, our directed call graph constructionexploits \nconstraints discovered during symbolic analysis and requires no whole-program pointer analysis. Calysto \ndid not perform summary-based interprocedural analysis,but instead inlined callee representations. Systems \nsuch asDART[17] and CUTE [28] use symbolic anal\u00adysis in concert with concrete execution to improve coverage \nof random testing. To address scalability problems with theDART\u00adlike approach, the SMARTsystem [16] incorporated \nfunction sum\u00admaries.Follow-onwork [1] described a demand-driven computa\u00adtion of summary edges, similar \nin spirit to the tabulation algo\u00adrithm [26] we use. This work [1] also described the use of unin\u00adterpreted \nfunctions to skip processing of callees. KLEE [9] and Java PathFinder [31] perform interprocedural symbolic \nexecution, each with novel techniques for aggressive on-the-.y simpli.cation and path pruning, optimized \nrepresenta\u00adtions of the symbolic search space, and informed search heuristics. SNUGGLEBUG builds on this \napproach, applying analogous tech\u00adniques to backward symbolic analysis of Java. Jackson and Vaziri [22] \ntranslate a program into constraints in the Alloy speci.cation language, .nitizing the problem via bounded \nunrolling of loops, method inlining, and bounding the heap. Miniatur [13] extended this work, using program \nslicing to reduce the search space. Demand-driven backwards analysis obvi\u00adates the need for such slicing.WorkbyTaghdiri \n[30]extended this approach to avoid inlining callees, instead inferring partial method speci.cations \nvia abstraction re.nement. Abstraction-based approaches Counter-example guided abstrac\u00adtion re.nement \n(CEGAR) systems [4, 20] perform abstract inter\u00adpretationoverapredicate abstraction and re.ne predicates \nbased on feedback from a precise symbolic execution. These systems could be used to concretize paths \nto errors, such as the experiment we considered. Directed call graph construction can be viewed as a \nform of CEGAR. Synergy [19] presented an algorithm to combine CEGAR anal\u00adysis withDART-like underapproximate \nsearch. Dash[7]extended Synergy to the interprocedural setting and introduced an algorithm to re.ne pointer \nabstractions without whole-program pointer anal\u00adysis. In general, CEGAR approaches and SNUGGLEBUG are \ncom\u00adplementary:S NUGGLEBUG may bene.t froma feedback loop with abstraction re.nement, and CEGAR scenarios \ncould bene.t from directed call graph construction. 9. Conclusion We have presented SNUGGLEBUG, a new \napproach to demand\u00addriven interprocedural symbolic analysis. We presented sev\u00aderal novel techniques to \nimprove scalability, including directed call graph construction, generalization to improve reuse, and \nlightweight domain-speci.c on-the-.y simpli.cation and path pruning. Results for bug validation tasks \non large Java libraries indicate that the techniques work in concert to improve perfor\u00admance signi.cantly, \nbringing practical tools based on this technol\u00adogy within reach. References [1] S. Anand,P. Godefroid, \nand N.Tillmann. Demand-driven composi\u00adtional symbolic execution. In TACAS, 2008. [2] L. O. Andersen. \nProgram Analysis and Specialization for the C Programming Language. PhD thesis, University of Copenhagen, \nDIKU, 1994. [3] D. Babic and A. J. Hu. Calysto: scalable and precise extended static checking. In ICSE, \n2008. [4] T. Ball and S. K. Rajamani. The SLAM project: debugging system software via static analysis. \nIn POPL, 2002. [5] M. Barnett, B. E. Chang, R. Deline, B. Jacobs, and K. R. Leino. Boogie:Amodular reusableveri.er \nfor object-oriented programs. In FMCO, 2005. [6]C. BarrettandC.Tinelli. CVC3.In CAV, 2007. [7] N. E. \nBeckman, A.V. Nori, S. K. Rajamani, and R. J. Simmons. Proofs from tests. In ISSTA, 2008. [8] W. R. Bush, \nJ. D. Pincus, and D. J. Sielaff. A static analyzer for .nding dynamic programming errors. Softw. Pract. \nExper., 30(7):775 802, 2000. [9] C. Cadar,D. Dunbar,and D. Engler. KLEE: Unassisted and automatic generation \nof high-coverage tests for complex systems programs. In OSDI, 2008. [10] P. Cousot and R. Cousot. Modular \nstatic program analysis. In CC, 2002. [11] C. Csallner, Y. Smaragdakis, and T. Xie. Dsd-crasher: A hybrid \nanalysis tool forbug .nding. ACMTOSEM, 17(2):1 37, 2008. [12] E.W. Dijkstra. ADiscipline of Programming. \nPrentice Hall PTR, Upper Saddle River, NJ, USA, 1997. [13]J.Dolby,M.Vaziri,andF.Tip. Findingbugsef.cientlywithaSAT \nsolver. In FSE, 2007. [14] C. Flanagan and K. R. M. Leino. Houdini, an annotation assistant for ESC/Java. \nIn FME, 2001. [15] C. Flanagan, K. R. M. Leino, M. Lillibridge, G. Nelson, J. B. Saxe, and R. Stata. \nExtended static checking for Java. In PLDI, 2002. [16] P. Godefroid. Compositional dynamic test generation. \nIn POPL, 2007. [17] P. Godefroid, N. Klarlund, and K. Sen. DART: directed automated random testing. In \nPLDI, 2005. [18] D. Grove and C. Chambers. Aframework for call graph construction algorithms. ACMTrans.Program. \nLang. Syst., 23(6):685 746, 2001. [19]B.S. Gulavani,T.A. Henzinger,Y. Kannan,A.V. Nori, andS.K. Rajamani. \nSYNERGY: a new algorithm for property checking. In FSE, 2006. [20] T. A. Henzinger, R. Jhala, R. Majumdar, \nand G. Sutre. Lazy abstraction. In POPL, 2002. [21] D. Hovemeyer andW. Pugh. Findingbugs is easy. In \nOOPSLA Companion, 2004. [22]D. JacksonandM.Vaziri. Findingbugswitha constraintsolver.In ISSTA, 2000. \n[23] J. C. King. Symbolic execution and program testing. Commun.ACM, 19(7):385 394, 1976. [24] R. Manevich, \nM. Sridharan, S. Adams, M. Das, and Z.Yang. PSE: explaining programfailures via postmortem static analysis. \nFSE, 2004. [25] J. McCarthy. A basis for a mathematical theory of computation. Technical report, MIT, \nCambridge, MA, USA, 1962. [26] T. Reps, S. Horwitz, and M. Sagiv. Precise interprocedural data.ow analysis \nvia graph reachability. In POPL, 1995. [27] J. C. Reynolds. Separation logic:A logic for shared mutable \ndata structures. In LICS, 2002. [28] K. Sen, D. Marinov, and G. Agha. CUTE: a concolic unit testing engine \nfor C. In FSE, 2005. [29] M. Sharir and A. Pnueli. Two approaches to interprocedural data .ow analysis, \nchapter 7, pages 189 233. Prentice-Hall, 1981. [30] M. Taghdiri. Inferring speci.cations to detect errors \nin code. Automated Software Engineering, International Conference on, 0:144 153, 2004. [31]W.Visser,C.S.P.as.areanu, \nandS. Khurshid. Test input generation with java path.nder. In ISSTA, 2004. [32]T.J.Watson Libraries for \nAnalysis(WALA). http://wala.sf.net. [33]Y.XieandA.Aiken. Saturn:Ascalable frameworkfor error detection \nusing boolean satis.ability. ACMTOPLAS, 29(3):16, 2007.    \n\t\t\t", "proc_id": "1542476", "abstract": "<p>Symbolic analysis shows promise as a foundation for bug-finding, specification inference, verification, and test generation. This paper addresses demand-driven symbolic analysis for object-oriented programs and frameworks. Many such codes comprise large, partial programs with highly dynamic behaviors--polymorphism, reflection, and so on--posing significant scalability challenges for any static analysis.</p> <p>We present an approach based on interprocedural backwards propagation of weakest preconditions. We present several novel techniques to improve the efficiency of such analysis. First, we present <i>directed call graph construction</i>, where call graph construction and symbolic analysis are interleaved. With this technique, call graph construction is guided by constraints discovered during symbolic analysis, obviating the need for exhaustively exploring a large, conservative call graph. Second, we describe <i>generalization</i>, a technique that greatly increases the reusability of procedure summaries computed during interprocedural analysis. Instead of tabulating how a procedure transforms a symbolic state in its entirety, our technique tabulates how the procedure transforms only the pertinent portion of the symbolic state. Additionally, we show how integrating an inexpensive, custom logic simplifier with weakest precondition computation dramatically improves performance.</p> <p>We have implemented the analysis in a tool called <sc>Snugglebug</sc> and evaluated it as a bug-report feasibility checker. Our results show that the algorithmic techniques were critical for successfully analyzing large Java applications.</p>", "authors": [{"name": "Satish Chandra", "author_profile_id": "81100394237", "affiliation": "IBM T. J. Watson Research Center, Hawthorne, NY, USA", "person_id": "P1464319", "email_address": "", "orcid_id": ""}, {"name": "Stephen J. Fink", "author_profile_id": "81100118324", "affiliation": "IBM T. J. Watson Research Center, Hawthorne, NY, USA", "person_id": "P1464320", "email_address": "", "orcid_id": ""}, {"name": "Manu Sridharan", "author_profile_id": "81100641428", "affiliation": "IBM T. J. Watson Research Center, Hawthorne, NY, USA", "person_id": "P1464321", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1542476.1542517", "year": "2009", "article_id": "1542517", "conference": "PLDI", "title": "Snugglebug: a powerful approach to weakest preconditions", "url": "http://dl.acm.org/citation.cfm?id=1542517"}