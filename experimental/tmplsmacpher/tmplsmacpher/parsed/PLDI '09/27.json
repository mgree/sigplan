{"article_publication_date": "06-15-2009", "fulltext": "\n Veri.ed Validation of Lazy Code Motion Jean-Baptiste Tristan Xavier Leroy INRIA Paris-Rocquencourt \nINRIA Paris-Rocquencourt jean-baptiste.tristan@inria.fr xavier.leroy@inria.fr Abstract Translation validation \nestablishes a posteriori the correctness of a run of a compilation pass or other program transformation. \nIn this paper, we develop an ef.cient translation validation algorithm for the Lazy Code Motion (LCM) \noptimization. LCM is an inter\u00adesting challenge for validation because it is a global optimization that \nmoves code across loops. Consequently, care must be taken not to move computations that may fail before \nloops that may not terminate. Our validator includes a speci.c check for anticipabil\u00adity to rule out \nsuch incorrect moves. We present a mechanically\u00adchecked proof of correctness of the validation algorithm, \nusing the Coq proof assistant. Combining our validator with an unveri.ed implementation of LCM, we obtain \na LCM pass that is provably semantics-preserving and was integrated in the CompCert formally veri.ed \ncompiler. Categories and Subject Descriptors D.2.4 [Software Engineer\u00ading]: Software/Program Veri.cation \n-Correctness proofs; D.3.4 [Programming Languages]: Processors -Optimization; F.3.1 [Log\u00adics and Meanings \nof Programs]: Specifying and Verifying and Reasoning about Programs -Mechanical veri.cation; F.3.2 [Log\u00adics \nand Meanings of Programs]: Semantics of Programming Lan\u00adguages -Operational semantics General Terms Languages, \nVeri.cation, Algorithms Keywords Translation validation, lazy code motion, redundancy elimination, veri.ed \ncompilers, the Coq proof assistant 1. Introduction Advanced compiler optimizations perform subtle transformations \nover the programs being compiled, exploiting the results of delicate static analyses. Consequently, compiler \noptimizations are some\u00adtimes incorrect, causing the compiler either to crash at compile\u00adtime, or to silently \ngenerate bad code from a correct source program. The latter case is especially troublesome since such \ncompiler-introduced bugs are very dif.cult to track down. Incor\u00adrect optimizations often stem from bugs \nin the implementation of a correct optimization algorithm, but sometimes the algorithm itself is faulty, \nor the conditions under which it can be applied are not well understood. The standard approach to weeding \nout incorrect optimizations is heavy testing of the compiler. Translation validation, as introduced Permission \nto make digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior speci.c permission and/or a fee. PLDI 09, June 15 \n20, 2009, Dublin, Ireland. Copyright c &#38;#169; 2009 ACM 978-1-60558-392-1/09/06. . . $5.00 by Pnueli \net al. (1998b), provides a more systematic way to detect (at compile-time) semantic discrepancies between \nthe input and the output of an optimization. At every compilation run, the input code and the generated \ncode are fed to a validator (a piece of software distinct from the compiler itself), which tries to establish \na posteriori that the generated code behaves as prescribed by the input code. If, however, the validator \ndetects a discrepancy, or is unable to establish the desired semantic equivalence, compilation is aborted; \nsome validators also produce an explanation of the error. Algorithms for translation validation roughly \nfall in two classes. (See section 9 for more discussion.) General-purpose validators such as those of \nPnueli et al. (1998b), Necula (2000), Barret et al. (2005), Rinard and Marinov (1999) and Rival (2004) \nrely on generic techniques such as symbolic execution, model-checking and theorem proving, and can therefore \nbe applied to a wide range of program transformations. Since checking semantic equivalence between two \ncode fragments is undecidable in general, these val\u00adidators can generate false alarms and have high complexity. \nIf we are interested only in a particular optimization or family of re\u00adlated optimizations, special-purpose \nvalidators can be developed, taking advantage of our knowledge of the limited range of code transformations \nthat these optimizations can perform. Examples of special-purpose validators include that of Huang et \nal. (2006) for register allocation and that of Tristan and Leroy (2008) for list and trace instruction \nscheduling. These validators are based on ef.cient static analyses and are believed to be correct and \ncomplete. This paper presents a translation validator specialized to the Lazy Code Motion (LCM) optimization \nof Knoop et al. (1992, 1994). LCM is an advanced optimization that removes redundant computations; it \nincludes common subexpression elimination and loop-invariant code motion as special cases, and can also \neliminate partially redundant computations (i.e. computations that are redun\u00addant on some but not all \nprogram paths). Since LCM can move computations across basic blocks and even across loops, its vali\u00addation \nappears more challenging than that of register allocation or trace scheduling, which preserve the structure \nof basic blocks and extended basic blocks, respectively. As we show in this work, the validation of LCM \nturns out to be relatively simple (at any rate, much simpler than the LCM algorithm itself): it exploits \nthe re\u00adsults of a standard available expression analysis. A delicate issue with LCM is that it can anticipate \n(insert earlier computations of) instructions that can fail at run-time, such as memory loads from a \npotentially invalid pointer; if done carelessly, this transformation can turn code that diverges into \ncode that crashes. To address this issue, we complement the available expression analysis with a so\u00adcalled \nanticipability checker , which ensures that the transformed code is at least as de.ned as the original \ncode. Translation validation provides much additional con.dence in the correctness of a program transformation, \nbut does not com\u00adpletely rule out the possibility of compiler-introduced bugs: what if the validator \nitself is buggy? This is a concern for the develop\u00adment of critical software, where systematic testing \ndoes not suf.ce to reach the desired level of assurance and must be complemented by formal veri.cation \nof the source. Any bug in the compiler can potentially invalidate the guarantees obtained by this use \nof formal methods. One way to address this issue is to formally verify the compiler itself, proving that \nevery pass preserves the semantics of the program being compiled. Several ambitious compiler veri.ca\u00adtion \nefforts are currently under way, such as the Jinja project of Klein and Nipkow (2006), the Verisoft project \nof Leinenbach et al. (2005), and the CompCert project of Leroy et al. (2004 2009).  Translation validation \ncan provide semantic preservation guar\u00adantees as strong as those obtained by formal veri.cation of a \ncom\u00adpiler pass: it suf.ces to prove that the validator is correct, i.e. re\u00adturns true only when the two \nprograms it receives as inputs are semantically equivalent. The compiler pass itself does not need to \nbe proved correct. As illustrated by Tristan and Leroy (2008), the proof of a validator can be signi.cantly \nsimpler and more reusable than that of the corresponding optimizations. The translation val\u00adidator for \nLCM presented in this paper was mechanically veri.ed using the Coq proof assistant (Coq development team \n1989 2009; Bertot and Cast\u00b4 eran 2004). We give a detailed overview of this proof in sections 5 to 7. \nCombining the veri.ed validator with an unveri.ed implementation of LCM written in Caml, we obtain a \nprovably correct LCM optimization that integrates smoothly within the CompCert veri.ed compiler (Leroy \net al. 2004 2009). The remainder of this paper is as follows. After a short presenta\u00adtion of Lazy Code \nMotion (section 3) and of the RTL intermediate language over which it is performed (section 2), section \n4 develops our validation algorithm for LCM. The next three sections outline the correctness proof of \nthis algorithm: section 5 gives the dynamic semantics of RTL, section 6 presents the general shape of \nthe proof of semantic preservation using a simulation argument, and section 7 details the LCM-speci.c \naspects of the proof. Section 8 discusses other aspects of the validator and its proof, including completeness, \ncomplexity, performance and reusability. Related work is discussed in section 9, followed by conclusions \nin section 10. 2. The RTL intermediate language The LCM optimization and its validation are performed \non the RTL intermediate language of the CompCert compiler. This is a standard Register Transfer Language \nwhere control is represented by a control .ow graph (CFG). Nodes of the CFG carry abstract instructions, \ncorresponding roughly to machine instructions but operating over pseudo-registers (also called temporaries). \nEvery function has an unlimited supply of pseudo-registers, and their values are preserved across function \ncalls. An RTL program is a collection of functions plus some global variables. As shown in .gure 1, functions \ncome in two .avors: ex\u00adternal functions ef are merely declared and model input-output op\u00aderations and \nsimilar system calls; internal functions f are de.ned within the language and consist of a type signature \nsig, a param\u00adeter list rr, the size n of their activation record, an entry point l, and a CFG g representing \nthe code of the function. The CFG is im\u00adplemented as a .nite map from node labels l (positive integers) \nto instructions. The set of instructions includes arithmetic operations, memory load and stores, conditional \nbranches, and function calls, tail calls and returns. Each instruction carries the list of its succes\u00adsors \nin the CFG. When the successor l is irrelevant or clear from the context, we use the following more readable \nnotations for register\u00adto-register moves, arithmetic operations, and memory loads: '' r := rfor op(move,r,r,l) \nr := op(op, rr) for op(op, rr,r,l) r := load(chunk, mode, rr) for load(chunk, mode, rr, r, l) A more \ndetailed description of RTL can be found in (Leroy 2008). RTL instructions: i ::= nop(l) no operation \n| op(op, rr,r,l) arithmetic operation | load(chunk, mode, rr, r, l) memory load | store(chunk, mode, \nrr, r, l) memory store | call(sig, (r | id), rr, r, l) function call | tailcall(sig, (r | id), rr) function \ntail call | cond(cond, rr, ltrue,lfalse) conditional branch | return | return(r) function return Control-.ow \ngraphs: g ::= l . i .nite map RTL functions: fd ::= f | ef f ::= id { sig sig; internal function params \nrr; stack n; start l; graph g } ef ::= id { sig sig } external function Figure 1. RTL syntax 3. Lazy \nCode Motion Lazy code motion (LCM) (Knoop et al. 1992, 1994) is a data.ow\u00adbased algorithm for the placement \nof computations within control .ow graphs. It suppresses unnecessary recomputations of values by moving \ntheir .rst computations earlier in the execution .ow (if necessary), and later reusing the results of \nthese .rst computations. Thus, LCM performs elimination of common subexpressions (both within and across \nbasic blocks), as well as loop invariant code mo\u00adtion. In addition, it can also factor out partially \nredundant compu\u00adtations: computations that occur multiple times on some execution paths, but once or \nnot at all on other paths. LCM is used in produc\u00adtion compilers, for example in GCC version 4. Figure \n2 presents an example of lazy code motion. The original program in part (a) presents several interesting \ncases of redundan\u00adcies for the computation of t1 +t2: loop invariance (node 4), simple straight-line \nredundancy (nodes 6 and 5), and partial redundancy (node 5). In the transformed program (part (b)), these \nredundant computations of t1 + t2 have all been eliminated: the expression is computed at most once on \nevery possible execution path. Two instructions (node n1 and n2) have been added to the graph, both of \nwhich compute t1 + t2 and save its result into a fresh temporary h0. The three occurrences of t1 + t2 \nin the original code have been rewritten into move instructions (nodes 4 , 5 and 6 ), copying the fresh \nh0 register to the original destinations of the instructions. The reader might wonder why two instructions \nh0 := t1 + t2 were added in the two branches of the conditional, instead of a single instruction before \nnode 1. The latter is what the partial redundancy elimination optimization of Morel and Renvoise (1979) \nwould do. However, this would create a long-lived temporary h0, therefore increasing register pressure \nin the transformed code. The lazy aspect of LCM is that computations are placed as late as possible while \navoiding repeated computations. The LCM algorithm exploits the results of 4 data.ow analyses: up-safety \n(also called availability), down-safety (also called antic\u00adipability), delayability and isolation. These \nanalyses can be imple\u00admented ef.ciently using bit vectors. Their results are then cleverly combined to \ndetermine an optimal placement for each computation performed by the initial program. Knoop et al. (1994) \npresents a correctness proof for LCM. How\u00adever, mechanizing this proof appears dif.cult. Unlike the program \ntransformations that have already been mechanically veri.ed in the CompCert project, LCM is a highly \nnon-local transformation: in\u00ad   1 2  4 6 5 (a) Original code  1 2  n1 n2 4 3 6 5 (b) Code after \nlazy code motion  Figure 2. An example of lazy code motion transformation structions are moved across \nbasic blocks and even across loops. Moreover, the transformation generates fresh temporaries, which adds \nsigni.cant bureaucratic overhead to mechanized proofs. It ap\u00adpears easier to follow the veri.ed validator \napproach. An additional bene.t of this approach is that the LCM implementation can use ef\u00ad.cient imperative \ndata structures, since we do not need to formally verify them. Moreover, it makes it easier to experiment \nwith other variants of LCM. To design and prove correct a translation validator for LCM, it is not important \nto know all the details of the analyses that indicate where new computations should be placed and which \ninstructions should be rewritten. However it is important to know what kind of transformations happen. \nTwo kinds of rewritings of the graph can occur: The nodes that exist in the original code (like node \n4 in .gure 2) still exist in the transformed code. The instruction they carry is either unchanged or \ncan be rewritten as a move if they are arithmetic operations or loads (but not calls, tail calls, stores, \nreturns nor conditions).  Some fresh nodes are added (like node n1) to the transformed graph. Their \nleft-hand side is a fresh register; their right-hand side is the right-hand side of some instructions \nin the original code.  There exists an injective function from nodes of the original code to nodes of \nthe transformed code. We call this mapping .. It connects each node of the source code to its (possibly \nrewritten) counterpart in the transformed code. In the example of .gure 2, . maps nodes 1 ... 6 to their \nprimed versions 1 ' ... 6 '. We assume the unveri.ed implementation of LCM is instrumented to produce \nthis function. (In our implementation, we arrange that . is always the identity function.) Nodes that \nare not in the image of . are the fresh nodes introduced by LCM. 4. A translation validator for Lazy \nCode Motion In this section, we detail a translation validator for LCM. 4.1 General structure Since LCM \nis an intraprocedural optimization, the validator pro\u00adceeds function per function: each internal function \nf of the origi\u00adnal program is matched against the identically-named function f ' of the transformed program. \nMoreover, LCM does not change the type signature, parameter list and stack size of functions, and can \nbe assumed not to change the entry point (by inserting nops at the graph entrance if needed). Checking \nthese invariants is easy; hence, we can focus on the validation of function graphs. Therefore, the validation \nalgorithm is of the following shape: validate(f, f ' ,.)= let AE = analyze(f ' ) in f ' .sig = f.sig \nand f ' .params = f.params and f ' .stack = f.stack and f ' .start = f.start and for each node n of f, \nV (f, f ' , n, ., AE)= true As discussed in section 3, the . parameter is the mapping from nodes of the \ninput graph to nodes of the transformed graph pro\u00advided by the implementation of LCM. The analyze function \nis a static analysis computing available expressions, described below in section 4.2.1. The V function \nvalidates pairs of matching nodes and is composed of two checks: unify, described in section 4.2.2 and \npath, described in section 4.3.2. V (f, f ' , n, ., AE)= unify(RD(n ' ), f.graph(n),f ' .graph(.(n))) \nand for all successor s of n and matching successor s ' of n ' , path(f.graph,f ' .graph,s ' ,.(s)) As \noutlined above, our implementation of a validator for LCM is carefully structured in two parts: a generic, \nrather bureaucratic framework parameterized over the analyze and V functions; and the LCM-speci.c, more \nsubtle functions analyze and V . As we will see in section 7, this structure facilitates the correctness \nproof of the validator. It also makes it possible to reuse the generic framework and its proof in other \ncontexts, as illustrated in section 8. We now focus on the construction of V , the node-level validator, \nand the static analysis it exploits.  4.2 Veri.cation of the equivalence of single instructions Consider \nan instruction i at node n in the original code and the cor\u00adresponding instruction i ' at node .(n) in \nthe code after LCM (for example, nodes 4 and 4 ' in .gure 2). We wish to check that these two instructions \nare semantically equivalent. If the transformation was a correct LCM, two cases are possible: i = i \n' : both instructions will obviously lead to equivalent run\u00adtime states, if executed in equivalent initial \nstates.  i ' is of the form r := h for some register r and fresh register h, and i is of the form r \n:= rhs for some right-hand side rhs, which can be either an arithmetic operation op(...) or a memory \nread load(...).  In the latter case, we need to verify that rhs and h produce the same value. More precisely, \nwe need to verify that the value contained in h in the transformed code is equal to the value produced \nby evaluating rhs in the original code. LCM being a purely syntactical redundancy elimination transformation, \nit must be the case that the instruction h := rhs exists on every path leading to .(n) in the transformed \ncode; moreover, the values of h and rhs are preserved along these paths. This property can be checked \nby performing an available expression analysis on the transformed code.  4.2.1 Available expressions \nThe available expression analysis produces, for each program point of the transformed code, a set of \nequations r = rhs between registers and right-hand sides. (For ef.ciency, we encode these sets as .nite \nmaps from registers to right-hand sides, represented as Patricia trees.) Available expressions is a standard \nforward data.ow analysis: \\ AE(s)= {T (f ' .graph(l), AE(l)) | s is a successor of l} The join operation \nis set intersection; the top element of the lattice is the empty set, and the bottom element is a symbolic \nconstant U denoting the universe of all equations. The transfer function T is standard; full details \ncan be found in the Coq development. For instance, if the instruction i is the operation r := t1 + t2, \nand R is the set of equations before i, the set T (i, R) of equations after i is obtained by adding the \nequality r = t1 + t2 to R, then removing every equality in this set that uses register r (including the \none just added if t1 or t2 equals r). We also track equalities between register and load instructions. \nThose equalities are erased whenever a store instruction is encountered because we do not maintain aliasing \ninformation. To solve the data.ow equations, we reuse the generic imple\u00admentation of Kildall s algorithm \nprovided by the CompCert com\u00adpiler. Leveraging the correctness proof of this solver and the de.ni\u00adtion \nof the transfer function, we obtain that the equations inferred by the analysis hold in any concrete \nexecution of the transformed code. For example, if the set of equations at point l include the equality \nr = t1 +t2, it must be the case that R(r)= R(t1)+R(t2) for every possible execution of the program that \nreaches point l with a register state R. 4.2.2 Instruction uni.cation Armed with the results of the \navailable expression analysis, the unify check between pairs of matching instructions can be easily expressed: \nunify(D, i, i ' )= if i ' = i then true else case (i, i ' ) of | (r := op(op, r ),r := h) . (h = op(op, \nr )) . D | (r := load(chunk, mode,r ),r := h) . (h = load(chunk, mode,r )) . D | otherwise . false Here, \nD = AE(n ' ) is the set of available expressions at the point n ' where the transformed instruction i \n' occurs. Either the original instruction i and the transformed instruction i ' are equal, or the former \nis r := rhs and the latter is r := h, in which case instruction uni.cation succeeds if and only if the \nequation h = rhs is known to hold according to the results of the available expression analysis.  4.3 \nVerifying the .ow of control Unifying pairs of instructions is not enough to guarantee semantic preservation: \nwe also need to check that the control .ow is pre\u00adserved. For example, in the code shown in .gure 2, \nafter checking that the conditional tests at nodes 1 and 1 ' are identical, we must . n .(n) m .  \n.(m) Figure 3. Effect of the transformation on the structure of the code make sure that whenever the \noriginal code transitions from node 1 to node 6, the transformed code can transition from node 1 ' to \n6 ' , executing the anticipated computation at n2 on its way. More generally, if the k-th successor of \nn in the original CFG is m, there must exist a path in the transformed CFG from .(n) to .(m) that goes \nthrough the k-th successor of .(n). (See .gure 3.) Since instructions can be added to the transformed \ngraph during lazy code motion, .(m) is not necessarily the k-th successor of .(n): one or several anticipated \ncomputations of the shape h := rhs may need to be executed. Here comes a delicate aspect of our validator: \nnot only there must exist a path from .(n) to .(m), but moreover the anticipated computations h := rhs \nfound on this path must be semantically well-de.ned: they should not go wrong at run-time. This is required \nto ensure that whenever an execution of the original code transitions in one step from n to m, the transformed \ncode can transition (possibly in several steps) from .(n) to .(m) without going wrong. Figure 4 shows \nthree examples of code motion where this prop\u00aderty may not hold. In all three cases, we consider anticipating \nthe computation a/b (an integer division that can go wrong if b =0) at the program points marked by a \ndouble arrow. In the leftmost example, it is obviously unsafe to compute a/b before the condi\u00adtional \ntest: quite possibly, the test in the original code checks that b 0 before computing a/b. The middle \nexample is more subtle: = it could be the case that the loop preceding the computation of a/b does not \nterminate whenever b =0. In this case, the original code never crashes on a division by zero, but anticipating \nthe division before the loop could cause the transformed program to do so. The rightmost example is similar \nto the middle one, with the loop be\u00ading replaced by a function call. The situation is similar because \nthe function call may not terminate when b =0. How, then, can we check that the instructions that have \nbeen added to the graph are semantically well-de.ned? Because we dis\u00adtinguish erroneous executions and \ndiverging executions, we can\u00adnot rely on a standard anticipability analysis. Our approach is the following: \nwhenever we encounter an instruction h := rhs that was inserted by the LCM transformation on the path \nfrom .(n) . . .     Figure 4. Three examples of incorrect code motion. Placing a computation of \na/b at the program points marked by . can po\u00adtentially transform a well-de.ned execution into an erroneous \none.  1 function ant checker rec (g,rhs,pc,S) = 2 3 case S(pc) of 4 | Found .(S,true) 5 | NotFound .(S,false) \n6 | Visited .(S,false) 7 | Dunno . 8 9 case g(pc) of 10 | return .(S{pc .NotFound},false) 11 | tailcall \n( , , ) .(S{pc .NotFound},false) 12 | cond( , ,ltrue ,lfalse) . 13 let (S ,b1) = ant checker rec (g,rhs,ltrue \n,S{pc .Visited}) in 14 let (S ,b2) = ant checker rec (g,rhs,lfalse,S ) in 15 if b1&#38;&#38; b2then (S \n{pc .Found},true) else (S {pc .NotFound},false) 16 | nop l. 17 let (S ,b) := ant checker rec (g,rhs,l,S{pc \n.Visited}) in 18 if b then (S {pc .Found},true) else (S {pc .NotFound},false) 19 | call( , , , ,l) .(S{pc \n.NotFound},false) 20 | store( , , , ,l) . 21 if rhs reads memory then (S{pc .NotFound},false) else 22 \nlet (S ,b) := ant checker rec (g,rhs,l,S{pc .Visited}) in 23 if b then (S {pc .Found},true) else (S {pc \n.NotFound},false) 24 | op (op,args,r,l) . 25 if r is an operand of rhs then (S{pc .NotFound},false) \nelse 26 if rhs = (op op args) then (S{pc .Found},true) else 27 let (S ,b) = ant checker rec (g,rhs,l,S{pc \n.Visited}) in 28 if b then (S {pc .Found},true) else (S {pc .NotFound},false) 29 | load (chk,addr,args,r,l) \n. 30 if r is an operand of rhs then (S{pc .NotFound},false) else 31 if rhs = (load chk addr args) then \n(S{pc .Found},true) else 32 let (S ,b) = ant checker rec (g,rhs,l,S{pc .Visited}) in 33 if b then (S \n{pc .Found},true) else (S {pc .NotFound},false) 34 35 36 function ant checker (g,rhs,pc)= let (S,b) \n= ant checker rec(g,rhs,pc,(l .Dunno)) in b Figure 5. Anticipability checker to .(m), we check that the \ncomputation of rhs is inevitable in the original code starting at node m. In other words, all execution \npaths starting from m in the original code must, in a .nite number of steps, compute rhs. Since the semantic \npreservation result that we wish to establish takes as an assumption that the execution of the original \ncode does not go wrong, we know that the computation of rhs cannot go wrong, and therefore it is legal \nto anticipate it in the transformed code. We now de.ne precisely an algorithm, called the anticipability \nchecker, that performs this check. 4.3.1 Anticipability checking Our algorithm is described in .gure \n5. It takes four arguments: a graph g, an instruction right-hand side rhs to search for, a program point \nl where the search begins and a map S that associates to every node a marker. Its goal is to verify that \non every path starting at l in the graph g, execution reaches an instruction with right\u00adhand side rhs \nsuch that none of the operands of rhs have been rede.ned on the path. Basically it is a depth-.rst search \nthat covers all the path starting at l. Note that if there is a path starting at l that contains a loop \nso that rhs is neither between l and the loop nor in the loop itself, then there exists a path on which \nrhs is not reachable and that corresponds to an in.nite execution. To obtain an ef.cient algorithm, we \nneed to ensure that we do not go through loops several times. To this end, if the search reaches a join \npoint not for the .rst time and where rhs was not found before, we must stop searching immediately. This \nis achieved through the use of four different markers over nodes: Found means that rhs is computed on \nevery path from the current node.  NotFound means that there exists a path from the current node in \nwhich rhs is not computed.  Dunno is the initial state of every node before it has been visited.  Visited \nis the state when a state is visited and we do not know yet whether rhs is computed on all paths or not. \nIt is used to detect loops.  Let us detail a few cases. When the search reaches a node that is marked \nVisited (line 6), it means that the search went through a loop and rhs was not found. This could lead \nto a semantics discrepancy (recall the middle example in .gure 4) and the search fails. For similar reasons, \nit also fails when a call is reached (line 19). When the search reaches an operation (line 24), we .rst \nverify (line 25) that r, the destination register of the instruction does not modify the operands of \nrhs. Then, (line 26) if the instruction right\u00adhand side we reached correspond to rhs, we found rhs and \nwe mark the node accordingly. Otherwise, the search continues (line 27) and we mark the node based on \nwhether the recursive search found rhs or not (line 28).  The ant checker function, when it returns \nFound, should im\u00adply that the right-hand side expression is well de.ned. We prove that this is the case \nin section 7.3 below.  4.3.2 Verifying the existence of semantics paths Once we can decide the well-de.nedness \nof instructions, checking for the existence of a path between two nodes of the transformed graph is simple. \nThe function path(g, g ' , n, m) checks that there exists a path in CFG g ' from node n to node m, composed \nof zero, one or several single-successor instructions of the form h := rhs. The destination register \nh must be fresh (unused in g) so as to preserve the abstract semantics equivalence invariant. Moreover, \nthe right-hand side rhs must be safely anticipable: it must be the case that ant checker(g, rhs,.-1(m)) \n= Found, so that rhs can be computed before reaching m without getting stuck. 5. Dynamic semantics of \nRTL In preparation for a proof of correctness of the validator, we now outline the dynamic semantics \nof the RTL language. More details can be found in (Leroy 2008). The semantics manipulates val\u00adues, written \nv, comprising 32-bit integers, 64-bit .oats, and point\u00aders. Several environments are involved in the \nsemantics. Memory states M map pointers and memory chunks to values, in a way that accounts for byte \naddressing and possible overlap between chunks (Leroy and Blazy 2008). Register .les R map registers \nto values. Global environments G associate pointers to names of global vari\u00adables and functions, and \nfunction de.nitions to function pointers. The semantics of RTL programs is given in small-step style, \nas a transition relation between execution states. Three kinds of states are used: Regular states: S(S, \nf, s, l, R, M) . This state corresponds to an execution point within the internal function f, at node \nl in the CFG of f. R and M are the current register .le and memory state. S represents the call stack, \nand s points to the activation record for the current invocation of f.  Call states: C(S, fd, rv, M). \nThis is an intermediate state repre\u00adsenting an invocation of function Fd with parameters rv.  Return \nstates: R(S, v, M). Symmetrically, this intermediate state represents a function return, with return \nvalue v being passed back to the caller.  Call stacks S are lists of frames F(r, f, s, l, R), where \nr is the destination register where the value computed by the callee is to be stored on return, f is \nthe caller function, and s, l and R its local state at the time of the function call. The semantics is \nde.ned by the one-step transition relation t G f S . S ', where G is the global environment (invariant \nduring execution), S and S ' the states before and after the transition, and t a trace of the external \nfunction call possibly performed during the transition. Traces record the names of external functions \ninvoked, along with the argument values provided by the program and the return value provided by the \nexternal world. To give a .avor of the semantics and show the level of detail of the formalization, .gure \n6 shows a subset of the rules de.ning the one-step transition relation. For example, the .rst rule states \nthat if the program counter l points to an instruction that is an operation of the form op(op, r , rd,l \n' ), and if evaluating the operator op on the values contained in the registers r of the register .le \nR returns the value v, then we transition to a new regular state where the register rd of R is updated \nto hold the value v, and the program counter moves to the successor l ' of the operation. The only rule \nthat produces a non-empty trace is the one for external function invocations (last rule in .gure 6); \nall other rules produce the empty trace e. f.graph(l)= op(op, r , rd,l ' ) v = eval op(G, op, R(r )) \ne G fS(S, f, s, l, R, M) .S(S, f, s, l ' ,R{rd . v},M) f.graph(l)= call(sig,rf , r , rd,l ' ) G(R(rf \n)) = fd fd.sig = sig S ' = F(rd, f, s, l ' ,R).S e G fS(S, f, s, l, R, M) .C(S ' , fd, rv, M) f.graph(l)= \nreturn(r) v = R(r) e G fS(S, f, s, l, R, M) .R(S, v, M) S= F(rd, f, s, l, R).S ' e G fR(S, v, M) .S(S \n' , f, s, l, R{rd . v},M) alloc(M, 0, f.stacksize)=(s, M ' ) l = f.start R =[f.params . rv] e G fC(S, \nf, rv, M) .S(S, f, s, l, R, M) t =(ef .name, rv, v) t G fC(S, ef , rv, M) .R(S, v, M) Figure 6. Selected \nrules from the dynamic semantics of RTL Sequences of transitions are captured by the following closures \nof the one-step transition relation: t G f S . * S ' zero, one or several transitions G f S tone or several \ntransitions .+ S ' T G f S .8 in.nitely many transitions The .nite trace t and the .nite or in.nite trace \nT record the ex\u00adternal function invocations performed during these sequences of transitions. The observable \nbehavior of a program P , then, is de\u00ad.ned in terms of the traces corresponding to transition sequences \nfrom an initial state to a .nal state. We write P . B to say that program P has behavior B, where B is \neither termination with a .\u00adnite trace t, or divergence with a possibly in.nite trace T . Note that computations \nthat go wrong, such as an integer division by zero, are modeled by the absence of a transition. Therefore, \nif P goes wrong, then P . B does not hold for any B. 6. Semantics preservation for LCM Let Pi be an input \nprogram and Po be the output program produced by the untrusted implementation of LCM. We wish to prove \nthat if the validator succeeds on all pairs of matching functions from Pi and Po, then Pi . B . Po . \nB. In other words, if Pi does not go wrong and executes with observable behavior B, then so does Po. \n 6.1 Simulating executions The way we build a semantics preservation proof is to construct a relation \nbetween execution states of the input and output programs, written Si ~ So, and show that it is a simulation: \n Initial states: if Si and So are two initial states, then Si ~ So.  Final states: if Si ~ So and Si \nis a .nal state, then So must be a .nal state.  Simulation property: if Si ~ So, any transition from \nstate Si with trace t is simulated by one or several transitions starting in state So, producing the \nsame trace t, and preserving the simulation relation ~.  The hypothesis that the input program Pi does \nnot go wrong plays a crucial role in our semantic preservation proof, in particular to show the correctness \nof the anticipability criterion. Therefore, we re.ect this hypothesis in the precise statement of the \nsimulation property above, as follows. (Gi,Go are the global environments corresponding to programs Pi \nand Po, respectively.)  DEFINITION 1 (Simulation property). Let Ii be the initial state of program Pi \nand Io that of program Po. Assume that Si ~ So (current states are related) t Gi f Si . Si ' (the input \nprogram makes a transition) tt Gi f Ii .'* Si and Go f Io .'* So (current states are reachable from \ninitial states)  Gi f Si ' . B for some behavior B (the input program does not go wrong after the transition). \n t Then, there exists So ' such that Go f So .+ So ' and Si ' ~ So' . The commuting diagram corresponding \nto this de.nition is de\u00adpicted below. Solid lines represent hypotheses; dashed lines repre\u00adsent conclusions. \nt ' t does not Input program: Ii Si S ' i go wrong * ~ ~ ' t t ' Output program: Io S * So + o It is \neasy to show that the simulation property implies semantic preservation: THEOREM 1. Under the hypotheses \nbetween initial states and .nal states and the simulation property, Pi . B implies Po . B.  6.2 The \ninvariant of semantic preservation We now construct the relation ~ between execution states before and \nafter LCM that acts as the invariant in our proof of semantic preservation. We .rst de.ne a relation \nbetween register .les. DEFINITION 2 (Equivalence of register .les). f f R ~ R ' if and only if R(r)= \nR ' (r) for every register r that appears in an instruction of f s code. This de.nition allows the register \n.le R ' of the transformed function to bind additional registers not present in the original func\u00adtion, \nespecially the temporary registers introduced during LCM op\u00adtimization. Equivalence between execution \nstates is then de.ned by the three rules below. DEFINITION 3 (Equivalence of execution states). validate(f, \nf ' ,.)= true f f R ~ R ' G, G ' f S ~F S ' G, G ' fS(S, f, s, l, R, M) ~S(S ' ,f ' , s, .(l),R ' ,M) \nTV (fd)= fd ' G, G ' f S ~F S ' G, G ' fC(S, fd, rv, M) ~C(S ' , fd ' , rv, M) G, G ' f S ~F S ' G, G \n' fR(S, v, M) ~R(S ' , v, M) Generally speaking, equivalent states must have exactly the same memory \nstates and the same value components (stack pointer s, arguments and results of function calls). As mentioned \nbefore, the register .les R, R ' of regular states may differ on temporary regis\u00adters but must be related \nby the f f R ~ R ' relation. The function parts f, f ' must be related by a successful run of validation. \nThe program points l, l ' must be related by l ' = .(l). The most delicate part of the de.nition is the \nequivalence be\u00adtween call stacks G, G ' f S ~F S '. The frames of the two stacks S and S ' must be related \npairwise by the following predicate. DEFINITION 4 (Equivalence of stack frames). ' f f R ~ R ' validate(f,f \n,.)= true .v,M,B, G fS(S, f, s, l, R{r . v},M) . B =..R '' ,f f R{r . v}~ R '' . G ' fS(S,f ' , s, l \n' ,R ' {r . v},M) e .+ S(S,f ' , s, .(l),R '' ,M) G, G ' fF(r, f, s, l, R) ~F F(r, f ' , s, l ' ,R ' \n) The scary-looking third premise of the de.nition above captures the following condition: if we suppose \nthat the execution of the initial program is well-de.ned once control returns to node l of the caller, \nthen it should be possible to perform an execution in the transformed graph from l ' down to .(l). This \nrequirement is a consequence of the anticipability problem. As explained earlier, we need to make sure \nthat execution is well de.ned from l ' to .(l). But when the instruction is a function call, we have \nto store this information in the equivalence of frames, universally quanti.ed on the not-yet-known return \nvalue v and memory state M at return time. At the time we store the property we do not know yet if the \nexecution will be semantically correct from l, so we suppose it until we get the information (that is, \nwhen execution reaches l). Having stated semantics preservation as a simulation diagram and de.ned the \ninvariant of the simulation, we now turn to the proof itself. 7. Sketch of the formal proof This section \ngives a high-level overview of the correctness proof for our validator. It can be used as an introduction \nto the Coq development, which gives full details. Besides giving an idea of how we prove the validation \nkernel (this proof differs from earlier paper proofs mainly on the handling of semantic well-de.nedness), \nwe try to show that the burden of the proof can be reduced by adequate design. 7.1 Design: getting rid \nof bureaucracy Recall that the validator is composed of two parts: .rst, a generic validator that requires \nan implementation of V and of analyze; second, an implementation of V and analyze specialized for LCM. \nThe proof follows this structure: on one hand, we prove that if V satis.es the simulation property, then \nthe generic validator im\u00adplies semantics preservation; on the other hand, we prove that the node-level \nvalidation specialized for LCM satis.es the simulation property. This decomposition of the proof improves \nre-usability and, above all, greatly improves abstraction for the proof that V sat\u00adis.es the simulation \nproperty (which is the kernel of the proof on which we want to focus) and hence reduces the proof burden \nof the formalization. Indeed, many details of the formalization can be hidden in the proof of the framework. \nThis includes, among other things, function invocation, function return, global variables, and stack \nmanagement. Besides, this allows us to prove that V only satis.es a weaker version of the simulation \nproperty that we call the validation prop\u00aderty, and whose equivalence predicate is a simpli.cation of \nthe equivalence presented in section 6.2. In the simpli.ed equivalence predicate, there is no mention \nof stack equivalence, function trans\u00adformation, stack pointers or results of the validation. DEFINITION \n5 (Abstract equivalence of states). f f R ~ R ' l ' = .(l) G, G ' fS(S, f, s, l, R, M) S S(S ' ,f ' , \ns, l ' ,R ' ,M) G, G ' fC(S, fd,rv, M) C C(S ' , fd ' , rv, M)  G, G ' fR(S, v, M) R R(S ' , v, M) \nThe validation property is stated in three version, one for regular states, one for calls and one for \nreturn. We present only the property for regular states. If S = S(S, f, s, l, R, M) is a regular state, \nwe write S.f for the f component of the state and S.l for the l component. DEFINITION 6 (Validation property). \nLet Ii be the initial state of program Pi and Io that of program Po. Assume that Si S So t . S ' Gi \nf Si . ' * i . ' *  Gi f Iit Si and Go f Iot So  Si ' . B for some behavior B  V (Si.f, So.f, Si.l, \n., analyze(So.f)) = true ' .+ S '  Then, there exists So such that Soto and Si ' So' . We then prove \nthat if V satis.es the validation property, and if the two programs Pi,Po successfully pass validation, \nthen the simulation property (de.nition 1) is satis.ed, and therefore (theo\u00adrem 1) semantic preservation \nholds. This proof is not particularly interesting but represents a large part of the Coq development \nand requires a fair knowledge of CompCert internals. We now outline the formal proof of the fact that \nV satis.es the validation property, which is the more interesting part of the proof.  7.2 Veri.cation \nof the equivalence of single instructions We .rst need to prove the correctness of the available expression \nanalysis. The predicate S |= E states that a set of equalities E inferred by the analysis are satis.ed \nin execution state S. The predicate is always true on call states and on return states. DEFINITION 7 \n(Correctness of a set of equalities). S(S, f, s, l, R, M) |= RD(l) if and only if (r = op(op, r )) . \nRD(l) implies R(r)= eval op(op, R(r )) (r = load(chunk, mode,r )) . RD(l) implies eval addressing(mode,r \n)= v and R(r)= load(chunk,v)  for some pointer value v. The correctness of the analysis can now be stated: \nLEMMA 2 (Correctness of available expression analysis). Let S0 be the initial state of the program. For \nall regular states S such that S0 tS, we have S |= analyze(S.f). . * Then, it is easy to prove the correctness \nof the uni.cation check. The predicate W is a weaker version of S , where we remove the S requirement \nthat l ' = .(l), therefore enabling the program counter of the transformed code to temporarily get out \nof synchronization with that of the original code. LEMMA 3. Assume Si S So t Si . Si '  unify(analyze(So.f),Si.f.graph,So.f.graph,Si.l, \nSo.l)= true  . ' * Iot So . S '' ' S '' i WThen, there exists a state So '' such that Soto and S S o \nt Indeed, from the hypothesis Io . * So and the correctness of the analysis, we deduce that So |= analyze(So.f), \nwhich implies that the equality used during the uni.cation, if any, holds at run\u00adtime. This illustrate \nthe use of hypothesis on the past of the execu\u00adtion of the transformed program. By doing so, we avoid \nto maintain the correctness of the analysis in the predicate of equivalence. It remains to step through \nthe transformed CFG, as performed by path checking, in order to go from the weak abstract equivalence \nW S to the full abstract equivalence S.  7.3 Anticipability checking Before proving the properties of \npath checking, we need to prove the correctness of the anticipability check: if the check succeeds and \nthe semantics of the input program is well de.ned, then the right-hand side expression given to the anticipability \ncheck is well de.ned. LEMMA 4. Assume ant checker(f.graph, rhs,l)= true and S(S, f, s, l, R, M) . B for \nsome B. Then, there exists a value v such that rhs evaluates to v (without run-time errors) in the state \nR, M. Then, the semantic property guaranteed by path checking is that there exists a sequence of reductions \nfrom successor(.(n)) to .(successor(n)) such that the abstract invariant of semantic equivalence is reinstated \nat the end of the sequence. LEMMA 5. Assume Si ' W S '' S o  path(Si' .f.graph,So '' .f.graph,So '' \n.l, .(Si.l)) = true  Si ' . B for some B  e Then, there exists a state So ' such that So '' . * So \n' and Si ' So ' This illustrates the use of the hypothesis on the future of the execution of the initial \nprogram. All the proofs are rather straight\u00adforward once we know that we need to reason on the future \nof the execution of the initial program. By combining lemmas 3 and 5 we prove the validation property \nfor regular states, according to the following diagram. Si t S ' does not i go wrong W S S t ' t S \n'' e S ' Io Soo o * * The proofs of the validation property for call and return states are similar. 8. \nDiscussion Implementation The LCM validator and its proof of correctness were implemented in the Coq \nproof assistant. The Coq develop\u00adment is approximately 5000 lines long. 800 lines correspond to the speci.cation \nof the LCM validator, in pure functional style, from which executable Caml code is automatically generated \nby Coq s extraction facility. The remaining 4200 lines correspond to the cor\u00adrectness proof. In addition, \na lazy code motion optimization was implemented in OCaml, in roughly 800 lines of code. The following \ntable shows the relative sizes of the various parts of the Coq development. Part Size General framework \n37% Anticipability check 16% Path veri.cation 7% Reaching de.nition analysis 18% Instruction uni.cation \n6% Validation function 16% As discussed below, large parts of this development are not speci.c to LCM \nand can be reused: the general framework of section 7.1,  anticipability checking, available expressions, \netc. Assuming these parts are available as part of a toolkit, building and proving correct the LCM validator \nwould require only 1100 lines of code and proofs. Completeness We proved the correctness of the validator. \nThis is an important property, but not suf.cient in practice: a valida\u00adtor that rejects every possible \ntransformation is de.nitely correct but also quite useless. We need evidence that the validator is rel\u00adatively \ncomplete with respect to reasonable implementations of LCM. Formally specifying and proving such a relative \ncomplete\u00adness result is dif.cult, so we reverted to experimentation. We ran LCM and its validator on \nthe CompCert benchmark suite (17 small to medium-size C programs) and on a number of examples hand\u00adcrafted \nto exercise the LCM optimization. No false alarms were reported by the validator. More generally, there \nare two main sources of possible incom\u00adpleteness in our validator. First, the external implementation \nof LCM could take advantage of equalities between right-hand sides of computations that our available \nexpression analysis is unable to capture, causing instruction uni.cation to fail. We believe this never \nhappens as long as the available expression analysis used by the validator is identical to (or at least \nno coarser than) the up-safety analysis used in the implementation of LCM, which is the case in our implementation. \nThe second potential source of false alarms is the anticipability check. Recall that the validator prohibits \nanticipating a computa\u00adtion that can fail at run-time before a loop or function call. The CompCert semantics \nfor the RTL language errs on the side of cau\u00adtion and treats all unde.ned behaviors as run-time failures: \nnot just behaviors such as integer division by zero or memory loads from incorrect pointers, which can \nactually cause the program to crash when run on a real processor, but also behaviors such as adding two \npointers or shifting an integer by more than 32 bits, which are not speci.ed in RTL but would not crash \nthe program during actual execution. (However, arithmetic over.ows and under.ows are correctly modeled \nas not causing run-time errors, because the RTL language uses modulo integer arithmetic and IEEE .oat \narith\u00admetic.) Because the RTL semantics treats all unde.ned behaviors as potential run-time errors, our \nvalidator restricts the points where e.g. an addition or a shift can be anticipated, while the external \nim\u00adplementation of LCM could (rightly) consider that such a compu\u00adtation is safe and can be placed anywhere. \nThis situation happened once in our tests. One way to address this issue is to increase the number of \noper\u00adations that cannot fail in the RTL semantics. We could exploit the results of a simple static analysis \nthat keeps track of the shape of values (integers, pointers or .oats), such as the trivial int or .oat \ntype system for RTL used in (Leroy 2008). Additionally, we could re.ne the semantics of RTL to distinguish \nbetween unde.ned op\u00aderations that can crash the program (such as loads from invalid ad\u00addresses) and unde.ned \noperations that cannot (such as adding two pointers); the latter would be modeled as succeding, but returning \nan unspeci.ed result. In both approaches, we increase the number of arithmetic instructions that can \nbe anticipated freely. Complexity and performance Let N be the number of nodes in the initial CFG g. \nThe number of nodes in the transformed graph g ' is in O(N). We .rst perform an available expression \nanalysis on the transformed graph, which takes time O(N3). Then, for each node of the initial graph we \nperform an uni.cation and a path checking. Uni.cation is done in constant time and path checking tries \nto .nd a non-cyclic path in the transformed graph, performing an anticipability checking in time O(N) \nfor instructions that may be ill-de.ned. Hence path checking is in O(N2) but this is a rough pessimistic \napproximation. In conclusion, our validator runs in time O(N3). Since lazy code motion itself performs \nfour data-.ow analysis that run in time O(N3), running the validator does not change the complexity of \nthe lazy code motion compiler pass. In practice, on our benchmark suite, the time needed to validate \na function is on average 22.5% of the time it takes to perform LCM. Reusing the development One advantage \nof translation valida\u00adtion is the re-usability of the approach. It makes it easy to experi\u00adment with \nvariants of a transformation, for example by using a dif\u00adferent set of data-.ow analyzes in lazy code \nmotion. It also happens that, in one compiler, two different versions of a transformation co\u00adexist. It \nis the case with GCC: depending on whether one optimizes for space or for time, the compiler performs \npartial redundancy elimination (Morel and Renvoise 1979) or lazy code motion. We believe, without any \nformal proof, that the validator presented here works equally well for partial redundancy elimination. \nIn such a con.guration, the formalization burden is greatly reduced by using translation validation instead \nof compiler proof. Classical redundancy elimination algorithms make the safe re\u00adstriction that a computation \ne cannot be placed on some control .ow path that does not compute e in the original program. As a consequence, \ncode motion can be blocked by preventing regions (Bod\u00b4ik et al. 1998), resulting in less redundancy elimination \nthan expected, especially in loops. A solution to this problem is safe speculative code motion (Bod\u00b4ik \net al. 1998) where we lift the re\u00adstriction for some computation e as long as e cannot cause run-time \nerrors. Our validator can easily handle this case: the anticipability check is not needed if the new \ninstruction is safe, as can easily be checked by examination of this instruction. Another solution is \nto perform control .ow restructuring (Steffen 1996; Bod\u00b4ik et al. 1998) to separate paths depending on \nwhether they contain the computa\u00adtion e or not. This control .ow transformation is not allowed by our \nvalidator and constitutes an interesting direction for future work. To show that re-usability can go \none step further, we have mod\u00adi.ed the uni.cation rules of our lazy code motion validator to build a \ncerti.ed compiler pass of constant propagation with strength re\u00adduction. For this transformation, the \navailable expression analysis needs to be performed not on the transformed code but on the ini\u00adtial one. \nThankfully, the framework is designed to allow analyses on both programs. The modi.cation mainly consists \nof replacing the uni.cation rules for operation and loads, which represent about 3% of the complete development \nof LCM. (Note however that uni\u00ad.cation rules in the case of constant propagation are much bigger because \nof the multiple possible strength reductions). It took two weeks to complete this experiment. The proof \nof semantics preser\u00advation uses the same invariant as for lazy code motion and the proof remains unchanged \napart from uni.cation of operations and loads. Using the same invariant, although effective, is questionable: \nit is also possible to use a simpler invariant crafted especially for con\u00adstant propagation with strength \nreduction. One interesting possibility is to try to abstract the invariant in the development. Instead \nof posing a particular invariant and then de\u00advelop the framework upon it, with maybe other transformations \nthat will luckily .t the invariant, the framework is developed with an un\u00adknown invariant on which we \nsuppose some properties. (See Zuck et al. (2001) for more explanations.) We may hope that the result\u00ading \ntool/theory be general enough for a wider class of transforma\u00adtions, with the possibility that the analyses \nhave to be adapted. For example, by replacing the available expression analysis by global value numbering \nof Gulwani and Necula (2004), it is possible that the resulting validator would apply to a large class \nof redundancy elimination transformations.  9. Related Work Since its introduction by Pnueli et al. \n(1998a,b), translation valida\u00adtion has been actively researched in several directions. One direc\u00adtion \nis the construction of general frameworks for validation (Zuck et al. 2001, 2003; Barret et al. 2005; \nZaks and Pnueli 2008). An\u00adother direction is the development of generic validation algorithms that can \nbe applied to production compilers (Rinard and Marinov 1999; Necula 2000; Zuck et al. 2001, 2003; Barret \net al. 2005; Ri\u00adval 2004; Kanade et al. 2006). Finally, validation algorithms spe\u00adcialized to particular \nclasses of transformations have also been de\u00adveloped, such as (Huang et al. 2006) for register allocation \nor (Tris\u00adtan and Leroy 2008) for instruction scheduling. Our work falls in the latter approach, emphasizing \nalgorithmic ef.ciency and relative completeness over generality. A novelty of our work is its emphasis \non fully mechanized proofs of correctness. While unveri.ed validators are already very useful to increase \ncon.dence in the compilation process, a formally veri.ed validator provides an attractive alternative \nto the formal veri.cation of the corresponding compiler pass (Leinenbach et al. 2005; Klein and Nipkow \n2006; Leroy 2006; Lerner et al. 2003). Several validation algorithms or frameworks use model checking \nor automatic theorem proving to check veri.cation conditions pro\u00adduced by a run of validation (Zuck et \nal. 2001, 2003; Barret et al. 2005; Kanade et al. 2006), but the veri.cation condition generator itself \nis, generally, not formally proved correct. Many validation algorithms restrict the amount of code motion \nthat the transformation can perform. For example, validators based on symbolic evaluation such as (Necula \n2000; Tristan and Leroy 2008) easily support code motion within basic blocks or extended basic blocks, \nbut have a hard time with global transformations that move instructions across loops, such as LCM. We \nare aware of only one other validator that handles LCM: that of Kanade et al. (2006). In their approach, \nLCM is instrumented to produce a detailed trace of the code transformations performed, each of these \ntransformations being validated by reduction to a model-checking problem. Our approach requires less \ninstrumentation (only the . code mapping needs to be provided) and seems algorithmically more ef.cient. \nAs mentioned earlier, global code motion requires much care to avoid transforming nonterminating executions \ninto executions that go wrong. This issue is not addressed in the work of Kanade et al. (2006), nor in \nthe original proof of correctness of LCM by Knoop et al. (1994): both consider only terminating executions. \n10. Conclusion We presented a validation algorithm for Lazy Code Motion and its mechanized proof of correctness. \nThe validation algorithm is signi.cantly simpler than LCM itself: the latter uses four data.ow analyses, \nwhile our validator uses only one (a standard available expression analysis) complemented with an anticipability \ncheck (a simple traversal of the CFG). This relative simplicity of the algorithm, in turn, results in \na mechanized proof of correctness that remains manageable after careful proof engineering. Therefore, \nthis work gives a good example of the bene.ts of the veri.ed validator approach compared with compiler \nveri.cation. We have also shown preliminary evidence that the veri.ed val\u00adidator can be re-used for other \noptimizations: not only other forms of redundancy elimination, but also unrelated optimizations such \nas constant propagation and instruction strength reduction. More work is needed to address the validation \nof advanced global op\u00adtimizations such as global value numbering, but the decomposi\u00adtion of our validator \nand its proof into a generic framework and an LCM-speci.c part looks like a .rst step in this direction. \nEven though lazy code motion moves instructions across loops, it is still a structure-preserving transformation. \nFuture work in\u00adcludes extending the veri.ed validation approach to optimizations that modify the structure \nof loops, such as software pipelining, loop jamming, or loop interchange. Acknowledgments We would like \nto thank Beno it Razet, Damien Doligez, and the anonymous reviewers for their helpful comments and suggestions \nfor improvements. This work was supported by Agence Nationale de la Recherche, grant number ANR-05-SSIA-0019. \nReferences Clark W. Barret, Yi Fang, Benjamin Goldberg, Ying Hu, Amir Pnueli, and Lenore Zuck. TVOC: \nA translation validator for optimizing compilers. In Computer Aided Veri.cation, 17th Int. Conf., CAV \n2005, volume 3576 of Lecture Notes in Computer Science, pages 291 295. Springer, 2005. Yves Bertot and \nPierre Cast\u00b4eran. Interactive Theorem Proving and Pro\u00adgram Development Coq Art: The Calculus of Inductive \nConstructions. EATCS Texts in Theoretical Computer Science. Springer, 2004. Rastislav Bod\u00b4ik, Rajiv Gupta, \nand Mary Lou Soffa. Complete removal of redundant expressions. In PLDI 98: Proceedings of the ACM SIGPLAN \n1998 conference on Programming language design and implementation, pages 1 14. ACM, 1998. Coq development \nteam. The Coq proof assistant. Software and documen\u00adtation available at http://coq.inria.fr/, 1989 2009. \nSumit Gulwani and George C. Necula. A polynomial-time algorithm for global value numbering. In Static \nAnalysis, 11th Int. Symp., SAS 2004, volume 3148 of Lecture Notes in Computer Science, pages 212 227. \nSpringer, 2004. Yuqiang Huang, Bruce R. Childers, and Mary Lou Soffa. Catching and identifying bugs in \nregister allocation. In Static Analysis, 13th Int. Symp., SAS 2006, volume 4134 of Lecture Notes in Computer \nScience, pages 281 300. Springer, 2006. Aditya Kanade, Amitabha Sanyal, and Uday Khedker. A PVS based \nframe\u00adwork for validating compiler optimizations. In SEFM 06: Proceedings of the Fourth IEEE International \nConference on Software Engineering and Formal Methods, pages 108 117. IEEE Computer Society, 2006. Gerwin \nKlein and Tobias Nipkow. A machine-checked model for a Java\u00adlike language, virtual machine and compiler. \nACM Transactions on Programming Languages and Systems, 28(4):619 695, 2006. Jens Knoop, Oliver R\u00a8uthing, \nand Bernhard Steffen. Lazy code motion. In Programming Languages Design and Implementation 1992, pages \n224 234. ACM Press, 1992. Jens Knoop, Oliver R\u00a8 uthing, and Bernhard Steffen. Optimal code motion: Theory \nand practice. ACM Transactions on Programming Languages and Systems, 16(4):1117 1155, 1994. Dirk Leinenbach, \nWolfgang Paul, and Elena Petrova. Towards the formal veri.cation of a C0 compiler: Code generation and \nimplementation correctness. In Int. Conf. on Software Engineering and Formal Methods (SEFM 2005), pages \n2 11. IEEE Computer Society Press, 2005. Sorin Lerner, Todd Millstein, and Craig Chambers. Automatically \nproving the correctness of compiler optimizations. In Programming Language Design and Implementation \n2003, pages 220 231. ACM Press, 2003. Xavier Leroy. A formally veri.ed compiler back-end. arXiv:0902.2137 \n[cs]. Submitted, July 2008. Xavier Leroy. Formal certi.cation of a compiler back-end, or: programming \na compiler with a proof assistant. In 33rd symposium Principles of Programming Languages, pages 42 54. \nACM Press, 2006. Xavier Leroy and Sandrine Blazy. Formal veri.cation of a C-like memory model and its \nuses for verifying program transformations. Journal of Automated Reasoning, 41(1):1 31, 2008. Xavier \nLeroy et al. The CompCert veri.ed compiler. Development available at http://compcert.inria.fr, 2004 2009. \n Etienne Morel and Claude Renvoise. Global optimization by suppression of partial redundancies. Communication \nof the ACM, 22(2):96 103, 1979. George C. Necula. Translation validation for an optimizing compiler. \nIn Programming Language Design and Implementation 2000, pages 83 95. ACM Press, 2000. Amir Pnueli, Ofer \nShtrichman, and Michael Siegel. The code validation tool (CVT) automatic veri.cation of a compilation \nprocess. International Journal on Software Tools for Technology Transfer, 2:192 201, 1998a. Amir Pnueli, \nMichael Siegel, and Eli Singerman. Translation validation. In Tools and Algorithms for Construction and \nAnalysis of Systems, TACAS 98, volume 1384 of Lecture Notes in Computer Science, pages 151 166. Springer, \n1998b. Martin Rinard and Darko Marinov. Credible compilation with pointers. In Workshop on Run-Time Result \nVeri.cation, 1999. Xavier Rival. Symbolic transfer function-based approaches to certi.ed compilation. \nIn 31st symposium Principles of Programming Languages, pages 1 13. ACM Press, 2004. Bernhard Steffen. \nProperty-oriented expansion. In Static Analysis, Third International Symposium, SAS 96, volume 1145 of \nLecture Notes in Computer Science, pages 22 41. Springer, 1996. Jean-Baptiste Tristan and Xavier Leroy. \nFormal veri.cation of translation validators: A case study on instruction scheduling optimizations. In \n35th symposium Principles of Programming Languages, pages 17 27. ACM Press, 2008. Anna Zaks and Amir \nPnueli. Covac: Compiler validation by program analysis of the cross-product. In FM 2008: Formal Methods, \n15th International Symposium on Formal Methods, volume 5014 of Lecture Notes in Computer Science, pages \n35 51. Springer, 2008. Lenore Zuck, Amir Pnueli, and Raya Leviathan. Validation of optimizing compilers. \nTechnical Report MCS01-12, Weizmann institute of Science, 2001. Lenore Zuck, Amir Pnueli, Yi Fang, and \nBenjamin Goldberg. VOC: A methodology for translation validation of optimizing compilers. Journal of \nUniversal Computer Science, 9(3):223 247, 2003.    \n\t\t\t", "proc_id": "1542476", "abstract": "<p>Translation validation establishes <i>a posteriori</i> the correctness of a run of a compilation pass or other program transformation. In this paper, we develop an efficient translation validation algorithm for the Lazy Code Motion (LCM) optimization. LCM is an interesting challenge for validation because it is a global optimization that moves code across loops. Consequently, care must be taken not to move computations that may fail before loops that may not terminate. Our validator includes a specific check for anticipability to rule out such incorrect moves. We present a mechanically-checked proof of correctness of the validation algorithm, using the Coq proof assistant. Combining our validator with an unverified implementation of LCM, we obtain a LCM pass that is provably semantics-preserving and was integrated in the CompCert formally verified compiler.</p>", "authors": [{"name": "Jean-Baptiste Tristan", "author_profile_id": "81342514111", "affiliation": "INRIA Paris-Rocquencourt, Rocquencourt, France", "person_id": "P1464306", "email_address": "", "orcid_id": ""}, {"name": "Xavier Leroy", "author_profile_id": "81100078576", "affiliation": "INRIA Paris-Rocquencourt, Rocquencourt, France", "person_id": "P1464307", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1542476.1542512", "year": "2009", "article_id": "1542512", "conference": "PLDI", "title": "Verified validation of lazy code motion", "url": "http://dl.acm.org/citation.cfm?id=1542512"}