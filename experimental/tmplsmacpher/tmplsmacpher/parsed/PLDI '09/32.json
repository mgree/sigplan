{"article_publication_date": "06-15-2009", "fulltext": "\n Control-.ow Re.nement and Progress Invariants for Bound Analysis Sumit Gulwani Sagar Jain Eric Koskinen \nMicrosoft Research, Redmond IIT Kanpur University of Cambridge sumitg@microsoft.com sagarj@iitk.ac.in \nejk39@cam.ac.uk Abstract Symbolic complexity bounds help programmers understand the performance characteristics \nof their implementations. Existing work provides techniques for statically determining bounds of pro\u00adcedures \nwith simple control-.ow. However, procedures with nested loops or multiple paths through a single loop \nare challenging. In this paper we describe two techniques, control-.ow re.ne\u00adment and progress invariants, \nthat together enable estimation of precise bounds for procedures with nested and multi-path loops. Control-.ow \nre.nement transforms a multi-path loop into a seman\u00adtically equivalent code fragment with simpler loops \nby making the structure of path interleaving explicit. We show that this enables non-disjunctive invariant \ngeneration tools to .nd a bound on many procedures for which previous techniques were unable to prove \ntermination. Progress invariants characterize relationships between consecutive states that can arise \nat a program location. We further present an algorithm that uses progress invariants to compute pre\u00adcise \nbounds for nested loops. The utility of these two techniques goes beyond our application to symbolic \nbound analysis. In partic\u00adular, we discuss applications of control-.ow re.nement to proving safety properties \nthat otherwise require disjunctive invariants. We have applied our methodology to over 670,000 lines \nof code of a signi.cant Microsoft product and were able to .nd symbolic bounds for 90% of the loops. \nWe are not aware of any other published results that report experiences running a bound analysis on a \nreal code-base. Categories and Subject Descriptors C.4 [Performance of Sys\u00adtems]: Measurement techniques; \nReliability, availability, and ser\u00adviceability; D.2.4[Software Engineering]: Software/ProgramVer\u00adi.cation; \nD.4.5 [Operating Systems]: Reliability Veri.cation; D.4.8 [Operating Systems]: Performance Modeling and \npre\u00addiction; F.3.1 [Logics and Meanings of Programs]: Specifying andVerifying and Reasoning about Programs; \nF.3.2[Logics and Meanings of Programs]: Semantics of Programming Languages Program analysis General Terms \nVeri.cation, Performance,Reliability Keywords Bound analysis,Termination,Control-.ow re.nement, Progressinvariants, \nProgramveri.cation,Formalveri.cation Permission to make digital or hard copies of all or part of this \nwork for personal or classroomuseisgrantedwithout feeprovidedthat copies arenot madeordistributed forpro.torcommercialadvantage \nandthatcopiesbearthisnoticeandthefullcitation onthe .rstpage.To copy otherwise,to republish,topostonserversorto \nredistribute tolists, requirespriorspeci.cpermission and/ora fee. PLDI 09, June15 20,2009,Dublin, Ireland. \nCopyright c &#38;#169;2009ACM978-1-60558-392-1/09/06. . .$5.00 1. Introduction Software engineerslackthe \ntoolstheyneedtobuildrobust,ef.cient software. They are therefore forced to rely on existing techniques \nsuch as testing and performance pro.ling which are limited, leaving many usage scenarios uncovered. In \nparticular, as processor clock speeds begin to plateau, there is an increasing need to focus on software \nperformance. This paper addresses the problem of statically generating sym\u00adbolic complexity bounds for \nprocedures in a program, given a cost model for atomic program statements. Automated methods of gen\u00aderating \nsymbolic complexity bounds offer a signi.cant advance in aiding the performance aspects of the software \nlife cycle. Though programmers are often cognizant of the intended complexity of an algorithm, concrete \nimplementations can differ. Moreover, sym\u00adbolic bounds can highlight the impact of changes and illuminate \nperformance of unfamiliar APIs. The most challenging aspect of computing complexity bounds is calculating \na bound on the number of iterations of a given loop. Two kinds of techniques have been proposed for automatically \nbounding loop iterations: pattern matching [18] and counter in\u00adstrumentation [17, 12, 15]. Unfortunately, \nthese techniques have limitations: (i) They compute bounds for simple loops that have a single path (a \nstraight-line sequence of statements) or a set of paths with similar effect, but not multi-path loops \nthat have multi\u00adple paths with different effects and non-trivial interleaving patterns. (ii) Theycompute \nconservative bounds in presence of nested loops since they simply compose bounds for individual loops \nbased on structural decomposition of the program. In this paper, we present techniques that address these \nlimitations. The .rst technical contribution of our work is a novel technique called control-.ow re.nement, \nwhich is a semantics-and bound\u00adpreserving transformation on procedures. Speci.cally, a loop that consists \nof multiple paths (arising from conditionals) is trans\u00adformed into a code fragment with one or more loops \nin which the interleavingof pathsis syntactically explicit.We describe an algo\u00adrithm (Section 4) that \nexplores all path interleavings in a recursive fashion using an underlying invariant generation tool. \nThe proce\u00addure with transformed loop enables a more precise analysis (e.g. with the same invariant generation \ntool) than would have been pos\u00adsible with the original loop. The additional program points created by \nre.nement allow the invariant generator to store more informa\u00adtion about the procedure. Different invariants \nat related program points in the re.ned loop correspond to a disjunctive invariant at the original location \nin the original loop. We detail the application of the control-.ow re.nement technique for symbolic bound \nanal\u00adysis (Section 4.3). The technique enables bound computation for loops for which no other technique \ncan even establish termination (Section 2.1).  (a) Original Procedure (b) Change of notation (c) Expanded \nLoop (d) Final Re.ned Loop cyclic(int id, maxId): cyclic(int id, maxId): cyclicref (int id, maxId): cyclicpruned \n(int id, maxId): assume(0 = id < maxId); assume(0=id<maxId); 1 assume(0=id<maxId); 10 assume(0 = id < \nmaxId); int tmp := id+1; int tmp := id+1; 2 int tmp := id+1; 11 int tmp := id+1; while(tmp =id &#38;&#38; \nnondet) Repeat(Choose({.1,.2})); 3 Choose({ 12 Choose({ if (tmp = maxId) 4 skip, 13 skip, tmp := tmp \n+ 1; 5 Repeat+(.1), 14 Repeat+(.1);.2;Repeat(.1), else 6 Repeat+(.2), 15 Repeat+(.1) tmp := 0; 7 Repeat+(.1);.2 \n;Repeat(Choose({.1,.2})), 16 }); 8 Repeat+(.2);.1 ;Repeat(Choose({.1,.2})) 9 }); .1 \" assume(tmp . tmp=maxId); \ntmp:=tmp+1; .2 \" assume(tmp . tmp>maxId); tmp:=0; =id =id Figure 1: (a) illustrates an iteration pattern \nseen in product code. (b),(c), and (d) show control-.ow re.nement of the multi-path loop in (a). The \nsecond technical contribution of our work is the notion of progress invariants that describe relationships \nbetween any two consecutive states that can arise at a given program location. We present an algorithm \nfor computing such relationships using a stan\u00addard invariant generator (Section 5). The algorithm runs \nthe invari\u00adant generation tool over a procedure appropriately modi.ed and instrumented with extra variables \nthat copythe program state at ap\u00adpropriate locations. We observe that progress invariants are more precise \nthan the related notion of transition invariants [24] or vari\u00adance analyses [5] (recently described in \nliterature for proving ter\u00admination), which describe relationships between a state at a pro\u00adgram location \nand any other previous state at that location. Tran\u00adsition invariants can be generated from progress \ninvariants but not vice-versa. (See discussion in Section 9).We believe the notion of progress invariants \nto have applications beyond bound analysis. Afurther contribution is that we show (in Section 6) how \nto use progress invariants to compute a precise bound for nested loops. This technique applies to procedures \nthat may have been control\u00ad.ow re.ned. Thekeyidea is to use progress invariants to illuminate relationships \nbetween any two consecutive states of an inner loop per iteration of some dominating outer loop (as opposed \nto the immediately dominating outer loop). This information is then used to compute the amortized complexity \nof an inner loop. Such an amortized complexity yields a more precise bound when nested loops share same \niterators, which occurs often in practice. In summary, we make the following contributions: 1. We introduce \ncontrol-.ow re.nement, a novel program trans\u00adformation that allows standard invariant generators to reason \nabout loops with structured interleaving between paths in the loop body. This transformation has applications \nbeyond bound analysis, and brie.y discuss one such application on proving non-trivial safety properties \nof procedures that otherwise re\u00adquire specialized analyses [13, 19, 14, 10, 17] (Section 8). 2. We de.ne \nprogress invariants, which describe relationships be\u00adtween a state at a program location and the previous \nstate at the same location, and show how to compute them. Progress in\u00advariantshave applicationsbeyond \nbound analysis.Forexample they can be applied to the problem offair termination (proving procedure termination \nunderfairness constraints). 3. We de.ne an algorithm for computing precise bounds of nested loops using \nprogress invariants (Section 6). 4. To the best of our knowledge, we present the .rst experimental results \nfor bound analysis on the source of a signi.cant Mi\u00adcrosoft product (Section 7). We have built a full \ninterproce\u00addural bound analysis for C++, using C# and F# on top of the Phoenix [1] compiler. Our results \nshow that 90% of non-trivial procedures can be bounded with our technique (Section 7.2).   2. Overview \nIn this section we illustrate the challenges offered by multi-path loops and nested loops in computing \nprecise bounds for procedures. We also brie.y describe our twokey techniques that address these challenges; \nthese techniques are described in more detail in the subsequent sections. The examples are adapted from \nthe source of a large Microsoft product. For clarity, we have distilled their core control .ow and renamed \nsome variables. 2.1 Multi-PathLoops Consider the example in Figure 1(a), which is adapted from the product \ncode. This procedure is a form of cyclic iteration: ini\u00adtially tmp is equal to id+1, tmp is incremented \nuntil it reaches maxId+1 (along the tmp = maxId branch), tmp is then reset to 0 (along the else branch), \nand .nally tmp is incremented until it reaches id. We would like to automatically conclude that the total \nnumber of iterations for this loop is bounded above by maxId+1. None of the bound analysis techniques \nthat we know of can automatically compute a bound for such a loop. This is because path-sensitive disjunctive \ninvariants are required to establish a bound. Recent work [15] proposes elaborate counter instrumen\u00adtation \nstrategies to reduce dependence on disjunctive invariants, yet would fail to compute a bound because \nthe invariants required are path-sensitive. In fact, we do not know of any technique that can even prove \ntermination of this procedure. Recent techniques [6, 5] based on disjunctively well-founded ranking functions \n[24] fail for this example because there does not exist a disjunctively well\u00adfounded linear ranking function. \nThe mildly complex control .ow in the loop foils all known approaches. A detailed analysis of the failure \nof these approaches on this example would illustrate that these approaches tend to con\u00adsider all possible \ninterleavings between the two paths through the loop. However, the two paths are interleaved in a more \nstructured manner. Let us represent the control-.ow using a regular expres\u00adsion, letting .1 and .2 denote \nthe increment and reset branches, respectively. Then, the path interleavings in the example loop can \nbe more precisely described by the re.nement (. * 1 .2.1 *)|(. * 1 ) of the original control-.ow (.1|.2)* \n. While (.1|.2)* suggests that paths .1 and .2 can interleave in an arbitrary manner, the re.ne\u00adment \n(.1 * .2 . * 1 )|(.1 *) explicitly indicates that path .2 executes at most once. Next, we brie.y describe \nhow such a re.nement can be carried out automatically, and how it enables bound computation. Control-.ow \nRe.nement The .rst key idea of this paper is a technique called control-.ow re.nement. Rather than abstracting \nthe control-.ow, which blurs interleavings, we instead re.ne the control-.ow by making interleavings \nmore explicit. Subsequently, an invariant generation tool may determine that some paths are infeasible, \noften resulting in a procedure that is easier to analyze.  Figure 1(b) shows the original program re-written \nusing our no\u00adtation (formally described in Section 3) that uses assume state\u00adments to replace all conditionals \nwith non-deterministic choice. Repeat repeatedly executes its argument a non-deterministic (0 or more) \nnumber of times, as long as the corresponding assume statements are satis.ed. Repeat+is identical to \nRepeat except that it executes its argument at least once. Choose selects non\u00addeterministically among \nits arguments (i.e. among those that satisfy the corresponding assume statements). Figure 1(c) illustrates \nthe key ingredient of the control-.ow re.nement: a semantics and bound preserving expansion of a multi\u00adpath \nloop, wherein Repeat(Choose({.1 ,.2})) is replaced by a choice between one of the following: Loop does \nnot execute: skip  Only .1 executes, at least once: Repeat+(.1)  Only .2 executes, at least once: Repeat+(.2) \n .1 executes .rst, at least once, followed by the execution of .2, and .nally a non-deterministic interleaving \nof .1 and .2: Repeat+(.1);.2;Repeat(Choose({.1,.2 }))  .2 executes .rst, at least once, followed by \nthe execution of .1, and .nally a non-deterministic interleaving of .1 and .2: Repeat+(.2);.1;Repeat(Choose({.1,.2 \n})) .  A general form of this expansion for loops with more than two paths is described in Property \n4.1. Figure 1(d) shows the re.ned version of the program obtained from the expanded program in Figure \n1(c) after simpli.cation with the help of an invariant generation tool that can establish the fol\u00adlowinginvariants:(i)The \nmulti-pathloopatLine7hastheinvariant tmp = id < maxId;hence only path.1 is feasible inside the multi\u00adpath \nloop at Line 7. (ii) Line 3 has the invariant tmp = maxId; hence path .2 is infeasible at the start of \nLines 8, and 6. These in\u00advariants are easily computed by several standard (conjunctive, path\u00adinsensitive) \nlinear relational analyses [22, 7]. The simpli.cation used to obtain Figure 1(d) from Figure 1(c) may \nnot always be possible after one expansion, but may require repeated expansion of multi-path loops. This \nraises the issue of ter\u00admination of the expansion step, addressed in detail in Section 4.2. We can easily \nbound the number of iterations of each loop in Figure 1(d) using our technique of progress invariants \n(de\u00adscribed next). In particular, our technique can establish that the two Repeat+(.1)loops at Line 14 \nrun for at mostmaxId -id and id iterations respectively, combined with the single execution of .2 to \nyield a total of at most maxId +1 iterations. Meanwhile, the Repeat+(.1)loop on Line 15 runs for at mostmaxId \n-id itera\u00adtions. Thus, we can conclude a bound of maxId +1 on the number of iterations of the loop in \nthe original program in Figure 1(a).  2.2 Nested Loops Consider the procedure in Figure 2, which is \nan example of nested loops with related iterator variables, seen commonly in product code. Such loops \noften arise when an inner loop is used to skip ahead through progress bounded by an outer loop. It is \nnot dif.cult to see that the values of the loop iterator variables i, j, and k increase in each iteration \nof the corresponding loop, and hence the complexity of the above loop is O(n\u00d7m\u00d7N). However, this is an \noverly conservative bound. Observe that the total number of iterations of the innermost loop L3 is bounded \nby N (as opposed to n \u00d7 m \u00d7 N)since the value of the iterator k at the entry to loop L3 is greater than \nor equal to the value of k when loop L3 was last executed. Hence, the total combined iterations of all \nthe three loops is bounded above by n +(m \u00d7 n)+ N. We do not know of any existing bound analysis technique \nthat can compute a precise bound for the above procedure. Recent work [15] proposes elaborate counter \ninstrumentation strategies to Consider the following triple-nested loop. NestedLoop(int n, int m, int \nN): 1 assume(0 = n . 0 = m . 0 = N); 2 i := 0; 3 L1 : while (i<n &#38;&#38; nondet) 4 j := 0; 5 L2: while \n(j<m &#38;&#38; nondet) 6 j := j+1; 7 k := i; 8 L3: while (k<N &#38;&#38; nondet) 9 k := k +1; 10 i := \nk; 11 i := i+1; Figure2:Anexampleof nested loops with related iteratorvariables. compute a counter-optimal \nbound; it generates a bound of (n + N) \u00d7 (1 + m), which is not the most precise bound, but still bet\u00adter \nthan the conservative cubic bound1. Recent termination tech\u00adniques [6, 5] based on disjunctively well-founded \nranking func\u00adtions [24] would come up with the termination argument that ei\u00adther i increases or j increases \nor k increases at each cut-point, and all three of them are bounded. Such an argument would again im\u00adply \na conservative cubic bound. Our technique can compute the precise bound of n+(m\u00d7n)+ N for the total number \nof all loop iterations.We illustrate here the challenging aspect of proving that the total number of \niterations of the innermost loop are bounded above by N. Note that the procedure in Figure 2 is already \ncontrol-.ow re.ned (there are no multi-path loops) so that doesn t help here. Our bound computation technique \nuses the notion of progress invariants described below. Progress Invariants The secondkeyidea of this \npaper is the no\u00adtion of progress invariants that characterize the sequence of states that arise at a \ngiven program location in between any two visits to another program location. Progress invariants are \nessential to our bound computation algorithm (described in Section 6), which .nds more a precise bound \nthan previous techniques based on sim\u00adple structure decomposition. Our progress invariants (parameter\u00adized \nover an abstract domain D)are: INITD(P,p1 ,p2) denotes the property of the initial state of procedure \nP that can arise during the .rst visit to location p2 after anyvisit to location p1.  NEXTD (P,p1,p2) \ndenotes the relationship between a state (over program variables Xx)at a given program location p2 and \nthe previous state (over fresh variables Xxold ) at that location, without an intervening visit to location \np1.  We present algorithms in Section 5 to compute the progress in\u00advariants INITD and NEXTD given a \nstandard invariant generation tool. For NestedLoop (Figure 2), standard relational linear analy\u00adses [22, \n7] can generate the following progress invariants: NEXTD (NestedLoop,p0 ,p3):(k = kold + 1) . k<N INITD \n(NestedLoop,p0 ,p3): k = 0 where p0 is the entry point of procedure NestedLoop, and p3 is the program \npoint just inside loop L3. Our bound analysis engine (presented in Section 6) can con\u00adclude from the \nabove invariants that the number of times location 9 is visited (after the last visit to location 1)is \nbounded above byN. 1Counter instrumentation [15] computes a bound of (n + N) \u00d7 (1 + m) because of its \ngreedy heuristic to use up the smallest number of counters. It ends up using the same counter to count \nthe total number of iterations of the loops L1 and L3, which gets bounded by n + N. This results in the \nnumber of iterations of loop L2 to get bounded by m \u00d7 (n + N).   3. Notation We now turn to a formal \nmodel of our techniques. In this section, we introduce some notation which we will use in the subsequent \nsections when we present path re.nement and our precise method of calculating procedure bounds. 3.1 Program \nModel For simplicity of presentation, we assume that each procedureP is described as a statement s using \nthe following structural language: s ::= s1; s2 | Repeat(s) | Choose({s1,..,st}) | x := e | assume(cond) \n| skip where x is a variable from the set of all variables Xx, e is some expression, and cond is some \nboolean expression. The expression e can contain procedure calls2. The above model has the following \nintuitive semantics. Since there are non-deterministic conditionals, its semantics can be char\u00adacterized \nby showing its operational semantics on a set of states. The following function [s]s illustrates how \na statement s trans\u00adforms a set s of concrete states. [skip]s = s [s1; s2]s = [s2]([s1]s) [Choose({s1,..,st})]s \n= [s1]s ... . [st]s [Repeat(s)]s = s .[s; Repeat(s)]s [x := e]s = {d[x . d(e)] |d . s} [assume(cond)]s \n= {d |d . s,d(cond)= true} where d(e) and d(cond) respectively denote the value of expres\u00adsion e or cond \nin state d. We often use the notation Repeat+(s) to denote s; Repeat(s). Standard deterministic control \n.ow in branches and loops can be modeled in our notation as follows. if(c)s1 else s2 . Choose({(assume(c); \ns1), (assume(\u00acc); s2)}) while(c)s1 . Repeat(assume(c); s1 ); assume(\u00acc); 3.2 Abstract Domain Our framework \nis parameterized by a standard abstract domain D, with an abstract element denoted E. However operations \nin the ab\u00adstract domain only occur behind the curtains of the invariant gen\u00aderator I NVARIANTD . The \nonly abstract element which appears ex\u00adplicitly in our algorithms is the minimal element .D . Our tech\u00adniques \nare inter-operable with a variety of existing tools, so we will use some APIs throughout the paper.We \nassume an invariant gen\u00ad ' erator INVARIANTD (P,p,SD (Xx)) . SR x,Xx ) which takes a D (Xprocedure P, \na program point p, an abstract state SD over program variables Xx, and returns an invariant SR which \nholds at p. This D invariant generator can be for anyabstract domain D.  4. Control-.ow Re.nement In \nthis section, we present a technique called control-.ow re.ne\u00adment, which is a semantics-preserving and \nbound-preserving trans\u00adformation of loops within a procedure. Speci.cally, a loop consist\u00ading of multiple \npaths (resulting from a conditional) is re.ned into one or more loops in which the interleaving of paths \nis syntactically explicit. Subsequently, an invariant generation tool may determine that some paths are \ninfeasible, often resulting in an overall proce\u00addure that is easier to analyze. Our algorithm is described \nin Section 4.2. It uses an operation called FLATTEN that we introduce next. 2Procedure calls may have \nside effects, but for simplicity we de.ne the semantics of [x := e]s assuming the absence of side effects. \nREFINE(P:Procedure, sloop:Repeat statement) 1 let sloop be Repeat(s) occurring at location p in P. 2 \nE := INVARIANTD (P, p,true); 3 s := FLATTEN(s); 4 Q := Push(E,Empty Stack); 5 (sresult ,Z) := R(s, Q); \n6 return P with sloop replaced by sresult ; R(s:Flattened stmt, Q:stack of abstract elements) 1 let s \nbe of the form Choose({.1 , . . , .t}). 2 E := Top(Q); 3 for i = 1 to t 4 si := (Repeat+(.i); Choose({.1 \n, . . , .i-1, .i+1, .t})); 5 pex := exit point of si; 6 E ' := INVARIANTD (si, pex, E); 7 if (E ' = .D \n) si := .; 8 else if (.Et . Q s.t.E ' = Et) Zi := {E ' }; 9 else (s ' , Zi) := R(s, Push(Q, E ' )); si \n:= si; s ' ; 10 Sif := {skip}; Swh := \u00d8; 11 for i = 1 to t 12 Sif := Sif .{Repeat+(.i)}; 13 if (si = \n.) continue; 14 if (.Et . Zi s.t.Et = E) Swh := Swh .{si}; 15 else Sif := Sif .{si}; 16 Z := Z .Zi -{E}; \n17 return (Choose(Sif .Repeat(Choose(Swh ))), Z); Figure3: The algorithmREFINE for re.ning the control-.owofa \nloop Repeat(s) in initial state E by invoking REFINE(s,E). 4.1 Flattening of a statement De.nition 4.1 \n(Flatten). Given a statement s, we de.ne FLATTEN(s) to be a statement of the form Choose({.1 ,..,.t}) \nsuch that for any set of states s, we have: [s]s = [Choose({.1,..,.t})]s where each .i is a straight \nline sequence of atomic x := e or assume statements or Repeat loops (and, importantly, no Choose statements).Werefer \nto such .i as a path. The .atten operation can be implemented as: FLATTEN(s)= Choose(F(s)) where the \nfunction F(s) maps a statement s into a set of straight\u00adline sequences as follows: F(s1; s2 )= {.1; \n.2 |.1 .F(s1),.2 .F(s2)} F(Choose({s1,..,st})) = F(s1) ... .F(st) F(s)= {s} for all other s Example 1. \nConsider the following code fragment. def s = if c then s1 else s11 ; s2; if c ' then s3; Flattening \nof the above code fragment yields, in our notation: Choose({assume(c); s1 ; s2; assume(c ' ); s3 , assume(\u00acc); \ns11; s2; assume(c ' ); s3, assume(c); s1; s2; assume(\u00acc ' ), assume(\u00acc); s11; s2; assume(\u00acc ' ) })  \n4.2 Re.nement of a loop The REFINE algorithm in Figure 3 performs control-.ow re.ne\u00adment of a multi-path \nloop sloop in the initial state E, and returns a procedure that is semantically equivalent in the input \nstate E (The\u00adorem 4.1). The key idea is to use the following property that de\u00adscribes how a .attened, \nmulti-path loop can be unfolded into 2t+1 different cases depending on which loop path iterates .rst, \nand whether any other path iterates afterwards. This is the generaliza\u00adtion of the two-path loop discussed \nin Section 2.1.  Property 4.1. Let s and si (for 1 = i = t)be as follows. def s = Choose({.1, . .,.t}) \ndef si = Repeat+(.i); Choose({.1, . .,.i-1,.i+1,. .,.t}); Repeat(s) def ' s = Repeat+(.i); Then, for \nany set of states s, we have: [Repeat(s)]s = [Choose({skip,s1,..,st,s 1' ,..,s t' })]s Of these 2t +1 \ncases, there are t cases (corresponding to s1,..,st)that have multi-path loops, which are then further \nre.ned recursively. To ensure termination, we use an underlying invariant generator INVARIANTD to compute \nthe state before each newly created multi-path loop. We then either stop the recursive explo\u00adration (if \nI NVARIANTD can establish unreachability), put a back\u00adedge (if INVARIANTD .nds a state already seen), \nor use widening heuristics (in case INVARIANTD generates invariants over an in.\u00adnite domain). Note that \nalthough our algorithm is exponential in the number of paths through the body of the loop, we use a strict \nslicing technique tokeep the constants small. Our slicing technique (described in Section 7.1) reduces \nthe number of paths by collaps\u00ading branches that do not impact the iterations of the loop into a single \npath. The REFINE algorithm invokes a recursive algorithm R on the .attened body s of the input loop, \nalong with a stack containing the element E, which is the only input con.guration seen before any loop. \nR consumes a .attened loop body s and a stack Q of abstract elements. Qrepresent the input abstract states \nimmediately before the while loop Repeat(s) seen during the earlier (but yet '' '' un.nished) recursive \ncalls to R. Rreturns a pair (s ,Z) where s is a statement and Z is a set of input abstract states that \nwere re\u00advisited by the recursive algorithm during the re.nement and used to terminate exploration at \nthe promise of arranging a nested loop at appropriate places. The .rst loop in R (Lines 3-9) recursively \nre.nes the t cases (s1,..,st) from Property 4.1 that have multi-path loops, one by one. R re.nes si by \nchoosing between one of the following 3 possibilities depending on the element E ' computed before the \nmulti-path loop in si: Stop exploration (Line 7) if E ' = .D , denoting unreachability.  Create a nested \nloop (Line 8) if E ' belongs to stack Q(i.e. it is an input state that has been seen before). Further \nexploration is stopped and E ' is returned to denote the place where the nested loop needs to be created. \n Pursue more exploration (Line 9) otherwise, recursively.  If the abstract domain D is a .nite domain, \nthen the .rst loop in Rterminates because the algorithm is never recursively invoked with the same input \nstate E twice. Otherwise additional measures are required to ensure termination. A trivial way to ensure \ntermi\u00adnation this would be to override the equality check in Line 8 with return true if the size of stack \nQi becomes equal to some prese\u00adlected constant.Abetterwayto accomplish thisis witha widening algorithm \nassociated with the domain D, wherein the contents of the stack Qi are treated as that of the corresponding \nwidening se\u00adquence for purpose of checking equality. The second loop in R (Lines 11-16) puts together \nthe result of re.ning the t recursive cases along with the other t +1 cases. Swh collects all the cases \nto be put together inside a loop at the current level of exploration (thereby meeting the promise of \narranging a nested loop), while Sif collects all other cases. The following theorem states that control-.ow \nre.nement is semantics-and bound-preserving. Original Figure 1(a) with maxId renamed by n. Re.ned Figure \n1(d) Bound: n Example 2. assume(n>0 . m>0); v1:= n; v2:= 0; while (v1>0 &#38;&#38; nondet) if (v2<m) \nv2++; v1--; else v2:=0; assume(n > 0 . m > 0); v1:= n; v2:= 0; Choose({ skip, Repeat(Repeat+(.1);.2), \nRepeat+(.1) }); assume(v1= 0); where.2 \u00a3 assume(v1>0); v2:=0; .1 \u00a3 assume(v1>0.v2<m);v2++;v1--; Bound: \nn + n m Example 3. assume (0<m<n); i := 0; j := 0; while (i<n &#38;&#38; nondet) if (j<m) j++; else j \n:= 0; i++; assume(0<m<n); i := n; Choose({ skip, Repeat(Repeat+(.1); .2), Repeat+(.1) }) where.1 \u00a3 assume(i<n.j<m);j++; \n.2 \u00a3 assume(i<n.j=m);j:=0;i++; Bound: n \u00d7 m Example 4. assume (0<m<n); i := n; while (i>0 &#38;&#38; \nnondet) if (i<m) i--; else i := i-m; assume(0<m<n); i := n; Choose({ skip, Repeat+ (.2 ); Repeat(.1), \nRepeat+ (.2 ) }) where.1 \u00a3 assume(i>0.i<m);i--; .2 \u00a3 assume(i>0.i=m);i:=i-m; Bound: n + m m Example 5. \nassume(0 < m < n); i := m; while (0 < i < n) if (dir=fwd) i++; else i--; assume(0 < i < n); Choose({ \nskip, Repeat+ (.1 ), Repeat+ (.2 ), }) where.1 \u00a3 assume(dir=fwd);i++; .2 \u00a3 assume(dir =fwd);i--; Bound: \nmax(m, n -m) Theorem 4.1. (Control-.ow Re.nement) For any loopsloop inside a procedure P, and any set \nof initial states s  Figure 4: Some non-trivial iterator patterns from product code that all have a \nsimilar multi-loop structure with 2 paths, but very different path-interleavings, and as a result, different \nbounds. [REFINE(P,sloop)]s = [P]s Also, REFINE(P,sloop) and P have the same complexity bound.  4.3 Case \nStudies The table in Figure 4 shows several non-trivial iterator patterns found in product code that \nshare very similar syntactic structure: a single multi-path loop with 2 paths (iterating over variables \nthat range over 0 to n or m). However, the process of control-.ow re.nement results in signi.cantly different \nlooping structures, be\u00adcause of the different ways in which the 2 paths interleave (which is made explicit \nby our control-.ow re.nement technique). In par\u00adticular, we obtain nested loops for 2nd and 3rd example, \nsequential loops for 1st and 4th example, and a choice of loops for the 5th example. This leads to signi.cantly \ndifferent bounds.   5. Progress Invariants As discussed in Section 2, existing techniques for computing \ncomplexity bounds are often imprecise. In this section, we in\u00adtroduce a special form of invariants, we \ncall progress invariants: the INITD (P,p1,p2) and NEXTD (P,p1,p2) relations, which are associated with \ntwo program locations p1 and p2 inside a proce\u00addure P. While progress invariants may have other applications, \nwe use them in this paper to be able to reason about the progress of one particular loop with respect \nto another loop. As a result, our bound computation algorithm (discussed in the next section) can be \nprecise. We will refer back to Figure2throughout this section.NestedLoop is triple-nested and the innermost \nloop (effectively) increments the same counter as the outermost loop. As discussed in Section 2, previous \ntechniques would compute an overly conservative bound of m \u00d7 n \u00d7 N rather than n +(m \u00d7 n)+ N. We start \nby describing a simple transformation on a proce\u00addure called SPLIT that is useful for computing INITD \nand NEXTD . SPLIT(P,p) takes a procedure P and a program location p (inside P) as inputs and returns \n(P ' ,p ' ,p '' ), where P ' is the new pro\u00adcedure obtained from P by splitting program location p into \ntwo locations p ' and p '' such that the predecessors of p are connected to p ' and the successors of \np are connected to p '' , and there is no connection between p ' and p '' . The SPLIT transformation \nis a fun\u00addamental building block that is used to compute the two progress invariant relations we describe \nin the remainder of the section. 5.1 The NEXTD (P,p1,p2) Relation We de.ne NEXTD (P,p1 ,p2) to be a relation \nover variables Xx (those that are live at location p2) and their counterparts Xxold that describes the \nrelationship between any two consecutive states that arise at p2 without an intervening visit to location \np1. More formally, let s1,s2,..., denote any sequence of program states that arise at location p2 after \nany visit to location p1, but before any other visit (to p1). Let si,i+1 denote the state over Xx . xX' \nsuch that for any variable x . xX' , si,i+1(xold )= si(x) and si,i+1(x)= si+1(x). Then, for all i, si,i+1 \nsatis.es the relation NEXTD(P,p1 ,p2). We can compute NEXTD as follows using an invariant generator INVARIANTD \n. NEXTD (P,p1,p2): 1 E1 := INVARIANTD (P,p2,true); 2 (P1 ,p 1' ,p 1 '' ) := SPLIT(P,p1); 3 (P2 ,p 2' \n,p 2 '' ) := SPLIT(P1,p2); 4 Let P3 be P2 with entry point changed to p2 '' and instrumented with Xxold \n:= Xx at p2 '' ; 5 E2 := INVARIANTD (P3,p 2' ,E1); 6 return E2; This algorithm begins by using an invariant \ngeneration proce\u00addure to generate an abstract element as a loop invariant for p2 (Line 1). We then perform \ntwo transformations on the .ow graph: the region of interest (all paths from p2 to p2 which do not pass \nthrough p1) is isolated by eliminating the path from p1 to p2 (Lines 2 and 4), and p2 is instrumented \nwith Xxold := Xx (Lines 3 and 4). Finally, we compute a new invariant at p2 ' (Line 5) seeded with the \noriginal loop invariant. We now return to the example in Figure 2. As we will describe in the next section, \nit is useful to obtain a NEXTD invariant for each nested loop L with respect to its dominating loops \nL ' . Let p1 be the program point just inside loop L1;similar forp2 and p3. Let p0 be the entry point \nof procedure NestedLoop.For this example, an invariant generator may .nd (among other things): NEXTD \n(NL,p0,p1) : i = iold +1 .i<n NEXTD (NL,p1,p2 ) : j = jold +1 .j<m NEXTD (NL,p0,p3 ) : k = kold +1 .k<N \n We later explain (Section 6) how to use these invariants to obtain a bound. However, for now note that \nthese expressions describe the progress of variables with respect to outer loop iterations. For example, \nwe see that at p3, k is always greater than or equal to kold +1, and the loop invariant is that k<N. \nFrom this, along with initial conditions on k, we will later (Section 6) conclude that the total number \nof loop iterations of L3 is bounded by N. 5.2 The INITD (P,p1,p2) Relation We de.ne INITD (P,p1,p2) \nto be a relation over variables Xx (those that are live at location p2) that describes the state that \ncan arise during the .rst visit to p2 after the last visit to location p1. We can compute INITD as follows \nusing an invariant generator INVARIANTD. INITD(P,p1 ,p2): 1 E1 := INVARIANTD (P,p1, true); 2 (P1,p 1' \n,p 1 '' ) := SPLIT(P,p1); 3 (P2,p 2' ,p 2 '' ) := SPLIT(P1,p2); 4 Let P3 be P2 with entry point changed \nto p1 '' . 5 E2 := INVARIANTD (P3,p 2 ' ,E1); 6 return E2; This algorithm is similar to the algorithm \nused to compute NEXTD ,but has important differences. First, the initial abstract ele\u00adment E1 holds at \np1 (Line 1). Second, the transformation preserves the path from p1 to p2 (Line 4) and false holds on \nall edges out of p2 '' . Finally, we are not interested in computing invariants over rela\u00adtionships over \nthe value of variables between two successive states (hence there is no instrumentation step). The algorithm \ntherefore computes invariants which hold the .rst time p2 is reached coming from p1, rather than loop \ninvariants over p2. We again return to Figure 2, where a standard invariant genera\u00adtion tool may .nd \n(among other things): INITD (NL,p0,p1) : i =0 INITD (NL,p1,p2) : j =0 INITD (NL,p0,p3) : k = 0 The purpose \nof INITD is to study properties of the .rst element represented in the sequence N EXTD (invoked with \nthe same argu\u00adments).We laterexplain (Section6)howto use theseinvariantsto obtain a bound.  6. Bound \nComputation In this section, we describe how progress invariants (introduced in Section 5) can be used \nto compute precise bounds. This technique can be applied to any procedure, but we apply it to procedures \nfor which we .rst perform control-.ow re.nement (introduced in Section 4) to reason about path interleavings. \nWe introduce some useful notation.For anyloopLin procedure P, we de.ne T(L) to be the upper bound on \nthe total number of iterations of L in procedure P. For any loops L, L ' such that L is nested inside \nL ' , we de.ne I(L,L ' ) to be the upper bound on the total number of iterations of L for each iteration \nof L ' . 6.1 Bounding Loop Iterations Fundamental to computing complexity bounds is the task of calcu\u00adlating \nthe numberof iterationsofa loop.We denote this procedure BOUNDFINDER3. It consumes an abstraction of \nthe initial state of the loop (given in some abstract domain D)as well as an abstrac\u00adtion of the relation \nbetween any two successive states in a loop. These abstractions are given by the progress invariants \nINITD and NEXTD described in Section 5. The output is both 3This name follows the spirit of RankFinder \n[23], which accomplishes a similar task of .nding a ranking function for a transition invariant.  B(s) \n= (1, \u00d8) (1) where s . {skip, x:=e, assume(c)} B(s1; s2) = (c1 + c2, Z1 .Z2) (2) where (c1, Z1) = B(s1) \nand (c2, Z2) = B(s2 ) B(Choose({s1, . . , st})) = (Max{c1, . . ,ct}, Z1 .. . .Zt) (3) = where (ci, Zi) \n= B(si) ' B(L : Repeat(s )) = (0, Z .(c, L)) (4) X ' '' '' where c = c + (c \u00d7 I(L ,L)) (c '' ,L'' ).Z' \n,P arent(L'' )=L '''' ''''' '' and Z = {(c ,L ) where (c ,L ) . Z ,Parent(L )= L} '' ' and (c ,Z )= B(s \n) X BOUND(s)= c + c ' \u00d7 T(L ' ) (c ' ,L' ).Z where (c,Z)= B(s) Figure 5: Calculating the precise bound \nBOUND(s) on a statement s. I(L,L ' )= BOUNDFINDERD (INITD (P,p ' ,p), NEXTD (P,p ' ,p),V ) T(L)= BOUNDFINDERD \n(INITD (P,pen ,p), NEXTD (P,pen ,p),V ) where p is the .rst location inside loop L, p ' is the .rst location \ninside loop L ' , pen is the entry point of procedure P, and V is the set of all input variables. Continuing \nwith the example in Figure 2, from the progress invariants given in Section 5, BOUNDFINDER would bound \nthe total number of loop iterations as: T(L3)= N and T(L1)= n. Moreover, B OUNDFINDER would conclude \nthat the number of iterations of loop L2 per iteration of L1 is: I(L2,L1)= m. These quantities allow \nus to compute an overall bound of n+(m\u00d7n)+N using the equations given in the next section. BOUNDFINDER \ncan be implemented in a variety of ways. One potential way to implement B OUNDFINDER is with counter \ninstru\u00admentation by using ideas from previous work [12, 15]. Alterna\u00adtively it can be implemented via \nuni.cation against a database of known loop iteration lemmas.We implemented the latter technique, as \nwe expected it would be more ef.cient and comprehensive for the experiments discussed in Section 7. \n 6.2 Intraprocedural Bound Computation In order to compute a precise bound BOUND(s) on a statement s, \nwe de.ne B(s) recursively as shown in Figure 5. For any loop L, we use Parent(L) to denote the outermost \ndominating loop L ' such that I(L,L ' )= 8, if any such loop L ' exists and if T(L)= 8. Otherwise Parent(L)= \nundefined. B recurs over the annotated syntax of the statement s. It is aided by I(L,L ' ) and T(L) computed \nas described in the previous section. B returns a pair (c,Z), where c denotes the bound of s excluding \nthe contribution of any loop Li such that (ci,Li) . Z. Furthermore, for anyloop Li, there is at most \none entry of the form (ci,Li) in Z, and ci denotes the bound of the body of loop Li. The base cases are \nskip, assignment, and assume statements (Eqn. 1) where the bound is one and there are no loops excluded. \nSequential composition (Eqn. 2) is merely the sum of the bounds and combines loop exclusions; non-deterministic \nchoice is similar (Eqn. 3). When the Breaches a loop L(Eqn. 4), bound calculation is more subtle. The \nbound in this case is not given directly because the context of the loop is unknown. Instead, the bound \nis deferred by accumulating a pair (c,L) where c is the bound of the body of the loop, which will be \nmultiplied in a future recursive call by outer loops where the context is known. However, we must process \nthe bound of other inner loops L '' that have been deferred to be processed in the current context of \nL. Ultimately, we reach the base case, where B OUND(s) can now be obtained directly (right-hand side \nof Figure 5). Theorem 6.1. (Bound Computation via Progress Invariants) The complexity of a procedure, \nassuming a unit cost model for all atomic statements and procedure calls, is bounded by BOUND(P). 6.3 \nExamples Example 6. Consider the following procedure P with two disjoint parallel inner loops L1 and \nL2 nested inside an outer loop L. i:=j:=k:=0; while(i++<n) { if (nondet) while(j++<m); else while(k++<m); \n} Given that T(L1) = T(L2) = m and T(L)= n, we obtain BOUND(P)= n+2m. (Note n+ m is not a correct answer, \nwhile n \u00d7 m is correct but conservative.) This example demonstrates a subtle aspect of B. The elements \nof a pair of cost and deferred loop (c,Z) (arising from recursive invocations on sub-structures of s) \nmust be tallied differently. Where Z is tallied identically under se\u00adquential composition (Eqn. 2) and \nnon-deterministic choice (Eqn. 3), c is instead aggregated as summation and max, respectively. Previous \nExamples. We return to the example in Figure 2, where we concluded (in Section 6.1) that T(L3)= N, T(L1)= \nn, and I(L2,L1)= m. Using the above de.nitions of BOUND and Bit is easy to show that BOUND(NestedLoop)= \nn +(m \u00d7 n)+ N. Let us also consider the original example in Figure 1, listed in Figure 1(a), and then \nre.ned in Figure 1(d). Let L14a and L14b be the .rst and second loops on Line 14, and let L15 be the \nloop on Line 15. There are no nested loops, but using INITD and NEXTD , BOUNDFINDER would .nd that T(L14a)= \nT(L15a)= maxId -id and that T(L14b)= id. It is now easy to check that BOUND(cyclic)= maxId +1.  6.4 \nInterprocedural Extension The bound computation described in the above section assigns a unit cost to \nall atomic statements including procedure calls. How\u00adever, in order to obtain an interprocedural computation \ncomplexity, we can compute the cost for a procedure call x := P(y) using the following standard process \n[15,3].We replace the formal inputs of procedure P by actuals y in the bound expression BOUND(P), and \nthen translate this to a bound only in terms of the inputs of the en\u00adclosing procedure by using the invariants \nat the procedure call site that relateywith the procedure inputs. This processworks only for non-recursive \nprocedures that need to be analyzed in a top-down order of the call-graph.   Figure 6: Success rates \nfor non-trivial procedure bounds.  7. Evaluation 7.1 Implementation We implemented a static interprocedural \nanalysis for computing symbolic complexity bounds of C and C++ procedures, based on the Phoenix [1] compiler \nframework. Our tool includes support for standard C++ control-.ow structures (e.g. if, switch, for, while, \ndo-while). An important heuristic is our slicing technique. We slice each loop by preserving only statements \nthat control the loop iteration behavior this is done by computing a backward slice starting with the \nconditionals that exit the loop. Slicing is an important optimiza\u00adtion that helps generate small loop \nskeletons that usually do not in\u00adcurablowupwhen .atteningis appliedtotheloopbody.Weimple\u00admented procedure \nslicing, and .attening in C#.Bound computation (including control-.ow re.nement and progress invariants) \nis then accomplished in F#. This library consists of modules for manipu\u00adlating relational .ow graphs \nand an abstract interpreter, which uses the Z3 [2] theorem prover. Implementation of BOUNDFINDER. We \nimplemented the search for boundexpressionsina style similarto uni.cation.Wehave im\u00adplemented several \nlemma patterns for each of the iteration classes described below, and search for a pattern which matches \nthe output of progress invariants NEXTD and INITD 4. Arithmetic Iteration. Manyloops use simple arithmetic \naddition for iteration, consisting of an initial value for the iterator, a maximum (or minimum) loop \ncondition, and an increment (or decrement) step in the body of the loop.  Bit-wise Iteration. Some loop \nbodies either consistofa left/right shift or an inclusive OR operation with a decreasing operand.  Data \nStructure Iteration. We implemented patterns for itera\u00adtions over linked list .elds (e.g. x = x->next), \nencapsulated iterators (e.g. x = GetNext(l)), and destructive iteration (e.g. x = RemoveHead(l)).  Loop \niterators beyond these categories are discussed in our limita\u00adtions (Section 7.3) and an area for future \nwork.  7.2 Experiments We evaluated our technique by running several experiments over the source code \nof a large Microsoft product. All experiments below were run on a Hewlett-Packard XW4600, with a Dual \nCore 3 GHz processor. The hardware included 4 GB of RAM and 250 GB of hard disk space. Our software stack \nconsisted of Windows Vista, the Phoenix April 2008 SDK, F# version 1.9.4.19, and Z3 [2] 4Note that BOUNDFINDER \ncan also be implemented using counter instru\u00admentation [15], though we found uni.cation to be more ef.cient. \nFigure 7: Performance of our tool. (a) Lines of Code (b)Individual Loop Bounds Module L.O.C. Module1 \n110,469 Module2 132,803 Module3 80,348 Module4 221,120 Module5 126,028 Total: 670,768 Module Loops Bounded \nSuccess Module1 1574 1513 0.96 Module2 1749 1570 0.90 Module3 1165 1035 0.89 Module4 535 491 0.92 Module5 \n2511 2410 0.96 Total: 7534 7019 0.93 Figure 8: (a) Lines of Code and (b) effectiveness of computing \nloop bounds foravarietyof modules from the product source code.For legal reason, the module name is suppressed. \nModule Module1 Module2 Module3 Module4 Module5 Proc. 7192 10816 6280 4871 9363 Non-Triv. 1746 1956 1181 \n744 2862 Isolated Proc. Inter-procedural Count 1639 1674 973 629 2714 Rate 0.94 0.86 0.82 0.85 0.95 Count \n1578 1527 897 578 2601 Rate 0.90 0.78 0.76 0.78 0.91 Total: 38522 8489 7629 0.90 7181 0.84 Figure 9: \nEffectiveness of computing procedure bounds for a vari\u00adety of modules from the product source code. The \nchart considers two cases: (1) procedures individually ( Isolated Proc. ), without regard to procedure \ncall sites and (2) effectiveness after including an interprocedural analysis. version 1.3.5. Our analysis \nwas run over a variety of modules from a large Microsoft product; the line count of each module is given \nin Figure 8(a). Loop Bounds. For our .rst experiment, we quantify the util\u00adity of our technique for bounding \nmulti-path loops, by measuring how frequently BOUNDFINDER is able to .nd a bound for indi\u00advidual loops. \nWe ran our analysis on several modules and counted the number of loops L for which our technique could \ncompute a bound. Successful bound computation for a loop L means being able to compute T(L) if Lis an \noutermost loop, or I(L,L ' ) where L ' is the loop that immediately dominates L. The results are sum\u00admarized \nin Figure 8(b). Each module consists of many source .les, themselves each consisting of several procedures \nwith loops. Out of the total number of loops in the second column, our technique found a bound for the \namount in the third column, yielding the suc\u00adcess rate in the .nal column. Across all modules, our technique \nfound a bound for 93% of the loops. Isolated Procedures. Procedures are more dif.cult to bound, because \nthey may contain multiple (possibly nested) loops, all of which must be bounded. Our next experiment \ntests BOUND, which calculates a cumulative bound across arbitrary procedure structures.To study this \nproblem, we measured our tool s ability to compute a bound for individual procedures, without regard \nto call sites to other procedures. Many procedures are trivial (contain no loops), so our analysis focuses \non the non-trivial procedures. The results of our experiment for several of the largest modules is given \nin Figure9 (and pictoriallyin Figure6) labeled Isolated Proc.  Inter-procedural Analysis. We then evaluated \nthe effective\u00adness of our inter-procedural technique, which properly accounts the cost of call sites \n(see Section 6.4). While this is a more accurate measure of a procedure s cost, it decreased our success \nrate to 84%. Thisis becauseofa cascadeeffect :ifwefailto computea bound for procedure A, then any other \nprocedure B that involves a call site to Awill also be unbounded. The results of this experiment are \nalso given in Figures 9and 6, labeled Inter-procedural. Limitation. One experimental limitation is that, \ndue to the size of the source, we were unable to comprehensively check the pre\u00adcision of the complexity \nbounds. However, we manually inspected manyof the bounds and con.rmed that theywere, indeed, precise. \nPerformance. For each non-trivial procedure we also mea\u00adsured the time it took to .nd a bound for the \n.attened version of the procedure. This includes control-path re.nement via REFINE, progress invariant \ngeneration via INITD and NEXTD (which use INVARIANTD ), .nding loop bounds via BOUNDFINDER, and .\u00adnally \ncalculating the total bound via BOUND. Across all modules, the performance is given in Figure 7. This \ngraph illustrates the time it takes to .nd a bound for a single procedure (in seconds). A per\u00adfectly \nef.cient tool would calculate bounds for 100% of procedures instantly. When our technique is successful, \nover 90% of proce\u00addures are bounded within 640ms.Forfailed attempts, only 70% of procedures are bounded \nwith 640ms. This suggests a possible im\u00adprovement in the performance of our tool by aborting the search \nfor a bound after, say, 640ms. Figure 7 also illustrates the ef.cacy of our slicing heuristic. Most non-trivial \nfunctions have at most 8 paths after slicing; thus our algorithm typically completes in a fraction of \na second. In less than 10 cases (among the 670,000 lines of code we evaluated) the number of paths was \nlarge enough for the analysis to time-out.  7.3 Limitations There are some loops (roughly 7%) for which \nour tool is unable to .nd a bound. As with anylarge code base, the modules vary in cod\u00ading styles and \nparadigms, yet we were surprised by how widely ap\u00adplicable our techniquewas.We categorized the unsuccessful \nloops (somewhat automatically) into the following challenges, postponed to future work: Concurrency. \nMany procedures contained concurrent algo\u00adrithms, such as spin-locks or work queues, in which case the \nthe number of loop iterations depends on other threads.  I/O. Some modules contained procedures which \ninteracted with the .lesystem. In these rare cases, the bound again depends on the size (or availability) \nof non-deterministic input.  Recursion.We currently do not address the issue of computing bounds for \nrecursive procedures (though we believe that ideas presented in this paper can potentially be used to \ncompute bounds for recursive procedures).  Procedure Calls. Usually, procedure calls inside a loop do \nnot affect the value of loop iterators. But when they do, we can either inline the appropriately sliced \nversion of the procedure, or use an interproceduralinvariant generation tool.We currently do not implement \nanysuch strategy.  Exponential Paths. Slicing drastically reduces the number of paths in a .attened \nloop body; however, in rare cases, .attening generates an intractable, exponential number of paths. \n  8. OtherApplication: Safety Properties The control-.ow re.nement technique presented in Section 4 \nis more fundamental than the sole application of bound analysis. In particular, it can be used to prove \nsafety properties that otherwise require disjunctive invariants or a path-sensitive analysis. For this \npurpose, we simply use a given simple (path-insensitive) invariant generation tool I to .rst re.ne the \ncontrol-.ow of the procedure, and then analyze the re.ned procedure using I. We need a small extension \nof our control-.ow re.nement algo\u00adrithm describedin Figure3forittobepowerful enoughto establish non-trivial \nsafety properties at the end of the loop. We explicitly add anypost-dominating assume statement at the \nend of the multi\u00adpath loop to all top-level choices in the expansion of the loop. This is done to enable \na path-insensitive invariant generation tool to .l\u00adter out paths that leave the loop prematurely. This \nextension is not required for bound analysis because the focus there was to reason about what happens \ninside the loop, and not outside the loop. Figure 10 presents a list of some examples, each of which \nhas been used as a .agship example to motivate a new technique for proving non-trivial safety assertions. \nProving validity of the asser\u00adtions in all these examples requires disjunctive loop invariants. Figure \n10 also shows the resulting (semantically equivalent) procedure after control-.ow re.nement is applied \nusing either the octagonal [22] or polyhedra [26] analysis as the invariant genera\u00adtion tool. The safety \nassertions in all these procedures can now be validated by running either the octagonal or the polyhedra \nanalysis on the control-re.ned procedure. (Note that running these analyses on the original procedure \nwould fail to validate any of these as\u00adsertions with the exception that octagonal domain can validate \nthe assertion in the last procedure.) For the .rst example, the (induc\u00adtive) loop invariants d = t = \n3 and d = s = 2 for the loops Repeat(.1)andRepeat(.2)respectively imply the assertion. The dotted portion \ndenotes irrelevant code that does not contain any as\u00adsertions. For the second example, the loop invariant \nx = y . x = 50 for the .rst loop Repeat+(.1)helps establishx = y = 51 at the end of the loop; and then \nthe loop invariant x + y = 102 . x = 52 . y = 0 for the second loop Repeat(.2) helps establish the desired \nassertion after the loop. For the third example, the loop invariant x = 50 . y = 50 for the .rst loop \nhelps establish x = y = 50 at the end of the .rst loop Repeat+(.1), and the loop invariant x = y .x = \n100 for the second loop Repeat+(.2) helps establish y = 100 at the end of the second loop. For the fourth \nexample, the assertion is trivially established. For the last example, the invariant x = y gets established \nafter the inner loop on .2, which then propagates itself as the loop invariant for the outer loop too. \n9. RelatedWork Control Flow Re.nement Control-.ow re.nement is related to other approaches that have \nbeen proposed for doing a more pre\u00adcise program analysis given an underlying invariant generation tool. \nThis includes widening strategies (such as look-ahead widen\u00ading [10] and upto widening [17]) or disjunctive \nextensions of domains [13, 25, 11, 14]. The primary goal of these techniques is to compute precise invariants \nat different program points in the original program, while we instead focus on creating a precise ex\u00adpansion \nof the program into one with simpler loops. Our tech\u00adnique is useful for bound computation, a process \nthat is more ef\u00adfective for simple loops, as opposed to complex loops annotated with precise invariants. \nOther work [25] does a CFG elaboration, but only as a means to perform ef.cient computation over some \nre.nementofthepowersetextension.In particular k bounded elab\u00adoration, wherein a program location is duplicated \nat most k times. In contrast, the expansion decision in our algorithm is independent ofa differentexploration \nbranch. More signi.cantly, ourexpansion granularity focuses on interleavings between different paths, \nas op\u00adposed to what happens in different iterations of the same path.  Original Example After Path Re.nement \nHalbwachs et al. 1997, t:=0, d:=0, s:=0; P. 14, F. 7. Choose({ t:=0; d:=0; s:=0; asm(sec&#38;&#38;met);Repeat(.1 \n), while (*) asm(\u00acsec&#38;&#38;met);Repeat(.2 ), if (sec) asm(sec&#38;&#38;\u00acmet); .... s:=0; asm(\u00acsec&#38;&#38;\u00acmet); \n.... if (t++ = 4) break; });... if (met) if (s++ = 3) break; assert(d++ = 10); where .1 \u00a3 s:=0;asm(t++ \n= 4); asm(s++=3);assert(d++=10); .2 \u00a3 asm(s++=3);assert(d++=10); Gopan and Reps 2006, P. 3, F. 1. x:=0, \ny:=0; while (*) if (x = 50) y++; else y--; if (y<0) break; x++; assert(x=102) x:=0; y:=0; Repeat+ (.1 \n); .2; Repeat(.2); if (x = 50) y++; else y--; assume (y < 0); assert(x=102); where .1 \u00a3 asm(x=50);y++;asm(y=0);x++; \n.2 \u00a3 asm(x>50);y--;asm(y=0); x++; Gulwani and Jojic 2007. x:=0; y:=50; P. 7, F. 3. Repeat+ (.1 ); .2; \nRepeat+(.2); x:=0; y:=50; asm(x = 100); while (x<100) assert(y=100); if (x<50) x++; else x++; y++; where \nassert(y=100); .1 \u00a3 asm(x<100&#38;&#38;x<50); x++; .2 \u00a3 asm(x<100&#38;&#38;x=50);x++;y++; Gulavani et \nal. 2006. P. 5, F. 3. lock := 0; assume (x = y); Henzinger et al. 2002. P. 2, F. 1. Choose({ lock := \n0; Repeat+(.2);.1, assume (x = y); Repeat+(.1) while (x = y) }); lock := 1; x := y; asm(x = y); if (*) \nassert(lock = 1); lock := 0; y++; assert(lock = 1); where .1 \u00a3 asm(x = y); lock := 1; x := y; .2 \u00a3 asm(x \n= y); lock := 1; x := y; lock := 0; y++; Figure 10: Prominent disjunctive invariant challenges from recent \nliterature. Our technique .nds suitable disjunctive invariants for eachexample.For brevity, assume(c) \nis denoted asm(c).  Part of our algorithm is based on pruning infeasible paths. Balakrishnan et al. \n[4] present a technique for .nding infeasible paths with backward and forward analyses. Infeasible paths \nare removed from the transition system ( static language re.nement ). The fundamental distinction of \nour work is that we prune infeasible unwound paths, rather than simple, statically occurring paths. With \nrespect to proving safety properties (as discussed in Sec\u00adtion 8), our technique is more precisebut less \nef.cient than widen\u00ading approaches (since CFG expansion allows our technique to ef\u00adfectively compute \ndisjunctive invariants). Our technique is orthog\u00adonal to techniques based on disjunctive extensions (each \nof which is unique w.r.t. the number of disjuncts, and their merging). Progress Invariants Our notion \nof progress invariants is related to transition invariants [24, 6, 21] or variance analyses [5] (re\u00adcently \ndescribed in literature for proving termination), which de\u00adscribe relationships between a state at a \nprogram location and any other (as opposed to immediately) previous state at that location. Hence, theoretically, \nprogress invariants are more precise in the sense that transition invariants can be generated from progress \nin\u00advariantsbut not vice-versa. While both transition invariants and progress invariants are used to measure \nhow a program state evolves by studying relations over pairs of states (as opposed to just a single state), \nit is interesting to observe the differences in the methodologies involved. Transition invariants have \nbeen used for proving termination by computing disjunctive transition invariants and then showing that \neach disjunct is well-founded. In contrast, progress invariants are used for com\u00adputing a bound after \nre.ning the program, which obviates the need for disjunctive invariants. We believe that the separation \nof con\u00adcerns of dealing with non-regular (disjunctive) program behavior and relations on pairs of states \nenables our approach to go beyond proving termination (i.e. computinga precise bound).Forexample, it \nis trivial to prove the nested loop in Section 2.2 terminating,but computing a precise bound requires \na more sophisticated machin\u00adery, such as our notion of progress invariants. On the other hand, our technique \ncan compute bounds for programs for which exist\u00ading termination techniques fail to even prove termination \n(e.g. the cyclic iteration example in Figure 1). Symbolic Bound Computation Recent work by Gulwani et \nal. [15] describes elaborate counter instrumentation strategies for computing a bound on loop iterations \nusing a linear arithmetic in\u00advariant generation tool. However, the strategy is not effective for multi-path \nloops, which seem to occur frequently in practice. For example, the strategy cannot compute a bound for \nanyof the multi\u00adpath loops (except the 3rd example) in Figure 4. For nested loops, the strategy is not \nas precise as the ideas presented here (see fur\u00adther discussion in Section 2.2). Moreover, counter instrumentation \nworks well only for arithmetic programs. To overcome this re\u00adstriction, Gulwani et al. also introduced \nthe notion of user-de.ned quantitative functions for data-structures (such as length of list, height \nof tree) and their updates to express and enable computation of bounds for loops that iterate over data-structures. \nBut the strat\u00adegy is still limited to computing polynomial bounds as opposed to, say, logarithmic (e.g. \nbinary search) or square-root bounds. In contrast, since we do not use counter instrumentation, we need \nnot arithmetize a program using quantitative functions; neither are we restricted to computing polynomial \nbounds. However, we do require that the underlying invariant generation tool support an additional interface \nof generating bounds from progress invariants. Gulavani and Gulwani [12] have described the design of \na rich numerical domain to generate non-linear disjunctive invariants, and they have applied it to generating \nbounds for timing analysis. However, it requires the user to describe important expressions (over which \nlinear relationships are to be tracked) as well as the set of variables that should occur within a max \noperator for each loop. Furthermore, the technique only applies to arithmetic programs. There is a large \nbody of work on estimating worst case execu\u00adtion time (WCET) in the embedded and real-time systems commu\u00adnity \n[27]. The WCET research is largely orthogonal, focused on distinguishing between the complexity of different \ncode-paths and low-level modeling of architectural features such as caches, branch prediction, instruction \npipelines. For establishing loop bounds, WCET techniques either require user annotation, or use simple \ntechniques based on pattern matching [18] or some numerical anal\u00adysis (e.g., relational linear analysis \nto compute linear bounds on the delay or timer variables of the system [17], interval analysis based \napproach [16], and symbolic computation of integer points in a polyhedra [20]). These WCET techniques \ncannot compute precise bounds for the examples considered in this paper.  Goldsmith et al. [9] compute \nsymbolic bounds by curve-.tting timing data obtained from pro.ling. Their technique has the advan\u00adtage \nof measuring real amortized complexity; however the results are not sound for all inputs. CraryWeirich \n[8] presentsa type sys\u00adtem for certifying (as opposed to inferring) resource consumption, including time. \n10. Conclusion and FutureWork We have introduced novel techniques for automatically determin\u00ading symbolic \ncomplexity bounds of procedures. We .rst showed how control-.ow re.nement enables standard invariant \ngenerators to reason about mildly complex control-.ow, which would other\u00adwise require impractical disjunctive \ninvariants. For example, we have proven symbolic complexity bounds for procedures which no previous technique \ncaneven prove termination.We then introduced progress invariants and showed how to use them to compute \nprecise procedure bounds. Finally, our experience with a large Microsoft product illustrates the effectiveness \nof our techniques: our tool was able to .nd bounds for 90% of procedures that involve loops. We believe \nthat a productive direction for future work would be to study broader applications for control-.ow re.nement \nand progress invariants. As discussed in Section 8 there seem to be applications in proving safety as \nwell as liveness properties. Acknowledgments. We thank MatthewParkinson and the anony\u00admous reviewers \nfor their valuable feedback which improved this paper. We also thank the product teams at Microsoft for \ntheir as\u00adsistance with this project and acknowledge funding from a Gates scholarship (Koskinen).   \nReferences [1] Phoenix Compiler. research.microsoft.com/Phoenix/. [2] Z3 Theorem Prover. research.microsoft.com/projects/Z3/. \n[3] Elvira Albert, Puri Arenas, Samir Genaim, and Germ\u00b4Puebla. an Automatic inference of upper bounds \nfor recurrence relations in cost analysis. In SAS, 2008. [4] G. Balakrishnan, S. Sankaranarayanan, F. \nIvancic, O. Wei, and A. Gupta. SLR: Path-sensitive analysis through infeasible-path detection and syntactic \nlanguage re.nement. Lecture Notes in Computer Science, 5079, 2008. [5] Josh Berdine, Aziem Chawdhary, \nByron Cook, Dino Distefano, and Peter W. O Hearn. Variance analyses from invariance analyses. In POPL, \n2007. [6] Byron Cook, Andreas Podelski, and Andrey Rybalchenko. Termina\u00adtion proofs for systems code. \nIn PLDI, 2006. [7] Patrick Cousot and Nicolas Halbwachs. Automatic discovery of linear restraints among \nvariables of a program. In POPL, 1978. [8] Karl Crary and Stephanie Weirich. Resource bound certi.cation. \nIn POPL, 2000. [9] Simon Goldsmith, Alex Aiken, and Daniel Shawcross Wilkerson. Measuring empirical computational \ncomplexity. In ESEC/SIGSOFT FSE, 2007. [10] Denis Gopan and Thomas W. Reps. Lookahead widening. In CAV, \n2006. [11] Denis Gopan and Thomas W. Reps. Guided static analysis. In SAS, 2007. [12] BhargavS. Gulavani \nand Sumit Gulwani.Anumerical abstract domain based on expression abstraction and max operator with application \nin timing analysis. In CAV, 2008. [13] Bhargav S. Gulavani, Thomas A. Henzinger, Yamini Kannan, Aditya \nV. Nori, and Sriram K. Rajamani. SYNERGY: A new algorithm for property checking. In FSE, 2006. [14] Sumit \nGulwani and NebojsaJojic. Programveri.cation as probabilis\u00adtic inference. In POPL, 2007. [15] Sumit Gulwani, \nKrishna Mehra, and Trishul Chilimbi. SPEED: Precise and ef.cient static estimation of program computational \ncomplexity. In POPL, 2009. [16] Jan Gustafsson, Andreas Ermedahl, Christer Sandberg, and Bjorn Lisper. \nAutomatic derivation of loop bounds and infeasiblepaths for wcet analysis using abstract execution. In \nRTSS, 2006. [17] Nicolas Halbwachs, Yann-Erick Proy, and Patrick Roumanoff. Veri.cationof real-timesystems \nusing linear relation analysis. FMSD, 1997. [18] Christopher A. Healy, Mikael Sjodin, Viresh Rustagi, \nDavid B. Whalley, and Robert van Engelen. Supporting timing analysis by automatic bounding of loop iterations. \nReal-Time Systems, 18(2/3), 2000. [19] Thomas A. Henzinger, RanjitJhala, Rupak Majumdar, and Gr\u00b4egoire \nSutre. Lazy abstraction. In POPL, 2002. [20] Bj\u00a8orn Lisper. Fully automatic, parametric worst-case execution \ntime analysis. In WCET, 2003. [21] Ali Mili. Re.exive transitive loop invariants: A basis for computing \nloop functions. In WING, 2007. [22] Antoine Min\u00b4e. The Octagon Abstract Domain. Higher-Order and Symbolic \nComputation, 19(1), 2006. [23] A. Podelski andA. Rybalchenko.Acomplete method for thesynthesis of linear \nranking functions. LNCS, 2003. [24] A. Podelski and A. Rybalchenko. Transition invariants. In LICS, 2004. \n[25] Sriram Sankaranarayanan, Franjo Ivancic, Ilya Shlyakhter, and Aarti Gupta. Static analysis in disjunctive \nnumerical domains. In SAS, 2006. [26] Chao Wang, Zijiang Yang, Aarti Gupta, and Franjo Ivancic. Using \ncounterexamples for improving the precision of reachability compu\u00adtation with polyhedra. In CAV, 2007. \n[27] Reinhard Wilhelm, Jakob Engblom, Andreas Ermedahl, Niklas Holsti, Stephan Thesing, David Whalley, \nGuillem Bernat, Christian Ferdinand, Reinhold Heckmann, Frank Mueller, Isabelle Puaut, Peter Puschner, \nJan Staschulat, and Per Stenstr\u00a8om. The Determination of Worst-Case Execution Times Overview of the Methods \nand Survey of Tools. In TECS, 2007.  \n\t\t\t", "proc_id": "1542476", "abstract": "<p>Symbolic complexity bounds help programmers understand the performance characteristics of their implementations. Existing work provides techniques for statically determining bounds of procedures with simple control-flow. However, procedures with nested loops or multiple paths through a single loop are challenging.</p> <p>In this paper we describe two techniques, <i>control-flow refinement</i> and <i>progress invariants</i>, that together enable estimation of precise bounds for procedures with nested and multi-path loops. Control-flow refinement transforms a multi-path loop into a semantically equivalent code fragment with simpler loops by making the structure of path interleaving explicit. We show that this enables non-disjunctive invariant generation tools to find a bound on many procedures for which previous techniques were unable to prove termination. Progress invariants characterize relationships between consecutive states that can arise at a program location. We further present an algorithm that uses progress invariants to compute precise bounds for nested loops. The utility of these two techniques goes beyond our application to symbolic bound analysis. In particular, we discuss applications of control-flow refinement to proving safety properties that otherwise require disjunctive invariants.</p> <p>We have applied our methodology to over 670,000 lines of code of a significant Microsoft product and were able to find symbolic bounds for 90% of the loops. We are not aware of any other published results that report experiences running a bound analysis on a real code-base.</p>", "authors": [{"name": "Sumit Gulwani", "author_profile_id": "81100315615", "affiliation": "Microsoft Research, Redmond, Redmond, WA, USA", "person_id": "P1464322", "email_address": "", "orcid_id": ""}, {"name": "Sagar Jain", "author_profile_id": "81435597941", "affiliation": "IIT Kanpur, Kanpur, India", "person_id": "P1464323", "email_address": "", "orcid_id": ""}, {"name": "Eric Koskinen", "author_profile_id": "81350575010", "affiliation": "University of Cambridge, Cambridge, United Kingdom", "person_id": "P1464324", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1542476.1542518", "year": "2009", "article_id": "1542518", "conference": "PLDI", "title": "Control-flow refinement and progress invariants for bound analysis", "url": "http://dl.acm.org/citation.cfm?id=1542518"}