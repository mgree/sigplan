{"article_publication_date": "06-15-2009", "fulltext": "\n Trace-based Just-in-Time Type Specialization for Dynamic Languages Andreas Gal*+, Brendan Eich*, Mike \nShaver*, David Anderson*, David Mandelin* , Mohammad R. Haghighat$, Blake Kaplan*, Graydon Hoare*, Boris \nZbarsky*, Jason Orendorff* , Jesse Ruderman*, Edwin Smith#, Rick Reitmaier#, Michael Bebenita+, Mason \nChang+#, Michael Franz+ Mozilla Corporation* {gal,brendan,shaver,danderson,dmandelin,mrbkap,graydon,bz,jorendorff,jruderman}@mozilla.com \nAdobe Corporation# {edwsmith,rreitmai}@adobe.com Intel Corporation$ {mohammad.r.haghighat}@intel.com \nUniversity of California, Irvine+ {mbebenit,changm,franz}@uci.edu Abstract Dynamic languages such as \nJavaScript are more dif.cult to com\u00adpile than statically typed ones. Since no concrete type information \nisavailable, traditional compilers needto emit generic code that can handle all possible type combinationsat \nruntime.We present an al\u00adternative compilation technique for dynamically-typed languages that identi.es \nfrequently executed loop traces at run-time and then generates machine code on the .y that is specialized \nfor the ac\u00adtual dynamic types occurring on each path through the loop. Our method provides cheap inter-procedural \ntype specialization, and an elegant and ef.cient way of incrementally compiling lazily discov\u00adered alternative \npaths through nested loops.Wehave implemented a dynamic compiler for JavaScript based on our technique \nand we have measured speedups of 10x and more for certain benchmark programs. Categories and Subject \nDescriptors D.3.4[Programming Lan\u00adguages]: Processors Incremental compilers, codegeneration. General \nTerms Design, Experimentation, Measurement, Perfor\u00admance. Keywords JavaScript, just-in-time compilation, \ntrace trees. 1. Introduction Dynamic languages such as JavaScript, Python, and Ruby, are pop\u00adular since \nthey are expressive, accessible to non-experts, and make deployment as easy as distributing a source \n.le. They are used for small scripts as well as for complex applications. JavaScript, for example,is \nthedefacto standard for client-side web programming Permission to make digital or hard copies of all \nor part of this work for personal or classroom use is granted without fee provided that copies are not \nmade or distributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page.To copyotherwise, to republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. PLDI 09, June 15 20, 2009, Dublin, Ireland. and is used for the \napplication logic of browser-based productivity applications such as Google Mail, Google Docs and Zimbra \nCol\u00adlaboration Suite. In this domain, in order to provide a .uid user experienceand enableanewgenerationof \napplications, virtual ma\u00adchines must provide a low startup time and high performance. Compilers for statically \ntyped languages rely on type informa\u00adtionto generateef.cient machine code.Inadynamically typed pro\u00adgramming \nlanguage such as JavaScript, the types of expressions may vary at runtime. This means that the compiler \ncan no longer easily transform operations into machine instructions that operate on one speci.c type.Withoutexact \ntype information, the compiler must emit slower generalized machine code that can deal with all potential \ntype combinations. While compile-time static type infer\u00adence might be able to gather type information \nto generate opti\u00admized machine code, traditional static analysis is very expensive and hence not well \nsuited for the highly interactive environment of a web browser. We present a trace-based compilation \ntechnique for dynamic languages that reconciles speed of compilation with excellent per\u00adformanceofthe \ngenerated machine code.Our system usesamixed\u00admodeexecution approach:the system starts runningJavaScriptina \nfast-starting bytecode interpreter. As the program runs, the system identi.es hot (frequently executed) \nbytecode sequences, records them, and compiles them to fast native code. We call such a se\u00adquence of \ninstructions a trace. Unlike method-based dynamic compilers, our dynamic com\u00adpiler operates at the granularity \nof individual loops. This design choice is based on the expectation that programs spend most of their \ntime in hot loops. Even in dynamically typed languages, we expecthotloopstobe mostlytype-stable,meaningthatthe \ntypesof values areinvariant.(12)Forexample,wewouldexpectloop coun\u00adters that start as integers to remain \nintegers for all iterations. When both of these expectations hold, a trace-based compiler can cover the \nprogramexecutionwitha small numberof type-specialized,ef\u00ad.ciently compiled traces. Each compiled trace \ncovers one path through the program with onemappingofvaluestotypes.WhentheVMexecutesacompiled trace, \nit cannot guarantee that the same path will be followed c Copyright &#38;#169; 2009ACM 978-1-60558-392-1/09/06... \n$5.00 or that the same types will occur in subsequent loop iterations. Hence, recording and compilingatrace \nspeculates that the path and typingwillbeexactlyastheywereduring recordingfor subsequent iterations of \nthe loop. Every compiled trace contains all the guards (checks) required to validate the speculation. \nIf one of the guards fails (if control .ow is different, or a value of a different type is generated), \nthe trace exits. If an exit becomes hot, the VM can record a branch trace starting at the exit to cover \nthe new path. In this way, the VM records a trace tree covering all the hot paths through the loop. Nested \nloops can be dif.cult to optimize for tracing VMs. In a na\u00a8ive implementation, inner loops would become \nhot .rst, and the VM would start tracing there. When the inner loop exits, the VMwould detectthatadifferent \nbranchwastaken.TheVMwould try to record a branch trace, and .nd that the trace reaches not the inner \nloop header,but the outer loop header.At this point, theVM could continue tracing until it reaches the \ninner loop header again, thus tracing the outer loop inside a trace tree for the inner loop. But this \nrequires tracing a copyof the outer loop for every side exit and type combination in the inner loop. \nIn essence, this is a form of unintended tail duplication, which can easily over.ow the code cache. Alternatively,theVMcouldsimplystoptracing,andgiveup \non ever tracing outer loops. We solve the nested loop problem by recording nested trace trees. Our system \ntraces the inner loopexactly as the na\u00a8iveversion. The system stops extending the inner tree when it \nreaches an outer loop,butthenit startsanew traceatthe outerloop header.When the outer loop reaches the \ninner loop header, the system tries to call the tracetree for the inner loop. If the call succeeds, the \nVM records the call to the inner tree as part of the outer trace and .nishes the outer trace as normal. \nIn this way, our system can trace any number of loops nested to anydepth without causing excessive tail \nduplication. These techniques allow a VM to dynamically translate a pro\u00adgram to nested, type-specialized \ntrace trees. Because traces can cross function call boundaries, our techniques also achieve the ef\u00adfects \nof inlining. Because traces haveno internal control-.owjoins, they can be optimized in linear time by \na simple compiler (10). Thus, our tracing VM ef.ciently performs the same kind of op\u00adtimizations that \nwould require interprocedural analysis in a static optimization setting. This makes tracing an attractive \nand effective tool to type specialize even complex function call-rich code. We implemented these techniques \nfor an existing JavaScript in\u00adterpreter, SpiderMonkey.We call the resulting tracing VM Trace\u00adMonkey.TraceMonkeysupports \nall the JavaScript features of Spi\u00adderMonkey, with a 2x-20x speedup for traceable programs. This paper \nmakes the following contributions: We explain an algorithm for dynamically forming trace trees to coveraprogram, \nrepresenting nested loops as nested trace trees.  Weexplainhowto speculatively generateef.cient type-specialized \ncode for traces from dynamic language programs.  We validate our tracing techniques in an implementation \nbased on the SpiderMonkeyJavaScript interpreter, achieving 2x-20x speedups on manyprograms.  The remainderofthispaperisorganizedasfollows. \nSection3is a generaloverviewof tracetree based compilationwe useto cap\u00adture and compile frequently executed \ncode regions. In Section 4 we describe our approach of covering nested loops using a num\u00adber of individual \ntrace trees. In Section 5 we describe our trace\u00adcompilation based speculative type specialization approach \nwe use to generate ef.cient machine code from recorded bytecode traces. Our implementation of a dynamic \ntype-specializing compiler for JavaScript is described in Section 6. Related work is discussed in Section8.In \nSection7weevaluate our dynamic compiler basedon 1 for (var i = 2; i < 100; ++i) { 2 if (!primes[i]) 3 \ncontinue; 4 for(vark=i+i;i<100;k+=i) 5 primes[k] = false; 6} Figure 1. Sample program: sieve of Eratosthenes. \nprimes is initialized to an array of 100 false values on entry to this code snippet. Figure 2. State \nmachine describing the major activities ofTrace-Monkey and the conditions that cause transitions to a \nnew activ\u00adity. In the dark box, TM executes JS as compiled traces. In the light gray boxes, TM executes \nJS in the standard interpreter. White boxes are overhead. Thus, to maximize performance, we need to maximize \ntime spent in the darkest box and minimize time spent in the white boxes. The best case is a loop where \nthe types at the loop edge are the same as the types on entry then TM can stay in native code until the \nloop is done. a set of industry benchmarks. The paper ends with conclusions in Section9andan outlookon \nfutureworkis presentedin Section10. 2. Overview: Example Tracing Run This section provides an overview \nof our system by describing how TraceMonkey executes an example program. The example program, shown in \nFigure 1, computes the .rst 100 prime numbers withnestedloops.The narrativeshouldbereadalongwithFigure2, \nwhich describes the activitiesTraceMonkeyperforms and when it transitions between the loops. TraceMonkey \nalways begins executing a program in the byte\u00adcode interpreter. Every loop back edge is a potential trace \npoint. When the interpreter crosses a loop edge, TraceMonkey invokes the trace monitor, which may decide \nto record or execute a native trace. At the start of execution, there are no compiled traces yet, so \nthe trace monitor counts the number of times each loop back edge is executed untila loop becomes hot,currently \nafter2crossings. Note thatthewayourloopsare compiled,theloopedgeis crossedbefore entering the loop, so \nthe second crossing occurs immediately after the .rst iteration. Here is the sequence of events broken \ndown by outer loop iteration:  v0 := ld state[748] // load primes from the trace activation record st \nsp[0], v0 // store primes to interpreter stack v1 := ld state[764] // load k from the trace activation \nrecord v2 := i2f(v1) // convert k from int to double st sp[8], v1 // store k to interpreter stack st \nsp[16], 0 // store false to interpreter stack v3 := ld v0[4] // load class word for primes v4 := and \nv3, -4 // mask out object class tag for primes v5 := eq v4, Array // test whether primes is an array \nxf v5 // side exit if v5 is false v6 := js_Array_set(v0, v2, false) // call function to set array element \nv7 := eq v6, 0 // test return value from call xt v7 // side exit if js_Array_set returns false. Figure \n3. LIR snippet for sample program. This is the LIR recorded for line5 of the sample program in Figure \n1. The LIR encodes the semantics in SSA form using temporary variables. The LIR also encodes all the \nstores that the interpreter would do to its data stack. Sometimes these stores can be optimized away \nas the stack locations are live only on exits to the interpreter. Finally, the LIR records guards and \nside exits to verify the assumptions made in this recording: that primes is an array and that the call \nto set its element succeeds. mov edx, ebx(748) // load primes from the trace activation record mov edi(0), \nedx // (*) store primes to interpreter stack mov esi, ebx(764) // load k from the trace activation record \nmov edi(8), esi // (*) store k to interpreter stack mov edi(16), 0 // (*) store false to interpreter \nstack mov eax, edx(4) // (*) load object class word for primes and eax, -4 // (*) mask out object class \ntag for primes cmp eax, Array // (*) test whether primes is an array jne side_exit_1 // (*) side exit \nif primes is not an array sub esp, 8 // bump stack for call alignment convention push false // push last \nargument for call push esi // push first argument for call call js_Array_set // call function to set \narray element add esp, 8 // clean up extra stack space mov ecx, ebx // (*) created by register allocator \ntest eax, eax // (*) test return value of js_Array_set je side_exit_2 // (*) side exit if call failed \n... side_exit_1: mov ecx, ebp(-4) // restore ecx mov esp, ebp // restore esp jmp epilog // jump to ret \nstatement Figure 4. x86 snippet for sample program. This is the x86 code compiled from the LIR snippet \nin Figure 3. Most LIR instructions compile to a single x86 instruction. Instructions marked with (*) \nwould be omitted by an idealized compiler that knew that none of the side exits wouldeverbe taken. The17 \ninstructions generatedby the compiler comparefavorably with the 100+ instructions that the interpreterwould \nexecute for the same code snippet, including4indirect jumps. i=2. This is the .rst iteration of the outer \nloop. The loop on lines 4-5 becomes hot on its second iteration, soTraceMonkey en\u00adters recording mode \non line 4. In recording mode, TraceMonkey records the code along the trace in a low-level compiler intermedi\u00adate \nrepresentation we call LIR. The LIR trace encodes all the oper\u00adations performed and the types of all \noperands. The LIR trace also encodes guards, which are checks that verify that the control .ow and types \nare identical to those observed during trace recording. Thus, on later executions, if and only if all \nguards are passed, the trace has the required program semantics. TraceMonkey stops recording when execution \nreturns to the loop header or exits the loop. In this case, execution returns to the loop header on line \n4. After recording is .nished,TraceMonkeycompiles the trace to native code using the recorded type information \nfor optimization. The result is a native code fragment that can be entered if the interpreter PC and \nthe types of values match those observed when trace recording was started. The .rst trace in our example, \nT45, coverslines4and5.This tracecanbe enteredifthePCisatline4, i and k are integers, and primes is an \nobject. After compiling T45, TraceMonkeyreturns to the interpreter and loops back to line 1. i=3. Now \nthe loop header at line1 has become hot, soTrace-Monkey starts recording. When recording reaches line \n4, Trace-Monkeyobserves that it has reached an inner loop header that al\u00adready hasa compiled trace, soTraceMonkey \nattempts to nest the inner loop inside the current trace. The .rst step is to call the inner traceasasubroutine.Thisexecutesthelooponline4to \ncompletion andthen returnstothe recorder.TraceMonkey veri.esthatthecall was successful and then records \nthe call to the inner trace as part of the current trace. Recording continues until execution reaches \nline 1, and at which pointTraceMonkey.nishes and compilesa trace for the outer loop, T16  i=4. On this \niteration,TraceMonkeycalls T16. Because i=4,the if statement on line2is taken. This branchwas not takenin \nthe original trace, so this causes T16 tofaila guard and takea sideexit. Theexitisnotyethot,soTraceMonkey \nreturnstothe interpreter, which executes the continue statement. i=5. TraceMonkeycallsT16, which in turn \ncalls the nested trace T45. T16 loops back to its own header, starting the next iteration without ever \nreturning to the monitor. i=6. On this iteration, the sideexit on line2is takenagain. This time, the \nside exit becomes hot, so a trace T23,1 is recorded that coversline3and returnstotheloop header.Thus,theendof \nT23,1 jumps directly to the start of T16. The side exit is patched so that on future iterations, it jumps \ndirectly to T23,1. Atthispoint,TraceMonkeyhas compiledenough tracestocover the entire nested loop structure, \nso the rest of the program runs entirely as native code. 3. Trace Trees In this section, we describe \ntraces, trace trees, and how they are formed at run time. Although our techniques apply to anydynamic \nlanguage interpreter, we will describe them assuming a bytecode interpretertokeeptheexposition simple. \n 3.1 Traces A trace is simply a program path, which may cross function call boundaries.TraceMonkeyfocuses \non loop traces, that originate at a loop edge and represent a single iteration through the associated \nloop. Similar to an extended basic block, a trace is only entered at thetop,butmayhave many exits.In \ncontrasttoanextended basic block, a trace can contain join nodes. Since a trace always only follows one \nsingle path through the original program, however,join nodes are not recognizable as such in a trace \nand have a single predecessor node like regular nodes. Atyped trace is a trace annotated with a type \nfor every variable (including temporaries) on the trace.Atyped trace also has an entry type map giving \nthe required types for variables used on the trace beforetheyare de.ned.Forexample,atracecouldhaveatypemap \n(x: int, b: boolean), meaning that the trace may be entered only if the value of the variable x is of \ntype int and the value of b is of type boolean. The entry type map is much like the signature of a function. \nIn this paper, we only discuss typed loop traces, and we will refer to them simply as traces . Thekey \nproperty of typed loop traces is that they can be compiled to ef.cient machine code using the same techniques \nused for typed languages. InTraceMonkey, traces are recordedin trace-.avored SSA LIR (low-level intermediate \nrepresentation). In trace-.avored SSA (or TSSA), phi nodes appear only at the entry point, which is reached \nboth on entry and via loop edges. The important LIR primitives are constant values, memory loads and \nstores (by address and offset), integer operators, .oating-point operators, function calls, and conditionalexits.Type \nconversions, suchas integerto double, are represented by function calls. This makes the LIR used by TraceMonkey \nindependent of the concrete type system and type conversion rules of the source language. The LIR operations \nare generic enough that the backend compiler is language independent. Figure3shows anexample LIR trace. \nBytecode interpreters typically represent values in a various complex data structures (e.g., hash tables) \nin a boxed format (i.e., with attached type tag bits). Sincea traceis intended to represent ef.cient \ncode that eliminates all that complexity, our traces oper\u00adate on unboxed values in simple variables and \narrays as much as possible. Atrace records all its intermediate values in a small activation record area.To \nmakevariable accessesfast on trace, the trace also imports local and global variables by unboxing them \nand copying them to its activation record. Thus, the trace can read and write thesevariables withsimple \nloads and stores fromanativeactivation recording, independently of the boxing mechanism used by the interpreter. \nWhen the trace exits, the VM boxes the values from this native storage location and copies them back \nto the interpreter structures. For every control-.ow branch in the source program, the recorder generates \nconditionalexit LIR instructions. These instruc\u00adtions exit from the trace if required control .ow is \ndifferent from what it was at trace recording, ensuring that the trace instructions are run only if they \nare supposed to. We call these instructions guard instructions. Most of our traces represent loops and \nend with the special loop LIR instruction. This is just an unconditional branch to the top of the trace. \nSuch traces return only via guards. Now, we describe thekeyoptimizations that are performed as part of \nrecording LIR. All of these optimizations reduce complex dynamic language constructs to simple typed \nconstructs by spe\u00adcializing for the current trace. Each optimization requires guard in\u00adstructions to \nverify their assumptions about the state and exit the trace if necessary. Type specialization. All LIR \nprimitives apply to operands of speci.c types. Thus, LIR traces are necessarily type-specialized, and \na compiler can easily produce a translation that requires no type dispatches. A typical bytecode interpreter \ncarries tag bits along with each value, and to perform anyoperation, must check the tag bits, dynamically \ndispatch, mask out the tag bits to recover the untagged value, perform the operation, and then reapplytags. \nLIR omits everything except the operation itself. Apotential problem is that some operations can produce \nvalues of unpredictable types. For example, reading a property from an object could yield a value of \nany type, not necessarily the type observed during recording. The recorder emits guard instructions that \nconditionally exit if the operation yields a value of a different type from that seen during recording. \nThese guard instructions guarantee that as long as execution is on trace, the types of values match those \nof the typed trace. When the VM observes a side exit along such a type guard, a new typed trace is recorded \noriginating at the side exit location, capturing the new type of the operation in question. Representation \nspecialization: objects. In JavaScript, name lookup semantics are complex and potentially expensive because \ntheyinclude features like object inheritance and eval.To evaluate an object property read expression \nlike o.x, the interpreter must search the property map of o and all of its prototypes and parents. Property \nmaps can be implemented with different data structures (e.g., per-object hash tables or shared hash tables), \nso the search process also must dispatch on the representation of each object found during search.TraceMonkeycan \nsimply observethe resultof the search process and record the simplest possible LIR to access the propertyvalue.Forexample,the \nsearchmight.ndsthevalueof o.x in the prototype of o, which uses a shared hash-table represen\u00adtation that \nplaces x inslot2ofa propertyvector.Thenthe recorded can generate LIR that reads o.x with just two or \nthree loads: one to getthe prototype, possibly onetogetthe propertyvaluevector,and one moretogetslot2fromthevector.Thisisavast \nsimpli.cation and speedup compared to the original interpreter code. Inheritance relationships and object \nrepresentations can change during execu\u00adtion, so the simpli.ed code requires guard instructions that \nensure theobject representationisthe same.InTraceMonkey,objects rep\u00adresentations are assigned an integer \nkey called the object shape. Thus, the guard is a simple equality check on the object shape.  Representation \nspecialization: numbers. JavaScript has no integer type, only a Number type that is the set of 64-bit \nIEEE\u00ad754 .oating-pointer numbers ( doubles ). But many JavaScript operators, in particular array accesses \nand bitwise operators, really operate on integers, so they.rst convert the number to an integer, and \nthen convert any integer result back to a double.1 Clearly, a JavaScriptVM thatwantstobefast must .ndawayto \noperateon integers directly and avoid these conversions. InTraceMonkey, we support two representations \nfor numbers: integers and doubles. The interpreter uses integer representations as much as it can, switching \nfor results that can only be represented as doubles. When a trace is started, some values may be imported \nand represented as integers. Some operations on integers require guards.Forexample, addingtwo integers \ncan produceavalue too large for the integer representation. Function inlining. LIR traces can cross function \nboundaries in either direction, achieving function inlining. Move instructions need to be recorded for \nfunction entry and exit to copy arguments in and returnvalues out. These move statements are then optimized \naway by the compiler using copypropagation. In order to be able to return to the interpreter, the trace \nmust also generate LIR to record that a call frame has been entered and exited. The frame entry and exit \nLIR saves just enough information to allow the intepreter call stack to be restored later and is much \nsimpler than the interpreter s standard call code. If the function being entered is not constant (which \nin JavaScript includes anycall by function name), the recorder must also emit LIR to guard that the function \nis the same. Guards and side exits. Each optimization described above requires one or more guards to \nverify the assumptions made in doing the optimization.Aguardis justa groupof LIR instructions that performs \na test and conditional exit. The exit branches to a side exit, a small off-trace piece of LIR that returns \na pointer to a structure that describes the reason for the exit along with the interpreter PC at the \nexit point and anyother data needed to restore the interpreter s state structures. Aborts. Some constructs \nare dif.cult to record in LIR traces. For example, eval or calls to external functions can change the \nprogram state in unpredictable ways, making it dif.cult for the tracer to know the current type map in \norder to continue tracing. Atracing implementation can also have anynumber of other limi\u00adtations, e.g.,a \nsmall-memory device may limit the length of traces. When anysituation occurs that prevents the implementation \nfrom continuing trace recording, the implementation aborts trace record\u00ading and returns to the trace \nmonitor.  3.2 Trace Trees Especially simple loops, namely those where control .ow, value types, value \nrepresentations, and inlined functions are all invariant, can be represented by a single trace. But most \nloops have at least some variation, and so the program will take side exits from the main trace. Whena \nsideexit becomes hot,TraceMonkey startsa new branchtrace from that point and patches the side exit to \njump directlytothat trace.Inthisway,a single traceexpandson demand to a single-entry, multiple-exit trace \ntree. This section explains how trace trees are formed during execu\u00adtion. The goal is to form trace trees \nduring execution that cover all the hot paths of the program. 1Arrays are actually worse than this: if \nthe index value is a number, it must be converted from a double to a string for the property access operator, \nand then to an integer internally to the array implementation. Starting a tree. Tree trees always start \nat loop headers, because theyarea naturalplacetolookforhotpaths.InTraceMonkey,loop headers are easy to \ndetect the bytecode compiler ensures that a bytecode is a loop header iffit is the target of a backward \nbranch. TraceMonkeystarts a tree when a given loop header has been exe\u00adcuted a certain number of times \n(2 in the current implementation). Starting a tree just means starting recording a trace for the current \npointandtypemapandmarkingthe traceastherootofa tree.Each treeis associatedwithaloop headerandtypemap,so \ntheremaybe several trees foragiven loop header. Closing the loop. Trace recording can end in several \nways. Ideally, the trace reaches the loop header where it started with the same type map as on entry. \nThis is called a type-stable loop iteration. In this case, the end of the trace can jump right to the \nbeginning, as all the value representations are exactly as needed to enter the trace. The jump can even \nskip the usual code that would copyout the state at the end of the trace and copyit back in to the trace \nactivation record to enter a trace. In certain cases the trace might reach the loop header with a different \ntype map. This scenario is sometime observed for the .rst iterationofaloop.Somevariables insidetheloopmight \ninitiallybe unde.ned,beforetheyaresettoaconcretetypeduringthe.rstloop iteration. When recording such \nan iteration, the recorder cannot link the trace back to its own loop header since it is type-unstable. \nInstead, the iteration is terminated with a side exit that will always fail and return to the interpreter. \nAt the same time a new trace is recorded with the new type map. Every time an additional type\u00adunstable \ntraceisaddedtoaregion,itsexittypemapis comparedto the entry map of all existing traces in case theycomplement \neach other.With this approach we are able to cover type-unstable loop iterations as long theyeventually \nform a stable equilibrium. Finally, the trace might exit the loop before reaching the loop header, for \nexample because execution reaches a break or return statement. In this case, the VM simply ends the trace \nwith an exit to the trace monitor. As mentioned previously, we may speculatively chose to rep\u00adresent \ncertain Number-typedvaluesas integerson trace.Wedoso when we observe that Number-typed variables contain \nan integer value at trace entry. If during trace recording the variable is unex\u00adpectedly assigned a non-integer \nvalue, we have to widen the type of the variable to a double. As a result, the recorded trace becomes \ninherently type-unstable since it starts with an integer value but ends with a double value. This represents \na mis-speculation, since at trace entry we specialized the Number-typed value to an integer, assuming \nthatattheloopedgewewouldagain .ndan integervalue inthevariable,allowingustoclosetheloop.Toavoid futurespec\u00adulativefailuresinvolvingthisvariable,andto \nobtaina type-stable tracewe notethefactthatthevariablein questionasbeen observed to sometimes hold non-integer \nvalues in an advisory data structure which we call the oracle . When compiling loops, we consult the \noracle before specializ\u00ading values to integers. Speculation towards integers is performed only if no \nadverse information is known to the oracle about that particular variable. Whenever we accidentally compile \na loop that is type-unstable due to mis-speculation of a Number-typed vari\u00adable, we immediately trigger \nthe recording of a new trace, which based on the now updated oracle information will start with a dou\u00adble \nvalue and thus become type stable. Extending a tree. Side exits lead to different paths through the loop, \nor paths with different types or representations. Thus, to completely cover the loop, the VM must record \ntraces starting at all side exits. These traces are recorded much like root traces: there is acounter \nfor each sideexit, and when the counter reachesahotness threshold, recording starts. Recording stops \nexactly as for the root trace, using the loop header of the root trace as the target to reach.  Our \nimplementation does not extend at all side exits. It extends onlyifthesideexitisfora control-.ow branch,andonlyiftheside \nexit does not leave the loop. In particular we do not want to extend a trace tree along a path that leads \nto an outer loop, because we want to cover such paths in an outer tree through tree nesting.  3.3 Blacklisting \nSometimes, a program follows a path that cannot be compiled into a trace, usually because of limitations \nin the implementation. TraceMonkey does not currently support recording throwing and catching of arbitrary \nexceptions. This design trade off was chosen, because exceptions are usually rare in JavaScript. However, \nif a program opts to use exceptions intensively, we would suddenly incur a punishing runtime overhead \nif we repeatedly try to record a trace for this path and repeatedly fail to do so, since we abort tracing \nevery time we observe an exception being thrown. Asa result,ifahotloop contains tracesthatalwaysfail,theVM \ncould potentially run much more slowly than the base interpreter: theVM repeatedly spendstime tryingto \nrecord traces,butisnever ableto runany.Toavoid this problem, whenevertheVMis about to start tracing, \nit must try to predict whether it will .nish the trace. Our prediction algorithm is based on blacklisting \ntraces that havebeentriedandfailed.WhentheVMfailsto .nishatrace start\u00adingatagivenpoint,theVM recordsthatafailurehas \noccurred.The VMalsosetsacountersothatitwillnottrytorecordatracestarting at that point until it is passed \na few more times (32 in our imple\u00admentation). This backoff counter gives temporary conditions that prevent \ntracinga chance to end.Forexample,a loop may behave differently during startup than during its steady-stateexecution. \nAf\u00adteragiven numberoffailures(2in our implementation), theVM marks the fragment as blacklisted, which \nmeans the VM will never again start recording at that point. After implementing this basic strategy, \nwe observed that for small loops that get blacklisted, the system can spend a noticeable amount of time \njust .nding the loop fragment and determining that ithasbeen blacklisted.Wenowavoidthatproblemby patchingthe \nbytecode.We de.ne anextra no-op bytecode that indicatesa loop header. The VM calls into the trace monitor \nevery time the inter\u00adpreter executes a loop header no-op. To blacklist a fragment, we simply replace \nthe loop header no-op with a regular no-op. Thus, the interpreter will never again even call into the \ntrace monitor. Thereisarelatedproblemwehavenotyetsolved,which occurs when a loop meets all of these conditions: \n The VM can form at least one root trace for the loop.  There is at least one hot side exit for which \nthe VM cannot complete a trace.  The loop body is short.  In this case, the VM will repeatedly pass \nthe loop header, search for a trace, .nd it, execute it, and fall back to the interpreter. With a short \nloop body, the overhead of .nding and calling the trace is high, and causes performance to be even slower \nthan the basic interpreter. So far, in this situation we have improved the implementation so that the \nVM can complete the branch trace. But it is hard to guarantee that this situation will never happen. \nAs future work, this situation could be avoided by detecting and blacklisting loops for which the average \ntrace call executes few bytecodes before returning to the interpreter. 4. Nested Trace Tree Formation \nFigure7shows basic trace tree compilation(11) appliedtoa nested loop where the inner loop contains two \npaths. Usually, the inner loop (with header at i2)becomes hot .rst, and a trace tree is rooted atthatpoint.Forexample,the.rst \nrecorded tracemaybeacycle Figure 5. A tree with two traces, a trunk trace and one branch trace. The trunk \ntrace contains a guard to which a branch trace was attached. The branch trace containa guard that mayfail \nand trigger asideexit.Boththetrunkandthe branch traceloopbacktothetree anchor, which is the beginning \nof the trace tree.  Figure 6. We handle type-unstable loops by allowing traces to compile that cannot \nloop back to themselves due to a type mis\u00admatch. As such traces accumulate, we attempt to connect their \nloop edges to form groups of trace trees that can execute without having to side-exit to the interpreter \nto cover odd type cases. This is par\u00adticularly important for nested trace trees where an outer tree tries \nto call an inner tree (or in this case a forest of inner trees), since inner loops frequently have initially \nunde.ned values which change type to a concrete value after the .rst iteration. through the inner loop, \n{i2,i3,i5,a}. The a symbol is used to indicate that the trace loops back the tree anchor. When execution \nleaves the inner loop, the basic design has two choices.First,thesystemcanstoptracingandgiveupon compiling \nthe outer loop, clearly an undesirable solution. The other choice is to continue tracing, compiling traces \nfor the outer loop inside the inner loop s trace tree. For example, the program might exit ati5 and record \na branch trace that incorporates the outer loop: {i5,i7,i1,i6,i7,i1,a}. Later, the program might take \nthe other branch at i2 and then exit, recording another branch trace incorporating the outer loop: {i2,i4,i5,i7,i1,i6,i7,i1,a}. \nThus, the outer loop is recorded and compiled twice, and both copies must be retained in the trace cache. \n  Figure 7. Control .ow graph of a nested loop with an if statement inside the inner most loop (a). \nAn inner tree captures the inner loop, and is nested inside an outer tree which calls the inner tree. \nThe inner tree returns to the outer tree once it exits along its loop condition guard (b). In general, \nif loops are nested to depth k, and each loop has n paths (on geometric average), this na\u00a8ive strategy \nyields O(n k) traces, which can easily .ll the trace cache. In order to execute programs with nested \nloops ef.ciently, a tracing system needsatechnique for covering the nested loops with native code without \nexponential trace duplication. 4.1 Nesting Algorithm Thekeyinsightis thatif eachloopis representedbyitsown \ntrace tree, the code for each loop can be contained only in its own tree, and outerlooppathswillnotbe \nduplicated. Anotherkeyfactisthat we are not tracing arbitrary bytecodes that might have irreduceable \ncontrol .ow graphs,but ratherbytecodes producedbya compiler for a language with structured control .ow. \nThus, given two loop edges, the system can easily determine whether they are nested and which is the \ninner loop. Using this knowledge, the system can compile inner and outer loops separately,and make the \nouter loop s traces call the inner loop s trace tree. The algorithm forbuilding nested trace trees is \nas follows.We start tracing at loop headers exactly as in the basic tracing system. When we exit a loop \n(detected by comparing the interpreter PC with the range given by the loop edge), we stop the trace. \nThe key step of the algorithm occurs when we are recording a trace for loop LR (R for loop being recorded) \nand we reach the header of a different loop LO (O for other loop). Note that LO must be an inner loop \nof LR because we stop the trace when we exit a loop. If LO has a type-matching compiled trace tree, \nwe call LO as a nested trace tree. If the call succeeds, then we record the call in the trace for LR. \nOn future executions, the trace for LR will call the inner trace directly.  If LO does not have a type-matching \ncompiled trace tree yet, we have to obtain it before we are able to proceed. In order to do this, we \nsimply abort recording the .rst trace. The trace monitor will see the inner loop header, and will immediately \nstart recording the inner loop. 2  Ifalltheloopsinanestare type-stable,thenloopnesting creates no duplication. \nOtherwise,if loops are nestedtoadepth k,and each 2Instead of aborting the outer recording, we could principally \nmerely sus\u00adpendthe recording,but thatwould requirethe implementationtobe able to record several traces \nsimultaneously, complicating the implementation, while saving only a few iterations in the interpreter. \nFigure 8. Control.owgraphofaloopwithtwo nestedloops(left) and its nested trace tree con.guration (right). \nThe outer tree calls the two inner nested trace trees and places guards at their side exit locations. \nloop is entered with m differenttype maps(on geometricaverage), then we compile O(m k) copies of the \ninnermost loop. As long as m is close to 1, the resulting trace trees will be tractable. An important \ndetail is that the call to the inner trace tree must act like a function call site: it must return to \nthe same point every time. The goal of nesting is to make inner and outer loops independent; thus when \nthe inner tree is called, it must exit to the same point in the outer tree every time with the same type \nmap. Because we cannot actually guarantee this property, we must guard on it after the call, and side \nexit if the property does not hold. A common reason for the inner tree not to return to the same point \nwould be if the inner tree took a new side exit for which it had never compiled a trace. At this point, \nthe interpreter PC is in the inner tree, so we cannot continue recording or executing the outer tree. \nIf this happens during recording, we abort the outer trace, to give the inner treea chanceto .nishgrowing.Afutureexecutionofthe \nouter tree would then be able to properly .nish and record a call to the inner tree.If an inner tree \nsideexit happens duringexecutionof a compiled trace for the outer tree, we simply exit the outer trace \nand start recording a new branch in the inner tree.  4.2 Blacklisting with Nesting The blacklisting \nalgorithm needs modi.cation to work well with nesting. The problem is that outer loop traces often abort \nduring startup (because the inner tree is not available or takes a side exit), which would lead to their \nbeing quickly blacklisted by the basic algorithm. Thekeyobservation is that when an outer trace aborts \nbecause the inner tree is not ready, this is probably a temporary condition. Thus, we should not count \nsuch aborts toward blacklisting as long asweareabletobuildup more tracesfortheinner tree. In our implementation, \nwhen an outer tree aborts on the inner tree, we increment the outer tree s blacklist counter as usual \nand back off on compiling it. When the inner tree .nishes a trace, we decrement the blacklist counter \non the outer loop, forgiving the outer loop for aborting previously.Wealso undo the backoffso that the \nouter tree can start immediately trying to compile the next time we reach it. 5. Trace Tree Optimization \nThis section explains how a recorded trace is translated to an optimized machine code trace. The trace \ncompilation subsystem, NANOJIT, is separate from the VM and can be used for other applications.  5.1 \nOptimizations Because traces are in SSA form and have no join points or f\u00adnodes, certain optimizations \nare easy to implement. In order to get good startup performance, the optimizations must run quickly, \nso we chose a small set of optimizations. We implemented the optimizations as pipelined .lters so that \nthey can be turned on and offindependently,andyetall runinjusttwoloop passesoverthe trace: one forward \nand one backward. Every time the trace recorder emits a LIR instruction, the in\u00adstruction is immediately \npassed to the .rst .lter in the forward pipeline. Thus, forward .lter optimizations are performed as \nthe trace is recorded. Each .lter may pass each instruction to the next .lter unchanged, write a different \ninstruction to the next .lter, or write no instruction at all.Forexample, the constant folding .lter \ncan replace a multiply instruction like v13 := mul3, 1000 with a constant instruction v13 = 3000. We \ncurrently apply four forward .lters: On ISAs without .oating-point instructions, a soft-.oat .lter converts \n.oating-point LIR instructions to sequences of integer instructions.  CSE (constant subexpression elimination), \n expression simpli.cation, including constant folding and a few algebraic identities (e.g., a - a =0), \nand  source language semantic-speci.c expression simpli.cation, primarily algebraic identitiesthatallow \nDOUBLEtobe replaced withINT.Forexample, LIR that converts an INT toa DOUBLE and then back again would \nbe removed by this .lter.  When trace recording is completed, nanojit runs the backward optimization \n.lters. These are used for optimizations that require backward program analysis. When running the backward \n.lters, nanojitreadsoneLIR instructionatatime,andthereadsarepassed through the pipeline. We currently \napply three backward .lters: Dead data-stack store elimination. The LIR trace encodes many stores to \nlocations in the interpreter stack. But these values are never read back before exiting the trace (by \nthe interpreter or another trace). Thus, stores to the stack that are overwritten before the next exit \nare dead. Stores to locations that are off the top of the interpreter stack at future exits are also \ndead.  Dead call-stack store elimination. This is the same optimization as above, except applied to \nthe interpreter s call stack used for function call inlining.  Dead code elimination. This eliminates \nany operation that stores to a value that is never used.  After a LIR instruction is successfully read \n( pulled ) from the backward .lter pipeline, nanojit s code generator emits native machine instruction(s) \nfor it.  5.2 Register Allocation We use a simple greedy register allocator that makes a single backward \npass over the trace (it is integrated with the code gen\u00aderator). By the time the allocator has reached \nan instruction like v3 = add v1,v2, it has already assigned a register to v3. If v1 and v2 have not yet \nbeen assigned registers, the allocator assigns a free register to each. If there are no free registers, \na value is selected for spilling.We usea class heuristic that selects the oldest register\u00adcarried value \n(6). The heuristic considers the set R of values v in registers imme\u00addiately after the current instruction \nfor spilling. Let vm be the last instruction before the current where each v is referred to. Then the \nTag JSType Description xx1 number 31-bit integer representation 000 object pointer to JSObject handle \n010 number pointer to double handle 100 string pointer to JSString handle 110 boolean enumeration for \nnull, unde.ned, true,false null, or unde.ned Figure 9. Tagged values in the SpiderMonkey JS interpreter. \nTesting tags, unboxing (extracting the untagged value) and boxing (creating taggedvalues) are signi.cant \ncosts.Avoiding these costs isakeybene.tof tracing. heuristic selects v with minimum vm. The motivation \nis that this freesuparegisterforaslongas possiblegivena single spill. If we need to spill a value vs \nat this point, we generate the restore code just after the code for the current instruction. The corresponding \nspill code is generated just after the last point where vs was used.Theregisterthatwas assignedto vs \nis marked free for the preceding code, because that register can now be used freely without affecting \nthe following code 6. Implementation To demonstrate the effectiveness of our approach, we have im\u00adplemented \na trace-based dynamic compiler for the SpiderMonkey JavaScript Virtual Machine (4). SpiderMonkey is the \nJavaScript VM embedded in Mozilla s Firefox open-source web browser (2), whichisusedbymorethan200 million \nusersworld-wide.The core of SpiderMonkeyis a bytecode interpreter implemented in C++. In SpiderMonkey, \nall JavaScript values are represented by the type jsval.A jsval is machinewordin whichuptothe3ofthe least \nsigni.cant bits are a type tag, and the remaining bits are data. See Figure6for details. All pointers \ncontainedin jsvals point to GC-controlled blocks aligned on 8-byte boundaries. JavaScript object values \nare mappings of string-valued property names to arbitrary values. Theyare represented in one of two ways \nin SpiderMonkey. Most objects are represented by a shared struc\u00adtural description, called the object \nshape,that maps property names to array indexes using a hash table. The object stores a pointer to the \nshape and the array of its own property values. Objects with large, unique sets of property names store \ntheir properties directly in a hash table. Thegarbage collector is an exact, non-generational, stop-the\u00adworld \nmark-and-sweep collector. Intherestofthis sectionwe discusskeyareasoftheTraceMon\u00adkeyimplementation. \n6.1 Calling Compiled Traces Compiled traces are stored in a trace cache, indexed by intepreter PC and \ntype map. Traces are compiled so that they may be called as functions using standard native calling conventions \n(e.g., FASTCALL on x86). The interpreter must hit a loop edge and enter the monitor in order to call \na native trace for the .rst time. The monitor computes the current type map, checks the trace cache for \na trace for the current PC and type map, and if it .nds one, executes the trace. To execute a trace, \nthe monitor must build a trace activation record containing imported local and global variables, temporary \nstack space, and space for arguments to native calls. The local and global values are then copied from \nthe interpreter state to the trace activation record. Then, the traceis called likea normalCfunction \npointer.  When a trace call returns, the monitor restores the interpreter state. First, the monitor \nchecks the reason for the trace exit and applies blacklisting if needed. Then, it pops or synthesizes \ninter\u00adpreter JavaScript call stack frames as needed. Finally, it copies the imported variables back from \nthe trace activation record to the in\u00adterpreter state. At least in the current implementation, these \nsteps have a non\u00adnegligible runtime cost, so minimizing the number of interpreter\u00adto-trace and trace-to-interpreter \ntransitions is essential for perfor\u00admance. (see also Section 3.3). Our experiments (see Figure 12) show \nthat for programs we can trace well such transitions hap\u00adpen infrequently and hence do not contribute \nsigni.cantly to total runtime. In a few programs, where the system is prevented from recording branch \ntraces for hot side exits by aborts, this cost can rise to up to 10% of total execution time.  6.2 Trace \nStitching Transitions from a trace to a branch trace at a side exit avoid the costs of calling traces \nfrom the monitor, in a feature called trace stitching. At a side exit, the exiting trace only needs to \nwrite live register-carriedvaluesbacktoits traceactivation record.Inourim\u00adplementation, identical type \nmaps yield identical activation record layouts, so the trace activation record can be reused immediately \nby the branch trace. In programs with branchy trace trees with small traces, trace stitching has a noticeable \ncost. Although writing to memory and then soon reading back would be expected to have a high L1 cache \nhit rate, for small traces the increased instruction count has a noticeable cost. Also, if the writes \nand reads are very close in the dynamic instruction stream, we have found that current x86 processors \noften incur penalties of 6 cycles or more (e.g., if the instructions use different base registers with \nequal values, the processor may not be able to detect that the addresses are the same right away). The \nalternate solution is to recompile an entire trace tree, thus achieving inter-trace register allocation \n(10). The disadvantage is that tree recompilation takes time quadratic in the number of traces. We believe \nthat the cost of recompiling a trace tree every time a branch is added would be prohibitive. That problem \nmight be mitigated by recompiling only at certain points, or only for very hot, stable trees. In the \nfuture, multicore hardware is expected to be common, making background tree recompilation attractive. \nIn a closely re\u00adlated project (13) background recompilation yielded speedups of up to 1.25x on benchmarks \nwith many branch traces.We plan to applythis techniquetoTraceMonkey as futurework. 6.3 Trace Recording \nThe job of the trace recorder is to emit LIR with identical semantics to the currently running interpreter \nbytecode trace.Agood imple\u00admentation should have low impact on non-tracing interpreter per\u00adformance and \na convenient way for implementers to maintain se\u00admantic equivalence. In our implementation, the only \ndirect modi.cation to the inter\u00adpreter is a call to the trace monitor at loop edges. In our benchmark \nresults (see Figure 12) the total time spent in the monitor (for all activities) is usually less than \n5%, so we consider the interpreter impact requirement met. Incrementing the loop hit counter is ex\u00adpensivebecause \nit requires us to look up the loop in the trace cache, but we have tuned our loops to become hot and \ntrace very quickly (on the second iteration). The hit counter implementation could be improved, which \nmight give us a small increase in overall perfor\u00admance, as well as more .exibility with tuning hotness \nthresholds. Once a loop is blacklisted we never call into the trace monitor for that loop (see Section \n3.3). Recording is activated by a pointer swap that sets the inter\u00adpreter s dispatch table to call a \nsingle interrupt routine for ev\u00adery bytecode. The interrupt routine .rst callsa bytecode-speci.c recording \nroutine. Then, it turns off recording if necessary (e.g., the trace ended). Finally, it jumps to the \nstandard interpreter byte\u00adcode implementation. Some bytecodes haveeffects on the type map that cannot \nbe predicted before executing the bytecode (e.g., call\u00ading String.charCodeAt, which returns an integer \nor NaN if the indexargumentisoutof range).For these,we arrangeforthe inter\u00adpreter to call into the recorder \nagain after executing the bytecode. Since such hooks are relatively rare, we embed them directly into \nthe interpreter, with an additional runtime check to see whether a recorder is currently active. While \nseparating the interpreter from the recorder reduces indi\u00advidual code complexity,it also requires careful \nimplementation and extensive testing to achieve semantic equivalence. In some cases achieving this equivalence \nis dif.cult since Spi\u00adderMonkeyfollows a fat-bytecode design, which was found to be bene.cial to pure \ninterpreter performance. In fat-bytecode designs, individual bytecodes can implement complex processing \n(e.g., the getprop bytecode, which imple\u00adments fullJavaScript propertyvalue access, includingspecial \ncases for cached and dense array access). Fat bytecodes have two advantages: fewer bytecodes means lower \ndispatch cost, and bigger bytecode implementations give the compiler more opportunities to optimize the \ninterpreter. Fat bytecodes are a problem for TraceMonkey because they require the recorder to reimplement \nthe same special case logic in the same way. Also, the advantages are reduced because (a) dispatch costs \nare eliminated entirely in compiled traces, (b) the traces contain only one special case, not the interpreter \ns large chunk of code, and (c)TraceMonkeyspends less time running the base interpreter. Oneway wehave \nmitigated these problemsisby implementing certain complex bytecodes in the recorder as sequences of simple \nbytecodes. Expressingthe original semanticsthiswayisnottoodif\u00ad.cult, and recording simple bytecodes is \nmuch easier. This enables usto retaintheadvantagesoffat bytecodes whileavoiding someof their problems \nfor trace recording. This is particularly effective for fat bytecodes that recurse back into the interpreter, \nfor example to convert an object into a primitive value by invoking a well-known method on the object, \nsince it lets us inline this function call. Itis importantto notethatwesplitfat opcodesinto thinnerop\u00adcodes \nonly during recording. When running purely interpretatively (i.e. code that has been blacklisted), the \ninterpreter directly and ef\u00ad.cientlyexecutes thefat opcodes.  6.4 Preemption SpiderMonkey, like manyVMs, \nneeds to preempt the user program periodically. The main reasons are to prevent in.nitely looping scripts \nfrom locking up the host system and to schedule GC. In the interpreter, this had been implemented by \nsetting a pre\u00adempt now .ag that was checked on every backward jump. This strategy carriedover intoTraceMonkey: \ntheVM insertsa guard on the preemption .ag atevery loop edge.We measured less thana 1% increase in runtime \non most benchmarks for this extra guard. In practice,thecostis detectableonlyfor programswithveryshort \nloops. We tested and rejected a solution that avoided the guards by compiling the loop edge as an unconditional \njump, and patching the jump target to an exit routine when preemption is required. This solution can \nmake the normal case slightly faster, but then preemption becomes very slow. The implementation was also \nvery complex, especially tryingto restartexecution afterthe preemption.  6.5 Calling External Functions \nLike most interpreters, SpiderMonkeyhas a foreign function inter\u00adface(FFI)thatallowsittocallCbuiltinsandhostsystem \nfunctions (e.g., web browser control and DOM access). The FFI has a stan\u00addard signaturefor JS-callable \nfunctions,thekeyargumentof which is an array of boxed values. External functions called through the FFI \ninteract with the program state through an interpreter API (e.g., to read a property from an argument). \nThere are also certain inter\u00adpreterbuiltinsthatdonotusetheFFI,but interactwiththe program state in the \nsame way, such as the CallIteratorNext function used with iterator objects.TraceMonkey must support this \nFFI in order to speed up code that interacts with the host system inside hot loops. Callingexternal functions \nfromTraceMonkeyis potentially dif\u00ad.cult because traces do not update the interpreter state until exit\u00ading. \nIn particular, external functions may need the call stack or the globalvariables,but theymaybe outof \ndate. For the out-of-date call stack problem, we refactored some of the interpreter API implementation \nfunctions to re-materialize the interpreter call stack on demand. We developed a C++ static analysis \nand annotated some inter\u00adpreter functions in order to verify that the call stack is refreshed at any \npoint it needs to be used. In order to access the call stack, a function must be annotated as either \nFORCESSTACK or RE-QUIRESSTACK. These annotations are also requiredin orderto call REQUIRESSTACKfunctions, \nwhich are presumed to access the call stack transitively. FORCESSTACK is a trusted annotation, applied \nto only5functions, that means the function refreshes the call stack. REQUIRESSTACK is an untrusted annotation \nthat means the func\u00adtion may only be called if the call stack has already been refreshed. Similarly, \nwe detect when host functions attempt to directly read or write globalvariables, and force the currently \nrunning trace to side exit. This is necessary since we cache and unbox global variables into the activation \nrecord during trace execution. Since both call-stack access and global variable access are rarely performedbyhost \nfunctions, performanceis not signi.cantly affectedby these safety mechanisms. Another problemis thatexternal \nfunctions can reenterthe inter\u00adpreter by calling scripts, which in turn again might want to access the \ncall stack or globalvariables.Toaddress this problem, we made theVMseta.agwheneverthe interpreterisreenteredwhileacom\u00adpiled \ntrace is running. Everycalltoanexternal functionthen checksthis.agandexits the trace immediately after \nreturning from theexternal function call ifitis set. There are many external functionsthat seldomornever \nreenter, and they can be called without problem, and will cause trace exit only if necessary. The FFI \ns boxed value array requirement has a performance cost, so we de.ned a new FFI that allows C functions \nto be an\u00adnotated with their argument types so that the tracer can call them directly, without unnecessary \nargument conversions. Currently, we do not support calling native property get and set override functions \nor DOM functions directly from trace. Support is planned future work.  6.6 Correctness During development, \nwe had access to existing JavaScript test suites, but most of them were not designed with tracing VMs \nin mind and contained few loops. One tool that helped us greatly was Mozilla s JavaScript fuzz tester, \nJSFUNFUZZ, which generates random JavaScript programs by nesting random language elements. We modi.ed \nJSFUNFUZZ to generate loops, and also to test more heavily certain constructs we suspectedwould reveal \n.awsin our implementation.Forexam\u00adple,we suspectedbugsinTraceMonkey shandlingof type-unstable Figure \n11. Fraction of dynamic bytecodes executed by inter\u00adpreter and on native traces. The speedup vs. interpreter \nis shown in parentheses next to each test. The fraction of bytecodes exe\u00adcuted while recording is too \nsmall to see in this .gure, except for crypto-md5, where fully 3% of bytecodes are executed while recording. \nIn most of the tests, almost all the bytecodes are exe\u00adcuted by compiled traces. Three of the benchmarks \nare not traced at all and run in the interpreter. loops and heavily branching code, and a specialized \nfuzz tester in\u00addeed revealed several regressions which we subsequently corrected. 7. Evaluation We evaluated \nour JavaScript tracing implementation using Sun-Spider, the industry standard JavaScript benchmark suite. \nSunSpi\u00adder consists of 26 short-running (less than 250ms, average 26ms) JavaScript programs. This is \nin stark contrast to benchmark suites such as SpecJVM98 (3) used to evaluate desktop and server Java \nVMs. Manyprograms in those benchmarks use large data sets and executefor minutes.The SunSpider programs \ncarryoutavarietyof tasks, primarily 3d rendering, bit-bashing, cryptographic encoding, mathkernels, and \nstring processing. All experiments were performed on a MacBook Pro with 2.2 GHz Core2processor and2GB \nRAM running MacOS 10.5. Benchmark results. The main question is whether programs runfaster with tracing.For \nthis, we ran the standard SunSpider test driver, which starts a JavaScript interpreter, loads and runs \neach program once for warmup, then loads and runs each program 10 times and reports theaverage time takenby \neach.We ran4differ\u00adent con.gurations for comparison: (a) SpiderMonkey, the baseline interpreter, (b)TraceMonkey, \n(d) SquirrelFish Extreme (SFX), the call-threaded JavaScript interpreter used in Apple s WebKit, and \n(e) V8, the method-compiling JavaScript VM from Google. Figure10showsthe relativespeedups achievedbytracing,SFX, \nandV8against the baseline (SpiderMonkey).Tracing achieves the best speedups in integer-heavy benchmarks, \nup to the 25x speedup on bitops-bitwise-and. TraceMonkey is the fastest VM on 9 of the 26 benchmarks \n(3d-morph, bitops-3bit-bits-in-byte, bitops-bitwise\u00adand, crypto-sha1, math-cordic, math-partial-sums, \nmath\u00adspectral-norm, string-base64, string-validate-input). \n\t\t\t", "proc_id": "1542476", "abstract": "<p>Dynamic languages such as JavaScript are more difficult to compile than statically typed ones. Since no concrete type information is available, traditional compilers need to emit generic code that can handle all possible type combinations at runtime. We present an alternative compilation technique for dynamically-typed languages that identifies frequently executed loop traces at run-time and then generates machine code on the fly that is specialized for the actual dynamic types occurring on each path through the loop. Our method provides cheap inter-procedural type specialization, and an elegant and efficient way of incrementally compiling lazily discovered alternative paths through nested loops. We have implemented a dynamic compiler for JavaScript based on our technique and we have measured speedups of 10x and more for certain benchmark programs.</p>", "authors": [{"name": "Andreas Gal", "author_profile_id": "81100568135", "affiliation": "Mozilla Corporation, Mountain View, CA, USA", "person_id": "P1464357", "email_address": "", "orcid_id": ""}, {"name": "Brendan Eich", "author_profile_id": "81100620870", "affiliation": "Mozilla Corporation, Mountain View, CA, USA", "person_id": "P1464365", "email_address": "", "orcid_id": ""}, {"name": "Mike Shaver", "author_profile_id": "81435601503", "affiliation": "Mozilla Corporation, Mountain View, CA, USA", "person_id": "P1464366", "email_address": "", "orcid_id": ""}, {"name": "David Anderson", "author_profile_id": "81435600125", "affiliation": "Mozilla Corporation, Mountain View, CA, USA", "person_id": "P1464367", "email_address": "", "orcid_id": ""}, {"name": "David Mandelin", "author_profile_id": "81100521702", "affiliation": "Mozilla Corporation, Mountain View, CA, USA", "person_id": "P1464368", "email_address": "", "orcid_id": ""}, {"name": "Mohammad R. Haghighat", "author_profile_id": "81100227733", "affiliation": "Intel Corporation, Santa Clara, CA, USA", "person_id": "P1464369", "email_address": "", "orcid_id": ""}, {"name": "Blake Kaplan", "author_profile_id": "81435601320", "affiliation": "Mozilla Corporation, Mountain View, CA, USA", "person_id": "P1464370", "email_address": "", "orcid_id": ""}, {"name": "Graydon Hoare", "author_profile_id": "81435610565", "affiliation": "Mozilla Corporation, Mountain View, CA, USA", "person_id": "P1464371", "email_address": "", "orcid_id": ""}, {"name": "Boris Zbarsky", "author_profile_id": "81435609503", "affiliation": "Mozilla Corporation, Mountain View, CA, USA", "person_id": "P1464372", "email_address": "", "orcid_id": ""}, {"name": "Jason Orendorff", "author_profile_id": "81435608419", "affiliation": "Mozilla Corporation, Mountain View, CA, USA", "person_id": "P1464358", "email_address": "", "orcid_id": ""}, {"name": "Jesse Ruderman", "author_profile_id": "81435609741", "affiliation": "Mozilla Corporation, Mountain View, CA, USA", "person_id": "P1464359", "email_address": "", "orcid_id": ""}, {"name": "Edwin W. Smith", "author_profile_id": "81414607380", "affiliation": "Adobe Corportation, San Jose, CA, USA", "person_id": "P1464360", "email_address": "", "orcid_id": ""}, {"name": "Rick Reitmaier", "author_profile_id": "81435600771", "affiliation": "Adobe Corporation, San Jose, CA, USA", "person_id": "P1464361", "email_address": "", "orcid_id": ""}, {"name": "Michael Bebenita", "author_profile_id": "81338487543", "affiliation": "University of California, Irvine, Irvine, CA, USA", "person_id": "P1464362", "email_address": "", "orcid_id": ""}, {"name": "Mason Chang", "author_profile_id": "81450592973", "affiliation": "University of California, Irvine, Irvine, CA, USA", "person_id": "P1464363", "email_address": "", "orcid_id": ""}, {"name": "Michael Franz", "author_profile_id": "81100089579", "affiliation": "University of California, Irvine, Irvine, CA, USA", "person_id": "P1464364", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1542476.1542528", "year": "2009", "article_id": "1542528", "conference": "PLDI", "title": "Trace-based just-in-time type specialization for dynamic languages", "url": "http://dl.acm.org/citation.cfm?id=1542528"}