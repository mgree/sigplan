{"article_publication_date": "06-15-2009", "fulltext": "\n Programming Model for a Heterogeneous x86 Platform Bratin Saha, Xiaocheng Zhou, Hu Chen, Ying Gao, Shoumeng \nYan, Mohan Rajagopalan, Jesse Fang, Peinan Zhang, Ronny Ronen, Avi Mendelson* Intel Corporation firstname.lastname@intel.com \nABSTRACT The client computing platform is moving towards a heterogeneous architecture consisting of a \ncombination of cores focused on scalar performance, and a set of throughput-oriented cores. The throughput \noriented cores (e.g. a GPU) may be connected over both coherent and non-coherent interconnects, and have \ndifferent ISAs. This paper describes a programming model for such heterogeneous platforms. We discuss \nthe language constructs, runtime implementation, and the memory model for such a programming environment. \nWe implemented this programming environment in a x86 heterogeneous platform simulator. We ported a number \nof workloads to our programming environment, and present the performance of our programming environment \non these workloads. Categories and Subject Descriptors D.3.3 [Programming Languages]: Language Constructs \nand Features concurrent programming structures, patterns, data types and structures. General Terms \nPerformance, Design, Languages. Keywords heterogeneous platforms, programming model 1. Introduction \nClient computing platforms are moving towards a heterogeneous architecture with some processing cores \nfocused on scalar performance, while other cores are focused on throughput performance. For example, \ndesktop and notebook platforms may ship with one or more CPUs (central processing unit), primarily focused \non scalar performance, along with a GPU (graphics processing unit) that can be used for accelerating \nhighly data parallel kernels. These heterogeneous platforms can be used to provide significant performance \nboost on highly parallel non-graphics workloads in image processing, medical imaging, data mining[6] \nand other domains[10]. Several vendors have also come out with programming environments for such platforms \nsuch as CUDA [11], CTM [2] and OpenCL [12]. Heterogeneous platforms have a number of unique architectural \nconstraints such as: . The throughput oriented cores (e.g. a GPU) may be connected in both integrated \nand discrete forms. For example, current Intel graphics processors are integrated with the chipset. On \nthe other hand, other current GPUs are attached in a discrete manner over PCI-Express. A system may also \nhave a hybrid configuration where a low-power lower-performance GPU is integrated with the chipset, and \na higher-performance GPU is attached as a discrete device. Finally, a platform may also have multiple \ndiscrete GPU cards. The platform configuration determines many parameters such as bandwidth and latency \nbetween the different kinds of processors, the cache coherency support, etc. . The scalar and throughput \noriented cores may have different operating systems. For example, Intel s upcoming Larrabee [16] processor \ncan have its own kernel. This means that the virtual memory translation schemes (virtual to physical \naddress translation) can be different between the different kinds of cores. The same virtual address \ncan be simultaneously mapped to two different physical addresses one residing in CPU memory and the \nother residing in Larrabee memory. This also means that the system environment (loaders, linkers, etc.) \ncan be different between the two. For example, the loader may load the application at different base \naddresses on the different cores. . The scalar and throughput oriented cores may have different ISAs \nand hence the same code can not be run on both kinds of cores.  A programming model for heterogeneous \nplatforms must address all of the above architectural constraints. Unfortunately, existing models such \nas CUDA and CTM address only the ISA heterogeneity by providing language annotations to mark out code \nthat must run on GPUs, but do not take other constraints into account. For example, CUDA does not address \nthe memory management issues between CPU and GPU. It assumes that the CPU and GPU are separate address \nspaces with the programmer using separate memory allocation functions for the CPU and the GPU. Further, \nthe programmer must explicitly serialize data structures, decide on the sharing protocol, and transfer \nthe data back and forth. In this paper, we propose a new programming model for heterogeneous x86 platforms \nthat addresses all the above issues. First, we propose a uniform programming model for different platform \nconfigurations. Second, we propose using a shared memory model among all the cores in the platform (e.g. \nbetween the CPU and Larrabee cores). Instead of sharing the entire virtual address space, we propose \nthat only a part of the virtual address space be shared to enable an efficient implementation. Finally, \nlike conventional programming models we use language annotations to demarcate code that must run on the \ndifferent cores, but improve upon conventional models by extending our language support to include features \nsuch as function pointers. We break from existing CPU-GPU programming models and propose a shared memory \nmodel since it opens up a completely new programming paradigm that improves overall platform performance. \nA shared memory model allows pointers and data structures to be seamlessly shared between the different \ncores (e.g. CPU and Larrabee) without requiring any marshalling. For example, consider a game engine \nthat includes physics, AI, and rendering. A shared memory model allows the physics, AI and game logic \nto be run on the scalar cores (e.g. CPU), while the rendering runs on the throughput cores (e.g. Larrabee), \nwith both the scalar and throughput cores sharing the scene graph. Such an execution model *Avi Mendelson \nis currently at Microsoft at avim@microsoft.com Permission to make digital or hard copies of all or part \nof this work for personal or classroom use is granted without fee provided that copies are not made or \ndistributed for profit or commercial advantage and that copies bear this notice and the full citation \non the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires \nprior specific permission and/or a fee. PLDI 09 June 15-20, 2009, Dublin, Ireland Copyright &#38;#169; \n2009 ACM 978-1-60558-392-1/09/06 $5.00. is not possible in current programming environments since the \nscene graph would have to be serialized back and forth. We implemented our full programming environment \nincluding the language and runtime support and ported a number of highly parallel non-graphics workloads \nto this environment. In addition, we have ported a full gaming application to our system. Our implementation \nworks with different operating system kernels running on the scalar and throughput oriented cores. Like \nother environments such as CUDA and OpenCL our programming environment is also based on C. This allows \nus to target the vast majority of existing and emerging throughput oriented applications. But this also \nimplies that we do not support the higher level programming abstractions found in languages such as X10 \n[14]. Nevertheless our environment significantly improves the programmability of heterogeneous platforms \nby eliminating explicit memory management and serialization. For example, using existing models, a commercial \ngaming application (that was ported to our system) required about 1.5 weeks of coding to handle data \nmanagement of each new game feature, with the game itself including about a dozen features. The serialization \narises since the rendering is performed on the Larrabee, while the physics and game logic are executed \non the CPU. All of this coding (and the associated debugging, etc.) goes away in our system since the \nscene graph and associated structures are placed in shared memory and used concurrently by all the cores \nin the platform. We ported our programming environment to a heterogeneous x86 platform simulator that \nsimulates a set of Larrabee-like throughput oriented cores attached as a discrete PCI-Express device \nto the CPU. We used such a platform for 2 reasons. First, we believe Larrabee is more representative \nof how GPUs are going to evolve into throughput oriented cores. Second, it poses greater challenges due \nto the heterogeneity in the system software stack as opposed to simply ISA heterogeneity. Section 6 presents \nperformance results on a variety of workloads. To summarize, this paper discusses the design and implementation \nof a new programming model for heterogeneous x86 (CPU-Larrabee) platforms. We make the following contributions: \n. Provide a shared memory semantics between the CPU and Larrabee allowing pointers and data structures \nto be shared seamlessly. This extends previous work in the areas of DSM and PGAS languages by providing \nshared memory semantics in a platform with heterogeneous ISA, operating system kernels, etc. We also \nimprove application performance by allowing user-level communication between the CPU and Larrabee. . \nProvide a uniform programming model for different platform configurations. . Implement the model in \na heterogeneous x86 platform and show the performance of a number of throughput oriented workloads in \nour programming environment.  In the rest of the paper, we first provide a brief overview of the Larrabee \narchitecture, then discuss the proposed memory model, and describe the language constructs for programming \nthis platform. We then describe our prototype implementation, and finally present performance numbers. \n1.1 Larrabee Architecture Larrabee [16] (also referred to as LRB in this paper) is a many-core x86 visual \ncomputing architecture that is based on in-order cores that run an extended version of the x86 instruction \nset, including wide vector processing instructions and some specialized scalar instructions. Each of \nthe cores contains a 32 KB instruction cache and a 32 KB L1 data cache, and accesses its own subset of \na coherent L2 cache to provide high bandwidth L2 cache access. The L2 cache subset is 256 KB and the \nsubsets are connected by a high bandwidth on-die ring interconnect. Data written by a CPU core is stored \nin its own L2 cache subset and is flushed from other subsets, if necessary. Each ring data path is 512 \nbits wide per direction. The fixed function units and memory controller are spread across the ring to \nreduce congestion. Each core has 4 hyper-threads with separate register sets per thread. Instruction \nissue alternates between the threads and covers cases where the compiler is unable to schedule code without \nstalls. The core uses a dual issue decoder and the pairing rules for the primary and secondary instruction \npipes are deterministic. All instructions can issue on the primary pipe, while the secondary pipe supports \na large subset of the scalar x86 instruction set, including loads, stores, simple ALU operations, vector \nstores, etc. The core supports 64 bit extensions and the full Pentium processor x86 instruction set. \nLarrabee has a 16 wide vector processing unit which executes integer, single precision float, and double \nprecision float instructions. The vector unit supports gather-scatter, masked instructions, and supports \ninstructions with up to 3 source operands. 2. Memory Model The memory model for our system provides a \nwindow of shared addresses between the CPU and LRB, such as in PGAS [17] languages, but enhances it with \nadditional ownership annotations. Any data structure that is shared between the CPU and LRB must be allocated \nby the programmer in this space. The system provides a special malloc function that allocates data in \nthis space. Static variables can be annotated with a type qualifier to have them allocated in the shared \nwindow. However, unlike PGAS languages there is no notion of affinity in the shared window. This is because \ndata in the shared space must migrate between the CPU and LRB caches as it gets used by each processor. \nAlso the representation of pointers does not change between the shared and private spaces. The remaining \nvirtual address space is private to the CPU and LRB. By default data gets allocated in this space, and \nis not visible to the other side. We choose this partitioned address space approach since it cuts down \non the amount of memory that needs to be kept coherent and enables a more efficient implementation for \ndiscrete devices. The proposed memory model can be extended in a straightforward way to multi-LRB configurations. \nThe window of shared virtual addresses extends across all the devices. Any data structures allocated \nin this shared address window is visible to all agents, and pointers in this space can be freely exchanged \nbetween the devices. In addition, every agent has its own private memory.  Figure 1: CPU-LRB memory \nmodel We propose using release consistency in the shared address space due to several reasons. First, \nthe system only needs to remember all the writes between successive release points, not the sequence \nof individual writes. This makes it easier to do bulk transfers at release points (e.g. several pages \nat a time). This is especially important in the discrete configuration since it is more efficient to \ntransfer bulk data (e.g. a 4KB page) over PCI-Express. Second, it allows memory updates to be kept completely \nlocal till a release point, which is again important in a discrete configuration. In general, the release \nconsistency model is a good match for the programming patterns in CPU-GPU platforms since there are natural \nrelease and acquire points in such programs. For example a call from the CPU into the GPU is one such \npoint. Making any of the CPU updates visible to the GPU before the call does not serve any purpose, and \nneither does it serve any purpose to enforce any order on how the CPU updates become visible, as long \nas all of them are visible before the GPU starts executing. Finally, the proposed C/C++ memory model \n[5] can be mapped easily to our shared memory space. In general, race-free programs do not get affected \nby the weaker consistency model of our shared memory space, and we did not want to constrain the implementation \nfor the sake of providing some guarantees on racy programs. We augment our shared memory model with ownership \nrights to enable further coherence optimizations. Within the shared virtual address window, the CPU or \nLRB can specify at a particular point in time that it owns a specific chunk of addresses. If an address \nrange in the shared window is owned by the CPU, then the CPU knows that LRB can not access those addresses \nand hence does not need to maintain coherence of those addresses with LRB for example, it can avoid \nsending any snoops or other coherence information to LRB. The same is true of LRB owned addresses. If \na CPU owned address is accessed by LRB, then the address becomes un-owned (with symmetrical behavior \nfor LRB owned addresses). We provide these ownership rights to leverage common usage models. For example, \nthe CPU first accesses some data (e.g. initializing a data structure), and then hands it over to LRB \n(e.g. computing on the data structure in a data parallel manner), and then the CPU analyzes the results \nof the computation and so on. The ownership rights allow an application to inform the system of this \ntemporal locality and optimize the coherence implementation. Note that these ownership rights are optimization \nhints and it is legal for the system to ignore these hints. 3. Language Constructs To deal with the platform \nheterogeneity we add constructs to C that allow the programmer to specify whether a particular data item \nshould be shared or private, and to specify whether a particular code chunk should be run on the CPU \nor LRB. The first construct is the shared type qualifier, similar to UPC [17], which specifies a variable \nthat is shared between the CPU &#38; LRB. The qualifier can also be associated with pointer types to \nimply that the target of the pointer is in shared space. For example, one can write: shared int var1; \n    // int is in shared space int var2;       // int is not in shared space shared int* ptr1; \n   // ptr1 points to a shared location int* ptr2;       // ptr2 points to private space shared \nint *shared ptr1; // ptr1 points to shared and is shared It is the programmer s responsibility to tag \nall data that is shared between the CPU &#38; LRB with the shared keyword. The compiler allocates global \nshared variables in the shared memory space, while the system provides a special malloc function to allocate \ndata in the shared memory. The actual virtual address range in each space is decided by the system and \nis transparent to the user. Variables with automatic storage (e.g. stack allocated variables) are not \nallowed to be marked with the keyword shared. We use an attribute, __attribute(LRB), to mark functions \nthat should be executed on the LRB. For such functions, the compiler generates LRB-specific code. When \na non-annotated function calls a LRB annotated function, it implies a call from the CPU to LRB. The compiler \nchecks that all pointer arguments have shared type and invokes a runtime API for the remote call. Function \npointer types are also annotated with the attribute notation implying that they point to functions that \nare executed on LRB. Non annotated function pointer types point to functions that execute on the CPU. \nThe compiler checks type equivalence during an assignment e.g., a function pointer with the LRB attribute \nmust always be assigned the address of a LRB annotated function. Our third construct denotes functions \nthat execute on the CPU but can be called from LRB. One could provide a similar functionality by modifying \nthe loader and linker, rather than through a language construct. These functions are denoted using __attribute(wrapper). \nWe used this in 2 ways. First, many programs link with precompiled libraries that can execute on the \nCPU. The functions in these libraries are marked as wrapper calls so that they execute on the CPU if \ncalled from LRB code. Second, while porting large programs from a CPU-only execution mode to a CPU-LRB \nmode, it is very helpful to incrementally port the program. The wrapper attribute allows the programmer \nto stop the porting effort at any point in the call tree by calling back into the CPU. When a LRB function \ncalls a wrapper function, the compiler invokes a runtime API for the remote call from LRB to the CPU. \nMaking LRB to CPU calls explicit allows the compiler to check that any pointer arguments have the shared \ntype. We also provide a construct and the corresponding runtime support for making asynchronous calls \nfrom the CPU to LRB. This allows the CPU to avoid waiting for LRB computation to finish. Instead the \nruntime system returns a handle that the CPU can query for completion. Since this does not introduce \nany new design issues, we focus mostly on synchronous calls in the rest of this paper. Data annotation \nrules 1. Shared can be used to qualify the type of variables with global storage. Shared cannot be used \nto qualify a variable with automatic storage unless it qualifies a pointer s referenced type. 2. A pointer \nin private space can point to any space. A pointer in shared space can only point to shared space but \nnot to private space. 3. Shared cannot be used to qualify a single member of a structure or union unless \nit qualifies a pointer s referenced type. A structure or union type can have the shared qualifier which \nthen requires all fields to have the shared qualifier as well.  The following rules are applied to pointer \nmanipulations: 1. Binary operator (+,-,,==,......) is only allowed between two pointers pointing to the \nsame space. The system provides API functions that perform dynamic checks. When an integer expression \nis added to or subtracted from a pointer, the result has the same type as the pointer.  2. Assignment/casting \nfrom pointer-to-shared to pointer-to-private is allowed. If a type is not annotated we assume that it \ndenotes a private object. This makes it difficult to pass shared objects to legacy functions since their \nsignature requires private objects. The cast allows us to avoid copying between private and shared spaces \nwhen passing shared data to a legacy function. 3. Assignment/casting from pointer-to-private to pointer-to-shared \nis allowed only through a dynamic_cast. The dynamic_cast checks at runtime that the pointer-to-shared \nactually points to shared space. If the check fails, an error is thrown and the user has to explicitly \ncopy the data from private space to shared space. This cast allows legacy code to efficiently return \nvalues.  Our casting rules try to strike a compromise between efficiency and safety. We found that large \napplications heavily use precompiled libraries and the program performed significant copying if we did \nnot provide casts. The disadvantage with the casting rules is that we can no longer leverage some static \nproperties for example eliding synchronization between CPU and LRB while accessing a private object. \nIn future we expect to investigate more sophisticated type systems such as polymorphic type systems. \nOur language can allow casting between the two spaces (with possibly a dynamic check) since our data \nrepresentation remains the same regardless of whether the data is in shared or private space. Even pointers \nhave the same representation regardless of whether they are pointing to private or shared space. Given \nany virtual address V in the shared address window, both CPU and LRB have their own local physical address \ncorresponding to this virtual address. Pointers on CPU and LRB read from this local copy of the address, \nand the local copies get synced up as required by the memory model. Privatization and globalization: \nShared data can be privatized by copying from shared space to the private space. Non-pointer containing \ndata structures can be privatized simply by copying the memory contents. While copying pointer containing \ndata structures, pointers into shared data must be converted to pointers into private data. Private data \ncan be globalized by copying from the private space to the shared space and made visible to other computations. \nNon-pointer containing data structures can be globalized simply by copying the memory contents. While \ncopying pointer containing data structures, pointers into private data must be converted as pointers \ninto shared data (converse of the privatization example). For example, consider a linked list of nodes \nin private and shared space. The type definition for the private linked list is standard: typedef struct \n{ int val; // just an int field Node* next; } Node; The type definition for the shared linked list \nis shown below. Note that the pointer to the next node is defined to reside in shared space. The user \nmust explicitly declare both the private and shared versions of a type. typedef struct { shared int \nval;  shared Node *shared next;  } shared Node; Now the user can explicitly copy a private linked list \nto shared space by using the following:   myNode = (shared Node*) sharedMalloc(..); // head points \nto the private linked list myNode->val = head->val myNode->next = (shared Node*) sharedMalloc(..); \n       Code annotation rules 1. A __attribute(LRB) function is not allowed to call a non-annotated \nfunction. This is to ensure that the compiler knows about all the CPU functions that can be called from \nthe LRB. 2. A __attribute(wrapper) function is not allowed to call into a __attribute(LRB) function. \nThis is primarily an implementation restriction in our system. 3. Any pointer parameter of a LRB or \nwrapper annotated function must point to shared space.  The calling rules for functions also apply \nto function pointers for example a __attribute(LRB) function pointer called from a non-annotated function \nresults in a CPU-LRB call. Similarly, un-annotated function pointers can not be called from LRB functions. \nThe runtime API used by the compiler is shown below: // Allocate and free memory in the private address \nspace. Maps to regular malloc void* privateMalloc(int); void privateFree(void*); //Allocation &#38; free \nfrom the shared space. shared void* sharedMalloc(size_t size); void sharedFree(shared void *ptr); // \nMemory consistency for shared memory void sharedAcquire(); void sharedRelease(); The runtime also provides \nAPIs for mutexes and barriers to allow the application to perform explicit synchronization. These constructs \nare always allocated in the shared area. The language provides natural acquire and release points. For \nexample, a call from the CPU to LRB is a release point on the CPU followed by an acquire point on the \nLRB. Similarly, a return from the LRB is a release point on the LRB and an acquire point on the CPU. \nTaking ownership of a mutex and releasing a mutex are acquire and release points respectively for the \nprocessor doing the mutex operation, while hitting a barrier and getting past a barrier are release and \nacquire points as well. 3.1 Ownership constructs We also provide an API that allows the user to allocate \nchunks of memory (hereafter called arenas) inside the shared virtual region. The programmer can dynamically \nset the ownership attributes of an arena. Acquiring ownership of an arena also acts as an acquire operation \non the arena, and releasing ownership of an arena acts as the release operation on the arena. The programmer \ncan allocate space within an arena by passing the arena as an argument to a special malloc function. \nThe runtime grows the arena as needed. The runtime API is shown below: // Owned by the caller or shared \nArena *allocateArena(OwnershipType type); //Allocate and free within arena shared void *arenaMalloc ( \nArena*, size_t); void arenaFree( Arena *, shared void *); // Ownership for arena. If null changes ownership \nof entire shared area OwnershipType acquireOwnership(Arena*); OwnershipType releaseOwnership(Arena*); \n//Consistency for arena void arenaAcquire(Arena *); void arenaRelease(Arena *); The programmer can optimize \nthe coherence implementation by using the ownership API. For example, in a gaming application, while \nthe CPU is generating a frame, LRB may be rendering the previous frame. To leverage this pattern the \nprogrammer can allocate two arenas with the CPU acquiring ownership of one arena and generating the frame \ninto that arena, while the LRB acquires ownership of the other arena and renders the frame in that arena. \nThis prevents coherence messages being exchanged between the CPU and LRB while the frames are being processed. \nWhen the CPU and LRB are finished with their current frames, they exchange ownership of their arenas \nso that they continue to work without incurring coherence overhead. 4. Code example This section illustrates \nthe proposed programming model through a code example that shows a simple, vector addition (addTwoVectors) \nbeing accelerated on LRB. int addTwoVectors(int* a, int* b, int* c) { for (i = 1 to 64) {  c[i] = \na[i] + b[i] } } int someApp( ) { int *a = malloc (..); int *b = malloc (..); int *c = malloc (..); \n for (i = 1 to 64) //initialize {a[i] = ; b[i] = ; c[i] = ;}  addTwoVectors(a, b, c);  } In our programming \nmodel, this would be written as: __attribute(LRB) int addTwoVectors(shared int* a, shared int* b, shared \nint* c) {  for (i = 1 to 64) {  c[i] = a[i] + b[i];  } } int someApp(..) { // allocate in shared region \n shared int* a = sharedMalloc (..);   shared int* b = sharedMalloc (..);  shared int* c = sharedMalloc \n(..);  for (i = 1 to 64) // initialize {a[i] = ; b[i] = ; c[i] = ;}  // compiler converts into remote \ncall addTwoVectors(a, b, c);   } In the above implementation, arrays a, b, c are allocated in shared \nspace by calling the special malloc function. The remote call (addTwoVectors) acts as the release/acquire \npoint and causes the memory region to be synced up between CPU &#38; LRB. Finally, the programmer can \nadd in the appropriate ownership calls to make the implementation more efficient. In this particular \nexample we hand over ownership of the entire shared address space. If the CPU makes an asynchronous call, \nthen the application would hand over ownership of specific arenas so that the CPU can perform some computation \nconcurrently with LRB. __attribute(LRB) int addTwoVectors(shared int* a, shared int* b, shared int* c) \n{ acquireOwnership(NULL);  releaseOwnership (NULL); } int someApp(..) {   releaseOwnership(NULL); \n addTwoVectors(a, b, c);   acquireOwnership(NULL);  } 5. Implementation The compiler generates 2 \nbinaries one for execution on LRB and another for CPU execution. We generate two different executables \nsince the two operating systems can have different executable formats. The LRB binary contains the code \nthat will execute on LRB (annotated with the LRB attribute), while the CPU binary contains the CPU functions \nwhich include all un-annotated and wrapper annotated functions. Our runtime library has a CPU and LRB \ncomponent which are linked with the CPU and LRB application binaries to create the CPU and LRB executables. \nWhen the CPU binary starts executing, it calls a runtime function that loads the LRB executable. Both \nthe CPU and LRB binaries create a daemon thread that is used for CPU-LRB communication. 5.1 Implementing \nCPU-LRB shared memory Our implementation reuses some ideas from software distributed shared memory [9][4] \nschemes, but there are some significant differences as well. Unlike DSMs, our implementation is complicated \nby the fact that the CPU and LRB can have different page tables and different virtual to physical memory \ntranslations. Thus, when we want to sync up the contents of virtual address V between the CPU and LRB \n(e.g. at a release point), we may need to sync up the contents of different physical addresses, (say) \nP1 on CPU and P2 on LRB. Unfortunately, the CPU does not have access to LRB s page tables (hence does \nnot know P2) and LRB can not access the CPU s page tables and does not know P1. We solve this problem \nby leveraging the PCI aperture in a novel way. During initialization we map a portion of the PCI aperture \nspace into the user space of the application and instantiate it with a task queue, a message queue, and \ncopy buffers. When we need to copy pages, say from the CPU to LRB, the runtime copies the pages into \nthe PCI aperture copy buffers and tags the buffers with the virtual address and the process identifier. \nOn the LRB side, the daemon thread copies the contents of the buffers into its address space by using \nthe virtual address tag. Thus we perform the copy in a 2 step process the CPU copies from its address \nspace into a common buffer (PCI aperture) that both CPU and LRB can access, while the LRB picks up the \npages from the common buffer into its address space. LRB-CPU copies are done in a similar way. Note that \nsince the aperture is pinned memory, the contents of the aperture are not lost if the CPU or LRB process \ngets context switched out. This allows the two processors to execute asynchronously which is critical \nsince the two processors can have different operating systems and hence the context switches can not \nbe synchronized. Finally, note that we map the aperture space into the user space of the application \nthus enabling user level CPU-LRB communication. This makes the application stack vastly more efficient \nthan going through the OS driver stack. To ensure security, the aperture space is partitioned among the \nCPU processes that want to use LRB. At present, a maximum of 8 processes can use the aperture space. \nWe exploit one other difference between traditional software DSMs and CPU-LRB platforms. Traditional \nDSMs were designed to scale on medium to large clusters. In contrast, CPU-LRB systems are very small \nscale clusters. We do not expect more than a handful of LRB cards and CPU sockets to be used well into \nthe future. Moreover, the PCI aperture provides a convenient shared physical memory space between the \ndifferent processors. Thus we are able to centralize many data structures and make the implementation \nmore efficient. For example, we put a directory in the PCI aperture that contains metadata about the \npages in the shared address region. The metadata says whether the CPU or LRB holds the golden copy of \na page (home for the page), contains a version number that tracks the number of updates to the page, \nmutexes that are acquired before updating the page, and miscellaneous metadata. The directory is indexed \nby the virtual address of a page. Both the CPU and the LRB runtime systems maintain a private structure \nthat contains local access permissions for the pages, and the local version numbers of the pages. When \nLRB performs an acquire operation, the corresponding pages are set to no-access on LRB. At a subsequent \nread operation the page fault handler on the LRB copies the page from the home location if the page has \nbeen updated and released since the last LRB acquire. The directory and private version numbers are used \nto determine this. The page is then set to read-only. At a subsequent write operation the page fault \nhandler creates the backup copy of the page, marks the page as read-write and increments the local version \nnumber of the page. At a release point, we perform a diff with the backup copy of the page and transmit \nthe changes to the home location, while incrementing the directory version number. The CPU operations \nare done in a symmetrical way. Thus, between acquire and release points the LRB and CPU operate out of \ntheir local memory and communicate with each other only at the explicit synchronization points. At startup \nthe implementation decides the address range that will be shared between CPU and LRB, and makes sure \nthat this address range always remains mapped. This address range can grow dynamically, and does not \nhave to be contiguous, though in a 64 bit address space the runtime system can reserve a continuous chunk \nupfront. 5.2 Implementing shared memory ownership Every arena has associated metadata that identifies \nthe pages that belong to the arena. Suppose LRB acquires ownership of an arena. We make the corresponding \npages non-accessible on the CPU. We copy any arena pages from the home location that have been updated \nand released since the last time the LRB performed an acquire operation. We set the pages to read-only \nso that subsequent LRB writes will trigger a fault, and the system can record which LRB pages are being \nupdated. In the directory we note that LRB is the home node for the arena pages. On a release operation, \nwe simply make the pages accessible again on the other side, and update the directory version number \nof the pages. The CPU ownership operations are symmetrical. Note the performance advantages of acquiring \nownership. At a release point we no longer need to perform diff operations, and do not need to create \na backup copy at a write fault, since we know that the other side is not updating the page. Second, since \nthe user provides specific arenas to be handed over from one side to the other, the implementation can \nperform a bulk copy of the required pages at an acquire point. This leads to a more efficient copy operation \nsince the setup cost is incurred only once and gets amortized over a larger copy. 5.3 Implementing remote \ncalls A CPU-LRB or LRB-CPU remote call is complicated by the fact that the two processors can have different \nsystem environments, for example different loaders. The two binaries are also loaded separately and asynchronously. \nSuppose that the CPU code makes some calls into the LRB. When the CPU binary is loaded, the LRB binary \nhas still not been loaded and hence the addresses for LRB functions are still not known. Therefore, the \nOS loader can not patch up the references to LRB functions in the CPU binary. Similarly, when the LRB \nbinary is being loaded, the LRB loader does not know the addresses of any CPU functions being called \nfrom LRB code and hence can not patch those addresses. We implement remote calls by using a combination \nof compiler and runtime techniques. Our language rules ensure that any function involved in remote calls \n(LRB or wrapper attribute functions) is annotated by the user. When compiling such functions, the compiler \nadds a call to a runtime API that registers function addresses dynamically. The compiler creates an initialization \nfunction for each file that invokes all the different registration calls. When the binary gets loaded, \nthe initialization function in each file gets called. The shared address space contains a jump table \nthat is populated dynamically by the registration function. The table contains one slot for every annotated \nfunction. The format of every slot is <funcName, funcAddr> where funcName is a literal string of the \nfunction name and funcAddr is the runtime address of the function. The translation scheme works as follows. \n1. If a LRB (CPU) function is being called within a LRB (CPU) function, the generated code will do the \ncall as is. 2. If a LRB function is being called within a CPU function, the compiler generated code \nwill do a remote call to LRB: 2.1. The compiler generated code will look up the jump table with the \nfunction name and obtain the function address. 2.2. The generated code will pack the arguments into \nan argument buffer in shared space. It will then call a dispatch routine on the LRB side passing in the \nfunction address and the argument buffer address.  There is a similar process for a wrapper function \nexcept that it is a remote call to CPU if a wrapper function is called in a LRB code. For function pointer \ninvocations, the translation scheme works as follows. When a function pointer with LRB annotation is \nassigned, the compiler generated code will look up the jump table with the function name and assign the \nfunction pointer with obtained function address. Although the lookup can be optimized out when LRB annotated \nfunction pointer is assigned within LRB code, we forsake the optimization to use a single strategy for \nall function pointer assignments. If a LRB function pointer is being called within a LRB function, the \ncompiler generated code will do the call as is. If a LRB function pointer is being called within a CPU \nfunction, the compiler generated code will do a remote call to LRB side. The process is similar for a \nwrapper function pointer except that there is a remote call to CPU side if wrapper function pointer is \ncalled in a LRB function. The CPU-LRB signaling happens with task queues in the PCI aperture space. \nThe daemon threads on both sides poll their respective task queues and when they find an entry in the \ntask queue, they spawn a new thread to invoke the corresponding function. The API for remote invocations \nis described below. // Synchronous and asynchronous remote calls RPCHandler callRemote(FunctionType, \nRPCArgType); int resultReady(RPCHandler); Type getResult(RPCHandler); Finally, the CPU and LRB co-operate \nwhile allocating memory in the shared area. Each processor allocates memory from either side of the shared \naddress window. When one processor consumes half of the space, the two processors repartition the available \nspace. 6. Experimental Evaluation We used a heterogeneous platform simulator for measuring the performance \nof different workloads on our programming environment. It simulates a modern out of order CPU and a LRB \nlike system. The CPU simulation uses a memory and architecture configuration similar to that of the Intel \nCore2Duo processor. The LRB-like system was simulated as a discrete PCI-Express device with the interconnect \nlatency and bandwidth similar to PCI-Express 2.0. The LRB-like system only simulated the Pentium instruction \nset. It did not simulate the new Larrabee instructions and parts of the Larrabee memory hierarchy such \nas the interconnect. For brevity, in the rest of this section, we use LRB to denote the LRB-like system. \nThe simulator ran a production quality SW stack on the two processors. The CPU ran Windows Vista, while \nLRB ran a lightweight operating system kernel. We used a number of well known parallel non-graphics workloads[6] \nto measure the performance of our system. These include the Black Scholes financial workload that does \noption pricing using the Black Scholes method; the FFT (fast fourier transform) workload that does a \nradix-2 FFT algorithm used in many domains such as signal processing; the Equake workload that is part \nof SpecOMP and performs an earthquake modeling and is representative of HPC applications; Art which is \nalso part of SpecOMP and performs image recognition. The reported numbers are based on using the standard \ninput sets for each of the applications. All these workloads were rewritten using our programming constructs \nand compiled with our tool chain.  Figure 2: Percent of shared data in memory accesses Figure 2 shows \nthe fraction of total memory accesses that were to shared data in the above workloads. A vast majority \nof the accesses were to private data. Note that read-only data accessed by multiple threads was privatized \nmanually. This helped in certain benchmarks like Black Scholes. It is not surprising that most of the \naccesses are to private data since the computation threads in the workloads privatize the data that they \noperate on to get better memory locality. We expect workloads that scale to a large number of cores to \nbehave similarly since the programmer must be conscious of data locality and avoid false sharing to get \ngood performance. The partial virtual address sharing memory model lets us leverage this by cutting down \non the amount of data that needs to be kept coherent. We next show the performance of our system on the \nset of workloads. We ran the workloads on a simulated system with 1 CPU core and varied the number of \nLRB cores from 6 to 24. The workload computation was split between the CPU and LRB cores with the compute \nintensive portions executed on LRB. For example, all the option pricing in Black Scholes, and the earthquake \nsimulation in Equake is offloaded to LRB. We present the performance improvement relative to a single \nCPU and LRB core. Figure 3 compares the performance of our system when the application does not use any \nownership calls to the case when the user optimizes using ownership calls. The bars labeled Mine/Yours \nrepresent the performance with ownership calls (Mine implies pages were owned by CPU and Yours implies \nowned by LRB). The bars labeled Ours represent the performance without any ownership calls. FFT Speedup00.511.522.5612241CPU+xx \nLRBsSpeedupMine/YoursOurs ART Speedup00.511.522.533.5612241CPU+xx LRBsSpeedupMine/YoursOurs BlackScholes \nSpeedup01234567612241CPU+xx LRBsSpeedupMine/YousOurs Equake Speedup012345612241CPU+xx LRBsSpeedupMine/YoursOurs \n Figure 3: Ownership performance comparison As expected, the applications perform better with ownership \ncalls than without. To understand the reason for this, we broke down the overhead of the system when \nthe application was not using any ownership calls. Figure 4 shows the breakdown for Black Scholes. We \nshow the breakdown for only one benchmark, but the ratios of the different overheads are very similar \nin all the benchmarks. We break up the overhead into 4 categories. One relates to handling the page faults \nsince we use a virtual memory based shared memory implementation and reads/writes to a page after an \nacquire point trigger a fault. The second relates to the diff operation performed at release points to \nsync up the CPU and LRB copies of a page. The third is the amount of time spent in copying data from \none side to the other. The copy operation is triggered from the page fault handler when either processor \nneeds the latest copy of a page. We do not include the copy overhead as part of the page fault overhead, \nbut present it separately since we believe different optimizations can be applied to optimize it. Finally, \nwe show the overhead spent in synchronizing messages. Note that in a discrete setting LRB is connected \nto the CPU over PCI-Express. The PCI-Express protocol does not include atomic read-modify-write operations. \nTherefore we have to perform some synchronization and hand shaking between the CPU and LRB by passing \nmessages. When the application uses ownership of arenas, the diff overhead is completely eliminated. \nThe page fault handling is reduced since the write page fault handler does not have to create a backup \ncopy of the page. Moreover, since we copy all the pages at one shot when we acquire ownership of an arena, \nwe don t incur read page faults anymore. This also significantly reduces the synchronization message \noverhead since the CPU and LRB perform the handshaking at only ownership acquisition points rather than \nat many intermediate points (e.g. whenever pages are transferred from one side to the other). Figure \n5 shows the overhead breakdown with ownership calls. BlackScholes PageProtectDiffWaitForCopySendRequest \n Figure 4: Overhead breakdown without ownership  Figure 5: Overhead breakdown with ownership FFT Speedup00.511.522.53612241 \nCPU+xx LRBsSpeedupidealdiscrete ART Speedup00.511.522.533.54612241 CPU+xx LRBsSpeedupidealdiscrete BlackScholes \nSpeedup012345678612241 CPU+xx LRBsSpeedupidealdiscrete Equake Speedup00.511.522.533.544.55612241 CPU+xx \nLRBsSpeedupidealdiscrete Figure 6: Overall performance comparison Finally, Figure 6 shows the overall \nperformance of our system. All the workloads used the ownership APIs. The ideal bar represents HW supported \ncache coherence between the CPU and LRB cores in other words this is the best performance that our shared \nmemory implementation can provide. For Equake, since the amount of data transferred is very small compared \nto the computation involved, we notice that ideal and discrete times are almost identical. In all cases \nour shared memory implementation has low overhead and performs almost as well as the ideal case. Black \nScholes shows the highest comparable overhead since it has the lowest compute density ie, the amount \nof data transferred per unit computation time was the highest. In Black Scholes we transfer about 13MB \nof data per second of computation time, while we transfer about 0.42MB of data per second of computation \ntime in Equake. Hence the memory coherence overhead is negligible in Equake. The difference between the \nideal scenario and our shared memory implementation increases with the number of cores mainly due to \nsynchronization overhead. In our implementation, synchronization penalty increases non-linearly with \nthe number of cores, which would not be the case in HW based coherence. 7. Related work The most closely \nrelated work to this paper are the CUDA[11], OpenCL [12] and CTM [2] programming environments. Like us, \nOpenCL uses a weakly consistent shared memory model but this is restricted to the GPU. Our work differs \nfrom CUDA, OpenCL and CTM in several ways unlike these environments we define a model for CPU-LRB communication; \nwe provide direct user level CPU-LRB communication, and we consider a bigger set of C language features \nsuch as function pointers. Implementing a similar memory model is challenging on current GPUs. (This \nis due to the limitations of current GPUs and also because the architecture and low-level details about \nthese platforms are not documented.) For example, the programmer can not use dynamic pointer containing \ndata structures and would be restricted to simple serialized data structures such as arrays and vectors. \nThe release and acquire points would perform a memory copy of the data with the programmer explicitly \nspecifying the data to be copied back and forth. The Cell processor [8] is another heterogeneous platform. \nWhile the PPU is akin to a CPU, the SPUs are much simpler than LRB. For example, they do not run an operating \nsystem kernel. Unlike the SPU-PPU pair, the LRB-CPU pair is much more loosely coupled since LRB can be \npaired as a discrete GPU with any CPU running any operating system. Unlike our model, Cell programming \ninvolves explicit DMA between the PPU and SPU. Our memory model is similar to that of PGAS languages[14][17], \nand hence our language constructs have similarity to UPC [17], however UPC does not consider ISA or operating \nsystem heterogeneity. Higher level PGAS languages such as X10 [14] do not support the ownership mechanism \nwhich is crucial for a scalable coherence implementation in a discrete scenario. Our implementation has \nsimilarities to software distributed shared memory [9][4] which also leverage virtual memory. Many of \nthese S-DSM systems also use release consistency and copy pages lazily on demand. The main differences \nwith S-DSM systems is the level of heterogeneity. Unlike S-DSM systems we need to consider a computing \nsystem where the processors have different ISAs and system environments. In particular, we need to support \ndifferent processors with different virtual to physical page mappings. Finally, the performance tradeoffs \nare different since S-DSMs were meant to scale on large clusters, while CPU-LRB systems should remain \nsmall scale clusters for some time in the future. The CUBA [7] architecture proposes hardware support \nfor faster communication between the CPU and GPU. But the programming model assumes that the CPU and \nGPU are separate address spaces. The EXO [18] model provides shared memory between a CPU and accelerators, \nbut it requires the page tables to be kept in sync which is infeasible in a discrete accelerator. 8. \nConclusions Heterogeneous computing platforms composed of a general purpose scalar oriented CPU and throughput \noriented cores (e.g. a GPU) are increasingly being used in client computing systems. These platforms \ncan be used for accelerating highly parallel workloads. There have been several programming model proposals \nfor such platforms, but none of them address the CPU-GPU memory model. In this paper we propose a new \nprogramming model for a heterogeneous x86 system with the following key features: we propose a shared \nmemory model for all the cores in the platform; we propose a uniform programming model for different \nconfigurations; and we propose user annotations to demarcate code for CPU and LRB execution. We implemented \nthe full software stack for our programming model including compiler and runtime support. We ported a \nnumber of parallel workloads to our programming model and evaluated the performance on a heterogeneous \nx86 platform simulator. We show that our model can be implemented efficiently so that programmers are \nable to benefit from shared memory programming without paying a performance penalty. Acknowledgements: \nWe thank Jeremy Siek and the anonymous reviewers for significantly improving the paper with their comments. \n9. References [1] Adve S, Adve V, Hill M.D. and Vernon M.K. Comparison of Hardware and Software Cache \nCoherence Schemes. ISCA 1991. [2] AMD CTM http://ati.amd.com/companyinfo/researcher/documents/ ATI_CTM_Guide.pdf \n [3] AMD Stream SDK, ati.amd.com/technology/streamcomputing. [4] Amza C., Cox A.L., Dwarkadas S., Keleher \nP., Lu H., Rajamony R., Yu W., Zwaenepoel W. TreadMarks: Shared Memory Computing on Networks of Workstations. \nIEEE Computer, Feb 1996. [5] Boehm H., Adve S. Foundations of the C++ memory model. Programming Language \nDesign and Implementation (PLDI). 2008. [6] Dubey P. Recognition, Mining, and Synthesis moves computers \nto the era of tera. Technology@Intel, Feb 2005. [7] Gelado I., Kelm J.H., Ryoo S., Navarro N., Lumetta \nS.S., Hwu W.W. CUBA: An Architecture for Efficient CPU/Co-processor Data Communication. ICS, June 2008. \n [8] Gschwind M., Hofstee H.P., Flachs B., Hopkins M., Watanabe Y., Yamakazi T. Synergistic Processing \nin Cell s Multicore Architecture. IEEE Micro, April 2006. [9] Kontothanasis L., Stets R., Hunt G., Rencuzogullari \nU., Altekar G., Dwarkadas S., Scott M.L. Shared Memory Computing on Clusters with Symmetric Multiprocessors \nand System Area Networks. ACM Transactions on Computer Systems, Aug 2005. [10] Luebke, D., Harris, M., \nKr\u00fcger, J., Purcell, T., Govindaraju, N., Buck, I., Woolley, C., and Lefohn, A. 2004. GPGPU: general \npurpose computation on graphics hardware. SIGGRAPH 2004. [11] Nvidia Corp, CUDA Programming Environment, \nwww.nvidia.com/object/cuda_what_is.html. [12] OpenCL 1.0, http://www.khronos.org/opencl/. [13] Ryoo \nS., Rodrigues C.I., Baghsorki S.S., Stone S.S., Kirk D.B., Hwu W.W. Optimization Principles and Application \nPerformance Evaluation of a Multithreaded LRB using CUDA. PPoPP 2008. [14] Saraswat, V. A., Sarkar, \nV., and von Praun, C. 2007. X10: concurrent programming for modern architectures. PPoPP 2007. [15] Saha, \nB., Adl-Tabatabai, A., Ghuloum, A., Rajagopalan, M., Hudson, R. L., Petersen, L., Menon, V., Murphy, \nB., Shpeisman, T., Sprangle, E., Rohillah, A., Carmean, D., and Fang, J. 2007. Enabling scalability and \nperformance in a large scale CMP environment. Eurosys 2007. [16] Seiler L., Carmean D., Sprangle E., \nForsyth T., Abrash M., Dubey P., Junkins S., Lake A., Sugerman J., Cavin R., Espasa R., Grochowski E., \nJuan T., Hanrahan P. Larrabee: A Many-Core x86 Architecture for Visual Computing. ACM Transactions on \nGraphics, August 2008. [17] UPC Consortium, UPC language specifications. Lawrence Berkeley National \nLab Tech Report LBNL-59208, 2005. [18] Wang P., Collins J.D., Chinya G. N., Jiang H., Tian X., Girkar \nM., Yang N. Y., Lueh G., Wang H. Exochi: Architecture and programming environment for a heterogeneous \nmulti-core multithreaded system. PLDI 2007.  \n\t\t\t", "proc_id": "1542476", "abstract": "<p>The client computing platform is moving towards a heterogeneous architecture consisting of a combination of cores focused on scalar performance, and a set of throughput-oriented cores. The throughput oriented cores (e.g. a GPU) may be connected over both coherent and non-coherent interconnects, and have different ISAs. This paper describes a programming model for such heterogeneous platforms. We discuss the language constructs, runtime implementation, and the memory model for such a programming environment. We implemented this programming environment in a x86 heterogeneous platform simulator. We ported a number of workloads to our programming environment, and present the performance of our programming environment on these workloads.</p>", "authors": [{"name": "Bratin Saha", "author_profile_id": "81100311903", "affiliation": "Intel Corporation, Santa Clara, USA", "person_id": "P1464341", "email_address": "", "orcid_id": ""}, {"name": "Xiaocheng Zhou", "author_profile_id": "81435596225", "affiliation": "Intel Corporation, Beijing, China", "person_id": "P1464343", "email_address": "", "orcid_id": ""}, {"name": "Hu Chen", "author_profile_id": "81435597979", "affiliation": "Intel Corporation, Beijing, China", "person_id": "P1464344", "email_address": "", "orcid_id": ""}, {"name": "Ying Gao", "author_profile_id": "81435595806", "affiliation": "Intel Corporation, Beijing, China", "person_id": "P1464345", "email_address": "", "orcid_id": ""}, {"name": "Shoumeng Yan", "author_profile_id": "81435603934", "affiliation": "Intel Corporation, Beijing, China", "person_id": "P1464346", "email_address": "", "orcid_id": ""}, {"name": "Mohan Rajagopalan", "author_profile_id": "81100428238", "affiliation": "Intel Corporation, Santa Clara, USA", "person_id": "P1464347", "email_address": "", "orcid_id": ""}, {"name": "Jesse Fang", "author_profile_id": "81100350392", "affiliation": "Intel Corporation, Santa Clara, USA", "person_id": "P1464348", "email_address": "", "orcid_id": ""}, {"name": "Peinan Zhang", "author_profile_id": "81435605834", "affiliation": "Intel Corporation, Santa Clara, USA", "person_id": "P1464349", "email_address": "", "orcid_id": ""}, {"name": "Ronny Ronen", "author_profile_id": "81332524016", "affiliation": "Intel Corporation, Haifa, Israel", "person_id": "P1464350", "email_address": "", "orcid_id": ""}, {"name": "Avi Mendelson", "author_profile_id": "81100039008", "affiliation": "Microsoft Corporation, Haifa, Israel", "person_id": "P1464342", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1542476.1542525", "year": "2009", "article_id": "1542525", "conference": "PLDI", "title": "Programming model for a heterogeneous x86 platform", "url": "http://dl.acm.org/citation.cfm?id=1542525"}