{"article_publication_date": "06-15-2009", "fulltext": "\n Progress GuaranteeforParallelPrograms viaBounded Lock-Freedom Erez Petrank* MadanlalMusuvathi Bjarne \nSteensgaard Computer Science Department Microsoft Research Microsoft Research Technion One Microsoft \nWay One Microsoft Way Haifa 32000 Redmond, WA98052 Redmond, WA98052 Israel USA USA erez@cs.technion.ac.il \nmadanm@microsoft.com Bjarne.Steensgaard@microsoft.com Abstract Parallel platforms are becoming ubiquitous \nwith modern comput\u00ading systems. Many parallel applications attempt to avoid locks in order to achieve \nhigh responsiveness, aid scalability, and avoid deadlocks andlivelocks.However, avoidingthe use ofsystemlocks \ndoes notguaranteethatnolocks are actually used,becauseprogress inhibitors mayoccurin subtle waysthrough \nvariousprogram struc\u00adtures. Notions of progress guarantee such as lock-freedom, wait\u00adfreedom, and obstruction-freedom \nhavebeenproposed in the liter\u00adature toprovide variouslevels ofprogressguarantees. Inthispaper weformalizethe \nnotions ofprogressguarantees us\u00adinglineartemporallogic(LTL).We concentrate onlock-freedom and propose \na variant of it denoted bounded lock-freedom, which is more suitableforguaranteeingprogressinpracticalsystems.We \nuse thisformalde.nition tobuild a tool that checks if a concurrent programisboundedlock-freefor agivenbound.Wethen \nstudythe interactionbetweenprograms withprogressguarantees andthe un\u00adderlying system(e.g.,compilers, \nruntimes,operating systems,and hardwareplatforms).Wepropose a means to arguethatan underly\u00ading system \nsupports lock-freedom. A composition theorem asserts that bounded lock-free algorithms running on bounded \nlock-free supporting systems retainboundedlock-freedomfor the composed execution. Categories and Subject \nDescriptors D.1.3[Software]: Program\u00admingTechniques Parallelprogramming; D.4.1[Software]: Op\u00aderatingSystems \nSynchronization;Concurrency General Terms Performance,Reliability,Theory,Veri.cation Keywords Boundedlock-freedom,Lock-freedom,ProgressGuar\u00adantees,ParallelComputation,ModelChecking. \n* Supported by THE ISRAEL SCIENCE FOUNDATION (grant No. 845/06). Part of this work was done while the \nauthor was on a sabbatical leave atMicrosoftResearch. Permission to make digital or hard copies of all \nor part of this work for personal or classroomuseisgranted withoutfeeprovided that copiesarenot madeordistributed \nforpro.tor commercial advantage andthat copiesbearthis notice andthefull citation onthe .rstpage.Tocopy \notherwise,torepublish,topostonserversortoredistribute tolists, requiresprior speci.cpermission and/or \nafee. PLDI 09, June15 20,2009,Dublin,Ireland. Copyright c &#38;#169; 2009ACM978-1-60558-392-1/09/06. \n. .$5.00 1. Introduction Much effort is devoted these days into creating parallel programs that execute \nef.ciently on parallel platforms. Often, attempt is made to avoid using locks, since locks have been \nknown to in\u00adtroduce deadlocks (or livelocks), reduce the responsiveness, and harm the scalabilityof the \napplication.Traditional synchronization via critical sections guarded by locks are poorly suited for \nasyn\u00adchronous systems.If onethreadis slowdueto apage-fault, a cache\u00admiss, aCPUpreemption or even completefailure, \nall other threads maybe consequentlydelayed.However,the attempttojust avoid employing system locks does \nnot provide the desired application behavior, since .ner forms of synchronization (such as compare\u00adand-swap) \nmay block program execution in various subtle ways, even though no speci.c systemlockisheld. Severalbasic \nconcepts inparallel computation wereintroduced to capturethe essence ofprogressguarantee.Amongthem are \nlock\u00adfreedom (alsoknown as non-blocking),wait-freedom, and obstruc\u00adtionfreedom.The strongest ofthese \nnotionsis wait-freedom, which requires each thread to makeprogress whenever itis scheduled for a suf.cient \nnumber of steps, independently of other concurrently executing threads. Such behavior is indeed desirable, \nbut this re\u00adquirement is very strong and often resultsin a complicated andin\u00adef.cient programs. Lock-freedom \nrequires that when the program threads are run suf.ciently long, at least one of the threads make progress. \nThis requirement ensures that the program as a whole makes progress and is never blocked. Lock-free algorithms \nare known for various tasks and are often ef.cient. Obstruction free\u00addomis the weakest requirement, stating \nthatprogressis onlyguar\u00adanteed if we let one of the program threads run in isolation suf.\u00adcientlylong.Typically, \nthreadsdo not runinisolation on aparallel system. Nevertheless, obstruction freedom is better than no \nguar\u00adantee at all, and if no progress is observed, the system may resort to running isolated threads. \nIn this paper we concentrate on lock\u00adfreedom, which seems tobebothpracticable as well as adesirable guarantee.We \nalso shortlydiscuss wait-and obstruction-freedom. Existingde.nitionsforlock-freedom(and otherprogressguar\u00adantees) \nin the literature only provides guarantees for eventual progress.Thereisnospeci.cbound onthetimetomakeprogress, \nexceptthatprogresstimeis .nite.Furthermore,the(mostlyinfor\u00admal) existingde.nitions are oftenlimitedtodata \nstructures and not speci.edforgeneralprograms. Theeffortstofacilitatenon-blockingparallelprogramming \nare ubiquitous, appearing for example,ingeneralforms in the support providedbyparallelprogramminglibrariesforJava[8], \norinthe effortsputinto concurrent non-blocking memory management[2, 17,24], andin the algorithmicdesign \nofparallel algorithms[13].  In this paper we formalize the three notions above using lin\u00adeartemporallogic(LTL)[25].Next,wepropose \nade.nitionfor bounded lock-freedom in which the time for progress is bounded, so that the pace of progress \ncan be clearly determined from the lock-freedom guarantee. It turns out that the details of the formal \nde.nition are subtle and simple variations of the de.nition, that seem intuitively satisfactory, lead \nto various pitfalls. We discuss those as well. We use our de.nition to build an automated tool for checking \nbounded lock-freedom of concurrent programs. Our tool is based onthe CHESS model checker[22] that systematically \nenumerates all thread interleavings of a concurrent program for a given input harness. We show results \nfrom running this tool on the stack data structure, takenfromChapter11 of[13]. Returning to the design \nof parallel programs, a common ap\u00adproach in program design in general, is to develop algorithms in a \ntop-down manner. The algorithm is designed at a high level and then implemented on a high-level language. \nThe program is then compiled with a modern compiler, and run on a contemporary op\u00aderating system andhardwareplatform.We \nexpect such runs topre\u00adserve thegoodproperties of thehighleveldesign, andinparticular thelock-freeguarantee. \nHowever, lock-free algorithms that are implemented on top of modernlanguages maylose theirlock-freedombecause \nthe system services that they employ do not support lock-freedom. In some cases,systemservices may eveninclude \nexplicitorimplicitlocks. For example,Valois lock-freelinked-listalgorithm[26]has a well known C++implementation \nbyBush[1], which uses theC++ new operation. On most systems, the new operation does not maintain lock-freedom. \nTo preserve lock-freedom, a special allocator that preserveslock-freedom mustbe used.Further examples,discussed \nin Section 5, show various scenarios in which care is required to maintain lock-freedom through the supporting \nsystem and hard\u00adware. An interesting question that arises is whether one must ex\u00adplore the entire implementation \nto the bones of the highly com\u00adplex hardware in order to make sure that an algorithm satis.es the promisedguarantees. \nInthispaper,wepropose aframework that avoidssuch aneed, by separating algorithmdesignfrom system support.Weformalize \nand advocate adevelopment framework in which reasoning canbe made separatelyfor theprogram andfor the \nservicesit uses.Thus, the algorithm developer is able to claim that an algorithm runs in a lock-free \nmanner on any system that supports lock-freedom. The systemdeveloper is able to reason that the system \ncan support any lock-free algorithm. And if both make their claims using our framework, then a simple \ncomposition theorem, asserted in this paper,provides the connection,guaranteeing that the run of alock\u00adfree \nalgorithm on a system that supportslock-freedomislock-free. We stress that the algorithm developer does \nnot need to know the systemdetails and vice versa. System services are ubiquitous. Examples include runtime \nser\u00advices (such as memory management, initialization of program structs, and initialization/termination \nof threads), interpreter ser\u00advices executing instructions of a program, or micro-instructions thatexecute \nmachine code, memoryhardwarethat supports caching for memory accesses, virtual memory that supports paging \nfor memory accesses, device drivers that supports access to I/O, pro\u00ad.ling code executed with theprogramtomonitoritsbehavior, \netc. All of these must support lock-freedom to ensure that theprogram running at thehighestlevelisindeedlock-free. \nThispaper makes thefollowing contributions. Formalizing the progress guarantee notions using LTL, in \na way that allows formally arguing about progress guarantees of programs. Extending thelock-freeprogressguarantee \ninto the moreprac\u00adtical bounded lock-freedom, for which the progress guarantee isbounded for allpossible \nexecutions.  Exemplifying the use of the de.nitions for a veri.cation of the lock-freedomproperty of \na standardlock-free stackimplemen\u00adtation.  Putting up a framework for arguing about system support for \nlock-freedom, enabling a systemdesigner to show that alllock\u00adfreeprograms willindeed runlock-free onhis \nsystem.  Asserting(andproving) a compositiontheoremthat underlies the usefulness of the system supportframework. \n Organization. In Section 2 we provide basic de.nitions. We de\u00ad.ne lock-freedom and discuss variants \nin Section 3. This de.ni\u00adtion is used to check an implementation of a lock-free stack using a model checker \nin Section 4. In Section 5 we demonstrate why a formal approach is advisable in this context by bringing \nsome examplesfrom the real world.A speci.cation ofthe serviceframe\u00adwork,i.e.,de.nitionsforlock-freeprograms \nandlock-free support\u00ading servicesisgiveninSection6.InSection7 we state andprove the composition theorem. \nRelated work is discussed in Section 8 and we conclude inSection9. 2. Preliminaries Lock-freedomis aboutaprogram \nmakingprogress andinparticular about an operation execution terminating. To formalize that, we must .rst \nestablish the terminology for programs, operations, and computation steps. We will view a program asbeing \ncomposed of a series of oper\u00adations, where operation is composed of a sequence of computation steps with \ncontrol statements that direct the .ow of execution. We assume that each computation stepis executedby \na threadbelong\u00ading to the set T. Note that the same program can be partitioned into operations in various \ndifferent ways, according to a choice of granularity. Progress is obtained when an operation terminates. \nIt is possible that a program will be lock-free for one de.nition of operations and not lock-free with \nanother choice of operation de.nition. We believe this freedom to choose what progress means is important \nfor proving guarantees that are appropriate for a given program. Thus, operations must be speci.ed and \ncannot be derived from a program description. In contrast, the computation steps should be automatically \ndeducible from the program. One may choose to think of computation steps ofdifferentgranularities, such \nas aTur\u00adingMachine d-transitions,instructions of some machinelanguage, instructions of microcode, or \nclock ticks of the machine hardware. Terminology should be well set before aproof can be constructed, \nbutfor the context of thispaper, they should allbe equivalent. To reason about theprogram execution, \nwede.ne a setofpred\u00adicates on computation steps. For a thread t . T, the predicate sched(t) (standing \nfor schedule oft ) is true for a computation stepif and onlyif t is the thread executing the step.Also, \nthepred\u00adicate prog(t) (standing for progress of t ) is true for a computa\u00adtion stepif and onlyif t completes \nan operation, orin other words, makesprogressinthis step.Obviously, prog(t) . sched(t).Also, .tprog(t) \nis true in a step if some thread makes progress in this step. An execution of a program P is a sequence \nof computational steps.Let P(n) bethesetofallexecutions oftheprogram P for any input of size n . N. With \na slight abuse of notation, we represent S P to be .n.N P(n), the set of all executions for the program \nP. Regarding the model of parallelism, in this work we adopt the model of sequentially consistent executions.This \nallowsdiscussing points in the execution. Extensions for non-sequentially consistent platforms are outside \nthe scope ofthispaper.  3. De.ningLock-Freedom We now turn to de.ning lock-freedom. The formal de.nitions \nbe\u00adlow are closein spiritto existingde.nitionsintheliterature, except that they are speci.edforgeneralprograms. \nWe use linear temporal logic (LTL) [25] as our formalism. For completeness, we provide a brief introduction \nto the temporal operators Gand F,specializedfor our setting. 1Given an execution s0 ,s1 ,s2 ,..., and \na predicate p, the predicate Gp (standing for globally p )is true at a step si if and onlyif p is truefor \nall steps sj ,j = i.Also, Fp (standing for .nally p )is true at a step si if and only if p is true for \nsome step sj ,j = i. Apredicate p is true for an execution s0 ,s1 ,s2 ,..., if and only if p holds for \nthe .rst step s0 . For example, an execution satis.es Gsched(t) if t is the only thread scheduled in \nthis execution. Similarly, F prog(t) holds for an execution if the thread t eventually completes an operation \nin this execution. Finally, GF prog(t) is true for an execution if the thread t completes operations \nin.nitely-often in other words, the thread t makes continuous progress in this execution. Thede.nition \noflockfreedomfollows. DEFINITION 3.1 (Lock-Freedom). An execution e . P is lock\u00adfreeif and onlyif e satis.es \nGF .tprog(t). Aprogram P islock-freeif every execution e . P islock-free. To reason about bounded progress \nguarantees, we introduce a temporal operator Fk de.ned for k . N as follows. Given an execution s0 ,s1 \n,s2 ,..., and a predicate p, we say Fk p (standing for .nally p within k ) is true at a step si if and \nonly if p is true for some step sj ,i = j<i + k. Intuitively, Fk p is true at a step if p holds within \nthe next k steps in this execution. Furthermore, to later de.ne bounded wait-freedom we will need to \nextend this operator into Fkq p which is true at a step si if p holds within the next k steps that satisfy \nthepredicate q. Bounded lock-freedom is speci.ed with respect to the input length n. This choice will \nbe discussed in Section 3.3 below. We also provide an equivalent non-LTL de.nition there, in which we \nspell outthequanti.ers anddiscusspossible alternativede.nitions. DEFINITION 3.2 (Bounded Lock-Freedom). \nAn execution e . P is k-bounded lock-freeif the execution satis.es GFk .tprog(t). A program P is bounded \nlock-free if for any n . N, there exists a k such that all executionsin P(n) are k-bounded lock-free. \nTo stress thebound, we sometimes say that P isbounded lock\u00adfree withprogressguarantee k(n) (andexplicitly \nspecify thefunc\u00adtion k(n)). Thetraditional(unbounded)lock-free notion nicelycapturesthe theoreticalessence \nof an algorithmthat must makeprogress eventu\u00adally.Athreaddoes notget stuckforeverjustbecause anotherthread \nis delayed for a long time or even fails to execute. Theoretically, this is an important guarantee, but \nthe down side is that for such lock-freeprograms, a delayed(or failing) thread can create ahuge delay \nfor the other running threads. It is only guaranteed that it cannotdelay themforever.Fromapracticalpoint \nof view,and es\u00adpecially whenbuilding asupporting serviceforapractical system, 1Interested readers should \nread [25] for a more rigorous and complete treatment. the bounded variant is very attractive since it \nlimits delays even ina worst case scenario.Such aguaranteeisessentialfor systems that must support high \nresponsiveness, and in particular real-time behavior. In the rest of this paper we concentrate on the \nbounded lock-freedom variant. A simple analogue of the de.nitions, prop\u00aderties, and claims can be also \nmade with the unbounded variant. We chose to work with thebounded de.nition because of itsprac\u00adtical \nvalue and since the claims and de.nitions are slightly more involved to state and prove. It is better \nto formally cover the more dif.cult case. In Section 3.3 below, we demonstrate the care re\u00adquiredinsuchde.nitionby \nshowing that slight,seemingly correct variationsdo not work well. To complete this section, we alsoformallyde.ne \nwait-freedom and obstruction-freedom. Wait-freedom ensures that if a thread is scheduled in.nitely many \ntimes, then it must make progress in.nitely many times. Although this is not the focus of this paper, \nwealsoprovide an extensionforbounded wait-freedom(andlater forbounded obstruction-freedom). DEFINITION \n3.3 (Wait-Freedom). An execution e . P is wait\u00adfreeif and onlyif e satis.es .t(GF sched(t)) . (GF prog(t)) \n. Aprogram P is wait-freeif every execution e . P is wait-free. DEFINITION 3.4 (BoundedWait-Freedom). \nAn execution e . P is k-bounded wait-freeif theexecutionsatis.es sched(t) .tGFk prog(t). Aprogram P is \nbounded wait-free if for any n . N, there exists a k such that all executionsin P(n) are k-bounded wait-free. \nFinally,weformallyde.ne obstruction-freedom.Thisde.nition makes surethatprogressis obtained when athread \nrunsinisolation. DEFINITION 3.5 (Obstruction-Freedom). An execution e . P is obstruction-free if and \nonly if e satis.es .tGF ( sched(t) . prog(t)) . A program P is obstruction-free if every execution e \n. P is obstruction-free. DEFINITION 3.6 (BoundedObstruction-Freedom). An execution e . P is k-bounded \nobstruction-free if the execution satis.es .tGFk ( sched(t) . prog(t)) . A program P is bounded obstruction-free \nif for any n . N, there exists a k such that all executions in P(n) are k-bounded obstruction-free. As \nexplained in the introduction we focus on lock-freedom in the rest of this paper. Wait-freedom is desirable \nbut strong and oftenhard to achieve forinteresting tasks.in contrast, obstruction\u00adfreedom provides a \nguarantee that only holds if a thread is run in isolation. Lock-freedom is practicable and provides a \nstrong enoughguarantee. 3.1 RemarksabouttheDe.nitions The wait-free and obstruction-free conditions \narelocal to a thread. This locality may make the requirement simpler to prove. In con\u00adtrast, proving \nlock-freedom for a set of threads that create a pro\u00adgram, requires a global view of the threads in the \nset. The proof must take care of theinteractionbetween all threadsin the set, and make sure that other \nthreads do not interfere. In particular, it will haveto ensurethatthethreads oftheprogram makeprogress(when \nscheduled) independently of otherthreadsinthe system.The notion ofindependence willbediscussedinSection3.2below. \n De.nitions 3.2 and 3.1 have been stated with respect to a pro\u00adgram. An equivalent de.nition may be \nstated for a method, or any otherpart oftheprogram that consists offull operations2.Thus, we can consider \nalsolock-free methods andlock-free sub-programs.It followstriviallythatifaprogramislock-freethen soisanysub-part \nofit. Next, we wouldliketo stressthatwaitingfor some event should not be considered a single computation \nstep. A processor that is waiting for a lock to become available or waiting for an event to happen is \nconsidered as executing no-operation steps or running a busywait.Ifa waitfor alock cannotbebounded, thenthe \nalgorithm is notlock-free. Finally, we would like to point out a property that is implied by De.nition \n3.2 but is not explicitly stated. For most programs bounded lock-freedom implies a bound on the number \nof threads that can run simultaneously during theprogram s execution. OBSERVATION 3.7. Any non-trivially-progressing \nbounded lock\u00adfree program P and any n . N, there is a bound t . N on the number of threads that run concurrently \nat any point in any execution of P oninputs oflength n. Non-trivially-progressingprograms willbede.nedbelow(seeDef\u00adinition3.8).But \ntheir essencebecomes clearfrom theproof sketch of the observation that follows herein. Suppose a program \nP is lock-free, but for has an input x, such that and an execution on x, for which it can spawn an unbounded \nnumber of threads, depend\u00ading on the execution sofar.Let n be thelength of x and k = k(n) be theprogressguaranteed \nforinputs oflength n.In a way of con\u00adtradiction, webuild a schedulein whichtheprogramdoes not make progress \nin k collaborative steps, thus foiling the lock-freedom guarantee. The scheduler create an execution \nin which the pro\u00adgram spawns k (or more)threads that run concurrently. It then lets each suchthread run \nuntiljustbefore making anon-progressing computation step.Typically, not allprogram steps areprogressing, \nand thus the thread will arrive at executing a non-progressing step. When all threads are about to execute \na non-progressing step, the scheduler willlet eachthreadperform one single(non-progressing) step, thus \nmaking the program execute k (or more)steps with no progress, contradictingthelock-freedomguarantee. \nTheaboveproofiscorrectonly if thescheduler canbring each thread to execute a non-progressing computation \nstep. This holds for standardprograms thatmakeprogress only oncein a while,but weformalize thispropertyin \nthefollowingde.nition. DEFINITION 3.8 (Trivially-progressingprograms). A program P is trivially-progressing \nif there exists an input x, a state S in the execution of P on x, and a thread of execution T such that \nif T is scheduled to run exclusively(without any other thread executing any steps) from state S, T makes \nprogress in each and every computation step.  3.2 SynchronizationIndependence Since lock-freedom is \nabout executing cooperatively on a set of threads, then we should be able to also talk about independence \nof other sets of threads. We thus introduce a simple notion of independent execution.Intuitively, we \nsay thatif twoprograms are lock-free and if they do not communicate in any way, then when they executetogether \nonthe same system, each ofthemis stilllock\u00adfree. 2We must partition programs on operation boundaries. \nIf we break a pro\u00adgram into a sub-program that ends in the middle of an operation, then op\u00aderation termination \ncannot be guaranteed when the sub-program .nishes executing. DEFINITION 3.9 (SynchronizationIndependence). \nTwoprograms (orsub-programs)are synchronization-independentifforanypos\u00adsible inputs and any possible \nthread scheduling, the two programs (or sub-programs) never communicate or use shared resources, neitherdirectly, \nnor viaindirect communication with other threads. We are now ready to make a simple observation aboutindepen\u00addence \nand lock-freedom. The idea is that if two programs do not communicate, then they cannotinterfere with \neach other sprogress guarantee. This is true because if one of the programs is allowed to run enough \ncomputation steps, thenbyits originallock-freedom guarantee it must complete an operation, and the second \nindepen\u00addentprogram srundoesnotin.uencetherunof the .rstprogram. Theformal statementfollows. OBSERVATION \n3.10. Let P1 and P2 be two programs that are synchronization independent. If P1 is lock-free, then running \nthese programs in parallel maintains the lock-freedom of P1 . Namely, P1 slock-freedomguarantee holds \nevenif P2 runsinparallel.  3.3 Onde.ningboundedlock-freedom In this section we argue that our de.nition \nof bounded lock\u00adfreedom is the right one. As will be shown, slightly modi.ed de.nitions which may seem \nadequate do not work out well. The main difference between bounded de.nition and the standard one is \nthe order of quanti.ers. In the bounded version, the bound is guaranteedfor all executions(on agiveninputlength) \nandinthe standard one the bound may depend on the execution. To make the discussion more straightforward, \nwe spell out the quanti.ers explicitlyin the(non-LTL)de.nitionbelow. DEFINITION 3.11(BoundedLock-Freedom \n-nonLTL version). A program is bounded lock-free if for any n . N there exists a natural number k such \nthat, for any possible input x of length n, any possible execution of the program on the input x, and \nfor any possible point t in the execution, there exists an operation whose execution terminates before \nthe program threads collectively exe\u00adcute k steps after time t. De.nition 3.11 (and its equivalent de.nition \n3.2) specify the guaranteed bound as a function of the input length. We argue that simpler andintuitively \ncorrect alternativesfail. First, consider the simplestde.nition whichguarantees a single constantbound, \nthat should hold for all executions.Thisde.nition would .rst specify an existential quanti.er k and demand \nthat all executions must makeprogress after k collaborate steps.We claim thatthisde.nitionistoo strong,thwarting \ntheability toprovethat any non-trivial algorithm is lock-free. The argument is similar to the one made \nfor Observation 3.7. Consider any multithreaded al\u00adgorithm that may run an unbounded number of threads \nsimulta\u00adneously3 and in which progress is not trivial, i.e., operations are composed of several steps \nanddo not terminatein each step.Given abound k on the number of steps required to obtainprogress, think \nof a worst-casein whichtheprogram spawns k+1 threads that run concurrently and the scheduler lets each \nof them execute a single computation step thatdoes not terminate an operation.This execu\u00adtion runs k \n+1 steps without making any progress, contradicting the k-bounded lock-freedom guarantee. Thus, an independent \nex\u00adistentialquanti.eris too restrictivefor modern systems,in spite of itsdesirable strongguarantee and \nsimplicity. Aseeminglygood remedyforthe aboveproblemisto make the boundkdependonthenumber ofthreadsthat \nmay run concurrently 3Note that spawning an unbounded number of concurrent threads does not mean spawning \nanin.nite number of threads, whichisnot realistic.Itjust means that for each .nite number l, there exists \nan execution that spawns more than l threads.  inthe execution.Namely,thede.nition wouldsay thatfor \nanypos\u00adsible t, there exists a k such that if not more than t threads are executed concurrentlyin the \nexecution, thenprogress within k col\u00adlaborative steps is guaranteed. Such a de.nition seems good in its \nrestrictiveness, providing a very strong guarantee. However, since the bound k depends on the number \nof concurent threads then this de.nitionistooliberalandis not adequate as well.Denotetheguar\u00adantee k(t) \nto stress thefact that thebound k depends on thebound on the number of concurrently running threads t. \nWe claim that with this de.nition any unbounded lock-free algorithm becomes also bounded lock-free, by \nshowing how to slightly modify any unbounded lock-free algorithm to make it bounded lock-free un\u00adder \nthis de.nition. The idea is that when the algorithm encounters an execution thatfailsto satisfytheguarantee \nk(t),it spawns more concurrentthreads(thatdo nothing)toincrease k(t) intok(t ' ) and buy more time. After \nspawning enough threads, the desired bound for the speci.c execution is obtained and the program continues \nwhile satisfying thebound k(t ' ).Thisis why we set theguarantee as afunction of theinputlength. Our \nchosen de.nition 3.11 lets the lock-free bound k be deter\u00admined by the input length n. When proving a \nlock-freedom prop\u00aderty,one willhaveto show(aspart of theproof) abound t(n) on the number ofthreads as \nwell.Such aboundis requiredto complete the proof, as our .rst observation implies, even though our simple \nde.nition does not specify it explicitly. A more cumbersome def\u00adinition could explicitly state the existence \nof a bound t(n) on the number of threads and then a bound k(t) on the steps to make progress. (Note that \nthis would .x t and k for the run, and not allow buying more time by spawning more threads.) Our simpler \nde.nition does not allow the bound k to change during the execu\u00adtion, andimplicitly achieves thedesirableproperties. \nSometimes aninputis notintuitivelyassociated with aprogram, e.g., if it is interactive. In such cases, \nwe adopt a convention from computational complexity, andlet theprogram receive an arti.cial unary input \n1n that sets a complexity bound for its operations and guarantees. 4. VerifyingBoundedLock-Freedom Given \nthe formal de.nition of lock-freedom it is possible to build an automated tool for verifying or falsifying \nlock-freedom of con\u00adcurrentprograms.In this section, wedescribe aprototype of such a toolbuilt on top \nofthe CHESS [22]model checker.Given a concur\u00adrent program and a test harness, CHESS exhaustively enumerates \nall thread interleavings of the program for the given test harness, verifying safety andlivenesspropertiesfor \nthegivenharness. Ourtoolworks on the simple observation thatfor agivenbound k, the De.nition 3.2 of bounded \nlock-freedom can be converted intoasafetyproperty.Accordingly,webuild a .nitestatemonitor, showninFigure1, \nthat checksforthe violation of k-boundedlock\u00adfreedom. Essentially, the monitor counts the number of steps \nsince the last progress step and fails if this count becomes greater than k.The model checkerperforms \na synchronousproduct of this monitor with the state space of theprogram, and checksif afailure stateis \nreachable. We chose to check a simple lock-free implementation of the stackdata structure, takenfromChapter11 \nof[13].We used anin\u00adputharnessthattakes anintegern and creates n concurrentthreads, .n 2 . of whichperform \napush operation and the restpeform apop operation. Each thread terminates after performing its single \nop\u00aderation on the data structure. The stack initially contains enough elements for all the pop operations \nto succeed. Table 1 contains the results of our experiment. We interpreted computation steps as instructions \nin this scenario. Therefore, we used a monitor that counted the number of instructions executed. We also \nran an addi-State : s . [1..k + 1] Init : s =0 Transition(t) : prog(t) . s ' =0 !prog(t) . s ' = s +1 \nfailif s ' >k Figure 1. Finite state monitor that checks for bounded lock\u00adfreedom: GFk .tprog(t).The \nstate contains a bounded integer s. On atransition ofsomethread t,the new state s ' is setto zero when \nprog(t) is true, other s ' is set to s +1. Bound # threads # accesses # instructions Time (s) 2 3 17 \n0.156 3 6 30 0.390 4 9 43 7.800 5 12 56 128.790 6 15 69 2514.238 Table 1. Results for checking bounded \nlock-freedom for the LockFreeStack implementation in Chapter 11 of [13]. We mea\u00adsurebounds in terms of \nnumber of shared memory accesses andin the number ofinstructions. tional monitor, that counted the number \nof memory accesses.Such a counter maybe valuablefor memory-bound applications. For each attempted number \nof threads andfor each monitor, we iteratively increased the bound till we found a bound with which thetestsucceeded.Table1 \nreportsthetimetakenforthe succeeding bound when run on anIntelCore2Duo1.6GHzlaptop with4GB of memory \nrunning a32-bitWindowsVista operating system. The time roughly re.ects the fact that the model checking \nproblem is exponentialin the number of threads andlinearin thebound. On looking at the error paths returned \nby the model checker on failures, we inferred why the memory access bound grows as 3 * (n -1), where \nn is the number of threads. The push and pop operations essentiallyperform thefollowing code: while(true){ \n 1: Node* oldTop = top; 2: Node* newTop = Compute(oldTop); 3: if(CompareAndSe t(top, oldTop, newTtop)) \n4: return; } Eachoperation reads the current version of top, the variablepoint\u00ading to the top of the \nstack and then, depending on the function executed (push or pop), computes the new version of top. This \ncomputation requires one memory access. Finally, the operation updates the value if top did not change \nduring the computation. TheCompareAndSet operationperforms one memory operation on failure and two memory \noperations on success. For n threads, the maximum length of non-progress computation occurs in the fol\u00adlowing \nscenario. All of the n threads have read the same value for top are atline3.One of the thread succeeds.The \nremaining n -1 threads fail and retry till they reach line 3 again. This takes a total of 3 memory accessesper \nthreadduring which none of the threads makeprogress. Our model checker is currently unable to symboli\u00adcallyprovethat \n3 *(n-1) isthelock-freedomboundforn threads. Similar argument canbe madefor theinstructionbound measure. \n 5. SystemSupportforLock-Freedom: Motivation In this section we motivate extending thede.nitions tohandle \nser\u00advice supportforlock-freedom.We startinSection5.1 with a simple counter. It is a service that exists \nin many systems. We show that standardimplementations may ormay not supportlock-freedom, so some care \nmustbe used.Next,inSection5.2 we note that some of the common hardware does not support a worst-cae lock-freedom \nguarantee. Thus, even when the software is designed and imple\u00admented with the proper care, strong guarantees \ncan not be made aboutprogress in the execution.Beforegoinginto these examples, we should also mention \nthatthe examples ofSection6.1, wherethe new command is usedin alock-freeimplementation,demonstrates another \nvulnerability of real lock-free implementations and moti\u00advates theformal treatment oflock-freedom support \nas advocatedin thispaper. 5.1 AnExample:UsingSoftwareEventCounters An important tool used to monitor \nprogram behavior for perfor\u00admance improvement or debugging is the performance and event counters service.Anevent \ncounter servicetypicallyprovides a set of methods for creating counters and updating counter values. \nTo provide such a servicein a multithreaded environment, the service mustimplement somebasic atomic counter \naccess, sothat multiple threads can updateit consistently.Some systemsprovideprimitives foratomically \nupdating acounter(increment,decrement,and zero its value), while other systems may require use of loops \nthat re\u00adpeatedly attempt an atomic compare-and-swap (CAS)operation to atomically update the counter. \nBoth cases allow alock-free imple\u00admentation of counter updates. In the latter case, when one thread fails \nto modify the counter atomically, another thread must have succeeded. If t is a limit on the number of \nthreads that may con\u00adcurrently execute on inputs of length n, and m is the number of computation steps \nused to modify the counter in the service im\u00adplementation, then after the threads execute m \u00b7 t service \nsteps at\u00adtempting to modify the counter, weknow that aprogram attemptto execute a service operation musthave \nterminated. The situation becomes more complicated when a background thread is employed. Service support \nin general may sometimes include a background thread that performs some off-line support. In this case, \na background service thread can be used to serve counter values to a separatepresentationprogram(e.g., \nagraphical performance monitoringtool).As willbediscussedformallyin the next section,we wouldliketoguaranteethattheprogram \nsprogress isnothinderedby thebackground servicethread. Inthe counter example, the easycaseis whenthe \nservice thread does notchangethe counter value.Inthiscase, allraces on updating the counter value are \ndue to program threads calling the counter services. Thus, progress must be made by the program on at \nleast one ofits threads and the service supportslock-freedom.However, if the service thread may change \nthe counter value, for example to resetthe counter value afterhandlingittothe monitoring tool,then lock-freedom \nsupport can not be guaranteed. Repeated updates of the counter values by the service background thread \nmay prevent the runningprogram threadsfrom makingprogress. Thus, a simple counter service may interfere \nwith the system s progress guarantees. In this paper, we propose a framework for arguing that a service \nsupportslock-freedom.  5.2 Weak LL/SC operationsinISAs Let us now move to demonstrating the importance \nof making the system support lock-freedom. In particular, we will claim that widely used systems cannotbe \nused toguarantee supportforlock\u00adfreedom at this stage.Some substantialhardware and system mod\u00adi.cations \nare required to remedy theproblemsdescribedbelow. Most lock-free algorithms proposed in the literature \nare ex\u00adpressedeitherby using CAS operations orby usingload-linked and the store-conditional (LL/SC)operations. \nThe LL/SC operations are apair ofinstructions oftenfound in reduced instruction set(RISC) instruction \nset architectures (ISAs). The load-linked (sometimes also called load-with-reservation)instruction retrieves \nthe value of a memory location and simultaneously places a reservation on the memorylocation.Other threads \nmay modify the memorylocation, butindoingsothey will clearthe reservation.The store-conditional instruction \nstoresa valuein amemorylocationifthereservationleft by a prior load-conditional instruction has not been \ncleared. If the write operation is performed, the instruction is said to succeed. If the write operation \nis notperformed, the instruction is said to fail. In its simplest most restricted form, a reservation \ncan be left on at mostonememorylocation(by asingleload-linked operation) and the store-conditional operation \nmust operate on the memory loca\u00adtion of thepriorload-linkedinstruction. Many lock-free algorithms have \nbeen described in terms of the LL/SC operations.Thedescriptionsgenerally assume strong LL/SC operations, \nwhere the store-conditional fails if and only if the re\u00adserved memory location has been modi.ed. Unfortunately, \nreal hardwaredoes not alwaysimplement strong LL/SC operations.For example, thePowerPC andARMISAsimplement \nweak LL/SC op\u00aderations for which spurious failures of the store-conditional op\u00aderations arepermitted4.For \nexample, reservations willbe cleared whenperforming context switchesorwheninterrupthandlersper\u00adforms \nLL/SC operations of their own. Note that interrupt handlers (handlingpage faults, networkcommunication, \netc.)happen at un\u00adpredictable times. Many published lock-free algorithms described in terms of LL/SCoperations \nare notlock-free whenthe store-conditional oper\u00adations can spuriouslyfail.Tobelock-free,there mustbe \nabound on the number of steps required to ensure progress, and such bounds can almost always be exceeded \nby a suf.cient number of spurious failures of store-conditional operations, that may be caused by an \nunfortunate timing of interrupts and context switches. We stress that the event of not making progress \nis of extremely low prob\u00adability. In practice, these algorithms will not get stuck. However, a robust \nworst-case guarantee cannot be claimed. It can only be claimed withhighprobability.5 Thus, the support \nthat these widely available architectures pro\u00advideforlock-freedom viatheimplementation of the LL/SC instruc\u00adtionsisnotgood \nenough tofullyguaranteeprogressin worst-case scenarios for lock-free algorithms. This demonstrates the \nimpor\u00adtance of checking the underlying support whenimplementinglock\u00adfree algorithms on agiven system \nandhardware. 6. ServicesthatSupportLock-Freedom Let us now consider programs and services. We start with \nthe generalde.nition ofa service. DEFINITION 6.1 (AService). A service S is a support for a set of operations \nthat a caller program may invoke. A service consists of four components. 4Supposedly, the MIPS and Alpha \nISAs also only implement weak LL/SC operations, but we have not seen the manufacturers documentation \nsup\u00adporting ordisproving this claim. 5One may argue that enough disruptions, say, in the forms of interrupts, \nwill foil progress is any case, simply because the program will not get enough CPU cycles to make progress. \nIndeed lock-free programs cannot makeprogressif theydo notget suf.cientCPU cycles.However, a worst\u00adcase \nscenario of spurious failures make lock-free programs fail to make progresseven whentheydogetsuf.cientCPU \ncycles.  1. Aninternal state. 2. A set of operations with a functionality speci.cation for each operation. \nThe functionality speci.cation includes the speci.\u00adcation of inputs, the side-effects on the internal \nstate, and the speci.cation of the output as a function of the input and the internal-state. 3. A speci.cation \nof a valid operation schedule. 4. A background functionality executed concurrently with the callerprogram \nexecution.  Note that some services do not require all four components in order to provide the service. \nFor example, a mathematical library would typically contain only the second component and empty versions \nof the other components. A concurrent memory manager will makegood use of allfour components asfollows:Theinternal \nstate would be the heap location, size, content, and additional allocator information (e.g., a free-list \nheader). The background functionality would consist of the concurrent garbage collector threads. The \noperations include allocation of an object, and access (read or write) of a .eld of an object in the \nheap or local and globalvariables.A valid schedule of memory management requests will typically allow \nmodifying an object .eld only after the object has been allocated. It will implicitly require that the \nobject be reachablefromtheprogram slocal orglobal variables(the roots), by requiring that the program \nsupplies a pointer to the object. Formally, it is possible to specify the service operations and valid \nscheduleby specifyingits operational semantics. The above de.nition of a service does not include the \nactual implementation.Suchanimplementation mayimply moreinvolved interactionbetweenthe components, andthe \nactualimplementation is what determines the properties of the service. We now separate abstract speci.cationfrom \nactualimplementation. DEFINITION 6.2 (AnAbstract-ActualServiceSpeci.cation). An abstract-actual speci.cation \nof a service consists of two separate speci.cations of a service. 1. An abstract speci.cation Aspeci.es \nthe service operations and theirfunctionality,the valid service operation schedule, and the internal \nstate. 2. An implementation speci.cation I speci.es the entire service including theinternal state representation, \nthe steps thatimple\u00adment the functionality of operations, and the steps that imple\u00adment thebackground \nfunctionality.  We proceed with the de.nitions that separate reasoning about programsfromreasoningabout \nservicesprovidedtothem.The .rst de.nition speci.es what should be proven for a program to make sure that \nit is lock-free in the presence of a lock-free supporting service. We make sure that such a proof is \nindependent of the serviceimplementation, as the sameprogram may run ondifferent systems and the guarantees \nshould be proven once for all possible underlying(supportive) system services. DEFINITION 6.3 (ProgramLock-Freedom). \nAprogram P islock\u00adfree with respect to an abstract service A if the following two conditions occur. 1. \nP is lock-free under the assumption that any service operation is executed immediately, in a single computation \nstep, which produces thefunctionality speci.ed for the service operation. 2. For any n . N there exists \na t . N such that for any input x of length n, and any execution of P on x, there are at most t concurrent \nthreads at anypointin the execution. 3. If theprogram runs on aninputoflength n, then it only makes \ncalls to the service operations whose inputs are of length n or less.  4. For any execution of the program \nP, all its calls to the service operations create a valid service operation schedule. The .rst condition \nin De.nition 6.3 formalizes the intuitive manner in which one expects to prove lock-freedom of a program \nwhile ignoring the underlying system implementation. The ser\u00advice is assumed to happen at almost no cost \nand to never delay the execution. The second condition explicitly requires a bound on the number of threads \nthat the program runs concurrently. As discussed in Observation 3.7, this is implicitly required for \nmost practical programs, but to make the composition clean, we make this requirement explicit. The third \ncondition ensures that an exe\u00adcution cannot delay a deadline by creating a new service request with a \nlong input. If a service request is made, its input must have alimited size and the responseguaranteedoes \nnot change 6.This extra conditionis not neededin case thebound on thetimeto make progress is a constant \nthat does not depend on the input.7 The last condition ensures that theprogrambehavesproperly. Wenowturntoconsidering \ntheserviceanditssupport oflock\u00adfreedom. In order to separate the lock-freedom support from any speci.cation \nof an actual lock-free program, we conceptually ig\u00adnore the program operations except for its calls to \nservice opera\u00adtions. This way, we concentrate on the run of any possible valid service requests and ignore \nthe rest of the program operations and the concurrent background threads. Formally, for any execution \ne of a program P using a service S. we will consider a projection eS of the execution e on the service \noperations. This includes all the computation stepsthat runduringthe service operation, and ex\u00adcludes \nall operations of the background threads and operations of theprogram that are not service requests. \nTheprojection eS maybethought of astaking avalidprogram execution and removing allprogram statements \nthatdo notinclude a service operation call.The service supportguarantees should not be concerned with \nthe actualintent of theprograms.Therefore, we remove all non-service steps. Note that it is possible \nto run the operations onthe service(giventheirinputs) without running the entire program, assuming the \nprogram only accesses the service s internal state via the service operations. In the projected execution \neS we care how many steps it takes to .nish an execution of a service operation. For service operation \nsteps the predicate S-prog is de.ned to be true if and only if t .nishes an execution of a service operation \nat this step. We call aprogram valid ifallits executions make only valid service calls. DEFINITION 6.4 \n(SupportingLock-Freedom). A projection eS of an execution e on the operations of Service S is k-bounded \nlock\u00adfree service execution if eS satis.es GFk .t S-prog(t). A service S is supporting bounded lock-freedom \nif for any n . N, and any t . N,there exists a k, such thatfor any validprogram P, and any execution \ne of P that runs at most t concurrent threads, and invokes S s operations with inputs of length at most \nn, the projected execution eS is a k-bounded lock-free service execution. Note that typically, the lock-freedom \nguarantee depends on the number of threads that may be executed concurrently, and hence the dependence \nof the guarantee k on the number t of concurrent 6the limit n canbemadea(reasonably small) function of \nn,but we chose to keep the de.nitions simple. The formal input to the program may be arti.cially padded \nwith 1n for a large enough n suf.cing for all service calls. 7Sometimes it is natural to have a constant \nbound, such as when imple\u00admenting a stack data structure. At other times, it is important to allow the \nbound to be dependent on the input of the service request, e.g., when an allocation procedure zeros the \nspaceitis allocating.  threads that may be spawned by the calling program. The number t provides a bound \non the number of threads that may run service operations concurrently. Once the program is proven to \nbe lock-free with respect to an abstract service according toDe.nition6.3 above, and the service isproven \nto supportlock-freeprograms accordingtoDe.nition6.4 above, then the composition theorem asserts that \nthe system as a whole is lock-free. The composition theorem is stated and proven inSection7below. We \ndepict the separation of the program from the service in Figures 2-3. Figure 2 depicts the run of a program \nwith service calls andbackground threads.Theprogram threads aredepicted as continuouslines, whose calls \nto the service operations aredepicted aslong ovals.Thebackground threads aredepicted asdashedlines. Figure2showsthreeprogramthreads \nandthreebackground threads running concurrently.InFigure3 thetwoentitiesareseparated.In 3-atheprogrampartisdepicted \nandthelong ovals(representing the service operations) are replaced by a single computation step highlighted \nby a small circle. Figure 3-b depicts the service part with thebackground threads and the attainable \nschedule of service requests obtainedfrom the original execution of theprogram. A point worth stressing \nfor the De.nition 6.4 in the presence of background threads is that it implicitly requires that enough \nprogressis madebythebackgroundthreadstoguarantee abounded execution time for each operation execution. \nThe service imple\u00admentation I makes suchprogress while executing the operations or via thebackground \nthreads.If the service islatebecause the back\u00adground threads do not make enough progress, then a service \noper\u00adation maybe stalled, and theprogress requirementin thede.nition cannot be guaranteed. Returning \nto our memory management ex\u00adample, the memory manager may support further allocations only if the garbage \ncollector manages to free space in the background before the heap is exhausted. Therefore, in order to \nprove that the service supportslock-freedominthis scenario, one may need to as\u00adsume something about the \nscheduling of the background threads, ensuring that theyget a chance to make suf.cientprogress.Exten\u00adsions \nin this spirit, that allow lock-freedom support to be condi\u00adtioned on some eventshappening during theprogram \nrun, are easy following thede.nitionsprovidedin thispaper. 6.1 AnExample:Lock-freeLinked-ListwithAllocation. \nWe nowlook at a speci.clock-free algorithmfrom theliteratureto demonstrate our framework. Several lock-free \nalgorithms for con\u00adcurrent data structures use helper objects. As a speci.c example we chose thelock-freelinked-list \nalgorithmbyValois[26].This linked-list algorithmusesauxiliary nodesinbetweenthe real el\u00adements ofthelistto \nmanage racesbetweendeletions andinsertions. Details can be found in the original paper, we only highlight \nrele\u00advant issues. In order to execute an insert to the linked list, the ab\u00adstract insert method is given \na new node, a pointer to an object in thelist,and anavailable auxiliary node.Butinpractice,animple\u00admentation \nof an insert operation must somehow acquire an auxil\u00adiary node in order to insert the real one into the \nlist. A natural implementation ofacquiring an auxiliary node wouldbeto allocate such a node using the \nsystem s memory manager, or to manage a pool of available auxiliary nodesfor useby theprogram. Using \nour framework to reason about this algorithm, one can show that in its abstract form, this algorithm \nis lock-free, when assuming that the allocation of an auxiliary node is executed in a single computation \nstep. This is, in fact, the way such algorithms were shown lock-free in the .rst place. A natural way \nto de.ne an operationin this context.i.e.,de.neprogress,is to think of each operation onthedata structure, \nandinparticular aninsert operation, as an operation of the program, and its termination be considered \nprogress. Moving on to the allocation service, a careless implementation of the allocator may uselocking \nto coordinate all allocationsin the system,foilingthelock-freeguarantee.Inparticular,theimplemen\u00adtationin[1]usestheC++ \nnew operation, which on most system will acquire a lock and not maintain lock-freedom. This simple use \nof allocation whileimplementing lock-free algorithms is widely seen in the literature, see for example, \nthe two code examples in the JavaConcurrencybook[8].Topreservelock-freedom, alock-free allocator mustbe \nused.Anotherpossibleimplementation of the al\u00adlocation serviceis apool ofpre-allocated objects that the \nallocator can use.Thispool of objects canbe managed via alock-free stack. Such a pool does not use background \nthreads, and one can show that when several threads try to allocate an auxiliary object from a non-empty \nstack, one of them always succeeds prior to executing k2 \u00b7 t operations, where k2 is a constant and t \nis a bound on the number of threads. Given such a guarantee, the composition theo\u00adremguarantees that \nthe entireimplementationislock-free. Typically, apool is managed bypre-allocating a number of ob\u00adjectsforthepool,and \nmaybe extendingitwhen necessary.A simple implementation willguaranteelock-freedom onlyif thepooldoes \nnot need to be extended. This scenario can be phrased as a predi\u00adcate on program run. Denote by N the \nnumber of objects initially allocatedfor thepool.Wede.ne thepredicate P on aprogram ex\u00adecution, to be \nTRUE if a program execution uses at most N auxil\u00adiaryobjects simultaneouslythroughoutthe run.Thepooling \nservice supportslock-freedom onlyforprogramsthat satisfy thepredicate P.Usingtheconditionallock-freedom \nsupportterminology, we say that for programs that always satis.es the predicate P, the use of thepooling \nserviceisguaranteed tobelock-free. Havingestablishedtheframeworkto reason aboutlock-freedom separatelyforprograms \nand services, we are now readytoprovethe composition theorem. 7. TheCompositionTheorem The theorem that \nbinds the lock-free program guarantee and the lock-free supporting service guarantee into a single lock-free \nrun isthecompositiontheorem statedbelow.Again,wework with the bounded version of lock-freedom, and everything \nin this section canbeeasily translatedintotheunbounded version. THEOREM 7.1. Let P be aprogram andlet \nS be a service with an abstraction Aand animplementation I.Ifthefollowing conditions hold: 1. Program \nP islock-free with respect to the abstract service A. 2. The service S supports lock-freedom. 3. TheprogramP \ndoes notcommunicate withthe service I except via the service operations, and the effects of service calls \nare exactly and only those effects speci.ed by the abstract service de.nition.  Then thejoint execution \nof theprogram P using the service S is lock-free. When we say that execution of the program P using the \nservice S is lock-free in the assertion above, we mean that after the program runs a limited number of \nsteps, a program operation terminates. In contrast to the abstract step count in De.nition 6.3, the counting \nof program steps in this case includes steps of the program code as well as steps of the service implementation \nthat executewhentheprograminvokes aservicerequest.But notsteps executed on thebackground threads.8 8To \nmotivate this exclusion, think of concurrent garbage collection. Progress made by the concurrent collector \nis not considered program s progress.  Figure2. Aprogram running a service.Program threads are representedby \ncontinuouslines with service operationsdepicted aslong ovals; servicebackground threads aredepicted asdashedlines. \n3-a:Theprogrampart with each service operationsbeing a single computation step. 3-b:The servicepart \nwithbackground threads and service requests. Figure 3. Our framework separates the program from the service. \nFigure 3-a depicts the program part and Figure 3-b depicts the service part. Proof: In order to prove \nlock-freedom of the system, we need to showthatfor any n . N there exists a ksuch thatfor all executions \noninputs oflength n, GFk .tprog(t). Fix an input x of length n. By De.nition 6.3 Since the program is \nlock-free with respect to S, it must make calls to the service operations which create a valid service \nschedule and the input lengths to the service operations are allbounded by n.In addition, it never runs \nmore than t concurrent threads for some t that may depend on n. Let kP be the number of computation steps \nthatguaranteeprogram operation termination asguaranteed by the lock-freedom of the program for inputs \nof length n, according to De.nition 6.3. Let kS be the number of computation steps that guarantee, according \nto De.nition 6.4 and on inputs of length at most n and at most t program threads, that a service operation \nmust terminate after a projected execution eS runs kS steps. Fix any execution of the program e, and \n.x any point in the schedule p. We claim that one of the program operations terminates within k = kP \n\u00b7 kS computation steps executed by the program threads after p. Since it is guaranteed that the program \ndoes not touch the ser\u00advice data structure, except via the service operation invocations, then by De.nition \n6.4, whenever kS computation steps of service operations are run, at least one service operation completes, \ninde\u00adpendently of all other steps executed by the program. Thus, after any kS steps are executed, one \nevent must occur. Either a service step terminates, or aprogram non-service stepis executed.We can therefore \ndeduce that after executing k = kS \u00b7 kP computation steps, at least kP steps were executed in which either \na service request terminated or a non-service step of the was executed. In the abstract scenario of De.nition \n6.3, we obtain at least kP steps when service operations are counted as a single step. By the pro\u00adgram \nlock-free guarantee, if at least kP such steps are executed, then the program makes progress and we are \ndone with the proof ofTheorem7.1. A similar theorem may be stated for the unbounded variant of lock-freedom. \nIn this case, the proof will .rst .x the execution of the program with the service; it will then obtain \nthe relevant constantsfortheprogram and the service(giventhe execution) and derivethe multiplication,whichformstheguaranteedboundforthe \njointexecution. Wait-freedom satis.es composition in the above sense almost trivially, since each thread \nmakesprogress after making k steps,in\u00addependently of the other threads in the system. Obstruction free\u00addom \nis also closed under composition, but some subtleties may come up inde.ning compositionproperly(similarly \ntoissuesthat come up with nested atomic transactions) 8. RelatedWork Wait-freedom wasinitiallyde.nedbyHerlihy[10]:any \nthread can complete any operation in a .nite number of steps, regardless of the execution speeds ofthe \nother threads.Theterm wait-freedom is strictly stronger than the term lock-freedom studied in this paper. \nIn the same paper, Herlihy [10] also introduced the concept of nonblocking implementations. According \nto that de.nition some thread is guaranteed to complete an operation in a .nite number of steps, regardless \nof the execution speed of the threads.Massalin introduced the concept of lock-free implementations[15], \nwhichis synonymous withHerlihy s nonblocking concept. Both termshave sincebeen used todescribe various \nthings,but a consensus appears tohave emergedin recentyearsto usetheterm lock-freetodescribe something \nclose to whatHerlihy originally called nonblocking. In a subsequent paper, Herlihy et al. [12] also introduced \nobstruction-freedom.Michael andScott[21] earlierdescribed this property as lock-free but not non-blocking, \nhowever, they used the term non-blocking in the sense of lock-free and the term lock-free to mean withoutlocks. \nSinceHerlihy et al.de.ned the term obstruction-free , a concensus appears to have emerged on making a \ndistinction between lock-free and obstruction-free. Prior to the de.nition of obstruction-free , the \nterm lock-free was sometimes used todescribeboth categories.  Thereisabody of work onwait-freeimplementions \nof various synchronization operationsin terms of other synchronization oper\u00adations, e.g., Jayanti s implementations \nof CAS in terms of LL/SC and LL/SC in terms of CAS [14] and Michael s implementation of thelatter[19].Such \nwait-free translations preserveproperties of algorithms such as wait-freedom, lock-freedom, or obstruction\u00adfreedom. \nMany lock-free algorithms were designed and we do not at\u00adtempttolistthemhere.Many example appearin[13] \nRelatedto thediscussion on memory management above, one should notethat lock-free supporting algorithms \nfor manually allocating and delet\u00adingobjects exist andhigh-performanceimplementationshavebeen writtenbyMichael[20] \nandGidenstam et al.[7]. Our observation thatabound onthe number of simultaneous ac\u00adtive threads is an \nimportant parameter for bounded lock-freedom, has an analoguein apaperby Merritt andTaubenfeld[16],where \nthey study the signi.cance of this bound for several classical dis\u00adtributed algorithms. Other restrictions \non the asynchronous model are typically required to obtain interesting algorithms, see, for ex\u00adample[4,5]. \nDongol[3] formalizes(unbounded) progressguarantees using their proposed logic capable of expressing progress. \nGotsman et al. [9] have recently built a tool for arguing about (unbounded) progressguarantees using \nrely-guarantee reasoning. Automatic reclamation of auxiliary nodes used in various lock\u00adfreedata structures(suchasValois \nlock-freelinked-lists) canhave a lock-free implementation provided a lock-free mechanism for node allocation \nand reclamation.Lock-free allocation and reclama\u00adtion canbe achievedby using a concurrently markinggarbage \ncol\u00adlector that supportslock-free allocation, e.g., STOPLESS, CLOVER and CHICKEN [23,24].Alternatively,the \nnodes canbe managed bycustom mechanisms, e.g., the reference counting schemebyGi\u00addenstam etal.[6], or \nthepointerguarding schemes ofMichael[18] andHerlihy et al.[11], which alllimitthe number of cursorsinto \na data structure a single thread may maintain. The cost of certain operations areproportional to the \nnumber of cursors allowed, soif the limit on the number of cursors is removed, the algorithms will nolongerbelock-free. \n9. Conclusion We extended the de.nition of lock-freedom into bounded lock\u00adfreedom, which seems more attractivefor \napplied systems.Wefor\u00admalized the notion of bounded lock-freedom using LTL, incorpo\u00adrateditintothe CHESS \nmodel checker, and useditto check a simple standardlock-free stackimplementation.Next, we advocated a \nsep\u00adarate reasoning about lock-freedom for a program and the system thatsupportsit.Aframework wasproposedforseparately \nreason\u00adingaboutthelock-freedom of a service and aboutthelock-freedom offaprogram.Finally, a compositiontheorem \nwas shown, asserting thatif aprogramhasbeen shownlock-free according to ourframe\u00adwork and the services \nit uses support lock-freedom, then the real run, given the service implementations is guaranteed to be \nlock\u00adfree as well. Acknowledgement We thank Roy Friedman, Tim Harris, Maurice Herlihy, Victor Luchangco, \nMaged Michael, Mark Moir, Nir Shavit, and David Tarditifor manyhelpfuldiscussions. References [1] Lawrence \nBush. Lock free linked list using compare &#38; swap. http://www.cs.rpi.edu/ bushl2/project web/page5.html,April2002. \n[2] David L. Detlefs, Paul A. Martin, Mark Moir, and Guy L. Steele. Lock-free reference counting. Distributed \nComputing, 15:255 271, 2002. [3] Brijesh Dongol. Formalising progress properties of non-blocking programs. \nIn Formal Methods and Software Engineering, 8th International Conference on Formal Engineering Methods, \nICFEM 2006,pages284 303,2006. [4] CynthiaDwork,Nancy Lynch, andLarryStockmeyer. Consensusin the presence \nof partial synchrony. Journal of the ACM, 35(2):288 323,1988. [5] Faith Ellen Fich, Victor Luchangco, \nMark Moir, and Nir Shavit. Obstruction-free algorithms can be practically wait-free. In DISC, pages78 \n92,2005. [6] Anders Gidenstam, Marina Papatrianta.lou, H\u00b0akon Sundell, and Philippas Tsigas. Practical \nand ef.cient lock-free garbage collection based on reference counting. Technical Report 04, Chalmers \nUniversity ofTechnology andG\u00a8oteborg University,2005. [7] Anders Gidenstam, Marina Papatrianta.lou, and \nPhilippas Tsigas. Allocating memoryinalock-freemanner.In Proceedings of the13th AnnualEuropeanSymposium \nonAlgorithms, number3669inLNCS, pages329 342.Springer-Verlag, October2005. [8] Brian Goetz, Tim Peierls, \nJoshua Block, Joseph Bowbeer, David Holmes, and Doug Lea. Java Concurrency in Practice. Addison\u00adWesley,2006. \n[9] Alexey Gotsman, Byron Cook, Matthew Parkinson, and Viktor Vafeiadis. Proving that non-blocking algorithms \ndon t block. In ZhongShao andBenjaminC.Pierce, editors, Proceedings ofthe36th ACM SIGPLAN-SIGACT Symposium \non Principles of Programming Languages, POPL2009,pages16 28.ACM,2009. [10] Maurice Herlihy. Wait-free \nsynchronization. ACM Transactions on ProgrammingLanguages andSystems,13(1):124 149,January1991. [11] \nMaurice Herlihy, Victor Luchangco, Paul Martin, and Mark Moir. Nonblocking memory management support \nfor dynamically-sized data structures. ACMTransactions onComputerSystems,23(2):146 196,May2005. [12] \nMauriceHerlihy,VictorLuchangco,andMarkMoir.Obstruction-free synchronization:Double-endedqueues as an \nexample.In Proceedings of the 23rd International Conference on Distributed Computing Systems,pages522 \n529,May2003. [13] Maurice Herlihy and Nir Shavit. The Art of Multiprocessor Programming. MorganKaufmann,March2008. \n[14] Prasad Jayanti. A complete and constant time wait-free implemen\u00adtation of cas from ll/sc and vice \nversa. In Proceedings of the 12th InternationalSymposium onDistributedComputing, volume1499 of LNCS,pages216 \n230,September1998. [15] Henry Massalin. Synthesis: An Ef.cient Implementation of Funda\u00admental Operating \nSystem Services. PhD thesis,Columbia University, 1992. [16] Michael Merritt and Gadi Taubenfeld. Computing \nwith in.nitely manyprocesses. In DISC 00: Proceedings of the 14th International Conference on Distributed \nComputing, pages 164 178. Springer\u00adVerlag,2000. [17] Maged Michael. Scalable lock-free dynamic memory \nallocation. In Proceedings of SIGPLAN 2004 Conference on Programming Languages Design and Implementation, \nACM SIGPLAN Notices, Washington,DC,June2004.ACMPress. [18] MagedM.Michael. Safememory reclamationfordynamiclock-free \nobjects using atomic reads and writes. In Proceedings of the 21st Annual ACM Symposium on Principles \nof Distributed Computing, pages21 30,July2002. [19] Maged M. Michael. Practical lock-free and wait-free \nLL/SC/VL implementations using 64-bit CAS. In Proceedings of the 18th International Conference on Distributed \nComputing, volume3274 of LNCS,pages144 158.Springer-Verlag, October 2004.  [20] Maged M. Michael. Scalable \nlock-free dynamic memory alloca\u00adtion. In Proceedings of the 2004 ACM SIGPLAN Conference on Programming \nLanguage Design and Implementation, pages 35 46, June2004. [21] Maged M. Michael and Michael L. Scott. \nSimple, fast, and practical non-blocking and blocking concurrent queue algorithms. In Proceedings of \nthe15thInternationalSymposium onPrinciples of Distributed Computing,pages267 275,May1996. [22] Madanlal \nMusuvathi, Shaz Qadeer, Thomas Ball, Gerard Basler, Piramanayagam Arumuga Nainar, and Iulian Neamtiu. \nFinding and reproducing heisenbugs in concurrent programs. In OSDI 08: Operating Systems Design and Implementation, \npages 267 280, 2008. [23] Filip Pizlo, Daniel Frampton, Erez Petrank, and Bjarne Steensgard. STOPLESS: \nA real-time garbage collector for multiprocessors. In MoolySagiv, editor, ISMM 07Proceedings of theFifthInternational \nSymposium on Memory Management, pages 159 172, Montr\u00b4eal, Canada, October2007.ACMPress. [24] Filip Pizlo, \nErez Petrank, and Bjarne Steensgaard. A study of concurrent real-time garbage collectors. In Proceedings \nof SIGPLAN 2008 Conference on Programming Languages Design andImplementation, ACMSIGPLANNotices,pages33 \n44,Tucson, AZ,June2008.ACMPress. [25] Amir Pnueli. The temporal logic of programs. In FOCS 77: Foundations \nofComputerScience,pages46 57,1977. [26] John D. Valois. Lock-free linked lists using compare-and-swap. \nIn Proceedings of the fourteenth annual ACM symposium on Principles ofdistributed computing,pages214 \n222,Ottawa,Ontario,CA,1995.   \n\t\t\t", "proc_id": "1542476", "abstract": "<p>Parallel platforms are becoming ubiquitous with modern computing systems. Many parallel applications attempt to avoid locks in order to achieve high responsiveness, aid scalability, and avoid deadlocks and livelocks. However, avoiding the use of system locks does not guarantee that no locks are actually used, because progress inhibitors may occur in subtle ways through various program structures. Notions of progress guarantee such as lock-freedom, wait-freedom, and obstruction-freedom have been proposed in the literature to provide various levels of progress guarantees.</p> <p>In this paper we formalize the notions of progress guarantees using linear temporal logic (LTL). We concentrate on lock-freedom and propose a variant of it denoted <i>bounded lock-freedom</i>, which is more suitable for guaranteeing progress in practical systems. We use this formal definition to build a tool that checks if a concurrent program is bounded lock-free for a given bound. We then study the interaction between programs with progress guarantees and the underlying system (e.g., compilers, runtimes, operating systems, and hardware platforms). We propose a means to argue that an underlying system supports lock-freedom. A composition theorem asserts that bounded lock-free algorithms running on bounded lock-free supporting systems retain bounded lock-freedom for the composed execution.</p>", "authors": [{"name": "Erez Petrank", "author_profile_id": "81100377919", "affiliation": "Technion, Haifa, Israel", "person_id": "P1464259", "email_address": "", "orcid_id": ""}, {"name": "Madanlal Musuvathi", "author_profile_id": "81100333862", "affiliation": "Microsoft Research, Redmond, WA, USA", "person_id": "P1464260", "email_address": "", "orcid_id": ""}, {"name": "Bjarne Steesngaard", "author_profile_id": "81100440791", "affiliation": "Microsoft Research, Redmond, WA, USA", "person_id": "P1464261", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1542476.1542493", "year": "2009", "article_id": "1542493", "conference": "PLDI", "title": "Progress guarantee for parallel programs via bounded lock-freedom", "url": "http://dl.acm.org/citation.cfm?id=1542493"}