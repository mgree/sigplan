{"article_publication_date": "06-15-2009", "fulltext": "\n Semantics-Aware Trace Analysis Kevin Hoffman Patrick Eugster Suresh Jagannathan Computer Science Department, \nPurdue University {kjhoffma,peugster,suresh}@cs.purdue.edu Abstract As computer systems continue to \nbecome more powerful and com\u00adplex, so do programs. High-level abstractions introduced to deal with complexity \nin large programs, while simplifying human rea\u00adsoning, can often obfuscate salient program properties \ngleaned from automated source-level analysis through subtle (often non\u00adlocal) interactions. Consequently, \nunderstanding the effects of pro\u00adgram changes and whether these changes violate intended protocols become \ndif.cult to infer. Refactorings, and feature additions, mod\u00adi.cations, or removals can introduce hard-to-catch \nbugs that often go undetected until many revisions later. To address these issues, this paper presents \na novel dynamic pro\u00adgram analysis that builds a semantic view of program executions. These views re.ect \nprogram abstractions and aspects; however, views are not simply projections of execution traces, but \nare linked to each other to capture semantic interactions among abstractions at different levels of granularity \nin a scalable manner. We describe our approach in the context of Java and demonstrate its utility to \nimprove regression analysis. We .rst formalize a subset of Java and a grammar for traces generated at \nprogram execution. We then introduce several types of views used to analyze regression bugs along with \na novel, scalable technique for semantic differenc\u00ading of traces from different versions of the same \nprogram. Bench\u00admark results on large open-source Java programs demonstrate that semantic-aware trace \ndifferencing can identify precise and useful details about the underlying cause for a regression, even \nin pro\u00adgrams that use re.ection, multithreading, or dynamic code genera\u00adtion, features that typically \nconfound other analysis techniques. Categories and Subject Descriptors D.2.5 [Software Engineer\u00ading]: \nTesting and Debugging debugging aids, diagnostics, testing tools, tracing General Terms Algorithms, Reliability \n1. Introduction The ability to understand and analyze interactions among program components to infer \nor verify salient program properties is critical as software complexity increases. Permission to make \ndigital or hard copies of all or part of this work for personal or classroom use is granted without fee \nprovided that copies are not made or distributed for pro.t or commercial advantage and that copies bear \nthis notice and the full citation on the .rst page. To copy otherwise, to republish, to post on servers \nor to redistribute to lists, requires prior speci.c permission and/or a fee. PLDI 09, June 15 20, 2009, \nDublin, Ireland. Copyright c &#38;#169; 2009 ACM 978-1-60558-392-1/09/06. . . $5.00 This paper introduces \na novel semantics-aware dynamic analysis for understanding and analyzing complex programs. Our approach \nachieves accuracy, .exibility and scalability through the concept of semantic views. Semantic views are \ntrace abstractions which selectively aggregate collections of events with shared semantic traits found \nin a program execution trace, allowing for speci.c aspects and abstractions in programs to be captured \naccurately. Flexibility is achieved by allowing for new, speci.c views to be de.ned and applied. Views \nare linked among each other to capture program semantics in a scalable manner, and can be leveraged by \npro.lers, optimizers, and bug-.nders to quickly sift through a program execution, focusing attention \non those parts that signify interesting deviations or properties. We illustrate the usefulness of our \napproach by examining its utility to identify regressions in large (well-tested) software applications. \nWe de.ne a regression as a behavior that executed correctly un\u00adder some input in a prior software version \nthat is no longer correct under the same input in a newer version. Revisions are common in complex software \nsystems, and refactorings and feature updates can introduce subtle regression bugs that are often not \ndetected in a timely manner. Identifying and .xing these regressions can be a complex and time-consuming \ntask, aggravated by the presence of advanced program mechanisms such as dynamic dispatch, code generation, \nand loading. For example, a query of the Apache Soft\u00adware Foundation bug tracking database1 shows 455 \nbugs created during 2007 that involve a regression.2 Out of these 455 bugs, 36% took longer than two \nmonths to resolve, 15% took longer than six months, and 11% remain unresolved as of November 2008. Our \ntechnique provides a scalable solution to identifying precisely the cause of regressions in large, evolving \nJava programs. Given two traces corresponding to executions of a correct and regressing version of a \nprogram, we employ a novel differencing algorithm that extracts trace abstractions from these traces, \nand correlates executions based on their similarity by means of a novel, tractable, longest-common subseqeuence \n(LCS)-based approximation. Motivating example. To illustrate the problem, Fig. 1 shows code snippets \npatterned after a known regression identi.ed as MYFACES-11303 in the Apache MyFaces project,4 an open \nsource implementation of the Java Server Faces standard. The exam\u00adple centers around functionality in \nthe MyFaces framework that automatically converts non-7-bit safe characters in the output of an HTTP \nrequest response into their equivalent HTML numeric entities. In this example, this conversion only occurs \nif the output document type is text/html, and each character is only converted 1 http://issues.apache.org/jira/ \n2 Bugs marked as duplicate or invalid were not included. 3 https://issues.apache.org/jira/browse/MYFACES-1130 \n4 http://myfaces.apache.org/   (a) (b) Figure 1. (a) an original non-regressing and (b) newer, regressing \nversion of a program. if it is not in the range [32..127]. This range is de.ned program\u00admatically and \nkept in mutable variables. A bug was introduced in a new version of this program that inadvertently sets \nthe range to [1..127] instead of [32..127], causing a regression when given a document of type text/html \nthat contains characters in the range [1..31]. In the original version, the ServletProcessor class directly \ninstan\u00adtiates the NumericEntityUtil with the correct range. This object is not used until after the HTTP \nrequest has been fully processed and the output generated; the actual character conversion process is \naffected by dynamic state initialized much earlier during program execution. In the new version, the \nBinaryCharFilter class was ex\u00adtracted from the ServletProcessor as part of a new generic I/O .ltering \nabstraction, thus confounding static analysis as no read\u00adily apparent structural property is violated. \nThe BinaryCharFilter class however provides an incorrect range of [1..127] to the new NumericEntityUtil \nobject, causing the character .ltering process to later produce incorrect results, but only for certain \ninputs. This example is a pattern for an entire class of regressions caused when a piece of code incorrectly \nalters some dynamic state in the program, with the manifestation of the error appearing, only in certain \ncases, at some later point in the execution.5 The causal distance between the point where the error occurs \nand where it manifests makes it dif.cult to precisely analyze such regressions manually, or correlate \ninformation derived from dynamic program slicing techniques that identify all semantically related operations \nto a regression. Fig. 2 illustrates the intuition underlying our approach: different semantic views are \nused to capture individual aspects of the pro\u00adgram and link them together. In the .gure, we show views \nfor the main thread of execution, as well as an object view for the log object, and a method view for \nmethod setRequestType. Observe that these specialized views record events that may be temporally far \nremoved from one another. Our technique correlates these views with views generated by the regressing \nversion to allow the regress\u00ading behavior to be localized and identi.ed. Making such correla\u00adtions precise \nand scalable is feasible because views discard actions unrelated to the behavior of the abstraction being \ntraced. 5 See also https://issues.apache.org/jira/browse/SOAP-169. Figure 2. A portion of the execution \ntrace for our example is shown on the left. The example is single threaded, so there is a single thread \nview which is identical to the full execution trace. There are many target object and method views. One \ntarget object view is shown for the .rst LOG object, containing only method calls and .eld accesses on \nthat object. One method view is shown for the SP.setRequestType method, showing only actions that oc\u00adcurred \nwhen SP.setRequestType was on the top of the call stack. Contributions. This paper makes the following \ncontributions: 1. Semantic views: Because execution traces can be millions of entries long, we de.ne \na new view-based trace abstraction that allow traces to be tractably analyzed and traversed. These views \nre.ect the different levels of abstraction that naturally arise in object-oriented programs (e.g. objects, \nmethods, threads, etc.), and provides a semantics-based means of structuring trace data. We present a \nformalization of traces and views for a subset of Java extending Featherweight Java with references, \nassign\u00adments, and threads. We furthermore de.ne how these views can be correlated, linked, and navigated. \n 2. Tractable trace differencing: A new technique for differenc\u00ading view-based traces is introduced, \nto correlate common points of execution by ef.ciently and accurately approximating the longest-common \nsubsequence (LCS) [3] between full program traces. Notably, our differencing technique has linear complex\u00adity \nin both time and space, allowing full program traces that include dynamic state to be used in the analysis \nwhile keeping running times and memory requirements reasonable. 3. Regression cause analysis: We show \nhow to use our differ\u00adencing technique to accurately identify the causes of various regressions. The \ncomparisons between traces for all difference sets are analyzed to produce a .nal candidate set of regression \ncauses. Besides identifying differences that likely caused the re\u00adgression, the analysis outputs a full \nsemantic diff between the original and new versions, allowing these potential causes to be viewed in \ntheir full context, with dynamic state.  These techniques are implemented in a fully automated way in \nour tool, RPRISM, requiring no code annotations or access to source code. Benchmark results on realistic \nwell-engineered Java pro\u00adgrams demonstrate that our techniques yield high accuracy (few false positives \nor negatives produced), is scalable (traces with mil\u00adlions of entries can be analyzed), and is applicable \neven to programs that exploit advanced features such as multithreading, re.ection, and dynamic code generation. \nTo our knowledge, RPRISM is the .rst tool capable of performing automated regression analysis on programs \nwith such a feature set.  The remainder of the paper is organized as follows: Sections 2 and 3 formalize \nour approach in the context of a subset of Java and view\u00adbased trace differencing respectively. Section \n4 elaborates on these concepts in the context of regression analysis. An empirical eval\u00aduation of our \ntechniques, including several large case studies, is presented Section 5. Section 6 describes related \nwork; conclusions are given in Section 7. 2. View Model In this section, we present (i) an abstract model \nof execution traces and a language whose evaluation produces such traces, and (ii) the views trace abstraction. \n2.1 Language We .rst introduce a simple object-oriented language whose syntax is shown in Fig. 3; its \nsyntax and semantics follows in the spirit of other core object-based calculi such as Featherweight Java \n(FJ) [11] or Classic Java [7]. Our language augments FJ modulo casts with locations (of the form l(C \n)), .eld assignments (t.f =t), sequences of terms (t), value objects (new D(d)), and threads (T(t;)). \nA corresponding program is de.ned as such a thread term. Note that for brevity, locations l(C ) are sometimes \nsimply written l when the type C is not germane to the context. program P ::= T(t;) class CL ::= class \nC extends C {Af ; KM} creation K ::= C(Af){super(f);this.f=f;} method M ::= Am(Ax){t; return t;} type \nA ::= C | D term t ::= x | v | t.f | t.f=t | t.m(t) | new C(t) | new D(d) | T(t;) value v ::= l(C) | \nD(d) primitive d . D (D, d, D) .{(Bool, b, B), (Int, z, Z), . , (Float, r, R)} Figure 3. Core language \nsyntax. 2.2 Traces Notably, program evaluation yields traces, whose structure is de\u00adscribed below and \noutlined in Figure 4. Such a trace is a sequence of trace entries t =t1 .. .t n, with |t | denoting its \nlength (n). Differ\u00adent traces are identi.ed by their names, shown in superscripts (e.g., tL). A trace \nentry entry(eid, tid, m, \u00b5, e) is a .ve-tuple consist\u00ading of an entry identi.er eid (the index of the \nentry in the trace), and four items forming a generic context , namely: an identi.er of the active thread \n(tid), the method under execution (m), a rep\u00adresentation of the object on which m is executing (\u00b5), and \nan event e that captures a speci.c action. For now an object is considered to be represented simply by \nits location l. We make use of stacks to trace method calls. A stack S is a sequence of stack entries \nof the form s(m,l,l ), representing the invocation of method m found in object l from object l. We write \nS.s(m,l,l ) for appending such an entry to a stack S . We write S to denote an ordered set of stacks; \neach element in this set captures the dynamic context of some actively executing thread. We write S1 \n\u00b7 . \u00b7Sn to enumerate the elements in this set. Similar notation is used to enumerate an ordered set of \nthreads in a program state. As shorthand, we write S\u00b7S' to denote the ordered set of stacks derived by \nconcatenation of the sets de.ned by S and S'; similar shorthand is used to de.ne concatenation over ordered \nsets of threads. Fig. 5 presents relevant de.nitions. Besides the usual auxiliary functions for .eld \nand method body lookup, this includes evalu\u00adation contexts used in the de.nition of the language s operational \nsemantics, and a function E. () for obtaining the representation of an object to be stored in a trace, \nrevisited in the next section. For now, we ignore the representation of primitive values (thus, E.(D(d))= \n), but assume that any object representation contains the original location, retrievable via l.(\u00b5) for \na given \u00b5. event e ::= FE | ME | KE | TE field event F E ::= get(\u00b5, f, \u00b5) | set(\u00b5, f, \u00b5) method event \nME ::= call(\u00b5, m, \u00b5) | return(\u00b5, m, \u00b5) object event KE ::= init(A, \u00b5, \u00b5) thread event T E ::= fork(S) \n| end(S) object \u00b5 ::= l trace entry t ::= entry(eid, tid, m, \u00b5, e) entry ID eid . Z thread ID tid . Z \nstack S ::= S\u00d8 |S.s(m,\u00b5,\u00b5) Figure 4. Trace syntax. 2.3 Dynamic Semantics Fig. 6 de.nes an operational \nsemantics for our language. Global '' evaluation is of the form (t, t, E , S)=.(t,t.t, E ', S) in which \nt is an ordered sequence of thread terms, t is the trace being generated, E is an object store, and S \nis the ordered set of stacks corresponding to these threads. Local evaluation is of the form (t, t, E \n, S) -.(t',t.t, E ', S') with j the index of the thread term j t being reduced at that step. Rules for \nlocal evaluation only include one stack, S, corresponding to the single thread term reduced. Rule CONGR-E \nrelates local and global evaluation. Rule FORK-E creates a new thread of control, and records a new entry \nin the trace whose event captures the stacks representing the newly created thread s parentage. Note \nthat we track the creation context for the full ancestry of a thread (spawn-point call stack, call stack \nof spawn-point of spawning thread, etc.), in order to increase the accuracy of correlating threads between \ndifferent traces. Rule END-E records the completion of a thread. Rule CONS-E de.nes object creation; \nthe event associated with the object creation trace entry records the class name, the representation \nof the parameters to the constructor, and the representation of the created object itself. Rule CONS-VAL-E \nis de.ned similarly for primitives. Rules FIELD-ACC-E and FIELD-ASS-E record trace entries for .eld access \nand assignment. The de.nition of method call (rule METH-E) records a trace event that captures the object \nand method being invoked, along with its arguments (the calling context being captured by the enclosing \nentry). Similarly, rule RETURN-E records a trace entry that captures details relevant to a return action; \nthis includes a trace event that records the method and object being returned from, and a representation \nof the return value.  2.4 Views Trace Abstraction Although complete, the traces yielded by our evaluation \nrules re\u00adquire tedious interpretation to understand the underlying program behavior, since these traces \ncapture features of higher-level con\u00adstructs. We consequently formulate named projections over these \nlow-level traces, which link semantically related trace events. These projections, termed views, represent \nvarious levels of ab\u00adstraction in the executing program. Views are formed by mapping each trace entry \nto a set of view names, describing which speci.c views an entry is a member of. This set of names is \nobtained by creating a union of the results of all of the view name mapping  De.nitions class C extends \nC ' {A f ; . } ' ' fields(C ' )=A f fields(Object)=\u00d8 ' ' fields(C)=A f , A f ' class C. {. M} class \nC extends C {. M} ' A m(A x) {t;} . M .. .m. . M mbody(m, C)=(x, t) mbody(m, C)=mbody(m, C ' ) E.(D(d))= \nE.(l(C) )=l Evaluation contexts E ::= [] | E.f | E.f=t | v.f=E | E.m(t) | v.m(E) | new C(E) | v,E,t \n| v;E;t | T(E;) | E;return t | v;return E Figure 5. De.nitions and evaluation contexts. functions (.) \n(one mapping function is de.ned for each type). Spe\u00adci.c views are then obtained by trace projections \nover view names of the form pp.f|. : T . T(see Figure 7), where Tis the domain of traces. p.f|. ranges \nover predicates of the form T.Bwith Tthe domain of trace entries. p.f|. models whether a given trace \nentry is a member of a speci.c view (as named by .) of a particular view type (f). We focus on four view \ntypes, which are de.ned by their view name mapping functions: Thread views, .TH : One thread view is \nde.ned for each thread tid executing in the program; it contains precisely those events that occur within \nthat thread, in the order of execution.  Method views, .CM : One method view is de.ned for each fully \nquali.ed method name m (for simplicity the class name is omitted in the .gures) in the program. Each \nmethod view contains precisely those events that occur while that particular method was at the top of \nthe call stack.  Target object views, .TO : For each object a target object view is de.ned, containing \nprecisely those events that occur when it is the target of a method call or .eld access.  Active object \nviews, .AO : For each object an active object view is de.ned containing those events that occur when \nit is on the top of the call stack.  The key to effective program analysis using our view model of tracing \nis that all these views are linked together. In the present context it is suf.cient to view these links \nas being implicitly given by retaining indices of the original trace in the projected views. For example, \na trace event recording a method call o.m(...) will be recorded in the thread view of the thread in which \nthe call is performed, in a method view for m, in the active object view of the method performing the \ncall, and in the target object view for target object o. The trace index found in the entry can be used \nto navigate from the entry found in one view to its position in another. In this way, a program as a \nwhole may be modeled as a complex web of interconnected views. At any arbitrary point in any view, one \ncan use these links to visit all semantically related views (as modeled by the de.ned view types), thereby \norganizing both the exploration of program execution as well as the presentation of the results of such \nan analysis in more meaningful ways. 3. Views-Based Trace Differencing The comparison of traces through \ncomparison of their correspond\u00ading view trace webs allows for a powerful program analysis. The (t,t ,E \n,S) =.(t ,t.t ,E ,S ) '''' (t, t , E , Sj ) -.(t,t , E , Sj ) j (CONGR-E) '' (T(. ) \u00b7 Tj ' (E[t]) \u00b7 T(. \n) ,t, E , S\u00b7Sj \u00b7S ' )=. '' ''' (T(. ) \u00b7 Tj (E[t ]) \u00b7 T(. ) '' ,t , E , S\u00b7S \u00b7S ' ) j (t,t ,E ,S) -.(t \n,t.t ,E ,S ) j Sj.s(m, \u00b5, \u00b5 ' ) .S t =entry(|t |, j, m, \u00b5 ' , fork(S)) (FORK-E) '' ' (T(. ) \u00b7 Tj (E[T(t;)]) \n\u00b7 T(. ) ,t, E, S)=. '' ' (T(. ) \u00b7 Tj (E[]).T(. ) \u00b7 T(t;),t.t, E, S\u00b7S\u00d8) ' S=S ' \u00b7Sj.s(m, \u00b5, \u00b5 ) \u00b7S '' \nt =entry(|t |, j, m, \u00b5 ' , end(S)) ' '' '' ''' (T (. ) \u00b7 Tj (v;) \u00b7 T (. ) , t , E , S) =.(T (. ) \u00b7 T \n(. ) , t.t , E , S \u00b7 S ) (END-E) ' ' ' l . dom(E ) E ={l .[f1:v1, . ., fn:vn]}E S=S .s(m, \u00b5, \u00b5 ) t =entry(|t \n|, j, m, \u00b5 ' , init(C, E.(v), E.(l))) fields(C)=Af (new C(v),t, E , S) -.(l(C) ,t.t, E ' , S) j (CONS-E) \nS=S ' .s(m, \u00b5, \u00b5 ' ) t =entry(|t |, j, m, \u00b5 ' , init(D, , E.(D(d)))) (new D(d),t, E , S) -.(D(d),t.t, \nE ' , S) j (CONS-VAL-E) E (l)=[. ., fi:v, . ] S=S ' .s(m, \u00b5, \u00b5 ' ) t =entry(|t |, j, m, \u00b5 ' , get(E.(l),fi, \nE.(v))) (FIELD-ACC-E) =entry(|t |, j, m, \u00b5 ' (l.fi, t , E , S) -.j (v, t.t , E , S) E (l)=[. ., fi:v, \n. ] E ' ={l .[. . , fi:v ' , . ]}E S=S ' ' .s (m, \u00b5, \u00b5 ) ' t, set(E.(l),fi, E.(v ))) ' '' (l.fi=v ,t, \nE , S) -.(v ,t.t, E , S) j (FIELD-ASS-E) mbody(m, C)=(x, t) ''''' ' S=S .s(m ,\u00b5,\u00b5 ) S =S.s(m,\u00b5 , E.(l)) \nt =entry(|t |, j, m ' ,\u00b5 ' , call(E.(l), m, E.(v))) (METH-E) (l(C).m(v),t, E , S) -.({/lthis , v/x }t, \nt.t, E , S '' ) j '' ' S=S .s(m, , ).s(m, \u00b5, \u00b5 ) t =entry(|t |, j, m ' , \u00b5, return(\u00b5 ' , m, E.(v ' ))) \n(RETURN-E) (v;return v ' ,t, E , S) -.(v ' ,t.t, E , S ' ) j Figure 6. Program evaluation. foundation \nfor such an analysis consists in the comparison of trace pairs. This section develops a semantics for \nevaluating pairs of such traces to identify semantic differences. 3.1 Trace Differencing Semantics We \npresent a small-step semantics for evaluating pairs of traces (t L ,t R) (termed the left and right traces) \nby comparing them in order to formalize the trace differencing process. The evaluation produces a set, \n., containing those trace entries that are considered to be similar between the two traces. The set of \ndifferences is then easily computed from . and the original trace. We assume that before evaluation a \nspecial eof trace entry is appended to each trace, and is also appended as many times as needed to the \nshorter  t1 .pp(t ) p(t1)pp(t1 .t )= pp(t ) otherwise p.f|. (t )=(. = .f(t )) = .. . .TH (entry(i, j, \nm, \u00b5, e))= (TH,j) .CM (entry(i, j, m, \u00b5, e))= (CM,m) ' '' ) (TO,l.(\u00b5 ' )) e = call(\u00b5 ,m, \u00b5 ' )) ' '' \n) (TO,l.(\u00b5e = return(\u00b5 ,m, \u00b5 ' )) ' '' ) (TO,l.(\u00b5e = get(\u00b5 ,f, \u00b5 .TO (entry(i, j, m, \u00b5, e))= ' )) ' '' \n) (TO,l.(\u00b5e = set(\u00b5 ,f, \u00b5 '' (TO,l.(\u00b5 ' )) e = init(A, \u00b5 ,\u00b5 ' ) . otherwise .AO (entry(i, j, m, \u00b5, e))= \n(AO,l.(\u00b5)) 8 >>>>>>>< >>>>>>>: ( Figure 10. The LCS of two strings. Note that moved subsequences (e.g., \nXY ) are not detected. (tL ,tR , .) -.(tL' ,t R' , . ' ) L ( {t 1} t 1 . lcs(t OL ,t OR ) . ' = {} otherwise \n(STEP-LEFT-LCS) '' (t 1 .t L,t R , .) -.(t L,tR , . . . ' ) L (Similar for (STEP-RIGHT-LCS) rule)Figure \n7. Projection (pp), the view identi.cation predicate (p.f|. ), and entry to view name mappings (.f) Figure \n11. LCS comparison; -. is parameterized over L tOL ,tOR , which are de.ned to be the traces produced \nby trace until both traces are the same length. The resulting augmented trace syntax is presented in \nFig. 8, along with an extended object evaluation of the two programs to be compared, before evaluated \nunder -. or -.. representation. Note that locations by themselves are unsuitable LV for comparison across \ndifferent program versions. We thus extend object representations to tuples which now include, besides \ntheir We present two trace differencing semantics one leveraginglocation in the case of non-value objects, \nan identi.er (or hash) that the longest common subsequence (LCS) algorithm, and the other represents \na recursively computed value representation. leveraging our views trace abstraction. ' object \u00b5 ::= (l, \nr) serialization r ::= D:[d] | C:[r] ' trace entry t ::= entry(eid, tid, m, \u00b5, e) | eof E ' .(D(d))=( \n,D:[d])E (l)=[f1:v1, . ., fn:vn] E ' .(l(C) )=(l, C:[E ' .(v1), . ., E ' .(vn)]) Figure 8. Extended trace \nsyntax and object representation used for differencing. One important aspect to effective view-based \ndifferencing (or any analysis operating over views from multiple executions or versions) is in the formulation \nof effective view correlation functions (X),  3.2 LCS-based Trace Differencing Semantics Well-known \ndifferencing tools such as Unix diff are founded on longest common subsequence (LCS) algorithms [3] in \norder to de\u00adtermine a minimal set of differences between two sequences (e.g., lines of text in two .les). \nFig. 10 visualizes how the LCS identi.es the differences between two strings. Fig. 11 presents an evaluation \nsemantics that leverages the LCS of the two traces in order to de\u00adtermine which trace entries should \nbe placed in .. Note that two trace entries are considered to correspond based on the equality predicate, \n= e . The LCS evaluation relation relies on preserving the original traces before any evaluation takes \nplace. The computationwhich are used to determine if a given view in one execution trace semantically \ncorresponds to a given view in a different execution trace. Fig. 9 de.nes the type signature for all \ncorrelation functions, as well as some notation and helper relations for our differenc\u00ad ing evaluations. \nOne correlation function needs to be de.ned for each view type. The correlation function accepts two \ntrace entries instead of two view names as arguments, because the correlation function may be context-sensitive \n(e.g., based on value representa\u00adof the LCS also results in a correspondence mapping between all common \nentries. This allows each contiguous run of differences in the traces to be viewed as either an insertion, \ndeletion, or modi.\u00ad cation. Using the LCS to understand differences between program traces is bene.cial \nfor correlating similar yet not exactly identical events. For example, if a new version of a program \nadds a new pa\u00ad rameter to a function, the LCS will gravitate towards correlating identical values, thereby \nidentifying the new parameter as the onetions) as opposed to solely the view names. difference. Thread \nview correlation (XTH ) is determined by considering all pos\u00adsible thread correlations (pairs of threads) \nand forming a correlation with the closest match based on the spawning call stack of the thread (and \nthe thread s ancestors). Method correlation (XCM ) cor\u00adrelates two methods if their full type signatures \nare equal. Target object and active object correlation (XTO and XAO ) correlate two objects if either \nthe value representations or class-speci.c object creation sequence number (derivable from trace data) \nare equal. Because view correlations attempt to identify relationships among program abstractions (i.e., \nthreads, methods, objects) found across executions based only on the structure of their views, they are \nbest regarded as heuristics. Nonetheless, experimental results (Sec\u00adtion 5) show that the implemented \ncorrelation functions are effec\u00adtive in practice for regression cause analysis. We believe other cor\u00adrelation \nde.nitions may be useful for other kinds of analyses. However, there are two major challenges in applying \nthe LCS al\u00adgorithm to execution traces. First, the algorithm for computing the LCS is not aware of program \nsemantics and may blindly correlate neighboring entries in the original trace with entries very far apart \nin the new trace (e.g., consider commonly occuring values, such as 0 or null). Second, the computational \ncomplexity of bare LCS is O(n 2) [3], making it intractable on long program traces. Faster solutions \nare known if the input alphabet is .xed, but are not applicable in the present case, as the input alphabet \nincludes value representations, which are innumerable. The standard dynamic pro\u00adgramming algorithm requires \nO(n 2) space in order to reconstruct the LCS (not just its length). The algorithm becomes impractical \nwhen applied to longer program traces, requiring huge memory be\u00adsides time to compute the LCS. Existing \nalgorithms with reduced space complexity require roughly twice the computation time [9].  t1 = e t2 \nEvent equality True if the underlying primitive values (including those generated by E. in Fig. 8) of \nthe events of the two entries are equal Trace index index(t,t ) The index of the entry in t which has \nan eid that index(t, entry(j, k, m, \u00b5, e))= ( '' matches the eid of the entry t it = t1 .. .ti-1 .entry(j, \nn, m ' ,\u00b5 ,e ).. -8 otherwise ( ' Trace t n= e t t with only those elements that are also in t ' according \nt1 ' ) ' : t1 ti .(t n= e t .ti . t = e ' (t1 .t ) n= e t = intersection to event equality (= e ) ' t \nn= e t otherwise Trace window win(t,.)(t ) Correlation Xf(t1,t2) ' ) LCS lcs(t, t Trace t with only \nthose elements whose index is in the range [index(t,t ) \u00b1 .] (. constant, t . t ) Returns (.f(t1),.f(t2)) \nif the views of type f of the two entries are correlated or (., .) otherwise Longest common subsequence \nof t and t with respect to event equality (= e ) win(t,.)(t )= pp. (t ) p.(entry(i, j, m, \u00b5, e))=(i \n. [index(t,t ) \u00b1 .]) View correlation functions, as described in Section 3.1 Refer to [3] for details \n Figure 9. Helper relations for trace differencing evaluation semantics. (t L ,tR , .) -.(t L ' ,t R \n' , . ' ) V t 1 = e t 2 (t 1 .t L ' ,t 2 .t R ' , .) -.(t L ' ,t R ' , . .{t 1,t2}) V (STEP-VIEW-MATCH) \n. ' = {t r | LinkedSimilarEntries(t1,t3,t r)} t 1 = e t 3 t2 = e t4 t L ' n= e tR ' = () .t L '' .tR \n'' (t 1 .t L ' .t 2 ,t3 .tR ' .t4 , .) -. V .t L '' .t R '' index(tVL ,t 5) . index(tVL ,t 1) \u00b1 . ( t \n2 , t4 , . . . ' ) (STEP-VIEW-NOMATCH)  index(t VR ,t 6) . index(tVR ,t3) \u00b1 . (.L,.R) = Xf(t5,t 6) \ntr . lcs(win(t5 ,.)(pp(tOL )), .f|.L win(t6 ,.)(pp(t OR ))) .f|.R LinkedSimilarEntries(t1,t 3,t r) (SIMILAR-FROM-LINKED-VIEWSf) \nFigure 12. View-based comparison; -. is parameterized over V tOL ,tOR ,t VL ,t VR . We de.ne t VL ,t \nVR to be the two thread views being compared, but before being evaluated under -.. V  3.3 Views-based \nTrace Differencing Semantics The aforementioned challenges of LCS can be overcome by lever\u00adaging our \nviews trace abstraction. Instead of differencing the raw pair of traces produced by the contextual semantics, \nwe instead ap\u00adply a trace differencing evaluation semantics to one or more pairs of correlated thread \nviews. Evaluation of each pair of views (-.) V proceeds by removing the head elements and if they are \nequal, plac\u00ading them in set .. For the other case where the head elements are different, any secondary \nviews linked to nearby entries in the main views are explored to .nd correlations between entries in \nthe left and right traces, and then removing from the heads of the left and right main views any entries \nup until the next common point of correlation. Fig. 12 formalizes the above intuition of how one pair \nof thread views is evaluated. The rule (STEP-VIEW-MATCH) handles the case where the heads of the evaluated \nthread view pair have equal events, in which case the entries are removed and added to .. The (STEP-VIEW-NOMATCH) \nrule handles the other case. It adds to . those entries that were found to be similar by comparing cor\u00adresponding \nviews nearby the differing entries, as modeled by LinkedSimilarEntries. The evaluation step also removes \nfrom the heads of the traces any differing entries up until the next pair of similar entries. Evaluation \nthus alternates between the two rules until the end of the traces is reached. The generic rule (SIMILAR-FROM-LINKED-VIEWSf) \nde.nes how to construct LinkedSimilarEntries for a given view type, f. Not shown are the concrete rules, \none for each view type (with f instantiated to TH, CM, TO, or AO). To further explain the LinkedSimilarEntries \nrelation, for a given pair of trace en\u00adtries from the left and right thread views (t1,t3) for whom secondary \nviews should be explored to identify similar entries, a similar entry tr is identi.ed according to the \nantecedent of (SIMILAR-FROM-LINKED-VIEWS). The .rst and second lines of the an\u00adtecedent constrain the \nfree variables t5 and t6 to be trace entries within a constant distance from the entries t1 and t3 respectively. \nThe third line requires that entries t5 and t 6 either have corre\u00adsponding thread views, method views, \nor object views (which views the two entries have in correspondence are called matching views). Line \nfour then constrains free variable t r such that it must be in the LCS of .xed-size windows of the matching \nviews. In this way, similar entries in corresponding secondary views are identi.ed us\u00ading LCS, but over \n.xed-size windows of the nearby entries in the views as opposed to the entire view or the entirety of \nthe raw traces. When differencing a raw pair of traces produced by a program, we evaluate each pair of \ncorresponding thread views (as determined by XTH ) under -., each producing a different set .. These \nsets are V unioned together to form the .nal similarity set, .f . The .nal set of differences is then \nderived from .f by set subtraction. We have shown our technique to exhibit O(n) complexity in both space \nand time; the proof has been omitted due to space restrictions and is available in a tech report [10]. \n 3.4 Example Fig. 13 shows how the differencing algorithm works for the mo\u00adtivating example shown in \nFig. 1. As each trace evaluation rule removes one or more entries from the head of the traces, the numbered \ncircles show the progression of the evaluation as rules are applied. From the start of the trace up to \nand including the call to the setRequestType method (from circle 0 to 1), the (STEP-VIEW-MATCH) is applicable, \nadding these entries to set .. At circle 1, the (STEP-VIEW-NOMATCH) rule becomes applicable. t 1 and \nt3 correspond to entry 9 in the left and right traces, respectively (the fact that the entry id is the \nsame is coincidental here). LinkedSimilarEntries for (t 1 ,t3) identi.es those entries that should be \nplaced in set . due to the exploration of corresponding secondary views nearby to these entries. The \ntwo secondary views displayed in the .gure represent two such corresponding views. Circles 2 and 3 depict \nthe relevant .xed windows in the secondary views over which the LCS is computed, and the checkmarks in \n  denotes entries that differ. denotes entries placed in set . via evaluation of the (STEP-VIEW-MATCH) \nrule.  denotes entries placed in set . via evaluation of the (STEP-VIEW-NOMATCH) rule. For example, \nthe thread view entry at circle 5 (set NUM-1._maxCharRange = 127) was marked with  due to comparisons \nwithin the NUM-1 object view. these views depict which entries are thereby identi.ed as corre\u00adsponding \n(and thus should be in set .). Entries thus identi.ed as corresponding due to exploration in secondary \nviews are denoted with the anchor symbol in the .gure. Note that even though these entries appear nearby \nto the current positions in the thread views in this example, in large traces these entries identi.ed \nas similar from secondary views could be thou\u00adsands or hundreds of thousands of trace entries away. In \nthis way exploration of secondary views allows for recognizing semantically correlating events that could \nbe very far apart in the thread views. This characteristic is one reason this approach allows for greater \naccuracy than LCS, as this approach remains resilient to reorder\u00adings of operations in the thread views \nwhereas LCS identi.es these reorderings as differences. With LinkedSimilarEntries fully formed for the \ngiven point (t1 ,t 3), the antecedent of (STEP-VIEW-NOMATCH) identi.es the next closest point of correspondence \n(circle 5) in the left and right thread views (t 2 ,t4) and removes all entries from the heads of the \ntraces up to but not including this next point of correspondence (circle 4). Evaluation thus proceeds \nalternating between these two rules un\u00adtil the end is reached: at circle 5, (STEP-VIEW-MATCH) is applied, \nplacing entry 11 (left) and entry 12 (right) into set . and pro\u00adceeding to circle 6. Here (STEP-VIEW-NOMATCH) \nis applied again, LinkedSimilarEntries is formed again for this unique point (thereby exploring nearby \nsecondary views that correspond), and identifying the next point of correspondence as the position at \ncir\u00adcle 8. The (STEP-VIEW-MATCH) is then applied for all remaining traces until the end is reached at \ncircle 9. 4. Regression Cause Analysis We envision many types of dynamic analyses bene.ting from our \nviews trace abstraction and our views-based trace differencing se\u00admantics, including object protocol \ninference, property checking (e.g., typestate), impact analysis, and automated debugging. As mentioned \nwe focus our attention in this paper on how semantics\u00adaware trace differencing empowers regression-cause \nanalysis. 4.1 Algorithm To identify the causes of regressions, we employ our semantics\u00adaware trace differencing \nto identify all the semantic differences between an original, non-regressing version and a new, regressing \nversion of a program for one or more regressing test cases. Let A represent this set of semantic differences, \ntermed the suspected differences set. The size of set A is usually too large to be effective for .nding \nthe regression cause through manual inspection (having in practice hundreds or thousands of differences). \nThe goal of the analysis algorithm is thus to remove from set A those differences that are not likely \nto have caused the regressing behavior. The algorithm proceeds as follows:  1. A set of differences \nthat are expected to occur under correct execution between the original and the new version is built \n(call this set B), by comparing execution traces for runs of the two versions for test case(s) with correct \nbehavior. These differences are not likely to be related to the cause of the regression, because the \nregressing behavior was not observed during execution of these correct test case(s). This set B is the \nexpected differences set. 2. A set of differences for the new version between a correct test case and \nthe regressing test case is built (C). This set of differences includes the difference(s) that cause \nthe regressing behavior. The correct test case should be similar in functionality to the regressing test \ncase so that the resulting set of differences is small and yet still includes the difference(s) causing \nthe regression (this is similar to the requirements of [21] and can be computed automatically using techniques \nfrom [22]). This set C is termed the regression differences set. 3. The set of differences between the \noriginal and new version that are highly likely to be responsible for the cause of the regression, termed \nD, is now calculated as follows:  D =(A - B) n C The choice of set B impacts the effectiveness of this \napproach because the cause for a regression can appear within the execution trace for non-regressing \ntest cases. Eliminating the differences may thereby eliminate the cause, introducing false negatives. \nIn practice, however, we .nd that .ltering differences as described does not compromise accuracy for \nthree reasons. First, our dynamic traces are complete, so the cause of the regression must be in A, implying \nthere are no false negatives at this stage. Second, elim\u00adinating the expected differences (B) eliminates \nmany false posi\u00adtives, increasing accuracy. Since these differences did not exhibit within the program \noutput, it is highly likely they are related to correct program evolution instead. Third, intersecting \nwith the re\u00adgression differences set (C) further eliminates false positives. As C only contains differences \nwithin the new program version, there are fewer differences (only those caused by the differing inputs \nof the regressing and non-regressing test cases and excludes any dif\u00adferences due to program evolution), \nand yet is still likely to contain the regression cause. The other source of false negatives is due to \nAnC. If the regression is caused by the removal of code in the new version, then there is no possibility \nfor set C to contain the regression-inducing differences. For these cases, the analysis can be changed \nto: D =(A - B) - C Subtracting set C for these cases will further reduce false positives without introducing \nfalse negatives, allowing the analysis to effec\u00adtively .nd regression causes due to the removal of code \nin newer versions. We empirically evaluate the effectiveness of our approach in practice in Section 5. \n 4.2 Example Revisited Recall that a regression is caused in our example when the class BinaryCharFilter \nis instantiated with the incorrect range of [1..127] instead of [32..127]. The most interesting code \nfragments related to this regression are depicted in Fig. 1. Even though there are many differences in \nthe new version of the code, only seven are relevant to the regression and its cause. Our tool correctly \nidenti.es these seven changes, with no false positives. It also correctly identi.es 20 other runs of \ndifferences as not being related to the regression. To achieve these results we created two test cases: \n(a) a test case that reproduced the regression, and (b) a test that used a different document type, so \nconversion of the characters was not applied in both versions (and thus does not exhibit the regressing \nbehavior). First, we collected dynamic traces for the original and new versions for both test cases. \nNext, the semantic differencing tool was run on the following pairs of traces: (i) original version vs. \nnew version for the regressing test case (forms the suspected differences set); (ii) original version \nvs. new version for the non-regressing test case (forms the expected differences set); (iii) a new version \nof the non\u00adregressing test case vs. a new version of the regressing test case (this forms the regression \ndifferences set). 5. Evaluation RPRISM employs aspect-oriented programming (AOP) [12] to dynamically \ninstrument Java programs via AspectJ s load-time weaver. Pointcuts provide a .exible mechanism to capture \nthe seg\u00adments of the execution trace that should be recorded. The imple\u00admentation is layered and provides \nthe following features: trace seg\u00admentation, call stack tracking, event recording, view construction, \ntrace serialization, and tracing control. Tracing of long-running programs are accomodated through smart \ntrace segmentation AspectJ pointcuts are used to specify relatively short regions of program execution \nto record as a single trace segment, and once a trace segment has .nished executing, all trace data is \nof.oaded to disk and the associated tracing memory is reclaimed. While the program is running the execution \ntrace is collected but not ana\u00adlyzed the analysis is performed of.ine after the trace data has been \nserialized to disk. RPRISM implements the view correlation of Section 3.1, and also relaxes method and \nobject view correlation (to be more tolerant to refactorings) using a context-sensitive correlation function \nthat correlates views if their entries are the same distance (number of trace entries) away from two \npoints in the traces that are known to be semantically correlated to each other. This relaxation improves \nrobustness in the presence of refactorings, such as if methods or classes have been renamed, or methods \nhave been split or com\u00adbined. For example, if a method has been renamed, then there will be a difference \nat the point of the call site, but it is probable that there will be call sites in the versions where \neither the immedi\u00adately preceding or succeeding statement is semantically correlated. Under the relaxation, \nthe secondary views will be explored at the point of the call site of the renamed method. When the secondary \nviews are analyzed, the (mostly) unchanged code in the method whose name was refactored produces many \nnew semantic correla\u00adtions back in the main view, thus providing tolerance to the method rename refactoring. \nRPRISM approximates the value representations in our formalism using the Java hashCode and toString (truncated \nto 128 chars) methods, which we found is a good approximation in practice, providing both reasonable \nperformance and accuracy. Note that if an object uses the default java.lang.Object implementation of \nthese methods, the value representation is forced to be empty as it is not meaningful across different \nprogram versions. In this section we .rst quantitatively assess the effectiveness of our semantics-aware \ntrace differencing, comparing it to an optimized version of LCS with respect to both accuracy and performance. \nThe differing software versions for this assessment are based on the iBUGS project [5]. We further evaluate \nour regression-cause analysis by discussing in detail RPRISM s accuracy and perfor\u00admance on four real-life \nregressions. All tests were executed on a server with eight 1.8Ghz dual-core Opteron processors and 32GB \nof RAM. Note that RPRISM is currently single-threaded, with the extra cores only in use during the VM \ns parallel garbage collection.  (a) Accuracy (b) Speedup Figure 14. Quantitative results based on iBugs \nRhino dataset com\u00adparing RPRISM with an optimized LCS implementation. 5.1 Quantitative Assessment Our \ngoal is to assess both (i) the semantics-aware trace differenc\u00ading, and (ii) the regression-cause analysis. \nExisting research bug databases contain few if any real regressions (focusing on explicit bugs instead). \nWithout any immediately suitable benchmark regres\u00adsion bug database, we built on the Rhino dataset in \nthe iBUGS project [5]. The Rhino dataset is a set of 29 bugs from the Mozilla Rhino project,6 which implements \nJavaScript in Java and consists of 304 KLOC (including tests), 242 classes, and 15K tests. Rhino compiles \nJavaScript into an intermediate form, which is then ei\u00adther interpreted or compiled to standard Java \nclasses. Our data here is based on the interpretive mode, as it produced longer and more complex traces, \nbut RPRISM runs equally well with the compiled mode. We integrated RPRISM into the automated build process, \nprovid\u00ading unattended runs over all bugs. We introduced regressions into each post-.x version by either \nusing the actual cause of the bug it\u00adself if the bug was a regression or by using a distribution of root \ncauses that matches the distribution found for semantic bugs in the Mozilla project in an empirical study \n[13]. Root causes considered are categorized as missing features (26.4%), missing cases (17.3%), boundary \nconditions (10.3%), control .ow (16.0%), wrong expres\u00adsions (5.8%), or typos (24.2%). We ensured that \neach injected re\u00adgression caused the test case associated with the bug to fail. We utilized RPRISM to \ntrace the working and regressing versions and to calculate the differences between the traces. As we \nare modeling what RPRISM would provide to developers if it were part of a completely automated build \nprocess, we do not follow the .nal step of manually creating similar non-regressing test cases, which \nwould only increase accuracy further. A developer could complete this .nal step if they wished to further \nre.ne RPRISM output when .xing the regression. 6 http://www.mozilla.org/rhino/ Measurements. To assess \nthe effectiveness of our view-based trace differencing technique we de.ne two measures: accuracy and \nspeedup. Accuracy measures how many semantic correlations are identi.ed with RPRISM vs the number of \ncorrelations identi.ed with the LCS comparison. Accuracy is precisely de.ned using the following formula: \n((totalEntries - rprismNumDiffs)/totalEntries)Accuracy = ((totalEntries - lcsNumDiffs)/totalEntries) \nNote that because RPRISM can identify reorderings of operations it often identi.es fewer semantic differences \nthan LCS (resulting in more semantic correlations), resulting in an accuracy value greater than 100%. \nAn accuracy of 100% in this section should be read as meaning RPRISM identi.ed the same number of semantic \ndiffer\u00adences as in the LCS comparison. Speedup is de.ned as the number of trace entry compare operations \nperformed during the LCS com\u00adparison divided by the number of compare operations performed during comparison \nwith RPRISM. Results. Salient measurements from our experiments are shown in Fig. 14. Most traces were \nbetween 10K and 100K entries, with a few outliers ranging up to 1.9 million entries. Trace size was optimized \nby leveraging AspectJ pointcuts to exclude the internal workings of unrelated code, such as libraries \nand data structures. RPRISM organizes contiguoussets of differences into difference se\u00adquences, thereby \norganizing tool output into comprehensible units. More than two-thirds of the bugs produced less than \n50 difference sequences, with the remainder ranging from 50 to 130 difference sequences. We found that \nthe cause of the divergence is often ob\u00adserved in the .rst handful of differences sequences. If further \nre.ne\u00adments are required, an alternate but non-regressing test case can be created, and then the number \nof difference sequences is typically cut by an order of magnitude or more (see next section). We calculated \nthe precise LCS where possible using an optimized version of the LCS algorithm (common-pre.x/suf.x optimiza\u00adtions). \nFig. 14(a) measures accuracy by comparing the relative number of trace entries marked as different by \neach approach. RPRISM achieves greater than 100% accuracy in all but 3 cases because it is able to make \nsemantically correct correlations (such as detecting moved trace entries) that the LCS is inherently \nnot able to detect. The remaining 3 cases had accuracy greater than 99%. We evaluate RPRISM s ef.ciency \nby measuring speedup relative to the number of compare operations (Fig. 14(b)). The LCS approach failed \non traces longer than 100K entries (due to memory exhaus\u00adtion), whereas RPRISM successfully analyzed \ntraces as long as 1.9 million entries. RPRISM achieved speedups of more than 100x vs the LCS algorithm. \nFor two very small traces RPrism had speedups less than 1x, because of the extra comparisons in secondary \nviews. Observed wall clock times for trace differencing are also reason\u00adable running time of the algorithm \ntook less than 1 second in all cases, except for the trace with 1.9 million entries, which took 110 seconds. \nNote that the histograms include data for only 14 of the 29 bugs in the iBugs Rhino suite, for the following \nreasons: Two bugs were not runnable due to a problem with the iBugs distribution. The LCS algorithm failed \ndue to memory exhaustion for 3 bugs. The remaining bugs had trouble generating tracing data, due to problems \nwith the AspectJ weaver (invalid bytecode was produced by the AspectJ weaver).  5.2 Real-life Regressions \nTo assess RPRISM s utility in identifying regression causes, we analyze four previously documented regressions \nin signi.cantly  LCS-based Differencing Views-based Differencing Benchmark LOC Trace Tracing Num Diff. \nRegression False False Analysis Mem Num Diff. Regression False False Analysis Mem Speedup Entries Secs. \nDiffs. Seqs. Diff. Seqs. Pos. Neg. Secs. (GB) Diffs. Seqs. Diff. Seqs. Pos. Neg. Secs. (GB) Daikon 169K \n15K 185 352 43 3 0 1 44 0.85 179 42 3 0 1 3.4 0.07 12.9x Xalan-1725 365K 98K 90 2,338 145 0 0 1 1,515 \n26.9 1,197 296 1 0 0 18.3 0.10 82.8x Xalan-1802 286K 44K 99 4,269 117 11 0 0 582 7.43 3,602 125,562 184 \n2,663 10 6 0 4 0 0 61.5 80.1 0.21 0.34 9.4x - Derby-1633 720K 337K 182 (out of memory failure at 32GB) \n Table 1. Benchmark and analysis characteristics (time/memory are median of 3 runs). Number of Views \nSize of Analysis Sets Benchmark Total views Thread views Method views Target object views A B C D Daikon \n559 1 127 431 42 NA 22 3 Xalan-1725 1,679 1 446 1,232 296 243 113 1 Xalan-1802 1,811 1 497 1,313 184 \n183 10 10 Derby-1633 6,874 3 1,761 5,110 2,663 4 310 10 Table 2. Number of views (in the original program \nversion only) and size of the sets in the regression-cause analysis algorithm for our benchmarks. Set \nA is the suspected differences set, set B is the expected differences set, set C is the regression differences \nset, and set D is the result of the analysis. sized open-source software projects, namely Daikon [6], \nApache Xalan7 (2 regressions), and Apache Derby.8 Our reasons for choos\u00ading these regressions are as \nfollows: The Daikon regression was chosen because this exact same regression was also evaluated by JUnit/CIA \n[17], providing a comparison to a previously established method for regression-cause analysis. The .rst \nXalan regression was chosen because it involved an extreme separation of cause and effect, as the cause \nis within a dynamic bytecode compiler and the visible effects are not exhibited until the compiled bytecode \nis exe\u00adcuted. The second Xalan regression was chosen because the cause of the regression was in a completely \nrearchitected module in the code, and exhibited a large amount of code churn in general (79K lines); \nwe wanted to observe how this level of code churn would affect accuracy. The Derby regression was chosen \nbecause it in\u00advolved multiple threads, a larger code base (2x), and offered larger, longer-running traces \n(3x) than the other regressions. The versions to use when analyzing each regression were deter\u00admined \nas follows: With Daikon, we used the versions as published in the evaluation of JUnit/CIA. For the others, \nwe chose the last publicly released version that was working correctly, and the .rst publicly released \nversion after the regression was reported but not yet .xed (at least 5 months between the versions chosen \nin all cases). Table 1 summarizes characteristics of the benchmarks and our anal\u00adysis results. For comparison, \nwe present the results of the regression analysis based on both the LCS-based and view-based differenc\u00ading \nsemantics. Num Diffs. states the number of distinct differences identi.ed. Diff. Seqs. states how many \ndifference sequences (each representing one higher-level semantic difference that manifests as a contiguous \nset of differences) were formed from the raw differ\u00adences. Regression Diff. Seqs. states how many of \nthese sequences were identi.ed by RPRISM as being regression related. False Pos. states the number of \nsemantic differences incorrectly identi.ed as regression-related by RPRISM; False Neg. states the number \nof regression-inducing differences (as identi.ed by the developers in the bug description) that exist \nbetween the non-regressing and re\u00adgressing versions that were not identi.ed. Speedup is based on wall \nclock time of the differencing analysis. Table 2 summarizes the number of views and the sizes of the \nregression analysis sets. Note that the contents of sets A, B, C, and D can all be very different, which \nis why |D| can be much smaller than |C| and why |D| can be larger than |A|-|B|. 7 http://xml.apache.org/xalan-j/ \nDaikon. Daikon is an extensible tool for dynamically detecting likely program invariants (169 KLOC, 1100 \nclasses). We revisit a regression .rst considered in the evaluation for JUnit/CIA [17]. This regression \nis exhibited by an outdated version of the testXor test case. The regression was caused by changes in \ntwo meth\u00adods in class daikon.diff.XorVisitor, namely shouldAddInv1 and shouldAddInv2 [17]. The older \ntestXor version was selected as the regressing test case and the newer testXor version was selected as \nthe non-regressing test case. Note that Daikon took the longest to trace even though it produced the \nshortest traces because the test case involved the most number of distinct classes, resulting in 98% \nof the time for tracing spent performing aspect weaving. Out of 42 difference sequences, RPRISM identi.ed \n3 of these as relating to the regression; 2 of these differences correctly identi\u00ad.ed changes in the \nshouldAddInv2 method as the regression cause, although the change in shouldAddInv1 was not identi.ed \n(a false negative). The other difference was related to the effect of the re\u00adgression but not the causes. \nNotably, RPRISM was more precise than JUnit/CIA for this regression. Whereas JUnit/CIA also cor\u00adrectly \nidenti.ed the changed methods that caused the regression, it also labeled 2 other changes as Red (highly \nlikely to be the cause), and 31 other changes as Yellow (changes that might be a cause) [17]. Apache \nXalan. Xalan is an implementation of XSLT, an XML transformation language (365 KLOC, 1500 classes). We \nconsider a bug from Xalan s bug database, XALANJ-1725,9 involving a regression from version 2.5.1 to \n2.5.2. Version 2.5.2 incorporates 4 months of code changes, including 84 feature enhancements and bug \n.xes. The cause of the regression is quite dif.cult to pinpoint because the bug is in the XSLT compiler \n(which generates Java bytecode). This is an extreme case of separation of cause and effect, as the former \nlies in incorrectly generated bytecode, so the latter only manifests upon execution of that bytecode. \nThis makes it extremely dif.cult for any static analysis technique to accurately understand and identify \nprecisely both cause and effect. The bug report provided an XSLT .le that was correct on version 2.5.1 \nbut not on version 2.5.2. To obtain a similar non-regressing test case, we modi.ed the XSLT .le and removed \nthe small section of the .le that was causing incorrect behavior while leaving the remainder of the .le \nthe same, and it was constructed without foreknowledge of the regression cause. Alternatively, automated \ntechniques could be applied to construct the alternate test case [22]. 8 http://db.apache.org/derby/ \n9 https://issues.apache.org/jira/browse/XALANJ-1725 Views-based differencing was more precise than LCS, \nas it only produced about half as many differences as LCS produced. Note that the number of difference \nsequences is larger for views\u00adbased differencing because there are more similarities interspersed among \nthe differences. Consequently, the views-based difference sequences tended to be shorter and more concise, \nas evidenced by the average number of differences per sequence (4.04 vs 16.12 for LCS). This trend is \nalso true for all other bugs where LCS was com\u00adputable, with RPRISM producing .ner-grained and thereby \nmore precise results. This .ner granularity allowed the views-based dif\u00adferencing to precisely identify \nthe regression cause of this bug, whereas the LCS-based differencing failed to produce any regres\u00adsion \ndifferences. RPRISM identi.ed 296 semantic differences between the origi\u00adnal and new versions for the \nregressing test case. After apply\u00ading our regression cause analysis algorithm the suspected dif\u00adferences \nset was reduced to 1 difference. This identi.ed differ\u00adence is in the checkAttributesUnique method and \ncalled by the LiteralElement.translate method, which was identi.ed as the regression cause in the Xalan \nbug database. Xalan-1802. We consider another regression in Xalan exhibited between versions 2.4.1 and \n2.5.1 (12 months, 79K new or changed lines of code, 97 bug.xes/feature changes).10 As before we gener\u00adated \na similar, non-regressing input .le from the regressing input .le provided with the bug. In this case \nthe regression was caused not by small incremental changes but by a bug for a corner case in a completely \nre-architected portion of the code relating to names\u00adpaces. Note that the views-based analysis took more \ntime for this bug than the other Xalan bug even though the traces were half the size, because there were \nmore differences in the main views, and consequently more secondary views had to be explored during anal\u00adysis. \nNote that this trend does not apply to the LCS-based approach, whose running time is approximately quadratic \nwith respect to the trace size (sometimes less because of optimizations). Apache Derby. Derby is a mature \nJava implementation of a re\u00adlational database system. We consider a regression from version 10.1.2.1 \nto 10.1.3.1 relating to query predicates and subqueries, as documented in Derby bug DERBY-1633.11 The \nbug descrip\u00adtion provided a regressing SQL query and sample database. We formed the alternate test case \nby modifying the predicate causing the regression in the SQL query. Both code size and trace size here \nare substantially larger than for the other regressions we consid\u00adered. Of unique note here is that Derby \nis multithreaded and gen\u00aderated multiple thread views during tracing. Our views trace ab\u00adstraction allowed \nfor proper analysis and elimination of behavior on other threads not related to the regression. The bug \nwas caused by an incomplete corner case in new query optimizations intro\u00adduced in version 10.1.3.1. The \nlarge number of differences (125K) was caused by observing version 10.1.2.1 executing the query vs 10.1.3.1 \nthrowing an error during query compilation. The analysis algorithm was able to effectively eliminate \nthese regression side\u00adeffects and non-relevant differences, instead identifying 6 differ\u00adence sequences \nall directly related to the regression (as con.rmed by reviewing the posted code patch for the regression). \nFour false positives were also observed, relating to differences from use of database locks that were \nnot related to the regression cause. 10 https://issues.apache.org/jira/browse/XALANJ-1802 11 https://issues.apache.org/jira/browse/DERBY-1633 \n 5.3 Impact of Code Refactorings on Accuracy of RPRISM Refactorings and greater chronological distance \nbetween the work\u00ading and regressing version certainly increase the size of the sus\u00adpected differences \nset (set A). The differences due solely to refac\u00adtoring or other modi.cations that are not regression-related \nare modeled by the expected differences set (set B) (difference be\u00adtween non-regressing test cases for \nthe two versions). When set B is subtracted from set A, this removes most of these unrelated differences. \nFurthermore, intersection with the regression differ\u00adences set (between similar regressing and non-regressing \ntests on the same version) also serve to further eliminate other unrelated refactoring changes. Our evaluation \nshows that even in cases where there were months of active development and lots of code churn RPRISM \neffectively identi.ed the regression cause with precision. For example, Xalan-1802 exhibited 12 months \nof active develop\u00adment and 79K new or changed lines of code between versions, and RPRISM correctly trimmed \nthe suspected differences set down from 184 difference sequences to 10 difference sequences. 6. Related \nWork Dynamic program slicing [1] is a technique that identi.es all state\u00adments that directly or indirectly \naffect a variable s value for given program inputs. This produces far more information than the hand\u00adful \nof differences produced by RPRISM. In dynamic slicing the number of statements is often measured as a \npercentage of exe\u00adcuted statements, with percentages in the 0.1% to 1% range being considered excellent \n(e.g., see [19]). By this measure, the results for RPRISM are 0.02% (Daikon), 0.001% (Xalan-1725), 0.0035% \n(Xalan-1802), and 0.003% (Derby-1633). Execution indexing [20] is a technique that can be used to corre\u00adlate \nrelated program points between executions. It leverages prop\u00aderties of an execution s dynamic state based \non its nesting struc\u00adture (loops, calls, etc.) to uniquely label program points. Our view\u00adbased projections \ncan be regarded as a form of execution indexing that correlates events across different executions based \non causal semantic properties (e.g., order of method calls, object allocations, object .eld reads and \nwrites, etc.). Rather than using full context in\u00adformation to determine an index, we use anchor points \nderived from a derivative of an LCS calculation on program traces; our technique is especially well-suited \nfor regression analysis over program ver\u00adsions. In general, there has been substantial work on software \nfault localization using dynamic execution traces that are also related to our work. These approaches \nemploy a variety of techniques in\u00adcluding statistical machine learning [14], program slicing [24, 18] \ncontrol-.ow similarity metrics [8], or state-space exploration and re.nement [21]. These techniques cannot \nbe easily adapted to iden\u00adtify regression failures between different versions of a program. Pothier et \nal. [15] present a portable Trace-Oriented Debugger for Java which uses ef.cient instrumentation techniques \nfor event gen\u00aderation and a scalable storage system for completeness and ef.cient querying. RPRISM is \na complementary infrastructure for regres\u00adsion cause analysis. While RPRISM could query Trace-Oriented-Debugger \nto effectively construct views, we present an approach for implementing tracing using AspectJ, which \nautomatically pro\u00advides semantic information by identifying trace boundaries. JDiff [2] is a tool for \nidentifying the differences between two Java programs. In their approach, the authors use method level \nrepre\u00adsentation to model object-oriented features, build a representation for pairs of matching methods, \nand subsequently identify the dif\u00adferences/similarities across methods. RPRISM operates at a .ner granularity \nto detect such differences, and is robust even in the pres\u00adence of concurrency, re.ection, dynamic class \nloading and other advanced language features.  CHIANTI is a tool for change impact analysis of Java \nprograms by Ren .et al [16]. It identi.es a set of changes responsible for a mod\u00adi.ed test s behavior \nand the set of tests that are affected by a mod\u00adi.cation. The differences between two versions are decomposed \ninto a set of atomic changes and, based on static or dynamic call graph sequences, the above mentioned \ndetails are estimated. This system is extended in JUNIT/CIA [17] to classify which atomic changes are \nlikely to have caused speci.c test cases to fail. Unlike RPRISM, this approach requires source code and \nis not well-suited for cases involving dynamic code generation (such as in our Xalan case study). DSD-Crasher \n[4] is a proactive bug-.nding technique that auto\u00admatically .nds bugs via a three-phase approach: (i) \nautomatically computing program invariants via dynamic analysis, (ii) statically analyzing the code to \nlook for possible execution paths where in\u00advariants fail, (iii) validating these potential failures through \nauto\u00admatic test case generation. 7. Conclusion This paper presents a novel view-based technique for tractably \ncomparing large execution traces in semantically meaningful ways. We have illustrated the usefulness \nof this technique through the problem of precisely identifying deeply buried causes of regres\u00adsions introduced \nin evolving complex applications. We have presented an automated regression cause analysis algo\u00adrithm \nbased on our technique. RPRISM, an implementation of these ideas, was applied to large, real-world Java \napplications, and was able to identify the cause of regressions with a high degree of preci\u00adsion, even \nwhen the applications employ multi-threading, dynamic code generation, and re.ection, features that confound \npreviously proposed analyses. Acknowledgments We would like to thank Murali Krishna Ramanathan and the \nanony\u00admous reviewers for their time and effort spent improving this paper. References [1] H. Agrawal \nand J.R. Horgan. Dynamic Program Slicing. In PLDI 90, pages 246 256, 1990. [2] T. Apiwattanapong, A. \nOrso, and M.J. Harrold. JDiff: A Differencing Technique and Tool for Object Oriented Programs. ASE 07, \n14(1):3 36, 2007. [3] L. Bergroth, H. Hakonen, and T. Raita. A Survey of Longest Common Subsequence Algorithms. \nIn SPIRE 00, page 39, 2000. [4] C. Csallner and Y. Smaragdakis. DSD-Crasher: A Hybrid Analysis Tool for \nBug Finding. In ISSTA 06, pages 245 254, 2006. [5] Valentin Dallmeier and Thomas Zimmermann. Extraction \nof Bug Localization Benchmarks from History. In ASE, pages 433 436, 2007. [6] M. Ernst, J. Cockrell, \nW. Griswold, and D. Notkin. Dynamically Dis\u00adcovering Likely Program Invariants to Support Program Evolution. \nTSE, 27(2):1 25, 2001. [7] Matthew Flatt, Shriram Krishnamurthi, and Matthias Felleisen. Classes and \nMixins. In Proceedings of the 25th ACM SIGPLAN-SIGACT symposium on Principles of Programming Languages, \npages 171 183. ACM Press, 1998. [8] Liang Guo, Abhik Roychoudhury, and Tao Wang. Accurately Choosing \nExecution Runs for Software Fault Localization. In CC, pages 80 95, 2006. [9] D. S. Hirschberg. A Linear \nSpace Algorithm for Computing Maximal Common Subsequences. CACM, 18(6):341 343, 1975. [10] Kevin Hoffman, \nPatrick Eugster, and Suresh Jagannathan. RPrism: Ef.cient Regression Analysis Using View-Based Trace \nDifferencing. Technical Report dynt-200811-1, Purdue University, 2008. [11] Atsushi Igarashi, Benjamin \nC. Pierce, and Philip Wadler. Feath\u00aderweight Java: A Minimal Core Calculus for Java and GJ. ACM TOPLAS, \n23(3):396 450, May 2001. [12] G. Kiczales, J. Lamping, A. Menhdhekar, C. Maeda, C. Lopes, J.-M. Loingtier, \nand J. Irwin. Aspect-Oriented Programming. In ECOOP 97, pages 220 242, 1997. [13] Zhenmin Li, Lin Tan, \nXuanhui Wang, Shan Lu, Yuanyuan Zhou, and Chengxiang Zhai. Have Things Changed Now?: An Empirical Study \nof Bug Characteristics in Modern Open Source Software. In ASID, pages 25 33, 2006. [14] Ben Liblit, Alex \nAiken, Alice X. Zheng, and Michael I. Jordan. Bug Isolation via Remote Program Sampling. In PLDI 03, \npages 141 154, 2003. [15] G. Pothier, E. Tanter, and J. Piquer. Scalable Omniscient Debugging. In OOPSLA \n07, pages 535 552, 2007. [16] X. Ren, F. Shah, F. Tip, B. Ryder, and O. Chesley. Chianti: A Tool for \nChange Impact Analysis of Java Programs. In OOPSLA 04, pages 432 448, 2004. [17] M. Stoerzer, B. Ryder, \nX. Ren, and F. Tip. Finding Failure-Inducing Changes in Java Programs Using Change Classi.cation. In \nESEC\u00adFSE-14, pages 57 68, 2006. [18] F. Tip. A Survey of Program Slicing Techniques. Journal of programming \nlanguages, 3:121 189, 1995. [19] G. A. Venkatesh. Experimental results from dynamic slicing of C programs. \nACM Transactions on Programming Languages and Systems, 17(2):197 216, 1995. [20] Bin Xin, William N. \nSumner, and Xiangyu Zhang. Ef.cient Program Execution Indexing. In PLDI 08, pages 238 248, 2008. [21] \nAndreas Zeller. Isolating Cause-Effect Chains from Computer Programs. In FSE-10, pages 1 10, 2002. [22] \nAndreas Zeller and Ralf Hildebrandt. Simplifying and Isolating Failure-Inducing Input. TSE, 28(2):183 \n200, 2002. [23] X. Zhang and R. Gupta. Cost Effective Dynamic Program Slicing. In PLDI 04, pages 94 106, \n2004. [24] X. Zhang and R. Gupta. Matching Execution Histories of Program Versions. In ESEC/FSE-13, pages \n197 206, 2005.    \n\t\t\t", "proc_id": "1542476", "abstract": "<p>As computer systems continue to become more powerful and complex, so do programs. High-level abstractions introduced to deal with complexity in large programs, while simplifying human reasoning, can often obfuscate salient program properties gleaned from automated source-level analysis through subtle (often non-local) interactions. Consequently, understanding the effects of program changes and whether these changes violate intended protocols become difficult to infer. Refactorings, and feature additions, modifications, or removals can introduce hard-to-catch bugs that often go undetected until many revisions later.</p> <p>To address these issues, this paper presents a novel dynamic program analysis that builds a <i>semantic view</i> of program executions. These views reflect program abstractions and aspects; however, views are not simply projections of execution traces, but are linked to each other to capture semantic interactions among abstractions at different levels of granularity in a scalable manner.</p> <p>We describe our approach in the context of Java and demonstrate its utility to improve <i>regression analysis</i>. We first formalize a subset of Java and a grammar for traces generated at program execution. We then introduce several types of views used to analyze regression bugs along with a novel, scalable technique for semantic differencing of traces from different versions of the same program. Benchmark results on large open-source Java programs demonstrate that semantic-aware trace differencing can identify precise and useful details about the underlying cause for a regression, even in programs that use reflection, multithreading, or dynamic code generation, features that typically confound other analysis techniques.</p>", "authors": [{"name": "Kevin J. Hoffman", "author_profile_id": "81541928856", "affiliation": "Purdue University, West Lafayette, IN, USA", "person_id": "P1464354", "email_address": "", "orcid_id": ""}, {"name": "Patrick Eugster", "author_profile_id": "81100562902", "affiliation": "Purdue University, West Lafayette, IN, USA", "person_id": "P1464355", "email_address": "", "orcid_id": ""}, {"name": "Suresh Jagannathan", "author_profile_id": "81100208907", "affiliation": "Purdue University, West Lafayette, IN, USA", "person_id": "P1464356", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1542476.1542527", "year": "2009", "article_id": "1542527", "conference": "PLDI", "title": "Semantics-aware trace analysis", "url": "http://dl.acm.org/citation.cfm?id=1542527"}