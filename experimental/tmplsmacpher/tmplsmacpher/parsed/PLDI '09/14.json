{"article_publication_date": "06-15-2009", "fulltext": "\n Parallelizing Sequential Applications on Commodity Hardware using a Low-cost Software Transactional \nMemory Mojtaba Mehrara Jeff Hao Po-Chun Hsu Scott Mahlke Advanced Computer Architecture Laboratory University \nof Michigan Ann Arbor, MI 48109 {mehrara,je.hao,pchsu,mahlke}@umich.edu Abstract Multicore designs have \nemerged as the mainstream design paradigm for the microprocessor industry. Unfortunately, providing multiple \ncores does not directly translate into performance for most appli\u00adcations. The industry has already fallen \nshort of the decades-old performance trend of doubling performance every 18 months. An attractive approach \nfor exploiting multiple cores is to rely on tools, both compilers and runtime optimizers, to automatically \nextract threads from sequential applications. However, despite decades of research on automatic parallelization, \nmost techniques are only ef\u00adfective in the scienti.c and data parallel domains where array dom\u00adinated \ncodes can be precisely analyzed by the compiler. Thread\u00adlevel speculation offers the opportunity to expand \nparallelization to general-purpose programs, but at the cost of expensive hard\u00adware support. In this \npaper, we focus on providing low-overhead software support for exploiting speculative parallelism. We \npro\u00adpose STMlite, a light-weight software transactional memory model that is customized to facilitate \npro.le-guided automatic loop paral\u00adlelization. STMlite eliminates a considerable amount of checking and \nlocking overhead in conventional software transactional mem\u00adory models by decoupling the commit phase \nfrom main transac\u00adtion execution. Further, strong atomicity requirements for generic transactional memories \nare unnecessary within a stylized automatic parallelization framework. STMlite enables sequential applications \nto extract meaningful performance gains on commodity multicore hardware. Categories and Subject Descriptors \nD.3.3 [Programming Lan\u00adguages]: Language Constructs and Features Concurrent program\u00adming structures; \nD.3.4 [Programming Languages]: Processors Code generation, Compilers General Terms Languages, Algorithms, \nDesign, Performance Keywords Software transactional memory, Automatic paralleliza\u00adtion, Thread-level \nspeculation, Loop level parallelism, Pro.le\u00adguided optimization Permission to make digital or hard copies \nof all or part of this work for personal or classroom use is granted without fee provided that copies \nare not made or distributed for pro.t or commercial advantage and that copies bear this notice and the \nfull citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. PLDI 09, June 15 20, 2009, Dublin, Ireland. \nCopyright c &#38;#169; 2009 ACM 978-1-60558-392-1/09/06. . . $5.00 1. Introduction As the scaling of \nclock frequency and complexity of uniproces\u00adsors has reached physical limitations, the industry has turned \nto multicore designs. Example systems include special purpose pro\u00adcessors like the Sony/Toshiba/IBM Cell \nprocessor that consists of 9 cores [20], the NVIDIA GeForce 8800 GTX that contains 16 streaming multiprocessors \neach with eight processing units [30], and the Cisco CRS-1 Metro router that utilizes 192 Tensilica pro\u00adcessors \n[12] and more general purpose processors including the Sun UltraSparc T1 that has 8 cores [22]. Furthermore, \nIntel and AMD are producing quad-core x86 systems today and larger systems are on their near term roadmaps. \nOne of the most dif.cult challenges going forward is software: if the number of devices per chip contin\u00adues \nto grow with Moore s law, can the available hardware resources be converted into meaningful application \nperformance gains? Mul\u00adtiple cores readily help where threads are plentiful, such as web servers. However, \nthey provide little or no gains for sequential ap\u00adplications. In fact, performance of sequential applications \nmay suf\u00adfer due to the use of simpler cores and smaller caches per core. Many new languages have been \nproposed to ease the burden of writing parallel programs, including Atomos [5], Cilk [14], and StreamIt \n[41]. Despite these and other languages, the effort in\u00advolved in creating correct and ef.cient parallel \nprograms is still far more substantial than writing the equivalent single-threaded ver\u00adsion. Developers \nmust be trained to program and debug their ap\u00adplications with the additional concerns of deadlock, livelock, \nand race conditions. Converting an existing single-threaded application is often more challenging, as \nit may not have been developed to be easily parallelized in the .rst place. The lack of necessary com\u00adpiler \ntechnology is increasingly apparent as the push to run general\u00adpurpose software on multicore platforms \nis required. Techniques for parallelizing Fortran programs [3, 8, 15] usu\u00adally target counted loops that \nmanipulate array accesses with af.ne indices, where memory dependence analysis can be precisely per\u00adformed. \nUnfortunately, these techniques do not often translate well to C and C++ applications. These applications, \nincluding those in the scienti.c and media processing domains, are much more dif.\u00adcult for compilers \nto analyze due to the extensive use of pointers, recursive data structures, and dynamic memory allocation. \nMore sophisticated memory dependence analysis, such as points-to anal\u00adysis [31], can help, but parallelization \noften fails due to unresolv\u00adable memory accesses. Thread-level speculation (TLS) offers an opportunity \nfor paral\u00adlelizing C and C++ applications. With TLS, the architecture allows optimistic execution of \ncode regions before all values are known [16, 21, 40, 45]. Hardware and/or software structures track \nreg\u00adister and memory accesses to determine if any dependence vio\u00adlations occur. In such cases, register \nand memory state are rolled back to a previous correct state and sequential re-execution is ini\u00adtiated. \nWith TLS, the programmer or compiler can delineate re\u00adgions of code believed (but not provably) to be \nindependent and amenable to parallelization [7, 11, 25, 27]. Pro.le data is often utilized during this \nprocess. The POSH compiler is an excellent example where TLS yielded approximately 1.3x speedup for a \n4\u00adway CMP on SPECint2000 benchmarks [25]. More recent work has shown that additional loop-level parallelism \nis covered up by a small number of register and control dependences, but can be unlocked with several \ndependence breaking transformations [44]. Outer-loop pipeline parallelism has also been identi.ed as \na key parallelization opportunity. Bridges et al. report a geometric mean of 5.5x gain on SPECint2000 \n(with variable number of threads up to 32) using decoupled software pipelining [4]. Proponents of TLS \nadvocate hardware support for speculation generally in the form of transactional memory or similar tech\u00adniques \n[16, 40]. Bulk tracking of memory dependences using signa\u00adtures along with dedicated structures for managing \nspeculative state provide an ef.cient environment for TLS [6]. However, the cost and complexity of implementing \nhardware or hybrid hardware/software TMs are high. With the notable exception of the Sun Rock pro\u00adcessor, \nhardware support for TLS has not made it into mainstream multicore systems yet. Alternatively, software \nTMs, or STMs, offer the opportunity for TLS support without any dedicated hardware. The .rst STM by Shavit \net al. maintained read and write access locations in order to roll back in case of a transaction abort \n[35]. Many other works [19, 17, 26, 32, 10] proposed different forms of STM to tackle various performance \nand correctness issues involved in the STM paradigm. However, these STM implementations are far too expen\u00adsive \nin terms of run-time overhead. For parallel applications, STMs typically result in visible slowdowns \nof 2x or more. The problem is even worse for compiler parallelized sequential applications where all \nthe gains and more are typically wiped out by the STM. STMs generally focus on .exibility to support \na wide variety of transactions and scalability to enable many concurrent threads. STM control is fully \ndistributed to the running threads. In this pa\u00adper, we take the opposite approach by introducing STMlite, \na lean and ef.cient STM speci.cally customized for compiler paralleliza\u00adtion. With our focus on compiler \nparallelization, the goal is man\u00adaging a modest number of speculative threads (2-8) that a compiler can \nrealistically expect to .nd in C and C++ applications. Further, we focus on tightly integrating the STM \nwith the compiler paral\u00adlelization framework to ensure low overhead. Some requirements of more generic \nSTMs such as strong atomicity [36] and special handling of local variables are not needed in this setting. \nLocks are removed by centralizing the TM bookkeeping on a single, perhaps idle, core. In this manner, \nbookkeeping tasks occur in parallel with transaction execution and the overhead on each work thread is \nmin\u00adimized. Most importantly, centralized control obviates the need for locks and their associated overhead. \nThe obvious downside of cen\u00adtralized control is the lack of scalability, but for a modest number of threads, \nlarge increases in ef.ciency are possible for both paral\u00adlelized and multithreaded applications. This \npaper is organized as follows. In Section 2, we discuss challenges in STM systems and customization opportunities \nbased on our main goal exploiting loop-level parallelism. Section 3 de\u00adscribes STMlite, our proposed \nSTM model. We discuss our par\u00adallelization framework and the interaction between the compiler\u00adgenerated \ncode and STMlite in Section 4. In Section 5, we present our experimental results. Finally, Section 6 \ndiscusses related work and Section 7 concludes the paper.  Figure 1. Single-threaded runtime breakdown \nof a state-of-art STM system on two STAMP transactional benchmarks. 2. Motivation 2.1 Challenges in \nSoftware Transactional Memory Systems STMs have the advantage of requiring no additional hardware to \nrun. However, since it is implemented entirely in software, it entails a large runtime overhead in maintaining \ntransactional state. The high overheads of an STM are due to several reasons. The largest bottleneck \nin STMs is the maintenance and validation of read sets in read-write transactions. These sets keep track \nof every address read by a transaction, and are used to maintain coherence between transactions. For \neach load, the STM has to execute at least one transactional load and revalidate its timestamp when the \ntransaction commits. As transactions read larger amounts of data, this overhead becomes substantial. \nSecondly, global locks are necessary for transactions to write back their .nal correct data. During a \ntransactional store, the address and value are stored into a write set, deferring any change in memory \nuntil commit. This allows transactions to remain coherent with each other, but adds a considerable overhead \nduring commit time for obtaining the locks on these addresses and writing them back to their .nal location. \nThe use of locks in the data write back is expensive as it involves atomic instructions. In order to \nget a better understanding of what the major sources of overhead are in an advanced STM system, we performed \nan ex\u00adperiment on two STAMP benchmarks [28] using a state-of-art STM system -Sun s Transactional Locking \n2 (TL2) [10]. We measured the time spent in each TM component of a single threaded trans\u00adactional execution \nof these benchmarks using the TL2 library. A similar analysis has also been done in [29]. Figure 1 shows \nthe re\u00adsult of this experiment. The vertical axis in these charts shows the execution time normalized \nto the sequential runtime. The vertical bars show the fractions of runtime spent in the main application, \ntransactional commits (TxCommit), transactional stores (TxStore), and transactional loads (TxLoad). The \nchart clearly shows the large overhead of read set mainte\u00adnance in the Vacation benchmark, which has \nlarge transactions with many transactional reads. Keeping track of the read set causes con\u00adsiderable \noverhead, as depicted by the TxLoad portion of each bar. Additionally, the checks required during commit \nto maintain read set coherence are extremely costly [38], representing over half the runtime in Vacation \nwith high contention. For the Kmeans bench\u00admark, the overheads are not as severe because its read sets \nare smaller, but it still exhibits similar behavior. 2.2 Speculation Requirements for Loop Parallelization \nThere are several aspects of STM models that are crucial for cor\u00adrectness in general. However, we can \nloosen some of these limita\u00adtions and requirements in the loop parallelization domain to make the software-based \nspeculation more ef.cient. 1. One of the shortcomings in STM models is the lack of strong atomicity \nguarantees, which raises correctness issues in paral\u00adlel programs. Previous works [1, 36, 33] have addressed \nthe is\u00adsue of strong atomicity in STMs. While being effective, these approaches incur a non-trivial amount \nof complexity or perfor\u00admance overhead on the system. However, using STM for specu\u00adlation in loop parallelization \nobviates the need for strong atom\u00adicity, because the execution consists of at most a single in-.ight \nparallel loop at each point. Since all the code in the loop is run\u00adning inside transactions, there can \nbe no non-transactional code running at the same time as transactional code. 2. Special handling of \nlocal variables in a STM is not required for loop parallelization, because the loop iterations are not \nsup\u00adposed to share any local variables on stack. Otherwise, they cause unresolvable cross iteration dependences, \nwhich prevent loop parallelization to begin with. Therefore, there is no need to have specialized transactional \nloads and stores for local vari\u00adables. 3. Zombie transactions are transactions that have read a stale \nvalue or pointer from memory and have taken an incorrect code path which might lead to an in.nite loop. \nOne of the main sources of zombie transactions are loops with complicated linked-list operations. These \nloops are generally not parallelizable and therefore, we do not need to provide ef.cient and complicated \nways for handling zombies in a STM for loop parallelization. However, to ensure correctness in other \ncases, we provide a mechanism for handling zombies in later sections that does not affect normal execution \nof transactions.  With these challenges in mind, we aim to tackle the two main sources of STM overhead: \nread-set maintenance and lock-based writeback mechanism. In addition, based on the speci.c specula\u00adtion \nrequirements in loop-level parallelism, we make simpli.ca\u00adtions to STMlite that makes it even more ef.cient. \n  3. STMlite In this section, we describe our proposed STM model, STMlite. As was mentioned in Section \n2, in traditional STM models, a consider\u00adable part of the execution time is spent in maintaining auxiliary \ndata structures needed for providing correctness guarantees. In particu\u00adlar, one of the major bottlenecks \nis construction, maintenance, and frequent checking of read logs. The read log structure keeps track \nof the addresses (or objects in object-based implementations) read by each transaction. At transaction \ncommit, these logs are walked over, and each address is checked for consistency. In addition, al\u00adthough \nthe programmer does not have to deal with the subtleties of lock-based programming, thanks to the usage \nof atomic blocks and TM primitives, the performance of the underlying runtime system still suffers from \nthe downsides of using locks in many implementa\u00adtions. In order to address these problems, and as a step \ntowards styl\u00adized customization for speculation used in loop-level paralleliza\u00adtion, we developed a new \nsoftware-based model that eliminates the need for read log maintenance during transaction execution and \nex\u00adplicit locking during memory writebacks. We assign a dedicated software thread for managing the exe\u00adcution \nof the transactions involved in the main computation. This thread, which runs on an individual core, \nis referred to as the Trans\u00adaction Commit Manager (TCM). Having a central commit manager provides an \nenvironment in which the manager is responsible for ensuring that, at any given time, at most one transaction \nis writ\u00ading to a particular memory location. With higher numbers of trans\u00adactions, there can be several \ncoordinating TCMs with each TCM Figure 3. STMlite data structures. Each transaction has an indi\u00advidual \nheader. The TCM has a single commit log and there is a precommit log for each execution core inside the \nTCM.  managing a group of execution transactions (TCM virtualization). In this way, we can avoid having \na single point of serialization in highly parallel applications. The STMlite model essentially consists \nof several execution cores for running individual transactions and a TCM core for main\u00adtaining transactional \nconsistency in the system. In the following subsections, we explain in more detail how each step works. \n3.1 Overview Figure 2 summarizes the operation of STMlite. The top rectangle shows the execution .ow \ninside each transaction. The bottom part is a summary of what happens inside the TCM. Centralized management \nof individual transactions is made pos\u00adsible by using transactional read and write signatures, which \nare essentially hash-based representations of all reads and writes per\u00adformed during execution. Using \nsignatures in hardware was .rst proposed in [6]. However, unlike hardware, hash-based computa\u00adtions can \nbecome quite expensive in software. Therefore, choosing TxLoad(Addr){ if SignatureFind(Addr, Self->wrSig) \nLoad the correct value from the wrSet else Load from memory SignatureInsert(Addr, Self->rdSig) } TxStore(Addr, \nData){ Store Data to the WriteSet SignatureInsert(Addr, Self->wrSig) } Figure 4. Pseudocode for transactional \nloads and stores. the right set of hash functions and the proper size for signatures is crucial in software \nsystems to ensure minimal overhead and few false positives at the same time. In [18], hashing schemes \nare used to remove duplicates in the read-log and undo-logs of the same transaction. However, in order \nto use signatures for con.ict detec\u00adtion between different transactions, a central manager is needed \nto check signatures against each other. In signature-based HTMs [6, 43], this is done in the coherence \nprotocol. Here, it is done by the TCM. Each transaction maintains a transaction header which is shown \nin Figure 3. The transaction header contains some information gathered during transaction execution and \nis used during the com\u00admit process. The main idea behind STMlite is that all transactions compute read \nand write signatures during their execution. At com\u00admit, they copy these signatures to a list called \nthe precommit log (Figure 3). This log is basically a single-reader/single-writer buffer that is read \nby the TCM and written by transactions. Its operation is inspired by the reservation station in traditional \nout-of-order pro\u00adcessors. Committed transactions reside in another data structure called commit log (Figure \n3). The commit log is only updated and read by the TCM. The TCM goes through precommit log entries and \nchecks whether their read signatures have con.icts with the write signa\u00adtures of overlapping already-committed \ntransactions in the commit log. If there is no hash collision, the transaction is noti.ed to start writing \nback its write set. Otherwise, the transaction aborts and restarts its execution. During the write back \nprocess, the TCM is responsible for preventing concurrent writes to the same addresses in memory. TCM \noperation is detailed in Section 3.3. In order to keep track of the relative start and commit times of \ntransactions, we use a global clock mechanism similar to [9]. The TCM increments the global clock value \nwhenever a writing trans\u00adaction commits. We de.ne the start version for each transaction as the value \nof the global clock at transaction start. Likewise, the com\u00admit version is the value of the global clock \nat commit time.  3.2 Transactional Loads and Stores Figure 4 shows the pseudocode for STMlite s transactional \nload and store functions. TxLoad .rst checks the transaction s write signature (wrSig) to see if this \ntransaction has previously written to Addr. If so, it reads the data from the write set (wrSet) and returns. \nIn order to avoid walking through the entire write set when the number of store-to-load forwarding instances \nis high, we added a hash map to each transaction that caches the latest stored addresses and values for \nquick retrieval. Therefore, if Addr is found in the write signature, this hash table is checked before \nwalking through the write set. This helps to lower transactional load overhead in many cases. If the \ntransaction hasn t written to Addr, data is loaded from memory and Addr is inserted into the read signature \n(rdSig). TxStore stores Data to the write set and inserts Addr to the write signature. As can be seen, \nthe only major extra overhead in transactional loads and stores is due to the signature insert and .nd \noperations, TCM() { for entry precommitTX in PrecommitLogs if (precommitTX.Ready) if (ConflictCheck(precommitTX)) \nGrant commit permission to precommitTX else Abort precommitTX } ConflictCheck(precommitTX) { for entry \ncommittedTX in CommitLog { if (precommitTX.startVersion < committedTX.commitVersion) if HashCollision(precommitTX.rdSig, \ncommittedTX.wrSig) return 0; } if !(precommitTX.readOnly){ Go through WBActionList wait for concurrent \nconflicting WBs to finish } return 1; } Figure 5. Commit management in the TCM. though they remain low-cost \nfor moderately sized signatures. Fur\u00adthermore, the signature operations can be inserted in a decom\u00adposed \nfashion, separate form transactional loads and stores, en\u00adabling more aggressive compiler optimizations \nsuch as hoisting the signature calculations out of the loops with the aid of pointer alias analysis. \n 3.3 Transaction Commit Manager As mentioned before, the TCM has two main data structures: the precommit \nlog and the commit log (Figure 3). The commit log keeps track of committed transactions, and the precommit \nlog con\u00adtains transactions waiting to be served by the TCM. In order to reduce contention among transactions, \na separate precommit log is assigned to each core. Figure 5 provides a summary of what hap\u00adpens in the \nTCM during runtime. The TCM constantly polls Ready .ags of precommit log entries (.rst for loop in the \n.gure). When it detects a Ready is set, it reads the transaction s start version and checks it against \nthe commit versions of commit log entries (in the ConflictCheck function). If the start version of the \ncommitting transaction is less than the commit version of a commit log entry, we know that their execution \nhas overlapped at some point in time. Therefore, they should be checked for possible con.icts (in the \nHashCollision function). In case of a hash collision between the signatures, the committing transaction \nis instructed to abort by setting the Abort .ag in its header. If the committing transaction passes the \ncheck against all overlapping commit log entries, it is safe to be committed. This is all that needs \nto be done for read-only transactions. Therefore, the TCM sets the Commit .ag in the transaction header. \nIt is not necessary to copy any information about read-only transactions to the commit log. However, \nthe mechanism is more subtle for writing transactions. Since we want to avoid having individual locks \nfor writing back the write set to memory, the TCM needs to make sure no concur\u00adrent writes are happening \nto the same address during writeback. The TCM uses a secondary structure called the writeback action\u00adlist \n(WBActionList) for this purpose. The action-list has the same number of entries as the active threads \nin the system. At any given time, it contains the write signatures of the transactions that have passed \nthe commit check in the TCM and are writing back their write set to the memory. When a transaction is \nready to commit, the commit manager checks its write signature against all write signa\u00adtures in the writeback \naction-list. If there is no collision, the commit manager sets the Commit .ag in the transaction header \nand writes the transaction s write signature to the action-list. Otherwise, it keeps checking the list \nuntil the colliding entry has .nished writing back. An extra bit is added to the list to make sure that \nTCM does not repeatedly keep checking the signatures that have passed the collision test with the current \ncommitting transaction before. These checks could potentially become the TCM s bottleneck, though we \ndid not notice any considerable busy waiting in our experiments. Subsequently, the TCM writes the necessary \ninformation about the committed transaction to the commit log, and moves on to checking the next entry \nin the precommit log. Since commit log entries are no longer needed after all overlap\u00adping transactions \nhave .nished, a clean up mechanism is required to remove unnecessary entries. For this purpose, we maintain \na mini\u00admum start version (minSV) log which contains the start versions of all in-.ight transactions. \nEach transaction adds an entry to this log at start time and removes it at commit or abort. After each \ntransac\u00adtion commit or abort, the TCM starts from the commit log head entry and checks it against the \nstart versions in the minSV log. If there are no overlapping in-.ight transactions with the commit log \nhead entry, that entry is removed and the head pointer is incre\u00admented. We keep doing this until the \nhead entry in the commit log has an overlapping in-.ight transaction. The reason we decided to use a \ncircular buffer for the commit log (as opposed to a linked-list buffer) is to avoid the extra overhead \nof maintaining a linked list. Our commit log model only allows us to remove entries from the head of \nthe log and add entries to the tail.  3.4 Individual Transaction Commits When a transaction reaches \nthe commit point, it .lls up an entry in its precommit log with a pointer to its transaction header and \nsets the entry s Ready .ag. Subsequently, it keeps polling Commit and Abort .elds, waiting for them to \nbe .lled by the TCM. In order to avoid busy waiting at this point, we can relinquish the core1 which \nis particularly useful when we have a larger number of threads than cores. After a transaction receives \ncommit permission from the TCM, it walks through its write set and writes back the actual values to memory. \nBecause the TCM has already made sure that there are no concurrent transactions writing to the same locations, \nthe com\u00admitting transaction does not need to lock any memory locations. We chose to use a lazy version \nmanagement strategy, because an eager version management system without locks introduces many complications \nin rolling back updates to memory locations after a con.ict. To minimize the overhead of individual transactional \nloads, a lazy con.ict detection scheme is employed. This works particularly well for speculation support \nin loop parallelism, because minimum transactional load overhead is important for gaining performance \nfrom parallelizing loops. Furthermore, con.icts are rare due to the smart loop selection, and trying \nto detect con.icts eagerly at each transactional load provides no extra bene.t. In eager con.ict detection \nmechanism, since transactions are checked for con.icts at each load and store, the possibility of having \nzombie transactions is really low. However, eager con.ict detection incurs substantial overhead on individual \ntransactional operations. Lazy con.ict detection makes STMlite vulnerable to zombie transactions. These \ntransactions may never reach the commit point and the commit manager normally does not get the chance \nto force them to abort. As a matter of fact, zombie transactions are partic\u00adularly bad for our implementation \nbecause their corresponding en\u00adtries remain valid within the minSV log and prevent the other com\u00ad 1 In \nLinux, this can be done using sched yield function.  mit log entries from being cleaned up. However, \nwe can exploit the minSV log to resolve the zombie transaction issue. Each time we go through the commit \nlog reading the minSV entries, if the dif\u00adference between the start version of a particular transaction \nand the global clock is more than a threshold, the TCM identi.es the corre\u00adsponding transaction as a \npotential zombie. Subsequently, the TCM checks the suspicious transaction s read signature against write \nsig\u00adnatures of the commit log entries (although it has not reached the commit point yet). If there is \na con.ict, the TCM forcibly aborts the zombie transaction by sending an abort signal. We have a signal \nhandler in each transaction that calls the abort function whenever it receives the TCM s abort signal. \nOtherwise, the TCM concludes that the suspicious zombie was just a long running transaction and avoids \naborting it. In this work, since we do not parallelize loops with complicated linked list operations \n(which are the main sources of zombies transactions), the possibility of having zombies is quite low \nin our framework.  4. Loop Parallelization Using STMlite In this section, we introduce our loop parallelization \nframework and customizations made to STMlite for parallelizing speculative DOALL loops. Our framework \nsuccessfully handles loops with cross iteration control dependences (e.g., while loops) as well as normal \ncounted loops. The general structure of our parallelization framework follows the code generation schema \nused in [44]. However, using that framework without the extra hardware support imposes a large overhead \non the execution time. At the same time, STMlite gives us the opportunity to simplify the parallelization \nframework by exploiting some of its underlying features that are already used for providing transactional \ncorrectness. 4.1 Baseline Parallelization Framework The purpose of the parallelization framework is to \ndistribute loop execution across multiple cores. In this framework, DOALL loops are categorized into \nDOALL-counted and DOALL-uncounted types. In DOALL-counted loops, the number of iterations is known at \nruntime, whereas in DOALL-uncounted loops, this number is dependent on the loop execution (e.g. while \nloops). In these cases, starting every iteration is dependent on the outcome of exit branches in previous \niterations (cross iteration control dependence). Figure 6 shows the detailed implementation of the framework. \nIn this scheme, loop iterations are divided into chunks. The operat\u00ading system passes the number of available \ncores to the application and the framework is .exible enough to use any number of cores for loop execution. \nAn outer loop is inserted around the original loop body to manage parallel execution between different \nchunks. The main thread (THREAD 0), which runs the sequential parts of the program, spawns the required \nnumber of threads at the start of the application. When a parallel loop is reached, a function pointer \ncontaining the proper loop chunk along with necessary parameters is sent to each spawned thread and they \nstart the execution of loop chunks. In order to capture the correct live-out registers after parallel \nloop execution, we use a set of registers called last upd idx, one for each conditional live-out (i.e., \nupdated in an if-statement). When a conditional live-out register is updated, the corresponding last \nupd idx is set to the current iteration number to keep track of the latest modi.cations to the live-out \nvalues. If the live-out register is unconditional (i.e., updated in every iteration), the .nal live\u00adout \nvalue can be retrieved from the last iteration and no tracking by last upd idx is needed. It should be \nnoted that loop chunks in the framework do not share any local memory variables on stack. Otherwise, \nthe loop would have unresolvable cross iteration dependences and would be unparallelizable. This leads \nto one of Figure 6. Overview of the parallelization framework (CS: chunk size, IS: iteration start, IE: \niteration end, SS: step size, TC: thread count).      the simpli.cations we made in STMlite which \nis the elimination of the handling mechanism needed for speculative local memory variables. Following \nis a description of the functionality of each segment in Figure 6. Spawn: THREAD 0, the main thread, \nsends the function pointer pointing to the start of loop chunks to the in-.ight threads through memory. \nIt also sends along the necessary parameters (chunk size, thread count, etc.) and live-in values. Parallel \nLoop: The program stays in the parallel loop segment as long as there are some iterations to run and \nno break has hap\u00adpened. In this segment, each thread executes a set of chunks. Each chunk consists of \nseveral iterations starting from IS (iteration start) and ending at IE (iteration end). The value of \nIS and IE are up\u00addated after each chunk using the chunk size (CS), thread count (TC), and step size (SS). \nEach chunk is enclosed in a transaction using TxBegin and TxCommit function calls. In order to ensure \ncorrectness, an abort signal is sent to transactions running higher iterations if a con.ict is detected. \nOne important requirement for parallelizing loop chunks is to force in-order chunk commit. This is necessary \nfor maintaining cor\u00adrect execution and enabling partial loop rollback and recovery. The TCM in STMlite \nalready provides the means to enforce ordering among transactions in the commit log. The same infrastructure \ncan be used for in-order chunk execution as well. Therefore, there is no need for send/receive instructions \nand a scalar operand network as was used in [44]. However, some extra book-keeping data is re\u00adquired \nboth for STMlite and the parallelization framework. Since this is mostly done in STMlite and it is almost \ntransparent to the generated code, we explain these necessary steps in the next sub\u00adsection detailing \nthe interaction between STMlite and loop paral\u00adlelization. For uncounted loops, if a break happens in \nany thread, higher transactions are not aborted immediately because thread execu\u00adtion is speculative \nand the break could be false. Instead, the local brk flag variable in each thread is used to keep track \nof breaks in individual chunks. If a transaction commits successfully with its local brk flag set, the \nbreak is no longer speculative, and a transaction abort signal is sent to all threads. In addition, a \nglobal brk flag is set, so that all threads break out of the outer loop after restarting the transaction \nas a result of the abort sig\u00adnal. The reason for explicitly aborting higher iterations is that, if an \niteration is started by misspeculation after the loop breaks, it could produce an illegal state. The \nexecution of this iteration might cause unwanted exceptions or might never .nish if it contains inner \nloops. This procedure of explicit handling of breaks has the bene.t of avoiding zombie transactions, \nand although STMlite can handle zombies, this explicit handling has much lower cost. Consolidation: \nAfter all cores are done with the execution of iteration chunks, they enter the consolidation phase. \nEach core sends its live-outs and last upd idx array to THREAD 0 through memory. THREAD 0 picks the last \nupdated live-out values. All other threads keep waiting for chunks from other parallel loops later in \nthe program. Since the goal is to provide a low-cost software-based paral\u00adlelization mechanism, most \nof the extra code is kept outside the main loop body, and is executed only once per chunk. 4.2 Interaction \nof Parallel Loops with STMlite As mentioned in the previous subsection, in-order commit of indi\u00advidual \nloop chunks is crucial for correct parallel execution. In order to enforce that requirement, we add another \nlog structured called the loop chunk commit log (LCCL) to the TCM. This log contains the loop ID of the \nlast committing parallel loop and the chunk ID of the last committed chunk in that loop. The loop ID \nis assigned to each loop statically at compile time. It should be noted that our model allows only one \nin-.ight parallel loop at a time by includ\u00ading a lightweight barrier at end of each chunk. Thus, there \nwill be no problem if a parallel loop is invoked twice, because there is guaranteed to be no previous \ninstances of this loop running. This is important, because if two in-.ight loops have the same loop ID, \nthey can completely distort each other s execution. The only prob\u00adlem is the case of loops in recursive \nfunctions. In this work, we do not parallelize loops with recursion. However, even in that case, a hash \nvalue based on the call site trace of the loop can be used to uniquely identify individual loops [34]. \nWe reuse the initial value of IS (iteration start) which is com\u00adputed at the beginning of each loop chunk \nas the chunk ID. When a loop chunk reaches the commit instruction, it writes its loop ID, chunk ID, chunk \nsize, and the loop s .rst chunk ID to the precom\u00admit log. After the TCM reads in an entry from the precommit \nlog, it performs one of the following two operations: 1. If the loop ID in the precommit log does not \nmatch the LCCL s committing loop ID, it infers that a new loop has started com\u00admitting. Subsequently, \nit writes the new loop ID and the loop s .rst chunk ID to the LCCL. If the committing chunk is the .rst \nchunk in the loop, the TCM proceeds with the commit process. Otherwise, it just moves on to checking \nthe next precommit log entry. This is because a chunk s commit process should not be started until all \nearlier chunks have been committed (i.e. have got commit permission from the TCM and started the writeback \nprocess). 2. If the loop ID of the committing chunk matches the entry in the LCCL, the TCM checks to \nsee if the current chunk is right after the last committed chunk. If so, it proceeds with the chunk s \ncommit process. Otherwise, it starts checking the next precommit log entry.        The above mechanism \nprovides low-cost commit ordering by adding minimal complexity to the STMlite library. This integration \nof loop parallelization with STMlite leads to an ef.cient parallel loop execution platform.  5. Results \nWe set up two sets of experiments. First, we evaluated how STMlite performs in a typical transactional \nenvironment using the STAMP transactional benchmarks [28]. In the second set of experiments, we implemented \nthe code generation part of the parallelization framework in the LLVM compiler [24]. Using this framework, \na set of SPECfp benchmarks and several kernel benchmarks are par\u00adallelized. All benchmarks were written \nin C or converted from For\u00adtran to C2. While the original Fortran applications can be paral\u00adlelized using \ncompilers such as SUIF [15], Fortran to C conver\u00adsion introduces a large number of pointer variables, \nthus compiler analysis alone was insuf.cient to parallelize all applications. For SPECint benchmarks, \nas previous works have shown [44, 25], the level of loop-level parallelism is quite low, thus the overhead \nof using an all-software parallelization approach is too large to yield meaningful performance gains. \nMore sophisticated parallelization techniques for integer applications are possible, such as those pro\u00adposed \nby [4], and can lead to substantial gains. However, we have not implemented these transformations within \nour compiler system, yet they are orthogonal to what we are doing here. 5.1 STMlite on STAMP We measured \nthe performance of the STAMP benchmarks on a SunFire T2000 with an 8-core UltraSPARC T1 processor, running \nSolaris 10. We compare our performance with an implementation of a state-of-art STM -Transactional Locking \n2 (TL2) [9]. Figure 7 shows the benchmark speedups on STMlite and TL2, both normal\u00adized to sequential \nexecution. The number of cores in the STMlite results include the one extra core used for the TCM. For \nexample, the 8 core results in STMlite have 7 computation cores and one TCM core. Thus, STMlite results \nstart from two cores on the hori\u00adzontal axis. As can be seen, STMlite noticeably outperforms TL2 in both \nhigh and low contention executions of the Vacation benchmark. This is mainly because this benchmark has \nlong transactions with a large number of loads. Therefore, the traditional STM performs poorly due to \nthe high overhead of transactional loads and it can hardly achieve speedup over sequential even with \n8 cores. However, using STMlite is particularly bene.cial in these types of bench\u00admarks. The overhead \nof transactional loads in our model is mini\u00admal due to the complete elimination of the read set. Furthermore, \nlong length transactions and relatively low contention amortize the slight serialization effect that \nhappens at commit time. Therefore, our model achieves about 2.5x and 3.1x speedup over TL2 with 8 cores, \nwhich is quite close to the speedup achieved by previous hybrid schemes [29]. STMlite follows the performance \nof TL2 in Kmeans, Labyrinth and Bayes. First, it should be noted that poor scalability from 4 to 8 cores \nin Kmeans and from 2 to 8 cores in Bayes is mainly due to the fact that these benchmarks contain heavy \n.oating point compu\u00adtations. Since the UltraSPARC processor only has a single .oating point unit that \nis shared by all processors, these .oating point com\u00adputations become the sequential bottleneck of parallel \nexecution, especially with higher number of threads. We did not have any 8\u00adcore x86 processors available \nto investigate the scalability in a more fair environment. However, the Bayes benchmark scales .ne from \n2 to 4 cores on a quad-core x86 machine. 2 Fortran to C conversion was done using the f2c tool with -a \n.ag.  STMlite Traditonal STM STMlite -lock instead of short TX  Figure 7. STMlite performance on \nSTAMP benchmarks. The ver\u00adtical axis shows the speedup compared to the sequential execution and horizontal \naxis is the number of cores. The number of cores in STMlite includes one core that is used for the TCM. \n/* Original Kmeans Code*/ | /* Lock-based Kmeans Code */ TxBegin; | pthread_mutex_lock(&#38;mutex1); \nstart = TxLoad(global_i); | start = global_i; TxStore(global_i, | global_i = start + CHUNK; (start + \nCHUNK)); | TxCommit(); | pthread_mutex_unlock(&#38;mutex1); TxBegin(); | pthread_mutex_lock(&#38;mutex2); \nTxStore_f(global_delta, | global_delta = TxLoad_f(global_delta) | global_delta + delta; + delta); | \nTxCommit(); | pthread_mutex_unlock(&#38;mutex2); Figure 8. Small transactions in Kmeans working on global \ndata and their equivalent lock-based implementation. The main reason STMlite performs similarly to TL2 \nin these benchmarks is the short length of transactions in Kmeans and relatively high rate of contention \nin Bayes and Labyrinth. So, the savings STMlite gets in transactional loads, transactional stores, and \nwritebacks gets offset by the extra overhead of communications between execution transactions and the \nTCM. However, STMlite is still about 15% to 30% faster than TL2 in Kmeans for 4 and 8 threads. An interesting \nissue we found while looking through the performance bottlenecks of STMlite in Kmeans, is that there \nis a small transaction in the source code towards the end of the program that increments a global variable \nin all transactions (Figure 8). This part of the code causes a large number of transaction aborts in \nSTMlite, which incurs a high cost considering the short transaction lengths. Whereas in TL2, since the \nlibrary is acquiring locks for each address during writeback and uses a back-off mechanism if the lock \nis not free, there are fewer transaction aborts. In order to validate this observation, we placed a global \nlock around the transaction in Figure 8 and changed the transactional loads and stores to normal ones. \nThe performance of the resulting execution Pro.led Coverage Provable Coverage Selected Coverage 100 \n90 80 70 60 50 40 30 20 10 0 052.alvinn 056.ear 102.swim 107.mgrid 173.applu 183.equake Figure 9. Pro.led \nDOALL, provable DOALL and selected paral\u00adlel loop coverage. The vertical axis shows fraction of sequential \nexecution.  STMlite HTM TL2 with sofware chunk sync RLS FmRadio 8.0 3.0 2.5 6.0 2.0 4.0 1.5 1.20 1.00 \n1.0 2.0 0.80 0.60 0.5 0.40 0.0 0.0 0.20 0.00 1 2 3 4 5 6 7 8 1 2 3 4 5 6 7 8 1 2 3 4 5 6 7 8 DCT Number \nof cores BeamFo rmer 2.5 5.0 2.0 4.0 1.5 3.0 1.0 2.0 0.5 1.0 0.0 0.0 1 2 3 4 5 6 7 8 1 2 3 4 5 6 7 8 \nFigure 11. STMlite performance on automatically parallelized SPECfp benchmarks. The vertical axis shows \nthe speed up com\u00adpared to the sequential execution and horizontal axis is the number of cores. The number \nof cores in STMlite includes one core that is used for the TCM.  Figure 10. STMlite performance on automatically \nparallelized kernel benchmarks. The vertical axis shows the speedup compared to the sequential execution \nand horizontal axis is the number of cores. The number of cores in STMlite includes one core that is \nused for the TCM. is also shown in Figure 7. This change in the benchmark did not affect the runtime \nfor TL2 since TL2 essentially does the same thing in short transactions. As can be seen in the .gure, \nalthough STMlite still suffers from lack of enough .oating point units, it performs better after replacing \nthe small transaction with locks in Kmeans and Bayes.   5.2 STMlite on Parallelized Sequential Programs \nFigure 9 shows the fraction of dynamic sequential execution that can be parallelized in several SPECfp \nbenchmarks.3 The .rst bar, pro.led coverage, shows the fraction of sequential execution in loops identi.ed \nas DOALL after pro.ling. The second bar, prov\u00adable coverage, is the fraction of sequential execution \nspent in loops that could be statically identi.ed as DOALL at compile time us\u00ading LLVM s memory dependence \nanalysis. As can be seen, a non\u00adtrivial percentage of DOALL coverage is obtained only after pro.l\u00ading, \nFinally the third bar, selected coverage, shows fraction of loops that were eventually parallelized. \nIt should be noted that not all the loops included in the coverage numbers are suitable for parallelization. \nThere are many DOALL loops in these applications that do not contain any computation, 3 These applications \nare a subset of SPECfp92/95/2000 that had moderate to high amount of loop level parallelism. STMlite \nHTM TL2 with sofware chunk sync 052.alvinn 183.equake 2.0 2.0 1.5 1.5 2.00 1.50 1.0 1.0 1.00 0.50 0.5 \n0.5 0.00 0.0 1 2 3 4 5 60.0 7 8 1 2 3 4 5 6 Number of cores 7 8 1 2 3 4 5 6 7 8 056.ear 102.swim 4.0 \n4.0 3.0 3.0 2.0 2.0 1.0 1.0 0.0 0.0 1 2 3 4 5 6 7 8 1 2 3 4 5 6 7 8 107.mgrid 173.applu 2.0 2.5 1.5 2.0 \n1.5 1.0 1.0 0.5 0.5 0.0 0.0 1 2 3 4 5 6 7 8 1 2 3 4 5 6 7 8 or the computation is not substantial. For \ninstance, parallelizing a loop which initializes an array s elements to zero or increments all elements \nin an array, can not provide much bene.t, since the overhead of parallelization would be more than the \nactual work in these loops. Therefore, we added a loop selection heuristic in our compiler which, according \nto the pro.le data, computes a parallelizability metric based on the total number of dynamic operations \nin the loop, number of iterations and total number of loop invocations in the program. The last bars \nin Figure 9 shows the total coverage of DOALLs that passed this metric. We have parallelized all these \nloops using the framework intro\u00adduced in Section 4.1. During the code generation pass, according to the \nstatic memory dependence analysis data, we performed a se\u00adlective replacement of the loops loads and \nstores with TxLoad and TxStore function calls. We essentially avoid changing loads and stores that can \nbe proved to cause no cross iteration dependences. As a step towards showing the effectiveness of our \napproach, we .rst tried the parallelization framework and STMLite on four kernel benchmarks: RLS, FMradio, \nDCT and beamformer. RLS is an im\u00adplementation of recursive least squares .lter which is used in sys\u00adtem \nidenti.cation problems and time series analysis. DCT performs a discrete cosine transform and is used \nin image processing appli\u00adcations. FMradio and beamformer are two streaming applications from the StreamIt \nbenchmark suite [41]. All these benchmarks have very high pro.led DOALL coverage. Figure 10 shows the \nachieved speedup using STMlite and TL2. The STMlite results include the resource used for the TCM (1 \nextra core). Furthermore, since TL2 doesn t have any primitives for supporting chunk commit serializa\u00adtion, \nwe implemented a software-based send/recv mechanism sim\u00adilar to [44]. Lastly, we estimated the results \non a similar system with HTM support by replacing all transactional loads and stores with normal ones. \nThis would represent a best-case HTM, and since we re only doing this for performance measurement, we \nignore the possibility of incorrect execution due to the lack of proper spec\u00adulation and we only take \ninto account the performance numbers for executions that complete successfully. As can be seen, STM\u00adlite \noutperforms TL2 with software based chunk synchronization by as much as a factor of 3x in FMradio. In \nbeamformer and DCT, STMlite follows the HTM results quite closely. For RLS, STMlite performs poorly compared \nto HTM results due to high number of transactional operations, yet it still achieves 2x speedup over \nse\u00adquential for 8 threads. Returning to SPECfp, Figure 11 shows the speedup for these benchmarks. Runtime \nvalues are normalized to the sequential ex\u00adecution of the program. The .gure shows that we achieve 0.6x \nto 2.2x speedup compared to sequential by going from two to eight cores. One of the reasons for performance \ndegradation in TL2 with software synchronization is the lack of library support for enforc\u00ading commit \nordering in TL2. Adding this explicit software synchro\u00adnization has a noticeably negative impact on the \nperformance. Per\u00adformance degradation would be even more in traditional TM sys\u00adtems with eager con.ict \ndetection, like [39]. As previous works have also suggested [38], workloads with transactions that have \nlarge readsets and low contention (similar to our parallelized se\u00adquential workloads), perform poorly \nwith eager con.ict detection. This is because eager con.ict detection adds extra overhead to transactional \nloads and stores, but since con.icts are rare, it does not help improving the performance. STMlite achieves \ndecent speedup compared to HTM results and outperforms TL2 with software chunk synchronization in 052.alvinn, \n056.ear and 102.swim. This is due to the lower over\u00adhead of transactional operations in STMlite which \nmakes it quite ef.cient with moderate number of these operations. However, the relative STMlite achieved \nspeedup, while being noticeably higher than TL2 with software synchronization, is quite low compared \nto HTM in other benchmarks. In SPECfp benchmarks, the paral\u00adlelized loops contain a large number of memory \noperations that may cause cross iteration dependences based on the static analysis and therefore need \nto be transacti.ed. Changing these operations to transactional versions causes the parallelized versions \nto become slow in some cases. Software-based speculation mechanisms are useful for parallelization in \ncases that the number of speculative variables is low, otherwise, the speculation mechanism amortizes \nthe bene.t caused by parallelization.   5.3 Effects of static memory analysis and signature sizes To \nbetter understand the tradeoffs involved in compilation and exe\u00adcution parameters, we ran two other experiments. \nIn the .rst exper\u00adiment, we measured the achieved speedup with and without selec\u00adtive replacement of \nloads and stores with transactional versions. As mentioned before, LLVM s memory dependence analysis \nis used to avoid transactifying memory instructions that provably do not cause cross iteration dependence. \nFigure 12 shows the result of this experiment on the 052.alvinn benchmark. As can be seen, .ltering out \nunnecessary transactional operations, while keeping the neces\u00adsary ones, has a great impact on performance \nin both STMlite and TL2. This result further proves that software speculation systems are best suited \nfor applications in which speculation is applied to a limited number of memory variables. Our second \nexperiment involves changing the signature size and studying the resulting performance impact. The effect \nof changing signature sizes on STMlite s performance is interesting. There is a subtle tradeoff involved \nin determining the right signature size. Larger sizes reduce the number of false positives and thereby \nre\u00adduce re-execution of correct transactions. However, at the same time, they lead to more time consuming \nsignature operations. Since STMlite is dependent on these operations in several parts of the implementation, \nthis can cause a noticeable performance degrada\u00ad  Speedup Signature size 2 1.5 1 0.5 0 12345678 4 32 \n1024 4096 Number of cores Figure 13. Effect of varying signature size on speedup for RLS. tion. Figure \n13 illustrates this effect on the RLS kernel benchmark. Speedup values keep increasing up to signature \nsizes of 32, after which they start going down.  6. Related Work There is a signi.cant amount of previous \nefforts in the area of transactional memory. Larus and Rajwar go through a detailed survey of different \ntransactional memory techniques in [23]. In particular, Shavit et al. proposed the .rst implementation \nof software transactional memory in [35]. Several other works such as DSTM [19] and OSTM [17] proposed \nnon-blocking STM imple\u00admentations. A major part of non-blocking STMs is maintenance of publicly shared \ntransaction structures which contain the undo infor\u00admation. In our implementation, the transaction structures \nonly need to be visible to the TCM and individual executing transactions, keeping contention on those \nstructures to a minimum. The authors in [18, 2] proposed a lock-based approach where write locks are \nacquired when an address is written. Also, they maintain a read set which needs to be validated before \ncommit. In our STMlite design, no locks are required and correctness is guaranteed by the com\u00admit manager. \nFurthermore, we eliminate the need for the read set, which reduces the overhead of transactional loads \nand transaction commits. [10] proposes the Transactional Locking implementation which maintains a read \nset and a write set during transaction ex\u00adecution. Subsequently, at commit time, it acquires locks for \neach individual write set entry and writes back the data after the lock is secured. Also, the read set \nis checked during commit to ensure consistency. There is also a large body of work in parallelization \nof se\u00adquential applications. Hydra [16] and Stampede [40] were two of the earlier efforts in the area \nof general purpose program paral\u00adlelization. The POSH compiler [25] uses loop-level parallelization with \nTLS hardware support. The authors in [44] proposed compiler transformation to extract more loop level \nparallelism from sequen\u00adtial programs. The compiler transformation part of that work is or\u00adthogonal to \nwhat we are doing and can be applied simultaneously here. Speculative decoupled software pipelining [42] \nis another ap\u00adproach that focuses on extracting parallelism from loops with cross iteration dependencies. \nIn that work, they distribute a single itera\u00adtion of the loop over several cores. The SUDS framework \n[13] per\u00adforms automatic speculative parallelization of applications for the RAW processor. This system \nrelies on the special architectural fea\u00adtures in RAW to accomplish ef.cient speculative state management \nand synchronization, such as the scalar operand network. However in all these works, hardware TLS or \ntransactional memory support and additional hardware mechanisms for synchronization are re\u00adquired. Whereas \nin this work, we are looking at a software-only solution and although our achieved speed up in some cases \nis lower than these works, we have the advantage of running our system on commodity hardware. Ceze et \nal. [6] proposed the idea of using Bloom .lters to rep\u00adresent read and write sets for transactions. They \nshowed how, with specialized hardware, transaction state can be maintained through signatures with less \noverhead. This technique was extended in LogTM-SE [43] and SigTM [29], which are hybrid TM systems requiring \nno modi.cations to hardware caches. Our work uses the idea of storing Bloom .lter-based read and write \nsets in software data structures, alleviating the need for the extra hardware. Authors in [18] use software \nhashing to remove duplicates in the read-log and undo-log of the same transaction, whereas in STMlite, \nit is used for con.ict detection between different transactions. The most similar speculation management \nmechanism to ours is RingSTM [39] that uses a global ring structure to organize com\u00admitting transactions. \nThey use Bloom .lters to represent read and write sets for transactions. However, because the ring is \nglobal, all threads face contention for ownership of the ring during com\u00admit, and prioritization is required \nto prevent starvation. Meanwhile, STMlite has thread local precommit logs and can relinquish the cores \nwhile the corresponding transaction is waiting for the com\u00admit manager to validate the transaction. Our \ncommit log works in a round-robin fashion, ensuring all threads waiting to commit are ser\u00adviced equally. \nFurthermore, in [39], the read signature is checked against several write signatures at each transactional \nload (eager con.ict detection), which adds considerable overhead. However, in STMlite, transactional \nload overhead is minimal because the only extra operation added is insertion of the address in the read \nsig\u00adnature. This makes our model more prone to zombie transactions, but as mentioned in Section 3.4, \nthe possibility of having zombies in parallelized loops is quite low, though STMlite can still handle \nthem successfully. Furthermore, we have customized STMlite to work for loop parallelization. This customization \nwould be more complicated in RingSTM. The reason is that transaction commit is done by individual transactions \nafter checking against the write signatures of ring elements. Therefore, if a loop chunk does not get \na chance to commit in the .rst try (due to an un.nished previous chunk), there would be no ef.cient way \nof checking again later in the execution. The only way would be to use a back off mechanism and check \nback from time to time, which is inef.cient. Whereas in STMlite, since the TCM is in charge of ordering \nloop chunks for commit, even if a chunk misses its chance, the TCM makes sure that it would be checked \nagain in a timely manner. An interesting, recently-proposed transactional memory model called FlexTM \n[37] adds mechanisms in hardware to coordinate read and write signature checking, speculative updates \nto caches and eager noti.cations to transactions about coherence events. They propose software mechanisms \nfor deciding how to manage con.icts and for choosing appropriate con.ict management and commit protocols. \n 7. Conclusion As we move further into the multicore era, a major challenge in both hardware and software \ncommunities is exploiting the abun\u00addant computing resources made available by technology advance\u00adments. \nAutomatic parallelization of applications is an appealing solution for utilizing these resources; however, \nparallelization ef\u00adforts are commonly dependent on complex hardware changes such as adding speculation \nsupport. These changes are not yet popular among hardware manufacturers. On the other hand, software-based \nspeculation support is still quite expensive in terms of performance to be widely used in parallel and \nparallelized applications. In this work, we try to tackle these issues from two closely related angles. \nFirst, we try to minimize the overheads of software based transac\u00adtional memory models by decoupling \nand centralizing the commit stage in STMlite. We also eliminate the need for maintaining a read set during \nloads and checking them during commit. Secondly, we are able to lower the overhead of loop parallelization \nby reusing some of the underlying structures of STMlite. We have shown that our work outperforms the \nstate-of-art transactional memory im\u00adplementations on transactional benchmarks with large transactions \nwhile achieving similar performance in smaller transactions. Fur\u00adthermore, we show that achieving real \nspeculative speedup on se\u00adquential applications is possible without extra hardware support. We believe \nthe value of this work lies in the idea that we make par\u00adallelization of sequential applications feasible \non commodity hard\u00adware.  Acknowledgments We would like to thank Dr. Tim Harris for his useful feedback \non earlier drafts of this paper. We extend our thanks to anonymous reviewers for their excellent comments. \nWe also thank Ganesh Dasika, Shuguang Feng, Shantanu Gupta and Amir Hormati for providing feedback on \nthis work. This research was supported by the National Science Foundation grant CCF-0811065, Semicon\u00adductor \nResearch Corporation (Task 1789.001), and the Gigascale Systems Research Center, one of .ve research \ncenters funded un\u00adder the Focus Center Research Program, a Semiconductor Research Corporation program. \nEquipment was kindly provided by Sun Mi\u00adcrosystems and Intel Corporation. References [1] M. Abadi, T. \nHarris, and M. Mehrara. Transactional memory with strong atomicity using off-the-shelf memory protection \nhardware. In Proc. of the 14th ACM SIGPLAN Symposium on Principles and Prac\u00adtice of Parallel Programming, \npages 185 196, 2009. [2] A.-R. Adl-Tabatabai, B. T. Lewis, V. Menon, B. R. Murphy, B. Saha, and T. Shpeisman. \nCompiler and runtime support for ef.cient software transactional memory. In Proc. of the SIGPLAN 06 Conference \non Programming Language Design and Implementation, pages 26 37, 2006. [3] R. Allen and K. Kennedy. Optimizing \ncompilers for modern architec\u00adtures: A dependence-based approach. Morgan Kaufmann Publishers Inc., 2002. \n[4] M. J. Bridges et al. Revisiting the sequential programming model for multi-core. In Proc. of the \n40th Annual International Symposium on Microarchitecture, pages 69 81, Dec. 2007. [5] B. D. Carlstrom \net al. The Atomos transactional programming lan\u00adguage. In Proc. of the SIGPLAN 06 Conference on Programming \nLanguage Design and Implementation, pages 1 13, June 2006.  [6] L. Ceze, J. Tuck, J. Torrellas, and \nC. Cascaval. Bulk disambiguation of speculative threads in multiprocessors. In Proc. of the 33rd Annual \nInternational Symposium on Computer Architecture, pages 227 238, Washington, DC, USA, 2006. IEEE Computer \nSociety. [7] M. K. Chen and K. Olukotun. Exploiting method-level parallelism in single-threaded Java \nprograms. In Proc. of the 7th International Conference on Parallel Architectures and Compilation Techniques, \npage 176, Oct. 1998. [8] K. Cooper et al. The ParaScope parallel programming environment. Proceedings \nof the IEEE, 81(2):244 263, Feb. 1993. [9] D. Dice, O. Shalev, and N. Shavit. Transactional Locking II. \nIn Proc. of the 2006 International Symposium on Distributed Computing, 2006. [10] D. Dice and N. Shavit. \nUnderstanding tradeoffs in software transac\u00adtional memory. In Proc. of the 2007 International Symposium \non Code Generation and Optimization, pages 21 33, 2007. [11] Z.-H. Du et al. A cost-driven compilation \nframework for speculative parallelization of sequential programs. In Proc. of the SIGPLAN 04 Conference \non Programming Language Design and Implementation, pages 71 81, 2004. [12] W. Eatherton. The push of \nnetwork processing to the top of the pyramid, 2005. Keynote address: Symposium on Architectures for Networking \nand Communications Systems. [13] M. Frank. SUDS: Automatic parallelization for Raw Processors.PhD thesis, \nMIT, 2003. [14] M. Frigo, C. E. Leiserson, and K. H. Randall. The implementation of the Cilk-5 multithreaded \nlanguage. In Proc. of the SIGPLAN 98 Conference on Programming Language Design and Implementation, pages \n212 223, June 1998. [15] M. Hall et al. Maximizing multiprocessor performance with the SUIF compiler. \nIEEE Computer, 29(12):84 89, Dec. 1996. [16] L. Hammond, M. Willey, and K. Olukotun. Data speculation \nsupport for a chip multiprocessor. In Eighth International Conference on Architectural Support for Programming \nLanguages and Operating Systems, pages 58 69, Oct. 1998. [17] T. Harris and K. Fraser. Language support \nfor lightweight transactions. Proceedings of the OOPSLA 03, 38(11):388 402, 2003. [18] T. Harris, M. \nPlesko, A. Shinnar, and D. Tarditi. Optimizing memory transactions. Proc. of the SIGPLAN 06 Conference \non Programming Language Design and Implementation, 41(6):14 25, 2006. [19] M. Herlihy, V. Luchangco, \nand M. Moir. The repeat offender problem: A mechanism for supporting dynamic-sized, lock-free data structures. \nIn Proceedings of the 16th International Conference on Distributed Computing, pages 339 353. Springer-Verlag, \n2002. [20] H. P. Hofstee. Power ef.cient processor design and the Cell processor. In Proc. of the 11th \nInternational Symposium on High-Performance Computer Architecture, pages 258 262, Feb. 2005. [21] T. \nA. Johnson, R. Eigenmann, and T. N. Vijaykumar. Min-cut program decomposition for thread-level speculation. \nIn Proc. of the SIGPLAN 04 Conference on Programming Language Design and Implementa\u00adtion, pages 59 70, \nJune 2004. [22] P. Kongetira, K. Aingaran, and K. Olukotun. Niagara: A 32-way multithreaded SPARC processor. \nIEEE Micro, 25(2):21 29, Feb. 2005. [23] J. Larus and R. Rajwar. Transactional Memroy. Morgan &#38; Claypool \nPublishers, 2007. [24] C. Lattner and V. Adve. LLVM: A compilation framework for life\u00adlong program analysis \n&#38; transformation. In Proc. of the 2004 Interna\u00adtional Symposium on Code Generation and Optimization, \npages 75 86, 2004. [25] W. Liu et al. POSH: A TLS compiler that exploits program structure. In Proc. \nof the 11th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, pages 158 167, \nApr. 2006. [26] V. J. Marathe, W. N. Scherer, and M. L. Scott. Adaptive software transactional memory. \nIn Proc. of the 2005 International Symposium on Distributed Computing, pages 354 368, Sept. 2005. [27] \nP. Marcuello and A. Gonzalez. Thread-spawning schemes for specu\u00adlative multithreading. In Proc. of the \n8th International Symposium on High-Performance Computer Architecture, page 55, Feb. 2002. [28] C. C. \nMinh, J. Chung, C. Kozyrakis, and K. Olukotun. STAMP: Stan\u00adford transactional applications for multi-processing. \nIn Proceedings of IISWC08, 2008. [29] C. C. Minh, M. Trautmann, J. Chung, A. McDonald, N. Bronson, J. \nCasper, C. Kozyrakis, and K. Olukotun. An effective hybrid trans\u00adactional memory system with strong isolation \nguarantees. In Proc. of the 34th Annual International Symposium on Computer Architecture, pages 69 80, \nNew York, NY, USA, 2007. ACM. [30] J. Nickolls and I. Buck. NVIDIA CUDA software and GPU parallel computing \narchitecture. In Microprocessor Forum, May 2007. [31] E. Nystrom, H.-S. Kim, and W. Hwu. Bottom-up and \ntop-down context-sensitive summary-based pointer analysis. In Proc. of the 11th Static Analysis Symposium, \npages 165 180, Aug. 2004. [32] B. Saha, A. Adl-Tabatabai, and Q. Jacobson. Architectural support for \nsoftware transactional memory. In Proc. of the 39th Annual International Symposium on Microarchitecture, \npages 185 196, Nov. 2006. [33] F. T. Schneider, V. Menon, T. Shpeisman, and A.-R. Adl-Tabatabai. Dynamic \noptimization for ef.cient strong atomicity. In Proceedings of the OOPSLA 08, pages 181 194, 2008. [34] \nM. L. Seidl and B. G. Zorn. Segregating heap objects by reference behavior and lifetime. In Eighth International \nConference on Archi\u00adtectural Support for Programming Languages and Operating Systems, pages 12 23, Oct. \n1998. [35] N. Shavit and D. Touitou. Software transactional memory. Journal of Parallel and Distributed \nComputing, 10(2):99 116, Feb. 1997. [36] T. Shpeisman, V. Menon, A.-R. Adl-Tabatabai, S. Balensiefer, \nD. Grossman, R. L. Hudson, K. F. Moore, and B. Saha. Enforcing isolation and ordering in STM. In Proc. \nof the SIGPLAN 07 Confer\u00adence on Programming Language Design and Implementation, pages 78 88, 2007. \n[37] A. Shriraman, S. Dwarkadas, and M. L. Scott. Flexible Decoupled Transactional Memory Support. In \nProc. of the 35th Annual Interna\u00adtional Symposium on Computer Architecture, pages 139 150, 2008. [38] \nM. F. Spear, V. J. Marathe, W. N. S. Iii, and M. L. Scott. Con.ict de\u00adtection and validation strategies \nfor software transactional memory. In Proc. of the 2006 International Symposium on Distributed Computing, \n2006. [39] M. F. Spear, M. M. Michael, and C. von Praun. RingSTM: scalable transactions with a single \natomic instruction. pages 275 284, 2008. [40] J. G. Steffan and T. C. Mowry. The potential for using \nthread\u00adlevel data speculation to facilitate automatic parallelization. In Proc. of the 4th International \nSymposium on High-Performance Computer Architecture, pages 2 13, 1998. [41] W. Thies, M. Karczmarek, \nand S. P. Amarasinghe. StreamIt: A lan\u00adguage for streaming applications. In Proc. of the 2002 International \nConference on Compiler Construction, pages 179 196, 2002. [42] N. Vachharajani, R. Rangan, E. Raman, \nM. Bridges, G. Ottoni, and D. August. Speculative Decoupled Software Pipelining. In Proc. of the 16th \nInternational Conference on Parallel Architectures and Compilation Techniques, pages 49 59, Sept. 2007. \n [43] L. Yen et al. LogTM-SE: Decoupling hardware transactional memory from caches. In Proc. of the 13th \nInternational Symposium on High-Performance Computer Architecture, pages 261 272, Feb. 2007. [44] H. \nZhong, M. Mehrara, S. Lieberman, and S. Mahlke. Uncovering hidden loop level parallelism in sequential \napplications. In Proc. of the 14th International Symposium on High-Performance Computer Architecture, \nFeb. 2008. [45] C. Zilles and G. Sohi. Master/slave speculative parallelization. In Proc. of the 35th \nAnnual International Symposium on Microarchitec\u00adture, pages 85 96, Nov. 2002.  \n\t\t\t", "proc_id": "1542476", "abstract": "<p>Multicore designs have emerged as the mainstream design paradigm for the microprocessor industry. Unfortunately, providing multiple cores does not directly translate into performance for most applications. The industry has already fallen short of the decades-old performance trend of doubling performance every 18 months. An attractive approach for exploiting multiple cores is to rely on tools, both compilers and runtime optimizers, to automatically extract threads from sequential applications. However, despite decades of research on automatic parallelization, most techniques are only effective in the scientific and data parallel domains where array dominated codes can be precisely analyzed by the compiler. Thread-level speculation offers the opportunity to expand parallelization to general-purpose programs, but at the cost of expensive hardware support. In this paper, we focus on providing low-overhead software support for exploiting speculative parallelism. We propose STMlite, a light-weight software transactional memory model that is customized to facilitate profile-guided automatic loop parallelization. STMlite eliminates a considerable amount of checking and locking overhead in conventional software transactional memory models by decoupling the commit phase from main transaction execution. Further, strong atomicity requirements for generic transactional memories are unnecessary within a stylized automatic parallelization framework. STMlite enables sequential applications to extract meaningful performance gains on commodity multicore hardware.</p>", "authors": [{"name": "Mojtaba Mehrara", "author_profile_id": "81319497515", "affiliation": "University of Michigan, Ann Arbor, MI, USA", "person_id": "P1464265", "email_address": "", "orcid_id": ""}, {"name": "Jeff Hao", "author_profile_id": "81435597315", "affiliation": "University of Michigan, Ann Arbor, MI, USA", "person_id": "P1464266", "email_address": "", "orcid_id": ""}, {"name": "Po-Chun Hsu", "author_profile_id": "81435602272", "affiliation": "University of Michigan, Ann Arbor, MI, USA", "person_id": "P1464267", "email_address": "", "orcid_id": ""}, {"name": "Scott Mahlke", "author_profile_id": "81100622742", "affiliation": "University of Michigan, Ann Arbor, MI, USA", "person_id": "P1464268", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1542476.1542495", "year": "2009", "article_id": "1542495", "conference": "PLDI", "title": "Parallelizing sequential applications on commodity hardware using a low-cost software transactional memory", "url": "http://dl.acm.org/citation.cfm?id=1542495"}