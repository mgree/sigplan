{"article_publication_date": "09-17-2002", "fulltext": "\n A Compiled Implementation of Strong Reduction Benjamin Gr\u00b4Xavier Leroy egoire INRIA Rocquencourt INRIA \nRocquencourt Domaine de Voluceau, B.P. 105 Domaine de Voluceau, B.P. 105 78153 Le Chesnay, France 78153 \nLe Chesnay, France Benjamin.Gregoire@inria.fr Xavier.Leroy@inria.fr Abstract Motivated by applications \nto proof assistants based on depen\u00addent types, we develop and prove correct a strong reducer and \u00df\u00adequivalence \nchecker for the .-calculus with products, sums, and guarded .xpoints. Our approach is based on compilation \nto the bytecode of an abstract machine performing weak reductions on non-closed terms, derived with minimal \nmodi.cations from the ZAM machine used in the Objective Caml bytecode interpreter, and complemented by \na recursive read back procedure. An imple\u00admentation in the Coq proof assistant demonstrates important \nspeed\u00adups compared with the original interpreter-based implementation of strong reduction in Coq. Categories \nand Subject Descriptors D.3.1 [Programming Languages]: Language Classi.cations applicative (functional) \nlanguages; D.3.4 [Programming Lan\u00adguages]: Processors compilers, interpreters; F.3.2 [Logics and Meanings \nof Programs]: Semantics of Programming Languages operational semantics, partial evaluation; F.4.1 [Mathematical \nLogic and Formal Languages]: Mathematical Logic lambda calculus and related systems, proof theory; I.1.3 \n[Symbolic and Algebraic Manipulation]: Languages and Sys\u00adtems evaluation strategies; I.2.3 [Arti.cial \nIntelligence]: Deduc\u00adtion and Theorem Proving General Terms Languages, Theory, Experimentation, Veri.cation \n Keywords Strong reduction, beta-equivalence, normalization by evaluation, abstract machine, virtual \nmachine, Calculus of Constructions, Coq Permission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior \nspeci.c permission and/or a fee. ICFP 02, October 4-6, 2002, Pittsburgh, Pennsylvania, USA. Copyright \n2002 ACM 1-58113-487-8/02/0010 ...$5.00 1 Introduction It is folklore that \u00df-reduction in the .-calculus \nis the computation model underlying functional programming languages. Actually, functional languages \nimplement only weak \u00df-reduction, whereby reductions are not performed on function bodies until functions \nare applied to actual arguments. Ef.cient compiled implementations of weak reduction are widely known \nand deployed, based either on environments and closures or on graph reduction, and implemented either \nas abstract machines or by direct machine code generation. In contrast, this paper focuses on strong \nreduction, where \u00df\u00adreductions are also performed on function bodies. The practical need for strong reduction \nappears in two areas. The .rst area is par\u00adtial evaluation, also known as program specialization: specializing \na function applied to some known arguments (the other arguments remaining unknown) amounts to strongly \nnormalizing a partial ap\u00adplication of the function. The other area where strong reduction is required, \nwhich prompted the work presented here, is type checking / proof checking in type systems / logics based \non dependent types, such as LF or the Calculus of Constructions [14, 7, 21], which are at the basis of \nproof assistants such as Alf, Coq, Elf, Lego and NuPRL. In these systems, dependent types may contain \narbitrary terms, and types are compared up to \u00df-equivalence of the terms they contain, as captured by \nthe following conversion rule: \u00df E l a : tt t' (conv) E l a : t' Thus, type checking in these systems, \nor equivalently proof check\u00ading, involve performing strong \u00df-reductions inside the types to be compared, \nuntil the reducts are syntactically equal. Most, if not all, proof assistants of the Coq/HOL family implement \nthese strong reductions in a purely interpretative way, by walk\u00ading over a tree-based representation \nof terms. Despite various im\u00adplementation tricks involving explicit substitutions, this interpreta\u00adtive \napproach can become a performance bottleneck when devel\u00adoping and checking proofs with large computational \ncontent, such as proofs based on re.ection. Proof by re.ection is characterized by the use of ef.cient \ndecision procedures, proved correct once and for all, to replace long proof derivations. A typical example \nis the use of computations on binary decision diagrams to prove results on boolean formulas [23]. Another \nexample is Appel and Haken s famous proof of the 4-color theorem [2], which involves check\u00ading colorability \nof a .nite, but large, number of well-chosen planar graphs: rather than developing a proof of 4-colorability \nfor every such graph, a proved decision procedure is invoked on all of them. The present paper reports \non the design and implementation of a strong evaluator and equivalence tester for the Coq proof assistant \nthat eliminates most of the interpretive overhead via compilation to the byte-code of a virtual machine. \nIn sections 2 and 3, we .rst show how strong reduction can be implemented by a recursive com\u00adbination \nof weak symbolic reduction (weak reduction of terms con\u00adtaining free variables) and readback (reconstruction \nof a term in normal form from the head normal form returned by the weak sym\u00adbolic evaluator). While developed \nindependently, this combination is similar to online type-directed partial evaluation [12, 22]; how\u00adever, \nwe give a new presentation of this approach that paves the way to an implementation of weak symbolic \nreduction that avoids explicit run-time tests is this term symbolic or not? . In section 4, we develop \none such ef.cient implementation of weak symbolic reduction, as an abstract machine and its associated \ncom\u00adpilation scheme. The abstract machine is a minor extension of the ZAM abstract machine [16] that \nis at the heart of the Objective Caml bytecode interpreter [17], and reuses all the work that has been \nexpended in making the latter ef.cient. Both the abstract machine and its compilation scheme have been \nproved correct with respect to the weak reduction semantics, and the proof was mechanically checked by \nCoq; section 5 reports on this proof development. Performance .gures obtained on a mod\u00adi.ed version of \nthe Coq proof assistant are reported in section 6. We end this paper by a discussion of related work \nin section 7, and some concluding remarks in section 8. 2 Strong Reduction for the Pure .-calculus We \n.rst present our strong reduction technique on the simplest functional language of all: the pure .-calculus. \n2.1 The Pure .-calculus The syntax of the calculus is given by Terms: a ::=x | .x.a | a1 a2 Terms are \nidenti.ed up to renaming of .-bound variables (a\u00adconversion). The strong reduction relation . consists \nof the fa\u00admiliar \u00df-reduction rule, plus a context rule allowing reduction in arbitrary subterms, including \nbelow a lambda: ' (.x.a) a . a{x . a '} (\u00df) ' G(a) . G(a ') if a . a (context) with G ::=.x.[] | [] a \n| a []. * We write . for the re.exive and transitive closure of .. In the following, we assume that all \n.-terms a considered are strongly normalizing: there are no in.nite reduction sequences starting from \na. In our setting, this property is guaranteed by the type system of the Calculus of Constructions. We \nare interested in two computational problems. The .rst is to compute the normal form N (a) of a closed, \nstrongly normalizing * term a. This is the unique term such that a . N (a) and N (a) does not reduce. \nThe second problem is to decide whether two closed, strongly normalizing terms a1 and a2 are \u00df-equivalent, \nwritten a1 a2. This equivalence is de.ned by a1 a2 if and only if there exists ** a term a such that \na1 . a and a2 . a. 2.2 Strong Reduction by Iterated Symbolic Weak Reduction and Readback To compute \nthe normal form of a closed term a, our approach is .rst to compute the head normal form (also called \nvalue) of a us\u00ading an off-the-shelf weak evaluator. Such an evaluator performs \u00df-reductions anywhere \nin the term except under a .-abstraction. ' Thus, the value of a is of the form .x.a ' , where a is, \nin general, not in normal form. To obtain the normal form of a, all that remains to do is recursively \ncompute the normal form N (a ') of a '; then, we will have N (a)=N (.x.a ')= .x. N (a '). ' A slight \ndif.culty arises here: a is not necessarily closed, since ' the formal parameter x may occur free in \na . Thus, we cannot just run our off-the-shelf weak evaluator on a ', since such evaluators work only \non closed terms. To circumvent this problem, we enrich the term algebra with the ability to represent \nand manipulate free variables during weak reduction. More formally, we inject the pure .-terms a into \nthe following algebra of extended terms b: Extended terms: b ::= xv1 x | .x.b | b1 b2 | [ ... vn] Values: \nv ::= xv1 .x.b | [ ... vn] Here, x is a constant, uniquely associated with the identi.er x,but not subject \nto alpha-conversion. The extended term [x ] is a run\u00adtime representation of the free variable x. Since \nfree variables can be applied during weak reduction, we also need run-time represen\u00adtations [ ... vn]for \napplications of a free variable x to arguments xv1 v1,...,vn. We now formalize the symbolic weak reduction \nof extended, closed terms b. ( Symbolic here means that this weak reduction knows how to handle free \nvariables as represented by [...] terms.) To be more speci.c, and to match exactly the implementation \nof symbolic weak reduction by an abstract machine presented in section 4, we impose a call-by-value, \nright-to-left evaluation strategy. The sym\u00adbolic weak reduction relation . is de.ned by the following \nthree rules: (.x.b) v . b{x . v} (\u00dfv) [xv 1 ... vn] v . [ ... vn v] xv1 (\u00dfs) ' Gv(a) . Gv(a ') if a \n. a (contextv) with Gv ::=[] v | b []. Rule (\u00dfv) is the familiar call-by-value function application rule. \nIt handles the case where the function part of an application evaluates to a known function. Rule (\u00dfs)( \nsymbolic beta reduction) handles the case where the function part evaluates to the representation of \na free variable [x ] or of an application of a free variable [ ... vn]. xv1 Finally, our choice of contexts \nGv precludes reductions under func\u00adtion abstractions, and forces the argument part of an application \nto be evaluated to a value before starting evaluation of the function part (right-to-left strategy). \n* As usual, we write . for the transitive and re.exive closure of .. We de.ne the value V (b) of an extended \nclosed term b as the * normal form of b for the relation .. Notice that such a normal form is necessarily \na value: reduction cannot get stuck. Now that we know how to reduce weakly terms containing free variables, \nwe can express precisely the strong normalization proce\u00addure outlined earlier: .rst, normalize weakly; \nsecond, read back the resulting value as a normalized term, recursing over the bodies of functions if \nneeded. N (b)= R (V (b)) (1) R (.x.b)= .y. N ((.x.b)[y ]) (y fresh) (2) R ([xv 1 ... vn]) = x R (v1) \n... R (vn) (3) The readback function R transforms values v into normalized source terms a. For function \nvalues .x.b (equation 2), reading back consists in applying the value to the run-time representation \nof a fresh free variable [y ]. ( Fresh , here, means that y does not occur at all in b; the freshness \ncondition can be made fully precise by us\u00ading negative de Bruijn indices.) We then compute the value \nof this term, or equivalently of the term b{x . [y ]}, and read it back as a normalized term a. The normal \nform of .x.b is then .y.a. Reading back the value [ ... vn](equation 3) simply consists in xv1 extracting \nthe variable x from x, reading back the values to which it is applied, and reconstructing the application \nx R (v1) ... R (vn). Example: Consider the following source term a =(.x.x)(.y. (.z.z)y (.t.t)) Weak symbolic \nevaluation reduces it to v = .y. (.z.z) y (.t.t). This is a functional value, hence the readback procedure \nR restarts weak symbolic evaluation on b =(.y. (.z.z) y (.t.t)) [u ]. The value returned by the second \nround of weak symbolic evaluation is v ' =[u (.t.t)]. Readback of .t.t triggers the evaluation of (.t.t)[w \n], which returns [w ]. Thus, R (.t.t)=.w.w. Then, R (v ')=u (.w.w). Finally, N (a)=R (v)=.u. u (.w.w) \nRemark: The readback of functional values (equation 2) could al\u00adternatively be written as R (.x.b)= .y. \nN (b{x . [y ]}) The two forms are equivalent, since the .rst step of weak reduc\u00adtion of (.x.b)[y ]transforms \nthis term into b{x . [y ]}. However, by writing (2) as a function application rather than a direct substitution, \nwe make it clear that the readback procedure does not need to look inside function values: it remains \napplicable even if function val\u00adues are represented as opaque closures of compiled code, like our abstract \nmachine-based weak evaluator of section 4 does. The only requirement that the readback function R puts \non the representation of values used by the weak evaluator V is that the representations of functions \n.x.b and of accumulators [ ... vn] can be dis\u00ad xv1 tinguished at run-time, and that the components x,v1,...,vn \nof the latter can be recovered from the latter. 2.3 Correctness of the Normalization Procedure We now \nproceed to show the correctness of the function N de.ned above. We start with partial correctness: assuming \nthe computation of N (a) terminates, we show that it is indeed the normal form of the pure term a. REMARK \n1. For all extended terms b, N (b)and R (b), if de.ned, are terms that belong to the following grammar: \nn ::=.x.n | xn1 ... nk Hence, they are pure .-terms in normal form. Now, de.ne the following translation \nb from an extended term b to a pure .-term, obtained by erasing the distinction between run-time representations \nof free variables and the free variables themselves: x = x .x.b = b1 b2 = b1 b2 [ ... vn]= ... vn xv1 \nxv1 * LEMMA 2. If b . b ' , then b . b' . PROOF. By cases on the reduction rule used. If b . b ' by rule \n(\u00dfv), then b . b' by rule (\u00df).If b . b ' by rule (\u00dfs), then b = b' . Finally, for context reductions, \nnotice that translations of call\u00adby-value contexts Gv are a subset of G contexts, and conclude by induction \non the number of context rules used.  * LEMMA 3. If N (b)is de.ned, then b . N (b).If R (v)is de.ned, \n* then v . R (v). PROOF. Both statements are proved simultaneously by course of value induction (induction \non the number of recursive calls to N and R ). * Since N (b)= R (V (b)), we have b . V (b) by lemma \n2, and * * V (b). R (V (b)) by induction hypothesis. It follows that b . N (b). Consider now the case \nR (.x.b). Write b ' =(.x.b)[y ]. By induc\u00ad * tion hypothesis, b'. N (b '). Since N (b ') is in normal \nform by remark 1, and b'. b{x . y}, con.uence of the .-calculus ensures * * that b{x . y}. N (b '). \nHence, .y. b{x . y}. .y.N (b '). Since .y. b{x . y} =.x.b up to alpha-conversion, the expected result \n* .x.b . N (b ')follows. The last case to consider is R ([ ... vn]). xv1 Applying the in\u00ad * duction hypothesis \nto the vi, we obtain that vi . R (vi) for i = * 1,...,n. Thus, [ ... vn]=xv1 ... vn . x R (v1) ... R \n(vn)= xv1 R ([ ... vn]). xv1 LEMMA 4. For all pure .-terms a, if N (a) is de.ned, then it is the normal \nform of a. PROOF. Since a is a pure .-term (containing no [...]), we have * a =a. By lemma 3, a . N (a). \nMoreover, N (a)is in normal form by remark 1. We now turn to proving that N always terminates when given \na strongly-normalizing argument. LEMMA 5. If b is strongly normalizing, then V (b)is de.ned. PROOF. \nBy way of contradiction, consider an in.nite weak reduc\u00adtion sequence starting at b and performing in.nitely \nmany \u00dfv and \u00dfs reductions (possibly under context). There can only be a .nite number of \u00dfs reductions \nin a row, without an intervening \u00dfv step, since the number of application nodes in the term decreases \nby one during a \u00dfs reduction. Thus, the in.nite weak reduction sequence contains in.nitely many \u00dfv reductions. \nAs shown in lemma 2, each such \u00dfv reduction corresponds to a \u00df reduction on translated terms. Thus, we \ncan construct an in.nite sequence of \u00df reductions starting at b. This contradicts the hypothesis that \nb is strongly normaliz\u00ading.  LEMMA 6. If v is strongly normalizing, then R (v) is de.ned. PROOF. Consider \nthe ordering . on pure terms de.ned as the tran\u00adsitive closure of the following two cases: ' a . a if \na . a ' ; '' a . a if a is a strict subterm of a up to a renaming of free variables. It is easy to see \nthat . is well-founded on strongly normalizing terms, in the sense that there exists no in.nitely decreasing \nchains a . a1 . ...starting with a strongly normalizing term a. (From such a chain, we could trivially \nconstruct an in.nite reduction sequence starting with a.) The result then follows by well-founded induction \nusing the . or\u00addering. For the function case, we have R (.x.b)= .y.R (V (b ')) with b ' = b{x . [y ]})). \nSince b' = b{x . y}, b' is a strict subterm of .x.b up to renaming of x by y. Hence, b' is strongly normaliz\u00ading, \nand lemma 5 establishes the termination of V (b '). Moreover, V (b') is a reduct of b' by lemma 2. Hence, \n.x.b . b'> V (b'). We can therefore apply the induction hypothesis to R (V (b ')), and obtain the expected \nresult. For the case R ([ ... vn]) x R (v1) ... R (vn), it is obvious xv1 = that v1,...,vn are strict \nsubterms of [xv 1 ... vn]= xv1 ... vn, and the result follows by induction hypothesis. As a corollary \nof lemmas 4, 5 and 6, we obtain the (total) correct\u00adness of the strong normalization procedure: THEOREM \n1. If a is a closed, strongly normalizing pure .-term, then N (a) is de.ned and is the normal form of \na. 2.4 Deciding \u00df-equivalence The previous results give a naive procedure to decide the \u00df\u00adequivalence \nof two strongly normalizing terms a1 and a2: com\u00adpute N (a1)and N (a2)and compare the normal forms for \nsyntactic equality. However, it is often not necessary to go all the way to nor\u00admal forms to .nd a common \nreduct. The following more ef.cient procedure E(a1,a2) reduces weakly the terms a1 and a2, compares their \nvalues, and recurses only if syntactically different functions are obtained. E(b1,b2) if and only if \nEv(V (b1),V (b2)).  Ev(v1,v2) if v1 = v2.  Ev(.x1.b1,.x2.b2) if Ev((.x1.b1)[y ],(.x2.b2)[y ]) where \ny is a fresh variable.  Ev([xv 1 ... vn],[ ... wn])if Ev(vi,wi)for all i =  xw1 1,...,n. Ev(v1,v2) \nis false otherwise. In this de.nition, the = relation stands for any relation that is .ner than \u00df-convertibility \nand can be computed cheaply, so that the early return test (second case above) can be pro.tably applied \nat each step. Syntactic equality up to a-conversion is an obvious can\u00addidate for = in an interpreted \nsetting. In a compiled implementa\u00adtion such as that of section 4, syntactic equality might not be easily \ndecidable, in which case = can be equality of machine representa\u00adtions. The correctness of the E predicate \nde.ned above can easily be proved along the lines of section 2.3, and we omit the proofs.  3 Extension \nto the Calculus of Constructions We now progressively extend the approach presented in section 2 with \nthe additional features of the (type-erased) term language of the Calculus of Constructions: inductive \ntypes (a generalization of products and sums) and .xpoints. 3.1 Booleans and Conditional Before considering \ninductive types in their full generality, it is helpful to present a familiar special case: booleans \nand the if... then... else. . . construct. The syntax of the source calculus be\u00adcomes: Terms: a ::=x \n| .x.a | a1 a2 | true | false | if a1 then a2 else a3 The associated reduction rules are: ' (.x.a) a \n. a{x . a '}' if true then a else a . a '' if false then a else a . a ' G(a) . G(a ') if a . a with \nG ::= .x.[] | [] a | a [] | if [] then a2 else a3 |if a1 then [] else a3 | if a1 then a2 else []. We \nmust also add booleans and the conditional construct to the al\u00adgebra of extended terms. However, a new \ncase arises: the run-time representation of a free variable [x ] can now appear in the if part of a conditional. \nJust like rule (\u00dfs)simply remembers the arguments to an application of a free variable, we need to remember \nthe if... then... else. . . construct that could not be reduced because the condition was a free variable. \nTo capture this phenomenon, we introduce a new syntactic category k of accumulators, or run-time representations \nof non-closed, non-reducible terms. Extended terms: b ::= x | .x.b | b1 b2 | true | false | if b1 then \nb2 else b3 | [k] Accumulators: k ::= x | kv | if k then b else b ' Values: v ::= .x.b | true | false \n| [k] Symbolic weak reduction is then de.ned by the following rules: (.x.b) v . b{x . v} [k] v . [kv] \nif true then b else b '. b if false then b else b '. b ' if [k] then b else b '. [if k then b else b \n'] ' Gv(a) . Gv(a ') if a . a with Gv ::=[] v | b [] | if [] then b else b ' . It only remains to extend \nthe readback procedure to booleans and the new syntac\u00adtic class of accumulators. The only point worth \nnoticing is that reading back a suspended conditional if k then b else b ' involves normalizing both \narms b and b ' of the conditional, and reconstruct\u00ading an if... then... else with the readback of k and \nthe normal forms of b and b ' . (.x.b) v [k] v case C(vv) of (Ci(vxi) . bi)i.I case [k] of (Ci(vxi) . \nbi)i.I Gv(a) N (b) R (.x.b) R (C(v1,...,vn) R ([k]) R '(x ) R '(kv) R '(case k of (Ci(vxi) . bi)i.I \n) . b{x . v} . [kv]  . bj{vxj .vv} if C = Cj and |vv| = |vxj| . [case k of (Ci(vxi) . bi)i.I]  ' \n . Gv(a ') if a . a = R (V (b)) = .y. N ((.x.b)[y ]) (y fresh) = C(R (v1),...,R (vn)) = R '(k) = x = \nR '(k) R (v) -. = case R '(k) of (Ci(vyi) . N (b (Ci([y i]))))i.I where b = .x. case x of (Ci(vxi) \n. bi)i.I and the vyi are sequences of fresh variables with |vyi| = |vxi| Figure 1. Symbolic weak reduction \nand readback for inductive types N (b)= R (.x.b)= R (true)= R (false)= R ([k]) = R '(x )= R '(kv)= R \n(V (b)) .y. N ((.x.b)[y ]) (y fresh) true false R '(k) x R '(k) R (v) R '(if k then b else b ') = if \nR '(k) then N (b) else N (b ')  3.2 Inductive Types Inductive types in the Calculus of Constructions \n[21] are similar to datatypes in ML and Haskell: they consist in one or several alterna\u00adtives, identi.ed \nby unique constructors C, each constructor carrying zero, one or several terms as arguments. A built-in \ncase construct allows shallow pattern-matching on terms of inductive types. Once enriched with inductive \ntypes, the source calculus becomes: Terms: a ::= x | .x.a | a1 a2 | C(va) | case a of (Ci(vxi) . ai)i.I \nWe writevz for a sequence of elements of the syntactic class z, and |vz| for the number of elements in \nsuch a sequence. The reduction rules for inductive types are: ' (.x.a) a . a{x . a '}case C(va) of (Ci(vxi) \n. ai)i.I . aj{vxj .va}if C = Cj and |va| = |vxj|' G(a) . G(a ') if a . a Extended terms and weak symbolic \nreduction are modi.ed accord\u00adingly, taking into account the fact that a case construct can be ap\u00adplied \nto an accumulator, and needs to be remembered unevaluated as a whole in this case. Extended terms: b \n::= x | .x.b | b1 b2 | [k] | C(vb) | case b of (Ci(vxi) . bi)i.I Accumulators: k ::= x | kv | case k \nof (Ci(vxi) . bi)i.I Values: v ::= .x.b | C(vv) | [k] Reduction contexts: Gv ::=[] v | b [] | C(vb,[],vv) \n| case [] of (Ci(vxi) . bi)i.I Figure 1 de.nes the weak reduction rules and the normalization and readback \nprocedures in the presence of inductive types. As in the case of the conditional construct in section \n3.1, there are two weak reduction rules for case, depending on whether the argument is a constructed \nvalue C(vv) or an accumulator [k]. In the former case, re\u00adduction proceeds with the appropriate arm of \nthe case construct. In the latter case, an accumulator is built that remembers the whole case statement. \nThe readback functions are as in section 3.1, with the treatment of case generalizing that of if... then... \nelse: we recursively nor\u00admalize each arm of the case, after replacing the pattern-bound vari\u00adables xi \nby run-time representations of fresh free variables; then, a case statement is reconstructed from the \nnormal forms of the arms and the readback of the accumulator being matched upon. To em\u00adphasize the fact \nthat the case construct can be compiled, and thus its arms are not necessarily recoverable from the code, \nwe present the recursive normalization of each arm Ci(vxi) . bi as an appli\u00adcation of the matching function \n.x.case x of ... to the argument -. Ci([y i]), composed of the constructor of the ith arm applied to \nthe correct number of fresh free variables. The compiled code for the matching function then selects \nthe ith arm and perform the substitu\u00ad -. tion vxi . [y i] all by itself.  3.3 Fixpoints The Calculus \nof Constructions supports the de.nition of recursive functions via .xpoints. Fixpoints are introduced \nby a family of op\u00aderators fixn, where the positive integer n indicates the position of the argument that \nis used to guard the recursion and prevent in.nite unrolling: Writing b = . f ..x1 ....xn.b ' , fixn(b) \nv1 ... vn-1 (C(vv)) . b '{ f . b,x1 . v1,...,xn-1 . vn-1,xn . C(vv)} fixn(b) v1 ... vn-1 [k] . [fixn(b) \nv1 ... vn-1 k] R (fixn(b) v1 ... vi)= N (fixn(b)) R (v1) ... R (vi) if i <n R '(fixn(b) v1 ... vn-1 k)= \nN (fixn(b)) R (v1) ... R (vn-1) R '(k) N (fixn(b)) = fixn(.g..y1 ....yn.N (b [g ][y 1] ... [y n]) where \ng, y1, ..., yn are fresh Figure 2. Symbolic weak reduction and readback for .xpoints Terms: a ::= ... \n| fixn(. f ..x1 ....xn.a) The reduction rule for the fixn operators is not the standard un\u00adrolling rule \nfixn(. f .a) . a{ f . fixn(. f .a)} since such a rule would allow in.nite unrolling, and thus ren\u00adder \ntype-checking undecidable. Instead, the calculus provides a guarded unrolling rule, allowing unrolling \nof the recursive def\u00adinition only if the nth argument is a constructed term: writing a = fixn(. f ..x1 \n....xn.a '), we have aa1 ... an-1 (C(van)) . a '{ f . a,x1 . a1,...,xn-1 . an-1,xn . C(van)} ' Further \nstatically-checked restrictions on the body a of the recur\u00adsive de.nition ensures that it recursively \napplies f only to strict subterms of the nth argument C(van). This ensures that all recursive de.nitions \nproceed by structural induction, and are thus guaranteed to normalize strongly. At .rst sight, this restriction \nmight appear to restrict severely the expressiveness of the logic, but this is not so: the guard argument \nover which the function is structurally recursive can also be an inductive proof term such as a proof \nof accessibil\u00adity in a well-founded ordering, guaranteeing the termination of the function. Thus, general \nrecursive de.nitions can also be handled, as long as they are provably total in the logic. As a consequence \nof the guarded unrolling rule, applications of fixn(b) to fewer than n values are values (weak normal \nforms), while applications of fixn(b) to n - 1 values and an accumulator are themselves accumulators: \nExtended terms: b ::= ...| fixn(. f ..x1 ....xn.b) Values: v ::= ...| fixn(. f ..x1 ....xn.b) v1 ... \nvi if 0 = i < n Accumulators: k ::= ...| fixn(. f ..x1 ....xn.b) v1 ... vn-1 k The additional reduction \nrules and readback rules for .xpoints are shown in .gure 2. The main point to notice is that in the source \ncalculus, the normal form of a non-applied .xpoint ' fixn(. f ..x1 ....xn.a) is simply fixn(. f ..x1 \n....xn.a ') where a is the normal form of a. In other terms, we normalize a without assuming anything \nknown on the function name f nor on the pa\u00adrameters xi, and in particular without assuming that f unrolls \nto fixn(...): the guarded unrolling rule does not apply here. For the same reason, an application of \nfixn(a) to fewer than n arguments in normal form, or to n -1 normal forms and one normal form that is \nnot a constructor application, is itself in normal form.  4 An Abstract Machine for Weak Symbolic Reduction \nWe now turn to implementing weak symbolic reduction by com\u00adpilation to a suitable abstract machine. This \nabstract machine is a slight extension of the ZAM, which underlies the bytecode inter\u00adpreter of Objective \nCaml [16, 17], In the terminology of [20], the ZAM is an environment-and closure-based abstract machine \nfol\u00adlowing the push-enter model, and implementing a call-by-value evaluation strategy. The purpose of \nthis section is to demonstrate that minor modi.ca\u00adtions of an existing abstract machine for weak reduction \nsuf.ce to turn it into an abstract machine for weak symbolic reduction, with\u00adout impacting signi.cantly \nthe evaluation speed for closed terms. Consequently, we present a minimal subset of the ZAM, omitting \nseveral aspects of the real ZAM that are irrelevant for our purposes, such as the optimization of tail-calls, \nthe use of a register to cache the top of the stack, and the actual representation of environments (as \na stack-allocated part and a heap-allocated part) and closures (single block, minimal environments). \nThese omitted aspects are discussed in [16] and in the .rst author s forthcoming dissertation. 4.1 Machine \nStates and Machine Values The machine state has four components: A code pointer c representing the code \nbeing executed as a sequence of instructions.  An environment e: a sequence of machine values v1 ...v \nn as\u00adsociating the value vi to the variable having de Bruijn index i.  A stack s (a sequence of machine \nvalues and return contexts) holding function arguments, intermediate results, and func\u00adtion return contexts. \n An integer n counting the number of function arguments avail\u00adable on the stack.  The machine-level \nvalues v manipulated by the abstract machine are pointers to heap blocks, written [T : v1 ...v n], where \nT is a tag attached to the block (a small integer), and v1 ...v n are the values contained in the block. \nWe use tag 0 for representations of accumu\u00adlator terms [k], tags 1...n to encode the constructor C of \nconstructed terms C(vv), and a distinct tag T. for function closures. The values of the calculus are \nrepresented by heap blocks as follows: A function value is represented by a closure [T. : c,e] of the \ncompiled code c for the function body, and an environment e associating values to the variables free \nin .x.b.  For inductive types, we assume that the constructors of an in\u00adductive type are numbered consecutively \nstarting at 1 when  the type is declared. We write #C for the tag number associ\u00adated with constructor \nC. The value C(v ) is then represented by the heap block [#C :vv ], wherevv are the representations of \nv . Finally, an accumulator [k] is represented by the heap block [0: ACCU,k ]. This is a pseudo-closure \nwith the single machine instruction ACCU as code part, and an encoding k of k as envi\u00adronment part. This \nencoding is as follows: [0: x] represents the free variable x.  [1: k ,v 1,...,v n] represents the \nsuspended application kv1 ... vn.  [2: k ,c,e] represents the suspended case state\u00adment case k of (Ci(vxi) \n. bi), where c and e are the code and the environment for the function  .x.case x of (Ci(vxi) . bi). \n[3: c,e,k ] represents the suspended .xpoint application fixn(. f ..x1 ....xn.b) v1 ... vn-1 k, where \nc and e are the code and the environment for the partially applied .xpoint fixn(. f ..x1 ....xn.b) v1 \n... vn-1. The transitions of the abstract machine are shown in .gure 3, and the halting con.gurations \nin .gure 4. 4.2 Compilation and Execution of Functions The compilation scheme for the ZAM is presented \nas a function [[b]] c, where b is an expression and c an instruction sequence repre\u00adsenting the continuation \nof b. It returns an instruction sequence that evaluates b, leaves its value at the top of the stack, \nand continues in sequence by executing the code c. For the pure .-calculus, the compilation scheme is \nde.ned as follows: [[x]] c = ACCESS(i);c where i is the de Bruijn index of x [[.x1 ...xm.b]] c = CLOSURE(GRAB;...;GRAB;[[b]] \nRETURN);c 'v \" m times [[bb1 ... bm]] c = PUSHRETADDR(c); [[bn]] ... [[b1]] [[b]] APPLY(m) A reference \nto a variable x is compiled to an ACCESS instruction carrying the de Bruijn index of the variable. The \nexecution of ACCESS(i) looks up the ith entry in the machine environment and pushes it on the stack. \nA curried function .x1 ...xm.b compiles to a CLOSURE instruction, which at run-time builds a closure \nof its argument (a piece of code) with the current environment, and pushes the closure on the stack. \nThe argument of CLOSURE is the code for the body b of the func\u00adtion, preceded by m GRAB instructions \nand followed by a RETURN instruction. As its name suggests, GRAB attempts to pop one function argument \nfrom the stack and add it in front of the current environment. (This corresponds exactly to a \u00dfv reduction \nstep.) This is possible only if the count n of available arguments is non-zero, and causes n to be decremented. \nIf n = 0, all arguments have been consumed al\u00adready by previous GRAB instructions, hence the curried \nfunction is partially applied. The virtual machine then returns a closure of the current code (including \nthe aborted GRAB) and the current environ\u00adment to the caller of the function. The RETURN instruction \nis the dual of GRAB: if all arguments have been consumed (n = 0), the value of the function body is returned \nto its caller; if some arguments remain (n >0), the curried function was over-applied , and its result, \nwhich must be a function, is to be applied to the remaining argument. The latter is achieved by tail\u00adcalling \nthe closure found on the top of the stack with the remaining arguments. The code for a multiple application \nbb1 ... bm .rst pushes a re\u00adturn frame (c,e,n) containing the code c to be executed when the applied \nfunction returns, as well as the current environment and argument count (instruction PUSHRETADDR). Then, \nthe arguments bm ...b1 and the function b are evaluated right-to-left, and their val\u00adues pushed on the \nstack. Finally, the APPLY(m) instruction branches to the code of the closure obtained by evaluating b, \nsetting the ar\u00adgument count to m. The push-enter nature of the ZAM is apparent in the fact that we compile \ncurried functions and multiple applications as a whole, and evaluate arguments right-to-left. For an \napplication bb1 ... bm, this avoids the construction of intermediate closures representing the applications \nbb1, (bb1) b2, ..., like an eval-apply machine such as the SECD would do. The compilation scheme given \nabove remains correct if applied to non-maximal curried functions and applications (e.g. if we treat \nbb1 b2 as two applications, (bb1) b2), but becomes less ef.cient at run-time. At this point, the reader \nis probably wondering how we treat the \u00dfs reduction rule, corresponding to the application of an accumulator \n[k]. Recall that such an accumulator is represented by a pseudo\u00adclosure [0: ACCU,k ]. Thus, the APPLY \ninstruction will cause the ACCU instruction to be executed with k being moved to the environ\u00adment register \nof the machine. The ACCU instruction simply pops all provided arguments v1,...,v n off the stack, constructs \nan accumu\u00adlator [0: ACCU,[1: k ,v 1,...,v n]] representing the application of k to these arguments, and \nreturns this accumulator to the caller. Thus, the effect of the \u00dfs reduction rule is achieved without \ntesting at run\u00adtime whether the function part of an application is a real closure or an accumulator. \nIn other terms, we implement symbolic weak re\u00adduction of function applications without any run-time overhead \non applications of regular functions.  4.3 Inductive Types Inductive types lead to the addition of the \nfollowing two compila\u00adtion rules: [[C(b1,...,bn)]] c =[[bn]] ... [[b1]] MAKEBLOCK(n,#C);c [[case b of \n(C1(vx1) . b1 ... Cn(vxn) . bn)]] c = PUSHRETADDR(c);SWITCH([[b1]] RETURN,...,[[bn]] RETURN) In the latter \nrule, we assume that the arms of the case are ordered so that #Ci = i for i = 1,...,n. No generality \nis lost since the type system of the Constructions guarantees the exhaustiveness of case statements. \nThe compilation and execution of constructor applications is straightforward: the arguments of the constructor \nare evaluated, and the MAKEBLOCK instruction then creates the representation of the constructed term, \ntagged with the constructor number. For the case statement, a return frame to the continuation c is pushed \n.rst. The SWITCH instruction then discriminates on the tag of the matched value, and branches to the \ncode of the corresponding case arm, after adding the .elds of the matched value to the envi\u00adif n >0 \nCode Environment Stack Num. arg. ACCESS(i);c c e e s e(i).s n n CLOSURE(c ');c c e e s [T. : c ' ,e].s \nn n PUSHRETADDR(c ');c c e e s (c ' ,e,n).s n n APPLY(i) c ' e e ' [T : c ' ,e '] : s s n i GRAB;c c \ne v.e v : s s n + 1 n c0 = GRAB;c c ' e e ' (c ' ,e ' ,n ' ).s [T. : c0,e] : s 0 n ' RETURN c ' e e ' \nv.(c ' ,e ' ,n ' ).s v.s 0 n ' RETURN c ' e e ' [T : c ' ,e '].s s n n ACCU c ' k e ' v1 ...vn.(c ' ,e \n' ,n ' ) : s [0: ACCU,[1: k,v1,...,vn]].s n n ' MAKEBLOCK(T,m);c c e e v1 ...vm.s [T : v1,...,vm].s n \nn SWITCH(c1,...,cm) cT e vp ...v1.e [T : v1,...,vp].s s n 0 c0 = SWITCH(c1,...,cm) RETURN e e [0: ACCU,k].s \n[0: ACCU,[2: k,c0,e]].s n 0 CLOSUREREC(c ');c c e e s v.s n n GRABREC;c c e [T :v ].e [T :v ].s s n + \n1 n GRABREC;c RETURN e e [0: ACCU,k].s [0: ACCU,[3: c,e,k]].s n + 1 n c0 = GRABREC;c c ' e e ' (c ' ,e \n' ,n ' ).s [T. : c0,e].s 0 n ' if 1 = T = m ' where v =[T. : c ,v.e] if T > 0 Figure 3. Transitions \nof the abstract machine. Each two-line entry represents a transition; the .rst line is the machine state \nbefore the transition, and the second line is the state after. Code Environment Stack Num. arg. Result \nvalue RETURN e v.e 0 v c0 = GRAB;c e e 0 [T. : c0,e] c0 = GRABREC;c e e 0 [T. : c0,e] GRABREC;c e [0: \nACCU,k].e 1 [0: ACCU,[3: c,e,k]] Figure 4. Final con.gurations for the abstract machine. The rightmost \ncolumn is the result value (weak head normal form) for the execution ronment, thus binding the pattern \nvariables vxi. The RETURN at the end of the code for each arm then restores the original environment \nand branches back to the continuation c. (The SWITCH instruction is careful to set the count of extra \narguments to 0, ensuring that the RETURN will never perform over-application.) If the matched value has \ntag 0, denoting a case on an accumula\u00adtor k, the SWITCH instruction builds the accumulator [0: ACCU,[2: \nk,c0,e]] where c0 is the code consisting of the SWITCH instruction and the code for the arms of the case. \nLater, the readback proce\u00addure can restart the abstract machine with code c0, environment e, stack v.e \nand number of arguments 0, where v is the machine repre\u00adsentation of Ci(vy i). The code for the ith arm \nwill evaluate, then stop on its .nal RETURN instruction, since a .nal con.guration is reached (.rst case \nin .gure 4). Thus, this achieves the effect of computing the value of bi{vxi .vy i}. This replay facility \nis the reason why a PUSHRETADDR-RETURN pair is used to implement the control-.ow merging of the arms \nof the case, rather than a more standard GOTO instruction. Apart from the slightly higher cost of the \nPUSHRETADDR-RETURN combination compared with a GOTO, this compilation scheme support symbolic execution \nand readback for case statements without impacting the evaluation speed on non-symbolic, closed terms. \n(The actual im\u00adplementation uses a jump table for the SWITCH instruction, cover\u00ading both the case tag \n= 0 and tag = 1, so that an additional test for tag = 0 is not required.)  4.4 Fixpoints The compilation \nof .xpoint de.nitions is as follows: [[fixn(. f ..x1 ....xn.b)]] c = CLOSUREREC(GRAB;...;GRAB;GRABREC;[[b]] \nRETURN);c 'v \" n-1 times The CLOSUREREC instruction constructs a closure for a recursive function. In \nthe simpli.ed presentation given in this paper, this is a cyclic closure v =[T. : c,v.e] where the .rst \nslot of the environment, corresponding to the recursive variable f in the source term, points back to \nthe closure itself [8]. (The actual implementation uses the scheme described in [1] instead of cyclic \nclosures.) The code part of the recursive closure consists of the compilation of the body b of the de.nition, \npreceded by n - 1 GRAB instructions and one GRABREC instruction. The n - 1 GRAB instructions absorb the \n.rst n - 1 parameters, which are not subject to a guard condi\u00adtion. The last parameter, however, is guarded: \nevaluation should not proceed if it is bound to an accumulator. Hence, GRABREC checks whether the argument \nis an accumulator, and if so, constructs and returns an accumulator representing the suspended application \nof the .xpoint.  5 A Mechanically-checked Proof of Correct\u00adness The .rst author has developed a fully \nformal speci.cation and proof of correctness of the approach described in this paper, using the Coq proof \nassistant. Such a mechanically-checked correctness proof is required to ensure that we do not compromise \nthe logical consis\u00adtency of the Coq system by plugging a buggy evaluator in it. (The correctness of the \nkernel of the Coq proof-and type-checker has previously been mechanically proved in Coq by Barras [5, \n3].) We now give a high-level overview of this 5000-line development; a detailed presentation is beyond \nthe scope of this paper and will be published separately. The bulk of the development consists of proving \nthat the modi.ed ZAM machine and its compilation scheme (including the optimiza\u00adtions that we left out \nin section 4) faithfully implement the weak symbolic reduction semantics. The remainder of the proof \nshows the correctness of the readback, strong normalization and equiva\u00adlence testing procedures; this \npart of the proof is much shorter and easier, and follows the lines of section 2.3, using de Bruijn indices \ninstead of variable names. Following [13], the correctness of the modi.ed ZAM and its compi\u00adlation scheme \nis established by proving a simulation result between the reductions of the source term and the transitions \nof the abstract machine executing the compiled code for the term. First, we set up a decompilation relation \nS . b between machine states S =(c,e,s,n) and source terms b, and also between machine values and source \nvalues v . v. This decompilation relation can be viewed as a left inverse of the compilation function: \nif we compile a term and build an initial machine state with this code, the decompilation of this initial \nstate gives us back the original term. However, decompila\u00adtion is also de.ned for intermediate machine \nstates, reached after one or several transitions, and where the code part of the state is not the image \nof a source term by the compilation function. Several examples of decompilation relations for different, \nsimpler abstract machines are shown in [13]. Ours is broadly similar, with the ex\u00adception that [13] uses \nexplicit substitutions in the source language, while we perform classical substitution as part of the \ndecompilation predicate in order to keep the source language unchanged. Once the decompilation relation \nis set up, we can show the main simulation result: one transition of the abstract machine corre\u00adsponds \nto zero, one or several reductions on the source term. LEMMA 7(SIMULATION). If S . b and the machine \nperforms a transition from state S to state S ' , then there exists a source term b ' * such that S '. \nb ' and b . b ' . We also show that initial states of the machine decompile to the original term. There \nare two kinds of initial states of interest: ([[b]] RETURN,e,e,0), corresponding to executing the code \nof a closed term b, and (APPLY(n),e, f .v 1 ...v n.e,0), corresponding to applying the closure f to the \narguments v1,...,v n during the read\u00adback procedure. LEMMA 8(INITIAL STATES). 1. ([[b]] RETURN,e,e,0) \n. b if b is closed. 2. (APPLY(n),e, f .v 1 ...v n.e,0) . fv1 ... vnif f . f and v i . vi for i = 1,...,n. \n Symmetrically, .nal machine con.gurations decompile to values. LEMMA 9(FINAL STATES). If S is a .nal \nmachine con.guration and v the associated return value, as de.ned in .gure 4, then there exists a source \nvalue v such that S . v and v . v. To show the correctness of the abstract machine and its compilation \nscheme, it remains to show two properties. First, the machine does not get stuck when executing the compiled \ncode of a term that is not stuck. LEMMA 10 (PROGRESS). If S . b, and S is not a .nal state, and Test \nOur system Coq CBV Coq Lazy OCaml bytecode 1. Normalization of factorial(9) (Peano integers) 2. Normalization \nof is even(factorial(9)) (Peano integers) 3. Normalization of 256 \u00d7 64 (Church integers) 4. Normalization \nof .x..y. (128 + x) \u00d7 (128 + y) (Church integers) 5. Equivalence of factorial(8) and factorial '(8) (Peano \nintegers) 6. Equivalence of 256 \u00d7 64 and 64 \u00d7 256 (Church integers) 14.2s 0.447s 0.106s 0.082s 0.096s \n0.094s 61.6s 46.9s 0.116s 0.107s n/a n/a 466s 4.82s 1.99s 1.81s 9.02s 2.00s 0.347s 0.357s n/a n/a 0.085s \nn/a Figure 5. Benchmark results for the synthetic tests (on a Pentium III 1Ghz, 256 Mb) Proof Our system \nCoq OCaml bytecode OCaml native Coq s standard theories 4-color theorem, perimeter 11 4-color theorem, \nperimeter 12 4-color theorem, perimeter 13 4-color theorem, perimeter 14 135s 1.68s 6.50s 14.8s 69.6s \n131s 56.7s 259s 680s out of memory n/a 1.18s 6.18s 15.5s 73.1s n/a 0.30s 1.92s 4.11s 19.8s Figure 6. \nBenchmark results for the checking of actual proofs (on a Pentium III 1Ghz, 256 Mb) b is not stuck (i.e. \nb reduces or b is a value), then the machine can perform a transition from S. Second, the abstract machine \nalways terminates when given the code for a term that evaluates to a value. Given the simulation lemma \n7, the only way the machine could fail to terminate is by performing an in.nite number of consecutive \nsilent transitions, that is, transitions between two states that decompile to the same term. To show \nthat this cannot happen, we de.ne a non-negative integer measure |S| on machine states S, and show that \nit strictly decreases at each silent transition. Again, [13] gives examples of such measures for other \nmachines. LEMMA 11 (NO STUTTERING). If the machine performs a tran\u00adsition from S to S ' , and S . b and \nS '. b, then |S '| < |S|. As a corollary of the previous lemmas, we obtain the total correct\u00adness of \nthe abstract machine and its compilation scheme: THEOREM 2. Let S be a machine state that decompiles \nto b. If * b . V (b), then the abstract machine started in state S terminates with a return value If \nb diverges (re\u00ad v that decompiles to V (b). duces in.nitely), the abstract machine started in state S \nperforms an in.nite number of transitions.  6 Experimental Results The .rst author has implemented the \napproach presented here in the context of the Coq proof assistant, version 7. The implementa\u00adtion consists \nof a bytecode compiler and a readback procedure and equivalence tester written in OCaml, and an abstract \nmachine inter\u00adpreter written in C. The Coq proof checker was modi.ed to use our equivalence tester. Synthetic \ntests. Figure 5 gives the time it takes to strongly normal\u00adize or to decide the equality of various arti.cial \ntest terms. For com\u00adparison purposes, we also give timings for the current interpreter\u00adbased normalization \nand equivalence testing procedures of the Coq system. For normalization, Coq supports both call-by-value \nand lazy call-by-name strategies, while only the latter is supported for equivalence testing. When the \nexample does not require normal\u00adization under ., we also give the times for the OCaml bytecode interpreter. \nThe timings for our system include compilation times in addition to evaluation times, in order to make \nthe comparison with Coq s interpreter fairer. Overall, our compiled implementation is 10 to 100 times \nfaster than Coq s lazy call-by-name interpreter, and 1.1 to 100 times faster than Coq s call-by-value \ninterpreter. Example 1 computes factorial 9 using Peano integers. This involves no normalization under \n.. The result of the weak evaluation is 362880 applications of the constructor Succ to the constant Zero, \nwhich is then transformed into an isomorphic source term during readback. Example 2 computes the parity \nof factorial 9. Compared with example 1, example 2 involves more work during weak re\u00adduction, but much \nless work during readback, since its result is an atomic constant. It therefore normalizes much faster \nthan exam\u00adple 1 in our system, approaching the speed of the OCaml bytecode interpreter. Example 3 computes \n256 \u00d7 64 using Church integers. Normaliza\u00adtion under . is required, and the normal form is quite large, \nrequir\u00ading signi.cant readback effort. Example 4 computes .x..y. (128 + x) \u00d7 (128 + y) using Peano arithmetic; \nit requires not only normal\u00adization under ., but also large amounts of normalization of case and fix \nconstructs applied to the free variables x, y. In both cases, we observe signi.cant speed-ups compared \nwith Coq s lazy call\u00adby-name interpreter, but only a 10% to 30% improvement compared with Coq s call-by-value \ninterpreter. Examples 5 and 6 perform equivalence testing instead of strong nor\u00admalization. In both cases, \nwe compare two terms that evaluate to the same (large) normal form, but are syntactically different enough \nto force full reduction to determine their equivalence. Again, we observe speedups of 20 to 100 compared \nwith Coq s original equiv\u00adalence test. Proof checking. Figure 6 gives total proof checking times obtained \nwith the original Coq implementation and with our modi.ed imple\u00admentation. The ef.ciency of our implementation \ndepends greatly on whether the proofs being checked involve signi.cant amount of computation. For proofs \nthat involve few computations, such as the standard library of theories distributed with the Coq system, \nour im\u00adplementation actually leads to a 3% slowdown compared with the original Coq implementation. This \nwas to be expected, since the beta-equality tests performed are trivial: the terms to be compared are \noften in normal form already, thus requiring very little computa\u00adtion to show that they are equal. This \nis the least favorable case for our implementation, since we pay the price of compiling the terms down \nto bytecode, yet gain essentially nothing in return. However, this compilation overhead is low enough \nto be acceptable. At the other end of the spectrum, proofs that involve signi.cant amounts of computation \nexhibit speedups by one order of magni\u00adtude. As a representative example, we used Gonthier and Werner \ns Coq proof of the 4-color theorem, and more precisely the part of the proof that checks 4-colorability \nof a large number of elementary planar graphs. This part of the proof consists mostly in evaluating a \nfunction deciding 4-colorability on the elementary graphs. Here, our implementation is 33 to 45 times \nfaster than the original Coq system. Owing to its more ef.cient use of memory, our implemen\u00adtation is \nable to complete the checking of the whole proof, while the original Coq runs out of memory on the largest \ncon.gurations (of perimeter 14). To compare the performances of our reducer with those of the OCaml system, \nwe used Coq s extraction facility to extract from the Coq development the Caml code that de.nes the decision \nprocedure and applies it to the elementary graphs. As .gure 6 shows, our re\u00adducer runs about as fast \nas OCaml s bytecode interpreter; the speed ratio varies between 1.4 and 0.95. Compiling the extracted \nCaml code with the OCaml native-code compiler results in speed ratios between 3.5 and 5.6, which is typical \nof the speed-ups obtained by going from bytecode interpretation to native-code generation. 7 Related \nWork Implementations of strong reduction. The work most closely re\u00adlated to us is Cr\u00b4egut s abstract \nmachine for strong reduction [9, 10]. This machine is derived from Krivine s machine and implements a \nlazy evaluation strategy. The code is executed by expansion to Motorola 68000 assembly code. Like ours, \nCr\u00b4egut s machine can handle terms containing free variables. However, reductions un\u00adder lambdas are \nnot performed by a separate readback phase as in our approach, but directly by the abstract machine: \nwhen encoun\u00adtering a term .x.b in head position, Cr\u00b4egut s machine immediately proceeds to reducing b \n(with a free x variable). This is potentially faster than our readback scheme when the goal is to compute \nthe normal form of a term, but prevents early stopping when the goal is to compare two terms for \u00df-equality. \nAlso, strong reduction of case statements and recursive de.nitions is mentioned in [10] but not formalized. \nSome of the mechanisms we use can be found in optimized inter\u00adpreters performing strong reduction. Barras \nreducer [4] uses envi\u00adronments and closures, and a reduction strategy based on Cr\u00b4egut s machine. Nadathur \nand Wilson [19] represent terms in such as way that reduction can be easily stopped at weak head normal \nform, while keeping enough information to restart strong evaluation later. Still, these approaches remain \nessentially based on interpretation of a tree-based data structure, without compilation to bytecode. \nOnline partial evaluation. The connections between strong nor\u00admalization and partial evaluation (program \nspecialization) [15] are well known: specializing a function .x..y.a for x = b amounts to strongly normalizing \nthe partial application (.x..y.a) b. Classical partial evaluation comes in two .avors: of.ine partial \nevaluation, where a preliminary binding-time analysis classi.es sub-terms into dynamic and static terms; \nand online partial evaluation, where the tests static or dynamic? are performed on the .y during special\u00adization. \nOur approach does not rely on binding-time analysis, and therefore is a form of online partial evaluation. \nOur decomposition of strong normalization into (compiled) weak symbolic reduction and (interpreted) readback \nis non-standard in traditional online partial evaluation. However, a similar decompo\u00adsition is used in \nthe online type-directed partial evaluation of Danvy [12] and Sumii and Kobayashi [22]. (The type-directed \nquali.er is a bit of a misnomer, since this approach does not exploit static typing information, unlike \nthe original type-directed partial evalua\u00adtion discussed below.) The main difference between online TDPE \nand our work is that online TDPE, like traditional online partial evaluation, operates on a standard \ntwo-level language, and instru\u00adments elementary operations of the language (function applications, case \nstatements, etc) with run-time tests static or dynamic? . In contrast, our presentation in terms of accumulators \ninstead of a 2\u00adlevel language allows implementing function application and case statements without explicit \nrun-time tests proper value or accumu\u00adlator? , thus entailing essentially no additional cost compared \nwith a standard, non-symbolic weak evaluator. However, our approach requires a specially adapted virtual \nmachine, while online TDPE can be implemented on top of any functional language using only source-level \ntransformations. TDPE and normalization by evaluation. Type-directed partial evaluation (TDPE), also \nknown as normalization by evaluation, computes normal forms by combining a standard weak evaluator with \na type-directed rei.cation procedure that reconstructs normal\u00adized terms from values [11, 6]. A function \nvalue is rei.ed by ap\u00adplying it to a generic argument derived from the type of the func\u00adtion domain (an \n.-long form of a free variable, where all dynamic application nodes are replaced by static application \nnodes), and re\u00adcursively normalizing this application. This alternation between evaluation and rei.cation \nlooks super.\u00adcially similar to our approach, but differs in several key aspects. First, TDPE uses .-expansions \nand produces an .-long \u00df. normal form, while our approach does not perform .-expansions and pro\u00adduces \npure \u00df normal forms. Since . conversion is not valid in the Calculus of Constructions, TDPE is not applicable \nin our context. The second difference between TDPE and our approach is that the rei.cation procedure \nof TDPE is based on the types of the terms to be normalized, while our readback procedure proceeds by \nexamina\u00adtion of the shape of their values, without needing type information. While the recursion on the \ntype structure that underlies TDPE is extremely elegant, it has been worked out only for simply-typed \nlambda calculus and for system F [24]. Extending TDPE to a type system as rich as that of the Constructions \nis an open issue. By abandoning the guidance of the type system, our approach loses one of the advantages \nof TDPE: the ability to reuse an existing weak evaluator unchanged. Indeed, we had to modify the abstract \nmachine so that it deals correctly with free variables (accumula\u00adtors). However, our modi.cations are \nminimal, and this advantage of TDPE holds only when the language contains only functions and products: \nto handle sums, TDPE requires a weak evaluator that fea\u00adtures control operators (prompts or call/cc), \nwhich few do; in contrast, our approach requires a few more modi.cations for sums, but the total of these \nmodi.cations still requires less implementa\u00adtion work than adding control operators to a weak evaluator \nthat does not have them initially. 8 Conclusions and Future Work We have shown how minimal modi.cations \nto an existing abstract machine-based weak evaluator, combined with a simple, type\u00adoblivious readback \nprocedure, lead to an ef.cient implementation of strong reduction and equivalence testing for a purely \nfunctional language featuring products, sums, and guarded recursion. We have presented this approach \nin the context of call-by-value weak evaluation and an environment-based abstract machine. It would be \ninteresting to investigate their applicability to other weak evaluators, such as lazy evaluation and \ngraph reduction. The application of this work to the Coq proof assistant raises in\u00adteresting type-theoretic \nissues. First, we test equivalence between type-erased terms, while the Calculus of Construction performs \nconversion between type-annotated terms. The logical consistency of the former approach follows from \nMiquel s model-theoretic re\u00adsults [18] in the case of the core Calculus of Constructions, but remains \nto be extended to inductive types. A related issue is the treatment of proof terms during equivalence \nchecking. The types being compared for equivalence can contain arbitrary proof terms. Proof terms are \noften large and costly to normalize. However, the general principle of proof irrelevance sug\u00adgests that \nit might not be necessary to normalize them, since a proof of a given proposition is (morally) just as \ngood as any other proof of this proposition. This suggests replacing all proof terms con\u00adtained in types \nby a single constant P (a nullary constructor) before testing the equivalence of two types. Again, more \nwork is needed to prove that the corresponding relaxed conversion rule is logically consistent.  Acknowledgments \nHenri Laulh`ere conducted preliminary experiments on modifying the Objective Caml compiler to implement \nstrong normalization circa 1998; the results of these experiments were communicated and explained to \nus by Alexandre Miquel, and partly inspired this work. We thank Bruno Barras and Benjamin Werner for \ntheir exper\u00adtise on Coq and the Calculus of Constructions, and the anonymous reviewers for pointing us \nto the work on online TDPE. References [1] A. W. Appel. Compiling with continuations. Cambridge Uni\u00adversity \nPress, 1992. [2] K. Appel and W. Haken. Every planar map is four colorable. Illinois J. Math, 21:429 \n567, 1977. [3] B. Barras. Auto-validation d un syst`eme de preuves avec familles inductives. PhD thesis, \nUniversity Paris 7, 1999. [4] B. Barras. Programming and computing in HOL. In Theorem Proving in Higher \nOrder Logics 2000, volume 1869 of LNCS, pages 17 37. Springer-Verlag, 2000. [5] B. Barras and B. Werner. \nCoq in Coq. Submitted for publica\u00adtion, 2000. [6] U. Berger and H. Schwichtenberg. An inverse of the \nevalu\u00adation functional for typed .-calculus. In Logic in Computer Science 91, pages 203 211. IEEE Computer \nSociety Press, 1991. [7] T. Coquand and G. Huet. The calculus of Constructions. Inf. and Comp., 76(2/3):95 \n120, 1988. [8] G. Cousineau, P.-L. Curien, and M. Mauny. The categor\u00adical abstract machine. Science of \nComputer Programming, 8(2):173 202, 1987. [9] P. Cr\u00b4egut. An abstract machine for lambda-terms normaliza\u00adtion. \nIn Lisp and Functional Programming 1990, pages 333 340. ACM Press, 1990. [10] P. Cr\u00b4egut. Machines `a \nenvironnement pour la r\u00b4eduction sym\u00adbolique et l \u00b4evaluation partielle. PhD thesis, University Paris \n7, 1991. [11] O. Danvy. Type-directed partial evaluation. In 23rd symp. Principles of Progr. Lang, pages \n242 257. ACM Press, 1996. [12] O. Danvy. Online type-directed partial evaluation. Technical Report RS-97-53, \nBRICS, 1997. [13] T. Hardin, L. Maranget, and B. Pagano. Functional runtimes within the lambda-sigma \ncalculus. Journal of Functional Pro\u00adgramming, 8(2):131 176, 1998. [14] R. Harper, F. Honsell, and G. \nPlotkin. A framework for de.n\u00ading logics. J. ACM, 40(1):143 184, 1993. [15] N. D. Jones, C. K. Gomard, \nand P. Sestoft. Partial Evaluation and Automatic Program Generation. Prentice-Hall, 1993. [16] X. Leroy. \nThe ZINC experiment: an economical implementa\u00adtion of the ML language. Technical report 117, INRIA, 1990. \n[17] X. Leroy, D. Doligez, J. Garrigue, and J. Vouillon. The Ob\u00adjective Caml system. Software and documentation \navailable on the Web, http://caml.inria.fr/, 1996 2002. [18] A. Miquel. Le Calcul des Constructions implicite: \nsyntaxe et s\u00b4emantique. PhD thesis, University Paris 7, 2001. [19] G. Nadathur and D. S. Wilson. A notation \nfor lambda terms: A generalization of environments. Theoretical Comput. Sci., 198:49 98, 1998. [20] S. \nL. Peyton Jones. Implementing lazy functional languages on stock hardware: the spineless tagless G-machine. \nJournal of Functional Programming, 2(2):127 202, 1992. [21] F. Pfenning and C. Paulin-Mohring. Inductively \nde.ned types in the Calculus of Constructions. In Mathematical Founda\u00adtions of Programming Semantics, \nvolume 442 of LNCS, pages 209 228. Springer-Verlag, 1990. [22] E. Sumii and N. Kobayashi. Online type-directed \npartial eval\u00aduation for dynamically-typed languages. Computer Software, 17(3):38 62, 2000. Iwanami Shoten. \n[23] K. N. Verma, J. Goubault-Larrecq, S. Prasad, and S. Arun-Kumar. Re.ecting BDDs in Coq. In 6th Asian \nComputing Science Conference (ASIAN 2000), number 1961 in LNCS, pages 162 181. Springer-Verlag, 2000. \n[24] R. Vestergaard. The polymorphic type theory of normalisation by evaluation. Draft, 2001.  \n\t\t\t", "proc_id": "581478", "abstract": "Motivated by applications to proof assistants based on dependent types, we develop and prove correct a strong reducer and &#223;-equivalence checker for the &#955;-calculus with products, sums, and guarded fixpoints. Our approach is based on compilation to the bytecode of an abstract machine performing weak reductions on non-closed terms, derived with minimal modifications from the ZAM machine used in the Objective Caml bytecode interpreter, and complemented by a recursive \"read back\" procedure. An implementation in the Coq proof assistant demonstrates important speed-ups compared with the original interpreter-based implementation of strong reduction in Coq.", "authors": [{"name": "Benjamin Gr&#233;goire", "author_profile_id": "81381590776", "affiliation": "INRIA Rocquencourt, Le Chesnay, France", "person_id": "P394757", "email_address": "", "orcid_id": ""}, {"name": "Xavier Leroy", "author_profile_id": "81100078576", "affiliation": "INRIA Rocquencourt, Le Chesnay, France", "person_id": "PP39026141", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/581478.581501", "year": "2002", "article_id": "581501", "conference": "ICFP", "title": "A compiled implementation of strong reduction", "url": "http://dl.acm.org/citation.cfm?id=581501"}