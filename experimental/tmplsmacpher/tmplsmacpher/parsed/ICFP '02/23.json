{"article_publication_date": "09-17-2002", "fulltext": "\n An Experimental Study of Renewal-Older-First Garbage Collection Lars T Hansen William D Clinger Opera \nSoftware Northeastern University Oslo, Norway Boston, MA 02115 lth@opera.com will@ccs.neu.edu Abstract \nGenerational collection has improved the ef.ciency of garbage col\u00adlection in fast-allocating programs \nby focusing on collecting young garbage, but has done little to reduce the cost of collecting a heap \ncontaining large amounts of older data. A new generational tech\u00adnique, older-.rst collection, shows promise \nin its ability to manage older data. This paper reports on an implementation study that compared two \nolder-.rst collectors to traditional (younger-.rst) generational col\u00adlectors. One of the older-.rst collectors \nperformed well and was often effective at reducing the .rst-order cost of collection relative to younger-.rst \ncollectors. Older-.rst collectors perform especially well when objects have queue-like or random lifetimes. \n Categories and Subject Descriptors D.3.4 [Programming Languages]: Processors Memory manage\u00adment (garbage \ncollection)  General Terms Algorithms, Measurement, Performance, Experimentation  Keywords generational \ngarbage collection, older-.rst 1 Introduction Garbage collection is a technology that automatically reclaims \nun\u00adreachable heap storage [24]. (As is common in the literature on garbage collection, we use live as \na synonym for reachable, and dead as a synonym for unreachable.) Generational garbage col\u00adlectors divide \nthe heap into two or more regions, known as gen\u00aderations because they often group objects of similar \nage, and col- Permission to make digital or hard copies of all or part of this work for personal or classroom \nuse is granted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. ICFP \n02, October 4-6, 2002, Pittsburgh, Pennsylvania, USA. Copyright 2002 ACM 1-58113-487-8/02/0010 ...$5.00 \nlect these generations at different times [24, 27]. Most generational garbage collectors attempt to collect \nyounger generations more fre\u00adquently than older generations, so we call them younger-.rst col\u00adlectors. \nYounger-.rst generational garbage collectors usually outperform non-generational garbage collectors. \nWhy? One pop explanation is that most objects die young , but Henry Baker revealed the inadequacy of \nthis explanation by consider\u00ading a radioactive decay model of object lifetimes, in which each live object \nhas a 50% probability of dying during the next inter\u00adval whose duration is the half-life that parameterizes \nthe model [2]. If this half-life is short, then most objects die young, but a con\u00adventional generational \ncollector will actually perform worse than a non-generational collector [9]. A more sophisticated explanation \nfor the effectiveness of genera\u00adtional garbage collectors is that they try to predict which objects are \nlikely to die soon, and how well they work depends upon the accuracy of these heuristic predictions. \nThis explanation is also incorrect. The radioactive decay model makes it impossible to pre\u00addict which \nobjects will die soon. No heuristic predictor can do bet\u00adter or worse than chance, so if this explanation \nwere correct we would expect generational collectors to perform the same as non\u00adgenerational collectors \nfor the radioactive decay model. In fact, conventional generational collectors perform worse. How do \nthey manage to do that? By collecting the wrong gener\u00adations. Younger-.rst collectors collect young objects \nmore often than old. These young objects haven t had much time to die, so collecting them doesn t recover \nmuch storage. An older-.rst gener\u00adational collector, which collects old objects more often than young, \nwould recover more storage for a similar amount of effort [9]. Why then do younger-.rst collectors usually \nperform well in prac\u00adtice? Because most programs satisfy the weak generational hypoth\u00adesis, which asserts \nthat young objects die at a considerably faster rate than older objects [19]. Hence the young generations \noften contain a higher fraction of unreachable objects than the older gen\u00aderations, despite the fact \nthat young objects have not had much time to die. The weak generational hypothesis is important. Its \ngeneralization, the strong generational hypothesis, postulates a negative correlation between age and \nmortality rate even for long-lived objects [19, 29, 30, 36]. There is little empirical support for the \nstrong generational hypothesis. Although some of our own data on object lifetimes provide new support \nfor the strong hypothesis, our data also show why the strong hypothesis does not matter very much: There \ncannot be a strong negative correlation between age and mortality rate that holds for objects of all \nages, because the mortality rate cannot be less than zero. Even if the strong generational hypothesis \nholds, the difference between the mortality rate for a group of objects of age t and the mortality rate \nfor older objects will tend to approach zero as t increases. In other words, the strong generational \nhypothesis implies that the mortality among suf.ciently long-lived objects must resemble that of a radioactive \ndecay model. Since conventional younger-.rst col\u00adlectors perform poorly for radioactive decay models, \nthis would in turn imply that conventional generational collection is inappropriate for long-lived objects. \nThis conclusion is remarkable because the strong generational hypothesis had been regarded as the primary \njusti.cation for younger-.rst collection of long-lived objects. In a previous paper, we described a novel \nalgorithm for older-.rst generational garbage collection, and calculated that, for radioac\u00adtive decay \nmodels of object lifetimes, our new collector should out\u00adperform non-generational and younger-.rst generational \ncollectors. We also described our design and implementation of a hybrid col\u00adlector that we hoped would \ncombine the advantages of younger-.rst and older-.rst collection [9]. In this paper we report on the \nperformance of that hybrid collector. 2 Renewal-Older-First (ROF) Collection In this section we describe \na pure renewal-older-.rst (ROF) gener\u00adational collector. (This algorithm is exactly the same as the non\u00adpredictive \nalgorithm that we described previously, but we have adopted the more descriptive name that Darko Stefanovi\u00b4c \ngave to it [9, 31]. Our ROF algorithm described here should not be confused with Stefanovi\u00b4c s deferred-older-.rst \n(DOF) algorithm [17, 31].) A pure renewal-older-.rst collector divides the heap into two gener\u00adations, \nand always collects the older generation. Instead of group\u00ading objects according to their actual age, \nhowever, the ROF algo\u00adrithm groups objects according to their renewal age, which is de\u00ad.ned as the time \nthat has passed since the object was last classi.ed as reachable by a collection within its generation, \nor as its actual age if it has never been considered for collection. After each collection, therefore, \nthe objects in the older generation that survived the collection are considered to be the youngest ob\u00adjects. \nWe implement this by dividing the heap into steps that contain objects of similar age, and relabel the \nsteps following each collec\u00adtion. A pure ROF collector never performs a full collection. Figure 1 illustrates \nthe ROF collector as implemented in Larceny, our implementation of Scheme [8, 16, 26]. The steps of the \nROF heap are arranged by age, from youngest to oldest; step 1 holds the youngest objects. Additional \nsteps are kept in reserve for use by a copying collector, and are not available for allocation. A policy \nparameter j determines the dividing line between the younger and older generations of the ROF heap. Steps \n1j are in the younger generation, and steps j +1k are in the older. When the dynamic area is collected, \nthe boldly outlined older gen\u00aderation (steps j +1k) is collected by evacuating its live data into the \nreserve. Then the steps are rearranged: the younger genera- Young Old Reserve 8 9 jk 6 7 k 8 9 \njk 8 9 jk 9 3 k 8 9 jk 8 9 jk 6 7 k 8 9 jk Figure 1. The ROF collector as implemented in \nLarceny. The .gure shows triples of collector con.gurations: before a collec\u00adtion, after a collection \nbut before renumbering of steps and se\u00adlection of j, and after renumbering and selection of a new value \nfor j. Shaded steps are full, and unshaded steps are empty. The thick frame surrounds the steps that \nare subjected to garbage collection, and the heavy shading shows the steps that contain the survivors \nof the collection. Steps that are not collected are simply moved. tion (steps 1 j) become the oldest \nsteps (steps k -j +1 k) of the ROF heap, and the shaded survivors of the collected gen\u00aderation (the s \nsteps of the reserve) become the youngest steps (k -j -s +1 k -j). (That is, the survivors are treated \nas if they were newly allocated objects; this is what gives the renewal-older\u00ad.rst collector its name.) \nSome of the free steps are used to replenish the reserve, and the remaining free steps become available \nfor allo\u00adcation (steps 1 k -j -s). Following collection, Larceny sets j to 1/2 the number of free steps. \nThis ensures that large circular garbage structures will be collected and that the younger generation \nof the ROF heap will not be unrea\u00adsonably large. 3 Hybrid (3ROF) Collection We implemented a hybrid \nROF collector by allowing any number of younger-.rst generations to precede the ROF heap. The hybrid \nROF collector usually performs best with only one younger-.rst generation, so that is the con.guration \nwe describe here. Our 3ROF collector is a 3-generational collector that consists of two generations that \nare collected by the ROF algorithm, plus a youngest generation, the nursery, which is collected and evacuated \nas part of every collection. The hybrid algorithm is younger-.rst in the sense that it collects the nursery \nmost often, but is older-.rst in the sense that the oldest generation is collected more often than the \nintermediate generation. In fact, the intermediate (younger ROF) generation is never col\u00adlected at all. \nDead objects within that generation are collected only after they have been folded into the older ROF \ngeneration. In addition to the usual write barrier for mutator code, the 3ROF collector itself incorporates \nan additional write barrier that records pointers from the intermediate (younger ROF) generation into \nthe older ROF generation. These pointers are created when objects are promoted from the nursery into \nthe intermediate generation, and must be traced during a major collection. This additional write bar\u00adrier \nis used only during a minor collection that promotes into the intermediate generation; the mutator does \nnot use it, nor does a ma\u00adjor collection, nor does a minor collection that promotes directly into the \nolder ROF generation. Even so, the barrier adds a cost to promotions, and this cost is par\u00adticularly \nnoticeable in Scheme programs that use many pairs. The standard collector has been tuned to handle pairs \nparticularly ef.\u00adciently, and the additional cost of the write barrier, though low in absolute terms, \nmakes up a substantial fraction of the cost of copy\u00ading and scanning a pair in the hybrid ROF collector. \nThis cost would be less signi.cant in languages like Java.  4 Larceny and its Collectors This section \ndescribes our Larceny implementation of Scheme and some characteristics of its garbage collectors [8, \n16, 17, 23, 25]. 4.1 Compiler Larceny uses the Twobit optimizing compiler to compile Scheme to SPARC \nmachine code. Previous measurements have estab\u00adlished that Twobit and Larceny together have performance \nthat is roughly competitive with Standard ML of New Jersey and with commercial Common Lisp and Scheme \nsystems [8]. A few prelim\u00adinary benchmarks also suggest that Larceny s default generational garbage collector \nis competitive with the default collector used in Sun s HotSpot Java system. Twobit incorporates a number \nof optimizations that limit heap allo\u00adcation largely to allocation performed explicitly by the source \npro\u00adgram. In particular, neither continuation frames nor environment structures are allocated on the \nheap unless the program explicitly captures the continuation or creates a closure using a lambda ex\u00adpression \nthat escapes (as determined by a .rst-order closure anal\u00adysis). Program variables that are updated by \nassignment are heap\u00adallocated, but Scheme programs are largely functional in nature and heap-allocated \nvariables are few in practice. 4.2 Collectors Larceny currently supports .ve interchangeable garbage \ncollectors, including four precise collectors that use Cheney s copying algo\u00adrithm: a conventional younger-.rst \ngenerational collector, two dif\u00adferent (ROF and DOF) hybrid older-.rst collectors, and a non\u00adgenerational \nstop-and-copy collector [7]. The three generational collectors also use a non-copying algorithm to collect \nthe large\u00adobject ( 4 Kby) spaces that are associated with each generation [20]. Generations are grouped \ninto three areas. The ephemeral area con\u00adtains the youngest collected generations, the dynamic area contains \nthe oldest collected generations, and the static area contains ob\u00adjects that are permanent. The static \narea is never collected during normal operation. In addition, some memory is kept in a reserve for copying \ncollection. The youngest ephemeral generation is called the nursery; all object allocation takes place \nin that generation. Objects are allocated by incrementing an allocation pointer; the pointer is kept \nin a machine register, and allocation is very fast. Garbage collection is driven by allocation: When \nthe nursery is full, the collector evacuates all live objects from the nursery into some older generation. \nThe collector may also choose to collect other generations at that time. A collection that collects only \nthe nursery is known as a minor col\u00adlection. A collection that collects the entire heap is a full collection. \nA collection that collects at least one generation in addition to the nursery but does not collect the \nentire heap is classi.ed as a major collection. The heap may be resized following a major or full collection. \nAll collectors use a command-line parameter 1/L, called the load fac\u00adtor, to compute the new heap size. \nThe inverse load factor L tells the collector how much memory it is permitted to use, as a multi\u00adple \nof the amount of reachable storage l. The new heap size H. is computed as H. =lL. Since l memory is already \nlive, and l mem\u00adory will be needed as reserve for the next garbage collection, the amount of memory available \nfor allocation is H.-2l.  4.3 Write Barrier and Remembered Set Generational collectors use a write barrier \nand a remembered set to keep track of intergenerational pointers, which come into play when only part \nof the heap is collected. Program Lines of code Allocation volume Peak live (est.) Promotion rate Mutator \ntime (stop+copy) Major GC (msec) Minor GC (msec) Ratio 5earley:12 658 299.0 13.5 0.21 11.04 1392 1684 \n0.83 5earley:13 658 35.0 53.17 7455 4045 1.84 * gcbench:5 226 1757.0 16.8 0.27 19.12 18824 10534 1.79 \ngcold:25,1,0 381 347.0 26.5 0.44 5.37 6947 4072 1.71 gcold:25,1,1000 381 402.0 26.5 0.38 40.56 7025 4186 \n1.68 gcold:25,10,100 381 264.0 26.5 0.06 36.89 919 538 1.71 gcold:100,1,0 381 1389.0 101.5 27.35 28840 \n11798 2.44 * gcold:100,1,1000 381 1606.0 101.5 121.72 30282 11962 2.53 * nboyer:3 767 99.0 13.0 0.45 \n4.30 1498 1154 1.30 nboyer:4 767 266.5 35.0 7.98 4905 2708 1.81 * nboyer:5 767 846.0 100.0 23.42 22660 \n8692 2.61 * 5nboyer:3 767 497.0 13.0 0.44 20.40 6784 5169 1.31 5nboyer:4 767 1470.0 35.0 38.77 21535 \n12840 1.68 * 5sboyer:4 781 259.0 13.4 0.26 61.41 1831 1816 1.01 5sboyer:5 781 687.0 30.0 121.77 7000 \n4218 1.66 * perm:200,8,1 324 229.0 11.5 1.00 4.44 11393 3955 2.88 perm:25,8,8 324 229.0 11.5 1.00 4.55 \n5578 4262 1.31 perm:200,9,1 324 2059.0 100.0 1.00 30.43 82003 33550 2.44 * perm:25,9,8 324 2059.0 100.0 \n1.00 30.53 81742 33125 2.47 * twobitlong 23,789 665.0 7.9 0.08 138.13 892 4009 0.22 twobitshort 23,789 \n119.5 7.5 0.18 22.17 652 907 0.72 5twobitshort 23,789 575.5 7.5 0.17 108.17 2804 3461 0.81 Table 1. \nCharacteristics of the benchmark programs. Allocation and peak live volume are reported in megabytes. \nThe promotion rate is the fraction of allocation that is promoted out of a 1 megabyte nursery in the \ngenerational collectors. The mutator time, in seconds, is the average across several runs of the stop-and-copy \ncollector. The gc times, in milliseconds, are the average across several runs of the 2GEN collector. \nThe gc times for major and minor collections are reported separately, and their ratio is shown. An asterisk \n(*) in the last column indicates that the benchmark was run by Clinger on a bigger and slightly faster \nmachine. In an assignment *lhs=rhs the write barrier .rst determines whether rhs is a pointer, and if \nit is, performs table lookups on lhs and rhs to determine their generation numbers. If the gener\u00adation \nnumber of rhs is less than that of lhs, then lhs is inserted into a sequential store buffer, which will \nlater be folded into the re\u00admembered set [21]. Larceny s remembered set currently uses hash tables to \n.lter duplicates from its component subsets. Larceny s older-.rst collectors require objects to be recorded \nin sev\u00aderal subsets at the same time. That requirement makes card marking and header marking less attractive, \nsince each card or object would need one mark bit for each subset of the remembered set in which it might \nbe recorded. For the benchmarks reported in this paper, the size of the extra re\u00admembered subset that \nis required by the 3ROF collector was limited to 32768 entries. When the size of that subset exceeded \nthis limit, that remembered subset and the intermediate (younger ROF) gen\u00aderation were both cleared by \nreducing the value of the parameter j that determines the boundary between the younger and older ROF \ngenerations. This effectively protected the 3ROF collector from ex\u00adcessively large remembered sets by \nallowing it to degrade into a conventional 2-generational younger-.rst collector. For more engineering \ndetails on the write barrier and remembered set, see our earlier paper and Hansen s PhD thesis [9, 17]. \n  5 Benchmarks Many programs pose little challenge to even a simple garbage col\u00adlector, usually because \nthey have little live data or a low rate of allocation. Furthermore all of the generational algorithms \n(2GEN, 3GEN, 3ROF) perform well even on most allocation-intensive pro\u00adgrams, because those programs usually \nsatisfy the weak genera\u00adtional hypothesis. We are therefore interested primarily in how well a collector \nper\u00adforms on programs that are abnormal in the sense that garbage col\u00adlection accounts for a substantial \nfraction of their execution time. Most of these abnormally gc-intensive programs do not satisfy the weak \ngenerational hypothesis as well as more typical programs, which results in an unusually high rate of \npromotion out of the youngest generation instead of the typical 1% promotion rate, gc\u00adintensive programs \nmay have promotion rates of 10 100%. The benchmarks we selected are small Scheme programs that we knew \nto be gc-intensive, plus one larger benchmark (an optimiz\u00ading Scheme compiler) that is not particularly \ngc-intensive but had been observed to perform poorly with the Boehm-Demers-Weiser conservative (imprecise) \ncollector. Three of the programs are syn\u00adthetic garbage collection benchmarks, which are especially useful \nfor studying the best-case and worst-case behavior of garbage col\u00adlectors. Some of the other programs \nmake sense as garbage collection benchmarks only if they are run several times back-to-back in the same \nprocess and measurements are taken for all the iterations as a whole. Neither earley nor sboyer are suitable \nuniterated, because their live storage grows monotonically and most garbage is short\u00adlived, so these \nprograms reach their peak size with little opportunity for garbage collection of older objects. Although \nthe use of iterated benchmarks is a common practice, it should be noted that iteration skews the distribution \nof object lifetimes in a way that should favor older-.rst collection. These benchmarks are available \nat our web site [10]. Mark/cons ratio 2GEN 3GEN 3ROF Major GCs 2GEN 3GEN 3ROF Rem. Set (Mby) 2GEN 3GEN \n3ROF 5earley:12 0.34 0.60 0.32 8.37 7.79 7.95 0.27 0.40 0.84 5earley:13 17.00 15.00 15.25 1.30 gcbench:5 \n0.69 0.43 0.89 84.26 31.07 122.44 0.27 0.40 0.53 gcold:25,1,0 1.02 1.61 0.77 10.84 11.05 8.95 0.27 0.40 \n0.53 gcold:25,1,1000 0.90 1.54 0.73 11.00 11.21 9.53 0.27 0.40 0.54 gcold:25,10,1000 0.14 0.29 0.14 1.16 \n0.95 1.21 0.27 0.40 0.55 gcold:100,1,0 1.22 1.84 0.91 14.25 14.50 11.50 0.54 gcold:100,1,1000 1.07 1.70 \n0.80 14.50 14.50 11.25 0.57 nboyer:3 0.87 1.16 0.90 7.00 4.88 7.27 0.27 0.40 0.99 nboyer:4 1.15 1.44 \n1.06 12.00 9.50 10.75 1.15 nboyer:5 1.44 1.80 1.32 21.00 17.00 18.50 1.60 5nboyer:3 0.87 1.15 0.90 33.71 \n24.67 35.40 0.27 0.40 1.05 5nboyer:4 1.04 1.36 0.97 52.00 41.75 46.25 1.26 perm:200,8,1 2.76 3.26 2.50 \n40.50 36.69 40.56 0.27 0.40 0.53 perm:25,8,8 1.74 2.41 1.36 29.00 27.20 25.89 0.27 0.40 0.53 perm:200,9,1 \n2.80 3.70 2.39 38.25 37.25 31.00 0.53 perm:25,9,8 2.65 3.56 2.25 59.00 57.5 47.75 0.53 5sboyer:4 0.46 \n0.77 0.45 10.31 9.69 10.50 0.27 0.40 0.91 5sboyer:5 0.66 1.03 0.61 17.25 15.25 15.00 1.18 twobitlong \n0.12 0.15 0.11 8.53 3.37 7.89 0.29 0.45 0.84 twobitshort 0.32 0.65 0.30 3.89 2.95 3.58 0.34 0.49 0.90 \n5twobitshort 0.31 0.68 0.29 18.79 12.89 17.06 0.34 0.53 1.00 Table 2. Averages, across all runs, for \nmark/cons ratio, major garbage collections, and peak size of remembered set. 2GEN 3ROF (for varying L) \n2.25 2.5 2.75 3.0 Mean 3ROF GEN 5earley:12 24.6 27.6 29.8 31.7 33.3 30.6 1.24 gcbench:5 23.8 26.6 26.7 \n27.1 27.6 27.0 1.13 gcold:25,1,0 25.8 27.4 27.8 28.8 30.1 28.5 1.10 gcold:25,1,1000 26.4 27.5 28.2 29.5 \n31.0 29.0 1.10 gcold:25,10,1000 35.3 33.8 34.3 38.2 37.2 35.9 1.02 nboyer:3 24.0 28.8 33.6 34.5 38.0 \n33.7 1.40 5nboyer:3 23.5 29.3 30.5 32.4 33.4 31.4 1.34 perm:200,8,1 19.7 22.6 23.8 24.4 24.9 23.9 1.21 \nperm:25,8,8 19.8 22.3 23.2 24.7 25.3 23.9 1.21 5sboyer:4 26.2 30.3 33.7 36.3 38.6 34.7 1.32 twobitlong \n78.8 89.4 89.4 91.6 94.7 91.3 1.16 twobitshort 45.5 49.7 49.4 49.5 53.4 50.5 1.11 5twobitshort 36.3 44.6 \n45.8 47.7 48.6 46.7 1.29 Table 3. Average promotion cost per volume (ms/MB) 100 5earley:12. gbenh:5+ \n80 gold:1,0. . gold:1,1000 gold:10,1000 60 . nboyer:3 5nb oyer:3 -20 0 20 40Heap Spae  . . . . . . \n. . . + +++ ++ + + +.. .... . . .... ..                               \n         . . . + ++++++ perm:200,1 perm:25,85sboyer:4 twobitlongtwobitshort5twobitshort + -40 \n-100 -50 0 50 100 150 Totaltime Figure 2. A space/time diagram comparing Larceny s stop-and-copy and \n2-generational younger-.rst collectors. Negative coordi\u00adnates indicate that the stop-and-copy collector \nis better; positive coordinates indicate that the 2-generational collector is better. For example, the \npoint at (68,35)shows that on this particular run of earley, the stop-and-copy collector requires 68% \nmore CPU time and 35% more heap space than the 2-generational collector. earley is an implementation \nof Earley s parsing algorithm, writ-boyer is a toy term-rewriting theorem prover derived from the ten \nby Marc Feeley [13]. In our benchmarks, this program Boyer benchmark in the Gabriel benchmark suite [14]. \nThe .rst creates a parser from an ambiguous grammar, and then program uses pairs extensively and constructs \nits data struc\u00adcomputes all parses of an input of length 12 or 13 that has tures functionally. The versions \nused here, nboyer and an exponential number of parse trees; this is iterated 5 times. sboyer, .x some \nbugs, are written in portable Scheme, and earley constructs its output in a functional manner, and the \nincorporate a problem scaling parameter [1, 3, 4, 10]. nboyer resulting data structure necessarily contains \nall young-to-old and sboyer differ only in that sboyer uses a local tweak pointers. shared consing to \nreduce the amount of storage allocation; this tweak results in a radically different live storage pro.legcbench \nis a synthetic garbage collection benchmark written by for sboyer [3, 10, 17]. John Ellis and Pete Kovac, \nmodi.ed by Hans Boehm, and translated from Java into Scheme by Will Clinger. To reduce nboyer was run \nwith scaling parameters 3, 4, and 5, as a sin\u00adedge effects, we modi.ed gcbench to perform its stretching \ngle iteration or iterated .ve times. sboyer was run with scal\u00adphase once and then to iterate the rest \nof the program n times. ing parameters 4 and 5, iterated .ve times. gcbench was run with n =5. perm is \na synthetic garbage collection benchmark written by Will gcold is a synthetic garbage collection benchmark \nwritten by Clinger, Gene Luks, and Lars Hansen. It represents the worst David Detlefs and translated \nfrom Java to Scheme by Will case for a younger-.rst generational collector: no objects die Clinger. The \nprogram creates a number of large trees and young, and all object deaths are oldest-.rst. then repeatedly \nreplaces random subtrees with newly allo- The program maintains a queue of data structures, each rep\u00adcated \ntrees and moves subtrees around in the data structure resenting all permutations of the .rst N integers. \nAs new databy swapping them. structures are allocated, old data structures are removed from gcold was \nrun in .ve con.gurations. Three con.gurations the queue and become garbage. have 25 megabytes of large \ntrees and run for 200 iterations, For N =8 there are 40,320 permutations in each list, occupy\u00adand the \nother two con.gurations have 100 megabytes of large ing about 1.2 megabytes (sharing is extensive). The \nlists are trees and run for 800 iterations. At each of these two sizes, generated without creating any \ngarbage whatsoever, and withthe con.gurations differ in the ratio of short-lived to long\u00adthe settings \nwe have chosen the survival rate out of the nurserylived storage and the amount of mutation work: we \nused the is 100%. settings (1,0), (1,1000), and (10,1000), omitting the third setting for the larger \nsize. perm was run in four con.gurations. perm200,8,1 and perm25,8,8 allocate the same amount of data \ncomputing For each size, the different con.gurations were intended to al\u00adall permutations of 8 things \n200 times and have the samelocate the same amount of storage, but several problems with peak live size, \nbut differ in the lifetime of the data: the formerthe benchmark code that were discovered late prevent \nthe al\u00adremoves one datum from the queue on every iteration, the lat\u00adlocation volumes from being equal. \nWe elected not to correct ter removes eight. these problems, which makes the results from this benchmark \na little harder to interpret. twobit is the Twobit optimizing compiler for Scheme [8, 25]. It is a typical \nold-style Lisp program: lists are used to represent many data structures, and most of the objects allocated \nare pairs. twobit creates graph representations of the program being compiled and then annotates and \nupdates those repre\u00adsentations using side effects. twobit is run in two con.gurations. The twobitshort \nbenchmark compiles another program, Nucleic2, in whole\u00adprogram optimization mode. Nucleic2 is about 3200 \nlines of source. The twobitlong benchmark compiles the source for twobit itself, about 23,800 lines. \nThe measurements for twobit include all allocation for I/O, but the time measurements do not include \ntime spent waiting for I/O. Some characteristics of these benchmark programs are shown in Table 1. The \npeak live sizes include the size of the static area be\u00adcause the collectors include it when they compute \nthe heap size, and the peak live size for earley includes a worst-case stack of 1.4 megabytes. An important \nquestion about the benchmarks is whether enough time is spent in garbage collection in the dynamic area. \nThe suite will not be a good test of dynamic-area collection algorithms if garbage collection time is \ndominated by minor collections. Table 1 shows that the larger fraction of the time is spent on major \ncollec\u00adtions in most of these programs, but 55% 60% of the GC time on twobitshort and fully 80% of the \nGC time on twobitlong are spent promoting data. Thus, the opportunities for the dynamic-area collector \nto improve the collection times are good in most cases, but notably restricted in the case of twobitlong. \n 6 Measurements Hansen performed most of our measurements on a Sun Ultra 5 workstation running Solaris \n2.6. It had an UltraSPARC-IIi pro\u00adcessor running at 333 MHz, 2 megabytes of secondary cache, and 128 \nmegabytes of RAM. It provides CPU time accounting with a resolution of 10 milliseconds. Measurements \nwere obtained while this machine was connected to a network and operating in multi\u00aduser mode, but no \nother users were on the system. X Windows was running on the machine s console but was inactive. Clinger \nsupplemented these measurements by running larger ver\u00adsions of the more scalable benchmarks on a Sun \nUltra 80 Model 4450 with 4 UltraSPARC-II processors running at 450 MHz, 4 megabytes of secondary cache, \nand 2 gigabytes of RAM. These benchmarks were run using a newer version of Larceny, without testing as \nmany con.gurations or collecting as much data. Their timings are reported separately in Figures 5 and \n6. To reduce measurement noise, we report CPU times and the best of three runs (rather than elapsed time \nand the average of three runs, say). Paging was never an issue. In reporting the space used by a collector, \nwe count all heap space that is actually allocated by the collector, exclusive of storage for remembered \nsets. (Some representations for remembered sets are more compact than others, so our representation could \nbe regarded as biased. We report the space required by our remembered sets separately in Table 2.) Ef.cient \ngarbage collection is a tradeoff between time and space, so when benchmarking it is important to control \nthe amount of space that the different collectors use. Unfortunately, it is not much easier Per3ROF .oat \ncent Promo .oat Volume (Mby) 3ROF .oat Promo .oat 5earley:12 0.0 0.8 0.0 0.6 gcbench:5 17.5 48.5 149.5 \n278.7 gcold:25,1,0 0.0 0.0 0.0 0.0 gcold:25,1,1000 8.2 0.0 19.1 0.0 gcold:25,10,1000 35.7 8.6 nboyer:3 \n0.0 0.0 0.0 0.0 5nboyer:3 0.0 0.0 perm:200,8,1 0.0 0.0 0.0 0.0 perm:25,8,8 -1.2 0.0 -3.4 0.0 5sboyer:4 \n0.0 0.0 0.0 0.0 twobitlong 4.1 16.1 2.9 9.7 twobitshort 0.2 11.4 0.1 3.5 5twobitshort 2.4 31.3 3.7 37.6 \n Table 4. 3ROF and promotion .oat, by average percent\u00adage of excess copying and average excess volume \ncopied (in megabytes), for L =30. The volumes of promotion .oat are very close to the volumes reported \nfor 2GEN. Blank entries were not measured but will be zero. to keep space constant across collectors \nwhile measuring gc time than it is to hold the time constant while measuring space. Our compromise is \nto report both time and space as in Figure 2, while attempting to control space in three different ways: \n.One set of runs placed an upper limit on the heap size, though each collector was free to use less memory \nthan the limit. These runs were conducted with .ve heap sizes, spaced at least two megabytes apart, ranging \nbetween two and three times the peak storage required by the benchmark. .Another set of runs placed both \nupper and lower limits on the heap size. In all runs the upper limit was set as above, and the lower \nlimit was set 2 megabytes below the upper limit; thus the collectors had a little room to maneuver, but \nnot much. .The third set of runs instructed the garbage collector to resize the heap as necessary to \nuse no more than L times the collec\u00adtor s estimate of live storage. These runs were conducted for L =2 \n25, 2.5, 2.75, and 3.0. In addition to measuring CPU time, we measured the number of words of storage \nthat a collector copies during the execution of a benchmark. Dividing this by the number of words allocated \nby the benchmark yields the mark/cons ratio. The mark/cons ratio allows us to separate the abstract (theoretical) \nef.ciency of a garbage col\u00adlector from its concrete cost of copying a word, which has a lot to do with \nhow tightly the collector s inner loops are coded. Both the mark/cons ratio and the cost of copying a \nword are of interest, and both together are more informative than CPU time alone would be.  7 Experimental \nResults In this section we compare the performance of Larceny s hybrid 3ROF collector to Larceny s conventional \n2-generational (2GEN) and 3-generational (3GEN) younger-.rst collectors. The space/time diagrams in Figures \n3, 4, and 5 provide a quick impression of how these collectors compare with respect to space and overall \nCPU time. Figure 6 shows how the 2GEN and 3ROF collectors compare with respect to garbage collection \ntime for the larger benchmarks. Many more .gures and tables can be found in Hansen s thesis, which is \nonline [17]. 80 5earley:12.. gbenh:5+ 60 gold:1,0. gold:1,1000 40gold:10,1000 . nboyer:3 20 5nboyer:3 \n . . . .  Heap++.. +++ .. .  0 ++ + +.  Spae +.+ +. . .. +. . .  perm:200,1 . -20 perm:25,8 \n.+  5sboyer:4 .-40twobitlong + twobitshort -60  5twobitshort . -80 -100-80-60-40-2002040 Totaltime \nFigure 3. Space/time diagram comparing 2GEN (better along the negative axes) and 3ROF (better along the \npositive axes). 100 . 80 60 40  20 . . . .. ++ . ... + .. .....Heap 0 +++ .. ++ +  .. ... Spae \n-20 .. + -40 +. . -60 -80 . -100 . .  -120 -60-40-20020406080 Totaltime Figure 4. Space/time diagram \ncomparing 3GEN (better along the negative axes) and 3ROF (better along the positive axes). 15 10 5 0 \n. -5 Spae -10 . -15 . -20 -25 . -30 -35  + + + . . + .  5earley:13gold:100,1,0gold:100,1,1000 . + \n. nb oyer:4 nb oyer:5 5nb oyer:4 . 5sboyer:5 perm:200,9,1 perm:25,9,8   -50510152025 Totaltime Figure \n5. Space/time diagram comparing 2GEN (better along the negative axes) and 3ROF (better along the positive \naxes) for the larger benchmarks run on a bigger machine. The x-axis is total CPU time. 15 10 5  . 0 \n-5 Spae-10 . . -15 -20 . -25 -30 -35   + + + + . .  . .   -30-20-10010203040 50 GCtime Figure \n6. Space/gc-time diagram comparing 2GEN (better along the negative axes) and 3ROF (better along the positive \naxes) for the larger benchmarks run on a bigger machine. The x-axis is CPU time expended on garbage collection. \n7.1 Summary of Findings The 2GEN collector had the best overall performance, the 3ROF collector was a \nclose second, and the 3GEN collector was a distant third. It was mildly surprising that 2GEN outperformed \n3GEN on most of our benchmarks. We have identi.ed several reasons for this, but the most important is \nthat, by de.nition, gc-intensive benchmarks are atypical. In particular, they often have an unusually \nhigh fraction of fairly long-lived but not permanent objects. The 2GEN collector promotes these objects \nby copying them just once, whereas the 3GEN collector must copy them at least twice be\u00adfore they enter \nthe oldest dynamic generation. The 3ROF collector promotes these objects into the intermediate generation \nby copying them just once, and promotes them into the older ROF generation without copying them, where \nthey often died without being copied a second time. Table 2 shows that the 3ROF collector s mark/cons \nratios are fre\u00adquently better than those of the younger-.rst collectors. It should be noted that 2GEN \nand 3GEN sometimes use less heap memory than 3ROF, and lower heap sizes tend to pull up the averages \nfor the mark/cons ratio and number of major collections, making 2GEN and 3GEN look worse relative to \n3ROF than they actually are. It is clear, however, that the 3ROF collector often marks fewer words than \nthe younger-.rst collectors, especially on the gcold, perm, and the larger nboyer and sboyer benchmarks. \nOnly on one program, gcbench, does the 3ROF collector have worse mark/cons ratios, and that program has \nobject lifetimes that just do not .t the models for which the 3ROF collector was de\u00adsigned. With gcbench, \nmost of the objects in the younger ROF generation will be dead when a major collection occurs, and the \npermanently live storage of that benchmark will account for a large fraction of the data in the older \nROF generation. Therefore the ma\u00adjor collection will reclaim a smaller amount of storage than would a \nfull collection with 2GEN, and the frequency of collections must increase. 3GEN does even better than \n2GEN on this one bench\u00admark because, once it has promoted the permanent storage into the oldest generation, \nit doesn t have to look at it again except during full collections. Adding a fourth generation to the \n3ROF collector would improve its performance on gcbench, but whether this can be done without compromising \nits performance on other programs is an open question. Except for gcbench, the 3ROF collector was faster \nthan the 3GEN collector, but sometimes used more space. The 3ROF collector did not always compare well \nagainst 2GEN, as it tends to use more space and has a higher average cost per word promoted out of the \nnursery. The 3ROF collector did perform im\u00adpressively on the gcold and perm benchmarks, however. gcold \ns random mutations give it object lifetimes that resemble a radioac\u00adtive decay model, and perm has queue-like \nlifetimes, both of which favor older-.rst collection. The 3ROF collector usually spends about as much \ntime in major collections as the 2GEN collector, but its minor collections tend to be more expensive, \nso its total gc time is often slightly higher. On the smaller benchmarks, this is often offset by a lower \nmutator time, despite the fact that the mutator code is identical for both [17]. Unlike 2GEN, the 3ROF \ncollector never performs a full collection, which may have some advantages for cache performance. This \nef\u00adfect did not show up in the larger benchmarks, possibly because their heaps are much larger than the \nhardware caches. The 3ROF collector has more .oating garbage than the younger\u00ad.rst collectors, and requires \nmore space for its remembered sets, but in both cases the increases are moderate. We have not calculated \naverages or geometric means across bench\u00admarks because our benchmark programs are unlikely to be repre\u00adsentative \neven of gc-intensive programs, and we have further dis\u00adtorted the averages by running more benchmarks \nwith some pro\u00adgrams (gcold) than others (gcbench). Also, averages would tend to direct a reader s attention \ntoward the goal of optimizing for the (ill-de.ned) average gc-intensive program, when the more promis\u00ading \nand important goal is to match each program to a collector that helps it to perform well. 7.2 Cost of \nCopying Though the 3ROF collector usually has a lower mark/cons ratio than 2GEN, its GC times are often \nhigher, and the reason is that it is more expensive to perform some collections. The collector must use \na write barrier during promotions into the ROF-young genera\u00adtion (steps 1 j) to allow the remembered \nset to track pointers into the ROF-old generation; it must also insert any intercepted pointers into \nthe remembered set. The write barrier is not particularly ex\u00adpensive in itself, nor is remembered set \ninsertion expensive, but the cost of copying and scanning a word is already small, so even the small \nabsolute costs of the extra operations add up to a substantial relative cost. Table 3 shows the cost, \nexpressed in milliseconds per megabyte, of promoting into the dynamic area (both generations). This is \nnot pure copying cost: it also includes the cost of scanning the remembered set, which in some cases \nwill be slightly larger than for the remembered set of 2GEN. The high promotion costs for twobit are \ncaused by a performance bug that affects all of Larceny s generational collectors [17]. The 3ROF collector \ns unit promotion cost rises with the inverse load factor L because a larger value of L increases the \nproportion of pro\u00admotions into the younger ROF generation, where the extra write barrier comes into play. \n 7.3 Floating Garbage The garbage collector must assume that all objects in the remem\u00adbered set are live. \nThat is not always true, as objects may die after they have been inserted into the remembered set but \nbefore their generation is collected. When the dead objects in the remembered set refer to other dead \nobjects in the collected region, those dead objects must be copied by the collector. They are called \n.oating garbage, or just .oat. We measured the amount of .oat in the 3ROF collector by using a special \nmark/sweep collector that marks all live objects but sweeps only the remembered set. This operation does \nnot disturb the heap, so the resulting reductions in copying are accurate representations of how the \nprograms are affected by .oat. Table 4 shows 3ROF .oat (.oat due to major collections not being full \ncollections) and promotion .oat (.oat due to minor collections that promote into the ROF heap), across \nall settings for the 3ROF collector. Overall, 3ROF .oat is moderate across the benchmarks the high value \nshown for gcold:10,1000 results from the very small number of collections recorded for this benchmark. \nPromo\u00adtion .oat is often high, but we measured a comparable amount of promotion .oat for the 2GEN collector, \nso the 3ROF collector does not appear to create much more .oat than conventional generational collectors. \n 7.4 Remembered Sets The 3ROF collector uses more space for the remembered set than either of the younger-.rst \ncollectors, and in many cases the remem\u00adbered set grows to over 3% of the heap size (see Table 2).1 The \nrea\u00adson for the larger remembered set is that the 3ROF collector needs to track pointers from the ROF-young \ngeneration into the ROF-old generation. Many data structures in typical Scheme programs cre\u00adate young-to-old \npointers, so the extra remembered set is expected to grow large in some of these programs [9]. More imperative \nlanguages, such as Java, have less bias toward young-to-old pointers.  8 Related Work There are now \nseveral general surveys of garbage collection [11, 24, 36]. Generational collection and the weak generational \nhypothesis were .rst described in 1983 by Lieberman and Hewitt [27], although ear\u00adlier work foreshadowed \nthe technique [5, 12, 18]. Lieberman and Hewitt motivated the younger-.rst technique by appealing to \nthe predominance of pointers from young data to old data, though they presented no evidence that such \na predominance existed. Hayes stated and cast doubt on the strong generational hypothesis [19]. Important \nearly contributions to generational collection were made by Ungar and Moon, both of whom described actual \nimplementa\u00adtions [28, 34]. Not all generational collectors have been strictly younger-.rst. Moon reported \nthat the garbage collector of the Symbolics Lisp machine would only collect those ephemeral generations \nthat were full [28], and the mature object space collector (usually called the train algorithm), invented \nby Hudson and Moss, will collect data in an order that is similar to older-.rst but that is also in.uenced \nby the topology of the heap [15, 22]. Spurred by Baker s conjecture that generational collection had \nno theoretical advantage over non-generational collection for the ra\u00addioactive decay model, we invented \nthe ROF algorithm described here, and showed its theoretical advantage by calculation [2, 9]. We also \noutlined the implementation of our prototype 3ROF collector, but did not provide details of its performance \n[9]. Stefanovi\u00b4c invented a better name for our ROF collector, which we 1The remembered set sizes in \nthe table are the allocated size, not the peak size measured in the number of entries in the set. 2GEN \nhas two remembered set data structures, 3GEN has three, and 3ROF has four so the minimum remembered set \nsize for the 3ROF col\u00adlector is twice that of 2GEN, as the entry for e.g. gcbench shows. The imprecision \nin the remembered set size measurements is re\u00adgrettable but does not preclude one from concluding, as \nthe table shows, that the remembered sets for the 3ROF collector grow be\u00adyond their minimum size more \noften than the remembered sets for 2GEN and 3GEN. had originally called non-predictive, and invented \nseveral other older-.rst algoritms, notably his deferred-older-.rst (DOF) algo\u00adrithm [31, 32]. His simulations \nof these algorithms showed that his DOF algorithm has lower mark/cons ratios than the other algo\u00adrithms \non a suite of Smalltalk and Java programs, and he argued that a DOF collector should also perform well \nin practice. Imple\u00admentations of the DOF and the related Beltway collectors in Java have now con.rmed \nthis [6, 33]. Hansen implemented and benchmarked a hybrid version of the DOF algorithm as well as the \nhybrid 3ROF algorithm presented here. The hybrid DOF algorithm was usable but did not perform as well \non our benchmarks as the 3ROF algorithm. Hansen s mea\u00adsurements revealed two main reasons for this: a \npure DOF algo\u00adrithm would have performed better than our hybrid version, and the DOF algorithm tends \nto create excessive amounts of .oating garbage [17]. 9 Conclusions and Future Work Older-.rst generational \ngarbage collection is a new technology that greatly expands the design space for generational garbage \ncollec\u00adtors. For example, the DOF and Beltway algorithms are inherently less disruptive than our ROF \ncollector, which is in turn less disrup\u00adtive than conventional younger-.rst collectors. It would be desirable \nto replicate our experiments with more bench\u00admarks, in other systems, and for other programming languages. \nClinger has been using linear combinations of radioactive decay models with different half-lives to model \nobject lifetimes, and to analyze the theoretical performance of idealized algorithms for garbage collection. \nThis work provides quantitative explanations for the success of younger-.rst generational collectors, \nand explains why they perform better than older-.rst or hybrid algorithms on some programs but worse \non others. Several runtime systems now offer a choice of multiple garbage collectors. Eventually we can \nexpect runtime systems to select a garbage collector based on dynamic observation of the programs they \nare executing. 10 References [1] Henry G. Baker. The Boyer benchmark at warp speed. ACM Lisp Pointers \n5(3), July September 1992, pages 13 14. [2] Henry G. Baker. Infant mortality and generational garbage \ncollection. ACM SIGPLAN Notices 28(4), April 1993, pages 55 57. [3] Henry G. Baker. The Boyer benchmark \nmeets linear logic. ACM Lisp Pointers 6(4), October December 1993, pages 3 10. [4] Henry G. Baker. Personal \ncommunication via electronic mail, 6 November 1995, quoting a personal communication via fax from Bob \nBoyer dated 3 December 1993. [5] A. Bawden, R. Greenblatt, J. Holloway, T. Knight, D. A. Moon, and D. \nWeinreb. Lisp machine progress report. AI Memo 444, MIT AI Lab, August 1977. [6] Steve M. Blackburn, \nRichard Jones, K. S. McKinley, and J. Eliot B. Moss. Beltway: Getting Around Garbage Collection Gridlock. \nACM SIGPLAN Conference on Programming Lan\u00adguage Design and Implementation (PLDI), June 17-19, 2002. [7] \nC. J. Cheney. A nonrecursive list compacting algorithm. Com\u00admunications of the ACM 13(11), November 1970, \npages 677 678. [8] William D Clinger and Lars Thomas Hansen. Lambda, the ultimate label, or a simple \noptimizing compiler for Scheme. Proceedings of the 1994 ACM Conference on Lisp and Functional Programming. \nACM LISP Pointers VIII(3), July September 1994, pages 128 139. [9] William D Clinger and Lars T Hansen. \nGenerational Garbage Collection and the Radioactive Decay Model. Proceedings of the 1997 ACM SIGPLAN \nConference on Programming Lan\u00adguage Design and Implementation (PLDI), ACM SIGPLAN Notices 32(5), May \n1997, pages 97 108. [10] William Clinger. Data provided via the World-Wide Web at http://www.ccs.neu.edu/home/will/GC/index.html. \n[11] Jacques Cohen. Garbage collection of linked data structures. ACM Computing Surveys 13(3), September \n1981, pages 341 367. [12] L. Peter Deutsch and Daniel G. Bobrow. An ef.cient incre\u00admental automatic garbage \ncollector. Communications of the ACM 19(7), July 1976, pages 522 526. [13] J Earley. An ef.cient context-free \nparsing algorithm. Commu\u00adnications of the ACM 13(2), 1970, pages 94 102. [14] Richard P. Gabriel. Performance \nand Evaluation of Lisp Sys\u00adtems. The MIT Press, 1985. [15] Jacob Seligmann and Steffen Grarup. Incremental \nmature garbage collection using the train algorithm. In Proceedings of 1995 European Conference on Object-Oriented \nProgram\u00adming, Lecture Notes in Computer Science, Springer-Verlag, August 1995. [16] Lars Thomas Hansen. \nThe impact of programming style on the performance of Scheme programs. M.S. Thesis, University of Oregon, \n1992. [17] Lars Thomas Hansen. Older-.rst Garbage Collection in Prac\u00adtice. Ph.D. Thesis, Northeastern \nUniversity, November 2000. Available online see [10]. [18] David R. Hanson. Storage Management for an \nImplemen\u00adtation of SNOBOL4. Software Practice and Experience 7, 1977, pages 179 192. [19] Barry Hayes. \nUsing key object opportunism to collect old ob\u00adjects. In Conference on Object Oriented Programming Sys\u00adtems, \nLanguages, and Applications (OOPSLA 91), October 1991, pages 33 46. [20] Michael W. Hicks, Luke Hornof, \nJonathan T. Moore and Scott M. Nettles. A Study of Large Object Spaces. ISMM 98 International Symposium \non Memory Management, pages 138 147. ACM Press, 1998. [21] Anthony L. Hosking, J. Eliot B. Moss, and \nDarko Stefanovic.\u00b4A Comparative Performance Evaluation of Write Barrier Im\u00adplementations. OOPSLA 92 Conference \nProceedings, ACM SIGPLAN Notices 27(10), October 1992, pages 92 109. [22] Richard L. Hudson and J. Eliot \nB. Moss. Incremental garbage collection for mature objects. In Proceedings of Interna\u00adtional Workshop \non Memory Management, (Yves Bekkers and Jacques Cohen, editors), Lecture Notes in Computer Science 637. \nSpringer-Verlag, September 1992. [23] IEEE Standard for the Scheme Programming Language. IEEE Std 1178-1990. \n[24] Richard Jones and Rafael Lins. Garbage Collection: Algo\u00adrithms for Automatic Dynamic Memory Management. \nJohn Wiley &#38; Sons, 1996. [25] Richard Kelsey, William Clinger, and Jonathan Rees (editors). Revised5 \nReport on the Algorithmic Language Scheme. ACM SIGPLAN Notices 33(9), September 1998, pages 26 76. [26] \nThe Larceny home page at http://www.larceny.org/. [27] Henry Lieberman and Carl Hewitt. A Real-Time Garbage \nCollector Based on the Lifetimes of Objects. Communications of the ACM 26(6), 419 429, June 1983. [28] \nDavid A. Moon. Garbage Collection in a Large Lisp System. In ACM Conference on Lisp and Functional Programming, \n235 246, 1984. [29] Patrick M. Sansom and Simon L. Peyton Jones. Generational garbage collection for \nHaskell. In Conference on Functional Programming Languages and Computer Architecture, 1993, pages 106 \n116. [30] Darko Stefanovi\u00b4c and J. Eliot B. Moss. Characterisation of ob\u00adject behaviour in Standard ML \nof New Jersey. In ACM Con\u00adference on Lisp and Functional Programming, 1994, pages 43 54. [31] Darko Stefanovic.\u00b4Properties \nof Age-Based Automatic Mem\u00adory Reclamation Algorithms. Ph.D. thesis, University of Mas\u00adsachusetts, Amherst, \nMassachusetts, February 1999. [32] Darko Stefanovi\u00b4c, Kathryn S. McKinley, and J. Eliot B. Moss. Age-Based \nGarbage Collection. OOPSLA 99 Conference Proceedings, ACM SIGPLAN Notices 34(10), October 1999, pages \n370 381. [33] Darko Stefanovi\u00b4c, Matthew Hertz, Steve M. Blackburn, K. S. McKinley, and J. Eliot B. Moss. \nOlder-.rst Garbage Collec\u00adtion in Practice: Evaluation in a Java Virtual Machine. Work\u00adshop on Memory \nSystem Performance, Berlin, Germany, June 2002. [34] David Ungar. Generation Scavenging: A Non-disruptive \nHigh Performance Storage Reclamation Algorithm. In ACM SIGSOFT-SIGPLAN Practical Programming Environments \nConference, Pittsburgh PA, April 1984, pages 157 167. [35] Paul R. Wilson and Thomas G. Moher. Design \nof the oppor\u00adtunistic garbage collector. OOPSLA 89 Conference Proceed\u00adings, ACM SIGPLAN Notices 24(10), \nOctober 1989, pages 23 35. [36] Paul R. Wilson. Uniprocessor garbage collection techniques. ACM Computing \nSurveys, to appear. Available via anonymous ftp from cs.utexas.edu,in pub/garbage.  \n\t\t\t", "proc_id": "581478", "abstract": "Generational collection has improved the efficiency of garbage collection in fast-allocating programs by focusing on collecting young garbage, but has done little to reduce the cost of collecting a heap containing large amounts of older data. A new generational technique, older-first collection, shows promise in its ability to manage older data.This paper reports on an implementation study that compared two older-first collectors to traditional (younger-first) generational collectors. One of the older-first collectors performed well and was often effective at reducing the first-order cost of collection relative to younger-first collectors. Older-first collectors perform especially well when objects have queue-like or random lifetimes.", "authors": [{"name": "Lars T. Hansen", "author_profile_id": "81100463673", "affiliation": "Opera Software, Oslo, Norway", "person_id": "P168837", "email_address": "", "orcid_id": ""}, {"name": "William D. Clinger", "author_profile_id": "81100543143", "affiliation": "Northeastern University, Boston, MA", "person_id": "P298778", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/581478.581502", "year": "2002", "article_id": "581502", "conference": "ICFP", "title": "An experimental study of renewal-older-first garbage collection", "url": "http://dl.acm.org/citation.cfm?id=581502"}