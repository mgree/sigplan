{"article_publication_date": "09-17-2002", "fulltext": "\n Composing Monads Using Coproducts Christoph L\u00a8uth Neil Ghani FB 3 Mathematik und Informatik Dept. \nMathematics and Computer Science Universit\u00a8University of Leicester at Bremen cxl@informatik.uni-bremen.de \nng13@mcs.le.ac.uk Abstract Monads are a useful abstraction of computation, as they model di\u00adverse computational \neffects such as stateful computations, excep\u00adtions and I/O in a uniform manner. Their potential to provide \nboth a modular semantics and a modular programming style was soon recognised. However, in general, monads \nproved dif.cult to com\u00adpose and so research focused on special mechanisms for their com\u00adposition such \nas distributive monads and monad transformers. We present a new approach to this problem which is general \nin that nearly all monads compose, mathematically elegant in using the standard categorical tools underpinning \nmonads and computation\u00adally expressive in supporting a canonical recursion operator. In a nutshell, we \npropose that two monads should be composed by tak\u00ading their coproduct. Although abstractly this is a \nsimple idea, the actual construction of the coproduct of two monads is non-trivial. We outline this construction, \nshow how to implement the coproduct within Haskell and demonstrate its usage with a few examples. We \nalso discuss its relationship with other ways of combining monads, in particular distributive laws for \nmonads and monad transformers. Categories and Subject Descriptors D.1.1 [Programming Techniques]: Functional \nProgramming  General Terms Algorithms, Languages, Theory 1 Introduction It has long been a goal of \nresearch within the theoretical computer science community to provide a modular semantics for program\u00adming \nlanguages. In detail, one would like to give a semantics for individual features of a programming language \nsuch as exception handling, non-determinism and state-based computation and, by Permission to make digital \nor hard copies of all or part of this work for personal or classroom use is granted without fee provided \nthat copies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. ICFP 02, October 4-6, 2002, Pittsburgh, Pennsylvania, \nUSA. Copyright 2002 ACM 1-58113-487-8/02/0010 ...$5.00 suitably composing these, obtain a semantics for \nthe programming language as a whole. If successful, reasoning about large pieces of software could be \nachieved by reasoning about smaller (and hence more tractable) subcomponents and then lifting these results \nto the original problem. One exciting possibility was Moggi s proposal [17] to use mon\u00adads to structure \ndenotational semantics. Moggi pointed out that a number of computational features (including all of those \nmentioned above) could be modelled by monads. Wadler took Moggi s ideas further by showing how one could \nactually program with monads. For example monads can be used to support imperative features within a \npurely functional language. Soon, however, it became clear that despite the undoubted value of monads \nfrom both the semantic and programming perspectives, composing monads would prove to be a signi.cant \nchallenge. Put brie.y, monads just do not seem to compose in any general manner. Thus, a variety of different \nmeth\u00adods to combine monads were proposed, most notably in the use of distributivity laws and monad transformers. \nWhile being useful in speci.c situations, these theories do not cover all situations, and furthermore, \nas we comment later, can sometimes seem rather ad\u00adhoc. What these approaches have in common is the observation \nthat monads are functors carrying extra structure. Thus the obvious way to compose a monad T and a monad \nR is to use the functo\u00adrial composition R.T . Unfortunately, R.T is not a monad and one can see both \nmonad transformers and distributivity laws as an at\u00adtempt to coax R.T into a monad. An alternative point \nof view is to observe that, just as functors are the objects of the functor cat\u00adegory, so monads are \nthe objects of the category of monads and monad morphisms. Then the canonical way of putting two objects \ntogether is take their coproduct, and this is what this paper is about. Our interest in this topic arose \nfrom previous research of ours which used the coproduct of two monads as a framework for modular\u00adity \n[14] and, in particular, modular term rewriting [12, 13]. In modular term rewriting, terms built over \ntwo signatures are decom\u00adposed into terms built over each of the two signatures, called layers. Within \nthese papers, we argued that the layer structure is the key concept of modularity, and should be taken \nas a primitive notion rather than as a derived concept. The relevance of monads is that they provided \nan abstract and axiomatic formulation of the notion of a layer. When we tried to apply this work to the \ncombination of monads within functional programming we found that we had used several assumptions which \nimportant computational monads did not satisfy. Thus, the composition of computational monads, in particular \nfrom the functional programmer s perspective, became a challenge for us, and this paper is the result \nof our research. Its concrete contributions are: to explain the construction of the coproduct of two \nmonads using the idea of layers to describe how the individual monads are interleaved to form the coproduct \nmonad;  to explain how the coproduct can be regarded as an abstract datatype with a canonical recursion \noperator similar to fold;  to explain that, unlike current approaches, virtually all mon\u00adads can be \ncomposed via the coproduct construction;  to explain how the coproduct of two monads can be imple\u00admented \nin Haskell. This is non-trivial since the coproduct is not a free datatype but rather a quotient;  to \nexplain how the coproduct approach relates to current ap\u00adproaches. For example, in the presence of strong \ndistributiv\u00adity laws, the functorial composite of two monads is not just a monad, but also the coproduct \nof the two monads in question.  To summarise, we feel that coproducts provide the right general framework \nfor composing monads. The fact that almost all monads compose in this way and that the coproduct has \na universal property and an associated recursion operator are two powerful arguments in its favour over \ncurrent approaches. However, there is a price to be paid, namely that we are keeping track of all possible \nlayers. As we show later, one can regard monad transformers and distributivity laws as an attempt to \nsquash all these layers into one speci.c layer which, when possible, results in a data structure which \nis potentially easier to manage. However, even in such situations, the coproduct analysis still contributes \nto our understanding, e.g. in reminding us that however we compose monads, there should be an associated \nrecursion operator. Our aim here is to explain our ideas to the functional programming community. Consequently \nwe keep the categorical jargon down to a minimum, focusing on intuitions and examples rather than detailed \nproofs; however, at times we need to be technically exact (in partic\u00adular in Sect. 4.2). Then, a basic \nknowledge of category theory (such as categories, functors and natural transformations) will be helpful. \nThe remainder of this paper is structured as follows: in Sect. 2 we introduce monads, both as term algebras \nand computational monads as found in Haskell, and discuss how distributivity laws and monad transformers \nhave been used to compose monads. In Sect. 3, we construct the coproduct of two monads and give some \nexamples of its use. In Sect. 4, we evaluate the construction and in particular show how distributive \nmonads form a special case. 2 A Brief History of Monads Monads originally arose within category theory \nas models of term algebras. Such monads provide good intuitions as to how to con\u00adstruct the coproduct \nof two monads and so we begin with them. Since this is standard material, we refer the reader to general \ntexts [15] for more details. 2.1 Monads and Term Algebras Term algebras are built from signatures which \nare de.ned as fol\u00adlows: De.nition 1. A (single-sorted) signature consists of a function S : N. Set. The \nset of n-ary operators of S is de.ned Sn = S(n). De.nition 2. Given a signature S and a set of variables \nX, the term algebra TS(X) is de.ned inductively: x . Xf . Snt1,...,tn . TS(X) x . TS(X) f (t1,...,tn) \n. TS(X) Quotes are used to distinguish a variable x . X from the term x . TS(X) and can be seen as introducing \nlayer information into terms. As we shall see later, when constructing the coproduct of two monads, this \nlayer structure is the central concept. For every set X, the term algebra TS(X) is also a set; categorically \nTS : Set . Set is a functor over the category of sets. In addition, for every set of variables X, there \nis a function X . TS(X) sending each variable x to the associated term x. Lastly, substitution takes \nterms built over terms and .attens them, as described by a func\u00adtion TS(TS(X)) . TS(X). These three pieces \nof data, namely the construction of a theory from a set of variables, the embedding of variables as terms \nand the operation of substitution are axiomatised as a monad: De.nition 3. A monad T = (T,.,\u00b5) on a category \nC is given by an endofunctor T : C . C , called the action, and natural transforma\u00adtions, . :1 . T , \ncalled the unit, and \u00b5 : TT . T , called the multipli\u00adcation of the monad, satisfying the monad laws: \n\u00b5.T . = 1 = \u00b5..T , and \u00b5.T\u00b5 = \u00b5.\u00b5T . We have already sketched how the term algebra construction TS has \nan associated unit and multiplication. The equations of a monad correspond to substitution being well \nbehaved, in particular being associative with the variables forming left and right units. If we think \nof TS(X) as a layer of terms over X, the unit converts each variable into a trivial layer and the multiplication \nallows us to col\u00adlapse two layers of the same type into a single layer. Monads model a number of other \ninteresting structures in com\u00adputer science, such as (many-sorted) algebraic theories, calculi with variable \nbinders [4], term rewriting systems [12], and, via compu\u00adtational monads [17], state-based computations, \nexceptions, con\u00adtinuations etc. Several of these computational monads are used throughout this paper \nand their de.nition is given in Appendix A. Importantly, these applications involve base categories other \nthan Set (in fact, every monad on Set can be considered as an algebraic theory [16]), and possibly even \nenrichment. Even in this abstract setting, monads can be considered as a generalised form of alge\u00adbraic \ntheories [9, 21]. We do not pursue the theory in its full gen\u00aderality here, but should bear in mind that \nalthough we draw our intuitions from monads on Set, this is just a particular case which we do well not \nto restrict ourselves to. In the rest of this paper, we often regard T (X) abstractly as a layer in the \nexamples above, layers were terms, rewrites, or compu\u00adtations; monads then abstract from the nature of \na layer and rather provide a calculus for such layers where the actual layer, the empty layers, and the \ncollapsing of two layers of the same type are are taken as primitive concepts. The coproduct construction \nwe detail later will then consist of all interleavings of layers from the compo\u00adnent monads. We end this \nsection with two minor technical comments about .ni\u00adtariness and enrichment; while not important for \nthe basic under\u00adstanding, they are essential for the mathematical correctness. Firstly, a monad is .nitary \nif its action on in.nite objects is deter\u00admined by its action on .nite objects. For example, for a .nitary \nsignature S and an in.nite set X of variables, we have . TS( X)= TS( X0) X0.X,X0 .nite Categorically, \nthis property is de.ned in terms of preservation of directed colimits, but we shall not need this level \nof detail; however, some of our constructions will only be valid for .nitary monads, so we introduce \nthe terminology on an informal level here. For the precise de.nitions, see [1]. All known computational \nmonads are .nitary, except for the continuation monad [19]. Secondly, when reasoning about the semantics \nof functional lan\u00adguages, we usually pass from sets to order structures such as com\u00adplete partial orders \n(cpos). Categorically, this means not only a change of the base category, but imposing an order structure \non the morphisms as well (see e.g. [23]; crucially the set of functions be\u00adtween two cpo s forms in turn \na cpo.) This is called enrichment [8]. We note that all of the theory in the following can be enriched \n[9], but prefer to keep the presentation simple. The de.nition of a monad given above is unlike what \nreaders may have seen in a functional language like Haskell. Here is how the two are related. 2.2 Monads \nin Haskell In the programming language Haskell, a monad is given by two operations >>= and return, which \nform a type class: class Monad m where (>>=) :: m a-> (a-> m b)-> m b return::a -> ma This is the so-called \nKleisli-category of a monad: De.nition 4. For a monad T = (T,.,\u00b5), the associated Kleisli\u00adcategory CT \nhas the same objects as C . Each morphism f : X . TY in C gives a morphism f : X . Y in CT , with composition \nis given g . f =( \u00b5.Tg. f ) and identity 1X = .X : X . TX. The Kleisli-category for a signature S has \nas objects sets (of vari\u00adables or terms), and as morphisms assignments of the form s : X . TS( Y ) . \nComposition is by variable substitution, i.e. given def t : Y . TS( Z) ,wehave t.s : X . TS( Z) as t.s( \nx)= t[ t( y) /y] for s( x)= t. The Kleisli-category can be considered the free syntactic category generated \nfrom the signature, and as such it has some attractive properties (for example, uni.cation can be expressed \nelegantly as a coequaliser [22]). However, we prefer to work with Def. 3 since it is unclear how to form \nthe coproduct of two Kleisli-categories di\u00adrectly. It is most certainly not the coproduct of the two \ncategories, which would just be their disjoint union. In contrast, as we have already seen, the presentation \nof monads in Def. 3 supports the in\u00adtuition based on layers which turns out to be the central concept \nin constructing the coproduct of two monads. Fortunately, Def. 3 and Def. 4 are equivalent in the sense \nthat given one we can always calculate the other in a bijective manner. For ex\u00adample, given a monad T, \none constructs functors FT : C . CT ,GT : CT . C which on objects are respectively the identity and T, \nwhile on morphisms: FT ( f : X . Y )=( .Y . f ) X . TY GT ( f : X . TY )=( \u00b5Y .Tf ) : TX . TY The functor \nFT is left adjoint to GT , and their composition results in a monad which is isomorphic to T [15, Theorem \nVI.5.1]. We can directly express this in Haskell as follows:1 class Functor t=> Triple t where eta ::a->ta \nmu ::t(t a) -> ta instance Triple f=> Monad f where x >>= y= (mu .(fmap y)) x return = eta instance (Functor \nm, Monad m) => Triple m where eta = return mu x =x >>=id Of course, we could have de.ned a new class \nMonad or Triple which has all of (>>=), eta and mu, but the advantage of this ap\u00adproach is that we can \nuse Haskell s syntactic sugar when writing down monads which we combine using the Triple class, and we \ncan further use Haskell s prede.ned monads as instance of Triple, combining the best of both worlds. \n 2.3 Composing Monads Given a monad modelling exceptions and a monad modelling state transformations, \ncan we derive a monad modelling computations which can either raise exceptions or modify the state. More \ngener\u00adally, given monads T and R which we may think of as performing T -computations and R-computations \nrespectively, is there a monad which performs both? We .nish this section by discussing two ap\u00adproaches \nto this problem. Distributivity: A .rst guess would be to consider if the functo\u00adrial composite T .R \nis a monad. This would require a multiplication of the form T .R.T .R . T .R but there is no reason one \nshould ex\u00adist. One solution [6, 10] is to restrict attention to those monads for which there is a natural \ntransformation . : R.T . T .R and then, given some coherence laws, the multiplication can be constructed \nas T .R \u00b5T \u00b5R T .R.T .R =====. T .T .R.R ======= . T .R Such a natural transformation . is called a distributive \nlaw [2] and ensures T.R is a monad. A practical example occurs with the excep\u00adtion monad which distributes \nover any other monad. That is, if E is a .xed object of exceptions and T is a monad, then the assignment \nmapping X to T ( X + E) is also a monad. From our layer based perspective, the functorial composite T.R \ncorresponds to a T -layer over an R-layer. In general, combining a monad T with a monad R should not \ninclude just this speci.c layering, but all layerings, e.g. T.R.T or R.T .R. Thus it is not sur\u00adprising \nthat in general T.R is not a monad. Now, a distributive law corresponds to an interchange law which permutes \nT -layers over R\u00adlayers and hence allows us to squash an arbitrary layering into the 1Triple is another \nword for monad [2]. This code will produce overlapping and undecidable class instances, and will be rejected \nby ghc and hugs unless the appropriate options are given. speci.c layering T.R. We later formalise this \nobservation by show\u00ading that when there is a strong distributive law, not only is T.R a monad, but it \nis also the coproduct monad. Monad Transformers: Monad transformers provide a partial an\u00adswer to the \nquestion of composing monads when there is no dis\u00adtributivity law present. In a nutshell, a monad transformer \n[18] is a pointed endofunctor on the category Mon(C ) of monads over a .xed base category. That is, a \nfunctor F : Mon(C ) . Mon(C ) mapping every monad T to a monad F(T), and for every monad T a natural \nfamily of monad morphisms aT : T . FT. We think of the functor F transforming a monad T to the monad \nF(T), while aT ensures the monad T is sitting inside F(T). Given that the identity functor is a monad, \nwe may thus regard F as adding to every monad T the monad F(1). For example, the de.nition of the exception \nmonad transformer [11] is type ExnT m a= m(Exn a) As we have already seen, there is a distributivity \nlaw present and in this case, the action of the monad transformer takes any monad T to TExn. However, \nthe de.nition of the state monad transformer is type StateT sm a= s-> m(s, a) There is no distributive \nlaw here between the monad m and the state monad which allows us to permute one layer over the other. \nNevertheless, one can see that the monad m has been partially per\u00admuted past the pairing operation although \nnot past the outer func\u00adtion space. So, monad transformers allow us a more .ne-grained control of the \ncombination allowing some form of mixing of the layers between the state monad and an arbitrary monad \nm. In our opinion, the concept of a monad transformer is rather elegant but the de.nition is too general \nto support an adequate meta-theory. By this we mean that given a monad, it is not clear whether it is \npossible, and if so how, to de.ne an associated monad transformer. For example,there is no monad transformer \nfor the list monad asso\u00adciated to the monadic treatment of non-determinism [11]. Another disadvantage \nis that we have to pick an order in which to combine the monads, which does make a considerable difference. \nMoreover, when adding a new monad transformer we have to consider the pos\u00adsible combinations with all \nexisting monad transformers separately, and this number of combinations grows quadratically. Thus, monad \ntransformers are not really modular. To summarise, both distributivity laws and monad transformers at\u00adtempt \nto de.ne a composite monad by squashing all the different layers obtained by interleaving the component \nmonads into one spe\u00adci.c layer. As we shall see, the coproduct approach simply keeps all the layers. \nThis shift in emphasis means we buy generality since we can always compose monads. Of course the price \nto be paid is that the data structure has to manage all layers and this imposes some computational overhead. \n  3 The Coproduct of Monads Having discussed the current approaches to composing monads, we now explain \nour approach based upon coproducts. We .rst discuss abstractly what the coproduct means and show how \nthe associated universal property provides a recursion operator. We then discuss the construction of \nthe coproduct of two monads before implement\u00ading it in Haskell and discussing its correctness brie.y. \n3.1 Using the Coproduct Given two monads T1,T2, their coproduct can be thought of as the smallest monad \ncontaining both monads. Formally, the coproduct of two monads is simply the coproduct in the category \nof monads. For de.nitiveness we give a formal de.nition including the relevant universal property: De.nition \n5. Given two monads T and R, their coproduct is a monad denoted T + R such that there are monad morphisms \nin1: T . T + R and in2: R . T + R;  for any other monad S and monad morphisms a : T . S, \u00df : R . S, \nthere is a unique monad morphism [a,\u00df] : T + R . S, called the mediating morphism, such that  [a,\u00df].in1 \n= a [a,\u00df].in2 = \u00df (1) A monad morphism is just a natural transformation commuting with the unit and multiplication \n[2, Sect. 3.6]. The universal prop\u00aderty allows us to treat the coproduct of two monads as an abstract \ndatatype; so given monads Triple t1, Triple t2, we have a type Plus t1 t2 a and two injections: inl :: \nTriple t1=> t1 a-> Plus t1 t2 a inr :: Triple t2=> t2 a-> Plus t1 t2 a Morever, given any two monad morphism \nfrom t1 and t2 respec\u00adtively to another monad s, we have a function out of the coproduct. The monad morphisms \nhave the type t1 a-> sa, but each has to be de.ned uniformly for all a, hence the type of coprod uses \nrank-2 polymorphism and is not standard Haskell98 anymore: coprod ::(Triple t1, Triple t2, Triple s)=> \n(forall a.t1 a-> s a)-> (forall a.t2 a-> s a)-> Plus t1 t2 a-> s a Just like fold on lists is given by \nthe universal property (i.e. free\u00adness, initiality) of the datatype [a], coprod is given by the univer\u00adsal \nproperty of the coproduct, and just like fold, it can be used to implement recursive functions on the \ncoproduct. We now consider a small example. Assume that we have an excep\u00adtion monad Exn (see Sect. A.2). \nUsing this monad, we can de.ne a function check :: Char-> Exn Char check c = if isPrint c || isSpace \nc then return (toLower c) else raise \"Illegal character\" which takes its argument to lower case, but \nraises an exception if the character is neither printable nor a white space. We now want to combine this \nexception-raising function with a state monad Store (see Sect. A.3) to implement a function which imperatively \ncounts the occurrence of a particular character in a string, raising an excep\u00adtion if the string contains \nnon-printing non-space characters. The type of the function will be count :: Ref Int-> Char-> String-> \nPlus Exn (Store Int) Int as it uses both Store (of which Ref is a part) and Exn. Within the function \ncount, we can use both stateful computations and exception-raising computations, injecting them into \nthe coproduct using inl and inr: count r _ [] = inr (readRef r) count r x (c:cs) = do c <-inl (check \nc) if c == x then inr (incRef r) else return () count rxcs To run this computation, we have to map Store \nand Exn to another monad using coprod. Note that we have not de.ned coprod yet (we will do so in Sect. \n3.2); the point here is that to use the coprod\u00aduct all we need is the universal property, as given by \nthe mediat\u00ading morphism coprod. This follows good programming practice in hiding the implementation of \nan abstract data type from the pro\u00adgrammer. The simplest monad possible is the identity monad Id, as \nde.ned in the Appendix A.1. We can use the identity monad as a target monad if for both t1 and t2, we \nhave maps t1 a->a,t2 a-> a. For the identity monad we get a special case of coprod: coprod :: (Triple \nt1, Triple t2)=> (forall a. t1 a-> a)-> (forall a. t2 a-> a)-> Plus t1 t2 a-> a coprod f g x = r where \nId r = coprod (Id. f) (Id. g) x To evaluate exceptions, we catch all exceptions which might occur, and \nto evaluate stateful computations, we have the runSt combina\u00adtor. Then, we can run the count computation \nwith run :: Char-> String-> Int run x s = coprod catch runSt (do r<-inr (newRef 0) count r xs) Now we \nwant to augment the function by having it print a dot each time it encounters an occurrence of the character \nit is supposed to count. Thus, we add the prede.ned IO monad to the type by replacing Store Int with \nPlus (Store Int) IO, and the type becomes count :: Ref Int-> Char-> String-> Plus Exn (Plus (Store Int) \nIO) Int The code remains largely unchanged, except that we need to change the injections of the stateful \ncomputations to inr.inl. This change seems inconvenient at this point, but we will show how to avoid \nit later (see Sect. 3.4): count r _ [] = inr (inl (readRef r)) count r x (c:cs) = do c <-inl (check c) \nif c == x then do inr (inl (incRef r)) inr (inr (putStr \".\")) else return () count r x cs This example \nalso shows that we can use both self-de.ned and built-in monads. To run this example, we cannot use the \nidentity monad as the target anymore, since we can not get out of the IO monad; hence, we use IO as the \ntarget monad: run :: Char-> String-> IO Int run x s= coprod (return.catch) (coprod (return. runSt) id) \n(do r<-inr (inl (newRef 0)) count r x s) Summing up, if we have two different monads and we want to im\u00adplement \na computation using both of these, we can use the coprod\u00aduct of the two monads. The universal property \nof the coproduct gives the mediating morphism coprod, which allows us to de.ne functions (like run and \nrun above) out of the coproduct, by de.n\u00ading functions f and g on the component monads. So, we can have \nmore than one function out of the coproduct monad, but coprod f g is determined uniquely by f and g. \nNow that we have seen how to use the coproduct in a practical situ\u00adation, we turn to the actual construction \nof the coproduct monad. 3.2 Constructing the Coproduct To motivate our general construction, we consider \nthe simple case of the coproduct of two term algebra monads. Given two signatures S and . with corresponding \nterm algebra monads TS and T., the coproduct TS +T. should calculate the terms built over the disjoint \nunion of the signatures, S +., i.e. TS +T. = TS+..2 Terms in TS+.(X) have an inherent notion of layer, \nas a term in TS+. either is a variable or decomposes into a term from TS (or T.), and strictly smaller \nsubterms with head symbols from . (or S respectively). This suggests that we can build the action of \nthe coproduct TS+.(X) by successively applying the two actions TS and T.: (TS +T.)(X)= X +TS(X)+T.(X)+ \nTSTS(X)+TST.(X)+ (2) T.TS(X)+T.T.(X)+ TST.TS(X)+... In terms of Haskell, each layer corresponds to a \nparticular compu\u00adtational feature, and the coproduct allows arbitrary interleavings of computations from \nthe two monads. This re.ects one of our key points, namely that, in general, T.R is too simple a data \nstructure to represent the interaction of T and R since one only has a T -layer sitting above one R-layer. \nUnfortunately, equation (2) is too simple in that different elements of the sum represent the same element \nof the coproduct monad. To see this, note that variables from X are contained in each of the sum\u00admands \nin the right hand side. Similarly, in the example of Sect. 3.1, two or more computations from the same \nmonad are layered above each other and should be composed using the multiplication from that monad. Therefore, \nthe coproduct is a quotient of the sum in equation (2). Kelly [7, Sect. 27] has shown the construction \nof colimits of ranked monads, from which we can deduce coproducts of monads as a spe\u00adcial case. Rank \nis a generalisation of .nitariness to higher cardinals which basically allows operations of in.nitary \narity provided they are bounded above by the rank of the monad. Roughly, the con\u00adstruction of the coproduct \nfor such monads proceeds in two steps: we .rst construct the coproduct of pointed functors, and then \nthe coproduct of two monads. A pointed functor is an endofunctor S : C . C with a natural trans\u00adformation \ns :1 . S (this is called premonad in [6]). Every monad is pointed, so taking the coproduct of pointed \nfunctors is a .rst step towards the construction of the coproduct of monads. In the term algebra example, \nthe natural transformation .T :1 . TS models the 2This relies on the fact that the mapping of signatures \nto monads preserves the coproduct, which it does because it is a left adjoint. variables, and the coproduct \nof two pointed functors S,T should be the functor which for any given set X returns the union of TX and \nSX with the variables identi.ed. In Set, we identify elements of a set by taking the quotient. Thus, \nfor example to share the variables from X in TS(X)+T.(X),we quotient the set by the equivalence relation \ngenerated by x ~ x where the quote of the left-hand side injects the variable into TS(X), whereas the \nquote on the right injects the variable into T.(X). Cat\u00adegorically, this process is modelled by a pushout: \nDe.nition 6. Given two pointed functors (T,.T ) and (R, .R), their coproduct is given by the functor \nQ : C . C which maps every object X in C to the colimit in (3) X .T. TX .R . sT. (3) RX sR . QX The \ncoproduct of monads is constructed pointwise as well: the co\u00adproduct monad maps each object X to the \ncolimit of a speci.c dia\u00adgram. De.nition 7. Given two .nitary monads T =(T,.T ,\u00b5T ) and R = (R, .R,\u00b5R), \nthe coproduct monad T +R maps every object X to the colimit of sequence X\u00df de.ned as follows: T +R(X)=colim \n\u00df<. X\u00df X0 =XX1 =QX X\u00df+1 =colim(D\u00df) where Q,sT ,sR are given by Def. 6, and D\u00df is the diagram in Fig. \n1 with the colimiting morphism x\u00df : D\u00df . X\u00df+1. Given the shape of the diagram, x\u00df is a single morphism \nx\u00df : QX\u00df . X\u00df+1 making all arrows in the diagram commute. being a general answer to the construction \nof the coproduct of two monads, the usefulness of Fig. 1 is limited in practice since the shape, size \nand contents of the diagram make it dif.cult to reason with directly. We now turn to our contribution, \nnamely an alterna\u00adtive construction which trades less generality for greater simplicity.  3.3 Implementing \nthe Coproduct De.nitions 6 and 7 show the main dif.culty we are faced when implementing the coproduct: \nnamely, that the coproduct is not a free datatype, but a quotient. (The usual construction of the colimit \nof a diagram is to take the coproduct of all the objects appearing in the diagram, and quotient it by \nthe relation described by the arrows [15, V.2].) The solution to this is to choose a representing type \nfor the coprod\u00aduct, treat it as an abstract datatype with the three operations from Sect. 3.1 (and the \nmonad operations), and have the operations op\u00aderate on representatives of the equivalence class. Thus, \nwe need a decision procedure for this equivalence. Unfortunately, in general this is impossible: for \nexample if the monads are modelling alge\u00adbraic theories (ie term algebras quotiented by equations), then \nthis amounts to asking if two elements s,t . TX are equal under the equations of the theory. This is \nin general undecidable. However, in order to decide the equivalence from Fig. 1, we do not need to decide \nthe full equational theory of the two monads in\u00advolved, but merely when an element t is in the image \nof the unit ., i.e. t =.(s). Such a layer is called a variable layer and the following example demonstrates \ntheir importance: an element of TST.TS(X)consists of a S-layer over a .-layer which is itself over a \nS-layer. If the middle .-layer is a variable layer, then in the quo\u00adtient this element will be equal \nto the element of TSTS(X)consist\u00ading of the two S-layers. In turn, this element will be equal to the \nre\u00adsult of applying the multiplication of TS. Our construction concerns itself with layered monads for \nwhich this question is decidable. The name is chosen to indicate that for such monads we can tell if \nan el\u00adement is a proper layer or a variable layer. Concretely a monad is TX\u00df-1 layered if there is a \nfunction .-1: TX . 1 +X for all X, which X for t in TX either returns an element x of X such that .(x)=t,or \nT sTTx\u00df . . TX\u00df 1 . returns the canonical element * of 1, if there is no such x. (In other TTX\u00df-1 TTX\u00df-1 \nTQX\u00df-1 words, .-1 is partial.) While the coproduct for any two monads with rank exists, and is given \nby Def. 7, our construction will only apply to layered monads but, for these monads, our construction \nis QX\u00df simpler and hence easier to reason about. . RRX\u00df-1 . RQX\u00df-1 RsR De.nition 8. A layered monad is \na monad T =(T,.,\u00b5) such that there is a natural transformation .-1 RRX\u00df-1 1 Rx. \u00df RX\u00df X : TX . 1 +X, \nwhich is a partial left inverse for .X , i.e. for all X, .-1..X =in1 (where X RX\u00df-1 in1: X . X +1 is \nthe inclusion). Figure 1. The diagram de.ning the coproduct. Note that the two triangles on the left \nof Fig. 1 are not the unit laws of the two monads T,R (see Def. 3), otherwise the diagram would be trivial. \nDef.3 has been given for .nitary monads, and can easily be ex\u00adtended to monads with rank, using trans.nite \ninduction. However, it is unclear how this would work out in practical terms, so we re\u00adstrict ourselves \nto .nitary monads here. As has been pointed out earlier, all known computational monads are .nitary except \nthe con\u00adtinuation monad, which actually does not even have a rank. Despite Layered monads allow us to \ndecide the equivalence on QX from Def. 6 as follows: s is equivalent to t iff .-1(s)=.-1(t). =*. The \ntype class of layered monads is a straightforward extension of the Triple class; the codomain X +1of \n.-1 corresponds to Haskell s Maybe type. In fact, we will only consider layered monads, hence we will \nadd this function to the class: class Functor t=> Triple t where ... etaInv :: t a-> Maybe a It might \nbe tempting to provide etaInv _ = Nothing as a default de.nition, but that would be wrong in being semantically \nincorrect. Analysing Fig. 1, we can see the coproduct will either consist of el\u00adements of the form TX\u00df \nor RX\u00df, where X\u00df is again a representation of the coproduct, or for the base case, X0 = X. Hence given \ntwo monads (triples t1 and t2), the coproduct will be represented by a recursive datatype which contains \neither a layer from t1, or from t2, or a variable: data Plus t1 t2 a = T1 (t1 (Plus t1 t2 a)) | T2 (t2 \n(Plus t1 t2 a)) | Var a where t1 and t2 are instances of Triple.3 The coproduct will be a quotient of \nthis datatype by the equivalence relation generated by the diagram D\u00df (Fig. 1). To understand this equivalence, \nwe come back to equation (2) where there are three forms of equalities. In the following, let S = {F,G} \nand . = {H}. For a variable x . X, we have x . X, x . TS(X) and x . T.TS(X) (and many more); all of \nthese should denote the same term. This equivalence is generated by Def. 6, and the arrows sT ,sQ in \nthe diagram. For the implementation, we need to use the map etaInv to detect and remove these vari\u00adable \nlayers.  The terms t1 = GG x . TS(X) and t2 = G G x . TS(TS(X)) are both equivalent to GG x . TS+.(X). \nBy collapsing the two layers, one identi.es these terms. This equivalence is generated by the arrows \n\u00b5T ,\u00b5R in the diagram while the im\u00adplementation will need to check for repeated layers and use the relevant \nmultiplication to collapse such layers.  The sum (2) over-simpli.ed matters by considering only terms \nwhich, when descending from the root to any leaf, we pass through the same number of quotes. However, \nterms such as F( G x, H x) in TS(TSX + T.(X)) must also be consid\u00adered. Given that the symbol G comes \nfrom the same signature as F, we create a repeated S-layer so that the F and G can be collapsed together. \nIn effect, we try to create a S-sublayer un\u00adderneath the top S-layer to which the multiplication can \nthen be applied.  Thus, a normal form for the equivalence generated from D\u00df called a witness should \nbe a term which has no variable layers and no subterms whose top symbol comes from the same monad as \nthe term as a whole. Drawing together all the three quotients just mentioned, all of the following terms \nhave the same witness, namely t3: t1 = F( G x, H y) t2 = F( G x, H y) t3 = F(G x, H y) Note that for \nlegibility we do not distinguish between the quotes associated to S and those associated to .. Thus from \nDef. 2, we have that if x . X, then x . TS(X); hence, if t . TS(X), then t . TS(TS(X)),or t . T.(TS(X)). \nThus, the quotes syntactically represent the layer information (which would otherwise only be present \nimplicitly). In order to calculate the witness of a term, we recursively strip away all unnecessary quotes \nand collapse layers wherever possible. To de.ne the function calculating the witness, we use the fact \nthat the datatype Plus is an initial algebra \u00b5Y.FY of the functor FY = X + 3Haskell would allow us to \nput class constraints on the variables t1 and t2 here, but this only constrains the types of the construc\u00adtors, \nwhich is a bit pointless. TY + RY (X are the variables), and we can give a recursive function \u00b5Y.FY . \nA out of this datatype by giving an F-algebra structure to A, i.e. a map FA . A, which in turn means \nthree functions X . A, TA . A, RA . A. As a higher-order function, this is called fold (just like its \ncounterpart on lists): fold :: (Functor t1, Functor t2)=> (a-> b)-> (t1 b-> b)-> (t2 b-> b)-> Plus t1 \nt2 a-> b fold e f1 f2 (Var a) =e a fold e f1 f2 (T1 t) = f1 (fmap (fold e f1 f2) t) fold e f1 f2 (T2 \nt) = f2 (fmap (fold e f1 f2) t) De.ning the function strip which strips away all unnecessary quotes is \nstraightforward: strip1 :: Triple t1=> t1 (Plus t1 t2 a)-> Plus t1 t2 a strip1 t = case etaInv t of Justx \n-> x Nothing -> T1 t strip2 :: Triple t2=> t2 (Plus t1 t2 a)-> Plus t1 t2 a strip2 t = case etaInv t \nof Justx -> x Nothing -> T2 t strip :: (Triple t1, Triple t2)=> Plus t1 t2 a-> Plus t1 t2 a strip = fold \nVar strip1 strip2 Collapsing layers is achieved by the multiplication \u00b5 : TT . T ,so for example \u00b5(F( \nG x, H y))= t3 above. But we also have to collapse the term t1 to t3, and t1 is not element of TS(TS(X)),but \nof TS(TS(X)+ T.(X)). In other words, we could collapse in the .rst argument, but not the second. However, \nthe second argument is equivalent to the term H x, which is in TS(T.(X)),so t1 is equivalent to t(= \nF( G x, H x) . TS(TS(X + T.(X))) which 1 now has a repeated layer to which the multiplication can be \napplied to give t3. This latter process we call lifting as we are raising a sub-layer. All of this motivates \nthe following de.nition: to calculate the wit\u00adness, we .rst recursively calculate the witness of the \nsubterms; then lift the top sublayer to create a repeated layer if possible; then apply the multiplication; \nand .nally strip of the top layer if it is a variable layer. Of these, the recursive calculation is achieved \nby de.ning the witness function in terms of fold from above, which applies its argument functions recursively \nto all subterms: lift1 :: Triple t1=> Plus t1 t2 a-> t1 (Plus t1 t2 a) lift1 (T1 t) = t lift1t =etat \nwit1 :: Triple t1=> t1 (Plus t1 t2a) -> Plus t1t2 a wit1 t = strip1 (mu (fmap lift1 t)) lift2 :: Triple \nt2=> Plus t1 t2 a-> t2 (Plus t1 t2 a) lift2 (T2 t) = t lift2t =etat wit2 :: Triple t2=> t2 (Plus t1 \nt2a) -> Plus t1t2 a quadratic in the number of layers, since it uses fold. Here is a wit2 t = strip2 \n(mu (fmap lift2 t)) more ef.cient version, which is linear in the number of layers: wit :: (Triple t1, \nTriple t2)=> Plus t1 t2 a-> Plus t1 t2 a wit = fold Var wit1 wit2 We can now implement eta and mu, and \nhence make Plus an in\u00adstance of Triple. For this, we need to start by making it an instance of Functor: \ninstance (Functor t1, Functor t2)=> Functor (Plus t1 t2) where fmap f = fold (Var. f) T1 T2 The simplest \nde.nition of mu would be mu :: (Triple t1, Triple t2)=> Plus t1 t2 (Plus t1 t2 a)-> Plus t1 t2 a mu = \nwit. fold id T1 T2 But the argument of mu consists of Plus t1 t2 terms built over Plus t1 t2 terms, and \nwe can assume that these are already in normal form, so we only need to compute the witnesses of the \nup\u00adper layer. instance (Triple t1, Triple t2)=> Triple (Plus t1 t2) where etax =Varx etaInv (Var x) = \nJust x etaInv (T1 t) = Nothing etaInv (T2 t) = Nothing mu = fold id wit1 wit2 All that remains are now \nthe injections into the coproduct, and the unique mediating morphism. The injections are simple (we only \ngive one): inl :: Triple t1=> t1 a-> Plus t1 t2 a inl t = T1 (fmap Var t) For the de.nition of the unique \nmediating morphism, we recur\u00adsively evaluate the layers of coproduct in the target monad. Given monad \nmorphisms f or g, we apply f and g to each layer of the coproduct and compose the resulting computation \nwith the multi\u00adplication of the target monad: coprod :: (Triple t1, Triple t2, Triple s)=> (forall a. \nt1 a-> s a)-> (forall a. t2 a-> s a)-> Plus t1 t2 a-> s a coprod f g = fold eta (mu. f) (mu. g) Note \nthat f and g above have to be monad morphisms, i.e. com\u00admute with unit and multiplication. Just like \nthe monad laws, we cannot denote this in Haskell, so it is an external assumption that the programmer \nis responsible for. To sum up, the coproduct of two monads with rank, a mild techni\u00adcal condition, always \nexists and is given by Def. 7. This de.nition, however, is very abstract, and dif.cult to reason with \ndirectly, so we have given a simple implementation for a large class of monads called layered monads. \nTo be precise, this implementation works for all .nitary layered monads, which include all usual computa\u00adtional \nmonads except for the continuation monad. In our implementations above, we have striven for clarity, \nnot ef\u00ad.ciency. For example, the fmap operation for the coproduct is instance (Functor t1, Functor (Plus \nfmapf(T1t) =T1 fmapf(T2t) =T2 fmap f (Var x) = Var Functor t2)=> t1 t2) where (fmap (fmap f) t) (fmap \n(fmap f) t) (f x) The witness operation as given above is quadratic in the number of layers, and hence \nthe multiplication (mu) of the coproduct is quadratic in the number of layers of the upper monad (i.e. \nin the (>>=) operation of the Kleisli category, the right argument). That could be improved, since we \ndo not need to recompute the whole witness of the upper layer, but merely need to check whether we can \ncollapse layers. However, in a typical situation the upper lay\u00aders will only consist of one layer anyway \n(see the example from Sect. 3.1), so the present de.nition seems a good mix of simplic\u00adity and ef.ciency. \nAn optimised implementation should be linear in the number of layers, since in principle we only need \nto check whether the new layer which is added can be collapsed with any of the top layers from the term \nit is added to, but we have not pursued the matter further yet.  3.4 Monad Transformers Revisited We \nhave claimed that monad transformers can be seen as squash\u00ading the different layers in the coproduct \nmonad into one particular monad. While the de.nition is elegant, there seems little meta\u00adtheoretic support, \ne.g. it is not clear when and how one can de.ne a monad transformer for a speci.c monad. A related problem \nis that monad transformers can not be used for to combine existing monads (like the ubiquitous IO monad). \nHaving said that, the coproduct gives us a canonical monad trans\u00adformer, which has some practical relevance. \nTHEOREM 3.1. Given any monad T, the functor T + which takes any other monad S to the coproduct with T \nis a monad transformer. PROOF. The coproduct construction is functorial. Pointedness re\u00adquires for every \nmonad S a monad morphism S . T + S. This can be taken to be the inclusion of the coproduct and then naturality \nfollows from the naturality of inclusions, as does the fact that it is a monad morphism. We can use this \nto introduce some conventions which make the coproduct more readily usable. Recall that in Sect. 3.1 \nabove, as we added the IO monad to the coproduct, we had to replace some existing injections, because \nwe changed the type of the ex\u00adpression from Plus Exn (Store Int) Int to Plus Exn (Plus (Store Int) IO) \nInt. Clearly, this is inconvenient; if we want to add a monad to an already existing computation, we \ndo not want to have to change existing injections. Now, the category Mon(C ) of monads over C has an \ninitial object, namely the identity monad Id (see Sect. A.1), and in any category with coproducts and \nand an initial object 0, we have X + 0 ~ = X. In other words, for every monad T, we have T + Id ~Thus, \n= T. if we have a function which has the result type ta, we can al\u00adways replace this with the type Plus \nt Id a, and insert the injec\u00adtions inl e for terms e::ta. If we now want to add a second monad s, all \nwe need is to change the type to Plus t (Plus s Id a); previously existing code pertaining to the monad \nt remains unchanged, while new code is transformed in that we need to write inr (inl e ) for expressions \ne :: sa. To add a third monad m, we change the type to Plus t (Plus s (Plus m Id)), and write inr (inr \n(inl e)) etc. Thus, the monad Id serves as a placeholder for future extensions and these extensions do \nnot alter the currently existing code.4 As a brief example, assume that above we would have started \nwith a recursive version of the count function. This would have type count :: Char-> String-> Plus Exn \nId Int Then, the .rst extension would be to add the imperative counting, leading to the type count :: \nRef Int-> Char-> String-> Plus Exn (Plus (Store Int) Id) Int and .nally, with the addition of IO, we \nwould obtain count :: Ref Int-> Char-> String-> Plus Exn (Plus (Store Int) (Plus IO Id)) Int but unlike \nabove, the second version would already use inr.inl to embed the stateful computations, so when adding \nthe IO monad we would not have to change the existing code. In Haskell, this would need some syntactic \nsugar to reduce the clut\u00adter (we want to read and write t instead of Plus t Id, write per\u00adhaps inn for \ninrn(inl t) etc.), akin to the do notation.  4 Properties of the Coproduct Of course, we have to justify \nthe constructions of the previous sec\u00adtion. A full formal correctness proof would be very categorical \nand hence outside the scope of this paper, but we will sketch how one goes about proving correctness. \nMoreover, the de.nitions intro\u00adduced in this section will also be used later on. 4.1 The Coproduct as \na Free Algebra First off, note that the datatype Plus t1 t2a is not the coproduct ~ of the two monads, \ni.e. (T + R)(X) .t1t2 a. As has been = Plus pointed out, Plus t1 t2 a only represents the coproduct which \nis actually a quotient of Plus t1t2 a The straightforward way to prove this quotient is the coproduct \nmonad would be to .rst show that the monad laws hold for the quo\u00adtient, and then prove that on the equivalence \nclasses, the injections inl and inr are monad morphisms, that coprod f g is a monad morphism, that it \nis unique, and that it satis.es equations (1). This is a lot of work; fortunately, from the categorical \nconstructions we employ, there is an easier way. This alternate proof rests on the idea that one can \nunderstand a monad through its algebras: De.nition 9. An algebra (X,h) for a monad T = (T,.,\u00b5) on a cat\u00adegory \nC is given by an object X in C , and a morphism h : TX . X which commutes with the unit and multiplication \nof the monad, i.e. 1X = h..Xh.\u00b5X = h.Th (4) 4Note that the quadratic complexity of the mu operator men\u00adtioned \nabove does not have a detrimental effect here, since by its construction the Id monad does not actually \nbuild any proper layers everything is a variable. The category of algebras for T and morphisms between \nthem is called T -Alg. We think of a T-algebra (X,h) as being a model with carrier X. The map h ensures \nthat if one builds terms over a such a model, then these terms can be reinterpreted within the model. \nThis is exactly what one is doing in the term algebra case where one assigns to every function symbol \nf of arity n an interpretation [[ f ]] : Xn . X. Since monads construct free algebras, we can prove a \nfunctor to be equal to a monad if we can prove that the functor constructs free algebras. In particular, \nwe can prove a functor to be the coproduct monad if we can prove it constructs free T + R-algebras which \nare de.ned as follows: De.nition 10. The category T+R-Alg has as objects triples (A, ht ,hr) where (A,ht \n) is a T-algebra and (A,hr) is an R-algebra. A morphism from (A,ht ,hr) to (A(,ht(,h() consists of a \nmap f : A . A( rwhich commutes with the T and R-algebra structures on A and A(. There is an obvious forgetful \nfunctor U : T+R-Alg . C , which takes a T + R-algebra to its underlying object, and we have the following: \nPROPOSITION 4.1 ([7, PROP. 26.4]). If the forgetful functor U : T+R-Alg . C functor has a left adjoint \nF : C . T+R-Alg, i.e. if for every object in C there is a free T+ R-algebra, then the monad resulting \nfrom this adjunction is the coproduct of T and R. Thus to show that a functor S is the coproduct T + \nR, we can show that for every object X, SX is a T + R-algebra and, moreover, it is the free T + R-algebra; \nin other words, if there is any other T + R\u00adalgebra (A, ht(,h(h) and a morphism f : X . A, then there \nis a unique algebra morphism !f : X . Y . Prop. 4.1 shows that action of the coproduct monad creates \nfree algebras. Functions de.ned using such morphisms are sometimes called catamorphisms [3], and the \ncanonical example is the free datatype of lists, and functions using fold on lists. So, coprod is for \nthe coproduct of monads what fold is for lists, and hence we can use it in this way as we did in Sect. \n3.1.  4.2 Distributivity Revisited From our perspective, composing two monads T and R means com\u00adbining \nall possible interleavings of layers from the component mon\u00adads in T + R. In contrast, the approach of \ndistributivity is to con\u00adsider only one possible layering, namely T.R consisting of a T -layer above \nan R-layer. However, in the presence of a strong distributivity law, the monad T.R actually is the coproduct. \nThus an alternative analysis is that distributivity is a special situation in which all layers can be \nsquashed into the layer T.R. First the relevant de.nitions [2]: De.nition 11. Given two monads T = (T,.,\u00b5),S \n= (S, ., .),a dis\u00adtributive law is a natural transformation . : T.S . S.T satisfying four coherence laws \n[2, Sect. 9.2]. The four coherence laws state that . respects the unit and multipli\u00adcation of the monads, \ne.g. .T = ..T . or S\u00b5..T .T . = ..\u00b5S. Given such a distributive law ., we have the compatible monad [2, \nProp. 9.2.2] T = (S.T, .* ,\u00b5 *) with .* = S...,\u00b5 * = S\u00b5..TT .S.T . However, with a slightly stronger \ndistributive law, the compatible monad is also the coproduct of T and S. De.nition 12. Given two monads \nT = (T,.,\u00b5),S = (S, ., .),a uses naturality of . and (4) for (K,a); for the second, we addition\u00adstrong \ndistributive law is a distributive law . : T.S . S.T such that, ally need strong distributivity (5). \nfor any T + S-algebra (X,a,\u00df), diagram (5) commutes: Now assume we have another T + S-algebra morphism \nm : STX . TSX T \u00df . TX K s.t. m..* = f , and consider diagram (8); the lower two parallelo\u00ad . . ST . \n STX a (5) STX . STSX . Sa . . \u00df SX . X THEOREM 4.2. Given two monads T = (T,., \u00b5),S = (S,.,.) and \na strong distributive law . : T.S . S.T , the compatible monad T = 1 SSTX STSTX (S.T, .* ,\u00b5 *) is the \ncoproduct of T and S. PROOF. To show the theorem, we .rst give T a T+S-algebra struc\u00ad ture, and then \nshow it is a free algebra. By Prop. 4.1, it is then the . coproduct T + S. STX 1 STm . SSTTX STK . The \nalgebra structure is given by two maps .T.TS\u00b5 SSTX . STX TSTX . STTX . STX (8) 1 SSTX Equations (4) easily \nfollow from the coherence laws of the distribu\u00ad tive law, and the unit laws of the monads. We now have \nto show that the algebra structure is free. To this end, we show that given any other T + S-algebra (K, \na, \u00df) and a morphism f : X . K, there is a unique morphism !f : STX . K STX SK such that !f ..* = f , \nand ! f is a T + S-algebra morphism. The unique morphism is de.ned as !f = \u00df.Sa.ST f . Diagram (6) m \nshows that !f ..* = f , where the squares are naturality squares, and the triangles are the left equation \nof (4) for the T + S-algebra (K,a,\u00df). K X . . SX S.. STX f Sf ST f K . . . SK . S.. ST K . grams are \n(7) with m for !f (since m is a T + S-algebra morphism), the triangles on the left are the unit laws \nof the monads, the diamond is naturality of ., and the triangle on the top coherence of ..Now, the arrow \non the left-hand face of the diagram is m, and it equals the arrows on the right-hand face of the diagram, \nand we have (with the assumption f = m..* = m.S...) m = \u00df.Sa.STm.STS..ST . = \u00df.Sa.ST (m.S...) = \u00df.Sa.ST \nf \u00df Sa (6) = ! f K SK making !f unique as required. It should be pointed out that the strong distributivity \nrequirement is \u00df more than a mere technical condition. For example, consider the two monads given by \nsignatures S = {F},. = {G} with a unary operation each. The terms in TS(X) are of the form Fn(x) (i.e. \nF K To show that !f is an algebra morphism, we have to show that SSTX S! f . SK TSTX T ! f . TK . . .T \n\u00df ST T X a (7) ST X . ! f . K . ST X S\u00b5 . ! f . K . The proofs are simple diagram chases, which we omit \nhere; the .rst applied n times). Then clearly there is a distributive law . : TST. . T.TS, which takes \nFn(Gm(x) to Gm(Fn(x). However, there is no strong distributive law as this would require that taking \nany carrier set with any two unary operations, these should commute. The relevance of Theorem 4.2 is \nthat even when working with dis\u00adtributivity laws, we are close to working with the coproduct. The prime \nexample here is if one of the monads in question is the ex\u00adception monad, then we always have a strong \ndistributivity law .Exn : Exn.T . T .Exn, hence for any other monad T, we have T + Exn(X)= T (Exn(X)) \n(9)  5 Conclusion This paper has introduced the coproduct of two monads as their canonical combination: \nthe smallest non-interacting combination of their computational effects. From the general categorical \ncon\u00adstruction [7], we have derived an implementation in Haskell for a wide class of monads, the so-called \nlayered monads. Computations in the coproduct monad can be thought of as se\u00adquences of steps in the two \ncomponent monads. In particular, the monads retain their laziness. Thus, Store a + Store b is se\u00adquences \nof steps in Store a and Store b; this is however not the same as the monad Store (a, b), since in the \nlatter we can store and retrieve tuples of (a,b), which is not possible in the coproduct. We have investigated \nthe relationship of other combinations based on distributivity, and have shown that if the two monads \nare strongly distributive, then their coproduct coincides with the functorial com\u00adposition. Comparison \nwith monad transformers has shown that the coproduct is a more general, .exible approach with a stronger \nmeta\u00adtheory. In Haskell (and in particular the Glasgow Haskell Compiler, GHC), the IO monad is the mother \nof all monads; it has mutable references (i.e. state transformers), input/output, concurrency, exceptions \nand much more. In our approach, we would suggest to dismantle the IO monad into its constituting monads, \nand combine them with the co\u00adproduct as needed. This has the advantage that the effects of most monads \ncan be localised, so using e.g. state threads or exceptions in one part of the program will not make \nthe type of every func\u00adtion using this part monadic, as is currently the case with the IO monad. Of course, \nthe IO monad can be recovered (and continued to be used) as the coproduct of all its components. It remains \nto be investigated whether the optimisations currently afforded to the IO monad by GHC can still be carried \nout in our style, but it seems unlikely this should not be the case, as all that would be done is to \nprovide a more precise typing. We are con.dent that this approach will scale to the combination of more \nthan a handful monads, not least because the coproduct is only the simplest way of combining monads. \nIf we allow arbitrary colimits of monads, this will allow shared computation effects (for example, shared \nstate or shared exceptions), and imposed equations such as the distributivity laws (by using coequalisers). \nThis situa\u00adtion is not uncommon; for example, when combining a state monad with another monad, we typically \ndo not want a stateful computa\u00adtion, followed by a computation from the other monad, followed by another \nstateful computation, as then the second stateful computa\u00adtion knows nothing about the .rst, and in particular \ncannot access its state; we would want the stateful computation to distribute over the other computation, \nso the second stateful computation can use the state left by the .rst stateful computation. This is related \nto re\u00adcent work by Plotkin, Power and Hyland [5], where they describe a commutative combination . which \ncorresponds to an imposed dis\u00adtributivity law in our sense. Technically, their work uses so-called Lawvere \ntheories but these are equivalent to monads [20]. In summary, the coproduct of monads is a simple, modular \nway of combining two monads. It is more general than distributivity and monad transformers, is applicable \nin nearly all cases, and based on sound mathematical foundations. We believe it should the func\u00adtional \nprogrammers .rst choice when combining monads. 6 References [1] J. Adamek and J. Rosick\u00b4y. Locally Presentable \nand Accessi\u00adble Categories. LMS Lecture Notes 183. Cambridge Univer\u00adsity Press, 1994. [2] M. Barr and \nC. Wells. Toposes, Triples and Theo\u00adries. Grundlehren der mathematischen Wissenschaften 278. Springer \nVerlag, 1985. [3] R. Bird and O. de Moor. Algebra of Programming. Prentice-Hall, 1997. [4] M. Fiore, \nG. Plotkin, and D. Turi. Abstract syntax and variable binding. In Proc. LICS 99, pages 193 202. IEEE \nComputer Society Press, 1999. [5] Martin Hyland, Gordon Plotkin, and John Power. Combin\u00ading computational \neffects: Commutativity and sum. In TCS 2002, 2nd IFIP International Conference on Computer Sci\u00adence, \nMontreal, 2002. [6] M. Jones and L. Duponcheel. Composing monads. Technical Report YALEU/DCS/RR-1004, \nYale University, Dept. Comp. Sci, Dec 1993. [7] G. M. Kelly. A uni.ed treatment of trans.nite constructions \nfor free algebras, free monoids, colimits, associated sheaves and so on. Bulletins of the Australian \nMathematical Society, 22:1 83, 1980. [8] G. M. Kelly. Basic Concepts of Enriched Category Theory, LMS \nLecture Notes 64. Cambridge University Press, 1982. [9] G. M. Kelly and A. J. Power. Adjunctions whose \ncounits are coequalizers, and presentations of .nitary monads. Journal for Pure and Applied Algebra, \n89:163 179, 1993. [10] David King and Philip Wadler. Combining monads. In J. Launchbury and P.M. Samson, \neditors, Glasgow Workshop on Functional Programming, Workshops in Computing Se\u00adries, Ayr, July 1992. \nSpringer Verlag. [11] S. Liang, P. Hudak, and M. Jones. Monad transformers and modular interpreters. \nIn Proceedings of the 22nd ACM Symposium on Principles of Programming Languages.ACM Press, Jan 1995. \n[12] C. L\u00a8uth. Categorical Term Rewriting: Monads and Modular\u00adity. PhD thesis, University of Edinburgh, \n1998. [13] C.L\u00a8uthandN.Ghani.Monadsandmodulartermrewriting.In Category Theory in Computer Science CTCS \n97, LNCS 1290, pages 69 86. Springer Verlag, September 1997. [14] C. L\u00a8uth and N. Ghani. Monads and modularity. \nIn Frontiers of Combining Systems FroCoS 02, LNAI 2309, pages 18 32. Springer Verlag, 2002. [15] S. Mac \nLane. Categories for the Working Mathematician, Graduate Texts in Mathematics 5. Springer Verlag, 1971. \n[16] Ernest G. Manes. Algebraic Theories, Graduate Texts in Mathematics 26. Springer Verlag, 1976. [17] \nE. Moggi. Computational lambda-calculus and monads. In Fourth Annual Symposium on Logic in Computer Science. \nIEEE, Computer Society Press, June 1989. [18] E. Moggi. An abstract view of programming languages. Tech\u00adnical \nReport ECS-LFCS-90-113, LFCS, 1990. [19] Gordon Plotkin and John Power. Notions of computation de\u00adtermine \nmonads. In M. Nielsen and U. Engeberg, editors, Proc. FOSSACS 2002, LNCS 2303, pages 342 356, 2002. [20] \nJohn Power. Enriched Lawvere theories. Theories and Appli\u00adcations of Categories 6:83 93, 2000. [21] E. \nRobinson. Variations on algebra: monadicity and generali\u00adsations of equational theories. Technical Report \n6/94, Sussex Computer Science, 1994. [22] D. E. Rydeheard and J. G. Stell. Foundations of equational \ndeduction: A categorical treatment of equational proofs and uni.cation algorithms. In Category Theory \nand Computer Science, LNCS 283, pages 114 139. Springer Verlag, 1987. [23] M. B. Smyth and G. D. Plotkin. \nThe category-theoretic solu\u00adtion of recursive domain equations. SIAM Journal on Com\u00adputing, 11(4):763 \n783, 1982. A Some Useful Monads This appendix contains the de.nitions for the monads as used in the \ntext. A.1 The identity monad The identity monad is very simple: newtype Id a=Id a instance Functor Id \nwhere fmap f(Id x) =Id (f x) instance Triple Id where etax =Idx etaInv (Id x) = Just x mu (Idx) =x The \nidentity monad is the initial object of Mon(C), as for any other monad T = (T,.,\u00b5), the unit is monad \nmorphism from Id to T; this gives a unique monad morphism !T : Id . T. The identity monad is trivially \n.nitary. A.2 The exception monad The exception monad adds an error element to the base. In category \ntheory, this is also known as the lifting monad. type Error = String data Exn a = Exn Error | Base a \ninstance Functor Exn where fmap f (Base x) = Base (f x) fmapf(Exne) =Exne instance Triple Exn where eta \n= Base etaInv (Base a) = Just a etaInv (Exn e) = Nothing mu(Exne) =Exne mu (Base (Exn e)) = Exn e mu \n(Base (Base b)) = Base b The identity monad is .nitary: if we add an error element to all .\u00adnite subsets \nof an in.nite set of variables and take their union, all the exceptions are uni.ed, leaving us with an \nin.nite set of vari\u00adables and one error element. An easier way to see this is that the exception monad \nis the free monad on a signature consisting only of one constant. A.3 The store monad The store monad \nis a simple state transformer monad. The state is a list, and references are indices into the list. This \nis not very ef.cient, but simple. We leave out the implementations of newRef etc, since they are standard. \ndata Store a b = St ([a]-> ([a], b)) instance Functor (Store a) where fmap f (St s0) = St (\\s-> let (s1, \nx)= s0 s in (s1, f x)) instance Triple (Store a) where eta a = St (\\s-> (s, a)) etaInv (St s) = Nothing \nmu (St s0) = St (\\s-> let (s1, St c)= s0 s in cs1) dataRefa =RefaInt newRef :: a -> Store a (Ref a) readRef \n:: Ref a-> Store a a writeRef :: Ref a -> a-> Store a () runSt :: Store a b-> b incRef :: Num a=> Ref \na-> Store a () incRef r = do i <-readRef r writeRef r (i+1) The state transformer monad, of which this \nstore monad is a par\u00adticular example, is .nitary provided the underlying state is .nite (a reasonable \nassumption). So the store monad is .nitary as long as we restrict ourselves to .nite lists of stores. \nThe full sources for the code used in this paper can be found at http://www.informatik.uni-bremen.de/~cxl/papers/ \nicfp02-src.tar.gz.   \n\t\t\t", "proc_id": "581478", "abstract": "Monads are a useful abstraction of computation, as they model diverse computational effects such as stateful computations, exceptions and I/O in a uniform manner. Their potential to provide both a modular semantics and a modular programming style was soon recognised. However, in general, monads proved difficult to compose and so research focused on special mechanisms for their composition such as distributive monads and monad transformers.We present a new approach to this problem which is general in that nearly all monads compose, mathematically elegant in using the standard categorical tools underpinning monads and computationally expressive in supporting a canonical recursion operator. In a nutshell, we propose that two monads should be composed by taking their <i>coproduct</i>. Although abstractly this is a simple idea, the actual construction of the coproduct of two monads is non-trivial. We outline this construction, show how to implement the coproduct within Haskell and demonstrate its usage with a few examples. We also discuss its relationship with other ways of combining monads, in particular distributive laws for monads and monad transformers.", "authors": [{"name": "Christoph L&#252;th", "author_profile_id": "81100103518", "affiliation": "Universit&#228;t Bremen", "person_id": "P394759", "email_address": "", "orcid_id": ""}, {"name": "Neil Ghani", "author_profile_id": "81100427276", "affiliation": "University of Leicester", "person_id": "PP43120510", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/581478.581492", "year": "2002", "article_id": "581492", "conference": "ICFP", "title": "Composing monads using coproducts", "url": "http://dl.acm.org/citation.cfm?id=581492"}