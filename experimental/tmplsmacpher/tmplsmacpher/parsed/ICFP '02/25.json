{"article_publication_date": "09-17-2002", "fulltext": "\n Final Shift for Call/cc: Direct Implementation of Shift and Reset Martin Gasbichler Michael Sperber \nUniversit\u00a8ubingen at T\u00a8 {gasbichl,sperber}@informatik.uni-tuebingen.de Abstract We present a direct \nimplementation of the shift and reset con\u00adtrol operators in the Scheme 48 system. The new implementation \nimproves upon the traditional technique of simulating shift and reset via call/cc. Typical applications \nof these operators exhibit space savings and a signi.cant overall performance gain. Our tech\u00adnique is \nbased upon the popular incremental stack/heap strategy for representing continuations. We present implementation \ndetails as well as some benchmark measurements for typical applications. Categories and Subject Descriptors \nD.3.3 [Programming Languages]: Language Constructs and Fea\u00adtures Control structures General Terms Languages, \nPerformance  Keywords Continuations, Implementation, Scheme 1 Introduction Call/cc (or call-with-current-continuation) \n[4] is becom\u00ading an established part of the vocabulary of the programming language community. Call/cc \ncaptures or rei.es the current continuation the control state of the underlying machine into an escape \nprocedure and passes it to its argument, which must be a one-argument procedure. The escape procedure, \nwhen called, dis\u00adcards its current continuation and throws or re.ects the original con\u00adtinuation back \ninto the machine state. The translation provided by the Danvy/Filinski CPS transforma\u00adtion [9] gives \na de.nition of the semantics of call/cc. The meat of Permission to make digital or hard copies of all \nor part of this work for personal or classroom use is granted without fee provided that copies are not \nmade or distributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. ICFP 02, October 4-6, 2002, Pittsburgh, Pennsylvania, USA. Copyright \n2002 ACM 1-58113-487-8/02/0010 ...$5.00 [x]. = .k.(k (. x)) [.x.M]. = .k.k (.v..k'.([M])(.[x . v]) k') \n[E1 E2]. = .k.[E1]. (. f .[E2]. (.a. fak) Figure 1. Continuation semantics this transformation has a \ndirect counterpart as a semantic speci.ca\u00adtion of the . calculus; Figure 1 shows such a semantic speci.cation \nfor the bare . calculus: each . is an environment mapping variables to values, and each k is a continuation \na function from values to values. An abstraction denotes a function from an environment to a function \naccepting an argument and a continuation. In the context of the semantics, the rule for call/cc is this: \n[call/cc E]. = .k.[E]. (. f . f (.v..k'.kv) k) Call/cc E evaluates E and calls the result f ; it then \napplies f to an escape function which discards the continuation k' passed to it and replaces it by the \ncontinuation k of the call/cc expression. The applications of call/cc are numerous and include the imple\u00admentation \nof interpreters and compilers, exception systems, corou\u00adtines, multi-threading, and non-deterministic \ncomputation. Because of the central role of call/cc in these applications, implementors of functional \nlanguages have developed representations for contin\u00aduations enabling fast implementations of call/cc \n[5]. The control, shift and reset operators are a more recent inven\u00adtion [11, 8]. In contrast to call/cc \nthat always captures the entire control state, control, shift and reset allow capturing only a portion \nof one a so-called composable continuation thus allow\u00ading for more .exibility and expressiveness. We \nfocus on shift and reset in the high-level presentation. We later show how to imple\u00adment shift in terms \nof the slightly more primitive control. Reset introduces a context delimiter; shift turns the context \nbe\u00adtween the shift construct and the enclosing delimiter into a pro\u00adcedure and removes that part of the \ncontext. Shift and reset are best understood by some examples: A sole reset has no effect: (+ 1 (reset \n3)) . 4 Shift deletes the context up to the next enclosing reset: (+ 1 (reset (* 2 (shift k 4)))) . 5 \n(+ 1 (reset (* 2 (shift k (k 4))))) . 9 Multiple invocations are also possible: (+ 1 (reset (* 2 (shift \nk (k (k 4)))))) . 17 Semantically, shift and reset have the following de.nitions in the context of the \nsemantics in Figure 1 [9]: [reset E]. = .k.k([E]. (.v.v)) [shift cM]. = .k.[M](.[c . .v..k ' .k ' (kv)]) \n(.v.v) Reset seeds the evaluation of E with an empty continuation the identity function. It then passes \nthe result of evaluating E to k. This does not affect evaluation as long as there is no shift: after \nall, k would be applied to the .nal result of evaluating E anyway. How\u00adever, call/cc and shift expressions \nevaluated as part of E now see a truncated continuation, as k has been moved out of their view. Shift \nrei.es this truncated (or delimited) continuation into a function that composes it with its own continuation \nand binds that function to c. It then evaluates the body expression M within the empty continua\u00adtion. \nA number of applications make inherent use of shift and reset, among them direct-style formulations of \nmonads [13, 14], continu\u00adation-based partial evaluation [21, 22, 26], and type-directed partial evaluation \n[6]. However, implementations of these control opera\u00adtors are usually formulated in terms of call/cc. \nThis is a reason\u00adable and quite portable approach. However, these indirect imple\u00admentations of shift \nand reset exhibit suboptimal performance. Therefore, we have implemented shift and reset directly. As \nit turns out, the direct implementation provides signi.cant speedup of applications using these control \noperators. Contributions Here are the contributions of our work: We show how to implement control, shift, \nand reset di\u00adrectly in the context of the popular incremental stack/heap strategy for implementations \nof continuations, avoiding some of the overhead associated with indirect implementations of these operators \nvia call/cc.  We show that most typical applications of shift and reset bene.t signi.cantly from the \nperformance improvements gained by their direct implementations. We evaluate a num\u00adber of standard benchmarks. \nIn particular, we show that the direct implementation is an enabling technology for shift/reset-based \nthread systems, avoiding some of the pit\u00adfalls of call/cc-based implementations.  We explain how the \nresults of this paper apply to other pro\u00adgramming language implementations.  Overview The paper is organized \nas follows: The next section reviews the traditional, indirect implementation technique of shift/reset \nvia call/cc. The following Section 3 explains brie.y the general idea of implementing shift/reset directly. \nThe two sections after that explain a direct implementation in more depth: Section 4 gives a brief overview \nof the architectural elements of Scheme 48 involved in the implementation. Section 5 details the direct \nimplementation of shift/reset in Scheme 48. Section 6 presents results from a number of benchmarks. Section \n7 reviews alternative implementa\u00adtion strategies for continuations, and if and how they may bene.t from \na direct implementation of shift/reset. Section 8 surveys some related work, and Section 9 concludes. \n 2 Shift/Resetand Call/cc The traditional implementation of shift/reset [13] involves man\u00adaging a meta-continuation. \nMeta-continutations arise from CPS\u00adtransforming the continuation semantics. The result then gives rise \nto an implementation of shift/reset in terms of call/cc. This section reviews this indirect implementation \ntechnique. CPS-transforming the continuation semantics is a natural step as the rule for reset in the \ncontinuation semantics does not conform to CPS; it contains a non-tail call. Thus, with the rule for \nshift in place, the semantics manages both the usual explicit continuation as well as a new implicit \ncontinuation. Their composition yields the usual intuition of the entire future of the computation. [x]. \n= .k..m.k (. x) m [.x.M]. = .k..m.k (.v..k ' ..m ' .[M](.[x . v]) k ' m ') m [E1 E2]. = .k.[E1]. (. f \n..m ' .[E2]. (.a..m '' . fa k m '') m ') m Figure 2. Meta-continuation semantics CPS-transforming the \nsemantics makes this new implicit continu\u00adation, now called the meta-continuation, explicit again. Figure \n2 shows the result a meta-continuation semantics for the . calcu\u00adlus [8]: the new meta-continuation is \ncalled m, and the old con\u00adtinuation, k, accepts a meta-continuation as an argument. For stan\u00addard . terms, \nnothing fundamental is changed compared to the continuation-semantics the new semantics simply threads \nan ad\u00additional continuation through the evaluation process. The meta-continuation does not yet do anything \nit is shift and reset that manipulate it. Here are the rules for shift/reset: ' ' [reset E]. = .k..m.[E]. \n(.x..m .mx)(.r.krm) '' [shift cM]. = .k..m.[M](.[c . .v..k ' ..m .kv (.w.k ' wm '')]) ' ' (.w..m .mw) \nm Just like in the continuation semantics, reset seeds the evaluation ' ' of E with an empty continuation \n(.x..m .mx) that passes its argu\u00adment to the meta-continuation, ignoring k. Whereas the continua\u00adtion \nsemantics applies k directly, the meta-continuation semantics does this in the meta-continuation. Shift \nalso seeds with an empty continuation, and binds c to a function that applies the currently delimited \ncontinuation k to its parameter v, calls the result w and passes that to its continuation k ' . Call/cc \nalso has a place in the new semantics: ' '' [call/cc E]. = .k..m.[E]. (. f ..m . f (.v..k ' ..m .kvm \n'') km ') m Note that, just like shift, it only rei.es the k part of the entire con\u00adtinuation. This means, \nthat reset in.uences call/cc. The meta-continuation semantics is the key for implementing shift/reset \nin terms of call/cc and assignment. Olivier Danvy contributed an implementation of shift/reset to the \nScheme 48 distribution [20], which is closely based on the original SML for\u00admulation by Andrzej Filinski \n[13]. First, macros for reset and shift package up their expres\u00adsion operands as procedures and pass \nthem to procedural cousins *reset and *shift that do the actual work: (define-syntax reset (syntax-rules \n() ((reset ?e) (*reset (lambda () ?e))))) (define-syntax shift (syntax-rules () ((shift ?k ?e) (*shift \n(lambda (?k) ?e))))) The *meta-continuation* variable holds the current meta\u00adcontinuation: (define (*meta-continuation* \nv) (error \"You forgot the top-level reset...\")) Note that *meta-continuation* will be assigned to procedures \nwhich replace the current continuation. Hence, the *abort proce\u00addure has the effect of discarding the \ncurrent continuation (k in the semantics), leaving only the meta-continuation: (define (*abort thunk) \n(let ((v (thunk))) (*meta-continuation* v))) *Reset is next. In the meta-continuation semantics, reset \nextends the meta-continuation by the current continuation. *Reset uses the escape procedure created by \ncall/cc as the representation of the current continuation: (define (*reset thunk) (let ((mc *meta-continuation*)) \n (call-with-current-continuation (lambda (k) (begin (set! *meta-continuation* (lambda (v) (set! \n*meta-continuation* mc) (k v))) *Reset replaces the meta-continuation by one that calls the current \ncontinuation after restoring the old meta-continuation. As the meta\u00adcontinuation is single-threaded through \nthe semantics [13], the as\u00adsignments have the same effect as the composition in the semantics. Finally, \n*reset discards the current continuation; this is exactly what *abort does: (*abort thunk)))))) *Shift \nmust call its argument with a procedure that composes the continuation of the *shift call with its own \ncontinuation: (define (*shift f) (call-with-current-continuation (lambda (k) (*abort (lambda () (f (lambda \n(v) (reset (k v))))))))) The call (k v) discards its own continuation. Therefore, it is surrounded by \na reset that moves this continuation into *meta-continuation*, effectively protecting it. Again, *shift \nmust discard its own current continuation with *abort. Filiniski gives a rigorous derivation of this \nimplementation of shift/reset it is extensionally equivalent to the meta\u00adcontinuation semantics. However, \nintensionally, the implementa\u00adtion is different from the semantics: whereas the semantics has the continuation \nproper parameterized over the meta-continuation, the implementation uses the underlying call/cc which \nalways rei.es the entire machine-level continuation. This is too large, sometimes by a sizable amount, \nand, as Section 6 demonstrates, causes a sig\u00adni.cant performance penalty. It is possible to reduce the \ndead storage taken up by the representa\u00adtions of composable continuations by having the *abort procedure \napply thunk with an empty or very small continuation, thus poten\u00adtially reclaiming some dead storage \nearly. (In fact, the Scheme 48 version does just that.) However, as it turns out, this optimization does \nnot affect run-time performance in a signi.cant way. (See Section 6.5.)  3 Direct Implementation of \nshift/reset Consider shift and reset in the context of a representation for continuations using linked \nframes. Reset marks a place in the con\u00adtinuation chain which delimits the context later rei.ed by shift. \nShift rei.es the continuation section up to the enclosing mark cre\u00adated by reset, and creates a procedure \nthat will add this section to the chain. This procedure must also set a mark at the link point in the \nchain which corresponds to the statically most recent reset for the rei.ed section. Figure 3 displays \nthe evaluation of (shift k e) in terms of linked continution frames: It cuts off the section of the chain \nconsisting of frames Top and Reset and binds the rei.ed procedure to k. The continuation for the evaluation \nof the body e starts with frame C2. (shift k e) k Top  Figure 3. Evaluation of a shiftexpression If \nthe continuation chain resides in the heap, this strategy involves copying of continuation frames both \nduring rei.cation and re.ec\u00adtion. However, most implementations of Scheme use a represen\u00adtation for continuations \ninvolving a stack cache [5] which contains the most recent continuation frames. In these implementations, \ncap\u00adturing the current continuation via call/cc .ushes the stack cache and moves the continuation frames \nto the heap. A direct implementation of shift/reset can exploit the presence of a stack cache in a simple \nmanner: Instead of copying continu\u00adation frames one-by-one, shift simply block-copies a slice from the \nstack into the heap starting at the current continuation up to the frame marked by reset. During re.ection, \nthe slice is copied back on the stack. This strategy yields a fast and simple implementation of shift/reset. \nOf course, shift and reset are not always this lucky: The marked continuation frame might reside in the \nheap instead of the stack cache. Also, the rei.ed slice may be bigger than the stack cache. An implementation \nhas to consider these complications, along with several others. We consider a direct implementation of \nshift/reset for a pre\u00adrelease of Scheme 48 1.0 [20], a byte-code implementation of Scheme. Scheme 48 \nis attractive because its VM is written in Pre-Scheme [19], a low-level dialect of Scheme; this simpli.es \npresent\u00ading actual code. This section gives a short overview of the architec\u00adture of Scheme 48, highlighting \nthe most important aspects. 4.1 The Incremental Stack/Heap Strategy Scheme 48 uses the incremental stack/heap \nstrategy for represent\u00ading the current continuation; it stores the most recent continuation frames on \nthe stack, which is thus effectively a cache. Scheme 48 pushes new continuation frames on the stack upon \nprocedure calls and pops them upon return. If the continuation does not .t com\u00adpletely in the stack cache, \nthe early frames reside in the heap. The bottom frame in the stack cache is always an under.ow con\u00adtinuation \nframe. The code of this frame copies its parent from the heap into the stack and invokes the parent afterwards. \nHence, ev\u00adery continuation frame can safely return to its parent: the under.ow frame will mediate between \nstack and heap. . Heap2 Heap1 Underflow   Figure 4. Incremental stack/heap strategy The top half \nof Figure 4 shows a typical setup for the incremental stack/heap strategy: The topmost frame of the current \ncontinuation Top is on the stack with one of its parents Top1. Two frames Heap1 and Heap2 have been migrated \nto the heap and linked to the under\u00ad.ow continuation frame (Underflow). The bottom half of Figure 4 shows \nthe situation after Top and Top1 have returned: The under\u00ad.ow frame has copied Heap1 from the heap to \nthe stack. In Scheme 48, stack continuation frames usually consist of three parts: 1. Optional pointers \nto the current environment and the current template. The template contains descriptors for constants \nthe code may need. 2. The operands of the currently active procedure call. 3. A code pointer. Scheme \n48 maintains the size of the continu\u00adation frames in headers in the byte code.  The topmost frame does \nnot contain an explicit code pointer be\u00adcause it is currently executing in the VM. The frames need not \nbe explicitly linked on the stack, as the compiler computes their sizes statically, and places the sizes \ninside the code. Scheme 48 creates fresh, .attened environments upon closure cre\u00adation. It maintains \nsharing of state by creating heap-allocated cells for shared mutable variables. A heap continuation \nobject consists of a copy of a stack continua\u00adtion frame, along with three added values: 1. the code \nof the continuation represented as a code vector, 2. an offset within the code vector, and 3. a descriptor \nfor the next continuation object.  Heap continuations occur upon call/cc or when the stack over\u00ad.ows. \nIn these cases, the VM walks down the stack and creates heap objects for the continuation frames, computing \ncode vector and offset along the way, and chaining the frames together. Among the various strategies \nfor implementing continuations in the presence of call/cc, the incremental stack/heap strategy compares \nquite favorably [5]. The compact stack frame representation used by Scheme 48 makes the implementation \nparticularly fast. 4.2 The Virtual Machine The VM is essentially a stack machine. The stack grows towards \nlower addresses and holds parameters for procedure calls as well as continuation frames and environment \nframes. The VM manages a number of registers in global variables: *code-pointer* points to the current \ninstruction *cont* points to the continuation frame on the stack just be\u00adneath the current one *bottom-of-stack* \npoints to the under.ow continuation *heap-continuation* is the part of the current continuation which \nresides in the heap *val* contains the .rst parameter or a return value The Scheme 48 VM represents \nall data objects as 32-bit descriptors. Its lower 2 bits indicate the type of the descriptor; the remaining \n30 bits are data which are either an immediate representation of a small value or an encoded pointer \ninto the heap. 5 Implementation This section describes our direct implementation of shift/reset. All \ncode examples are taken verbatim from our implementation atop the Scheme 48 1.0 Virtual Machine. Our \nimplementation builds straightforwardly upon the stack/heap strategy. However, a few technical complications \narise, and therefore our presentation is fairly detailed. We start with accounts of reset and shift for \nthe stack-bound case and then show how to extend the approach for continuations residing in the heap. \n5.1 Reset Reset delimits the section of the continuation that shift may cap\u00adture later. Reset evaluates \nits argument under the empty continua\u00adtion, and applies its current continuation to the result. Thus, \nreset effectively erects a barrier in the continuation frame chain. An easy way to erect the barrier \nis to set a .ag in the current contin\u00aduation frame, and ensure that subsequent calls to shift will only \nreify the part of the continuation chain up to the most recent mark. The compiler merges the continuations \nof differnet calls into a sin\u00adgle frame on the stack whenever possible. Therefore, reset has to make \nsure that the last continuation frame to be rei.ed is repre\u00adsented by a separate frame by creating a \nthunk and evaluating an unknown call to it. As a place for the mark, continuation objects receive a further \n.eld which is set to zero in normal continuations. A new VM operation mark-reset-contsets the mark in \nthe topmost frame of the current continuation. The mark-reset-cont operator is implemented by the add-reset-marker! \nprocedure: (define (add-reset-marker!) (if (address= *cont* *bottom-of-stack*) (if (not (= *heap-continuation* \nfalse)) (set-heap-continuation-marker! *heap-continuation* 1)) (set-stack-continuation-marker! *cont* \n1))) Add-reset-marker! checks whether the current continuation is an ordinary frame on the stack or \nif it is *bottom-of-stack* which means that the the current continuation resides in the heap. Due to \nthe different representation of stack and heap continuations, differ\u00adent procedures to mark the frames \nmust be used. Here is the de.nition of reset: (define-syntax reset (syntax-rules () ((reset body ...) \n(call-thunk (lambda () (mark-reset-cont) body ...)))))  5.2 Shift Shift must do two things: First, \nit must reify the section of the continuation up to the last reset into a heap object. Second, shift \nmust construct a procedure that composes the current continuation with the saved section and applies \nthe result to its argument. Shift must ensure that the context of the rei.ed continuation is itself de\u00adlimited. \nMaking this explicit corresponds to implementing shift via Felleisen s control [11] and is also known \nas the F-calcu\u00adlus [8]. Control is a macro which calls the procedure control* with the body wrapped in \nprocedure. Control* takes the one-argument pro\u00adcedure as its argument and passes it the composition procedure. \nCreate-stack-slice is responsible for turning the section of the continuation into a value. Copy-slice-to-stack \ncomposes the saved section with the current continuation: (define (control* proc) (let ((slice (create-stack-slice))) \n (proc (lambda (val) (copy-slice-to-stack slice) val)))) Control implements the (control ve) binding \nform which binds the continuation to v and evaluates e under that binding: (define-syntax control (syntax-rules \n() ((control c body) (control* (lambda (c) (reset body)))))) The macro for shift relies upon control \nand inserts the needed reset to get F-: (define-syntax shift (syntax-rules () ((shift c body) (control \ncc (let ((c (lambda (x) (reset (cc x))))) body)))))))  In our actual implementation we depart from the \nF-approach for ef\u00ad.ciency reasons: Properly supporting multiple-value returns would incur too much overhead \nconverting reifying multiple return values into lists and vice versa. We inline control into shift: (define-syntax \nshift (syntax-rules () ((shift c body) (shift* (lambda (c) body))))) This de.nition omits the reset \nbecause create-stack-slice will leave the marked continuation on the stack. Here is the de.nition of \nshift*: (define (shift* proc) (let* ((slice (create-stack-slice)) (c (lambda vs (copy-slice-to-stack \nslice) (apply values vs)))) (proc c)))  The reset omitted here is performed by copy-slice-to-stack. \n5.2.1 Rei.cation The following code implements the primitive create-stack\u00adslice within the VM: (define-primitive \ncreate-stack-slice () (lambda () (let ((v (copy-slice-to-heap))) (set! *val* v) (goto continue 0)))) \n It calls copy-slice-to-heap which returns the slice, loads the value register with it and jumps back \ninto the main interpreter loop via continue. Here is the .rst part of the relevant procedure, copy-slice-to\u00adheap: \n(define (copy-slice-to-heap) (ensure-*cont*-in-stack!) (receive (cont is-reset-cont?) (find-next-stack-reset-cont) \n (if (address= cont (integer->address 0)) (create-empty-slice) (if is-reset-cont? (create-slice-from-stack \ncont) (continued in next section) To avoid excessive special casing in the rest of the code, copy\u00adslice-to-heap \n.rst calls ensure-*cont*-in-stack to make sure there is at least one continuation frame on the stack. \nThen, it calls find-next-stack-reset-cont to obtain the last continu\u00adation frame to be rei.ed.1 If the \nsection of the continuation to be rei.ed is empty, copy-slice-to-heap simply creates a heap ob\u00adject representing \nan empty slice via the create-empty-slice pro\u00adcedure. Otherwise, the code determines whether the section \nto be rei.ed indeed resides entirely within the stack. For this section, the 1Receive is syntactic sugar \nfor call-with-values, as de\u00adscribed in SRFI 8 (http://srfi.schemers.org/srfi-8/). In this example, it \nbinds cont and is-reset-cont? to the return values of find-next-stack-reset-cont. The find-next-reset-cont \nprocedure walks down the continu\u00adation frames on the stack until it encounters a reset mark. It then \nreturns the parent frame or 0 if the current continuation is marked. If find-next-reset-cont does not \n.nd a marked continuation on the stack, it returns the last continuation frame on the stack (the par\u00adent \nof *bottom-of-stack*). The second return value indicates the case which occurred by a boolean .ag: (define \n(find-next-stack-reset-cont) (let lp ((cont *cont*) (prev (integer->address 0)) (i 0)) (if (address= \ncont *bottom-of-stack*) (values prev (not (heap-continuation-marker-zero? *heap-continuation*))) \n(if (not (= (stack-continuation-marker cont) 0)) (values prev #t)  (lp (stack-cont-continuation cont) \ncont (+ i 1)))))) Create-slice-from-stack performs the actual work by copying the stack slice represention \nthe section of the continuation to a heap object en bloc. Top ...  Top Reset  ... Reset Underflow \n ...  Figure 5. Representing continuation chains with offsets Scheme 48 uses relative offsets rather \nthan absolute addresses for the intra-stack references as shown in Figure 5. Offsets are repre\u00adsented \nby arrows with angles. The continuation reset in the heap has no parent in the copied slice. Stack slices \npresent a challenge to the garbage collector; the Scheme 48 GC ordinarily only handles heap objects consisting \nen\u00adtirely of descriptors or entirely of bitmap data; stack slices contain both. Accordingly, we have \nextended the garbage collector by a custom trace procedure. The .rst continuation frame of a stack slice \nis the root of the slice. Here is the code for create-slice-from-stack: (define (create-slice-from-stack \nreset-cont) (let* ((len (stack-slice-size reset-cont))  (key (ensure-space (needed-bytes-for-stack-slice \nlen))) (slice (make-stack-slice len key))) (copy-slice! slice len) (set! *cont* (stack-cont-continuation \nreset-cont)) slice)) Create-slice-from-stack allocates a slice object and calls copy-slice! to copy \nthe stack section into it. After this, it sets the current continuation to the parent of reset-cont, \nalso setting the reset mark. This corresponds to the deletion of the section of the continuation rei.ed \nby shift. Create-slice-from-stack re\u00adturns the created slice. Copy-slice! calls the Pre-Scheme primitive \ncopy-memory! which is translated into C s memcpy: (define (copy-slice! slice len) (let ((to-address \n(address-after-header slice)) (start *cont*)) (copy-memory! start to-address len)))  5.2.2 Re.ection \nCopy-slice-to-stack is comparatively simple: (define (copy-slice-to-stack! slice) (add-reset-marker!) \n (if (double-slice? slice) (install-double-slice! slice) (if (not (empty-slice? slice)) (set! *cont* \n(really-copy-slice-to-stack slice)))))  This .rst adds a reset marker to the current continuation frame \nas described in Section 5.2. The .rst conditional determines if the slice about to be re.ected will not \n.t within the stack cache; this exceptional case is described in the next section. For empty slices, \nnothing needs to be done. For normal slices, the work is relegated to the really-copy-slice-to-stack \nprocedure: (define (really-copy-slice-to-stack slice) (save-temp0! slice) (let* ((len (b-vector-length \nslice))) (ensure-stack-space! (bytes->cells len)) (let ((slice (recover-temp0!)) (new-cont (address\u00ad \n(address+ *stack* (cells->a-units (operands-on-stack))) (bytes->a-units len)))) (copy-args-above-incoming-cont! \nnew-cont (operands-on-stack)) (copy-slice-bytes slice new-cont) new-cont)))  Ensure-stack-space! \nensures there is enough place on the stack, .ushing the stack cache to the heap if necessary. If this \ntriggers a garbage collection, save-temp0! tells the collector that slice is live, and recover-temp0! \nrecovers it if the collector has moved the slice. Next, the code computes the target address for the \ntop\u00admost continuation frame of the slice: This is len bytes below the current top of the stack plus the \noperands that are currently lying on the stack. These operands must be moved from their current lo\u00adcation \nto the new top of the stack which is just above new-cont. Copy-args-above-incoming-cont! is responsible \nfor this. Now there is room on the stack, and the continuation current prior re.ec\u00adtion starts len bytes \naway from new-cont. Copy-slice-bytes then copies the actual data. The copy-slice-bytes computes the source \naddress and the number of bytes to be copied from the slice and simply calls copy-memory! to perform \nthe copying. (define (copy-slice-bytes slice to-start) (let* ((from-start (address-after-header slice)) \n(len (b-vector-length slice))) (copy-memory! from-start to-start len))) 5.3 Reifying Heap Continuation \nFrames The previous discussion assumed that the continuation section to be rei.ed resides entirely within \nthe stack. As described in Section 4.1, continuation frames are migrated to the heap upon call/cc or \nin the case of a stack over.ow. The direct implementation of shift must cope with two problems in this \nsituation: 1. The block-copy strategy is no longer suf.cient. Instead, the VM needs to to copy heap continuation \nframes sequentially into the slice. 2. The resulting slice may be larger than the stack cache which \nmeans a simple block copy is not enough to implement re.ec\u00adtion.2  The remaining code of the copy-slice-to-heap \nprocedure deals with these cases; it .rst searches for the reset continuation in the heap and then branches \non the condition of the second problem: (receive (reset-cont required-size-on-stack required-size-on-heap) \n (find-next-heap-reset-cont) (if (< (+ (current-stack-size) required-size-on-stack) ; approximation (maximum-stack-size)) \n (create-slice-in-two-steps cont reset-cont required-size-on-stack) (create-double-slice cont reset-cont \nrequired-size-on-heap))))))) Find-next-heap-reset-cont returns the continuation along with the amount \nof space required to copy these continuations to the stack or to the heap, respectively. Due to the different \nrepresen\u00adtations of continuations these numbers differ. Top  Reset ... ...  Heap1 Cont Underflow \n  . Figure 6. Creating a double slice If a part of the section resides in the heap but the entire section \nis not larger than the maximum size of the stack, create-slice-in\u00adtwo-steps creates a slice big enough \nto hold the chain and .lls it. If the section is larger than the stack, create-double-slice creates a \nspecial object (a double slice) that holds two slices: one for the stack, the other for the continuation \nframes that must go to the heap. Figure 6 shows the creation of a double slice. During re.ection of a \ndouble slice, only the .rst slice is copied on the stack, while a copy of the second is linked to the \nbottom frame. Note that copying the heap frames (and thus breaking sharing) is necessary to modify the \nlast continuation s parent during re.ection. The code for handling heap continuation frames is mostly \nas in the original Scheme 48. We have therefore omitted it here. 2Note that this issue does not occur \nwith call/cc: The under\u00ad.ow frame simply copies continuation frames residing in the heap into the stack \ncache one-by-one.   6 Benchmarks This section describes a number of benchmarks of the direct imple\u00admentation \nof shift/reset, notably: Filinski s representation of monads  combinator-based partial evaluation \n type-directed partial evaluation  We have also used shift/reset to implement a thread system. The \n.rst set of benchmarks shows that all of these applications exhibit absolute performance gains under \nthe direct implementation. The direct implementation of shift/reset is an enabling technology for implementing \nef.cient thread systems via shift/reset. It is dif.cult to precisely account for the speedup in each \nseparate case, as the indirect and direct implementations of shift/reset lead to very different access \npatterns to the stack and to the heap: The indirect implementation tends to .ush the stack cache of\u00adten, \nkeeping only the (non-meta-)continuation within the cache. Each .ush is accompanied by a representation \nchange. In con\u00adtrast, many rei.cations in the direct implementation do not .ush the stack cache, and \ndo not involve a representation change. The direct implementation potentially performs less sharing than \nthe in\u00addirect implementation. Moreover, speed-ups necessarily vary with the amount of computation performed \nbetween rei.cations and re\u00ad.ections of continuations. However, across the benchmarks, the direct implementation \nperforms uniformly better than the indirect one. Note that all performance gains were obtained under \na compara\u00adtively slow evaluator. These applications, when run under a native\u00adcode implementation of Scheme \nshould bene.t signi.cantly more, as there is almost no interpretive overhead in executing shift and reset. \nWe expect to obtain timings from the upcoming native-code version of Scheme 48 soon. All timings were \nobtained on a Pentium III system with 666 Mhz and 128 MB RAM, running FreeBSD 4.3. We used our modi.ed \nversion of a prerelease of Scheme 48 1.0 with an initial heap size of 80 MB and the standard stack size \nof 10 Kb; none of the tests triggered a garbage collection. 6.1 Monads The essence of Filinski s work \non representing monads [13] is the introduction of two combinators for conversion between value-level \nexpressions and meta-level computations. Reify uses the monadic constructor eta to turn a computation \ninto an expression. Eta uses reset to limit the extent of the computation. Its dual operation, reflect, \ncalls the monadic combination function extend to apply a computation (which corresponds to a section \nof the continuation obtained by shift) to an expression: (define (reflect meaning) (shift k (extend \nk meaning))) (define (reify thunk) (reset (eta (thunk)))))))  We have run applications using the parsing \nmonad and the ambiva\u00adlence monad. For benchmarking monadic parsing, we constructed a parser for arithmetic \nexpressions similar to Hutton s [18] and ap\u00adplied it to a term with about 450 operators. Monads 900000 \n.ned as:  Number of copy operations 600000 500000 400000 300000 200000 (define-syntax amb (syntax-rules \n() ((amb x ...) (amb* (lambda () x) ...)))) (define (amb* . t) (reflect (apply append (map reify t)))))) \n (define (www)  (let ((f (lambda (x) (+ x (amb 6 4 2 8) (amb 2 4 5 4 1))))) 100000 (reify (lambda \n() (f (f (amb 0 2 3 4 5 32))))))) (define (wwww) 0 (let ((f (lambda (x) (+ x (amb 6 4 2 8) (amb 2 4 5 \n4 1))))) Ambivalence monad: www Monadic parser Ambivalence monad: wwww (reify (lambda () (f (f (f (amb \n0 2 3 4 5 32)))))))) Figure 9. Copy operations for monads Monads  6.2 Combinator-Based Partial Evaluation \nPGG 5  4.5 4 3.5 3 Seconds Seconds 2.5 2 1.5 Ambivalence monad: www Monadic parser Ambivalence monad: \nwwww 1 Figure 7. Timings for monads (in seconds) 0.5 0 Figure 7 shows the timings for the monad examples. \nThe bars la\u00ad mini-scheme: app mini-scheme: boyer mixwell tiny mini-scheme: matrix mini-scheme: gpscheme \nbeled Indirect list the results for the call/cc-based implementa- tion of shift/reset, whereas the column \nlabeled Direct contains the timings for our direct implementation of shift/reset. The speedups for these \nvery shift/reset-intensive benchmarks are in the range of a factor of three. To see where the direct \nimplemen\u00adtation improves the performance consider Figure 8 which shows the number of bytes copied from \nthe stack to the heap and vice versa. Our direct implementation copies signi.cantly fewer bytes. Another \nimprovement shown in Figure 9 is the number of copy op\u00aderations. It drops by a factor of three to .ve \nfrom the indirect to the direct implementation, whereas the average number of bytes moved by a single \ncopy-memory! increases from 12 to 20 bytes. Monads  seconds) A partial evaluator takes a program as \ninput and specializes it with regard to some of the arguments of the program. Continuation\u00adbased approaches \nto partial evaluation use composable continua\u00adtions to represent dynamic context accumulated during specializa\u00adtion \n[21]. Thiemann s PGG system [27] uses a direct-style version of this transformation implemented via shift/reset. \nWe have used benchmarks from Helsen s and Thiemann s paper on comparing combinator-based and type-directed \npartial evalua\u00adtion [16]. These benchmarks are interpreters; partial evaluation yields compiled versions \nof the input programs. These are the pro\u00adgramming languages accepted by the interpreters:  Mixwell is \na .rst-order purely functional language. Tiny is a small imperative language with expressions, assignment, \nif, while and sequencing. Mini-Scheme is a large subset of Scheme, lacking only call/cc and support \nfor multiple return values. We used a standard test program for the Mixwell interpreter, and the Copied \nKbytes factorial procedure for the Tiny interpreter. The Mini-Scheme inter\u00adpreter was specialized to \nseveral programs from Andrew Wright s Monadic parser benchmark suite for Scheme: app, the append procedure, \nboyer, a term-rewriting system, matrix that tests a random matrix to be Figure 8. Continuation frames \ncopied for monads (in Kbytes) maximal under permutation and negation of rows and columns, and gpscheme, \nan implementation of genetic programming. PGG Type directed partial evaluation 8  Copied Kbytes Seconds \n4 3 2 1 0 0 mini-scheme: boyer mixwell tinymini-scheme: boyer mixwell tiny mini-scheme: matrix mini-scheme: \ngpschememini-scheme: matrix mini-scheme: gpscheme  onds) partial evaluation (in Kbytes) PGG Type directed \npartial evaluation  2500 2000 120000 100000 80000 Number of copy operations Copied Kbytes 1500 1000 \n500 0 0 mini-scheme: app mini-scheme: boyer mixwell tiny mini-scheme: matrix mini-scheme: gpscheme Figure \n10 contains the specialization timings. Here, the speedup is quite uniformly between 10% and 20%. Again, \nFigure 11 shows the summarized size of the moved continuation frames. It is interest\u00ading to note that \nindirect approach copies exactly the same amount of memory from the stack as vice versa whereas our direct \nimple\u00admentation moves more memory from the heap to the stack. Fig\u00adure 12 shows the number of memory copy \noperations during the benchmarks. Our direct approach typically cuts down the number of operations to \na third. 6.3 Type-Directed Partial Evaluation Danvy s type-directed partial evaluation (TDPE) [6] is \nan alterna\u00adtive approach to partial evaluation which operates on the compiled version of the subject \nprogram. TDPE also uses shift/reset to capture context. We have applied TDPE to the same examples as \nPGG. Figure 13 shows the result. The speedups obtained were sim\u00adilar to those obtained with PGG. Figure \n14 shows a peculiarity: For the specialization of the Mixwell interpreter, our implementation copies \nan order of magnitude more than the implementation based on call/cc but is still faster. The speedup \ncan be explained by the number of copying operations: Figure 15 shows that this number drop by a factor \nof three. This also dramatically increases the bytes per copy operation from 12 in the indirect implementation \nto 354 in our implementation. mini-scheme: app mini-scheme: boyer mixwell tiny mini-scheme: matrix mini-scheme: \ngpscheme 6.4 Threads The use of .rst-class continuations for ef.cient implementations of thread systems \nis not new: On a context switch, the system rei\u00ad.es the continuation of the running thread and stores \nit within the scheduler. Next, the scheduler replaces the current continuation by the stored continuation \nof another thread, thus invoking that thread. However, call/cc rei.es not just the continuation of the \nthread but also the continuation of the scheduler. This also can lead to a space leak [2]. In a thread \nsystem using shift and reset, it is possible to delimit the continuation of the thread with a reset. \nTo measure the effects of this approach, we have modi.ed the thread system of Scheme 48 to use shift/reset. \nAs the continua\u00adtion of a thread may receive multiple values it was necessary to ex\u00adtend the implementation. \nInstead of using variable-arity procedures in the implementation described in Section 5, we built directly \non the slice-copying operations provided by the VM just as the stock Scheme 48 system uses VM-level continuation \nprimitives. If the user has access to shift/reset,a reset would prevent the thread system s shift from \n.nding the reset mark delimiting the continuation of the thread [2]. This could easily be remedied in \nour implementation by changing the reset mark to be the thread uid of the thread which set the mark. \nAs the scheduler is a thread by itself, it does not interfere with the other threads. Type directed partial \nevaluation Threads 80000 70000  Number of copy operations 40000 30000 20000 Copied bytes in Kbytes \n10000 0 0 mini-scheme: app mini-scheme: boyer mixwell tiny 1 2 4 8 16 32 64 128 256 mini-scheme: matrix \nmini-scheme: gpscheme mark (in Kbytes) Threads 12 Threads 10 8  1e+06 1e+07  log (Copy operations) \nSeconds 100000 10000 2 0 1 2 4 8 16 32 64128256 Figure 16. Timings for the thread benchmark (in seconds) \nFor the actual measurements we adopt the benchmarks by Brugge\u00adman et al. who use one-shot continuations \nfor implementing threads [3]. Each thread computes the 20th Fibonacci number. We vary the number of procedure \ncalls between two context switches for a .xed number of 100 threads. Figure 16 shows the resulting timings. \nJust like Bruggemann et. al. we observe a speedup only for very frequent context switches whereas the \ntimings are equal when the context switches happen at a lower rate than every 64 pro\u00adcedure calls. Figure \n17 shows that the number of copied bytes is much larger in the shift/reset case than in the call/cc imple\u00admentation. \nThe number of copy operations, however, is linear to the number of context switches in the shift/reset \ncase as Figure 18 reveals. This .gure also shows that the standard call/cc approach cannot hold this \nlimit when context switches become less frequent as the running thread is more likely to return to a \ncontinuation on the heap the longer it runs. As the context-switch rate decreases, the pure execution \ntime of the threads dominates, yielding similar per\u00adformance with both approaches. These observations \ncoincide with those of Bruggeman at al. 6.5 Optimizing *Abort As mentioned in Section 2, the indirect \nimplementation of shift/reset in Scheme 48 uses a version of the *abort primi\u00adtive that discards its \nentire continuation before calling the meta\u00adcontinuation. This enables the garbage collector to reclaim \nmore unreachable continuation frames. However, this optimization does not affect the run-time performance \nof the benchmarks: The dead frames just reside in the heap, and the program never touches them. 1000 \n Figure 18. Number of copy operations for the thread bench\u00admark 7 Other Implementation Strategies The \nincremental stack/heap strategy employed by Scheme 48 is not the only common implementation strategy \nfor continuations in the presence of control operators. The main attractive alternative implementations \nare the gc strategy, the Hieb-Dybvig-Bruggeman strategy, and the stack/heap strategy, along with the \nsimple-to\u00adimplement stack strategy [5]. This section brie.y reviews how the results obtained for this \npaper carry over to these alternatives. 7.1 The GC Strategy Implementations using the gc strategy keep \nall continuations in the heap, relying on a fast garbage collector and compile-time opti\u00admizations for \na fast implementation. The strategy is not a zero\u00adoverhead strategy in the sense of Clinger et al. [5], \nmeaning that it incurs overhead for all programs over stack-based strategies. In the gc strategy, call/cc \nas well as re.ecting a continuation are especially fast and incur only small constant run time. In particu\u00adlar, \ncall/cc involves no copying. In contrast, a direct implementa\u00adtion of shift/resetwould need to copy continuation \nframes corre\u00adsponding to a composable continuation, which likely makes a direct implementation more expensive \nin terms of run time than the indi\u00adrect implementation. The only potential savings are space savings \ndue to not retaining dead meta-continuations (see Section 2). 7.2 The Hieb-Dybvig-Bruggeman Strategy \nThe Hieb-Dybvig-Bruggeman strategy [17] represents continua\u00adtions as linked lists of stack segments. \nWhen the current stack seg\u00adment over.ows, the system simply allocates a fresh one, linking it to the \nprevious segment. Call/cc creates a small object pointing to the current frame within a stack segment. \nIt then splits the seg\u00adment by installing an under.ow continuation frame, similar to the incremental \nstack/heap strategy. Re.ecting the continuation also works as in the incremental stack/heap strategy, \nby copying a por\u00adtion of the captured continuation into the current stack segment. As in the gc strategy, \ncall/cc itself is a constant-time operation. On the other hand, re.ecting a rei.ed continuation involves \noverhead similar to the incremental stack/heap strategy. A direct implementation of shift/reset would \ncreate a stack seg\u00adment corresponding to the rei.ed composable continuation, termi\u00adnating the link at \nthe bottom. The system would re.ect the contin\u00aduation by making a copy of the stack segment and setting \nits link pointer. It is hard to assess the tradeoffs of a direct implementation of shift/reset in this \ncase; this warrants further research. 7.3 The Stack/Heap Strategy The stack/heap strategy differs from \nthe incremental stack/heap strategy in that it can return to a continuation residing in the heap directly, \ninstead of .rst copying portions back into the stack cache. This means that a continuation resides either \nentirely within the stack or entirely within the heap. The stack/heap strategy is also not a zero-overhead \nstrategy because each return needs to check where the continuation resides. A direct implementation of \nshift/reset in this context would be similar to the one presented here, with sim\u00adilar performance tradeoffs \nas for the implementation of call/cc. The stack/heap strategy incurs an additional overhead for procedure \ncall returns, as it needs to check whether the current continuation resides in the stack or in the heap. \nHence, it is not a zero-overhead strategy. The stack/heap strategy seems comparatively rare in con\u00adtemporary \nprogramming language implementations. 7.4 The Stack Strategy In implementations of programming languages \nwhich do not sup\u00adport .rst-class continuations, the most common strategy is the stack strategy that keeps \nall continuation frames on a global stack. The stack strategy is even used in some systems with call/cc \n(MzScheme, Bigloo and scm, for example.) This strategy effec\u00adtively discourages the use of call/cc. However, \na direct imple\u00admentation of shift/reset for the stack strategy is quite simple as it can use a contiguous \nrepresentation for rei.ed continuations. In particular, it does not have to deal with the stack-over.ow \nsitua\u00adtion. Hence, we expect the relative performance bene.ts of a direct implementation with this strategy \nto be signi.cantly greater than with strategies allowing an ef.cient implementation of call/cc. This \nin turn might make shift/reset attractive for programming languages which do not support call/cc at all, \nsuch as Objective Caml. 8 Related Work Felleisen et al. originally came up with ideas for control operators \nfor composable continuations [12, 11]. Danvy and Filinski discov\u00adered shift and reset in the course of \ntheir investigation of the CPS transformation [8]. The seminal work on the CPS transforma\u00adtion and on \nshift/reset is Danvy s and Filinski s 1992 paper [9]. Filinski shows how to implement shift/reset via \ncall/cc and a mutable reference [13]. Danvy and Filinski note that shift/reset coincides with Sitaram \ns and Felleisen s F-operationally [24, 8]. Moreau and Queinnec also employ marks on the stack to de.ne \nmarker/call/pc, another pair of control operators for compos\u00adable continuations [23]. Gunter, R\u00b4 emy, \nand Riecke investigate an\u00adother alternative approach to composable continuations via named prompts [15]. \nThey also mention the possibility of a direct imple\u00admentation. To our knowledge, this has not been pursued \nto date. Clinger, Hartheimer and Ost offer a comprehensive account of ef.\u00adcient implementation strategies \nfor .rst-class continuations [5] and present detailed comparative measurements. Danvy formalizes two \nstack-based implementation strategies for .rst-class continuations by constructing special abstract machines \nwhich are proven equiv\u00adalent to a standard abstract machine for CPS programs [7]. The performance problems \nof indirect implementations of shift/reset have been noted for a while [1]. In particular, im\u00adplementors \nof partial evaluators have been trying to replace the use of shift/reset for performance reasons this \nis possible for some but not all applications of shift/reset in that context [25]. Oper\u00adating systems \nresearch also has a long history of trying to over\u00adcome the inef.ciencies stemming from capturing and \nreinstating complete continuations. Draves allows kernel functions to spec\u00adify a composable continuation \nexplicitly for a context switch in\u00adstead of forcing the kernel substrate to capture the complete con\u00adtinuation \n[10]. This corresponds to using shift/reset instead of call/cc for context switching. 9 Conclusion Shift \nand reset have long gained a critical mass of applications to warrant .rst-class support in modern functional \nprogramming languages. However, research on the pragmatics of supplying and ef.ciently implementing shift/reset \nhas only just begun. Our work is a .rst indicator that direct implementations of shift/reset as opposed \nto indirect ones using call/cc are indeed worthwhile: the ef.ciency gains for applications of shift/reset \nare signi.cant. However, more research is needed, in particular into the pragmat\u00adics of having call/cc, \nshift/reset, and threads in the same sys\u00adtem, as well as in alternative implementation strategies, possibly \nin\u00advolving more advanced garbage-collection technology or exploiting representations derived from extended \ncontinuation-passing style. Another area for future work is relating our direct implementation formally \nto the semantic speci.cation of shift/reset. Danvy s work [7] should provide a good starting point for \nthis. Acknowledgments We thank Simon Helsen and Peter Thiemann for providing us with their benchmark \nsuite. Richard Kelsey always kept us supplied with the latest development releases of Scheme 48. Olivier \nDanvy pro\u00advided us with helpful discussions about shift/reset as well as excellent suggestions on how \nto improve our presentation. We also thank Matthias Felleisen for valuable insights and pointers. [1] \nVincent Balat and Olivier Danvy. Strong normalization by type-directed partial evaluation and run-time \ncode generation. In Proceedings of the ACM SIGPLAN Workshop on Types in Compilation (TIC 98), number \n1473 in Lecture Notes in Computer Science, Kyoto, Japan, March 1998. [2] Edoardo Biagioni, Ken Cline, \nPeter Lee, Chris Okasaki, and Chris Stone. Safe-for-space threads in Standard ML. Higher-Order and Symbolic \nComputation, 11(2):209 225, December 1998. [3] Carl Bruggeman, Oscar Waddell, and R. Kent Dybvig. Rep\u00adresenting \ncontrol in the presence of one-shot continuations. In Proc. of the ACM SIGPLAN 96 Conference on Program\u00adming \nLanguage Design and Implementation, pages 99 107, Philadelphia, PA, USA, May 1996. ACM Press. [4] William \nD. Clinger, Dan P. Friedman, and Mitchell Wand. A scheme for a higher-level semantic algebra. In John \nReynolds and Maurice Nivat, editors, Algebraic Methods in Semantics: Proceedings of the US-French Seminar \non the Application of Algebra to Language De.nition and Compilation, pages 237 250, Cambridge, 1985. \nCambridge University Press. [5] William D. Clinger, Anne Hartheimer, and Eric Ost. Im\u00adplementation strategies \nfor .rst-class continuations. Higher-Order and Symbolic Computation, 1(12):7 45, April 1999. [6] Olivier \nDanvy. Type-directed partial evaluation. In John Hatcliff, Torben \u00c6. Mogensen, and Peter Thiemann, ed\u00aditors, \nPartial Evaluation Practice and Theory. Proceed\u00adings of the 1998 DIKU International Summerschool, number \n1706 in Lecture Notes in Computer Science, pages 367 411. Springer-Verlag, Copenhagen, Denmark, 1999. \n[7] Olivier Danvy. Formalizing implementation strategies for .rst-class continuations. In Gert Smolka, \neditor, Proc. 9th Eu\u00adropean Symposium on Programming, number 1782 in Lecture Notes in Computer Science, \npages 88 103, Berlin, Germany, March 2000. Springer-Verlag. [8] Olivier Danvy and Andrzej Filinski. Abstracting \ncontrol. In Proceedings of the 1990 ACM Conference on Lisp and Func\u00adtional Programming, pages 151 160, \nNice, France, 1990. ACM Press. [9] Olivier Danvy and Andrzej Filinski. Representing control: A study \nof the CPS transformation. Mathematical Structures in Computer Science, 2:361 391, 1992. [10] Richard \nP. Draves. Control Transfer in Operating System Ker\u00adnels. PhD thesis, School of Computer Science, Carnegie \nMel\u00adlon University, Pittsburgh, PA 15213, May 1994. CMU-CS\u00ad94-142. [11] Matthias Felleisen. The theory \nand practice of .rst-class prompts. In Proc. 15th Annual ACM Symposium on Princi\u00adples of Programming \nLanguages, pages 180 190, San Diego, California, January 1988. ACM Press. [12] Matthias Felleisen, Dan \nP. Friedman, Bruce Duba, and J. Mer\u00adrill. Beyond continuations. Technical Report 216, Computer Science \nDepartment, Indiana University, Bloomington, Indi\u00adana, 1987. [13] Andrzej Filinski. Representing monads. \nIn Proceedings of the 1994 ACM SIGPLAN Symposium on Principles of Pro\u00adgramming Languages, pages 446 457, \nPortland, OR, January 1994. ACM Press. [14] Andrzej Filinski. Representing layered monads. In Alexander \nAiken, editor, Proceedings of the 1999 ACM SIGPLAN Sym\u00adposium on Principles of Programming Languages, \npages 175 188, San Antonio, Texas, USA, January 1999. ACM Press. [15] Carl Gunter, Didier R\u00b4emy, and \nJohn Riecke. A generalization of exceptions and control in ML-like languages. In Simon Peyton Jones, \neditor, Proc. Functional Programming Lan\u00adguages and Computer Architecture 1995, pages 12 23, La Jolla, \nCA, June 1995. ACM Press, New York. [16] Simon Helsen and Peter Thiemann. Two .avors of of.ine partial \nevaluation. In J. Hsiang and A. Ohori, editors, Ad\u00advances in Computing Science -ASIAN 98, number 1538 \nin Lecture Notes in Computer Science, pages 188 205, Manila, The Philippines, December 1998. [17] Robert \nHieb, R. Kent Dybvig, and Carl Bruggeman. Rep\u00adresenting control in the presence of .rst-class continuations. \nIn Proc. Conference on Programming Language Design and Implementation 90, pages 66 77, White Plains, \nNew York, USA, June 1990. ACM. [18] Graham Hutton and Erik Meijer. Monadic parsing in Haskell. Journal \nof Functional Programming, 8(4), 1998. [19] Richard A. Kelsey. Pre-Scheme: A Scheme dialect for sys\u00adtems \nprogramming. June 1997. [20] Richard A. Kelsey and Jonathan A. Rees. A tractable Scheme implementation. \nLisp and Symbolic Computation, 7(4):315 335, 1995. [21] Julia Lawall and Olivier Danvy. Continuation-based \npartial evaluation. In Proceedings of the 1994 ACM Conference on Lisp and Functional Programming, pages \n227 238, Orlando, Florida, USA, June 1994. ACM Press. [22] Julia Lawall and Olivier Danvy. Continuation-based \npar\u00adtial evaluation. Technical Report Technical Report CS-95\u00ad178, Brandeis University, Waltham, Massachusetts, \n1995. Extended version of [21] from ftp://ftp.brics.dk/pub/ danvy/Papers/lawall-danvy-lfp94-extended.ps.gz. \n[23] Luc Moreau and Christian Queinnec. Partial continuations as the difference of continuations a duumvirate \nof control operators. In Manuel V. Hermenegildo and Jaan Penjam, ed\u00aditors, International Symposium on \nProgramming Languages, Implementations, Logics and Programs (PLILP 94), number 844 in Lecture Notes in \nComputer Science, Madrid, Spain, September 1994. Springer-Verlag. [24] Dorai Sitaram and Matthias Felleisen. \nControl delimiters and their hierarchies. Lisp and Symbolic Computation, 3(1):67 99, January 1990. [25] \nEijiro Sumii and Naoki Kobayashi. A hybrid approach to on\u00adline and of.ine partial evaluation. Higher-Order \nand Symbolic Computation, 14(2/3):101 142, 2001. [26] Peter Thiemann. Combinators for program generation. \nJour\u00adnal of Functional Programming, 9(5):483 525, September 1999. [27] Peter Thiemann. The PGG System \nUser Manual. Uni\u00adversit\u00a8Avail\u00ad at Freiburg, Freiburg, Germany, March 2000. able from http://www.informatik.uni-freiburg.de/ \nproglang/software/pgg/.    \n\t\t\t", "proc_id": "581478", "abstract": "We present a direct implementation of the <i>shift</i> and <i>reset</i> control operators in the <i>SFE</i> system. The new implementation improves upon the traditional technique of simulating <i>shift</i> and <i>reset</i> via <i>callcc</i>. Typical applications of these operators exhibit space savings and a significant overall performance gain. Our technique is based upon the popular incremental stack/heap strategy for representing continuations. We present implementation details as well as some benchmark measurements for typical applications.", "authors": [{"name": "Martin Gasbichler", "author_profile_id": "81100649955", "affiliation": "Universit&#228;t T&#252;bingen", "person_id": "PP14222682", "email_address": "", "orcid_id": ""}, {"name": "Michael Sperber", "author_profile_id": "81100100127", "affiliation": "Universit&#228;t T&#252;bingen", "person_id": "PP14044834", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/581478.581504", "year": "2002", "article_id": "581504", "conference": "ICFP", "title": "Final shift for call/cc:: direct implementation of shift and reset", "url": "http://dl.acm.org/citation.cfm?id=581504"}