{"article_publication_date": "09-17-2002", "fulltext": "\n Bootstrapping One-sided Flexible Arrays Ralf Hinze Institut f\u00a8ur Informatik III Universit\u00a8 at Bonn R\u00a8 \nomerstra\u00dfe 164, 53117 Bonn, Germany ralf@informatik.uni-bonn.de Abstract 1 Introduction The abstract \ndata type one-sided .exible array, also called random-One-sided .exible arrays, also called random-access \nlists, are a hy\u00adaccess list, supports look-up and update of elements and can grow brid of arrays and \nlists: they support array-like operations such and shrink at one end. We describe a purely functional \nimplemen-as look-up and update of elements and list-like operations such as tation based on weight-balanced \nmultiway trees that is both simple cons, head, and tail. Flexible arrays are typically used when ef.\u00adand \nversatile. A novel feature of the representation is that the run-cient random access is required and \nthe arrays are used in a non\u00adning time of the operations can be tailored to one s needs even single threaded \nmanner or when the size of the arrays varies dy\u00addynamically at array-creation time. In particular, one \ncan trade namically. A variety of implementations is available. Braun trees the running time of look-up \noperations for the running time of up-[11], for instance, support all operations in T(log n)time. An \nunri\u00addate operations. For instance, if the multiway trees have a .xed valled data structure is the skew \nbinary random-access list [15, 16], degree, the operations take T(log n)time, where n is the size of \nwhich provides logarithmic array operations and constant time list the .exible array. If the degree doubles \nlevelwise, look-up speeds operations. e up to T(log n)while update slows down to T(2logn). We show \nthat different tree shapes can be conveniently modelled after mixed-A common characteristic of the tree-based \nimplementations is the radix number systems. logarithmic time bound for the look-up operation. By contrast, \npure functional arrays as to be found in the Haskell standard library [19] support constant-time look-up. \nOn the negative side, updating a Categories and Subject Descriptors real array is prohibitively expensive \nas it takes time linear in the size of the array. This paper describes an alternative, purely func\u00ad D.1.1 \n[Programming Techniques]: Applicative (Functional) Pro-tional data structure that mediates between the \ntwo extremes. gramming; D.3.2 [Programming Languages]: Language Clas\u00adsi.cations applicative (functional) \nlanguages; D.3.3 [Program-The data structure itself is quite simple: we employ multiway trees ming Languages]: \nLanguage Constructs and Features abstract where each node consists of an array of elements and an array \nof data types; E.1 [Data]: Data Structures arrays, trees; I.1.2 subtrees. Thus, our implementation is \nbootstrapped from an exist\u00ad[Computing Methodologies]: Algorithms analysis of algo-ing implementation \nof arrays. The base array type can be chosen rithms at will: it may be a real array, an array of bounded \nsize, a Braun tree, or even an ordinary list. Of course, different choices result in different running \ntimes of the bootstrapped operations.  General Terms The performance is furthermore in.uenced by the \nsize of the nodes. Algorithms, design, performance Consider bootstrapping from an array with constant \ntime access and linear time update. If the nodes have a .xed degree, then the opera\u00adtions take T(log \nn)time. However, if the degree doubles levelwise, e Keywords then look-up speeds up to T(log n)while \nupdate slows down to log n). T(2A pleasant feature of our implementation is that the Purely functional \ndata structures, .exible arrays, sub-logarithmic structure of a multiway tree is only determined when \nan array is look-up, mixed-radix number systems, Haskell .rst created. Much like an egg cell the initial \narray incorporates the blue print for its future development. We show that a large class of tree shapes \ncan be conveniently modelled after so-called mixed\u00adradix number systems. By choosing an appropriate number \nsystem bootstrapped arrays can be tuned for single-threaded or persistent use, for monotonic (arrays \nmay only grow) or non-monotonic use. Permission to make digital or hard copies of all or part of this \nwork for personal or The rest of the paper is structured as follows. Sec. 2 introduces the classroom \nuse is granted without fee provided that copies are not made or distributed abstract data type of one-sided \n.exible arrays. Sec. 3 de.nes multi\u00ad for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nway trees and describes the implementation of the basic operations. to lists, requires prior speci.c \npermission and/or a fee. Sec. 4 deals with the creation of arrays and determines the perfor- ICFP 02, \nOctober 4-6, 2002, Pittsburgh, Pennsylvania, USA. mance of bootstrapped arrays for different choices \nof tree shapes. Copyright 2002 ACM 1-58113-487-8/02/0010 ...$5.00 in.xl 9! class Array a where --array-like \noperations (!):: ax -Int -x update :: (x -x)-Int -ax -ax --list-like operations empty :: ax -Bool size \n:: ax -Int nil :: ax copy :: Int -x -ax cons :: x -ax -ax head :: ax -x tail :: ax -ax --mapping functions \nmap :: (x -y)-(ax -ay) zip :: (x -y -z)-(ax -ay -az) --conversion functions list :: ax -[x] array :: \n[x]-ax Figure 1. Signature of one-sided .exible arrays. Sec. 5 shows how to implement additional operations \nsuch as con\u00adverting from and to lists. Sec. 6 presents preliminary measurements (micro-benchmarks) comparing \nmultiway trees to other implemen\u00adtations of .exible arrays. Finally, Sec. 7 reviews related work and \nSec. 8 concludes.  2 One-sided .exible arrays Fig. 1 lists the signature of one-sided .exible arrays, \nphrased as a Haskell type class. The operations can be roughly divided into four categories: array-like \noperations ( ! and update), list-like op\u00aderations (empty, size, nil, copy, cons, head, and tail), mapping \nfunc\u00adtions (map and zip), and conversion functions (list and array). Most of the operations should be \nself-explanatory, so we content our\u00adselves with describing the less common ones. The array-operation \nupdate applies its .rst argument to the array element at the given position and returns the modi.ed array. \nThe function copy nx cre\u00adates an array of size n that contains n copies of x. App. A contains a reference \nimplementation using lists. Since we use map and zip quite heavily in the sequel, we adopt the following \nnotational convenience: we write both map f and zip f simply as f*(unless there is danger of confusion). \n 3 Multiway trees The data type of multiway trees is parameterized by the type of the base array a and \nby the element type x. data Treeax =(ax,a (Tree a x)) A node (xs,ts)is a pair consisting of an array \nxs of elements, called the pre.x, and an array ts of subtrees. In what follows we show how to turn Tree \na into an instance of Array provided that a is already an instance. instance (Array a)=Array (Tree a)where \nWe require the multiway trees to satisfy a number of invariants. But, rather than stating the conditions \nfrom the outset, we will introduce them as we go along. 3.1 List-like operations Let us start with the \nlist-like operations since they will determine the way indexing is done. The idea for consing elements \nto an array is as follows: .rst .ll up the element array in the root node until it contains as many elements \nas there are subtrees. Then, if the root is full up, distribute the elements evenly among the subtrees \nand start afresh. cons x (xs,ts) Isize xs <size ts =(cons x xs,ts) Iotherwise =(cons x nil,cons*xs ts) \nTo make this algorithm work, we have to maintain the following invariant. The number of elements must \nnot exceed the number of subtrees. Furthermore, each node must contain at least one subtree so that the \nsecond line of cons creates a legal node. Clearly, this invariant requires non-strict evaluation as the \ntrees are inherently in.nite. We will come back to this aspect when we study the creation of arrays in \nSec. 4. For the moment, just note that cons never changes the number of subtrees. The cons operation \nmaintains a second invariant: since the elements are distributed evenly among the subtrees, each subtree \ncontains the same number of elements (denoted ItI). In other words, the multiway trees are perfectly \nweight-balanced. Invariant 1 implies that the array of subtrees is never empty. This raises the question, \nhow we can effectively check whether a given tree is empty. We simply agree upon that a tree is empty \nif and only if the pre.x is empty. This gives a particularly simple implementation of empty. empty (xs,ts)=empty \nxs The de.nition of size builds upon all three invariants. size (xs,ts) Iempty xs =0 Iotherwise =size \nxs +size ts *size (head ts) Note that size takes time linear in the height of the tree. Since a non-empty \ntree has a non-empty pre.x, accessing the .rst element of an array is straightforward. head (xs,ts) Iempty \nxs =error \"head: empty array\" Iotherwise =head xs  s 4 Array creation __ The shape of multiway trees \nand consequently the running time of the operations is solely determined by the array creation functions \nnil, copy, and array. Conceptually, we may think of an initial array as an in.nite tree, whose branching \nstructure is .xed and which will be populated through repeated applications of the cons function. An \nq initial array is very much like an egg cell in that it incorporates the blueprint for its future development. \n))))) )))) )  . . . . . . . . . . . . . . . . . . . . .  That said, it becomes clear that there is \nno single collection of con\u00ad _ _ b Figure 2. Indexing multiway trees. The tail operation is essentially \nthe inverse of cons. tail (xs,ts)Iempty xs =error \"tail: empty array\" Isize xs >1 =(tail xs,ts)Iempty \n(head ts)=(nil,ts)Iotherwise =(head*ts,tail*ts) Note that we have to be careful to return an empty array \nif the argu\u00adment array has size 1 (third equation). 3.2 Array-like operations The list-like operations \nhave been carefully crafted so that the array operations can be implemented ef.ciently. To see how indexing \nworks consider the evolution of an initially empty array: After the .rst over.ow, the r-th subtree contains \na single element, namely the one at position s +r, where s is the size of the pre.x. After the second \nover.ow, it contains two elements, the ones at positions s +r and s +b +r, where b is the total number \nof subtrees. In structor functions. Rather, each possible tree shape gives rise to one collection. (Of \ncourse, for the instance declaration we have to commit ourselves to one particular, yet arbitrary collection \nof ar\u00adray creation functions.) Through suitable de.nitions the user can adjust the running time of the \narray operations to her needs even dynamically at array-creation time. As an aside, note that we do not \nemploy lazy evaluation in an es\u00adsential way it is more a matter of convenience. The proposed data structure \ncould be easily implemented in a strict language just by adding a suitable constructor for empty nodes. \nOf course, the empty constructor has to incorporate information about the future branching structure. \nTo be able to analyze the running times reasonably well, we make one further assumption: we require that \nnodes of the same level have the same size. This is quite a reasonable requirement if the elements are \naccessed with equal probability. If the access char\u00adacteristic is different, a less regular layout may \nbe advantageous. Given this assumption the structure of trees can be described using a special number \nsystem, the so-called mixed-radix system [13]. A mixed-radix numeral is given by a sequence of digits \nd0, d1, d2, . . . (determining the size of the element arrays) and a sequence of bases b0, b1, b2, . \n. . (determining the size of the subtree arrays).  general, after q over.ows it comprises the q elements \nat positions d0,d1,d2,... =. di *wi where wi =bi-1 **b1 *b0 s +0 *b +r, s +1 *b +r, ..., s +(q -1)*b \n+r. Fig. 2 illustrates b0,b1,b2,... i=0 the situation. The .rst row is the pre.x of the array; each column \nof the matrix below corresponds to one subtree. Reading from left to right and from top to bottom we \nobtain the elements of the array In our case, the bases are positive numbers 1 bi and we require the \ndigits to lie in the range 0 di bi (cf Invariant 1). Furthermore, ordered by index. we require di =0 \n==di+1 =0 (cf Invariant 3). In other words, if we ignore trailing zeros, then the digits must lie in \nthe range 1 To determine the location of the i-th element in a given multiway di bi. Perhaps surprisingly, \neach natural number has a unique tree, we .rst check whether the element is contained in the pre.x. If \nrepresentation for a .xed sequence of bases. Mixed-radix numerals this is not the case, then the remainder \nmod (i -s)b determines the satisfy the appealing recursion equation subtree where the element is to be \nfound and the quotient div (i \u00ads)b determines the position within the subtree.   d0,d1,d2,... d1,d2,... \n=d0 +b0 *, b0,b1,b2,...b1,b2,... (xs,ts)! i Iempty xs = error \"(!): index out of range\" Ii <size xs = \nxs ! i Iotherwise = (ts ! r)! q where (q,r) = divMod (i -size xs)(size ts) which corresponds nicely \nto the de.nition of size. Likewise, cons corresponds to incrementing a mixed-radix numeral and tail to \ndecrementing one. In Haskell, we can represent mixed-radix nu\u00ad merals by a list of pairs: Note that different \noccurrences of ! refer to different instances of the overloaded operation: in (ts ! r)! q the .rst occurrence \noperates on the base array while the second constitutes the recursive call. The update operation is implemented \nanalogously. update f i (xs,ts)Iempty xs =error \"update: index out of range\" Ii <size xs =(update f i \nxs,ts)Iotherwise =(xs,update (update f q)rts) where (q,r)=divMod (i -size xs)(size ts) type Mix =[(Int,Int)]. \nGiven a list of bases we can quite easily convert a natural number into a mixed-radix number. type Bases \n=[Int] encode :: Bases -(Int -Mix) encode (b : bs)n In 0 =zip (repeat 0)(b : bs) Iotherwise =(r +1,b): \nencode bs q where (q,r)=divMod (n -1)b   From a list of bases we can also construct an empty array. \ngnil :: (Array a)=Bases -Tree a x gnil (b : bs)= (nil,copy b (gnil bs)) The function gnil can be seen \nas a generic array creation function that is indexed by a mixed-radix number system. In a similar vein \nwe can de.ne a generic copy function that creates an array of a certain size. gcopy :: (Array a)=Mix \n-x -Tree a x gcopy ((d,b): s)x =(copyd x,copy b (gcopy s x)) The array creation functions for a .xed \nsequence of bases bs are then given by nil =gnil bs and copy n x =gcopy (encode bs n)x (the creation \nfunction array will be dealt with in Sec. 5.3). Before we look at particular examples of mixed-radix \nsystems, let us .rst study the running time of look-up and update operations in the general setting. \nTo simplify the analysis, we pretend to work in a strict setting. Furthermore, we assume that the arrays \nare used in a single-threaded manner (Sec. 4.5 deals with persistent use). The dominant factor of array \nlook-up is the height of the multiway tree. Let H(n)be the height of the tallest tree with size n (counting \nonly non-empty nodes). The running time of ! and update is H{n)-1 \u00af T!(n)= . T!(bi) i=0 H{n)-1 \u00af = Tupdate(n). \nTupdate(bi), i=0 \u00af where Top is the running time of op on base arrays. Actually, both operations are \nslightly faster: the number of accesses is less than the height if the indexed element is located in \na pre.x high up the tree. In many cases, the height function can be conveniently derived from the size \nfunction. Let S(h)be the size of the smallest tree with height h. The two functions are related by S(h)n \n<=h H(n). (1) In math speak, S:I-Iand H:I-Iform a Galois connection between the orders (I,)and (I,). \nIn the case of multiways trees, S(h)equals the mixed-radix numeral whose .rst h digits are ones. 1,...,1,0,... \nh-1 S(h)= =. wi. b0,...,bh-1,bh,... i=0 Turning to the cons operation let us .rst note that its worst-case \nrunning time is proportional to the size of the tree: if we have a cascading carry as in b0,...,bn-1,0,... \n1,...,1,1,0,... +1 = , b0,...,bn-1,bn,... b1,...,bn-1,bn,bn+1,... then the whole tree must be rearranged. \nHowever, this worst-case only happens once in a while. We obtain a much better estimate of the running \ntime if we conduct an amortized analysis. The amor\u00adtized running-time of cons is given by H{n)-1 1 n \n\u00af = Tcons(n)n . wi wi Tcons(bi) i=0 H{n)-1 \u00af = . Tcons(bi). i=0 The formula on the .rst line can be understood \nas follows: the sum calculates the costs of n successive cons operations. If we divide the result by \nn, we obtain the amortized running-time. Each summand describes the total costs at level i: the i-th \nlevel is touched every n/wi steps; if it is touched, then wi nodes must be rearranged; and \u00af the rearrangement \nof one node takes Tcons(bi)time in the worst\u00adcase. Perhaps surprisingly, the amortized running time of \ncons is given by the same formula as the worst-case running time of look\u00adup and update. Note, however, \nthat the analysis assumes that there are no interfering tail operations. Otherwise, cascading carries \nor borrows may occur in every step. Sec. 4.5 deals with the general case. In the sequel, we study three \ninstances of mixed-radix systems. For each instance, we will analyze the asymptotic growth of the sum \n{n)-1 \u00af \u00af .HTop(bi)for different choices of Top. Keep in mind that this i=0 sum captures the worst-case \nrunning time of ! and update, but the amortized running time of cons. 4.1 b-ary trees The simplest case \nof a mixed-radix number system is the positional system where the radix is the same for all positions. \nd0,d1,d2,...,dn,... b,b, b,...,b,... It is worth noting, however, that this number system is still some\u00adwhat \nunusual as we disallow the digit zero except in rightmost positions we have a so-called zeroless representation \n[16]. As an example, in radix 2 the decimal number 27 is 11220...rather than 110110.... Since the radix \nis .xed, the bootstrapped arrays are just b-ary trees. The array creation functions are given by bary \n:: Int -Bases bary b =repeat b nil1 b = gnil (bary b) copy1 bnx = gcopy (encode (bary b)n)x. Actually, \nin this simple case the de.nition of nil1 can be slightly improved. The following implementation uses \nonly constant space. nil1 b =t where t =(nil,copy b t) Turning to the performance, the size of the smallest \ntree of height h is given by the sum of the geometric progression: .ni=-01 bi =(bn \u00ad1)/(b -1). The height \nfunction can be easily calculated from S using relation (1). S(h)=(bh -1)/(b -1) H(n)=llogb(n(b -1)+1) \nFor the running time of the bootstrapped operations we calculate H{n)-1 Top(n) = . \u00afTop(bi) i=0 = llogb(n(b \n-1) +1)* \u00afTop (b) \u00af lg n *Top(b)/lg b, . where lg is the binary logarithm. As an example, assume that \nthe base operation takes logarithmic time T\u00afop(n)=c *lgn. In this case, the running time of the bootstrapped \noperation, Top(n)=c *lg n,is exactly the same. The formula above shows that the dependence of the bootstrapped \noperation on the base operation is linear: if we improve the running time of the base operation by a \nconstant factor, then the bootstrapped operation speeds up by exactly the same fac\u00adtor. It is instructive \nto take a look at some concrete .gures. Say, we want to represent an array of size 228 =268,435,456 elements.1 \nThe following table displays Top(228)for different choices of b and \u00af Top. b 2 4 16 256 1024 \u00af Top(n)=1281474 \n3 \u00af Top(n)=lg n 2929 29 29 29 \u00af Top(n)=n 29 43 106 781 2302 If we bootstrap from a real array and if \nwe use radix-256 trees, then we need at most 4 steps to access an arbitrary element. Updating an element \nis considerably more expensive: roughly 800 steps are required on an average. Of course, if we abstract \naway from multiplicative constants, we obtain logarithmic asymptotic running times irrespective of the \nun\u00adderlying base array. base array bootstrapped array T(1)T(log n)T(n) T(log n) T(log n) T(log n)  \n4.2 Arithmetic progression trees Can we improve upon the logarithmic worst-case complexity? The answer \nis an emphatic Yes! . The idea is to steadily increase the size of nodes as we move down a tree: the \nroot node has adescen\u00addants, the nodes on the second level have a+\u00dfdescendants, the nodes on the third \nlevel a+2\u00dfand so forth. d0,d1,d2,...,dn,... a,a+\u00df,a+2\u00df,...,a+n\u00df,... Since the radices form an arithmetic \nprogression, we call the cor\u00adresponding trees arithmetic progression trees. The array creation functions \nenjoy straightforward de.nitions. arithmetic :: Int -Int -[Int] arithmetic a\u00df =a: arithmetic (a+\u00df)\u00df nil2 \na\u00df =gnil (arithmetic a\u00df) copy2 a\u00dfnx =gcopy (encode (arithmetic a\u00df)n)x For the asymptotic analysis let \nus consider one particular instance of arithmetic progression trees .xing a=\u00df=1. For this choice, the \nmixed-radix number system specializes to a variant of the so-called factorial number system (the standard \nfactorial number system re\u00adquires the digits to lie in the range 0 di <bi =i +1). d0,d1,d2,d3,d4,... \n1,2,3,4,5,... The decimal number 271965, for instance, is represented by 123214650.... The size of the \nsmallest tree is given by the so\u00adn-1 called left factorial function !n =.k=0 k!. The height function \nis roughly the inverse of the left factorial function denoted \u00a1n. S(h)=!h \u00a1n -1H(n)\u00a1n Let n\u00a1 be the inverse \nof the factorial function. Note that n\u00a1 = T(log n/log log n). Since (h -1)!!hh! and consequently n\u00a1\u00a1n \n(n +1)\u00a1, we have H(n)=T(log n/log log n). nn Using the summation formulas .i=0 log i =T(nlog n)and .i=0 \ni = T(n2)we can estimate the running time of the bootstrapped opera\u00adtions. base array bootstrapped array \nT(1)T(log n)T(n) T(log n/log log n) T(log n) T((log n)2/(log log n)2) Since log n/log logn grows more \nslowly than log n, we have beaten the logarithmic look-up time of traditional tree-based implemen\u00adtations. \nFigure 3 inserts the functions above into the asymptotic hierarchy. Alas, the function loglogn grows \nvery, very slow, so, in practice, the speed-up boils down to constant factor.  4.3 Geometric progression \ntrees We have seen in the previous section that array look-up speeds up if we steadily increase the size \nof nodes. Now, instead of enlarging the nodes by a constant amount, we can alternatively enlarge them \nby a constant factor: the root node has adescendants, the nodes on the second level have a\u00dfdescendants, \nthe nodes on the third level a\u00df2 and so forth. d0,d1,d2,...dn,... n a,a\u00df,a\u00df2 ,...a\u00df,... The radices now \nform the elements of a geometric progression. Ac\u00adcordingly, the corresponding trees are called geometric \nprogression trees. Here are the array creation functions. geometric :: Int -Int -[Int] geometric a\u00df =a: \ngeometric (a*\u00df)\u00df nil3 a\u00df =gnil (geometric a\u00df) copy3 a\u00dfnx =gcopy (encode (geometric a\u00df)n)x Turning to \nthe asymptotic analysis we will again consider one par\u00adticular instance .xing a=1 and \u00df=2. Since wi =2i{i-1)/2, \nthe size function is n-1 2i{i-1)/2 S(h)= . . i=0 The size of a geometric progression tree is dominated \nby the size of the nodes on the lowest level. We have 2{h-1){h-2)/22 *2{h-1){h-2)/2 S(h). A little calculation \nyields the following estimation. 2lg n -1 H(h)2lg n +2. (2) Consequently, 1If we assume that every \nelement .ts into 64 bits, then a con\u00ad  ventional array of that size would require 1GB of main memory. \nH(h)=T(log n). logn .n loglogn .log n .log n/log log n .log n .(logn)2/(loglog n)2 .2 Figure 3. Logarithmico-exponential \nfunctions ranked by order of growth. Using (2) we can furthermore determine the running time of the bootstrapped \noperations. base array bootstrapped array e T(1)T(log n) T(log n)T(log n) log n) T(n)T(2 As to be expected, \ngeometric progression trees are more extreme than arithmetic progression trees: look-up is considerably \nfaster but update is also considerably slower (see also Figure 3).  4.4 Fat nodes Using mixed-radix \nnumber systems we can nicely steer the out\u00addegree of nodes, that is, the size of the subtree arrays. \nHowever, the size of the element arrays escapes our control, since the size is determined by the overall \nnumber of elements. In the worst case the pre.xes are singletons, whereas in the best case they are as \nlarge as the subtree arrays. We use the terms worst case and best case because large pre.xes are generally \npreferable as they improve both locality and space usage. As an example, consider representing an array \nof 1099 elements using geometric progression trees (a =1 and \u00df =2). The tree cor\u00adresponding to 1,2,4,8,16,0,0,... \n1,2,4,8,16,32,64,... consumes 1174 cells (simply summing up the array sizes). Now, if we add one element, \nwe obtain 1,1,1,1,1,1,0,... 1,2,4,8,16,32,64,..., which consumes 2199 cells (87% more). Furthermore, \nadjacent array elements are always located in different parts of the tree. Note in this respect, that \nindexing works in little-endian order (from least to most signi.cant bits). Now, to ensure that the pre.xes \nare reasonably large, we can simply use larger digits. Currently, we require the signi.cant digits to \nlie in the range 1 di bi. We generalize this condition by shifting the range to d *bi +1 di (d +1)*bi \nfor some .xed natural number d. Of course, to be able to represent all natural numbers we allow the rightmost \nsigni.cant digit to be smaller than d *bi +1. The following function converts a natural number into the \nnew number system given some d and a list of bases. encode :: Int -Bases -(Int -Mix) encode d (b : bs)n \nIn <dmin =(n,b): zip (repeat 0)bs Iotherwise =(r +dmin,b): encode d bs q where dmin =d *b +1 (q,r)=divMod \n(n -dmin)b As an example, .xing d =16 we have (1099 +1 =1100) 17,34,68,114,0,... 17,33,65,115,0,... +1 \n= , 1,2,4,8,16,... 1,2,4,8,16,... Though there is a cascading carry, the second tree requires only one \nadditional memory cell (1110 versus 1111 cells). On the downside, since the pre.xes are larger, consing \nslows down by a constant factor. The asymptotic running times are not affected, though. Note that the \nlist operations must be slightly adapted to work with the new number system (the array operations work \nwith\u00adout change). The details are left to the reader. 4.5 Redundant and lazy number systems Recall that \nthe amortized analysis of cons assumed that there were no interleaving cons and tail operations. If both \noperations are al\u00adlowed, then cons s running time degrades to T(n). Consider, for instance, an array \nthat cycles between b0,...,bn-1,0,...1,...,1,1,0,... and . b0,...,bn-1,bn,...b1,...,bn-1,bn,bn+1,... \nThis problem is typical of non-redundant number systems, where each number has a unique representation. \nThe cure is to switch to a redundant number system, for instance, by using digits in the range 1 di bi \n+1. A similar performance problem shows up if the arrays are used per\u00adsistently. Consider an array of \nsize b0,...,bn-1,0,... b0,...,bn-1,bn,... and suppose that we repeatedly cons elements to this array. \nAgain, cons takes time linear in the size of the array. In this case, lazy eval\u00aduation saves the day. \nIf we switch to a lazy setting (or add explicit delays to the de.nitions), then the calculated time bounds \nhold re\u00adgardless of whether the arrays are used persistently. For a more in\u00addepth treatment of amortization, \npersistence and lazy evaluation the interested reader is referred to Okasaki s excellent textbook [16]. \n  5 Mapping and conversion functions 5.1 Mapping functions The implementation of map and zip is entirely \nstraightforward. map f (xs,ts)=(f * xs,(map f )*ts) zip f (xs1,ts1)(xs2,ts2)=(f * xs1 xs2,(zip f )*ts1 \nts2) Note that zip expects its arguments to have the same size. 6 5.2 Array destruction The array conversion \nfunction list makes use of the following de\u00adstructor functions. elems :: (Array a)=Treeax -[x] elems \n(xs,ts)=list xs subs :: (Array a)=Treeax -[Treeax] subs (xs,ts)=list ts Recursive solution Here is the \nvanilla implementation of list: the subtrees are .attened recursively; the lists thus obtained are rif.ed \n(cf Fig. 2) and the result is appended to the pre.x. list :: (Array a)=Tree a x -[x] list t Iempty t \n=[] Iotherwise =elems t ++rif.e (list*(subs t)) The auxiliary function rif.e merges n lists of length \nm into a single list of length mn. rif.e :: [[x]]-[x] rif.e x Iall empty x =[] Iotherwise =head* x ++rif.e \n(tail* x) Note that rif.e =concat transpose. The recursive implementation of list takes super-linear \ntime in gen\u00aderal. As an example, listifying b-ary trees has a running time of T(nlog n). Iterative solution \nIf we know that the nodes of one level have the same size (so that there is an underlying number system), \nthen list can be made to run in linear time. The principle idea is to transform the given tree levelwise \nby working on a list of subtrees. list :: (Array a)=Treeax -[x] list t =ilist [t] ilist :: (Array a)=[Treeax]-[x] \nilist ts Iempty (head ts)=[] Iotherwise =rif.e (elems*ts) ++ilist (rif.e (subs*ts)) Note that the subtrees \nare rif.ed before they are passed to the recur\u00adsive call. It is not at all clear why and how this scheme \nworks. For\u00adtunately, one can derive the iterative implementation from the re\u00adcursive one. However, since \nthe derivation proceeds in a point-free style, the calculations are relegated to an appendix (see App. \nB). 5.3 Array construction The smart constructor node needed below is the inverse of the de\u00adstructor \nfunctions elems and subs. node :: (Array a)=[x]-[Treeax]-Treeax node xs ts =(array xs,array ts) Recursive \nsolution As for the other array creation functions we de.ne a generic version of array that is parametric \nin the underlying number system. The generic version takes as an additional argument the size of the \ninput list represented as a mixed-radix numeral. garray :: (Array a)=Mix -[x]-Tree a x garray ((d,b): \ns)xs Id 0 =gnil (b : snd*s)  Iotherwise =node xs1 ((garray s)*(unrif.e b xs2)) where (xs1,xs2)=splitAt \nd xs The helper function unrif.e n splits a list of length mn into n lists of length m. In a sense which \nis made precise in App. B unrif.e is the inverse of rif.e. unrif.e :: Int -[x]-[[x]] unrif.e n xs Iempty \nxs =copy n [] Iotherwise =cons * xs1 (unrif.e n xs2) where (xs1,xs2)=splitAt n xs Iterative solution \nAgain, we can achieve a linear time behaviour if we turn the recur\u00adsive solution into an iterative one. \nThe following variant of garray builds the tree levelwise. garray :: (Array a)=Mix -[x]-Treeax garray \ns xs =head (iarray 1 s xs) iarray :: (Array a)=Int -Mix -[x]-[Treeax] iarray w ((d,b): s)xs Id 0 =copy \nw (gnil (b : snd*s)) Iotherwise =node*(unrif.e w xs1)  (unrif.e w (iarray (w *b)s xs2)) where (xs1,xs2)=splitAt \n(w *d)xs Note that unrif.e is applied only locally before the trees of one level are constructed. Furthermore, \nnote that the iterative version also improves sharing: the empty tree gnil (b : snd*s)is con\u00adstructed \nonly once being shared among all nodes on the lowest level. That said it becomes clear that there is \nfurther room for improve\u00adment. Observe that the nodes on the lowest level contain identical subtree arrays, \neach of which consists of b copies of the empty tree. These subtree arrays can be shared, as well. The \nrelevant laws for improving iarray are unrif.e m (copy (m *n)x)=copy m (copy n x) f *(copy n x)=copy \nn (fx). The details of the modi.cation are left to the reader.  6 Benchmarks This section presents \npreliminary measurements comparing various instances of multiway trees to Haskell 98 arrays, Int-indexed \narrays (starting at zero), lists, Braun trees [11], and skew binary random\u00adaccess lists [15, 16]. The \nprograms were compiled with Version 5.04 of the Glasgow Haskell Compiler (-O2); the executables were \nrun on a Pentium III (645 MHz) with 192 MB of main memory. All multiway tree implementations are bootstrapped \nfrom Int-indexed arrays using Haskell s type class mechanism. Despite appearance, the parameters of the \ndifferent variants (b, a, \u00df, and d) were chosen at will. We plan to conduct more systematic measurements \nin the future. n = 1e3 5e3 1e4 5e4 1e5 5e5 1e6 standard array 0.05 0.37 1.38 9.25 18.95 105.08 230.48 \nInt-indexed array 0.05 0.27 1.29 8.00 16.08 83.57 170.98 list 1.01 23.52 121.12 Braun tree 0.24 1.65 \n4.33 26.85 56.99 324.02 685.17 random-access list 0.18 1.29 3.42 21.47 46.56 263.14 567.01 b-ary (b =28, \nd =0) 0.11 0.64 2.00 16.44 33.81 177.31 478.79 b-ary (b =256, d =0) 0.07 0.49 1.65 9.59 27.71 151.63 \n330.90 arithmetic (a =28, \u00df =4, d =0) 0.11 0.63 2.02 15.35 33.18 177.82 362.67 geometric (a =6, \u00df =6, \nd =0) 0.10 0.62 1.89 16.23 33.24 161.52 354.16 b-ary (b =28, d =32) 0.05 0.46 1.58 10.53 21.93 128.81 \n300.08 b-ary (b =256, d =32) 0.05 0.33 1.52 9.44 19.55 114.95 245.46 arithmetic (a =28, \u00df =4, d =32) \n0.05 0.46 1.61 10.89 22.28 118.58 245.47 geometric (a =6, \u00df =6, d =32) 0.07 0.46 1.76 10.85 22.15 130.95 \n272.22 Figure 4. Repeated random indexing (100 *n). The benchmarks are so-called micro-benchmarks: a \ncertain oper\u00adation or a certain sequence of operations is repeated a number of times. In the .rst test, \nan array of size n is created and subse\u00adquently indexed 100 *n times in a random fashion. The results \nof this benchmark are displayed in Fig. 4. As to be expected, standard arrays are the data structure \nof choice when only look-up is used. Perhaps surprisingly, however, arrays are only three times faster \nthan skew binary random-access lists, which perform amazingly well. Braun trees are slightly slower as \nthey exhibit poor locality, see [15]. The multiway tree implementations show the expected behaviour: \nthe larger the nodes, the better the running time. In par\u00adticular, trees with fat nodes (cf Sec. 4.4) \nperform very well they consistently outperform random-access lists by a factor of two. The second and \nthe third benchmark feature a combination of random indexing and consing: starting with an array of a \ngiven size we repeatedly perform a cons operation followed by 10 or 100 look-ups, respectively. The results \nare displayed in Fig. 5 and 6. Random-access lists are superior up to an array size of 100,000 el\u00adements. \nFor larger arrays arithmetic or geometric progression trees are preferable. Braun trees are competitive \nfor larger trees, as well (though this may be an artifact of lazy evaluation as not every ele\u00adment is \naccessed). Fig. 7 and 8 display the result of the fourth and the .fth benchmark, which test the combination \nof random index\u00ading and updating. The results are similar to the two previous bench\u00admarks (perhaps slightly \nmore in favour of bootstrapped arrays). Of course, the measurements are far from being conclusive. There \nis some indication that skew binary random-access lists are the data structure of choice if little is \nknown about the requirements of a par\u00adticular application. If the arrays are reasonably large and if \nupdates are less frequent than look-ups, then multiway trees have something to offer in particular, as \nthey can be adapted to the needs of an ap\u00adplication. As a rule of thumb, the size of the nodes or the \nrate of growth should be chosen according to the look-up/update ratio.  7 Related work The data structure \nof multiway trees generalizes fork-node trees in\u00adtroduced by Hinze [9] and discovered independently by \nXi (private communication). Fork-node trees correspond to the simplest in\u00adstance of multiway trees, namely \n2-ary trees. The idea of trading the running time of look-up for the running time of update is due to \nOkasaki (private communication). Okasaki proposed to generalize binary tries [17] to n-ary tries. Both \ndata structures, however, do not support list-like operations. Virtually all implementations of .exible \narrays are based on tree structures. The .rst design due to Braun and Rem [2] uses bi\u00adnary, node-oriented \ntrees featuring a rigid balancing scheme: for any given node, the left subtree has size i(n -1)/2land \nthe right subtree has size l(n -1)/2. Braun trees support look-up and up\u00addate in T(log i)time and cons \nand tail in T(log n)time. Skew bi\u00adnary random-access lists [15] meet the time bound for array oper\u00adations \n(albeit in the expected case) and allow the list operations to be implemented in T(1)time. Random-access \nlists are sequences of complete binary, node-oriented trees; they are modelled after the skew-binary \nnumber system. Several variants of this data struc\u00adture with different underlying number systems can \nbe found in the textbook [16]. Random-access lists based on binary numbers cor\u00adrespond to the leaf trees \nof Dielissen and Kaldewaij [3, 4], which are leftist left-perfect trees (the offsprings of the nodes \non the right spine form a sequence of complete leaf trees). A detailed compari\u00adson of these tree structures \ncan be found in [9]. Number systems serve admirably as templates for data structures. Various examples \nof this cross-fertilization can be found in the lit\u00aderature [21, 6, 16, 8]. To the best of the author \ns knowledge the use of mixed-radix number systems is original. Radically different implementations are \navailable for rigid arrays that cannot grow or shrink. Version tree arrays [10, 1] are a blend of association \nlists and real arrays: the initial version is represented by a mutable array; update operations build \na difference list in front of this array. Thus, updates take T(1)time, but look-ups degrade to T(u)time \nwhere u is the number of updates between the original version and the most recent one. By dynamically \nrestructuring the trailers look-up and update can be improved to T(1)for the special case of single-threaded \naccess. A different approach that provides better support for persistent use represents an array by a \npair consisting of a unique version stamp and a pointer to a master array, whose elements are .nite maps \nfrom version stamps to associated values. The implementation by O Neill and Burton [18] uses splay trees \n[20] for the .nite maps and a version stamp scheme based on the list order problem [5]. Their implementation \nexhibits T(1)bounds for single-threaded use and T(log n)amortized bounds for persistent use. A common \ncharacteristic of the two approaches above is the inter\u00adnal use of destructive updates. Purely functional \nimplementations of rigid arrays include AVL trees [14] and binary tries [17]. In fact, since an array \ncan be seen as a .nite map from indices to values, every implementation of .nite maps will do. Similarly, \none may n = standard array Int-indexed array list Braun tree random-access list b-ary (b =28, d =0) \nb-ary (b =256, d =0) arithmetic (a =28, \u00df =4, d =0) geometric (a =6, \u00df =6, d =0) b-ary (b =28, d =32) \nb-ary (b =256, d =32) arithmetic (a =28, \u00df =4, d =32) geometric (a =6, \u00df =6, d =32) 1e3 5.31 0.07 0.13 \n0.04 0.02 0.03 0.03 0.03 0.03 0.06 0.08 0.05 0.04 5e3 26.56 0.25 0.54 0.09 0.04 0.07 0.06 0.08 0.07 0.12 \n0.29 0.11 0.11 1e4 0.55 2.59 0.14 0.05 0.10 0.10 0.10 0.10 0.16 0.51 0.15 0.17 5e4 4.51 24.24 0.56 \n0.14 0.59 0.34 0.69 0.76 0.51 0.86 0.71 0.51 1e5 13.56 1.00 0.30 0.96 1.35 0.88 1.08 0.83 1.29 1.10 \n0.92 5e5 69.65 4.21 3.89 3.86 4.59 3.15 3.36 3.81 5.11 3.85 4.71 1e6 137.37 9.13 13.94 11.00 10.50 \n8.31 6.30 9.24 11.26 7.84 10.77 Figure 5. Random indexing and consing (one cons followed by 10 look-ups \nrepeated 1000 times). n = standard array Int-indexed array list Braun tree random-access list b-ary (b \n=28, d =0) b-ary (b =256, d =0) arithmetic (a =28, \u00df =4, d =0) geometric (a =6, \u00df =6, d =0) b-ary (b \n=28, d =32) b-ary (b =256, d =32) arithmetic (a =28, \u00df =4, d =32) geometric (a =6, \u00df =6, d =32) 1e3 4.97 \n0.18 1.11 0.32 0.24 0.20 0.16 0.20 0.20 0.18 0.19 0.17 0.18 5e3 434.74 0.39 4.91 0.46 0.32 0.26 0.23 \n0.27 0.27 0.27 0.44 0.28 0.27 1e4 0.68 15.37 0.58 0.36 0.31 0.27 0.31 0.31 0.33 0.66 0.33 0.36 5e4 \n4.68 302.75 1.21 0.70 1.07 0.69 1.04 1.34 0.84 1.00 1.06 0.82 1e5 13.75 2.00 0.89 1.60 1.76 1.79 1.75 \n1.29 1.50 1.69 1.25 5e5 69.61 5.12 5.16 4.84 5.26 4.17 4.89 5.12 6.27 4.54 5.56 1e6 137.16 11.06 \n14.60 15.03 9.71 9.71 7.31 8.84 13.60 9.08 11.33 Figure 6. Random indexing and consing (one cons followed \nby 100 look-ups repeated 1000 times). n = standard array Int-indexed array list Braun tree random-access \nlist b-ary (b =28, d =0) b-ary (b =256, d =0) arithmetic (a =28, \u00df =4, d =0) geometric (a =6, \u00df =6, d \n=0) b-ary (b =28, d =32) b-ary (b =256, d =32) arithmetic (a =28, \u00df =4, d =32) geometric (a =6, \u00df =6, \nd =32) 1e3 2.64 0.05 0.26 0.04 0.04 0.03 0.03 0.04 0.04 0.05 0.05 0.05 0.03 5e3 15.76 0.20 1.72 0.10 \n0.06 0.07 0.08 0.08 0.08 0.08 0.24 0.08 0.10 1e4 0.43 5.23 0.15 0.07 0.13 0.11 0.12 0.11 0.11 0.40 0.11 \n0.14 5e4 4.20 51.70 0.55 0.17 0.61 0.35 0.70 0.80 0.45 0.53 0.51 0.43 1e5 13.13 1.03 0.33 0.95 1.41 \n0.93 1.16 0.85 1.02 0.85 0.88 5e5 67.94 4.05 4.55 3.41 4.36 3.02 4.18 3.56 4.86 4.02 4.47 1e6 133.36 \n 8.44 14.13 12.68 11.05 8.37 6.71 7.74 10.17 7.15 11.30 Figure 7. Random indexing and updating (one \nupdate followed by 10 look-ups repeated 1000 times). n = 1e3 5e3 1e4 5e4 1e5 5e5 1e6 standard array 2.24 \n15.17 Int-indexed array 0.15 0.33 0.58 4.44 13.39 67.80 133.58 list 1.40 7.16 19.09 229.30 Braun tree \n0.32 0.47 0.57 1.08 1.70 5.18 9.53 random-access list 0.27 0.36 0.39 0.68 1.03 5.18 16.72 b-ary (b =28, \nd =0) 0.21 0.27 0.31 1.03 1.52 4.94 13.62 b-ary (b =256, d =0) 0.18 0.23 0.27 0.69 1.81 5.28 13.37 arithmetic \n(a =28, \u00df =4, d =0) 0.21 0.28 0.31 1.22 1.77 4.17 8.01 geometric (a =6, \u00df =6, d =0) 0.21 0.26 0.30 1.27 \n1.72 4.85 9.14 b-ary (b =28, d =32) 0.16 0.23 0.28 0.73 1.12 4.21 9.45 b-ary (b =256, d =32) 0.15 0.38 \n0.59 0.82 1.17 5.44 10.78 arithmetic (a =28, \u00df =4, d =32) 0.16 0.24 0.28 0.82 1.21 3.83 7.31 geometric \n(a =6, \u00df =6, d =32) 0.17 0.25 0.32 0.73 1.19 5.42 12.03 adapt an implementation of ordered lists. Instances \nbased on .nger search trees [12, 7], for example, support cons in T(1)time and allow to access or update \nthe i-th element in T(log i)time.  8 Conclusion and future work We have presented a purely functional \nimplementation of one\u00adsided .exible arrays based on weight-balanced multiway trees. The data structure \nis simple to implement and fully persistent. Multi\u00adway trees are bootstrapped from an existing implementation \nof ar\u00adrays. The performance of the underlying base array type and the branching structure of the multiway \ntrees determine the overall per\u00adformance. Both parameters can be chosen dynamically at array\u00adcreation \ntime. We have shown that a large class of tree shapes can be conveniently modelled after mixed-radix \nnumber systems and we have analysed three obvious choices (b-ary trees, arithmetic pro\u00adgression trees, \nand geometric progression trees) in detail. Prelim\u00adinary measurements show that multiway trees perform \nreasonably well in practice. The work described here is the end-product of several abstractions generalizing \nfork-node trees [9] to b-ary trees and b-ary trees to multiways trees based on mixed-radix number systems. \nEach gen\u00aderalization had a tremendous effect on both implementation and presentation: without exception, \nthe generalized operations were simpler to implement and easier to understand than their instances. In \nparticular, the introduction of mixed-radix number systems uni\u00ad.ed and simpli.ed the implementation of \nthe array creation func\u00adtions. Much is left to be done. The generality of the data structure opens up \nquite a huge design space. Clearly, a more systematic explo\u00adration of this space is desirable: What is \nthe optimal tree shape for a given look-up/update ratio? Likewise, what is the preferred shape if the \nsize of the array is known to lie in a certain range? To obtain more convincing .gures, we also plan \nto port the implementation to other languages such as O Caml, Standard ML, and Clean. Acknowledgements \nI am indebted to Chris Okasaki for a very stimulating discussion on functional arrays (the idea of trading \nthe running time of look\u00adup operations for the running time of update operations is due to Chris). Thanks \nare due to Peter Thiemann for spotting an error in a previous version of tail. Furthermore, thanks go \nto the ART team at York and the IFIP 2.8 working group for valuable feedback. Finally, I would like to \nthank four anonymous referees for their constructive comments.  References [1] Annika Aasa, S\u00a8oren Holmstr\u00a8om, \nand Christina Nilsson. An ef.ciency comparison of some representations of purely func\u00adtional arrays. \nBIT, 28(3):490 503, 1988. [2] W. Braun and M. Rem. A logarithmic implementation of .ex\u00adible arrays. Memorandum \nMR83/4, Eindhoven University of Technology, 1983. [3] Victor J. Dielissen and Anne Kaldewaij. A simple, \nef.cient, and .exible implementation of .exible arrays. In Third Inter\u00adnational Conference on Mathematics \nof Program Construc\u00adtion, MPC 95, Kloster Irsee, Germany, volume 947 of Lecture Notes in Computer Science, \npages 232 241. Springer-Verlag, July 1995. [4] Victor J. Dielissen and Anne Kaldewaij. Leaf trees. Science \nof Computer Programming, 26(1 3):149 165, May 1996. [5] Paul Dietz and Daniel Sleator. Two algorithms \nfor maintain\u00ading order in a list. In Alfred Aho, editor, Proceedings of the 19th Annual ACM Symposium \non Theory of Computing, pages 365 372, New York City, NY, May 1987. ACM Press. [6] R. Fagerberg. A generalization \nof binomial queues. Informa\u00adtion Processing Letters, 57(2):109 114, 1996. [7] Ralf Hinze. Numerical representations \nas higher-order nested datatypes. Technical Report IAI-TR-98-12, Institut f\u00a8ur Infor\u00admatik III, Universit\u00a8at \nBonn, December 1998. [8] Ralf Hinze. Constructing red-black trees. In Chris Okasaki, editor, Proceedings \nof the Workshop on Al\u00adgorithmic Aspects of Advanced Programming Languages, WAAAPL 99, Paris, France, \npages 89 99, September 1999. The proceedings appeared as a technical report of Columbia University, CUCS-023-99, \nalso available from http://www.cs.columbia.edu/ cdo/waaapl.html. [9] Ralf Hinze. Manufacturing datatypes. \nJournal of Functional Programming, Special Issue on Algorithmic Aspects of Func\u00adtional Programming Languages, \n11(5):493 524, September 2001. [10] S\u00a8oren Holmstr\u00a8om. A simple and ef.cient way to handle large datastructures \nin applicative languages. In Proceedings of the Joint SERC/Chalmers Workshop on Declarative Program\u00adming, \nUniversity College, London, UK, 1983. [11] R.R. Hoogerwoord. A logarithmic implementation of .exi\u00adble \narrays. In R.S. Bird, C.C. Morgan, and J.C.P. Woodcock, editors, Proceedings of the Second International \nConference on Mathematics of Program Construction, pages 191 207. LNCS, Springer-Verlag, 1992. [12] Haim \nKaplan and Robert E. Tarjan. Purely functional rep\u00adresentations of catenable sorted lists. In Proceedings \nof the Twenty-Eighth Annual ACM Symposium on the Theory of Computing, pages 202 211, Philadelphia, Pennsylvania, \nMay 1996. [13] Donald E. Knuth. The Art of Computer Programming, Vol\u00adume 2: Seminumerical Algorithms. \nAddison-Wesley Publish\u00ading Company, 3rd edition, 1997. [14] Eugene W. Myers. Ef.cient applicative data \ntypes. In Ken Kennedy, editor, Conference Record of the 11th Annual ACM Symposium on Principles of Programming \nLanguages, pages 66 75, Salt Lake City, UT, January 1984. ACM Press. [15] Chris Okasaki. Purely functional \nrandom-access lists. In Pro\u00adceedings of the Seventh International Conference on Func\u00adtional Programming \nLanguages and Computer Architecture (FPCA 95), pages 86 95, La Jolla, California, June 1995. ACM SIGPLAN/SIGARCH \nand IFIP WG2.8, ACM Press. [16] Chris Okasaki. Purely Functional Data Structures. Cam\u00adbridge University \nPress, 1998. [17] Chris Okasaki and Andy Gill. Fast mergeable integer maps. In The 1998 ACM SIGPLAN Workshop \non ML, Baltimore, Mary\u00adland, pages 77 86, 1998. [18] Melissa E. O Neill and F. Warren Burton. A new method \nfor functional arrays. Journal of Functional Programming, 7(5):487 513, September 1997. [19] Simon Peyton \nJones [editor], John Hughes [editor], Lennart Augustsson, Dave Barton, Brian Boutel, Warren Burton, Si\u00admon \nFraser, Joseph Fasel, Kevin Hammond, Ralf Hinze, Paul Hudak, Thomas Johnsson, Mark Jones, John Launch\u00adbury, \nErik Meijer, John Peterson, Alastair Reid, Colin Runciman, and Philip Wadler. Standard libraries for \nthe Haskell 98 programming language. Available from http://www.haskell.org/definition/, February 1999. \n [20] Daniel D Sleator and Robert E Tarjan. Self-adjusting binary search trees. Journal of the ACM, 32(3):652 \n686, July 1985. [21] Jean Vuillemin. A data structure for manipulating priority queues. Communications \nof the ACM, 21(4):309 315, 1978.  A A reference implementation of Array The declaration below makes \nthe list data type an instance of the Array class. Its main purpose is to specify the intended behaviour \nof the member functions. instance Array []where []! i =error \"index out of range\" (x : xs)!0 =x (x \n: xs)! (i +1)=xs ! i  update f i [] =error \"index out of range\" update f 0 (x : xs)=fx : xs update f \n(i +1)(x : xs)=x : update f i xs empty [] =True empty (x : xs)=False size [] =0 size (x : xs)=1 +size \nxs nil =[] copy 0 x =[] copy (n +1)x =x : copy n x cons =(:) head (x : xs)=x tail (x : xs)=xs map f \n[] =[] map f (x : xs)=fx : mapf xs zip f [][] =[] zip f (x : xs)(y : ys)=fxy : zipf xsys list =id array \n=id  B Derivation of ilist This section derives the iterative de.nition of list from the straight\u00adforward \nrecursive de.nition. Functions The recursive de.nition of list uses lists a lot: as an intermediate result \nit produces a list of lists, which is then rif.ed into a list of elements. It is important to note that \nrif.e only works if the inner lists have the same length. Fortunately, this property is guaranteed by \nInvariant 2. Now, for the derivation it is useful, even vital to keep track of this size information. \nFor that reason, we replace the ubiquitous list type [X ]by tuple (or vector) types of the form Xn . \nIn a sense, we view the type of lists as a family of tuple types. Likewise, a list-processing function \nis seen as a family of functions operating on tuples. We require the following operations. wrap :: X \n-X1 unwrap :: X1 -X .m,n :: (Xn)m -Xmn ..m,n :: Xmn -(Xn)m catm,n :: Xm Xn -Xm+n uncatm,n :: Xm+n -Xm \nXn zipn :: Xn Yn -(X Y)n unzipn :: (X Y)n -Xn Yn Here .m,n denotes one instance of rif.e and ..m,n denotes \none in\u00adstance of unrif.e. For reasons of readability, we will usually omit the second index writing .m \ninstead of .m,n and likewise for ..m,n, catm,n and uncatm,n. Properties The functions satisfy a variety \nof properties that are needed in the derivation. The functions come in pairs. Since they operate on tuple \ntypes, each operation has a natural inverse: wrap and unwrap are inverse to each other (wrap unwrap =id \nand unwrap wrap =id), .m,n and ..m,n, catm,n and uncatm,n, zipn and unzipn. All the functions are polymorphic \nin the element type(s). As such they enjoy quite general properties, often referred to as naturality \nlaws. f 1 wrap =wrap f f mn (fn)m .m,n =.m,n (fn gn)unzipn =unzipn (fg)n Here, ()n is the mapping function \non n-tuples and  is the map\u00adping function on pairs. The following law states a basic property of ., \nwhich could be put to use in a divide and conquer implementation of rif.e. .m,n+n. (catn,n. )m zipm =catmn,mn. \n(.m,n .m,n. )(3) Given two matrices of type (Xn)m and (Xn. )m we can either join them horizontally (catenating \nthe rows) and rif.e the result or else rif.e them separately and catenate the resulting lists. The .nal \nre\u00adsult is the same in both cases. Finally, . satis.es two monad-like properties. .1,n wrap =id (4) .m,nn. \n(.n,n. )m =.mn,n. .m,n (5) If we ignore the size constraints, we obtain . wrap =id and ..* = .., which \nare two of the monad laws with concat replaced by .. That said note that the input for the last equation \nis a three\u00addimensional matrix of type ((Xn. )n)m . Speci.cation Before we specify the iterative version \nof list let us .rst rephrase Tree and recursive version in the current framework. The data type of multiway \ntrees is indexed by a mixed-radix nu\u00admeral, which speci.es the size of the pre.xes and the degree of \nthe nodes. [][] d0,d1, d1, Tree X =Xd0 (Tree X)b0 b0,b1, b1, Let [] d0,d1, n =. b0,b1, The type of list \nis then [][]Xnd0,d1, d0,d1, list :: Tree X -. b0,b1, b0,b1, The straightforward recursive implementation \nis given by 0,0, list []=const () b0,b1, [] [] d0,d1, d1, list =catd0 (id .b0 (list )b0 ). b0,b1, b1, \nNow, the idea of the iterative version is to replace the b0 recursive calls to list by a single recursive \ncall that operates on b0 trees simul\u00adtaneously. This motivates the following speci.cation (we renamed \nb0 to w). [][] d0,d1, d0,d1, Xwn ilistw :: (Tree X)w \u00ad b0,b1, b0,b1, [][] d0,d1, d0,d1, ilistw =.w (list \n)w b0,b1, b0,b1, Derivation Let us introduce the following abbreviations: [][][] 0,0, d0,d1, d1, s0 =, \ns =, and s'=. b0,b1, b0,b1, b1, For the base case we calculate. ilistw s0 ={speci.cation of ilist } .w \n(list s0)w ={de.nition of list } .w (const ())w ={const ()=f 0 and naturality of . } const ().w ={const \n()f =const ()} const () For the inductive case we reason. ilistw s ={speci.cation of ilist } .w (list \ns)w ={de.nition of list } .w (catd0 (id .b0 (list s')b0 ))w ={()n functor } .w (catd0 )w (id .b0 (list \ns')b0 )w ={zipn unzipn =id } .w (catd0 )w zipw unzipw (id .b0 (list s')b0 )w ={property (3) } catwd0 \n(.w .w)unzipw (id .b0 (list s')b0 )w = {naturality of unzip } catwd0 (.w .w) (idw (.b0 (list s')b0 )w) \nunzipw = {functor  and ()n } catwd0 (.w .w (.b0 )w ((list s')b0 )w) unzipw = {property (5) } catwd0 \n(.w .wb0 .w ((list s')b0 )w) unzipw = {naturality of . } catwd0 (.w .wb0 (list s')wb0 .w) unzipw = {speci.cation \nof ilist } catwd0 (.w ilistwb0 s' .w) unzipw Noting that unzipw elems*6subs*we have derived the point-free \ncounterpart of the de.nition given in Sec. 5.2. It remains to de.ne list in terms of ilist. list s ={property \n(4) } .1 wrap list s ={naturality of wrap } .1 (list s)1 wrap ={speci.cation of ilist } ilist1 s wrap \n C Derivation of iarray Since list s and ilistw s are isomorphisms, we can specify garray s and iarrayw \ns simply as function inverses. list s garray s =id =garray s list s ilistw s iarrayw s =id =iarrayw s \nilistw s As an illustration, here is the straightforward derivation of iarray (inductive case only). \nilistw s iarrayw s =id <={de.nition of ilist } catwd0 (.w ilistwb0 s'.w)unzipw iarrayw s =id <={cat and \nuncat are inverses } (.w ilistwb0 s'.w)unzipw iarrayw s =uncatwd0 <={. and ..are inverses, speci.cation \nof ilistw } unzipw iarrayw s =(..w ..w iarraywb0 s')uncatwd0 <={zip and unzip are inverses } iarrayw \ns =zipw (..w ..w iarraywb0 s')uncatwd0 Noting that zipw node*we have derived the point-free counterpart \nof the de.nition given in Sec. 5.3.  \n\t\t\t", "proc_id": "581478", "abstract": "The abstract data type <i>one-sided flexible array</i>, also called random-access list, supports look-up and update of elements and can grow and shrink at one end. We describe a purely functional implementation based on weight-balanced multiway trees that is both simple and versatile. A novel feature of the representation is that the running time of the operations can be tailored to one's needs---even dynamically at array-creation time. In particular, one can trade the running time of look-up operations for the running time of update operations. For instance, if the multiway trees have a fixed degree, the operations take &#952;(log <i>n</i>) time, where <i>n</i> is the size of the flexible array. If the degree doubles levelwise, look-up speeds up to &#952;(sqrtlog <i>n</i>) while update slows down to &#952;(2<sup>sqrt log <i>n</i></sup>). We show that different tree shapes can be conveniently modelled after mixed-radix number systems.", "authors": [{"name": "Ralf Hinze", "author_profile_id": "81332504302", "affiliation": "Universit&#228;t Bonn, Bonn, Germany", "person_id": "PP43126402", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/581478.581480", "year": "2002", "article_id": "581480", "conference": "ICFP", "title": "Bootstrapping one-sided flexible arrays", "url": "http://dl.acm.org/citation.cfm?id=581480"}