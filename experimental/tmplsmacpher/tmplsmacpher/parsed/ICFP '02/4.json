{"article_publication_date": "09-17-2002", "fulltext": "\n Packrat Parsing: Simple, Powerful, Lazy, Linear Time Functional Pearl Bryan Ford Massachusetts Institute \nof Technology Cambridge, MA baford@lcs.mit.edu Abstract Packrat parsing is a novel technique for implementing \nparsers in a lazy functional programming language. A packrat parser provides the power and .exibility \nof top-down parsing with backtracking and unlimited lookahead, but nevertheless guarantees linear parse \ntime. Any language de.ned by an LL(k) or LR(k) grammar can be rec\u00adognized by a packrat parser, in addition \nto many languages that conventional linear-time algorithms do not support. This additional power simpli.es \nthe handling of common syntactic idioms such as the widespread but troublesome longest-match rule, enables \nthe use of sophisticated disambiguation strategies such as syntactic and se\u00admantic predicates, provides \nbetter grammar composition properties, and allows lexical analysis to be integrated seamlessly into parsing. \nYet despite its power, packrat parsing shares the same simplicity and elegance as recursive descent parsing; \nin fact converting a back\u00adtracking recursive descent parser into a linear-time packrat parser often involves \nonly a fairly straightforward structural change. This paper describes packrat parsing informally with \nemphasis on its use in practical applications, and explores its advantages and disadvan\u00adtages with respect \nto the more conventional alternatives. Categories and Subject Descriptors D.3.4 [Programming Languages]: \nProcessors Parsing; D.1.1 [Programming Techniques]: Applicative (Functional) Program\u00adming; F.4.2 [Mathematical \nLogic and Formal Languages]: Grammars and Other Rewriting Systems Parsing General Terms Languages, Algorithms, \nDesign, Performance Keywords Haskell, memoization, top-down parsing, backtracking, lexical analysis, \nscannerless parsing, parser combinators Permission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior \nspeci.c permission and/or a fee. ICFP 02, October 4-6, 2002, Pittsburgh, Pennsylvania, USA. Copyright \n2002 ACM 1-58113-487-8/02/0010 ...$5.00 1 Introduction There are many ways to implement a parser in a \nfunctional pro\u00adgramming language. The simplest and most direct approach is top-down or recursive descent \nparsing, in which the components of a language grammar are translated more-or-less directly into a set \nof mutually recursive functions. Top-down parsers can in turn be divided into two categories. Predictive \nparsers attempt to pre\u00addict what type of language construct to expect at a given point by looking ahead \na limited number of symbols in the input stream. Backtracking parsers instead make decisions speculatively \nby try\u00ading different alternatives in succession: if one alternative fails to match, then the parser backtracks \nto the original input position and tries another. Predictive parsers are fast and guarantee linear\u00adtime \nparsing, while backtracking parsers are both conceptually sim\u00adpler and more powerful but can exhibit \nexponential runtime. This paper presents a top-down parsing strategy that sidesteps the choice between \nprediction and backtracking. Packrat parsing pro\u00advides the simplicity, elegance, and generality of the \nbacktracking model, but eliminates the risk of super-linear parse time, by sav\u00ading all intermediate parsing \nresults as they are computed and en\u00adsuring that no result is evaluated more than once. The theoretical \nfoundations of this algorithm were worked out in the 1970s [3, 4], but the linear-time version was apparently \nnever put in practice due to the limited memory sizes of computers at that time. However, on modern machines \nthe storage cost of this algorithm is reason\u00adable for many applications. Furthermore, this specialized \nform of memoization can be implemented very elegantly and ef.ciently in modern lazy functional programming \nlanguages, requiring no hash tables or other explicit lookup structures. This marriage of a classic but \nneglected linear-time parsing algorithm with modern functional programming is the primary technical contribution \nof this paper. Packrat parsing is unusually powerful despite its linear time guar\u00adantee. A packrat parser \ncan easily be constructed for any language described by an LL(k)orLR(k) grammar, as well as for many \nlan\u00adguages that require unlimited lookahead and therefore are not LR. This .exibility eliminates many \nof the troublesome restrictions im\u00adposed by parser generators of the YACC lineage. Packrat parsers are \nalso much simpler to construct than bottom-up LR parsers, mak\u00ading it practical to build them by hand. \nThis paper explores the manual construction approach, although automatic construction of packrat parsers \nis a promising direction for future work. A packrat parser can directly and ef.ciently implement common \ndisambiguation rules such as longest-match, followed-by, and not\u00adfollowed-by, which are dif.cult to express \nunambiguously in a context-free grammar or implement in conventional linear-time Additive . Multitive \n+ Additive | Multitive Multitive . Primary * Multitive | Primary Primary . ( Additive ) | Decimal Decimal \n. 0 | ... | 9 Figure 1. Grammar for a trivial language parsers. For example, recognizing identi.ers or \nnumbers during lexical analysis, parsing if-then-else statements in C-like lan\u00adguages, and handling do, \nlet, and lambda expressions in Haskell inherently involve longest-match disambiguation. Packrat parsers \nare also more easily and naturally composable than LR parsers, making them a more suitable substrate \nfor dynamic or extensible syntax [1]. Finally, both lexical and hierarchical analysis can be seamlessly \nintegrated into a single uni.ed packrat parser, and lexi\u00adcal and hierarchical language features can even \nbe blended together, so as to handle string literals with embedded expressions or literate comments with \nstructured document markup, for example. The main disadvantage of packrat parsing is its space consumption. \nAlthough its asymptotic worst-case bound is the same as those of conventional algorithms linear in the \nsize of the input its space utilization is directly proportional to input size rather than maxi\u00admum recursion \ndepth, which may differ by orders of magnitude. However, for many applications such as modern optimizing \ncompil\u00aders, the storage cost of a pacrkat parser is likely to be no greater than the cost of subsequent \nprocessing stages. This cost may therefore be a reasonable tradeoff for the power and .exibility of linear-time \nparsing with unlimited lookahead. The rest of this paper explores packrat parsing with the aim of pro\u00adviding \na pragmatic sense of how to implement it and when it is useful. Basic familiarity with context-free grammars \nand top-down parsing is assumed. For brevity and clarity of presentation, only small excerpts of example \ncode are included in the text. However, all of the examples described in this paper are available, as \ncomplete and working Haskell code, at: http://pdos.lcs.mit.edu/ baford/packrat/icfp02 The paper is organized \nas follows. Section 2 introduces packrat parsing and describes how it works, using conventional recursive \ndescent parsing as a starting point. Section 3 presents useful ex\u00adtensions to the basic algorithm, such \nas support for left recursion, lexical analysis, and monadic parsing. Section 4 explores in more detail \nthe recognition power of packrat parsers in comparison with conventional linear-time parsers. Section \n5 discusses the three main practical limitations of packrat parsing: determinism, statelessness, and \nspace consumption. Section 6 presents some experimental re\u00adsults to demonstrate the practicality of packrat \nparsing for real lan\u00adguages. Section 7 discusses related work, Section 8 points out di\u00adrections for future \nexploration, and Section 9 concludes. 2 Building a Parser Packrat parsing is essentially a top-down \nparsing strategy, and as such packrat parsers are closely related to recursive descent parsers. For this \nreason, we will .rst build a recursive descent parser for a trivial language and then convert it into \na packrat parser. 2.1 Recursive Descent Parsing Consider the standard approach for constructing a recursive \ndescent parser for a grammar such as the trivial arithmetic expression lan\u00adguage shown in Figure 1. We \nde.ne four functions, one for each of the nonterminals on the left-hand sides of the rules. Each func\u00adtion \ntakes takes the string to be parsed, attempts to recognize some pre.x of the input string as a derivation \nof the corresponding nonter\u00adminal, and returns either a success or failure result. On success, the function \nreturns the remainder of the input string immediately following the part that was recognized, along with \nsome semantic value computed from the recognized part. Each function can re\u00adcursively call itself and \nthe other functions in order to recognize the nonterminals appearing on the right-hand sides of its corresponding \ngrammar rules. To implement this parser in Haskell, we .rst need a type describing the result of a parsing \nfunction: data Result v = Parsed v String | NoParse In order to make this type generic for different \nparse functions pro\u00adducing different kinds of semantic values, the Result type takes a type parameter \nv representing the type of the associated semantic value. A success result is built with the Parsed constructor \nand contains a semantic value (of type v) and the remainder of the input text (of type String). A failure \nresult is represented by the sim\u00adple value NoParse. In this particular parser, each of the four parse \nfunctions takes a String and produces a Result with a semantic value of type Int: pAdditive :: String \n-> Result Int pMultitive :: String -> Result Int pPrimary :: String -> Result Int pDecimal :: String \n-> Result Int The de.nitions of these functions have the following general struc\u00adture, directly re.ecting \nthe mutual recursion expressed by the gram\u00admar in Figure 1: pAdditive s = ... (calls itself and pMultitive) \n... pMultitive s = ... (calls itself and pPrimary) ... pPrimary s = ... (calls pAdditive and pDecimal) \n... pDecimal s = ... For example, the pAdditive function can be coded as follows, us\u00ading only primitive \nHaskell pattern matching constructs: --Parse an additive-precedence expression pAdditive :: String -> \nResult Int pAdditive s = alt1 where --Additive <-Multitive + Additive alt1 = case pMultitive s of Parsed \nvleft s -> case s of ( + :s ) -> case pAdditive s of Parsed vright s -> Parsed (vleft + vright) s _ \n-> alt2 _ -> alt2 _ -> alt2 --Additive <-Multitive alt2 = case pMultitive s of Parsed v s -> Parsed \nv s NoParse -> NoParse To compute the result of pAdditive, we .rst compute the value of alt1, representing \nthe .rst alternative for this grammar rule. This alternative in turn calls pMultitive to recognize a \nmultiplicative\u00adprecedence expression. If pMultitive succeeds, it returns the se\u00admantic value vleft of \nthat expression and the remaining input s following the recognized portion of input. We then check for \na + operator at position s , which if successful produces the string s representing the remaining input \nafter the + operator. Finally, we recursively call pAdditive itself to recognize another additive\u00adprecedence \nexpression at position s , which if successful yields the right-hand-side result vright and the .nal \nremainder string s .If all three of these matches were successful, then we re\u00adturn as the result of \nthe initial call to pAdditive the semantic value of the addition, vleft + vright, along with the .nal \nremainder string s . If any of these matches failed, we fall back on alt2, the second alternative, which \nmerely attempts to recognize a single multiplicative-precedence expression at the original input position \ns and returns that result verbatim, whether success or failure. The other three parsing functions are \nconstructed similarly, in direct correspondence with the grammar. Of course, there are easier and more \nconcise ways to write these parsing functions, using an appro\u00adpriate library of helper functions or combinators. \nThese techniques will be discussed later in Section 3.3, but for clarity we will stick to simple pattern \nmatching for now. 2.2 Backtracking Versus Prediction The parser developed above is a backtracking parser. \nIf alt1 in the pAdditive function fails, for example, then the parser effec\u00adtively backtracks to the \noriginal input position, starting over with the original input string s in the second alternative alt2, \nregard\u00adless of whether the .rst alternative failed to match during its .rst, second, or third stage. \nNotice that if the input s consists of only a single multiplicative expression, then the pMultitive function \nwill be called twice on the same string: once in the .rst alternative, which will fail while trying to \nmatch a nonexistent + operator, and then again while successfully applying the second alternative. This \nbacktracking and redundant evaluation of parsing functions can lead to parse times that grow exponentially \nwith the size of the input, and this is the principal reason why a naive backtracking strategy such as \nthe one above is never used in realistic parsers for inputs of substantial size. The standard strategy \nfor making top-down parsers practical is to design them so that they can predict which of several alterna\u00adtive \nrules to apply before actually making any recursive calls. In this way it can be guaranteed that parse \nfunctions are never called redundantly and that any input can be parsed in linear time. For ex\u00adample, \nalthough the grammar in Figure 1 is not directly suitable for a predictive parser, it can be converted \ninto an LL(1) grammar, suit\u00adable for prediction with one lookahead token, by left-factoring the Additive \nand Multitive nonterminals as follows: Additive . Multitive AdditiveSuf.x AdditiveSuf.x . + Additive \n| e Multitive . Primary MultitiveSuf.x MultitiveSuf.x . * Multitive | e Now the decision between the \ntwo alternatives for AdditiveSuf.x can be made before making any recursive calls simply by check\u00ading \nwhether the next input character is a + . However, because the prediction mechanism only has raw input \ntokens (characters in this case) to work with, and must itself operate in constant time, the class of \ngrammars that can be parsed predictively is very re\u00adstrictive. Care must also be taken to keep the prediction \nmechanism consistent with the grammar, which can be dif.cult to do manu- C1 C2 C3 C4 C5 C6 C7C8 column \npAdditive (7,C7) (4,C7) pMultitive (3,C5)  (4,C7) pPrimary (3,C5) (4,C7) pDecimal (3,C5) (4,C7) \ninput 2 * ( 3 + 4 ) (end) Figure 2. Matrix of parsing results for string 2*(3+4) ally and highly \nsensitive to global properties of the language. For example, the prediction mechanism for MultitiveSuf.x \nwould have to be adjusted if a higher-precedence exponentiation operator ** was added to the language; \notherwise the exponentiation operator would falsely trigger the predictor for multiplication expressions \nand cause the parser to fail on valid input. Some top-down parsers use prediction for most decisions \nbut fall back on full backtracking when more .exibility is needed. This strategy often yields a good \ncombination of .exibility and perfor\u00admance in practice, but it still suffers the additional complexity \nof prediction, and it requires the parser designer to be intimately aware of where prediction can be \nused and when backtracking is required. 2.3 Tabular Top-Down Parsing As pointed out by Birman and Ullman \n[4], a backtracking top-down parser of the kind presented in Section 2.1 can be made to operate in linear \ntime without the added complexity or constraints of predic\u00adtion. The basic reason the backtracking parser \ncan take super-linear time is because of redundant calls to the same parse function on the same input \nsubstring, and these redundant calls can be eliminated through memoization. Each parse function in the \nexample is dependent only on its sin\u00adgle parameter, the input string. Whenever a parse function makes \na recursive call to itself or to another parse function, it always sup\u00adplies either the same input string \nit was given (e.g., for the call by pAdditive to pMultitive), or a suf.x of the original input string \n(e.g., for the recursive call by pAdditive to itself after matching a + operator). If the input string \nis of length n, then there are only n + 1 distinct suf.xes that might be used in these recursive calls, \ncounting the original input string itself and the empty string. Since there are only four parse functions, \nthere are at most 4(n + 1) dis\u00adtinct intermediate results that the parsing process might require. We \ncan avoid computing any of these intermediate results multiple times by storing them in a table. The \ntable has one row for each of the four parse functions and one column for each distinct position in the \ninput string. We .ll the table with the results of each parse function for each input position, starting \nat the right end of the input string and working towards the left, column by column. Within each column, \nwe start from the bottommost cell and work upwards. By the time we compute the result for a given cell, \nthe results of all would-be recursive calls in the corresponding parse function will already have been \ncomputed and recorded elsewhere in the table; we merely need to look up and use the appropriate results. \nFigure 2 illustrates a partially-completed result table for the in\u00adput string 2*(3+4) . For brevity, \nParsed results are indicated as (v,c), where v is the semantic value and c is the column number at which \nthe associated remainder suf.x begins. Columns are labeled C1, C2, and so on, to avoid confusion with \nthe integer semantic val\u00adues. NoParse results are indicated with an X in the cell. The next cell to be \n.lled is the one for pPrimary at column C3, indicated with a circled question mark. The rule for Primary \nexpressions has two alternatives: a parenthe\u00adsized Additive expression or a Decimal digit. If we try \nthe alter\u00adnatives in the order expressed in the grammar, pPrimary will .rst check for a parenthesized \nAdditive expression. To do so, pPrimary .rst attempts to match an opening ( in column C3, which suc\u00adceeds \nand yields as its remainder string the input suf.x starting at column C4, namely 3+4) . In the simple \nrecursive-descent parser pPrimary would now recursively call pAdditive on this remain\u00adder string. However, \nbecause we have the table we can simply look up the result for pAdditive at column C4 in the table, which \nis (7,C7). This entry indicates a semantic value of 7 the result of the addition expression 3+4 and a \nremainder suf.x of ) start\u00ading in column C7. Since this match is a success, pPrimary .nally attempts \nto match the closing parenthesis at position C7, which suc\u00adceeds and yields the empty string C8 as the \nremainder. The result entered for pPrimary at column C3 is thus (7,C8). Although for a long input string \nand a complex grammar this re\u00adsult table may be large, it only grows linearly with the size of the input \nassuming the grammar has a .xed number of nonterminals. Furthermore, as long as the grammar uses only \nthe standard opera\u00adtors of Backus-Naur Form [2], only a .xed number of previously\u00adrecorded cells in the \nmatrix need to be accessed in order to compute each new result. Therefore, assuming table lookup occurs \nin con\u00adstant time, the parsing process as a whole completes in linear time. Due to the forward pointers \nembedded in the results table, the computation of a given result may examine cells that are widely spaced \nin the matrix. For example, computing the result for pPrimary at C3 above made use of results from columns \nC3, C4, and C7. This ability to skip ahead arbitrary distances while making parsing decisions is the \nsource of the algorithm s unlimited looka\u00adhead capability, and this capability makes the algorithm more \npow\u00aderful than linear-time predictive parsers or LR parsers. 2.4 Packrat Parsing An obvious practical \nproblem with the tabular right-to-left parsing algorithm above is that it computes many results that \nare never needed. An additional inconvenience is that we must carefully determine the order in which \nthe results for a particular column are computed, so that parsing functions such as pAdditive and pMultitive \nthat depend on other results from the same column will work correctly. Packrat parsing is essentially \na lazy version of the tabular algorithm that solves both of these problems. A packrat parser computes \nre\u00adsults only as they are needed, in the same order as the original re\u00adcursive descent parser would. \nHowever, once a result is computed for the .rst time, it is stored for future use by subsequent calls. \nA non-strict functional programming language such as Haskell pro\u00advides an ideal implementation platform \nfor a packrat parser. In fact, packrat parsing in Haskell is particularly ef.cient because it does not \nrequire arrays or any other explicit lookup structures other than the language s ordinary algebraic data \ntypes. First we will need a new type to represent a single column of the parsing result matrix, which \nwe will call Derivs ( derivations ). This type is merely a tuple with one component for each nonter\u00adminal \nin the grammar. Each component s type is the result type of the corresponding parse function. The Derivs \ntype also contains one additional component, which we will call dvChar, to represent raw characters of \nthe input string as if they were themselves the results of some parsing function. The Derivs type for \nour example parser can be conveniently declared in Haskell as follows: data Derivs = Derivs { dvAdditive \n:: Result Int, dvMultitive :: Result Int, dvPrimary :: Result Int, dvDecimal :: Result Int, dvChar \n:: Result Char} This Haskell syntax declares the type Derivs to have a single con\u00adstructor, also named \nDerivs, with .ve components of the speci.ed types. The declaration also automatically creates a corresponding \ndata-accessor function for each component: dvAdditive can be used as a function of type Derivs . Result \nInt, which extracts the .rst component of a Derivs tuple, and so on. Next we modify the Result type so \nthat the remainder compo\u00adnent of a success result is not a plain String, but is instead an instance of \nDerivs: data Result v = Parsed v Derivs | NoParse The Derivs and Result types are now mutually recursive: \nthe suc\u00adcess results in one Derivs instance act as links to other Derivs instances. These result values \nin fact provide the only linkage we need between different columns in the matrix of parsing results. \nNow we modify the original recursive-descent parsing functions so that each takes a Derivs instead of \na String as its parameter: pAdditive :: Derivs -> Result Int pMultitive :: Derivs -> Result Int pPrimary \n:: Derivs -> Result Int pDecimal :: Derivs -> Result Int Wherever one of the original parse functions \nexamined input char\u00adacters directly, the new parse function instead refers to the dvChar component of \nthe Derivs object. Wherever one of the original functions made a recursive call to itself or another \nparse function, in order to match a nonterminal in the grammar, the new parse func\u00adtion instead instead \nuses the Derivs accessor function correspond\u00ading to that nonterminal. Sequences of terminals and nonterminals \nare matched by following chains of success results through multiple Derivs instances. For example, the \nnew pAdditive function uses the dvMultitive, dvChar, and dvAdditive accessors as follows, without making \nany direct recursive calls: --Parse an additive-precedence expression pAdditive :: Derivs -> Result Int \npAdditive d = alt1 where --Additive <-Multitive + Additive alt1 = case dvMultitive d of Parsed vleft \nd -> case dvChar d of Parsed + d -> case dvAdditive d of Parsed vright d -> Parsed (vleft + vright) \nd _ -> alt2 _ -> alt2 _ -> alt2 --Additive <-Multitive alt2 = dvMultitive d Finally, we create a special \ntop-level function, parse, to produce instances of the Derivs type and tie up the recursion between all \nof the individual parsing functions: --Create a result matrix for an input string parse :: String -> \nDerivs parse s = d where d = Derivs add mult prim dec chr add = pAdditive d mult = pMultitive d prim \n= pPrimary d dec = pDecimal d chr =casesof (c:s ) -> Parsed c (parse s ) [] -> NoParse The magic \nof the packrat parser is in this doubly-recursive func\u00adtion. The .rst level of recursion is produced \nby the parsefunction s reference to itself within the case statement. This relatively con\u00adventional form \nof recursion is used to iterate over the input string one character at a time, producing one Derivs instance \nfor each input position. The .nal Derivs instance, representing the empty string, is assigned a dvChar \nresult of NoParse, which effectively terminates the list of columns in the result matrix. The second \nlevel of recursion is via the symbol d. This identi.er names the Derivs instance to be constructed and \nreturned by the parse function, but it is also the parameter to each of the individual parsing functions. \nThese parsing functions, in turn, produce the rest of the components forming this very Derivs object. \nThis form of data recursion of course works only in a non-strict lan\u00adguage, which allow some components \nof an object to be accessed before other parts of the same object are available. For example, in any \nDerivs instance created by the above function, the dvChar component can be accessed before any of the \nother components of the tuple are available. Attempting to access the dvDecimal com\u00adponent of this tuple \nwill cause pDecimal to be invoked, which in turn uses the dvChar component but does not require any of \nthe other higher-level components. Accessing the dvPrimary com\u00adponent will similarly invoke pPrimary, \nwhich may access dvChar and dvAdditive. Although in the latter case pPrimaryis accessing a higher-level \ncomponent, doing so does not create a cyclic de\u00adpendency in this case because it only ever invokes dvAdditive \non a different Derivs object from the one it was called with: namely the one for the position following \nthe opening parenthesis. Every component of every Derivsobject produced by parsecan be lazily evaluated \nin this fashion. Figure 3 illustrates the data structure produced by the parser for the example input \ntext 2*(3+4) , as it would appear in memory under a modern functional evaluator after fully reducing \nevery cell. Each vertical column represents a Derivs instance with its .ve Result components. For results \nof the form Parsed vd , the seman\u00adtic value v is shown in the appropriate cell, along with an arrow representing \nthe remainder pointer leading to another Derivs in\u00adstance in the matrix. In any modern lazy language \nimplementation that properly preserves sharing relationships during evaluation, the arrows in the diagram \nwill literally correspond to pointers in the heap, and a given cell in the structure will never be evaluated \ntwice. Shaded boxes represent cells that would never be evaluated at all in dvAdditive dvMultitive dvPrimary \ndvDecimal dvChar Figure 3. Illustration of Derivs data structure produced by parsing the string 2*(3+4) \nthe likely case that the dvAdditive result in the leftmost column is the only value ultimately needed \nby the application. This illustration should make it clear why this algorithm can run in O(n) time under \na lazy evaluator for an input string of length n. The top-level parse function is the only function that \ncreates instances of the Derivs type, and it always creates exactly n + 1 instances. The parse functions \nonly access entries in this structure instead of making direct calls to each other, and each function \nexamines at most a .xed number of other cells while computing a given result. Since the lazy evaluator \nensures that each cell is evaluated at most once, the critical memoization property is provided and linear \nparse time is guaranteed, even though the order in which these results are evaluated is likely to be \ncompletely different from the tabular, right-to-left, bottom-to-top algorithm presented earlier.  3 \nExtending the Algorithm The previous section provided the basic principles and tools re\u00adquired to create \na packrat parser, but building parsers for real appli\u00adcations involves many additional details, some \nof which are affected by the packrat parsing paradigm. In this section we will explore some of the more \nimportant practical issues, while incrementally building on the example packrat parser developed above. \nWe .rst examine the annoying but straightforward problem of left recursion. Next we address the issue \nof lexical analysis, seamlessly integrat\u00ading this task into the packrat parser. Finally, we explore the \nuse of monadic combinators to express packrat parsers more concisely. 3.1 Left Recursion One limitation \npackrat parsing shares with other top-down schemes is that it does not directly support left recursion. \nFor example, sup\u00adpose we wanted to add a subtraction operator to the above example and have addition \nand subtraction be properly left-associative. A natural approach would be to modify the grammar rules \nfor Addi\u00adtive expressions as follows, and to change the parser accordingly: Additive . Additive + Multitive \n| Additive - Multitive | Multitive In a recursive descent parser for this grammar, the pAdditivefunc\u00adtion \nwould recursively invoke itself with the same input it was pro\u00advided, and therefore would get into an \nin.nite recursion cycle. In a packrat parser for this grammar, pAdditive would attempt to access the \ndvAdditive component of its own Derivs tuple the same component it is supposed to compute and thus would \ncre\u00adate a circular data dependency. In either case the parser fails, al\u00adthough the packrat parser s failure \nmode might be viewed as slightly friendlier since modern lazy evaluators often detect circular data dependencies \nat run-time but cannot detect in.nite recursion. Fortunately, a left-recursive grammar can always be \nrewritten into an equivalent right-recursive one [2], and the desired left\u00adassociative semantic behavior \nis easily reconstructed using higher\u00adorder functions as intermediate parser results. For example, to \nmake Additive expressions left-associative in the example parser, we can split this rule into two nonterminals, \nAdditive and AdditiveSuf.x. The pAdditive function recognizes a single Multitive expression followed \nby an AdditiveSuf.x: pAdditive :: Derivs -> Result Int pAdditive d = case dvMultitive d of Parsed vl \nd -> case dvAdditiveSuffix d of Parsed suf d -> Parsed (suf vl) d _ -> NoParse _ -> NoParse The pAdditiveSuffix \nfunction collects in.x operators and right\u00adhand-side operands, and builds a semantic value of type Int \n. Int , which takes a left-hand-side operand and produces a result: pAdditiveSuffix :: Derivs -> Result \n(Int -> Int) pAdditiveSuffix d = alt1 where --AdditiveSuffix <- + Multitive AdditiveSuffix alt1 = case \ndvChar d of Parsed + d -> case dvMultitive d of Parsed vr d -> case dvAdditiveSuffix d of Parsed suf \nd -> Parsed (\\vl -> suf (vl + vr)) d _ -> alt2 _ -> alt2 _ -> alt2 --AdditiveSuffix <-<empty> alt3 \n= Parsed (\\v -> v) d  3.2 Integrated Lexical Analysis Traditional parsing algorithms usually assume \nthat the raw input text has already been partially digested by a separate lexical ana\u00adlyzer into a stream \nof tokens. The parser then treats these tokens as atomic units even though each may represent multiple \nconsecu\u00adtive input characters. This separation is usually necessary because conventional linear-time \nparsers can only use primitive terminals in their lookahead decisions and cannot refer to higher-level \nnonter\u00adminals. This limitation was explained in Section 2.2 for predictive top-down parsers, but bottom-up \nLR parsers also depend on a sim\u00adilar token-based lookahead mechanism sharing the same problem. If a parser \ncan only use atomic tokens in its lookahead decisions, then parsing becomes much easier if those tokens \nrepresent whole keywords, identi.ers, and literals rather than raw characters. Packrat parsing suffers \nfrom no such lookahead limitation, how\u00adever. Because a packrat parser re.ects a true backtracking model, \ndecisions between alternatives in one parsing function can depend on complete results produced by other \nparsing functions. For this reason, lexical analysis can be integrated seamlessly into a packrat parser \nwith no special treatment. To extend the packrat parser example with real lexical analysis, we add some \nnew nonterminals to the Derivs type: data Derivs = Derivs { --Expressions dvAdditive :: Result Int, ... \n --Lexical tokens dvDigits :: Result (Int, Int), dvDigit :: Result Int, dvSymbol :: Result Char, dvWhitespace \n:: Result (), --Raw input dvChar :: Result Char} The pWhitespace parse function consumes any whitespace \nthat may separate lexical tokens: pWhitespace :: Derivs -> Result () pWhitespace d = case dvChar d of \n Parsed c d -> if isSpace c then pWhitespace d else Parsed () d _ -> Parsed () d In a more complete \nlanguage, this function might have the task of eating comments as well. Since the full power of packrat \nparsing is available for lexical analysis, comments could have a complex hier\u00adarchical structure of their \nown, such as nesting or markups for liter\u00adate programming. Since syntax recognition is not broken into \na uni\u00addirectional pipeline, lexical constructs can even refer upwards to higher-level syntactic elements. \nFor example, a language s syntax could allow identi.ers or code fragments embedded within com\u00adments to \nbe demarked so the parser can .nd and analyze them as actual expressions or statements, making intelligent \nsoftware engi\u00adneering tools more effective. Similarly, escape sequences in string literals could contain \ngeneric expressions representing static or dy\u00adnamic substitutions. The pWhitespace example also illustrates \nhow commonplace longest-match disambiguation rules can be easily implemented in a packrat parser, even \nthough they are dif.cult to express in a pure context-free grammar. More sophisticated decision and dis\u00adambiguation \nstrategies are easy to implement as well, including general syntactic predicates [14], which in.uence \nparsing deci\u00adsions based on syntactic lookahead information without actually consuming input text. For \nexample, the useful followed-by and not\u00adfollowed-by rules allow a parsing alternative to be used only \nif the text matched by that alternative is (or is not) followed by text match\u00ading some other arbitrary \nnonterminal. Syntactic predicates of this kind require unlimited lookahead in general and are therefore \nout\u00adside the capabilities of most other linear-time parsing algorithms. Continuing with the lexical analysis \nexample, the function pSymbol recognizes operator tokens consisting of an operator character followed \nby optional whitespace: --Parse an operator followed by optional whitespace pSymbol :: Derivs -> Result \nChar pSymbol d = case dvChar d of Parsed c d -> if c elem \"+-*/%()\" then case dvWhitespace d of Parsed \n_ d -> Parsed c d _ -> NoParse else NoParse _ -> NoParse Now we modify the higher-level parse functions \nfor expressions to use dvSymbol instead of dvChar to scan for operators and paren\u00adtheses. For example, \npPrimary can be implemented as follows: --Parse a primary expression pPrimary :: Derivs -> Result Int \npPrimary d = alt1 where --Primary <- ( Additive ) alt1 = case dvSymbol d of Parsed ( d -> case dvAdditive \nd of Parsed v d -> case dvSymbol d of Parsed ) d -> Parsed v d _ -> alt2 _ -> alt2 _ -> alt2 \n --Primary <-Decimal alt2 = dvDecimal d This function demonstrates how parsing decisions can depend \nnot only on the existence of a match at a given position for a nontermi\u00adnal such as Symbol, but also \non the semantic value associated with that nonterminal. In this case, even though all symbol tokens are \nparsed together and treated uniformly by pSymbol, other rules such as pPrimary can still distinguish \nbetween particular symbols. In a more sophisticated language with multi-character operators, iden\u00adti.ers, \nand reserved words, the semantic values produced by the token parsers might be of type String instead \nof Char, but these values can be matched in the same way. Such dependencies of syn\u00adtax on semantic values, \nknown as semantic predicates [14], provide an extremely powerful and useful capability in practice. As \nwith syntactic predicates, semantic predicates require unlimited looka\u00adhead in general and cannot be \nimplemented by conventional parsing algorithms without giving up their linear time guarantee. 3.3 Monadic \nPackrat Parsing A popular method of constructing parsers in functional languages such as Haskell is using \nmonadic combinators [11, 13]. Unfortu\u00adnately, the monadic approach usually comes with a performance penalty, \nand with packrat parsing this tradeoff presents a dif.cult choice. Implementing a packrat parser as described \nso far assumes that the set of nonterminals and their corresponding result types is known statically, \nso that they can be bound together in a single .xed tuple to form the Derivs type. Constructing entire \npackrat parsers dynamically from other packrat parsers via combinators would re\u00adquire making the Derivs \ntype a dynamic lookup structure, asso\u00adciating a variable set of nonterminals with corresponding results. \nThis approach would be much slower and less space-ef.cient. A more practical strategy, which provides \nmost of the convenience of combinators with a less signi.cant performance penalty, is to use monads to \nde.ne the individual parsing functions comprising a packrat parser, while keeping the Derivs type and \nthe top-level recursion statically implemented as described earlier. Since we would like our combinators \nto build the parse functions we need directly, the obvious method would be to make the combi\u00adnators work \nwith a simple type alias: type Parser v = Derivs -> Result v Unfortunately, in order to take advantage \nof Haskell s useful do syntax, the combinators must use a type of the special class Monad, and simple \naliases cannot be assigned type classes. We must instead wrap the parsing functions with a real user-de.ned \ntype: newtype Parser v = Parser (Derivs -> Result v) We can now implement Haskell s standard sequencing \n(>>=), result-producing (return), and error-producing combinators: instance Monad Parser where (Parser \np1) >>= f2 = Parser pre where pre d = post (p1 d) post (Parsed v d ) = p2 d where Parser p2 = f2 v post \n(NoParse) = NoParse return x = Parser (\\d -> Parsed x d) fail msg = Parser (\\d -> NoParse) Finally, \nfor parsing we need an alternation combinator: (<|>) :: Parser v -> Parser v -> Parser v (Parser p1) \n<|> (Parser p2) = Parser pre where pre d = post d (p1 d) post d NoParse = p2 d post dr = r With these \ncombinators in addition to a trivial one to recognize speci.c characters, the pAdditive function in the \noriginal packrat parser example can be written as follows: Parser pAdditive = (do vleft <-Parser dvMultitive \n char + vright <-Parser dvAdditive return (vleft + vright)) <|> (do Parser dvMultitive) It is tempting \nto build additional combinators for higher-level id\u00adioms such as repetition and in.x expressions. However, \nusing it\u00aderative combinators within packrat parsing functions violates the assumption that each cell \nin the result matrix can be computed in constant time once the results from any other cells it depends \non are available. Iterative combinators effectively create hidden re\u00adcursion whose intermediate results \nare not memoized in the result matrix, potentially making the parser run in super-linear time. This problem \nis not necessarily serious in practice, as the results in Sec\u00adtion 6 will show, but it should be taken \ninto account when using iterative combinators. The on-line examples for this paper include a full-featured \nmonadic combinator library that can be used to build large packrat parsers conveniently. This library \nis substantially inspired by PARSEC [13], though the packrat parsing combinators are much simpler since \nthey do not have to implement lexical analysis as a separate phase or implement the one-token-lookahead \nprediction mechanism used by traditional top-down parsers. The full combinator library provides a variety \nof safe constant-time combinators, as well as a few dan\u00adgerous iterative ones, which are convenient but \nnot necessary to construct parsers. The combinator library can be used simultane\u00adously by multiple parsers \nwith different Derivs types, and supports user-friendly error detection and reporting.  4 Comparison \nwith LL and LR Parsing Whereas the previous sections have served as a tutorial on how to construct a \npackrat parser, for the remaining sections we turn to the issue of when packrat parsing is useful in \npractice. This sec\u00adtion informally explores the language recognition power of packrat parsing in more \ndepth, and clari.es its relationship to traditional linear-time algorithms such as LL(k) and LR(k). Although \nLR parsing is commonly seen as more powerful than limited-lookahead top-down or LL parsing, the class \nof languages these parsers can recognize is the same [3]. As Pepper points out [17], LR parsing can be \nviewed simply as LL parsing with the grammar rewritten so as to eliminate left recursion and to delay \nall important parsing decisions as long as possible. The result is that LR provides more .exibility in \nthe way grammars can be expressed, but no actual additional recognition power. For this reason, we will \ntreat LL and LR parsers here as being essentially equivalent. 4.1 Lookahead The most critical practical \ndifference between packrat parsing and LL/LR parsing is the lookahead mechanism. A packrat parser s de\u00adcisions \nat any point can be based on all the text up to the end of the input string. Although the computation \nof an individual result in the parsing matrix can only perform a constant number of basic oper\u00adations, \nthese basic operations include following forward pointers in the parsing matrix, each of which can skip \nover a large amount of text at once. Therefore, while LL and LR parsers can only look ahead a constant \nnumber of terminals in the input, packrat parsers can look ahead a constant number of terminals and nonterminals \nin any combination. This ability for parsing decisions to take ar\u00adbitrary nonterminals into account is \nwhat gives packrat parsing its unlimited lookahead capability. To illustrate the difference in language \nrecognition power, the fol\u00adlowing grammar is not LR(k) for any k, but is not a problem for a packrat \nparser: S . A | B A . xAy | xzy B . xByy | xzyy Once an LR parser has encountered the z and the .rst \nfollowing y in a string in the above language, it must decide immediately whether to start reducing via \nnonterminal A or B, but there is no way for it to make this decision until as many y s have been en\u00adcountered \nas there were x s on the left-hand side. A packrat parser, on the other hand, essentially operates in \na speculative fashion, pro\u00adducing derivations for nonterminals A and B in parallel while scan\u00adning the \ninput. The ultimate decision between A and B is effectively delayed until the entire input string has \nbeen parsed, where the deci\u00adsion is merely a matter of checking which nonterminal has a success result \nat that position. Mirroring the above grammar left to right does not change the situation, making it \nclear that the difference is not merely some side-effect of the fact that LR scans the input left-to-right \nwhereas packrat parsing seems to operate in reverse. 4.2 Grammar Composition The limitations of LR parsing \ndue to .xed lookahead are frequently felt when designing parsers for practical languages, and many of \nthese limitations stem from the fact that LL and LR grammars are not cleanly composable. For example, \nthe following grammar rep\u00adresents a simple language with expressions and assignment, which only allows \nsimple identi.ers on the left side of an assignment: S . R | ID = R R . A | AEQA | ANEA A . P | P + P \n| P - P P . ID | ( R ) If the symbols ID, EQ, and NE are terminals i.e., atomic to\u00adkens produced by a \nseparate lexical analysis phase then an LR(1) parser has no trouble with this grammar. However, if we \ntry to integrate this tokenization into the parser itself with the following simple rules, the grammar \nis no longer LR(1): ID . a | a ID EQ . = = NE . ! = The problem is that after scanning an identi.er, \nan LR parser must decide immediately whether it is a primary expression or the left\u00adhand side of an assignment, \nbased only on the immediately fol\u00adlowing token. But if this token is an = , the parser has no way of \nknowing whether it is an assignment operator or the .rst half of an == operator. In this particular case \nthe grammar could be parsed by an LR(2) parser. In practice LR(k) and even LALR(k) parsers are uncommon \nfor k > 1. Recently developed extensions to the traditional left-to-right parsing algorithms improve \nthe situation somewhat [18, 16, 15], but they still cannot provide unrestricted lookahead capability \nwhile maintaining the linear time guarantee. Even when lexical analysis is separated from parsing, the \nlimita\u00adtions of LR parsers often surface in other practical situations, fre\u00adquently as a result of seemingly \ninnocuous changes to an evolving grammar. For example, suppose we want to add simple array in\u00addexing \nto the language above, so that array indexing operators can appear on either the left or right side of \nan assignment. One possi\u00adble approach is to add a new nonterminal, L, to represent left-side or lvalue \nexpressions, and incorporate the array indexing operator into both types of expressions as shown below: \nS . R | L = R R . A | AEQA | ANE A A . P | P + P | P - P P . ID | ( R ) | P [ A ] L . ID | ( L ) | L \n[ A ] Even if the ID, EQ, and NE symbols are again treated as terminals, this grammar is not LR(k) for \nany k, because after the parser sees an identi.er it must immediately decide whether it is part ofaPor \nL expression, but it has no way of knowing this until any following array indexing operators have been \nfully parsed. Again, a pack\u00adrat parser has no trouble with this grammar because it effectively evaluates \nthe P and L alternatives in parallel and has complete derivations to work with (or the knowledge of their \nabsence) by the time the critical decision needs to be made. In general, grammars for packrat parsers \nare composable because the lookahead a packrat parser uses to make decisions between al\u00adternatives can \ntake account of arbitrary nonterminals, such as EQ in the .rst example or P and L in the second. Because \na packrat parser does not give primitive syntactic constructs (terminals) any spe\u00adcial signi.cance as \nan LL or LR parser does, any terminal or .xed sequence of terminals appearing in a grammar can be substituted \nwith a nonterminal without breaking the parser. This substitution capability gives packrat parsing greater \ncomposition .exibility. 4.3 Recognition Limitations Given that a packrat parser can recognize a broader \nclass of lan\u00adguages in linear time than either LL(k)orLR(k) algorithms, what kinds of grammars can t \na packrat parser recognize? Though the precise theoretical capabilities of the algorithm have not been \nthoroughly characterized, the following trivial and unambiguous context-free grammar provides an example \nthat proves just as trou\u00adblesome for a packrat parser as for an LL or LR parser: S . xSx | x The problem \nwith this grammar for both kinds of parsers is that, while scanning a string of x s left-to-right in \nthe LR case or right\u00adto-left in the packrat case the algorithm would somehow have to know in advance \nwhere the middle of the string is so that it can apply the second alternative at that position and then \nbuild out\u00adwards using the .rst alternative for the rest of the input stream. But since the stream is \ncompletely homogeneous, there is no way for the parser to .nd the middle until the entire input has been \nparsed. This grammar therefore provides an example, albeit contrived, requiring a more general, non-linear-time \nCFG parsing algorithm.  5 Practical Issues and Limitations Although packrat parsing is powerful and \nef.cient enough for many applications, there are three main issues that can make it inappro\u00adpriate in \nsome situations. First, packrat parsing is useful only to construct deterministic parsers: parsers that \ncan produce at most one result. Second, a packrat parser depends for its ef.ciency on being mostly or \ncompletely stateless. Finally, due to its reliance on memoization, packrat parsing is inherently space-intensive. \nThese three issues are discussed in this section. 5.1 Deterministic Parsing An important assumption we \nhave made so far is that each of the mutually recursive parsing functions from which a packrat parser \nis built will deterministically return at most one result. If there are any ambiguities in the grammar \nthe parser is built from, then the pars\u00ading functions must be able to resolve them locally. In the example \nparsers developed in this paper, multiple alternatives have always been implicitly disambiguated by the \norder in which they are tested: the .rst alternative to match successfully is the one used, indepen\u00addent \nof whether any other alternatives may also match. This behav\u00adior is both easy to implement and useful \nfor performing longest\u00admatch and other forms of explicit local disambiguation. A parsing function could \neven try all of the possible alternatives and produce a failure result if more than one alternative matches. \nWhat parsing functions in a packrat parser cannot do is return multiple results to be used in parallel \nor disambiguated later by some global strategy. In languages designed for machine consumption, the requirement \nthat multiple matching alternatives be disambiguated locally is not much of a problem in practice because \nambiguity is usually un\u00addesirable in the .rst place, and localized disambiguation rules are preferred \nover global ones because they are easier for humans to understand. However, for parsing natural languages \nor other gram\u00admars in which global ambiguity is expected, packrat parsing is less likely to be useful. \nAlthough a classic nondeterministic top-down parser in which the parse functions return lists of results \n[23, 8] could be memoized in a similar way, the resulting parser would not be linear time, and would \nlikely be comparable to existing tabu\u00adlar algorithms for ambiguous context-free grammars [3, 20]. Since \nnondeterministic parsing is equivalent in computational complexity to boolean matrix multiplication [12], \na linear-time solution to this more general problem is unlikely to be found. 5.2 Stateless Parsing A \nsecond limitation of packrat parsing is that it is fundamentally geared toward stateless parsing. A packrat \nparser s memoization system assumes that the parsing function for each nonterminal de\u00adpends only on the \ninput string, and not on any other information accumulated during the parsing process. Although pure \ncontext-free grammars are by de.nition stateless, many practical languages require a notion of state \nwhile parsing and thus are not really context-free. For example, C and C++ require the parser to build \na table of type names incrementally as types are declared, because the parser must be able to distinguish \ntype names from other identi.ers in order to parse subsequent text correctly. Traditional top-down (LL) \nand bottom-up (LR) parsers have little trouble maintaining state while parsing. Since they perform only \na single left-to-right scan of the input and never look ahead more than one or at most a few tokens, \nnothing is lost when a state change occurs. A packrat parser, in contrast, depends on statelessness for \nthe ef.ciency of its unlimited lookahead capability. Although a stateful packrat parser can be constructed, \nthe parser must start building a new result matrix each time the parsing state changes. For this reason, \nstateful packrat parsing may be impractical if state changes occur frequently. For more details on packrat \nparsing with state, please refer to my master s thesis [9]. 5.3 Space Consumption Probably the most \nstriking characteristic of a packrat parser is the fact that it literally squirrels away everything it \nhas ever computed about the input text, including the entire input text itself. For this reason packrat \nparsing always has storage requirements equal to some possibly substantial constant multiple of the input \nsize. In contrast, LL(k), LR(k), and simple backtracking parsers can be de\u00adsigned so that space consumption \ngrows only with the maximum nesting depth of the syntactic constructs appearing in the input, which in \npractice is often orders of magnitude smaller than the total size of the text. Although LL(k) and LR(k) \nparsers for any non\u00adregular language still have linear space requirements in the worst case, this average-case \ndifference can be important in practice. One way to reduce the space requirements of the derivations \nstruc\u00adture, especially in parsers for grammars with many nonterminals, is by splitting up the Derivs \ntype into multiple levels. For exam\u00adple, suppose the nonterminals of a language can be grouped into several \nbroad categories, such as lexical tokens, expressions, state\u00adments, and declarations. Then the Derivs \ntuple itself might have only four components in addition to dvChar, one for each of these nonterminal \ncategories. Each of these components is in turn a tuple containing the results for all of the nonterminals \nin that category. For the majority of the Derivs instances, representing character positions between \ntokens, none of the components representing the categories of nonterminals will ever be evaluated, and \nso only the small top-level object and the unevaluated closures for its com\u00adponents occupy space. Even \nfor Derivs instances corresponding to the beginning of a token, often the results from only one or two \ncategories will be needed depending on what kind of language con\u00adstruct is located at that position. \nEven with such optimizations a packrat parser can consume many times more working storage than the size \nof the original input text. For this reason there are some application areas in which packrat parsing \nis probably not the best choice. For example, for parsing XML streams, which have a fairly simple structure \nbut often encode large amounts of relatively .at, machine-generated data, the power and .exibility of \npackrat parsing is not needed and its storage cost would not be justi.ed. On the other hand, for parsing \ncomplex modern programming lan\u00adguages in which the source code is usually written by humans and the top \npriority is the power and expressiveness of the language, the space cost of packrat parsing is probably \nreasonable. Standard pro\u00adgramming practice involves breaking up large programs into mod\u00adules of manageable \nsize that can be independently compiled, and the main memory sizes of modern machines leave at least \nthree orders of magnitude in headroom for expansion of a typical 10 100KB source .le during parsing. \nEven when parsing larger source .les, the working set may still be relatively small due to the strong \nstructural locality properties of realistic languages. Finally, since the entire derivations structure \ncan be thrown away after parsing is complete, the parser s space consumption is likely to be irrelevant \nif its result is fed into some other complex computation, such as a global optimizer, that requires as \nmuch space as the packrat parser used. Section 6 will present evidence that this space consumption can \nbe reasonable in practice.  6 Performance Results Although a detailed empirical analysis of packrat \nparsing is outside the scope of this paper, it is helpful to have some idea of how a packrat parser is \nlikely to behave in practice before committing to a new and unfamiliar parsing paradigm. For this reason, \nthis section presents a few experimental results with realistic packrat parsers running on real source \n.les. For more detailed results, please refer to my master s thesis [9]. 6.1 Space Ef.ciency The .rst \nset of tests measure the space ef.ciency of a packrat parser for the Java1 programming language. I chose \nJava for this experi\u00adment because it has a rich and complex grammar, but nevertheless adopts a fairly \nclean syntactic paradigm, not requiring the parser to keep state about declared types as C and C++ parsers \ndo, or to per\u00adform special processing between lexical and hierarchical analysis as Haskell s layout scheme \nrequires. The experiment uses two different versions of this Java parser. Apart from a trivial preprocessing \nstage to canonicalize line breaks and Java s Unicode escape sequences, lexical analysis for both parsers \nis fully integrated as described in Section 3.2. One parser uses monadic combinators in its lexical analysis \nfunctions, while the other parser relies only on primitive pattern matching. Both parsers use monadic \ncombinators to construct all higher-level pars\u00ading functions. Both parsers also use the technique described \nin Sec\u00adtion 5.3 of splitting the Derivs tuple into two levels, in order to in\u00adcrease modularity and reduce \nspace consumption. The parsers were compiled with the Glasgow Haskell Compiler2 version 5.04, with optimization \nand pro.ling enabled. GHC s heap pro.ling system was used to measure live heap utilization, which excludes \nunused heap space and collectible garbage when samples are taken. 1Java is a trademark of Sun Microsystems, \nInc. 2http://www.haskell.org/ghc/ Source file size (bytes) Figure 4. Maximum heap size versus input \nsize The test suite consists of 60 unmodi.ed Java source .les from the Cryptix library3, chosen because \nit includes a substantial number of relatively large Java source .les. (Java source .les are small on \naverage because the compilation model encourages programmers to place each class de.nition in a separate \n.le.) Figure 4 shows a plot of each parser s maximum live heap size against the size of the input .les \nbeing parsed. Because some of the smaller source .les were parsed so quickly that garbage collection \nnever occurred and the heap pro.ling mechanism did not yield any samples, the plot includes only 45 data \npoints for the fully monadic parser, and 31 data points for the hybrid parser using direct pat\u00adtern matching \nfor lexical analysis. Averaged across the test suite, the fully monadic parser uses 695 bytes of live \nheap per byte of input, while the hybrid parser uses only 301 bytes of heap per in\u00adput byte. These results \nare encouraging: although packrat parsing can consume a substantial amount of space, a typical modern \nma\u00adchine with 128KB or more of RAM should have no trouble pars\u00ading source .les up to 100-200KB. Furthermore, \neven though both parsers use some iterative monadic combinators, which can break the linear time and \nspace guarantee in theory, the space consump\u00adtion of the parsers nevertheless appears to grow fairly \nlinearly. The use of monadic combinators clearly has a substantial penalty in terms of space ef.ciency. \nModifying the parser to use direct pattern matching alone may yield further improvement, though the degree \nis dif.cult to predict since the cost of lexical analysis often dominates the rest of the parser. The \nlexical analysis portion of the hybrid parser is about twice as long as the equivalent portion of the \nmonadic parser, suggesting that writing packrat parsers with pattern matching alone is somewhat more \ncumbersome but not unreason\u00adable when ef.ciency is important. 6.2 Parsing Performance The second experiment \nmeasures the absolute execution time of the two packrat parsers. For this test the parsers were compiled \nby GHC 5.04 with optimization but without pro.ling, and timed on a 1.28GHz AMD Athlon processor running \nLinux 2.4.17. For this test I only used the 28 source .les in the test suite that were larger than 10KB, \nbecause the smaller .les were parsed so quickly that the Linux time command did not yield adequate precision. \nFig\u00adure 5 shows the resulting execution time plotted against source .le size. On these inputs the fully \nmonadic parser averaged 25.0 Kbytes 3http://www.cryptix.org/ 0 20000 40000 60000 80000 100000 120000 \n140000 Source file size (bytes) Figure 5. Execution time versus input size per second with a standard \ndeviation of 8.6 KB/s, while the hybrid parser averaged 49.8 KB/s with a standard deviation of 16 KB/s. \nIn order to provide a legitimate performance comparison between packrat parsing and more traditional \nlinear-time algorithms, I con\u00adverted a freely available YACC grammar for Java [5] into a gram\u00admar for \nHappy4, an LR parser generator for Haskell. Unfortunately, GHC was unable to compile the 230KB Haskell \nsource .le result\u00ading from this grammar, even without optimization and on a ma\u00adchine with 1GB of RAM. \n(This dif.culty incidentally lends credi\u00adbility to the earlier suggestion that, in modern compilers, \nthe tem\u00adporary storage cost of a packrat parser is likely to be exceeded by the storage cost of subsequent \nstages.) Nevertheless, the generated LR parser worked under the Haskell interpreter Hugs.5 Therefore, \nto provide a rough performance comparison, I ran .ve of the larger Java sources through the LR and packrat \nparsers under Hugs using an 80MB heap. For fairness, I only compared the LR parser against the slower, \nfully monadic packrat parser, because the LR parser uses a monadic lexical analyzer derived from the \nlatter packrat parser. The lexical analysis performance should therefore be comparable and only the parsing \nalgorithm is of primary importance. Under Hugs, the LR parser consistently performs approximately twice \nthe number of reductions and allocates 55% more total heap storage. (I could not .nd a way to pro.le \nlive heap utilization under Hugs instead of total allocation.) The difference in real execution time \nvaried widely however: the LR parser took almost twice as long on smaller .les but performed about the \nsame on the largest ones. One probable reason for this variance is the effects of garbage collection. \nSince a running packrat parser will naturally have a much higher ratio of live data to garbage than an \nLR parser over time, and garbage collection both increases in overhead cost and decreases in effectiveness \n(i.e., frees less space) when there is more live data, garbage collection is likely to penalize a packrat \nparser more than an LR parser as the size of the source .le increases. Still, it is encouraging that \nthe packrat parser was able to outperform the LR parser on all but the largest Java source .les.  7 \nRelated Work This section brie.y relates packrat parsing to relevant prior work. For a more detailed \nanalysis of packrat parsing in comparison with other algorithms please refer to my master s thesis [9]. \n4http://www.haskell.org/happy 5http://www.haskell.org/hugs Birman and Ullman [4] .rst developed the formal \nproperties of de\u00adterministic parsing algorithms with backtracking. This work was re.ned by Aho and Ullman \n[3] and classi.ed as top-down limited backtrack parsing, in reference to the restriction that each parsing \nfunction can produce at most one result and hence backtracking is localized. They showed this kind of \nparser, formally known as a Generalized Top-Down Parsing Language (GTDPL) parser, to be quite powerful. \nA GTDPL parser can simulate any push-down au\u00adtomaton and thus recognize any LL or LR language, and it \ncan even recognize some languages that are not context free. Nevertheless, all failures such as those \ncaused by left recursion can be detected and eliminated from a GTDPL grammar, ensuring that the algo\u00adrithm \nis well-behaved. Birman and Ullman also pointed out the pos\u00adsibility of constructing linear-time GTDPL \nparsers through tabula\u00adtion of results, but this linear-time algorithm was apparently never put into \npractice, no doubt because main memories were much more limited at the time and compilers had to operate \nas streaming .l\u00adters that could run in near-constant space. Adams [1] recently resurrected GTDPL parsing \nas a component of a modular language prototyping framework, after recognizing its superior composability \nin comparison with LR algorithms. In addi\u00adtion, many practical top-down parsing libraries and toolkits, \ninclud\u00ading the popular ANTLR [15] and the PARSEC combinator library for Haskell [13], provide similar \nlimited backtracking capabilities which the parser designer can invoke selectively in order to over\u00adcome \nthe limitations of predictive parsing. However, all of these parsers implement backtracking in the traditional \nrecursive-descent fashion without memoization, creating the danger of exponential worst-case parse time, \nand thereby making it impractical to rely on backtracking as a substitute for prediction or to integrate \nlexical analysis with parsing. The only prior known linear-time parsing algorithm that effectively supports \nintegrated lexical analysis, or scannerless parsing, is the NSLR(1) algorithm originally created by Tai \n[19] and put into prac\u00adtice for this purpose by Salomon and Cormack [18]. This algorithm extends the \ntraditional LR class of algorithms by adding limited support for making lookahead decisions based on \nnonterminals. The relative power of packrat parsing with respect to NSLR(1) is unclear: packrat parsing \nis less restrictive of rightward lookahead, but NSLR(1) can also take leftward context into account. \nIn prac\u00adtice, NSLR(1) is probably more space-ef.cient, but packrat parsing is simpler and cleaner. Other \nrecent scannerless parsers [22, 21] for\u00adsake linear-time deterministic algorithms in favor of more general \nbut slower ambiguity-tolerant CFG parsing. 8 Future Work While the results presented here demonstrate \nthe power and practi\u00adcality of packrat parsing, more experimentation is needed to evalu\u00adate its .exibility, \nperformance, and space consumption on a wider variety of languages. For example, languages that rely \nextensively on parser state, such as C and C++, as well as layout-sensitive lan\u00adguages such as ML and \nHaskell, may prove more dif.cult for a packrat parser to handle ef.ciently. On the other hand, the syntax \nof a practical language is usually designed with a particular parsing technology in mind. For this reason, \nan equally compelling question is what new syntax de\u00adsign possibilities are created by the free unlimited \nlookahead and unrestricted grammar composition capabilities of packrat parsing. Section 3.2 suggested \na few simple extensions that depend on inte\u00adgrated lexical analysis, but packrat parsing may be even \nmore useful in languages with extensible syntax [7] where grammar composi\u00adtion .exibility is important. \nAlthough packrat parsing is simple enough to implement by hand in a lazy functional language, there would \nstill be practical bene\u00ad.t in a grammar compiler along the lines of YACC in the C world or Happy [10] \nand M\u00b4imico [6] in the Haskell world. In addition to the parsing functions themselves, the grammar compiler \ncould automatically generate the static derivations tuple type and the top-level recursive tie-up function, \neliminating the problems of monadic representation discussed in Section 3.3. The compiler could also \nreduce iterative notations such as the popular + and * repetition operators into a low-level grammar \nthat uses only primitive constant-time operations, preserving the linear parse time guarantee. Finally, \nthe compiler could rewrite left-recursive rules to make it easier to express left-associative constructs \nin the grammar. One practical area in which packrat parsing may have dif.culty and warrants further study \nis in parsing interactive streams. For exam\u00adple, the read-eval-print loops in language interpreters often \nexpect the parser to detect at the end of each line whether or not more input is needed to .nish the \ncurrent statement, and this requirement vio\u00adlates the packrat algorithm s assumption that the entire \ninput stream is available up-front. A similar open question is under what condi\u00adtions packrat parsing \nmay be suitable for parsing in.nite streams. 9 Conclusion Packrat parsing is a simple and elegant method \nof converting a backtracking recursive descent parser implemented in a non-strict functional programming \nlanguage into a linear-time parser, without giving up the power of unlimited lookahead. The algorithm \nrelies for its simplicity on the ability of non-strict functional languages to express recursive data \nstructures with complex dependencies di\u00adrectly, and it relies on lazy evaluation for its practical ef.ciency. \nA packrat parser can recognize any language that conventional deter\u00administic linear-time algorithms can \nand many that they can t, pro\u00adviding better composition properties and allowing lexical analysis to be \nintegrated with parsing. The primary limitations of the algo\u00adrithm are that it only supports deterministic \nparsing, and its consid\u00aderable (though asymptotically linear) storage requirements. Acknowledgments I \nwish to thank my advisor Frans Kaashoek, my colleagues Chuck Blake and Russ Cox, and the anonymous reviewers \nfor many help\u00adful comments and suggestions. 10 References [1] Stephen Robert Adams. Modular Grammars \nfor Program\u00adming Language Prototyping. PhD thesis, University of Southampton, 1991. [2] Alfred V. Aho, \nRavi Sethi, and Jeffrey D. Ullman. Compilers: Principles, Techniques, and Tools. Addison-Wesley, 1986. \n[3] Alfred V. Aho and Jeffrey D. Ullman. The Theory of Parsing, Translation and Compiling -Vol. I: Parsing. \nPrentice Hall, Englewood Cliffs, N.J., 1972. [4] Alexander Birman and Jeffrey D. Ullman. Parsing algorithms \nwith backtrack. Information and Control, 23(1):1 34, Aug 1973. [5] Dmitri Bronnikov. Free Yacc-able Java(tm) \ngrammar, 1998. http://home.inreach.com/bronikov/grammars/java.html. [6] Carlos Camar ao and Luc\u00b4ilia \nFigueiredo. A monadic com\u00adbinator compiler compiler. In 5th Brazilian Symposium on Programming Languages, \nCuritiba PR Brazil, May 2001. Universidade Federal do Paran\u00b4a. [7] Luca Cardelli, Florian Matthes, \nand Mart\u00b4in Abadi. Extensible syntax with lexical scoping. Technical Report 121, Digital Systems Research \nCenter, 1994. [8] Jeroen Fokker. Functional parsers. In Advanced Functional Programming, pages 1 23, \n1995. [9] Bryan Ford. Packrat parsing: a practical linear-time algorithm with backtracking. Master s \nthesis, Massachusetts Institute of Technology, Sep 2002. [10] Andy Gill and Simon Marlow. Happy: The \nparser generator for Haskell. http://www.haskell.org/happy. [11] Graham Hutton and Erik Meijer. Monadic \nparsing in Haskell. Journal of Functional Programming, 8(4):437 444, Jul 1998. [12] Lillian Lee. Fast \ncontext-free grammar parsing requires fast boolean matrix multiplication. Journal of the ACM, 2002. To \nappear. [13] Daan Leijen. Parsec, a fast combinator parser. http://www.cs.uu.nl/ daan. [14] Terence J. \nParr and Russell W. Quong. Adding semantic and syntactic predicates to LL(k): pred-LL(k). In Computational \nComplexity, pages 263 277, 1994. [15] Terence J. Parr and Russell W. Quong. ANTLR: A predicated\u00adLL(k) \nparser generator. Software Practice and Experience, 25(7):789 810, 1995. [16] Terence John Parr. Obtaining \npractical variants of LL(k) and LR(k) for k > 1 by splitting the atomic k-tuple. PhD thesis, Purdue University, \nApr 1993. [17] Peter Pepper. LR parsing = grammar transformation + LL parsing: Making LR parsing more \nunderstandable and more ef.cient. Technical Report 99-5, TU Berlin, Apr 1999. [18] Daniel J. Salomon \nand Gordon V. Cormack. Scannerless NSLR(1) parsing of programming languages. In Proceedings of the ACM \nSIGPLAN 89 Conference on Programming Lan\u00adguage Design and Implementation (PLDI), pages 170 178, Jul 1989. \n[19] Kuo-Chung Tai. Noncanonical SLR(1) grammars. ACM Transactions on Programming Languages and Systems, \n1(2):295 320, Oct 1979. [20] Masaru Tomita. Ef.cient parsing for natural language. Kluwer Academic Publishers, \n1985. [21] M.G.J. van den Brand, J. Scheerder, J.J. Vinju, and E. Visser. Disambiguation .lters for scannerless \ngeneralized LR parsers. In Compiler Construction, 2002. [22] Eelco Visser. Scannerless generalized-LR \nparsing. Technical Report P9707, Programming Research Group, University of Amsterdam, 1997. [23] Philip \nWadler. How to replace failure by a list of successes: A method for exception handling, backtracking, \nand pattern matching in lazy functional languages. In Functional Pro\u00adgramming Languages and Computer \nArchitecture, pages 113 128, 1985.   \n\t\t\t", "proc_id": "581478", "abstract": "Packrat parsing is a novel technique for implementing parsers in a lazy functional programming language. A packrat parser provides the power and flexibility of top-down parsing with backtracking and unlimited lookahead, but nevertheless guarantees linear parse time. Any language defined by an LL(<i>k</i>) or LR(<i>k</i>) grammar can be recognized by a packrat parser, in addition to many languages that conventional linear-time algorithms do not support. This additional power simplifies the handling of common syntactic idioms such as the widespread but troublesome longest-match rule, enables the use of sophisticated disambiguation strategies such as syntactic and semantic predicates, provides better grammar composition properties, and allows lexical analysis to be integrated seamlessly into parsing. Yet despite its power, packrat parsing shares the same simplicity and elegance as recursive descent parsing; in fact converting a backtracking recursive descent parser into a linear-time packrat parser often involves only a fairly straightforward structural change. This paper describes packrat parsing informally with emphasis on its use in practical applications, and explores its advantages and disadvantages with respect to the more conventional alternatives.", "authors": [{"name": "Bryan Ford", "author_profile_id": "81100262251", "affiliation": "Massachusetts Institute of Technology, Cambridge, MA", "person_id": "PP14099531", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/581478.581483", "year": "2002", "article_id": "581483", "conference": "ICFP", "title": "Packrat parsing:: simple, powerful, lazy, linear time, functional pearl", "url": "http://dl.acm.org/citation.cfm?id=581483"}