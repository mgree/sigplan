{"article_publication_date": "10-01-1998", "fulltext": "\n Object Lessons Learned from a Distributed System for Remote Building Monitoring and Operation * Frank \nOlken Hans-Am0 Jacobsen+ Chuck McParland Mary Ann Piette Mary F. Anderson Lawrence Berkeley Berkeley, \n Abstract In this paper we describe our experiences with the design, the deployment, and the initial \noperation of a distributed sys- tem for the remote monitoring and operation of multiple het-erogeneous \ncommercial buildings across the Internet from a single control center. Such systems can significantly \nreduce building energy usage. Our system is distinguished by its ability to interface to multiple heterogeneous \nlegacy building Energy Management Control Systems (EMCSs), its use of the Common Object Request Broker \nArchitecture (CORBA) standard communi- cation protocols for the former task, development of a stan- dardized \nnaming system for monitoring points in buildings, the use of a relational DBMS to store and process time \nseries data, automatic time and unit conversion, and a scripted time series visualization system. We \ndescribe our design choices and our experiences in de- velopment and operation. We note requirements \nfor future distributed systems software for interoperability of hetero- geneous real-time data acquisition \nand control systems. *This work was supported by the Office of Energy Research, Office of Computational \nand Technology Sciences, Mathematical, Information, and Computational Sciences Division. of the U.S. \nDepartment of Energy under Contract DE-ACO3-76SF00098. Contact: Frank Olken, Lawrence Berkeley National \nLaboratory, Mailstop 50B-3238, I Cyclotron Road, Berkeley, CA 94720. Tel: (5 10) 486-5891, Fax: (5 10) \n486-4004, Email: olken@Ibl.gov t Current affiliation is Humboldt IJniversity, Berlin, Germany, Email: \njacobsen@wiwi.hu-berlin.de Permlsslon to make dlgltal 0, hard copes of all 0, part of th,s work for personal \n0, class,oom use IS granted wthout fee provided that copses are not made 0, dlstrlbuted for proflt 0, \ncommeraal advan tage and that cop,es bear th,s not,ce and the full c,tat,on on the f,,st page. To copy \notherwse. to republish. to post on se,ve,s o, to redlstrlbute to 11~1s. repu~res pno, specific pe,m,ss,on \nand/o, a fee OOPSLA 98 10198 Vancouver. B.C. $8 1998 ACM l-581 13.005.8/98/0010...$5.00 National Laboratory \nCA 94720 1 Introduction Buildings consume one-third of all energy in the United States at a cost of $200 \nbillion per year, with $85 billion per year for commercial buildings. A large amount, perhaps half of \nthis energy is wasted compared to what is cost-effectively achievable. Much of this waste is related \nto our inability to optimally control and maintain today s complex building systems. Recent building \nperformance case studies suggests that typical savings of about 15%, and as much as 40% of annual energy \nuse can be gained by compiling, analyzing, and acting upon energy end-use data. The largest source of \nsavings arises from turning off equipment when not needed. Other sources of savings include better equipment \nschedul- ing and optimizaton of temperature set point controls. [ 171, [7]. For an example of the building \nenergy efficiency analy- ses which detailed building monitoring enables see the paper by Piette, et al. \n[28]. The applications under development for the Remote Building Monitoring and Operations (RBMO) project \nare based on utilizing the capabilities of, and expanding upon information available through legacy Energy \nManagement Control Systems (EMCSs) which control the HVAC (heat-ing ventilating and air conditioning \nequipment) in commer- cial buildings. Several studies have explored EMCS monitoring capabil- ities (Heinemeier, \nAkbari, and Tseng). EMCSs for com-mercial buildings are readily available. There are over 150 EMCS manufacturers \n[ 141. About 50% of all buildings over 45,000 meter2 (500,000 sq. ft.) have an EMCS, and they are present \nin nearly all large office buildings [ 121. For buildings built since 1992, almost 50% of the floor area \nis in buildings with EMCSs. This project is developing software systems to support remote monitoring \nand control of multiple buildings, i.e., HVAC, lighting, etc., across the Internet, using CORBA -Common \nObject Request Broker Architecture protocols. It is intended to work with heterogeneous EMCS and HVAC \nsystems We are also developing a remote building monitoring and control center which provides data visualization, \ndatabase management, building energy simulation, and energy usage analysis tools (cf. Section 4 for a \nmore detailed presenta- tion of the application.) The remote monitoring facility is comprised of two \ncomponents: A data acquisition and database management server which is built on a Unix system. Several \nanalysis (Unix) workstations, which are used for data visualization, statistical analysis of the data, \nand simulation. The ultimate users of the system will be owner/operators of multiple buildings e.g., \nU.S. GSA (federal buildings), school districts, universities, retail chains, banks, property management \nfirms, ESCOs, utilities, et al. The research project has three primary goals: demonstra- tion of technical \nfeasibility of the proposed design, evalua- tion and experimentation with component software technol- \nogy and their interoperability across heterogeneous environ- ments, and demonstration of the utility \nof the system. Close attention and careful analysis of the data from build- ing EMCS s typically affords \nmany opportunities to improve building operations, resulting in improvements to occupant comfort and \nreductions in energy use. However, for most buildings, such close attention and analysis are not undertaken \ndue to lack of appro-priate software, operator training, staff, and/or inaccu-rate/malfunctioning sensors. \nRemote building monitoring offers the possibility to reduce the labor costs involved in monitoring/analyzingEMCS \ns by spreading such labor costs over a large number of buildings. This will also make it eco- nomically \nfeasible to employ expert HVAC engineers. Figure 1: The RBMO system setup. Use of the Internet, as underlying \ncommunication infras-tructure, has two aspects: use of the Internet communications protocols, and use \nof the actual Internet network. IJse of Internet networking protocols permits access to a wide variety \nof affordable interoperable hardware and soft- ware from many different vendors over many different me-dia. \nInternet markets are huge, in the millions of users, hence vendors can readily amortize development costs. \nIn contrast, proprietary protocols typically entail more expense, because of lack of competition and \nbecause development costs are being amortized over smaller markets. Industry specific pro- tocols, such \nas BACnet, fall in between these two extremes. Internet protocols are clearly the most popular for wide \narea networking applications. Use of the Internet network permits users to share the costs of a common \ncommunications infrastructure. When building automation applications can share existing Inter-net infrastructure, \nsubstantial savings compared to telephone dial-up systems are possible [16]. The Internet also offers \nthe prospect of inexpensive access to high bandwidths when needed - e.g., for remote video surveillance \nfor security, fire, or remote diagnostics. Finally, the Internet offers a reliable, redundantly connected, \nbackbone. The downside of using the Internet is the necessity of more stringent security pre- cautions, \ndiscussed further below.  2 Background 2.1 Energy Management and Control Systems Today s EMCS have \nbeen built primarily for control. EM-CSs are special-purpose computerized control systems, pro- grammed \nto operate building equipment such as chillers, fans, pumps, dampers, valves, motors, and lighting systems. \nEMCSs are installed for many different purposes such as: controlling equipment, alerting operators to \npossible equip- ment malfunction, maintaining comfortable conditions, and permitting modification of \ncontrol specifications. EMCSs typically monitor building operational data such as damper positions, set \npoints, state variables of the work- ing fluids, and equipment status. They are not generally used to \nmonitor electrical demand. In some cases, more accurate sensors may be needed for diagnostics than are \ntypically in-cluded in an EMCS. For example, to track chiller efficiency, a very accurate measurement \nof the temperature drop across and water flow rate through the chiller is needed. The sensors in a typical \nEMCS may not be sufficient for this calculation. We have added additional flow sensors and better temper- \nature sensors to the cooling plant at Soda Hall (but not at Oakland Federal Building) where we are conducting \nsome of our research.  2.2 Building Data Model Standards 3.1 System architecture ASHRAE in 1995 approved \na new communications stan-dard, for building automation systems - The Building Au-tomation and Control \nNetw,ork (BACnet) Standard [ 11, [3]. This standard views the HVAC (Heating, Ventilating and Air Conditioning) \nsystem as a collection of objects. However, the standard does not (yet) include standard applications- \nlevel objects such as chillers or cooling towers. We expect that BACnet will shortly be used to connect \nbuilding gate-ways to EMCSs. The International Alliance for Interoperability [19], [6] is a consortium \nof architectural/ engineering/ construction soft- ware vendors (e.g., Autodesk and Bentley Systems) who \nare working on developing a standard data model for description of buildings. The data model is intended \nto be used by archi- tectural and engineering design tools, construction cost esti- mation software, \nconstruction scheduling software, etc. We are tracking these efforts for use in our naming conventions \nfor monitoring points. 2.3 The Common Object Request Broker Architecture The Common Object Request Broker \nArchitecture (CORBA) is a standard for distributed computing which has been developed by the Object Management \nGroup (OMG) [27], a consortium of independent companies. CORBA aims at providing a uniform communication \ninfrastructure for building distributed applications. It supplies unifying mech-anisms for interoperating \nsoftware components, operating on various hardware platforms, and running under different operating systems \nimplemented in different programming languages. Interoperability is gained by specifying all component \nin- terfaces in a universal interface definition language (IDL). IDL is a declarative lingua-franca for \nspecifying interfaces, following a C++-like syntax, [22].  3 RBMO system design In this section we describe \nthe architecture of theRBM0 sys-tem in further detail, motivate our primary design decisions, and describe \nthe following components of the system: Gateway systems in each building Data acquisition sub-system \nApplications-level object specifications and unit con- version Time series database Application Tier \nDatabase Tier Gateway Tier To proprietary bldg. EMCS Figure 2: RBMO system design The RBMO system architecture \nis shown in Figure 2. It con- stitutes a three tier architecture with the individual compo-nents being: \nthe applications, the database management sys- tem, and the building gateway system. The individual tiers \nare entire sub-systems inter-connected through the Internet via CORBA adaptors. The system has been designed \nin a modular manner to ease the evaluation of alternative technologies for database, user interfaces \nand visualization components (cf. Section 3.5). The following mysubsection will describe the individual \ncomponents and their design in further detail. The RBMO system constitutes a highly heterogeneous system \nincorporating several different hardware platforms and operating systems, ranging from extremely proprietary \nsystems (building EMCSs) to common platforms (Sun work- stations/UNIX and PC/NT). This large heterogeneity \nwas one primary motivation for using CORBA instead alternate protocols, such as OLE/Active X/DCOM, SNMP, \nMMS, and netDDE. protocols. For a more detailed exposition see either our earlier papers [24], [25] or \nour web site [23]. For additional information on MMS see [4] (including a bibliography) and [9], or [13] \nfor utility applications. For information on SNMP see [8], [33], [20], [32], or [35]. For information \non netDDE see [34]. 3.2 Gateway The system architecture requires that each participating building have \nan internet point-of-presence, also referred to as a building gateway . These building gateways are ad- \ndressed in the usual internet fashion (e.g. northTowerBuild- ing.myCompany.com). Within the gateway itself, \nfurther addressing of building-specific objects (more below) is ac- complished by a CORBA-compliant object \nrequest broker (ORB). The gateway ORB acts as a mediator between in- coming CORBA object references and \nthe actual objects res- ident in the gateway process. Currently, building gateways are Pentium-class \nPCs running the Windows NT operating system. One of our goals was to present a network view of a build- \ning s monitoring and control data that was consistent across multiple buildings - regardless of EMCS \nsystem differences. In order to provide this homogeneous view at the building gateway level, it was necessary \nto have building-to-building (i.e. EMCS to EMCS) differences resolved within the gate- way process itself. \nThis has led to a three part gateway design consisting of an EMCS adaptation layer, an internal building \ndata point database, and an external CORBA inter-face layer. The building data point database portion \nof the gateway contains entries for all building monitoring and control data points accessible through \nthe building s EMCS. Each entry contains attributes for units, point name, and description, as well as \nmethods for reading and modifying values. Since this database has to mirror that of the EMCS itself, \nmeth-ods have been developed for deciphering and re-creating data point lists for each vendor-specific \nEMCS encountered. Spe- cific attention has been given to mapping data point units information from vendor \nor installation-specific descriptions to standard representations. Standard units representations are \nrequired for automatic unit system transformation facil-ities located further downstream. Since the logical \nplace to resolve any mapping discrepancies is at the building level, these issues are dealt with by the \ninternal gateway database. The EMCS adaptation layer is responsible for interact- ing with the building \nEMCS via the specific communica- tions formats and protocols required by the EMCS vendor. In some cases, \ne.g., the interfacing to the Barrington EMCS in Soda Hall, this involves use of proprietary message proto- \ncols using standard serial communications media. In others, e.g., interfacing to the Johnson Controls \nEMCS in the Oak- land Federal Building, the netDDE protocol over LAN-based media is used. In all cases, \nspecific message formatting and handling algorithms are encapsulated in methods associated with each \nelement of the internal data points database (see above) and vendor-specific differences are not visible \nabove the EMCS adaptation layer. The external CORBA interface layer provides a common layer of C++ objects \nthat are accessible to the gateway s ORB. These objects have full access to the internal data points \ndatabase and are able, through appropriate overloaded methods, to query the EMCS system for current monitor \nand set point values. Since these objects execute as part of the gateway s ORB process and not as a part \nof the gateway pro- gram itself, synchronization mutexes are provided to assure safe and exclusive access \nto building data object methods and attributes.  3.3 Data Acquisition System Our telemetry data on the \nbuilding state are initially available as cross-sectional data, i.e., we sample the entire building state \nperiodically, typically at l-2 minute intervals. Such high resolution data permits us to study oscillations \nin the control system. However, typical analyses use only a few state variables, but will typically want \nto look at long time series of each such state variable. Thus it is expedient for analysis purposes to \nhave the stored data in a separate time series for each state variable. The question then arises as to \nwhen and where the data are to be transposed from cross-sectional to time series storage formats. This \ncan be done at collection time, analysis time, or some intermediate time. Transposition at collection \ntime slows the collection pro-cess, but speeds the analysis phase. Transposition at analysis time permits \nvery fast data collection, but slows the analy- sis phase. Transposition at intermediate times permits \nboth very fast data collection and fast analysis. However, the soft- ware architecture becomes more complex \nand costly to im- plement and maintain. Transposition can also occur either at the gateway machines, \nthe database server, or the analysis workstations. We chose to transpose the data at the database server \nas it is collected. This will generate at least one disk I/O op- eration per data point monitored for \neach sampling period. This appears to be the simplest architecture to implement, and should have adequate \nperformance for our prototype sys- tem. The data acquisition system (DAQ) presently is a polling system \nresiding on the central server together with the time series DBMS, Placing the DAQ on the TS DB server \nis useful for performance reasons as the DAQ does a lot of I/O to the DB server. Polling (pull architecture) \nwas chosen initially over a push architecture from the gateways due to perceived simplicity of implementation. \nWe anticipate moving to a push archi- tecture, which we expect will offer better reliability against \nsingle point failures of the central system (assuming it has been replicated), easier support for replicated \nor partitioned central DAQ/DBMS systems. Our data acquisition system is multi-threaded, with one DAQ \nprocess serving multiple buildings. This offers per- formance (and software licensing) advantages over \na single threaded, one process per building design, at the expense of reliability. Any failure in the \nDAQ process now causes data acquisition from all buildings to fail. Replication or parti- tioning of \nthe DAQ would be desirable for large scale appli- cations.  3.4 Measurement Units and Time Zones In \ndealing with multiple buildings we were forced to con- front the diversity of the measurement units employed \nfor various monitoring points and EMCSs. There are 3 approaches to dealing with this issue: 1. record \nall measurements as pairs (value, units), 2. record the units for each monitoring point as part of the \nmetadata for the monitoring point, 3. record all measurements in canonical set of units.  Conventional \npractice in EMCS systems is method 2. The IA1 data model adopts method 1. However, the variation in measurement \nunits across monitoring points and EM- CSs render comparisons and computations in the DBMS ex-tremely \nawkward. We have adopted the convention of stor- ing all measurements in standard SI (metric) units in \nthe DBMS (the conversion is done by the data acquisition sys-tem). This expedites the construction of \nindices and com- parisons or computations in query processing. We plan to permit users to specify a preferred \nsystems of units for dis- play of query results. Similar difficulties arise with time zones. There are \ntwo problems -a collection of monitored buildings may span multiple time zones (e.g., a large retail \nchain), and the use of daylight savings time introduces anomalies. We address both problems by converting \nall timestamps to UTC (Universal Coordinated Time (a.k.a. Greenwich Mean Time (GMT)). This is done in \nthe data acquisition system prior to storing data. 3.5 Time Series Database The bulk of the data we \nare collecting will be time series, e.g., of temperatures, power usage, etc. Although the raw data may \nbe irregular time series, we will transform these to regular time series for analysis. Regular time series \nas- sume periodic temporal sampling. Hence, our most impor- tant criterion for selection of the database \nmanagement sys- tem (DBMS) is its suitability for time series data and queries. It is possible to support \ntime series data on any of several DBMS: relational, object-oriented (00) or hybrid object- relational. \nUse of relational DBMS for regular time-series data typically offers mediocre performance, weak support \nfor operations such as smoothing and calendars. Object-oriented DBMSs permit the implementation of time-series \ndata collections with better support for calendars, smooth- ing, etc. To minimize development costs we \ninitially chose an early object-relational DBMS, Illustra (since merged with Informix) with built-in \ntime series support. When this DBMS proved unreliable we switched to a relational DBMS, Oracle. See discussion \nbelow. Rbcently, relational DBMS vendors (e.g., Oracle) have announced built-in time series support. \nMost of the data which are not time series are building description data - i.e., CAD (Computer Aided \nDesign) data describing the physical configuration of the building, HVAC system, and the EMCS system. \nObject-oriented DBMSs (OODBMSs) are generally much better suited to CAD data than relational DBMSs (RDBMSs). \nCORBA interfaces, for at least some of the commercial CORBA implementations, are available for several \nobject- oriented DBMSs, but not yet for most of the relational or object-relational DBMSs. However, Oracle \nhas announced it intends to support CORBA interfaces. Third party CORBA interfaces are available for \nrelational DBMSs [ 181. Support for client-server DBMS configurations is typi- cally available for all \nof the DBMSs. This will allow us to move the analysis and visualization codes to the PC- workstations \nwhile maintaining the database on a robust database server. 3.6 System partitioning We have introduced \nCORBA interfaces at several points in the system. This partitioning serves two key purposes: l distribution \nof components l decoupling for reengineering The CORBA protocols between both the time series database, \nthe graphical user interface, and the session man- agement facility (cf. Figure 2) permit us to vary \nthe distribu- tion of these components depending on the communications bandwidth and client machine performance. \nMoreover, they allow us to better decouple the individual components for reengineering purposes, e.g., \nInternet browser integration of GUI.  4 The RBMO applications soft-ware The RBMO applications are designed \naround providing the following three capabilities: l Archiving historical time series data in a database \n formance analysis 0 Performing a series of regressions ysis techniques to (1) define (2) evaluate energy \nperformance tions and maintenance changes, for historical tracking. The archiving and visualization on \nSun workstations in C++. We env future deployment of the visualizatio applications will primarily be \nPC-ba Windows NT or a web browser. The whole-building and cooling pla Hall consists of a few dozen sensor \npo sensor points in the building) that cove tricity use, weather variables, plus chi tower status, and \nseveral temperatures We added several power and flow sensors to the cooling plant to corn inal EMCS sensors \nand augment in available from the EMCS. At the Oak we collect 120 monitoring points con plant, from a \ntotal of 8,000 points mon We are developing a standard set ing plots to allow users to view archtv friendly \nmenu. An example of the ce the application is depicted in Figur alization capabilities showing some also \nshown. This includes daily (24 minute operating data for all values; w weekly load shapes (hourly data); \nand shapes. The chiller and cooling plan of plots such as power versus cooling and efficiency versus \nload (kW/ton ve The applications are intended for Most of the focus on the graphics i ever, the capability \nexists for the use such as time series of cooling plant cooling tower fan start-stop intervals. vide \na tool for a remote user who mi cations. The menu of standard gr the user to easily view the most formance \ndata, while providing additi detailed analysis of micro-level dat scripted graphs should allow the use \nis the building and cooling plant op To keep the application extensible be performed online, or by prepa \nlanguage developed is simple, we ther discussion. In retrospect, we should ha d statistical anal- n, \nhowever, that a water flow rates. formance track- ata with a user- 1 control menu of time series visu- \nnalysis will consist (kW versus tons) by a remote expert. energy use. How-ook at detailed data r temperatures \nor 1 means for more other words, the nswer the question, developed a script- e omit a detailed tional \nscripting language. For further information on applications see our earlier pa- pers [24], [26].  5 \nDiscussion This section presents a detailed account of our experiences in the design, development, and \noperation of a prototype of the RBMO system. 5.1 Interoperability platform concerns 5.1.l Event notification \ncapabilities Central to any remote monitoring and control system is some mechanism for the remote sites \nto notify the monitor- ing sites of interesting events. Such event notifications need to be asynchronous, \npersistent, and (often) multicast. Ideally, events should be typed (so that certain processes can listen \nfor specific types of events). Event notification services address the need to propa-gate notification \nof significant occurrences to relevant system components. Depending on system functional requirements, \nthis notification may be considered unreliable (e.g. periodic monitoring value updates) or may be required \nto be reliable (e.g. equipment status changes). In a distributed environ-ment where system components \ncan fail or become unreach- able, the technical requirements for implementing these ser- vices can be \nsubstantial. Although low level event delivery protocols are defined in CORBA, the semantics of describing \nand specifying higher level event service behavior remains (thus far) vendor- specific. Event service \nreliability and delivery persistence capabilities are not yet consistently defined and imple-mented. \nIt is therefore important to identify and describe a significant subset of high level event service functionality. \nThese event service functions should be sufficiently well de- scribed to allow multiple implementations \nthat exhibit inter-operable behavior. Some of these issues are being addressed in recent OMG proposals \non improved Event Notification Services. 5.1.2 Security and privacy The principal security requirement \nfor RBMO is to pre- clude unauthorized persons from altering the programming of the HVAC equipment. Aside \nfrom disrupting the activities of building occupants, it is possible to damage HVAC equip- ment with \ninappropriate (or malicious) commands. Hence, some scheme for secure authentication and access control \nis required. The absence of such facilities in CORBA imple-mentations at the beginning of the project \ncaused us to defer Figure 3: Graphical user interface and time series plots of RBMO application. control \napplications. In addition, some sites have indicated a desire for privacy of HVAC utilization data (e.g., \noccupancy data), and hence asked for encryption of all communications. One of our major concerns has \nbeen the interoperability of the security services. We have been unable as yet to identify interoperable \nCORBA security services software. Further-more, we would have liked to be able to construct interop-erable \nsoftware composed of different services (e.g., event services, security services, naming services, et \ncetera) from different vendors. We were unable to do so. 5.1.3 Performance Our experience with CORBA \nwas that the RPC facilities were too slow to permit the modeling of individual measure-ment points as \nseparate objects. It proved necessary to aggre- gate many of these measurement points together into named \ncollections (called scan lists ) and to query the groups of points. In database terms these constituted \npre-compiled queries. This dramatically reduced the number of CORBA invocations (messages) and improved \nperformance. Such ag- gregation of messages appears to be a commonplace strat-egy for improving the performance \nof distributed systems. However, it can be tedious and error prone to implement and maintain. It forces \nimplementors to mix logical and physical design issues. While faster ORBS would certainly help, we believe \nthat network latency in wide area networks will ul-timately constratin performance of fine grained monitoring \nsystems. Many distributed systems have found it efficient to aggregate messages. Automatic (or convenient \nsemi-automatic) tools for aggre- gating individual monitoring points into larger scan objects are very \ndesirable. Such tools are also needed to support multicast publish/subscribe communication protocols. \nSuch tools would permit a clean separation of logical and physical design considerations in the design \nand implementation of distributed systems. For latency and throughput measurements of several CORBA ORB \nimplementations (albeit over much higher speed networks) see the work of Schmidt, et al. in [15] and \n[29]. Brose [2] also reports recent performance numbers across a high speed local area network, indicating \nthat com- mercial ORBS require about 2 milliseconds for a roundtrip no-op. Note that many remote building \nmonitoring appli-cations require operation across wide area networks (with longer latency and lower bandwith \nthan local area networks). Part of our performance problems arose from a desire to permit run-time binding \nof the schema of the monitoring points. Such late binding was perceived as necessary to ac- commodate \nchanges in the HVAC configuration (e.g., due to maintenance, rennovation, etc.) without requiring recompi- \nlation of the RBMO code. We could have used CORBA s dynamic invocation capabilities -however these were \nper- ceived as too slow for large scale RBMO performance re-quirements. Finally, in this application \nand other related applications such as electric power grid monitoring there is often a need to deal with \na great many objects/points being monitored. It is our understanding that current ORBS maintain in-memory \nobject tables (several tens of bytes per object) for all objects in the system. Such an approach has \nobvious problems when scaling to very large collections of objects. It is our view that some sort of \nautomatic migration to secondary storage (and related pointer swizzling) similar to that in object-oriented \ndatabases will be needed to properly support very large ap- plications.  5.2 Legacy system integration \nissues In the integration of heterogeneous building EMCS into a seamless global distributed system we \nhave made the follow- ing observations. Wrapping an legacy system with object can hide essential system \nfeatures that are critical to both ro i ustness and oper- ation. Even if one is able to hide most of \nthe system s dis- tributed nature by use of distributed object technologies, the underlying system may \nstill consist of seoerate components that were designed as stand-alone systems with no expecta- tion \nof co-operating with, or being controlled from, outside processes. Centralized input and maintenance \nof configuration and description data is critical. This feature was mostly discov- ered by its absence. \nAlthough EMCS configuration informa-tion may be naturally maintained by thatisub-system, these systems \nare designed as single, stand-alone entities. While it may be possible to create automatic me hanisms \nfor incor- porating configuration change informatio %from one compo- nent (e.g., EMCS) into other systems \n(e.gl., building simula-tion), there are typically no mechanisms to effectively sig-nal participating \ncomponents that such changes have been made. The result is that, although the co lection of individ- \nual systems into a single larger one has 4een accomplished via distributed 00 techniques (see above], \nthe new, larger system depends on a higher, heuristically riven mechanism (e.g. system manager) for both \nco-ordinat r on and correctness of operation. The ability to seamlessly interact with Iobjects within \nthis new larger system can lead one to believe that the under- lying components are cooperating in a \nanner that yields synchronized, correct operation. Since ach of the respec- tive components (EMCS, automated \ndat loggers, etc.) are typically designed as stand-alone syste s, they often lack any mechanism for synchronization \nor ex lusive control path il lockout. Therefore, although it may be possible for our sys- tem to have \na component that tracks chiller performance, fac- tors in near-term weather effects, and ch nges cooling \nplant operating setpoints, there is no way to %nsure that such a component has exclusive control of all \nrelevent system com- ponents. A control layer can be inserted into the architecture; but, it s connection \nto all components mny prove problem- atic. We believe that part of these issues m y be addressed by having \nstronger support for event speci cation, event noti- fication, and a system design based on t is. We \nenvision a lose coupling of stand-alone components sending and recev- r ing events. For a more complete \ndiscussion on event service support see our discussion above. 5.3 Semantic heterogeneity and data management \n5.3.1 Naming and directory services Effective configuration description and control is the bane of distributed \nsystems. Distributed systems are constructed, in part, by binding together objects distributed over many \nplatforms. Without a mechanism for describing these bind- ings, the resulting system lacks flexibility \nand it s complexity remains hidden to all but the original implementors. Naming services provide a way \nto bind meaningful human names to object instances. These problems take on greater importance in real-time \ncontrol environments where systems must accommodate equipment replacement and/or upgrade cycles as well \nas live system reconfiguration. Stable and ubiquitous naming and directory services are a key tool in \nthe design of systems that will accommodate these changes. Examples include the CORBA naming services \nand LDAP Directory services. The function of these services goes well beyond the nuts and bolts issues \nof publishing specific object names and at- tributes. An effective distributed naming and directory ser-vice \ncan expose, through the use of multiple hierarchical views, objects aggregated by function, location, \ncontrol sub- system, etc. In addition, these services can provide a sin- gle, ubiquitous repository for \nobject meta information. In the case of control systems, this can include information about sensors and \ninstrumentation used in generating object method values, resolution of object name aliases due to dif- \nfering local and supervisory object namespaces, and provide automated mechanisms for locating physical \nand/or logical control system components. 5.3.2 Mediation Services While standards efforts offer some \nhope in the long run for reconciling semantic heterogeneity among various network devices and controllers \n(HVAC or electric power grid), the existence of many incompatible legacy systems requires that some facilities \nfor mapping between the global schema and local schemas be provided. At present, e.g., in RBMO (and other \nsystems) this is often done in an ad hoc fashion with hand-crafted mapping tables. Such facilities are \nvariously referred to as mediators (partial mappings, late binding) or schema integration tools (nearly \ncomplete mappings, early binding). Such tools will require access to metadata (e.g. DB or object schemas). \nExamples of related work include OMG Object Interface Repository, and Meta Object Facilities. 5.3.3 \nStatic vs. dynamic schemas Traditional database designs specify complete detailed schemas statically. \nSuch static schemas facilitate systematic design, query optimization, etc. However, for many moni- toring \nand control applications (shipped as shrink wrap soft- ware), it is desirable to be able to alter the \nschema (e.g., to accommodate new types of equipment) without rebuild-ing the DB or recompiling the code. \nExamples of such approaches include: frame-based knowledge representation systems, and the dynamic invocation \ninterface of CORBA, together with interface repositories. Because such dynamic schema capabilities are \ntypically significantly slower than static interfaces, some groups have built hybrid schemas - ex- amples \ninclude the International Alliance for Interoperabil- ity (building data model) which provide both static \nschemas (e.g., for geometry) and dynamic schemas (frames) for ex- tensibility. Conceivably, on-the-fly \ncompilation techniques might provide a reasonable performance. 5.3.4 Units conversion services We found \nit necessary to provide measurement unit con-version capabilities for our RBMO project. Unit conversions \nhave been a chronic source of errors in data acquisition/management systems. The ubiquity of these issues \n(in both industrial measurement and commercial trad- ing) suggests their provision as standard services. \nIdeally, one would like an extensible type systems which incorpo-rated dimensionality and automatic units \nconversions. How- ever, impure unit conversions (e.g., between mass and vol- ume) are commonplace and \ndifficult to deal with systemati- cally. Our choice was to store all measurements in canoni-cal units \nin the DBMS. While this facilitated indexing and querying, it made it very difficult to diagnose errors \nin the data acquisition system. 5.3.5 Time Zones and Synchronization For large scale monitoring applications \nwe believe that recording all times in UTC (Universal Coordinated Time) is essential. Conversion routines \nto local time zones, including daylight savings time are essential. Some protocol for synchronizing the \nclocks is needed for monitoring systems. Such protocols, e.g., NTP [5], have been developed For other \ndistributed applications. We note that synchronization requirements for monitoring of electric power \ngrids are sufficiently stringent (to measure phase dif- ferences among geographically distributed generators) \nas to lead to the use of GPS timing signals for synchronization. 5.3.6 Time series DBMS Our first implementation \nused the early release of the Il- lustra DBMS product, chosen because it offered facilities for storing \nand querying time series data, including calendars. We encountered reliability problems with the DB server. \nWe therefore switched to storing our time series data in a more mature conventional relational DBMS, \ni.e., Oracle Version 7.3 (at present). This has solved our server reliability prob-lems. Whereas Illustra \nprovided direct support for storing and querying time series data, in Oracle we have had to construct \nthese facilities atop a relational system. We have not (yet) replicated the calendar facilities, etc. \nNote that Oracle has announced time series facilities similar to Illustra/Informix. Whereas Illustra \nstorage usage was asymptotically bytes/data value (for real numbers), Oracle is presently con- suming \napproximately 25 bytes/data value, because each time series data value has become a full tuple in the \nDBMS. For additional research papers concerned with time se-ries database management systems and data \nmodels see the works [lo], [31], [30], [ 111. 5.3.7 Data Quality Assurance We greatly underestimated \nthe difficulty and effort re-quired to assure proper data quality. We have encoountered a variety of \nproblems with respect to data quality - some due to errors in units conversion, some due to missing or \nerro- neous documentation of system configuration files, some due to overflow problems with small (16 \nbit) counters. We should have paid much more attention to the develop- ment of filtering software in \nthe data acquisition system. The presence of dirty data in the time series database has been a barrier \nto the use of conventional database query facilities for aggregation. Dirty data is a ubiquitous problem \nin all sorts of data warehousing applications. Obviously, the preferred approach is to clean up the data \nat the source. If this is not feasbile, concise, powerful methods are needed to specify constraints on \ndata validity.  5.4 System operation A prototype of the system was operational for over one year. Data \nwas acquired from two remote sites, Soda Hall and Oakland Federal Building. The central data storage \nand management site is located at Lawrence Berkeley National Laboratory. We have collected approximately \none year of data from Soda Hall (plus one year of historical data). We have at present approximately \n2 months of data from Oakland Fed- eral Building. We are presently collecting several dozen monitoring \npoints for both Soda Hall and Oakland Federal Building. Sampling rates for both buildings are once per \nminute. Basic data rate is presently approximately 1 KB per minute. Soda Hall Soda Hall, a 109,000 ft2 \nComputer Science building was selected as our case-study site for several reasons. The build- ing was \nequipped with an Energy Management Control Sys- tem that the research team had worked with before. Second, \nthe building was located near LBNL, permitting easy access. Finally, the building has two, 220 ton screw \nchillers, for a total of 440 tons of cooling plant capacity (or 248 sqft/ton). Early in the project we \ndecided to focus on chillers, which are the largest single energy-using component in buildings with central \ncooling plants. The building has served as the subject of several research tasks that involved commission-ing \nand performance. Results from this work are reported in [28], and [21] A total of 46 points were monitored \nfor about two years, start- ing in September, 1995 during the first few months of inital occupancy. Oakland \nFederal Building A total of 107 points were monitored at the Oakland Fed- eral Building. (Note, not all \nof these points are sampled, many are computationally derived from sampled points.) This building is \na 1.1 M sqft twin-tower, 18-story office building complex. The building was constructed in 1989 and houses \nFederal agencies such as the IRS, US District Courts, the US DOE, and Department of Veteran Affairs. \nThe build- ing is served by a central plant with five chillers, three at 1000 tons and two at 450 tons \ntotaling 3900 tons of capacity. This is 280 sqft/ton. The 107 points cover systems similar to those at \nSoda Hall, including whole-building power, chiller power and tons, plus cooling tower, and pump data. \nAdditional points include heating loop flow, a series of outside temperatures from 5 air handlers, and \nseveral internal zone temperatures. One of the interesting questions we are examining at both sites is \nchiller capacities and over sizing. The operators re- port that the last 1000 tons of cooling is not \nneeded, even in the warmest weather.  Related work An extensive discussion of related work can be \nfound in our earlier paper, [24]. Honeywell recently announced a new building monitor-ing system, Atrium, \n(Honeywell, 1998) quite similar to our project. Atrium employs a different operating system and window \nsystem (Windows NT), different relational database management system (SQLserver). However, it has a very \nsimilar overall architecture. They have also chosen to use a (popular) proprietary distributed object \nmanagement sys- tem, Active X/DCOM, in contrast to our use of the standard CORBA distributed object system. \nAtrium also employs an standard building automation communications protocol, BACnet, to communicate with \nthe building EMCS systems. BACnet is a recently deployed standard. BACnet equipped EMCSs were not available \nto us when we developed RBMO. Other major differences between RBMO and Atrium lie in the time series \ndatabase. Honeywell has chosen to only store changed data values (discarded repeated measures within \ndead bands). This results in a smaller, but irregular time series. Honeywell has also implemented a simple \ntime se- ries query language and temporal join operator to query and extract align multiple (irregular) \ntime series. The Enflex System from CT1 Ltd. (CTI, 1998) employs proprietary applications protocols atop \nTCP/IP. Finally, readers should be aware that remote building man-agement presents many of the same issues \naddressed in the management of communications networks. Network man-agement systems have employed SNMP \nand RMON as the principal (standard) communications protocols. See discus- sion above under alternativ? \nprotocols and citations there. 7 Future work We plan to investigate the use of the CORBA Security Service \nand Secure IIOP for remote control the HVAC op-erations, e.g., authentication. Remote monitoring of large \nheterogeneous collections of buildings poses serious problems of software distribution to the buildings. \nWe plan to study static and dynamic software distribution over the Internet onto heterogeneous target \nplat- forms. 8 Conclusions We have described the design and development of the re- mote building monitoring \nand operation system, which em-ploys the CORBA protocol across the Internet for: com-munications between \nremote buildings and a timeseries database, and to distribute individual upplication compo-nents locally \nacross different machines. We have discussed design tradeoffs, our development and operational experi- \nences, and related work. We have argued for the use of CORBA within our project context. We have discussed \nthe design of a distributed sys-tem that has to monitor events across large distances, across multiple \ntime zones, heterogeneous sources and diverse mea- surement units. The development of the time series \ndatabase atop different commercial DBMSs has been described. We found it necessary (for performance reasons) \nto ag- gregate individual monitoring points into larger objects for CORBA-based retrieval. Such aggregation \nis typical of many distributed applications. None of the commercial DBMS available at the time of the \nproject proved entirely satisfactory for storing and retrieving time series. Recently announced products \nmay remedy this. The most serious issues were dealing with the heteroge- niety of the various building \nEMCSs, i.e., units and naming, and problems with dirty and missing data. Developers of distributed monitoring \nsystems for legacy systems would do well to address such issues thoroughly early in their projects. This \nCORBA-based approach is both feasible and com- petitive with other systems based on proprietary communi-cations \nprotocols. The attraction of our approach lies in the extensive availability of commercial software products \nwhich support distributed computing applications based on the CORBA model and underlying Internet communications \nprotocols.  Acknowledgements Our collaborators include: Kristin Heinemeier (Honeywell), Sarah Shirazi, \nRomeo Cruz, and James Patterson (UC Berke- ley Facilities Management), Giles Adi and William Curran (Barrington \nSystems), Sue Williams, Diana Williams, and Ed Grey (at Oakland Federal Bldg). Prof. Carlo Sequin (UCB) \narranged access to Soda Hall. Eleanor Lee (LBNL) assisted us in gaining access to Oakland Federal Building. \nVladimir Bezjanac (LBNL) has kept us abreast of devel- opments with the IA1 data model. Ed Wong wrote \nthe time series DB code. Brian Smith wrote the first version of the graphics code. Alex Gitelman wrote \nthe second version of the graphics application. Brian O Clair worked on gateway code. David Gould wrote \nthe unit conversion code. Graham Carter and Kris Kinney worked on data analysis and moni- toring point \nselection. For further information see the web page: http://www.Ibl.gov/-olken/RBO/rbo.html References \n111 ASHRAE. Bacnet - a data communication protocol for building automation and control networks. Technical \nReport ANSYASHRAE Standard 1351995, American Society of Heating, Refrigerating and Air-conditioning Engineers, \n1995. URL: http://www.ashrae.org/-BOOWbacnetcd.htm. PI Gerald Brose. Jacorb performance. Technical report, \nFree Unviversity, 1998. http://www.inf.fu-berlin.de/--brose/jacorb/performance/results_09.html. [31 J.F. \nButler. Bacnet: An object-oriented network pro- tocol for distributed control and monitoring. Tech-nical \nreport, Cimetrics Technology, 1996. URL: http://www.cimetrics.com/tipcic96.htm. [41 Pierre Castori. Manufacturing \nmessaging specification. Technical report, Ecole polytechnique federale de Lau- sanne (EPFL), Institute \nfor Computer Communications and Applications, 1997. URL: http://litpc13.epfl.ch/-MMS/mms-main.htm. [51 \nNorman Chang et al. Time synch -time synchroniza- tion server. Technical report, Univ. of Delaware, 1997. \nURL: http://www.eecis.udel.edul-ntp/. [61 IA1 UK Chapter. United kingdom chaper of industry al- liance \nfor interoperability home page. Technical report, UK Chapter of Industry Alliance for Interoperability, \n1996. URL: http://helios.bre.co.uk/iai/. [71 D. Claridge, J. Haberl, M. Liu, J. Houcek, and A. Athar. \nCan you achieve 150% of predicted retrofit savings? is time for recommissioning? In Proceedings from \nthe ACEEE 1994 Summer Study on Energy EsJiciency in Buildings, volume 5. American Council for an Energy- \nEfficient Economy, 1994. VI Yoram Cohen. SNMP -the simple network manage- ment protocol. Technical report, \nRAD Data Commu- nications, 1995. URL: http://www.rad.co.il/networks/- 1995/snmp/snmp.html. [91 ESPRIT \nCCE-CNMA Consortium. MMS: A Communi- cation Language for Manufacturing. Springer-Verlag, 1995. ISBN 3-540-59061-7. \n 1101 Werner Dreyer, Angelika Kotz Dittrich, and Duri Schmidt. Research perspectives for time series \nman- agement systems. SIGMOD Record (ACM Special In- terest Group on Management of Data), 23(1):10-15, \nMarch 1994. [Ill Werner Dreyer, Angelika Kotz Dittrich, and Duri Schmidt. Using the calanda time series \nmanagement system. In Michael J. Carey and Donovan A. Schnei- der, editors, Proceedings of the 1995 ACM \nSIGMOD Internatioinal Conference on the Management of Data, volume 24, no. 2 of SIGMOD Record, page 489,1995. \n[W EIA. Annual Energy Review for 1991. Energy Infor- mation Administration, U.S. DOE., 1992. 1131 EPRI. \nUtility communications architecture. Technical report, Electric Power Research Institute, 1997. URL: \nhttp://www.epri.comluca/index.html. [I41 EUN. Eun product guides: Building automation sys- tems. Energy \nUser News Magazine, pages 50-5 1, Oc- tober 1994. A. Gokhale and D.C. Schmidt. Evaluating the perfor- \nmance of demultiplexing strategies for real-time corba. In Proceedings of the GLOBECOM 97 Conference, \nNovember 1997. [I61 Jeff Haberl, 1996. Personal Communication. [I71 P. Herzog and L. LaVine. Identification \nand quan- tification of the impact of improper operation of mid- size minnesota office buildings on energy \nuse, a seven building case study. In Proceedings from the ACEEE 1992 Summer Study on Energy Eficiency \nin Buildings, volume 3. American Council for an Energy-Efficient Economy, 1992. 1181 I-Kinetics. I-kinetics \nhome page. Technical re-port, I-Kinetics Corp., 1997. URL: http://www.i-kinetics.com/. [I91 IAI. Industry \nalliance for interoperability home page. Technical report, Industry Alliance for Interoperability, 1996. \nURL: http://iaiweb.lbl.gov/. WI IETF SNMPv3 Working Group. SNMP Ver-sion 3 Working Group Home Page. Technical \nreport, Internet Engineering Task Force, 1997. URL: http://www.ietf.org/html.charters/snmpv3-charter.html. \n[211 S.R. Meyers. Chiller modeling error analysis: Implica- tions for energy-saving retrofits and control \nstrategies. Technical report, Mechanical Engineering Department, U. C. Berkeley, 1996. Master s Thesis. \n WI Thomas J. Mowbray and R Zahavi. The Essential CORBA, Systems lntegration using Distributed Ob-jects. \nJohn Wiley &#38; Sons, 1995. [231 Frank Olken. Remote building monitoring and operations project home \npage. Technical re-port, Lawrence Berkeley National Laboratory, 1997. http://www.lbl.gov/-olken/RBO/rbo.html. \nu41 Frank Olken et al. Remote building monitoring and control. In Proceedings of the ACEEE Summer Study \nConference, volume 4, pages 285-296, August 25-3 1 1996. URL: http://www.lbl.gov/-olken/mendel/RBO/- \npubslaceee96.ps. P51 Frank Olken, H.-Arno Jacobsen, and Chuck McPar- land. Middleware requirements for \nremote monitoring and control. In Proceedings of the OMG-DARPA-MCC Workshop on Compositional So&#38;are \nArchitectures, January 6-8 1998. http://www.objs.comlworkshops/-ws980 I/index.html. 1261 Frank Olken, \nHans-Arno Jacobsen, Chuck McParland, Mary Ann Piette, and Mary F. Anderson. Development of a remote building \nmonitoring system. In Proceed- ings of the 1998 ACEEE Summer Study on Energy Ef-ficiency in Buildings. \nAmerican Council for an Energy Efficient Economy, August 1998. ~271 OMG. Object management group home \npage. Tech- nical report, Object Management Group, 1996. URL: http:J/www.omg.org/. WI Mary Ann Piette, \nGraham Carter, Steve Meyers, Os-man Sezgen, and Steve Selkowitz. Model-based chiller energy tracking \nfor performance assurance at a univer- sity building. In Proceedings of the Cool Sense Na- tional Forum \non Integrated Chiller Retrojits, Sept. 23 - 24 1997. LBNL Report-4078 1. ~91 DC. Schmidt, A. Gokhale, \nT.H. Harrison, and G. Parulkar. A high-performance endsystem architec- ture for real-time corba. IEEE \nCommunications Maga- zine, 14(2), February 1997. 1301 Duri Schmidt, Werner Dreyer, Angelika Kotz Dittrich, \nand Robert Marti. Time series, a neglected topic in tem- poral database research? In James Clifford and \nAlexan- der Tuzhilin, editors, Recent Advances in Temporal Databases, Proceedings of the International \nWorkshop on Temporal Databases, Workshops in Computing, pages 214-232. Springer Verlag, September 1995. \n[311 A. Segev and R. Chandra. A data model for time-series analysis. In Nabil R. Adam and Bharat K. Bhargava, \neditors, Advanced Database Systems, volume 759 of Lecture Notes in Computer Science, pages 191-2 12. \nSpringer Verlag, 1993. WI SimpleGroup. The simpleweb. Technical report, Centre for Telematics and Information \nTechnology (CTIT) of the University of Twente (UT), 1997. URL: http://wwwsnmp.cs.utwente.nl/. [331 SNMP \nResearch International, Inc. SNMP Re-search International Web Page. Technical report, SNMP Research International, \nInc, 1997. URL: http://www.int.snmp.com/. [341 Wonderware. Common questions about netdde. Tech- nical \nreport, Wonderware, Corp., 1996. URL: http://www.wonderware.com/products/netddeq2.html. [351 Yahoo. Snmp \n(simple network management pro- tocol). Technical report, Yahoo, 1997. URL: http://pbm.yahoo.com/Computersand-Internet/-Information-andDocumentation/ProtocoIs/SNMP--Simple-Network--Management-Protocolf. \n \n\t\t\t", "proc_id": "286936", "abstract": "In this paper we describe our experiences with the design, the deployment, and the initial operation of a distributed system for the remote monitoring and operation of multiple heterogeneous commercial buildings across the Internet from a single control center. Such systems can significantly reduce building energy usage.Our system is distinguished by its ability to interface to multiple heterogeneous legacy building Energy Management Control Systems (EMCSs), its use of the Common Object Request Broker Architecture (CORBA) standard communication protocols for the former task, development of a standardized naming system for monitoring points in buildings, the use of a relational DBMS to store and process time series data, automatic time and unit conversion, and a scripted time series visualization system.We describe our design choices and our experiences in development and operation. We note requirements for future distributed systems software for interoperability of heterogeneous real-time data acquisition and control systems.", "authors": [{"name": "Frank Olken", "author_profile_id": "81100186019", "affiliation": "Lawrence Berkeley National Laboratory, Berkeley, CA", "person_id": "PP43117333", "email_address": "", "orcid_id": ""}, {"name": "Hans-Arno Jacobsen", "author_profile_id": "81452606405", "affiliation": "Lawrence Berkeley National Laboratory, Berkeley, CA", "person_id": "PP15035345", "email_address": "", "orcid_id": ""}, {"name": "Chuck McParland", "author_profile_id": "81100438252", "affiliation": "Lawrence Berkeley National Laboratory, Berkeley, CA", "person_id": "P48220", "email_address": "", "orcid_id": ""}, {"name": "Mary Ann Piette", "author_profile_id": "81100300835", "affiliation": "Lawrence Berkeley National Laboratory, Berkeley, CA", "person_id": "P193397", "email_address": "", "orcid_id": ""}, {"name": "Mary F. Anderson", "author_profile_id": "81100101272", "affiliation": "Lawrence Berkeley National Laboratory, Berkeley, CA", "person_id": "P192958", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/286936.286968", "year": "1998", "article_id": "286968", "conference": "OOPSLA", "title": "Object lessons learned from a distributed system for remote building monitoring and operation", "url": "http://dl.acm.org/citation.cfm?id=286968"}