{"article_publication_date": "10-01-1998", "fulltext": "\n Vclusters: A Flexible, Fine-Grained Object Clustering Mechanism* Mark L. McAuliffe TimesTen Performance \nSoftware Michael J. Carey IBM Almaden Research Center Marvin H. Solomon Computer Sciences Department \n2085 Landings Drive Mountain View, CA 94043 650 Harry Road, K55/Bl San Jose, CA 95120 University of Wisconsin-Madison \nMadison, WI 53706 mlm@timesten.com careyQalmaden.ibm.com solomonOcs.wisc.edu Abstract We consider the \nproblem of delivering an effective fine-grained clustering tool to implementors and users of object- \noriented database systems. This work emphasizes on-line clustering mechanisms, as contrasted with earlier \nwork that concentrates on clustering policies (deciding which objects should be near each other). Existing \non-line clustering meth-ods can be ineffective and/or difficult to use and may lead to poor space utilization \non disk and in the disk block cache, particularly for small- to medium-size groups of objects. We introduce \nvariable-size clusters ( Vclusters), a fine-grained object clustering architecture that can be used directly \nor as the target of an automatic clustering algorithm. We describe an implementation of Vclusters in \nthe Shore OODBMS and present experimental results that show that Vclusters sig-nificantly outperform \nother mechanisms commonly found in object database systems (fixed-size clusters and near hints). Vclusters \ndeliver excellent clustering and space utilization with only a modest cost for maintaining clustering \nduring updates. 1 Introduction Clustering has long been recognized as one of the most ef-fectivc ways \nto improve database system performance. The goal of clustering is to place related data objects near \none another in physical storage in order to minimize the num-ber of I/O operations needed for common \ndatabase oper-ations. Good clustering has two components: a policy for grouping related objects, and \na mechanism for storing these groups and maintaining their integrity over time. The de-sign of object \nclustering policies has been a popular topic for object-oriented database research, but the development \nof general-purpose clustering mechanisms has received rel-atively little attention. The coarse-grained \nproblem of al- locating disk pages (or extents) near one another has been thoroughly studied in the file \nsystem community and is con- sidered a solved problem [MJLF84], but the problem of ef- *This research \nwas sponsored by the Advanced Research Project .4genry, ARPA order number 018 (formerly S230), monitored \nby the U.S. Army Research Laboratory under contract DAAB07-91-C-Q518. Pwmlsslon to make dlg!tal or hard \ncopses of all or part of this work ior personal or classroom use IS granted wIthout fee provided that \ndupes are not made or dlstrlbuted for protlt or commercial advan~ tage and that cup~es hear this not~ce \nand the full c~tat~on on the tlrst page To copy otherwsc, lo republish, to post on servers or 10 redastrfhute \nto 11~s. requwes pr,or specltlc perrmss~on and/or a tee OOPSLA 98 10198 Vancouver, 13.C (( 1998 ACM l-581 \n13.005.8/98/0010.. 55.00 fectively clustering small data objects together, which we will refer to as \nthe fine-grained object clustering problem, seems to have received little attention in either the file \nsys- tem or object database communities. This is particularly true for small-to medium-sized groups of \nobjects, and es-pecially in the face of size-changing updates to objects and their clusters. Despite \ntheir popularity as a research topic, clustering algorithms have seen little technology transfer. A large \nnum-ber of automatic clustering algorithms exist in the research literature, but we are aware of none \nthat has been imple-mented yet in a commercial product. In contrast, most object-oriented database systems, \nboth commercial and ex-perimental, offer some mechanism for clustering. However, the mechanisms provided \ncan be awkward to use and, as we shall show, are not particularly effective. We believe that a first \nstep in delivering effective clustering to commercial database systems is the development of a general-purpose \nclustering mechanism. Therefore, in this research we con-centrate on the mechanism side of the problem. \nWe assume that the policy is given either by hand or as the output of some automatic clustering algorithm. \nOur design is directed by the following goals: Effectiveness: The mechanism should deliver good clus-tering \nacross a wide variety of workloads. Low cost: The cost for using and maintaining clusters should be minimal. \nEase of use: Use of the mechanism should be straightfor- ward, and should not require a priori knowledge \nof, or limits on, the sizes of clusters. Applicability: The mechanism should be usable either as the \nback-end of an automatic clustering algorithm or directly by application programmers. Flexibility: The \nmechanism should support clusters of vary- ing sizes, ranging all the way from fine-grained (less than \none page, up to a few pages) to large (multi-megabyte) clusters. Support for incremental cluster maintenance: \nThe mechanism must gracefully maintain clusters as data items are created or deleted, or as they change \nin size. Good space utilization: The mechanism should not waste excessive amounts of disk space. The \nrest of this paper is organized as follows. Section 2 surveys previous work in object clustering and \nconcludes that existing mechanisms fall into two categories: near hints and fixed-size clusters. Section \n3 introduces a new fine-grained object clustering mechanism that we call Vclus-ters, for variable-sized \nclusters, and Section 4 describes two algorithms for implementing them. Sections 5 and 6 re-port on our \nexperiences with Vclusters in the context of Shore: Section 5 describes our implementation of Vclus-ters \nin Shore, and Section 6 presents selected results from a performance study comparing the performance \nof Vclusters with near hints, fixed-size clusters, and no clustering at all. These results show that \nnear hints are scarcely better than no clustering, and that fixed-size clusters impose a penalty in disk-space \nutilization that adversely affects throughput as well. Vclusters are shown to add a modest overhead to \npop- ulating and updating the database, but this investment is shown to be quickly repaid through improved \nperformance for clustered traversals. We also show that Vclusters do a better job than the other methods \nin terms of preserving object clustering across updates. Section 7 summarizes our work. 2 Related Work \nThe two largest categories of object clustering research are automatic clustering algorithms and special-purpose \nclus-tering mechanisms. Due to space limitations, we discuss related research only very briefly here. \nFor a more thorough survey of work in the area of object clustering, we refer the reader to Bertino et \nal [BSI94]. 2.1 Automatic Clustering Algorithms Automatic clustering algorithms focus on policy more \nthan mechanism, and they are therefore only tangentially related to our work. They might be more aptly \ncalled reclustering algorithms, because their focus is usually on reorganizing an existing object database \nto match common access pat- terns. A common approach for these algorithms is to allow database clustering \nto degrade over time and then to use pe- riodic reclustering to restore performance. In contrast, our \nwork focuses on supporting good initial placement followed by effective incremental maintenance of clusters, \nin hopes of making such reorganization unnecessary except in the event of changing access patterns. Examples \nof automatic object clustering algorithms include the work by Hudson and King [HK89], Tsangaris and Naughton \n[TN91, TN92], McIver and King [MK94], and Gerlhof et al [GKKM94]. 2.2 Special-Purpose Clustering Mechanisms \nSeveral clustering studies have considered clustering mech-anisms that are designed to work in a particular \ncontext. Several of these methods are designed to work with spe-cific kinds of data structures. For example, \nBanerjee et al [BKKG88] describe a method for clustering DAGs. As another such example, a recent paper \nby Zou, Salzberg, and Ladin (ZSL97] describes a similar technique for clustering tree-structured data. \nOther special-purpose clustering mechanisms are designed to work with a specific type system or in a \nspecific soft-ware context. Chang and Katz [CK89] describe a cluster- ing mechanism that makes use of \nthe structural and inheri- tance relationships of a CAD data model. Gruber and Am- saleg [GA941 describe \na method that works in conjunction with garbage collection. Semantic clustering [SSSO] is an in- teresting \ntechnique that combines clustering, object caching, and pointer swizzling. Finally, Cactis [DKHSO], a \nsystem for managing derived data, uses a special-purpose (off-line) clustering mechanism. 2.3 Other \nClustering Research Placement trees [BD89] are a clustering system that was designed in the context of \n02 [DeuSO]. Placement trees use programmer-supplied schema annotations to derive a template for clustering \ncomplex objects. Some network database systems have methods for clustering the mem-bers of a parent-child \nset together, possibly with the set owner [Sal86]. Carey er al [CSL+SO] describe a similar mech-anism, \nfor relational database systems, that was implemented as the Starburst IMS attachment [HCL+SO]. In all \nthree cases, the initial placement of records is considered, but clus-ter maintenance is not. Soderlund \n[Sod811 considers the cost effectiveness of incremental, on-line cluster maintenance in network databases. \nCheng and Hurson [CH91] study the same problem in the context of object-oriented databases, although \ntheir focus is on large, multi-page clusters (i.e., not on fine-grained clustering). Both studies concluded \nthat such reorganization can indeed be beneficial. Finally, Hor-nick and Zdonik (HZ871 describe a general-purpose \ncluster-ing method in the context of the ENCORE OODB. This clustering method is based on multi-page segments, \nand is similar to the fixed-size clusters described in the next sec-tion. 2.4 General-Purpose Clustering \nMechanisms Unlike clustering policies and special-purpose clustering mechanisms, we are aware of no published \nresearch in the area of general-purpose clustering mechanisms for database systems. A survey of commercial \nand experimental OODBs finds only two such mechanisms, near hints and fixed-size clusters. 2.4.1 Near \nHints The most common general-purpose clustering mechanism in object-oriented database systems is near \nhints. A near hint is given as the object identifier (OID) of an existing persistent object. When creating \na new object, an application can use a near hint to request that the new object be placed on the same \npage as the near hint object, if possible. Virtually every object-oriented database system, both commercial \nand experimental, supports near hints. While near hints are easy for application programmers to use and \nfor system builders to implement, they have several drawbacks. To begin with, OODBs do not record near \nhint information persistently in the database, meaning that a near hint can be used only for initial \nobject placement, not for cluster maintenance. Another serious drawback is that there is no way to tell \nthe database system that a given object will be used as a near hint in the future, and that unrelated \nobjects should therefore not be placed near it (to leave space for its related objects). To address this \nproblem, some systems support fill factors. A fill factor is typically expressed as a percentage, and \nit indicates that the system should not place new objects onto data pages that are fuller than the fill \nfactor unless so instructed by a near hint. The use of a fill factor can increase the chances that a \nnear hint will succeed, but it is a rather coarse mechanism and, as our performance study will show, \nit does little to improve the effectiveness of near hints. 2.4.2 Fixed-Size Clusters A clustering mechanism \nthat addresses some of the prob-lems with near hints is fixed-size clusters, such as those implemented \nin ObjectStore [Obj95]. A fixed-size cluster is a physical storage structure that is pre-sized to an \nintegral number of pages by the application that creates the clus-ter. Fixed-size clusters solve the \nfuture near hint problem mentioned above, leaving space to satisfy future clustering requests, by refusing \nto place objects from different clusters onto the same page. However, this can lead to severe frag-mentation \nwhen cluster sizes are not an even multiple of the system page size. An additional drawback to fixed-size \nclus-ters is that the eventual size of a cluster may not be known in advance. The size and number of \nobjects in a complex object graph may vary greatly, and the size of an object that contains an embedded \ncollection (set, bag, etc.) may be im- possible to guess if the eventual cardinality of the collection \nor its implementation overheads are not known in advance. With fixed-size clusters, if an accurate cluster \nsize can-not be calculated in advance, the application programmer must simply guess. If a cluster fills \nits pre-allocated space, subsequent attempts to add new objects to the cluster will fail. When deciding \nhow the application should handle such failures, the application programmer is faced with few rea-sonable \noptions. For some complex data structures, it may be possible to spread the structure over a pool of \nclusters, although this method may require modifications to persis-tent data structures and extra application \ncode to manage the cluster pool. Another option is to maintain a designated overflow area for objects \nthat cannot be placed into their preferred cluster, although the use of a single shared over-flow area \nlargely defeats the purpose of clustering. 3 A New Fine-Grained Clustering Mecha-nism: Vclusters The \nshortcomings of the clustering mechanisms described above call for a mechanism that is fine-grained, \neasy to use, provides good clustering and space utilization, and facili-tates both smart initial object \nplacement and incremental cluster maintenance. To fill this need, we propose a new fine-grained object \nclustering mechanism based on variable- size clusters, or Vchsters. As we will show later, Vclusters \nare as easy to use as near hints, yet they are capable of pro- viding the same quality of clustering \nas fixed-size clusters without their inherent fragmentation problems. Vclusters are physical storage \ncontainers that can grow and shrink in size as objects are added and removed. They are designed to support \na wide range of cluster sizes efficiently, ranging from fine-grained clusters of hundreds of bytes up \nto large clus-ters of megabytes or more. Vclusters are also designed to function as sticky near hints \n; information about cluster-ing is stored persistently in the database and can therefore be used for \nboth initial object placement and incremental cluster maintenance. Vclusters can be used in one of two \nways. They can be used directly by application programmers, much like near hints and fixed-size clusters \nare currently used, or they can be used as the back-end of an automatic clustering policy. In the case \nof direct use by application programmers, we believe that Vclusters support for variable-sized clusters \nis essential to achieve good performance in combination with ease of use. In the back-end case, variable-sized \nclusters can still be very useful. For example, the placement tree approach [BD89] uses programmer-supplied \nannotations to derive logical clusters, which can be of any size, and then maps these logical clusters \nonto fixed-size pages. The design- ers of placement trees used near hints to perform this map-ping, but \nVclusters would have been a much better back-end for placement trees due to the strong similarity between \nVclusters and logical clusters. Similarly, Vclusters would be an excellent fit for addressing problems \nsuch as incre- mentally maintaining parent-child set clustering in network database systems or in the \nIMS attachment of the Starburst system (see Section 2). Basically, the Vcluster mechanism is directly \napplicable to the cluster creation and maintenance problems of any clustering approach that identifies \nlogical clusters as a step in the object clustering process. In con-trast, certain clustering policies, \nsuch as those of Tsangaris and Naughton [TN91, TN92], directly compute an exact de-sired grouping of \nobjects together into individual disk pages. Policies such as these, which go directly from access traces \nto pages without identifying logical groupings of objects, would not take advantage of Vclusters flexible \nsupport for cluster size variability; however, the stickiness of the Vcluster mechanism would still be \nhelpful in maintaining the pre-scribed clustering over time. The interface to Vclusters looks much like \nnear hints to the application programmer: To create an object in a spe- cific Vcluster, the application \nprovides the OID of an object that already resides in the cluster. An application may also choose to \nopen a Vcluster if it plans to create several new objects in it. The open operation accepts the OID of \nan ex-isting object and returns a transient cluster descriptor for the cluster that contains the object. \nCluster descriptors are analogous to the file descriptors bound to open files in oper- ating systems. \nIn addition to providing a convenient way to name the cluster in subsequent object-creation operations, \nthe open operation (and the corresponding close operation) serves as a hint to the database system that \nthe cluster will be growing-information that the system can use to opti-mize performance. As a further \naid to the implementation, the open operation accepts an optional size hint indicating the application \ns best guess as to the amount of data that wilI be added to the cluster while it is open. (We emphasize \nthat this is strictly an optional size hint; we will see later that it can be helpful, but is by no means \nnecessary, to the operation of Vclusters.) 3.1 Layout Issues The algorithms and heuristics to be presented \nin Section 4 try to keep Vclusters close to their ideal layout on disk. Before we consider these algorithms, \nhowever, we must an-swer the question of what constitutes an ideal layout. For example, consider an application \nin which each cluster is precisely 2/3 of a page in size. Two possible layouts for this application are \na sparse layout, in which each cluster lives on a page by itself, and a dense layout, which packs three \nclusters into every two pages, splitting one-third of the clusters (see Figure 1). The dense layout is \nclearly prefer-able from the point of view of space utilization (100% vs. 67%), but which layout gives \nbetter runtime performance for a highly clustered database traversal? Legend   Cluster I @ Cluster \n2 cluster 3 la Figure 1: Sparse and dense cluster layouts. Buffer pool size (pages) Figure 2: Predicted \nI/O cost for traversals of sparse and dense layouts. The answer depends on the size of the buffer pool \nand the nature of the access pattern. Consider a traversal that repeatedly chooses a random cluster and \nvisits every object in that cluster. If the buffer pool is very small, each cluster must be read into \nmemory before it is traversed. The average cost to read a cluster is one I/O for the sparse layout and \n4/3 I/OS for the dense layout (since unsplit clusters require one I/O, while split clusters require two). \nOn the other hand, if the database contains n clusters, a buffer pool of (2/3)n pages can hold the entire \ndatabase with the dense layout (so the I/O cost drops to zero in the steady state), but the same buffer \npool can hold only Z/3 of the database for the sparse layout (yielding an expected cost of l/3 I/O per \ncluster). The expected cost, in I/O operations per cluster, is a linear function of the buffer pool size, \nas shown in Figure 2. A sparse layout is preferable if the buffer pool holds fewer than n/3 pages, while \na dense layout gives better performance for larger buffer pools. Although this simple analysis ignores \nsuch issues as vari- ations in cluster sizes, complex patterns of locality, and CPU costs, it shows that \nin some cases, the best traversal perfor-mance is obtained by a policy that minimizes the number of split \nclusters, while in other cases, the best performance is obtained by a policy that optimizes space utilization. \nHow-ever, it is important to note that although the dense layout is never more than a bounded factor \nworse than that sparse layout, it may be better by an arbitrary factor. For example, in Figure 2), the \ndense layout is 33% worse than the sparse layout when the buffer pool holds two pages, but for a buffer \npool exceeding 2n/3 pages, the sparse layout is worse by an infinite factor (a non-zero cost divided \nby a zero cost) For medium-sized clusters (up to a few pages in size), we are faced with a similar choice. \nIn a sparse layout, each cluster is allocated as many pages as necessary to hold it. In a dense layout, \nsome pages are mixed -they contain frag-ments of multiple clusters. The sparse layout can waste up to \nhalf of the disk (e.g., if each cluster is just slightly more than one page in size). On the other hand, \nthe dense layout is much harder to maintain as clusters grow and shrink. The space penalty (and corresponding \nI/O performance penalty) for the sparse layout becomes less significant as clusters get larger, becoming \nvanishingly small for multi-megabyte clus-ters. The greatest challenge, therefore, is to manage smaller \nclusters of objects efficiently. Finally, we should note that our approach intentionally treats clusters \nas logical units of access, as we assume that all of the objects in a cluster have similar co-access \naffinities. Thus, our approach to the clus-tering problem is flat -our implementation supports files \nof objects partitioned into clusters-as opposed to a mul-tiple level clustering model. This is clearly \nappropriate for fine-grained clustering, and we believe that relatively little additional performance \nbenefit would be gained from a more complex model supporting nested or overlapping clusters. These considerations \nled us to an implementation of Vclusters with three representations of clusters and two kinds of pages. \nAllocated pages are designated as mixed or dedi-cated. A mixed page may hold objects from multiple clus-ters \n, while a dedicated page may hold objects from just one cluster. Clusters are designated as small, medium, \nor large. A small cluster is less than a page in size, residing on a portion of one mixed page; large \nclusters consist entirely of dedicated pages, while medium clusters can include a com- bination of dedicated \npages and portions of mixed pages. A cluster that grows beyond a certain size is promoted to large by \nmoving objects off of mixed pages onto newly allo-cated dedicated pages. A large cluster that gets too \nsmall is demoted, allowing it to share pages with other clusters. To prevent clusters from bouncing back \nand forth from the large representation, we use two size thresholds. In our Shore implementation, clusters \nbecome designated as large when they exceed twelve pages, and they are reclassified as medium when they \ndrop below eight pages. These thresh-olds are somewhat arbitrary, but seem to work well in our ex- periments. \nThe lower threshold was chosen to ensure space utilization of at least 87% (7/8) for large clusters even \nwhen one page is nearly empty. The higher threshold was chosen to be comfortably larger in order to provide \nhysteresis as protection against bouncing . 4 Vcluster Layout Algorithms Having discussed the tradeoffs \ninvolved in mapping the ob- jects in Vclusters to disk pages, we now consider algorithms for performing \nthese mappings. Prompted by the analysis of Section 3.1, we have developed two Vcluster algorithms: Vcl-dense, \nwhich tries to maintain good space utilization at all times, even if it is necessary to split clusters \nto do so, and Vcl-minsplit, which tries to minimize cluster splitting, even Our mixed pages are similar \nin concept to fragments in the 4.2 BSD fast Alesystem [MJLFS4]. if space utilization suffers as a result. \nAlthough each algo- rithm has a distinct bias, both algorithms try to minimize cluster splits and maximize \nspace utilization when possible. 4.1 Free Space Management Both of our Vcluster algorithms are based \nupon an earlier algorithm that we developed for allocating and freeing ob-jects [MCS96]. We briefly review \nthat space management algorithm here; the reader is referred to the full paper for further details. We \nassume that objects are allocated from a file of pages. As objects are deleted, pages become partially \nemptied. When a new object is to be allocated, storage allocation algorithms in existing database systems \nuse one of two ap-proaches. They may search all of the pages of the file for one with enough free space \n(an exhaustive-search algorithm ) or allocate a new page and add it to the file (an append-only algorithm). \nExhaustive search can leads O(n ) perfor-mance for adding n objects to a file. Append-only leads to poor \nspace utilization: If free space on a page is not reused until the page becomes empty, space utilization \ncan be ex-pected to approach l/Hk in the limit, where k is the num- ber of objects that fit on a page \nand Hk is the kth harmonic number. For example, for 256-byte objects allocated from 8k-byte pages, utilization \ndrops to l/H32 M 25%, and for 32- byte objects, it drops to l/H256 GZ 16% (see [MCS96] for a proof). \nMany commercial systems provide a reorganization utility that consolidates under-filled file pages, releasing \nany pages that become completely empty in the consolidation process. However, these utilities tend to \nbe highly disrup-tive to normal system operation. Our solution to this problem was a hybrid of exhaus- \ntive search and append-only called HY(n,u). The algorithm maintains an estimate of the current space \nutilization of the file (the sum of the sizes of the objects in the file divided by the sum of the sizes \nof the pages). If that utilization drops below u (a parameter of the algorithm), the allocator uses a \nform of exhaustive search to allocate new objects. Other-wise, it only searches the n most recently used \npages. In all experiments described in this paper, u = 718 and n = 8. To keep track of free space, we \nuse one persistent data structure and two transient data structures created when a file is opened. The \npersistent structure is a space map, an array of 4-bit entries, one for each page of a file. Each en-try \nindicates the amount of free space on a page, classifying it into one of 16 free-space classes.2 The \ntransient struc-tures are a 16-entry free-space histogram, which counts the number of pages of a file \nin each free-space class, and a pid cache, which records the page identifier and free-space class of \neach of n recently used pages. The histogram is easily derived from the contents of the space map. The \nalgorithm also maintains an estimate of the current space utilization, computed from the free-space histogram. \nWhen creating a new object in a given file, HY(n,u) al-ways attempts to allocate the object on a page \nwhose id is listed in the pid cache. If no page listed in the cache can accommodate the object, the algorithm \nwill either se<arch the file s space map (using a next-fit strategy) to find an *This low-precision representation \nof the amount of free space not only allows a more compact space-map, it decreases the frequency of changes \nto entries, an important consideration in a database system that must log all changes to persistent data \nstructures to support crash recovery. existing page that can hold the object, or allocate a new page \nto the file. It will search for an existing page only if the overall space utilization of the file is \ncurrently below u and the free-space histogram indicates that there is at least one page in the file \nthat can hold the new object. For simplicity, we do not concern ourselves here with the prob-lem of allocating \nthe individual pages of a Vcluster close to one another; as mentioned earlier, that is a relatively well-understood \nproblem in the operating systems community, e.g., see [MJLF84], and it was also addressed in the work \nof Cheng and Hurson [CHSl]. Whatever page is used to al- locate the new object is added the pid cache, \nreplacing the pid cache entry for the fullest page in the cache. Our ex-periments indicate that a small \npid cache (containing just 8 entries) is adequate to maintain a high utilization (87%) with low overhead \nover a wide variety of workloads, even for very large files [MCSSG]. 4.2 The Vcl-minsplit and V&#38;dense \nAlgo-rithms To create and delete new objects in large clusters (which consist entirely of dedicated pages), \nboth algorithms simply use the HY(n,u) algorithm within the dedicated pages of the target cluster. Henceforth \nin this section, we will omit men-tion of large clusters, focusing instead on how the algorithms treat \nsmall and medium clusters. We use the term fragment for the portion of a cluster that resides on any \none page. Each cluster consists of a set of dedicated pages plus a sequence of fragments on mixed pages. \nIn a slight abuse of terminology, we shall refer to these latter fragments as mixed fragments. Vcl-minsplit \nmaintains the invariant that every cluster has zero or more dedicated pages plus at most one mixed page. \nIn contrast, (non-large) Vcl-dense clusters have only mixed pages; in other words, Vcl-dense does not \nuse dedicated pages for its medium clusters. The fragments of a Vcl-dense small cluster are numbered \nin order of creation, starting at zero. To con- trol the number of fragments, Vcl-dense attempts to keep \nthe size of fragment i above min(2i . MST, pagesize), where MST, the minimum size threshold, is a constant \n(l/8 page in our implementation). This limit prevents Vcl-dense from spreading a cluster s objects over \nan excessive number of fragments; they start out small, but V&#38;dense tries to make each successive \nfragment at least twice as big as the previ-ous one. The highest numbered mixed fragment is called the \nactive fragment, and its page is termed an active page. A fragment is said to outgrow its page when that \npage does not have enough free space to accommodate an object creation request. The principal difference \nbetween the two Vcluster algorithms is how they cope with outgrowing a page. Vcl-minsplit may move the \nfragment to an emptier page, while Vcl-dense may decide to leave the fragment where it is and begin a \nnew fragment on another mixed page. Figures 3 and 4 on the next page illustrate these ideas. In both \nfigures, Cluster 1 is a large cluster containing nine objects stored on dedicated pages l-3. In Figure \n3, Cluster 2 is a medium cluster stored on dedicated pages 4 and 5 and mixed page 8. Medium cluster 3 \nalso used part of mixed page 8, as well as dedicated page 6. Small cluster 4 is on page 9, while small \nclusters 5 and 6 share page 7. In Figure 4, Cluster 23 is split into four fragments stored 3There is \nno distinction between medium and small clusters with the Vcl-dense algorithm. Dedicated Pages Mixed \nPages Page 1 Page 4 Page 7 --I I 9 Cluster 3 Page 3 Page 6 Page 9 Figure 3: A typical algorithm. layout \nof clusters with the Vcl-minsplit Dedicated Pages Mixed Pages A Cluster 1 Cluster 2 Page 1 Page 4 Page \n7 V r-l:: Cluster 43 I 1 Page 3 Page 6 Page 9 Figure 4: The objects of Figure 3 as laid out by the \nVcl-dense algorithm. on pages 8, 5, 6, and 4 (in that order). The active page for cluster 2 is page 4. \nSimilarly, cluster 3 has fragments on pages 5, 7, and 8, while cluster 4 is on pages 5 and 7. Both algorithms \ntake advantage of the close operation to reorganize a cluster, thereby improving space utilization and/or \ndecreasing the number of fragments. (V&#38;dense tries harder to do the former, while Vcl-minsplit optimizes \nfor the latter.) Unfortunately, a shrinking cluster, unlike a growing cluster, is unlikely to be open, \nas an application may know little or nothing a priori about the cluster membership of objects that it \nis deleting. We therefore use a heuristic to detect shrinking clusters and reorganize them when they \nstabilize. With each transaction, we associate a fixed-size LRU list of clusters from which the transaction \nhas deleted objects and that require further attention. (This list is sim-ple to maintain since each \nobject on disk is tagged with its cluster id in our implementation). When the deletion of an object from \na page causes the page s utilization to drop be-low u, the cluster that contained the deleted object \nis added to the LRU list (or is moved to its head if it was already there). When a cluster is ejected \nfrom the LRU list, it is reorganized as if it were the subject of a close operation. Clusters remaining \nin the LRU list when the transaction commits are reorganized at that time to ensure that the re-organization \nwill occur even in the event of a post-commit system crash. In somewhat more detail, the algorithms work \nas follows: Allocating a mixed page When either algorithm needs to allocate a mixed page for creating \nor copying a fragment, it allocates a new, empty mixed page unless doing so would cause the overall space \nutilization of the file to drop below the target u. It also refuses to use the active page of another \ncluster or a page with insufficient free space. Specifically, a new fragment requires a page with at \nleast MST bytes of free space, and a fragment being moved from another mixed page requires an amount \nof free space that is at least twice the current size of the fragment. These restrictions attempt to \nlimit the number of times a growing segment will end up being recopied. Because a fragment is active, \nit stakes a claim on the free space on its page, and the doubling of the claim on each copy ensures that \nthe total number of bytes copied is less than twice the ultimate size of the cluster. Creating a new \nobject In the Vcl-minsplit algorithm, new objects are added to ded-icated pages of medium clusters using \nHY(n,v) (dedicated pages are only used for large clusters in Vcl-dense). Other-wise, both algorithms \nattempt to allocate them on the active page of the cluster. Outgrowing a page This case occurs when an \nattempt to add a new object to the active page of a small cluster fails because there is not enough free \nspace on the page. Vcl-minsplit: If the active page contains more than one cluster, the active fragment \nis copied to another mixed page, chosen as above. Otherwise, the cluster is pro-moted to medium status \nand is given a dedicated page which is used to allocate the new object. Vcl-dense: If the active fragment \nis less than its mini-mum size (min(2 . MST, pagesize)), or if the file con-tains a mixed page with an \namount of free space that is at least twice the size of the active fragment, then the active fragment \nis copied to a new mixed page (chosen as above). Otherwise, a new fragment is added to the list of fragments, \nallocated to a mixed page (again, chosen as above), and designated to be the cluster s active fragment \nand used to allocate the new object. Reorganizing a cluster When an open cluster is closed, or when a \nshrinking cluster is closed because it is expelled from a transaction s LRU list of shrinking clusters, \nit is reorganized. Vcl-minsplit: If the cluster is small, occupying some frac-tion of a single mixed \npage, there is nothing to do. Otherwise, the reorganization proceeds in two phases. In the first phase, \nVcl-minsplit coalesces free space on the cluster s dedicated pages by moving objects onto fuller dedicated \npages from its emptier dedicated pages and mixed page. Any pages that become empty dur-ing this phase \nare freed. If, at the completion of this phase, there are any remaining under-full dedicated pages (ones \nwith utilization less than u), the second phase combines the cluster fragment on the emptiest of the \nunder-full pages with the cluster fragment on the mixed page (if any), placing this new cluster frag-ment \nonto the tightest-fitting mixed page in the file, allocating a new mixed page for it if needed. Vcl-dense: \nAny under-full pages of the cluster are arranged into a list ordered by fullness. They are then com-pacted \nby moving objects from emptier to fuller pages of the cluster, starting at the ends of the list and mov-ing \ntowards the middle as pages fill or as cluster frag-ments have been completely moved. If an under-full \npage remains and its fragment can be moved to an-other mixed page whose utilization would thereby be \nincreased beyond u, the fragment is moved. 4.3 Size Hints One of the primary benefits of Vclusters over \nfixed-size clus-ters is that the creator of a Vcluster is not required to supply the size of the cluster \nat the time of its creation. However, in some cases, an application may in fact know the size of a cluster \nin advance, or may at least be able to supply an estimate of the cluster s expected size. Such information, \nif accurate, could be used by our Vcluster algorithms to im-prove performance by avoiding some copying \nof fragments. Our Vcluster algorithms use these optional size hints, when provided, as follows: Vcl-minsplit: \nWhen creating the first object in a new clus- ter, Vcl-minsplit considers the cluster to be large or \nmedium-sized from the outset if the size hint indicates that the cluster will be larger than a page. \nIt then places the cluster onto a dedicated page. If the size hint indicates that the cluster will be \nsmaller than a page, then a mixed page is chosen that can hold an entire cluster of that size, even if \na new page must be allocated to do so. Vcl-dense: V&#38;dense initially places the cluster onto an empty \nmixed page if the size hint indicates that the cluster will be larger than one page, which usually re-quires \nthat a new page be allocated. If the size hint indicates that the cluster will be smaller than one page, \nthen the cluster will be placed onto a mixed page that can hold an entire cluster of the predicted size, \nif such a page exists. However, unlike Vcl-minsplit, Vcl-dense will only allocate a new mixed page for \nthe cluster in this case if there is no mixed page that has at least MST free bytes. 5 Implementing Clustering \nin Shore Section 6 presents a performance study based on actual implementations of Vclusters, fixed-size \nclusters, and near hints in Shore. We modified Shore s file data structure, which formerly supported \nonly a flat set of pages, to consist of a collection of lists of pages. To implement Vclusters, we used \none page list in each file to keep track of all of the mixed pages in the file, and we used separate \npage lists to keep track of the set of dedicated pages for each medium or large cluster in the file. \nHY(n,u) was used to manage space within each individual page list. The header of each Shore data object \nwas extended to include a cluster id. Finally, each Shore volume also had an associated persistent B+tree \nfor mapping <file, cluster id> pairs to the associated list of mixed pages to make each cluster s mixed \npage list readily accessible. To implement fixed-sized clusters, each cluster was rep-resented as a page \nlist whose pages are all allocated when the cluster is initially created. For the near hints algorithm \n(and our baseline no clustering algorithm), each file had only one page list. When creating an object \nin the presence of a near hint, our implementation allocates the new object on the page that contains \nthe indicated object if possible. Otherwise, it uses HY(n,u) to place the object elsewhere in the file. \nFurther details of our modifications to Shore, as well as of our implementations of the different clustering \nalgorithms, can be found in [McA97]. 5.1 Issues in Moving Objects Our Vcluster algorithms depend on the \nability to move ob-jects from one page to another. Shore supports object move-ment with logical OIDs \nimplemented by an OID index and an LRU cache of logical-to-physical translations (the OID cache ). Two \nissues related to transactions complicate the task of object movement. The first concerns lock conflicts \nthat arise when an object to be moved for cluster-mainten- ance reasons is locked by another transaction. \nOur current implementation simply waits for the lock; this is adequate for the tests described here, \nbut could lead to poor con-currency and occasional deadlocks in a production system. More thorough solutions \nare possible, e.g., based on special- purpose locking and logging protocols to allow clustering-related \nmoves of locked objects, or based on the mainte-nance of a persistent to-do list of the objects that \nneed to be moved later on and a background thread that processes this list. Experimentation with such \nhigh-concurrency solu-tions is beyond the scope of the current study. The second issue concerns what \nhappens when a transac- tion aborts after moving one or more objects for clustering reasons. Our implementation \nlogs the moves as ordinary op-erations in the context of the transaction that caused them, which means \nthat if the transaction aborts, its moves will be undone. Because of this, when a transaction moves an \nobject, it must reserve the space on the old page in order to ensure that it can move the object back \nto its original loca-tion in the event of a rollback. Furthermore, the transaction must retain a lock \non the object until it commits in order to prevent other transactions from increasing the size of the \nobject, as a larger object might not fit on the old page, were it to be moved back. However, it is safe \nfor the transaction to consume any reserved space that it may itself have on the new page when placing \nthe object there. An alternative to this implementation would have been to enclose object movements inside \nnested top-actions [MHL+92], which are subsequences of the actions of a transaction that are not un-done \neven if the enclosing transaction aborts. Nested top-actions are typically used when a transaction makes \nchanges to persistent structures, such as splitting a B+tree page, that have no semantic effect on the \ndatabase but that do affect other active transactions. An examination of the relative benefits and drawbacks \nof using nested top-actions for ob-ject movement is also beyond the scope of this paper. Performance \nResults To study the costs and benefits of the clustering mechanisms described in this study, we constructed \na set of experiments that measure the performance characteristics of cluster cre-ation, traversal, and \nmaintenance for each of our algorithms and compared the measured results for Vclusters with im-plementations \nof fixed-size clusters and near hints with a fill factor. We also tested the performance of a baseline \nno-clustering option, for which we simply used the HY(n,u) algorithm with no clustering extensions. Table \n1 summa- rizes the clustering algorithms and the labels that we will use to identify them when describing \nthe results. We con-sidered the possibility of using an existing OODB bench-mark such as 007 [CDN93] \nfor these experiments; however, benchmarks like 007 have placed too little emphasis on clustering-related \noperations to be effective for this purpose [CDK+94], and the planned extensions to 007 in this area \nwere never completed. After exploring the design space for clustering perfor-mance studies for quite \na while, we settled on a set of five ex- periments for use in evaluating clustering algorithm perfor-mance. \nThe first experiment measures the cost of building a database, while the second and third evaluate the \nquality of clustering achieved by measuring the cost of two kinds of clustered traversals. The fourth \nexperiment applies a series of updates to the database, creating and deleting random clusters and objects, \nand measures both the performance of the updates themselves and the effects of the updates on subsequent \nread-only traversals. Together, this set of exper- iments subjects the set of clustering algorithms to \na variety of important OODB workloads and operating conditions; both loading and incremental update conditions \nare cov-ered, and several varieties of traversals are considered. The final experiment measures the effects \nof both accurate and inaccurate size hints on the performance of the two Vcluster algorithms. No clustering \n1 NONE Table 1: Clustering algorithms and their labels. The hardware platform for our experiments consisted \nof a Digital Equipment Corporation DECpc XL 590 (90 Mhz Pentium processor) with 32 megabytes of main \nmemory; the operating system used was Solaris 2.4. The data volume was a two-gigabyte Seagate ST12400N \nhard disk that was mounted as a raw device entirely under the control of Shore. The log was stored on \na loo-megabyte raw partition of a Conner CFP 1060s hard disk. To avoid interference from outside sources, \nthe machine was removed from all networks, and all unnecessary processes were terminated. The parameters \nunderlying the performance results shown here are summarized in Table 2. To keep the exper- imentation \ntime manageable, the database size chosen for our experiments was fairly small (approximately 32 MB); \nto balance the database size and main memory size appropri- ately, we used a correspondingly small buffer \npool (512 8 KB pages, totaling 4 MB). On the other hand, we used a rela- tively large OID cache-3 MB-and \nwe initialized this cache before each traversal experiment to hold all OID translations in the database. \nThis was done not to mimic a realistic setting, but rather to minimize the (potentially distracting) \neffects of logical ID lookups on our traversal results; our goal in doing this was to obtain less Shore-specific \nresults by fac- toring out the performance effects of Shore s particular OID translation mechanism. All \nof our clustering methods are based on HY(8,87) (i.e., HY with an S-entry pid cache and a target utilization \nof 718, which as mentioned earlier was previously shown to be very effective [MCS96]). For Vcl-minsplit \nand Vcl-dense, the minimum size threshold (MST) was set to one-eighth of the system page size. Finally, \nfor near hints, a default fill factor of 87% was used to achieve a balance between leaving too much and \ntoo little free space; we also explored alternative fill factor settings in one of our experiments (which \nwill be discussed shortly). Parameter Value Page size 8 Kbytes Buffer ~001 size 512 naaes I OID cache \nsize I 3 Mbvtes I Page cache size (n) 8 Target space utilization (u) 87% Minimum size threshold (MST) \n1 Kbvte Interleaving factor 8 Fill factor (near hints only) 87% Root node size 428 bytes Leaf node size \n1 200-800 bytes Number of leaves/tree I See Figure 6 Table 2: Parameters used in most of the clustering \nexperi-ments. Our test data itself was designed to provide the algo-rithms with a database consisting \nof groups of related ob-jects that tend to be accessed together. We wanted this database to be easy to \ngenerate in a controlled way, and to provide an easy means to vary the size of these groups of objects \nin our experiments. In general, the problem of deciding how to group application objects into clusters \nis a challenging problem, but it is outside the scope of this work. We assume that the application designer \nor one of the cluster-defining algorithms cited earlier has grouped objects into clusters in such a manner \nthat clustered traversals -traversals in which references to members of a cluster have high temporal \nlocality-are common. To these ends, we constructed a test, database of two-level trees of objects (see \nFigure 5). (We experimented with several more complicated structures as well, but we gained no additional \ninsights from their added complexity.) Each of our two-level trees has up to 48 leaf objects and a root \nobject large enough to hold 48 object identifiers (428 bytes including overhead). The size of a leaf \nobject is a random variable uniformly distributed between 200 and 800 bytes. tion destroys an object \ncluster by destroying all of its tree s size leaves and then destroying its root. The grow and shrink \n  kd-7 El I l-l .  FL I I Leaf objects Root object 4 -48 objects, 428 bytes 200 -800 bytes each Figure \n5: A typical tree in the test database. The number of leaf nodes also varies randomly from tree to tree. \nExcept as noted below, the number of leaves in each tree is drawn from the distribution 3X + 4, where \nX is a Gamma random variable with density function f(x) = xe- (see Figure 6). As the graph indicates, \nthis distribution gives most trees between four and 15 leaves. In more detail, the distribution has a \nminimum of 4, a mean of 10, a standard deviation of 3&#38;, and an effective maximum of 48: The probability \nthat the number of leaves is greater than 48 (the maximum number of leaf nodes the data structure can \nac-commodate) is less than 10e5; any such values were simply discarded. The resulting trees are between \n30% and 300% of the system page size, with a mean of 67%, providing quite a wide range of cluster sizes \nfor testing the abilities of dif-ferent clustering algorithms to manage fine-grained object clusters. \n0.15 I E \\ i. 0 0 5 IO 15 20 25 30 Tree size (number of leaf nodes) Figure 6: Distribution of tree sizes. \n For our test workloads, as described above, we wanted sets of accesses and updates that would test the \nalgorithms ability to expedite clustered traversals and maintain good performance in the face of updates. \nWe thus created work-loads from combinations of five operations: create, destroy, grow, shrink, and visit. \nThe create operation creates a clus-ter of objects by creating a new tree and populating it with a number \nof leaf nodes determined by the above Gamma dis-tribution. The remaining operations each choose a random \ncluster, i.e., an existing tree, by selecting uniformly from among all of the trees in the database. \nThe destroy opera-operations change the size of a cluster by adding or remov-ing some number of leaves \nfrom the cluster s tree structure. The visit operation accesses all of the objects in a cluster by visiting \nall of its tree s leaves in random order. In the case of the grow and shrink operations, the num-ber \nof leaves added to or deleted from the tree is chosen uniformly from the range 4-8. The shrink operation \ncan cause underflow if the number of leaves chosen for deletion is greater than the number of leaves \nin the chosen tree. This situation is handled by first removing all of the leaves (but not the root) \nfrom the tree and then choosing another tree upon which to continue the operation. A similar process \nis used to handle tree overflows (as root nodes can hold only 48 leaf pointers each). In all tests, overflow \nand underflow occur so infrequently as to have no significant effect on our results. For Vclusters, we \nplaced each tree of objects into a sep-arate cluster. For near hints, the root node of each tree was \ncreated with no near hint, and each leaf node was created using the root of its tree as a near hint. \nFixed-size clusters complicated our application program considerably. Since the size of a tree is not \nknown in advance and trees vary in size from one-third of a page to three pages, it would be impractical \nto make the fixed cluster-size big enough to ac-commodate the largest possible tree, and we needed a \nway to recover when a tree overflowed a typical cluster. There-fore, we used multiple clusters for each \ntree and added to the root node a list of cluster ids and an indication of the current cluster. When \nadding a node to a tree, the pro-gram tries to add it to each of its clusters, starting with the current \none. If they are all full, it adds a new cluster and makes it current. Note that our implementation of \nfixed-size clusters does not need to read pages from disk to determine whether a cluster has room for \nan object-creation request, as this information is available from the space map. We chose a cluster size \nof one page (8 KB) because an average tree is 213 of a page in size. Because a system based on near hints \n(or no cluster-ing ) does not attempt to separate growing clusters from one another, it may not be able \nto maintain good clustering in the presence of concurrently-growing clusters. To study the effects of \nmultiple concurrently growing and shrinking clusters, we introduced a workload parameter called the in-terleaving \nfactor and created a table containing that many active operations. The main body of the test program \nis a loop that repeatedly chooses one of the active operations from the table and performs the next step \n(create, delete, or visit a leaf node) in that operation. This use of inter-leaving captures the storage \nmanagement impact of having concurrent transactions working on one file; it also models what would happen \nwith a single transaction concurrently updating multiple clusters within one file. The interleaving factor \nwas eight in all experiments except as noted. Finally, to simulate applications that operate on differ-ent \nportions of a database at different times, i.e., to model temporal locality, our test program places \neach tree in the database into one of eight separate files when it is created. Varying the number of \nfiles from which trees are selected during database traversals enables us to model working sets of different \nsizes. It is important to note that, while our synthetic database consists of two-level trees, and our \nworkload is therefore made up of tree operations, our results are not specific to trees. This database \nand workload combination was sim-VMIN comes close (85%). If each cluster were exactly 67% ply used as \na convenient way to create and operate upon a database containing a number of fine-grained clusters con-taining \nvarious numbers of objects, themselves of varying sizes, in interesting ways that would allow us to explore \nthe database creation, incremental update, and access perfor-mance charact,eristics of our clustering \nalgorithms in a well- controlled manner. 6.1 Experiment 1: The Create Workload Our first experiment measures \nthe ext,ra cost required to initially build a database using Vclusters as compared to the simpler clustering \nmechanisms. The workload consists entirely of create operations. The workload creates 6,400 clusters, \nspread uniformly over eight initially-empty files, for a total of approximately 32 Mbytes of data. VDENSE \n- VMlN - FlXED - NEAR87 -1 NONE -I  I  I 0 100 200 300 Time (seconds) Figure 7: Create workload: Elapsed \ntime. VDENSE - VMIN - FIXED - NEAR87 - NONE - I I I I r I 0 20 40 60 80 100 Space utilization (percent) \nFigure 8: Create workload: Space utilization. Figure 7 shows the elapsed time required to build the database \nfor each of the tested clustering methods, and Figure 8 shows the resulting space utilization. (Table \n1 lists the abbreviations for all of the clustering algorithms tested here.) The three methods that actually \ncreate clusters (VDENSE, VMIN and FIXED) are clearly slower than the simpler methods (NEAR.87 and NONE), \nwith VMIN taking twice as long to build the database as NEAR87 or NONE. In the case of the Vcluster algorithms, \nthe extra time stems almost entirely from the cost of moving objects from page to page as clusters grow. \nMoving objects generates additional log traffic, requires updates to the OID index, and can also result \nin extra page reads and writes. Thus, it is not sur-prising that Vclusters are slower than the other \nmet,hods. FIXED is slightly slower than NEAR87 and NONE because it produces a much worse space utilization \nthan the other methods do (just 58%) and therefore performs more page writes during the database building \nprocess. The space utilization results shown in Figure 8 show that VDENSE achieves its target space utilization \nof 87% and of the page size, then VMIN s space utilization would also be exactly 67% because it refuses \nto split any small clusters. However, because it exploits the variation in cluster sizes by placing clusters \n(or cluster fragments) of different sizes together on the same page, it yields much better utilization. \nIn contrast, FIXED gets only 58% utilization. Although the average tree uses 2/3 of a page, trees that \nare just over one page in size waste most of a second page, driving the average utilization even lower. \nNONE and NEAR87 achieve over 97% utilization since they are at liberty to pack pages at the object, rather \nthan cluster, granularity.  6.2 Experiment 2: Uniform Traversals Although Experiment 1 shows that Vclusters \nare slower than the simpler methods for create-only workloads, Experiments 2 and 3 show that the cost \nis quickly recouped by improve- ments in the performance of clustered traversals. These ex-periments \nuse the database created by Experiment 1 and visit each cluster an average of four times. Experiment \n2 repeatedly chooses a random cluster se-lected uniformly from the database (with replacement) and traverses \nall of the nodes of its tree, repeating 25,600 times. Figure 9 shows the elapsed time and number of page \nreads for each algorithm. These graphs indicate that I/O cost is virtually the dominating factor determining \nthe traver-sal times, particularly in the less efficient cases. All of the other traversals are similarly \nI/O bound; in subsequent cases where the I/O graphs closely match the elapsed-time graphs, we will show \nonly the elapsed-time graphs. Page reads (thousands) 0 20 40 60 80 VDENSE VMlN n Elapsed Time FIXED NEAR87 \nNONE 550 lo;00 Time (seconds) Figure 9: Uniform traversal. As the analytical model of Figure 2 predicts, \nVDENSE is marginally worse than VMIN for a relatively small buffer pool. The difference is less marked \nthan the model pre-dicts because we are only requiring VDENSE to achieve 87% space utilization rather \nthan lOO%, and because the variation in cluster sizes allows VDENSE to reach that uti-lization by splitting \nfewer clusters than would be required if all clusters were exactly 67% of a page. The most striking results \nof this experiment are l Near hints have virtually no effect. NEAR87 is only 5% faster than NONE. l The \nhigher cost of Vclusters for creating the database in Experiment 1 is quickly repaid by improvements \nin clustered traversals. The traversals of the Vcluster databases are nearly 2.5 times as fast as the \nones built with near hints or no clustering. In particular, the extra 100 seconds or so to build the \ndatabase was paid back after visiting each cluster on the average only slightly more than one time. \nFixed clusters, despite their poor space utilization, work about as well as Vclusters in speed of traversal, \nbecause the poor utilization has a small relative effect on the buffer-pool miss rate. The VMIN database \nis 10 times the buffer-pool size, for a miss rate of about 90%, while the FIXED database is 14 times \ntimes the buffer-pool size, for a miss rate of 93%, yielding only a 3% difference. VDENSE NEAR50 NEAR75 \nNEAR87 NEAR100 0 500 1000 1500 Time (seconds) Figure 11: Uniform traversal (varying fill factor): Elapsed \ntime. Effects of the Interleaving Factor The performance of NEAR87 and NONE for the Uniform traversals \nshown in Figure 9 suggests that near hints are in- effective at clustering objects when there are multiple \nclus-ters growing at once. This hypothesis is verified by Fig-ure 10, which shows the results of re-running \nExperiment 2 on databases constructed without any interleaving (i.e., with an interleaving factor of \none.) For comparison, the elapsed time data from Figure 9 is repeated in Figure 10. This graph shows \nclearly that the inability of NEAR87 and NONE to separate growing clusters accounts for their poor performance \nin Experiment 2. VDENSE q Interleave factor 8 VMIN n lnterleave factor 1 FIXED NEAR87 NONE 0 560 Id00 \n1500 Time (seconds) Figure 10: Uniform traversal (with and without interleav-ing): Elapsed time. Effects \nof Fill Factors A fill factor provides a method to try to improve the effec-tiveness of near hints in \nthe presence of interleaving. The idea behind fill factors is to set aside some amount of space on a \npage for objects created without near hints to allow clusters to grow around them. To examine the effects \nof fill factors, we ran the Uniform traversals over databases built with near hints using fill factors \nof lOO%, 75%, and 50% (all with an interleaving factor of 8). The elapsed time results for these traversals \nare shown in Figure 11, along with the previous results for near hints with an 87% fill factor and for \nV&#38;dense (both copied from Figure 9). Although the use of fill factors in conjunction with near hints \nclearly has some effect on traversal performance, this effect is minimal-even with a fill factor of 50%, \nthe traversal is nearly twice as slow with near hints as it is with VDENSE. The reason for the ineffectiveness \nof fill factors is that they do not address the basic problem with near hints: Near hints allow objects \nfrom too many different clusters to be placed onto the same page, resulting in many split clusters. Fill \nfactors only prevent pages from becoming completely full of objects that belong to different clusters. \nTheir effectiveness is therefore minimal. 6.3 Experiment 3: Working Set Traversals Our second traversal \nmethod is called the Working Set traver- sal because it models an application that is active in different \nportions (working sets) of the database at different times. The Working Set traversal repeatedly picks \na file to be its current working set, visits some number of clusters from that file only, and then chooses \nanother file (with replacement). As before, the traversal visits 25,600 clusters in all, visit-ing 1,600 \nclusters in each file before moving on to another file. To examine the effects of memory size and disk-space \nutilization on Working Set traversals, we ran the traversals with buffer pool sizes varying from 200 \npages (1.6 Mbytes) to 1,152 pages (9 Mbytes). Figure 12 shows the elapsed-time results for the Working \nSet traversals. The most striking aspect of this graph is the range of performance observed for NEAR87 \nand NONE. With a small buffer pool, they perform up to three times slower than VMIN, VDENSE and FIXED, \nbut with a large buffer pool, they are actually the fastest algorithms. As in the previous experiments, \nthe small buffer pool results are explained by the fact that clusters end up being split over many pages \nwith NEAR87 and NONE. As a result, if the individual working sets do not fit in the buffer pool, these \nmethods have a very high I/O cost per cluster read. The greatly improved performance for NEAR87 and NONE \nwith the large buffer pool is explained by their better space utilization; if the working sets do fit \nin the buffer pool, then the only remaining I/O cost is the cost of switching from one working set to \nanother. This cost is lowest when space utilization is the highest, as higher space utilization implies \nthat each working set covers fewer pages, thereby reducing the number of I/O operations needed to transfer \nthe working set from disk to memory. Since NEAR87 and NONE yield the best space utilization, they perform \nslightly better than VMIN and VDENSE at this end of the buffer pool spectrum. The difference in performance \nbetween FIXED and the Vcluster algorithms is dwarfed by the NEAR87 and NONE curves in Figure 12, but \nit is significant nonetheless. The right-hand side of Figure 12 focuses on the portion of the graph for \nlarger buffer pools. FIXED is up to 38% slower than VMIN and VDENSE. The worse performance of FIXED is \ndue to its worse space utilization, as more page I/OS are required to read each working set. In other \nwords, FIXED not only suffers from poor disk-space utilization, but it also exhibits inferior run-time \nperformance due to poor utiliza-tion of disk/memory bandwidth. P 3 t g 500 % % w 0 600 800 loo0 1200 \n0 500 1000 Buffer pool size (pages) Buffer pool size (pages) Detail Figure 12: Working Set traversal: \nElapsed time.   6.4 Experiment 4: Updates Our fourth experiment has two parts. We first measure the \nperformance of a workload that creates and deletes clusters and objects, and then we measure the extent \nto which the al- gorithms preserve clustering by rerunning Experiment 2 on the updated database. The \nUpdate workload is designed to hold the size of the database approximately constant. Each tree operation \nis chosen with equal probability to be create, delete, grow, or shrink. The number of leaves added or \nre-moved by grow and shrink operations was chosen uniformly from the range of 4-8. The results of running \nthis workload for 25,600 oper-ations are shown in Figure 13. As expected, VMIN and VDENSE do have a somewhat \nhigher cost than the other methods; VMIN is roughly 27% slower than FIXED. In-terestingly, NONE is also \nslower than FIXED or NEAR87. This effect can be explained by the fact that the destroy and shrink operations \nare similar to traversals in that they visit the root of a tree and then visit one or more of its leaves. \nBetter clustering makes these operations faster. VDENSE VMIN -I FIXED -1 NEAR87 -I NONE - I mrrrrpTI~~~~~~I~~~~~~~~~l0 \n1000 2000 3000 4000 Time (seconds) Figure 13: Update workload: Elapsed time. Figures 14 and 15 show the \neffect of updates on cluster- ing by comparing the space utilization and elapsed time for a Uniform traversal \n(Experiment 2) before and after the Up- date workload was applied to the database. NEAR87 shows the smallest \ndegradation of performance (9%), partly be-cause the free-space created on each page by deletions helps \nnear hints to succeed, but its post-update traversal perfor-mance is still worse than any other method \nexcept NONE. In contrast, the performance of VMIN degrades by just lo%, and in fact it manages to give \nthe best traversal performance of all. FIXED suffers the greatest performance degradation (37%) primarily \ndue to cluster overflows. Over time, more and more trees become spread over multiple clusters (pages). \n0 20 40 60 80 100 Space utilization (percent) Figure 14: Update workload: Space utilization before and \nafter updates. VDENSE tZ Before VMIN FIXED NEAR87 NONE moo Time (seconds) Figure 15: Update workload: \nElapsed time before and after updates. Shrink operations are unlikely to remove clusters since a cluster \nis removed only if it becomes empty. If a tree spans multiple clusters, one of which is nearly empty, \na leaf chosen randomly to be deleted from the tree is unlikely to lie in the nearly empty cluster. Therefore, \nshrink operations tend to decrease utilization, causing a 10% drop in space utilization (from 58% to \n48%) for FIXED. The 28% drop in performance for VDENSE is due to its willingness to split clusters to \nmaintain space utilization. Apparently, the attempt to rejoin clusters during reorgani-zation is not \nparticularly successful. VDENSE and VMIN do manage to hold space utilization nearly constant. Uti-lization \nfor NEAR87 and NONE drop to near the HY(n,u) parameter u.  6.5 Experiment 5: Size Hints Experiment 5 \nis designed to measure the extent to which our algorithms can take advantage of size hints. The Create \nand Update workloads were modified to supply size hints on create and grow operations. Since any mechanism \nthat accepts hints must allow for the possibility that the hints are wrong, we measured the performance \nwith both valid and invalid hints. The methods used for calculating size hints are summarized in Table \n3. For create, the CLOSE, HALF, and DOUBLE size hints are computed from the number of leaves in the cluster \n(which is known when the cluster s root is created) by assuming an average leaf-node size of 500 bytes \nand a root-node size of 428 bytes. CLOSE is the actual estimate, while HALF and DOUBLE are each deliberately \nwrong by a factor of two. AVG (average) always supplies a size hint of 5,428 bytes, the overall average \ntree size. Hints for grow operations are computed similarly, based on the number of leaves to be added. \nFigure 16 shows the effect of size hints on Create perfor-mance for VMIN. It also includes data for FIXED, \nNEAR87, and NONE from Figure 7 for comparison. The VMIN al- 1 Size hint calculation 1 Label Size hint \n= Estimated tree size Size hint = Estimated tree size 2 Size hint = Estimated tree size * 2 Size hint \n= Overall average tree size AVG Table 3: Methods for calculating size hints and their labels. gorithm \nbenefits significantly from accurate size hints, while successfully tolerating incorrect hints. The CLOSE \nhints de-creased the elapsed time by 28%, bringing it to within 20% of the time to create the database \nwith fixed-size clusters. Figure 17 shows that size hints had a similar but less pro- nounced effect \non the Update workload. We also measured the performance of Uniform traversals on databases created with \nsize hints (not shown). We saw practically no variation, showing that size hints have little impact on \nthe quality of the resulting database layout. Results for VDENSE (also not shown) were similar. NOHINT \nDOUBLE HALF AVG CLOSE FIXED NEAR87 NONE I\"\"\"\"'I\"\"\"\"'1 0 100 200 300 Time (seconds) Figure 16: Create \nworkload with size hints: Elapsed time (VMIN). AVG CLOSE FIXED NEAR87 NONE 1000 2000 3000 Time (seconds) \nFigure 17: Update (VMIN). workload with size hints: Elapsed time Conclusions Effective fine-grained \nclustering algorithms for object-ori-ented database systems must not just avoid splitting clus-ters across \nmultiple pages-they must also provide good space utilization and must be easy to use. Two general-purpose \nclustering mechanisms available in current commer-cial and experimental object-oriented database systems \nare near hints and fixed-size clusters. Near hints are found in virtually every OODB system available \ntoday, as they are easy to implement, easy to use, and achieve excellent space utilization. Unfortunately, \nas we have shown, they don t work at at all well for clustering. They are only marginally better than \nnothing when populating a database, and they are no help for maintaining clusters over time. Fixed-size \nclusters achieved excellent clustering in some of our tests, but in general they are cumbersome to use \nand can lead to poor space utilization due to internal fragmentation. To address the shortcomings of \nnear hints and fixed-size clusters, we introduced Vclusters, a new fine-grained, general-purpose clustering \nmechanism that supports clusters whose sizes can vary dynamically according to the amount of data that \nthey contain. Vclusters are designed to be as easy to use as near hints, but to be much more effective \nthan near hints at clustering data and supporting incremental cluster maintenance. They can be used either \nas the back-end of an automatic clustering algorithm or directly by applica-tion programmers. We presented \ntwo specific algorithms for implementing Vclusters. Both algorithms attempt to avoid splitting clusters \nwhile maintaining good space utilization. The algorithms differ only when these goals conflict; each \nal- gorithm favors one goal at the possible expense of the other. We implemented both algorithms in Shore \nand compared them to near hints and fixed-size clusters. The results of our performance study point out \nthe ineffectiveness of near hints, even when used in conjunction with a fill factor. IYa-versals of databases \nbuilt using our Vcluster algorithms were more than twice as fast as traversals of databases built using \nnear hints, except in simple cases where the database was built with no interleaving. Traversal performance \nfor near hints improved by about 18% when a fill factor of 50% was used, but Vclusters still outperformed \nnear hints by nearly a factor of two. Our results also show that for some workloads, such as our Uniform \ntraversals, it is possible for fixed-size clusters to be as effective as Vclusters in terms of traversal \nperformance. However, fixed-size clusters were shown to give very poor space utilization in all of our \nexperiments. As a result, other traversals, such as our Working Set traversals, were up to twice as slow \nwith fixed-size clusters as they were for Vclusters. Vclusters gave superior performance on all of the \nread-only traversals in our performance study. Although this performance came at the expense of somewhat \npoorer per-formance for update-intensive workloads, our results suggest that Vclusters will provide better \noverall performance than near hints or fixed-size clusters when traversals are more common than updates. \nA comparison of our two Vclus-ter algorithms shows that Vcl-minsplit has a higher update cost than Vcl-dense \nbut it is also more effective at improving traversal performance, even for heavily updated databases. \nFinally, we saw that the extra cost of creating Vclusters can be significantly decreased with the provision \nof accurate size hints when clusters are created. For example, accurate size hints improved creation \nperformance as much as 38% for Vcl-minsplit and also helped with updates (although not as much). On the \nother hand, inaccurate size hints were shown to be harmless, both in terms of performance and in terms \nof the quality of clustering produced in the face of inaccurate hints. Several fertile areas remain for \nfuture work. One such area is related to the concurrency and recovery aspects of moving objects. While \nwe mentioned possible approaches here, experimentation is needed to explore their tradeoffs. More generally, \nexperimentation with true multiuser work-loads is needed to investigate the impact of locking, par-titularly \nfor create-intensive workloads, to verify that no unexpected concurrency bottlenecks axe introduced by \nour clustering algorithms. References [BD89] V. Benzaken and C. Delobel. Dynamic Clustering Strategies \nin the 02 Object-Oriented Database Sys-tem. Technical Report TR 34-89, Altdr, August 1989. [BKKG88] J. \nBanerjee, W. Kim, S. Kim, and J. Garza. Clus-tering a DAG for CAD Databases. IEEE TSWE, 14(11):1684-1699, \nNovember 1988. [BS194] E. Bertino, A. Saad, and M.A. lsmail. Clustering Techniques in Object Bases: A \nSurvey. Data and Knowledge Engineering, 12(3):255-275, May 1994. [CDK+94] M. Carey, D. Dewitt, C. Kant, \n, and J. Naughton. A Status Report on the 007 OODBMS Benchmarking Effort. In Proceedings International \nConference on Object-Oriented Progmmming Systems, Languages, and Applications, pages 414-426, 1994. (CDN93] \nM. Carey, D. Dewitt, and J. Naughton. The 007 Benchmark. In Proceedings ACM SIGMOD, pages 12-21, Washington, \nDC, May 1993. [CHSl] J. Cheng and A.R. Hurson. Effective Clustering of Complex Data in Object-Oriented \nDatabase Sys-tems. In Proceedings ACM SIGMOD, pages 22-31, 1991. [CK89] E. Chang and R. Katz. Exploiting \nInheritance and Structure Semantics for Effective Clustering in an Object-Oriented Database System. In \nProceedings ACM SIGMOD, pages 348-357, 1989. [csL+so] M. Carey, E. Shekita, G. Lapis, B. Lindsay, and \nJ. McPherson. An Incremental Join Attachment for Starburst. In Proceedings VLDB, pages 662-673, 1990. \n[DeuSO] 0. Deux. The Story of 02. IEEE TKDE, 2(1):91-108, 1990. [DKHSO] P. Drew, R. King, and S. Hudson. \nThe Performance and Utility of the Cactis Implementation Algorithms. In Proceedings VLDB, pages 135-147, \n1990. [GA941 0. Gruber and L. Amsaleg. Object Grouping in Eos. In M. &#38;zu, U. Dayal, and P. Valduriez, \neditors, Dis-tributed Object Management, pages 117-131. Morgan Kaufmann, 1994. [GKKM94] C. Gerlhof, A. \nKemper, C. Kilger, and G. Moerkotte. Partition-Based Clustering in Object Bases: From Theory to Practice. \nIn Proceedings FODO, pages 301-316, 1994. [HCL+SO] L. Hass, W. Chang, G. Lohman, J. McPherson, P. Wil&#38;, \nG. La&#38;, B. Lindsay, H. Pirahesh, M. Carev. and E. Shekita. Starburst Mid-Flight: As the Dust Clears. \nIEEE TKDE, 2(1):143-160;March 1990. [HK89] S. Hudson and R. King. Cactis: A Self-Adaptive, Concurrent \nImplementation of an Object Oriented DBMS. ACM TODS, 14(3):291-321, September 1989. [HZ871 M. Hornick \nand S. Zdonik. A Shared, Segmented Memory System for an Object-Oriented Database. A CM TOIS, 5( 1):273-285, \n1987. [McA97] M. McAuliffe. Storage Management Methods for Ob-ject Database Systems. Computer Sciences \nDepart-ment, University of Wisconsin, Madison, WI, June 1997. [MCS96] [MHL+92] [MJLF84] (MK94] [Obj95] \n[Sal861 [S6d81] [SS90] [TN911 [TN921 [ZSL97] M. McAuliffe, M. Carey, and M. Solomon. To-wards Effective \nand Efficient Free Space Manage-ment. In Proceedings ACM SIGMOD, pages 389-400, Montrkal, Canada, June \n1996. C. Mohan, D. Haderle, B. Lindsay, H. Pirahesh, and P. Schwarz. ARIES: A Transaction Recovery Method \nSupporting Fine-Grain Locking and Partial Rollbacks Using Write-Ahead Loagine;. ACM TODS. 17(1):94-162, \nGarth 1992. --- M. McKusick, W. Joy, S. Leffler, and R. Fabry. A Fast File System for UNIX. ACM Transactions \non Computer Systems, 2(3):181-197, August 1984. W. Mclver, Jr. and R. King. Self-Adaptive On-Line Reclustering \nof Complex Object Data. In Proceedings ACM SIGMOD, pages 407-418, 1994. Object Design, Inc., Burlington, \nMA. ObjectStore C++ API User Guide, Release 4.0, June 1995. B. Salzburg. An Introduction to Data Base \nDesign. Harcourt Brace Jovanovich, 1986. L. S8derlund. Concurrent Database Reorganiza-tion-Assessment \nof a Powerful Technique Through Moduling. In Proceedings VLDB, pages 499-509, 1981. K. Shannon and R. \nSnodgrass. Semantic Clustering. In Proceedings of the Fourth International Work-shop in Persistent Object \nSystems. Martha s Vin-yard, MA, pages 361-374, September 1990. M. Tsangaris and .I. Naughton. A Stochastic \nAp-proach for Clustering in Object Bases. In Proceedings ACM SIGMOD, pages 12-21, 1991. M. Tsangaris \nand J. Naughton. On the Performance of Object Clustering Techniques. In Proceedings ACM SIGMOD, pages \n144-153, 1992. C. Zou, B. Salzburg, and R. Ladin. Back to the Fu-ture: Dynamic Hierarchical Clustering. \nIn Proceed-ings ICDE, 1997.  \n\t\t\t", "proc_id": "286936", "abstract": "We consider the problem of delivering an effective fine-grained clustering tool to implementors and users of object-oriented database systems. This work emphasizes on-line clustering <i>mechanisms</i>, as contrasted with earlier work that concentrates on clustering <i>policies</i> (deciding which objects should be near each other). Existing on-line clustering methods can be ineffective and/or difficult to use and may lead to poor space utilization on disk and in the disk block cache, particularly for small- to medium-size groups of objects. We introduce variable-size clusters (<i>Vclusters</i>), a fine-grained object clustering architecture that can be used directly or as the target of an automatic clustering algorithm. We describe an implementation of Vclusters in the Shore OODBMS and present experimental results that show that Vclusters significantly outperform other mechanisms commonly found in object database systems (fixed-size clusters and near hints). Vclusters deliver excellent clustering and space utilization with only a modest cost for maintaining clustering during updates.", "authors": [{"name": "Mark L. Mcauliffe", "author_profile_id": "81100378474", "affiliation": "TimesTen Performance Software, 2085 Landings Drive, Mountain View, CA", "person_id": "P328172", "email_address": "", "orcid_id": ""}, {"name": "Michael J. Carey", "author_profile_id": "81100229036", "affiliation": "IBM Almaden Research Center, 650 Harry Road, K55/B1 San Jose, CA", "person_id": "PP14089385", "email_address": "", "orcid_id": ""}, {"name": "Marvin H. Solomon", "author_profile_id": "81452606415", "affiliation": "Computer Sciences Department, University of Wisconsin-Madison, Madison, WI", "person_id": "PP39079397", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/286936.377403", "year": "1998", "article_id": "377403", "conference": "OOPSLA", "title": "Vclusters: a flexible, fine-grained object clustering mechanism", "url": "http://dl.acm.org/citation.cfm?id=377403"}