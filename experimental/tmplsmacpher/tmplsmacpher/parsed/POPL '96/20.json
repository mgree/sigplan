{"article_publication_date": "01-01-1996", "fulltext": "\n T~pe-Directed Partial Evalt~ation Olivier Danvy * Computer Science Department Aarhus University t (danvyQdaimi.aau. \ndk) Abstract We present a strikingly simple partial evaluator, that is type\u00ad directed and reifies a compiled \nprogram into the text of a re\u00ad sidual, specialized program. Our partial evaluator is concise (a few lines) \nand it handles the flagship examples of off\u00ad line monovariant partial evaluation. Its source programs \nare constrained in two ways: they must be closed and mono\u00ad morphic ally t ypable. Thus dynamic free variables \nneed to be factored out in a dynamic initial environment . Type\u00addirected partial evaluation uses no symbolic \nevaluation for specialization, and naturally processes static computational effects. Our partial evaluator \nis the part of an offline partial eval\u00aduator that residualizes static values in dynamic contexts. Its \nrestriction to the simply typed lambda-calculus coincides with Berger and Schwichtenberg s inverse of \nthe evaluation functiona~ (LICS 91), which is an instance of normalization in a logical setting. As such, \ntype-directed partial evaluation essentially achieves lambda-calculus normalization. We ex\u00adtend it to \nproduce specialized programs that are recursive and that use disjoint sums and computational effects. \nWe also analyze its limitations: foremost, it does not handle in\u00adductive types. This paper therefore \nbridges partial evaluation and J\u00adcalculus normalization through higher-order abstract syn\u00adtax, and touches \nupon parametricity, proof theory, and type theory (including subtyping and coercions), compiler op\u00adtimization, \nand rut-time code generation (including recom\u00adpilation ). It also offers a simple solution to denotational \nsemantics-based compilation and compiler generation. 1 Background and Introduction Given a source program \nand parts of its input, a partial evaluator reduces static expressions and reconstructs dy\u00ad namic expressions, \nproducing a residual, specialized pro\u00adgram [15, 36]. To this end, a partial evaluator needs some method \nfor inserting (lifting) arbitrary statically-calculated * Supported by BRICS (Basic Research in Computer \nS., emce, Centre of the Danish National Research Foundation) tNY M~&#38;e@e, Bulldmg 540, 11~-80011 Aarhw \nc, Denmark Home page http:/luww. daLml .aau. dk/-danvy Permission to make digitdhard copies of all or \npart of this material for personal or classroom use is granted without fee provided that the copies are \nnot made or distributed for ~rotit or commercial advantage, the copy\u00ad right notice, the title of the \npubkation and its date appear, and notice ia given that copyright is by permission of the ACM, Inc. To \ncopy otberwiae, to republish, to post on servers or to redistribute to lists, requires specific permission \nandlor fee. POPL 96, St. Petersburg FLA USA @ 1996 ACM @89791 _769.3/95/ol . .$3.5(3 values into the \nresidual program i.e., for residualizing them (Section 1.1). We present such a method (Section 1.2); \nit is type directed and we express it using Nielson and Niel\u00adson s two-level A-calculus [44]. After a \nfirst assessment (Sec\u00adtion 1.3), we formalize it (Section 1.4) and outline a first application: given \na compiled normal form and its type, we can recover its text (Section 1.5). We implement type\u00addirected \nresidualization in Scheme [IO] and illustrate it (Sec\u00adtion 1.6). The restriction of type-directed residualization \nto the simply typed ~-calculus actually coincides with Berger and Schwlchtenberg s normalization algorithm \nas presented in the proceedings of LICS 91 [3], and is closely related to Pfenning s normalization algorithm \nin Elf [46] (Section 1.7). Residualization also exhibits a normalization effect. Moving beyond the pure \nA-calculus, we observe that this effect ap\u00adpears for constants and their operators as well (Section 1.8). \nWe harness it to achieve partial evaluation of compiled pro\u00adgrams (Section 2). Because this form of partial \nevaluation is completely directed by type information, we refer to it as type-directed parttal evaluation. \nSection 3 extends type-directed partial evaluation to dis\u00adjoint sums. Section 4 analyzes the limitations \nof type\u00addirected partial evaluation. Section 5 reviews related work, and Section 6 concludes. 1.1 The \nproblem Let us consider the example term where the infix operator @ denotes function application (and \nassociates, as usual, to the left). Both g and d are unknown, t. e., dynamic. d has type bl and g has \ntype bl ~ (bl + bl ) -+ bz, where bl and bz are dynamic base types. ~ occurs twice in this term: as a \nfumction in an ap\u00adplication (where its denotation Aa.u could be reduced) and as an argument in a dynamic \napplication (where its denota\u00adtion ~a. a should be residualized). The question is: what are the binding \ntimes of this term? This paper addresses closed terms. Thus let us close the term above by abstracting \nits two free variables. For added clarity, let us also declare its types. We want to decorate each A-abstraction \nand application with static annotations (overlines) and dynamic annotations (un\u00adderlines) in such a way \nthat static reduction of the decorated term does not go wrong and yields a completely dynamic This annotation \nis correct. Static reduction yields the fol\u00ad term. These are the usual rules of binding-time analysis, \nwhich is otherwise abundantly described in the literature [4, 6, 11, 15, 36, 37, 41, 44]. In the rest \nof this paper, we use Nielson and Nielson s two-level J-calculus, which is sum\u00admarized in Appendix A. \nBefore considering three solutions to analyzing the term above, let us mention a non-solution and why \nit is a non\u00adsolution. Non-solution: This annotation is appealing because the application of ~ is static \n(and thus will be statically reduced away), but it is incorrect because the type of g is not entirely \ndynamic. Thus after static reduction, the residual term is not entirely dynamic either: Ag : bl~(bl=bl)~bz.~d: \nbl.g~d~~a : bl.a In Scheme, the residual program would contain a closure: > (let* ([g (gensym! g )] \n[d (gensym! d )]) (lambda (,g) (lambda (,d) ,((lsnbda (f) ((, g ,(f d)) ,f)) (lambda (a) a))))) (lambda \n(g15) (lambda (d16) ((g15 d16) #<procedure>))) > In summary, theannotation is incorrect because~a : bl.a \ncan only declassified to be static (i. e., of type bl~bl) if ~ is always applied. Thus it should declassified \nto be dynamic (t.e., of type b, ~bl ), as done in Solution 1. Solution 1: This solution is correct, but \nit does not give a satisfactory result: static reduction unfolds the outer call, duplicates the denotation \nof $, and creates an inner /3-redex: ~g : blti(bltibl)tibz. Ad : bl.gg((~a : bl.a)~d)g~a : bl.a To remedy \nthis shortcoming, the source program needs a binding-time improvement [36, Chapter 12], z.e., a modifica\u00adtion \nof the source program to make the binding-time analysis yield better results. The particular binding-time \nimprove\u00adment needed here is eta-expansion, as done in Solution 2. Solution 2: Eta-expanding the second \noccurrence of ~ makes it always occur in position of application. Therefore Au : bl .a can be classified \nto be static. lowing term: The result is optimal. It required, however, a binding-time improvement, \ni.e., human intervention on the source term. Recent work by Malmkjzer, Palsberg, and the author [18, \n19] shows that binding-time improvements compensate for the lack of binding-time coercions in existing \nbinding\u00adtime analyses. Source eta-expansion, for example, provides a syntactic representation for a binding-time \ncoercion between a higher-order static value and a dynamic context, or con\u00adversely between a dynamic \nvalue and a static higher-order context. Binding-time analyses therefore should produce annot\u00adated terms \nthat include binding-time coercions, as done in Solution 3. We use a down arrow to represent the coercion \nof a static (overlined) value into a dynamic (underlined) ex\u00adpression. Solution 3: In this solution, \nthe coercion of ~ from bl ~ bl to bl ~bl is written JbI+bl ~: One possibility y is to represent the coercion \ndirectly with a two-level eta-redex, to make the type structure of the term syntactically apparent [18, \n19]. The result of binding-time analysis is then the same as for Solution 2. Another possibility is to \nproduce the binding-time coer\u00adcion as such, without committing to its representation, and to leave it \nto the static reducer to treat this coercion appro\u00adpriately. This treatment 2s the topic of the present \npaper. In Scheme: > (let* ([g (gensym! g )] [d (gensym! d )] ) (lambda ( ,g) (lambda (,d) ,((lambda \n(f) ((, g ,(f d)) ,(residualize f (a -> a)))) (lambda (a) a)) ) ) ) (lambda (g23) (lambda (d24) ((g23 \nd24) (lambda (x25) x25)))) > Specifically, this paper is concerned with the residualiza\u00adtion of static \nvalues in dynamic contexts, in a type-directed fashion. The solution to this seemingly insignificant \nproblem turns out to be widely applicable. 1.2 Type-directed residualization We want to map a static \nvalue into its dynamic counterpart, given its type. The leaves of a type are dynamic and ground. In the \nrest of the paper, types where all constructors are static (resp. cly\u00adnamic) are said to be completely \nstatic (resp. completely dynamic ). At base type, residualization acts as the identity function: $bu \n=u Residualizing a value of product type amounts to resid\u00adualizing each of its components and then reconstructing \nthe product: pair(~ Giu,J ~t) 4t xt2rJ = Onecan infer from Solution 3 that Jb +b v = &#38;l. LIGxl (where.zl \nis fresh) andmore generally that b+t J-u= ~.c.~t (w@x). At higher types, the fresh variable needs to \nbe coerced from dynamic to static. A bit of practice with two-level eta\u00adexpansion [16, 17, 18] makes \nit clear that, for example: ~(b,+h)+b, ~ = &#38;l.2JC!(AJ3.zlgz3) It is therefore natural to define \na function ~ that is sym\u00admetric to .J, i.e., that coerces its argument from dynamic to static, and to \ndefine the residualization of functions as follows. Jt,+ , u = &#38;l. J (u6(~t1 Zl)) The functions ~and \n~essentially match the insertion of two-level eta-redexes for binding-time improvements [18, 19]. Figure \n1 displays the complete definition of residualization. Type-directed residualization maps a completely \nstatic two-level A-term into a completely dynamic one. First, reify (~) and reflect (T) fully eta-expand \na static two-ievel ~-terrn with two-level eta-redexes. Then, static reduction evaluates all the static \nparts and reconstructs all the dynamic parts, yielding the residual term.  1.3 Assessment So far, we \nhave seen (1) the need for binding-time coercions at higher types in a partial evaluator; (2) the fact \nthat these binding-time coercions can be represented as two-level eta\u00adredexes; and (3) an interpreter \nfor these binding-time coer\u00adcions t. e., type-directed residualization. Let us formalize type-directed \nresidualization, and then describe a first application, 1.4 Formalization Proposition 1 In Figure 1, \nr-ez fy maps a szmply typed com\u00adpletely static A-term tnto a well-typed two-level A-term. Proo&#38; by \nstructural induction on the types (see Appendix A for the notion of well-typing). Property 1 In the simply \ntyped case, static reduction in the two-level A-calculus enjoys both strong normalization and subject \nreductton [4J]. Corollary 1 Static reduction after reification (see Figure 1) does not go wrong and yzelds \ncompletely dynamtc terms. t c Type ::= bltl+t21tlxt2 v E Value ::= c I .z I Xz. zl I vo6rJl I pair(ul, \nu2) [ fst u I snd u ecExpr ::= cIz I&#38;.e IeOQel \\ pair(el, ez) I fste I snde . reify = At. Ju : t.~t \nv Type -+ Value -+ TLT &#38;bu = u &#38;l.Jt (rJ6(~t, z,))4 1+ 2 rJ = where z 1 is fresh. pair(~ = v,~t \n~ u)4 1 2 t = reflect = At.Ae : t.~t e Type -+ Expr ~ TLT ~t,e =e = Xul.tt, (e CJ(J ZJI))Tt, +t, e Tt,xt,e \n= pair(~t, &#38;e, Tt, @e) residualize = statically-reduce o reify Type + Value + Expr Since the definition \nof type-directed residualization is based jolely on the structure of types, we have omitted type an\u00ad \nnotations. The domains Value and Expr are defined inductively, fol\u00ad owing the structure of types, and \nstarting from the same set ]f (dynamic) base types. TLT is the domain of (well-typed) iwo-level terms; \nit contains both Value and Expr. The down arrow is read reify itmaps a static value and ts type into \na two-level J-term that statically reduces to the iynamic counterpart of this static value. Conversely, \nthe up mrow is read reflect: it maps a dynamic expression into a two-level A-term representing the static \ncounterpart of this dynamic expression. In residualize, reify (resp. reflect) is applied to types oc\u00ad \ncurring positively (resp. negatively) in the source type. N.B. One could be tempted to define ffevap \n= dynamically-reduce o reflect by symmetry, but the result is not an evaluation functional because base \ntypes are dynamic. Figure 1: Type-directed residualization Corollary 2 The type of a source ter-m and \nthe type of a r+\u00adsidual term have the same shape (z. e., erast,ng thetr annota\u00adtions ytelds the same \nszmple type). This last property extends to terms that are in long ~rp normal form [30], t. e., to normal \nforms that are completely eta-expanded. By Proposition 1, we already know that static reduction of a \ncompletely static term in long /3q-normal form yields a completely dynamic term in long /3q-normal form \nWe have independently proven the following proposition. (define-record (Base base-type) ) (define-record \n(Func domain range)) (define-record (Prod type type) ) (define residualize (lambda (V t) (letrec ([reify \n(lambda (t V) (case-record t [(Base -) VI [@me tl t2) (let ( [xl (gensym! )] ) (lambda (,x1) , (reify \nt2 (V (reflect tl xi)))))] [(prod tl t 2) (cons ,(reify ti (car v)) ,(reify t2 (cdr v)))]))] [reflect \n(lambda (t e) (case-record t [(Base -) e] [(Func tl t 2) (lambda (vI) (reflect t2 (, e ,(reify tl VI))))] \n[(Prod tl t2) (cons (reflect tl (car ,e)) (reflect t2 (cdr ,e)))] ))]) (begin (reset-gensym! ) (reify \n(parse-type t) v))))) Figure 2: Type-directed partial evaluation in Scheme Proposition 2 Residualizinq \na comdetelv statzc term an long -> C) is mapped into (make-Func (make-Prod (make-Base A) /3q-hormal form \nyields a ~erm w;th th; same shape (i. e~, (make-Base B) ) (make-Base C)). erasing the annotations of \nboth terms yields the same szmply The following Scheme session illustrates this implement typed A-term, \nmodulo a-renammg). ation: In other words, residualization preserves both the shape > (define S (lambda \n(f) (lambda (g) (lambda (x) of types and the shape of expressions that are in long ~q\u00ad normal form. \n((f x) (g x)))))) >s  1.5 Application #<procedure S> > (residualize S We can represent a completely \nstatic expression with a com\u00ad ((A -> B -> C) -> (A->B) ->A -> C)) piled representation of this expression, \nand a completely dy\u00ad (lambda (xO) namic expression with a (compiled) program constructing (lambda (xl) \nthe textual representation of this expression. 1 In Consel s (lambda (x2) partial evaluator Schism, for \nexample, thk representation is ((XO X2) (xl x2))))) used to optimize program specialization [13]. Since \n(1) re-> (define I*K (cons (lambda (x) x) (lambda (y) (lambda (z) y)))) ification amounts to mapping \nthis static expression into a > I*K two-level term and (2) static reduction amounts to running (#<procedure> \n#<procedure>) both the static and the dynamic components of this two\u00ad > (residualize I*K ((A -> A) * \n(B -> C > B))) level term, type-directed residualization constructs the tex\u00ad (cons (lambda (xO) xO)tual \nrepresentation of the original static expression. There\u00ad (lambda (xl) (lambda (x2) xl) ) ) fore, in principle, \nwe can map a compiled program back into > its text under the restrictions that ( 1 ) this program ter\u00ad \n. minates, and (2) it has a type. s and I *K denote values that we wish to residualize. We know The following \nsection illustrates this application their type. Procedure residualize maps these (static, conl\u00adpiled) \nvalues and a representation of their type into the cor\u00adresponding (dynamic, textual) representation of \nthese val\u00ad 1.6 Type-directed residualization in Scheme ues. Figure 2 displays an implementation of type-directed \nre-At this point, a legitimate question arises: how does thzs sidualization in Scheme, using a syntactic \nextension for realty work ? Let us consider the completely static expression declaring and using records \n[10]. Procedure pars e-t ype maps the concrete syntactic representation of a type (an S-Xx.x expression) \ninto the corresponding abstract syntactic repres\u00adtogether with the type b -+ b. This expression is mapped \nentation (a nested record structure} # For example, ((A * B) into the following eta-expanded term: lThe \nsame situation occurs with interpreted instead of compiled representations, s.e., If one uses an interpreter \ninstead of a compder Az(ix.x)a :. Static ~-reduction yields the completely dynamic residual term Az. \nz which constructs the text of the static expression we started with. Similarly, the S combinator is \nmapped into the term Ja.Ab.A..SG (Xd.Xe.(a Qd)Qe)~(Xj.bQ~)@c . which statically L-reduces to the completely \ndynamic resid\u00adual term ~a.~b.~c.(a~c)tl(b~c). Let us conclude with a remark: because residual terms are \neta-expanded, refining the type parameter yields different residual programs, as in the following examples. \n> (residualize (lambda (x) x) ((a * b) -> a * b)) (lambda (xo) (cons (car xo) (cclr xo))) > (residualize \n(lambda (x) x) (((fI -> B) -j c) -j (A -> B) -j C)) (lambda (xol (lambda (xl) (xO (lambda (x2) (xl x2)J))) \n> 1.7 Strong normalization The algorithm of type-directed residualization is actually well known. In \ntheir paper An Inverse of the Evaluation Func\u00adtional for Typed J-Calculus [3], Berger and Schwichten\u00adberg \npresent a normalization algorithm for the simply typed ~-calculus. It is used for normalizing proofs \nas programs. Berger and Schwichtenberg s algorithm coincides with the restriction of type-directed residualization \nto the simply typed J-calculus. Reify maps a (semantic, meta-level) value and its type into a (syntactic, \nobject-level) representation of this value ( syntactic in the sense of abstract-syntax tree ), and conversely, \nreflect maps a syntactic represent\u00adation into the corresponding semantic value. Disregarding the dynamic \nbase types, reflect thus acts as an evaluation f~mctional, and reify acts as its inverse hence probably \nthe title of Berger and Schwichtenberg s paper [3]. In the implementation of his Elf logic programming \nlan\u00adguage [46], Pfenning uses a similar normalization algorithm to test extensional equality, though \nwith no static/dynamic notion and also with the following difference. When pro\u00adcessing an arrow type \nwhose co-domain is itself an arrow type, the function is processed en bloc with all its arguments: Jt1+t2+ \n+t.+1 ~ = XI-,.Aq. . Arn.J ~+ (zl@(tt, zl)Q(tt2z2)@... @(ttnrn)) where tn+ ~ is not an arrow type and \nxl, .... x~ are fresh. (The algorithm actually postpones reflection until reification reaches a base \ntype. ) Residualization also etilbits a normalization effect, as illustrated below: we residualize the \nresult of applying a procedure to an argument. This program contains an ap\u00adplication and thzs applicataorz \nis performed at residualization time.2 > (define foo (lambda (f) (lambda (x) (f x)))) > (residualize \n(foo (lambda (z) z)) (A -> A)) (lambda (xO) xO) > Or, viewing resldual]zation as a form of decompdatlon \n(an analogy due to Goldberg [27]). at decompile time The same legitimate question as before arises: how \ndoes this really work ? Let us consider the completely static ex\u00adpression This expression is mapped into \nthe following eta-expanded term: ~g. ((xf.xz.faz) a(xz.z))ay. Static @reductions yield the completely \ndynamic residual term ~y.y. As an exercise, the curious reader might want to run the residualizer on \nthe term S d 1( 6 K with respect to the type b + b. The combinators S and K are defined as above [2, \nDefinition 5.1.8, Item (i)]. 1.8 Beyond the pure J-calculus Moving beyond the pure )-calculus, let us \nreiterate this last experiment: we residualize the result of applying a procedure to an argument, and \na multiplication is computed at resid\u00ad uaii,zatton tzme. > (define bar (lambda (x) (lambda (k) (k (* \nx 5))))) > (residualize (bar 100) ((Int -> Ans) -> Ans)) (lambda (xO) (xO 500)) > The usual legitimate \nquestion arises: how does this really work ? Let us consider the completely static expression (Iz.Nc.k6(xY5)) \n5100. This expression is mapped it into the following eta-expanded term: &#38;.((Xz.R.k ?i3(z X5))@ 100) \nG(Xn.a Qra) Static /3-reduction leads to &#38;LaQ(looY5) which is statically &#38;reduced to the residual \nterm Au. a @ 500. Remark: Introducing a static fixed-point operator does not compromise the subject-reduction \nproperty, so the second part of Corollary 1 in Section 1.4 can be rephrased with the proviso if static \nreduction terminates . 1.9 This paper The fact that arbitrary static reductions can occur at re\u00adsidualization \ntime suggests that residualization can be used as a full-fledged partial evaluator for closed compiled \npro\u00adgrams, given their type. In the following section, we apply it to various examples that have been \npresented as typical or even significant achievements of partial evaluation, in the literature [15, 33, \n36]. These examples include the power and the format source programs, and interpreters for Paulson s \nimperative language Tiny and for the A-calculus. The presentation of each example is structured as follows: \nwe consider interpreter-like programs, t. e., programs where one argument determines a part of the control \nflow (Abelson, [24, Foreword]); > (define power (lambda (x n) (letrec ([1OOP (lambda (n) (cond [(zero? \nn) 11 [(odd? n) (* x (1OOP (1-n)))] [else (sqr (loop (/ n 2)))1) )1) (loop n)))) > (define sqr (lambda \n(x) (* x x))) > (power 2 10) 1024 > (define power-abstracted ; ; ; Int -> (Int -> Int) * (Int * Int \n=> Int) => Int > Int (lembda (n) (lambda (sqr *) (lambda (x) (letrec f [loop (lambda (n) (concl [(zero? \nn) 11 [(odd? n) (* x (1OOP (1-n)))] [else (sqr (loop (/ n 2)))1))1) (loop n)))))) > (((power-abstracted \n10) sqr *) z) 1024 > (residualize (poper-abstracted 10) ((Int -> Int) * (Int * Int => Int) => Int -> \nInt)) (lembda (xO xl) (lambda (x2) (xO (xI X2 (xO (xO (x1 x2 l))))))) > (((lambda (xO xI) (lambda (x2) \n(xO (x1 X2 (xO (xO (xl x2 1))))))) sqr *) 2) 1024 > The residualized code reads better after a-renaming. \nIt is the specialized version of power when n is set to 10: (lambda (sqr *) (lembda (x) (sqr (* x (sqr \n(sqr (* x 1)))))))  N.B. For convenience, our implementation of residual ize, unlike the simpler version \nshown in Figure 2, handles Scheme-style uncurried rz-ary procedures. Their types are indicated in type \nexpressions by 1 => preceded by the n-ary product of the argument types. Figure 3: Type-directed partial \nevaluation of power (an interactive session with Scheme) we residualize the result of applying these \n(separately 2.1 Power compiled) programs to the corresponding argument. Figure 3 displays the usual definition \nof the power procedure Because residualization is type-directed, we need to know in Scheme. and its abstracted \ncounter~art where we have the type of the free variables in the residual program. We factored out the \nresidual operators sq; and *. The figure will routinely abstract them in the source program, as a form \nillustrates that residualizing the partial application of pove r of initial rumtime environment , hence \nmaking the residual to an exponent yields the specialized version of power with program a closed A-term. \nrespect to this exponent Type-Directed Partial Evaluation 2.2 Format For lack of space, we omit the \nclassical example of par-The following examples illustrate that residualization yields tial evaluation: \nformatting stringe. Its source code can be program is a simply typed combinator i.e., with no free found \nin Figure 1 of Consel and Danvy s tutorial notes on specialized programs, under the condition that the \nresidual  variables and with a simple type. The static parts of the partial evaluation at POPL 93 [15]. \nType-directed partial source program, however, are less constrained than when evaluation yields the same \nrest dual code as the one presen\u00ad using a partial evaluator: they can be untyped and impure. ted in the \ntutorial notes (modulo of course the factoriza- In that sense it is symmetric to a partial evaluator \nsuch as tion of the residual operators write-string, urite-nurnber, Gomard and Jones s J-Mix [35, 36] \nthat allows dynamic com-urite-newline, and list -ref). putations to be untyped but requires static computations \nto same be typed. In any case, residualization produces the 2.3 Definitional interpreter for Paulson \ns Tiny language result as conventional partial evaluation (i. e., a specialized Recursive procedures \ncan be defined with fixed-point operat\u00ad program) but is naturally more efficient since no program ors. \nThis makes it simple to residualize recursive procedures analysis other than type inference and no symbolic \ninter\u00ad . by abstracting their (typed) fixed-point operator. pretation take place. As an example, let \nus consider Paulson s Tiny language 3X.Mix and type-directed partial evaluation both consider closed \n[45], which is a classical example in partial evaluation [6,8. source programs They work alike for typed \nsource programs whose bmdlng t]mes have been improved by source eta-expansion block res, val, aux in \nval := read ;aux := 1; while val > 0 do aux := aux *val ;val := val -1 end ; res := aux encl Figure \n4: Source factorial program [lsmbda (add sub mul eq gt read fix true? lookup update) (lambda (k8) (lambda \n(s9) (read (lsmbda (vIO) (update 1 v1O S9 (lambda (s11) (update 2 1 sII (lsmbda (s12) ((fix (lambda (while) \n (lsmbda (s14) (lookup 1 s14 (lsmbda (v15) (gt v15 O (lambda (v16) (true? v16 (lambda (s17) (lookup 2 \n.17 (lambda (v18) (lookup 1 s17 (lambda (vi9) (mul v18 vi9 (lambda (v20) (update 2 v20 517 (lambda (s21) \n(lookup 1 s21 (lsmbda (v22) (sub v22 1 (lambda (v23) (update i v23 s2i (lsmbda (s24) (while s24)) )))))))))))))) \n(lsmbda (s25) (lookup 2 s25 (lambda (v26) (update O v26 s25 (lambda (s27) (k8 s27)) )))) SIB)))))))) \nSIB))))))))))  rhis residual program is a specialized version of the Tmy nterpreter (Figures 9 and 10) \nwith respect to the source pro\u00adg-am of Figure 4. As can be observed, it is a continuation\u00ad~assing Scheme \nprogram threading the store throughout. 17he while loop of Figure 4 has been mapped into a fixed\u00ad~oint \ndeclaration. All the location offsets have been com\u00admted at partial-evaluation time. Figure 5: Residual \nfactorial program (after a-renaming and pretty printing) (define instantiate-type (lambda (t) (((() \n=> Exp) > Exp) * ;;; reset -gensym-c ((Str -> Exp) -> Exp) * ;;; gensym-c (EXP -~ Exp) * ::: unparse-expression \n(Str -> Var) * ;;; make-Var (Str * Exp => Exp) * ;;; make-Lam (EXP * Exp > EXP) * ;;; make-App (EXP * \nEXP > EXP) * ;;; make-Pair (EXP j EXP) * ;;; make-Fst (EXP -> EXP) ;;; make-Snd => ,t -> Exp))) Figure \n6: Type construction for self-application I 9, 11, 14, 35, 36, 37, 41, 48]: (P9~) ::= (name) (cmd) (cmcl) \n::= skip \\ (crrwl) ; (cd) I (tale) := (ezp) I if (ezp) then (crnd) else (crnd) I while (ezp) do (crnd) \nend (ezp) ::= (Znt) I (de) \\ (ezp) (op) (exp) I read (W):: =+ I-IX I=12 It is a simple exercise (see \nFigures 9 and 10 in appendix) to write the corresponding definitional interpreter, to apply It to, e.g., \nthe factorial program (Figure 4), and to residualize the result (Figure 5). Essentially, type-directed \npartial evaluation of the Tiny interpreter acts as a front-end compiler that maps the ab\u00adstract syntax \nof a source program into a A-expression repres\u00adenting the dynamic semantics of this program [14]. This \nA\u00adexpression is in continuation-passing style [49], t. e., in three\u00adaddress code. We have extended the \ndefinitional J-interpreter described in this section to richer languages, including typed higher\u00adorder \nprocedures, block structure, and subt yping, d la Re yn\u00adolds [47]. Thus this technique of type-directed \ncompilation scales up in practice. In that sense, type-directed partial evaluation provides a simple \nand effective solution to (de\u00adnotational ) semantics-directed compilation in the J-calculus [32, 43]. \n2.4 Residualizing the residualizer To visualize the effect of residualization, one can residualize the \nresidualizer with respect to a type. As a first approxim\u00ad ation, given a type t, we want to evaluate \n(residualize (lambda (v) (residualize v t)) t)  To this end, we first need to define an abstracted version \nof the residualizer (with no free variables). We need to factor out all the abstract-s yntax constructors, \nthe unparser, 4 and the gensym paraphernalia, which we make continuation\u00adpassing to ensure that new symbols \nwill be generated cor\u00adrectly at run time. To be precise: (define abstract-residualize (lambda (t) (lsmbda \n(reset-gensym-c gensym-c unparse-expression make-Var make-Lsm make-App make-Pair make-Fst make -Snd) \n(lambda (v) (letrec ([reify . ..1 ;;; asin [reflect .1) ; ; ; Figure 2 (reset-gensym-c (lsmbda () (unparse-expression \n(reify (parse-type t) v))))))))) The type of abstract-residualize is a dependent type in that the value \nof t denotes a representation of the t,ype of V. Applying abstract-residualize to a represention of a \ntype, however, yields a simply typed value. We can then write a 4Flgure 2 uses quas]quote and unquote \nfor readablllty, thus avoid. Ing the need for an unpars, er > (define meaning-expr-cps-cbv (lambda (e) \n(Ietrec ( [meaning (lambda (e r) (lambda (k) (case-record e [(Var i) (k (r i))] [(LsM i e) (k (lambda \n(v) (meaning e (lambda (i v r) (lambda (j) (if equal? i j) v (r j)))))))] [(APP eO ei) ((meaning eO r) \n(lambda (vO) ((meaniruz ei r) (lambda (vi) ((vO Vi) k)))))])))]) (meaning (parse-expression e) (lembda \n(i) (error init-env undeclared identifier: -s i) )))) > (define meaning-type -cps-cbv (lambda (t) (letrec \n( [computation (lambda (t) (make -Func (make-Fun. (value t) (make-Base Ans ) ) (make-Base Ans) ) 1 [value \n(lambda (t) (case-record t [(Base -) t] [(Func tl t2) (make-Func (value ti) (computation t2) )1 ) )1 \n) (unparse-type (computation (parse-type t) ) ) ) ) ) > (residualize (mean ing-expr-cps-cbv (lambda \n(x) x)) (meaning-type-. ps-cbv (a -> a))) (lambda (xO) (xO (lambda (xl) (lambda (x2) (x2 xl))))) > N.B. \nThe interpreter is untyped and thus we can only residualize interpreted terms that are closed and simply \ntyped. Untyped or polymorphically typed terms, for example, are out of reach. Figure 7: Type-directed \npartial evaluation of a call-by-value CPS A-interpreter m-ocedure instant iate-tme that maps the representation \nof ~he input type to a repre;~ntation o{ the typ; of that simply typed value (see Figure 6). We are now \nready for self-application with respect to a type t: (residualize (abstract-residualize (instantiate-type \nt)) t)  The result is the text of a Scheme procedure. Applying this procedure to the initial environment \nof the residualizer (i. e., the abstract-syntax constructors, etc.) and then to a com\u00adpiled version of \nan expression of type t yields the text of that expression. Self-application eliminates the overhead \nof interpreting the type of a source program. For example, let us consider the S combinator of Section \n1.6. Residualizing the residualizer with respect to its type es\u00adsentially yields the eta-expanded two-level \nversion we wrote in Section 1.6 to visualize the residualization of S. For another example, we can consider \nthe Tiny inter\u00adpreter of Section 2.3. Residualizing the residualizer with respect to its type (see Figure \n9) yields the text of a Tiny compiler (whose run-time support includes the Tiny inter\u00adpreter). 2,5 The \nart of the A-interpreter We consider various A-interpreters and residualize their ap\u00adplication to a A-term. \nThe running question is as follows: which type should drive residualization? Direct style: For a direct-style \ninterpreter, the type is the same as the type of the interpreted X-term and the residual term is structurally \nequivalent to the interpreted J-term [36, Section 7.4]. Continuation-passing style: For a continuation-style \ninter\u00adpreter, the type is the CPS counterpart of the type of the interpreted J-term and the residual \nterm is the CPS counterpart of the interpreted Xterm for each possible continuation-passing style [28]. \nFigure 7 illustrates the point for left-to-right call-by-value. Other passing styles: The same technique \napplies for store\u00adpassing. etc. interpreters, be they direct or continuation\u00adpassing, and in particular \nfor interpreters that simulate lazy evaluation with thunks. Monadic style: We cannot, however, specialize \na mon\u00adadic interpreter with respect to a source program because the residual program is parameterized \nwith polymorphic functions [42, 50] and these polymorphic functions do not have a simple type. Thus monadic \ninterpreters provide an example where traditional partial evaluation wins over t,ype\u00addirected partiaJ \nevaluation.  2.6 Static corn mutational effects It is simple to construct a program that uses computational \neffects (assignments, 1/0, or call/cc) statically, and that type-directed partial evaluation specializes \nsuccessfully something that comes for free here but that (for better or for worse ) no previous partial \nevaluator does. We come back to this point in Section 4.4. 3 Disjoint Sums Let us extend the language \nof Figure 1 with disjoint sums and booleans. (Booleans are included for pedagogical value. ) Reifying \na disjoint-sum value is trivial: Reflecting upon a disjoint-sum expression is more chal\u00adlenging. By symmetry, \nwe would like to write end (where Z1 and z, are fresh) but this would yield ill-typed two-level J-terms, \nas in the non-solution of Section 1.1. Static values would occur in conditional branches and dv-. namic \nconditional expressions would occur in static contexts a clash at higher types. The symmetric definition \nrequires us to supply the con\u00adtext of reflection (which is expecting a static value) both with an appropriate \nleft value and an appropriate right value, and then to construct the corresponding residual case expression. \nUnless the source term is tail-recursive, we thus need to ab\u00adstract and to relocate this context. Context \nabstraction is achieved with a control operator. This context, however, needs to be delimited, which \nrules out call/cc [10] but invites one to use shift and reset [16, 17] (though of course any other delimited \ncontrol operator could do as well [21] ).5 The extended residualizer is displayed in Figure 8. The following \nScheme session illustrates this extension. > (residuali,ze (lambda (x) x) ((A + B) -> (A ~ B))) (lambda \n(xO) (case-record XO [(Left xl) (make-Left xi)] [(Right x2) (make-Right x2)1 ) ) > (residualize (lambda \n(x) 42) (Boo1 -> Int)) (lambda (xO) (if XO 42 42)) > (residualize (lambda (call/cc fix null? zero? * \ncar cdr) (lambda (X, ) (call/cc (lambda (k) ( (+ix (lambda (m) (lambda (XS) (if (null? XS) 1 (if (zero? \n(car XS)) (k O) (* (car x5) (m (cdr XS))) ))))) Xs) )))) 5An overview of shift and reset can be found \nm Appendix B t E Type .._., bltl+t21tl xtzltl+t2\\Bool u ~ Value .. .. C\\zl Xz:t. ulvoaull pair(vl, \nuz) I ~v I sndv I inleft (v) I inright (u) I case u ;f inleft(zl) * .1 1 inright(z~) ~ vz end e c Expr \n.. .. c121&#38;:t. eleOQel\\ pair(el, ez) ] fste I snde \\ -(e) I inright(e) I ~eof inleft(zl) * el [ \ninsight + e2 $&#38;l reify = At.Av : t..J.t u Type + Value -+ TLT u where z 1 is fresh. case v ;f inleft(vl) \n+ M(.Jt WI) Dinright (VZ) + inright (~t .2) end BOOI J w= ~ v then true else false reflect = At . At. \nAe : t.~~ e Type + Type + Expr + TLT pair(ttl Me,ti, we) shift ~:tl+tz-+t where x ~ and X2 are fresh. \nshift ~ : Bool + t fiife ~ resett (K @true) &#38; resett (~ 5false) leset and reflect are annotated with \nthe type of the valu~ :xpected by the delimited context, residualize = statically-reduce o reify : Type \n-+ Value + Expr Figure 8: Type-directed residualization ( ( ((Hum -> Hum) -> Hum) -> Num) * (((LMum \n-> Num) -> LNum -> Mum) -> LNum -> Num) * (LNum -> Bool) * (Num -> Bool) * (Mum * Num => Num) * (LNum \n-> Num) * (LMum -> LNum) => LNum -> Num) ) (lambda (xO xl x2 x3 x4 X5 x6) (lambda (x7) (xO (lambda (x8) \n((xl (lambda (x9) (lambda (xIO) (if (x2 xIO) 1 (if (x3 (x5 xIO) ) (x8 O) (x4 (x5 Xlo) (x9 (x6 Rio)))))))) \nx7))))) > In the first interaction, the identity procedure over a disjoint sum is residualized. In the \nsecond interaction, a constant, procedure is residualized. The third interaction features a s~andard \nexample in the continuations community: a pro\u00adcedure that multiplies numbers in a list, and escapes if \nit encounters zero. Residualization requires both the type of fix (to traverse the list) and of call/cc \n(to escape). The same legitimate question as in Section 1 arises: how dogs tlus reaily work? Let us residualize \nthe static application f Cl g with respect to the type Bool + Int, where f = xh.Xz.l+h Gs = ~g.~ g then \n2 else 3 9 We want to perform the addition in ~ statically. This re\u00adquires us to reduce the conditional \nexpression in g, even though g s argument is_mk~own. During residualization, the delimited context [~@ \ng @ [.]] is abstracted and relocated in both branches of a dynamic conditional expression: Bool+Int(f~g) \n= 4 ~b.a,nt (J (f @ g @ (-&#38;&#38;,, b))) = Ab.if b ~reset, , (.f~g~true) ~ resetI., (~a9afa1se) \nwhich statically reduces to Jb.if b @ 3 else 4. . 4 Limitations Our full type-directed partial evaluator \nis not formally proven. Only its restriction to the simply typed J-calculus has been proven correct, \nbecause it coincides with Berger and Schwichtenberg s algorithm [3]. (The t we-level J-calculus, though, \nprovides a more convenient format for proving, e.g., that static reduction does not go wrong and yields \na com\u00adpletely dynamic term. ) This section addresses the practical limitations of type\u00addirected partial \nevaluation. 4.1 Static errors and non-termination may occur As soon as we move beyond the simply-typed \nA-calculus, nothing a prio n guarantees that type-directed partial eval\u00aduation yields no static errors \nor even terminates. (As usual in partial evaluation, one cannot solve the halting problem. ) For example, \ngiven the looping thunk loop, the expression (residualize (lambda (dummy) ((loop) (/ 1 O))) (Dummy -> \nWhatever) )  may either diverge or yield a division by zero error, de\u00adpending on the Scheme processor \nat hand, since the order in which sub-expressions are evaluated, in an application, is undetermined [10]. \n 4.2 Residual programs must have a type We must know the type of every residual program, since it is \nthis type that directs residualization. (The static parts of a source program, however, need not be typed. \n) Residual programs can be polymorphically typed at base type (names of base types do not matter), but \nthey must, be monomorphically typed at higher types. Overcoming this limitation would require one to \npass type tags to poly\u00admorphic functions, and to enumerate possible occurrences of type tags at the application \nsite of polymorphic functions (an F2 analogue of control-flow analysis / closure analysis for higher-order \nprograms). Examples include definitional interpreters for program\u00ading languages with recursive definitions \nthat depend on the type of the source program. Unless one can enumer\u00adate all possible instances of such \nrecursive definitions (and thus abstract all the corresponding fixpoint operators in the definitional \ninterpreter), these interpreters cannot be resid\u00adualized. Inductive types are out of reach as well because \neta\u00adexpanding a term of type tthat includes an inductive type # does not terminate if t occurs in negative \nposition within t.For example, we can consider lists. case w ;f m+~ flC6iiS(ff, w)* -(J ., + ( ) w) \nshift - ~ : List(t) -+ t where x and y are fresh. Reflecting upon a list-typed ex\u00adpression diverges. \nThe problem here is closely related to coding fixed-point operators in a call-by-value language. A similar \nsolution can be devised and gives rise to a notion of lazy insertion of coercions. 4.3 Single-threading \nand computation duplication must be addressed Type-directed partial evaluation does not escape the prob\u00adlem \nof computation duplication: a program such as (lambda (f g x) ((lambda (y) (f y y)) (g x))) is residualized \nas (lambda (f g x) (f (g x) (g x))) Fortunately, the Similix solution applies: a residual let ex\u00adpression \nshould be inserted [8]. The residual term above then reads: (lambda (T g x) (let ([y (g x)]) (f y y))) \n We have implemented type-directed partial evaluation in 4.6 Type-directed partial evaluation is monovariant \nsuch a way. This makes it possible to specialize a direct\u00adstyle version of the Tiny interpreter in Section \n2.3. The corresponding residual programs (see Figure 5) are in direct style as well. Essential y the \ny use let expressions let u = $@x in e instead of CPS (f@.z@~v.e . 4.4 Side effects Unless side-effecting \nprocedures can be performed statically, they need to be factored out and, in the absence of let inser\u00adtion, \nbe made continuation-passing. At first, this can be seen as a shortcoming, until one con\u00adsiders the contemporary \ntreatment of side effects in partial evaluators. Since Similix [8], all I/O-like side effects are re\u00adsidualized, \nwhich on the one hand is safe but on the other hand prevents, e.g., the non-trivial specialization of \nan inter\u00adpreter which finds its source program in a file. Ditto for spe\u00adcializing an interpreter that \nuses I/O to issue compile-time messages they all are delayed until run time. Similar heuristics can \nbe devised for other kinds of computational effects. In summary, the treatment of side effects in partial \neval\u00aduators is not clear cut. Type-directed partial evaluation at least offers a simple testbed for experiments. \n4,5 Primitive procedures must be either static or dynamic The following problem appears as soon as we \nmove beyond the pure A-calculus. During residualization, a primitive procedure cannot be used both statically \nand dynamically. Thus for purposes of residualization, in a source expression such as ((lambda (x) (lambda \n(y) (+ (+ x 10) y))) 100) the two instances of + must be segregated. The outer occur\u00ad rence must be \ndeclared in the initial run-time environment: (lambda (add) ((lambda (x) (lambda (y) (add (+ x 10) y))) \n100)) This limitation may remind one of the need for binding-time separation in some partial evaluators \n[36, 41]. A simple solution, however, exists, that prevents segreg\u00adation. Rather than binding a factorized \nprimitive operator such as + to the offline procedure (lambda (<fresh-name>) (lambda (al a2) (,< fresh-name> \n,al ,a2)))  one could bind it to an online procedure that probes its ar\u00adguments for static-reduction \nopportunities. (lambda (<fresh-name>) (lambda (al a2 ) (if (number? al) (if (number? a2) (+ al a2) (if \n(zero? al) a2 ( ,< fresh-name> ,al ,a2) ) ) (if (and (number? a2) (zero? a2)) al (, <fresh-nanw> ,ai \n,a2)) ))) Partial evaluation derives much power from polyvanance (the generation of mutually recursive \nspecialized versions of source program points). Polyvariance makes it possible, e.g., to derive linear \nstring matchers out of a quadratic one and to compile pattern matching and regular expressions efficiently \n[15, 36]. We are currently working on making type-directed partial evaluation polyvariant. 4.7 Residual \nprograms are hard to decipher A pretty-printer proves very useful to read residual pro\u00adgrams. We are \ncurrently experimenting with the ability to attach residual-name stubs to type declarations, as in Elf. \nThis mechanism would liberate us from renaming by hand, as in the residual program of Figure 5. 5 Related \nWork 5.1 A-calculus normalization and G6delization Normalization is traditionally understood as rewriting \nut\u00adtil a normal form is reached. In that context, (one-level) type-directed eta-expansion is a necessary \nstep towards long fl~-normal forms [31]. A recent trend, embodied by par\u00adtial evaluation, amounts to \nstaging normalization in two steps: a translation into an annotated language, followed by a symbolic \nevaluation. This technique of normalization by translation appears to be spreading [39]. Follow-up work \non Berger and Schwichtenberg s algorithm includes Alten\u00adkirch, Hofmann, and Streicher s categorical reconstruction \nof this algorithm [I].6 This reconstruction formalizes the environment of fresh identifiers generated \nby the reifica\u00adtion of A-abstractions as a categorical fibration. Berger and Schwichtenberg also dedicate \na significant part of their pa\u00adper to formalizing the generation of fresh identifiers (they represent \nabstract-syntax trees as base types). In the presence of disjoint sums, the existence of a nor\u00admalization \nalgorithm in the simply typed lambda-calculus is not known. Therefore our naive extension in Section \n3 needs to be studied more closely. The call-by-value nature of our implementation, for example, makes \nit possible to distin\u00adguish terms that are undistinguishable under call-by-name. > (residualize (lambda \n(f) (lambda (x) 42)) ((a-> (b+ c)) -> a-> Int)) (lsmbda (xO) (lambda (xl) 42.)) > (residuallze (lambda \n(f) (lsmbda (x) ((lambda (y) 42) (f x)))) ((a-> (b+ c)) -> a-> Int)) (lambda (xO) (lambda (xl) (case-record \n(xO xl) [(Left x2) 42] [(Right x3) 42] ))) > In his PhD thesis [27], Goldberg investigates C%del\u00adization, \ni,e., the encoding of a value from one language into another. He identifies Berger and Schwichtenberg \ns algorithm as one instance of Godelization, and presents a Godelizer for proper combinators in the untyped \nJ-calculus, An implementation of Berger and Schwlchtenberg s al\u00adgorithm in Standard ML can be found in \nFilinski s PhD 61n that work, re]fy IS quote and reflect is unquote) thesis [23]. This implementation \nhandles most of the ex\u00adamples displayed in the present paper, in ML. It is ingeni\u00adous because as expressed \nin Figures 1 and 2, type-directed partial evaluation reauires deDendent . . Jt can be tv~es. easilv translated \ninto Haskell (excl;ding disjoint sums, of course, for lack of computational effects). 5.2 Binding-time \nanalysis All of Nielson and Nielson s binding-time analyses dynamize functions in dynamic contexts because \nof the difficulties of handling contravariance in program analysis [44]. So do all other binding-time \nanalyses [36], with the exception of Con\u00adsel s [12] and Heintze s [40]. These analyses are polyvariant \nand thus they spawn another variant instead of dynamiting, In practice, type-directed partial evaluation \nneeds a simple form of binding-time analysis: a type inference where all base types are duplicated into \na static version and a dy\u00adnamic version. Whenever a primitive operation is applied to an expression which \nis not entirely static, it is factored out into the initial run-time environment. The other occurrences \nof this primitive operation do not need to be factored out, however (thus enabling a small form of polyvariance). \nTo use this binding-time analysis and more generally type-directed partial evaluation, the simplest is \nto define source programs as closed A-terms, by abstracting all free occurrences of variables. (To enable \nthe small form of poly\u00advariance mentioned in the last paragraph, each occurrence of primitive operator \ncan be abstracted separately. ) One can then curry this program with the static variables first, and \nthen residualize the application of this curried program to the static values, with respect to the type \nof the result. This simple use mat ches the statement of Kleene s S: -theorem. 5.3 Partial evaluation \nType-directed partial evaluation radically departs from all other partial evaluators (and optimizing \ncompilers) because it has no interpretive dimension whatsoever: its source pro\u00adgrams are compiled. If \nanything, it is closest to run-time code generation (the output syntax need not be Scheme s). The last \nten years have seen two flavors of partial evalu\u00adation emerge: online and offline. Offline partial evaluation \nis staged into two components: program analysis and program specialization. Online partial evaluation \nis more monolithic. Extensive work on both sides [6, 11, 15, 34, 36, 41, 48, 51] has led to the conclusions \nof both the usefulness of mom-am. analysis and the need for online partial evaluation in a pro\u00adgram specialize \n(as illustrated in Section 4.5). Because it relies on one piece of static information the type of the \nresidual program type-directed partial evaluation appears as an extreme form of offline partial evaluation. \nIn the spring of 1989, higher-order partial evaluation was blooming at DIKU [34]. In parallel with Bondorf \n(then visit\u00ading Dortmund), the author developed a version of Similix [8] that did not dynamize higher-order \nvalues in dynamic con\u00adtexts. In this unreleased version, instead, the specialize kept track of the arity \nof static closures and eta-expanded them when they occurred in dynamic contexts. Type-directed par\u00adtial \nevaluation stems from this unpublished work. The idea, however, did not Lake. Despite the analogy with \ncall unfold\u00ading, which is central to partial evaluation but unsafe without let insertion (under call \nby value), (Similix [...] refuses to lift higher-order values into residual expressions: lifting higher-order \nvalues and data structures is in general unsafe since it may lead to residual code that exponentially \nduplic\u00adates data structure and closure allocations [9, page 327]. All the later higher-order partial \nevaluators developed at DIKU have adopted the same conservative strategy a choice that Henglein questions \nfrom a type-theoretical stand\u00adpoint [29]. In practice, thk decision created the need for source binding-time \nimprovements in offline partial evalu\u00adation [36, Chapter 12]. In contrast, binding-time coercions improve \nbinding times without explicit eta-conversion , to paraphrase the title of Bondorf s LFP 92 paper [7] \n a prop\u00adert y which should prove crucial for multi-level binding-time analyses since it eliminates the \nneed for (unfathomed) multi\u00adlevel binding-time improvements [26]. Thus Mix-like partial evaluation [36] \nand type-directed partial evaluation fundamentally contrast when it comes to dynamic computations: Mix-like \npartial evaluators do not, associate any structure to the binding time dynamic , whereas we rely on the \ntype structure of dynamic computa\u00adtions in an essential way. As a consequence, and modulo the abstraction \nof the initial run-time environment (which must be defined in a partial evaluator anyway), type-directed \npar\u00adtial evaluation needs a restricted form of binding-time ana\u00adlysis (see Section 5.2) but it needs \nno specialization by sym\u00adbolic interpretation. It is, however, monovariant. We are currently integrating \nthe residualization algorithm in Pell-Mell, our partial evaluator for ML [40]. This al\u00adgorithm fulfills \nour need for binding-time coercions at higher types. Type-directed partial evaluation also formalizes \nand clarifies a number of earlier pragmatic decisions in the sys\u00adtem. For example, our treatment of inductive \ndata structures can be seen as lazy insertion of coercions (Section 4.2). 5.4 Self-application Self-application \nis the best-known way to optimize partial evaluation [6, 11, 15, 25, 36, 37, 41, 48]: rather than run\u00adning \nthe partial evaluator on a source program, with all the symbolic interpretive overhead this entails, \none could instead 1. generate aspecializer dedicated to this program (a.k.a. a generating extension ), \nand 2. run this dedicated specialize on the available input.  To be efficient, self-application requires \nagood binding-time analysis and a good binding-time division in the partial eval\u00aduator [36, Section 7.3]. \nType-directed partial evaluation is based on type infer\u00adence, needs no particular binding-time division, \nand as illus\u00adtratedin Section 2.4, is self-applicable as well. 5.5 Partial evaluation of definitional \ninterpreters In the particular cases where the source program p is a defin\u00aditional interpreter, or where \nthe source program is the partial evaluator PE itself and the static input is a definitional in\u00adterpreter \nor PE itself, the partial-evaluation equations run PE (p, (s, -)) = P(s)-) run p(s, d) = r~ p(s,_) (-1 \n+ { are known as the [Futamura Projections [15, 25, 36]. As il\u00adlustrated in Section 2.3 and (to a lesser \nextent) in Section 2.4, type-directed partial evaluation enables their implementa\u00adtion in a strikingly \nsimple way. (The third Futamura pl-o\u00adjection, however, is out of reach because of the polymorphic tYPe \nof PE(PE, _}.) Conclusion free reduction moof. In Peter Dvbier and Randy Pol- To produce a residual program, \na partial evaluator needs to residualize static values in dynamic contexts. Consider\u00ading higher-order \nvalues introduces a new challenge for resid\u00adualization. Most partial evaluators dodge this challenge \nby disallowing static higher-order values to occur in dynamic contexts i.e., in practice, by dynamiting \nboth, and more generally by restricting residualized values to be of base type [4, 6, 9, 36, 37]. Only \nrecently, some light has been shed on the residualization of values at higher types, given informa\u00adtion \nabout these types [18, 19]. We have presented an algorithm that residualizes a closed typed static value \nin a dynamic context, by eta-expandkig the value with two-level eta-redexes and then reducing all static \nredexes. For the simply typed A-calculus, the al\u00adgorithm coincides with Berger and Schwichtenberg s {inverse \nof the evaluation functional [3]. It is also interesting in its own right in that it can be used as a \npartial evaluator for closed compiled programs, given their type. This par\u00adtial evaluator, in several \nrespects, outperforms all previous partial evahators, e.g., in simplicity and in efficiency. It also \nprovides a simple and effective solution to (denotational) semantics-directed compilation in the A-calculus. \nFuture ~work includes formalizing type-directed partial evaluation, extending it to a richer type language \n(e. g., polymorphic or linear), making it more user-friendly, pro\u00adgramming more substantial examples, \nand obtaining poly \u00advariance. An implementation is available on the author s home page. Acknowledgements \nVarious parts of this work have benefited from the kind at\u00adtention and encouragement of Thorsten Altenkirch, \nDavid Basin, Luca Cardelli, Craig Chambers, Charles Consel, Jim des Rivi&#38;es, Dirk Dussart, John Hatcliff, \nMartin Hofmann, Thomas Jensen, Peter Mosses, Flemming Nielson, Dino Oliva, Bob Paige, Frank Pfenning, \nJon Riecke, Amr Sabry, Michael Schwartzbach, Helmut Schwichtenberg, Thomas Streicher, Tommy Thorn, and \nG] ynn Winskel. Thanks are due to the referees for insightful comments, and to Guy L. Steele Jr., for \nshepherding this paper. Particular thanks to Rowan Davies, Mayer Goldberg, Julia Lawall, Nevin Heintze, \nKaroline Malmkj cer, Jens Palsberg, and Ren4 Ves\u00adtergaard for discussions and comments. Hearty thanks \nto Andrzej Filinski for substantial e-mail interaction and com\u00adments. All of the examples were run with \nR. Kent Dybvig s Chez Scheme system and with Aubrey Jaffer s SCM system. Most were type-checked using \nStandard ML of New Jersey. The author gratefully acknowledges the support of the DART project (Design, \nAnalysis and Reasoning about Tools) of the Danish Research Councils during 1995. References [I] Thorsten \nAltenkirch, Streicher. Categorical Dawes and Pfennlng s loglcal lysis and offline partial evaluation \nMartin Hofmann, and Thomas reconstruction of a reduchon formalization of binding-time aria. opens a prom]sing \navenue [20] lack, editors, Informal Proceedings of the Joznt CLICS-TYPES Workshop on Categories and Type \nTheory, Goteborg, Sweden, May 1995. Report 85, Programming Methodology Group, Chalmers University and \nthe Uni\u00ad versity of Goteborg. [2] Henk Barendregt. The Lambda Calculus Its Syntax and Semanttcs. North-Holland, \n1984. [3] Ulrich Berger and Helmut Schwichtenberg. An inverse of the evaluation functional for typed \nA-calculus. In Pro\u00adceedings of the Szxth Annual IEEE Symposium on Logzc in Computer 5 czence, pages 203 \n211, Amsterdam, The Netherlands, July 1991. IEEE Computer Society Press. [4] Lars Birkedal and Morten \nWelinder. Partial evaluation of Standard ML. Master s thesis, DIKU, Computer Sci\u00adence Department, University \nof Copenhagen, August 1993. DIPU report 93/22. [5] Hans-J. Boehm, editor. Proceedings of the Tzuenty \n-Fzrst Annual ACM Symposzum on Principles of Pro\u00adgrammmg Languages, Portland, Oregon, January 1.9!74. \nACM Press. [6] Anders Bondorf. Self-Applicable Partial Eualuatzon. PhD thesis, DIKU, Computer Science \nDepartment, University of Copenhagen, Copenhagen, Denmark, 1990. DIKU Report 90-17. [7] Anders Bondorf. \nImproving binding times without ex\u00adplicit cps-conversion. In William Clinger, editor, Pro\u00adceedings of \nthe 1992 ACM Conference on Lisp and Functional Programming, LISP Pointers, Vol. V, No. 1, pages 1-10, \nSan Francisco, California, June 1992. ACM Press. [8] Anders Bondorf and Olivier Danvy. Automatic auto\u00ad \nprojection of recursive equations with global variables and abstract data types. Sczence of Computer \nProgram\u00adming, 16:151 195, 1991. [9] Anders Bondorf and Jesper J~rgensen. Efficient ana\u00adlyses for realistic \noff-line partial evaluation. Journal of Functional Programming, 3(3):315-346, 1993. [10]William Clinger \nand Jonathan Rees (editors). Revised4 report on the algorithmic language Scheme. LISP Poznters, 1V(3):1-55, \nJuly-September 1991. [11]Charles Consel. ,4nalyse de Programmed, Evaluation Partielle et G6n4rataon de \nCompdateurs. PhD thesis, LTniversit6 Pierre et Marie Curie (Paris VI), Paris, France, June 1989. [12] \nCharles Consel. Polyvariant binding-time analysis for applicative languages. In David A. Schmidt, editor, \nProceedings of the Second ACM SIGPL.4N Symposzum on Partial Evaluation and Semantics-Based Program Manipzdatzon, \npages 66 77, Copenhagen, Denmark, June 1993. ACM Press. [13] Charles Consel and Olivier Danvy. From interpreting \nto compiling binding times. In Neil D, Jones, editor, Pro\u00adceedings of the Third European Sympostum on \nProgram\u00adming, number 432 in Lecture Notes in Computer Sci\u00adence, pages 88 105, Copenhagen, Denmark, May \n1990. [14] Charles Consel and Olivier Danvy. Static and dynamic semantics processing. In Robert (Corky \n) Cartwright, editor, Proceedings of the Eighteenth Annual ACM Symposium on Principles of Programming \nLanguages, pages 14-24, Orlando, Florida, January 1991. ACM Press. [15] Charles Consel and Olivier Danvy. \nTutorial notes on partial evaluation. In Susan L. Graham, editor, Pro\u00adceeding~ of the Twentieth Annual \nACM Symposium on Principles of Programming Languages, pages 493-5o1, Charleston, South Carolina, January \n1993. ACM Press. [16] Olivier Danvy and Andrzej Filinski. Abstracting con\u00adtrol. In Mitchell Wand, editor, \nProceedings of the 1990 ACM Conference on hasp and lknctional Programmwzg, pages 151 160, Nice, France, \nJune 1990. ACM Press. [17] Olivier Danvy and Andrzej Filinski. Representing con\u00adtrol, a study of the \nCPS transformation. Mathematical Structures in Computer Science, 2(4):361-391, Decem\u00adber 1992. [18] Olivier \nDanvy, Karoline Malmkj~r, and Jens Palsberg. The essence of eta-expansion in partial evaluation. LISP \nand Symbolzc Computation, 8(3):209 227, 1995. An earlier version appeared in the proceedings of the 1994 \nACM SIGPLAN Workshop on Partial Evaluation and Semantics-Based Program Manipulation. [19] Olivier Danvy, \nKaroline Malmkja-, and Jens Palsberg. Eta-expansion does The Trick. Technical report BRICS RS-95-41, \nDAIMI, Computer Science Department, Aar\u00adhus University, Aarhus, Denmark, August 1995. [20] Rowan Davies \nand Frank Pfenning. A modal analysis of staged computation. In Guy L. Steele Jr., editor, Pro\u00adceedings \nof the Twenty-Third Annual ACM Sympostum on Principles of Programming Languages, St. Peters\u00ad burg Beach, \nFlorida, January 1996. ACM Press. [21] Matthias Felleisen. The theory and practice of first-class prompts. \nIn Jeanne Ferrante and Peter Mager, editors, Proceedings of the Fifteenth Annual ACM Symposium on Principles \nof Programming Languages, pages 180 190, San Diego, California, January 1988. [22] Andrzej Filinski. \nRepresenting monads. In Boehm [5], pages 446 457. [23] Andrzej Filinski. Controlling Effects. PhD thesis, \nSchool of Computer Science, Carnegie Mellon Uni\u00adversit y, Pitt sburgh, Pennsylvania, 1995. [24] Daniel \nP. Friedman, Mitchell Wand, and Christopher T. Haynes. Essentials of Programming Languages. The MIT Press \nand McGraw-Hill, 1991. [25] Yoshihito Futamura. Partial evaluation of computation process an approach \nto a compiler-compiler. Systems, Computers, Controls ,?, 5, pages 45-50, 1971. [26] Robert Gluck and \nJesper J@rgensen. Efficient multi\u00adlevel generating extensions for program specialization. In S. D. Swierstra \nand M. Hermenegildo, editors, Sew enth lnternattonal Symposzum on Programming Lan\u00adguage Implementation \nand Logic Programming, number 982 in Lecture Notes in Computer Science, pages 259 278, Utrecht, The Netherlands, \nSeptember 1995. [27] Mayer Goldberg. Recursiue Application Survival an the }-Calculus. PhD thesis, Computer \nScience Department, Indiana IJniversity, Bloomington, Indiana, 1996. Forth\u00adcoming. [28] John Hatcliff \nand Olivier Danvy. A generic account of continuation-passing styles. In Boehm [5], pages 458 471. [29] \nFritz Henglein. Dynamic typing: Syntax and proof the\u00adory. Sc$ence of Computer Programming, 22(3):197 \n230, 1993. Special Issue on ESOP 92, the Fourth European Symposium on programming, Rennes, February 1992. \n [30] G&#38;srd Huet. R&#38;olution d 6quations clans les langages d ordre 1, 2, .... u. Th&#38;e d Etat, \nUniversity de Paris VII, Paris, France, 1976. [31] C. Barry Jay and Neil Ghani. The virtues of eta-ex\u00adpansion. \nJournal of Functional Programming, 5(3):135\u00ad154, 1995. [32] Neil D. Jones, editor. Semantzcs-Dwected \nCompder Generation, number 94 in Lecture Notes in Computer Science, Aarhus, Denmark, 1980. [33] Neil \nD. Jones. Challenging problems in partial evalu\u00adation and mixed computation. In Partial Evaluation and \nMzxed Computation, pages 1 14. North-Holland, 1988. [34] Neil D. Jones. Mix ten years after. In William \nL. Scherlis, editor, Proceedings of the ACM SIGPLA N Symposium on Partial Evaluation and Semantics-Based \nProgram Manipulation, pages 24-38, La Jolla, Califor\u00adnia, June 1995. [35] Neil D. Jones, Carsten K. Gomard, \nAnders Bondorf, Olivier Danvy, and Torben A3. Mogensen. A self\u00adapplicable partial evaluator for the lambda \ncalculus. In K. C. Tai and Alexander L. Wolf, editors, Proceedings of the 1990 IEEE International Conference \non Com\u00adputer Languages, pages 49 58, New Orleans, Louisiana, March 1990. [36] Neil D. Jones, Carsten \nK. Gomard, and Peter Sestoft. Partial Evaluation and Automatic Program Generation. Prentice Hall International \nSeries in Computer Science. Prentice-Hall, 1993. [37] John Launchbury. Pro~ectton Factorisations zn Part~al \nEvaluation. Distinguished Dissertations in Computer Science. Cambridge University Press, 1991. [38] Julia \nL. Lawall and Olivier Danvy. Continuation\u00adbased partial evaluation. In Carolyn L. Talcott, ed\u00aditor, Proceedings \nof the 1994 ACM Conference on Lzsp and Functional Programming, LISP Pointers, Vol. VII, No. 3, Orlando, \nFlorida, June 1994. ACM Press. [39] Ralph Loader. Normalisation by translation. Tech\u00adnical report, Computing \nLaboratory, Oxford University, April 1995. [40] Karoline Malmkjam, Nevin Heintze, and Olivier Danvy. \nML partial evaluation using set-based analYsis. In John Reppy, editor, Record of the 1991 ACM SIG-PLAN \nWorkshop on hTL and its Applzcatzons, Rapport derecherche N02265, lNRIA, pages 112-119, Orlando, Florida, \nJune 1994. Also appears as Technical report CMU-CS-94-129. [41] Torben A3. Mogensen. Bznding Time Aspects \nof Par\u00adtzal Euaiuat!on. PhD thesis, DIKU, Computer Science Department, University of Copenhagen, Copenhagen, \nDenmark, March 1989. [42] Eugenio Moggi. Notions of computation and monads. Information and Computation, \n93:55-92, 1991. [43] Peter D. Mosses. S1S semantics implementation sys\u00adtem, reference manual and user \nguide. Technical Re\u00adport MD-30, DAIMI, Computer Science Department, Aarhus University, Aarhus, Denmark, \n1979. [44] Flemming Nielson and Hanne Ftiis Nielson. Two-Level Functional Languages, volume 34 of Cambridge \nTracts in Theoretical Computer Sczence. Cambridge Uni\u00adversity Press, 1992. [45] Larry Paulson. Compiler \ngeneration from denotational semantics. In Bernard Lorho, editor, Methods and Took for Cornpder Construction, \npages 219-250. Cambridge University press, 1984. [46] Frank Pfenning. Logic framework. In G&#38;-ard \nitors, Logical Frameworks, University Press, 1991. programming in the LF logical Huet [47] John C. Reynolds. \nThe Vliet, editor, International Languages, pages 345-372, Holland. [48] Erik Ruf. Topzcs in Online thesis, \nStanford University, ruary 1993. Technical report [49] Guy L. Steele Jr. Rabbit: A and Gordon Plotkin, \ned\u00adpages 149 181. Cambridge essence of Algol. In van Symposium on Algortthm~c Amsterdam, 1982. North- \nPartzal Evaluation. PhD Stanford, California, Feb\u00adCSL-TR-93-563. compiler for Scheme. Tech\u00ad nical Report \nAI-TR-474, Artificial Intelligence Laborat \u00adory, Massachusetts Institute of Technology, Cambridge, Massachusetts, \nMay 1978. [50] Philip Wadler. The essence of functional programming (tutorial). In Andrew W. Appel, editor, \nProceedings of the Nineteenth Annual ACM Syrnposzum on Principles of Programm~ng Languages, pages 1 14, \nAlbuquerque, New Mexico, January 1992. ACM Press. [51] Daniel Weise, Roland Conybeare, Erik Ruf, and \nScott Seligman. Automatic online partial evaluation. In John Hughes, editor, Proceedings of the Fzfth \nACM Confer\u00ad ence on Functional Programming and Computer Archi\u00ad tecture, number 523 in Lecture Notes in \nComputer Sci\u00adence, pages 165 191, Cambridge, Massachusetts, Au\u00adgust 1991. A Nielson and Nielson s Two-Level \nA-Calculus In its most concise form, the two-level simply typed A\u00adcalculus [44] duplicates all the constructs \nof the simply typed A-calculus (A, application (hereby noted C!), pair, fst, snd) into static constructs \n(A, ~, pair, fst, snd) and dynamic con\u00adstructs (~, Q pair, N, S@). define-record ~define-record ~define-record \n[define-record ~define-record [clef ine-record [define-record [clef ine-record [define-record [define-record \n[define-record (Program names command) ) (Skip)) (Sequence command command)) (Assign identifier expression) \n) (Conditional expression command command) ) (While expression command)) (Literal constant) ) (Boolean \nconstant)) (Identifier name) ) (Primop op expression expression)) (Read) ) (define meaning-type ((Int \n(Int (Int (Int (Int ((Int (((Sto (Int (Nat (Nat (Sto * Int * (Int -> Ans) => Ans) * ;;; add * Int * \n(Int -> Ans) => Ans) * ;;; sub * Int * (Int -> Ans) => Ans) * ;;; mul * Int * (Int -> Ans) => Ans) \n* ;;; eq * Int * (Int -> Ans) => Ans) * ;;; gt -> Ans) -> Ans) * ;;; read  -> Ans) -> Sto -> Ans) -> \nSto -> Ans) * * (Sto -> Ans) * (Sto -> Ans) * Sto => An. ) * * Sto * (Int -> Ans) => Ans) * ; ; ; lookup \n * Int * St. * (Sto -> Ans) => Ans,) => -> Ans) * ; ; ; continuation  Sto => ; ; ; store Ans) ) Figure \n9: Scheme interpreter for Tiny (abstract syntax and semantic algebras) A simply typed A-term is mapped \ninto a two-level J-term by a binding-time analysis. The intention is to formalize the following idea: \nStatically reducing a t we-level A-term, erasing the annotations of the residual term, and reducing this \nunannotated term should yield the same res\u00ad ult (normal form ) as reducing the original term. The two-level \nA-calculus thus provides an ideal medium for staged evaluation with more than one binding time. To this \nend, it makes use of three properties that are captured in its typing discipline: e static reduction \npreserves well-typing; e static reduction strongly normalizes; s static reduction yields normal forms \nthat are com\u00adpletely dynamic. Static reduction can be implemented directly in a func\u00adtional language: \noverlined constructs are treated as syntax constructs and dynamic constructs as syntax-building func\u00adtions. \nIt can also be implemented with quasiquote and ur\u00adquote in Scheme (as done in Section 1) and thus can \nbe seen as macro-expansion in a simply typed setting. B Abstracting Control with Shift and Reset This \nsection provides some intuition about the effect of shift, and reset. Shift and reset were introduced \nto capture composition and identity over continuations [16, 17]. Reset dehmits a context, and is identical \nto Felleisen s prompt; shift abstracts a delimited context, and is similar (though not in general (define \nmeaning (lambda (p) (lambda (add sub mul eq gt read fix true? lookup update) (lambda (k s) (letrec ( \n[meaning-program (lambda (p k s) (case-record p [(Program vs c) (meaning-declaration vs O (lambda (r) \n(meaning-command c r k s)) )1 ) )1 [meaning-declaration (lambda (d offset k) (if (null? d) (k (lambda \n(i) (error lookup undeclared identifier s i))) (mesning-declaration (cdr d) (addl offset) (lambda (r) \n(k (lambda (i) (if (eq? (car d) i) offset (r i))))))))] [meaning-command (lambda (c r k s) (case-record \nc [(Skip) (k s)1 [(sequence CI c2) (meaning-command c1 r (lambda (s) (meaning-command C2 r k S)) s)1 \n[(Assign i e) (Meaning-expression e r (lambda (v) (update (r i) v s k)) s)1 [(conditional e c-then c-else) \n(meaning-expression e r (lsmbda (v) (true? v (lambda (s) (meaning-command c-then r k s)) (lambda (s) \n(meaning-command c-else r k S)) s)) s)] [( Hhile e c) ((fix (lambda (while) (lambda (s) (meaning-expression \ne r (lsmbda (v) (true? v (lambda (s) (meaning-command c r while s)) k s)) s)))) s)]))] [meaning-expression \n(lambda (e r k s) (case-record e [(Literal 1) (k 1)1 [(Identifier i) (lookup (r i) s k)] [( Primop 0p \nei e2)  (meaning-expression el r (lambda (vI) (meaning-expression e2 r (lsmbda (v2) ((meaning-primop \nop) VI V2 k)) s)) s)1 [(Read) (read k)]))] [meaning-primop (lembda (op) (case op [(+) add] [(-) sub] \n[(*) mull [(=) eq] [(>) gt]))]) (meaning-program p k s)))))) Figure 10: Scheme interpreter for Tiny (valuation \nfunctions) equivalent ) to Fellei sen s control [2 I]. Since contexts are In the three terms, k is bound \nto an abstraction of the cle\u00ad delimited, their abstraction can be composed. limited context [2 x [ ]]. \nThis abstraction reads AzI.2 x v. In Let us consider some examdes. the first term, it is used twice: \nin the second and in the third, once; and in the last, it is not used. I +reset (2 x shift kin3 x ((,k4)+ \n(k5))) Shift and reset can be eliminated by CPS transformation = l+letk= Au.2xff in3X((k4)+(k 5)) = 55 \n[49]. A shift expression is CPS-transformed by abstract\u00ad ing the current (delimited ) continuation into \na procedure. 1 + reset (2 x shift k in 3 x (k4)) When this procedure is applied, the abstracted continuation \n= l+letk= ~u.2xuin3x(k4) = 25 is composed with the new current continuation. Finally, a re\u00ad set expression \nis CPS\u00ad transformed by supplying the identity 1 + reset (2 x shift k in k 10) procedure as a continuation \n[16, 17]. = l+letk= At,.2xvink10 = 21 Filinski s direct implementation of shift and reset can be found \nin the literature, both in Standard ML of New Jersey 1 + reset (2 x shift k in 10) [22] and in Scheme \n[38]. l+letk=Jzf.2xvin10 = 11   \n\t\t\t", "proc_id": "237721", "abstract": "", "authors": [{"name": "Olivier Danvy", "author_profile_id": "81100394275", "affiliation": "Computer Science Department, Aarhus University, Ny Munkegade, Building 540, DK-8000 Aarhus C, Denmark", "person_id": "PP15031217", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/237721.237784", "year": "1996", "article_id": "237784", "conference": "POPL", "title": "Type-directed partial evaluation", "url": "http://dl.acm.org/citation.cfm?id=237784"}