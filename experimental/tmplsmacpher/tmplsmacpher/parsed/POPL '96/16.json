{"article_publication_date": "01-01-1996", "fulltext": "\n Trace-Based Pr~gram Analysis* Christopher Colbyt Peter Lee School of Computer Science Carnegie Mellon \nUniversity Pittsburgh, Pennsylvania 15213-3891 colby@cs. emu. edu petel@cs.cmu.edu Abstract We present \ntrace-based program analysts, a semantics-based framework for statically analyzing and transforming pro\u00ad \ngrams with loops, assignments, and nested record structures. Trace-based analyses are based on tr-ans~er \ntransition ,sys\u00adtems, which define the small-step operational semantics of programming languages. Intuitively, \ntransfer transition sys\u00ad terns provide direct support for reasoning about the possible execution traces \nof a program, instead of just individual pro\u00ad gram states. Thetraces inatransfer transition system have \nmany uses, including the finite representation of all possi\u00ad ble terminating executions of a loop. Also, \ntraces may be systematically pieced together, thus allowing the compo\u00ad sitionof separately analyzed program \nfragments. The utility of the approach is demonstrated by showing three applica\u00ad tions: software plpelining, \nloop-invariant removal, and data alias detection. Introduction In this paper, we consider semantics-based \nprogram analy\u00adsis for the purposes of static optimization, transformation, program verification, and \ndebugging. We have two goals. First, to capture precise information about the run-time be\u00adhavior of programs \nthat contain loops, either from the use of looping constructs such as goto or from recursive functions. \nAnd second, to compose the analyses of separate code frag\u00adments to achieve the analysis of a larger piece \nof code. We propose a general framework of trace-based program analy\u00adsis to address these problems, and \nwe present a worked case *This research was sponsored in part by the Advanced Research Projects Agency \nCSTO under the title <The Fox Project. Advanced Languages for Systems Software , ARPA Order No. C533, \nissued by ESC!/ENS under Contract No. F19628-95-C-0050, and in part by the National Science Foundation \nunder PYI grant #CCR-9057567 The views and conclusions contained in this document are those of the authors \nand shoulcl not be interpreted as representing official policies, either expressed or Implied, of ARPA \nor the U S. Government Any opinions, findings, and conclusions or recommendations expressed in this material \nare those of the authors and do not necessarily reflect the views of the National Science Foundation. \ntThis research was performed while the first author was visiting LIX, Ecole Polytechrrique, 91128 Palaiseau \ncedex, France. Permission to make digital/hard copies of aII or part of this material for personal orciassroom \nuseisgrantedwi~out feeprovided tiatrhe copies arenotmade or distributed forprofit orcommercial advan~ge, \nthe copy\u00ad nght notice, Ute title of the publication and its date appear, and notice is given rhat copyright \nis bypermission oftie ACM, Inc. Tocopyothemise, torepublish, to post on servers orto~distribute to Iists, \nrequires specific permission and/or fee. POPL 96, St. Petersburg FLA USA @1996 ACM 0-89791-769-3/95/01. \n.$3.50 study along with some sample applications that illustrate the solution of our goals. Loops have \nlong been the bane of semantics-based pro\u00adgram analysis. In a programming language with a looping construct, \nthe semantic meaning is usually modeled by in\u00adfinite structures. For example, denotational semantics \n[28] uses limits of infinite chains to model the meaning of recur\u00adsive functions. In standard formulations \nof structural op\u00aderational semantics [26] and small-step operational seman\u00adtics [15], the behavior of \nloops is modeled by unbounded sequences of transitions. An analysis based on any of these semantic models \nis thus forced to develop some method for reasoning finitely about these (potentially) infinite objects. \nUsually, some form of approximation or abstraction is used and, as we shall argue in the next section, \nthe typical meth\u00adods often incur a substantial loss of precision. Determining precise information about \nloops, however, is often of critical importance for static optimization of code. One of the most striking \nexamples is software pipelining [2, 22], a strategy for statically transforming the structure of a loop \nin order to take advantage of the potential instruction-level paral\u00adlelism both within a single iteration \nand between adjacent iterations. Software pipelining algorithms are complex and specialized, but we demonstrate \nwith a worked-through ex\u00adample that trace-based program analysis provides a general semantics-based formulation \nfor a form of software pipelin\u00ad ing. In fact, with nothing more than the alteration of a three very simple \nequations, the same technique becomes a strategy for factoring out loop mvariants. Another common difficulty \nfor semantics-based program analyses is that they are rarely compositional. A well-known tradeoff between \ndenotational semantics and small-step op\u00aderational semantics is that denotational models are composi\u00adtional, \nbut usually abstract away from how the result of the program was computed, while small-step operational \nmodels expose useful details of the computation process itself, but usually lack the compositionality \nproperties provided by de\u00adnotational models. Some special cases of program analyses, such as strictness \nanalysis [24, 29], can be formalized quite elegantly and compositionally using a denotational model, \nand a general framework for denotational-semantics-based analysis has been developed in [25]. However, \nmany pro\u00adgram analyses are concerned with details of the actual com\u00adputation process itself and thus \nmust be based on an op\u00aderational model, making it difficult to analyze a program compositionally. Again, \nthis is a serious problem in prac\u00adtice. Consider for example the analysis of data aliases in a language \nwith assignment. A fragment of code (basic block, procedure, etc. ) that performs data structure mampulation \nand assignment may have several dlst inctly different b ehav\u00adiors depending on the data abases occurring \nat the beginning of its execution. Furthermore, each of these behaviors may produce different aliases \nwhen execution leaves that frag\u00adment and enters another. For this purpose, it is Important to have a \ncompositional analysls We give an example of how the same trace-based analysls that IS used for software \npipelining and loop optlmlzations also provides a composi\u00adtional alias analysis In fact, the output of \nthe analysis can uncover surprising posslbditles for the behavior of even sim\u00adple imperative code and \nhas clear applications for debugging purposes and program verification as well as the traditional uses \nof alias information m optimizmg compilers Trace-based program analysis is based on a framework for descrlbmg \nthe operational semantics of programs that we call transfer transttzon systems This approach to describ\u00ading \nthe semantics of programming languages can be derived from, and maintains all the computational detail \nof, the orig\u00adinal small-step semantics of the language, and yet provides some of the compositionality \nproperties enjoyed by deno\u00adtational semantics. The underlying motivation for transfer transition systems \nis to aid reasoning about the possible ex\u00adecution traces of the program instead of simply the possible \nstates at each program point. Intuitively, a trace IS a se\u00adquence of steps in the small-step operational \nsemantics of the program. But a single trace m the transfer transition system can represent precisely \na large or even infinite set of traces in the small-step operational semantics, thereby mak\u00ading it easier \nto reason about traces m the transfer transition system. In fact, a transfer transition trace of a loop \nbody can represent all possible terminating executions of the loop, which is ideal mformatlon for reasoning \nabout its behav\u00adior. Also, transfer transition traces may be systematically pieced together, thus forming \nthe basis for compositional analyses. We begin by discussing our original motivations and ex\u00adplain why \nprevious Ideas such as polyvariant analyses fall short of solving the problems Then, we present our trace\u00adbased \nprogram analysis, starting with an informal explana\u00adtion designed to provide the right intuitions, followed \nby a formal presentation of the analyem framework. As a worked case study, we take a small language with \n(parallel) assign\u00adment, loops (via got o), and data structures (nested records). Though simple, the language \nis expressive enough to serve as a core language for many reahstic programmmg languages. Finally, we \nshow apphcations of the trace-based analysis for this language, including a software pipelining transforma\u00adtion \nas well as analyses for removal of loop invariants and detection of aliases. 2 Limitations of State-13ased \nAnalyses It is perhaps surprising that loops are not handled entirely satisfactorily by current semantics-based \nmethods for ana\u00ad lyzing programs, The basic problem can be Illustrated info\u00ad rmally by some simple examples. \nConsider first the following program fragment, written m a kind of structured assembly lang uage: b + \nTRUE LX: gOtO LC (1) b + FALSE LY gOtO LC LC : if b then endif Suppose we would like to predict the set \nof possible val\u00adues that the variable b can have at each point in the pro\u00adgram. In the customary formulation, \na program analyzer would produce a function S that maps program points to the properties of interestl: \nS E Label + Property Here, Label is the set of program labels and Property is the set of properties. \nOften, a considerable amount of effort goes into the design of the property set, with the requmement \nthat its elements have finite descriptions so that the analyzer can terminate Examples of the kinds of \nproperties that are useful in practice include: . The pairs of data objects that may or must be aliases \nat a given label [3, 14] e Grammars giving all of the possible shapes of struc\u00adtured data at a given \nlabel [12, 20]. e Linear relationships amongst integer-valued variables (e g., x = y -z) at a given label \n[16, 21]. Typically, the construction of an analyzer begins with a so-called concrete or collecting semantics \n[11, 18] in which programs are assigned meamngs of the form C E Label + @(State) where @(State) is the \npowerset of program states Then, from the collecting semantics, a program analyzer (that is, an algorithm \nfor computing S from the program text) 1s derived in such a way that each element of Property cor\u00adresponds \nto a set of states, in particular, the set of states for which the property is true. Because of this \nrelationship between properties and sets of states, we refer to program analyzers like S as state-based \nanalyzers and their properties as state properties. Notationally, we will usually refer to the set of \nstate properties as Property [State] and thus write the functionality of S as S ~ Label + Property [State] \nIn order to prove that an analysis is sound, it must be shown that, for any program, C and S WI1l obey \na certain relationship Also, the analysis (i e., the computation of S) and the S function itself must \nbe computable and, for practical reasons, efficient. Abstract znterpretatzon [8] 1s a 1Although this \nis a rather naive formulation, an examination of the current literature on semantics-basecl program analysm \nreveals that a large number (perhaps the majority) of existing analyses tahe essentially this form comprehensive \nmathematical theory for devising and rea\u00adsoning about such relationships and decidability problems, and \nin addition it provides a semantics-based framework for devising the analysls algorithms. Returning now \nto Program 1, it is well known that with this simple formulation the two possible values of b at label \nLC are unavoidably (folded together, hence resulting m a loss of information, In particular, we lose \nthe fact that the value of b at label LC k TRUE if the flow of control entered LC from LX, and FALSE \nif from LY. In programs with functions, a similar loss of reformation can occur when a function is called \nfrom several ddferent points in the program. This is particularly problematic for recursive functions. \nA simdar situation arises with loops. For example, con\u00adsider the following (rather contrived) program: \nSemantically, we can view the program execution as step\u00adping through a transition system, with each step \nmoving from some label-state pair, 1:s, to a new label-state pair, 1 : s . (We will formalize this notion \nof language semantics in the next section. ) We use the following notation for such transitions: l.s+-+l \ns Then, the execution of the program goes as follows: L1:SI+L2:S2*L3:S3-L2:S4*L3:S5+ This program has \ninitial state LI : SI and immediately steps into a loop that alternates between labels Lz and L3. Note \nthat the states are continually changing throughout the loop. The state properties are essentially the \nset of states for which the property is true. Any state-based analysis will thus determme properties \n~(LI) z C(L1) = {SI} ~(Lz) ~ C(LZ) = {S2, S4,. .} ~(L3) ~ C(L3) = {s3, s5,. .} Of course, the practical \nquestion is what, exactly, are S(LI ), c$(L2), and S(LS ) The smaller they are, the more precise the \nproperties, but in general the occurrences of ~ cannot be replaced with = (up to isomorphism) because \nthose exact sets may not be computable. z In order to address this problem, there has been much recent \nwork on refining the notion of program point to something more descriptive of a run-time point of execution \nthan just the simple textual label at that point. The basic idea is to determine properties of the form \nS ~ Point --+ Property [State] where Point = Label x Occurrence In this improved formulation, we are \ngiven the possibility of partitioning the states that occur at a given label mto a finite number of occurrences \nand glvmg each such class its own 21n general, we may be satMled with easier questions than mem\u00adbership \n(i e,, the question of whether a given state has the property in question or not), hut interesting questions \nare incomputable in general. property, In the literature, this Idea is sometimes informally referred \nto as polyvarzance [7, 23] and is also closely related to the notion of cloning in dataflow analysis, \nA common form of polyvariance involves partitioning certain blocks of code, such as function bodies or \nloop bodies, by the label that preceded their entry, For Program 1, we might take Occur-rence = Label \nand use occurrences to keep track of the label that preceded the current one. This would allow the analysis \nto produce a result that maps (Lc, LX ) to a property that indicates that b must be TRUE, and (Lc, LY \n) to a property indicating that b must be F.4LSE. For Program 2, we might use Occurrence = {EVEN, ODD} \nto differentiate between the odd and even iterations of a 100D. This would allow an analvsis to infer \nthat the value of ,. the variable n in the program is odd during odd iterations of the loop, and even \nduring even iterations, Such polyvariant analyses can be quite effective, espe\u00adcially for analyzing programs \nwith non-recursive functions In these cases, functions might be called from several syntac\u00adtic points \nin the program, each of which might cause very dif\u00adferent run-time behaviors worth descrlbmg separately. \nAlso appeahng is the fact that it is straightforward to extend this idea and keep track of, say, the \nlast two calling points in\u00adstead of just the last one, or in general the last k for some finite k [19, \n27]. Indeed, there is quite a bit of room for cre\u00adativity here, as the only real requirement on the set \nPoint is that its elements must have finite descriptions. Some re\u00adcent proposals have used fairly complex \nmechanisms such as procedure strings [17]. Unfortunately, the idea of polyvariance, and indeed any formulation \nof program analysis that involves only a refin\u00adement of the notion of program point, is fundamentally \nlimited Consider the following program that uses Euclid s algorithm to compute the greatest common divisor: \nL1 : if b = Othen Lexit eke La Lz a, b+ b,amodb (3) L3 : if b = Othen Lexit eke Lz Lex,t : Note that \nwe use a parallel assignment operator in order to simplify some of the examples later on in this paper. \nAt first glance it seems that we should be able to ex\u00adpress the fact that the b of one iteration is always \nequal to the a of the next iteration (perhaps using a variation of the even/odd occurrences from above). \nThis property might en\u00adable a compiler optimization that unrolls the loop once and compiles the second \ncopy with the opposite register order, thereby cutting the number of assignments roughly m half LI : \nif b = Othen Lex,t else Lz L2 .a~amodb L3 if a = Othen Lswap eke LJ L4 :b+b moda L5 : if b = O then Lexit \nelse Lz LSWaP a, bi-b,a L exit:, ,, However, we cannot find a simple or elegant expression of this fact, \nno matter how precise our notion of program point, because we need somehow to associate sz with S4, S3 \nwith S5, Sd with ,S6, ad z,nfinztum, but the set Point of descriptions of program points must be finite. \nTo attempt to use polyvarl\u00ad ance, or any other technique that involves a more precise notion of run-time \npoints, 1s extremely difficult (perhaps 3.1 Notation even impossible) because we need to dlstmgulsh among \nan znfimte collection of state sets with only finite descriptions of program points. Fundamentally, all \nof the states that oc\u00adcur at the same label are described by the same property Thus, refining (or enlarging) \nthe set of program points does little more than delay the mewtable folding of states. In fact, this is \na problem that comes up m almost any analyzer for a program with loops (or recursive functions) and is \na common source of both complexity and imprecision in semantics-based program analyzers. Transfer Transition \nSystems In order to solve this problem, we abandon the approach of refining the notion of [program point \nand instead refine the notion of property. Specifically, we generahze from state properties to trace \nproperties. T= Point + Property [Trace] Traces will be formalized below, so we will begin with an informal \nexplanation. One can think of a trace as a record of a possible execution history of a program, starting \nat some label and state. In constrast to a state-based analy\u00adsis, which infers properties as sets of \nstates, a trace-based analysis infers properties as sets of entire evaluation traces, So, for example, \nfor the greatest-common divisor program of Example 3 we have the following properties: T(LI)g{LI:sI+L2s2*L3.s3+} \nT(L2)2{L2 :s21-+L3: s3++L2:s4* ., L2,s4+L3.s5-L2:&#38;j+ ., } T(L3)2{L3 :s3++L2: s41+L3:s51-+ ., L3:S5+L2: \nS6+L3:S7 +,,, .} For a label 1, the trace property T(1) is a superset of the set of traces that can begin \nat 1. Assuming that states are pair of integers [a, b] glvmg the values of variables a and b, the variable-swapping \nproperty of the program at label LZ can now be expressed (finitely) as follows: {L,z:[a, b]+ L3. [b, \namodb]~LZ: [b, amodb]la, bENat} In effect, this says that if we are at label L2, then the next time we \nreach label L2 the value of a will be equal to the cur\u00adrent value of b. (For the sake of simplicity we \nhave avoided writing out the full trace property for T(L2 ), which would also specify the behavior of \nthe loop exit. ) Note that this is exactly the information we desire! Furthermore, this prop\u00aderty gives \nus ezact information about a, rather than an up\u00adper approximation of its value, so in effect we have \nbeen able to model the program s loop by a fint.te transition sequence, without any loss of information. \nAs we shall see, this notion of trace properties will al\u00adlow us to construct very precise analyzers for \nprograms with loops and complex data structures. To do this, we shall introduce the semantic framework \nof transfer transit~on sys\u00adtems, which in addition to formalizing the notion of execu\u00adtion traces will \nallow us to define program transformation rules for complex optimizations such as software plpelinmg \nand classical loop optimization, We now present a formal framework for trace-based analy\u00adses We use following \nnotation If ~ is a function, Rng(f) denotes the range (co-domain) of ~. If f is partial, Dom(~) denotes \nthe domain of f The set of partial functions from A to B is written as A J B. Pairs (i.e.. elements of \na ./ set A x B) are written either as (a, b) or, when denoting a configuration of a transition system, \na: b, 3.2 Single-step transition systems Our framework starts with a small-step operational seman\u00adtics. \nIn order to have a standard basis for this, we use tran\u00adsition systems Definition 1 A transition system \nIS a pan-( Config, J) where Config zs the set of con$guratzons of the system and 6 < Config x Config \nu the transztzon relatton. We wrvte CCIC to mean that (c, c ) E 6, and we wrde c16 . . 6C% to mean that \nc,6c,+1 for all 1 ~ i < n. We write 6* foT the reflexive transztwe closure of d. Specdically, we are \ninterested in transition systems of the form (Label x State, +) that satisfy the following condition: \nCondition 1 For all 1,1 E Label and s E State, l:s \u00ad 1 s and 1:s + 1 ;s tmpltes s = s . Note that this \ncondition is much weaker than requiring that the system be deterministic, Any transition system will \nsat\u00adisfy this condition after some simple local transformations at each point in the system that fails \nthe condition. For ex\u00adample, lfl:s-1 :s andl:s++l :s but s #s , we can simply create a new label 1 and \nchange configuration 1 : s to 1 s Essentially, Condition 1 states that from a config\u00aduration 1:s there \ncan be many possible single steps, but all of the target configurations must have different labels. \n3.3 Transfer functions and transfer t ransi\u00ad tion systems A transfer functton A E State -State is a \npartial function that describes how a state evolves during program execu\u00adtion. In essence, a transfer \nfunction represents part of the computation of a program. So, if A(s) = s , then at some point the program \nwill compute state s when starting from state s. Given a transition system (Label x State, ~) that sat\u00adisfies \nCondition 1, we can define a szngle-step transfer func\u00ad tion for each pair of labels that captures exactly \nthe possible single-step transitions between those labels ifl, s~l :s A!, (S) = ;:defined otherwise { \n Then, llsl-l~:s~-...wl,, ,Sn if and only if This inspires the definition of a new transition system \nfrom the orlgmal one m which states are compound transfer func\u00adtions built up from compositions of single-step \ntransfer func\u00adtions. Definition 2 Gzven a transaction system (Label x State, +) that satisjies Cond~tion \n1, we define the transfer transition system (Label x (State J State), ==+) as l:A ==+ 1 : (A~, o A) for \nall A E State 2 State. Intuitively, if A(s) = s , then 1:A means that it is possi\u00adble to reach a configuration \n1: s when starting from state s. The definition then shows how single-step transfer functions may be \ncomposed to specify additional steps of a program s computation. Thus, there is a direct correspondence \nbe\u00adtween traces in a transfer transition system and traces in the standard single-step semantics. This \nis stated formally in the following lemma: Lemmal Ijll:Al -~~~ ==+ in.An and Al(s) = s1, then A,(s)=sZ \niflll:sl +-+. +1.: s., wher-el <Z ~n. Corollary 1 l: N .s ==+ 1 :A5 . s tff 1:s k--+ 1 . s The corollary \nstates that the transfer transitions between the constant transfer functions are isomorphic to the tran\u00adsitions \nin the original system. But more interesting is the case of non-constant transfer functions. For mst \nante, sup\u00adpose that ll:sll---+. ..+-+ln:sn and ll:si-l---+ ln:s; where S1 # sj. By the above lemma, \nboth of these sequences can be represented by the single transfer sequence where there exist two states \ns and s such that Al (s) = SI and AL (s ) = S;. The original two transition sequences are then exactly \nand lI:AI(S ) t+ I+ln:An (S ). It does not matter exactly what s and s are, only that they select the \ntwo starting states SI and sj. In fact, this finite sequence of transfer functions might specify an arbi\u00adtrary \nnumber of possible program behaviors, depending on the size of Dom(Al ). This is the key point about \ntrans\u00adfer transition systems: the transfer functions in a sequence carry information about how the state \nis altered relative to the jh-st transfer junction in the sequence. The relative nature of transfer functions \nallows two sep\u00adarate sequences of transfer transitions to be composed to create a third long sequence. \nThis is accomplished by changing, in a systematic way, the transfer functions of the second sequence \nso that they describe how the state is changed relative to the beginning of the first sequence. In\u00adtuitively, \ngiven a transfer function A, we can view its range Rng(A) ~ State to be a boolean property satisfied \nby all states s such that 1: A might correspond to z:s in some ap\u00ad plication of Lemma 1. If A(s) = s, \nthen s already satisfies this property. Suppose A (s) = s for all s E Rng(A). This intuitively means \nthat the output property of a sequence ending with A is stronger than the input property of a sequence \nbeginning with A , because A may also be de\u00adfined for some s @ Rng(A). In this case, the two sequences \nmay be composed to form a larger sequence. The follow\u00ad ing lemma specifies how the transfer functions \nof the second sequence must be modified in order to carry out the compo\u00ad sition. Lemma2 If ll:Al ==+ \n. . . * lm:Am and lj:A~ =+ =+ l~:A~ an dl~ = l! and A~(s) = s for ails E Rng(An), then Proof: By the \ndefinition, for all 2 ~ j ~ m, A; = A~~ 1 o J 0 A~j o A~. If A~(s) = s for all s c Rng(An), then 2 A~oAn \n=An, andthus A~ oAn =A~~ l o..oA~~ oAn, J 2 By definition, and so, since 1., = 1;, o This lemma provides \nthe theoretical foundation for the com\u00adpositlonality of trace-based analyses. Particularly useful is \nthe following corollary, which describes how a sequence that begins and ends at the same label may be \ncomposed with itself any number of times. Corollary 2 If 11:Al * =+-ln:An and 11 = 1. and AI(s) = s for \nall s E Rng(An), then, for all k >0, t times where f(i) = f 0 ...0 /, This means that some transfer \ntransition sequences of length n contain enough information to describe sequences in the original transition \nsystem of k(n 1) + 1 for any k. Later, we will use transfer transition systems to define program\u00adming \nlanguages, and this corollary will allow us to model and systematically examine all possible finite behaviors \nof loops with a transfer sequence that corresponds to just one iteration. In essence, one can think of \nRng(A, ) as encoding the loop invariant at label 1,. 4 A Case Study To illustrate how transfer transition \nsystems can be used to analyze programs, we will outline the development of the system for a simple assembly-like \nlanguage with data structures (records), assignment, and loops, three language features that typically \ncause difficulties for analyzers. The development is organized as follows: 1. Define the semantics of \nthe language as a small-step transition system. 2. Design a finite representation of transfer functions \nof this system.  3. Redefine the semantics in terms of single-step transfer functions and show equivalence \nto the first formula\u00adtion 4. Define an effectively-computable composition opera\u00adtion on the transfer \nfunction representation for the purposes of the transfer transition system  5 Choose a strategy for \nexploring the transfer transition system. Although the language we have chosen is rather low-level, this \nchoice 1s mainly for simplicity of presentation What is most important is that the set State that we \nwill choose for the transition semantics of the language is a very general choice essentially a store \ngraph and can be used for many different languages, including languages with functions. So the development \nm this section can be reused for any lan\u00adguage whose state of evaluation can be defined with this State. \n4.1 The language and a transition system semantics A program in this language is a command, where a com\u00admand \nis either a set of parallel assignments, a record cre\u00adation, a conditional command, an unconditional \njump, or a compound command On the left side of each assignment is an l-ezpreswon denoting an l-value, \nwhich is either a vari\u00adable or a field of a record For instance, the l-expression z .f. f denotes field \n.f of the record in field ~ of the record in variable x. Fields can be updated without restriction; for \ninstance, the assignment z.~ ~ z sets field f of the record in z to point to that record itself. Binary \noperations range over a set of basic constants. comm : = l,lel, . . ,len+-el, . . ..en parallel assign \nI l:le +-(~, = e~,. ,.fn = e.) ~n:~i~a:tlon I 1: if e then comm endif I l:goto 1 Jump \\ comm; comm sequencing \n= klleleope expressions denote values 1: ;= z I le.f l-expressions denote l-values := +1=1 prlmltive \noperations on values OP values (constants, record ptrs) = Wo 1; ::= Zl+f l-values (variables, record \nfields) 1G Label labels k E Const constants x E Var variables .f E Fzeld record field names d E Ptr record \npointers We define the small-step semantics of this language by a transition system (Label x State, \n~), where a configura\u00ad tion 1:s comprises a label representing the current syntac\u00ad tic point of evaluation \nand a state representing the current store. States are finite partial maps from l-values to values: s::= \n{ iv] +Vl). ... iv,, tiwn} Each command in the program induces a family of tran\u00adsitions given by the \nfollowing rules Here, next(l) denotes the label of the command that occurs Immediately after the command \nlabeled with t when the program 1s written out linearly (1 e , as a string) as given by Its parse tree. \nWe also use the notation s[lv * v] to denote the state that maps lV to v and is everywhere else equivalent \nto s, and new (s, i) returns the ith next pointer not occurring in s 3 llel,. ... len~el, , en 1 = nezt(l) \nh, = L&#38;[le,]s v, = f[e,]s l:swl : s[lvl+vl][lvn+vn] 1 le+(fl= el,..., fn= en) l =nezt(l) lV = L&#38;[le]s \nv, = &#38;[et]s @= new(s)l) 1:s * 1 . S[1V + @][@.fl + VI] [d f. * w] 1:if e then lt,ue:.endi~ lfal,e:. \n&#38;[e]s = v E {TRUE, F.mm} 1 goto 1 1,s-1. s l: SUI . S The tasks of looking up values m the store \nand applying primitive operations are done by the basic partial functions L&#38;[]s and &#38;[]s that \nevaluate l-expressions to l-values and expressions to values, respectively, in a given state. These functions \nare simple and do not modify the state, so they are performed within a single transition. LqT]s =z qk]s \n=k L&#38;[le.f]s = (&#38;[le]s).f E[le]s = s(fX[le]s) ~[e op e ]s = (t[e]s) op (&#38;[e ]s) Note that \nthe semantics of the parallel assignment command performs the bindings from left-to-right, and so If \nmultlple l-expressions evaluate to the same l-value it is the right\u00admost corresponding expression that \ngets bound. However, it will slrnphfy the presentation of the development to fol\u00adlow If we know that \nall the l-values in a parallel assignment are different, thus rendering the order of binding irrelevant \nTherefore, we make a syntactic restriction on vahd assign\u00adment commands: No two l-expressions on the \nleft side of a parallel assignment may be the same variable, nor may they termmate with the same field \nname.  4.2 Representing transfer functicms We know from Section 3.3 that we can define a single-step \ntransfer function Al, @ State -State to describe the tran\u00adsitions between configurations at label 1 and \nthose at 1 We rewew the definition here: ifls~l :s A!, (S) = ~ndefined otherwise { Furthermore, we can \ncompose these functions to define the transfer transition system (Label x (State J State), =+). Later, \nin Section 4.5, we will be generatmg traces of the transfer system But to output these traces, we first \nneed a way of repres entmg transfer functions. As a t echmcalit y, It wdl be convenient to augment the \nset of expressions and l-expressions with an integer representation of pointers. le ::= I i. j augmented \nl-expressions .. e . ]2 augmented expressions where i G Nat 3For this purpose, it is assumed that the \nset Ptr is equipped with an enumeration. Then, to represent a (potentially infinite) transition func\u00adtion \nA c State ~ State, we use the following ,grammar, i A ::= {(U1,C1), ... (an,en)} transfer functions ::= \n{lel N el, . . . . le,, H en} store modifications ~ ::= {cl,...,e~} condition sets  In this representation, \na transfer function consists of a finite set of pairs (a, C). Each pau handles a clisJoint subset of \nthe domain of A, and together they handle the entire do\u00admain of A. Suppose that the pair (o-, C) handles \nthe subset S ~ Dom(.A). Then a 1s a fimte map from l-expressions to expressions representing the modifications \nthat A makes to each state s E S, and where C is a finite condztton set of expressions that must all \nevaluate to true m any state s E S. The conditions maintam two kinds of information: con\u00adtrol constraints \n(for modeling conditional expressions) and sharing constraints. To give a feel for how these objects \nrepresent transfer functions, we now present some simple examples. A =1= {(0,0)} A(s) =s A = {({z *1/+ \nz}, O)} A(s) = S[z * s(y)+ s(z)] A = {({z* 1,1.cARNa CAR,l CDR++b}, O)} A(s) = S[Z + @][@.cAR M s((s(cz)).cAR)] \n[@.cDR N s(b)] where + = new(s, 1) A = {({x ++2}, {Z > ?4})} s[z ++ s(z)] if s(a) > s(y) A(s) = undefined \notherwise{ Note that we denote the identity function {(O, 0)} by I. All of these examples have just \na single pair (a, C), but in general an exact representation of a transfer function re\u00adquires more than \none pau-. This is because a fragment of code might behave completely differently when evaluated in contexts \nwith different sharing; an example of this will be given in SectIon 4.4. In general, the meaning of a \ntransfer function represen\u00adtation A 1s ({lelwel,...,le~ tie~}, C)~A I.X[lei]s = lv,, t[e, ]s = w, 1s \ni < n. A(s) = f VeEC. f[e]s =TRUE s = S[lvl l-+ LJl] .[lun * %] {{ undefined otherwise where ,C&#38;[]s \nand ~[]s are extended to handle the [aug\u00admented expressions and l-expressions as follows: L&#38;[i.~]s \n= (new(s, z)).~ t[i]s = new(s, i) At first glance it may appear that there are two potential sources \nof ambiguity in the definition of A(s). There may be more than one pair (a, C) in A that may be applicable \nto a given state, and also the order of the bindings is unspecified. These potential ambiguities are \navoided by maintaining the following representation invariants: 41rI an abuse of notation, we denoLe \nby A both the transfer function representation and the actual transfer functton that lt represents. 1 \nFor any state s, there is at most one pair (a, C) ~ A for which all e ~ C, &#38;[e]s = TRUE. Intuitively, \neach element of A imposes a disjoint set of sharing constraints on s. 2 For the unique pair ({lel + cl,... \n, le~ ++ en}, C) that does satisfy this condition, if that pair exists, the sharing constraints in C \nwill ensure that L&#38;[le, ]s = f. X[leJ ]s rmplies z = j, and thus the order of binding is irrelevant. \n  4.3 The semantics as single-step transfer functions Now that we have a useful fimte representation \nfor transfer functions, we will redefine our small-step semantics in terms of single-step transfer functions \nA!,. This 1s quite straight\u00adforward, and m fact the rules become even simpler because all apphcatrons \nof U[]s and &#38;[]s are handled implicitly by {he transfer functions themselves. Recall that I denotes \nthe identity transfer function {(0, 0)}. he,, .,le. +-e~,..., e,,. 1 = next(l) A;, = {({lel t-+el,...,len \n++ en},o)} 1:if e then ltrue. endif; lfal~e: v e {TRUE, FALSE] 1, goto 1 A~u ~ {(0, {e =uj)} A?, = The \nfollowing deed define the semantics. theorem correct states single-step that the transfer above fun rules \nctions do of in\u00adthe Theorem 1 ifl:s-1 :s A!, (S) = ;;defined otherwise { Proof: Straightforward for \neach of the four rules. o 4.4 The composition operation We now have a representation of transfer functions \nand we have defined all the single-step transfer functions AI,. These are the building blocks of a trace-based \nanalysis, but to use Definition 2 to build traces of the transfer system, and to use Lemma 2 and Corollary \n2 to compose the traces, we must also have an effectively computable composd~on op\u00ad eratzon on transfer \nfunction represent at ions. The composi\u00adtion operation is the universal tool of a trace-based analysis \nframework; it captures the essence of many seemingly unre\u00adlated analysis problems (several examples of \nwhich we give in Section 5). What makes a composition operation so subtle is the in\u00adteraction with the \npossible sharing in the two transfer func\u00adtions being composed. As an example, consider the following \ncode fragment that performs two record field assignments: LI a.F+bj L2 : c.F.FF d; L3 : The semantics \ndefines the following single-step transfer func\u00adtions: A; = {({a.F* b}) O)} A~~ = {({c.F.F + d},@)} \nBy the defimtlon of transfer transition systems, we can con\u00adsider the following transfer trace from the \nidentity transfer function I: Ll:I=+LZA~~OI=+L3: A~ oA~~oI We should expect A~~ o I = A~~, but what \nthen is A; o A: ? It is a functio~ that, given a state s, returns the state resulting from the two assignment \noperations But to represent this function, we need to take into consideration the different types of \nsharing that might occur in s, If there 1s no relevant sharmg, then the composition might be simply A;: \no A;: = {({Q,F +. b, c,F,F M d}, {a # c, u # c,F})} where the second component of the pair is the condition \nset that imposes the required sharing constraints, But suppose that s(a) = s(b) = s(c) for some imtial \nstate s Then after these two assignment operations take place, a F, b F, and c F are all equal to d, \nand the composition might then be represented by A&#38; oA~~ = {({a.FH d}, {a = c,a = b})}, Even though \nthis is a very simple code fragment, this result is rather surpmsing. It says that if the fragment IS \nevaluated from any state in which a = b = c, then the entire behavior of the code is to assign d to a.F, \nthat is, this sequence of assignment commands is equivalent to the single assignment command aF +-d \nIn particular, C,F.F might not equal d even though that was the final assignment that occurred. A more \ndetaded example of alias detection is given in Section 5.3. Both of the possibilities above must be included \nin the L2 AL1 and this is why a transfer func\u00ad transfer function AL3 o L2 > tion representation can \nhave more than one pair (a, C) For this particular example, the composition operation actually finds \ntwo additional cases. The result of the composition IS: A&#38; o~~~ ={({a FX d}, {a= c,a= b), ({a.F* \nb,b.FH d}, {a= c)a# b}), ({a FN d}, {a# c,a= c.F}), ({a F@ b,c FFxd}, {a#c)a#c.F})}  The general composition \nA o A of any two ransfer function representations is defined structurally as {(a[le~ * ej] [le~ N e~], \nC ) / f~~)~~:...,len +en}, {e~, e~}e~} E A , > where ~~[z]a = {(z, 0)} r&#38;[z.f]a = {((=w(a, t))..f) \no)} ~Z[le..f]ff = {(t.f, C) I (i, C) C ;[le]a} U {(le .f, CU {Zg = [e }) I (le , C) E &#38;[le]a, le \nf 6 Dom(a)} U {(le .f, C U {1< # le ] le . f E Dom(a)}) \\ (le , C) ~ t[le]a} qk]ff = {(k, 0)} /Y[el op \nez]a = {(ej ~ ej,Cl U CZ) I Here, a[le] equals a(le) if it 1s defined, and /e otherwise, n~w (a, i) \nreturns the ith first integer not appearing in a; and el @ el returns an expression e such that &#38;[el \nop ez]s = &#38;[e]s for all states s. It is always correct to choose Lel op ez for el @ e~, but it might \nbe desirable to apply a system of simplification equations on the primitive operations, if such a system \nexists. Conceptually, given (a, C) ~ A and (u , C ) 6 A , we need to find all possible ways that sharing \ninterference could affect the bindings in a . The functions L &#38;[]cr and ~[]a determme all such posslblhties \nand build up the associated condition sets describing the sharing constraints for each case. When that \nus done for all augmented 1-expressions and augmented expressions in (a , C ), then the bindings take \nplace, possibly overwriting some bindings aheady existing m CT, and the constraints are updated for that \nparticular set of bindings. The consistent condition is a structural requn-ement of the condition set \nthat ensures that there IS no e and e such that e = e and e # e m the transitive closure of the equalities \nand inequalities in the condition set. Lemma 3 For all transfer functzon representations A and A and \nstates s, (A o A)(s) = A (A(s)) (where o zs the composttzon operat%on defined above). 4.5 The transfer \ntransition system Definition 2 defines the transfer transition system (Label x (State J State), ==+) \nfrom the semantics. 1A ==+ 1 : (A!, o A) for all A G State -State, We now have a finite representation \nof these transfer func\u00adtions A, the single-step transfer functions A;,, and an ef\u00adfectively computable \ncomposition operation o on these rep\u00adresentations, Therefore, we have all the required tools for exploring \nthe transfer transition system. Of course, this system is of infinite size and cannot be explored completely, \nbut we can use our theoretical development in SectIon 3 to selectively explore the system. For instance, \nwe can use Lemma 2 to piece indlwdual traces together, and we can use Corollary 2 to reason about loops \nby exploring only one it\u00aderation. Many strategies are possible, and different ones will be appropriate \nfor different problems. Below we describe a strategy that wdl be useful m our example applications m \nSection 5. In general, given an initial transfer configuration 11: Al, we can start generating the possible \ntransfer traces 11: Al =+ 12: AZ =+ ~, Such traces fall mto two classes 1. Eventually, a configuration \n1,: A, is reached such that there exists a j < i such that 1, = lJ. These traces correspond to evaluations \nof the program from 11 that eventually take some form of loop. 2. AlternativeIv, these traces corres~oud \nto evaluations of the program from 11 that reach-a configuration from which no further transitions are \npossible before an,y form of loop 1s taken.  In both cases, the traces have length at most n + 1, where \nn is the number of labels in the program, and are thus com\u00adputable. It is particularly useful to compute \nthese traces from the identity transfer function I, because then we are sure to be able to use Lemma \n2 and Corollary 2 to piece together these traces in order to compute longer traces Given a label 11, \nwe can compute the set Zoops(ll ) of traces of the first kind. loops(lI)={tl:I~.~ln:Al %<n. lz=ln) m%<j<nll= \nl,)} and the set termvaals (11) of traces of the second kind: termmals(ll) = {11:1=+ . =+ ln:A I ~31, \nA . ln:A =+ 1A , 73i<j~n. lL =13} By repeated applications of Lemma 2, we can inductively define (but \nnot necessary compute) the sets traces (1) for all labels 1 of all traces from configuration 1: I. (The \noccurrences of Al and Aj in these rules are always equal to I.) 5 Applications In this section we present \nsome concrete examples of how trace-based analysis can be applied to solve problems m pro\u00adgram analysis \nand transformation.  5.1 Software pipelining One of the advantages of trace-based analysis is that it \npro\u00advides a direct way to reason about program equivalences, and thus it can serve as a basis for specifying \nand proving the correctness of program transformations. In this section, we demonstrate this by showing \nhow a form of software pipelin\u00ading can be developed using a transfer transition system. Software pipelming \n[2, 22] is a program transformation on loops that attempts to exploit instruction-level paral\u00adlelism \nin superscalar and VLIW architectures As an exam\u00adple, consider the following code fragment taken from \n[2] (but with a conditional statement added so that the loop exit can be expressed): LentrY : i.+--i+l; \nL1 : j+t+h; LZ k+i+g; L3 : l+j+l;  (4) L4 : test+-z< n; L5 if test then L6 gotO L.n@ endi~ ... Lexit \n The classical approach to optimizing this loop involves un\u00adrolling it once, analyzing the data dependencies, \nand then optimizing for maximum parallelism within the loop body This yields the following: Le~t~Y : \n2+2+1; L1 : j,k, test +--z +h, z+g, i<n; LZ if test then L3 1,2+--]+ 1,1+1; L4 ~,k, test +i+h, i+g, z<n; \nL5 if test then L6 l+ J+l; L7 goto Le~trY L8 : endif; L9 enciifi LIO : J+j+l LeXit :  While this is \ndefinitely an improvement, this simple ap\u00adproach fails to extract all of the parallelism available between \nadjacent loop Iterations. In particular, the assignments to 1 and z at labels LG and Lent,Y, respectively, \ncan be performed in parallel, but this is not detected by the classical approach. Software pipelining \nachieves this additional parallelism by first determining the patterns of potential parallelism across \nloop iterations and then using this information to transform the loop. The effectiveness of this technique \ncan be seen in the following code, which is the result of applying software pipelining to our original \nloop: LentrY : 2+2+1; L1 : j,k, test ei+h, i+g, i<n; LZ if test then l,2+j+l, z+l; (5); : goto L1 endi~ \nL5 : l+j+l Lexit : .. The state-of-the-art m software pipelining is quite pOW\u00aderful, but rather ad-hoc. \nThe transformations rely on much a priori knowledge (such as the lack of aliases), as well as knowledge \nof the loop structure and data dependencies. The correctness proofs are thus long and tedious and are \nnot based on satisfactory mathematical underpinnings. Using the transfer transition system semantics, \nwe can formal\u00adlze what it means for two looping program fragments to be equivalent and thus interchangeable \nwithout affecting the observational behavior of the whole program. Recall that a loop in our language \nis syntactically a com\u00admand, comm. We assume that there is a distinguished entry label denoted by entry \n(comrn) and a distinguished exit label denoted by exit ( cornm). In our current example, the entry and \nexit labels are Lentry and Lexit. Given the transition sys-Then, by the above lemma about function concatenation, \ntem semantics, (Label x State, -), the meaning of comm, this holds df (Aexit2 o AIOOP2 k) o Aent,rY2 \n)(s) = s , which in U( comm), is defined as follows turn holds Iff s c U( commz) (s) (by a reversal of \nthe above), Therefore, we can conclude that p( comml ) = U( commz), p(comm) = As. {s I (entry (comm))s \n~ (emt(comm)) .s } o Because our language is determmistic, the set p( comm) (s) E either @If evaluation \nfrom state s never reaches the exit la\u00adbel, or else {s }, where s is the unique resulting state at the \nexit label. We note, however, that the following nlethod\u00adology is apphcable to any transition system \nthat satisfies Condition 1, including nondetermirmtlc systems. If ~(comm) = p(comm ), it 1s safe to replace \ncomm by comm m an,y context. Of course, this question is unde\u00adcidable in general, and indeed one of the \nmost Important motivations for the formal semantics of programming lan\u00adguages is to reason about such \nprogram equivalences. In many difficult cases, an analysls of the transfer transition system can automatically \nprove such an equality and thus support optimizing program transformations such as soft\u00adware pipelining. \nIn general, we have the following theorem that proves semantic equivalence of two looping commands, Theorem \n2 Gxven two commands comml and commz, for z s {1, 2} let termtnals, and loops~ be computed from comm, \nas tn Section 4.5, whe re ~ denotes the transfer transt\u00ad tzon relatton of comm,, and let lent,Y, = entry \n(comm, ) and lexitt = ex%t(comm, ). If there ezzsts a ~,h,ft such that the following equatzons hold ~or \nz 6 {1, 2}, then p(comml) = p(commz). AentrY2 = Ashlft o Aentwl &#38;oop2 o A~hlft = Ashift 0 A1.opl \nAeXit2 0 A~hift, = AeXltl Proof: Fn-st we need an auxiliary lemma about function composition. If f, \ng, h, f , g , h , and d are functions m X_ Xand d (1): f=cY of ,(2) go fi=dog , and (3): hod = h , then \nho,g[n) of = h og (n) of for all n ~ 0. To prove this, we apply axiom (1) to the statement that h o g(n) \n06 = h o g (n) for all n ~ 0. The latter has a straightforward proof by mducticm on n, axiom (3) is the \nbase case, and axiom (2) proves the inductive case Let ~ be the transition relatlon of the semantics \nof comm,. Then s E M(cornml)(s) iff (entry (comm)) s ~ (ezzt (comm)): s , which m turn holds iff there \nexists a trace And, by Lemma 1, this trace exists IK there exists a transfer trace such that A(s) = s \n, By Lemma 2 and Corollary 2 (termznahl and loops ~), such a transfer trace emsts df there exists a k \nz O such that (A~~,tl o AIOO1jl (k) o Aentryll(s) = s The intuition behind this theorem is that comml \n1s a code fragment that contains a single loop; A~~ti-Yl repre\u00adsents the computation from the entry of \nthe command up to the first loop entry point, AIOOP ~ represents the com\u00ad put atlon from one loop entry \npoint to the next (i e., one loop iteration), and A exit ~ represents the computation from the loop entry \npoint to the exit of the command, Any evaluation of comml that exits will thus be represented by Aexltl \no Aloopl L) o AentrYl, where k is the number of times the loop was taken Everything 1s similar for comm~, \nexcept that lts loop 1s shifted by A~h,ft. This shifting essentially captures the notion of software \npipelining. To illustrate this theorem, take comml to be the original looping program (Program 4) and \ncommz to be the optimized version (Program 5), First, the computation of termmak and loops for each command \ngenerates Aentry ~= I f%oop, ={({z-z +1, jtiz+h+l, k*z+g+l, l~~+h+2, testw~-f-l <n}, {(1+1 <n) =TRUE})} \nAexit ~ ={({z*z +1, jtiz+h+l, k+z+g+l; l~z+h+2) test~i+l <n}, {(2 + 1 <n)= FALSE})} A,nt,y2 = {({z * \nz + 1},0)} &#38;oop2 ={({t*z +1, jtiz+h, kti Z+g, l+z+h+l, test+ i< n}, {(z < n) = TRUE})} AeXit2 ={({jtiz+h, \nktiz+g, lxt+h+l, testwt <n}, {(t <n) = FALSE})} Here, we have written bindings in lexicographic order \nand have assumed a canomcal form for expressions, where for clarlty we have replaced 1 + 1 with 2 Note \nthe ddlerences between the bindings of ,1, k, and 1 in AIOOPI and those of AI OOP,,. This 1s because \nin comml these assignments take place ~fter the increment of z, while m comm~ they take place before. \nNext, we find an appropriate A~h,ft. Since A.e,~trY ~ = 1, &#38;ll,ft must be the same as AentrY2 Ashlft= \nAentrY2 = {({z M z + 1}, 0)} This corresponds to the fact that the loop of cornml has been shifted in \ncommz In each increment of ~ in commz conceptually corresponds to that of the ne~t iteration. The final \nstep 1s the proof that the following hold: Aentry2 = Ashift o A entry ~ 0 &#38;hlft &#38;hlft iOOp=0 \nIAloop2 Aexit j o Ash,ft = ~exlt 1 First we compute the compositions and then approximate the equahty \ncheck by checking for structural equahty If two transfer function representations are structurally equivalent, \nthen they are guaranteed to represent the same function Much more sophisticated strategies could be developed, \nfor instance using algebras for expression equahty, but the im\u00adportant point 1s that whale structural \nequaht,y 1s usually of very little use in standard transition systems, it 1s quite pow\u00aderful for transfer \ntransition systems It m sufficient for the above example, and we beheve that it will be sufficient for \nmany uses. In short, the entire proof is completely automated and with a tractable complexity.5 The \nalgorithm used the fact that Aentry ~ = I for the derivation of AShift as Aentry2, but m general we need \nan algorithm to solve A = A~hift o A given any A and A One can imagine a unification algorithm for this \npurpose, but space does not permit further development. The automation of the proof suggests that the \nprocedure could be used directly to derwe correct program transfor\u00admations, not just for software pipelining, \nbut for loop op\u00adtimization in general. In the next section we see another example. 5.2 Loop-invariant \nremoval We can use a technique similar to that of software pipelining to reason automatically about a \ntransformation that factors out calculations that remain invariant in each iteration of a loop. The only \ndifference between software pipelining and loop-invariant removal is the set of axioms. Theorem 3 Gwen \ntwo commands comml and commz, tf there extsts a Ainv such that the followang equatzons hold, where ~entry,, \nAIOOP, J and, Aexit,, are defined as m Theo\u00ad rem 2, then ~(comml) = p(commz). AentrY2 = A,nv o Aentryl \nAIOOp,, o Ainv = AIOOpl Ainv ~AIOOP2 = A]oopz Aexit2 o Ainv = AeXitl Proof: The proof mirrors that of \nTheorem 2, but with a different lemma about function composition. If f, g, h, f , g , h , and 6 are functions \nin X -X and if (1): f = 50 f , (2): gor$= g )(3): 6og=g)and (4)ho6==h , then hog(~) o f = ~~og ( ) o \nf for all n ~ 0. To prove this, we apply axiom (4) to the statement that g(n) o f = 60 g ) o f for all \nn ~ O. The latter has a straightforward proof by induction on n; axiom (1) is the base case, and axioms \n(2) and (3) prove the inductive case as follows: g(n) o f = dogog(~ l) of=~o godo g ( l)o,f/=Jog/( )of/, \no The transfer function A,nv (6 in the proof) represents some computation that comml does inside every \nloop itera\u00adtion, but which commz does once and for all before the loop is first entered. 5.3 Alias analysis \nThe following program is intended to evaluate in a context m which a is bound to a linked list of at \nleast one element and b is bound to a linked list of at least two elements. The intended result of the \nprogram 1sthat b should be bound to a list whose first element is the same, whose second element is a \ns original first element, whose third element is b s original 5Construction of eight traces of no more \nthan n + 1 in length each (every step of which is a composition operation), four additional composition \noperations, and three O(n) structural equality tests. All composition operations are O(n) because no \nsharing constraints are generated. BEFORE AFTER 1. 2.  II b 3. % 4.  ab 5. Figure 1: Output of \nthe alias analysis. second element. and which is thereafter ecmal to the rest of a. The code does this \nby destructive assignment rather than by copying nodes. LI : b.~.~ 6 a.~; L2 : a.P +-b.~; L3 : b,~+-a; \nL4: . Here, the P field contains the link to the next list element. Examming the transfer transitions \nfrom the initial con\u00adfiguration LI I, representing all possible entry configura\u00adtions, terminates after \nthree steps at configuration L4: A, where A is a set of five pairs that describe all possible re\u00adsults: \n({b P.P @ a.p, a.p -b.p, b.p + a}, {a # b.p, b# b.p, b# a}) ({b P.P * a.p, a.p M a}, {a # b.p, b# b.p, \nb= a}) ({b.p + o}, {a # b.p, b = b.p}) ({a.p + a}, {a= b.P, b # b.p}) ({b.p * a}, {a = b.p, b = b.p}) \n The second component of each pair describes a possible set of sharing constraints on the input set, \nand the first el\u00adement describes the updates to the store that take place under those sharing constraints. \nThe updates should be in\u00adterpreted as one big parallel assignment on the initial state. The first pair \nis the intended result, and the other four pairs represent different types of undesirable behavior. The \nanal\u00adysis output can be equivalently described by the diagram shown in Figure 1. One advantage to this \nanalysls is that the composition operation on transfer functions automatically maintains only the relevant \nsharing constraints For instance, in this exam\u00adple the five different pairs correspond to five truly \ndifferent behaviors, and the sharing constraints in those pairs are the minimum constraints necessary \nto identify each case. This analysis has several potential uses. A sophisticated alias analysls that \nis ezacf, on straight\u00adline code, that can relate the output abases with the input abases, and that can \ncompose such relations to\u00adgether to analyze blocks of code separately or describe the aliasing of looping \ncode. Currently, there 1s little understood about relational alias analyses or compo\u00adsitional ahas analyses \n A symbolic debugging tool. One can look at all pos\u00adsible ways a fragment of code might go wrong, and \nm what contexts  A modular program verification tool. In the example above, the analysis infers an exact \nminimal set of input preconditions {a # b.~, b # b.~, b # a} that guaran\u00adtees the correct output. More \nsophisticated code with loops might use Corollary 2 to determine a sufficient, but perhaps not necessary, \nset of preconditions.  Conclusions In this paper we have presented transfer trans~tton systems, a formal \nframework for describing the operational seman\u00adtics of programs, and demonstrated its utility on a language \nwith loops, assignments, and nested record structures. This framework allows us capture the notion of \ntrace propert~es and trace-based program analysts, thereby realizing signif\u00adicant advantages in expressive \npower and elegance in solving complex static-analysis problems, particularly for programs with loops \nFurthermore, we have shown how transfer tran\u00adsition s,ystems can be used to formalize program transfor\u00admations \nfor carrying out optimlzations as complex as soft\u00adware pipelining, classlcal loop optimlzations, and \ndetection of abases, The idea of reasomng about traces instead of states is not new. The theoretical \nfoundations go back to Cousot and Cousot [9]. Them work on Gm SOS [10] is also related, but whereas the \nmain focus of their system is to achieve a unified compositional operational model of non-termmation, \nour motivation for transfer transition systems is ultimately a practical one: to provide a new framework \nnot only for static analysls of programs, but also for developing com\u00adplex program transformations and \noptlmizations that de\u00adpend on precise understanding of complex control flow and data structures There \nare a handful of [trace-based analyses m the lit\u00aderature, such as Harrison s data-dependency analysis \nthat uses procedure strings [17], Colby s analyses of aliasing, data dependency, and synchronizat lon \nof concurrent languages [4, 5, 6], and Deutsch s online alias analysis [13, Sect. 4,4]. It is also common \nto use some notion of execution traces when reasoning about concurrent computations. But m the areas \nof static program analysls and program transforma\u00adtion the technique IS little known. It 1s our behef \nthat this is due to the lack of a presentation of the method that ]s both general enough for wide applicability \nand specdic enough to be easily instantiable. One might reasonably ask why trace-based analysls is necessary, \nsince ad-hoc techniques, many based on dat aflow analysis, often work in practice, at least in simple \ncases [I]. Indeed, for the gcd example shown in Section 2, it is a simple matter to unroll the loop once \nand then perform standard optimizations such as constant propagation, One basic rea\u00adson is that semantics-based \napproaches provide a way to rea\u00adson about correctness and safety of analysls-based program transformations. \nBut there is a less obvious reason of great practical importance the formalism of semantics-based pro\u00adgram \nanalysls E extremely general, and thus yields insight mto ways to solve much more complex analysis problems \nIndeed, it is easy to see that the techmque of unroll once and then do constant propagation 1s not very \ngeneral, and in fact does not work for the purpose of software pipelining. Abstract interpretation, on \nthe other hand, clearly aided the solution to control-flow analysis of higher-order func\u00adtions [20, 27] \nEven further, it is dfflcult to imagine many of the more advanced alias and storage analyses (e g., [14, \n6]) without abstract interpretation, and nor 1s it hkely that the analyses of concurrency in [5, 4] would \nhave been found. The problem is that ad-hoc techniques rarely generahze or shed any hght on techniques \nthat might be adapted for other problems, while the methodology of semantics-based approaches root analyses \nm a general soil a semantics of the language from which other analyses may evolve. Acknowledgements The \nauthors wish to thank Mark Leone, Chris Okasaki, Frank Pfenning, and Stephen Weeks for their helpful \ncom\u00adments and suggestions on earlier drafts of this paper, Also, many thanks to Patrick and Radhia Cousot \nfor providing a stimulating research envuonrnent during this work, References [1] Alfred V. Aho, Ravl \nSethi, and Jeffrey D Unman. Com\u00adpders: Prtnczples, Techniques, and Tools Addlson-Wesley, 1986. [2] A. \nAlken and A. Nicolau. Perfect pipehning A new loop parallelization technique. In Proceedings of the 1988 \nEuropean Symposzum on Programming, LNCS Springer-Verlag, March 1988 [3] D.R Chase, M. Wegman, and F.K, \nZadeck. Analysis of pointers and structures. In Conference on Programming Language Deszgn and Implementation, \npages 296-310, June 1990 [4] Christopher Colby. Analysis of synchromzation and aliasing with abstract \ninterpretation, Unpublished. [5] Christopher Colby Analyzing the communication topology of concurrent \nprograms. In ACM SIGPLAN Symposzum on Parttal Evaluation and Semantics-Based Program Manupulatton, pages \n202-214, June 1995. [6] Christopher Colby. Determining storage properties of sequential and concurrent \nprograms with assignment and structured data, In Proceedings of the International Statzc Analysis Symposium, \nLNCS vol. 983. Springer-Verlag, September 1995. [7] Charles Consel. Polyvariant binding-time analy\u00adsis \nfor applicative languages. In Partzal Evalua\u00adtion and Semantzcs-Based Program Manipulation, New Haven, \nConnecticut (SIGPLAN Notices, vol. 26, no. 9, September 1991), pages 66-77, 1993. [8] P. Cousot and R. \nCousot. Abstract interpretation: A unified lattice model for static analysis of programs by construction \nof approximations of fixpoints. In Fourth Annual ACM Symposium on Principles of Program\u00adming Languages, \n1977. [9] P. Cousot and R. Cousot. Semantic design of program analysis frameworks, In Sixth Annual ACM \nSymposium on Principles of Programming Languagesj San Antonio, Texas, pages 269-282, 1979, [10] P. Cousot \nand R. Cousot. Inductive definitions, seman\u00adtics and abstract interpret ation. In Conference Record of \nthe 19th ACM Symposium on Principles of Program\u00adming Languages, pages 83 94, 1992. [11] P. Cousot and \nR. Cousot. Higher-order abstract in\u00adterpretation (and application to comportment analy\u00adsis generalizing \nstrictness, termination, projection and PER analysis of functional languages). In Proceedings of 1994 \nIEEE International Conference on Computer Languages (ICCL 9,/), Toulouse, France, pages 95-112, May 1994. \n[12] P. Cousot and R. Cousot. Formal language, grammar and set-constraint-based program analysis by abstract \ninterpretation. In Conference on Functional Program\u00adming and Computer Architecture, 1995. [13] Alain \nDeutsch. Operational Models of Programmmg Languages and Representations of Relatzons on Regular Languages \nwith Applzcatzon to the Statzc Determination of Dynamic Altasing Propert~es of Data, Phi) thesis, LIX, \nEcole Polytechnique, Palaiseau, France, 1992. [14] Alain Deutsch. A storeless model of aliasing and its \nabstractions using finite representations of right-regular equivalence relations, In Proceedings of the \nIEEE 1992 International Conference on Computer Languages, San Fransisco, California, pages 2-13, April \n1992. [15] M. Felleisen and D.P. Friedman. Control operators, the seed-machine, and the lambda-calculus. \nIn 3rd Workzng Conference on the Formal Description of Programming Concepts, August 1986. [16] P. Granger. \nStatic analysis on linear congruence equal\u00adities among variables of a program. In TAPSOFT 91, volume \n493 of Lecture Notes in Computer Sczence, pages 169-192. Springer Verlag, 1991. [17] Williams Ludwell \nHarrison, The interprocedural anal\u00adysis and automatic parallelisatlon of scheme programs. Lisp and Symbolic \nComputation, 2(3): 176-396, October 1989. [18] Paul Hudak and Jonathan Young. Collecting interpre\u00adtations \nof expressions. ACM Transactions on Pr-ogram\u00adming Languages and Systems, 13(2) :269 190, Aprd 1991. [19] \nSuresh Jagannathan and Stephen Weeks. A unified treatment of flow analysis in higher-order languages, \nIn Proceedings of the 22nd ACM Symposium on Principles of Programmmg Languages, 1995. [20] N. D. Jones \nand S. S. Muchnick. Flow analysis and opti\u00admization of LISP-like structures. In Sizth Annual ACM Symposium \non Principles of Programming Languages, San Antonioj Texas, pages 244-256, January 1979. [21] M. Karr. \nAffine relationships among variables of a pro\u00adgram. Acts lnformatica, 6.133-151, 1976. [22] Monica Lam. \nSoftware pipelining: An effective schedul\u00ading technique for VLIW machines. In SIGPLAN 88 Conference on \nProgramming Language Destgn and Im\u00adplementation, pages 318 328, June 1988. [23] Torben Mogensen. Binding \ntime analysis for poly\u00admorphically typed higher order languages. In J. Diaz and F. Orejas, editors, TAPSOFT \n89. Proc. Int. Conf. Theory and Practzce of Software Development, Barcelona, Spain, March 1989 (Lecture \nNotes in Com\u00adputer Sczence, vol. 5 52), pages 298 312. Springer-Verlag, 1989. [24] Alan Mycroft. Abstract \nInterpretation and Optzmzsing Transformations for Applzcatwe Programs. PhD thesis, Department of Computer \nScience, University of Edin\u00adburgh, Scotland, 1981. [25] Flemming Nielson. Strictness analysis and denotational \nabstract interpretation. Inforrnatton and Computation, 76(1) :29-92, January 1988. [26] Gordon Plotkin. \nA structural approach to operational semantics. Technical Report DAIMI FN 19, Computer Science Department, \nAarhus University, 1981. [27] Olin Shivers. Control-flow Analysts of Higher-Order Languages. PhD thesis, \nCarnegie Mellon University, Pittsburgh, Pennsylvania, May 1991. [28] Joseph E. Stoy. Denotatzonal Semantics \n: The Scott-Strachey Approach to Programmmg Language Theory. MIT Press, 1977. [29] Philip Wadler and \nR. J. M. Hughes. Projections for strictness analysis. In Third International Conference on Functional \nProgramming and Computer Architec\u00adture, 1987. \n\t\t\t", "proc_id": "237721", "abstract": "", "authors": [{"name": "Christopher Colby", "author_profile_id": "81100165190", "affiliation": "School of Computer Science, Carnegie Mellon University, Pittsburgh, Pennsylvania", "person_id": "P47547", "email_address": "", "orcid_id": ""}, {"name": "Peter Lee", "author_profile_id": "81100384353", "affiliation": "School of Computer Science, Carnegie Mellon University, Pittsburgh, Pennsylvania", "person_id": "PP39040384", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/237721.237776", "year": "1996", "article_id": "237776", "conference": "POPL", "title": "Trace-based program analysis", "url": "http://dl.acm.org/citation.cfm?id=237776"}