{"article_publication_date": "01-01-1996", "fulltext": "\n Static Analysis to Reduce Synchronization Costs in Data-Parallel Programs Manish Gupta and Edith Schonberg \nIBM T. J. Watson Research Center P. O. Box 704, Yorktown Heights, NY 10598 Abstract For a program with \nsufficient parallelism, reducing synchro\u00adnization costs is one of the most important objectives for achieving \nefficient execution on any parallel machine. This paper presents a novel methodology for reducing synchro\u00adnizeation \ncosts of programs compiled for SPMD execution. This methodology combines data flow analysis with commu\u00adnication \nanalysis to determine the ordering between produc\u00adtion and consumption of data on different processors, \nwhich helps in identifying redundant synchronization. The result\u00ading framework is more powerful than \nany that have been previously presented, as it provides the first algorithm that can eliminate synchronization \nmessages even from computa\u00adtions that need communication. We show that several com\u00admonly occurring computation \npatterns such as reductions and stencil computations with reciprocal producer-consumer relationship between \nprocessors lend themselves well to this optimization, an observation that is confirmed by an ex\u00ad amination \nof some HPF benchmark programs. Our frame\u00ad work also recognizes sit uations where the synchronize ation \n needs for multiple data transfers can be satisfied by a single synchronization message. This analysis, \nwhile applicable to all shared memory machines as well, is especially useful for those with a flexible \ncache-coherence protocol, as it identi\u00ad fies efficient ways of moving data directly from producers to \nconsumers, often without any extra synchronization.1 Introctuction Traditionally, parallel machines have \nbeen available in two distinct flavors shared memory and distributed memory, which have respectively \ngiven rise to the shared-memory and Permission to make digital/bard copies of all or part of this material \nfor personal or classroom use is granted without fee provided that the copies are not made or distributed \nfor profit or commercial advantage, the copy\u00adright notice, the title of the publication and its date \nappear, and notice is given that copyright is by permission of the ACM, hx. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires specific permission and/or fee. POPL 96, St. \nPetersburg FLA USA @ 1996 ACM 0-89791-769-3/95101. .$3.50 the message-passing programming models. The \ncomplemen\u00adtary nature of these architectures and programming models have led to many areas of convergence. \nWhile shared mem\u00adory provides the benefit of ease of programming, message\u00adpassing styles of writing parallel \nprograms usually lead to performance advantages [11, 3]. Languages like High Performance Fortran (HPF) \n[4] pro\u00advide the abstraction of a global address space on distributed memory machines, the program is \nmerely annotated with directives specifying the distribution of data across proces\u00adsors. The compilers \nfor these languages generate paral\u00adlel programs in single-program. rn.tdtiple-data (SPMD) form, and generate \nthe communication necessary to fetch values of non-local data referenced by each processor [9, 19, 15, \n10, 6]. The knowledge of data mapping and computation partition\u00ading at compile time offers many opportunities \nfor reducing synchronization costs. Tseng [18] and O Boyle and Bodin [13] have recently presented techniques \nthat exploit these op\u00adportunities on shared memory and distributed shared mem\u00adory (DSM) systems. However, \ntheir work only deals wit h recognizing the absence of synchronization requirements for statements with \nno communication and replacing barriers with cheaper producer-consumer synchronization. These optimizationsl \nwhile extremely useful, can be viewed as straightforward application of distributed memory compi\u00adlation \nstyle to shared memory machines, They do not elimi\u00adnate synchronization messagea from a computation that \nre\u00adquires interprocessor communication. This paper presents a novel methodology for reducing synchronization \ncosts of a compiler-generated data-parallel program. our work combines data flow analysis with com\u00admunication \nanalysis to establish an ordering between rele\u00advant events in the production and consumption of data \non different processors. That gives us the first compiler al\u00adgorithm that can eliminate synchronization \nmessages even from a computation that needs communication (our frame\u00adwork naturally eliminates synchronize \nation not associated with interprocessor communication by simply excluding it from consideration). For \ncomputations from which syn\u00adchronization messages cannot be completely eliminated, this work provides \na systematic framework for achieving synchro-use such a protocol, with no separate synchronization, for \nnization for multiple data transfers with a single or fewer synchronization messages. Our use of mapping \nfunctions to characterize the relationship between producers and con\u00adsumers enables the applicability \nof this framework to more general forms of computation than previous work. For in\u00adstance, O Boyle and \nBodin s approach [13] to replace bar\u00adrier synchronize ation with cheaper producer-consumer syn\u00adchronization \ncan only be applied to computations with con\u00adst ant synchronize ation directions. We show that several \ncom\u00admonly occurring computation patterns such as reductions and stencil computations with reciprocal \nproducer-consumer relationship between neighboring processors lend themselves well to the elimination \nof synchronization messages, as un\u00adavoidable data transfers themselves serve as synchronize ation mechanisms \nfor other data transfers. The precise informa\u00adtion about the ownership of data (that is available from \nHPF directives [4], but could also possibly be obtained from compiler analysis in preceding passes [5, \n1, 2]) and about computation partitioning makes this analysis more accurate than that possible with earlier \nwork on reducing synchro\u00adnisation [12, 17]. Our analysis is presented for distributed memory ma\u00adchines \nwhich support separate mechanisms for remote data access and synchronization, and is readily applicable \nto shared memory machines, which inherent ly support these mechanisms separately. Distributed memory \nmachines have traditionally support ed send and receive as the basic primi\u00adtives that provide the capability \nfor both interprocessor data transfer and producer-consumer synchronization. More re\u00adcently, many commercial \nmachines built with a distributed memory like Cray T3D, Meiko CS-2, and Fujitsu AP1OOO+ have begun to \nprovide primitives like get/put, which allow a processor to directly read data from or write data into \nthe remet e memory of another processor, with separate prim\u00aditives for synchronisation. Stricker et el. \n[16] and Hayashi et al. [7] have argued for separation of data transfer and synchronization, this paper \noffers more reasons in favor of such a separation. However, these researchers exclusively use barriers \nfor all synchronize at ion accompanying the data transfers. While these programs have fewer barriers \nto begin with (which are harder to remove) as compared to programs with fork-join parallelism that are \nSPMDized [18, 13], our work shows that even these barriers can often be completely eliminated or replaced \nby cheaper synchronization. Our work, while applicable to all shared memory ma\u00adchines, has a special \nsignificance for those machines, such as Stanford FLASH [8] and Wisconsin Typhoon [14], that have a flexible \ncache-coherence protocol. Falsafi et al. [3] have earlier reported results on improved performance of \ndifferent application programs with update protocols, where data is directly copied from producers to \nconsumers, rather than being read by consumers using a fixed coherence pro\u00ad tocol. Our work is the first \nto show how the compiler may many regular computations. The rest of this paper is organized as follows. \nSection describes an HPF compilation framework and the implica\u00adtions of using get or put as the basic \ncommunication primi\u00adtive instead of send-receive. Section 3 presents the analysis to detect situations \nwhere repeated communication patterns may be implemented using put operations without any extra synchronization \nmessage. Section 4 extends this analysis to more general pat terns, and presents additional techniques \nto reduce the cost of synchronization. Section 5 describes the results of a preliminary study on the \neffectiveness of our analysis for HPF programs. Finally, Section 6 presents conclusions. 2 HPF Compilation \nFramework We illustrate our ideas by discussing them in the context of pHPF [6], a prototype compiler \nfor HPF, that currently generates communication on IBM SP 1 and SP2 machines using send-receive as the \nbasic primitive. We describe how communication generation could instead be done, in a future version, \nusing get or put, which are being imple\u00admented in software on SP2 as part of a prototype run-time library. \nIn this work, we shall view collective communication primitives such as broadcast and reduction alzo \nin terms of the basic primitives, since they can be implemented using send-receive or get/put for data \ntransfer. During the translation of an HPF program to the SPMD form, pHPF uses the owner computes rule \n[9, 19], which as\u00adsigns a computation to the processor that owns the data being modified. In special \ncases like reductions, the compu\u00adtation is assigned to the owner of a reference other than the left hand \nside of the assignment statement. Given a right hand side reference corresponding to data not owned by \nthe processor executing the statement, the compiler generates communication to send this processor the \nnon-local values from the owner. pHPF performs optimizations like elimina\u00adtion of redundant communication \n[6] and message vectoriza\u00adtion to move communication outside the loops [9, 19]. The analysis presented \nin this work is performed after the com\u00adpiler has performed those optimizations and determined the placement \nof communication. Using Get /Put instead of Send-Receive We first note that using send-receive is inherently \nmore expensive than get or put for just transferring data, be\u00adcause its underlying protocol is more complex. \nThe imple\u00admentation of send-receive has to guarantee the ordering of messages sent from one processor \nto another and/or pro\u00advide mechanisms for matching sends and receives based on their tags. Furthermore, \nthe receiving processor has to buffer any incoming messages that arrive before the =. ce ive has been \nposted, leading to buffer-copying overheads, or it Producer Consumer Producer Consumer versions of overlap \nregions. Figure 1: Communication using get or put with producer\u00adconsumer synchronization has to engage \nin a hand-shake with the sender, leading to synchronization costs. However, in contrast to send-receive, \nwhich takes care of both data transfer and synchronization between sending and receiving processors, \na get or a put operation only re\u00adalizes the data transfer, and synchronization has to be done separately. \nA get operation is performed by the consumer there is synchronization needed to make sure that data \nhas been produced. Similarly, a put operation into a location that has previously been used to hold a \nvalue being con\u00adsumed requires synchronization so that the producer waits for that value to be consumed. \nFigure 1 illustrates a simple hand-shake protocol (omitting acknowledgement messages for simplicity) \nthrough which producer-consumer synchro\u00adnization can be naively realized together with a get or put. \nThe objective of our analysis is to eliminate the synchro\u00adnization message whenever possible. For regular \ncomputations where code generation is equally simple with get and put operations, we favor us\u00ading put \noperations for two reasons. The use of put leads to one fewer exchange of messages between the producer \nand the consumer, as shown in Figure 1. Secondly, while the synchronization message in case of get is \nneeded to satisfy the true dependence that data must reproduced before be\u00ading consumed, the synchronization \nmessage in case of put is needed for a storage-related dependence, and hence is eas\u00adier to eliminate. \nLet us consider a hypothetical case where the HPF compiler uses anew, statically allocated buffer to \nhold non-local values ontheconsumer foreachinstance ofa communication. There would be no need for a synchroniza\u00adtionmessage \nfrom theconsumer to the producer asit would always be legal for the producer to write into the buffer \non the consumer, there would be no problems of overwriting a value not used yet. However, it is not usually \npractical to use a new buffer for every instance of communication in the program. Apart from using up \nexcessive space, it could also lead to undesirable buffer-copying overhead. For example, for computations \nwith nearest-neighbor communications, it is desirable to use overlap regions [19] to hold non-local data \nfrom neighboring processors, both for performance and ease of code generation. Using a new buffer for \neach instance of nearest-neighbor communication would require giving up on the convenience of using overlap \nregions or incurring the cost of local memory-to-memory copies to create multiple Our analysis for eliminating \nsynchronization messages is explicitly presented only for put operations. On shared memory machines, \nthis directly corresponds to using the up\u00addate protocol [3], where data transfer is initiated by the \npro\u00adducer. A similar analysis can also be applied to get opera\u00adtions, which correspond to regular, consumer-initiated \nread operations on shared memory systems. However, in that case, synchronization messages cannot be completely \nelim\u00adinated, since the first instance of at least one data transfer needs a synchronization message from \nthe producer to the consumer. Our analysis can replace synchronization mes\u00adsages for multiple instances \nof data transfers and also those for multiple data transfers (with similar producer-consumer relationships) \nby a single synchronization message. Overall Procedure Following the communication analy\u00adsis which determines \nthe placement of communication, the next step is to apply the data flow procedure described in the next \nsection, to detect references for which no synchro\u00adnization message is required. For the remaining references \nthat need communication, the compiler examines introduc\u00ading producer-consumer synchronization, one at \na time. It checks if synchronization requirements of ot her references in that loop nest are met by the \nintroduced synchroniza\u00adtion, as described in Section 4. For a loop nest that needs more pro ducer-consumer \nsynchronization messages t han a threshold value (performance estimation to determine that threshold \nis beyond the scope of this paper), it chooses a single barrier synchronization instead. 3 Elimination \nof Synchronization In this section, we consider communication patterns which are identical for each repetition \nduring the program. We describe a data flow procedure which analyzes each such communication to check \nif the put operation used to imple\u00adment that communication is guaranteed to take place only after the \nvalue corresponding to the previous instance of that communication has been consumed. If so, each con\u00adsumer \ncan use the same buffer to hold non-local values dur\u00ading each instance of that communication, and there \nis still no synchronization message needed from the consumer to the producer. 3.1 Basic Idea We illustrate \na simplified version of this problem through Figure 2 in which a producer p repeatedly sends values cor\u00adresponding \nto a variable reference r to a consumer c, which are received by c at the same memory location. We refer \nto the successive versions of the value corresponding to r as 7J1,2J2, . . ..w.2 +1,1,.... We can statically \ninfer that the consumption of v; at c precedes the put operation at p for the next version Wi+l if c \nsends a message to p (as part of Figure 2: Precedence relationship between consumption and put communication \nfor some other reference r ) after consuming w, and If p realves this message before sending vi+l. The \ncommunication for ~ is said to kill the producer-consumer synchronization requirements of repetitions \nof communica\u00adtion for r. In actual practice, however, communication for a single reference may involve \nmultiple producers and/or consumers and varied relationships bet we en producers and consumers. Hence, \na compiler can draw inferences of the type described above only by a collective reasoning about communications \nfor each reference. We introduce mapping functions describ\u00ad ing producer-consumer relationships to enable \nsuch a rea\u00adsoning. We refer to the communication for a reference r as C,, and refer to the statement \nwith the computation using (consuming) the value of r as S,. For each communication C,, we define the \nfollowing functions: 1. Producers[C.] : The set of processors that send data. 2. Consumers[C.] : The \nset of processors that receive data. 3. ReceiveFrom[C,]( p,) : Given a processor p, the set of processors \nthat receive data from p.  We use the te~ms send and receive in the above def\u00adinitions merely to convey \nthe direction of data transfer, not to suggest that the communication primitives used are send and receive. \nGiven that an HPF compiler usually tries to move com\u00admunication out side 10ops, S? often appears at a \ndeeper nest\u00ading level than C.. We define CozusOver(C, ) (COT) as the statement before which communication \nC, must finish, and UseOver(Cr) (Uflr) as the statement after which there is no more reference to r corresponding \nto a given instance of C,. When communication Cr is placed at the same nesting level as s,, both COT \nand UO, are set to S,. However, when communication is hoisted outside loops to a nesting level 1, COV \nand UOV are respectively set to the beginning and the end of the loop at nesting level 1 + 1 surrounding \nstatement 57. For example, CO. is set to S1 and uO~ is set to Ss for the communication shown in Figure \n3. The following result forms the basis of our analysis. Theorem 1 Let z and y be two references with \ntheir corn\u00ad rnunicattons C. and CY being implemented as puts. Com\u00admunication CY kills the producer-consumer \nsynchronization requirements of identical repetitions of Cn if: 1. CY and COY always execute after UOZ \nand before the next repetition of C=, 2. Consumers[Cm] < Producers[CV], and 3. Vp c Producers[Cm], \nVq c ReceiveFro~C=](p), p ~ R.eceiveFrom[Cv] (q)  Proof : Consider an arbitrary processor p that performs \na put operation on a value v, as part of an arbitrary, ith in\u00adstance of C., referred to as C:. We show \nthat p cannot perform a put operation as part of C~+l until each receiver q has consumed w (and does \nnot refer to w any more) in its computation. Consider an arbitrary receiver q that receives data from \n p. By Condition 2, q E Producers[CV], i.e., q sends data under Cy. Furthermore, since q c ReceiveFrom[Cm] \n(p), by Condition 3, p c Recewel+om[CY](q), i.e., p receives data from q as part of CY. Condition 1 implies \nthat under CY, q sends data to p only after executing UO=, i.e., after consum\u00ading w, and p receives that \ndata at COY before participating in C:+ i. Since p cannot possibly receive data before it is sent by \nq} the consumption of vi at q takes place before par\u00adticipation of p in C~+l. o 3.2 Computation of Processor \nFunctions We first describe the analysis pertaining to the relationship between producers and consumers \nfor different communica\u00adtions, which forms the basis for testing the last two condi\u00adtions of Theorem \n1. In order to simplify the analysis, we deal with virtual processors, which correspond to template positions \nover which different array elements are aligned, as specified by HP F alignment directives. During the \nrest of this paper, we shall refer to HPF template positions as processors (we only consider the distributed \ndimensions of a template). An alignment function maps an array subscript i to a processor grid position \nc * i + o, where c and o are integers, and when c = O, 0 may take a special value *, which represents \nall processors along that grid dimension. We shall refer to this special value as U in this paper, to \navoid confusing it with the sign for multiplication. We now show how to determine the Producers, Con\u00adsumers, \nand ReceiveFrom functions for communication in\u00advolving array references in one dimension, where the arrays \nare mapped to a one-dimensional processor grid. We will later show the extension to the multi-dimensional \ncase. We assume that the subscripts in array references are constants or affine functions of loop indices. \nIn Figure 3, 13(a2 * i +&#38;) !HPF$ Align A(i) with VPROCS(CA * i + OA) ! HPF$ Align B(i) with VPROCS(CE \n* i + OB) ( communication for B ) SI do i= low, high S2 A(a,xi+~,)=B(a2*i+~2) S3 end do Figure 3: Example \nof regular communication in one dimen\u00adsion is sent to owner of A(al * i + 91). Hence, using the align\u00adment \ninformation, we infer that as part of this communi\u00adcation C,, VPROCS(CBctZ * i + cJ3,8Z + OB) sends a \nvalue to VPROCS(CACY1 x i + CAfll + OA). The sets of producers and consumers are obtained as triples \nby expanding these processor positions with respect to the loop bounds: p?d~cer~[c,] = (CJ3~L?bW + cj3~2 \n+ OB : cEcrzhigh + cl?/32 + OB : cJ3~2) con$~mer~[c.] = (cd~l low + c-@l + OA : cA~lh.igh + CA~l + OA \n: c-@l) If CB, az, CA, al all take non-zero values, the i-loop is said to be a producer-consumer traversal \nloop of the processor grid dimension for the given communication. If there is no producer-consumer traversal \nloop, ReceiveFrom[C,](p) be\u00adcomes a constant function that does not depend upon the value of p, and is \nobtained as Consumers[C.]. Otherwise, we obtain ReceiueFrom[Cr](p) by setting p = CBcf2 *~+cB82-koB and \nsubstituting for i in terms of p. Thus, we get: ReceiveFrom [C,](p) = [(cA@)/(cJ3~2)] *P + [cd~l + OA \n-(cBi% + oB)/(c~~2)] ifcB*@#() (C@,$:er+Cs;~, + OA : cA~,high + C@, + OA : C@,) { We can represent ReceiveFrom \nin the form of a coef\u00adficient C and a displacement D, such that it maps p to p = C x p + D. The displacement \nterm may be represented as a range (in the form of a triple), or may include the spe\u00adcial value U, which \nsignifies all processor positions. It is more convenient to represent the ReceiveFrom function as a single \ntransformation matrix, which is done by augmenting the representation of a k-dimensional processor grid \nwith an extra dummy element. Thus, for a l-dimensional processor grid, we use the vector ~, 1] to represent \nprocessor p, and get a 2 x 2 transformation matrix which maps ~, 1] to ~ , 1] as shown below: In general, \nfor a k-dimensional processor grid, we obtain the Receivel+om function as a (k + 1) x (k + 1) transformation \nmatrix. The algorithm for determining the transformation matrix RF representing ReceiveFrom is sketched \nin Figure 4. The entries in the last column of the transformation matrix correspond to the offset terms, \nrepresented as triples. Any loop index appearing in an offset term, which corresponds to a loop from \nwhich communication has been moved out, is expanded to its loop bounds. Compute-ReceiveFrom(l, ?, RF) \n/* inputs: rhs reference r, lhs reference 1, output: matrix RF */ { 1. Apply alignment function to each \nsubscript in distrib\u00aduted dimension to obtain information of the form VPROCS(pl, . . . . pk) sends data \nto VPROCS(p~, . . . ,pj), where pi and p: are expressions. 2. Identify a producer-consumer traversal \nloop index L<, if possible, for each dimension i of VPROCS. If there is more than one candidate for any \ndimension (due to the subscript being an affine function of more than one loop index), choose a loop \nso that over all k di\u00admensions, there are as many loops chosen as possible. 3. For each grid dimension \ni, if -L, is a valid loop index identified in the previous step, then pi would be ex\u00adpressible as p; \n= ~~ * L; + ti~. Regard pi as a variable now and L; as a function of p~ (swapping the roles of Pi and \nL), and obtain L = (pi ~i)/T< 4. For each grid dimension i, substitute any occurrence inp;ofLj,l<j<k,by(pj \n6~)/~J. Let thefinal expression for p; be of the form p: = t: xpl + + tj *  ~+1 The elements .RF(i, \nj), 1 < j < k + 1, are Pk+ti given by the terms t:, 1 < j < k + 1 in the expression for p:. The (k + \nl)th row of RF is given by RF(,k + l,j)=(),l <j<k, and RF(k+l, k+ 1)=1. 1 Figure 4: Algorithm to compute \nReceiveFrom function Example Consider the example shown in Figure 5. The communication for B involves \nVP ROCS(j, i + 1)sending data to VPROCS(i, U). We obtain the ReceiveFrom trans\u00ad formation matrix as: \n  01-1 RF . 00L4 001 () !HPF$ Align A(i, j) with VPROCS(i, *) ! HPF$ Align B(i, j) with VPROCS(i, j) \n( communication for B ) do j=l, n do i=l, n A(i, j) = B(jli + 1) end do end do Figure 5: Example of \ncommunication in multiple dimensions Check for Condition 2 of Theorem 1 Given two communications Cm and \nC v in a k-dimensional processor grid, let Consumers[Ce]~ and Producers[CV]{ represent the ith positions \nin the corresponding k-dimensional vectors. Let Corzsrnners[Cm]; be the triple (l: : u: : s;), and let \nProducers[CV]; be the triple (J~ : u; : s$). The triples are represented (if necessary, by a trivial \ntransformation) such that the strides s: and s: are positive. We can infer that ConsumeTs[Ca] ~ Producers[CV] \nif for each i in the range 1< i < k, we have (i) l; z 1;, (ii) u~ < u:, (iii) !HPF$ Distribute ~(block) \ndo i=l,rz s=g+ A(i) end do Figure 6: Example of reduction  (l: l;) mod ~ = O,and (iv) s: mod s~= O. \nCheck for Condition 3 of Theorem 1 Let RFZ and RFY represent the ReceiveFrom transformation matrices \nfor communications Cm and Cv respectively. We infer that Condition 3 of Theorem 1 is satisfied if the \nproduct ma\u00adtrix T = RFV * RFZ is a superset of the identity matrix 1. Given a k-dimensional processor \ngrid, we conclude that T ~ I (when applied to the producers of C= ) if for each i in the range 1 < i \n< k, we have one of the following conditions being satisfied: I. T(i, i) = 1; T(i, j)=O, l <j < k,j #i; \nand Oc T (i, k + 1),or 2. T(i, k+ 1) =U, or 3. T(i, j) = O, 1< j < k, and Producers[C~]i G T(i, k + \n1).  The above test ensures that for each processor p that plays the part of a producer in C=, the application \nof the mapping functions ReceiveFrom[C=] followed by ReceiueFrom[C3] maps it back to a set of processors \nthat includes p. Reductions Reduction operations across processors are handled in a special manner during \ncompilation, and have a special significance with respect to our analysis, Consider the example shown \nin Figure 6. In the SPMD program, the computations of partial sums are performed over the local portion \nof A on each processor, followed by a global reduc\u00adtion operation requiring communication, that takes \nplace after the loop. Let us view the sequence of operations in\u00advolving the global reduction as a communication \nfrom the participating processors to the root of the reduction fan-in tree (C=), followed by the reduction \ncomputation (UO. and CO. ), followed by a broadcast of the final value to all the processors (CY ), which \nin turn is followed by consumption synch~ppl y(SCINi, LOC, ) { SC.OUT, = SC.IN~ if ( Get Comm(LOCt) = \nr> then /* corm node */ if (SC_IN, [r] = U) then SC-OUT, [r] = CS else if (SC-IN; [T] = CS or SC_IN~[r] \n= Cons or SC_IN, [r] = NoSyncP) then SC. OIJT; [T] = Sync endif for k= 1 to ticozm do if (SC_IN%[k] = \nCons and repet it ions of Ch are identical and CT kills producer-consumes synchronization requirements \nof Ch by satisfying Conditions 2, 3 of I%eoxera 1] then SC.OUT< [k] = NoSyncP Add (k, r) to SC_ OUT~. \nTag endif end for else ~w computation node +/ if ( GetlJO(LOC, ) = r and SC-IN; [Tj = CS) then SC_OUT~ \n[r] = Cons endif if (GetCO(LOC~) = T) then for each k such that SC-INi. Tag includes (k, T) !.. if (SC-IN~[k] \n= NoSyncP) the; SC-OIJTi [k] = NoSync endif end for endif endif ret urn SC.OUTi } Figure 7: Pseudocode \nfor function SynchApply of that value (COY). If the global reduction operation over this group of processors \nis repeated in a loop, we observe that Cz and CV mutually kill the synchronization require\u00adments of repetitions \nof each other. Even though the actual implementation of the reduction primitive may deviate from this \nsimplified view, it can still be done so as to preserve the property that no synchronization messages \nare needed. 3.3 Data Flow Analysis We now describe the data flow procedure which deter\u00admines for each \ncommunication if its repetitions require any synchronization, given that the same buffer is used to hold \nnon-local values for each inst ante of that communication, and that the communication is implemented \nusing puts. Essentially, the data flow procedure provides a method for testing Condition 1 of Theorem \n1 for the relevant pairs of communications, while the remaining conditions are tested separately where \nneeded. The analysis is performed on the control flow graph represent at ion of the program, after the \nplacement of communication has been determined by the Sync u Figure 8: State transitions for communication \nC. SynchMeet u Cs No Sync NoSyrzcP Cons Sync u u C s NoSync NoSyncP Cons Sync c s Cs Cs No Sync NoSyncP \nCons Sync NoSync NoSync NoSync NoSync NoSyncP Cons Sync No SyncP NoSyncP NoSyncP NoSyncP NoSyncP Cons \nSync Cons Co 72s Cons Cons Cons Cons Sync Sync Sync Sync Sync Sync Sync Sync Table 1: The definition \nof S ynchkfee toleration over data flow values compiler. The augmented control flow graph has two kinds \nof nodes computation nodes and communication nodes, and edges that denote the flow of control. Each \nstatement in the source program has an associated computation node. Similarly, each communication for \na reference is represented by a single node. The only computation nodes that influence any data flow \nvalue are the join nodes in the CF G and. the nodes that correspond to UOT or CO? for some communication \nC,. We shall refer to the local information associated with a node i as LOC; . The functions Get UO(LOC,) \nand Get CO(LOC{) respectively return r when node i represents UO. or COT, and return null otherwise. \nThe function GetCornm(LOCi) returns r for a communication node i representing Cr, and returns null for \na computation node. The basic data flow variables computed by our analysis are the following: SC-.lN, \n: synchronization requirements of communications, as inferred at entry to node i. SC.OUT, : synchronization \nrequirements of communica\u00adtions, as inferred at exit from node i. For the purpose of data flow analysis, \nthe information on synchronization requirements of communications is main\u00adtained as a vector of a 6-valued \nenumeration type, with an entry for each reference that needs communication. The six possible values \nare: U (communication not seen yet), CS (communication seen), Cons (consumption of communi\u00adcated value \nover), NoSyncP (preliminary indication that no synchronization message is needed for repetitions), NoSgnc \n(confirmation that no synchronization message is needed for repetitions), and Sync (synchronization message \nneeded for repetitions). Associated with each reference, there is also a pointer to the relevant information \nabout its communica\u00adtion. For the entry node, SC-IN is initialized to all U s. The value of LOC for each \nnode is set by the compiler based on the placement of communication. The following data flow equations \nare applied in a traversal along the direction of edges. SC-OUT, = SynchApply(SC-IN~, LOC,) (1) SC_IN, \n= SynchMeetPcP,ed(~l (SC-O UT P) (2) The function SynchApply(SCIN,, LOC~ ) is described in Figure 7. \nThe transitions effected by this function are pictorially shown in Figure 8. At each join node, the function \nSynchMeet is applied elementwise over all the SC-OUT values of its predecessors. Table 1 defines the \nSynchMeet operation at the element level. FoIlow\u00ading the SynchMeet operation, a data flow value can only \nmove lower in the semi-lattice consisting of the values Sync (1), Cons, NoSyncP, NoSync, CS, and U (T). \nOur data flow procedure handles loops by applying the Equations 1 and 2 iteratively over the loop body \nuntil the solution converges. We can show that the solution will always converge within three iterations, \nwhich makes our data flow procedure quite efficient. Let us consider the data flow value with respect \nto communication Cm and a loop L. There are two cases: Case 1 : Cm appears inside the loop L. Let C, \ncorrespond to node i. At the end of the first iteration, SC.OUT, [r] can possibly take only a value other \nthan U. Hence, dur\u00ading the second iteration, along the back-edge of the loop, SC-lN; [r] cannot take \nthe value U. If SC.lN~[r] has been set to CS, NoSyncP, or Sync, then SC_OUTi[r] gets the value Sync, \nwhich cannot change any further. If SC_IN; [r] had the value NoSync, either it does not change during \nthe next and future iterations, or it changes to CS, NoSyncP, or Sync, in which case SC_ O?YT~[r] gets \nthe value Sync dur\u00ading the third iteration, which does not change during future iterations as well. Case \n2: C, does not appear inside the loop -L. Due to the nature of communication code generation, UO~ also \ncannot appear inside L. Hence, the only transitions possible in the data flow value are due to some \nCY or COY, which respec\u00adtively change the value from Cons to NoSyncP or NoSyncP to NoSync. Since these \ntransitions can happen only if the value on entry to Cy is Cons, and the value on entry to COY is NoSyncP, \nany change in these values at entry to those respective nodes stops further transitions. Finally, after \nthe data flow solution is obtained, the value of SC.OUTt [~] at exit from the node i containing CT can \nonly be CS (if there is no repetition possible of C, in the procedure), NoSync, or Sync. A value of CS \nor NoSync implies that no synchronization message is needed for C,, if implemented using put operation(s) \ninto a fixed buffer. In this work, we have assumed a static allocation of buffers holding non-local data. \nIf the compiler uses dynamic alloca\u00adtion instead, there would be a need for synchronization from the \nconsumers to the producers after the allocation of the buffer. In that case, a producer would specify \nthe remote address on consumer using indirect addressing mode in the put operation. 4 Extensions and \nAdditional Techniques We first describe extensions to our analysis that allow syn\u00adchronization messages \nassociated with put operations to be eliminated in more general conditions than those covered in Section \n3. We then describe additional techniques that help reduce the cost of synchronization messages, when \nthey cannot be eliminated. Different Producers The procedure described in the previous section eliminates \nsynchronization messages only for repetitions of identical communication patterns associ\u00adated with a \nreference. We now show how to deal with differ\u00adent instances of communication that involve different \npro\u00adducers. In the absence of dat a redistributions, this situation arises in the context of communication \nfor an array refer\u00adence with a subscript in a distributed dimension that is not invariant with respect \nto the repetitions of communication. For example, if in Figure 5 communication for B were to be placed \ninside the j-loop, different instances of that com\u00admunication would involve different producers. Given \nsuch a communication C=, we define the following function: NextProdticers[C~] (p) : Given a processor \np which produces a value for the current instance of C=, the set of processors which may produce the \nvalue for the next instance of C=. For the modified example in Figure 5 with communica\u00adtion for B placed \ninside the j-loop, we would get the trans\u00adformation matrix representing NextProducers function as:  \n101 NP =010 () 001 Let us consider references C* and Cv as defined in Theo\u00adrem 1, with the generalization \nthat different instances of C. may have different producers. The result stated in Theo\u00adrem 1 can be generalized \nto the following: Theorem 2 C ornmunication CY kdls the producer\u00ad consumer synchronization requirements \nof repetitions of dif\u00adferent instance. of C= if: 1.CY and COY always execute after UO. and before the \nnext repetition of C=, 2. Consumers[Cc] ~ Producers[CV], and 3. Vp E Producers[C.], Vq ~ R ecezveF rom[C. \n](p), NeztProducers[C. ](p) S ReceiweFrom[Cy](q)   Proof : Omitted, similar to the proof of Theorem \n1. The first two conditions are identical to those of The\u00adorem 1. Condition 3 can be verified by checking \nif RFY * RF. ~ NP., where NP. is the transformation matrix rep\u00adresenting NextProducers[C. ], and the \ntest for the superset relationship is done in the same manner as defined in Sec\u00adtion 3.2. When NextProducers[Cv] \nis an identity function, the above result reduces to Theorem 1. The NeztProducers function can usually \nbe specified in a simple form only for a single loop outside the point where communication is placed. \nBeyond that, it can always be conservatively estimated as mapping p to U, which signiiies all processors, \nHowever, if elimination of synchronize at ion is possible only with the pre\u00adcise information for NextProducers, \nsynchronization can at least be moved outside the immediately surrounding loop in which communication \nis repeated. Killing Synchronization Requirement with Multiple References Instead of a single reference \ny, let us consider a sequence of references yl, ~z,....y~,such that communi\u00adcation for y; starts after \ncommunication for y~-1, and com\u00adpletes before communication for y~+l, 1 < i < n. Using the above not \nation, we obtain the following generalization to Theorem 2: Theorem 3 The communication for a sequence \nof ref\u00aderences VI . . . . . Y=, as specified above, kdls the producer\u00adconsumer synchromzation requirements \nof repetitions of dif\u00adferent instances of C. zf: 1. CY= and S,, always execute after uO. and befove the \nnext repetition of C., l<i <n, ,2. Comwners[C.] ~ Producers[CY, ]; Consumer s[CY,] ~ Producers[Cy,+l], \n1 s i < n, and 3. RFY. * RFYn_, ~...*RFYlwRF=~NPz Using Distribution information for Templates For simplicity, \nour analysis is performed at the level of virtual processors, which correspond to template positions \nin HPF, and therefore does not take advantage of information about different virtual processors being \nmapped to the same physi\u00adcal processor. The R.eceweFrom and the NeztProducersmap\u00adpings require a significantly \nmore complex representation in the space of physical processors. However, it is relatively easy to perform \nthe test for Condition 2 of Theorems 1, 2 and 3 by computing the set of producers and consumers in physical \nprocessor space, when the number of processors is known statically. In fact, this computation is any \nway done by the pHPF compiler during code generation. Synchronizing Multiple Communications with a Sin\u00adgle \ni%lessage When the synchronization message for a given communication CT cannot be eliminated, the compiler \nneeds to generate synchronization from the consumer to the producer. Viewing this synchronization as \na new communi\u00ad cation CY 2 and treating Cr as the statement (COY in our notation) which executes only \nafter communication CV has completed, we can now apply our analysis to see if Cv kills the synchronization \nrequirements of repetitions of any other communication Cz. Clearly, this technique will succeed in cases \nwhere the message aggregation optimization [9] (com\u00adbining communication for multiple references with \nthe same communication pattern) is applicable. This approach can also be applied to other forms of syn\u00adchronization, \nsuch as barriers, that will service multiple com\u00admunications. For a barrier synchronization, each of \nthe func\u00adtions, Producers, Consumers, and lleceiveFrom(p) eval\u00aduates to the set of all processors. Thus, \na barrier out\u00adside a parallel 10 op covers the synchronization requirements of all references inside \nthat loop, as their communications are moved outside the loop by message vectorization. Un\u00adlike a program \nwith fork-join parallelism, no synchroniza\u00adtion is needed after the loop if computations following the \nloop body use put operations into separate buffers for inter\u00adprocessor data transfer. Using Extra Storage \nAs mentioned earlier, the com\u00ad piler can always use extra storage in conjunction with put to eliminate \nthe need for synchronization from the consumer(s) to the producer(s). In general, the compiler has the \nflexi\u00adbility of trading off memory with synchronization costs, and choosing the appropriate loop level \nupto which a buffer used to hold non-local values would be expanded. Unfortunately, The processor functions \nfor C, are computed in exactly the same manner as those for C. except that the roles of the rhs and the \nlhs references am reversed. do j =l, n,s ( cosusunication for A ) do j = j , min(j +s l,n) do i=l, n \n. = A(i, j) end do end do end do Figure 9: Loop stripmining to control message size as we noted in Section \n2, using this technique can lead to buffer-copying overhead, when different instances of an array reference \nin the program correspond to both local and non\u00adlocal data. For computations with nearest-neighbor com\u00admunication, \nit is usually better to use fixed overlap regions [19] to hold non-local data. Managing Storage with \nLoop Stripmining A well\u00adknown technique employed by compilers to control the size of messages is the \nstripmining transformation illustrated in Figure 9. This example shows the j-loop split into two levels \nand communication placed between those loops even though it could be legally moved beyond the outer 10op. \nIf syn\u00adchronization of producers and consumers is needed for this communication, the compiler can reduce \nthe cost by mov\u00ading synchronization outside the j loop. The initial syn. chronization is needed from \nthe producers to the consumers indicating that data is ready, and the actual data transfer can be realized \nusing gets inside the j -loop without any further synchronization. In fact, even the initial synchro\u00adnization \ncan be avoided by using put for the data transfer (into a statically allocated buffer) during the first \niteration of j -loop and using get during subsequent iterations. 5 Preliminary Experiments In order to \nexamine the applicability of the analysis pre\u00adsented in this paper to HPF programs, we conducted a study \nwith two programs representing different kinds of regular computations. We started with the information \non place\u00adment of communication generated by the pHPF compiler, and applied the steps of the data flow \nanalysis procedure by hand. Tred2 The program tred2 is from the EISPACK library. It reduces a real symmetric \nmatrix to a symmetric tridi\u00ad agonal matrix, and has a number of sum reductions in its computation. We \nadded HPF directives to align the 1-D ar\u00adrays in the program with the rows of the 2-D arrays and to distribute \nthose arrays by rows [5]. Figure 10 shows a repre\u00adsentative segment from the program. We have marked \nthe references which require communication m bold letters. The communications for the three references \nin the first j-loop are vectorized and moved before the j-loop. The synchro\u00ad do ii=2, n ... doj=l, l \nF = D(j) G = E(j)+ Z(jlj) * F ... end do ... doj=l, l E(j) = E(j)/H F = F + E(j)* D(j) end do ... end \ndo Figure 10: Program segment from tred2 !HPF$ Distribute grid(block, block,*) do it= l,ncycle3 doj=2, \nn+l doi=2, n+l gf-id(i, j, bzew)= ezp((alog(grd(i, j-l,i oki))+ alog(gd(i, j, iokZ))+ alog(grid(i, j \n+ 1, LA$)+ alog(grid(i 1,j 1,!oM))+ alog(grid(i 1, j, iold))+ alog(grid(i 1, j + 1, zold))+ alog(grid(i \n+ 1, j 1,io@)-l\u00adalog(grid(i + 1,j, ZCJM))+ alog(grid(i + 1, j + 1, ioM)))/9.0) end do end do ... end \ndo Figure 11: Program segment from grid nization requirements for their repetitions inside the ii-loop \nare killed by the communication due to the global reduction taking place after the next jloop. The reference \nto E(j)in the statement with reduction represents the array reference whose ownership determines the \nassignment of computation for the partial sum. For the entire program, thepHPF compiler identifies 19 \nreferences to array and scalar variables as requiring commu\u00adnication. Our analysis identifies 10of those \ncommunications that are repeated inside hoops (with potentia~y different producers) as not requiring \nany synchronization message. The synchronization requirements are killed by references involved in the \nsum reductions. Another 2 of the remain\u00ading communications are executed only once. Therefore, only 36. \n8% oft he 19 references with communication require syn\u00adchronization messages after this analysis. Grid \nThe grid program is taken from the benchmark suite developed by Applied Parallel Research, Inc. This \nprogram performs a 9-point stencil computation followed by global reductions, and is representative \nof computations with nearest-neighbor communication. The kernel of the computation iz shown in Figure \n11. The i and the j loops are marked as parallel using the HPF independent directive. In this kernel, \npHPF identifies 4 references as needing commu\u00adnication (communication for 4 other references is eliminated \nby the optimization for message coalescing and eliminating redundant communication [6]). Our analysis \nis unable to eliminate the synchronize ation need for any of these refer\u00adences. However, we observed \nthat the analysis would work much better if the program is rewritten by unrolling the outermost loop \nonce to make it clear that the variables iold, itzew alternatively take the values of [1,2] and [2,1] \nin differ\u00adent iterations of the outermost loop. In the resulting pro\u00adgram, our analysis would eliminate \nsynchronization message for every reference in this program segment. For example, the references to A(i \n 1, j 1, 1)in the first i, j-loop nest and to i4(i+ 1, j+ 1, 2) in the second i, j-loop nest would mu\u00adtually \nkill the synchronization requirements of each other s communication. This study suggests that it may \nbe possible to elimi\u00adnate synchronization messages to a considerable extent by using put operations for \ndata transfer, and using the analy\u00adsis presented in this paper. Other researchers [18, 13] have shown \nimpressive improvements in program performance with smaller reductions of synchronization than we are \nable to accomplish. Intuitivelyl our analysis is likely to work well on regular applications which are \nloosely synchronous or have reciprocal producer-consumer relationships of data in their basic computations. \nHowever, some cases may require intervention from the programmer or sophisticated capabil\u00ad ities for \nprogram analysis in the underlying compiler and transformations like loop-unrolling to enable our technique \nto work effectively. 6 Conclusions We have presented a framework, based on data flow analy\u00adsis and communication \nanalysis, for reducing synchroniza\u00adtion costs of programs compiled for SPMD execution. This framework \nis more powerful than any previously presented work. It can eliminate synchronize ation messages even \nfor comput at ions that need communication. A preliminary ex\u00adamination of some HPF benchmark programs \nconfirms that this optimization works extremely well with commonly oc\u00adcurring computation patterns like \nreductions and stencil computations. Our framework also recognizes situations where the synchronization \nneeds for multiple data transfers can be satisfied by a single synchronize ation message. We ex\u00adpect \nthe kind of analysis presented in this paper to play an increasingly important role in future high-performance \ncom\u00adpilers for parallel machines, as synchronization costs become more dominant with improvements in \nprocessor speeds. We plan to implement these ideas in the pHPF compiler and conduct a more detailed study \nof the performance bene\u00adfits of using this analysis. In the future, we will also investi\u00adgate integrating \nthis analysis with program transformations like limited loop-unrolling and with performance estimation \nfor optimal insertion of synchronization. Acknowledgements We would like to thank Bob Cypher, M. Raghunath, \nand Marc Snir for valuable discussions on the use of get and put primitives. References [1]S. P. Amarasinghe \nand M. S. Lam. Communication optimization and code generation for distributed mem\u00adory machines. In Proc. \nA CM SIGPLA N 93 GO nf erence on Programming Language Deszgn and Implementation$ Albuquerque, New Mexico, \nJune 1993. [2] S. Chatterjeel J. R. Gilbert, R. Schreiber, and S.-H. Teng. Automatic array alignment \nin data-parallel pro\u00adgrams. In PFOC. Twentieth Annual A CM Symposium on Principles of Programming Languages, \nCharleston, SC, January 1993. [3] B. Falsafi, A. R. Lebeck, S. K. Reinhardt, I. Schoinas, M. D. Hill, \nJ. R. Larus, A. Rogers, and D. A. Wood. Application-specific protocols for user-level shared memory. \nIn Proc. Supercomputirzg 94, Wash\u00adington D. C., November 1994. [4] High Performance Fortran Forum. High \nPerformance Fortran language specification, version 1.0. Technical Report CRPC-TR92225, Rice University, \nMay 1993. [5] M. Gupta and P. 13anerjee. Demonstration of automatic data partitioning techniques for \nparallelizing compilers on multicomputers. IEEE Transactions on Parallel and Distributed Systems, 3(2):179-193, \nMarch 1992. [6] M. Gupta, S. Midkiff, E. Schonberg, V. Seshadri, K.Y. Wang, D. Shields, W.-M. Ching, \nand T. Ngo. An HPF compiler for the IBM SP2. In Proc. Supercomputmg 95, San Diego, CA, December 1995. \n[7] K. Hayashi, T. Doi, T. Horie, Y. Koyanagi, O. Shiraki, N. Imamura, T. Shimizu, H. Ishihata, and T. \nShindo. AP1OOO+: Architectural support of put/get interface for parallehzing compiler. In Proc. 6th International \nConference on Architectural Support for Programming Languages and Operating Systemsj San Jose, CA, Oc\u00adtober \n1994. [8] J. Heinlein, K. Gharachorloo, S. Dresser, and A. Gupta. Integration of message passing in the \nStanford FLASH multiprocessor. In Proc. 6th International Conference on Architectural Support for Programmmg \nLanguages and Operating Systems, San Jose, CA, October 1994. [9] S. Hiranandanil K. Kennedy, and C. Tseng. \nCom\u00adpiling Fortran D for MIMD distributed-memory ma\u00adchines. Communications of the ACM, 35(8):66 80, Au\u00adgust \n1992. [10] C. Koelbel and P. Mehrotra. Compiling global name\u00ad space parallel loop. for distributed execution. \nIEEE Tramactzons on Parallel and D~strzbuted Systems, 2(4):440-451, October 1991. [11] D. Kranz, K. Johnson, \nA. Agarwal, J. Kubiatowicz, and B.-H. Lim. Integrating message passing and shared\u00admemory: Early experience. \nIn Proc. lth ACM SIG-PLAN Symposium on Principles and Practices of Par\u00adallel Programming, May 1993. [12] \nS. Midkiff and D. Padua. Compiler generated synchro\u00adnization for do loops. IEEE Transactions on Comput\u00aders, \n36:1485 1495, December 1987. [13] M. O Boyle and F. Bodin. Compiler reduction of syn\u00adchronization in \nshared virtual memory systems. In Proc. 9th ACM International Conference on Supercom\u00adputing, Barcelona, \nSpain, July 1995. [14] S. K. Reinhardt, J. R. Larus, and D. A. Wood. Tem\u00ad pest and Typhoon: User-level \nshared memory. In Proc. 21st International Symposium on Computer A~chitec\u00adture, April 1994. [15] A. Rogers \nand K. Pingali. Process decomposition through locality of reference. In Proc. SIGPLAN 89 Conference on \nProgrammmg Language Design and Im\u00adplementation, pages 69-80, June 1989. [16] T. Stricker, J. Stichnoth, \nD. O. Hallaron, S. Hinrichs, and T. Gross. Decoupling synchronization and data transfer in message passing \nsystems of parallel comput\u00aders. In Proc. 9th ACM International Conference on Supercomputmg, Barcelona, \nSpain, July 1995. [17] P. Tang, P. Yew, and C. Zhu. Compiler techniques for data synchronization in nested \nparallel loops. In Proc. 1990 ACM International Conference on Supercomput\u00ading, Amsterdam, The Netherlands, \nJune 1990. [18] C.-W. Tseng. Compiler optimization for eliminating barrier synchronization. In Proc. \n5th ACM Symposium on Principles and Practices of Parallel Progranzmmg, Santa Barbara, CA, July 1995. \n[19] H. Zima, H. Bast, and M. Gerndt. SUPERB: A tool for semi-automatic MIMD/SIMD parallelization. Parallel \nComputing, 6:1-18, 1988. \n\t\t\t", "proc_id": "237721", "abstract": "", "authors": [{"name": "Manish Gupta", "author_profile_id": "81100021061", "affiliation": "IBM T.J. Watson Research Center, P.O. Box 704, Yorktown Heights, NY", "person_id": "PP42053581", "email_address": "", "orcid_id": ""}, {"name": "Edith Schonberg", "author_profile_id": "81100198217", "affiliation": "IBM T.J. Watson Research Center, P.O. Box 704, Yorktown Heights, NY", "person_id": "P74764", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/237721.237799", "year": "1996", "article_id": "237799", "conference": "POPL", "title": "Static analysis to reduce synchronization costs in data-parallel programs", "url": "http://dl.acm.org/citation.cfm?id=237799"}