{"article_publication_date": "01-01-1996", "fulltext": "\n Discovering Auxiliary Information for Incremental Computation Yanhong A. Liu* Scott D. Stollert Tim \nTeitelbaum* Department of Computer Science, Cornell University, Ithaca, New York 14853 {yanhong, stoller,tt}@cs. \ncornell. edu Abstract This paper presents program analyses and transformations that discover a general \nclass of auxiliary information for any increment al computation problem. Combining these tech\u00adniques \nwith previous techniques for caching intermediate re\u00adsults, we obtain a systematic approach that transforms \nnon\u00adincremental programs into efficient incremental programs that use and maintain useful auxiliary information \nas well as useful intermediate e results. The use of auxiliary informa\u00adtion allows us to achieve a greater \ndegree of incrementality than otherwise possible. Applications of the approach in\u00adclude strength reduction \nin optimizing compilers and finite differencing in transformational programming. Introduction Importance \nof incremental computation. In essence, ever y program computes by fixed-point it erat ion, expressed \nas recursive functions or loops. This is why loop optimizations are so important. A loop body can be \nre\u00adgarded as a program ~ parameterized by an induction vari\u00adable z that is incremented on each iteration \nby a change operation @. Efficient iterative computation relies on effec\u00adtive use of state, i.e., computing \nthe result of each itera\u00adtion using stored results of previous iterations. This is why strength reduction \n[2] and related techniques [48] are crucial for performance. Given a program .f and an input change operation \n@, a program j that computes .f(z @ y) efficiently by using the result of the previous computation of \n~(z) is called an incremental version of ~ under @. Sometimes, information other than the result of ~(z) \nneeds to be maintained and used for efficient increment al computation of f(z @ y). We call a function \nthat computes such information an eztended version of ~. Thus, the goal of computing loops efficiently \n*The author gratefully acknowledges the support of the Office of Naval Research under contract No. NOO014-92-J-1973. \ntsupported In part by NSF/DARPA Grant No. CCR9014363, NASA/DARPA Grant NAG-2-893, and AFOSR Grant F49620-94-l\u00ad019S, \nAny opinions, findings, and conclusions or recommendations expressed in this publication are those of \nthe authors and do not reflect the views of these agencies. Permission to make digitalhard copies of \nall or part of tMs material for personal or classroom use is granted without fee provided that the copies \nare not made or dktnbuted for profit or co~ercial advantage, the copy\u00ad right notice, the title of the \npublication and Its date appear, and notice is given tha~ copyright is by pertnkgion of .ke. ACM, @. \nTO CO,PY0*.HL-, to repubkh, to post on servers or to redwtrtbute to Ma, requmx specific permission and/or \nfee. POPL 96, St. Petersburg FLA USA @1996 ACM (J_89791_769_3/95 /ol ..$3 .5(3 corresponds to constructing \nan extended version of a pro\u00adgram f and deriving an incremental version of the extended version under \nan input change operation @. In general, incremental computation aims to solve a prob\u00adlem on a sequence \nof inputs that differ only slightly from one another, making use of the previously computed out\u00adput in \ncomputing a new output, instead of computing the new output from scratch. Incremental computation is \na fundamental issue relevant throughout computer software, e.g., optimizing compilers [1, 2, 15, 20, \n60], transforma\u00adtional program development [7, 17, 47, 49, 59], and inter\u00adactive systems [4, 5, 9, 19, \n27, 33, 53, 54]. Numerous tech\u00adniques for increment al computation have been developed, e.g., [2, 3, \n22, 28, 29, 30, 41, 48, 51, 52, 55, 58, 61, 64]. Deriving incremental programs. We are enga\u00adged in an \nambitious effort to derive incremental extended programs automatically (or semi-automatically) from non\u00adincremental \nprograms written in standard programming lan\u00adguages. This approach contrasts with many other approaches \nthat aim to evaluate non-incremental programs incremen\u00ad tally. We have partitioned the problem (thus \nfar) into three subproblems: c PI. Exploiting the result, i.e., the return value, of f(z). P2. Caching, \nmaintaining, and exploiting intermediate e results of the computation j(z).  P3. Discovering, computing, \nmaintaining, and exploit\u00ading auxdiary information about x, i.e., information not computed by ~(z).  \nOur current approaches to problems P1 and P2 are described in [41] and [40], respectively. In thk paper, \nwe address issue P3 for the first time and contribute: A novel proposal for finding auxiliary information. \n A comprehensive methodology for deriving incremen\u00adtal programs that addresses all three subproblems. \n Some approaches to incremental computation have exploited specific kinds of auxiliary information, \ne.g., auxiliary arith\u00admetic associated with some classical strength-reduction rules [2], dynamic mappings \nmaintained by finite differencing rules for aggregate primitives in SETL [48] and INC [64], and auxiliary \ndata structures for problems with certain prop\u00aderties like stable decomposition [52]. However, until \nnow, systematic discovery of auxiliary information for arbitrary programs has been a subject completely \nopen for study. Auxiliary reformation is, by definition, useful informa\u00adtion about z that is not computed \nby f(z) Where, then, can one find it? The key insight of our proposal is: A. Consider, as candidate auxiliary \ninformation for ~, all intermediate computations of an in\u00adcremental version of ~ that depend only on \nz; such an incremental version can be obtained us\u00ading some techniques we developed for solving PI and \nP2,1 How can one discover which pieces of candidate auxiliary information are useful and how they can \nbe used? We pro\u00adpose: B. Extend ~ with all candidate auxiliary infor\u00admation, then apply some techniques \nused in our methods for PI and P2 to obtain an extended version and an incremental extended version that \ntogether compute, exploit, and maintain only useful intermediate results and useful auxiliary information. \nThus, on the one hand, one can regard the method for P3 in this paper as an extension to methods for \nPI and P2. On the other hand, one can regard methods for PI and P2 (suit ably revised for their different \napplications here) as aids for solving P3. The modular components complement one another to form a comprehensive \nprincipled approach for in\u00adcrement al computation and therefore also for efficient iter\u00adative computation \ngenerally. Although the entire approach seems complex, each module or step is simple. We summarize here \nthe essence of our methods: P 1. In [41], we gave a systematic transformational ap\u00adproach for deriving \nan incremental version f of a program ~ under an input change @. The basic idea is to identify in the \ncomputation of ~(z @ y) those subcomputations that are also performed in the computation of f(z) and \nwhose values can be retrieved from the cached result r of ~(z). The com\u00adputation of ~(z @ y) is symbolically \ntransformed to avoid re\u00adperforming these subcomputations by replacing them with corresponding retrievals. \nThis efficient way of computing f(z @ y) is captured in the definition of f (z, y, r). P2. In [40], we \ngave a method, called cache-and-prune, for statically transforming programs to cache all intermedi\u00adate \nresults useful for incremental computation. The basic idea 1s to (I) extend the program ~ to a program \n~ that returns all intermediate results, (II) incrementalize the pro\u00adgram ~ under @ to obtain an incremental \nversion F of j using our method for PI, and (111) analyze the dependencies in F, then prune the extended \nprogram ~ to a program $ that returm only the useful intermediate remlta, and prune the program fl to \nobtain a program ~ that incrementally maintains only the useful intermediate results. P3. This paper \npresents a two-phase method that discov\u00aders a general class of auxiliary information for any incremen\u00adtal \ncomputation problem. The two phases correspond to A 1 We use techniques developed for solving P1 and \nP2, instead of just Pl, so that the candidate auxdlary mformatlon Includes auxlhary information useful \nfor efficiently mamtammg the intermediate results and B above. For Phase A, we have developed an embedding \nanalysls that helps avoid including redundant information in an extended version, and we have exploited \na forward depen\u00addence analysis that helps identify candidate auxiliary infor\u00admation. Alf the program \nanalyses and transformations used in this method are combined with considerations for caching intermediate \nresults, so we obtain incremental extended pro\u00adgrams that exploit and maintain intermediate results as \nwell as auxdiary information. We illustrate our approach by applying it to problems in list processing, \nVLSI design, and graph algorithms. The rest of this paper is organized as follows. Section formulates \nthe problem. Section 3 discusses discovering can\u00addidate auxiliary information. Section 4 describes how \ncandi\u00ad date auxiliary information is used. Two examples are given in Section 6. Finally, we discuss related \nwork and conclude in Section 7. 2 Formulating the problem We use a simple first-order functional programming \nlan\u00adguage, with expressions given by the following grammar: e .:=7J variable I c(el,..., en) constructor \napplication I peel,...) en) primitive function application I ~(el,..., en) function application I if \nel then ez else e, conditional expression I let. =e~inez binding expression A program is a set F of mutually \nrecursive function defini\u00adtions of the form ~(vl,,.., vn)=e (1) and a function fn that is to be evaluated \nwith some inrmt z = (m, . . . . z~)~ -Figure 1 gives some example definiti~ns. The semantics of the language \nis strict. An input change operation @ to a function fO combines an old input z = (zl, . . ..x~) and \na change y = (ylj..., y~) to form a new input z = (zj, . . ..z~) = z @y, where each z; is some function \nof Xj s and y~ s. For example, an input change operation to the function cmp of Figure 1 may be defined \nby z = z @ y = cons(y, z). We use an asymptotic cost model for measuring time complexity and write t(f(vlw)) \nto the asymp\u00ad ,..,denote totic time of computing ~(vl, . . . . vn). Thus, assuming al primitive functions \ntake constant time, it is sufficient to con\u00adsider only the values of function applications as candidate \ninformation to cache. Of course, maintaining extra informa\u00adtion takes extra space. Our primary goal is \nto improve the asymptotic running time of the incremental computation. We attempt to save space by maintaining \nonly information useful for achieving this. Given a program jO and an input change operation 6), we use \nthe approach in [41] to derive an incremental version f~ of fO under @, such that, If fO (z) = r, then \nwhenever fo(~ @Y) returns a value, f[(z, y, r) returns the same value and is asymptotically at least \nas fast.2 For example, for the function sum of Figure 1 and input change operation x @ Y = COns(y, z), \nthe function sum in Figure 2 is derived. 2Whde jo(c) abbreviates fO(LCI, . . . . c~), and fo(zoy),abbrevlates \n.fo((~l, ! an) @ (Y], , v~)), f~(z, Y,r) abbreviates fO(ZI, ,Zr,, Y]> , Y-, T) Note that some of the \nparameters of .f~ may be dead and ehmmated [41] crop(z) = .wm(odd(x)) <prod(even(z)) compare sum of \nodd and product of even positions of list z o d d (z) = if null(z) then nd else con.s(car(z), eoen(cdr(z))) \neven(z) = if null(z) then nil else odd(cdr(z)) Figure 1: Example In order to use also intermediate results \nof .fo (z) to com\u00adpute ~o(z @ y) possibly faster, we use the approach in [40] to cache useful intermediate \nresults of fO and obtain a program that incrementally computes the return value and maintains these intermediate \nresults. For example, for the function cmp of Figure I and input change operation x @ (Y1, Y2 ) = cons(vI, \ncons(yz, z)), the intermediate results surn@d(z)) and prod(ewen(z)) are cached, and the functions cmp \nand C=P in Figure 2 are obtained. However, auxiliary information other than the intermedi\u00adate results \nof jO (x) is sometimes needed to compute ~0 (z @y) quickly. For example, for the function cmp of Figure \n1 and input change operation z @ y = cons (y, z), the val\u00adues of sum(eoen(z)) and prod(odd(z)) are crucial \nfor com\u00adputing cmp(cons(y, z)) incrementally but are not computed in crop(z). Using the method in this \npaper, we can de\u00ad rive the functions c=p and C=P in Figure 2 that compute these pieces of auxiliary information, \nuse them in computing cmp(cons(y, x)), and maintain them as well. c~p computes incrementally using only \n0(1) time. We use this example as a running example. Notation. We use <> to construct tuples that bundle \nintermediate results and auxiliary information with the orig\u00adinal return value of a function. The selector \nnth returns the nth element of such a tuple. We use z to denote the previous input to jO; r, the cached \nresult of .fo (x); y, the input change parameter; z , the new input z @ y; and fj, an incremental version \nof fo under @. We let ~. return all intermediate results of .fo, and let fo return candidate auxiliary \ninformation for jo under @. We use ~~ to denote a function that returns all intermediate results and \ncandidate auxiliary information; 7 , the cached result of ~~ (z); and ~~, an incremental version of ~~ \nunder @. Finally, we use ~ to denote a function that returns only the useful intermediate results and \nauxiliary information; ~, the cached result of JO (z); and ~0 , a function that incrementally maintains \nonly the useful intermediate results and auxiliary information. Note that (useful) intermediate results \ninclude the original return vaJue. Figure 3 summarizes the notation. 3 Phase A: Discovering candidate \nauxiliary information Auxiliary information is, by definition, useful information not computed by the \noriginal program so it can not be fO, obtained directly from f.. However, auxiliary information is information \ndepending only on z that can speed up the com\u00ad sum(z) = if null(z) then O else car(z) + sum(cdr(z)) prod(x) \n= if nrdl(z) then 1 else car(z) * prod(cdr(z)) function definitions denoted incremental punction return \nvalue as function original value r fo f; all i.r. F fo candidate a.i. fo all i.r. &#38; candidate a.i. \nT ~i 32 useful i.r. 3C useful a.i. F 52 Figure 3: Notation putation of jO (z @ y). Seeking to obtain \nsuch information systematically, we come to the idea that when computing fo(~ @ Y), for example in the \nmanner of ~j(z, y, r), there are often subcomputations that depend only on z and r, but not on y, and \nwhose values can not be retrieved from the return value or intermediate results of -fo (z). If the val\u00adues \nof these subcomputations were available, then we could perhaps make fj faster. To obtain such candidate \nauxiliary information, the basic idea is to transform fo (z @ y) as for incrementalization and to collect \nsubcomputations in the transformed fo (z @ y) that depend only on z and whose values can not be retrieved \nfrom the return value or intermediate results of f.(z). Note that computing intermediate results of f.(z) \nincrementally, with their corresponding auxiliary information, is often crucial for efficient increment \nal computation. Thus, we modify the basic idea just described so that it starts with jo(z (B y) instead \nof fo(z @ y). Phase A has three steps. Step 1 extends fo to a function ~o that caches all interm@iate \nresults. Step 2 transforms fo (z @ y) into a function fo that exposes ca;didate auxiliary information. \nStep 3 constructs a function fo that computes only the candidate auxiliary information in To . 3.1 Step \nA.1: Caching all intermediate re\u00ad sults Extending fo to cache all intermediate results uses the trans\u00adformations \nin Stage I of [40]. It first performs a straight\u00adforward eztens~on trans~ormation to embed all intermediate \nresults in the final return value and then performs adminis\u00adtrative simplifications. Certain improvements \nto the extension transformation are suggested, although not given, in [40] to avoid caching redundant \nintermediate results, i.e., values of function appli\u00adcations that are already embedded in the values \nof their en\u00adclosing computations, since these omitted values can be re\u00ad If sum(z) = r, then surn (y, \nr) = surn(cons(y, z)). For z of length n, sum (y, r) takes time 0(1); surn(corw(y, z)) takes time O(n). \nCrnp(z) = Ist(c=p(z)). For z of length n, c~p(z) takes time O(n); crnp(z) takes time O(n). If c~p(z) \n= ?, then c~p (yl, y,, i) = cwp(cons(yI, COTZS(YZ, c))). For z of length n, c=p (yI, YZ, f) takes time \n0(1); c~p(cons(yl, cons(yz, z))) takes time O(n). Crnp(z) = Ist(cFp(m)). For ~ of length n, c~p(z) takes \ntime O(n); crnp(z) takes time O(n). If c~p(z) = ;, then c~p (y, F) = c~p(cons(y, z)). For z of length \nn, c~p (y,~ takes time O(l); c~p(cons(y, z)) takes time O(n). Figure 2: Resulting trieved from the results \nof the enclosing applications. These improvements are more Important for discovering auxihary information, \nsince the resulting program should be much simpler and therefore easier to treat m subsequent analy\u00adses \nand transformations. These improvements also benefit the modified version of this extension transformation \nused in Step A.3. We first briefly describe the extension transformation in [40]; then, we describe an \nembedding analysis that leads to the desired improvements to the extension transformation. Extension \ntransformation. Basically, for each func\u00adtion definition ~(vl, . . . v~) = e, we construct a function \ndef\u00adinition f(v,, ..,, %) = Szi[e] (2) where fzt[e] extends an expression e to return the values of all \nfunction calls made in computing e, i e., it considers subexpressions of e in applicative and left-to-right \norder, introduces bindings that name the results of function calls, builds up tuples of these values \ntogether with the values of the original subexpressions, and passes these values from subcomputations \nto enclosing computations. The definition of Ext is given in Figure 4. We assume that each introduced \nbinding uses a fresh variable name. For a constructed tuple <>, while we use 1st to return the first \nelement, which is the original return value, we use rst to return a tuple of the remaining elements, \nwhich are the corresponding intermediate results here. We use an infix operation @ to concatenate two \ntuples. For transforming a conditional expression, the transformation Pad[e] generates a tuple of - s \nof length equal to the number of the function applications in e, where _ is a dummy constant that just \nOC\u00ad cupies a spot, The length of the tuple generated by ParZ~e] can easily be determined statically. \nThe use of Pad ensures that each possible intermediate result appears in a fixed po\u00ad sition independent \nof value of the Boolean expression. Administrative simplifications are performed on the re\u00adsulting functions \nto simplify tuple operations for passing in\u00ad Surn (y, T) = y+r C=p(z) = let w = szL7n(odd(z)) in let \nw = prod(euen(z)) in <7J1<?J2, VI, ?J2> Cwp (~l, y2, i) = let w = YI + 2nd(?) in let V2 = y2 * 3rd(?) \nin < 01 <V2, VI, 0 2> C=p(z) = let w = odd(z) in let al = SUm(VI) in let V2 = even(z) in let U2 = prod(vz) \nin <Ul < U2, uI, U2, Sum(vz), prod(vl)> c~p (y, q = <y+4th(q < 5th(q, y+4th(F), 5tl@-), 2nd(q, y*3rd(7 \n> function definitions termediate results, unwind binding expressions that become unnecessary as a result \nof simplifying them subexpressions, and lift bindings out of enclosing expressions whenever pos\u00adsible \nto enhance readabdity. The following improvements can be made to the above brute-force caching of all \nintermediate results. First, be\u00adfore applying the extension transformation, common sub\u00adcomputations in \nboth branches of a conditional expression are lift ed out of the conditional. This simplifies programs \nin general. For caching all intermediate results, this lifting saves the extension transformation from \ncaching values of common subcomputations at different positions in different branches, which makes it \neasier to reason about using these values for incremental computation. The same effect can be achieved \nby explicitly allocating, for values of common subcomputations in different branches, the same slot in \neach corresponding branch. Next, we concentrate on major improvements These improvements are based on \nan em beddzng analysis. Embedding analysis. First, we compute embedding relations. We use kf~(f, t) to \nindicate whether the value of v, is embedded in the value of f(q, .... Vn), and we use Me(e, v) to indicate \nwhether the value of variable v is embedded in the value of expression e. These relations must satisfy \nthe following safety requirements: if Mf(f, i) = true, then there exists an expression r such that, if \nu = ~(vl, . . ..vn). then v, = fil(u) (3) if Me(e, v) = true, then there exists an expression e7 such \nthat, if u = e, then v = e~l(u) For each function definition f(vl, . . . . vn) = e j, we define Mf(f, \ni) = Me(ef, v,), and we define Me recursively as in Figure 5. For a primitive function p, 3p~1 denotes \ntrue if p has an inverse for the zth argument, and false otherwise. For a conditional expression, if~j., \ndenotes true if the value of e] can be determined statically or inferred from the value &#38;xt [v] \n= <v> &#38;zt[g(el, . . ..e~)] where g is c or p = let w =Szt[e~] in < g(lst(vl ),...,lst(vn)) let vm=&#38;zt[en] \n> @ rst(vl) in Cl . . @ Tst(%) Szt[f(el,...,en)] = let WI =t%t[e~] in let on= E7t[en] let W= f(lst(vl), \n.. ..lst(o~)) in < lst(v) > @ Tst(wl) @ .. . @ r-st(vn) in @ < v > fzt~f el then ez else eJ] = let vl \n=tzt[el] if Ist(vl) then else in let V2 =tkt[e2] in < lst(u2) > @ rst(vl) let W3= tM~e3] in < lst(v3) \n>0 rst(v~) CD rst(v2) @ Pad~e~] @ Pad[e3] @ rst(v~) Szt[let v=el in ez] = let let v] =tlrt[el] v = lst(vl) \nin in let 02 =fzt[e~] in < lst(?J2) > @ rst(v]) @ r-st(v2) Figure 4: Definition of Ezt Me(u, Me(c(el, \nMe(p(el, v) . . ..en).v) . . ..e~). v) = = = { true false Me(el, v) (3p~ AMe(el, ifv=u otherwise V .,, \nV v)) Me(e~, V ,.. v) V (3p~ AMe(e~, v)) Me(f(el, . . . . en), v) = (Mf(~,l)AMe(el, v)) V... V (Mf(f, \nn) AMe(e~, v)) Me(if el thene2 else eq, v) = zf~je, A (ell-Me(ez, v)) A (elt-Me(e3, v)) Me(letu=el inez, \nv) = &#38;fe(ez, v) V (Me(el, v) AMe(eZ, u)) Figure 5: Definition of Me of if el then ez else e3, and \nfalse otherwise. For example, af~J.3 is true if el is T (for true) Or F (for false), or if the two branches \nof the conditional expression return applications of different constructors. For a Boolean expression \nel, el R Me(e, v) means that whenever e] is true, the value of v is embedded in the value of e. In order \nthat the embedding analysis does not obviate useful caching, it considers a value to be embedded only \nif the value can be retrieved from the value of its immediately enclosing computation in constant time; \nin particular, this constraint applies to the retrievals when 3p;l or if~~e3 is true. We can easily show \nby reduction that the safety require\u00adments (3) are satisfied. To compute Mf, we start with Mf (f, i) \n= true for every f and i and iterate using the above definitions to compute the greatest fixed point \nin the point\u00adwise extension of the Boolean domain with false L true. The iteration always terminates \nsince these definitions are monotonic and the domain is finite. Next, we compute embedding tags. For \neach function definition f (cq, . . . . v~) = e~, we associate an embedding tag Mtag(e) with each subexpression \ne of e ~, indicating whether the value of e is embedded in the value of e ~. Mtag can be defined in a \nsimilar fashion to Me. We define Mtag(et ) = true, and define the true values of Mtag for subexpressions \ne of ef as in Figure 6; the tags of other subexpressions of ej are defined to be false. These tags can \nbe computed directly once the above embedding relations are computed. Finally, we use the embedding tags \nto compute, for each function f, an embedding-ail property Mall(f) indicating whether all intermediate \nresults of f are embedded in the value of f. We define, for each function f(vl, . . . . rr~) = ef, Mall(f) \n= Mtag(g(el, . . ..em)) A Mall(g) A all function applications 9(el, , en) occurrurg in ef (4) where \nMtag is with respect to ef. To compute Mall, we start with Mall(f) = true for all f and iterate using \nthe definition in (4) until the greatest fixed point is reached. This fixed point exists for similar \nreasons as for Mf. Improvements. The above embedding analysis is used to improve the extension transformation \nas follows. First, if MalZ( f ) = true, i.e., if all intermediate results of f are embedded in the value \nof f, then we do not construct an extended function for f. This makes the transformation for caching \nall intermediate results idempotent. If there is a function not all of whose intermediate re\u00adsults are \nembedded in its return value, then an extended function for it needs to be defined as in (2). We modify \nthe definition of .&#38;zt[f(el, . . . . en)] as follows. If Ma/1(~) = true, which includes the case \nwhere f does not contain function applications, then, due to the first improvement, f is not extended, \nso we reference the value of ~ directly: if if if if if Mtag(c(el, Mtag(p(e~, lftag(~(el, Mtag(if Mtag(let \n. . . . en)) = true . . ..em)) = true . ..)en)) = true el then e2 else v=el in ez) = e3) true = true \nthen then then then then Mtag(e, Mtag(e, Mtag(e, Mtag(e, Mtag(ez) ) ) ) ) = = = = = true, true true true \ntrue; for t = In if 3p~l, for t = l.. rr If ibf~(~,t), for i = l,, n if ~f~~,,, for z = 1, 2, 3 Mtag(el \n) = true if Me(ez, v) Figure 6: Definition of Mtag tlct[f(el, . . . . en)] = let w =tkt[el] in ... let \nVm=M[e~] in (5) let w= .... Ist(vn)) f(lst(w), in <v > @rst(wl) Q . ..@ rst(%) @ < u> Furthermore, if \nMall(f) = true, and iWtag(~(el, . . . . en)) = true, i.e, the value of ~(el, . . . . en) is embedded \nin the value of its enclosing application, then we avoid caching the value of ~ separately: .%t[f(el, \n. . . . en)] = let w =tM[el] in let rJ. =Ect[em] in (6) <~(lst(vl), . . . . Ist(on)) > Q rst(vl) @ @ \nrst(vn) To summarize, the transformation tzt remains the same as in Figure 4 except that the rule for \na function application f (el, . . . . en) is replaced with the following: if Mall(f) = true and Mtag(f(el, \n.,., e~)) = true, then define &#38;rt[f(el, . . ..e~)] as in (6); else if Mall(f) = true but Mtag(f(el, \n. . ..en)) = false, then define tkt[f(el, . . . . e~)] as in (5); otherwise de\u00adfine t2ct~$(el, . . . \n. en)] as in Figure 4. Note that function applications f (el, .... en) such that Mall(f) = true and Mtag(f(el, \n.,., en)) = true should not be counted by Pad. The lengths of tuples generated by Pad can still be statically \ndetermined. For the function cmp of Figure 1, this improved extension transformation yields the following \nfunctions: Z77@(z) = let VI = odd(z) in let UI = RTi7t(vI) in let V2 = even(z) in let U2 = prod(tb) \nin < lst(ul) < lsf(uz), U1, Ul, 02, U2 > miqz) = if null(z) then < 0, > (7) else let w = 2ZZF7Z(CJT(Z)) \nin < car(z) + lst(vl), U1 > prod(z) = if null(z) then < 1, > else let VI = prod(cdr[x)) in < car(z) \n* lst(vl), 7JI > Functions odd and even are not extended, since all their intermediate results are embedded \nin their return values.  3.2 Step A.2: Exposing auxiliary Morma\u00ad tion by incrementalization This step \ntransforms ~0 (z @ y) to expose subcomputations depending only on x and whose values can not be retrieved \nfrom the cached result of .fo (z). It uses analyses and trans\u00adformations similar to those in [41] that \nderive an incremen\u00adtal program ~0 (z, y, T), by expanding subcomput ations of ~o (Z @ Y) depending on \nboth z and y and replacing those depending only on z by retrievals from F when possible. Our goal here \nis not to quickly retrieve values from F, but to find potentially useful auxiliary information, i.e., \nsubcom\u00adputations depending on z (and ~) but not y whose values can not be retrieved from ~. Thus, time \nconsiderations in [41] are dropped here but are picked up after Step A.3, as discussed in Section 5. \nIn particular, in [41], a recursive application of a function f is replaced by an application of an incremental \nversion f only if a fast retrieval from some cached result of the previ\u00adous computation can be used as \nthe argument for the param\u00adeter of f that corresponds to a cached result. For example, if an incremental \nversion f (zj y, r) is introduced to compute f(z @ y) incrementally for r = f(z), then in [41], a function \napplication f (g(~)@ h(y)) is replaced by an application of ~ only if some fast retrieval p(r) for the \nvalue of f(g(z)) can be used as the argument for the parameter ~ of f (z, y, r), in which case the application \nis replaced by f (g(z), h(y), p(r)). In Step A. 2 here, an application of f is replaced by an ap\u00adplication \nof f also when a retrieval can not be found; in this case, the value needed for the cache parameter is \ncomputed directly, so for this example, the application f(g(z) @ h(y)) is replaced by f (g(z), h(y), \nf(g(z))). It is easy to see that, in this case, f(g(z)) becomes a piece of candidate auxiliary information. \nSince the functions obtained from this step may be dif\u00adferent from the incremental functions ~ obtained \nin [41], we denote them by ~ . For the function -in (7) and input change opera\u00adtion z @ y = cons(y, z), \nwe transform the computation of -(corzs(y, z)), with -(~) = ~: 1. unfold cmp(cons(y, z))  let w = odd(cons(y, \nz)) in let UI = m(m) in let vz = ewen~co~s(y, x)) in let U2 = pTod(wz) in < Ist(ul) < lst(uz), U1, U1, \nV2> U2 > 2. unfold odd, sum, even and simplify definition of i5i@ (y, T), F G X[,,l. For every subexpression \ne . let o: = even(z) in in the definitions of Ri7i7(z) and =(z), Xiel = {z}. let uj = FiiEi(vj) in let \nvz = odd(z) in let U2 = p~od(oz) in < ?J+lst(ui) < lst(u2), cons(y, vj), <y+lst(u~), U! >, W, U2 > 3, \nreplace applications of even and odd by retrievals = let v; = 4th(r) in let u; = *(w;) in let w = 2nd(F) \nin let U2 = prod(oz) in < y+lst(d) < lst(?J2), Cons(y, o;), <y+lst(u; )j 74 >, V2, 7J2 > Simplification \nyields the following function w such that, . If crop(z) = F, then - (y, F) = -(cons(y,z)): Z7i@ (y, P) \n= let uj = 3117i7(4th(F)) in let uz = prod(2nd(P)) in <y+lst(r14 )<lsf(u2), (8) cons(y,4th(F)), < y+lst(~;), \nu; >, %d(~), U2 > 3.3 Step A.3: Collecting candidate auxil\u00ad iary information This step collects candidate \nauxiltary information, i.e., in\u00adtermediate results of to (z, y, F) that depend only on x and T. It is \nsimilar to Step A. 1 in that both collect intermediate results; they differ in that Step A. 1 collects \nall intermediate results, while this step collects only those that depend only on xand ?. Forward dependence \nanalysis. First, we use a forward dependence analysis to identify subcomputations of ~o (~, Y, F) that \ndepend only on z and F. The analysis is in the same spirit as binding-time analysis [32, 37] for partial \nevaluation, if we regard the arguments corresponding to z and F as static and the rest as dynamic. We \ncompute the following sets, called forward dependency sets, directly. For each function ~(vl, .... v~) \n= ef, we compute a set lZf that contains the indices of the arguments of ~ such that, in all uses of \n~, the values of these arguments depend only on z and F, and, for each subexpression e of ef, we compute \na set X[el that contains the free variables in e that depend only on z and F. The recursive definitions \nof these sets are given in Figure 7, where FV(e) denotes the set of free variables in e and is defined \nas follows: FV(V) = {v} FV(g(el, ....en)) = FV(el) U UFV(e~) where gis c,p, or f FV(if el then e2 else \ne,) = FV(el) U FV(ez) U FV(es ) FV(let v = el in e2) = FV(el) U (FV(ez) \\ {u}) To compute these sets, \nwe start with Xfo, the containing indices of the arguments of $0 corresponding to z and F, and, for \nall other functions f, Xj containing the indices of all arguments of f, and iterate until a fixed point \nis reached. This iteration always terminates since, for each function f, f has a fixed arity, Zf decreases, \nand a lower bound 0 exists. For the running example, we obtain Z= = {2} and X= = Z-= {1}. For every subexpression \ne in the collection transformation. Next, we use a collec\u00adtion transformation to collect the candidate \nauxiliary infor\u00admation. The mtin difference between this collection trans\u00adformation and the extension \ntransformation in Step A. 1 is that, in the former, the value originally computed by a subexpression \nis returned only if it depends only on x and T, while in the latter, the value originally computed by \na subexpression is always returned. Basically, for each function ~(vl, . . . . v~) = e called in the \nprogram for fo and such that Zf # 0, we construct a function definition ~(o,l,..., v,k) = Co/[e] (9) \n where Xf = {ii, ....i~} and 1 ~ iI < ... < ~k ~.n. ~l[e] collects the results of intermediate function \napphcations in e that have been statically determined to depend only on z and ?. Note, however, that \nan improvement similar to that in Step A. 1 is made, namely, we avoid constructing such a collected version \nfor f if Zf = {1, . . . . n} and Mall(f) = true. The transformation ~1 always first examines whether \nits argument expression e has been determined to depend only on z and F, i.e., FV(e) s X[el. If so, Co/[e] \n= fzt~e], where Ezt is the improved extension transformation defined in Step A. 1, Otherwise, Col[e] \nis defined as in Figure 8, where Pddue] generates a tuple of s of length equal to the number of the \nfunction applications in e, except that function ap\u00adplications feel,..., e~) such that Xf = 0, or Xf \n= {1, . ..)n} but Mall(f) = true and Mtag(f(el, . . . . en)) = true are not counted. Note that if e has \nbeen determined to depend only on z and F, then lst(Col[e]) is just the original value of e; other wise, \nCol[e] cent ains only values of intermediate func\u00adtion applications. Although this forward dependence \nanalysis is equivalent to binding time analysis, the application here is different from that in partial \nevaluation [31]. In partiaf evaluation, the goal is to obtain a residuaf program that is specialized \non a given set of static arguments and takes only the dy\u00adnamic arguments, while here, we construct a \nprogram that computes only on the static arguments. In this respect, the resulting program obtained here \nis similar to the slice obtained from forward slicing [63]. However, our forward dependence analysis \nfinds parts of a program that depend only on certain information, while forward slicing finds parts of \na program that depend possibly on certain information. Furthermore, our resulting program also returns \nall inter\u00admediate results on the arguments of interest. For the function Fi7@ in (8), collecting all \nintermediate results that depend only on its second parameter yields critp(F) = < FiI?ii(4th(F)), =(2nd(f)) \n> (lo) We can see that computing ctip(~) is no slower than com\u00adputing crop(z). We will see that this \nguarantees that incre\u00admental computation using the program obtained at the end is at least as fast as \ncomputing cmp from scratch. 4 Phase B: Using auxiliary information Phase B determines which pieces of \nthe collected candidate auxiliary information are useful for increment al comput a\u00adtion of f. (z @ y) \nand exactly how they can be used. The For each function $(w, .... vn) = ef, define Z[,fl = {v, I i E \nZf } and, for each subexpression e of ef, if e is c(el, . . .. en) or p(el, . . ..e~) then x[~ll = = \n~l.nl = E[.1 if eis ~l(el, . . .. en) then X[e,l = = X[.nl = X[,l and Xjl ={z I F v(e, ) G ~1,1} n ~~, \nif e is if el then ej else es then X[e,l = ~Ie,] = ~[e.] = ~[e] X1.] U {v} if lW(el) ~ ~1.1 ifeisletv=elinej \nthen X[ell =~[e] and ~[.z] = X1,] \\{v} otherwise { Figure7: Definition of.X 61[.] = <> Col[g(el, . \n. . . en)] where g is c or p = Cot[el] Q . Cl (bl[e~] C2Z[f(el, . . . . en)] = let vl=b~[el] in let v~=Col[e.] \nin ej @ @ej Q e rst(v,) if z C Zf where e( = / Vt otherwise if Xf=O e = <i(lst(u,,), . . ..lst(u.k)) \n> otherwise where .Zf={tl,...,~~} and l~tl<...<~~~n (<> f%i~f el then e2 else e3] = let V1 =Col[el] in \nif FV(el) ~ X[.ll if lst(vl) then let vz =Col[e2] in rst(w) @ vz Q! ?%d[e3] else let V3 =Col[e3] in rst(7JI) \n@ P&#38;d[e2] @ V3 = let q =G7i[el] in let w =G21[e2] in otherwise let V3 =Col[e3] in Vl@V2@tJ3 Col[let \nv=el in e2] = let VI =L%l[el] in let v = lsi(uI) in let vz =Cki[e2] in rst(vl) CD V2 VI Q ?.g let VI \n=Col[el] in let vz =Col[e2] in otherwise Figure 8: Definition of bl basic idea is to merge the candidate \nauxiliary information results and auxiliary information that are not useful. with the original computation \nof ~0 (z), derive an incremen\u00adtal version for the resulting program, and determine the least 4.1 Step \n13.1: Combining intermediate re.= information useful for computing the value of fO (z @ y) in that incremental \nversion. suits and auxiliary information However, we want the incremental computation of fO (z @ To merge \nthe candidate auxiliary information with ~0, we y) to have access to the auxiliary information in addztion \nto could simply attach it onto fO by defining fO to be the pair the intermediate results of fO (z). Thus, \nwe merge the candi\u00adof jO and jO: date auxiliary information in ~0 (z, T) with ~0 (z) instead of .fo \n(s). After deriving an incremental version for the result\u00ad7;(z) = let F= i.(z) in let 7 = io(x,~) in \n< ~,i > ing program, we prune out the useless auxiliary information and the useless intermediate results. \nand use the projection 110 (r] = Ist(lst(i]) to project out Phase B has three steps. Step 1 merges jO \nwith ~0 to the original return value of fo. However, we can do bet\u00adform a function ~~ that returns candidate \nauxiliary informa\u00ad ter by using a transformation to integrate the computation tion as well as all intermediate \nresults. It also determine~ra of ~0 more tightly into the computation of ~., as opposed projection IIo \nthat projects the return value of fO out of fO. to carrying out two disjoint computations. The integrated \nStep 2 incrementalizes ~~ under @ to obtain an incremental computation is usually more efficient; so \nis its incremental version ~0 . Step 3 prunes out of fO and fo the intermediate version, 164 We do not \ndescribe the integration in detail. Basically, it uses traditional transformation techniques [1 3] like \nthose used in tupling tactic [21, 50, 14]. We require only that IIo (~~ (z)) always project out lst(.fo \n(z)), which is the ~alue of jO (z), and that the values of all other components of ~0 (z) and j. (z, \nF) are embedded in the value of ~~ (z). This allows re-arranging the order of the components in the return \nvalue. For the functions w in (7) and Crnp in (10), we first define a function C-m p(z) = let F = crop(z) \nin let i = cfip(~) in < F, i > and a projection IIo (T2 = 1st (lst(l-]). Next, we transform c m-p(z) \nto integrate the computations of cmp and critp, 1. unfold C%-p, then i?ii@ and Crnp = let F = let VI \n= odd(z) in let u] = 31Z77i(tq ) in let m = even(z) in let w = prod(vz) in < lst(ul) < lst(u2), vl, \nul, v2, u2> in let 7 = < m(4th(7)), =(2nd(F)) > in <F,+> 2. lift bindings for VI, U1, w, uz, and simplify \n= let 7JI = odd(z) in let U1 = 31Zi7i(7Jl) in let vz = even(z) in let uz = pr-od(vz) in let T = <1.st(ul) \n< lst(u2), vl, ul, v2, u2> in let f = < Rt?7i(vz), =(v1) > in <?,?> 3. unfold bindings for f and 7 = \nlet m = odd(z) in let U1 = =(v1) in let w = even(z) in let uz = procl(vz) in << lst(ul) < lst(uz), 01, \nUI,7J2, U2 >, < 3m-i(v2), prod(m) >> Simplifying the return value and 110, we obtain the function c-m-p(z) \n= let VI = odd(z) in let U1 = Rii7i(vl) in let vz = even(z) in (11)let uz = prod(tb.) in < lst(ul) < \nM(w), 01, Ul, V2, Uz, sum(w), P~~~(w) > and the projection IIo (F) = Ist(rj.  4.2 Step B.2: Incrementalization \nTo derive an incremental version ~~ of ~~ under @, we can use the method in [41], as sketched in Section \n1. Depending on the power expected from the derivation, the method can be made semi-automatic or fully \nautomatic. For the function cilr] in (11) and input change operation z @ v = COnS(V, x), we derive an \nincremental version of c m-p under @: 1. unfold ciiii(cons(y, T)) = let VI = odd(cons(y, z)) in let u~ \n= FiFffi(vl) in let vz = even(cons(g, z)) in let U2 = p~od(vz) in < lst(ul) < lst(uz), ~1, U1, V2, \nU2, m(v2), m(vl) > 2. unfold odd, =, even, prod and simplify = let v; = even(z) in let u; = 3ZiZ(v~) \nin let w = odd(z) in let U2 = PT-00!(v2) in let uj = prod(v~) in <y + lst(uj) < lst(u2), cons(y, v~), \n<y+lst(u{), ui >, ~2, u2j H17E(V2), <y*lst(uj), uj >> 3. replace all applications by retrievals = let \nu: = 4th(ii3 in let u~ = 6th~r7 in let vz = znd~rj in let w = 7th@~ in let uj = 5th(jr] in <y+ lst(uj) \n< lst(uz), cons(7J, w;), <y+lst(u; ),uj >, 02, U2, 3Td(r), <y*lst(uj), Uj >> Simplification yields the \nfollowing incremental version cfi] such that, if C%-p(z) =T, then C m p (y, ?j = c m](cons(y, s)): C \n~ p (y, ?j = <y+lst(6th@_)) ~ lst(7th(j-_)), COnS(~, 4th~T]), < y+lst(6th@ )), 6th@) >, 2nd~ ], 7th(r], \n3Td(T~, < y* bt(5th(ii_)), 5th@] >> (12) Clearly, cfi p (y,?j computes cfi}(cons(y, x)) in only O(1) \ntime.  4.3 Step B.3: Pruning To prune ~~ and ~~, we use the analyses and transforma\u00adtions in Stage III \nof [40], A backward dependence analysis determines the components of?-and subcomputations of ~~ whose \nvalues are useful in computing II. (~~(z, y,i-]), which is the value of fO. A pruning transformation \nreplaces use\u00adless computations with _. Finally, the resulting functions are optimized by eliminating \nthe _ components, adjusting the selectors, etc. For the functions Cfi-p in (11) and CR-P in (12), we \nobtain c~p(z) = let v] = odd(z) in let al = Fii?i in Z(vl) let vz = even(z) in let w = @(vz) in < lst(ul) \n< M(U2), , < lst(ul), _ >, , < M(%), ->, < lst(Fz?7i(v2)), ->, < lst(a(vl)), ->> C%@ (7J~)<y+lst(6th@~) \nlst(7th@l), 6 Examples =s < y+lst(6th@_~), ->, ::< lst(7th(Y)),->, < lst(3ni(F)), _ >,< rJ*lst(5th(F)), \n_ >> Optimizing these functions yields the final definitions of c~p and c~p , which appear in Figure \n2: Discussion Correct ness. Auxiliary information is maintained in\u00adcrementally, so at the step of discovering \nit, we should not be concerned with the time complexity of computing it from scratch; this is why time \nconsiderations were dropped in Step A.2. However, to make the overall approach effective, we must consider \nthe cost of computing and maintaining the auxiliary information. Here, we simply require that the candidate \nauxiliary information be -computed at least as fast as the original program, i.e., t(fo(z, F)) ~ t(.fo(z)) \nfor ~ = fo(z). This can be checked after Step A.3. We guarantee this condition by simply dropping pieces \nof candi\u00addate auxiliary information for which it can not be confirmed. Standard constructions for mechanical \ntime analysis [57, 62] can be used, although further study is needed. Automatic space analysis and the \ntrade-off between time and space are problems open for study. Suppose Step B.1 projects out the original \nvalue using Ist. With the above condition, in a similar way to [40], we can show that, if ~o(z) = r, \nthen lst(~(z)) = r and t(~(z))< t(.fo(z)) (13) and if fo(z @ y) = r and ~o(z) = ~, then Ist(;d(z, y,q) \n= r , Fd(z, Y,q = fo(~ @ Y), (14) and t(~(z, y,~) s t(fo(z @ y)). i.e., the functions .fo and ~0 preserve \nthe semantics and com\u00ad pute asymptotically at least as fast. Note that Z (z) may terminate more often \nthan $0 (z), and ~~(z, y, ~ may termi\u00ad nate more often than fO (z (B y), due to the transformations used \nin Steps B.2 and B.3. Multi-pass discovery of auxiliary informa\u00adtion. The function Z can sometimes be \ncomputed even faster by maintaining auxiliary reformation useful for incre\u00admental computation of the \nauxiliary information already in ~. We can obtain such auxiliary information of auxiliary information \nby iterating the above approach. Other auxiliary information. There are cases where the auxiliary information \ndiscovered using the above approach is not sufficient for efficient incremental comput a\u00adtion. In these \ncases, classes of special parameterized data ~tructurea are often mmd, Ideally, we can colkt them m auxiliary \ninformation paramet erized with certain classes of data types. Then, we can systematically extend a program \nto compute such auxiliary information and maintain it in\u00adcrementally. In the worst case, we can code \nmanually discov\u00adered auxiliary information to obtain an extended program z, and then use OUr_systematic \napproach to derive an incre\u00admental version of ~0 that incrementally computes the new output using the \nauxiliary information and also maintains the auxiliary information. The running example on list processing \nillustrates the ap\u00adplication of our approach to solving explicit incremental problems for, e.g., interactive \nsystems and reactive systems. Other applications include optimizing compilers and trans\u00adformational programming. \nThis section presents an example for each of these two applications. The examples are based on problems \nin VLSI design and graph algorithms, respectively. 6.1 Strength reduction in optimizing com\u00ad pilers: \nbinary integer square root This example is from [45], where a specification of a non\u00adrestoring binary \ninteger square root algorithm is transformed into a VLSI circuit design and implementation. In that work, \na strength-reduced program was manually discovered and then proved correct using Nuprl [16]. Here, we \nshow how our method can automatically derive the strength re\u00adductions. This is of particular interest \nin light of the recent Pentium chip flaw [24]. The initial specification of the i-bit non-restoring binary \ninteger square root algorithm [23, 45], which is exact for perfect squares and off by at most 1 for other \nintegers, is ~ := 21 1 for z:= i 2 downto Odo   p:=n m2; if p > 0then (15) m:=m +2 else if p< 0then \nm:=m 2 In hardware, multiplications and exponential are much more expensive than additions and shifts \n(doublings or halvings), so the goal is to replace the former by the latter, To simplify the presentation, \nwe jump to the heart of the problem, namely, computing n m2 and 2 incrementally in each iteration under \nthe change m = m SC2 and i = i 1. Let fO be fo(~!m)O = PaX~ m2121) where pair is a constructor with \nselectors jst(a, b) = a and snd(a, b) = b, and let input change operation @ be (n , m , i )=(n, m,i)@()= \n(n, m+2 , i l) Step A.1. We cache all intermediate results of -fo, obtaining fo(n, m,i) = let v =m in \n<pair(n v, 2 ), v > Step A.2. We transform fO under @, obtaining fo (n,m,i, F) = let v = 2nd(7) + 2*m*snd(lst(T)) \n+ (snd(lst(T)))2 in < Imi?-(n u, s nd(lst(r))/2). v > Step A.3. We collect candidate auxiliary information, \nob\u00adt aining ~o(n, m, i, F) = < 2*m*snd(lst(T)), (snd(lst(F)))2 > (16) Step B.1. We merge the collected \ncandidate auxiliary in\u00adformation with fo, obtaining 11 (r ) = lst~rj and . fo(n, m,i) = let v =m2 inlet \nu = 2 in <pair(n v, u), v, 2*m*u, U2> 166 Step B.2. We derive an incremental version of ~~ under @, obtaining \nStep B.3. We prune the functions lo and ~~, eliminating their second components and obtaining fi(n,m,i) \n(17) = let u = 2 in < pair(n m21u)j ~*m*uj U2 > ~(n,m,i,~ = < pair(fst(lst(~) + 2nd(q 3r-d(F), snd(lst(q)/2), \n2nd(~/2 + 3rd(~, 3rd(F)/4 > (18) The expensive multiplications and exponential have been completely \nreplaced wit h additions and shifts. We even dis\u00adcover that an unnecessary shift is done in [45]. Thus, \na systematic approach such as ours is desirable not only for automating designs and guaranteeing correctness, \nbut also for reducing costs. 6.2 Transformational programming: pat h sequence problem This example is \nfrom [7]. Given a directed acyclic graph, and a string whose elements are vertices in the graph, the \nproblem is to compute the length of the longest subsequence in the string that forms a path in the graph. \nWe focus on the second half of the example, where an exponential-time recursive solution is improved \n(incorrectly in [7], correctly in [8]). The function llp defined below computes the desired length. The \ninput string is given explicitly as the argument to hp. The input graph is represented by a predicate \narc such that arc(a, b) is true if there is an edge from vertex a to vertex b in the graph. The primitive \nfunction max returns the maximum of its two arguments. llp(l) = if null(l) then O else max(llp(cdr(l)), \nl+~(car(l), co!r(l))) ~(n, 1) = if null(l) then O else if arc(n, car(l)) then max(~(n, cdr(l)), l+ f(car(l), \ncdr(i))) else f(n, cdr(l)) (19) The problem is to compute llp incrementally under the input change operation \nlot = cons(i, 1). Using the approach described in this paper, we obtain ~(l) = if null(J) then <0> else \nlet v = ~(car(l), cdr-(1)) in < max(llp(cdr(l)), l+lst(o)), v > ~(n, 1) = if null(l) then <O> else let \nu = ~(car(l), cdr-(1)) in if arc(n, car(l)) then <ma,x(j (n, Cxir(l)), 1+1.Yt(U), ~> else <~(n, cdr(l)), \nu> (20) and @(i, 1,7 = if nu~l(l) then <1,<0>> else let v = ~(i, 2, 2nd(~) in <max(lst(~, l+lst(v)), \nv> ~(i, 1,;)= if null(cdr(l)) then if arc(i, car(l)) then <1, <O>> else <O, <O>> else let u = ~(i, \ncdr(l), 2nd(fi)) in if arc(i, car(l)) then <max(lst(v), l+lst(fi)), fi > else < lst(v), r; > (21) Computing \nllp(cons(i, /)) from scratch takes exponential . time, but computing ~i~ (ij 1, ~ takes only O(n) time, \nwhere n is the length of 1, since ~ (i, 1, ~ calls ~, which goes through the list 1 once Finally, we \nuse these derived functions to compute the original function hp. Note that lip(l) = lst(~(l)) and, if \n~(t) = ~, then fi (i, 1,3 = ~(cons(z,l)). Using the definition of ~ in (21) in this last equation, we \nobtain: ~(cons(i, 1)) = if nrdl(l) then <1,<0>> else let F = G(l) in let v = 7(z, 1, 2nd(~) in <max(lst(Fj, \nl+lst(v)), v> Using this equation and the base case ~p(nil) = <0>, we obtain a new definition of ~: ~(l) \n= if null(l) then <0> else if null(cdr(l)) then <1, <0>> else let ~= ~(cdr(l)) in (22) let rJ = ~(car(l), \ncdr(l), 2nd(~) in <max(lst(~, l+lsf(v)), v > where ~ is defined in (21). This new F takes only O(n2) \ntime, since it calls ~ only O(n) times. 7 Related work and conclusion Work related to our analysis and \ntransformation techniques has been discussed throughout the presentation. Here, we take a closer look \nat related work on discoverirm auxiliary information for increment al computation. Interactive systems \nand reactive systems often use incre\u00admental algorithms to achieve fast response time [4, 5, 9, 19, 27, \n33, 53, 54]. Since explicit increment al algorithms are hard to write and appropriate auxiliary information \nis hard to discover, the general approach in this paper pro\u00advides a systematic method for developing \nparticular incre\u00adment al algorithms. For example, for the dynamic incremen\u00adtal attribute evaluation algorithm \nin [55], the characteris\u00adtic graph is a kind of auxiliary information that would be discovered following \nthe general principles underlying our a,pproach. For static incremental attribute evaluation algo\u00adrithms \n[34, 35], where no auxiliary information i~ needed, the approach can cache intermediate results and maintain \nthem automatically [40]. Strength reduction [2, 15, 60] is a traditional compiler op\u00adtimization technique \nthat aims at computing each iteration incrementally based on the result of the previous iteration. Basically, \na fixed set of strength-reduction rules for prim\u00aditive operators like times and plus are used. Our method \ncan be viewed as a principled strength reduction technique not limited to a fixed set of rules: it can \nbe used to reduce strength of computations where no given rules apply and, furthermore, to derive or \njustify such rules when necessary, as shown in the integer square root example. Finzte difler-enczng \n[46, 47, 48] generalizes strength re\u00adduction to set-theoretic expressions for systematic program development. \nBasically, rules are manually developed for differentiating set expressions. For continuous expressions, \nour method can derive such rules directly using properties of primitive set operations. For discontinuous \nset expres\u00adsions, dynamic expressions need to be discovered and rules for maintaining them derived. How \nto discover these dy\u00adnamic expressions remains to be studied, but once discov\u00adered, our method can be \nused to derive rules that maintain them. In general, such rules apply only to very-high-level languages \nlike SETL; our method applies also to lower-level languages like Lisp. Maintaining and strengthening \nloop inuariants has been advocated by Dijkstra, Grles, and others [18, 25, 26, 56] for almost two decades \nas a standard strategy for devel\u00adoping loops. In order to produce efficient programs, loop invariants \nneed to be maintained by the derived programs in an incremental fashion. To make a loop more efficient, \nthe strategy of strengthening a loop invariant, often by in\u00adtroducing fresh variables, is proposed [26]. \nThis corresponds to discovering appropriate auxiliary information and deriv\u00ading incremental programs \nthat maintain such information. Work on loop invariants stressed mental tools for program\u00adming, rather \nthan mechanical assistance, so no systematic procedures were proposed Inductzon and generalwation [10, \n44] are the logical foun\u00addations for recursive calls and iterative loops in deductive program synthesis \n[42] and constructive logics [16] These corpora have for the most part ignored the efficiency of the \nprograms derived, and the resulting programs are often wantonly wasteful of time and space [43]. In contrast, \nthe approach in this paper is particularly concerned with the ef\u00adficiency of the derived programs. Moreover, \nwe can see that induction, whether course-of-value induction [36], structural induction [10, 12], or \nwell-founded induction [10, 44], enables derived programs to use results of previous iterations in each \niteration, and generalization [10, 44] enables derived pro\u00adgrams to use appropriate auxiliary information \nby strength\u00adening induction hypotheses, just like strengthening loop in\u00advariant. The approach in this \npaper may be used for sys\u00adtematically constructing induction steps [36] and strength\u00adening induction \nhypotheses. The promotton and accumulation strategies are proposed by Bird [7, 8] as general methods \nfor achieving efficient trans. formed programs. Promotion attempts to derive a program that defines f(cons(a, \nz)) in terms of $(z), and accumula\u00ad tion generalizes a definition by including an extra argument, Thus, \npromotion can be regarded as deriving incremental programs, and accumulation as identifying appropriate \nin\u00adtermediate results or auxiliary information. Bird illustrates these strategies with two examples. \nHowever, we can discern no systematic steps being followed in [7]. As demonstrated with the path sequence \nproblem, our approach can be re\u00adgarded as a systematic formulation of the promotion and accumulation \nstrategies It helps avoid the kind of errors reported and corrected in [8]. Other work on transformational \nprogramming for im\u00adproving program efficiency, including the eztension tech\u00adnique in [17], the transformation \nof recursive functional pro\u00adgrams in the CIP project [11, 6, 49], and the finite differenc\u00ading of functional \nprograms in the semi-automatic program development system KIDS [59], can also be further auto\u00admated with \nour systematic approach. In conclusion, incremental computation has widespread applications throughout \ncomputing. This paper proposes a systematic approach for discovering a general class of aux\u00adiliary information \nfor incremental computation. It is natu\u00ad rally combined with increment alization and reusing interme\u00addiate \nresults to form a comprehensive approach for efficient incremental computation. The modularity of the \napproach lets us integrate other techniques in our framework and re\u00aduse our components for other optimizations. \nAlthough our approach is presented in terms of a first\u00adorder functional language with strict semantics, \nthe under\u00adlying principles are general and apply to other languages as well. For example, the method \nhas been used to improve imperative programs with arrays for the local neighborhood problems in image \nprocessing [39]. A prototype system, CA-CHET [38], based on our approach is under development. References \n[1]A. V. Aho, R. Sethi, and J, D. Unman. Compzlers, Prznczples, Techniques, and Tools. Addison-Wesley \nSe\u00adries in Computer Science. Addison-Wesley Publishing Company, Reading, Massachusetts, 1986. [2] F. \nE. Allen, J. Cocke, and K. Kennedy. Reduction of operator strength, In S. S. Muchnick and N, D, Jones, \neditors, Program Flow Analysts, chapter 3, pages 79 101. Prentice-Hall, Englewood Cliffs, New Jersey, \n1981. [3] B. Alpern, R. Hoover, B. Rosen, P. Sweeney, and K. Zadeck. Incremental evaluation of comput \nationaJ circuits. In Proceedings of the Ist Annual ACM-SIAM Symposzum on D~screte Algorithms, pages 32-42, \nSan Francisco, California, January 1990. [4] R. Bahlke and G. Snelting. The PSG system: From formal language \ndefinitions to interactive programming environments. ACM Transactions on Programming Languages and systems, \n8(4):547-576, October 1986. [5] R. A. Ballance, S. L. Graham, and M. L. Van De Van\u00adter, The Pan language-based \nediting system. ACM Transactions on Software Engineering and Methodol\u00adogy, 1(1):95 127, January 1992. \n[6] F. L Bauer, B. Moller, H. Partsch, and P. Pepper. Formal program construction by transformations \ncomputer-aided, mtmtion-guided programming. IEEE Transachons on So~troare Engineering, 15(2):165-180, \nFebruary 1989, [7] R. S. Bird. The promotion and accumulation strategies in transformational programming. \nA CM Transactions on programming Languages and Systems, 6(4):487 504, October 1984. [8] R. S, Bird. \nAddendum: The promotion and accumula\u00adtion strategies in transformational programming. A CM Transactions \non Programming Languages and Systems, 7(3):490-492, July 1985. [9] P. Borras and D. C16ment, CENTAUR: \nThe system. In Proceedtrrgs of the ACM SIGSOFT/SIGPLAN Soft\u00adware Engzneerzng Sympostum on Practtcal Sojtware \nDe\u00advelopment Enuzronments, pages 14 24, Boston, Mas\u00adsachusetts, November 1988. Published as SIGJ?LAN \nNotices, 24(2). [10] R. S. Boyer and J, S, Moore. A Computational Logzc. ACM Monograph Series. Academic \nPress, New York, 1979. [Ii] M. Broy. Algebraic methods for program construction: The project CIP. In \nP, Pepper, editor, Program Trans\u00adformation and Programm~ng Enutronm.nts, volume 8 of NATO Advanced Science \nInstitutes Series F: Computer and System Sciences, pages 199 222. Springer-VerIag, Berlin, 1984. Proceedings \nof the NATO Advanced Re\u00adsearch Workshop on Program Transformation and Pro\u00adgramming Environments, directed \nby F. L. Bauer and H. Remus, Munich, Germany, September 1983. [12] R. M. Burstall. Proving properties \nof programs by structural induction. The Computer Journal, 12( 1):41 48, 1969. [13] R. M. Burstall and \nJ. Darlington. A transformation system for developing recursive programs. Journal of the ACM, 24(1):44 \n67, January 1977. [14] W.-N. Chin. Towards an automated tupling strategy. In Proceedings of the A Chf \nSIGPLAN Symposium on PEPM, Copenhagen, Denmark, June 1993 [15] J. Cocke and K. Kennedy. An algorithm \nfor reduc\u00adtion of operator strength. Communz.ations of the A CM, 20(11):8.50-856, November 1977. [16] \nR. L. Constable et al. Implementing Mathematics wzth the Nuprl Proof Development System. Prentice-Hall, \nEnglewood Cliffs, New Jersey, 1986. [17] N. Dershowitz. The Evolution of Programs, volume 5 of Progress \nin Computer Science. Birkhauser, Boston, 1983. [18] E. W. Dijkstra. A Discipline oj Programming. Prentice-Hall \nSeries in Automatic Computation. Prentice-Hall, Englewood Cliffs, New Jersey, 1976. [19] V. Donzeau-Gouge, \nG. Huet, G. Kahn, and B. Lang. Programming environments based on structure editor: The Mentor experience. \nIn D. R. Barstow, H. E. Shrobe, and E. Sandewall, editors, Interactive Program\u00adming Environments, pages \n128 140. McGraw-Hill, New York, 1984. [20] J. Earley. High level iterators and a method for automatically \ndesigning data structure representation. Journal of Computer Languages, 1:321-342, 1976. [ZI] M. S. Feather. \nA system for assisting program transfor\u00admation. ACM Transactions on Programming Languages and Sgstems, \n4(1):1 20, January 1982 [22] J, Field and T. Teitelbaum. Incremental reduction in the lambda calculus, \nIn Proceedings of the ACM 90 Conference on LFP, pages 307-322, 1990. [23] 1. Flores. The Logzc of Computer \nArithmetic. Prentice-Hall International Series in Electrical Engineering. Prentice-Hall, Englewood Cliffs, \nNew Jersey, 1963, [24] J. Glanz. Mathematical logic flushes out the bugs in chip designs. Science, 267:332 \n333, January 20, 1995. [25] D. Gries. The Sc~ence of Programming. Texts and Monographs in Computer Science. \nSPringer-Verlag, New York, 1981. [26] D. Gries. A note on a standard strategy for developing loop invarlants \nand loops. Sczence of Computer Pro\u00adgramming, 2:207-214, 1984. [27] A, N. Hab ermann and D. Notkin. Gandalf \nSoft\u00adware development environments. IEEE Transactions on Software Engineering, SE-12(12):1117-1127, December \n1986. [28] R. Hoover Alphonse: Incremental computation as a programming abstraction. In Proceedings of \nthe ACM SIGPLAN 92 Conference on PLDI, pages 261-272, California, June 1992. [29] S. Horwitz and T. Teitelbaum. \nGenerating editing en\u00ad vironments based on relations and attributes. ACM Transactions on Programming \nLanguages and Systems, 8(4):577-608, October 1986. [30] F. Jalili and J, H. Gallier Building friendly \nparsers. In Conference Record of the 9th Annual ACM Symposium on POPL, pages 196 206, Albuquerque, New \nMexico, January 1982. [31] N, D. Jones C. K, Gomard, and P. Sestoft. Partzal Evaluation and Automatic \nProgram Generation. Pren\u00adtice Hall, Englewood Cliffs, New Jersey, 1993. [32] N. D. Jones, P. Sestoft, \nand H. S@ndergaard. An exper\u00adiment in partial evaluation: The generation of a com\u00adpiler generator. In \nJ.-P. .louannaud, editor, Rewrit\u00adzng Techniques and Applications, volume 202 of Lec\u00adture Notes in Computer \nSctence, pages 124 140, Dijon, France, May 1985. Springer-Verlag, Berlin. [33] G. E. Kaiser. Incremental \ndynamic semantics for language-based programming environments. ACM Transactions on Progr-ammtng Languages \nand Systemsj 11(2):168-193, April 1989. [34] U. Kastens, Ordered attributed grammars. Acts lnfor\u00admatica, \n13(3):229 256, 1980. [35] T. Katayama. Translation of attribute grammars into procedures. ACM Transactions \non Programming Lan\u00adguages and Systems, 6(3):345-369, July 1984. [36] S. C. Kleene. Introduction to Metamathematics. \nVan Nostrand, New York, 1952. Tenth reprint, Wolters-Noordhoff Publishing, Groningen and North-Holland \nPublishing Company, Amsterdam, 1991. [37] J. Launchbury. Projections for specialisation. In Par\u00adtial \nEvaluation and Mixed Computation, pages 299 315. North-Holland, 1988. [38] Y. A. Liu. CACHET: An interactive, \nmcremental\u00adattribution-based program transformation system for deriving incremental programs. In Pr-oceedzngs \nof the 10thKnowledge-Based Software Engineering Confer\u00adence, Boston, Massachusetts, November 1995, IEEE \nComputer Society Press. [39] Y. A. L,u. Incremental Cornputatzon: A Sernanttcs-Based Systematic Transformational \nApproach. PhD thesis, Department of Computer Science, Cornell Uni\u00adversity, Ithaca, New York, January \n1996, To appear as Cornell Technical Report, October, 1995, [40] Y. A. Liu and T. Teitelbaum. Caching \nintermediate results for program improvement. In Proceeding.s of the ACM SIGPLAN Sympostunr on PEPMI \npages 190-201, La Jolla, California, June 1995, [41] Y. A. Liu and T. Teitelbaum. Systematic derivation \nof incremental programs. Science of Computer Progrum\u00admzng, 24(1):1 39, February 1995. [42] Z. Manna and \nR. Waldinger. A deductive approach to program synthesis. ACM Transactions on Programming Languages and \nSystems, 2(1):90-121, January 1980, [43] Z. Manna and R. Waldinger. Fundamentals of deduc\u00adtive program \nsynthesis. IEEE Transactions on Software Engzneertng, 18(8):674-704, August 1992. [44] Z. Manna and R. \nWaldinger. The Deduct,ve Forsn\u00addations of Computer Programming. Addison-Wesley, Reading, Massachusetts, \n1993 [45] J. O Leary, M. Leeser, J. Hlckey, and M Aagaard, Non\u00adrestoring integer square root: A case \nstudy in design by principled optimization. In R. Kumar and T. Kropf, ed\u00aditors, Proceedings of TPCD 94: \nthe Z!nd International Conference on Theorem Provers an Carcuat Destgn Theory, Practzce and Ezperzence, \nvolume 901 of Lec\u00adture Notes zn Computer Sczence, pages 52 71, Bad Herrenalb (Black Forest), Germany, \nSeptember 1994, Springer-Verlag, Berlin, 1995. [46] B. Paige and J, T. Schwartz. Expression continmty \nand the formal differentiation of algorithms. In Conference Record of the Jth Annual ACM Sympostum on \nPOPL, pages 58-71, January 1977 [47] R. Paige. Transformational programming applica\u00adtions to algorithms \nand systems. In Con~erence Record of the 10th Annual ACM Symposzum on POPL, pages 73-87, January 1983. \n[48] R. Paige and S. Koenig. Finite differencmg of com\u00adputable expressions. ACM Transactions on Pr-ogram\u00adrn~ng \nLanguages and Systems, 4(3):402 454, July 1982, [49] H A Partsch. Specification and 7 ransforrnatzon \nof Programs A Formal Approach to Software Develop\u00adment. Texts and Monographs in Computer Science. Springer-Verlag, \nBerhn, 1990 [50] A. Pettorossi. A powerful strategy for deriving efi\u00adcient programs by transformation. \nIn Proceeclzngs of the ACM 84 Sympostum on LFP, Austin, Texas, August 1984, [51] L. L. Pollock and M. \nL. Soffa. Incremental global reoptimization of programs, ACM Transact~ons on Programming Languages and \nSystems, 14(2):173-200, April 1992. [52] W. Pugh and T, Teitelbaum. Incremental computa\u00adtion via function \ncaching, In Conference Record of the 16th Annual ACM Symposium on POPL, pages 315 328, January 1989, \n[53] S. P. Reiss, An approach to incremental compilation. In Proceedings of the ACM SIGPLAN 84 Sympostum \non, Compiler Construction, pages 144 156, Montreal, Canada, June 1984. Published as SIGPLAN Notices, \n19(6). [54] T, Reps and T. Teitelbaum. The Syntheswer Genera\u00adtor: A System for Constructing Language-Based \nEd\u00adttors. Texts and Monographs in Computer Science. Springer-Verlag, New York, 1988. [55] T. Reps, T. \nTeitelbaum, and A. Demers. Incremen\u00adtal context-dependent analysis for language-based ed\u00aditors. ACM Transachons \non Programming Languages and Systems, 5(3).449 477, July 1983. [56] J. C. Reynolds. The Craft of Programming. \nPrentice-Hall, 1981, [57] M. Rosendahl, Automatic complexity analysis, In Pro\u00adceedings of the ith International \nConference on FPCA, pages 144-156, London, U. K., September 1989, [58] B. G. Ryder and M C. Paull, Incremental \ndata flow analysis algorithms, ACM Transactions on Pro\u00adgramming Languages and Systems, 10(1):1 50, January \n1988. [59] D, R. Smith. KIDS: A semiautomatic program devel\u00ad opment system IEEE Transactions on Software \nEngi\u00adneering, 16(9):1024 1043, September 1990. [60] B, Steffen, J. Knoop, and O. Riithing. Efficient \ncode motion and an adaption to strength reduction. In Pro\u00adceedr,ngs of the ith International Joint Conference \non TAPSOFT, volume 494 of Lecture Notes in Computer Science, pages 394 415, Brighton, U. K., 1991. Springer-Verlag, \nBerlin. [61] R. S. Sundaresh and P. Hudak. Incremental computa\u00adtion vla partial evaluation. In Conference \nRecord of the 18th Annual ACM Sympostum on POPL, pages 1 13, January 1991. [62] B. Wegbreit. Mechanical \nprogram analysis. Communt\u00adcatzons of the ACM, 18(9):528 538, September 1975. [63] M. Weiser Program slicing. \nIEEE Transactions on 5 oftwure Engzneerirzg, SE-10(4):352 357, July 1984 [64] D. M. Yellin and R. E. \nStrom. INC: A language for incremental computations. A Clf Transactions on Programming Languages and \nSystems, 13(2).211-236, April 1991. 170  \n\t\t\t", "proc_id": "237721", "abstract": "", "authors": [{"name": "Yanhong A. Liu", "author_profile_id": "81350588448", "affiliation": "Department of Computer Science, Cornell University, Ithaca, New York", "person_id": "PP14186730", "email_address": "", "orcid_id": ""}, {"name": "Scott D. Stoller", "author_profile_id": "81100262890", "affiliation": "Department of Computer Science, Cornell University, Ithaca, New York", "person_id": "PP15027682", "email_address": "", "orcid_id": ""}, {"name": "Tim Teitelbaum", "author_profile_id": "81100391906", "affiliation": "Department of Computer Science, Cornell University, Ithaca, New York", "person_id": "P282428", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/237721.237769", "year": "1996", "article_id": "237769", "conference": "POPL", "title": "Discovering auxiliary information for incremental computation", "url": "http://dl.acm.org/citation.cfm?id=237769"}