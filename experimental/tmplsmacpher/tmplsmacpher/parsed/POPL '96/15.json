{"article_publication_date": "01-01-1996", "fulltext": "\n A Practical and Flexible Flow Analysis for Higher-Order Languages J. Michael Ashley* Computer Science \nDepartment Indiana University Bloomington, Indiana Abstract A flow analysis framework for higher-order, \nmostly\u00adfunctional languages is given. The framework unifies and extends previous work on flow analyses \nfor this class of programming languages. The analysis is based on abstract interpretation and is parametrized \nover the abstraction of literals, two polyvariance operators, and a projection operator. The polyvariance \noperators regulate the accuracy of the analysis while the projec\u00adtion operator regulates the speed. A \npreliminary im\u00adplementation of the analysis is incorporated and used in a production-quality Scheme compiler. \nThe analysis can process any legal Scheme program without modifi\u00adcation. While it has been demonstrated \nthat analyses at least as accurate as OCFA are useful for justifying program transformations, an instantiation \nof this anal\u00adysis less precise than OCFA is used to facilitate loop recognition, eliminate the construction \nof closures, and optimize procedure calls. This demonstr ates that rela\u00ad tively inaccurate analyses can \nstill be useful for justify\u00ad ing transformations, Introduction A flow analysis for a higher-order, \nmostly-functional language collects data-flow and control-flow information about programs in the language. \nThe information col\u00adlected can be used to drive program transformations like partial evaluation [9] and \nclosure conversion [22] as well as compiler optimizations like type recovery [19] and the selection of \nclosure representations [18]. There are several abstract interpretation-based ap\u00adproaches to the flow \nanalysifi problem [13, 15, 20, 24] Author s current address Snow Hall 415, Law,. ence, KS, 66045. E-mail \n~o, ~hley@eecs ukans. edti Permission to make dighal/lrsrd copies of all or part of this material for \npersonal or classroom use is granted without fee provided that the copies a~e not made or distributed \nfor profit or commercial advantage, the copy\u00ad n.ght notice, the title of the publication and its dste \nappear, and notice is gwen that copyright 1s by permission of the ACM, Inc. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires specific permission and/or fee. IWPL 96, St. \nPetersburg FLA USA @ 1996 ACM o-89791-769-3195101. .$3.50 and they collectively identify several important \naspects of it: e the accurate treatment of mutable data structures, e the use of type tests to constrain \nabstract values, . the use of polyvariance to increase accuracy, and e the use of projection (widening) \nto accelerate con\u00ad vergence to a fixpoint. These approaches each identify some but not all of the above \naspects. This paper develops a parameterized analysis that unifies them all. The analysis is parame\u00adtrized \nover the degree of polyvariance and projection as well as the abstract domain of constants. The analysis \nis not just a combination of previous results, however. Compared to previous work the anal\u00adysis is more \naccurate on assignment, can potentially recover more information from type tests, and can ex\u00adpress a \nwider range of polyvariance. The analysis is also implemented and used in a production-quality incre\u00admental \ncompiler for Scheme. The implemented analysis can process any Scheme program without modification. Furthermore, \nwhile most work on projection operators has been theoretical, the implementation incorporates a practical \nand useful projection operator to accelerate the analysis without losing too much information. The analysis \nis used to justify the optimization of procedure calls and closure representations as well as to facilitate \nloop recognition. The rest of the paper is organized as follows. Sec\u00adtion 2 develops the analysis by \nfirst giving a collecting operational semantics for a canonical Scheme language. The analysis is then \ngiven as a simple and intuitive ab\u00ad straction of the collecting semantics. Section 3 discusses the implementation \nof the analysis. It also gives some preliminary results on the cost of the analysis and its usefulness \nfor justifying optimizations. Section 4 de\u00ad scribes related work, and Section 5 gives conclusions. 2 \nThe analysis The analysis operates over closed programs in the core language given in Figure 1. It is \na subset of Scheme with some represent ative primitives added. While very sim\u00adple, the analysis developed \naround this core language can be extended to all of Scheme or ML, including mul\u00adtiple return values, \nvariable arity procedures, and pro\u00adgrams with free variables. The language could be given a semantics \ndirectly, but it is useful to work instead with a canonical lan\u00adguage. The canonical language is continuation-passing \nstyle (CPS) A-normal form. The grammar for this lan\u00adguage is given in Figure 2. It is like A-normal form \n[12], but each continuation of a procedure call and condi\u00adtional is made explicit through conversion \nto CPS. The continuation of a conditional is made explicit to avoid code duplication, and the syntax \nfor pair? anticipates the continuation being evaluated and bound to a tem\u00adporary variable. A core program \nis translated into canonical form by first converting it to A-normal form and then converting to continuation-passing \nstyle. The translation process also assigns a unique tag to each lambda expression, and each cons expression \nis translated to introduce two new variables that do not occur elsewhere in the pro\u00adgram. This extra \ninformation is used by the analysis. In the CPS conversion, the primitive halt is used to define the \ninitial continuation (lambda (t) (halt t)). An example translation is given in Figure 3. After conversion \nto canonical form, each lambda ex\u00adpression corresponds to a program point of the core pro\u00adgram. During \nrun time, control flow can enter a lambda expression s body only through a procedure call, and control \ncan leave only through a tail call. In other words, the body of each lambda expression corresponds to \na basic block. Defining the flow analysis on the canonical language follows the usual abstract interpretation-based \nmethod\u00adology [10]. First a collecting operational semantics is de\u00adfined that assigns an exact meaning \nto programs. The flow analysis is then given as an abstract operational semantics defined in terms of \nthe collecting semantics. 2.1 The collecting machine The collecting machine is an instrumented variant \nof a standard machine to execute canonical programs. It differs from a standard machine by building a \ncache as it executes a program. The meaning of the program is defined to be the cache that exists when \nthe collecting machine reaches a halt state. The cache expresses the set of execution states that arise \nat each procedure en\u00adtry point during the execution of the program. Since continuations are represented \nas procedures, the cache also expresses information about the execution state on procedure return. The \ncollecting machine is defined in Figure 4. A value is either a literal, a pair, or a closure. A pair \nis simply a pair of locations. A closure consists of a tag, a sequence of formals, the code for the closure, \nand an environment. An environment is a finite function map\u00adping variables to locations. A store is a \nfinite function mapping locations to values. The cache is a set of tag, environment, and state triples, \neach of which describes an execution state on entry to a procedure. A state in the machine is a four \ntuple consisting of the next expres\u00adsion to execute, an environment, a store, and a cache. The machine \nhalts if the current machine state does not match any of the transition rules or if the meaning of an \nauxiliary function is undefined. Given a canonical pro\u00adgram A, the initial state of the machine is (A, \n0,0, 0), where 0 denotes an empty finite function or set. The transition rules are straightforward. Each \nof the first three rules evaluates a primitive application and binds the result to a temporary variable. \nThe func\u00adtion [ ] is used to evaluate simple expressions, and the function new is used to obtain fresh \nlocations that do not occur in the domain of Stores. A pair? expression is evaluated by binding a continuation \nto a temporary variable, evaluating the condition, and then using the function truish ? to select one \nbranch or the other for continuing the evaluation. An application evaluates its operator to get a closure, \nextends the environment with new locations, and extends the store to bind those loca\u00adtions to the arguments. \nIt then updates the cache with a snapshot of the current execution state and continues with the evaluation \nof the closure s code. As given, the collecting machine in fact implements a flow analysis. After the \nmachine terminates success\u00adfully, a post processor can use the cache to determine control-and data-flow \ngraphs for the program. Unfor\u00adtunately, the collecting machine does not terminate for all programs, since \nnonterminating programs lead to an infinite number of execution states. From the collecting machine, \nhowever, it is possible to derive an abstract machine that implements a computable but approxi\u00admate flow \nanalysis.  2.2 The abstract machine To obtain a computable abstraction of the collecting machine, the \ncollecting machine s execution states must must be collapsed into a finite number of approximate states. \nThe collecting machine may produce an infi\u00adnite number of execution states, because a variable may be \nmapped to a possibly infinite number of locations. Thus, the store may be infinite. In the abstract ma\u00ad \n M = c I v I (if (pair? Ml) Mz Ms) I (call/cc M) I (lambda (v1 . . v~) M) I (MOM, .,. M.) I (cons Ml \nM2) I (car Ml) \\ (begin (set-car! Ml M2) M3) c E Constants v g Variables Figure 1: The core Scheme language \n A = ~lej jfv (cons VI Vz S l Sz))) A) I (let ((v (car S))) A) I . . . S~) I (pair? v S1 SZ Al Az) \\ \n(set-car! S1 Sz A) I (halt S) S = clvl(lambdaq(vl .,. v~)A) c .S Constants v c Variables q E Tags  \nFigure 2: The canonical Scheme language (let ((copy (lambda (1s copy) (if (pair? k) (cons (car 1s) (copy \n(cdr k) copy)) ())))) (copy (cons 1 (cons 2 ())) copy)) ((lambda qO (kO copy) (let ((v~ (cons v, V3 2 \n()))) (let ((v, (cons v, v, 1 v,) (copy k, v, copy)))) (lambda ql (V6) (halt V6)) (lambda qz (kl 1s copy) \n(pair? k~ k, 1s (let ((vT (cdr 1s))) (COPV (lambda TE (v,) (let ((v, (car 1s))) (let ((v~~ (cons v~ (kz \n7J,0)))) V7 Copfy)) (~2 ())))) Figure 3: The procedure copy and its equivalent in canonical Scheme p \nG Environments = Variables % Locations CTe Stores = Locations % Objects Objects = Constants u (Locations \nx Locations) u ( Tags x Variables x Programs x Environments) y E Caches = P( Tags x Environments x Stores) \n((let ((v (cons VI v, SI S,))) A), P,o, T) --+ (A, [v +\u00ad l]p, o ,~) where (1, 11, 12) = new((v, vl, vz), \n~) = [1, 11,12 +\u00ad (11, 12), [Sl]pa, [s2]Pa]a ((let ((v (car S~~ A), p, a,?) ---+ (A, [v +\u00ad l]p, [1 + \no(lo)]o, ?) where (1) = new((v), a) (1,, 1,) = [S]pa ((set-car! S l Sz A), p, m, -y) -----+ (A, p, [10 \n+\u00ad [S2]prJ]U, T) where (l., 11) = [Sl]pa ((Pair? v S1 5 2 Al A2)I P, ~17) + ((~7-~i5hm%lP~) -+ Al> AZ), \nPJ +\u00ad 11P)~ , 7) where (1) = new((v), a) = [1 + [S,p]o]a ((so s, . . .a S ~ ),p, o,~) -+ (A, P , o , \nvU {(q> P , ~ )}) where (q, (VI, . . . ,v~), A, p ) = [S,]po (Z17...7LJ = new((vl}., vn),~) P = [Vi,...,vn+ll,ln]pln]p \n0 = [t,, . . . ,ln + [S,]po , . . . . [Sn]pa]a [C]po = c [V]pa = a(p(v)) [(lambda q (q . . . v~) A)]po \n= (~, (vi, . . ..v~). A,P) Figure 4: Collecting machine chine, on the other hand, the locations are \ndivided into a finite number of partitions with one representative lo\u00adcation per partition. The store \nis thus rendered finite, but the value bound to each representative must ap\u00ad proximate all of the values \nbound to the locations in its partition. Figure 5 defines the abstract machine. An abstract store maps \na location to an abstract value. An ab\u00adstract value is a set of objects, and each object is ei\u00adther an \nabstract literal, pair, or closure. The domain of possible stores for a given program forms a com\u00adplete \npartial order (S~es,~) where ~ is defined as follows. Given two abstract stores 6 and 61, 60 E al if \nand only if dom(60 ) ~ dom(dl ) and for all v ~ dorn(@o), 60(v) c 61 (v). Similarly, 80 u 61 = 62 where \ndom(6z) = dom(bo) U dom(dl ) and for all v E dom(6z), 62(v) = 60(v) u 61 (v). Given a variable v and \nstores 60 and 61, 60(v) H 81 (v) is defined to be do(v) U 61 (v) if v c dom(bo) and v 6 dom(bl). If v \n@ dom(60) then 60(v) u 61 (v) = 61 (v), and the behavior when u Q dom(dl ) is symmetric. Given a source \nprogram A, the initial state of the machine is (A, 0,0, 0). The machine halts when the function pop is \napplied to an empty pending set X. As the machine executes, it builds a progressively more general cache \nuntil it is a safe approximation of the cache computed by the collecting machine. The execution rules \nof the abstract machine are sim\u00adilar to those of the collecting machine. The abstract machine rules must \nsimply take into account that a value is now a set of objects instead of a single ob\u00adject. Simple expressions \nare evaluated by injecting them into a singleton set using the abstraction function a to map~onstants \nin the domain Constants to the domain Constants. By customizing Q the analysis can be instan\u00adtiated to \nabstract over different properties of the basic datatypes. A cons expression evaluates to a singleton \nset consisting of one pair. Both the car and set-car! rules must anticipate their first argument evaluating \nto more than one pair. For a conditional, the test will evaluate to a set of objects, some of which may \nbe pairs and some of which may not be. The function true adds the true arm to the pending set if the \ntest value contains a pair. Likewise, ~ake does the same for the else arm if the test value cent ains \nsomething other than a pair. The application rule must anticipate the operator evaluating to a set of \nmultiple closures. The auxiliary 2 ~ S~es = Locations ~ 7( O~cts) i g o@ts = Con~ants U (Locations \nx Locations) U ( Tags x Variables x Programs x Environments) -j E Caches = ( Tags x Environments x Indices) \n% S%es E C Pendtng = Programs x Environments x S~es  ((let ((v (cons V1 uz S, Sz))) A), p,6 , ?, X) \n--+ (A, [v +-1]P,6 , ?, X) where (1,11,12) = n?i((v, vl, v2), p) = [1,1,)1, +-{(1,,1,)}, [S,]pb, [s2]p6]6 \n((let ((v (car S)~~ A), P,6, ?) ---+ (A, [v +-l]P, [1 +-6(10) U . . . u 6(l~)]t+, T,z) where (1) = n~w((v), \np) {10 ,...,ln} = {1 I (1,1 ) E [s]p@} ((set-car! SI S, A),p,6,T,X) -+ (A,P,[11,... ,ln + [s&#38;x?,..., \n[sz]po]b,~,z) where {11, . . . ,ln} = {1 I (1,1 } G [Sl]pb} ((pair? v sl L%Al A), P,~, ?, ~) -+ POP(?,he([sz]pd, \nAI, p , c? ,.false([Sz]pd, Az, /, 8 , z))) where (1) = n%((v), p) [v+ l]p ~r = [1+ [s,]p6] ((s0 s, . \n. sn),p, b,?, x) + pop(~ , x ) P = where {;l, ...,&#38;} = {(n, ~,A>P) I (q, fi, A>P) E [SO]8} = ([s,]pti, \n. . . . [sn]p6) (~ , X ; = apply(t~, 7,6,... apply(~n, ~, 6, (T, ~))) ((halt S), 6, ~, Y) + pop(~, X) \n [C]pb = {c!(c)} [V]pf? = b(p(v)) [(lambda rj (vl . . . v~) A)]p(? = {(q, (vi, ... vn), A, P)} apply((q, \n(vI, . . ,vn))A, P), (&#38;>. >:n)>~,(?>~)) = (? ,?(n, p , r(~ )) = ? (~,p ,m(b )) + ~,~U {( A,p , #)}) \nwhere (11, , . . ,1~) = nFZZw((vl,. ... vn), p) = [v, ,.. .,vn+ll, ln]pln]pP ~/ o = O([ll,...,ln +:l,; \nn] b)n]b)  -/ _ -[(~,P ,T(~ )) +-((T?, P , ~(~ )) 6 dom(?) -+ ?((77, p ,7r(#))) u 6 ,6 )]T POP(?,{( \nAP.6)} Ul) = (A, P,6, ?, E {( A, P,6)}) true(t, A, p, i?, X) = ? n ( Varzables x Vartables) = 0 + Z, \nZ U {(A, p, d)} false(;, A, p, 6, Z) = ; -( Variables x Varzables) = 0 -+ X,X U {(A, p, 6)} Figure 5: \nAbstract machine function apply is used to apply each closure to its argu-new, n?iw is a deterministic \nfunction that returns a se\u00adments. The environment and store are extended to bind quence of locations \nthat can be used to create new bind\u00adthe arguments, and the cache is updated appropriately. ings. ~w is \nrestricted, however, to deliver only a finite If the cache entry changes, the context of the closure \ns number of locations. Although the range must be finite, application has changed, and the closure s \ncode needs n~w can still affect the accuracy of the machine. Assum\u00adto be evaluated in the updated execution \ncontext. Since ing the program has been a-converted, a OCFA analysis the machine can process only one \nexpression at a time, can be obtained by setting Locatzons = Variables and the machine maintains a pending \nset, Z of expression, setting n7iw to be the identity function on its first ar\u00adenvironment, store triples \nawaiting evaluation. apply gument. A more accurate analysis can be obtained, adds the changed triple \nto Z. however, by using a more discriminating function. For The function n=w guarantees that the machine \ns en-example, a 1CFA analysis can be obtained by modify\u00advironments and abstract stores are finite and \nis also used ing n~w to allocate different locations depending on the to partially regulate the accuracy \nof the machine. Like textual context of the environment extension. The accuracy of the machine is also \naffected by the polyvariant function n used by apply. n is a partition\u00ading function that maps an abstract \nstore to an element of the finite set Indices. Assuming that n?iw is defined appropriately, setting n \nto the constant function would yield a OCFA analysis. Similar to iRw, a more accu\u00adrate analysis can be \nobtained by a nontrivial partition\u00ading. For example, a binding-time analysis may choose to partition \nthe continuation of an assignment based on whether the variable was assigned a static or dynamic value. \nWhile FiZi3 and ~ are used to increase the accuracy of the analysis, the projection operator @ is used \nby apply to increase t~speed. &#38;projection operator @ is a&#38;nc\u00adtion from Stores to Stores such \nthat for all 6 ~ Stores, 6 ~ @(6). By projecting an abstract store to a more general abstract store, \nthe convergence to a stable cache is accelerated. Using a projection operator may cause the analysis \nto generalize beyond the most accurate so\u00adlution, i.e., the least fixed point, but in exchange the analysis \nmay be faster. There are two extremes for projection operators. At one end the projection operator can \nbe just the identify function, and the accuracy of the analysis is not sacri\u00adficed at all. At the other \nend, @ can be defined such that for all 6, 0(6) = T. This yields a very fast analysis that collects little \nuseful information. An example of a more useful projection operator is given in Section 3. 2.3 An example \n Consider the canonical Scheme program in Figure 3 and assume a OCFA analysis in which the abstraction \nfunc\u00adtion Q is the identity function and SW is the identity function on its first argument. Upon termination \nof the abstract, machine, a portion of the abstract store on en\u00adtry to the initial continuation would \nlook like: 06 = {(V,,, V,2), ()} VI 1 = {1,2} VI 2 = {(?J,,, ?J,2), ()} In particular, the final answer \nis a list of indeter\u00adminable length, and the car of each pair is either 1 or 2. Given that the exact \nanswer is the list (1 2), this ab\u00adstract value is indeed a safe approximation of the exact answer.  \n2.4 Correctness The correctness of the abstract machine has two as\u00adpects: termination and safety. In \nparticular, the ma\u00adchine should terminate for all input programs, and upon termination, the machine s \ncache should be a safe ap\u00adproximation to the collecting machine s cache. For a given program A and finite \ndomain Con~ants, the machine terminates when the pending set is empty, so the machine terminates only \nif a finite number of program points are added to the pending set, Since n?iw produces a finite number \nof locations, the stores manipulated by the machine are finite and form a com\u00adplete partial order (CPO) \nunder sqsubseteq. Since the program is finite, Indices is finite, and the number of abstract stores is \nfinite, there are only a finite num\u00adber of program points, and Since the store at a pro\u00adgram point changes \nmonotonically, only a finite num\u00adber of programs points can be added to the pending set. Hence, the machine \nalways terminates. The correctness of the analysis is characterized with the following theorem. Theorem \n1 Let A be a program in canonical Scheme, and let (A, p, o, ~) and (A, p, 6, ~, 0) be halt states of \nthe collecting and abstract machines respectively such that (A, 0,0, 0) -+ (A, p, o, -y) in the collecting \nmachine and (A, 0,0, 0,0) -+ (A, p , 6, ~, 0) in the abstract machine. Then J is a safe approximation \nof ~. Proof Sketch: The proof proceeds using storage lay\u00adout relations [21], which is a proof technique \nfor prov\u00ading that one machine is a refinement of another. In this case, the abstract machine is a refmment \nof the collect\u00ading machine. The proof proceeds by induction on the execution steps of the collecting \nand abstract machines. The induction hypothesis states that on each step, the abstract machine maintains \na cache and pending set that approximates the cache of the collecting machine. 2.5 Discussion Not surprisingly, \nthe abstract machine can be param\u00adeterized to emulate the collecting machine. The nFiw function must \nbe modified to mimic new, a constant function is used for n-, and the identity function is used for the \nprojection operator 0 and the abstraction op\u00aderator a. Under this instantiation, all abstract values \nare singleton sets cent aining precisely one object. The pending set X is always empty on procedure entry, \nbut one triple is added to it on procedure application or a conditional test. This one element is immediately \nre\u00admoved by pop. The abstract machine does not explicitly use type tests to constrain abstract values. \nUsually, the con\u00adstraint is accomplished by rebinding the tested variable to a new location in each arm \nof the conditional and filling the locations with the constrained values of the variable. When control \nleaves the lexical context of the conditional, the variable reverts to its former binding. This strategy \nis easily incorporated into the abstract machine. Constraining abstract values in the arms of condi\u00adtionals \nis useful, but more can be done. Consider these two expressions that may result from macro expansion \nor procedure inlining. (lambda (z) (if (pair? z) #f (error)) (if (pair? z) (car z) (error))) (lambda \n(z) (+ (if (positive? z) z (abs z)) (if (positive? z) ~ (abs z)))) Assuming nothing is known about how \nthese proce\u00addures are used, the redundant type checks in both ex\u00adamples cannot be eliminated using the \nabove strategy for constraining abstract values. Because the abstract machine retains a store, it is \npossible to mutate the store to constrain abstract val\u00adues beyond the lexical scope of conditionals. \nThe precise extent of the constraint is unclear, however, because of the environment problem [17, page \n67], which refers to the problem of a value being illegally constrained in some contexts because of the \nfact that multiple environ\u00adments are folded together to render the analysis com\u00adputable. Nevertheless, \nit is at least possible to lengthen the extent of a constrained value until control leaves the procedure \nin which the value is constrained. In partic\u00adular, doing so enables the redundant test,s in the above \ntwo examples to be removed in a type-check elimination pass. Using both iiZw and T to regulate the accuracy \nof the analysis is not redundant. The two operators have different properties. FiiZWis able to partition \na variable s value at the point the variable is bound. This is done by binding the variable to different \nlocations in the en\u00advironment and storing the segregated values in different locations. Splitting in \nthis way has the advantage that the segregation survives join points. The ~ function, on the other hand, \nis able to partition the values of any lo\u00adcation at any time. This is more general, but the price is \nthat the segregation does not survive join points. Both operators are useful. nii has been used to seg\u00adregate \nlet-bound variables by type, leading to effective elimination of run-time type checks [15]. It has also \nbeen used to segregate the components of pairs to en\u00adable good specialization in off-line partial evaluation \n[9]. In general, niiw is useful for opportunistic polyvari\u00adance since segregation happens at the binding \npoint. n, on the other hand, is more useful for by-need poly\u00advariance to segregate values closer to the \npoint at which they are used. Since the segregation is more likely to happen only when it is profitable, \nby-need polyvariance may lead to cheaper but nevertheless effective analyses. 3 Implementation A preliminary \nimplementation of the analysis is used in the Chez Scheme [7] compiler to justify program op\u00adtimization. \nThe compiler processes R4RS Scheme [8] and directly supports multiple return values [I] and a variable \narity procedure interface [11]. As defined in Section 2, the analysis can process only closed programs, \ni.e., programs with no free variables other than recognized primitives. This is unacceptable for a realistic \nimplementation, since this restriction im\u00adplies that program parts cannot be analyzed in isola\u00adtion. \nThe implementation of the analysis therefore han\u00addles free variable references by introducing a unique \nab\u00ad stract value {unknown} to denote an unknown value. In the abstract machine, a free variab le reference \nevaluates to {unknown}. When unknown is used as a closure in an application, the arguments of the application \nmust escape. The con\u00adsequences of the escape depends on the escaped value. If the value is a procedure, \nthe analysis must assume that the procedure is applied to arguments that have the abstract value {unknown}. \nFor data structures, the values bound to all accessible locations also escape, and furthermore, the analysis \nmust conservatively assume that in the continuation of the call every mutable loca\u00adtion has the abstract \nvalue {unknown}. The goal of our initial experiments is to determine whether analyses that are faster \nbut less precise than OCFA can justify useful optimizations. That OCFA is useful has been established \nin previous work [13, 14, 15, 20]. Since the analysis is targeted for both interactive and noninteractive \nuse, however, it is important to know if a faster but less accurate instantiation of the analysis is \nstill useful. To determine this, the analysis is instantiated to em\u00adploy the identity function for @w \nand a nontrivial pro\u00adjection operator. The projection operator tracks the number of times the cache has \nbeen updated at each program point. If the number exceeds a threshold n for some point, the cache at \nthat point is not updated with the new store. Instead, the new values responsible for the update are \nconsidered escaping, and {unknown} is substituted in their place. The only exception is that unknown \nis not added to the values of let-and letrec\u00adbound variables in the source program. This is safe since \nthe values of those variables can never change. If the threshold is n, this projection operator limits \nthe analysis to a worst case of n + 1 passes over the pro\u00adgram, Setting n = O results in a one-pass intraprocedu\u00adral \nanalysis. Setting n = co results in a OCFA analysis. The information lost when the analysis is forced \nto generalize is not severe. There are two ways in which the loss is contained. First, only the information \nabout unstable portions of the code is generalized beyond the least fixpoint. For example, suppose n \n= 5 and the following program is analyzed, Benchmark Compiler DDD Similix SoftScheme Texer lines Description \n30,000 Chez Scheme recompiling itself 15,000 hardware derivation system [3] deriving Scheme machine 7,000 \nself-application of the Similix [2] partial evaluator 10,000 Wright s soft typer [23] checking a 2,500 \nline program 3,000 Scheme pretty-printer with ~ output [6] Table 1: Benchmarks ((lambda (~ g) (/1) (g \n2)) (lambda (z) . ..) (lambda (y) . ..)) Also assume that the code for ~ stabilizes in three it\u00aderations \nand the code for g stabilizes in ten iterations. The flow analysis will be forced to generalize the infor\u00admation \nabout g, but it will not have to generalize the information about f since the control-and data-flow for \nf does not depend on g. The other way in which the information loss is con\u00adtained is that when the threshold \nis exceeded at a pro\u00adgram point, information about what is already known at that, point is not discarded. \nCombined with type re\u00adcovery [16, 19], the flow analysis can still yield useful information for program \ntransformations. For example, suppose n = 1 and the following program is analyzed. (letrec ((jac (lambda \n(z) (if (= zO) 1 (* Z(fac (-Zl))))))) (fat 5)) Upon termination of the analysis, the abstract value of \nx is {5, unknown} when control enters the body of fat. In the arms of the conditional, however, the value \nof z can be constrained to be {5, integer}. The anal\u00adysis also determines that fac is a procedure, and \nthe call (fat ( x 1))is recursive. An aggressive optimizer might also use the fact that x may be bound \nto 5 to generate code for a specialized version of fac in the case that its input is a fixnum. The compiler \nuses the results of the analysis to op\u00adtimize direct calls, eliminate some closures, and facili\u00adtate \nloop recognition. A direct call is a procedure call where the operator will always evaluate to the same \nprocedure. Direct, calls can be optimized by branch\u00ading directly to the code for the called procedure \nand in some cases avoiding loading the procedure s closure pointer. GeneraTed code can avoid building \na closure if it is not needed. A closure is not needed if it does not have free variables, it does not \nescape, and all calls to it are direct calls. Loop recognition is done syntactically with the results \nof the analysis used to verify that all recursive calls to the loop head are in tail position. For each \nbenchmark in Table 1, Table 2 shows how much optimization was enabled by the flow analysis at n = O and \nn = m. For each of the three optimizations, the number of candidates considered is given along with the \nnumber optimized at n = O and n = m. For detect\u00ading direct calls, the number of candidates is the num\u00adber \nof applications in the benchmark code. For closure elimination, the number of candidates is the number \nof let-and letrec-bound lambda expressions. For loop detection, the number of candidates is the number \nof potential loops recognized syntactically. Even though the analysis is much more accurate at n = m, \nthe more precise information was not sufficient to enable significantly more optimization. This is a \ncon\u00adsequence of the chosen optimizations which are partic\u00adularly sensitive to the accuracy of the analysis. \nFor ex\u00adample, to make a direct call the analysis must be able to determine that exactly one procedure \narrives at a call site. It makes no difference if a more accurate analysis can reduce the number of procedures \nfrom four to two; the information is still not precise enough. For other optimizations such as type-check \nelimination, the more accurate information collected at n = co versus n = O would likely justify the \nelimination of significantly more checks. Table 3 shows the analysis costs and the benefits of the optimizations \nfor each benchmark. The analysis cost includes the time necessary to rewrite the source program to canonical \nform, to run the analysis, to an\u00adnotate the program with the collected flow information, and to convert \nthe program back to the compiler s in\u00adtermediate form. The stock compiler justifies the op\u00adtimization \nusing the results of a complex and ad hoc analysis. That compiler was modified by eliminating the analysis \nand restructuring the compiler to incorpo\u00adrate the new flow analysis. At n = O, the flow anal\u00adysis recovers \nthe same information as the ad hoc anal\u00adysis. The speedups were computed by comparing op\u00ad timized versions \nof each benchmark against an unop\u00ad timized baseline. The difference in flow information closures eliminated \ndirect calls loops recognized max n=() n.~ max n=() n.~ max n=o n=~ compiler 1955 536 571 18310 4893 \n4902 438 311 311 ddd 778 156 186 4967 627 617 147 58 58 similix 865 225 231 7047 1789 1808 100 46 46 \nsoftscheme 971 19 21 10001 771 772 57 13 13 texer 134 20 21 1075 304 304 48 39 39 Table 2: Thetable gives \nthe number of closures eliminated, direct calls recognized, andloops recognized at n=O and n = m. The \nmaximum numbers in each case indicate the number of candidates considered. compile time (see) analysis \ntot al run-time n=o n.~ n=o n.~ speedup compiler 11.4 34.0 42.7 65.7 18 % ddd 1.7 1.9 9.0 9.3 4% similix \n3.4 8.0 15.2 19.7 1% softscheme 7.9 12.4 51.0 53.9 6% texer 0.7 1.8 2.6 3.8 23% Table 3: The table measures \nthe compile-time cost of enabling the optimization computed by the analysis at n = O and n = m had an \nimmeasurable impact on the speedups of the opti\u00admized benchmarks. Hence, the average of the speedups \nfor n = Oand n = cc is reported. These preliminary results are very encouraging. On average the cost \nof the analysis at n = O was 21 YOof compile time and at n = cc it was 38% of compile time. These numbers \nare not unreasonable considering that the implementation is preliminary and not optimized, and that the \nstock compiler is tuned for compile-time speed. The projection operator chosen was successful in lowering \nthe analysis cost in every case and in some cases lowering it significantly. Despite such a coarse projection \noperator, the information collected was still able to justify optimizations Indeed, the extra informa\u00adtion \ncollected at 77 = cm had no practical impact on the compiler s ability to optimize the benchmarks. We \nmay conclude then that for some transformations, cheaper analyses can be used profitably. 4 Related \nwork Shivers [19] and Harrison [13] describe flow analyses for Scheme that are based on abstract interpretation. \nThey differ primarily in the details of the source language and the range of accuracy they can express. \nOur analysis draws from the advantages of each approach. Like Shiv\u00aders . we use CPS to make control transfers \nexplicit. Like Harrison s, primitive operations are ordered and bound the analysis and the run-time \nperformance increase aft er to temporary variables, and an abstract program state is computed for each \nbasic block of the program. Our analysis subsumes their analyses by expressing a wider range of polyvariance \nand using projection to acceler\u00ad ate convergence to a fixpoint. Also, their analyses are prototypes that \ndo not handle the full language, while our analysis is completely implemented and can analyze arbitrary \nScheme programs. Jagannathan and Weeks [15] describe an analysis for the polyvariant analysis of higher-order \napplicative pro\u00adgrams that also accommodates side-effects and call/cc. Their analysis is parameterized \nover a polyvariance op\u00aderator, but it is not parameterized over a projection operator. Also, their characterization \nof program state is different from ours. In our analysis, the program state is an environment and store. \nIn their analysis, the program state is an environment mapping variables to locations in a global store. \nTheir characterization of program state has consequences on polyvariance and the accuracy of assignment. \nAn assignment to a lo\u00adcation pollutes the location s dataflow by adding the assigned value to the dataflow \nat the point at which the location is bound instead of at the point of assign\u00adment. In our analysis, \nthe assigned value replaces the old value of the location in the continuation of the as\u00adsignment. With \nrespect to polyvariance, their approach limits how program points can be split, since it is not possible \nto use the n function to split on arbitrary loca\u00adtions at any time. In both cases, our approach implies \n that more accurate analyses can beexpressecl. Other researchers have observed that projection op\u00aderators \ncan be used in practice to reduce the number of iterations needed to stabilize. Yi and Harrison [24] \ndescribe a framework for the automatic generation of abstract interpreters. This framework incorporates \na notion of projection that is similar to ours. The differ\u00adence is that, they apply projections to values, \nand we apply them to the entire computation state. Further\u00ad more. they always project a value to the \ntop of the value lattice, while we permit the operator to project a value to any other value above it \nin the lattice. Bourdon\u00adcle [4, 5] uses widening operators in a theoretical set\u00adting to accelerate convergence \nto a solution. A widening operator V is a substitute for the least upper bound op\u00aderator. The constraint \nis that given two points z and ~, ~ u y ~ .zVy. Both of these approaches the promise the potential to \naccelerate the analysis without losing too much information. Our work delivers on that promise by exhibiting \na practical projection operator that still enables useful optimizations. Conclusions A problem with \nincremental compilers is that they must often sacrifice generated code quality for compile-time speed. \nThere are two obvious solutions to the problem. One is to use a batch mode compiler when develop\u00adment \nis complete in orde~ to generate good code. The other is to selectively turn on and off optimizations. \nThe disadvantage of the first solution is that two compilers must be written and maintained. The disadvantage \nof the second is that it is harder to detect undesirable in\u00adteractions among optimization passes. The \nflexibility of our analysis suggests the alterna\u00adti~e strategy of always doing the flow analysis neces\u00adsary \nfor optimizations. During development, however, the flow analysis is run with a coarse projection opera\u00adtor. \nAs a result, the analyzer is fast, but the compiler sometimes does not have enough information to perform \noptimizations. When the development cycle has ended, the coarse projection operator is replaced with \nthe iden\u00adtity operator, allowing the analysis to collect more pre\u00adcise information and perform stronger \noptimizations. It is not strictly necessary to translate source pro\u00adgrams to canonical form before analysis \nbut doing so has several advantages. First, it simplifies the abstract machine considerably. In some \nsense, the translation step can be seen as deriving a set of constraints, and the abstract machine simply \nviews the constraints as an executable specification. Also, because the machine makes the solution algorithm \nexplicit, an improvement w the algorithm can be proven correct by implementing the improvement as a revised \nstate machine and then proving that the revised machine is a correct refinment of the original machine. \nTranslating to a canonical lan\u00adguage is also an advantage in situations where the fSow analysis is the \ncore analysis engine for several optimiza\u00adtion in a compiler. Typically, a compiler will alter the abstract \nsyntax tree of the program on each step of the compilation. By compiling to an intermediate form and \nthen performing the flow analysis, the differences in the intermediate representations are isolated from \nthe anal\u00adysis. Adding a module system to the language would im\u00adprove the accuracy of the analysis. Currently, \nthe an\u00adalyzer assumes no information about the values of free variables in programs. While correct, this \nis not very satisfactory. With a module system, the analyzer could save the abstract values of exported \nbindings for later use during the analysis of an importing module. The analysis of the importing module \ncould then infer more accurate irrformation. References [1] J. Michael Ashley and R. Kent Dybvig. An \neffi\u00adcient implementation of multiple return values in Scheme. In Proceedings of the 1994 ACM Confer\u00adence \non LISP and Funcihonal Programming, pages 140-149, 1994. [2] Anders Bondorf. Similix Manual, System Version \n5.0. DIKU, University of Copenhagen, Denmark, 1993. [3] Bhaskar Bose. DDD A transformation system for \nDigital Design Derivation. Technical Report 331, Indiana University, Computer Science Department, May \n1991. [4] Francois Bourdoncle. Abstract interpretation by dynamic partitioning. Journal of Functional \nPro\u00adgramming, 2(4) :407 436, October 1992. [5] Francois Bourdoncle. Efficient chaotic iteration strategies \nwith widening. In Proceedings of the In\u00adternational Conference on Formal Methods in Pro\u00adgramming and \ntheir Applications, volume 735 of Lecture Notes in Computer Science, pages 128-141. Springer-Verlag, \n1993. [6] Robert G. Burger. The Scheme machine, Technical Report 413, Indiana University, Computer Science \nDepartment, August 1994. [7] Cadence Research Systems, Bloomington, Indiana. Chez Scheme System Manual, \nRev. 2.4, July 1994. [8] William Clinger and Jonathan Rees (editors). Revised* report on the algorithmic \nlanguage [9] [10] [11] [12] [13] [14] [15] [16] [17] [18] Scheme. Lisp Pointers, 5(3):1 55, July-September \n1991. Charles Consel. Polyvariant binding-time anal\u00adysis for higher-order, applicative languages. In \nProceedings of the Symposium on Partial Evalua\u00adtion and Semantics-Based Program Manipulation, PEPM 93, \npages 66-77, 1993. Patrick Cousot and Rhadia Cousot. Abstract in\u00adterpretation: a unified lattice model \nfor static anal\u00adysis of programs by construction or approximation of fixpoints. In Conference Record \nof the Fourth ACM Symposzum on Principles of Programming Languages, pages 238-252, 1977. R. Kent Dybvig \nand Robert Hieb. A new approach to procedures with variable arity. Lisp and Sym\u00adbolic Computation, 3(3):229-244, \nSeptember 1990. Cormac Flanagan, Amr Sabry, Bruce F. Dubs, and Matthias Felleisen. The essence of compiling \nwith continuations. In Proceedings of the ACM SIG-PLAN 93 Conference on Programmmg Language Design and \nImplementation, pages 237-247. ACM, 1993. W. L. Harrison III. The interprocedural analysis and automatic \nparallelization of Scheme programs. Lisp and Symbol~c Computation, 2(3/4):179-396, 1989. Nevin Heintze. \nSet-based analysis of ML programs. In Proceedings of the 1994 ACM Conference on LISP and Functional Programming, \npages 306-317, 1994. Suresh Jagannathan and Stephen Weeks. A uni\u00adfied treatment of flow analysis in higher-order \nlan\u00adguages. In Proceedings of the ACM Symposium on Principles of Programming Languages, pages 393\u00ad407, \n1995. Suresh Jagannathan and Andrew Wright. Effec\u00adtive flow-analysis for avoiding runtime checks. In \nProceedings of the 1995 International Static Anal\u00adysis Symposmm, volume 983 of Lecture Notes in Computer \nSc?ence, pages 207 224. Springer-Verlag, September 1995. Peter Lee, editor. Topics in Advanced Language \nImplementation. MIT Press, 1991. Zhong Shao and Andrew W. Appel. Space-efficient closure representations. \nIn Proceedings of the 1994 ACM Conference on LISP and Functional Pro\u00adgramming, pages 130-161, 1994. [19] \nOlin Shivers. Control-flow analysis of higher-order languages. PhD thesis, Carnegie Mellon University, \n1991. CMU-CS-91-145. [20] Olin Shivers. The semantics of Scheme control-flow analysis. In Proceedings \nof the Symposium on Par\u00adttal Evaluation and Semantzcs-based Program Ma\u00admpulatzon, PEPM 91, New Haven, \nConnecticut, 1991. [21] Mitchell Wand and Dino P. Oliva. Proving the cor\u00ad rectness of storage representations. \nIn Proceedings of the 1992 ACM Conference on LISP and Func\u00ad tional Programming, pages 1.5 160, 1992. \n [22] Mitchell Wand and Paul Steckler. Selective and lightweight closure conversion. In Proceedings of \nthe ACM Symposium on Principles of Program\u00adming Languages, pages 435 445, 1994. [23] Andrew K. Wright \nand Robert Cartwright. A prac\u00adtical soft type system for Scheme. In Proceedings of the 1994 ACM Conference \non LISP and Functional Programming, pages 250-262, 1994. [24] Kwangkeun Yi and William Ludwell Harrison \nIII. Automatic generation and management of inter\u00adprocedural program analyses. In Proceedings of the \nACM Symposium on Principles of Programming Languages, pages 246-259. ACM, 1993.  \n\t\t\t", "proc_id": "237721", "abstract": "", "authors": [{"name": "J. Michael Ashley", "author_profile_id": "81332488447", "affiliation": "Snow Hall 415, Lawrence, KS, 66045 and Computer Science Department, Indiana University, Bloomington, Indiana", "person_id": "PP39069151", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/237721.237773", "year": "1996", "article_id": "237773", "conference": "POPL", "title": "A practical and flexible flow analysis for higher-order languages", "url": "http://dl.acm.org/citation.cfm?id=237773"}