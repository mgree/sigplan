{"article_publication_date": "01-01-1996", "fulltext": "\n A General Approach for Run-Time Specialization and its Application to C Charles Consel Frangois Noel \nUniversity of Rennes / Irisa Campus Universitaire de Beaulieu 35042 Rennes Cedex, France {cOnsel,fnoel}@ \nirisa. fr Abstract Specializing programs with respect to run-time invariants is an optimization technique \nthat has shown to improve the performance of programs substantially. It allows a program to adapt to \nexecution contexts that are valid for a limited time. Run-time specialization is being actively investigated \nin a variety of areas. For example, recently, major operating system research projects have been focusing \non run-time specialization as a means to obtain efficiency from highly extensible and parameterized systems. \nThis paper describes a general approach to rrm-time spe\u00adcialization. For a given program and a declaration \nof its run\u00adtime invariants, it automatically produces source templates at compile time, and transforms \nthem so that they can be processed by a standard compiler. At run time, only mi\u00adnor operations need to \nbe performed: selecting and copying templates, filling holes with run-time values, and relocating jump \ntargets. As a consequence, run-time specialization is performed very efficiently and thus does not require \nthe spe\u00adcialized code to be executed many times before its cost is amortized. Our approach improves on \nprevious work in that: (1) templates are automatically produced from the source pro\u00adgram and its invariants, \n(2) the approach is not machine dependent, (3) it is formally defined and proved correct, (4) it is efficient, \nas shown by our implemen tation for the C language. Introduction Specializing programs at run time \nwith respect to dynamic invariants is an optimization technique that has already been explored in various \nareas such as operating systems [16] and graphics [14]. This technique is aimed at adapting programs \nto execution contexts by using run-time invariants. In the context of file system operations, examples \nof run\u00adtime invariants include the type of the file being opened, the device where it resides, and whether \nit is exclusively read. When a file is being opened, at run time, invariants become available and can \nbe exploited to specialize read and/or write routines. As reported by Pu et al. this specializa- Perrnission \nto make digital/had copies of all or part of this material for personal or classroom use is granted without \nfee provided that the copies a.m not made or distributed for profit or commercial advantage, the copy\u00adrtgbt \nnotice, the title of the publication and its date appear, and notice is given that copyright is by permission \nof the ACM, In.. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires \nspecific permission and/or fee. POPL 96, St. Petersburg FLA USA @ 1996 ACM 0-89791 -769-3/95/01.. $3.50 \ntion eliminates redundant interpretation of data structures and yields significant improvements [15]. \nIn fact, various forms of run-time specializations have been studied on practical systems, and substantial \nimprove\u00adments have been reported. Locanthi et al., for example, applied specialization to the bitblit \nprocedure [14, 12]; their specialized code ran about 4 times faster than a generic im\u00adplementation, In \nthe area of operating system, Massalin and Pu designed an operating system ~hich utilized run\u00adtime specialization \nas a fundamental technique to optimize a wide variety of system components. They report speedup factors \nthat range from 2 to 40 depending on the system component considered [13]. Although various forms of \nrun-time specializations have undoubtedly been shown to improve substantially the per\u00adformance of programs, \nthe specialization process has always been done manually [9]. The usual approach consists of defining \ncode templates, that is, code fragments parametri\u00adzed with respect to run-time values. Then at run time, \ntemplates are linked together depending on the control flow, and holes (Z.e., template parameters) are \nfilled with run-time values [10]. To minimize the cost of run-time specialization, templates are often \nrepresented in a binary form to avoid invoking an assembler, or even more expensive, a complete compiler, \nat run time. While the idea of run-time specialization is certainly at\u00adtractive, considering the degree \nof improvement it can yield, the approaches explored so far have fundamental drawbacks. They are manual. \nUsually templates are written by the programmer either directly in some low level language, or using \nsome syntactic facilities [6].  They are not clearly defined. Although existing ap\u00adproaches have shown \ntheir effectiveness, the process of run-time specialization has always been presented as a black-box; \nonly the functionalities were described not the techniques.  They are not portable. When templates are \nwritten in assembly language, they are limited to a given proces\u00adsor. Often, templates have to be optimized \nmanually to obtain good performance.  They are error-prone. Because templates are directly written by \nthe programmer in a low-level language, errors may easily be introduced.  In this paper we present \na general approach for run-time specialization and its application to the C programming lan\u00ad guage. Our \napproach can be decomposed in the follow\u00ading main stages. At compile time, a program is analyzed for \na given context of invariants declared by the program\u00admer. This analysis determines the program transformation \nto be performed for every syntactic construct in the pro\u00adgram. This information is used by a subsequent \nanalysis to produce a safe approximation of the possible specializations of this program, in the form \nof a tree grammar. Then, this tree grammar isusedto generate templates autornaticallyat the source level. \nThese templates capture thedynamiccom\u00adputations, that is, the computations that rely on data that vary. \nOnce compiled by a standard compiler (in the case of the C language), various linking information (i. \ne., labels and holes) iscollected from the compiled templates. Finally, the parts of the program corresponding \nto the static computa\u00adtions, i.e., the computations that rely onthe invariants, are compiled; they represent \nthe mm-time specialize. When it is executed at run time, in addition to computing invariants, therun-time \nspecializer also selects templates depending on the control flow, relocates jump targets, and fills template \nholes with the invariant values. Our approach has many advantages compared to the ex\u00adisting ones. * It \nis automatic. Templates are automatically gener\u00adated from a description of the possible specializations \nof a program, itself produced by an analysis. e It is formally based. We have formally defined the approach \nfor a subset of an imperative language and proved it correct. * It is general. In principle, our approach \napplies to a variety of languages, from imperative to applicative ones.  o It is portable. In our approach, \nmost of the special\u00adization process is in fact performed at the source level. Only minor operations in \nthe linking phase of tem\u00adplates need to be ported. These operations are limited to collecting locations \nof template holes and jump la\u00adbels within templates. It is efficient. we have applied the approach to \nthe C language: an implementation of a run-time specialize of C programs has been developed. It has been \nused on various kinds of programs. The run-time specializa\u00adtion process incurs a negligible overhead. \nPreliminary experimentation shows that on procedures exhibiting a clear interpretive layer (e. g., variations \nof printf ), run-time specialized code requires as little as 3 runs to amortize the cost of specialization, \nand it executes 5 times faster than the non-specialized version. Our run-time specialization approach \nis based, in part, on partial evaluation technology [8, 3]. In fact, it is inte\u00adgrated in a complete \npartial evaluation system for C pro\u00adgrams that performs compile-time specialization as well as run-time \nspecialization [4]; this aspect is further discussed in Section 2. This system has been applied to various \nkinds of programs such as operating system code. Plan. In Section 2, the underlying concepts of partial \neval\u00aduation are reviewed. In Section 3, the approach and its main components are described; examples \nare used to illus\u00adtrate the presentation. In Section 4, the approach is for\u00admally defined and proved \ncorrect. Section 5 then discusses the related work. Finally, Section 6 gives some concluding remarks, \nand outlines the future directions of this work. 2 Partial Evaluation Partial evaluation is a program \ntransformation technique aimed at specializing a program with respect to some parts of its input [8, \n3]. There are two main strategies to perform partial evaluation. The first strategy, called on-line, \nconsists of specializing a program in a single pass. As the program gets processed, the program transformations \nare determined and performed. Because program transformation occurs in the presence of concrete values, \non-line partial evaluation achieves a high-degree of specialization. The second strategy to partial evaluation, \ncalled off-line, is composed of two parts: preprocessing and specialization. For a given program and \na description of its input (known or unknown), the preprocessing phase essentially compiles the specialization \nphase. It does so by determining a program transformation for each syntactic construct in the program. \nThen, the specialization phase is performed with respect to some partial input value. This process is \nsolely guided by the information produced by the preprocessing phase. As a consequence, specialization \nis efficient. Whether on-line or off-line, partial evaluation has always been studied and understood \nas a source-to-source program transformation. In this paper, we introduce an approach that goes beyond \nthis view. We propose to use partial eval\u00aduation as a basis for run-time specialization. In fact, this \nwork is part of a complete partial evaluation system which specializes C programs at compile time as \nwell as at run time [4]. Let us briefly outline the salient features of this system. Our partial evaluation \nsystem is based on an off-line strategy. The preprocessing phase mainly consists of an alias analysis, \na binding-time analysis, and an action analysis. The alias analysis is needed because of the pointer \nfacilities offered by the C language. The binding-time analysis deter\u00admines the binding-time property \nof the variables in a pro\u00adgram, given a program and a description of its context (z. e., global variables \nand formals are declared as static/known or dynamic/unknown). While the binding-time analysis deter\u00admines \nwhat to do for each syntactic construct in a program, the action analysis determines how to do it [2]. \nIn other words, binding-time information is used to determine what specialization actzon (z. e., program \ntransformation) should be performed. A small subset of the actions used for the C language is presented \nin Section 4. Once the actions of a program are produced, various back-ends can exploit this information. \nFirstly, they can be interpreted; this situation corresponds to a specialize. Sec\u00adondly, they can be \ncompiled to produce a dedicated special\u00adize (also called a generating extension [8]). Finally, actions \ncan be used to achieve run-time specialization. The last al\u00adternative comes from the fact that actions \ndefine program transformations and thus can be used as a basis to determine what specialized programs \nan action-analyzed program can yield. In fact, in our approach, an analysis of an action\u00adanalyzed program \nis performed to determine an approxima\u00adtion of this set of possible specialized programs; this set is \ndescribed as a tree grammar. 3 An Approach to Run-Time Specialization As mentioned in Section 2, run-time \nspecialization is one int f(int z, int y) { int f(int x, int y) { } int 1; 1=2*X; if(l==2) l=l+y; eIsel=y \nreturn 1; *z; } int 1; (1= 2 * z) ifled ( 1== lzd =.eb else lad = eb (return l) d ; ed 2 ) ~e. ~reb \ny%d * b ; ~zd z .r.b ; eb (a) Source program (b) Action-analyzed (o= static, y = program dynamic) Figure \n1: An example program of the three ways of exploiting action-analyzed programs. This section presents \nan approach to run-time specialization based on actions. Let us explain in detail how a program gets \nannotated wit h actions by taking a concrete example. Actions are in\u00addeed a key aspect of our approach: \nit is the starting point of the run-time specialization process. Figure l-a presents a source program, \nwritten in the C language. Figure l-b shows this program annotated with actions. For readability, the \naction-analyzed program is rep\u00adresented in concrete syntax and decorated with actions. Let us describe \nhow actions are determined for procedure f in the example program, assuming that its first parameter \nis static and its second parameter dynamic. The first command in the procedure is assigned action erml \n(ev). This action annotates completely static program fragments. Such program fragments represent computations \nthat solely depend on available data. Assuming that the symbol ; , at the end of the first command, is \na sequence construct, then it is assigned the action reduce (red) because the first command will be evaluated \naway and thus the se\u00adquence command will be reduced. Likewise, the conditional command can also be reduced \nbecause the value of the test expression can be determined at specialization-time. Still, the branches \nof the conditional command have to be rebuilt. More specifically, the assignment in the true branch has \nto be rebuilt (reb) because the right-hand side does not par\u00adtially evaluate to a constant. This is caused \nby variable y which is dynamic. As such, this variable represents a com\u00adpletely dynamic code fragment; \nit is annotated with action identity (id). This action denotes code fragments that can be reproduced \nverbatim in the specialized program. A sim\u00adilak situation occurs in the false branch of the conditional \ncommand. Finally, the return command is globally anno\u00adtated with id since it is uniformly dynamic. To \nspecialize a program at run time based on its ac\u00adtions, a naive approach would simply consist of postponing \nspecialization until run time, that is, when the specializa\u00adtion values become available. Then, the specialized \ncode would be compiled and dynamically linked to the running executable. The obvious drawback of this \napproach is the cost of compilation which would require the run-time spe\u00adcialized program to be run many \ntimes to amortize the cost of specialization, compilation and linking. In fact, the reason why the compilation \nof a particular specialized program has to be postponed until run time is because we do not know the \nset of possible specializations an action-analyzed program can yield. If we knew such a set, or a description \nof it, then it could be processed at compile time instead of run time. Unfortunately, the set of all \npossible specializations is in general infinite (because of loop unrolling, for example). 3.1 Using \nTree Grammars A traditional way to finitely represent an infinite set of trees is to use tree grammars. \nHowever, determining the exact set of the possible specializations of an action-analyzed pro\u00adgram is \nundecidable in general, since specialization values are unknown at compile time. Yet, an approximation \ncan be defined; it corresponds to the least superset of the exact set. It is safe to consider a tree \ngrammar that describes more specializations than the actual ones if they are ignored dur\u00ading run-time \nspecialization; more precisely, if no execution context leads to these specializations. We have developed \nan analysis aimed at computing a tree grammar, called speciahmtion grammar in this context, which represents \na safe approximation of the set of all pos\u00adsible specializations of an action-analyzed program. Let us \nconsider an example of a specialization grammar. Figure 2-a redisplays the action-analyzed procedure \nf and Figure 2-b shows its corresponding specialization grammar. Like action-analyzed programs, specialization \ngrammars are rep\u00adresented in concrete syntax. The first rule F describes the possible specializations \nof procedure ~. Unlike compile-time specialization, when a procedure is specialized at run time, it does \nnot need to be renamed. Indeed, during execution, templates have a bi\u00adnary format. Only code addresses \nare manipulated. Since local variable 1 is involved in some dynamic computations, it is residual, and \nthus its declaration remains in the spe\u00adcialized program. Directly following this declaration, in\u00adstead \nof the first command of the original procedure, the non-terminal S occurs. This non-terminal defines \nthe spe\u00adcializations of the conditional command. In fact, the first command in ~ is not part of the specialization \ngrammar be\u00adcause it is completely static (ev ); consequent ly, it will be evaluated at specialization-time. \nNext to the occurrence of the non-terminal S, the return command appears. It is iden\u00adtical to the command \nin the original program because it is completely dynamic (id). As for the conditional command described \nby rule S, it will be reduced at run time since its test expression is purely static. As a result, rule \nS is com\u00adposed of two alternatives, one for each branch. Each branch is an assignment to be rebuilt at \nrun time. However, each right-hand fiide of thetie assignments includes a completely static expression: \nvariables 1 and x. The integer values re\u00adsulting from their run-time evaluation are described by the \ngeneric terminal In&#38; it is a placeholder for integer values. int } f(int z, int int 1; (1= 2 * z) \n i~ed ( 1== ~td =reb else 1 d = b (return l) d y) { ; d 2 ) ~ev +reb y=d * b ; ~,d Z * .reb ; b F S \n+ -i I int f.t(int int 1; s return } l= Int+y; l=y*Int; y) 1; { (a) Action-analyzed program (b) Specialization \ngrammar Figure 2: Specialization grammar generated from an action-analyzed procedure At this stage it \nis imp&#38;tant to notice that a special\u00adpurpose compiler could be developed to process right-hand sides \nof specialization grammar rules. In other words, such a tool could compile incomplete syntax trees parameterized \nwith constant values. Compilation would be done statically and thus run-time specialization would mainly \namount to assembling binary fragments and instantiating them with respect to run-time values. Although \nthis method is pos\u00adsible, it requires one to develop a complete compiler. This compiler would necessitate \ntime and effort to be competitive with advanced optimizing compilers currently available. A better approach \nwould consist of modifying an existing com\u00adpiler. However, real-size compilers are not as modular as \nthey claim to be: significant modifications may propagate throughout most of the compilation system. \nSuch modifi\u00adcations may therefore not be necessarily much simpler than the previous approach. An even \nbetter approach consists of using an existing compiler as is. This is discussed in the next sections. \n3.2 Introducing Templates An existing compiler can be used to process right-hand sides of specialization \ngrammars. In this section we present a transformation process aimed at converting these right-hand sides \ninto source code fragments parameterized with run\u00adtime values. We call these fragments source templates. \nTem\u00adplate parameters are often called holes in the literature [10]. At run time, part of the specialization \nprocess consists of physically replacing these parameters by values. In other words, template holes are \nfilled with run-time values. The resulting object is called an instance of the template. Transforming \nspecialization grammars into source level templates mainly amounts to unparsing the right-hand side of \nthe grammar rules and delimiting individual templates. The former task is fairly straightforward. The \nonly interest\u00ading aspect is concerned with the treatment of generic termi\u00adnals. This representation for \nrun-time values is transformed into holes. The concrete repre~entation of ELhole depends on both the \nlanguage and the compiler being used. To abstract over these issues, holes are just given a unique name \nwithin brackets (for example [hi] in Figure 3). Delimiting templates can be done in several ways. Let \nus present two approaches. The first approach consists of creating one template per right-hand side in \na specialization grammar. For example, based on the specialization grammar presented in Figure 2\u00ad b, \nthe first approach would yield three templates as shown in Figure 3-a: one for procedure f, and one per \nalternative in the conditional command. Just like the right-hand side of F in the specialization grammar \nincludes non-terminal S, its corresponding template includes a reference to other templates. The template \nto be selected cannot be deter\u00admined at compile time since it depends on the value of the test expression \nof the conditional command. Therefore, a placeholder (that is, some reserved space) is introduced to \ninsert the selected template. Although conceptually simple, this approach may be costly in practice. \nIndeed, it assumes that the physical layout of template tlat run time includes enough space to insert \nei\u00adther template tzor template ts.If they have different sizes, the size of largest template is used. \nAs a result, placehold\u00aders for templates may have a large size. A more important drawback occurs if loops \nare unrolled. In this case, the size of the unrolled loop cannot be determined at compile time. The second \napproach is aimed at eliminating nested tem\u00adplates such that no space be reserved to insert templates. \nTo do so, when the right-hand side of a specialization grammar rule includes a non-terminal, a template \nis created before and after this non-terminal. This approach is illustrated by Figure 3-b. Template tl \nrepresents a first fragment of the specialized version of procedure f. Then, either template tz or tsis \nappended. Finally, template tqcompletes a special\u00adized procedure. Notice that for the formal definition \nof the run-time spe\u00adcialization process, the first approach is used to simplify the presentation and \nabstract over these implementation issues.  3.3 Compiling Templates Statically Once templates are identified \nand transformed into concrete syntax, they can be compiled. Because they are available at compile time, \nthey can be compiled then. They become object templates. Of course, the way templates are compiled depends \non the language in which they are written, and the compiler which is used. In this section we discuss \nthe generaI issues arising for template compilation, mostly independently of a specific language or compiler. \nSo far, the templates of a procedure have been described as separate entities. However, if templates \nwere to be com\u00ad piled separately, the quality of the code would be poor since the compilation process \nwould not take advantage of the context in which they appear. Some compilation aspects such as register \nallocation and instruction scheduling would undoubtedly suffer from this situation. To circumvent this \nproblem, our approach consists of constructing a source code int f.t(int y) { tl int 1;   E3TEl \nk?&#38;q 1 = Ih l]+y; 1 = [hlj+y; 1 =y* [hZ]; (b) Approach 2 (a) Approach 1 I Figure 3: Templates for \nprocedure f void rt_spec-f(int x) { int l; case 1: dump-template(tl ); 1 [ hl +Y; t2 1=2*X; break; \nif(l== 2){ case 2: dump-template (tz); instantiate_hole( tz, 1); * 4 } else{ dump_template(ts); instantiate_hole \n(ts, r); , I 1I } dump-template (tl); Figure 4: Source representation of templates } Figure 5: Run-time \nspecialize for j I that combines all the templates and still expresses the un\u00adknowns as far as how exactly \nthese templates can be assem\u00adbled at run time. A concrete example of this transformation 3.4 Producing \nthe Run-Time Specialize is presented in Figure 4. Now that templates have been generated, compiled and \nex-As can be noticed the source representation of templates tracted from the object code, and that information \nneeded for procedure ~ follows the structure of the specialization to instantiate them has been collected, \nwe are ready to pro\u00adgrammar. In particular, because we do not know prior to duce the run-time specialize. \nThis procedure consists of run time which alternative of the conditional command will eval fragments \ninterleaved with operations aimed at select\u00adbe included in the specialized version of procedure ~, both \ning and dumping templates, filling holes wit h run-time val\u00adalternatives are included in a switch command \nwhose test ues, and relocating jump targets. The run-time specializevalue is unknown (variable unknown) \nto the compiler. This is generated based on an action-analyzed program. layout is directly derived from \nthe specialization grammar. Figure 5 displays the run-time specialize. for procedureEven though there \nis this unknown, the compiler can still ~. The control flow of this procedure can be seen as a subset \nprocess the templates globally, in that it knows the possible of the control flow of the original procedure \nin the sense that combinations that can occur. In fact, the source represen\u00adonly the static parts of \nthe original control flow graph appeartation of templates includes some form of markers around in the \nrun-time specialize. templates so that they can be identified and extracted from Since parameter x in \nthe original procedure was declared the object code. Object templates are used at run time by as static, \nit appears as a parameter of the run-time spe\u00adthe specialize.. cialize.. Local variable 1 was involved \nboth in static and Finding an appropriate representation for template holes dynamic computations. Therefore \nit appears in both a t em\u00adis an issue that depends on both the language and the com\u00adplate and the run-time \nspecialize.. The first operation of piler being studied. the run-time specialize. is to dump template \ntl, which is Once templates are compiled, information from the re\u00adthe header of the specialized procedure. \nThe first command sulting object code must be collected: the address of tem\u00adof the original procedure \ncan then be executed since it is plate holes needs to be recorded so that the run-time special\u00adpurely \nstatic. Next, the conditional command is executed. ize. knows where values need to be installed. Also, \ntemplate The test expression can be fully evaluated; the resultingaddresses have to be determined so \nthat jumps can be relo\u00advalue determines whether the first or the second templatecated if needed. should \nbe dumped. The dumped template is then instanti\u00adated with the appropriate run-time value. Finally template \nt4is dumped; it corresponds to the purely dynamic return command, and thus does not require any instantiation. \nAs can be noticed, the operations to perform the actual specialization are very simple and introduce \nlittle overhead at run time. Relocation of jump targets and hole filling are compiled. Copying of templates \ncan be implemented very efficiently on some processors provided their memory layout is carefully done. \nThe result of an invocation of the run-time specialize is a specialized code ready to be used. In our \nimplementation, the last operation of the run-time specialize consists of re\u00adturning the address of the \nspecialized code. For a procedure, it returns a procedure pointer which can then be invoked. 4 Semantic \nDefinition of Run-Time Specialization In this section, an imperative languague is introduced and its \nsemantics is defined. Then, a set of specialization actions for this language, as well as their semantics, \nare presented. Also, the semantic definition of the process of generating run-time specializes is given. \nFinally, the correctness criterion for this latter process is stated. It establishes that specializing \nprograms by interpreting actions, or by evaluating the run\u00adtime specialize yields the same specialized \nprogram, given the same specialization values. Even though this presentation covers a simple impera\u00adtive \nlanguage and a small set of actions, it still addresss the important steps of the run-time specialization \nprocess. Because this presentation is done in a denotational frame\u00ad work, it abstracts over implem entation \ndetails and focuses on conceptual aspects. 4.1 The Language Variations of the language being studied \n(and their semantic definition) are used in this presentation. To distinguish each of them, syntactic \ndomains and variables ranging over these domains are indexed by the abbreviated name of the varia\u00adtion \n(e. g., c E Cornz ), and similarly for valuation functions. The syntax of the imperative language being \nstudied is displayed in Figure 6. The first part of the figure (Corn and Ezp ) defines the language to \nbe handled by the spe\u00adcialize. This initial language consists of commands (empty commands noted Nop, \nassignments, sequences, and condi\u00adtionals) and expressions (variables, constants, and primitive calls). \nTo reason about run-time specialization, the initial lan\u00adguage is extended. To motivate these extensions, \nlet us dis\u00adcuss some issues involved in modeling run-time specializa\u00adtion in a denotational framework. \nFirst, as can be expected the denotational definition of run-time specialization does not manipulate \nobject templates. Instead, it manipulates source templates. More precisely, since source templates are \nessentially in a one-to-one corre\u00adspondence with the right-hand sides of grammar rules, the latter ones \nwill now be manipulated by the run-time special\u00adization process. As a consequence of this change, instead \nof dumping templates for each non-terminal and instantiating templates with constant values, run-time \nspecialization now substi\u00adtutes non-terminals by their right-hand side, and generic terminals (encoded \nas holes) by constant values. Two ex\u00adtensions to our initial language make it possible to perform these \noperations. Construct Rule(s, c s ) allows a non\u00ad terminal s to be replaced by its right-hand side Crhs. \nCon\u00ad struct Inst (h, e) substitutes a hole h by a constant resulting x~Id Identifiers n c Num Numbers \n o E Oper Binary operators hEHoles ={hl,. ... h~} Holes s E Nterms = {s1, . . . . s~} Non-terminals .. \nc E Corn .. Nop Assign(z, e ) Seq(cj, c>) I Cond(e , c;, c:) .. I e E Ezpl .. Var(z) Cst(n) I Call(o, \ne:, ej) .. I c E Corn .. Nop Assign(z, e) Seq(c~, cz) Cond(e, cl, C2) Rule(s, c ~s) Inst(h, e) e E \nExp = Ezp% I C.hs E ComThs ::= Nop Assign(z, e ~ ) Seq(c~hs, cj~s) Cond(e h , c~h , Cjk ) Nterm(s) Var(x) \nCst(n) Call(o, e~hs, ejhs) Hole(h) Figure 6: Language syntax from the evaluation of an expression e. \nThe extended lan\u00adguage is defined by domains Corn and Exp. Right-hand sides of rammar rules are defined \nby do-F mains Com hs and Ezp . Just as templates can be nested, right-hand side terms (rhs-terms) may \ninclude non-terminals (Nterm(s)). Also, expressions may include holes (Hole(h)). The end result of run-time \nspecialization now corresponds to the abstract syntax of the specialized program, without non-t erminals \nnor holes. The model we just described does not contradict the fact that source templates are available \nat compile time and can thus be compiled prior to run-time to achieve efficient spe\u00adcialization in practice. \n 4.2 Semantic Definition of the Extended Language In this section the denotational semantics of the extended \nlanguage is defined. It is not necessary to define the deno\u00adtational semantics of the initial language \nsince it is a subset of the extended one. The semantic domains as well as the valuation functions are \ndisplayed in Figure 7. Notice that the process of substituting non-terminals by their right-hand side, \nand holes by values is noted - . As discussed in the previous section, we define the seman\u00adtics of run-time \nspecialization at the abstract syntax level. To do so, we have introduced extra constructs (Rule and \nInst) to build a specialized program by repeated substitu\u00adtions. But we also need to define a place where \nthe program being specialized can be stored and incrementally built. To this end, a special identifier \nfj is introduced; the store maps Figure 8: A run-time specialize written in the extended i f 6 Int E \nFun, = Int x Int + Int Integer Binary, values integer functions ~, 6 G Store = Id C : Corn + Store ~ \n-+ (Int + comrh )~ Store  I c[Nop] C[Assign(x, CISeq(cl, C.[Cond(e, e)] c2)] cl, C2)] u= a u a = = = \nc[Rule(s, CIInst(h, c )] e)] a u = = E ~ .Exp + Store &#38;[Var(x)] &#38;[Cst (n)] i?[Call(o, el,ez)] \n4 Int u c7 cr = = = a(x) N[n] O[o](&#38;[el]a, &#38;[e~]u) N : Num --+Int D : Oper --+ Fun2 Figure 7: \nExtended semantics 1 i rule(so, (s1; return l;D) 1=2*X; if(l== 2){ rule(sl, (1 = [hi] + y;)); inst([hl], \n1); } else { rule(sl, 11 = y* [hZ]; )); inst([hz], z); } } language it to the specialized program being \nbuilt. For a specializa\u00adtion grammar of a given program, the initial state of the specialization process \nconsists of a store mapping identifier ~ to the right-hand side of the start symbol of the grammar. Notice \nthat holes and non-terminals are unique, as spec\u00adified by the generator of run-time specializes (see \nSection 4.4). Let us revisit the example of procedure f and examine the run-time specialize for its body; \nit is displayed in Figure 8. The declaration is omitted, and the return command is left for the sake \nof presentation although procedures are not included in the initial language. Since identifier ~ is initially \nmapped to the right-hand side of the start symbol of the specialization grammar, as the run-time specialize \nexecutes, the non-t erminals get re\u00adplaced by their right-hand side, and holes get substituted by constants. \n 4.3 Semantic Definition of Specialization Actions Now that the extended language is introduced, let \nus define the syntsx and semantics of specialization actions. They represent the starting point of the \nrun-time specialization process. The set of actions considered for this presentation c G Corn ::= Eval(ci \n) Id(c ) Rebassign(z, e ) Rebseq(c~, C: ) Redseq(c}, C; ) Rebcond(e~, c;, c; ) Redcond(e , c?, c;) e \nG Expa ::= Eval(e ) I Id(e ) Rebcall(o, e;, ej) Figure 9: Actions syntax is displayed in Figure 9. \nThe meaning of all these actions has been discussed ear\u00adlier except for Rebseq. This action is assigned \nto a se\u00adquence command to be rebuilt. Notice that eval and iden\u00adtit y commands (and expressions) only \ninvolve elements of the initial language. Indeed, in either case these commands (and expressions) do \nnot involve any specialization aspects and should thus be standard. A similar situation occurs for the \nfirst argument of both Redseq and Redcond which is purely static. The semantic definition of the actions \nis given in Figure 10. As discussed above, the semantic of action Redseq re\u00adquires the first command \nto be purely static; another action could be introduced to address the case when the second command is \npurely static. Lwtly, it is important to notice that the actions of a given program are assumed to be \ncorrect. Proving the correctness of actions is outside the scope of this paper. This issue is addressed \nby Consel and Khoo in the context of a functional language [5]. 4.4 Generating Run-time Specializes \nGiven that the semantics of actions are defined, the re\u00admaining step is aimed at generating the run-time \nspecialize from an action-analyzed program. This generator of run\u00ad C : Coma + Store -+ (Corn X @[Eval(c \n)] u = Ca[Id(c )] r= C [Rebassign(z, e )] a = C [Rebseq(c~, cj)] C= c [.lledseq(c~,c j)] u= C [Rebcond(e \n, c~, c;)] a = C [Redcond(e , c;, c;)] a = &#38; : Expa + Store + EZPL S [Eval(e )] u= t7[Id(e )] u = \n2 [Rebcall(o, e;, ej)] a = Figure 10: Semantic time specializes is defined as a non-standard interpretation \nof actions. For a given action-analyzed program, it produces two results: an rhs-term which corresponds \nto the unsubsti\u00adtuted specialized program, and a run-time specialize which includes substitution operations \nand eval fragments. The generator is defined in Figure 11. Let us describe in detail the treatment of \neach action, starting with the commands. An eval command produces an rhs-term which consists of the empty \ncommand since a command which can be completely evaluated will not appear in the specialized program. \nAs for the run-time specialize, it corresponds to the command itself since it can be com\u00adpletely evaluated. \nThe inverse situation happens for identity commands. Rebuilding an assignment means that this construct \nwill be in the specialized program and thus is included in the resulting rhs-term. This rhs-term corresponds \nto the original assignment where eval expressions (in the right-hand side) have been replaced by holes. \nAs for the run-time specialize, it is composed of the instantiation operations that may be needed to \nfill the holes in the right-hand side expression of the assignment. Rebuilding a sequence command means \nthat this con\u00adstruct will appear in the specialized program, and indeed, it is part of the resulting \nrhs-term. As for the run-time spe\u00adcialize, it is composed of the eval commands contained in the arguments \nof sequence command. When reducing a sequence command, the generated rhs\u00adterm only cent ains the commands \nfrom the second argument of sequence to be rebuilt (the first argument can be com\u00adpletely evaluated). \nThe run-time specialize is a sequence command which consistfi of the first argument of the original sequence, \nand the eval commands from the second argument of sequence. Rebuilding a conditional command is very \nsimilar to re\u00ad building a sequence command; its description is thus omit\u00ad ted. The reduction of a conditional \ncommand involves a new aspect: it produces a fresh non-terminal as the rhs\u00ad term. This is due to the \nfact that, although the conditional command is known to be reduced, the branch to consider is unknown. \nTherefore, a non-terminal is introduced as a Store) ([Nc)p], C[c ]a) ([cz], a) ([Assign(z, &#38; [e ]a)], \na) ([Seq(c~, c~)], a ) where (C:, a ) = Ca[c:]u (c;, a ) = Ca[c:]u Ca[c;](cfcj]a) ([Cond(&#38;ffi[ea] \na,c~, c~)], a ) where (C;, cr ) = C [cy]a (C;, ff ) = Cm[c;]a if t[e ]a then C [c~]a else C~[c~]a [Cst(t[.? \n]cr)] [e ] [Call(o, ~ [e~]a, E [ej]cr)] definition of actions placeholder for the rhs-term of either \nbranch. Consequently, ~he run-time specialize produced in this situation consists of a conditional to \nbe evaluated whose branches substitute the fresh non-terminal by the rhs-term of the appropriate branch, \nin addition to executing the eval commands con\u00adtained in the corresponding branch. In the case of an \neval expression, the result of its evalu\u00adation will be substituted for a hole at run time. Therefore, \nthe analysis of such an expression produces a hole freshly generated as the rhs-term. As for the run-time \nspecialize, it consists of an instantiation command aimed at replacing the hole by a value computed at \nspecialization time. When an identity expression is analyzed, it is reproduced verbatim as the rhs-t \nerm. As for the run-time specialize, it consists of the empty command since the expression is not processed \nduring specialization. Rebuilding a primitive call means that the rhs-term con\u00adsists of this construct, \nthe operator, and the rhs-term of each operand. The run-time specialize is a sequence con\u00adstruct composed \nof the instantiation commands caused by the possible eval expressions included in the call arguments. \n 4.5 Correctness Proving correct the process of generating run-time specializ\u00ades consists of showing \nthat, for an action-analyzed program and some specialization values, the specialized program pro\u00adduced \nby interpreting actions is the same as the one pro\u00adduced by executing the run-time specialize using the \nsame specialization values. This statement is formally expressed in the following the\u00adorem. Theorem 1 \n(Vc E Corn ) (Vu @ Store) Let (cTks , c) = %m[c ] Then, a = C[c]a[$ ~ cr~s] * (a ($),;) = C [c ]5 Where \nVcr ~ Store, 6 = cr[~ ~J_] The proof is included in Appendix A.  C;en: Corn +( ComThs xc~m) Cj.%UEval(ci)] \n= ([Nop], [c ]) C;.n[Id(c )] = ([c ], [NcIp]) C&#38;. [Rebassign(z, e )] = ([Assign(z, e ks)], [c]) \nwhere (e hs, c) = Ejen[e ] C~en [Rebseq(cf, c; )] = ([Seq(c~~:l~~~~j [Seq(c~, c~)]) where = C;en[c;] \n(C;h , c, ) = C;en[c;] C~em [Redseq(c}, c; )] = ([.; ], [~}y;:;)]) where = C;en [c;] Cj.n[Rebcond(ea, \nc;, c;)] = (UCond(;~~;~S, Cjh )], [Seq(c, Seq(cl, .2))]) where = t~en [. ] (Cy,c, ) = C;en[.;] (Cy, c,) \n= C;=n[c;] C~.. [Redcond(e , c;, c;)] = (Nterm(s), [Cond(e , Seq(Rule(s, C[hg), .1), Seq(Rule(s, cj s), \n.2))]) where (Cyh , c,) = C;en [c;] (Cjh , c,) = C;en[c;] s is a fresh non-terminal E ~,. : Expa -+ (Exp \nhs x Corn) Cj.~[Eval(ez)] = ([Hole(h)], [Inst(h, e )]) where h is a fresh hole = ([e ], [Nop]) &#38;jen[Rebcall(o, \ne~, ej)] ~j.~[Id(e )] = ([ Call(ol e;hs, e;ks)l) [Seq(cl, .2)1) where (e~hs, .1) = &#38;j.n [e;] (ejhs, \nc~) = &#38;j,. [ej] Figure 11: Abstract interpret ation of the actions 5 Related Work st rat egy makes \nit difficult to generate efficient code. In contrast, our approach ~nables the compiler to process Recently \ntwo approaches to run-time code generation have program fragments globally in that it is applied to the \npos\u00adbeen reported by Engler and Proebsting [6], and by Leone sible combinations of templates which can \nbe constructed at and Lee [11]. These approaches include some aspects of run-run-time. Because the compiler \nprocesses large code frag\u00adtime specialization and address issues related to compiling ments it is able \nto produce efficient code. code at time. Many existing approaches (e. g., [11, 6]) emphasize the run \nEngler and Proebsting s approach consists of providing need to perform elaborate optimizations at run \ntime based the programmer with operations to construct templates man-on the fact that much more information \nis available then. ually in the intermediate representation of the LCC compiler This is a difficult challenge \nbecause of the conflicting re\u00ad(a form of register transfer language) [7]. Then, at run time, quirements \nof a run-time code generator, namely, produc\u00adthe operations to construct templates are executed, and \na ing code at low cost to allow this process to be amortized fast code generator is invoked to compile \ntemplates into bi-quickly, and exploiting as much run-time information as pos\u00ad nary code. sible to produce \nhighly-optimized code. When the run-time Not only is this approach error-prone because templates code \ngenerator only focuses on the former requirement, even are written manually, but it also forces the code \ngeneration if the number of instructions being executed is smaller, the process to be overly simple because \nit needs to be fast (no quality of the generated code may be such that performance elaborate register \nallocation or instruction scheduling is per-is degraded. When the run-time code generator puts too formed). \nmuch effort on optimization, the overhead may be such that Leone and Lee s approach is developed for \na first-order the process may not be applicable to many situations. subset of a purely functional language. \nIt is aimed at post-Determining what kind of run-time code generation pro\u00adponing certain compilation \noperations until run time to bet-cess is most suitable for a given situation is a difficult prob\u00adter \noptimize programs. Operations such as register alloca-lem. Two import ant fact ors need to be taken into \naccount: tion may be performed at run time for some program frag-the overhead introduced by the run-time \ncode generator and ments. The binding-time of a given function is defined by the frequency of execution \nof the code fragment to be pro\u00adthe way it is curried. cessed at run time. Both approaches suffer from \nthe fact that the run-time To some extent run-time specialization simplifies the is\u00adcompiler does not \nhave a global view of the program to be sue in that it is not aimed at performing general-purpose specialized, \nnor does it know what kind of specialized pro-optimizat ions that may or may not improve performance. \ngrams can be produced at run time. Consequently, run-time Rather i+ is restricted to specializing programs \nwith respect code generation is not performed at the level of a procedure to some run-time invariants. \nIf the program fragments to or a basic block, it is done at the instruction level. This be processed \noffer good opportunities for specialization, the run-time specialization process will likely performance \nshould improve, provided the is executed many times. Techniques to specialize object-oriented time have \nalso been developed [1]. They timizing frequently executed code sections. specialization techniques do \nnot address arbitrary computa\u00adtions: they are limited to the optimization of certain object\u00adoriented \nmechanisms such as method dispatch. 6 Conclusions and Future Directions We have presented an approach \nto performing specializa\u00adtion at run time, based on partial evaluation technology. It consists of producing \ntemplates at compile time and trans\u00adforming them so that they can be processed by a standard compiler. \nAt run time, only minor operations need to be performed: selecting and with run-time values, and result, \nrun-time specialization and thus does not require many times before its cost Our approach has been using \nthe GNU C compiler, copying templates, relocating jump is performed a specialized code is amortized. \nimplemented for and evaluation system that specializes as well as at run time. Future directions for \nthis work ough experimentation with our performing more measurements, be amortized specialized programs \nare aimed However, filling holes targets. As a very efficiently to be executed the C language, is integrated \nin a programs at compile include conducting C run-time specialize developing specific niques to use \nrun-time specialization in operating code where specialized code may be executing when and code at run \nat op\u00adthese partial time a thor\u00adand tech\u00adsystem invari\u00ad ant become invalid, and applying the approach \nto different languages like ML. Acknowleginents The Partial Evaluation Group at Irisa and group at Oregon \nGraduate Institute provided back on our work and detailed comments on Thanks also due to Olivier Danvy, \nPierre Keppel and Mark Leone for helpful comments and stimulating discussions. References the Synthetix \nvaluable feed\u00adthe paper. Jouvelot, David on the paper [1] C. Chambers and D. Ungar. Customization: optimiz\u00ading \ncompiler technology for SELF, a dynamically-typed object-oriented programming language. In ACM SIG-PLAN \nConference on Programming Language Desagn and Implementation, pages 146 160, 1989. [2] C. Consel and \nO. Danvy. From interpreting ing binding times. In N. D. Jones, editor, European Symposium on Programming, \nSpr&#38;ger-Verlag, 1990. [3] C. Consel and O. Danvy. Tutorial notes uation. In ACM Symposium on Principles \nmtng Languages, pages 493 501, 1993. [4] C. Consel, L. Hornof, F. Noel, J. Noye, schi. A Uniform Approach \nfor Comptle-Ttme Specializahon. Technical Report, Rennes/Inria, 1995. In preparation. to compil-ESOP \n90, 3 d pages 88-105, on partial eval\u00adof PTogram\u00ad and N. Volan-Time and Run-University of [5] [6] [7] \n[8] [9] [10]  [11] [12] [13] [14] [15] [16] C. Consel and S.C. Khoo. On-line @ Off-line Par\u00adtial Evaluation: \nSemantic Specifications and Correct\u00adness Proofs. Research Report, Yale University, New Haven, Connecticut, \nUSA, 1993. Extended version. To appear in Jou?mal of Functional Programming. D. R. Engler and T. A. Proebsting. \nDCG: an efficient, retargetable dynamic code generation system. In ACM Conference on Architectural Support \nfor Programming Languages and Operating Systems, 1994. C. W. Fraser and D. R. Hanson. A code generation \nin\u00adterface for ANSI C. Software -Practice and Experience, 21(9):963-988, 1991. N. D. Jones, C. K. Gomard, \nand P. Sestoft. Par\u00adtial Evaluation and Automatic Program Generation. Prentice-Hall International, 1993. \nD. Keppel, S. Eggers, and R. Henry. A Case for Run\u00adtime Code Generation. Technical Report, University \nof Washington, Seattle, Washington, 1991. D. Keppel, S. Eggers, and R. Henry. Evaluating Run\u00adtime Compded \nValue-Specijlc Optimzzations. Techni\u00adcal Report 93-11-02, University of Washington, Seattle, Washington, \n1993. M. Leone and P. Lee. Lightweight run-time code gen\u00aderation. In ACM Workshop on Parttal Evaluation \nand Semantics-Based ProgTam Mampulatzon, pages 97-106, 1994. B. N. Locanthi. Fast bitblt with asmo and \ncpp. In European Unzx Usem Group Conference Proceedings (E UUG), 1987. H. Massalin and C. Pu. Threads \nand input/output in the Synthesis kernel. In ACM Symposaum on Operating Systems Principles, pages 191-201, \n1989. R. Pike, B. N. Locanthi, and J.F. Reiser. Hard\u00adware/software trade-offs for bitmap graphics on \nthe blit. Software -Practzce and Experience, 15(2):131-151, 1985. C. Pu, T. Autrey, A. Black, C. Consel, \nC. Cowan, J. Inouye, L. Kethana, J. Walpole, and K. Zhang. Opti\u00admistic incremental specialization: streamlining \na com\u00admercial operating system. In ACM Symposium on Op\u00aderating Systems Principles, 1995. To appear. C. \nPu, H. Massalin, and J. Ioannidis. The Synthesis kernel. ACM Computing Systems, 1(1):11 32, 1988. A \nCorrectness In this section, the correctness proof which relate the stores being produced expressions \nproduced by interpretation lemmas are simple, they are omitted. of our approach by a run-time of actions \nand is presented. specialize and by evaluation The proof of by standard of a run-time the main theorem \nrelies on four lemmas interpretation. It also relates specialized specialize. Because the proofs of these \n Lemma 1 states that the evaluation of a command written in the initial language does not affect (or \ndepend on) the program being specialized stored at location $. Lemma 1 (Vci c Corn ) (Vcr E Store) (Vcrhs \nc C om ks) c[c~]u[~ ++ c+ ] = (c[c ]a)[~ 1+ c ~ ]. Lemma 2 stipulates that for any command written in \nthe extended language, whether or not it is evaluated with a store defined at location ~ does not affect \nthe other values contained in the store. Lemma 2 (Vc ~ C om)(Vu 6 Store) C[c]cr = C[c]t7 The following \ntwo lemmas address a correctness issue regarding the expression included in assignments and conditionals. \nMore precisely, for a given action-analyzed expression and a store, a specialized expression can be produced \nby interpreting the actions using &#38; . Another alternative is to evaluate the run-time specialize \nproduced by &#38;$n for this action-analyzed expression. Lemmas 3 and 4 state that these different evaluation \nstrategies produce the same specialized expression and the same store modulo the value of the store at \nlocation $. The following lemma uses function Hole to collect holes in rhs-terms (expressions and commands). \nTheorem 1 (Vc 6 Corn ) (Vcr c Stor-e) Let (c ks, c) = C~en[c ] Then, a = C[c]cJ[~ b+ C k ] + (a ($), \ncJ) = C [c ]6 P roofi the proof is by structural induction on c If c = Eval(c ) then, C;=% [c ] = (Nop, \nC ) CT = C~c ]a[~ ++ Nop] By Lemma 1, CT = (C[c ]cr)[~ ++ Nop] + (a (~), ~) = (Nop, C[ci]~) = C [c ]@ \n If co = Id(c ) then, C;=% [c ] = (c , Nop) u = CINop]a[$ ~ c ] = a[$ ++ c ] * (a ($),; ) = (C%,5) \n= C [C ]6 If c = Rebassign(z, e ) then, C~.m [c=] = (Assign(z, e h ), CO) where (er~s, c.)= t~,n[e ] \n a = CICOla[!i ++ Assign($,e hs)l By lemma 3, (o ($),; ) = (Assign(z, S [e ]@), &#38;) * (a ($),; ) \n= C [c ]ti e If c = Rebseq(c~, c;) then, C~en [c ] =( Seq(c~k , c$~s), Seq(cl, c2)) where [~~~~~~~ ~ \n~n~~ gen { a =C. [Seq(cl, c2)]a[~~Seq(c~~ ,cjL )] =C[c2](C[cl]a[$ ++ Seq(c~~s, cjkS)]) Because only \ncl(resp. C2) can substitute non-terminals and holes introduced in c~hs (resp. c~ks), . 6 = c[clja[~Hc;~ \n] u = 6 [$ ++ Seq(6($),6 (~))] where 6 = c]c2]ti[~t-+cj~ ] { (c$(\\))F) = C [c:]d By induction, (ti \n(!j), t?) = ca[c;]~ { =+ (CT (s),; ) = C [c ]&#38;  [f c = Redseq(c~, c;) then, C~,n [c ] = (c~~s, Seq(cj, \nc,)) where (cj~s, CZ) = C~.m[c~] 0 = CUSeq(c~, cz)]a[$ + Cjh ] = C[c2](C[c~]a[\\ ++ c~hs]) By lemma 1, \nu = C[c2](C[cj]cr)[$ E+ Cjhs] By induction, (a (!j), ~ ) = Ca[C~](C[C~]a) By lemma 2, (a ($),; ) = C \n[cj](C[c~]~) + (a (~),; ) = C [c ]m. e If c = Rebcond(e , c~, c;) then, C g~~[c~~,= (Cond(e hs, Clhs, \nCjh ), Seq(co, Seq(c~, c,))) (e , co) = $~.nue ] where (c~h , c,) = C;en [c;] { (Cjh , c,) = C:en [c;] \nr = C[seq(co, Seq(cl, C2))]O[$ w Cond(e ks, c~hs, c~hs)]  u = CISeq(cl, cz)]8 where 6 = C[co]cr[$j ~ \nCond(e kS, c~ks, cjhs)] By construction, e h , c~hs and c~h do not share holes and lemma 4 gives, b(~) \n= Cond(t [e ]@, c~fis, Cjhs) and ; = 6 + 6 = u[$ R Cond(tY[e ]@, c~hs, c~hs)] > # = CISeq(cl, c2)]c \nT[3++ Cond(C [e ]6, c]hs, cjh ) o = CUc2](C[cl]c7[~ ~ Cond(&#38;~[e ]@, c~hs, cjhs)])  As in the case \nof Rebseq, we have, # = C[cl]u[$ H c;~ ] a = 6 [~ ++ Cond(E [e ]6,6 ($), 6 (fj))] where { b = cuc,]c$ \n[~ ++ C;h ] = C [c; ]ii By induction, (6 ($),0) = c~[c;]j (~ ($)j~) { * (a (!j), a) = Ca[c ]a  * If \nc = Redcond(e , c;, c;) then, C~en [c ] = (Nterm(s), Cond(e , Seq(Rule(s, c~ks), cl), Seq(Rule(s, c~h \n), cz))  (cp, c,) = C;en[c;] where (Cjh , c,) = C;en [c;]  { a = CICond(ez, Seq(Rule(s, c~h ), cl), \nSeq(Rule(s, Cjhs), Cz))]a[$ ++ Nterm(s)] a = if &#38;[eZ]a[$ ++ Nterm(s)] then C[cl](CIRuIe(s, c~~ )]a[~ \n~ Nterm(s)]) else C[cZ](CIFtule(s, Cjh )]a[$ ~ Nterm(s)])  61 = C[c,]a[fj t--+ Cihs] 0- = if J5[e ]@ \nthen &#38; else 62 where 62 = C[c,]o[g l-+ Cp ] { (61($ ),{,) = c~[c;]; By induction, (6,($ ),6,) = \nc~[c;]d { ~ (a (~),; ) = if S[e ]@ then C [c~]&#38; else C [cj]6 = C [c ]@  \n\t\t\t", "proc_id": "237721", "abstract": "", "authors": [{"name": "Charles Consel", "author_profile_id": "81100552270", "affiliation": "University of Rennes/Irisa, Campus Universitaire de Beaulieu, 35042 Rennes Cedex, France", "person_id": "PP39048247", "email_address": "", "orcid_id": ""}, {"name": "Fran&#231;ois No&#235;l", "author_profile_id": "81100181197", "affiliation": "University of Rennes/Irisa, Campus Universitaire de Beaulieu, 35042 Rennes Cedex, France", "person_id": "PP31091022", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/237721.237767", "year": "1996", "article_id": "237767", "conference": "POPL", "title": "A general approach for run-time specialization and its application to C", "url": "http://dl.acm.org/citation.cfm?id=237767"}