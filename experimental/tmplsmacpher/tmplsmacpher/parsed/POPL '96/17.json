{"article_publication_date": "01-01-1996", "fulltext": "\n Iterated Register Coalescing Lal George Andrew W. Appel Rm. 2A426 AT&#38;T Bell Labs Department of Computer \nScience 600 Mountain Avenue Princeton University Murray Hill, NJ 07974 Princeton, NJ 08544-2087 george@research. \natt . com appel@princeton. edu Abstract An important function of any register allocator is to tar\u00adget \nregisters so as Lo eliminate copy instructions. Graph\u00adcoloring register allocation is an elegant approach \nto this problem. If the source and destination of a move instruc\u00adtion do not interfere, then their nodes \ncan be coalesced in the interference graph. Chaitin s coalescing heuristic could make a graph uncolorable \n(i. e., introduce spills); Briggs et al. demonstrated a conservative coalescing heuristic that preserves \ncolorability. But Briggs s algorithm is too conser\u00advative, and leaves too many move instructions in our \npro\u00ad grams. We show how to interleave coloring reductions with Briggs s coalescing heuristic, leadlng \nto an algorithm that is safe but much more aggressive. Introduction Graph coloring is a powerful approach \nto register alloca\u00adtion and can have a significant impact on the execution of compiled code. A good register \nallocator does copy prop\u00adagation, eliminating many move instructions by coloring the source temporary \nand target temporary of a move with the same register. Having copy propagation in the register allocator \noften simplifies code generation. The generation of target machine code can make liberal use of temporaries, \nand function call setup can naively move actuals into their formal parameter positions, leaving the register \nallocator to minimize the moves involved. Optimizing compilers can generate a large number of move instructions, \nInstatic single assignment (SSA)form[8], each variable in the intermediate form may be assigned into \nonly once. To satisfy this invariant, each program variable is split into several different temporaries \nthat are live at differ\u00adent times, At a join point of program control flow, one tem\u00adporary is copied \nto another as specified by a +-function. The SSA transformation allows efficient program optimiza\u00adtion, \nbut for good performance these artificial moves must later be removed by good register allocation. Even \nnon-SSA based compilers may generate a large num. berof move instructions. At a procedure call, a caller \ncopies Permission to make digital/hard copies of all or part of this material for personal or classroom \nuse is granted without fee provided that the copies are not rnadeor+stributed forpro$t or commercial \nadvantage, the. copy\u00adright notice, thetwle of thepublicatlon and its date appear, and not{ce is given \nthat copyright is bypermission oftbe ACM, Inc. To copy otherwise, to republish, toposton servers ortoredistfibute \nto lists, requires specific permission and/or fee. POPL 96, St. Petersburg FLA USA @ 1996 ACMrJ-89791-769 \n-3/95 /01.. $3.5r3 actual parameters to formals; then upon entry to the pro\u00adcedure, the callee moves \nformal parameters to fresh tem\u00adporaries. The formal parameters themselves need not be fixed by a calling \nconvention; if a function is local and all its call sites are identifiable, the formals may be temporaries \nto be colored (assigned to machine registers) by a register allocator[12]. Again, copy propagation is \nessential. Briggs et al. [4] conjecture that SSA is a good heuristic for splitting live ranges to minimize \nspills at the cost of a few extra moves and our experience (with a similar in\u00adtermediate representation, \ncontinuation-passing style) bears them out. The copying done in the calling conventions de\u00adscribed in \nthe previous paragraph is very similar to the be\u00adhavior of ~-functions in SSA. In fact, Briggs splits \nless than SSA would (splitting at a @ node only if a constant-propagation algorithm gives the source \nand target of a move different tags ); and we split even more than SSA variables live but not defined \nacross a basic block are not split by SSA, but we split them on entry to each extended basic block. The \nextra splitting gives even greater flexibility to the coalescing algorithm, thus reducing spills. Extra \nsplits can do no harm, because our coalescing algorithm is powerful enough to put them back together \nsafely. Our new result can be stated concisely: Interleaving Chaitin-style simplification steps with \nBriggs-style conser\u00advative coalescing eliminates many more move instructions than Briggs s algorithm, \nwhile still guaranteeing not to in\u00adtroduce spills. Consider the interference graph of Figure 3. Briggs \ns conservative coalescing heuristic, aswe will explain, cannot coalesce the move-related pair 3 and b, \nor the pair d and c, because each pair is adjacent to too many high-degree nodes. Our new algorithm first \nsimplifies the graph, result\u00ading in the graph of Figure 4(a). Now each move-related pair can be safely \ncoalesced, because simplification has lowered the degree of their neighbors. 2 Graph coloring register \nallocation Chaitin et al. [5, 6] abstracted the register allocation prob\u00adlem as a graph coloring problem. \nNodes in the graph rep\u00adresent live ranges or temporaries used in the program, An edge connects any two \ntemporaries that are simultaneously live at some point in the program, that is, whose live ranges interfere. \nThe graph coloring problem is to assign colors to the nodes such that two nodes connected by an edge \nare not assigned the same color. The number of colors avail\u00adable is equal to the number of registers \navailable on the machmet K-coloring a general graph is NP-complete [9], so a polynomial-time approximation \nalgorithm is used. There are five principal phases in a Chaitin-style graph coloring register allocator: \n 1. Build: Construct the interference graph. Dataflow analysis is used to compute the set of registers \nthat are simultaneously live at a program point, and an edge is added to the graph for each pair of registers \nin the set. This is repeated for all program points. 2. Coalesce: Remove unnecessary move instructions. \nA move instruction can be deleted from the program when the source and destination of the move instruc\u00adtion \ndo not have an edge in the interference graph. In other words, the source and destination can be co\u00adalesced \ninto one node, which contains the combined edges of the nodes being replaced.  When all possible moves \nhave been coalesced, rebuild\u00ading the interference graph for the new program may yield further opportunities \nfor coalescing. The build\u00adcoalesce phases are repeated until no moves can be coalesced. 3. Simplify: \nColor the graph using a simple heuristic [n]. Suppose the graph G contains a node m with fewer than K \nneighbors, where K is the number of registers on the machine. Let G be the graph G {m} ob\u00adtained by \nremoving m. If G can be colored, then so can G, for when adding m to the colored graph G , the neighbors \nof m have at most K 1 colors among them; so a free color can always be found for m. This leads naturally \nto a stack-based algorithm for color\u00ading: repeatedly remove (and push on a stack) nodes of degree less \nthan K. Each such simplification will decrease the degrees of other nodes, leading to more opportunity \nfor simplification. 4. Spill: But suppose at some point during simplificatio~ the graph G has nodes \nonly of wgnificant degree, that is, nodes of degree z K. Then the simplify heuristic fails, and a node \nis marked for spilling. That is, we  choose some node in the graph (standing for a tempo\u00adrary variable \nin the program) and decide to represent it in memory, not registers, during program execution. An optimistic \napproximation to the effect of spilling is that the spilled node does not interfere with any of the other \nnodes remaining in the graph. It can therefore be removed and the simplify process continued. In fact, \nthe spilled node must be fetched from mem\u00adory just before each use; it will have several tiny live ranges. \nThese will interfere with other temporaries in the graph. If, during a simplify pass, one or more nodes \nare marked for spilling, the program must be rewritten with explicit fetches and stores, and new live \nranges must be computed using dataflow analysis. Then the build and simplify passes are repeated. This \nprocess iterates until simplify succeeds with no spills; in prac\u00adtice, one or two iterations almost always \nsuffice. 5. Setect: Assigns colors to nodes in the graph. Starting with the empty graph, the original \ngraph is built up Dy repeatedly adding a node from the top of the stack. When a node is added to the \ngraph, there must be a color for it, as the premise for it being removed in the Chaitin et al. /-\\ 7 \n> \\ v spill select( .1 Figure 1: Flowchart of Chaitin graph coloring algorithm simplify phase was that \nit could always be asszgned a color provided the remaining nodes in the graph could be successfully colored. \nFigure 1 shows the flowchart for the Chaitin graph-coloring register allocator [5, 6]. Example An example \nprogram is shown in Figure 2 and its interfer\u00adence graph in Figure 3. The nodes are labeled with the \ntem\u00adporaries they represent, and there is an edge between two nodes if they are simultaneously live. \nFor example, nodes d, k, and j are all connected since they are live simulta\u00adneously at the end of the \nblock. Assuming that there are four registers available on the machine, then the simplify phase can start \nwith the nodes g, h, c, and f in its working set, since they have less than four neighbors each. A color \ncan always be found for them if the remaining graph can be successfully colored. If the algorithm starts \nby removing h and g, and all their edges, then node k becomes a candidate for removal and can be added \nto the worklist. Figure 4(a) shows the state of the graph after nodes g, h, and k have been removed. \nContinuing in this fashion a possible order in which nodes are removed is represented by the stack shown \nin Figure 4(b), where the stack grows upwards. The nodes are now popped off the stack and the original \ngraph reconstructed and colored simultaneously. Starting with m, a color is chosen arbitrarily since \nthe graph at thk point consists of a singleton node. The next node to be put into the graph is c. The \nonly constraint is that it be given a color different from m, since there is an edge from m to c. When \nthe original graph has been fully reconstructed, a possible assignment for the colors is shown in Figure \n4(c). live In: kj := mern[~+12] h:= k-1 6 f :=g*h e := mem[j+8] m := mmn[j+i6] b := mem[f ] c:=e+8 \n d:=c k:=m+4 goto d live Out: dk j Figure 3: Interference graph Figure 2: Example program Dotted lines \nare not interference edges but indicate move instructions. m1 3 ;2 f2 e4 3 :4 k1 h2 4g stack assignment \n(a) (b) (c) Figure 4: (a) shows the intermediate graph after removal of nodes h, g, and k; (b) shows \nthe stack after all nodes have been removed; and (c) is a possible assignment of colors. Coalescing It \nis easy to eliminate redundant move instructions with an interference graph. If there is no edge in the \ninterfer\u00adence graph between the source and destination of a move instruction, then the move can be eliminated. \nThe source and destination nodes are coalesced into a new node whose edges are the union of those of \nthe nodes being replaced. Chaitin[6] coalesced any pair of nodes not connected by an interference edge \navoiding coalescing with real machine registers where possible. Thk aggressive form of copy propagation \nis very successful at eliminating move instruc\u00ad~ions. Unfortunately, the node being introduced is more \ncon\u00adstrained than those being removed, as it contains a union of edges. Thus, it is quite possible that \na graph, colorable with K colors before coalescing, may no longer be K-colorable af\u00adter reckless coalescing. \nIf some nodes are pre-colored -assigned to specific ma\u00adchine registers before register allocation (because \nthey are used in calling conventions, for example), they cannot be spilled. Some coloring problems with \npre-colored nodes have no solution: if a temporary interferes with K pre-colored nodes (all of different \ncolors), then the temporary must be spilled. But there is no register into which it can be fetched back \nfor computation! We say such a graph is uncolorable, and we have found that reckless coalescing often \nleads to uncolorable graphs. Most compilers have a few pre-colored nodes, used in standard calling conventions, \nbut significantly fewer than K of them; our compiler can potentially pre-color all registers for parameter \npassing, and therefore we cannot use reckless coalescing. Briggs et al. [4] describe a conservative coalescing \nstrat\u00adegy that addresses thk problem. If the node being coa\u00adlesced has fewer than K neighbors of significant \ndegree, then coalescing is guaranteed not to turn a K-colorable graph into a non-K-colorable graph. A \nnode of significant de\u00adgree is one with K or more neighbors. The proof of the guarantee is simple: after \nthe simplify phase has removed all the insignificant-degree nodes from the graph, the coa\u00adlesced node \nwill be adjacent only to those neighbors that were of significant degree. Since these are less than K \nin number, sirnpli~g can remove the coalesced node from the graph. Thus if the original graph was colorable, \nthe conser\u00advative coalescing strategy does not alter the colorability of the graph. The strategy is conservative \nbecause a graph may still be colorable (using the heuristic), when a coalesced node has more than K neighbors \nof significant degree. Conservative coalescing is successful at removing many move instructions without \nintroducing spills (stores and fetches), but Briggs found that some moves still remain. For these he \nused a biased coloring heuristic during the select phase: When coloring a temporary X that is involved \nin a move instruction X -Y or Y -X where Y is already colored, the color of Y is selected if Dossib}e.. \nOr. if Y is not vet col\u00ad . ored, then a color is chosen that might later be eligible for the coloring \nof Y. If X and Y can be given the same color (assigned to the same register), then no move instruction \nwill be necessary. In Figure 3 nodes c, d, b and J are the operands of move instructions. Using the conservative \ncoalescing strat\u00adegy, these nodes cannot be coalesced. Coalescing b and j would produce a node with four \nsignificant-degree neigh\u00ad bors, namely m, d, e, and k. However, during the selection phase it is possible \nto bias the coloring so that these nodes # - --\\ SSA constant propagation \\, I reckless coalesce \\ of \nequal-tag moves build I I poteiltial J spill ( aeict~ Figure 5: Briggs s algorithm get the same color. \nTherefore when coloring j, the color of b is given preference. If b has not been colored yet, then an \nattempt is made to prohibit the colors used by neighbors of b, to enhance the possibility of coalescing \nlater. The success of biased color selection is based on chance. In our example, b happened to be colored \nfirst with the reg\u00adister r2, and f was also assigned the same register, thus pro\u00adhibiting the choice \nof r2 for node j. Therefore, the move be\u00adtween b and i cannot be eliminated. If f had been assigned . \nanother register, then the move could have been eliminated. Thk type of lookahead is expensive. For similar \nreasons the move between c and d cannot be eliminated. In the example of Figure 3 none of the moves were \neliminated using either conservative coalescing or biased selection. Briggs et al. [4] introduced optimistic \ncolortng, which re\u00adduces the number of spills generated. In the simplify phase, when there are no low-degree \nnodes, instead of marking a node for spilling they just remove it from the graph and push it, on the \nstack. This is a potential spilL Then the select phase may find that there is no color for the node; \nthis is a spill. But in some cases select may find a color be\u00adcause the K (or more) neighbors will be \ncolored with fewer than K dktinct colors. Figure 5 shows the flow of control in Briggs s register allocator. \nRematerialization: Briggs et al. observe that variables with constant values can be spilled very cheaply: \nno store is nec\u00adessary, and at each use the value may be reloaded or re\u00adcomputed. Therefore, such variables \nare good candidates for spilling, and the spill selection algorithm should be in\u00adformed by the results \nof a good constant-propagation algo\u00adrithm. Thk technique is equally useful in the context of our new \nalgorithm; we have no novel spilling techniques, and all the known heuristics should be applicable. Briggs \nalso used constant-propagation information in co\u00adalescing decisions. When a and b are known to be constant, \nthe movea ~ bwillbe recklessly coalesced even ifthe result\u00ading live range would spill; this may be acceptable \nbecause the spill is cheap. In fact, Briggs also recklessly coalesceda~ bif neithera nor b is constant; \nthis is not really justifiable (it can lead to excess spilling) but it was necessary because hk conservative \ncoalescing heuristic is too weak to handle huge numbers of moves. Briggs also recklessly coalesced any \ncopy instructions in the original program, leaving only the splits induced by q5 functions where a and \nb had inequivalent tags (constant properties) for conservative coalescing. Our algorithm does not do \nany reckless coalescing, be\u00adcause we cannot afford to with so many pre-colored nodes; our coalescing \nis oblivious of constant-propagation informa\u00adtion. Difficult coloring problems Graph-coloring register \nallocation is now the conventional approach for optimizing compilers. With that in mind, we implemented \nan optimizer for our compiler (Standard ML of New Jersey [2]) that generates many short-lived tempo\u00ad \nraries with enormous numbers of move instructions. Several optimization techniques contribute to register \npressure. We do optimization and register allocation over several proce\u00addures at once. Locally defined \nprocedures whose call sites are known can use specially selected parameter temporaries i12, 1, 7]. Free \nvariables of nested functions can turn into ex\u00adtra arguments passed in registers [12, I]. Type-based \nrepre. sentation analysis [13, 15] spreads an n-tuple into n separate registers, especially when used \nas a procedure argument or return value. Callee-save register allocation [7] and callee\u00adsave closure \nanalysis [3, 14] spread the calling context into several registers. Our earlier phases have some choice \nabout the number of simultaneously live variables they create, For example, representation analysis can \navoid expanding large rz-tuples, closure analysis can limit the number of procedure param\u00adeters representing \nfree variables, and callee-save register al\u00adlocation can use a limited number of registers. In all the.. \ncases, our optimization phases are guided by the number of registers available on the target machhe. \nThus, although they never assign registers explicitly, they tend to produce register allocation problems \nthat are as hard as possible, but no harder: they don t spill much, yet there are often K 1 live variables. \nIn implementing these optimization techniques, we as. sumed that the graph-coloring register allocator \nwould be a-ble to eliminate all the move instructions and assign SSA constant propagation (optional) \ni build ( pote Ltlal J spill c1+/=\\ select ., 1 Figure 6: Iterated algorithm registers without too \nmuch spilling. But instead we found that Chaitin s reckless coalescing produced too many spills, and \nBriggs s conservative coalescing left too many move in\u00adstructions. It seems that our register-allocation \nand copy\u00adpropagation problems are more difficult than those produced by the FORTRAN compilers measured \nby Briggs. Our measurements of realistic programs show that con\u00adservative coalescing eliminates only \nZ4~0 of the move in\u00adstructions; biased selection eliminates a further Sg% (of the original moves), leaving \n3770 of the moves in the program. Our new algorithm eliminates all but 16% of the move in\u00adstructions. \nThk results in a speedup of 4.4~o over programs compiled using one-round conservative coalescing and \nbiased selection. 5 Iterated register coalescing Interleaving Chaitin-style simplification steps with \nBriggs\u00adstyle conservative coalescing eliminates many more move in\u00adstructions than Briggs s algorithm, \nwhile still guaranteeing not to introduce spills. Our new approach calls the coalesce and simplify pro\u00adcedures \nin a loop, with wnplijy called first. The building blocks of the algorithm are essentially the same, \nbut with a different flow of control shown in Figure 6. Our main contri\u00ad Figure 8: Interference graph \nafter coalescing b and J Figure 7: Interference graph after coalescing d and c bution is the dark backward \narrow. There are five principal phases in our register allocator: 1. Build: Construct the interference \ngraph, and catego\u00adrize each node as either move-related or not move\u00adrelated. A move-related node is one \nthat is the either the source or destination of a move instruction. 2. Sirnplijy: One at a time, remove \nnon-move-related nodes of low degree from the graph. 3. Coalesce: Perform Briggs-style conservative \ncoalescing on the reduced graph obtained in the simplification phase. Since the degrees of many nodes \nhas been re\u00adduced by simplify, the conservative strategy is likely to find many more moves to coalesce \nthan it would have in the initial interference graph. After two nodes have been coalesced (and the move \ninstruction deleted), if the resulting node is no longer move-related it will be available for the next \nround of simplification. Simplify and Coalesce are repeated until only significant-degree or move-related \nnodes remain. 4. Freeze: If neither simpli~y nor coalesce applies, we look for a move-related node of \nlow degree. We freeze the moves in which thk node is involved: that is, we give up hope of coalescing \nthose moves. Thk causes the node (and perhaps other nodes related to the frozen moves) to be considered \nnon-move-related. Now, sim\u00adplafy and coalesce are resumed.  ,.5Select: Same as before. Unlike Briggs, \nwe do not use biased selection because it is not ;;cessary with our improved coalescing heuristic. Our \ntechnical report [10] shows the algorithm in pseudocode. Consider the initial interference graph shown \nin Figure 3. Nodes b, c, d, and j are the only move-related nodes in the graph. The initial worklist \nused in the simplify phase must contain only non-move related nodes, and consists of nodes g, h, and \nf. Node c is not included as it is move related. Once again, after removal of g, h, and k we obtain the \ngraph in Figure 4(a). We could continue the simplification phase further, how\u00adever, if we invoke a round \nof coalescing at thk point, we discover that c and d are indeed coalescable as the coalesced node has \nonly two neighbors of significant degree namely m and b. The resulting graph is shown in Figure 7, with \nthe coalesced node labeled as dkc. From Figure 7 we see that it is possible to coalesce b and j as well. \nNodes b and j are adjacent to two neighbors of significant degree, namely m and e. The result of coalescing \nb and j is shown in Figure 8. After coalescing these two moves, there are no more move-related nodes, \nand therefore no more coalescing pos\u00adsible. The simplify phase can be invoked one more time to remove \nall the remaining nodes. A possible assignment of colors is shown below: e1 m2 f3 j&#38;b 4 d&#38;c 1 \nk2 h2 g 1 stack coloring Thk coloring is a valid assignment for the original graph in Figure 3. Theorem \nAssume an interference graph G is colorable using the simplify heuristic. (?ongervatiue coalescing on \nan intermediate graph that ia produced after some rounds of simplification of G produces a colorable \ngraph. Definition A $implified graph S(G) is one in which some or all low-degree, non-moue related nodes \nof G and thew edges have been removed. Nodes that have been removed from a graph G cannot affect the \ncolors of nodes that remain in S(G). Indeed, they are colored after all nodes in S(G) have been colored. \nThere\u00adfore, conservative coalescing applied to two nodes in S(G) cannot affect the colorability y of \nthe original graph G. 0 Thk technique is very successful: The first round of sim\u00adplification removes \nsuch a large percentage of nodes that the conservative coalescing phase can usually be applied to all \nthe move instructions in one pass. Some moves are neither coalesced nor frozen. Instead, they are con9t7ained. \nConsider the graph X, Y, Z, where (X, Z) is the only interference edge and there are two moves X + Y \nand Y + Z. Either move is a candidate for co\u00adalescing. But after X and Y are coalesced, the remaining \nmove XY + Z cannot be coalesced because of the inter\u00ad ference edge (XY, Z). We say thk move is constrained, \nand we remove it from further consideration: it no longer causes nodes to be treated as move-related. \nPessimistic or optimistic coloring Our algorithm is compatible with either pessimistic or op\u00adtimistic \ncoloring. With Chaitin s pessimistic coloring, we guarantee not to introduce new spills. With optimistic \ncol\u00adoring, we can only guarantee not to increase the number of potential spills; the number of actual \nspills might change. If spilling is necessary, build and simplify must be re\u00adpeated on the whole program. \nThe simplest version of our algorithm discards any coalescing found if build must be repeated. Then it \nis easy to prove that coalescing does not increase the number of spills in any future round of build. \nHowever, coalescing significantly reduces the number of temporaries and instructions in the graph, which \nwould speed up the subsequent rounds of build and 9imph.fy. It is safe to keep any coalescing done before \nthe first spill node is re\u00admoved from the graph. In the case of optimistic coloring, this means the first \npotential spill. Since many coalesces occur before the first spill, the graph used in subsequent rounds \nwill be much smaller; this makes the algorithm run significantly faster. (The algorithm we show in the \nap\u00adpendix is a simpler variant that discards al! coalesces in the event of a Spill. ) 6 Graph coloring \nimplementation The main data structure used to implement graph coloring is the adjacency list representation \nof the interference graph. During the selection phase, the adjacency list is used to derive the list \nof neighbors that have already been colored, and during coalescing, two adjacency lists are unioned to \nform the coalesced node. Chaitin and Briggs use a bit-matrix representation of the graph (that gives \nconstant time membership tests) in addition to the adjacency lists. Since the bit matrix is sym\u00admetrical, \nthey represent only one half of the matrix, so the number of bits required is n(n + 1)/2. In practicet \nn can be large (for us it is often over 4000), so the bit matrix representation takes too much space. \nWe take advantage of the fact that the matrix is sparse, and use a hash table of integer pairs. For a \ntypical average degree of 16 and for n = 4000, the sparse table takes 256 Kbytes (2 words per entry, \nassuming no collisions) and the bit matrix would take I Mbyte. Some of our temporaries are pre-colored, \nthat is, they represent machine registers. The front end generates these when interfacing to standard \ncalling conventions across mod\u00adule boundaries, for example. Ordinary temporaries can be assigned the \nsame colors as pre-colored registers, as long as they don t interfere, and in fact this is quite common. \nThus, a standard calling-convention register can be re-used inside a procedure as a temporary. The adjacency \nlists of machine registers are very large (see figure 10); because they re used in standard calling conventions \nthey interfere with many temporaries. Further\u00admore, since machine registers are precolored, their adjacency \nlis~s are not necessary for the select phase. Therefore, to save space and time we do not explicitly \nrepresent the adjacency liSLS of the machine registers. The time savings is significant: when -X is coalesced \nto Y, and X interferes with a machine register, then the long adjacency list for the machine register \nmust be traversed to remove X and add Y. In the absence of adjacency lists for machine registers, a simple \nheuristic is used to coalesce pseudo-registers with machine registers. A pseudo-register X can be coaiesceci \nto a machine register R, if for every T that is a neighbor of X, the coalescing does not increase the \nnumber of T s significant-degree neighbors from < K to ~ K. Any of the following conditions will suffice: \n1.T already interferes with R. Then the set of T s neigh\u00adbors gains no nodes. 2. T is a machine register. \nSince we already assume that all machine registers mutually interfere, this implies condition 1. 3. \nDegree(T) < K. Since T will lose the neighbor R and gai~ the neighbor T, then degree(T) wil~ continue \nto be <K.  The third condition can be weakened to require T has fewer than K 1 neighbors of significant \ndegree. This test would coalesce more liberally while still ensuring that the graph retains its colorability; \nbut it would be more expensive to implement. Associated with each move-related node is a count of the \nmoves it is involved in. This count is easy to maintain and is used to test if a node is no longer move-related. \nAssociated with all nodes is a count of the number of neighbors currently in the graph. This is used \nto determine whether a node is of significant degree during coalescing, and whether a node can be removed \nfrom the graph during simplification. To make the algorithm efficient, it is important to be able to \nquickly perform each sirnpli.fy step (removing a low\u00addegree non-move-related node), each coalesce step, \nand each freeze step. To do this, we maintain four work lists: Low-degree non-move-related nodes (simplify \nWoddist); b Coalesce candidates: move-related nodes that have not been proved uncoalesceable (worklistMoves); \nLow-degree move-related nodes (freeze Worklist). e High-degree nodes (spill Worklut). Maintenance of \nthese worklists avoids quadratic time blowup in findkg coalesceable nodes. When a node X changes from \nsignificant to low degree, the moves associated with its neighbors must be added to the move worklist. \nMoves that were blocked with too many significant neighbors (including X) might now be enabled for coalescing. \nMoves are added to the move worklist in only a few places: * During simplify the degree of a node X might \nmake the transition as a result of removing another node. Moves associated with neighbors of X are added \nto the workli9tMoves. When coalescing U and V, there may be a node X that interferes with both U and \nV. The degree of X is decre\u00admented as it now interferes with the single coalesced node. Moves associated \nwith neighbors of X are added. If X is move related, then moves associated with X it\u00adself are also added \nas both U and V may have been significant degree nodes.  When coalescing U to V, moves associated with \nU are added to the move worklist. This will catch other moves from U to V.  Benchmark knuth-bend>x \nvboyer mlyacc nucleic simple format ray Lines 580 924 7422 2309 904 2456 891 Type Symbolic Symbolic Symbolic \n F.P. F.P. F.P. F.P.  Figure Description The Knuth-Bendix completion algorithm The Boyer-Moore theorem \nprover using A parser generator, processing the SML Nucleic acid 3D structure determination A spherical \nfluid-dynamics program SML/NJ formatting library Ray tracing 9: Benchmark description vectors grammar \nBenchmark live ranges average degree instructions machine pseudo machine pseudo moves non-moves knuth-bendlx \n15 5360 1296 13 4451 9396 vboyer 12 9222 4466 10 1883 20097 mlyacc: yacc.sml 16 6382 1766 12 5258 12123 \nutils. sml 15 3494 1050 14 2901 6279 yacc.grm.sml 19 4421 1346 11 2203 9606 nucleic 15 9825 4791 46 1621 \n27554 simple 19 10958 2536 15 8249 21483 format 16 3445 652 13 2785 6140 ray 15 1330 331 16 1045 2584 \nFigure 10: Benchmark characteristics Benchmarks For our measurements we used seven Standard ML pro\u00adgrams, \nand SML/NJ compiler version 108.3 running on a DEC Alpha. A short description of each benchmark is given \nin Figure 9. Five of the benchmarks use floating point arith\u00admetic, namely: nucleic, simple, format, \nand ray. Some of the benchmarks consist of a single module, whereas others consist of multiple modules \nspread over multiple files. For benchmarks with multiple modules, we selected a mod\u00adule with a large \nnumber of live ranges. For the rnlyacc bench\u00admarks we selected the modules defined in the files yacc \n. sml, utlls. sml, and yacc .grm. sml. Each program was compiled to use six callee-save regis\u00adters. This \nis an optimization level that generates high reg\u00adister pressure and very many move instructions. Previous \nversions of SML /NJ used only three callee-save registers, be\u00adcause their copy-propagation algorithms \nhad not been able to handle six effectively. Figure 10 shows the characteristics of each benchmark Statis~ics \nof the interference graph are separated into those associated with machine registers and those with pseudo\u00adregisters. \nLive ranges shows the number of nodes in the in\u00adterference graph. For example, the knuth-bendix program \nmentions 15 machine registers and 5360 pseudo-registers. These numbers are inflated as the algorithm \nis applied to all the functions in the module at one time; in practise the func\u00adtions would be applied \nto connected components of the call graph. The average degree column, indicating the average length of \nadjacency lists, shows that the length of adjacen\u00adcies associated with machine registers is orders of \nmagnitude larger than those associated with pseudo-registers. The last two columns show the total number \nof move and non-move instructions. 8 Results Ideally, we would like to compare our algorithm dh-ectly \nagainst Chaitin s or Briggs s. However, since our compiler uses many precolored nodes , and Chaitin s \nand Brigg s al\u00adgorithms both do reckless coalescing (Chaitin s more than Briggs s), both of these algorithms \nwould lead to uncolorable graphs. What we have done instead is choose the safe parts of Brigg s algorithm \nthe early one-round conservative coa\u00adlescing and the biased coloring to compare against our al\u00adgorithm. \nWe omit from Brigg s algorithm the reckless coa\u00adlescing of same-tag splits. From both algorithms (ours \nand Brigg s) we omit opti\u00admistic coloring and cheap spilling of constant values (rema\u00adterialization); \nthese would be useful in either algorithm but their absence should not affect the comparison. We will \ncall the two algorithms one-round coalescing and itevated coalescing. Figure 11 shows the spilling statistics. \nThe number of spills not surprisingly is identical for both the iterated and Briggs s scheme. Most benchmarks \ndo not spill at all. From among the programs that contain spill code, the num\u00adber of store instructions \nis almost equal to the number of fetch instructions suggesting that the nodes that have been spilled \nmay have just one definition and use. m coalesced u biased ~ frozen w constrained   Comparison of moves \ncoalesced by two algorithms The black and dark-gray bars, lab~lled frozen and constrained, represent \nmoves remaining in the program. Benchmark Nodes Instructions Benchmark One-round Iterated Improvement \nspilled store fetch knuth-bendix 42900 40652 570 knuth-bendix o 0 o vboyer 84204 80420 4 vboyer 0 0 0 \nyacc.sml 55792 52824 5 yacc.sml 0 0 0 utils. sml 28580 26564 7 utils.sml 17 17 35 yacc.grm. sml 39304 \n39084 yacc.grm.sml 24 24 33 nucleic 112628 111408 1 nucleic 701 701 737 simple 102808 92148 10 simple \n12 12 24 format 28156 26448 6 format 0 0 0 ray 12040 10648 11 ray 6 6 10 Average I 57 0 Figure 11: Spill \nstatistics Figure 15: Comparison of code size  Figure 12 compares the one-round and iterated algorithms \nThe average improvement in code size is 5% (Figure 15). on the individual benchmarks. Since moves are \nthe very fastest kind of instruction, we Referring to the bar charts for the one-round coalescing would \nexpect that the improvement in speed would not be algorithm: coalesced are the moves removed using the \ncon-nearly this large. But taking the average timing from a se\u00adservative coalescing strategy; constrained, \nare the moves that ries of 40 runs, we measured a surprising speedup average of become constrained by \nhaving an interference edge added 4.4% using the iterated scheme over one-round-coalescing, to them as \na result of some other coalesce; btased are the Figure 16 shows the timings on the individual bench\u00admoves \ncoalesced using biased selection, and frozen are the marks. Each entry is the average of the sum of user, \nsystem, moves that could not be coalesced using biased selection. and garbage collection time. We believe \nthat the significant On an average 24~0 of the nodes are removed in the coalesce speed improvement is \npartly due to the better I-cache per\u00adphase and all the rest are at the mercy of biased selection. formance \nof smaller programs. Considering all benchmarks together, 62 % of all moves are There is a significant \nspeed improvement when using six removed. callee-save registers over three. The old register allocator \nin For the iterated scheme coalejced and constrained have the SML/NJ compiler showed a degradation in \nperformance the same meaning as above, but frozen refers to the moves when the number of callee-save \nregisters was increased be\u00adchosen by the Freeze heuristic. Biased selection is not needed yond three[3]. \nAppel and Shao attributed this to poor reg\u00adand so bzased does not apply. More than 8470 of all moves \nister targeting (copy propagation). The new compiler using are removed with the new algorithm. iterated \ncoalescing shows a distinct improvement when go- Figure 13 and 14 give more detailed numbers. Benchmark \n coalesced constrained biased freeze % coalesced knuth-bendix 1447 47 1675 1282 70% vboyer 146 0 783 \n954 49 mlyacc: yacc.sml 1717 56 1716 1769 65 utils.sml 576 96 1459 770 70 yacc.grm.sml ~ 775 66 1208 \n154 90 nucleic 440 144 578 459 63 simple 1170 209 2860 4010 49 format 884 12 1002 887 68 ray 177 6 442 \n420 59 Figure 13: Coalesce statistics for one-round coalescing algorithm Benchmark coalesced constrained \nfreeze II % coalesced knuth-bendix 3684 611 156 83~0 vboyer 1875 8 0 99 mlyacc: yacc.sml 4175 971 112 \n79 utils.sml 2539 362 0 88 yacc.grm,sml 2038 165 0 93 nucleic 1323 298 0 82 simple 6695 1482 72 81 format \n2313 208 264 83 ray 967 78 0 93 Figure 14: Coalescing statistics for iterated register allocator Benchmark \nOne-round Iterated Improvement knuth-bendlx 7.11 6.99 2% Benchmark callee- Improve\u00ad vboyer 2.35 2.30 \n2 ] save ment mlyacc 3.30 3.18 3 1% nucleic 2.91 2.59 11 4 simple 27.72 27.51 1 d 9 format 8.87 8.73 \n2 simple 28.21 27.51 2 ray 49.04 44.35 10 format 8.76 8.73 0 Average 4.4~o ray , 47.20 44.34 6 Figure \n16: Comparison of execution speed  ing from three to six callee-save registers, confirming Appel and \nShao s guess. Use of a better register allocator now al\u00adlows us to take full advantage of Shao s improved \nclosure analysis algorithm [14]. Figure 17 shows the average execu\u00ad tion time taken over 40 runs. All \nbenchmarks show some improvement with more callee-save registers. Conclusions Alternating the simplify \nand coalesce phases of a graph col\u00adoring register allo cat or eliminates many more moves than the older \napproach of coalescing before simplification. It ought to be easy to incorporate this algorithm into \nany ex\u00adisting implementation of graph-coloring-based register allo\u00adcation, as it is easy to implement \nand uses the same buildlng blocks. Figure 17: Execution time, varying the number of calleesave registers \nAcknowledgments Kenneth Zadeck provided many detailed and insightful crit\u00adicisms and suggestions about \nthe algorithm. Steven Walk, Chris Fraser, and Kent Wilken gave us feedback on an early draft of the manuscript. \nPreston Briggs helpfully commented in detail on a later draft. References [1] APPEL, A. W. Compiling \nwith Continuation. Cam\u00adbridge Univ. Press, 1992. ISBN 0-521-41695-7. [21 APwm, A. W., AND MACQUEEN, D. \nB. Standard ML of New Jersey. In Third Int 1 .$ymp, on Prog. Lang. im\u00adplementation and Logic Pvogrammtng \n(New York, Au\u00adgust 1991), M. Wirsing, Ed., Springer-Verlag, pp. 1-13. [3] APPEL, A. W., AND SHAO, Z. \nCallee-save registers in continuation-passing style. Lisp and SymlIolic Compu\u00adtation 5, 3 (1992), 191-221. \n[4] BRIGGS, P., COOPER, K. D., AND TORCZON, L. Im\u00adprovements to graph coloring register allocation. ACM \ntransacting on programming languages and systems 16, 3 (May 1994), 428-455. [5] CHAITIN, G. J. Register \nallocation and spilling via graph coloring. SIGPLAN Notices 17(6) (June 1982), 98-105. Proceeding of \nthe ACM SIGPLAN 82 Sympo\u00adsium on Compiler Construction. [6] CHAITIN, G, J., AUSLANDER, M. A., CHANDRA, \nA. K., COCKE, J., HOPKINS, M. E., AND MARKSTEIN, P. W. Register allocation via coloring. Computer Lan\u00adguages \n6 (January 1981), 47-57. [7] CHOW, F. C. Minimizing register usage penalty at pro\u00adcedure calls. In Proc. \nSIGPLA N 88 Conf. on Prog. Lang. Design and Implementation (New York, June 1988), ACM Press, pp. 85-94. \n[8] CYTRON, R., FERRANTE, J., ROSEN, B. K., WEG-MAN, M. N., AND ZADECK, F. K. Efficiently comput\u00ading \nstatic single assignment form and the control de\u00adpendence graph. ACM Transactions on programming languages \nand systems 13, 4 (October 1991), 451-490. [9] GAREY, M. R., AND JOHNSON, D. S. Comput\u00aders and Intractability, \nA guide to the theory of NP\u00adcompleteness. Freeman, 1979. ISBN 0-7167-1044-7. [10] GEORGE) L., AND APPEL, \nA. W. Iterated register co\u00adalescing. Tech. Rep. CS-TR-498-95, Princeton Univer\u00adsity, 1995. [11] KEMPE, \nA. B. On the geographical problem of the four colors. American Journal of Mathematics 2 (1879), 193-200. \n[12] KRANZ, D., KELSEY, R., REES, J., HUDAK, P., PHILBIN, J., AND ADAMS, N. ORBIT: An optimizing compiler \nfor Scheme. SIGPLAN Notices (Proc. Sigplan 86 Symp. on Compiler Construction) 21,7 (July 1986), 219-33. \n [13] LEROY, X. Unboxed objects and polymorphic tYp\u00ading. In Nineteenth Annual ACM Symp. on Principle$ \no-f Prog. Languages (New York, January 1992), ACM Press, pp. 177-188. [14] SHAO, Z., ANEI APPEL, A. W. \nSpace-efficient closure representations. In Proc. 1994 ACM Conf. on Lisp and Functional Programming (1994), \nACM Press, pp. 150\u00ad161, [15] SHAO, z,, AND APPEL, A. W. A type-based com\u00adpiler for Standard ML. In P.oc \n1995 ACM Conf. on Programming Language Design and Implementation (1995), ACM Press, pp. 116-129.  \n\t\t\t", "proc_id": "237721", "abstract": "", "authors": [{"name": "Lal George", "author_profile_id": "81100355594", "affiliation": "Rm. 2A426 AT&T Bell Labs, 600 Mountain Avenue, Murray Hill, NJ", "person_id": "PP31077122", "email_address": "", "orcid_id": ""}, {"name": "Andrew W. Appel", "author_profile_id": "81100498630", "affiliation": "Department of Computer Science, Princeton University, Princeton, NJ", "person_id": "PP14174176", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/237721.237777", "year": "1996", "article_id": "237777", "conference": "POPL", "title": "Iterated register coalescing", "url": "http://dl.acm.org/citation.cfm?id=237777"}