{"article_publication_date": "01-01-1996", "fulltext": "\n PURE VERSUS IMPURE LISP Nicholas Pippenger Department of Computer Science The University of British \nColumbia Vancouver, BC V6T 1Z4 CANADA Abstract: The aspect of purity versus impurity that we address \ninvolves the absence versus presence of mu\u00adtation: the use of primitives (RPLfLCA and RPLACD in Lisp, \nset-csr ! and set-cclr ! in Scheme) that change the state of pairs without creating new pairs. It is \nwell known that cyclic list structures can be created by im\u00adpure programs, but not by pure ones. In this \nsense, impure Lisp is more powerful than pure Lisp. If the inputs and outputs of programs are restricted \nto be se\u00adquences of at ornic symbols, however, this difference in computability disappears. We shall \nshow that if the temporal sequence of input and output operations must be maintained (that is, if computations \nmust be on\u00adline ), then a difference in complexity remains: for a pure program to do what an impure program \ndoes in n steps, O(n log n) steps are sufficient, and in some cases Q(n log n) steps are necessary. 1. \nIntroduction The programming language Lisp (see McCarthy [7] and McCarthy et aL [8]) was inspired by \nthe A\u00adcalculus (see Church [2]), and most of its basic features are frank imitations of aspects of the \nA-calculus (with the most essential differences being in the rules for order of evaluation). In this \nway Lisp became the first signifi\u00adcant programming language to allow the computation of all partial recursive \nfunctions by purely applicative or functional programs, without side-effects. One feature of Lisp that \ngoes beyond the applica\u00adtive realm is the inclusion of primitives for what is now Permission to make \ndigital/hafi copies of all or pat-i of this material for peraonsl or classroom use is granted without \nfee provided that tbe copies are not made or distributed for profit or commemial advantage, the copy\u00ad \nright notice, the title of the publication and its date appear, and notice is given that copyright is \nby permission of the ACM, Inc. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires specific permission and/or fee. POPL 96, St. Petersburg FLA USA 01996 ACM 0-$9791 \n-769-3/95/01.. $3.50 usually called mutation . These primitive have seman\u00adtics rooted in the von Neumann \narchitecture of the ma\u00adchines on which Lisp was implemented, rather than in the A-calculus. The main \nprimitives for mutation in Lisp are RPLACA and RPLACD, which mutate the compo\u00adnents of an existing pair \n(in contrast with CONS, which creates a new pair). We shall refer to Lisp with or with\u00adout these mutation \nprimitives as pure or impure Lisp, respectively. (This usage is fairly common, but it must be admitted \nthat these terms are often used with ref\u00aderence to other features of programming languages indeed, for \nany features that happen not to fit conve\u00adniently within the writer s conceptual framework.) Our goal \nin this paper is to assess the extent to which mutation primitives add something essential to the language, \nand the extent to which they can be simulated or eliminated in favor of the purely ap\u00adplicative primitives \nof the language. As is often the case, the question can be formulated in several ways; our immediate \ngoal is to describe the formulation we have in mind, and to explain why we have chosen it in preference \nto others. We begin with a trivial observation. If pairs, once created, can never be mutated, then their \ncomponents can only be references to previously existing objects: all the arrows in box-and-arrow diagrams \npoint backward in time, and thus these diagrams are acyclic. It follows that if we allow the outputs \nof programs to be the data structures represented by such diagrams, then RPLACA and RPLACD do indeed \nadd something essential, for they make possible the creation of structures whose diagrams contain cycles. \nThis answer is not completely satisfying, however, because it assumes we want our programs to produce \na particular representation of the answer, and it is the representation rather than the answer that is \nbeyond the power of pure Lisp. When we redirect our attention from representa\u00adtions of answers to the \nanswers themselves, we are led 104 of programs are words over a finite alphabet (or, alter\u00ad natively, \nnatural numbers) both pure and impure Lisp compute all and only the partial recursive functions, and \nthus are equivalent in power. This answer is also not completely satisfying: while it describes what \ncom\u00adputations can be performed, it ignores the issues of the efficiency of these computations. When we \nredirect our attention from computability theory to complexity theory we obtain a crisp formula\u00adtion \nof our problem. Consider computational problems that have as input a word over a finite alphabet (say \na sequence of Boolean values) and have as output a yes/no answer (another Boolean value). Can every impure \nLisp program solving such a problem be transformed into a pure Lisp program with the same input/output \nbehav\u00adior, in such a way that number of primitives executed by the pure program exceeds the number performed \nby the impure program by at most a constant factor? Unfortu\u00adnately, we are unable to answer this question. \nOnly after putting two additional restrictions on computations will we be able to delineate precisely \nthe additional compu\u00adtational power conferred by the impure primitives. WJe shall say that a computation \nis symboh c if its in\u00adput and output each consist of sequences of atomic sym\u00adbols. These symbols can \nbe incorporated as the com\u00adponents of pairs, and can be distinguished from pairs by a predicate ATOM, \nbut the only other operation that can be performed on them is the two-place predicate E(J, which tests \nfor equality of atomic symbols. The crucial property of atomic symbols is that there is an unlimited \nsupply of distinct symbols, so that a single symbol can carry an unbounded number of bits of information. \nThe fact that equality is the only predicate defined on sym\u00adbols makes it very inefficient to convert \nthe information the y carry into any other form. Nevertheless, atomic symbols are a very natural part \nof the Lisp world-view, and insisting that they be treated as such seems less artificial that allowing \nprimitives (such as the EXPLODE and IMPLODE in some dialects of Lisp) that allow them to be treated as \ncomposites. We shall say that a computation is on-line if its in\u00adput and output each comprise an unbounded \nsequence of symbols and if, for every n, the n-th output is pro\u00adduced by the computation before the (n \n+ 1)-st input is received. This notion refers to an unending compu\u00adtation, and some convention is necessary \nto reconcile it with the customary view of Lisp programs as functions with finitely many arguments and \na single value. We shall regard on-line computations as being performed by non-terminating programs with \nno arguments, which re\u00adceive their inputs using a primitive READ operation and produce their outputs \nusing a primitive WRITE opera\u00ad tion. These new primitives have side-effects, of course; it should be \nborne in mind that purit y refers to the absence of RPLACA and RPLACD operations, rather than to an absence \nof side-effects. On-line computation is not part of the classical Lisp world-view, but it is a natural \ncomponent of interactive transaction-processing systems. With these notions we can now state our main \nre\u00ad sult . Theorem 1.1: There is a symbolic on-line computation that can be performed by an impure Lisp \nprogram in such a way that at most O(n) primitive operations are needed to produce the first n outputs, \nbut for which ev\u00adery pure Lisp program must perform at least Q(n log n) primitive operations (for some \ninputs sequences, and for infinitely many values of n) to produce the first n out\u00adputs. (Here !J(f(n)) \nrepresents a function of n bounded below by a positive constant multiple of ~(n).) That this result is \nthe best possible, to within constant fac\u00adtors, is shown by our second result. Theorem 1.2: Every symbolic \non-line computation that can be performed by an impure Lisp program in such a way that at most T(n) primitive \noperations are needed to produce the first n outputs, can be per\u00adformed by a pure Lisp program that performs \nat most O (T(n) log T(n)) primitive operations (for all inputs se\u00adquences, and for all values of n) to \nproduce the first n outputs. To the objection that the efficiency of mutation is too well known or obvious \nto warrant proof, we offer the following argument. It is well known that a last-in\u00adfirst-out stack discipline \nis easily implemented in pure Lisp, but the obvious implementation of a first-in-first\u00adout queue relies \non mutation to add items at the tail of the queue. But Fischer, Meyer and Rosenberg [3] have shown by \nan ingenious construction that a queue (or even a dequeue, where items can be added or removed from either \nend) can be implemented in pure Lisp with O(1) primitive Lisp operations being performed for each queue \n(or dequeue) operation. In the face of this highly non-obvious implement ation, it is unconvincing to \nclaim without proof that there is not an even more ingenious and non-obvious implementation of a full \ninterpreter for impure Lisp in pure Lisp, with 0(1) primitive pure Lisp operations being performed for \neach primitive impure Lisp operation. Indeed, after rediscovering a special case of the Fischer, Meyer \nand Rosenberg result, Hood and Melville [5] conclude: It would be interesting to exhibit a problem for \nwhich the lower bound in Pure 105 2. Discussion The question we address seems implicit in much of the \nLisp literature, but the first explicit formulation we have found is due to Ben-Amram and Galil [1], \nwho mention the cyclic/acyclic distinction, then go on to ask about the complexity of simulation. The \nuse of input values to which only certain restricted operations can be applied is well established in \nthe comparison of programming languages according to schematology , as introduced by Paterson and Hewitt \n[9]. The first use of schematology for a comparison based on complexity rather than computability is \ndue to Pippenger [10]. The special case of atomic symbols, in the sense used here, was considered by \nTarjan [13]. The restriction to on\u00ad line computation is well established in the literature of automata \ntheory; see Hennie [4]. We should say a few words about the models we use to embody the powers of pure \nand impure Lisp. These models will be the pure and impure Lisp machines. Such a machine will be furnished \nwith a built-in program which takes the form of a flowchart, with recursion being implemented by explicit \nmanipulation of a pushdown stack. Specifically, we consider programs that manipu\u00adlate values (which may \nbe atomic symbols or pairs) in a fixed number of registers; these registers contain mu\u00adtable values, \neven in the pure case. The primitive op\u00aderations are the predicates ATOM and E~ (which appear in the \ndecision lozenges of flowcharts) and the opera\u00adtions READ, WRITE, CONS, CAR and CDR (which also take \ntheir arguments from and deliver their values to regis\u00adters) and, in the impure case, the mutation operations \nRPLACA and RPLACD. The use of flowchart models allows us to ignore questions of variable binding and \nscope, since the prim\u00aditives of even the pure model allow any of the common scoping conventions to be \nsimulated efficiently. (We are assuming here that a particular program, including any attendant subprograms, \ninvolves just a fixed number of variable names, whose current bindings can be kept in a fixed number \nof registers. There is no considera\u00adtion here of mechanisms such a EVAL that would allow symbols from \nthe input to be used as variable names and bound to values. ) Flowchart models also allow us to ignore \nquestions of control structures: as mentioned above, recursion can be simulated by manipulation of a \npushdown stack; many other control structures, such as explicit use of current continuations, could similarly \nbe simulated. We have not allowed for constants (such as NIL) to be incorporated into programs, or for \nother uses of QUOTE. Our justification for this is as follows. Any pro\u00adgram will involve only a fixed \nnumber of const ants, and their only use is to be compared (via EQ) to other atomic symbols. If k such \nconstants are needed, we may assume that the input sequence begins by presenting them in some agreed \nupon order, and that they are to be echoed back as the first k outputs. The program should test that \nthey are in fact pairwise distinct; if so it can then proceed with the original computation; if not it \ncan sub\u00adstitute some agreed upon dummy computation (such as eternally echoing back inputs and outputs). \nIf we overlook the presence of atomic symbols, the impure model is very similar to the Storage Modification \nMachines introduced by Schonhage [12] (which in turn have have the model of Kolmogorov and Uspenskii \n[6] as a precursor). 3. The Upper Bound We reformulate Theorem 1.2 in terms of machines as follows. Theorem \n3.1: Every on-line symbolic impure Lisp ma\u00adchine M can be simulated by a pure Lisp machine Ml in such \na way that all outputs produced by M within the first n steps are produced by M within the first O(n \nlog n) steps. This theorem is established by the construction of a trite simulation, which will not be \ngiven in detail here. It can be obtained by modification of arguments given by Ben-Amram and Galil [1], \nbut it is just as easy to describe the construction from scratch. The key idea is to represent the state \nof the store in the impure machine by a balanced tree. The construc\u00adtion of new pairs by CONS is accomplished \nby allocat\u00ading new paths in the tree, and the allocator issues new paths in order of increasing length, \nso that the tree is kept balanced. The fetches from store implicit in CAR and CDR operations, as well \nas the updates implicit in RPLACA and RPLACD operations, are performed by fol\u00adlowing paths in the tree \nfrom the root to the nodes cent aining the relevant information and, in the case of RPLACA and RPLACD, \nin rebuilding a modified version of the path while backtracking to the root. We observe that the constant \nimplicit in the O-notation is indepen\u00ad dent of the machine M. The path copying technique just described \nwas applied by Sarnak and Tarj an [11] to the implementa\u00ad tion of persistent data structures, in which \nold ver\u00ad sions of the data structure can always be copied and updated independently. One of the benefits \nof a pure 106 4. The Lower Bound In this section we shall concoct a computation that separates the power \nof pure and impure Lisp machines. The proof that producing the first n outputs can be accomplished with \nO(n) operations in impure Lisp will be easy. The proof that pure Lisp requires fl(n log n) operations, \nwhich we shall just sketch here, is the heart of the result. At a superficial level, this proof is an \ninformation-theoretic counting argument analogous, for example, to the one used to show that Q(n log \nn) com\u00adparisons are needed to sort n items. It is not at all obvious, however, how such an argument can \ndist in\u00adguish between creation and mutation. The key to the argument is to bring about a situation in \nwhich certain information, which can be measured by a counting ar\u00adgument, can be retrieved by impure \noperations at a rate of Q(log n) bits/operation, but by pure operations only at a rate of 0(1) bits/operation. \nConsider a set of s records RI,. ... R,, each of which comprises an atomic symbol together with two pointers \nthat are used to link the records into two linear chains. The first chain, which we call the A-chain, \nwill link the records in the order A-l+l-+ . . . _R$ --iNIL. The second chain, which we call the B-chain, \nwill link the records in the order B-R=(l)-. . . ----+R=($)+NIL. where m is a permutation on {1, . . \n. . s}. We shall now describe the computational problem by describing how an impure Lisp machine solves \nit. The problem consists of a prolog comprising S2 steps, followed by an unbounded sequence of phases, \neach com\u00adprising 2s steps. The prolog takes place as follows. After checking that it has received two \ndistinct input symbols to use as Boolean values, the impure machine ill reads a tally notation for s \n(as s 1 trues follows by a false), and constructs as it does so the s records linked in the A\u00adchain. \n(The B-chain links and atomic symbols are left unspecified.) Then, for each r from 1 to s, M reads a \ntally notation for T(T), and fills in as it does so the B-chain links. (The atomic symbols are still \nleft unspec\u00adified.) The prolog finishes by reading enough additional inputs to bring the number of steps \nup to S2. (This is done just to make the number of steps in the prolog independent of the permutation \nn.) The remainder of the computation is divided into phases of 2s steps each. During the first s steps \nof each phase, M reads in s atomic symbols and stores them in the records in their order according to \nthe A-chain. During the last s steps of each phase, M writes out these s atomic symbols from the records \nin their order according to the B-chain. We have not specified what symbols M writes out during the prolog, \nor during the first half of each phase; we shall stipulate that it ethos the last symbol read at each \nsuch step. (The output does not depend on the symbols M reads during the second half of each phase.) \nIt is clear that M can per\u00adform this computation using O(1) primitive operations between each READ and \nWRITE operation. We shall restrict our attention at this point to in\u00adput sequences for which all the \natomic symbols read and written in all of the phases are distinct. This will allow us to ascribe each \nsymbol produced by a WRITE oper\u00adation during a phase to a well defined READ operation earlier in that \nphase. We observe that for any value of s, there are just s! possible permutations T that might be described \nduring the prolog. It remains to show that any pure Lisp machine M requires at least Q(s logs) primitive \noperations in each phase, for most choices of T, say for all but (s 1)!/2 choices of m. Since M performs \njust 0(s2) operations in the prolog, and just O(s) operations in each phase, we can then obtain Theorem \n1.1 by considering the first s phases, for then M will produce 0(s2) outputs using 0(s2) operations, \nwhile M will require Q(s2 logs) op\u00aderations for all but at most S(S 1) !/2 = s!/2 choices of T. Consider \nan interval [a, b] of steps. Let us say that a set of input sequences is (a, b)-coherent for the pure \nLisp machine M if, from the processing of the a-th in\u00adput z. to the b-th output yb, all test operations \n(that is ATOM and EQ operations) have the same outcome. The operation of M , restricted to such an interval \nof oper\u00adations and to a set of coherent inputs, corresponds to that of a straight-line program , in which \na fixed se\u00adquence of the primitive operations CAR, CDR, CONS, READ and WRITE is performed. Let us say \nthat an input sequence is (a, b)-psittacine for M if each of the outputs Y., . . . . y~ is equal to one \nof the inputs x~, ....Zb. Lemma 4.1:Let C be any (a, b)-coherent set of inputs for a pure Lisp machine \nM , and suppose that the inputs in C are (a, b)-psittacine for I@. Then there exists a map h : {a,... \n,6} + {a,... ,b} such that, for every 107 Sketch of Proof. For i E {a,.. ., b}, start with the WRITE \noperation that produces yi and trace back through the computation to the READ operation that received \ncor\u00adresponding input Xj such that yi = Zj. For each in\u00adput sequence, we have that j is a well defined \nvalue in {a,... , b}, and we must show that j is the same value h(i) for all input sequences in C. We \ntrace back in the following way. The WRITE operation that produces yi takes the output value from some \nregister. We trace back to the operation that put this value into the register. If this operation was \na READ operation, we are done. Otherwise, it was a CONS, CAR or CDR operation. In this case we trace \nback to the op\u00aderation that put the relevant argument into a register. We continue until we reach the \nappropriate READ opera\u00adtion. (The process of tracing back must terminate with a READ operation, since \nwe have disallowed the use of QUOTE to introduce constants.) Since M is a pure Lisp machine, any pairs \ninvolved in the process above must have been constructed during the interval [a, b] (we are starting \nfrom an output in the interval [a, b], following pointers that point backward in time, and ending with \nan input in the interval [a, b]). It follows that all the operations involved in the process take place \nin the interval [a, b], during which itl exe\u00adcutes a straight-line program. Consequently, the same input \ncj is reached from the output y~, for every input sequence in C, which is what was to be shown. A It \nis worth observing that this lemma breaks down for impure Lisp machines for two reasons: (1) the value \nproduced by a CAR or CDR operation might trace back to a RPLACA or RPLACD rather than a CONS, and (2) \npointers do not necessarily point backward in time, so we cannot conclude that all relevant operations \ntake place in the interval [a, b]. Now suppose that during some phase, correspond\u00ading to an interval \n[a, b], M performs at most t test op\u00aderations for each of at least (s 1) !/2 choices of x. The outcomes \nof these operations partition a set of (s 1) !/2 input sequences into at most 2t (a, b)-coherent classes. \nDuring each phase, ~ writes out only symbols read in during the same phase, so each of these classes \nis (a, b)-psittacine for Ill . Thus, by Lemma 4.1, the out\u00adputs produced by M are specified by one of \nat most 2t maps. But the behavior of the impure Lisp machine M calls for at least (s 1) !/2 different \nmaps, since there are (s 1) !/2 different permutations T. Thus we have 2f > (s 1)!/2 or, by taking \nlogarithms, t = L?(s log s). (Since each of the s phases needs Q(s logs) comparisons for all but the \neasy permutations (where easy means that fewer comparisons are needed), and since there are at most (s \n 1) !/2 easy permutations for each phases and s phases, ther are at most S(S 1) !/2 permutations that \nare easy for some phase. Hence at least half of the permutations (s!/2) are not easy during any phase. \nHence at least one permutation is not easy during any phase.) We can see at this juncture the roles played \nby our two special restrictions. The symbolic inputs and outputs allow each step of a phase to involve \n!d(logs) bits of information (since there are s distinct symbols in each phase), while allowing the impure \nmachine to pro\u00adcess these bits using 0(1) operations, Thus, the lower bound would break down if we required \nthe input se\u00adquence to be over a finite rather than an infinite set of symbols (and the upper bound would \nbreak down if we charged logarithmically for point er manipulation oper\u00adations, reflect ing their implementation \nusing a finite set of symbols). The on-line assumption allows the basic ar\u00adguments to be repeated ins \ndisjoint phases; without this assumption a pure machine could read the inputs for s phases before writing \nall the outputs for these phases, all using 0(s2) operations. 5. Conclusion We have shown that mutation \ncan reduce the com\u00adplexity of computations, at least when the computations are required to be performed \non-line and when their in\u00adputs and outputs may be sequences of atomic values (rather than sequences of \nsymbols drawn from a finite alphabet). We have further shown that this reduction is sometimes by as much \nas a logarithmic factor, but can never exceed a logarithmic factor. Naturally it would be of interest \nto lift either or both of the special assumptions we have made. We would conjecture that a reduction \nin complexity can occur even for off-line computations and even for com\u00adputations in which the inputs \nand outputs are words over a finite alphabet. Such a result, however, seems far beyond the reach of currently \navailable methods in computational complexity theory. 6. References [1] A.M. Ben-Amram and Z. Galil, \nOn Pointers ver\u00adsus Addresses , J. ACM, 39 (1992) 617-648. [2] A. Ghurch, The C a!culi of Lambda Conversion, \nPrinceton University Press, Princeton, NJ, 1941. 108 [3] P. C. Fischer, A. R. Meyer and A. L. Rosenberg, \nReal-Time Simulation of Multihead Tape Units , J. ACM, 19 (1972) 590-607. [4] F. C. Hennie, (On-Line \nTuring Machine Compu\u00adtations , IEEE Trans. on Eiectron. Computers, 15 (1966) 34-44. [5] R. Hood and R. \nMelville, Real Time Queue Oper\u00adations in Pure LISP , Info. Proc, Lett., 13 (1981) 50-54. [6] A. N. Kolmogorov \nand V. A. Uspenski~, On the Definition of an Algorithm , AilIS Translations (2,), 29 (1963) 217-245. \n[7] J. McCarthy, Recursive Functions of Symbolic Expressions and Their Computation by Machine , C omm. \nACM, 3 (1960) 184-195. [8] J. McCarthy et al., List 1.5 Programmer s Manual, MIT Press, Cambridge. MA, \n1962. [9] M. S. Paterson and C. E. Hewitt, (Comparative Schematology , Project MAC Conf. on Concurrent \nSystems and Parallel Computation, (1970) 119\u00ad 128. [10] N. Pippenger, Comparative Schematology and Pebbling \nwith Auxiliary Pushdowns , ACM Symp. on Theory of Computing, 12 (1980) 351 356. [11] N. Sarnak and R. \nE. Tarjan, (Planar Point Loca\u00adtion Using Persistent Search Trees , Comm. ACM, 29 (1986) 669-679. [12] \nA. Schonhage, Storage Modification Machines , SIAM J. Comput., 9 (1980) 490-508. [13] R. E. Tarjan, (A \nClass of Algorithms That Re\u00adquire Nonlinear Time to Maintain Disjoint Sets , J. Comput. and Sys. Sci., \n18 (1979) 110-127. 109 \n\t\t\t", "proc_id": "237721", "abstract": "", "authors": [{"name": "Nicholas Pippenger", "author_profile_id": "81100564606", "affiliation": "Department of Computer Science, The University of British Columbia, Vancouver, BC V6T 1Z4, Canada", "person_id": "PP77030762", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/237721.237741", "year": "1996", "article_id": "237741", "conference": "POPL", "title": "Pure versus impure Lisp", "url": "http://dl.acm.org/citation.cfm?id=237741"}