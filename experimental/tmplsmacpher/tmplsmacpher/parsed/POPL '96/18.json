{"article_publication_date": "01-01-1996", "fulltext": "\n Generating Machine Specific Optimizing Compilers Roger Hoover* Kenneth Zadeckt Introduction Most portable \ncompilers ignore the target architecture for optimiza\u00adtion phases, deferring the introduction of machine \nspecific informa\u00adtion to a final code generation pass. Optimization phases in these compilers must make \narbitrary assumptions about which techniques will produce faster code. In contrast, most vendor compilers \nutilize aspects of the machine architecture at all phases of the back end and as a result have sustained \na track record of generating better code. Because of the extensive use of machine knowledge throughout \nall back end phases of a vendor compiler, these compilers have proved difficult to port. As architecture \nlifetimes have shortened and the number of architecture variants have increased, developing such optimizing \ncompi Iers has become prohibitive y expensive. In this paper, we show how these costs can be amortized \nacross a number of architectures with a framework for abstracting the machine information from a vendor \ncompiler. We also discuss the tools we are building to tailor the abstract compiler to an architecture \nspecified by a comprehensive machine description. In the TOAST (Tailored Optimization And Semantic Translation) \nabstract compiler, all references to target architecture specific in\u00ad formation are made indirectly \nthrough information derived from a machine description. A TOAST machine description contains the physical \nresources available, a timing model, and the semantics of each instruction expressed as a small set of \nprimitives. The ab\u00adstract compiler has no knowledge of the kinds of the instructions available on the \narchitecture or the relative performance of these instructions. In some cases, the information needed \nby the abstract compiler is easily derivable from the machine description. In other cases, analysis of \nthe semantics of the instructions is required. The tools to perform this analysis are the novel aspect \nof the TOAST system and are the primary focus of this paper. We use the following terminology throughout: \ntarget architecture: a computer architecture for which we wish to build a compiler. semantic primitives: \na small set of typed, low-level, machine independent primitives, such as constant, memory Read, add, \nmultiply, and, or, or shift. Computer Science Dcpmtment. IBM TJ Watson Research Center, PO Box 704, Yorktown \nHeghts, NY 10598, rhoover@ watson.ibm,com, (914) 784-7697 tComputer science Depafiment, IBM TJ Watson \nResearch center,F CJBOX704. Yorktown Heizhts. NY 10598. zadeck@ watson.ibm.tom, (914) 7X4-6681 Permission \nto make digital/hard copies of all or part of this material for personal or classroom use is granted \nwithout fee provided that the copies are not made or distributed for profit or commercial advantage, \nthe copy\u00adright notice, the title of the publication and its date appear, and notice is given that copyright \nis by permission of the ACM, Inc. To copy ottrerwise, to republish, to post on servers or to redistribute \nto lists, requires specific permission and/or fee. POPL 96, St. Petersburg FLA USA @1996 ACM 0-89791-769-3/95/01. \n.$3.50 semantic graph: a graph of semantic primitives. machine instruction: an instruction on the target \narchitecture. The meaning of each instruction is defined by a semantic graph, desired operation: a semantic \ngraph that represents the computa\u00adtions needed to perform some action. Usually this is some high-level \noperation that the compiled program must execute. Desired operations are specified with semantic graphs. \ntranslation: a graph of machine instructions that perform the same actions as a desired operation. semantic \ncomparator: a theorem proving function that takes as input a desired operation and a set of machine instructions \nand produces one or more translations. The machine description s timing model can be used to assign a \ncost to each translation. Some parts of the compiler can be tailored to an architecture by simply finding \na reasonable translation for each desired operation and replacing those desired operations with their \ntranslations. Other tailoring of the compiler requires that decision procedures be wri\u00adtten to test the \nrelative performance of the translations of different desired operations, 1.1 Strength Reduction as an \nExample Consider strength reduction. This optimization replaces mukiplica\u00adtions by a loop s induction \nvariable with additions to a loop invariant variable. Most optimizing compilers perform some form of \nstrength reduction. However, the significant differences among implemen\u00adtations lie not in the particular \ntechnique used, but to the set of operations to which the technique is applied. The original motivation \nfor strength reduction was that multipli\u00adcation was more expensive than addition. Most portable compilers \nconsider this to generally be true and apply the technique wher\u00adever they can. A vendor compiler can \nbe more selective, as it may know that multiplication and addition take the same time, or that some multiplications \n(such as those by a power of two) maybe less expensive with addressing modes that perform shifts. To \ntailor strength reduction for a particular architecture, TOAST compares the cost of multiply operations \nwith the cost of additions in several contexts that are amenable to strength reduction. One pair of desired \noperations is used for each context. The first desired operation of a pair is the the unreduced form. \nThe second desired operation is an expression containing the reduced form. The pair of desired operations \nare translated by the semantic comparator into instruction sequences and the resulting costs of these \nsequences are compared, If the best sequence for the second desired operation is Machine Description \n o I + Machine Description Compiler nstruction nstruction Register SchedulingSemantic Execution Allocation \nTables Graphs Routines Tables Cdba Figure 1: Machine Description Compiler no faster than the first, \nstrength reduction is inhibited for this class of multiplications. This type of analysis enables TOAST \nto tailor an implementa\u00adtion to avoid code transformations that are either not productive or potentially \nregressive. Many optimization can be tailored in this fashion. 1.2 Outline In this paper we discuss \nthe TOAST compiler design and show how several novel technologies (including semantic instruction descrip\u00adtions \nand semantic comparison) can enable such portable optimiza\u00adtion tailoring, We do this in four main sections: \no A high level overview, o A number of examples of how compilation and optimization tasks are tailored \nby TOAST, e Details of our machine description and semantic comparison technology, o A comparison with \nrelated work.  2 High Level Overview The compiler generated by TOAST looks similar to any vendor com\u00adpiler, \nincluding a front end, machine dependent optimizations, a scheduler, and a register allocator. TOAST \ndiffers from conven\u00adtional optimizing compilers in the way these phases are generated. TOAST consists \nof several parts: 1. A machine description that contains a declarative description of an architecture \ns resources, instructions, and timing, 2. A machine description compiler, illustrated in Figure 1, that \nconverts the machine description into data structures and pro\u00adgram fragments.  lThe tie here should \ngo to inhibmng the reduction sme a has a tendency to increase register pressure, Semantic Comparator \nm Figure 2: Semantic Comparator Figure 3: Scheduler 3. A semantic comparator, illustrated in Figure \n2, whose knowl\u00adedge of the target architecture is provided by data structures and program fragments produced \nby the machine descrip\u00adtion compiler. This semantic comparator is used to answer complex questions about \nthe target architecture. 4. An abstract compiler whose only direct knowledge of the target architecture \ncomes from the tables and code produced by the machine description compiler and the semantic com\u00adparator. \nFor instance, the machine specific parts of register allocation (Figure 3), scheduling (Figure 4), and \nconstant propagation (Figure 5) are computed directly by the machine description compiler, since this \ninformation can be derived by examination of individual instructions. However, the ma\u00adchine specific \nparts of the front end (Figure 6), code improve-  Specific Allocator (--) Figure 4: Register Allocator \nPropagator u Figure 5: Constant Propagation Semantic Comparator Figure 6: Front End with Code Generator \nII Semantic Comparator i Table Postprocessor u Code Improver Figure 7: Code Improvement Figure 8: Strength \nReduction ment (Figure 7), and strength reduction (Figure 8) require global knowledge of the semantics \nand timing of the target architecture, Here, the semantic comparator establishes the relationship between \nthe operations required by the abstract compiler and the instruction set of the target architecture. \n The TOAST machine description has a declarative specification of the semantics of each output of each \nmachine instruction. These specifications are based upon a small number of semantic primi\u00adtives (e.g. \nor, add or memoryRead) and are compiled (via the TOAST machine description compiler in Figure 1) into \nan internal graph representation that is a variation of PIM [Fie92]. Each desired operation that the \ncompiler needs to implement is likewise speci\u00adfied in terms of these same semantic primitives. Unlike \nmost other machine description based systems, TOAST knows the meaning of these specifications. It has \nan interpreter that can execute the se\u00admantic primitives and it has embedded transformations that relate \nstructurally dissimilar but equivalent specifications. The fundamental technique used to perform the \ncustomization analysis is performed by the semantic comparator. TOAST uses a simple theorem prover to \nimplement this function. The semantic comparator returns all combinations of instructions that it can \nprove are semantically equivalent to desired operation. A transkztion is one such collection of instructions. \nEach translation has a con\u00adrexr, which records the restrictions on instruction usage required for semantic \nequivalence (e.g. immediate input values and dead outputs). In typical usage, the desired operation is \nsome function that the compiler needs to implement and the instructions are actual machine instructions \non the target architecture. Other usages are of interest, however. Comparing a machine instruction (or \ncomposi\u00adtion of machine instructions) with other machine instructions yields instruction set equivalences. \nComparing a machine instruction on one architecture with machine instructions on a second architecture \nwill generate code for cross architecture simulation or translation. The timing model of the machine \nis used to determine the cost of each translation, and the list of translations is typically filtered \nusing this cost to determine the best translation. It is important to note that structural graph matching \ntechniques are not sufficient for the purpose of compiler generation. For ex\u00adample, the unsigned expression \nz * 2 is structurally quite different than leftShift(a, 1), but semantically they are equal. While it \nis possible to employ structural matching to find some equivalences between carefully crafted specifications, \nmachine description tech\u00adnology is on] y useful if the descriptions can be shared over a number of generated \ncompilers (which are likely to be written by different authors). In addition, instructions are typically \nmore general than what is needed in a specific operation. It is therefore necessary to compare graphs \nthat have different forms but have identical se\u00adman?ics. TOAST implements semantic comparison by considering \nall graphs that will match structurally when rewritten with a small collection of semantics preserving \ntransformations. TOAST is not intended to be a fixed system, We envision many potential uses of TOAST \nS specification and semantic comparison framework, including: * A workbench for the development of portable \noptimization phases, e Instruction set design and processor simulation using code from a TOAST compiler \nthat was generated specifically for a proposed instruction set, e A tool for transforming machine suecific \notrtimizations into ones that can be automatically poct~d. 3 Tailoring Optirnizations In most portable \ncompilers, code generation is necessarily a single monohthic phase that occurs after optimization. Vendor \ncompil\u00aders decompose the task of code generation into subproblems that are best solved in conjunction \nwith other optimization. For in\u00adstance, the problem of choosing the correct addressing modes may be spread \nover several optimization. Strength reduction is coded to avoid reducing multiplications that can be \ndone by an addressing mode. Reassociation is coded to arrange additions, subtractions. and multiplications \nto look like the addressing modes of the ma\u00adchine s instructions. Code generation in a TOAST compiler \nis done in the vendor com\u00adpiler style. Instruction selection is performed directly by a tailored front \nend and is improved by the subsequent optimization phases, While this gives the compiler additional flexibility, \nit also introduces complexity that other portable compilers avoid. The IL consists of a graph containing \ntarget architecture instructions rather than a stream of portable abstract operations. Every optimization \nmust be tailored to perform its optimization over this machine specific language, Because TOAST has a \ndescription of each instruction, it is possible to have it generate the necessary glue that allows each \nphase to un\u00adderstand all of the machine dependant forms in the IL. We consider instruction selection \nand several other important compiler phases in the following subsections. 3.1 Instruction Selection The \nTOAST machine dependent IL is emitted by a front-end that is tailored as follows, First, the front-end \nis written as a typical pro\u00adcedural program that parses the target language and performs any language \nspecific high level optimizations. All operations that the front-end emits, however, are coded as special \npurpose desired op\u00aderations using the same specification language used in the machine description. Second, \nthe specifications for these desired operations are compiled into semantic graphs by the TOAST machine \ndescrip\u00adtion compiler. Final Iy, TOAST replaces the desired operation in the front end with the lowest \ncost translation computed by a semantic comparison bet ween the desired operation and the machi ne s \nin\u00adstruction set. The process is illustrated in Figure 6 and the result is a front-end that emits an \nIL that is quite close to the instruction set of the given architecture (but may have the actual machine \ninstruction abstracted due to issues like offset size that cannot be resolved until actual object code \ngeneration). The front end produces code that is locally good but is globally mediocre. It is up to the \nsubsequent phases to globally improve this code. 3.2 Code Improvement It is the responsibility of the \ncode improver to replace subgraphs of instructions with more efficient subgraphs with equivalent se\u00admantics. \nThe code improver, ill ustrated in Figure 7, is tailored for a particular instruction set by comparing \nthat instruction set with itself to yield a table of many-to-one instruction transformations that result \nin faster execution. This table of subgraph to subgraph transformations are greedily applied at compile-time \nby a simple automaton that traverses the IL. We build the transformation table as follows. For each instruc\u00adtion \ni in the instruction set 1 we compute the set: {rn 6 sernanticCornpare(i, 1) I cost(rrz) > cost(i)} For \neach of the translations in this set, we record the context in which the translation was successful (dead \noutputs of instructions, instructions with specific immediate inputs, etc.). The instructions and their \ntranslation and their context become the left hand side of a transformation. Wherever m is successfully \nlocated in the code by the automaton, the entire subgraph is replaced by Z. 3.3 Predicate Rewriting \nAs processor pipelines have become longer, the cost of conditional branching has increased. For many \ndesired operations, it is possible to rewrite the code to reduce the number of conditional branches. \nUnfortunately, the relative performance of branches and their in\u00adteraction with other instructions varies \ngreatly among different ar\u00adchitectures and implementations. Therefore, a portable compiler cannot assume \nthat it is always desirable to choose a particular branching construct or to rewrite an expression using \nnon-branching operations. For commonly occurring idioms, such as those representable as conditional assignments, \nthe relative desirability of various code transformations can be tested via semantic comparison. The \npredi\u00adcate rewriter is tailored to use the best sequences for each idiom as computed by the semantic \ncomparator. 3.4 Reassociation Reassociation rewrites expressions into a canonical form (typically sums \nof products), and it is useful if this canonical form matches the capabilities of the target architecture. \nFor instance, the size, type, and cost of immediate fields depends upon the machine architecture. A machine \ndependent reassociation phase manipulates constants and offsets to tit into the available fields. For \nexample, the IBM 370 restricts some immediate fields to [0, 4095]. If an array reference of the form \nX(i 1) occurs within a loop where i is the index variable, reassociation can rewrite the expressions \nto have the base register point to X( l), so that all offsets are positive. The size, type, and relative \ncosts of each immediate field is explicitly represented in the instruction semantics and can be used \nto tailor reassociation,  3.5 Other Optirnizations There are many additional compiler tailoring tasks \nthat are not expressible as semantic comparison problems but are greatly sim\u00adplified given the comprehensive \nTOAST machine description. For example, the semantic instruction descriptions allow the generation of \nsimulation routines that propagate constants through individ\u00adual instructions. Scheduling is likewise \naided by the architectural cost model. Register allocation makes use of instruction resource constraints. \nThese and other bookkeeping tasks keep the need for special case procedural code to a minimum. DO INSTRUCTION \n Integer.Xor.W.Ret; FORMAT X(RegO.RS(RS=Opl), Regl.WRA(WRA=Target), Reg2.RB(RB=Op2), Opcode=31, EOpcode=316, \nRec=l); NAME xor. ; VAR Target : WRITE Word GPRIO. .31]; VAR Opl : READ Word GPRIO. .31]; VAR OP2 : \nREAD Word GPRIO. .31]; VAR PC MOD AD_4 Pc; VAR CRLT : WRITE Boolean CR[LT] DELAY 4; VAR CRGT : WRITE \nBoolean CR[GT] DELAY 4; VAR CREQ : WRITE Boolean CR[EQ] DELAY 4; VAR CRSO : WRITE Boolean CRISO] DELAY \n4; VAR XERSO : READ Boolean XERISO]; ASSIGN CRLT <- IntegerSignedLT (IntegerXor (Opl, 0P2) , O) ; ASSIGN \nCRGT z- IntegerSignedGT (IntegerXor (Opl, OP2) , O) ; ASSIGN CREQ <- IntegerEQ (IntegerXor (Opl, Op2) \n, O) ; ASSIGN CRSO <- XERSO; ASSIGN Target < IntegerXor(Opl, 0p2) ; ASSIGN PC <- AddrNext (PC, 4a); \nIN RiosExt PAGE 54; IN PcExt32 PAGE 66; TIME IntegerLogicalClass 1; END ; Figure 9: xor record instruction. \n A ToAsTTechnology In this section we describe the technical details ofi TOAST semantic specifications \nfor both machine instructions and the desired operations, and e The semantic comparator that allows TOAST \nto relate these specifications. 4.1 Semantic Representation TOAST requires a substantial machine description \nto describe an architecture as each instruction s full semantics and timing must be specified. The description \nis declarative rather than procedural and is processed by a machine description compi Ier that emits \ndata structure definitions and code to be compiled into the semantic comparator. We illustrate the important \naspects of the machine description using the RS6000 S xor record instruction, shown in Figure 9. The \nINSTRUCTION clause contains the internal TOAST name of the instruction. The FORMAT clause is used to \nguide both decoding and encoding of the object code representation. The NAME clause gives the assembly \nlanguage instruction name. 2Wedohavethe~bilily 10codespecialcasesprocedurally whenallelsefails. The VAR \nclauses describe registers that the instruction either reads or writes. The fields in this clause define \na local name for the register,howitwill beused(READ, WRITE,MODor IMMEDIATE), a type, a register pattern, \nand possibly a delay. The local name and type are used in the definition of the semantics. The register \npattern indicates onto which machine registers this variable can be mapped. The local variables Target, \nOpl, and 0P2 of the example xor record instruction can be mapped into any of the 32 general purpose registers \n(GPR), while CRLT can be mapped only to the LT portion of the condition register (CR). The DELAY clause \non the LT variable indicates that this value is not available until four cycles after the instruction \ns completion. FUNCTION IntegerXor(Opl:T, 0P2: T) :T TYPE T = INTEGER(*); EXPRESSION IntegerAnd( IntegerOr(Opl, \n0P2) , IntegerComplement ( IntegerAnd(Opl, 0P2))); END ; Figure 10: xor macro. The ASSIGN clause specifies \nthe semantics of each writable variable using a side-effect free function of semantic primitives over \nreadable variables. To keep these specification manageable we have a facility for polymorphic, strongly \ntyped macros that allow us to build more complex operations from the simple ones. AsshowninFigure 10, \nIntegerXor isimplementedin terrnsof the semantic primitives IntegerAnd, IntegerCompl ement, and IntegerOr \nfor INTEGER types of all signs and lengths. Opl Opz CR.SO IN may prefix any clause or block in the machine \ndescription to allow the specification of differences in architecture variants. In this example it modifies \nthe PAGE clause, which documents the instruction s different location In the RS6000 and PowerPC architecture \nreference manuals. While IN is typically used for TIME or PAGE clauses, it is also useful in front of \nentire instructions, PROCESSORUNIT and TYPE clauses. ref ref ref and or copy Hz ~1 XERSO IN Rios IExt \nPROCESSORUNIT IntegerUnlt 1 CONTAINS IntegerLogicalClass, IntegerAddClass, IntegerShiftClass, IntegerMultiplyClass, \nIntegerDivideClass ; IN Power604 PROCESSORUNIT SimpleIntegerUnit 2 CONTAINS IntegerLogicalClass, IntegerAddClass, \nIntegerShiftClass ; IN Power604 PROCESSORUNIT ComplexIntegerUnit 1 CONTAINS IntegerMultiplyClass, IntegerDivideClass; \n Figurell: Part of thedescription of the functional units. The TIME clause indicates the class of processor \nunit that an in\u00adstruction can be assigned to and the number ofcycies the instruction consumes on that \nunit. Since architecture implementations tend to differ in the number and type of functional units, part \nof the imple\u00admentation specific description is a block that describes the number of processor units and \nthe classes of instructions that can be assigned to each unit. lntheexample isshown in Figure 11, the \nRS6000 s IntegerLogicalClass instructions are assigrredto a single IntegerUnit, whereas the 604implementation \nofthe PowerPC canassignedthcm toeitheroftwo SimpleIntegerUni-ts.The PROCESSORUNIT statements, along with \nthe TIME and DELAY clauses allow us to build a fairly accurate timing estimate of the cost ofa sequence \nof instructions. However, since it is computed in isolation it cannot include all pipeline or any cache \neffects.  4.2 Machine Description Factoring A common feature of most machine description languages is \na facmritrg mechanism that allows the writer to isolate commonly occurring architectural features. For \nexample, the [ntel X86 ma\u00adchines have many addressing modes that apply to many instructions. Enumerating \nevery addressing mode with every instruction would result in an enormous specification. Factoring allows \nthe address\u00ading modes to be specified separately and then comelated with the instructions to which they \napply. Factoring mayserve two purposes. First, it drastically reduces thesize of the specification. Second, \nitmakes explicit commonly occurring patterns that can be used to build the code generator, While matching \non the factorization simplifies the task, the quality of the code generator produced by these tools is \ndependant on choosing the correct set of features to factor the instruction set, Because of this, TOAST \nuses factoring only for the first purpose. CREQ CRGT CR,LT Figure 12: Semantic Graph ofxcm record Unlike \nother compiler generators, our machine description compiler expands out all of the factoring before any \nother processing is performed. While the expansion of factoring and subsequent processing of the machine \ndescription gives TOAST an internal representation that isnottightly tied to the specification, theexpanded \nrepresen\u00adtation can grow to an unmanageable size if a large selection of addressing modes are available. \nFortunately, itispossible to auto\u00admatically factor the internal representation by identifying identical \nsuper-expressions that share multiple sub-expressions. Not only can this re-factored representation beusedto \nsave space, but since thesemantic comparator (described below) works from outputs to inputs, coalescing \nallows its work to be shared among the identical super-expressions. Forexample,the CRLT, CRGT, and CREQoutputsshown \nin Figure 9 are generated from three macros common to all of the many record variants inthe RS6000instruction \nset. Beginning with all semantic graph outputs of all machine instructions, we identify syntactically \nidentical semantic primitives and represent these iden\u00adticalprimitives byanode inacoalesced semantic \ngraph. Edges of this coalesced graph are actually avectorof edges, each element branching to nodes representing \nthe different semantic primitives in the instruction semantic graphs. Thus, the semantic primitives generated \nbyoneinstance of the IntegerSignedLT macro (as used in the xor record instruction of Figure 9) are shared \nwith the corresponding nodes of all other record instructions. The se\u00admantic graphs diverge at the nodes \nthat correspond tothemacro s arguments. Super-expressions also occur when a gi~ en Instruction has mul\u00adtiple \naddressing modes. Thesemantic graphs for each instruction variation is identical with the exception of \nthe semantic primitives used toobtain theinstruction s input values. The coalesced graph branches to \nrepresent these different inputs. 4.3 Machine Description Processing When invoked upon a particular architecture \nA, themachine de\u00ad scription compiler:  Extracts only the sections of the description that pertain to \nthe implementation of A, e Performs syntax checking and semantic checking (including type checking all \nsemantic specifications) of the description, . Expands all of the factoring, and e Emits a data structure \nrepresentation3 of the machine descrip\u00adtion. e Produces data structures and code used by phases of the \nab\u00adstract compiler that do not require processing by the semantic comparator. These include tables used \nby the register alloca\u00adtion and scheduling phases and code to interpret instructions in the constant \npropagation phase. These data structures are compiled with the semantic comparator and upon initialization \nthe following optimizations are performed over each semantic instruction description. common sub-expression \nelimination . unreachable code removal constant propagation These optimiza[tons are necessary because \noutputs are specified independent y and abstractly. Figure 12 shows the resulting repre\u00adsentation for \nthe RS6000 s xor record instruction. 4.4 Semantic Comparison Semantic comparison is the process of identifying \nequivalences between semantic representations. [n TOAST, we compare the se\u00admantics of the desired operation \nwith the semantic descriptions of the architecture s instruction set, As equal representations are likely \nto be syntactical y different, the semantic comparator must under\u00adstand the meaning of the representations \nand construct a proof of their equality. This theorem proving must be limited, however, as the number \nof semantic comparisons needed for compiler gener\u00adation is large. hr this section, we discuss our implementation \nof semantic comparison. The semantic comparator is implemented in three phases, which we discuss separately \nbelow. The first phase discovers toe prints, single output computations of instruction graphs that are \nsemanti\u00adcally equivalent to subgraphs of the desired operation. The second phase forms @M prints, which \nare valid combinations of toe prints that can be provided by machine instructions. The final phase com\u00adbines \nfoot prints into translations that have semantics equal to the desired operation graph. While all three \nphases use potentially ex\u00adponential exhaustive searches, our prototype implementation has demonstrated \nthat their computation is quite feasible in practice for real instruction sets.4 Much of the computation \nrequired by these three phases is concentrated in finding toe prints, a process that is easily parallelized. \n4.4.1 Toe Prints Each foe prinr represents an equivalence between a subgraph of the desired operation \nand a subgraph of a single instruction. While the subgraph of the desired operation is unrestricted, \nwe consider 3We cunvntly use ML as the implementiat!on language. 4We cangeneratethe code !mpmver uablesfor \nthe RS6000 architectwe in lessthan 36 CPUhours on an RS6000, 5We introduce a restriction on the desired \noperation in our later discussion of flawed foot prints. Opl op2 ref ref and or %3 complement and 52 \n Target Figure 13: Slice of xor record only instruction subgraphs that form a slice [Wei84] from a single \ninstruction output. That is, we consider exactly those nodes of the instruction that contribute to the \ngiven output, which corresponds to the smallest portion of a machine instruction z that can be utilized \nin a program. Figure 13 illustrates the slice from the Target output of the xor record instruction of \nFigure 12. We search for toe prints by comparing each output of i with every node of the desired operation. \nAt each successful comparison, the search is continued recursively to the respective predecessor nodes \nin both desired operation and instruction graphs. The search succeeds when the inputs of i are reached \nand a record of the matched nodes is stored on a list of toe prints for instruction i. To complete the \ndescription of the toe print search, we must specify how the nodes of the two graphs are compared. We \nneed to determine whether or not the nodes of the instruction implement the same semantics as the nodes \nof the desired operation. Clearly, a node implements an identical node as does an instruction node that \nprovides strictly more defined behavior than the desired operation node (such as two add operations, \nthe instruction add with larger in\u00adteger type6). Dissimilar nodes, however, might implement the same \nsemantics. We use two techniques satisfaction and transformation to identify such identities wherever \npossible. Satisfaction is used when the desired operation node is a constant and the instruction node \nrepresents some non-constant expression. The expression implements the constant if we can determine some \nset of input values for the instruction such that its expression equals the desired operation constant. \nThe satisfaction heuristic takes a list of magic values, special values based upon each input type, and \nexhaustively tries all input combinations with a semantic graph interpreter, terminating if an expression \nvalue equal to the constant is found. Examples of magic numbers include {O, 1} for boolean, { 1, 0,1,2,4, \n8,...} for integer, etc. If the input is via a register, a load immediate instruction is added to the \ntranslation. Input values of a successful satisfaction become part of the translation context. Transformation \nexhaustively applies a carefully selected set of semantics preserving transformations to the desired \noperation graph wherever they might help the instruction s semantic primitives to implement the desired \noperation, either by syntactic comparison or by satisfaction. The TOAST semantic comparator contains \na flexible transformer, which allows easy experimentation with the transformation set. The results shown \nlater in the paper (Figure 18), use the identity transformations: Z+z+o X*O+Z Z+Z*l X+1*Z X*XVO X*OVX \nx+-z A-1 z+ l Az 6Note that overOow condition bits have their own semantic specifications. Opl op2 ref \nGIG o Target Figure 14: Semantic Graph of and complement the annihilator transformations: O+star*O O=star*O \n1 ~ star V 1 1 + lV star ()+star AO 0~0 Astar and the data conversion transformation: x : t + cast(z, \nt). Consider an instruction operation a + b that is to match the corresponding desired operation t. Using \nthe first two identity transformations, we try matching t, t + O and O + t.In order for the translation \nto succeed, tmust be an addition of equal or smaller type, or t is matched by a (alternatively b) and \nb (alternatively a) matches the constant O. Of course, these subsequent matches may involve further transformation \nor satisfaction. Transformations are performed only to the desired operation and are selected only when \nthe right hand side is guaranteed to match the next instruction node in the output-to-input matching \ntraver\u00adsal. Since a transformed semantic graph node must be matched to an instruction semantic graph \nnode before undergoing subsequent transformation, all transformation sequences are bounded. As an example, \nconsider the RS6000 and complement instruc\u00adtion of Figure 14 as the desired operation in a semantic comparison \nwith the RS6000 instruction set (which includes the xor record in\u00adstruction of Figure 12)8. Since the \ntoe print search tries to compare all nodes of the desired operation with all instruction outputs, at \nsome point the complement node of Figure 14 will be compared to the bottom and node of the slice of Figure \n13. Since these nodes are dissimilar, a series of transformations shown in Figure 15 are applied to the \nsubgraphs of the desired operation, each bringing the desired operation structurally closer to the xor \nslice. The first (identity) transformation introduces an and to match the and at the Target output of \nxor record. The second (annihila\u00adtor) transformation replaces the 1 of the previous transformation with \nan or of star (which matches anything, in this case Opl) and 1 (which is matched with Op2 by trivial \nsatisfaction. The third (identity) transformation introduces another and, whose in\u00adputs match Opl (equal \nreferences) and Op2 (trivial satisfaction), A subsequent pass verifies that duplicate matches in the \nDAG are compatible, in this case Opl has been matched with star and ref; Op2 has been matched twice with \n1. The resulting toe print is illustrated in Figure 16. Note that we have only described one sequence \nof successful transformations here. Many other transfor\u00ad mations are tried and many are rejected, resulting \nin 34 toe prints in the semantic comparison over the entire instruction set.  4.4.2 Foot Prints Toe \nprints correspond to the use of a single output of an instruction, However, instructions often generate \nmultiple outputs (condition star ISa special semantic primitive that can be implemented by any other \nsemantic primmve. 8This computation is part of bu}lding the RS6000 code improver, u $ complement -l:int,32 \nM 1 =+star V 1 $K?JF ref -l:int.32 o complement Figure 15: Transformation and Satisfaction OD1 Opz  \n  GNi?l complement @and Target Figure 16: Toe Print of xor record codes, etc). A ,joot prinr represents \nthe use of an entire instruction, which might only involve a single output but in general could make \nuse of multiple outputs. Thus, we consider as potential foot prints all combinations of toe prints for \na given instruction. A potential foot print is elevated to become a foot print if it satisfies two properties. \nFirst, it must be conshrrr and second, it must beflu~less. A foot print is incmrsistcnf if any instruction \ninput has been assigned incompatible values by the satisfaction process. We consider a foot print to \nbe jawed if it only partially fulfills the computation needs of the desired operation. This occurs when \nthe all of the following hold. 1. The desired operation requires a sub-expression s of the in\u00ad struction \n 2. s is not available as an output of the instruction 3. s is used in the desired operation in computations \nnot matched by the foot print  Since the matching phase does not consider combinations that du\u00adplicate \nnontrivial expressions, flawed foot prints will inhibit a suc\u00adcessful translation. The example of Figure \n16 is both consistent and flawless and is therefore a valid foot print. Foot prints that pass these tests \nare added to a foot print set global to this semantic comparison. Note, however, that as many instructions \nfor a given architecture may provide similar capabili\u00adties, many foot prints will match identical subgraphs \nof the desired operation. As each of these instructions are interchangeable for the purpose at hand, \nwe filter the global foot print set into a set of foot print caregwries, Each category contains a set \nof foot prints that match the same subgraph of the desired operation and have identical translation contexts. \nThe semantic comparison example (above) finds all three possible foot print categories. 4.4.3 Translations \nThe translation phase searches the entire space of all combinations of foot print categories with two \nlimitations. First, each foot print must match at least one semantic primitive in the desired opera\u00adtion. \nSecond, only trivial amounts of the desired operation (i.e. constants) may be provided by more than one \nfoot print. Both re\u00adstrictions dramatically y reduce the search space. These restrictions avoid the inclusion \nof a potentially unbounded number of no-op instructions, completely dead instructions, or instructions \nthat du\u00adplicate the computation of other instructions. The translation algorithm exhaustively tiles the \ndesired oper\u00adation graph with all combinations of foot prints. There are two important optimization to \nnote: 1. initially, any foot print maybe placed. Once placed, however, this foot print will preempt the \nplacement of any foot prints that cover any of the same nodes of the desired operation. After each placement, \nthe foot print list is filtered to eliminate all foot prints that contain covered nodes. 2. Since all \nnodes of the desired operation must be covered for a translation, a check is made to see if all nodes \ncould possibly be covered by foot prints remaining after step(I). If it could not possibly cover, the \nsearch is abandoned.  All successful tilings of the desired operation graph are returned by the TOAST \nsemantic comparison function. While each transla\u00adtion represents many similar instruction matches, TOAST \nretains the Opl o Y Figure 17: Translation for and complement and. The semantic comparison example \ntiles and complement with foot print categories two ways. These two foot print cate\u00adgory matches represent \n114 different instruction combinations with semantics equal to the and complement instruction. 5 Related \nWork The major difference between TOAST and other compiler-compilers is the depth of analysis that the \nsystem performs and the amount of customization that is done to the generated compiler. Both improvements \nare a direct result of the comprehensive nature of our machine description. 5.1 Tree-based techniques \nMuch of the recent work on portable code generation technol\u00adogy [SU70, AJ76, GG78, AG89, Pro92] has used \ntree based instruc\u00adtion specifications to generate code from a tree based intermediate language. The \nprimary result of this work is that locally optimal code can be efficiently generated in this tree context. \nTOAST is based on the assumption that reasonable code genera\u00adtion over DAGS is better than optimal code \ngeneration over trees. With a few exceptions (most notably MIPS), architectures contain many instructions \nwith more than one output. While it may be acceptable to generate two instructions to obtain both outputs \nof a divide-remainder instruction, it is not acceptable to duplicate in\u00adstructions that produce condition \ncodes. Furthermore, intermediate code containing common sub-expressions is not reasonably handled by \ntree-based techniques.  5.2 Work of Kessler Our work originally started out in the style of Kessler \ns disserta\u00adtion [Kes84]g. He had a machine description that described the semantics of each instruction \nin terms of z small set of low-level factored list expanding by foot print categories only where indi\u00ad \n9Neither Kessler s dissertation norhissubsequentpaper[Kes86] describehismatch\u00ad vidual instruction matches \nare needed. Figure 17 shows the and ing algorithm in detail. Our knowledge is based on personal conversations \nwith him. complement instruction matched by foot prints of xor record and primitives. This description \nwas then used as input to a matching engine that attempted to discover special cases of one instruction \n(or a combination of instructions) that performed the same operations as another instruction. The problem \nwith this approach is that the set of matches is highly dependent on the sryle used to describe the instructions. \nKessler s matcher is almost entirely structural, though he did com\u00admutative and associative transformations. \nWhile many matches are found with such a simple approach, many are also missed. Our technique is more \npowerful than Kessler s because of our transfor\u00admation techniques. We believe that our semantic comparison \ntechniques find more translations than Kessler. A direct comparison would be diffi\u00adcult as Kessler s \ntest cases were older C[SC architectures (VAX, MC68000) and his system is no longer operational to perform \ncom\u00adparisons using modern machines,  5.3 Peephole Optimizer (PO) The PO system [DF84b, DF84a] contains \ntechnology similar to, though weaker, than our semantic comparator. Unlike our system, (and Kessler s \nsystem), PO determines the matching sequences by looking at combinations that occur in intermediate code \nderived by running the front end of the compiler on a training suite. Pairs and triples of instructions \nthat occur inside the range of the peephole are looked up in the machine description. The matching technique \nis structural but, because of the factoring done by the machine description writer, it often succeeds. \nIfa match is found, it is written to a log file. The generated compiler is derived by manipulating the \nthis log. The quality of matches found is dependent on the quality of the test suite. While a comprehensive \ntest suite will cover commonly occurring cases, it is unreasonable to assume that a test suite can be \ndeveloped to cover all possible cases. Comparisons made between Kessler s technique and PO seem to indicate \nthat PO rarely finds anything that the Kessler s technique would not find; though the converse is certainly \nnot true. Kessler s technique finds substantially more matches than PO (although it could be argued that \nmany of these matches are never utilized). While PO is capable of producing a code generator similar \nto ours, it does not have the ability to use this matching capability to customize other phases of the \ncompiler where high level actions are based on comparing the cost of matches as we do with strength reduction. \n5.4 GNU Superoptirnizer The Gnu SuperOptimizer (GSO) [GK92] uses a matching strategy by Massalin [Mas87] \nthat is significantly more powerful than our tiling and transformations. It is also significantly more \nexpensive and cannot be extended to consider the entire processor problem state instruction set. TOAST \nis based on manipulating the semantics description of instructions until a sequence can be tiled over \na single instruction, In GSO, the semantics are entirely operational: an interpreter is built for each \ninstruction that is to be simulated, if possible using the C asm construct. A C function is also written \nthat computes the ~esired operation that GSO is searching for, Every possible combination of an instruction \nsubset (up to a small sequence length) is tried using a small number of specially chosen inputs and the \noutputs are compared to the val ties computed by the desired operation. A match is declared when some \nsequence finds the right answer on all of the specially chosen inputs. GSO is capable of discovering \ntruly obscure sequences that perform some desired operations, but is limited by its high search costs. \nOn today s hardware, the Massalin technique is limited to testing short sequences (< 6 long) chosen from \na small instruction subset (< 10). It is unlikely that this search technique could be expanded to consider \nthe entire instruction set of a processor or to find matches that contain long sequences of instructions. \nThe TOAST satisfaction technique is similar to the GSO tech\u00adnique. Our matching discoveries are limited \nby the immediate input values and transformations considered. TOAST cannot discover ob\u00adscure mathematical \nidentities not provable by these manipulations. TOAST S primary advantage is its ability to match with \nthe entire instruction set. In addition, TOAST S matching performance vs. run\u00adning time can be scaled \nby adding or removing transformations and immediate inputs in the matching engine.  5.5 GNU CC The \nmost successftd portable compiler to date is the Gnu C compiler. Although it has a different implementation, \nthe GCC code generator is largely modeled on PO [GK92]. It has been ported to a wide array of processor \narchitectures and in many cases generates code that exceeds the performance of code compiled by the processor \nmanufacturers compilers. Our abstract compiler is, in principal, very similar to the Gnu C. Both compilers \nuse an IL derived directly from the instruction set and embed knowledge of the machine throughout the \nback end. However, the way that the two compilers are taught about the machine is entirely different. \nEach GCC port includes a set of hand specified configuration data that gives machine independent compiler \nhints about the target architecture. While the specification may contain enough informa\u00adtion to figure \nout which instruction sequences can be replaced by simpler sequences, the specification contains no timing \ninforma\u00adtion. The only cost model available is that shorter is better. No attempt is made to derive any \nnontrivial information from the spec\u00adification. For example, the person responsible for the port must \nidentify which forms of multiplication should be strength reduced. TOAST technology automates this process. \n6 Summary and Current Status There are many parts of a compiler that can benefit from customiza\u00adtion \nbased upon the target architecture. While much of the current portable compiler work limits this customization \nto code generation, allocation, and perhaps scheduling, the TOAST compiler architecture demonstrates \nthat virtually all phases of the compiler can benefit from knowledge of the machine architecture. The \nTOAST machine description compiler is written as an at\u00adtribute grammar using Linguist. We have a complete \nmachine description for the IBM Power family and a partial machine de\u00adscription for the Intel X86 family. \nOur machine descriptions are divided into two parts. The first is a machine independent part that is \ncommon to all architectures. The version that we currently use contains 37 semantic primitives and 89 \nhigher level macros. This specification is about 1000 lines long. The second part describes an entire \nfamily of architectures. The Power architecture contains the full description of 11 variations of both \nthe PowerPC and RS6000 lineage, and is about 8000 lines, We have built a prototype matching engine (in \nSML), which we have used to construct the code improvement table described in Section 3.2. Figure 18 \nillustrates the number of match categories found (o) and the number of implied instruction matches (+). \nAs of the writing of this paper, it is unclear what portion of these matches are useful and usable for \ncode improvement. There are several factors here that require further study. First, is unclear how T \nGranlund and R. Kenner. Eliminating branches us\u00ading a superoptimizer and the GNU C compiler. Proc. SIGPLAN \n92 Symp. on Compiler Construction, 7( 7):341\u00ad352, June 1992. Published as SIGPLANNotices Vol. 27, No. \n6. 80000 [GK92] ; 70000 t 600001 : 40000  +++ ~ [Kes84] e !2 ; ; s 30000\u00ad+++ 20000\u00ad++ o 10000-++ 00000000 \n0 -+$~ 12345 10 15 0 0 o 20 25 [Kes86] Matches of z instructions [Mas87] Figure 18: RS6000 Code Improver \noften matches of a dozen or two instructions in length will occur in code generated by real programs. \nAlso, some larger matches that do occur are composed of smaller matches, which our greedy replacement \nalgorithm may be able to process. Thus, we may wish to limit the code improver to smaller matches. Second, \nit will be necessary to preprocess the translations emitted by the code generator at compiler generation \ntime to limit the number of match categories that must be searched to find a match at compile time. Wehave \ndesigned TOAST with facilities for experimentation. For example, our semantic comparator easily allows \nitstransfor\u00admation set and other parameters to be manipulated at compiler generation. We are currently \nbringing the scheduler and register allocator on Iinc, which will complete our test bed for determining \nthe effects of changes in the underlying matching technology upon the quality of the generated code. \n[Pro92] [SU70] [Wei84] References [AG89] A. V. Ahoand S. W. K. Ganapathi, M. Tjiang. Codegen\u00aderation \nusing tree matching and dynamic programming. ACM Trans; on Pvogramrrjing Languages an> System;, 11(4):49I \n516, October 1989. [AJ76] A. V. Ahoand for expression S. C. Johnson. Optimal code generation trees. J. \nACM, 23(3):488-501, July 1976, [DF84a] J. W. Davidson and C. W. Fraser. Automatic generation of peephole \noptimization. Proc. SIGPLAIV 84 Symp. on Compiler Construction, pages 111-116, June 1984. Published as \nSIGPLANNotices Vol. 19, N0.6. [DF84b] J. W. Davidson and C. W. Fraser. Code selection through object \ncode optimization. ACM7 rans. on Programming Languages and Systems, 6(4):505-526, October 1984. [Fie92] \nJohn Field. Asimple rewriting semantics forrealisticim\u00adperative programs and its application to program \nanalysis. In Proc. ACM SIGPLAN Workshop cm Parrial Evalua\u00adtion and Seman?ics-Based Program Maniplilam?n, \npages 98-107, San Francisco, June 1992. Published as Yale University Technical Report YALEU/DCS/RR-909. \n[GG78] R. S. Glanville and S. L. Graham, A new method for compiler code generation. Corrfi Rec Fifih \nACM Symp. on Primiptes qf Pmgrarnming Lan<qua~es, pages 231\u00ad240, January 1978. P. B. Kessler. Automated \nDiscovery of Machine-Specific Code Improvements. PhD thesis, Computer Sci. Dept., U. of California at \nBerkeley, Berkeley, CA, December 1984. P. B. Kessler. Discovering machine-specific code im\u00adprovements. \nProc, SIGPLAN86 Symp. on Compiler Con\u00adstruction, pages 249-254, July 1986. Published as SIG-PLAN Notices \nVol. 21, No. 7. H. Massalin. Superoptimize~ A look at the smallest program. Secand Int. Conf. on Architectural \nSupport for Programming Languages and Operating Systems, 22(10): 122-126, October 1987. T. A. Proebsting. \nSimple and efficient burs table gener\u00adation. Proc. SIGPLAN 92 Symp. on Compiler Constric\u00adtion, 27(7):33 \n1-340, July 1992. R. Sethi and J. D. Unman. The generation of optimal code for arithmetic expressions. \nJ. ACM, 17(4):715-728, October 1970. M. Weiser. Program slicing. IEEE Trans. on Sofware Engineering, \n10(4):352 357, July 1984.  \n\t\t\t", "proc_id": "237721", "abstract": "", "authors": [{"name": "Roger Hoover", "author_profile_id": "81332504824", "affiliation": "Computer Science Department, IBM TJ Watson Research Center, PO Box 704, Yorktown Heights, NY", "person_id": "PP31079943", "email_address": "", "orcid_id": ""}, {"name": "Kenneth Zadeck", "author_profile_id": "81100344996", "affiliation": "Computer Science Department, IBM TJ Watson Research Center, PO Box 704, Yorktown Heights, NY", "person_id": "P160474", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/237721.237779", "year": "1996", "article_id": "237779", "conference": "POPL", "title": "Generating machine specific optimizing compilers", "url": "http://dl.acm.org/citation.cfm?id=237779"}