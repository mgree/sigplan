{"article_publication_date": "01-01-1996", "fulltext": "\n A Provably Time-Efficient Parallel Implementation of Full Speculation John Greiner and Guy E. Blelloch \nCarnegie Mellon University {jdg,blelloch}f2 cs. emu. edu Abstract Speculative evaluation, including \nleniency and futures, is of\u00adten used to produce high degrees of parallelism, Existing speculative implement \nat ions, however, may serialize com\u00adputation because of their implementation of queues of sus\u00adpended \nthreads. We give a provably efficient parallel im\u00adplementation of a speculative functional language on \nvazi\u00adous machine models. The implementation includes proper parallelization of the necessary queuing \noperations on sus\u00adpended threads. Our target machine models are a butterfly net work, hypercube, and \nPRAM. To prove the efficiency of our implementation, we provide a cost model using a profil\u00ading semantics \nand relate the cost model to implementations on the parallel machine models, Introcluction Futures, \nlenient languages, and several implement ations of graph reduction for lazy languages all use speculative \neval\u00ad uation (call-by-speculation [15]) to expose parallelism. The basic idea of speculative evaluation, \nin this context, is that the evaluation of a function body can start in parallel with the evaluation \nof the function arguments. The evaluation of the body is then blocked if it references an argument that \nis not yet available and react~vated when the argument be\u00ad comes available. With futures in languages \nsuch as Mu]\u00ad tilisp [13, 14, 27] and iVIultiScheme [22], the programmer explicitly states what should \nbe evaluated in parallel using the future annotation. In lenient languages, such as Id [25] and pH [26], \nby default all sttbexpressions can evaluate spec\u00ad ulatively. With parallel implementations of lazy graph \nre\u00ad duction [30, 18] speculative evaluation is used to overcome the inherent lack of parallelism of laziness \n[19, 36]. Although call-by-speculation is a powerful mechanism to achieve high degrees of parallelism, \nwith current impletnen\u00ad tations it can be hard to understand the performance char\u00ad acteristics of a program \nwithout a reasonably deep under\u00ad standing of the implementation. An important cause of this problem is \nthe handling of blocked threads. Most imple\u00ad mentations will suspend a blocked thread by placing it on \na queue associated with the value it is waiting for, and when the value becomes ready the implementation \nwill reactivate Permission to make digital/hard copies of all or part of this material for personal or \nclassroom use is granted without fee provided thst the copies are not made or distributed for profit \nor commercial advantage, the copy\u00adright notice, the title of the publication and its date appear, and \nnotice is given that copyright is by permission of the ACM, Inc. To copy otherwise. to republish, to \npost on servers or to redistribute to lists, requires specific permission and/or fee. POPL 96, St, Petersburg \nFLA USA G 1996 ACM 0-89791-769-3/95/01. .$3.50 Machine Model Time Butterfly (rand.) O(W/P + dlogp) Hypercube \n(rand.) O(wjp + dlogpj CRCW PRAM (rand.) O(W/p + dlogp/loglogp)  Figure 1: The mapping of work (w) and \ndepth (d) in the Parallel Speculative J-calculus to running time, with high probability, on various machine \nmodels with p processors. The results assume that the number of independent variable names in a program \nis constant, We assume the butterfly has p logz p switches, and the hypercube can communicate over all \nwires simultaneously (multiport version). all the threads waiting on the queue [22, 17, 27, 6, 20, 10, \n7]. The problem is that all implementations we know of sequen\u00adtialize these queues, which in turn can \nfully sequentialize programs that appear to be highly parallel. This will happen when all threads access \nthe same value and get suspended on a single queue. This sort of performance anomaly is, in fact, not \nunusual in implementations of parallel languages. With the aim of avoiding such performance anomalies, \nwe specify a provably time-efficient implementation of call\u00adby-speculation which fully parallelizes the \nqueues of sus\u00adpended threads as well as the other aspects of the imple\u00admentation. To make the cost of \nthe source language explicit we use a profiling semantics similar to that of Roe [33]. This semantics \nspecifies the cost of a computation in terms of the total work it performs and its parallel depth (the \nlength of the critical path of sequential dependence). We call this cost-augmented language the parallel \nspeculative A-calculus (PSL), We then specify an implementation of the PSL on various machine models \nand prove a relationship between the work and depth given by the profiling semantics and the running \ntime on these machines. This relationship is specified in terms of asymptotic bounds (see Figure 1) and \nincludes all costs of the computation, except for garbage collection. With sufficient parallelism (z. \ne., when the first term dominates) these bounds are work-efficient-the ma\u00adchine does no more than a constant \nfactor more work (pro\u00adcessor x time) than required. We keep our implementation of call-by-speculation \nrela\u00adtively simple to ease the proof of the simulations bounds. As such, and given that the bounds are \nbased on big-O anal\u00adysis, the implement ation is not concerned with constants and therefore not particularly \npractical. For example, we use a high ratio of communication to computation, aggres\u00ad ble tasks or only \nselectively execute tasks, thus reducing the work over call-by-value semantics. Figure 2: A comparison \nof the form of dependence graphs for an application el ez, where el evaluates to kc. e. On the left is \nthe graph for parallel call-by-value evaluation and on the right, for call-by-speculation evaluation. \nsive sparking of threads, no local caching of values, and no compiler optimization. In Section 6.1 we \ndiscuss how the implementation can be made more practical without effect\u00ading the bounds. In particular \nwe believe our implementation of parallel queues is quite practical. 1.1 The model The language we consider \nfor the paper is the pure J-calculus with some arithmetic primitives. In Section 2 we describe how most \ncommon language features can be added with only constant overheads. In the A-calculus it is safe to evaluate \nthe two expressions in a function application e 1 ez in paral\u00adlel. In previous work [2] we considered \ncall-by-value paral\u00adlelism in which el and ez are evaluated in parallel to return v 1 and V2. The processes \nthen synchronize at which point v 1 is applied to vz. This form of parallelism allows a pro\u00adgram to evaluate \nall the arguments to a function in parallel but must hold off on executing the function body until all \narguments are completed. In call-by-speculation it is assumed that in a function ap\u00adplication el ez not \nonly can e 1 and ez evaluate in parallel, but if el evaluates to Ax. e, then the body e can continue \nevaluating in parallel with ez (see Figure 2). This allows for a form of pipelined parallelism and can \nlead to asymptotic improvements in the depth of programs over parallel call\u00adby-value evaluation On the \nother hand, call-by-speculation is more difficult to implement since now every value can po\u00adtentially \nbe a synchronization point. Furthermore since an arbitrary number of threads can access any given value \nthe synchronization might be among a large number of threads. In call-by-value evaluation only pairs \nof threads can fork and synchronize. In this paper we distinguish between a fully speculative semantics, \nin which it is assumed that the body and argu\u00adment are always evaluated, and a partially speculative \nse\u00admantics for which speculation is limited. We use the term call-by-speculation to refer to a fully \nspeculative semantics. Such a semantics will execute the same work as the call-by\u00advalue semantics. In \nthis paper our main results are based on call-by-speculation, but in section 6.2 we consider partially \nspeculative implementations that can either kdl inaccessi\u00ad  1.2 The Problem We now look at an example \nof the problem with implement\u00ading call-by-speculation. Consider the following code, let y=exp in pmap \n(Ax. x + y) data which adds the result of expression ezp to each element of data. We assume that pmap \nis some form of parallel map, and that data has n elements and is either an array (as with I-structures \nin Id) or a tree (as could be implemented in Multilisp). In this code if exp requires more time to compute \nthan the time to fork the n threads for the parallel map, then all the n threads will need to b~ock waiting \nfor y. One possibility is to have the threads spin and keep checking if y is ready yet. Such spin waitzng \ncan be very inefficient since the processor will be tied up doing no useful work. In fact, without having \na fair schedule it can cause deadlock since the thread that is computing y might never be scheduled. \nTo avoid these problems most implementations will suspend a thread by adding it to a queue associated \nwith the variable y (some implementations will spin for a fixed amount of time and then suspend [5]). \nIn many cases such suspension works well, but the prob\u00adlem with the given code is that a large number \nof threads will try to suspend on a single variable almost simultaneously. Current implementations sequentialize \nthis process by using a linked list for the queue [22, 17, 6, 24, 7], and therefore would sequentialize \nthe code. We note that a call-by-value semantics would not have this problem since the value of y would \nbe computed before executing the pmap. Reading the value would require a concurrent read to the location \nof y, which is assumed in our machine models. Even without di\u00adrect hardware support for concurrent reads \nthe code would be reasonably well-handled by a cache since only one thread per processor would actually \nread the value from the shared memory (future threads would find it in the cache). To avoid this problem \nwe need to be able both to enqueue (when a thread blocks) and dequeue (when the threads are reactivated) \nin parallel. A potential solution is to use a tree instead of a linked list to represent the queue, The \nproblem with this is that although it would make it easy to dequeue in parallel, it is not clear how \nto implement the enqueue in parallel, Our solution is based on using a dynamically growing array for \nthe queue. The basic idea is to start with an array of fixed size. When the array overflows, we move \nthe elements to a new array of twice the number of elements in the queue. Adding to the array, growing \nof the array, and dequeuing from the array can all be implemented in parallel using a fetch-and-add operation \n[11, 31]. To account for the cost of growing the array, we amortized it against the cost of originally \ninserting into the queue. As well as handling the queues the implementation also has to handle the scheduling \nof the threads. This is some\u00ad what more complicated than in the in the call-by-value im\u00ad plementation \nsince completing a thread can reactivate an arbitrary number of suspended threads rather than just cre\u00ad \nating pairs of threads [2].  1.3 Structure of the implementation Our implementation and bounds are based \non simulating the PSL on the target machines. We stage this simulation into two parts to simplify the \nmapping. We define an inter\u00admediary model using an abstract machine called the Fully Speculative Abstract \nMachine (FSAM). It performs a series of transitions on sets of states, where a state includes an environment, \na J-calculus expression, a cent inuat ion, and a queue of suspended threads. We prove that a derivation \nin the PSL model can be simulated in the FSAM model with only constant overheads in work and depth. The \nsecond half of the PSL simulation is a mapping from the FSAM onto the target machines. At each step of \nthe FSAM, the active states are mapped to processors in a load\u00adbalanced manner, and the FSAM state transition \nsimulated locally on these states. The primary data structures are queues, with parallel enqueuing and \ndequeuing, and envi\u00adronments, represented as balanced binary trees. 2 The PSL Model We define our cost \nmodel by adding cost measures to a standard operational semantics of the A-calculus and call it the Parallel \nSpeculative A-calculus (PSL). It defines the work, minimum depth, and maxtmum depth of an evalua\u00adtion, \nwhich we use in our simulation bounds of Sections 3 and 4. The minimum depth represents the depth at \nwhich a handle on the value becomes available, and the maximum depth represents the depth at which all \nwork in the compu\u00adtation is complete. We use the A-calculus as it represents a minimal language and allows \na simple semantics. While it does not directly include many common features of lan\u00adguages, such as data \nstructures and conditionals, we discuss in Sections 2.2 and 2,3 how these can be encoded with con\u00adstant \noverheads. It also does not include recursion, and we discuss in Section 2.4 the choice between encoding \nand ex\u00adplicitly including it in the language and how this affects the costs of a computation. To gain \nintuition about the cost measures of a computa\u00ad tion we find it helpful to view the computation as a \ndepen\u00ad dence graph (see Figure 3). Each node is a st ate representing constant work and depth, and each \nedge in the graph repre\u00ad sents either a control or data dependence. The total number of nodes is the \nwork and the shortest path from a node to the root of the graph is its depth. The evaluation of every \nexpression has three important nodes, a source node, and minimum and maximum sink nodes, corresponding \nto the minimum and maximum depths. Constants, variables, and A-expressions require constant time and \ndepth to evaluate, and so correspond to sing~e edges. Graphs of applications have three subgraphs, one \nfor the function, one for the argument, and one for the evalua\u00ad tion of the function body on the argument. \nThere is a de\u00ad pendence from the minimum sink of the function subgraph to the source of the function \nbody subgraph. There can also be data dependence edges from either the function or argument subgraphs \nto the function body subgraph, which represent the need of the body for a value calculated by those subgraphs. \nThe standard syntax of the J-calculus with constants is e ::= c[zlk.elelez where z ranges over a countably \ninfinite set of variables, and c ranges over a countable set of constants. Without loss of A d dl A \n1 2 d2 d; Figure 3: Speculative dependence graphs. At the top is the general form, where the source \nnode is at depth d2 the mini\u00admum sink node at d , and maximum sink node at d. For con\u00adstants, variables, \nand abstract ions, the sink nodes coincide. For general applications, d = d~ and ~ = max( ~1, ~Z, ~s). \ngenerality, we assume that all constants have an arity of zero or one. The operational semantics of the \nlanguage defines the value resu~t ing from the evaluation of a program. Values are either constants or \nclosures: v .. .= c I cl(E, z, e) An envn-onment E is a finite mapping from variables to values and \ncan be the empty mapping [ ] or an environment extended with a new binding E[z + v]. We augment a standard \ncall-by-value semantics, written E b e &#38; v, that defines the evaluation of an expression in an environment \nto a value, We add intensional information which describes the call-by-speculation of our model, defin\u00ading \nwhen the result value of each subexpression is avail\u00adable [33]. To define the minimum depth of a variable, \nwe must tag values in the environment with the depths at which they become available. Costs, tagged values, \nand tagged en\u00advu-onments, respectively, are defined as follows. w,d, d ::= 0111 .<. lcxJ b ::= c I cl(fi, \nz, e) E: Variables + ( Tag Values x Costs) We assume that the untagged value denoted v and environ\u00adment \ndenoted E are those obtained by omitting all costs in v and E. The profiling semantics is given by the \nrelation A;dkc Ac; l,d+l, d+l (CONST) ti(z) = i; d (VAR) E;d k z ~ ti; l,max(d, d ) +I)max(d, d )+l \n ~;dl-k.e~c l(~,z,e); l,d+l, d +1 (ABS) fi; d+l E el ~ cl(~ , z,e); wl, dl, ~l E;d+l Fez~ti; wz, dz, \ndz @ [z * ti; dz]; dl } e ~ ti ;ws,ds,~s (APP) E;dl-e1e2~ ti ; wl+wZ+ws+ 1, d~,max(~l, ~z, J3) E;d+lkel~c;wl,dl,~l \nE;d+lFez~ti;wz,dz,dz (APPC) ~;d~ele~~cl(c,ti);wl+wz+ 2, max(dl, dz)+l, max(~l, 22) + 1 Figure 4: Profiling \nsemantics of the PSL model The dot\u00adte~ environments ~nd values reflect the fact that the model tags values \nin environments with cost information. Compare APP to the corresponding DAG diagram. defined in Figure \n4. Since it augments the operational se\u00ad mantics, it holds only if E 1-e &#38; u. Evaluation begins at \ndepth d, a handle on O is available at depth d , and e fin\u00adishes evaluation at depth ~. We are primarily \ninterested in the costs of a program e when evaluated in the empty envi\u00ad ronment starting at zero depth, \nz.e., []; O k e * ti; w, d , ~. Since each inference rule of the PSL semantics introduces only constant \noverheads, the judgments in a P SL evaluation derivation correspond to the nodes of the dependence graph. \nThese constant overheads also anticipate the overheads of the abstract machine of Section 3. The minimum \nand maximum depths of a computation are generally not equal for two reasons. First, if the result value \nis a closure, this closure is returned without waiting for values in its environments to finish evaluation. \nFor example, using any standard A-calcuIus encoding of lists, a cons op\u00aderation returns a cons-cell while \nits components are possibly still evaluating. The minimum depth tracks when the cell is leturned, whereas \nthe maximum depth tracks when its com\u00adponents are also done. Second, evaluation need not wait for other \ncomputations which are irrelevant to this one. For example, evaluation (Jz.0) e returns a value after \nconstant depth, regardless of e The minimum depth of the whole computation is independent of that of \ne, but the maximum depth is not. In particular, the minimum depth of this ap\u00ad plication is finite even \nif its work and maximum depth are not (z. e., if e does not terminate) Now consider each inference rule \nin Figure 4. We as\u00ad sume that evaluating constants and abstractions requires un,t work and depth. We \nalso assume that looking up a variable m an environment requires unit work and depth, but the lookup \ncannot happen until the minimum depth at ~(add, i) = add, ~(add,, i ) = 2 + i ~(rnul, i) = mul, ~(mul,,i \n) = i x i b(div, i) = div, ~(div~,i ) = [i/z j J(neg, i) = i J(<, i) = <% J(<t,2 ) = if z < i then cl([], \nz,k .z) else cl([], z,kz . z ) Figure 5: Example arithmetic constants. which the root of the value is \navailable. (We will account for the costs of environment lookups later. ) An application el ez leads \nto speculative evaluation in that the function and argument start evaluating at the same time, even if \nthe argument is never used. These subcomputations start on the next step since they correspond to child \nnodes in the computation graph. If el evaluates to a closure, then the evaluation of the function body \nbegins once el is evaluated, even if ez has not finished evaluating. The argument s min\u00adimum depth is \nonly included in that of the application if the argument is used, whereas its maximum depth is always \nincluded m that of the app~ication So, its minimum depth is stored in the relevant environment and accounted \nfor in VAR rule as needed. Since we eventually evaluate the ar\u00adgument regardless of whether it is needed, \nthe total work is the sum of all of its subcomputations, plus constant over\u00adhead. If el evaluates to \na constant function, then we use the function 6 to apply it. 2.1 Arithmetic constants So far, we have \nconsidered no specific constants in the model. However, for the sake of practicality, we would use a \nset of arithmetic constants such as c = iladd[mul lnegldivl<[ add, I mul, I div, I <, where z ranges \nover the integers, The primitive functions are addition, multiplication, division, negation, and the \nless\u00adthan comparison, and for syntactic simplicity, all primitive functions are curried, as defined in \nFigure 5. Since we nei\u00adther include pairs nor binary application in the the language (for brevky), we \nuse constants that represent partially ap\u00adplied constants, i e., a binary function applied to its first \nargument. Also, the result of a comparison is an encoding of true or fake , since we have not included \nbooleans. 2.2 Data Structures Using standard data structure encodings into the J-calculus ensures that \nthese data structures are also speculative. For example, an encoding of the list constructor cons is \ncons E kcl.Az2.Az. x xl z? car = Az.z (AZ1.AZ2. Z1) cdr = Ax. z (kl.kz. zz) Then expression cons el ez \nevaluates e] and e~ speculatively and returns a cons-cell in constant work and depth. This encoding 1s \nequivalent to including the inference rules E;d+21-el&#38;ul; wl, dl, dl E;d+lFez&#38;vz;wz,dz,~z (Coiw) \n E; dt-cons el ez&#38; (vl;dl,vz;dz); wl+wz+5, d+5, max(d+5, dl, ~2) &#38;;d+lt-e=&#38;(vl; dl, vz; d2.); \nw, d , d E;d Fear e=&#38; (CAR) V1; W+ 12, max(d +9, dl +2, d +6), max(d+9, dl +2, dz+l, d +6,2) plus \na similar CDR rule, where we use the following encoding for a cons cell: (V,; dl, vz; dz) CZ([XI * vl;dl,xz \n* vl; dz], z,z XI X?) 2.3 Other language constructs The standard A-calculus encodings of many other \nlanguage constructs behave as desired under speculative evaluation. Here we briefly describe conditionals \nand local bindings. Using the standard call-by-value encodings of condition\u00adals and booleans leads to \nnon-speculative evaluation of con\u00additional branches only the appropriate branch is evaluated. Here we \nhave if el then ez else es ~ el Ax.ez ~x.ez true = Azl.kcz.xl false = AzI.Az2.z2 where x is a fresh \nvariable. During the execution of a con\u00additional, both of the abstractions encoding the branches are \nevaluated speculatively. But since they are abstractions, they terminate in one step. Only the appropriate \nbranch, i. e., the body of the corresponding abstraction, is evaluated once the test has been evaluated. \nAn encoding which does not wrap e2 and e3 in abstractions would lead to specula\u00adtive evaluation of both \nbranches, an option offered in some languages [27]. Similarly, the standard definition of local binding: \nlet z= elin e~ = (Az.ez)el produces speculative evaluation of both expressions as in some languages \n[33]. A serialized equivalent can be encoded using a continuation passing style transform, CPS[el] Jz. \nez. Or we could add a special expression and inference rule such as E;d+lt-el~vl; wl, dl, dl &#38;[x \nH vi]; dl +1 + ez ==&#38;vz;wz,dz,dz (SLET) E;dksletx=elinzZ&#38; XZ; WI + wz + 1, dz,max(~l, ~z) Note \nhow the body does not begin evaluation until the bound expression s value is available (by the use of \ndl ). 2.4 Recursion There are two ways we can define recursion, either by encod\u00ad ing it within the basic \nJ-calculus, or by adding an additional construct, This distinction is asymptotically important for the \ndefinition of costs of recursive data structures. Providing an explicitly recursive language construct \ncan be favorable in terms of program costs. Consider the following recursive definition: let x= cons \nel xin ez With call-by-speculation, it is natural for this to create a cir\u00adcular list, rat her than \none infmit ely long, since its definition returns a cons-cell (in constant work and depth) and binds \nit to z while the cell s components are evaluating, delaying where necessary until x s value is available. \nWe would prefer to define that such recursive definitions create circular data structures in constant \nwork and depth, rather than cresting infinitely long data structures in infinite work and maximum depth. \n(Note that even with an infinite data structure, each component is available in finite minimum depth. \n) Encoding recursion with the call-by-value least fixed\u00adpoint combinator Y results in infinite data structures. \nEx\u00adisting lement languages allow circular data structures, and thus require an explicitly recursive construct. \nWithout ex\u00adplicit recursion, the only way to terminate with such data structures it to rewrite the program \nto delay and force the structures components. Adding an explicitly recursive operator to the seman\u00adtics \nwould require the use of stores. Such a change would be straightforward in the PSL, but would add to \nrule ver\u00adbosity. In the underlying FSAM model (Section 3), stores are needed anyway, and adding recursion \nwould be simple. 3 The Fully Speculative Abstract Machine We now examine how to implement the PSL on \nother ma\u00adchine models. As an intermediate step, we introduce the Fully Speculative Abstract Machine (FSAM), \nbased loosely on the P-ECD machine [2]. It executes a sequence of steps in parallel over a sets of states. \nEach thread of comput at ion is represented by a series of states over time. We prove the following relations \nbetween the costs in the PSL and FSAM models: . The total number of states processed is 0(w). e The number \nof steps until the original thread of the program finishes is 0(d). e The number of steps until all the \ncomputation finishes is O(i). A state s is a tuple (fi, e, K, r) consisting of + an environment &#38; \nmapping each variable to the result of the state computing that value, e an expression e to be evaluated, \ne a continuation K listing the result locations for the arguments of e, 9 result locations r, a pair \nof locataons containing the state s value, or NoValue if the value has not yet been computed, and a \nqueue of all the (inactive) states suspended on this one. (The projection functions ml and ml are used \nto obtain the individual locations from a pair.) We assume there is a countably infinite set of locations. \nWe will thread a store through the computation to map each location to a value or a queue of states. \nThe semantic domains used for the FSAM are slightly different than those for the PSL. For convenience, \nwe add an additional form of expression to represent the equivalent of a function body for a constant \nfunction application The expression @ c x represents c applied to the value to which z is bound, The \nFSAM uses another slightly different form of values and environments: i .:= c I cl(fi, z,e) E: Var~ables \n+ ResultLocs As before, we assume that v and E are the untagged equiv\u00adalents of w and ~. A single step \nof the machine is written A, u ~ A , U , where A is the current set of active states, and u is the cur\u00adrent \nstore. The machine starts with one state that represents the entire program and a store containing that \nstate s result locations. When there are no remaining active states, the machine finishes and the final \nvalue is ~tored in the result locations of the original state. In addition, each step may update the \nresult locations of some states. Over time, the number of active states may decrease as threads finish \nor suspend, or increase as threads are created or reactivated. We ensure that no currently active states \nhave the same result locations. The series, over time, of states which do share the same result locations \nrepresents a thread of com\u00adputation. Thus we use the pair of result locations as this t bread s identifier, \nAlso, while we generally describe the FSAM as manipulating states, we can equivalently describe it in \nterms of threads. While evaluating an application, the current thread evaluates the function and then \nthe function body, while a new thread evaluates the argument, The basic ideas of each step are as follows. \nIf a state s expression is a constant, we immediately have its value (the constant itself). So we check \nits continuation to see if it needs to be applied to an argument, If so, we start the eval\u00aduation of \nthe application; otherwise, we store the constant as the state s result value and reactivate anything \nblocked on this. If the expression is an abstraction, we build the ap\u00adpropriate closure, and similarly \nfinish or apply, depending on the continuation. If it is a variable, we look up its value, If the value \nis available, we finish or apply as appropriate; otherwise we suspend this thread. The same state will \nbe reactivated when the value is available. If it is an applica\u00adtion, the current thread will evaluate \nthe function, and we fork a new thread to evaluate the argument. And if it is a constant function body, \nwe try to lookup the variable, apply J if the variable s value is available, and then finish or apply. \nOn each step, each active state uses three substeps as defmed in Figure 6. The first substep, &#38;l, \nis the core of the evaluation of each active state. It performs the case analysis on a state expression \njust outlined, using the fin-app routine to check if a state finishes with a value or applies it to an \nargument. For each active state, it resu~ts in one of e States (A, o), with a set of new states to be \nactive on the next step and new store bindings, e Susp(s, 1), indicating that the state needs to be added \nto the queue at location 1, or e Fi n(ti, r), indicating that the thread has finished with the given \nvalue and result locations. The second substep, &#38;Z, places all of the suspending states on the appropriate \nqueues. The third, &#38;3, saves the final value and reactivates the blocked threads of all the finishing \nstates. The new and the reactivated states are taken as the result of the step ~ as a whole and are the \nactive states of the next step. Synchronization between the last two substeps is neces\u00adsary for correctness. \nWithout it, a state could be added to a queue of suspended states after that queue is reacti\u00advated, leaving \nthe state suspended forever. Additionally, the synchronization is also useful for cost predictability, \nas otherwise an arbitrarily long series of values could become available and used in a single step. For \nexample, for any k, and all i in {1, .,, ,k}, if the states sl, . . . . Sk could evaluate in a single \nstep, Definition 1 The FSAM evaluates e to C with w work, d . minimum steps, and ~ maximum steps, or \ne ~ i; w, d, ~, zf d is the mmzmum step such that where e at starts with one active state to evaluate \ne: Ao = {([1,e,[l, r)} Oo(mr) = NoValue c70(7r2 r) = emptyq . at finishes wtth no acttve states: A; ={} \ne the tnttial thread s value is ready at step d: jar all i > d, a,(mlr) = L, and * the number of actzve \nstates processed zs the amount of work: ~?=~ lJ4tl= W, where /A1 zs the swe of A. Thus, for a given \nstep, each active state requires constant work. Equivalence of the PSL Model and the FSAM We prove that \nthe PSL and FSAM models compute the same value with asymptotically equivalent costs To achieve this, \nwe must show that the accounting of work and depth in the PSL model accurately counts the steps involved \nin the cor\u00adresponding FSAM evaluation, However, whereas the FSAM explicitly deals with blocking and thus \nreflects its costs, the PSL model does not Since each thread can block at most once, the FSAM can spend \nat most half of its work block\u00ad ing. Thus it affects the complexity of the simulation only by a constant \nfactor. But to simplify the proof of equiva\u00adlence, we modify the PSL model to explicitly account for \nthe -(a, c, K, r), C7 &#38; jhapp , K , constant (2, x, K, ?-), a &#38; C(Z,. O(n, (-!$($))) of variable \nNoValue + Susp(s, nz(~(z))) G + jin-app v K r (~, Jx. e, K, r), a =1 Jln.app c1(E, x, e) K r abstraction \n(Z, el ez, K,, r), a =%1 let r = (~resh l, fresh 1 ) application .. m States({(E, el, r :: K,r), (E, \ne~, [], r )}, [1+ NoValue, 1 ++ emptyq]) (fi, @ c z, K, r), a 44-I case fJ(jTl(jj(Z))) ~~ constant application \nNoValue + Susp(s, nz(fi(z))) i + jin.app J(c, v) K r rhere jin -app ti [1 = Fin(ti, r) jkapp Cl(B , z, \ne) (r :: K) J = States({(&#38; [z h+ r ], e, K, T)}, []) jk-app c (r :: K) r = States({([z ++ r ], @ \nc Z,K, 7-)}, []) {SU5P(SI, 1,),... ,Susp(sm, in)}, a 42 c7[11 ++ enqueue {s, [1, = 1;} a(li),..., 1A \nt+ enqueue {s,11, = 1~} a(l~)] vhere l;, ,1A are the distinct locations in 11, . . . . in {Fin(til, rl),. \n. . ,Fin(ti~, m)}, a =3 (lJ~=l dequeue-.all (rzr, )), a[rlrl + til, . . . . nlrn + tin] {S,,,. !,sn}, \nu 4 A u (LfLl -4), CT u (Uu=l u,) if S,, u 41 x, {XIX, = Susp(, )},C7 &#38; , Cr {X, I.Y1 = Fin(,) })a \n~s A,u vhere {States(Al, ul), . . . . States(A~, an)} = {X, IX, = States(., .)} Figure 6: Fully speculative \nsubsteps, Each step of evaluation =%-consists of using the three substeps ~ 1, 32, and =3, synchronizing \nbetween them. i(z) = i); d 1. the FSAM finishes evaluating e and either stores its ~;d~z~t i;o+l, max(d,d \n)+o +1, (vAR) value or starts its application by step d~: at step d 1, jin.app M called on r, i.e., \nmax(d, d )+o+l . 2fK = [], then Ud/_l (7T2 r) C Ad, andfori 2 d , where o = one(d > c!) a,(7rl r) = i \nE;d+l Fel&#38;c; wl, dl, ~l * zfK=r ::~ andti=c, then E;d+lkez&#38;v;wz,dz,~z ([z H r ], @ c X, fi ,r) \n6 Ad (APPC ) ~;dt-el ez~d(c, i); wl+wz+ 0+2, o if ~= r :: K and w = cl(i , x, e ), then max(dl)dz)+o+l, \nmax(~l,~z)+o+l (J&#38; [z H r ], e , K , r) 6 Ad, where o = one(dz > dl) 2. the computation for this \nexpress~on requn-es the ind\u00ad where one (b) = 1 if b, O otherwise icated work and mazzmum depth: the sum \nover all d steps of the number of acttve states Figure 7: Modified profiling semantics of the blocking-PSL \nthat are descendants of s as w, and for all i > ~, A: model accounts for the higher constant costs when \nthreads contains no descendants ofs. block. The CONST , ABS , and APP rules are like the corresponding \nrules of the PSL model. Proof: cases CONST , e = c, and ABS , e = Jx. e : By the ap\u00ad costs of blocking, \nso that its costs are exactly equivalent to propriate inference rule, w = 1 and d = ~ = d + 1. those \nof the FSAM. This modified semantics, the blocking-PSL model (Figure 7), differs from the PSL model only \nin The FSAM fully evaluates e to the appropriate value in one step of unit work, calhng jin.app on step \nd. No that the constant overheads in VAR and APPC account for descendants of s are created, and the conclusion \nholds. blocking. The exact cost equivalence of the blocking-PSL case VAR , e = xi: There are two subcases, \ndepending on and FSAM (Corollary 1) leads directly to the asymptotic whether s blocks on this lookup: \nequivalence of the PSL and the FSAM (Corollary 2). Each of these follow from Theorem 1 which generalizes \nthe equiv\u00ad oIfd, >d, then w=2 and d =d=d, +2. In alence to individual subcomputations of the blocking-PSL \nthe FSAM, the variable s value is not available in and FSAM models. As part of this, we formalize the \nno\u00ad the current step, and this thread spends this step tion of subcomputations within the FSAM by defining \nthe suspending. It is reactivated at the end of step d, children and descendants of states (Definition \n2). A state s and active in step d, + 1 to fetch the new value. descendants are all those required for \nthe computation be- Thus, it requires two units of work and is done at gun by the state step d, + 2. \nDefinition 2 If {s}, u &#38; A, a w one step of a FSAM evaluation, then each of the states in A as a \nchild ofs. From * Otherwise, if d, < d, then w = 1 and d! = ~ = d, + 1, and the FSAM requires one step \nto fetch the value. thzs relation we also define the descendants of a state m the obvtous way (such that \na state is a descendent of ztself). [n either case, fiiapp is called on r at step d,+ 1, and s has no \ndescendants, so the conclusion holds. Theorem 1 If case APP , e = el ez: The FSAM uses unit work at step \nI. [xl+til;dl,...,x~+fi~;d~], dEe&#38;v, w,d , d, d to fork new states SI and sz. By induction on e] \nand ez, we know that for z c {1, 2), fin_app is called on r at step dl 1, and on TZ at step dz 1. Also, \nthe total work and maximum depth of these evaluations are WZ e at step d, there M an actvoe state to \nstart the eual\u00ad and ~,. In particular, we have (~ [z * rz], e , K, r) E uatzon of e that has an environment \ncorrespond- Ad , and for i z d:, u,(mlrz) = tiz 1 ing to that m the block mg-PSL model: Now by induction \non e , fin_app is called on r at step .4 ~ = Au {.} 13 1, and the total TVO.L and xmazixnuzn depth of \ns= ([zl*rl,..., znWrn], e,K, r) this evaluation are W3 and 23. Thus the total work and maximum depth \nof the entire evaluation are w = * the threads computing the envmonments contents wall obtain values \nat the mdacated depths: WI + wz + W3 + 1 and ~ = max(~l,~z,~s), so the conclusion holds. for all r, # \nr and]> d,, aj(m~rt) = w,, case APPC , e = el ez: This case is similar to a combi\u00ad (The restmction that \nr. # r ensures that we do nation of the VAR and APP cases, not assume part of the conclusion.) L1 then \n and therefore do not consider garbage collection, although there is nothing that precludes its use. \nWe first discuss how to implement the queues of sus\u00adpended states. We show that operations on these queues, \nCRCW PRAM (rand.) O(log p/ log log p) particularly enqueuing of n elements, requires O(n) amor-  EEEErEH \nFigure 8: Time bounds for implementing fetch-and-add on various machine models with p processors. As \nthese models are all randomized, the bounds hold with high probability. Proof: This follows from Theorem \n1 by using n = O, d = O, K = [], A = {}, and ao(~~r) = enzptyq. Corollary 2 If[]; Ok e ~ II; w,d, d, \nthen e ~ v;w , d , ~ such that 2W~ w , 2d ~ d , and 22 ~ ~ . Proof: The PSL and blocking-PSL models \ndiffer only in their constant overheads, with the latter having constants at most twice of those in the \nformer. 4 implementing the FSAM on Machine Models We introduced the FSAM only as an intermediary in the \nmapping of the PSL model onto more realistic machines. So now we complete this mapping and show how to \nimplement the FSAM on such machines, in particular, a butterfly net\u00adwork, hypercube, and PRAM. The implementation \nclosely follows the previous description of the FSAM, but it de\u00adpends on several underlying tools. Since \nspeculative evalua\u00adtion centers on the use of queues, the operations on queues are of particular importance. \nOur simulation bounds are parameterized by the asymp\u00adtotic time Tfetchadd (p) required to implement a \n~etch-and\u00ad add operation [11] (also called a multiprefix [31]) on p pro\u00adcessors. In a fetch-and-add operation, \neach processor has an address and an integer value i. In parallel all proces\u00adsors can atomically fetch \nthe value from the address while incrementing the value by i. This can be implemented in a butterfly \nor hypercube network by combining requests as they go through the network [31], and on a PRAM by various \nother techniques [21, 9]. The bounds for Tfetchadd (P) for these machine models are given in Figure 8: \nThese bounds assume the butterfly has p logz p switches which can do the combining, and the hypercube \ncan communicate and com\u00ad bine over all wires simultaneously (multiport version). For all these machines \nif each processor is making m requests, these requests will take a total of ~(m + Tfetchadd (p)) time. \nThe fetch-and-add operation is used in t hree places in our implement at ion: memory allocation, the \nenqueue operation, and allocating tasks to processors. To handle memory allocation throughout the implemen\u00adtation \nwe assume one global memory space and keep a counter pointed to the next available memory location. Whenever \na processor needs memory it can fetch-and-add from this counter using the size of the block it needs. \nFrom this, a processor receives the start address of its requested space. In our implementation we assume \nan unbounded memory tized work and O(n/p +Tfetchadd (p)) time. The other main FSAM data structure is \nan environment. Using balanced binary trees, we can bound the time for each environment access and update \nby v e, the logarithm of the number of variables in the program e [2]. Given these, implementing each \nof the FSAM substeps is straightforward. But we must also show how states map onto machine processors \nand how these processors are load-balanced.  4.1 Queues We must support three operations on these queues: \ncreating an empty queue in the application case of Al and the initialization of the FSAM, enqueuing elements \nin ~z, and dequeuing all elements in =3. A queue q is implemented as a pair of an array q. and a length \nql, where ql is the length of the queue, and ql < Iqa 1. To create a queue we return a new array (of \nsome constant size) and a length O. To dequeue all elements we just return the queue s array pointer \nand length data movement is left to the creation of the new active state array. As we enqueue data, we \nmay need to allocate a new, larger array a, copying the old cent ents into the new array. However, we \nmust be careful to bound the work spent on such copying. In an enqueue, many processors each add one \nelement onto one of many queues, where each queue may receive multiple elements. To implement the enqueue \neach proces\u00adsor fetch-and-adds 1 to the current length q of its destina\u00adtion queue, receiving the offset \no within the queue for its element. As a side effect ql is incremented appropriately. Now all processors \nthat receive an offset within the bounds of the destination array (o < ]qa 1), write their element into \nthe position o of the array. Some of the queues, however, might have an overflow (ql > Iqa [). For these \nqueues we will grow their arrays. To grow the arrays we (1) identify the queues with an overflow and \nallocate a new array of size 2ql for these queues, (2) copy the contents from the old array, and (3) \ncomplete the enqueue operations that were postponed because of over\u00adflow. To identify the queues with \noverflow each processor that has received an offset o = Iqa I + 1, remains active and the other processors \ndrop out. These active processors can allocate the space for the new queues by reading q~, and al\u00adlocating \na space of twice this size from the global pool. The copying is more dificult since the work for copying \nneeds to be balanced across the processors some queues might be very large, in fact much larger than \nthe current number of states. To properly balance the work for copying the queues we allocate a number \nof processors proportional to the size of the old queue (Iq. 1) to each queue. Such allocation can be \nimplemented with the fetch-and-add operation or with segmented operations [3], Each processor then copies \ntheir specified portion of a queue or set of queues into the new arrays. The length qi of each new queue \nis set to the number of e~ements that were copied (/qa I of the old array). TO com\u00adplete the enqueue \noperations the processors that received an out of bounds offset previously now enqueue again on the updated \nqueues. In the enqueue all the operations other than the copying can be implemented in work proportional \nto the number of enqueue requests. As discussed later, this work is load bal\u00adanced across the processors. \nThe actual copying can require more work since large queues might need to recopied. How\u00adever, the cost \nof these copies can be amortized against the time it took to enqueue into the queues in the first place. \nIn particular for each queue of size ql, we will have spent at most 2qJ aggregated work copying its elements. \nThis is because we are at least doubling its size on each expansion of the array so the cost of a copy \nis always at most half as much as the cost of the next copy. Since we are accounting forq work fortheenqueues, \ntheextra work forthe copies is amortized against them. The work for copying is properly Ioadbalanced, \nsotherunning time is also amortized. 4.2 FSAM Implementation and Costs Using the basic data structures \njust described, wenowsim\u00ad ulate the FSAM on our machine models. First we examine the time required for \neach step of the FSAM, then total this for all steps. Lemma 1 Each FSAM step starting wzth active states \nA and ending with active states A can be implemented on a p processor machine within O(ve((lAl + [A \n1)/P +T etchadd(p))) amortwed tame. Proof: We start with the set of active states in an array A of length \na. Each processor is responsible for up to m = (a/pi elements (u e., processor i is responsible for the \nelements from is/p to (i + l)a/p 1), We assume each processor knows its own processor number, so it \ncan calculate a pointer to its section of the array. The simulation of a step consists of the following: \n1. Locally evaluate the states (~1 ), and synchronize all processors. 2. Suspend all states requesting \nto do so (=2), and syn\u00adchronize all processors. 3. Save the result value and reactivate the queued states \n of all finishing states (As). 4. Create a new active state array for the next step.  We now show each \nof these is executed in the given bounds. Local evaluation of the states requires the time it takes to \nprocess m states. The implementation of ~1 is straight\u00adforward and requires O(U e ) time for environment \naccess and constant time for all other operations. Thus the total time for local evaluation of the states \non the machine models of interest is O(U e (m + T etchadd (P))), where Tfetchadd (P) provides an upper \nbound on any memory latency or space allocations. In the second substep, each of the active states may \nsus\u00adpend itself. This is accomplished with a single enqueue operation. This requires a constant number \nof fetch-and\u00adadds over at most a active states. As previously mentioned! such fetch-and-adds pipeline \nand require a total of O(m + As discussed earlier the copying of the fet~hadd (P)) time arrays for queues \nthat overflow might require more time, but this is amortized against previous steps. In the third substep, \neach processor has up to m states to finish. Each processor writes its states results, then de\u00adqueues \nits states suspended queues, t. e., returns the queues pointers and lengths. This requires O(m + Tfetchadd \n(P)) time, including memory latency. The new active state array is the union of the a states returned \nby the first and third substeps, We then create the array of size a in O((a + a )/p + Tfetchadd (p)) \ntime. This is implemented using a similar processor allocation technique as used to allocate processors \nfor copying arrays in the en\u00adqueue operation. Theorem 2 If []; 0; e H ti &#38; w, d, d; , then a p processor \nmachine can calculate 0 from e within o(ue(w/P +  ~T etchadd(P))) tzme. Analogous results hold for the \nother models. Proof: The proof uses Brent s scheduling principle [4]. We assume that step z of the FSAM \nprocesses a, active states. We know from Corollary 2 that ~~~~ a, = w. We also know from Lemma 1 that \nit takes kv e (at + at+l )/P +Tfetchadd (P) time for step i, for some constant k. The total time is then \nT = ~~~~ k~e ((a, i-G+l)/P + Tfetchadd (P)) = kve ~j~~(a, + a,+l )/P + Tfetchadd (P) = 2kve((~~!~ az/P) \n+ Tfetchadd(p)) = 2kve(w/p + ~Tfetchadd(P)) 5 Related Work Several researchers have used cost-augmented \nsemantics for automatic time analysis or definitional purposes, e.g., [34, 35, 38, 39], the most similar \nbeing that of Roe for a lenient language [32, 33]. The following are the primary differences of Roe s \nmodel as compared to ours: Looking up a variable does not wait for the value as in VAR. He waits for \nthe value only if it is needed, e.g., to apply it. Because of the previous difference, two separate depths \nindicate when the value becomes available (our min\u00adimum depth) and provide a (clock to serialize some \ncomputations. We have shown in Section 2.3 how we can serialize with the minimum depth. The maximum depth \nis not tracked. Lists are explicitly in the language, rather than en\u00adcoded. Each cons-cell component \nis tagged with the depth at which it completes, whereas in our encoding those tags are in the environment \nof the closure rep\u00adresenting the cons-cell. While lists can be encoded, a more pragmatic implementation \nwould need to include them. Flanagan and Felleisen also gave a cost-augmented opera\u00adtional semantics \nfor a language with futures, defining the total work and the mandatory work of a computation [8]. However, \nnone of these related the costs of the modelled language to those in machine models. Nikhil introduce \nthe P-RISC [24] abstract machine for implementing Id, a speculative language. The machine, however, is \nnot meant as a formal model and does not fully define the interprocess communication and the selection \nof tasks to evaluate. It is a more pragmatic concurrent model designed to reduce communication costs, \nrather than a syn\u00adchronous one designed to formally analyze runtime across a whole computation. Aside \nfrom the basic idea of having queues of blocked threads, the P-RISC and FSAM have little in common. 6 \nDiscussion We have specified a fully-speculative implementation of the A-calculus (and related languages) \nand proved asymptotic time bounds for several machine models. Our time bounds are good in the sense that \nthey are work efficient and within a logarithmic factor (for the butterfly and hypercube) of op\u00adtimal \nin terms of time. To obtain these bounds, we introduce fully parallel operations on queues of suspended \nthreads. An important contribution of the work is the use of a semantic model to define an abstract notion \nof costs and the tech\u00adniques used to relate these to the running time, including the introduction of \nthe FSAM. 6.1 Pragmatic Issues The paper has concentrated on asymptotic behavior at the cost of ignoring \nconstant overheads. In particular, while we account for communicant ion costs. /u we i more the fact \nthat communication is typically significantly slower than compu\u00adtation. Here we briefly discuss some \npragmatic issues about the implementation and how it could be modified to reduce the constants. Our implementation \naggressively creates many threads to maximize parallelism, and it frequently synchronizes all threads \nto guarantee load-balancing. Since most expressions are relatively simple, and the cost of thread management \nis high, creating a thread for each subexpression involves too much overhead [30]. Furthermore, the substeps \n~1 em\u00adbody little computation between each load-balancing. One way to improve this would be to group \nsets of these substeps between each load-balancing. As long as they were grouped into clusters of constant \nsize, this would not effect asymp\u00adtotic time bounds, but could greatly reduce load-balancing costs. This \nis the same basic idea as work examining heuris\u00adtics for building large sequential blocks of code, e.g., \n[16, 30]. Another way to reduce communication is to cache in the environment the results of fetching \nvalues from other threads. This is simple since the value of a thread never changes once computed. There \nare several approaches available to improve the space efficiency of the FSAM. One would be to introduce \ngarbage collection of the various semantics objects. Properly including the time costs of garbage collection \nwould require that we formally model the space taken by a computation and include space bounds. Another \nis to mutate the state of each thread, rather than creating a series of states over time. As mentioned \nin section 6.2, a partially speculative variant of the FSAM may be able to schedule states to minimize \nthe number of the active states. 6.2 Partial speculation Here we discuss two possible relation modifications \nto the FSAM. First, instead of fully evaluating all threads, any threads irrelevant to computing the \nfinal result may be dis\u00adcarded during evaluation [12], Clearly, this can reduce the asymptotic work and \nmaximum depth of some computa\u00adtions. Second, instead of using all the currently active states on each \nFSAM step, we could schedule only a subset, leav\u00ading the unscheduled ones to be active on the next step. \nIn conjunct ion wit h the previous modification, the goal is to minimize the number of irrelevant states \nthat are scheduled while still maintaining sufficient parallelism. In particular, we schedule at most \npTfetchadd (p) threads Per steP. AIso, if we can choose the scheduled states appropriately, we might \nalso be able to minimize the maximum number of active states on any step, for space efficiency [1]. But \nit seems unlikely that call-by-speculation allows an efficient imple\u00admentation of a depth-first p-traversal \nof the computation DAG. Consider a modification of the FSAM model, called the Partially Speculative Abstract \nMachine (PSAM), which in\u00adcorporates these changes. Before we can discard irrelevant threads, we must \nfirst detect them. For this we assume each thread has a reference count, and that a scheduled state marks \nits thread irrelevant if it has a zero reference count. It also adds its children to a pool of threads \nto be marked irrelevant. On each step, we mark some of those t breads ir\u00adrelevant and add their children \nto the pool. (We mark only as many as the number of scheduled active states on this step in order to \nmaintain our per-step work bound. ) We could also detect irrelevant threads statically with strict\u00adness \nanalysis. This would detect irrelevant threads less ac\u00adcur at ely, but obviously earlier. While for some \nexpressions, this may reduce work and maximum depth asymptotically, it is easy to construct expressions \nwhere most of each irrele\u00advant thread is evaluated before it is detected to be irrelevant. What we have \njust described is a form of garbage collection designed specifically not to increase our work bound. \nSome languages simply incorporate this into their usual garbage collection mechanism [27, 23]. For the \npurpose of minimizing the work spent evaluating irrelevant threads, scheduling a subset of the active \nstates is only beneficial if we can effectively prioritize states and threads. In order to have the same \ndepth bounds as the FSAM, the PSAM must prioritize threads known to be rel\u00adevant at the highest level. \nNote that at most one of those threads is active at a time. One approach is to prioritize the argument \nof an application lower than its function, since the argument may not be relevant even if the function \nis. While used in practice [27, 29, 28, 37], it is unclear whether this approach can reduce work asymptotically \nwithout increas\u00ading depth asymptotically. The problem is updating thread priorities efficiently. During \nevaluation, as we detect which threads are relevant, we adjust priorities. If changing one thread s priority \nchanges its children s priorities, we must propagate this change to its descendants. On the one hand, \nwe can only propagate some of these priorities within the work and depth bounds of a single PRAM step. \nOn the other, if we do not update priorities fast enough, the pri\u00adorities used for scheduling may be \ntoo inaccurate. Further\u00admore, if we have an unbounded number of priority levels, we cannot afford the \nwork to sort all the active states pri\u00adorities to grab those of highest priority. Although it might be \npossible to maintain extra order on the tree of threads so that we can efficiently grab those of highest \npriority. Acknowledgements We would like to thank Bob Harper for providing help\u00adful feedback. This research \nwas supported in part by the Defense Advanced Research Projects Agency (DARPA) un\u00adder grant number F33615-93-1-1330 \nand in part by an NSF Young Investigator Award. References [1] Guy Blelloch, Phil Gibbons, and Yossi \nMatias. Prov\u00adably efficient scheduling for languages with fine-grained parallelism. In ACM Symposzum \non Parallel Algorithms and Architectures, July 1995. [2] Guy Blellocb and John Greiner. Parallelism in \nsequen\u00adtial functional languages. In Proceedings 7th Inter\u00adnational Conference on Functional Programming \nLan\u00adguages and Computer Architecture, pages 226 237, June 1995. [3] Guy E. Blelloch. Vector Models for \nData-Parallel Com\u00adputmg. MIT Press, 1990. [4] Richard P. Brent. The parallel evaluation of general arithmetic \nexpressions. Journal of the ACM, 21(2) :201 206, 1974. [5] David Callahan and Burton Smith. A future-based \nparallel language for a general-purpose highly-paralle computer. In David Galernter, Alexander Nicolau, \nand David Padua, editors, Lanquages and Compders for Parallel Computing, Research Monographs in Parallel \nand Distributed Computing, chapter 6, pages 95 1 13. MIT Press, 1990. [6] Rohit Chandra, Anoop Gupta, \nand John L Hennessy. COOL: a language for parallel programming. In David Galernter, Alexander Nicolau, \nand David Padua, edi\u00adtors, Languages and Compders for Parallel Computzng, Research Monographs in Parallel \nand Distributed Com\u00adputing, chapter 8, pages 126 148. MIT Press, 1990. [7] Marc Feeley. An Eflcient and \nGeneral Implementation of Futures on Large Scale Shared-Memory iVlulttproces\u00adsors. PhD thesis, Brandeis \nUniversity, June 1993. [8] Cormac Flanagan and Mattias Felleisen. The semantics of future and its use \nin program optimization. In Pro\u00adceedings 22nd ACM Symposium on Prmczples of Pro\u00adgramming Languages, pages \n209 220, 1995. [9] Joseph Gil and Yossi Matias. Fast and efficient simula\u00adtions among CRCW PRAMs Journal \nof Parallel and Dzstrzbuted Computmg, 23(2):135-148, November 1994. [10]Ron Goldman and Richard P. Gabriel. \nQlisp. Expe\u00adrience and new directions In Proceedings ACM SIG-PLAN Symposium on Parallel Programmmq: Ezperz \n\u00adence wzth Apphcatzons, Languages and Systems, pages 111-123, July 1988. [11] Allan Gottlieb, B. D, Lubachevsky, \nand Larry Rudolph. Basic techniques for the efficient coordinatio~ of very large numbers of cooperating \nsequential processors. ACM Transactions on Programming Languages and Systems, 5(2), April 1983. [12] \nDale H. Grit and Rex L. Page. Deleting irrelevant tasks in an expression-oriented multiprocessor system. \nA CM Transactions on Programming Languages and Systems, 3(1):49 59, January 1981. [13] Robert H. Halstead, \nJr. Multilisp: A language for concurrent symbolic computation. ACM Transactions on Programming Languages \nand Systems, 7(4) :501 538, October 1985. [14] Robert H. Halstead, Jr. New ideas in parallel lisp: Lan\u00adguage \ndesign, implementation, and programming tools. In T. Ito and R. H. Halstead, Jr., editors, Parallel Lzsp: \nLanguages and Systems, US/Japan Workshop on Par\u00adallel Lisp, number 441 in Lecture Notes in Computer Science, \npages 2-51. Springer-Verlag, June 1989. [15] Paul Hudak and Steve Anderson. Pomset interpreta\u00adtions of \nparallel functional programs, In Proceedings 3rd International Conference on Functional Program\u00admmg Languages \nand Computer Architecture, number 274 in Lecture Notes in Computer Science, pages 234\u00ad 256. Springer-Verlag, \nSeptember 1987. [16] Lorenz Huelsbergen, James R. Larus, and Alexan\u00adder Aiken. Using the run-time sizes \nof data struc\u00adtures to guide parallel-thread creation, In Proceedings ACM Conference on LISP and Functional \nProgram\u00admmg, pages 79 90, July 1994. [17] Takayasu Ito and Manabu Matsui. A parallel lisp lan\u00adguage PaiLisp \nand its kernal specification. In T. Ito and R. H. Halstead, Jr., editors, Parallel Lzsp: Lan\u00adguages and \nSystems, US/Japan Workshop on Parallel Lzsp, number 441 in Lecture Notes in Computer Sci\u00adence, pages \n58 100. Springer-Verlag, June 1989. [18] Mike Joy and Tom Axford. Parallel combinator re\u00adduction: Some \nperformance bounds. Technical Report RR21O, University of Warwick, 19 32. [19] Richard Kennaway. A confhct \nbetween call-by-need computation and parallelism (extended abstract). In Proceedings Cond~ttonal Term \nRewrttmg Systems-9~, February 1994. [20] David A. Kranz, Jr. Robert H. Halstead, and Eric Mohr. Mu1-T: \nA high-performance parallel lisp. In Proceedings ACM SIGPLAN Conference on Program\u00adming Language Deszgn \nand Implementatzonj pages 81 90, June 1989. [21] Yossi Matlas and Uzi Vishkin. On parallel hashing and \ninteger sorting. Journal of Algorithms, 12(4) :573 606, 1991 [22] James S. Mdler. MultzScheme: A Parallel \nProcess\u00ading System Based on MIT Scheme. PhD thesis, Mas\u00adsachusetts Institute of Technology, September \n1987 . [23] James S. Miller and Barbara S. Epstein. Garbage col\u00adlection in MultiScheme (preliminary \nversion). In T. Ito and Jr. R. H. Halstead, editors, Parallel Lisp: Lan\u00adguages and Systems, US/Japan \nWorkshop on Parallel Lzsp, number 441 in Lecture Notes in Computer Sci\u00adence, pages 138 160, Springer-Verlag, \nJune 1989. [24] Rishiyur S. Nikhil. The parallel programming language Id and its compilation for parallel \nmachines. Techni\u00adcal Report Computation Structures Group Memo 313, Massachusetts Institute of Technology, \nJuly 1990. [25] R,ishiyur S. Nikhil, Id version 90.1 reference man\u00adual. Technical Report Computation \nStructures Group Memo 284-1, Laboratory for Computer Science, Mas\u00adsachusetts Institute of Technology, \nJuly 1991. [26] Rishiyur S. Nikhil, Arvind, James Hicks, Shail Aditya, Lennar Augustssonj Jan-Willem \nMaessen, and I uli Zhou. pH language reference manual, version l.O preliminary. Technical Report Computation \nStructures Group Memo 369, Laboratory for Computer Science, Massachusetts Institute of Technology, January \n1995. [27] Randy B. Osborne, Speculative Computation m Multzl\u00adtsp. PhD thesis, Massachusetts Institute \nof Technology, December 1989. [28] Andrew S. Partridge. Speculative Evaluation m Purallel Implementations \nof Lazy Functional Languages. PhD thesis, Department of Computer Science, University of Tasmania, 1991. \n[29] Andrew S. Partridge and Anthony H. Dekker. Specu\u00adlative parallelism in a distributed graph reduction \nma\u00adchine. In Proceedings Hawait International Conference on System Sciences, volume 2, pages 771 779, \n1989. [30] Simon L Peyton Jones. Parallel implementations of functional programming languages. The Computer \nJournal, 32(2):175-186, 1989. [31] Abhiram G. Ranade. Fluent Parallel Computation. PhD thesis, Yale University, \nNew Haven, CT, 1989. [32] Paul Roe. Calculating lenient programs performance. In Simon L Peyton Jones, \nGraham Hutton, and Carsten Kehler Hoist, editors, Proceedings Funcihonal Programmmg, Glasgow 1990, Workshops \nin computing, pages 227-236. Springer-Verlag, August 1990. [33] Paul Roe. Parallel Pr-ogrammmg uszng \nFunctional Lan\u00adguages. PhD thesis, Department of Computing Science, University of Glasgow, February 1991. \n[34] David Sands. Calcuh. for Tzme Analysis of Functional Programs. PhD thesis, University of London, \nImperial College, September 1990. [35] David B. Skillicorn and W. Cai. A cost calculus for parallel functional \nprogrammmg. To appear m Journal of Parallel and Distributed Computing. [36] Guy Tremblay and G. R. Gao. \nThe impact of laziness on parallelism and the limits of strictness analysis. In A. P Wim Bohm and John \nT. Fee, editors, Proceedings High Performance Functional Computing, pages 119 133, April 1995.  [37] \nC K Yuen, M D Feng, and J J Yee. Speculative par-\u00adallelism in BaLinda Lisp. Technical Report TR31/92, \nDepartment of Information Systems and Computer Sci\u00adence, National University of Singapore, November 1992. \n[38] Wolf Zimmerman. Automatic worst case complexity analysis of parallel programs. Technical Report \nTR-90\u00ad066, International Computer Science Institute, Decem\u00adber 1990. [39] Wolf Zimmerman. Complexity \nissues in the design of functional languages with explicit parallelism. In Pro\u00adceedings International \nConference on Computer Lan\u00adguages, pages 34-43, 1992.  \n\t\t\t", "proc_id": "237721", "abstract": "", "authors": [{"name": "John Greiner", "author_profile_id": "81100082046", "affiliation": "Carnegie Mellon University", "person_id": "P144053", "email_address": "", "orcid_id": ""}, {"name": "Guy E. Blelloch", "author_profile_id": "81100282539", "affiliation": "Carnegie Mellon University", "person_id": "P100820", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/237721.237797", "year": "1996", "article_id": "237797", "conference": "POPL", "title": "A provably time-efficient parallel implementation of full speculation", "url": "http://dl.acm.org/citation.cfm?id=237797"}