{"article_publication_date": "01-01-1996", "fulltext": "\n C: A Language for High-Level, Efficient, and Machine-Independent Dynamic Code Generation Dawson R. Engler, \nWilson C. Hsieh~ and M. Frans Kaashoek engler@ lCS.mit.edu, whsieh @es. washington.edu, kaashoek (ii?Ics.mit.edu \nLaboratory for Computer Science Massachusetts Institute of Technology Cambridge, MA 02139 Abstract Dynamic \ncode generation allows specialized code sequences to be created using runtime information. Since this \ninformation is by definition not available statically, the use of dynamic code generation can achieve \nperformance inherently beyond that of static code generation. Previous attempts to support dynamic code \ngeneration have been low-level, expensive, or machine\u00addependent. Despite the growing use of dynamic code \ngenera\u00adtion, no mainstream language provides flexible, portable, and efficient support for it. We describe \nC (Tick C), a superset of ANSI C that allows flexible, high-level. efficient, and machine-independent \nspeci\u00adfication of dynamically generated code. C provides many of the performance benefits of pure partial \nevaluation, but in the context of a complex, statically typed, but widely used lan\u00adguage. C examples \nillustrate the ease of specifying dynami\u00adcally generated code and how it can be put to use. Experiments \nwith a prototype compiler show that C enables excellent per\u00adformance improvement (in some cases, more \nthan an order of magnitude). Keywords: dynamic code generation, C 1 Introduction Dynamic code generation \n(i.e., generation of executable code at rwrtime) allows the use of runtime information to improve code \ngeneration. For instance, the propagation of runtime constants may be used to feed optimizations such \nas strength reduc\u00adtion, dead-code elimination, and constant folding. As another example, interpreters \ncan compile frequently used code and execute it directly [6]; this technique can improve performance \nby an order of magnitude, even compared to heavily tuned interpreters [12]. Author s current address: \nDepartment of Computer Scumce and Engineer\u00ading, University of Washington. Box 352350, Seattle, WA 98195. \nThiswork wassupportedinpanbytfreAdvancedResearchProjectsAgency under Contract NOOO14-94-1-0985; tbe last \naurbor is partially supported by an NSF National Young Investigator award The wews and conclusions contained \ninthisdocumentwehose oftheauthorsarrdshouldnotbeinterpretedasrepre\u00adserrtmgthe offic]al pohcles, either \nexpressedor implied, of the U S government Permission to make digital/hard copies of all or part of dris \nmstenal for personal or classmm use is granted without fee provided that the copies a,renot made or distributed \nfor profit or commercial advan~ge, the copy\u00ad rrght notice, the tide of the publication and its date appear, \nand notice is given that copyright is by permission of the ACM, Inc. To copy othewise, to republish, \nto post on servers or to redistribute to lists, requiras specific permission and/or fee. POPL 96, St. \nPetersburg FLA USA @1996 ACM 0-89791-769-3/95/01. .$3.50 Unfortunately, current dynamic code generation \nsystems are not satisfactory. Programmers must choose between porta\u00adbility, ease of programming (including \ndebugging), and effi\u00adciency: efficiency can be had, but only by sacrificing portabil\u00adity, ease of programming \nor, in the case of the fastest dynamic code generators [23], both. We attack all three of these prob\u00adlems \nby adding support for dynamic code generation directly to ANSI C: portability and ease of programming \nare achieved through the use of high-level, machine-independent specifica\u00adtions; efficiency is achieved \nthrough static typing, which allows the bulk of dynamic code generation costs to be paid at com\u00adpile \ntime. The result of our design effort is C (Tick C), which is ANSI C augmented with a small number of \nprimitives for dynamic code generation. C inherits many of the performance advantages of par\u00adtial evaluation \n[8, 19]. C differs from languages that support partial evalution in two ways. First, our language extensions \nand prototype implementation have been done in the context of ANSI C, a complex, statically typed, but \nvery widely used language. Second, it is not a source-to-source translation, but rather gives the programmer \npowerful, flexible mechanisms for the construction of dynamically generated code. This control allows \nprogrammers to use dynamic code generation both for improved efficiency in situations where it would \nnot normally be applicable (e.g., in the context of aliased pointers) and for simplicity (e.g., by using \nthe act of dynamic code generation to simplify an application s implementation). C provides support for \nspecifying dynamically generated code through the addition of two type constructors and three unary operators. \nThis paper makes two contributions. Tbe first is a set of efficient, flexible, high-level primitives \nfor ANSI C to specify dynamically generated code that can be statically type checked. The language design \nhas been challenging, because a static type system makes the runtime specification of arbitrary code \ndifficult (e.g., expressing functions whose number and type of arguments are not known at compile time). \nWhile the primitives are designed for ANSI C, we expect the primitives can also be added to other statically \ntyped languages. TO il\u00adlustrate how C can be used, we provide a range of examples that exploit dynamically \ngenerated code. The second contri\u00adbution is a prototype C compiler. This compiler demonstrates that the \nuse of dynamic code generation improves application performance by up to an order of magnitude. The remainder \nof this paper is structured as follows. Sec\u00adtion 2 describes the C language. Section 3 summarizes the \nC standard library. Section 4 illustrates the power of C by discussing a number of example programs. \nSection 5 reports how the prototype implementation performs on two example programs. Section 6 describes \nsome language extensions we. are considering. Section 7 relates C to other work. Section 8 summarizes \nour conclusions. Appendix A lists the base C grammar. 2 C Language We use the term dynamic code to mean \ndynamically generated code; we use static code to mean all other code. In C dynamic code is spectj$ed \nat rtmtime; these specifications can then ei\u00adther be composed to build kirger specifications, or instantiated \n(compiled at runtime) to produce executable code. We use the term compile lime to mean static compile \ntime, specification time to mean when code is being specified and composed. in\u00adstantiation time to mean \nwhen dynamic code is compiled, and runtime to mean when dynamic code is executed. 2.1 Design decisions \nThree conflicting goals have driven the design of C: 1. C must be a clean extension to ANSI C, both syntac\u00adtically \nand semantically. Extensions must not affect the syntax and semantics of ANSI C, and should be in the \nspirit of the ANSI C language. 2. C must allow flexible specification of dynamically gen\u00aderated code. \nFor instance, it must allow the construction and calling of functions whose parameters are unknown (both \nin number and in type) at static compile time. 3. C must allow an efficient implementation. The most \nimportant effect of this goal is that the majority of code generation costs must be paid at compile time, \n Designing a language that satisfies all three goals is a diffi\u00adcult problem. The second goal led us \nto avoid functional com\u00adposition for specifying dynamically generated code. Functional composition is \nconceptually elegant, but it would not give C sufficient expressive power: functional composition disallows, \nfor example, explicit manipulation of variables shared between fragments of dynamic code. An important \nissue that we faced was deciding between a dynamic or a static type system for dynamically generated \ncode. A dynamic type system aids in the flexible specification of dynamically generated code, as it adds \na form of polymor\u00adphism to the language. However, a static type system is more efficient, since it allows \nthe bulk of instruction selection and optimization to occur at compile time; in addition, it is also \nmore in the spirit of ANSI C. We chose to use a static type system for C. The resulting loss in flexibility \nis minor, especially since ANSI C does not provide any mechanisms for polymorphism, The gain in per\u00adformance \nshould be large: the information provided by a static type system allows us to push more dynamic code \ngeneration costs to compile time. As a result, we believe an aggressive compiler for C should achieve \nperformance close to that of hand-crafted dynamic code generators. The final major design decision we \nmade was to limit the construction of dynamic code to one function at a time; each invocation of the \nlibrary function compile, which instantiates dynamic code, ends one function definition. This restriction \nreduces bookkeeping complications that would otherwise be necessary in the language. Section 6 discusses \nhow it can be removed. 2.2 Language Modifications for C C adds two type constructors and three unary \noperators to ANSI C. The two new type constructors, cspec and vspec, are both postfix-declared types \n(similar to pointers). A cspec or vspec has an associated evaluation type, which is analogous to the \ntype to which a pointer points. The evaluation type allows dynamic code to be statically typed, which \nin turn allows us to select instructions at compile time. The three new unary operators, , @, and $, \nhave the same precedence as the standard unary prefix operators. 2.2.1 The Operator Dynamically generated \ncode is specified using the (back\u00adquote) operator, which is based on Lisp s usage of for spec\u00adifying \nlist templates. can be applied to an expression or a compound statement. However, backquote expressions \nmay not nest: C does not allow the specification of dynamic code that in turn may specify dynamic code. \nSome simple usages of backquote areas follows: I* specification of dynamic code for the integer 4 */ \n4 /* specification of dynamic code for ii ca// to printf j must be declared in an enclosing scope */ \nprintf( %d , j) /* specification of dynamic code for a compound statement *I { int i; for(i=O; i<l O; \ni++) printf( %d\\n , i); } Dynamic code is lexically scoped: variables in enclosing static code can be \ncaptured by free variables in dynamic code. The use of such a variable after its scope has been exited \nleads to undefined behavior; in other words, only downward funargs are legal. Lexical scoping allows \ntype-checking and instruction selection to occur at compile time. The use of several C constructs is \nrestricted within back\u00adquote expressions. In particular, a break, continue, case, or goto statement cannot \nbe used to transfer control outside its containing backquote expression. For example, the destination \nlabel of a goto statement must be contained in the backquote expression. This restriction is present \nso that a C compiler can statically determine that a control flow change is legal. The use of return \nis not similarly restricted, because dynamic code is always implicitly inside a function. 2.2.2 cspec \nTypes The type of a dynamic code specification is a cspec type (for code specification); the evaluation \ntype of Ihe cspec is the type of the dynamic value of the code. For example, the type of the expression \n4 is int cspec. By statically typing dynamic code specifications, a compiler can type-check code composition \n(described in Section 2.2.3) statically. Applying to a compound statement yields a result of type void \ncspec. The type void cspec is the type for a generic cspec type (analogous to the use of void * as a \ngeneric pointer). The code generated by may include implicit casts used to reconcile the result type \nof with its use; the standard promotion rules of ANSI C apply. Cspecs can be compiled using the compile \nfunction, which is part of the C standard library. The library is described in more detail in Section \n3. compile returns a void function pointer, which can then be cast to the appropriate type. The following \ncode fragment dynamically constructs and instantiates a tradi\u00adtional greeting: void cspec c = { printf( \nhello world\\n ); ); /* Compi/e and call the result. The Z. V indicates that the return type k void. */ \ncompile(c, TC.V)(); 2.2.3 The @ Operator The @Joperator allows dynamic code specifications to be com\u00adbined \ninto larger specifications. @Jcan only be applied inside a backquote expression; legal operands must \nbe cspecs or vspecs (vspecs are described in Section 2.2.4), and are evahrated at specification time. \n@ dereferences cspecs and vspecs: it re\u00adturns an object whose type is the evaluation type of @ s operand. \n(@on cspec and vspec types is analogous to the * operator on pointer types.) The returned object is incorporated \ninto the cspec in which the @ occurs. For example, in the following fragment, c is the additive composition \nof two cspecs: A Compose CT and c2, Evaluation of c yields 9 . *I int cspec c1 = 4, cspec C2 = 5; int \ncspec c = (@cl + @c2); Statements can be composed through concatenation: /* Concatenate two null statements. \n*/ void cspec SI = {}, cspec S2 = (}; void cspecs = { @sl; @s2; }; Applying @ inside a backquote expression \nto a function that returns a cspec or a vspec causes the function to be called at specification time \nand its result to be incorporated into the backquote expression. 2.2.4 vspec !&#38;pes A variable with \na vspec (for variable specification) type rep\u00adresents a dynamically generated lvalue; its evaluation \ntype is the type of the lvalue. Vspecs allow lvalues to be statically typed, so instruction selection \ncan be performed at compile time. Objects of type vspec may be initialized by calling the C library functions \nparam and local. Param is used to create a parameter for the function under construction; local is used \nto reserve space in its activation record (or allocate a register if possible). void vspec is used as \na generic vspec type. @is used to incorporate vspecs into cspecs. An incorporated vspec (like a variable \nin ANSI C) can be used both as an lvalue and an rvalue inside a backquote expression. The following function \nreturns code that takes a single integer argument, adds one to it, and returns the result: void cspec \npk.4 (vo!d) { /* param takes an argument index and its type. */ int vspec i = (int vspec) param(O, TC.1): \nreturn { return @i + 1; };} Vspecs allow us to construct functions that take a runtime\u00addetermined number \nof arguments; this power is necessary in applications such as the compiling interpreter described in \nSection 5.2. Consider a function sum that takes a rtmtime\u00addetermined number of integer arguments, sums \nthem, and re\u00adturns the result, While it is possible to construct such a function using C s variable argument \nfacilities, or by requiring that the arguments be marshaled in an integer vector, such solutions are \nclumsy and inefficient. With vspecs one can easily construct the desired function: /* Construct cspec \nto sum n integer arguments. */ void cspec construct-sum(int n) { int i, cspec c = O; for(i=O; i<n; i++)( \nI* create a parameter *I int vspec v = (int vspec) param(i, TC_l); /+ Add param v to current sum. */ \nc = (@c + W); } return { return @q }; } 2.2.5 The $ Operator The $ operator allows runtime constants \nto be incorporated into dynamic code. $ evaluates its operand at specification time; the resulting value \nis incorporated as a runtime constant into the containing cspec. $ may be applied to any expression within \ndynamic code that is not of type cspec or vspec. The use of $ is illustrated in the code fragment below. \nint cspec c1, cspec c2; void cspec c intx=l; A Bind x as a runtime constant with value 1 */ c1 = $x; \n/+ Bind x at runtime when the code for C2 is run */ C2= x; c = { printf( $x = %d, x = %d\\n , @cl, @?c2);); \nX=14; /* Compile and run: will print $x= 1, x = 14 . */ compile(c, TC-V) ();  2.3 Discussion C is a \nstrict superset of ANSI C. There are, however, necessary departures from the spirit of C at some points. \nFor example, the memory required to represent a cspec is the responsibility of the C rtmtime system. \nHence, cspecs are objects whose allocation and manipulation is not controlled by the programmer. This \nis a marked departure from ANSI C, in which dynamic allocation of objects is controlled by the programmer \nusing malloc. In addition, there is some runtime checking in C that is not present in ANSI C. For example, \nthe compiler must guard against conflicting parameter definitions. The return type of dynamically constructed \nfunctions is specified dynamically as well; there seems to be no good way to do it statically. As a result, \nchecking and promotion of return types must be done at runtime, 3 The C Standard Library We have designed \nand implemented a small library in order to minimize changes to ANSI C. For example, we have avoided \nchanging ANSI C to allow the construction of function calls at runtime. The library provides the following \nfunctions: void (*compile(void cspec code, int type-spec [, int type-size])) (void) compile generates \nmachine code from code. type\u00adspec gives the return type of generated function; type-size is an optional \nargument used to give the size of aggregate types. In the future, compile will take a number of flags \nthat relate to optimization, debugging and profiling. void vspec Iocal(int type-spec [, int type-size]) \nlocal returns a vspec that represents a local vari\u00adable of type type-spec, and reserves space in the \nactivation record of the function currently being specified. void vspec param(mt paranr-nurn, int type-spec \n[, int type-size]) param returns a vspec that represents a parameter of type type-spec and number param \n-num. void cspec arg(int arg-num, void cspec args, int type-spec, void cspec code-spec [, int type-size]) \n void cspec push (void cspec args, int type-spec, void cspec code-spec [, int type-size]) arg returns \na cspec that represents code to move code-spec into the correct position for argument arg-num. push performs \nthe same action as arg, except that the argument number is implicit. void cspec jump(void cspec target) \njump returns the cspec of a jump to target. This function is useful for constructing hardcoded finite \nstate machines. void (*self(void))(void) self returns a pointer to the function that the next invocation \nof compile will return. This function allows the construction of recursive dynamic pro\u00adcedures. 3.1 Type \nSpecifications A number of functions in the C standard library expect types as arguments: local, param, \narg, push, and compile. This type information is specified using enumerated types. Bui [t-in types are \nspecified by the first letter(s) of their type prefixed by TC.. For example, unsigned short is specified \nby TC.US. All point\u00ad ers are represented by TCJ? operationally, they me treated as void * pointers for \npurposes of storage and register allocation. Aggregates (arrays and strtmm.s) are specified by TC-B;their \nsize must be given as an additional argument. Finally, the flag TC.REGISTER can k bitwise-ored with the \ntype to indicate that the allocated object will not have its address taken. While in\u00ad formation about \naddressing could be derived at runtime, doing so quickly would add complexity to the code generator. \n 3.2 Runtime-Constructed Function Calls Function calls can be constructed on the fly by using the library \nfunctions arg and push. Consider the function sum, which is described in Section 2.2.4. To call sum one \nconstructs the argument list at runtime by using push. C allows a void cspec (which represents the argument \nlist) to be incorporated as a single argument in a call: SUm(@args) specifies code that calls sum using \nthe argument list specified by @args: int cspec construct.call(int nargs, int *arg-vet) { void cspec \nargs = (); /* initializatlorr */ int i; A For each argument in arg_vec, */ for (i = O;i < nargs; i++) \n( /* create cspec that pushes it on call stack. */ args = push(args, TC_V, $arg.vecb]); } return sum(@args); \n} 4 Examples This section gives program examples to illustrate C s power. These programs exhibit how \nflexibly dynamic code can be specified in C, as well as some of the different uses to which dynamic code \ngeneration can be put. 4.1 Runtime constants Dynamic code generation can be used to hardwire infrequently \nchanging rtmtime values into the instruction stream, so that the values need not be loaded from memory. \nAn example is a generic hash function, where the table size is determined at runtime, and where the function \nuses a runtime value to help its hash. Consider the following C code: A hash table entry structure */ \nstruct hte ( int val; /* key that entry is associated with */ struet hte *next; /* pointer to next entry*/ \n/* ... +/ 1; [* hash table structure */ struct ht { 134 !nt scatter; /* value used to scatier keys */ \nint norm; /* value used to normalize *I A vector of pokrters to hash table entries W/ struct hte ** hte; \n}; /* ,4 hash table pointer (ht) stores a po;nter to entries in its associated table of hash entries \n(hfe) along with a scatier value and size. hash returns a pointer to the hash table entry that matches \nval (or null, if there is no match). */ struet hte *hash(struct ht *ht, int val) { struct hte * hte; \nhte = ht+hte[(val * ht-+scatter) / ht+norm]; while (hte &#38;&#38; hte-+val != val) hte = hte-+next; \nreturn hte; The C function has three values that can be treated as runtime constants: ht-+hte, ht--+scatter, \nand ht-+norm. The following C code specializes the function for these values. A Type of the function \ngenerated by mk.hash: takes a value as input and produces a (possibly null) pointer to a hash table entry. \n*I typedef struct hte *(*hptr)(int val): /* Construct a hash function with the size, scatter and hash \ntable pokrter hard coded. (The function must be regenerated when any of these values are changed.) *I \nhptr mk-hash(struct ht *ht) { int vspec val = param(O, TC_l); void cspec hq hc= ( struct hte *hte; hte \n= ($ht+hte)[(@val * $ht--+scatter) / $ht+norm]; while (hte &#38;&#38; hte-+val != @val) hte = hte+next; \nreturn hte; ); /+ Compile and return the result. +/ return (hptr) compile(hcr TC.P); This function can \nbe specialized even further. For instance, an application could select from several different hash func\u00adtions, \ndepending on the characteristics of the input stream. The C version can be much faster than the equivalent \nC version, since a C compiler can exploit the rrmtime constants hte, scatter, and norm both by hardcoding \nthem directly in the instruction stream and by strength reducing the multipli\u00adcation and division to \nshifts and adds. Such optimizations are increasingly profitable on modern architectures, where cache \nmisses are very expensive and division and multiplication are frequently provided in only software [18, \n31]. This examples also illustrates that turning a C function into C requires few changes. The cost of \nusing the C generated function is an indirect jump on a function pointer. 4.2 Matrix Dot Product Matrix \nmanipulations such as dot product are a fruitful realm for dynamic code generation. Matrices often have \nruntime char\u00ad acteristics (e.g.. large numbers of zeros and small integers) that cannot be exploited \nby static compilation techniques. In addi\u00ad tion, sparse matrix techniques are only efficient for matrices \nwith a high degree of sparseness. In the context of matrix multiplication the use of dynamic code generation \naHows us to exploit zeros and small integers by crafting locally optimized code based on runtime values. \nBecause code for each row is specified once and then used n times (once for each column), the costs of \ncode generation are easily recouped. Consider the following C code to compute the dot product: void compute-dot(int \n*a, int *b, int n) { int sum, k; for (sum =k=O;kc n;k++) sum += a[k]*b[k]; return sum; } At runtime several \noptimizations can be employed. For example, the programmer can directly eliminate multiplication by zero. \nThe resulting C code is given below. /* Construct code to multiply arbitrary vector by a given row. */ \nvoid cspec mkdot(int row[], int n) { int k, cspec sum, * vspec COI; /* pointer to the COIto multiply \nrow by */ col = (W * vspec) param(O, TC.P); sum = O; for (k =O;k<n;k++) ( /* Only generate code for non-zero \nmultiplications. +/ if (row[k]) { /* Hardwire the index used to load COI;$row[k] yields a runtime constant \nthat allows the multiplication by col[$k] to be reduced in strength. This result is added to the accumulated \nsum. */ sum = (@sum + (@col)[$k] * $row[k]); } } return { return @sum; }; ) The dot product written in \nC can perform substantially better than the one programmed in C. The C code omits code generation for \nnon-zero multiplications. In addition, the C compiler can encode values as immediate in arithmetic instructions \nand can take advantage of the runtime constant row[$k] to replace multiplication with shifts and adds. \n 4.3 Parametrized Functions Many library routines are parametrized via function point\u00aders. For instance, \nthe standard C library provides quicksort and heapsort, which accept user-defined routines for perform\u00ad \ning comparisons. Many mathematical libraries also provide support for solving generic functions in the \nsame way, Unfor\u00adtunately, indirect function calls can eliminate many potential optimizations, since the \nfunction cannot be integrated with the library code. By using C to compose cspecs instead of func\u00adtion \npointers, library functions can be parameterized easily and efficiently, since cspecs can be integrated \ndirectly into C code without extra function calls. The C code for Newton s method [5] i Hustrates C s \nadvan\u00adtages for parametrized functions. The function newton takes asarguments thealIowed number of iterations, \na tolerance, an initial estimate, and two pointers to functions that return cspecs to compute a function \nand its derivative. The cspecs returned by these functions are incorporated directly into the dynamically \ngenerated code, which eliminates function calI overhead and allows inter-cspec optimization to occur \nat instantiation time. [* pointer to a furrct!on that takes a vspec as an argument and returnsa cspec. \n*/ typedef double cspec (*dptr)(double vspec); [* Dynamically create anewton raphson routine speciahzedto \nfhe given function and derivative. In a real implementation we would memoize the function and, if it \nwas used heaviliy enough, unroll the loop to the maximum number of iterations. n is the number of allowed \niterations; pOr IS the initial estimate. f is the function to solve and fprime is its derivative. */ \ndouble newton(int n, double tol, double usr.pO, dptr f, dptr fprime) { void cspec cs = { int 1; double \np, PO; pO = usr.pO; for (i= O;i<$n; I++) ( /+ Incorporate the cspec returned by f and fprime and use \nthem to calculate the next point in the (hopefully) convergent series. */ p = pO -@f(pO) / @prime; /+ \nWhen we converge to a given to!erance, return the result. */ if (abs(p pO) < tol) return p; pO = p; \n/* Seed the next Iteration. */ ) error( method failed after %d iterations\\ n , i); }; /* Compile, call, \nand return the result. */ return ((dptr)compile(cs, TC.D))(); ) Note that in the calls f(pO) and fprime(pO), \npO is sent as a vspec argument at instantiation time. The example below illustrates how to use newton \nto find the root of the function f(z) = (z+ l) : /* Function that constructs a cspec to compute f(x) \n= (x+ 1)-2. *I double cspec f(double vspec x) ( return ((@x + 1.0) * (@x + 1.0)); ) /+ Funct;on that \nconstructs a cspec to calculate the derivative off f(x) = 2(X+1)*/ double cspec fprime(double vspec \nx) { return (2.0 * (@x + 1.0)); } /+ Call newton to solve an equation. */ void use.newton(void) { double \nroot root = newton(l 00, ,000001, 10., f, fprime); printf( root is %f\\n , root); } 4.4 Small Language \nCompilation There are myriads of small, primitive languages that are both time-critical and amenable \nto dynamic compilation. The small query languages used to interrogate data bases are well-known targets \nfor dynamic code generation [21 ]; because databases are large, dynamically compiled queries will be \napplied many times. We provide a small example below. The function mkquery takes a vector of queries, \nwhere each query specifies a database record field (such as CHILDREN or INCOME), a value to com\u00adpare \nthis field to, and the comparison function to use. The compiler creates code fragments to access each \nfield and then generates code to compare it to the given value. typedef enum { INCOME, CHILDREN /* */} \nquery; typedef enum { LT, LE, GT, GE, NE, EQ } bool-op; /* Query. +/ struct query { query record_field; \n/* which field to use */ unsigned val; /* value to compare to */ bool-op bool-op; A operation used to \ncompare */ }; /+ Simple database record. */ struct record { int income; int children; /* .,. +/ }; \n/* Type of the function generated by mkquery. Takes a pointer to a database record as its sole argument \nand returns O or 1 depending on whether the record matched the query, */ typedef int (*iptr)(struct record \n*r); /* Compile the given query by constructing a boolean expression built up from its specified predicates. \nA predicate is made up of (field, val, booLop) where field indicates a record field, val IS the vahe \nto compare it to, and booLop is the operation used m the companion (<, >, etc ), */ iptr mkquery(struct \nquery *q, int n) { int cspec field, cspec exprr i; struct record * vspec r, /* r is a pointer to the \nrecord to compare to. +/ r = (struct record * vspec) param(O, TC.P); expr = 1; for(i=O; i<n; l++)( /* \nfirst load the appropriate field value*/ switch (q~l.record.field) { case INCOME: field = ((@r) +income); \nbreak; case CHILDREN: field = ((@r) +children); break; /* */ } /* compare the field value to the runtime \nconstant in qfil using the given boolean operation. */ switch (qhl.bool.op) { case LT expr = (@expr &#38;&#38; \n@field < $q~].val); break; case EQ: expr = (@expr &#38;&#38; @field == $q~l.val); break; case LE: expr \n= (@expr &#38;&#38; @field c= $qfi].val); break A .. */ } } return (iptr) compile( { return @expr; ), \nTCJ); } 4.5 Function Composition In a manner similar to function parameterization, C also allows modular \nfunction composition. Inexpensive function composi\u00adtion has many applications; an important one is the \noptimiza\u00adtion of networking code. The modular composition of different protocol layers has long been \na goal in the networking community [7]. Unfortu\u00adnately, each protocol layer frequently has data-touching \noper\u00adations associated with it (e.g., checksumming, byte-swapping, etc.). As a result, as data moves \nthrough each layer, data can be touched multiple times, which is expensive [7]. C can be used to construct \na network subsystem that solves this problem by dynamically integrating protocol data oper\u00adations into \na single pass over memory (e.g., by integrating encryption and compression into a single copy operation). \nA simple implementation of such a system would be to divide each data manipulation state into pipes that \neach consume a single input and produce a single output. These pipes can then be composed and incorporated \ninto a data copying loop. To allow pipes to manipulate state, they are allowed to specify initial and \nfinal code to call, which would be used by other ap\u00adplications to initialize variables and detect errors. \nFor instance, a checksum routine would have final code to check whether the checksum was valid. The following \npipe can be used to do byte-swapping. Since a byte swapper does not need to maintain any state, there \nis no need to specify initial and final code. /+ .Example pipe: construct a cspec that, given an input, \nproduces a byte swapped output. */ unsigned cspec byteswap(unsigned vspec input) { return ( (@input c< \n24) I ( (@input&#38; OxffOO) << 8) I ( (@input>> 8) &#38; OxffOO) I ( (@input >> 24) &#38; Oxff)); } \nI* byteswap does not maintain any state and so does not need initial and final statements. *\\ void cspec \nbyteswap-initial(void) { return (}; ) void cspec byteswap.final(void) ( return (}; } To construct the \nintegrated data copying routine, the initial, consumer, and final statements of each pipe are composed \nwith the code of its neighbors, respectively. The composed initial statements are placed at the beginning \nof the routine, the consumer statements are placed in a loop to provide them with input and store their \noutput, and the final code is placed at the end of the routine. A simplified code fragment is provided \nbelow. In a mature implementation the loop would be unrolled. Additionally, pipes would take different \nsize inputs and outputs (or gauges ) that the composition function would have to reconcile. I* pointer \nto function that returns a void cspec +/ typedef void cspec (*vptr)(); /* pointer to function that returns \nan unsigned cspec */ typedef unsigned cspec (*uptr)(unsigned cspec); /* Pipe structure: contains pointers \nto functions that return cspecs that specify the initialization, pipe, and finalization code for each \npipe. */ struct pipe { vptr initial; /* uvtial code */ uptr pipe; /* pipe*/ vptr final; /* final code*/ \n 1, /* Return cspec that results from composing the given vector of pipe tuples. Creates a function that \ngiven an input pointer, an output pointer and the number of words, will apply the pipe expression to \neach as it copies the data. */ void cspec compose(struct pipe *plist, int n) { struct pipe *p; unsigned \ncspec result, cspec pipes; unsigned * vspec input, * vspec output int vspec i, vspec nwords; void cspec \ninitial-stints = {}; void cspec final-.stmts = {); nwords = (int vspec) param(O, TC.I); /* pointer to \nmemory that provides pipe input*/ input = (unsigned * vspec) param(l, TC_P); /* pointer memory that accepts \npipe output*/ output = (unsigned * vspec) param(2, TC-P); i = (irrt vspec) local(TCJ); /* Compose all \nstages in parallel */ pipes = (@input) [@ii; I* base pipe input*/ for (p= &#38;p!ist[O]; p < &#38;plist[nl; \np++) ( /* compose initial statements *I initial_stmts = { @initiaLstmts; @p+initialo; h I* Compose pipes: \nthe previous pipe s output is the next pipe s input. Note: in the generated code, pipes are *not* function \ncalls, but simply arithmetic expressions that occur in a specified order, */ pipes = @p+pipe(pipes); \n /* compose final statements *I finalstmts = { @Mnal.stmts; @p+ fmalo; }; } /* Create a function with \ninitial statements first, consumer statements second and final 5tatemenf5 last. +/ return { @initlal_stmts; \n/* loop over input, piping through consumer. +/ for (@i = O; @i < @nwords; (@i)++) (@output)[@il = @pipes; \n@finaLstmts; k Note that and @ have the same precedences as unary minus. As a result, the expression \n@p--+ pipe(pipes) results in the function pointed to by p+pipe being called and its result being used \nas the value of the expression at specification time. 4.6 Marshaling and Unmarshaling Another example \nuse of C is the construction of code to mar\u00adshal and unmarshal arguments stored in a byte vector. These \noperations are frequently performed to support remote pro\u00adcedure call [4]. By generating specialized \ncode for the most active functions it is possible to gain substantial performance benefits [33]. The \ngeneration of marshaling code relies on C s ability to specify arbitrary numbers of incoming parameters. \nFigure 1 gives a simplified code fragment that generates a marshal\u00ading function for a particular type \nset (in this example, INT, POINTER, and DOUBLE). The code works as follows. First, it allocates storage \non the stack for a byte vector large enough to hold the arguments specified by the type format vector. \nThen, for every type in the type vector, it creates a vspec that points to the current parameter: it \nconstructed code to store the pa\u00adrameter s value into the byte vector at the current offset; and it adds \nthe size of the type to the offset, Finally, it specifies code to call a function pointer with the marshaled \narguments as an argument. At the end of code generation the function that has been constructed will store \nall of its parameters at fixed, non-overlapping offsets into a stack-allocated memory block. Since all \ntype and offset computations have been done during specification time, the generated code will be efficient. \nRrr\u00adther performance gains could be achieved if the code were to manage endianness, alignment, etc. The \ngeneration of unmarshaling code is equally profitable. Dynamic generation of unmarshaling routines relies \non our mechanisms for constructing calls (to arbitrary functions) at runtime. The ability to invoke arbitrary \nfunctions is not just useful for efficiency: it is also useful for functionality. For ex\u00adample, in TcI \n[25] the runtime system can make upcalls into an application. However, because Tcl cannot dynamically \ncreate code to call an arbitrary function, it marshals all of the upcall arguments into a single byte \nvector, and forces applications to explicitly unmarshal them. If systems such as Tcl used C to construct \nupcalls, clients would be able to write their code as normal C routines, which would increase the ease \nof expression and decrease the chance for errors. /* Type of the function called by the code ink-marshal \ngenerates. It takes a pointer to memory where the caller s arguments have been marshaled. */ typedef \nvoid (*vptr)(char *buf); A Type of the function generated by mk.marshal: takes an unspecified number \nof arguments. */ typedef void (*mptr)(); A The types of arguments ink-marshal can marshal. */ typedef \nenum { DONE, INTEGER, DOUBLE, POINTER, /. */) types-t; /* Generate a function at runtime to marshal arguments \ninto a byte vector and call vp with them. */ mptr rnk-marshal(vptr vp, types-t *types) { char * vspec \nm; void cspecs; int i, offset; vptr vp; /* Allocate a buffer on the stack to hold arguments. */ m = \n(char * vspec) local(TC-B, compute-size(ty pes)); s = {}; /* Initialize marsfrahng statements, */ for \n(i = offset= O; types[l] != DONE; i++) { switch(typeshl) { case [NTEGER: { A create a vspec for the \nith parameter *I int vspec v = (int vspec) param(i, TC.I); /* Concatenate the code to marshal an integer \ninto @m. The code uses the offset to compute the location in m. */ s = ( @s; * (int *) &#38;(@m)[$offset] \n= @v; }; A add the size of the argument to the current buffer offset */ offset += sizeof(int); break; \n } case DOUBLE: ( /* create a vspec for the ith parameter */ double vspec v = (double vspec) param(i, \nTC-D); I* concatenate the code to marshal a double */ s = { @s; * (double *) &#38;(@m)[$offsetl = W; \n}; offset += sizeof(double); break case POINTER: { /* create a vspec for the ith parameter*/ void * \nvspec v = (void * vspec) param(i, TC_P), /* concatenate the code to marshal a pointer */ s = ( c&#38;; \n* (void **) &#38;(@m)[$offsetl = @v; }; offset += sizeof(void *); break; /* */ } } /* after the arguments \nare marshaled, call the passed function with the marshaled data */ return (mptr) compile( { G%;($vp)(@m); \n}, TC.V); } Figure 1: C code to generate a function to marshal a byte\u00advector containing the given types. \n/* Type of the function generated by mk.marshal. It takes a pointer to memory where the marshaled arguments \nare stored. W/ typedef void (*vptr)(char *buf); A Generate a function to unmarshal a byte vector into \narguments and call a function. For simplicity, we ignore alignment and endian issues. */, vptr mk_unmarshal(ty \npe.t *types) { vptr vspec vp; char * vspec m; void cspec args; int offset, i; /* Function pointer: \nsent as first argument. *I vp = (vptr vspec) param(O, TC_P); 1* Byte vector: sent as second argument. \n*/ m = (char * vspec) param(l, TC-P); A Dynamically constructed argument list. */ args = {); for(i = \noffset= O; typesfi] != DONE; i++) ( switch(typesti]) { case INTEGER: A code to extract an int */ push(args, \nTCJ, * (int *) &#38;(@m)[$offsetl); offset += sizeof(int); break; case DOUBLE: /. code to extract a \ndouble */ push(args, TC.D, * (double*) &#38;(@m)[$offsetl); offset += sizeof(double); break; case POINTER: \nA code to extract a pointer */ push(args, TC_P, * (void **) &#38;(@m)[$offsetl); offset += sizeof(void \n*); break; /* ,. */ } } f* Call the given function pointer with the unmarshaled parameters. *I return \n(vptr) compile( { @vp(@args); ), TC-.V); ) Figure 2: C code to generate a function to unmarshal a byte\u00advector \ncontaining the given types. Figure 2 gives the code that generates the unmarshaling function to work \nwith the marshaling code specified in Figure 1. The generated code will take a function pointer as its \nfirst argument and a byte vector of marshaled arguments as its second. It unmarshals the values in the \nbyte vector into their appropriate parameter positions, and then invokes the function pointer. The code \nis generated as follows. First, the parameter variables for the generated functions incoming arguments \nare created. Second, the argument list is initialized. Then. for every type in the type vector the function \nperforms the following sequence of actions: it creates a cspec to index into the byte vector at a fixed \noffset; it pushes this cspec into its correct parameter position; and it adds the size of the unmarshaled \nelement to the current offset. Finally, the call to the vspec function pointer with the constructed argument \nlist is specified, and the result is compiled. 5 Performance We have implemented a prototype C compiler \nthat emits ANSI C code augmented with calls to DCG S [13] dynamic code gen\u00aderation primitives. The compiler \nparses, semantically checks, and generates code for C. It generates code correctly for most of the examples \nin this paper. Our prototype demonstrates that despite our lack of optimization and DCG S rudimentary \noptimization (it does not perform instruction scheduling nor peephole optimization), the generated code \nstill achieves good performance. We are developing a full C compiler that will generate fast code using \ntemplates and VCODE. VCODE [11] is a retargetable, extensible, very fast dynamic code generation system. \nIt is a portable assembly language that generates specialized code on the fly; the cost for dynamic code \ngeneration is about ten instructions per generated instruction. Templates are highly specialized code \nemitters where the instructions have been chosen statically; any holes in the instructions (e.g., rtmtime \nconstants and addresses of variables) are filled at runtime [23]. Since templates, combined with C s \nstatic type system, allow the bulk of code generation analysis to be done at compile time, we will emit \ncode very quickly. We expect the use of templates and VCODE to improve the speed of dynamic code generation \nby an order of magnitude. The rest of this section presents performance results using the prototype compiler \nfor two C programs that are based on the examples used in [13]. The experiments were conducted on a SPARC \n10 system that does integer divide and multi\u00adply in software. Times were measured using the Unix system \ncall getrusage and include both user and system time. The times given are the median time of three trials. \nStatic compila\u00adtion was done using gcc version 2.5.8. 5.1 Matrix Scaling Scaling a matrix by a rtmtime \nconstant allows ample opportu\u00adnity for speedup from the use of dynamic code generation. For instance, \nmultiplication can be reduced in strength to shifts and adds [2]; division can be reduced in strength \nto multiplication, and then to shifts and adds [15]. Additionally, loop bounds can be encoded in branch \nchecks as constants, which can alleviate register pressure. The C code for expressing matrix scaling \nby a rtmtime constant is shown in Figure 3. We compare its performance to that of a static matrix scaling \nroutine. We qm two experiments, one for division, the other for mukiplication. Multiplication is /+ Construct \ncode to scale matrix m of size n x n by run time constants. */ void cspec mkscale(lnt s, int n, int **m) \n{ return { int i,j; /* $n can be encoded directly in the loop termination check */ for (I= O;i<$n:i++) \n( int *v = ($m)fi]; for (j = O;j < $n; ]++) /* multiplication by %- can be strerr@h-reduced at runtime \n*/ V[jl = V[jl * $s; } 1, Figure 3: C code for scaling a matrix by a runtime constant done on a matrix \nthat contains ints; division on a matrix that contains shorts. The experimental times given in Figure \n5 mea\u00adsure the summation of the time required to scale a 1024xIO24 matrix by the integers 1 through 1024; \nin the C implementation we include the time to generate the code at runtime. The performance of multiplying \na 1024x I 024 matrix of ints by a runtime constant improved by a factor of 3. The performance of dividing \na 1024x1024 matrix of shorts by a runtime constant improved by ss~o. More dramatic improve\u00adments would \nbe possible with a more sophisticated factoriza\u00adtion scheme for reducing division in strength. The C \nmatrix scaling code is approximately a factor of 2 3 slower than hand\u00adoptimized DCG code[13 ], because \nthe prototype compiler emits naive DCG IR. and does not perform globaI optimization. 5.2 Compiling Interpreter \nInterpreters can use dynamic code generation technology to improve performance by compiling and then \ndirectly executing frequently interpreted pieces of code [6, 10]. To show that C can be used to do this \neasily and efficiently we present a recursive-descent compiling interpreter that accepts a subset of \nC, called Tiny C [13]. Tiny C has only an integer type; it supports most of C s relational and arithmetic \noperations (/, -, <, etc. ) and provides if statements, while loops, and function calls as control constructs. \nA subset of the parser is shown in Figure 4, What should be noted is the degree to which the flexibility \nof C is exercised: functions having an arbitrary number of parameters and local variables can be created, \nand code is specified and composed in a diffuse fashion. Without the flexibility afforded by C, this \nexample would be difficult to write. In addition, our experience has been that specifying dynamically \ngenerated code in C is easier than constructing an efficient interpreter. A recursive Fibonacci program \nis used to measure the per\u00adformance of three implementations of Tiny C. C: The C compiling interpreter \nfor Tiny C. gcc -02: Tiny C using gcc with optimization level 4M . This gives an upper bound on the quality \nof local code. Tree-interpreter: A simple interpreter that translates Tiny C into abstract syntax trees, \nwhich it then recursively eval\u00ad uates. Description c Statically compiled C code Multiplication 390 1100 \nDivision 570 770 Figure 5: Matrix scaling routines; times are m seconds, C \\ gcc -02 I Tree-Interpreter \n2.1 I 1.8I 102 Figure 6: Calculation of the 30th Fibonacci number: times are in seconds. Figure 6 summarizes \nthe results for computing the 30th Fibonacci number. The code that is generated using C s sim\u00adple backend \nm fairly efficient: its performance is 85% of gee s performance Since these numbers include the cost \nof dynamic code generation, these numbers are lower bounds on the perfor\u00admance of C code. Comparing the \nC results to the interpreter, we see that using dynamic code generation is 50 times faster than the evaluator. \nFrom a more global perspective, this same technique can give order of magnitude improvements in the performance \nof operating system extension languages such as packet filters [24]. The C results are within 10% of \nhand-generated DCG code, The simple IR generation that the C compiler performs does not lower the performance \nas much as in the matrix scaling example, because the dynamic code fragments are so simple. 6 Language \nExtensions Language design is an iterative process. This section describes extensions that we are considering \nfor C. The C prototype compiler does not implement them, since they were designed after the prototype \nwas written. We intend to have a publicly available compiler that implements the full C language in the \nnear future. 6.1 Partial Evaluation We have designed support for dynamic partial evaluation in C. While \npartial evaluation does not change the power of C, it is useful syntactic sugar. Incorporating partial \nevaluation was complicated by the requirement that we want to support specialization of functions with \nrespect to different arguments at different times. For instance, a programmer may want to specialize \nthe following simple function with respect to x at one call-site and y at another: int foo(lnt x, int \ny, short z) { return x + y + z; } In a statically-compiled language such as C, code gen\u00aderation of every \nfunction template happens at compile time. Therefore, to prevent the code explosion that would result \nfrom specializing every function for every possible combination of arguments, C provides partial signatures \nto statically specify the possible permutations of arguments with which a function will be specialized. \nPartial signatures (modeled on function prototypes) are used to indicate which arguments can be spe\u00adcialized \nin a function A partial signature is a function prototype prefixed with the pattial keyword; it contains \nthe bound type specifier before each argument that can be specialized, There [+ Set of helper parsing \nfuncbons */ /+ Remove token from the input stream; returns type. */ int gettolc(void); /* Put a token \nback into the input stream. */ void puttok(void); /* Result of compare of tok to next input symbol. +/ \nint look(int tok); /* Consumes expected token or gives parse error. +/ void expect(int tok); char *cur.tok; \n/* Pointer to current token, */ /* Symbol table functions */ /* Associate v with name; error to insert \nduphcate */ void insert_sym(char*name, mt vspecv); /* Return the vspec associated with a given name */ \nint vspec Iookup(char *name), A Return function pointer associated with name. */ int (* fptr.lookup(char \n*name))(); /* Parse declarabons */ void declare(vold) { if(! look(lNT)) return; I* no more declarations \n*I gettoko: while(gettoko == ID) { msert(cur-tok): switch(gettoko) { case , , break; /* another declaraborr \n*/ case ; . declareo; return; /* start next decLseq */ default parse-err( malformed declaration ); } \n) parse.err( expecting ID ); } /* Parse binary expressions */ int cspec exprl (mt cspec e) { switch(gettoko) \n{ case + : return (@e + @expro); case - : return (@e -@expro); case * : return (@e * @expro); case j \n; return (@e / @expro); case < : return (@e < @expro); case LE: return (@e <= @expro); case > : return \n(@e > @expro); case GE: return (@e >= @expro); case NE: return (@e != @expro); case EQ: return (@se== \n@expro); case ) : case ; : pushtoko; return e; default: parse.err( bogus exprl ), ) ) /* Parse unary \nexpressions and ( expr ) *I int cspec expr(vold) { switch(gettoko) { case CNST return exprl ( $atol(cur.tok)); \ncase + : return expro; case : return @expro; case ! : return !@expro; case (r: ( int cspec e = expro; \nexpect( ) ); return e; ) case ID: /* ID or function call*/ if (! Iook( ( )) return exprl ( @lookup(cur-tok)); \nelse return exprl (fcallo); default parse.err( bogus expr r); ) ) Figure 4: A Subset of the Tiny-C /* \nParse function talk */ void cspecfcall(vold) { mt (* ip)() = fptrJookup(cur-tok); void cspecargs= (}; \ngettoko; A consume ( */ if (look( ) )) { gettoko: /* no args; consume 7 */ return ($ip) (); ) while (1) \n{ /* get argumerrt list */ args = push(args, i , (void cspec) @expro); if (Iook( , )) gettoko: else \nif (look( ) )) break else parse-err( malformed arg hst ); ) gettoko; /* consume ) */ return ($ip)(@args); \n ) /* simple iteration and control flow statements*/ void cspec stmt(vold) { mt cspec e; void cspecs, \nSI, 52; switch(gettoko) { case RETURN: /* return expr ; */ s = { return @expro; L expect : ); return \ns; case WHILE: /* while ( expr ) stint*/ expect( ( ); e = expro; expect( ) ); s = stmto; return { while(@e) \n@s;k caseIF:/* if ( expr ) stint{ else stint)? */ expect( ( ); e = expro; expect( ) ); sl = stmto; if \n(1look(ELSE)) return ( if (@e)@l; h gettoko; S2= stmto; return { if (@e)W; else @s2;); case { : /* { \nstmt$+ 1 */ push.scopeo;declareo; s = {~ while (! look( ) )) s = ( c%; @stmto; L expect( ) ); pop-scopeo; \nreturn s; case 4; : return {~ case ID: /* ID = expr ; */ { mt vspec Ivalue = Iookup(cur-tok); expect \n= ); e = expro; expect( ;r); return { @lvalue = @e; k } default: /* expression statements not allowed*/ \nparse-err( expecting statement ); ) Recursive-Descent Parser can be multiple partial signatures for a \ngiven function; each unique partial signature must be in the same scope as the func\u00adtion definition. \nFor example, the following partial signatures allow specialization with respect to the first parameter \nx or the second parameter y: /* Evaluate foo with respect to x. *I partial int foo(bound int x, int y, \nshort z); /* Evaluate foo with respect toy. */ partial int foo(int x, bound int y, short z); Partial \nevaluation of a function M performed using the unary prefix operator eval. eva[ tales a function and \nits arguments, and returns a function pointer with the return type of the function and with parameter \ntypes corresponding to the non-evaluated parameters. The unbound keyword is used as a placeholder to \nindicate which arguments are not being provided during partial evaluation. Attempting to use eval with \nan operand whose type signature does not correspond to any partial signature in scope is an error. For \ninstance, the following are valid specializa\u00adtions of the function foo, whose partial signatures are \nprovided above: Int (*lpl)(int, short); int (*ip2)(int, short); /* create a partial evaluation with \nthe first parameter fixed */ ipl = eval foo(4, unbound, unbound); /* create a partial evaluation with \nsecond parameter fixed */ iP2 = eva~foo(unbound, 5, unbound): We expect that partial evaluation of statically \nspecified functions will be a common usage, which is why we added it to C. 6.2 Other Modifications In \nthe interests of full generality, we are exploring support to allow multiple functions to be generated \nsimultaneously. Every vspec would have to be explicitly associated with a context, and the compiler and \nruntime would have to ensure two conditions. First, all vspecs used in a cspec would have to be from \nthe same context. Second, all cspecs that are composed would have to contain only vspecs from the same \ncontext. One of the most unfortunate features of C is the use of manually supplied types in the rtmtime \nsystem. We are exper\u00adimenting with alternative approaches to provide a cleaner, less error-prone mechanism. \nWe are also experimenting with mechanisms to allow code fragments to be parametrized. Parameterization \naids the mod\u00adular composition of cspecs, since the internal names of a cspec can be hidden. We will most \nlikely introduce a lambda unary operator to create nameless functions. While this functional\u00adity can \nbe s]mulated in the original C language, the syntactic sugar of lambda can remove awkwardness in some \nsituations. 7 Related Work Dynamic code generation has a long history. R has been used to increase the \nperformance of operating systems [3, 12, 27, 28], windowing operations [26], dynamically typed languages \n[6, 10, 17], simulators [30, 34] and matrix manipulations [13]. C grew out of our previous work with \nDCG [13], an effi\u00ad cient, retargetable dynamic code generation system. C offers several improvements \nover DCG, but retains DCG S portabil\u00ad ity and flexibility. First, C provides a high-level interface for \ncode specification, whereas DCG S interface is based on the in\u00ad termediate representation of ICC [14]. \nSecond, it provides the opportunity for static analysis, which reduces the cost of dy\u00ad namic compilation; \nbecause it has no compiler support, DCG must do rtmtime analysis. Finally, because we have made dy\u00ad namic \ncode generation a first-class capability of a high-level language, both profiling and debugging facilities \ncan be added, Many languages, such as most Lisp dialects [29, 32], Tcl [25], and Perl [35], provide an \neval operation that allows code to be generated dynamically. This approach is extremely flexible but, \nunfortunately, comes at a high price: since these languages are dynamically typed, little code generation \ncost can be pushed to compile time. Many of the language design issues involved in C also ap\u00ad pear in \ndesigning macro languages, such as Weise and Crew s work [36]. The difference is that macro languages \nallow pro\u00ad grammers to specify code templates that are compiled s~ati\u00ad cally, whereas dynamic code templates \nare compiled at run\u00ad time. Interestingly, although perhaps not surprisingly, the syn\u00ad tax we chose turned \nout to be similar to that used by Weise and Crew. Massalin et al. briefly note that they are designing \na lan\u00adguage for code synthesis, Lambda-C [28]. They do not discuss design or implementation issues other \nthan to note that type\u00adchecking of synthesized code is non-trivial. Leone and Lee [22] use programmer-supplied \nhints to per\u00adform compile-time specialization in a primitive functional lan\u00adguage: their data structures \nare not mutable, and the only heap\u00adallocated data structures are pointers and integers. They achieve \nlow code generation costs through templates. In contrast to the rudimentary control provided by hints, \nC gives the program\u00admer powerful, flexible mechanisms for the construction of dy\u00adnamically generated \ncode: it is difficult to see how the compiler in Section 5.2 could be easily or efficiently realized \nusing their system, Additionally, our language extensions and prototype implementation have been done \nin the context of ANSI C, a complex non-functional language. Several other projects address the higher-level \nissue of auto\u00admatic compiler support for detecting rtmtime constants [1, 9]. They use programmer amotations \nto indicate some rtmtime constants; the compiler computes what variables are derived rtmtime constants. \nKeppel addressed some issues relevant to retargeting dy\u00adnamic code generation in [20]. He developed a \nportables ystem for modifying instruction spaces on a variety of machines. His system dealt with the \ndifficulties presented by caches and oper\u00adating system restrictions, but it did not address how to select \nand emit actual binary instructions. Keppel, Eggers, and Henry [21] demonstrated that dynamic code generation \ncould be effective for several different applications. Many Unix systems provide utilities to dynamically \nlink object files to an executing process. Thus, a retargetable dy\u00adnamic code generation system could \nemit C code to a file, spawn a process to compile and assemble this code, and then dynamically link in \nthe result. Preliminary tests on gcc indicate that the compile and assembly phases alone require approxi\u00admately \n30,000 cycles per instruction generated; our prototype implementation of C is two orders of magnitude \nfaster. 8 Conclusions Dynamic code generation should be efficient and portable; specifying dynamically \ngenerated code should be flexible and simple. We have described C, a superset of ANSI C, that satis\u00adfies \nboth of these constraints and also preserves the semantics and spirit of ANSI C. Examples of C programs \ndemonstrate the expressiveness of the language and illustrate how dynamic code generation can be used. \nThe C prototype compiler demon\u00adstrates that C programs can achieve excellent performance (the use of \ndynamic code generation can improve performance by up to an order of magnitude), even with little optimization. \nThe reliance on static type checking reduces the cost of runtime compilation: code generation operations \nsuch as instruction se\u00adlection can be performed at compile time. Furthermore, since types are known statically, \nthe compiler can optimize dynamic code as well as static code. J3y making dynamic code generation a language \nfacility, programs that generate dynamic code are more portable, easier to write, and easier to debug. \nIn addition, C s dynamic code generation facilitiy is more flexible than in partial evaluation or other \nautomatic specialization systems. For example, C (with some support for linkage) could be used by fast \ncompilers as a portable means of emitting efficient machine code. Finally, while the language design \nhas taken place in the context of ANSI C, we expect the mechanisms used to specify dynami\u00adcally generated \ncode can also be mapped onto other statically typed languages. 9 Acknowledgments Massimiliano Poletto \nhelped design the partial evaluation me\u00adchanics; both he and Eddie Kohler were involved in the discus\u00adsion \nof lambda. Andrew Myers and Raymie Stata provided us with substantial feedback on our work. We thank \nSanjay Ghe\u00admawat, David Kranz, Kevin Lew, and Massimiliano Poletto for valuable comments. Deborah Wallach \nprovided us with our base C to C compiler. Eddie Kohler helped immensely with the typesetting of this \ndocument. References [1] J. Auslander, M. Philipose, C. Chambers, S.J. Eggers, and B .N. Bershad. Fast, \neffective dynamic compilation. Submitted for publication, October 1995. [2] R. Bernstein. Multiplication \nby integer constants. Software-Practice and Experience, 16(7):641+52, July 1986. [3] B. N. Bershad, S. \nSavage, P. Pardyak, E.G. Sirer, M. Fi\u00aduczynski, D. Becker, S. Eggers, and C. Chambers. Ex\u00adtensibility \ny, safety and performance in the SPIN operating system. In Proceedings of the Ft~teenth ACM Symposium \non Operating Systems Principles, December 1995. [4] A.D. Birrell and B.J. Nelson. Implementing remote \npro\u00adcedure calls. ACM Transactions on Computer Systems, 2(1):39 59, February 1984, [5] R.I.,. Burden \nand J.D. Faires. Numerical Methods. PWS\u00adkent Publishing Company, Boston, MA, fourth edkion, 1989. [6] \nC. Chambers and D. Ungar. Customization: Optimiz\u00ading compiler technology for SELF, a dynamically-typed \nobject-oriented programming language. In Proceedings of the SIGPLAN 89 Conference on Programming L.un \n\u00adguage Design and Implementation, pages 146-160, Port\u00adland, OR, June 1989. [7] D. D. Clark and D. L. \nTennenhouse. Architectural con\u00adsiderations for a new generation of protocols. In ACM Communication Architectures, \nProtocols, and Applica\u00adtions (SIGCOMM) 1990, September 1990. [8] C. Consel and O. Danvy. Tutorial notes \non partial evalu\u00adation. In Proceedings of the 20th Annual Symposium on Principles of Programming Languages, \npages 493 501, Charleston, SC, January 1993. [9] C. Consel and F. Noel. A general approach for run-time \nspecialization and its application to C. In Proceedings of the 23th Annual Symposium on Principles of \nProgram\u00adming Languages, St. Petersburg, FL, January 1996. [10] P. Deutsch and A.M. Schiffman. Efficient \nimplementa\u00adtion of the Smalltalk-80 system. In Proceedings O) the lIth Annual Symposium on Principles \nof Programming Languages, pages 297 302, Salt Lake City, UT, January 1984. [11] D. R. Engler. VCODE: \na very fast, retargetable, and ex\u00adtensible dynamic code generation substrate. Technical Memorandum MIT/LCSiTM534, \nMIT, July 1995. [12] D.R. Engler, M.F. Kaashoek, and J. O Toole Jr. Exok\u00ademel: an operating system architecture \nfor application\u00adspecific resource management. In Proceedings of the Fifteenth ACM Symposium on Operating \nSystems Princi\u00adples, Copper Mountain, CO, December 1995. [13] D.R. Engler and T.A. Proebsting. DCG: An \nefficient, retargetable dynamic code generation system. Sixth International Conference on Architecture \nSupport for Programming Languages and Operating Systems, pages 263 272, October 1994. [14] C.W. Fraser \nand D.R. Hanson. A code generation in\u00adterface for ANSI C. Technical Report CS-TR-270-90, Princeton University, \nDept. of Computer Science, Prince\u00adton, New Jersey, July 1990. [15] T. Granlund and P.L. Montgomery. Division \nby invari\u00adant integers using multiplication. In Proceedings of the SIGPLAN 94 Conference on Programming \nL.unguage Design and Implementation, pages 61 72, June 1994. [16] S.P. Harbison and G.L. Steele Jr. C, \nA Reference Manual. Prentice Hall, Englewood Cliffs, NJ, third edition, 1991. [17] U. Holzle and D. Ungar. \nOptimizing dynamically\u00addispatched calls with run-time type feedback. In Proceed\u00adings of the SIGPLAN 94 \nConference on Programming Language Design and Implementation, pages 326-335, Orlando, Florida, June 1994. \n[18] SPARC International. The SPARC Architecture Manaal Versaz 9. Prentice Hall, Englewvod Cliffs, New \nJersey 07632, 1992. [19] [20] [21] [22] [23] [24] [25] [26] [27] [28] [29] [30] [31] [32] [33] IN. D. \nJones, P. Sestoft, and H. Sondergaard. Mix: a self\u00adapplicable partia~ evaluator for experiments in compiler \ngeneration. LISP and Symbolic Computation, 2(1 ):9-50, 1989, D. Keppel. A portable interface for on-the-fly \ninstruction space modification. In Fourth International Conference on Architectural Support for Programming \nLanguages and Operating Systems, pages 8&#38;95, April 1991. D, Keppel, S.J. Eggers, and R.R. Henry. \nEvahrating rontlme-compiled value-specific optimizations. TR 93\u00ad 11-02, Department of Computer Science \nand Engineer\u00ading, University of Washington, November 1993. M, Leone and P. Lee. Lightweight run-time \ncode genera\u00adtion. In Proceedings of the Workshop on Partial Evalua\u00adtion and Semantics-Based Program Manipulation, \npages 97 106, Copenhagen, Denmark, June 1994. H. Massalin. Synthesis: an eficient implementation of fundamental \noperating system services. PhD thesis, Columbia University, 1992. J,C. Mogul, R.F. Rashid, and M.J. Accetta. \nThe packet fil\u00adter: An efficient mechanism for user-level network code. In Proceedings of the Eleventh \nACh4Symposium on Oper\u00adating Systems Principles, pages 39 5 1, November 1987. J.K. Ousterhout, Tcl and \nthe Tk Toolkit. Addison-Wesley Professional Computing Series. Addison-Wesley, Read\u00ading, NL4, 1994. R. \nPike, B.N, Locanthi, and J.F. Reiser. Hard\u00adware/software trade-offs for bitmap graphics on the Blit. \nSoftware-Practice and Experience, 15(2):13 1\u00ad151, February 1985. C. Pu, T. Autry, A, Black. C. Consel, \nC. Cowan, J. Inouye, L. Kethana, J. Walpole, and K. Zhang. Optimistic incre\u00admental specialization: streamlining \na commerical operat\u00ading system. In Proceedings of the Fifteenth A CM Sympo\u00adsium on Operating Systems \nPrinciples, Copper Mountain, CO, December 1995. C. Pu, H. Massalin, and J. Ioannidis. The Synthesis \nker\u00adnel. Computing Systems, l(l): 11 32, 1988. J. Rees, W. Clinger (editors), et al. Revisedi report \non the algorithmic language Scheme, AIM 848b, MIT AI Lab, November 1992. M. Rosenbhrm, S, A. Herrod, \nE. Witchel, and A. Gupta. Complete computer simulation: The SimOS approach. IEEE Parallel and Distributed \nTechnology, 1995. To appear. R. L. Sites. Alpha AXP architecture. Communicatzbw of the ACM, 36(2), February \n1993.  G.L. Steele Jr. Common Lisp. Digital Press, second edition, 1990. C. A. Thekkath and H. M. Levy. \nLimits to low-latency communication on high-speed networks. ACM Transac\u00adtions on Computer Systems, 11(2): \n179 203, May 1993.  [34] J.E. Veenstra and R.J. Fowler. MINT: a front end for efficient simulation of \nshared-memory multiprocessors. In Modeling and Simulation of Computers and Telecom\u00admunications Systems, \n1994, [35] D. Wall. The Perl Programming ilmgaage. Prentice Hall Software Series, 1994. [36] D. Weise \nand R. Crew. Programmable syntax macros. In Proceedings of the SIGPLAN 93 Conference on Pro\u00adgramming \nLungaage Design and Implementation. pages 156-165, Albuquerque, NM, June 1993. A C Grammar The grammar \nfor C consists of the grammar specified in Har\u00adbison and Steele s C reference manual [16] with the following \nadditions: unaq-expression : backquote-expression at-expression dollar-expression backquote-expression \n: unaty-expression compound-statement at-expression : @ unary-expression dollar-expression : $ unav-expression \npointer: cspec type-qudi$er-listopt vspec type -quli$er-listopt cspec type -qaalijier-listopt pointer \nvspec type -qmlt~er-listopt pointer   \n\t\t\t", "proc_id": "237721", "abstract": "", "authors": [{"name": "Dawson R. Engler", "author_profile_id": "81100222430", "affiliation": "Laboratory for Computer Science, Massachusetts Institute of Technology, Cambridge, MA", "person_id": "P64637", "email_address": "", "orcid_id": ""}, {"name": "Wilson C. Hsieh", "author_profile_id": "81100546684", "affiliation": "Department of Computer Science and Engineering, University of Washington, Box 352350, Seattle, WA and Laboratory for Computer Science, Massachusetts Institute of Technology Cambridge, MA", "person_id": "P300415", "email_address": "", "orcid_id": ""}, {"name": "M. Frans Kaashoek", "author_profile_id": "81100521650", "affiliation": "Laboratory for Computer Science, Massachusetts Institute of Technology, Cambridge, MA", "person_id": "PP14181132", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/237721.237765", "year": "1996", "article_id": "237765", "conference": "POPL", "title": "C: a language for high-level, efficient, and machine-independent dynamic code generation", "url": "http://dl.acm.org/citation.cfm?id=237765"}