{"article_publication_date": "06-10-2007", "fulltext": "\n DITTO: Automatic Incrementalization of Data Structure Invariant Checks (in Java) Ajeet Shankar UC Berkeley \n aj@cs.berkeley.edu Abstract We present DITTO, an automatic incrementalizer for dynamic, side\u00adeffect-free \ndata structure invariant checks. Incrementalization speeds up the execution of a check by reusing its \nprevious executions, check\u00ading the invariant anew only on the changed parts of the data struc\u00adture. DITTO \nexploits properties speci.c to the domain of invariant checks to automate and simplify the process without \nrestricting what mutations the program can perform. Our incrementalizer works for modern imperative languages \nsuch as Java and C#. It can incremen\u00adtalize, for example, veri.cation of red-black tree properties and \nthe consistency of the hash code in a hash table bucket. Our source-to\u00adsource implementation for Java \nis automatic, portable, and ef.cient. DITTO provides speedups on data structures with as few as 100 el\u00adements; \non larger data structures, its speedups are characteristic of non-automatic incrementalizers: roughly \n5-fold at 5,000 elements, and growing linearly with data structure size. Categories and Subject Descriptors \nD.m [Miscellaneous] General Terms Algorithms, Languages, Performance Keywords Automatic, dynamic optimization, \nincrementalization, program analysis, data structure invariants, optimistic memoization  1. Introduction \nType safety of modern imperative languages such as Java and C# eliminates many types of programming errors, \nsuch as buffer over\u00ad.ows and doubly-freed memory. As a result, algorithmic errors present a proportionately \ngreater challenge during the development cycle. One such class of errors are data structure bugs. Many \ndata structures bugs can be detected as violations of high-level invariants such as the elements of this \nlist are ordered , no elements in this priority queue can be in that priority queue , or in a red-black \ntree, the number of black nodes on any path from the root node to a leaf is the same. Verifying such \ninvariants, however, remains non-trivial. Data structure invariants are particularly dif.cult for static \ntools to verify because static heap analysis scales poorly and current veri.ers require extensive annotations. \nAn alternative approach is dynamic veri.cation of invariant checks. Dynamic checks operate on the concrete \ndata structure and are thus typically simple to write and validate. Thanks to tools such Permission to \nmake digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior speci.c permission and/or a fee. PLDI 07 June 11 \n13, 2007, San Diego, California, USA. Copyright c . 2007 ACM 978-1-59593-633-2/07/0006. . . $5.00 Rastislav \nBod\u00b4ik UC Berkeley bodik@cs.berkeley.edu as jmlc [6], dynamic checking has become more accessible to \npro\u00adgrammers. However, dynamic checks can incur a signi.cant run time overhead, hindering the development \nand testing. Since checks are executed frequently and commonly traverse the entire data structure, a \nprogram with checks may run 10 100 times slower, which may be prohibitively slow for all but the most \npatient programmer. Conse\u00adquently, dynamic checks are rarely employed, even in debugging. This paper \nintroduces DITTO, an incrementalizer for a class of dynamic-data structure invariant checks written in \nmodern impera\u00adtive languages like Java and C#. We allow the programmer to write these checks in the language \nitself. DITTO then automatically incre\u00admentalizes such checks, rewriting them so that they only re-check \nthe parts of a data structure that have been modi.ed since the last check. Incremental checks typically \nrun linearly faster than the orig\u00adinal (about 10-times faster on data structures with 10,000 elements). \nWe believe that the incrementalization makes dynamic checks prac\u00adtical in a development environment. \nThe goal of incrementalization is to modify an algorithm so that it computes anew only on changed input \ndata and reuses all repeated subcomputations. Traditionally, incrementalization is designed and implemented \nby hand: an algorithm is modi.ed to be aware of data modi.cations and to cache and reuse its previous \nintermediate re\u00adsults [20]. While hand-incrementalization can produce the desired speedups of invariant \nchecks, manual incrementalization has several practical limitations: The programmer may overlook possible \nmodi.cations to the data structure (as in the infamous Java 1.1 getSigners bug [11]) and thus omit necessary \nincremental updates. The result is an incorrect invariant check that may fail to detect bugs.  Some \ninvariant checks may be dif.cult to incrementalize by hand. For example, after some effort, we gave up \non incrementalizing red-black tree invariants.  Manual incrementalization does not appear economical, \nas each data structure may require several checks. Programmers may also want to obtain an ef.cient check \nrapidly, for example, when writing data-breakpoint checks for explaining the symptoms of a particular \nbug.  Perhaps most importantly, incremental code is complex and scat\u00adtered throughout the program. The \ncomplexity of its maintenance may defeat the purpose of relying on invariant checks that are simple and \nveri.able by inspection.  Recent research by Acar et al. [1] developed a powerful general\u00adpurpose framework \nfor incrementalization of functional programs, based on memoization and change propagation. This framework \npro\u00advides an ef.cient incrementalization mechanism while offering the programmer considerable .exibility. \nTo incrementalize a program, the programmer (i) identi.es locations whose changes should trigger recomputation; \nand (ii) writes functions that carry out the incremen\u00adtal update on these locations. The actual memoization \nand recom\u00adputation are encapsulated in a library. Acar s incrementalized algo\u00adrithms exhibit signi.cant \nspeedup, so it is natural to ask how one could automate this style of incrementalization. In this paper, \nwe identify an interesting domain of computations for which we develop an automatic incrementalizer. \nOur domain in\u00adcludes recursive side-effect-free functions, which cover many invari\u00adants of common data \nstructures such as red-black trees, ordered lists, and hash tables. While we support only functional \nchecks, the checks can be executed from within arbitrary programs written in imperative languages such \nas Java and C#. In these languages, checks are useful to the programmer because manual veri.cation of \ninvariants is com\u00adplicated by the fact that data structure updates can occur anywhere in the program. \nFor the same reason, incrementalization is dif.cult, which should make automatic incrementalization attractive. \nProperties of invariant checks allow us not only to automate incre\u00admentalization but also to offer a \nsimple and effective implementation. Simplicity. An invariant check typically returns always the same \nresult (i.e., the check passed ) and so do its subcomputations that are recursively invoked on parts \nof the data structure. This observation allows us to develop optimistic memoization, a tech\u00adnique that \naggressively enables local recomputations to recon\u00adstruct a global result.  Effectiveness. The local \nproperties that establish the global prop\u00aderty of interest are typically mutually independent and recompu\u00adtation \nof one does not necessitate recomputation of others. For example, sortedness of a list is established \nfrom checking that adjacent elements are ordered; if an element is inserted into the list, we need to \ncheck its order only with respect to its neighbors. Independence of local computations means that incremental \ncom\u00adputation can produce signi.cant speedups.  The main contributions of this paper are: 1. The DITTO \nautomatic incrementalizer for a class of data structure invariant checks that are written in an object-oriented \nlanguage. 2. A portable implementation of DITTO in Java. 3. An evaluation of Java DITTO on several \nbenchmarks.  Section 2 outlines how a simple invariant check is incrementalized. Section 3 describes \nDITTO s incrementalization algorithms and Sec\u00adtion 4 provides some implementation details. Section 5 \nevaluates DITTO on several small and large benchmarks. Section 6 discusses related work and Section 7 \nconcludes.  2. De.nitions and Example In this section, we give a high-level overview of DITTO s incremen\u00adtalization \nprocess. First, we de.ne the class of invariant checks that DITTO can incrementalize. DEFINITION 1. The \ninputs to a function consist of its explicit ar\u00adguments, i.e., the values of its actual parameters; its \nimplicit argu\u00adments, i.e., values accessed on the heap; and its callee return values, i.e., the results \nof function calls it makes. Note that implicit arguments are de.ned not to include locations that are \nread (only) by the callees of the function. DEFINITION 2. A data structure invariant check is a set of \n(poten\u00adtially recursive) functions that are side-effect-free in the sense that they do not write to the \nheap, make system calls, or escape address of an object allocated in the invariant check. Furthermore, \nin each func\u00adtion, no loop conditional or function call can depend on any callee return values.1 1 This \ntechnical restriction, described further in Section 3.5, is required to ensure that the original functions \nand their incrementalized versions have the class OrderedIntList { IntListElem head; void insert(int \nn) { invariants(); ... invariants(); } void delete(int n) { invariants(); ... invariants(); } void \ninvariants() { if (! isOrdered(head)) complain(); } Boolean isOrdered(IntListElem e) { if (e == null \n|| e.next == null) return true; if (e.value > e.next.value) return false; return isOrdered(e.next); } \n} Figure 1. The example class OrderedIntList and its invariant check isOrdered. Throughout this paper, \nwe will often assume that a check is a single recursive function. However, DITTO supports also checks \ncomposed of multiple recursive functions, such as the one in Figure 9. When the check contains multiple \nfunctions, we identify the check by the entry-point function that is invoked by the main program. The \nincrementalizer memoizes the computation at the level of function invocations, so recursive checks are \nmore ef.cient than it\u00aderative ones. Most iterative invariant checks can be rewritten without loss of \nclarity into recursive checks. The main program has no restrictions on its behavior. We assume that invariant \nchecks running in multithreaded programs either oper\u00adate on thread-local data or are atomic to ensure \ndata integrity during the check. Toillustratehow DITTO works,wewalkthroughtheincremental\u00adization of a \nsimple invariant check, isOrdered, shown in Figure 1. The invariant veri.es that the list maintains its \nelements in sorted or\u00adder. The invariant is checked at method entries and exits. The former ensures that \nthe invariant is maintained by modi.cations performed from outside the class. Such modi.cations could \noccur if, say, an IntListElem object was mistakenly exposed to users of the class. The latter ensures \nthat the list operation itself maintains the invariant. The invariant check is simple and readable, but \nit is inef.cient. In common usage scenarios, the unoptimized isOrdered will dominate the performance \nof the program. However, the check is amenable to incrementalization under most common modi.cations to \nthe list. For instance, if an element e is inserted into the middle of the list, isOrdered needs to be \nre-executed only on e and its predecessor; the success of the invocation of isOrdered preceding the change \nguarantees the checked property for the remaining elements in the list, as they have not changed since \nthen. This incrementalization reduces the cost of the check from the time linear in the size of the list \nto constant time. Incrementalizing isOrdered. DITTO automatically incremen\u00adtalizes isOrdered using the \nfollowing simple process. same termination properties in the presence of optimistic memoization. This \nrestriction can be sidestepped but we have not found it to be an impediment in practice.      Figure \n2. Before and after a list operation: an element is inserted, and another is deleted. The elements modi.ed \nduring the operation are dashed. 1. During the .rst execution of invariants, we record the se\u00adquence \nof recursive calls to isOrdered, their inputs, and their results.  2. During the subsequent execution \nof the main program, we track changes to memory locations that served as implicit inputs to a check. \nThe tracking is performed with write barriers.  3. The next time invariants is invoked, we re-execute \nonly the recursive invocations to isOrdered whose inputs have changed; we reuse memoized results for \nthe remaining invocations. We update the memoized results so that further executions of the check can \nbe incrementalized.  We describe these steps in detail in the context of a modi.cation scenario, shown \nin Figure 2, where an element is inserted into the list and another element, further down the list, is \ndeleted. Note that we assume that the invariant check is performed only after both modi.cations, and \nnot in between the two modi.cations. DITTO stores all inputs foreach (recursive) invocation of isOrdered. \nThe function isOrdered has .ve inputs: one explicit argument, the formal parameter e; three implicit \narguments, the .elds e.next, e.value,and e.next.value; and one callee return value, that of the recursive \ncall to isOrdered. The modi.cation shown in Figure 2 updates two .elds already in the list: A.next and \nD.next. Based on the inputs stored in the previous execution of the check, DITTO determines that these \n.elds served as implicit inputs to invocations isOrdered(A) and isOrdered(D). These two invocations must \nbe re-executed on the new input values. Since isOrdered(A) occurred .rst in the previous execution, it \nis re-executed .rst. The re-running of isOrdered(A) uses the new implicit argu\u00adments, speci.cally the \nnew value of A.next, which now points to B. The execution thus continues to the invocation isOrdered(B). \nSince DITTO has not yet encountered isOrdered with the explicit argument B, it adds this new invocation \nto its memoization table and continues executing, reaching the recursive call to isOrdered(C). At this \npoint, DITTO determines that (i) isOrdered(C) has been memoized and (ii) the implicit arguments to isOrdered(C) \nhave not changed since the previous execution of the check. However, this is not suf.cient to safely \nreuse isOrdered(C) because there is no guarantee that the last input to isOrdered(C) the callee return \nvalue from isOrdered(C.next) will return the same value as in the previous execution of the check. The \ndanger is quite real: There is a modi.cation to an implicit input of isOrdered(D) further down the list; \nif isOrdered(D) returned a different value, this value could ultimately affect the value returned by \nisOrdered(C.next). So, a straightforward memoization algorithm cannot safely reuse isOrdered(C); instead, \nit must continue the re-execution until it is sure that no further callee return values might change. \nIn our example, it would have to re-execute past C all the way to D,re\u00adexecuting also all the intervening \nfunction invocations. DITTO follows a more sophisticated algorithm. To deal with the uncertainty of callee \nreturn values, DITTO optimistically assumes that the recursive call to isOrdered(C) will return the same \nvalue as it did previously. This is a sensible assumption since recursive invariant check often do return \nthe same value. (Typically, this is the success value.) This optimistic memoization strategy allows DITTO \nto reuse the cached result for isOrdered(C), which successfully terminates the re-execution. The execution \neventually returns back up to isOrdered(A), which returns true the same value it returned last time. \nThus, the function that invoked isOrdered(A) need not be re-executed since all of its inputs are the \nsame as last time; we are now done with the re-execution of the modi.ed portion of the data structure \naround nodes A and B. DITTO then re-executes isOrdered(D), the second call whose implicit inputs have \nchanged. The incrementalization then continues to isOrdered(F), which is successfully reused, terminating \nthe recursive calls. The invocation of isOrdered(D) evaluates to true, matching its previous result, \nso the entire recomputation ends. The invocation isOrdered(E) is no longer reachable in the computation \nand is ignored. DITTO now returns the cached result of the entire invariant check, true, to the caller, \ninvariants(). Consider now the case when isOrdered(D) returns a value dif\u00adferent than it did previously. \n(Note that our optimistic assumption is not necessarily wrong yet, as we assumed only that isOrdered(C), \nbut not isOrdered(D), returns the same value as it did previously.) The new return value would be propagated \nfrom isOrdered(D) back up to its caller, which would be re-executed. This process would continue until \neither (a) a caller is reached that returns the same result that it did previously; or (b) the execution \nreaches the .rst caller, isOrdered(head), and the new overall result is cached and returned. Note that \nif this upward propagation reaches isOrdered(B), the optimistic memoization decision made when reusing \nisOrdered(C) is shown to be incorrect. In this case, isOrdered(B) is re-executed like the other calls \nduring this propa\u00adgation phase. Whether the optimistic assumptions turned out wrong or not, the incrementalizer \nstores the new inputs and the result for each re\u00adexecuted call; the memoization data for isOrdered(E) \nis garbage collected. This maintenance ensures that DITTO will be able to in\u00adcrementalize the invariant \ncheck during its next execution. Of course, data structure modi.cations can take on more complex forms \nthan simple inserts and deletes. The next section describes how all possible modi.cations are handled \nin a general way, and Section 5 examines the performance of DITTO on invariants of considerably greater \ncomplexity.  3. Incrementalization Algorithm This section presents details of our incrementalization \nalgorithm. We start by describing the memoization cache and continue with a straightforward incrementalization \nalgorithm. The inef.ciency of this algorithm will motivate our optimistic incrementalizer, presented \nnext. We will conclude by explaining the steps taken when optimistic assumptions fail. 3.1 Computation \ngraph On the .rst invocation of an invariant check, DITTO caches the com\u00adputation of the check in a computation \ngraph, which records the com\u00adputation at the granularity of function invocations. Between invoca\u00adtions \nof the invariant check, the graph is used to track how the main program changes the check s implicit \narguments. On the subsequent invocation of the invariant check, the graph is used to identify mem\u00adoized \nfunction invocations whose inputs have been changed. These function invocations are re-executed and the \ngraph is updated; the remaining function invocations are reused from the graph. The in\u00adcrementally updated \ngraph is equivalent to re-running the invariant check from scratch on the current program state. The \ncomputation graph contains a node for each (dynamic) func\u00adtion invocation performed during the execution \nof the check. Di\u00adrected edges connect a caller with its callees. DITTO stores the graph in memory in \nthe form of a table. A table entry, shown below, rep\u00adresents one node of the graph, i.e., one function \ninvocation. We will use the terms function invocation and computation node (or node) interchangeably \nas appropriate. f explicit args implicit args calls return val dirty The entry contains six .elds: f \nis the invoked function; ex\u00adplicit args is a list of values passed as actual arguments to f; im\u00adplicit \nargs is a list static and heap locations read by the invocation; calls is a list of function invocations \nmade by this function invoca\u00adtion, represented as links to other entries in the table; return val is \nthe return value of this invocation; and dirty is used during the incre\u00admental computation to mark invocations \nwhose implicit inputs have been modi.ed. Recall that implicit args includes only the locations read by \nthis invocation, not by its callees. The table is indexed by a pair (f, explicit args). DITTO constructs \nthe computation graph by instrumenting the invariant check. The of.ine instrumentation diverts all invocations \nof an invariant check c i.e., all calls to a functions in c from a function not in c to the catch-all \nincrementalize runtime library function, described in detail later in this section (see Figure ??). For \ninstance, the call to isOrdered(head) in invariants() in Figure 1 is rewritten to invoke incrementalize() \ninstead. DITTO instruments each function f in the invariant check c to record the data necessary to construct \na memoization table entry. The instrumented version of the isOrdered function in Figure 1 is shown in \nFigure 3. The transformation inserts code at the beginning of f to check if this invocation has been \nmemoized. If a table entry with the same explicit arguments already exists, the function returns with \nthe cached result value; if not, a new entry is created and implicit arguments and the return value are \nrecorded. The try and catch are required by optimistic memoization; their purpose is described later \nin this section. In addition to recording the implicit arguments used by each function invocation, a \nreverse map, from heap locations (implicit arguments) to table entries, is created. This reverse map \nis used to determine which function invocations depend on modi.ed heap values. See Figure 4 for an example \ninitial computation graph. Instrumentation is also used to track updates to implicit inputs. These updates \ncan occur anywhere in the main program, so DITTO places write barriers into statements that might write \nthose locations. The write barriers are described in further detail in Section 4. When an update to an \nimplicit location is detected, function invocations whose implicit arguments have been modi.ed are marked \nas dirty, which prevents reuse of their memoized results (see Figure 5). 3.2 Naive incrementalizer DITTO \ncan reuse the cached result of a function invocation if the function is invoked with identical inputs \nas the cached invocation. However, checking whether all inputs are identical is non-trivial. Re\u00adcall \nthat, for the purpose of memoization, a function has three kinds of inputs: explicit inputs (i.e., actual \narguments); implicit inputs (i.e., values read by the function from the heap and static variables); and \nreturn values from its callees. Ideally, we want to reuse the cached result at the time when the function \nis invoked; but at this point we know only that the explicit arguments are identical. We may also know \nthat the values of implicit input locations have not changed since the last invocation, but is the function \ngoing to read the same set of locations? The answer depends on the return values from the function s \ncallees; if they differ from the previous return values, the function may read different locations and \nclearly cannot be reused. Boolean isOrdered(IntListElem e) { try {// creates a new entry if one doesn \nt exist MemoEntry n = getMemoEntry(isOrderedId, [e]); if (n.hasResult) return (Boolean) n.result; n.addImplicit(addressOf(e.next)); \nif (e == null || e.next == null) { n.setResult(true); return true; } n.addImplicit(addressOf(e.value)); \nn.addImplicit(addressOf(e.next.value)); if (e.value > e.next.value) { n.setResult(false); return false; \n} n.addCall(isOrderedId, e.next); n.setResult(isOrdered(e.next)); return n.result; } catch (Exception \ne) {throw new OptimisticMemoizationException(); } } Figure 3. The instrumented version of isOrdered(). \n Figure 4. Part of the computation graph after an initial run. The dotted lines from items on the heap \nto computation nodes (function invocations) indicate the implicit arguments used by those nodes. Not \nall dotted lines are shown. Figure 5. Memory locations with dashed outlines have been modi\u00ad.ed since \nthe last execution of the invariant. All computation nodes that used these memory locations are marked \nas dirty. function incrementalize(f, initial_args) return memo(f, initial_args) function memo(f, x) if \n(t[f,x] == null || // never been run before t[f,x].hasModifiedImplicitArgs()) return exec(f, x) foreach \n(c in t[f,x].calls) // did the call return the same value as last time? old_return_val = c.return_val \nif (memo(c.f, c.explicit_args) != old_return_val) // memo lookup failed somewhere in c.f s call tree \nreturn exec(f, x) // conditions described in Lemma 1 hold; reuse allowed return t[f,x].return_val function \nexec(f, x) // invoke f , the instrumented version of f return f (x) Figure 6. The naive incrementalizer. \nA conservative rule for reuse of memoized results is to ensure that (i) the explicit arguments are identical \nand that there has been no change to (ii) implicit input values as well as to (iii) callee return values. \n(To con.rm that the return values are identical, the naive incrementalizer will incrementally execute \nthe calls, meaning that it will try to reuse as much of the calls as possible.) LEMMA 1. Consider an \ninvocation of function f that (i) has explicit arguments e, (ii) accesses the set of heap locations I, \nand (iii) invokes functions g1(a1), ..., gn(an), which return values r1, ..., rn.The cached result for \nthis invocation is identical to the value of f(x) invoked in the current program state if the following \nconditions hold: (1) x = e; (2) the locations in I have not been modi.ed since the memoized invocation \nwas executed; (3) g1(a1), ..., gn(an),if invoked on the current program state, would return the same \nvalues r1, ..., rn as at the time of the previous invocation of f(e). The proof involves showing that \nthe current function invocation (1) accesses the same set of locations as the cached invocation; and \n (2) makes identical function invocations as the cached invocation. It is easy to show that if the previous \nimplicit locations have not been changed, the .rst call made by the function is identical to the .rst \ncall made by the cached invocation. If this call returns the same value as previously, the function will \ncontinue accessing the same implicit input locations. Since their values have not been changed, the second \ncall made by the function will be identical to the second call in the cached invocation. The proof then \nproceeds by induction.  The naive incrementalization algorithm is shown in Figure 6. In this code, initial \nargs are the arguments provided to the .rst, entry-point function call of the invariant check. t[f,x] \nrepresents a lookup in the memoization table of an entry with function f and explicit arguments x. The \nnaive incrementalizer is simple: starting from the .rst func\u00adtion invocation of the invariant check, \nit recursively follows the path of the computation, reusing memoized results where appropriate. However, \nit is very costly: in order to ascertain that child calls do in fact return the same values as in the \nprevious execution, it requires a memoization table lookup for every function invocation in the com\u00adputation, \neven those that are unaffected by any input modi.cations.  3.3 Optimistic incrementalizer Ideally, the \nincrementalizer should recompute only function invo\u00adcations whose inputs have changed. But how do we \ndetermine that calls made by f would return the same values if executed in the cur\u00adrent program state? \nThe naive incrementalizer does so by replaying the sequence of all calls indirectly made by f and ensuring \nthat all these transitive callees of f can be memoized. This process is ex\u00adpensive. A constant-time memoization \ncheck can be performed by time-stamping the invocation of each function and checking if any transitive \ncallee of f had its implicit arguments modi.ed. Such a time interval mechanism was used by by Acar et \nal. [1] to aid in identifying relevant functions with changed inputs. DITTO develops what we think is \na simpler mechanism based on the common property that invariant checks usually succeed. When a check \nsucceeds, it returns a success code; the same is true for all recursive function invocations made by \nthe check. Our optimistic as\u00adsumption thus is that a function invocation in an invariant check typi\u00adcally \nreturns the same value. This observation holds even when some of the transitive callees had their implicit \ninputs changed because in\u00advariants usually hold even after the data structure is modi.ed. The optimistic \nmemoization employed by DITTO simpli.es the naive incrementalizer: we optimistically reuse a cached invocation \nif the explicit and implicit arguments are the same; the callee return values are assumed to return the \nsame values. The optimistic memo() function is shown here: function optimistic_memo(f, x) if (t[f,x] \n== null || // never been run before t[f,x].hasModifiedImplicitArgs()) return exec(f, x) // optimistically \nassume that conditions // described in Lemma 1 hold and allow reuse return t[f,x].return_val Optimistic \nmemoization breaks dependencies of an invocation on its callees, whose return values are not yet known \nat the time when reuse of the invocation is attempted. This frees DITTO fromhavingtoper\u00adform the memoization \nlookup on many function invocations whose inputs have not changed. In other words, the bene.t of optimistic \nmemoization is that, in the common case of a successful check, we recompute only the local properties \nof those data structure nodes that have changed. DITTO must of course handle the case of an incorrectly \npredicted optimistic value. The steps for doing so are detailed in Section 3.5. 3.4 The complete algorithm \nThe complete incrementalization algorithm, shown in Figure ??, needs to take care of two more issues: \npruning of unreachable com\u00adputations and recomputation in response to changed return values. Pruning. \nIf a computation of a check has two function invoca\u00adtions with modi.ed inputs, f(x) and g(y),and g(y) \nis a transitive callee of f(x) then we should recompute f(x) before g(y). The rea\u00adson is that the new \ncomputation of f(x) may or may not lead to an invocation of g(y). Invoking g(y) could result in an exception \nor it could be costly (for example, node y could have moved to a different data structure on which the \nevaluation of the invariant check could be expensive). Thus, DITTO re-executes dirty nodes (i.e., nodes \nwith modi.ed implicit inputs) in a breadth-.rst search order (i.e., nodes closest to the root are executed \n.rst). After each node is re-executed, the incrementalizer prunes nodes that are no longer in the computa\u00adtion \ngraph; these nodes will not be re-executed. Changed return values. When a re-execution of an invocation \nevaluates to a return value that differs from the cached return value, the changed value must be propagated \nto the caller of the recomputed invocation. DITTO tracks all nodes with differing return values and re-executes \ntheir callers in reverse breadth-.rst-search order, which ensures that a node is re-executed only after \nall its children have been re-executed (if that was necessary). This re-execution along a path continues \nup the graph until either (i) the return value of a re\u00ad function incrementalize(f, initial_args) to_propagate \n= {} // identify memoized executions that have modified // implicit arguments (detected by write barriers) \nchanged = get_changed_implicit_locations() changed_fns = map_locs_to_memo_table_entries(changed) // need \nto re-run root if arguments have changed if (t[f,initial_args] == null) changed_fns.add((f, initial_args)) \nchanged_fns.sort_bfs_order() foreach((f,x) in changed_fns) t[f,x].dirty = true foreach ((f,x) in changed_fns) \n// only re-execute if still in graph (not pruned) // and dirty (hasn t already been re-executed) if (t[f,x] \n!= null &#38;&#38; t[f,x].dirty) exec(f, x) propagate_return_vals() return t[f,initial_args].return_val \n function memo(f, x) if (t[f,x] == null || // never been run before t[f,x].dirty) // changed implicit_args \nreturn exec(f, x) // thanks to optimistic memoization, don t // need to check callee return values return \nt[f,x].return_val function get_callers(f, x) // returns nodes that call f(x) function exec(f, x) oldentry \n= t[f,x] // f is the instrumented version of f newresult = f (x) if (newresult != oldentry.return_val) \nto_propagate.add((f,x)) foreach (c in oldentry.calls) if (get_callers(c.f, c.explicit_args).size() == \n0) prune(c.f, c.explicit_args) return newresult function propagate_return_vals() to_propagate.sort_reverse_bfs_order() \nwhile (to_propagate.size() > 0) e = to_propagate.remove(0) f, x, oldval = e.f, e.explicit_args. e.return_val \nnewval = f (x) if (oldval != newval) to_propagate.insert_reverse_bfs_order( get_callers(f,x)) function \nprune(f, x) var calls = t[f,x].calls t[f,x] = null foreach(c in calls) if (get_callers(c.f, c.explicit_args).size() \n== 0) prune(c.f, c.explicit_args) Figure 7. Pseudo-code for the main incrementalizing algorithm. executed \nancestor evaluates to the cached value; or (ii) the root node is reached, which changes the overall result \nof the invariant check. The DITTO incrementalizer is shown in Figure ??. In the imple\u00admentation, the \ngraph is not traversed using BFS; instead, the nodes are kept ordered using the order maintenance algorithm \ndue to Ben\u00adder, et al. [5]. An example of the algorithm in action (with pruning, optimistic memoization, \nand return value propagation) is shown in Figure 8. 3.5 Optimistic mispredictions Recall that when the \noptimistic incrementalizer encounters a call to (a non-dirty) invocation g(y), the incrementalizer reuses \nits old re\u00adturn value without .rst ensuring that the g(y) would return the same value in the current \nprogram state. When this optimistic assumption is wrong, the re-execution of f(x), the caller of g(y), \nmay go wrong in one of three ways: The invocation f(x) .nishes evaluation but yields an incorrect result. \nIn this scenario, no remedial action is needed. The correct return value will reach f(x) during the propagation \ndescribed in Section 3.4 and f(x) will thus be re-executed with the correct return value and will produce \nthe correct result. The incorrect return value causes f(x) to throw an exception. For example, g(y) may \nreturn an object that is no longer in the data structure and may thus have invalid .eld values. When \nf(x) reads those values, it may throw a divide-by-zero error or a null\u00adpointer deference. Since this \nexception would not be raised in the non-incremental check, it must be prevented from reaching the main \nprogram. The code transformation described in Section 3.1 encloses the entire function in a try-catch \nblock. If an exception is thrown due to a wrong optimistic assumption, the exception is caught and the \nexecution of the function is stopped. The function will eventually be re-executed with correct inputs, \nas in the previous scenario. If an exception still occurs at this stage, the exception is forwarded on \nto the main program. The incorrect value causes non-termination. Similar to the previ\u00adous case, an incorrect \nreturn value may cause a loop or a recursion to iterate forever. The return value did not cause non-termination \nwhen f(x) was executed previously because the explicit or implicit inputs were different. We offer two \nalternative remedial actions. The .rst one, currently used by DITTO, imposes a restriction on the way \nDITTO-incrementalizable invariant checks must be written: No loop conditional or function call can depend \non a callee return value. Here, depends includes both control-and data-dependence. Under this restriction, \neach loop and each call in the re-executed f(x) uses only (correct) values from the current state, and \nthus will not cause a spurious non-termination. Our practical experience is that this restriction is \nmore of a tech\u00adnicality than a real burden. We have yet to write a loop of any sort inside an invariant \ncheck function, and we have found it easy to over\u00adcome the function call restriction by avoiding short-circuit \nboolean evaluation. To ensure that programmers are unable to violate this restriction, we have written \na simple static analysis that checks for such a viola\u00adtion. The analysis is fairly trivial because aliasing \nis impossible in a side-effect-free function. The second solution for this situation is to implement \na timeout that would trigger when an optimistic execution takes far longer than it has taken historically. \nIn this case, the invariant check would be re-executed from scratch. A bene.t of this approach is that \nno programming restrictions are made on the function, though a cost is that its behavior may be unpredictable. \n  4. Implementation DITTO isimplemented as a Java bytecode transformation and accom\u00adpanying runtime \nlibraries. This approach does not allow for an opti\u00admized runtime implementation. For instance, the write \nbarriers are implemented in Java, which requires two null checks and one array bounds check per barrier; \nan ef.cient JVM implementation would require far less overhead, as the barriers could be inserted at \na lower level, circumventing these Java safety checks. However, the byte\u00adcode transformation approach \noffers the strong advantage of being as portable as Java is. It can be used with any JVM on any platform. \n   T / T new  T / F new F / T cached   (a) Re-execution and pruning (b) Re-execution and pruning \n(c) Changed return values Figure 8. Re-execution after modi.cation to the data structure shown in Figure \n5. (a) The .rst dirty node, R, is re-executed. The re-execution in the node execution encounters (i) \na new node, which is added to the graph, and (ii) a non-dirty node with a valid memoized value, which \nstops recursion early thanks to optimistic memoization. The dirty node P is pruned from the graph and \nwill not be re-executed. (b) The second dirty node is re-executed. A new node is added and the non-dirty \nnode marked P and its children are pruned. Though not shown in the .gure, memoization table entries are \nadded or modi.ed for the functions invoked in this step. The resulting computation graph re.ects the \nchanges made to the heap in Figure 5. (c) The results of re-executed nodes are compared with their old \ncached values. If they differ, the new results are propagated up through the graph. In this example, \nlet the invariant check be a test for the presence of a special object S. Assume that S has moved from \nthe left branch of the tree to the right; as a result, some node results differ ( F/T indicates an old \nresult of false and a new result of true), and are propagated up the graph. However, the propagation \nstops soon because an ancestor node s new result matches its old one. The implementation of DITTO supports \nmultiple invariants per class instantiation, multiple class instantiations per class, and mul\u00adtiple classes. \nBelow are speci.cs about some aspects of the imple\u00admentation. The bytecode transformation is implemented \nusing the excellent Javassist package [7]. Hashing of objects. In previous work on incrementalization \n[1], the de.nitions of object equality are left to the programmer. This .exibility allows the programmer \nto equate two objects if they differ only in .elds that she knows are irrelevant to the incremental compu\u00adtation. \nSince DITTO is automatic, an all-purpose strategy is required. DITTO s memoization table, which maps \na list of explicit argu\u00adments, stored in an Object[], to a particular entry that represents a function \ncall on those arguments, is implemented as a hash ta\u00adble. This requires a notion of argument array equality \nand hash\u00ading. In terms of equality, pointer equality of Object[] is obvi\u00adously insuf.cient. Instead, \nequality is de.ned as the conjunction of pointer equality for the elements (arguments) that are object \nref\u00aderences, and semantic equality for the elements that are primitive types; the hash code is de.ned \nanalogously, as a combination of System.identityHashCode(),or Object.hashCode() for prim\u00aditive types \nlike Integer or Boolean. This strategy conservatively preserves semantic equality of all arguments, while \npreventing shar\u00ading of non-primitive types (if the same computation node were to operate on two objects, \nsemantically equal but in different locations on the heap, and only one was updated, then the node s \ncached re\u00adsult could be incorrect for one set of arguments.) In theory, semantic equality and hashing \ncould be applied to any immutable type. Our benchmarks indicate that this conservative notion of equality, \nthough not optimally .exible, performs well in practice on DITTO s target domain. Ef.cient implementation \nof write barriers. Since the write bar\u00adriers are implemented in Java, some care must be taken to ensure \nrea\u00adsonable performance. DITTO employs two main optimization tactics. First, during the of.ine bytecode \ntransformation phase, DITTO gath\u00aders the set of .elds accessed by the invariant checks it is optimizing. \nWrite barriers are only inserted on updates to these .elds, since only writes to these .elds could possibly \naffect the implicit arguments to the invariant checks. Secondly, each memory address caught by the barriers \nincurs a hash table lookup to determine what computation nodes are affected by its mutation, even if \nthe object at that address is unrelated to any invariant checks and affects no computation nodes at all. \nIf there are many such other writes (or if the .rst optimization did not suf.ciently reduce the number \nof barriers inserted), these lookups can cause signi.cant overhead. To combat this phenomenon, the runtime \nportion of DITTO keeps a reference count of dependent invariant checks in the header of each object. \nThe write barriers are constructed to .rst check that the reference count is greater than zero, and only \nthen to add its .eld to the list of mutated ones. The reference count for a particular object is decremented \nwhen an invariant s hash table lookup is done and the dirty nodes identi.ed. This way, if any of its \ndirty nodes accesses the object again, its reference count will be incremented. If not, since the dirty \nnodes are the only ones that accessed it beforehand, it is no longer relevant to that invariant check \nand does not need to be monitored further. In practice, the inclusion of a header reference count is \nim\u00adplemented by creating a new class IncObject that inherits from java.lang.Object, and contains an integer \n.eld corresponding to the reference count. DITTO then sets the penultimate class in the class hierarchy \nof each object type used by invariant checks to in\u00adherit from this class instead of java.lang.Object. \nOptimizing leaf calls. If a function f is invoked with arguments a that do not lead to recursion, it \nis often faster to compute f(a) outright than to memoize it. This situation commonly occurs at the ends \nof data structures, when a .nal null value is reached. Thus, if all the non-primitive arguments to a \nfunction call are null, DITTO does not perform any cache lookups and instead runs f(a) to determine the \nreturn value. In addition, small commonly used non\u00adrecursive functions, such as hashCode() and size(), \nare special\u00adcased as well. In all cases, the implicit arguments to these functions, if any, are still \nrecorded.  5. Evaluation All measurements were performed on a Pentium M 1.6 GHz com\u00adputer with 1 gigabyte \nof RAM, running the HotSpot 1.5 JVM. 5.1 Data structure benchmarks Wemeasured DITTO on several datastructure \nbenchmarks. Eachdata structure is instantiated at several sizes and then modi.ed 10,000 times. We measured \nonly small sizes (from 50 to 3,200) to re.ect what we believe is common real-world usage. (Incrementalization \nBoolean checkHashBuckets(int i) { if (i >= buckets.length) return true; boolean b1 = checkHashElements(buckets[i], \ni), b2 = checkHashBuckets(i+1); return b1 &#38;&#38; b2; } Boolean checkHashElements(HashElement e, int \ni) { if (e == null) return true; return (e.key.hashCode() % buckets.length == i) &#38;&#38; checkHashElements(e.next, \ni); } Figure 9. Invariant for the hash table. The invariant is invoked as checkHashBuckets(0). void invariants() \n{ if (!isRedBlack(root) || checkBlackDepth(root) == -1 || ! isOrdered(root, Integer.MIN_VALUE, Integer.MAX_VALUE)) \ncomplain(); } Boolean isOrdered(Node n, int lower, int upper) { if (n == nil) return true; if (n.key \n<= lower || n.key >= upper) return false; if (n.key <= n.left.key || n.key >= n.right.key) return false; \nboolean b1 = isOrdered(e.left, lower, n.key), b2 = isOrdered(e.right, n.key, upper); return b1 &#38;&#38; \nb2; } Boolean isRedBlack(Node n) { if (n == nil) return true; Node l = n.left, r = n.right; if (n.color \n!= BLACK &#38;&#38; n.color != RED) return false; if ((l != nil &#38;&#38; l.parent != n) || (r != nil \n&#38;&#38; r.parent != n)) return false; if (n.color == RED &#38;&#38; (l.color != BLACK || r.color != \nBLACK)) return false; boolean b1 = isRedBlack(l), b2 = isRedBlack(r); return b1 &#38;&#38; b2; } Integer \ncheckBlackDepth(Node n) { if (n == nil) return 1; int left = checkBlackDepth(n.left); int right = checkBlackDepth(n.right); \nif (left != right || left == -1) return -1; return left + (n.color == BLACK ? 1 : 0); } Figure 10. Invariants \nfor the red-black tree. nil is a special dummy node in the implementation that is always black. generally \nproduces asymptotic improvement, so arbitrary speedups can be had at large data structure sizes.) In \neach case, wall-clock time, including GC and all other VM and incrementalization overheads, is measured. \nThe data structures and their modi.cation patterns are described below. If an operation requires a random \nelement, it is selected at random from the set of elements guaranteed to ful.ll the operation. For instance, \nthe element for a deletion is chosen at random from the elements already in the data structure. Ordered \nList. The OrderedIntList and its invariant isOrdered were described in Section 2. The modi.cations were \n50% insertion of a random element, 25% deletion of a random element, and 25% deletion of the .rst element \nin the list (as in a queue). Hash Table. The HashTable data structure maps keys to val\u00adues, using chaining \nto store multiple entries in the same bucket. The invariant check, shown in Figure 9, veri.es that no \nentry is in the wrong bucket. Note that the single invariant encompasses two func\u00adtions. The modi.cations \nwere 50% random insertions and 50% ran\u00addom deletions. Red-Black Tree. We used the open-source GNU Classpath \nver\u00adsion of TreeMap, which implements a red-black tree in 1600 lines of Java. The invariants verify the \nrequired properties of a red-black tree, and check the following properties: (i) the tree is well-ordered \n(ii) local red-black properties (e.g. a red node has black children) (iii) the number of black nodes \nalong any path from the root to a leaf is the same. See Figure 10 for the code. The modi.cations consisted \nof 50% random insertions and 50% random deletions. A red-black tree is particularly well suited to dynamic \ninvariant checks because 1. It is a data structure with nontrivial behaviors for even simple operations \nsuch as insert and delete that are hard to get right . 2. It has several invariants that are dif.cult \nto analyze statically but are relatively easy to write as code.  However, its complexity also challenges \nDITTO: a single op\u00aderation can alter the data structure layout signi.cantly, reordering, adding, and \nremoving nodes. Additionally, two of the invariants en\u00adforce global constraints, requiring nontrivial \nincremental updates to the computation graph. For these reasons, we considered the red\u00adblack tree an \nacid test for the feasibility of DITTO. 5.1.1 Analysis The results of incrementalization for these data \nstructures at vari\u00adous sizes are presented in Figure 11. In each case DITTO success\u00adfully incrementalized \nthe invariant, producing an asymptotic speedup over the unincrementalized version. The average speedup \nat 3200 el\u00adements is 7.5x. DITTO performs well for medium to large sized data structures. However, there \nis some baseline overhead due to write barriers and the incrementalization data structures that have \nto be maintained. To more closely analyze behavior on smaller data structures, for each structure we \nmeasured the crossover size, the data structure size at which it is faster to run DITTO s incrementalized \nversion of a check than the original, all overheads considered.2 Crossover size Ordered list 250 Hash \ntable 100 Red-black tree 200 These crossover sizes suggest that DITTO can be used as part of the development \nprocess for programs with relatively small data structures as well.  5.2 Sample applications Netcols \nis a Tetris-like game written by a colleague in 1600 lines of Java. Jewels fall from the sky through \na rectangular grid and must be made to form patterns as they land. The program keeps an array top of \nthe position of the highest landed jewels in each column, and maintains the invariant that no jewels \nare .oating i.e. there are no empty squares below the highest spot in each column, and there are no \nbejeweled squares about it; see Figure 12 for the code. 2 In [1], a crossover point is also mentioned, \noften occurring at size 1. Though our attempt to contact the author failed, we imagine that this point \nis mea\u00adsuring a different phenomenon, perhaps a theoretical crossover point without runtime overheads. \n Ordered list performance Hash table performance Red-black tree performance 3500 10000 9000 8000 1200 \n3000 1000 2500 7000 6000 800  No invariants Incrementalized Invariants Time (ms) 5000 4000 3000 600 \n400 1000 2000 200 500 1000 0 0 0 0 500 1000 1500 2000 2500 3000 Data structure size Figure 11. Results \nfor data structure benchmarks. Each graph compares the performance of code with (i) no invariant checks \n(ii) standard invariant checks (iii) incrementalized invariant checks on different sizes of the data \nstructure. Boolean checkTop(int col) { if (col == width) return; boolean b1 = checkEmpty(col, top[col]), \nb2 = checkFull(col, top[col]-1), b3 = checkTop(col+1); return b1 &#38;&#38; b2 &#38;&#38; b3; } Boolean \ncheckFull(int col, int row) { if (row == 0) return true; return jewels[col][row] != nullJewel &#38;&#38; \ncheckFull(col, row-1); } Boolean checkEmpty(int col, int row) { if (row == height) return true; return \njewels[col][row] == nullJewel &#38;&#38; checkEmpty(col, row+1); } Figure 12. The invariant check that \nveri.es that a netcols grid has no .oating jewels. Boolean goodMapping(JList names) { if (names == null) \nreturn true; String s = (String) names.value; if (Character.isUpperCase(s.charAt(0)) || Character.isDigit(s.charAt(0))) \nreturn false; boolean b1 = ! inReserved(s, 0), b2 = goodMapping(names.next); return b1 &#38;&#38; b2; \n} Boolean inReserved(String s, int off) { if (off == reserved_names.length) return false; return s.equals(reserved_names[off]) \n|| inReserved(s, off+1); } do not meet any exclusionary criteria. See Figure 13. To enable this invariant, \nwe maintain an auxiliary list of map keys, names. Figure 14 shows the results of feeding JSO JavaScript \ninputs of varying sizes. DITTO s incrementalized version of the check is able to mitigate much of the \noverhead.  6. Related Work Languages such as JML [14] and Spec# [4] provide motivation for this work. \nThese languages enable the user to write data structure in\u00advariant checks (among other speci.cations) \ndirectly into their code. In some cases, these checks are statically veri.able, in which case DITTO provides \na complimentary solution: very small of.ine over\u00adhead followed by a moderate runtime overhead and veri.cation \nfor testing inputs, as opposed to a larger of.ine overhead, no runtime overhead, and veri.cation for \nall inputs. On the other hand, the cases where the checks must be veri.ed at runtime are perfectly suited \nto DITTO. Software model checking [3, 10, 22] is a powerful technique for static veri.cation. However, \nmost model checkers do not perform well when required to maintain a precise heap abstraction, such as \nwhen verifying red-black tree invariants, often failing to verify structures of depth greater than .ve. \nRecent work by Darga et al. [8] has made progress toward veri.cation of complex invariants, but the depth \nbound is still small for complex data structures and ghost .elds and programmer annotations are required. \nAlgorithm incrementalization has been the subject of consider\u00adable research [9, 17, 18, 19, 12]; see \n[21] for a comprehensive bib\u00adliography of early work. Initial research often focused on hand\u00adincrementalizing \nparticular algorithms [20]. Liu et al. began to devise a systematic approach to incremen\u00adtalization [16], \nculminating with recent work [15] that presented a semi-automated incrementalizer for object-oriented \nlanguages. This work differs from DITTO in two respects. First, it incrementalizes JSO performance 25000 \n Figure 13. Invariant check for JSO that ensures that a protected function is not renamed. 20000 Time \n(ms) 15000 10000 The main event loop averaged 80ms end-to-end time with the invariant check running, \nnoticeably sluggish. With DITTO,the event loop averaged 15ms. JSO [13] is a JavaScript obfuscator written \nin 600 lines of Java. It renames JavaScript functions, and keeps a map from old names 5000 0 to new \nso that if the same function is invoked again, its correct new name will be used. However, functions \nwhose names have certain properties or that are on a list of reserved keywords should not be renamed. \nThus, we check the invariant that keys in the renaming map Figure 14. Performance numbers for JSO. algorithms \nprimarily through memoization (rather than a hybrid de\u00adpendence/memoization solution), which may require \nrecomputation even though true dependencies have not been modi.ed. Second, it requires a library of hints, \none for each type of input modi.cation, that describe how the modi.cation pertains to the incrementalization; \nDITTO allows for arbitrary updates. Most recently, Acar et al. [1, 2] have developed a robust frame\u00adwork \nfor incrementalization that uses both memoization and change propagation. This framework offers a number \nof library functions with which a programmer can incrementalize functional code func\u00adtions and achieve \nconsiderable speedups. Acar s work and DITTO differ in several respects. Acar s incrementalizer operates \nin the context of a purely func\u00adtional program in ML. Input changes and computation dependences must \nbe speci.ed explicitly by the programmer. The framework is general and, thanks to the functional environment, \ncan incremental\u00adize computations that return new objects. Dependencies are tracked at the statement level, \nwhich allows for very precise change propa\u00adgation. However, to achieve this granularity, functions must \nbe stati\u00adcally split into several components, so that individual statements can be executed directly. \nThese sub-functions must then be converted to continuation-passing style. In contrast, DITTO operates \nin Java. Incrementalization is done automatically via write barriers and automatic instrumentation. DITTO \noperates on the domain of data structure invariant checks: recursive, side-effect-free functions. Because \nthe rest of the program may be arbitrarily imperative, functions that return new objects are not allowed \n(such objects may be modi.ed and thus are unsuitable for memoization). However, many common invariant \nchecks can be written despite this restriction. Dependencies are tracked at the func\u00adtion level, which \nobviates the need for function splitting and CPS conversion (as well as optimizations required to elicit \ngood CPS performance from Java VMs). The suitability of optimistic memo\u00adization for invariant checks \nfurther enables a simple implementation. Though the function-level granularity can require more code \nto be re\u00adexecuted than necessary, invariant check functions tend to be small, and executing an entire \nfunction is often nearly as fast as identify\u00ading the few statements in that function that actually have \nmodi.ed dependences and rerunning just those.  7. Conclusion In this paper we have presented DITTO, \na novel incrementalizer tar\u00adgeted towards a valuable set of functions, data structure invariant checks. \nBy limiting its domain to a class of these checks and ex\u00adploiting their common properties, DITTO is able \nto incrementalize automatically, for imperative languages like Java and C#, and sim\u00adply, via optimistic \nmemoization. References [1] Umut A. Acar, Guy E. Blelloch, Matthias Blume, and Kanat Tang\u00adwongsan. An \nexperimental analysis of self-adjusting computation. In PLDI 06: Proceedings of the 2006 ACM SIGPLAN \nconference on Pro\u00adgramming Language Design and Implementation, pages 96 107, New York, NY, USA, 2006. \nACM Press. [2] Umut A. Acar, Guy E. Blelloch, and Robert Harper. Adaptive functional programming. In \nSymposium on Principles of Programming Languages, pages 247 259, 2002. [3] Thomas Ball, Rupak Majumdar, \nTodd D. Millstein, and Sriram K. Rajamani. Automatic predicate abstraction of c programs. In SIGPLAN \nConference on Programming Language Design and Implementation, pages 203 213, 2001. [4] Mike Barnett, \nK. Rustan M. Leino, and Wolfram Schulte. The spec-sharp programming system: An overview. http://research. \nmicrosoft.com/specsharp/papers/krml136.pdf. [5] M. Bender, R. Cole, E. Demaine, M. Farach-Colton, and \nJ. Zito. Two simpli.ed algorithms for maintaining order in a list. In Proceedings of the 10th Annual \nEuropean Symposium on Algorithms (ESA 2002), 2002. [6] Yoonsik Cheon and Gary T. Leavens. A runtime assertion \nchecker for the Java Modeling Language (JML). In Hamid R. Arabnia and Youngsong Mun, editors, Proceedings \nof the International Conference on Software Engineering Research and Practice (SERP 02), Las Vegas, Nevada, \nUSA, June 24-27, 2002, pages 322 328. CSREA Press, June 2002. [7] S. Chiba and M. Nishizawa. An easy-to-use \ntoolkit for ef.cient java bytecode translators. In Proceedings of the second International Conference \non Generative Programming and Component Engineering (GPCE 03), Erfurt, Germany, volume 2830 of LNCS, \npages 364 376, September 2003. [8] Paul T. Darga and Chandrasekhar Boyapati. Ef.cient software model \nchecking of data structure properties. SIGPLAN Not., 41(10):363 382, 2006. [9] Alan Demers, Thomas Reps, \nand Tim Teitelbaum. Incremental evaluation for attribute grammars with application to syntax-directed \neditors. In POPL 81: Proceedings of the 8th ACM SIGPLAN-SIGACT symposium on Principles of programming \nlanguages, pages 105 116, New York, NY, USA, 1981. ACM Press. [10] Matthew B. Dwyer, John Hatcliff, Roby \nJoehanes, Shawn Laubach, Corina S. Pasareanu, Robby, Hongjun Zheng, and W Visser. Tool\u00adsupported program \nabstraction for .nite-state veri.cation. In Interna\u00adtional Conference on Software Engineering, pages \n177 187, 2001. [11] Hotjava 1.0 signature bug, 1997. http://www.cs.princeton.edu/ sip/news/april29.html. \n[12] Allan Heydon, Roy Levin, and Yuan Yu. Caching function calls using precise dependencies. ACM SIGPLAN \nNotices, 35(5):311 320, 2000. [13] Jso. http://shaneng.awardspace.com/#jso_description. [14] Gary T. \nLeavens, Albert L. Baker, and Clyde Ruby. JML: A notation for detailed design. In Haim Kilov, Bernhard \nRumpe, and Ian Simmonds, editors, Behavioral Speci.cations of Businesses and Systems, pages 175 188. \nKluwer Academic Publishers, 1999. [15] Yanhong A. Liu, Scott D. Stoller, Michael Gorbovitski, Tom Rothamel, \nand Yanni Ellen Liu. Incrementalization across object abstraction. In OOPSLA 05: Proceedings of the 20th \nannual ACM SIGPLAN conference on Object oriented programming, systems, languages, and applications, pages \n473 486, New York, NY, USA, 2005. ACM Press. [16] Yanhong A. Liu and Tim Teitelbaum. Systematic derivation \nof incremental programs. Science of Computer Programming, 24(1):1 39, 1995. [17] Bob Paige and J. T. \nSchwartz. Expression continuity and the formal differentiation of algorithms. In POPL 77: Proceedings \nof the 4th ACM SIGACT-SIGPLAN symposium on Principles of programming languages, pages 58 71, New York, \nNY, USA, 1977. ACM Press. [18] Robert Paige and Shaye Koenig. Finite differencing of computable expressions. \nACM Trans. Program. Lang. Syst., 4(3):402 454, 1982. [19] W. Pugh and T. Teitelbaum. Incremental computation \nvia function caching. In POPL 89: Proceedings of the 16th ACM SIGPLAN-SIGACT symposium on Principles \nof programming languages, pages 315 328, New York, NY, USA, 1989. ACM Press. [20] G. Ramalingam. Bounded \nincremental computation. Technical Report 1172, Univ. of Wisconsin, Madison, Computer Sciences Dept., \n1210 West Dayton St., Madison, WI 53706, USA, 1993. [21] G. Ramalingam and Thomas Reps. A categorized \nbibliography on incremental computation. In POPL 93: Proceedings of the 20th ACM SIGPLAN-SIGACT symposium \non Principles of programming languages, pages 502 510, New York, NY, USA, 1993. ACM Press. [22] S. Graf \nand H. Saidi. Construction of abstract state graphs with PVS. In O. Grumberg, editor, Proc. 9th International \nConference on Computer Aided Veri.cation (CAV 97), volume 1254, pages 72 83. Springer Verlag, 1997. \n \n\t\t\t", "proc_id": "1250734", "abstract": "<p>We present DITTO, an automatic incrementalizer for dynamic, side-effect-free data structure invariant checks. Incrementalization speeds up the execution of a check by reusing its previous executions, checking the invariant anew only the changed parts of the data structure. DITTO exploits properties specific to the domain of invariant checks to automate and simplify the process without restricting what mutations the program can perform. Our incrementalizer works for modern imperative languages such as Java and C#. It can incrementalize,for example, verification of red-black tree properties and the consistency of the hash code in a hash table bucket. Our source-to-source implementation for Java is automatic, portable, and efficient. DITTO provides speedups on data structures with as few as 100 elements; on larger data structures, its speedups are characteristic of non-automatic incrementalizers: roughly 5-fold at 5,000 elements,and growing linearly with data structure size.</p>", "authors": [{"name": "Ajeet Shankar", "author_profile_id": "81100643801", "affiliation": "UC Berkeley, Berkeley, CA", "person_id": "P753312", "email_address": "", "orcid_id": ""}, {"name": "Rastislav Bod&#237;k", "author_profile_id": "81100033082", "affiliation": "UC Berkeley, Berkeley, CA", "person_id": "P517421", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1250734.1250770", "year": "2007", "article_id": "1250770", "conference": "PLDI", "title": "DITTO: automatic incrementalization of data structure invariant checks (in Java)", "url": "http://dl.acm.org/citation.cfm?id=1250770"}