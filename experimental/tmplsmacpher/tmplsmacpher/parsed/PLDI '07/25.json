{"article_publication_date": "06-10-2007", "fulltext": "\n Making Context-sensitive Points-to Analysis with Heap Cloning Practical For The Real World * Chris \nLattner Andrew Lenharth Vikram Adve Apple Inc. University of Illinois at University of Illinois at clattner@apple.com \nUrbana-Champaign Urbana-Champaign alenhar2@cs.uiuc.edu vadve@cs.uiuc.edu Abstract Context-sensitive \npointer analysis algorithms with full heap cloning are powerful but are widely considered to be too expen\u00adsive \nto include in production compilers. This paper shows, for the .rst time, that a context-sensitive, .eld-sensitive \nalgorithm with full heap cloning (by acyclic call paths) can indeed be both scal\u00adable and extremely fast \nin practice. Overall, the algorithm is able to analyze programs in the range of 100K-200K lines of C \ncode in 1-3 seconds, takes less than 5% of the time it takes for GCC to compile the code (which includes \nno whole-program analysis), and scales well across .ve orders of magnitude of code size. It is also able \nto analyze the Linux kernel (about 355K lines of code) in 3.1 sec\u00adonds. The paper describes the major \nalgorithmic and engineering design choices that are required to achieve these results, includ\u00ading (a) \nusing .ow-insensitive and uni.cation-based analysis, which are essential to avoid exponential behavior \nin practice; (b) sacri\u00ad.cing context-sensitivity within strongly connected components of the call graph; \nand (c) carefully eliminating several kinds of O(N2) behaviors (largely without affecting precision). \nThe tech\u00adniques used for (b) and (c) eliminated several major bottlenecks to scalability, and both are \ngeneralizable to other context-sensitive algorithms. We show that the engineering choices collectively \nre\u00adduce analysis time by factors of up to 3x-21x in our ten largest programs, and that the savings grow \nstrongly with program size. Finally, we brie.y summarize results demonstrating the precision of the analysis. \nCategories and Subject Descriptors D.3.4 [Processors]: Compil\u00aders General Terms Algorithms Keywords Pointer \nanalysis, context-sensitive, .eld-sensitive, in\u00adterprocedural, static analysis, recursive data structure \n* This work is supported in part by NSF under grant numbers EIA-0093426, EIA-0103756, CCR-9988482 and \nCCF-0429561, and in part by the Univer\u00adsity of Illinois. Permission to make digital or hard copies of \nall or part of this work for personal or classroom use is granted without fee provided that copies are \nnot made or distributed for pro.t or commercial advantage and that copies bear this notice and the full \ncitation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to \nlists, requires prior speci.c permission and/or a fee. PLDI 07 June 11 13, 2007, San Diego, California, \nUSA. Copyright c . 2007 ACM 978-1-59593-633-2/07/0006. . . $5.00  1. Introduction Context-sensitive \nalias analysis algorithms have been studied inten\u00adsively over the last two decades, with impressive improvements \nin algorithmic scalability [7, 10, 35, 14, 13, 22, 11, 12, 23, 34]. To achieve true context-sensitivity, \nsuch algorithms must distinguish heap objects by (acyclic) call paths, not just by allocation site; this \nproperty is sometimes referred to as heap cloning [26]. Heap cloning is important because it allows analyses \nto distinguish differ\u00adent instances of a logical data structure created at different places in a program, \neven if the data structure is implemented with a common set of functions (e.g., a data structure library) \nthat allocate memory internally. Such programming patterns are increasingly prevalent particularly in \nobject-oriented programs, where reusable libraries are an important feature. For example, in the LLVM \ncompiler sys\u00adtem [19], which is written in C++, there are no less than 25 static occurrences of a single \nclass (vector<unsigned>). Heap cloning also allows analyses to handle allocations that occur through \none or more levels of wrappers in a simple, general manner instead of handling them as a special case \n(e.g., single-level wrappers [15]). More quantitatively, Nystrom et al. [26] show that for many pro\u00adgrams, \nnaming heap objects only by allocation site (the most com\u00admon alternative) signi.cantly reduces analysis \nprecision compared with heap specialization by call paths, up to some threshold. Unfortunately, there \nis widespread skepticism that algorithms with heap cloning can be scalable and fast enough to be included \nin production compilers. To date, this skepticism is arguably justi\u00ad.ed: we know of no previous paper \nthat demonstrates that an alias analysis that uses heap cloning is scalable and fast enough for use in \nproduction compilers. Section 6 discusses the current state of the art in more detail. Brie.y, two recent \nalgorithms that use differ\u00adent forms of cloning with a subset-based (rather than a uni.cation\u00adbased) \nanalysis have been to shown to be scalable but are still quite slow in absolute running times [34, 25]. \nAt least for now, such algorithms appear too slow to be used in a production compiler, although they \nmay be reasonable for static analysis tools. On the other hand, there are several algorithms that use \nuni.cation to con\u00adtrol the exponential blow-up that can occur with context-sensitivity (with or without \nheap cloning). The MoPPA algorithm by Liang and Harrold [23] uses heap cloning and is quite fast but \nexhausts available memory on a workstation with 640MB of memory for their largest program, povray. Their \nresults (and many others) jus\u00adtify the common concern that, in practice, memory consumption can be a \nlimiting factor in scaling context-sensitive analyses. Many other context-sensitive alias analysis papers \neither do not use heap cloning [22, 11, 12] or (particularly in earlier papers) only report results for \nrelatively small programs of a few thousand lines of code or less and typically make limited or no claims \nabout scal\u00adability [7, 10, 35, 14, 13, 27]. In this paper, we describe an algorithm named Data Struc\u00adture \nAnalysis (DSA), and use it to present strong evidence that a context-sensitive, .eld-sensitive algorithm \nwith full heap cloning (by acyclic call paths) can indeed be both scalable and extremely fast in practice. \nDSA is able to analyze programs in the range of 100K-220K lines of C code in 1-3 seconds, taking less \nthan 5% of the time it takes GCC to compile the program at -O3 (which does no whole-program analysis), \nand scaling well across .ve orders of magnitude of code size in terms of both analysis time and mem\u00adory \nusage. It is also able to analyze the Linux kernel (about 355K lines of compiled code in our con.guration) \nin 3.1 seconds. These analysis times and scaling behavior appear to be very reasonable for production \ncompilers. More speci.cally, DSA is a points-to analysis algorithm that is .eld-sensitive and context-sensitive \nwith full heap cloning (by acyclic call paths). Like several previous papers, we combine these with a \n.ow-insensitive and uni.cation-based approach to improve scalability [22, 11, 12, 23]. There are three \nnovel features in our algorithm itself: First, DSA uses a new extension of Tarjan s SCC .nding algo\u00adrithm \nto incrementally construct a call graph during the analysis without any iteration, even in the presence \nof function point\u00aders and recursion. An algorithm by F\u00a8ahndrich et al. [11] is the only previous work \nwe know that achieves the same property, but they do it by using a constraint instantiation method that \nappears dif.cult to extend to incomplete programs, which is es\u00adsential for production compilers in practice. \n Second, it uses a .ne-grain completeness tracking technique for points-to sets as a uni.ed solution \nfor several dif.cult prob\u00adlems that have usually been solved via different techniques be\u00adfore (or ignored): \n(a) supporting .eld-sensitivity for non-type\u00adsafe languages; (b) correctly supporting incomplete programs, \ni.e., with unknown external functions; and (c) constructing a call graph incrementally during the analysis. \nThis technique can be used in any alias analysis that explicitly tracks reachability of objects (typically, \nanalyses that use an explicit representation of memory rather than just aliasing pairs of references \n[17]).  Third, DSA includes several essential engineering choices de\u00adveloped over time to avoid O(N2) \nbehaviors that were a ma\u00adjor bottleneck to scalability in practice. Two of these have been described \npreviously (the globals graph [28, 23] and ef.\u00adcient inlining [28]) but no experimental results on their \nbene.ts have been reported. These choices are generalizable to other context-sensitive algorithms. Our \nexperiments show that each choice contributes substantial speedups, and they collectively achieve 3x \n21x reduction in analysis time in our ten largest pro\u00adgrams. furthermore this reduction increases strongly \nwith pro\u00adgram size, demonstrating that they eliminate signi.cant scala\u00adbility bottlenecks.  In addition \nto speed and scalability, the algorithm has several practical strengths that we consider valuable for \nreal-world com\u00adpilers. Perhaps most important, DSA correctly handles incomplete programs (i.e., programs \nwith unknown external functions): it pro\u00adduces conservative results while still trying to provide aliasing \nin\u00adformation for as much of a program as possible. To our knowl\u00adedge, none of the previous context-sensitive, \nuni.cation-based al\u00adgorithms [22, 11, 12, 23] can correctly handle incomplete pro\u00adgrams. The algorithm \nsupports the full generality of C/C++ pro\u00adgrams, including type-unsafe code, function pointers, recursion, \nsetjmp/longjmp, C++ exceptions, etc. The algorithm does not require a call graph as input, as noted earlier \n(even though it is non\u00aditerative). We compare the precision of our algorithm for alias analysis to Andersen \ns algorithm [2] (a context-insensitive subset-based algo\u00adrithm), showing that DSA is about as precise \nAndersen s for many cases, is signi.cantly more precise for some programs, and is only worse in rare \ncases. Further, other work [18] shows that the mod\u00ad/ref information captured by DSA is signi.cantly better \nthan that computed by non-context-sensitive algorithms. In addition to alias analysis, DSA can also be \nused to extract limited information about entire linked data structures: identifying instances of these \nstructures, bounding lifetimes of each instance, and extracting available (.ow-insensitive) structural \nand type infor\u00admation for each identi.ed instance (this capability is the source of the name Data Structure \nAnalysis). Although this information is signi.cantly more limited than full-blown shape analysis [29, \n4, 16], it is suf.cient for many in\u00adteresting applications. One such application is the Automatic Pool \nAllocation transformation [20], which segregates heap objects into distinct pools if the points-to graph \ncan distinguish subsets. This can improve spatial locality signi.cantly for recursive data struc\u00adture \ntraversals, and can enable other compiler and run-time tech\u00adniques that wish to modify (or measure) per-data-structure \nbehav\u00adior [21]. Another example is the SAFECode compiler for C [8], which enforces safety properties \n(memory safety, control-.ow in\u00adtegrity, type safety for a subset of objects, and analysis soundness) \nautomatically and with relatively low overhead for unmodi.ed C programs. Type homogeneity of points-to \nsets enables elimination of run-time checks in SAFECode, reducing its overhead. The full source code \nfor DSA can be downloaded via anony\u00admous CVS at llvm.org. Because only the .rst ( local ) phase of the \nalgorithm directly inspects the program representation, the implementation should be relatively straightforward \nto incorporate into other mid-level or low-level compiler systems written in C++. The next section precisely \nde.nes the points-to graph represen\u00adtation computed by our analysis. Section 3 describes the algorithm \nand analyzes its complexity. Section 4 describes several important engineering choices required for scalability. \nSection 5 describes our experimental results for algorithm cost and very brie.y summa\u00adrizes the results \nof two studies of algorithm precision. Section 6 discusses related work and Section 7 brie.y summarizes \nthe major conclusions. typedef struct list { struct list *Next ; int Data ; } list ; int Global = 10; \nvoid do all( list *L, void (*FP )( int *)) { do { FP(&#38;L->Data ); L=L->Next ; } while (L); }void \naddG ( int *X) { (*X) += Global ; }void addGToList ( l i s t *L) { do all(L, addG); }list *makeList \n( int Num) { list *New = malloc ( sizeof (list )); New->Next = Num ? makeList (Num-1): 0; New->Data = \nNum; return New ; } int main () { / * X &#38; Y lists are disjoint */ list *X= makeList (10); list *Y \n= makeList (100); addGToList (X); Global = 20; addGToList (Y); } Figure 1. C code for running example \n 2. The Data Structure Graph Data Structure Analysis computes a graph we call the Data Struc\u00adture Graph \n(DS graph) for each function in a program, summarizing the memory objects accessible within the function \nalong with their connectivity patterns. Each DS graph node represents a (potentially in.nite) set of \ndynamic memory objects, and distinct nodes repre\u00adsent disjoint sets of objects, i.e., the graph is a \n.nite, static parti\u00adtioning of the memory objects. All dynamic objects which may be pointed to by a single \npointer variable or .eld are represented as a single node in a graph. In de.ning the DS graph, we assume \nthat input programs have a simple type system with structural equivalence, having primitive in\u00adteger \nand .oating point types of prede.ned sizes, plus four derived types: pointers, structures, arrays, and \nfunctions. The analysis ex\u00adplicitly tracks points-to properties only for pointer types and integer types \nof the same size or larger; we call these pointer-compatible types (other values are treated very conservatively \nif converted into a pointer-compatible type). For any type t, fields(t) returns a set of .eld names of \nt. This is a single degenerate .eld name if t is a scalar type or function type. An array type of known \nsize k may be represented either as a structure with k .elds (if all index expres\u00adsions into the array \nare compile-time constants) or by a single .eld; an unknown-size array is always represented as the latter. \nWe also assume a load/store program representation in which virtual reg\u00adisters and memory locations are \ndistinct, it is not possible to take the address of a virtual register, and virtual registers can only \nrep\u00adresent scalar variables (i.e., integer, .oating point, or pointer). Spe\u00adci.c operations in the input \nprogram representation are described in Section 3.2. 2.1 Graph De.nition The DS graph for a function \nis a .nite directed graph represented as a tuple DSG(F)= .N,E,EV ,Ncall.,where: N is a set of nodes, \ncalled DS nodes . DS nodes have several attributes described in Section 2.2 below.  E is a set of edges \nin the graph. Formally, E is a function of type .ns,fs...nd,fd.,where ns,nd .N, fs .fields(T(ns)) and \nfd .fields(T(nd)),and T(n) denotes type information computed for the objects of n as explained below. \nWe refer to a .node,.eld.pair as a cell . E is a function because a source .eld can have only a single \noutgoing edge. Non\u00adpointer-compatible .elds (and virtual registers) are mapped to  .null,0.. EV is \na partial function of type vars(f) ..n,f.,where vars(f) is the set of virtual registers in scope in function \nf. This includes global variables, which are treated as virtual reg\u00adisters of pointer type with global \nscope, pointing to an unnamed global memory object. Conceptually, EV (v) is an edge from register v to \nthe target .eld .n,f.pointed to by v,if v is of pointer-compatible type.  Ncall .N is a set of call \nnodes in the graph, which represent unresolved call sites within the current function or one of its (immediate \nor transitive) callees. Each call node c .Ncall is a k+2 tuple: (r,f,a1,...,ak), where every element \nof the tuple is a node-.eld pair .n,f.. r denotes the value returned by the call (if it is pointer-compatible), \nand f the set of possible callee functions. a1 ...ak denote the values passed as arguments to the call. \nConceptually, each tuple element can also be regarded as a points-to edge in the graph.  To illustrate \nthe DS graphs and the analysis algorithm, we use the code in Figure 1 as a running example. This example \ncreates and traverses two disjoint linked lists, using iteration, recursion, function pointers, a pointer \nto a subobject, and a global variable <type>: <flags> <field0> <field1> Return Called First  Second \nValue Function Argument Argument DS node Variable Call Node Figure 2. Graph Notation reference. Despite \nthe complexity of the example, Data Structure Analysis is able to prove that the two lists X and Y are \ndisjoint. Naming heap objects by allocation site would prevent the two lists from being disambiguated \nbecause nodes of both are allocated at the same site. The .nal DS graph computed for main is shown in \nFigure 10. To illustrate the DS graphs computed by various stages of our algorithm, we render DS graphs \nusing the graphical notation shown in Figure 2. Figure 3 shows the initial ( local ) graphs computed \nfor the do all and addG functions, before any interprocedural infor\u00admation is applied. The .gure includes \nan example of a call node, which (in this case) calls the function pointed to by FP, passing the address \nof the .eld pointed to by L->data as an argument, and ignores the return value of the call. 2.2 Graph \nNodes and Fields Each DS node n has three pieces of information describing the memory objects corresponding \nto that node: T(n): a language-speci.c type for the memory objects repre\u00adsented by n. This type determines \nthe number of .elds and out\u00adgoing edges in a node. Note that .elds are tracked separately only for type-homogeneous \nnodes, as explained below.  G(n): a set of global objects represented by n. Includes func\u00adtions, which \nrepresent the targets of function pointers and of the f .eld in call nodes.  flags(n): set of .ags associated \nwith n.{H,S,G,U, A, M,R, C, O }. These .ags are de.ned below.  Figure 3. Local DSGraphs for do all \nand addG Storage class .ags (H, S, G, U):The H , S , G and U .ags in flags(n) are used to distinguish \nHeap, Stack, Global (including functions), and Unknown objects. Multiple .ags may be present in a single \nDS node. The Unknown .ag is added to a DS node when a constant integer value is cast to a pointer or \nwhen unanalyzable address arithmetic is seen: the .ag signi.es that the instruction creating the target \nobject was not found (these cases are infrequent in portable programs). A node marked U must be treated \nas potentially overlapping with (i.e., representing common memory objects as) any other node in the graph. \nNodes representing objects created in an external, unanalyzed function are not marked U ,but are treated \nas incomplete, as described below. Mod/Ref .ags (M, R): These .ags simply mark whether store or load \noperations have been detected on a memory object at the node. This directly provides context-sensitive \nMod/Ref information for memory objects, including global, stack and heap objects. Completeness .ag (C):The \nComplete .ag denotes that all oper\u00adations on objects at a node have been processed. For example, the \nlist node in the local graph for function do all (Figure 3) has no C .ag because the list is accessible \nin an unprocessed callee, but both lists and the global in function main are marked Complete1. A node \nmay have no C .ag even at the end of analysis if it is reach\u00adable from unavailable external functions \n(we .nd it is common to have a few such nodes in large programs, because of unavailable libraries). At \nany point, if a node is not marked complete, the in\u00adformation calculated for the DS node represents partial \ninformation and must be treated conservatively. In particular, the node may later be assigned extra edges, \nextra .ags, a different type, or may even end up merged with another incomplete node in the graph. For\u00admally, \ntwo nodes with no C .ag (e.g., L and FP) may represent common objects, i.e., a client (e.g., alias analysis \nqueries) must as\u00adsume that pointers to the two nodes may alias. Nevertheless, other nodes in such a graph \nmay be complete and such nodes will never be merged with any other node, providing useful partial informa\u00adtion \nfor incomplete programs. (Of course, a pointer to node with the U .ag may alias a pointer to any other \nnode, even those with C set). The algorithm for inferring C .ags is described in Section 3.1.  An important \nbene.t of the Complete .ag within the analysis itself is that it allows DS analysis to assume speculatively \nthat all accesses to a node are type-safe, until an access to the node is found which con.icts with the \nother accesses. Because a node is not marked complete as long as there are potentially unprocessed accesses, \nthis is safe. DSA uses this to provide .eld-sensitive in\u00adformation for the type-safe subsets of programs, \nwhile collapsing .elds for type-unsafe structure types because tracking .elds for such types can be expensive \n[30]. Collapsed .ag (O): The collapsed .ag (O) marks nodes represent\u00ading multiple, incompatible types \nof objects. More precisely, if all uses of objects in a node (or to a .eld of the node) follow a consis\u00adtent \ntype t (or the .eld type within t ), then DSA assigns T (n)=t ; we refer to such a node as type-homogeneous. \nHere uses are de\u00ad.ned as operations on pointers to the node that interpret the type, viz., loads, stores, \nor structure and array indexing. In particular, pointer casts (e.g., from void*) are not counted as uses. \nIf uses with incompatible types (as de.ned in Section 3) are found, we no longer track the type or .elds \nof objects represented by the node. We mark the node with the O .ag (cOllapsed), set T (n)= char[], i.e., \nan unsized array of bytes, and merge all outgoing edges into a single edge. We do this using the following \nalgorithm: collapse(dsnode n) cell e =.null, 0. // null target .f .fields(T (n)) e = mergeCells(e, E(.n, \nf.)) // merge old target with e remove .eld f // remove old edge T (n)=char* // reset type information \nE(.n, 0.)=e // new edge from .eld 0 flags(n)=flags(n) .{O. , A} //node is cOllapsed, Array The function \nmergeCells(c1,c2) (described in the next sec\u00adtion) merges the cells c1 and c2 and therefore the nodes \npointed to by those cells. This ensures that the targets of the two cells are now exactly equal. The \nresult is the same as if the type information was never speculated for node n. Array .ag (A): This .ag \nis added to a node if any array indexing expresssion is applied to a pointer targeting that node. Note \nthat this does not eliminate type homogeneity for non-collapsed nodes: the node may represent singleton \nobjects and arrays of objects of type t .The A .ag is always set for Collapsed nodes since the degenerate \ntype they use is an array of bytes. 1 This is somewhat similar to the inside nodes of [33].   3. Construction \nAlgorithm DS graphs are created and re.ned in a three step process. The .rst phase constructs a DS graph \nfor each function in the program, using only intraprocedural information (a local graph). This is the \nonly phase that inspects the actual program representation; the next two phases operate solely on DS \ngraphs. Second, a Bottom-Up analy\u00adsis phase is used to eliminate incomplete information due to callees \nin a function, by incorporating information from callee graphs into the caller s graph (creating a BU \ngraph). By the end of the BU phase, the call graph construction is also complete. The .nal Top-Down phase \neliminates incomplete information due to incoming arguments by merging caller graphs into callees (creating \na TD graph). Both, the BU and TD phases operate on the known (i.e., partial or completed) Strongly Connected \nComponents (SCCs) in the call graph. Two properties are important for understanding how the analy\u00adsis \nworks in the presence of incomplete programs, and how it can incrementally construct the call graph even \nthough it operates on the SCCs of the graph. First, the DS graph for a function is correct even if only \na subset of its potential callers and potential callees have been incorporated into the graph (i.e., \nthe information in the graph can be used safely so long as the limitations on nodes with\u00adout C .ags are \nrespected, as described in Section 2). Intuitively, the key to this property simply is that a node must \nnot be marked complete until it is known that all callers and callees potentially affecting that node \nhave been incorporated into the graph. Second, the result of two graph inlining operations at one or \ntwo call sites is independent of the order of those operations. This follows from a more basic property \nthat the order in which a set of nodes is merged does not affect the .nal result. 3.1 Primitive Graph \nOperations Data Structure Analysis is a .ow-insensitive algorithm which uses a uni.cation-based memory \nmodel, similar to Steensgaard s algo\u00adrithm [31]. The algorithm uses several primitive operations on DS \ngraphs, shown in Figure 4. These operations are used in the algo\u00adrithm to merge two cells (mergeCells), \nmerge a callee s graph into a caller s graph at a particular call site (resolveCallee) and vice versa \n(resolveCaller), and compute the completeness property ( C .ag) for DS nodes (markComplete). The two \ngraph-merging operations are described later in this section. The fundamental operation in the algorithm \nis mergeCells, which merges two target nodes by merging the type information, .ags, globals and outgoing \nedges of the two nodes, and moving the incoming edges to the resulting node. If the two .elds have incompatible \ntypes (e.g., T (n1)= int, f1 =0, T (n2)= {int, short}, f2 =1), or if the two node types are compatible \nbut the .elds are misaligned (e.g., T (n1)=T (n2)={int, short}, f1 =0, f2 =1), the resulting node is \n.rst collapsed as described earlier before the other information is merged. Merging outgoing edges causes \nthe target node of the edges to be merged as well; if the node is collapsed, the resulting node for n2 \nwill have only one outgoing edge which is merged with all the out-edges of n1.We use Tarjan s Union-Find \nalgorithm [32] to make the merging ef.cient. The routine markComplete uses an ef.cient traversal of a \nDS graph, starting at formal arguments, the return node (p), and glob\u00adals, though the ef.cient traversal \nis now shown in Figure 4. In the Top-Down phase, if a function is not visible to external code (i.e., \nall its callers have been identi.ed), nodes reachable from formal arguments, the return value, and globals \nthat are not externally vis\u00adible are marked Complete. Identifying which functions and glob\u00adals may be \nexternally visible is done by the LLVM linker, which can link both LLVM and native code .les and can \nbe much more aggressive about marking symbols internal when linking complete programs than libraries \n[1]. (Merge two cells of same or different nodes; update n2,discard n1) Cell mergeCells(Cell .n1,f1.,Cell \n.n2,f2.,) if (IncompatibleForMerge(T(n1),T(n2),f1,f2)) collapse n2 (i.e., merge .elds and out-edges) \nunion .ags of n1 into .ags of n2 union globals of n1 into globals of n2 merge target of each out-edge \nof .n1,fj .with target of corresponding .eld of n2 move in-edges of n1 to corresponding .elds of n2 \ndestroy n1 return .n2,0.(if collapsed) or .n2,f2.(otherwise) (Clone G1 into G2; merge corresponding \nnodes for each global) cloneGraphInto(G1,G2) G1c = make a copy of graph G1 Add nodes and edges of G1c \nto G2 for (each node N .G1c) for (each global g .G(N)) merge N with the node containing g in G2 (Clone \ncallee graph into caller and merge arguments and return) resolveCallee(Graph Gcallee,Graph Gcaller, \nFunction Fcallee, CallSite CS) cloneGraphInto(Gcallee,Gcaller) clear S .ags on cloned nodes resolveArguments(Gcaller, \nFcallee, CS) (Clone caller graph into callee and merge arguments and return) resolveCaller(Graph Gcaller,Graph \nGcallee, Function Fcallee, CallSite CS) cloneGraphInto(Gcaller ,Gcallee) resolveArguments(Gcallee, Fcallee, \nCS) (Merge arguments and return value for resolving a call site) resolveArguments(Graph Gmerged, Function \nFC , CallSite CS) mergeCells(target of CS[1], target of return value of FC ) for (1 =i =min(Numformals(FC \n), NumActualArgs(CS)) mergeCells(target of arg i at CS, target of formal i of FC ) (Mark nodes Complete \nif safe in Local, BU phases; see text for TD) markComplete(Graph G) for (each node N .G), where C ./flags(N) \nif N is reachable from call nodes or other incomplete nodes, skip it if N is reachable from formal arguments \nor EV (p),skip it otherwise, flags(N) .= C Figure 4. Primitive operations used in the algorithm 3.2 \nLocal Analysis Phase The goal of the local analysis phase is to compute a Local DS graph for each function, \nwithout information about callers and callees (see Figure 5). We present this analysis in terms of a \nmin\u00adimal language which is still as powerful as C. The assumptions about the type system and memory model \nin this language were described in Section 2. We assume that the functions E(X) and EV (X) return a new, \nempty node with the type of X (by invoking makeNode(typeof(X))), if no previous target node existed. \nThe LocalAnalysis .rst creates empty nodes for pointer\u00adcompatible virtual registers and for globals, \nand then does a lin\u00adear scan to process each instruction of the function. We assume that operand types \nin all instructions are strictly checked; any non\u00admatching operand in an operation must .rst be converted \nwith an explicit cast. Also, operations that produce non-pointer-compatible values in variables or .elds \nare simply ignored because those loca\u00adtions are always mapped to .null, 0. in EV and E respectively. \nWe focus on a few, less obvious, cases of the local analysis here. First, note that the type of a cell, \nEV (Y ), is updated only when Y is used in a load, store, or indexing operation. No type is inferred \nat malloc, alloca and cast operations. return instructions are handled by creating a special p virtual \nregister which is used to capture pointer-compatible return values. Function calls result in a new call \nnode being added to the DS graph, such as the call node for the call to do all in addGToList, in Figure \n7(a). The node gets entries for the value returned, the function pointer (for both direct and indirect \ncalls), and each pointer-compatible argument. (Compute the local DS Graph for function F) LocalAnalysis(function \nF) Create an empty graph .virtual registers R, EV (R)= makeNode(T(R)) .globals X (variables and functions) \nused in F G. N = makeNode(T(X))); G(N) .= X; flags(N) .= .instruction I .F : case I in: X = malloc ...: \n(heap allocation) EV (X)= makeNode(void) flags(node(EV (X))) .= H X = alloca ...: (stack allocation) \nEV (X)= makeNode(void) flags(node(EV (X))) .= S X=*Y: (load) mergeCells(EV (X),E(EV (Y ))) flags(node(EV \n(X)) .= R *Y = X: (store) mergeCells(EV (X),E(EV (Y ))) flags(node(EV (X)) .= M X = &#38;Y->Z: (address \nof struct .eld) .n,f.= updateType(EV (Y ),typeof(*Y )) f. =0,if n is collapsed; field(field(n, f),Z) \notherwise mergeCells(EV (X),.n,f..) X = &#38;Y[idx]: (address of array element) .n,f.= updateType(EV \n(Y ),typeof(*Y )) mergeCells(EV (X),.n,f.) flags(node(EV (X)) .= A return X: (return pointer-compatible \nvalue) mergeCells(EV (p),EV (X)) X=(t)Y: (value-preserving cast) mergeCells(EV (X),EV (Y )) X = Y(Z1,Z2, \n... Zn): (function call) callnode c = new callnode Ncalls .= c mergeCells(EV (X),c[1]) (return value) \nmergeCells(EV (Y ),c[2]) (callee function) .i .{1...n}: mergeCells(EV (Zi),c[i +2]) (Otherwise) X=Y \nop Z: (all other instructions) mergeCells(EV (X),EV (Y )) mergeCells(EV (X),EV (Z)) flags(node(EV (X))) \n.= U collapse(node(EV (X))) MarkCompleteNodes() Figure 5. The LocalAnalysis function Using mergeCells \nfor each entry correctly merges type information in the case when the argument type does not match the \ntype of the formal Finally, if any other instruction is applied to a pointer\u00adcompatible value, or used \nto compute such a value (e.g., a cast from a pointer to an integer smaller than the pointer and vice \nversa), any nodes pointed to by operands and the result are collapsed and the Unknown .ag is set on the \nnode. The .nal step in the Local graph construction is to calculate which DS nodes are Complete, which \nis done as described in Section 3.1. For a Local graph, nodes reachable from a formal argument, a global, \npassed as an argument to a call site, or returned by a function call may not be marked complete. For \nexample, in Figure 7(a), neither of the nodes for the arguments to do all are marked C . 3.3 Bottom-Up \nAnalysis Phase The Bottom-Up (BU) analysis phase re.nes the local graph for each function by incorporating \ninterprocedural information from the callees of each function. The result of the BU analysis is a graph \nfor each function summarizing the total effect of calling that func\u00adtion (imposed aliases and mod/ref \ninformation) without any call\u00ading context information. It computes this graph by cloning the BU graphs \nof all known callees into the caller s Local graph, merging nodes pointed to by corresponding formal \nand actual arguments and by common globals. We .rst describe a single graph inlining oper\u00adation, then \nexplain how the call graph is discovered and traversed. Consider a call to a function F with formal \narguments f1,... ,fn, where the actual arguments passed are a1,... ,an.The procedure resolveCallee in \nFigure 4 shows how such a call is pro\u00adcessed in the BU phase. We describe a simple, na\u00a8ive, version here; \na better approach is described in Section 4. We .rst copy the BU graph for F using cloneGraphInto, which \nalso merges targets of common globals in the caller s graph with those in the cloned graph. We then clear \nall Stack .ags since stack objects of a callee are not legally accessible in a caller. Note that we cannot \ndelete reachable nodes with Stack .ags: the nodes may escape (making them incomplete), so we cannot tell \nwhether other .ags will be included in such a node later. We then merge the node pointed to by each actual \nargument ai of pointer-compatible type with the copy of the node pointed to by fi. If applicable, we \nalso merge the return value in the call node with the copy of the return value node from the callee. \nNote that any unresolved call nodes in F s BU graph are copied into the caller s graph, and all the objects \nrep\u00adresenting arguments of the unresolved call in the callee s graph are now represented in the caller \nas well. (Create a new, empty node of type t) makeNode(type t) n = new Node(type = t,.ags= f, globals \n= f) . f . fields(t), E(n, f)=< null, 0> return n (Merge type of .eld .n, f. with type t.This may collapse \n.elds and update in/out edges via mergeCells()) updateType(cell .n, f., type t) if (t .void . t = typeof(.n, \nf.)) =. m = makeNode(t) return mergeCells(.m, 0., .n, f.)) else return .n, f. Figure 6. makeNode and \nupdateType operations 3.3.1 Basic Analysis Without Recursion The complete Bottom-Up algorithm for traversing \ncalls is shown in Figure 8. We explain it for four different cases. In the simplest case of a program \nwith only direct calls to non-external functions, no recursion, and no function pointers, the call nodes \nin each DS graph implicitly de.ne the entire call graph. The BU phase simply has to traverse this acyclic \ncall graph in post-order (visiting callees before callers), cloning and inlining graphs as described \nabove. To support programs that have function pointers and external functions (but no recursion), we \nrestrict our post-order traversal to only process a call-site if its function pointer targets a Complete \nnode (i.e, its targets are fully resolved, as explained in \u00a72.2), and all potential callees are non-external \nfunctions (Line (1) in Figure 8). Such a call site may become resolved if the function passed to a function \npointer argument becomes known (typically, in some caller of the function containing the indirect call). \nFor example, the call to FP cannot be resolved within the function do all,but will be resolved in the \nBU graph for the function addGToList, where we conclude that it is a call to addG. Then, we clone and \nmerge the indirect callee s BU graph into the graph of the function where the call site became resolved, \nusing resolveCallee just as before (Line (2) in Figure 8). This technique of resolving call nodes as \ntheir function pointer targets are completed effectively discovers the call-graph on the .y, and preserves \ncontext-sensitivity of the analysis because the different function pointer may resolve to different callees \nin different contexts. We record the call graph as it is discovered for use in the TD pass. Note that \nthe function containing the original call still has the unresolved call node in its BU graph (and so \ndo intervening func\u00adtions into which the call node was inlined). We do not re-visit these  void (list*, \nvoid (int*)*): GC  void (int*): G  void: do_all addG (a) Local addGToList graph  int: GR Global \n list: MR list: R void (int*): GC addG list* int list*  (b) After inlining do all (c) Final BU graph \nFigure 7. Construction of the BU DS graph for addGToList functions to resolve the call node because that \nwould lose context\u00adsensitivity of the BU graph information; those call nodes will even\u00adtually be resolved \nin the top-down phase. Conceptually, the BU graph for a function acts like a procedure-summary that is \nused to resolve the effects of the function in different calling contexts. The BU graph for the function \nwhere the call was resolved now fully incorporates the effect of the call. For example, inlining the \nBU graph of addG into that of addGToList yields the .nished graph shown in Figure 7(c). The Modi.ed .ag \nin the node pointed to by L is obtained from the node EV (X) from addG (Figure 3). This graph for addGToList \nis identical to that which would have been obtained if addG was .rst inlined into do all (eliminating \nthe call node) and the resulting graph was then inlined into addGToList. After the cloning and merging \nfor a function is done, we identify newly complete nodes (Line (5)) and remove unreachable nodes from \nthe graph (Line (6)). 3.3.2 Recursion without Function Pointers To handle recursion, we essentially \napply the bottom-up process described above but on Strongly Connected Components (SCCs) of the call graph, \nhandling each multi-node SCC separately. The overall Bottom-Up analysis algorithm is shown in Figure \n8. DSA uses Tarjan s linear-time algorithm to .nd and visit Strongly Con\u00adnected Components (SCCs) in \nthe call graph in postorder. For each SCC, all calls to functions outside the SCC are .rst cloned and \nresolved as before, as shown on lines (1) and (2) in Figure 8 (these functions will already have been \nvisited because of the postorder traversal over SCCs). Once this step is complete, the only call nodes \nin the functions in the SCC are for intra-SCC calls and calls to external functions (the latter are ignored \nthroughout, because they can never be resolved). Within an SCC, each function will eventually need to \ninline the graphs of all other functions in the SCC at least once (either directly or through the graph \nof a callee). Ana\u00a8ive algorithm can produce an O(n 2) and even exponential number of inlining operations. \nTo avoid this cost, we build a single BU DS Graph for an SCC (instead of each function), giving up context \nsensitivity within an SCC. This is accomplished by lines (3) and (4) of Figure 8, which merges BU graphs, \nthen resolves all intra-SCC call sites (exactly once each) in the context of this single merged graph. \nThe speed bene.ts of this approach are evaluated in Section 5.2. 3.3.3 Recursion with Function Pointers \nThe .nal case is a recursive program with indirect calls. The key dif.culty here is that call edges are \nnot known before-hand because they are discovered incrementally by the algorithm, but some of these call \nedges may induce new cycles, and hence new SCCs, in the call graph. We make a key observation, based \non the proper\u00adties described earlier, that yields a simple strategy to handle such situations: some call \nedges of an SCC can be resolved before dis\u00ad  BottomUpAnalysis(Program P) . Function F . P . BUGraph{F} \n=LocalGraph{F}. Val[F] = 0; NextID = 0 while (. unvisited functions F . P)(visit main .rst if available) \nTarjanVisitNode(F, new Stack) TarjanVisitNode(Function F, Stack Stk) NextID++; Val[F] = NextID; MinVisit \n= NextID; Stk.push(F) . call sites C . BUGraph{F} . known non-external callees FC at C if (Val[FC ]==0) \n(FC unvisited) TarjanVisitNode(FC ,S) else MinVisit = min(MinVisit, Val[FC ]) if (MinVisit == Val[F]) \n(entire SCC is on the Stack) SCC S = { N: N =F . N appears above F on stack }. F . S:Val[F] = MAXINT; \nStk.pop(F) ProcessSCC(S, Stk) ProcessSCC(SCC S, Stack Stk) . Function F . S (1) . resolvable call sites \nC . BUGraph{F} (see text) . known callees FC at C if (FC ./S)(Process funcs not in SCC) (2) ResolveCallee(BUGraph{FC \n}, BUGraph{F}, FC , CS) (3) SCCGraph = BUGraph{F0},for some F0 . S . Function F . S, F .(Merge all BUGraphs \nof SCC)  =F0 cloneGraphInto(BUGraph{F}, SCCGraph) BUGraph{F} = SCCGraph (4) . resolvable call sites \nC . SCCGraph (see text) . known callees FC at C (Note: FC . S) ResolveArguments(SCCGraph, FC , CS) \n(5) MarkCompleteNodes() -Section 3.2  (6) remove unreachable nodes (7) if (SCCGraph contains new resolvable \ncall sites) . F . S: Val[F]=0 (mark unvisited) TarjanVisitNode(F0, Stk), for some F0 . S (Re-visit SCC) \nFigure 8. Bottom-Up Closure Algorithm covering that they form part of an SCC. When the call site closing \nthe cycle is discovered (say in the context of a function F0), the effect of the complete SCC will be \nincorporated into the BU graph for F0 though not the graphs for functions in the SCC that were handled \nearlier. Based on this observation, we extended Tarjan s algorithm to revisit the functions in an SCC \nwhen it is discovered (but visiting only unresolved call sites in it). After the current SCC is fully \nprocessed (i.e., after step (6) in Figure 8), we check whether the SCC graph contains any newly inlined \ncall nodes that are now resolvable. If so, we reset the Val entries for all functions in the SCC, which \nare used in TarjanVisitNode to check if a node has been visited. This causes all the functions in the \ncurrent SCC to be revisited, but only the new call sites are processed (since other resolvable call sites \nhave already been resolved, and will not be included in steps (1) and (4)). For example, consider the \nrecursive call graph shown in Fig\u00adure 9(a), where the call from E to C is an indirect call. Assume this \ncall is resolved in function D, e.g., because D passes C explic\u00aditly to E as a function pointer argument. \nSince the edge E . C is unknown when visiting E, Tarjan s algorithm will .rst discover the SCCs { F }, \n{ E },and then { D } (Figure 9(c)). Now, it will .nd a new call node in the graph for D, .nd it is resolvable \nas a call to C,and mark D as unvisited (Figure 9(b)). This causes Tarjan s algorithm to visit the phantom \nedge D . C, and therefore to discover the partial SCC { B, D,C }. After processing this SCC, no new call \nnodes are discovered. At this point, the BU graphs for B, D and C will all correctly re.ect the effect \nof the call from E to C, but the graph for E will not2. The TD pass will resolve the call from E to C \n(within E) by merging the graph for D into E.Note that even in this case, the algorithm only resolves \neach callee at each call site once: no iteration is required, even for SCCs induced by indirect calls. \nFigure 10 shows the BU graph calculated for the main func\u00adtion of our example. This graph has disjoint \nsubgraphs for the lists pointed to by X and Y . These were proved disjoint be\u00adcause we cloned and then \ninlined the BU graph for each call to addGToList(). This shows how context sensitivity with heap cloning \ncan identify disjoint data structures, even when complex pointer manipulation, indirect calls and recursion \nare involved (and despite uni.cation and .ow-insensitivity).   int: GMRC Global  list: HMRC list* \nint Figure 10. Finished BU graph for main  3.4 Top-Down Analysis Phase The Top-Down construction phase \nis very similar to the Bottom-Up construction phase, and the detailed pseudo-code is omitted here but \nshown in [18]. The BU phase has already identi.ed the call graph, so the TD phase can traverse the SCCs \nof the call graph directly using Tarjan s algorithm; it does not need to re\u00advisit SCCs as the BU phase \ndoes. Note that some SCCs may have been visited only partially in the BU phase, so the TD phase is responsible \nfor merging their graphs. Overall, the TD phase differs from the BU phase in only 4 ways: First, the \nTD phase never marks an SCC as unvisited as explained above: it uses the call edges discovered and recorded \nby the BU phase. Second, the TD phase visits SCCs of the call graph in reverse postorder instead of postorder. \nThird, the Top-Down pass merges each function s graph into that of each of its callees (rather than the \nreverse), and it does so directly: it never needs to defer this inlining operation since the potential \ncallees at each call site are known. The .nal difference is that formal argument nodes are marked complete \nif all callers of a function have been identi.ed by the analysis, i.e., the function is not accessible \nto any external functions. Similarly, global variables are marked complete unless they are accessible \nto external functions.  3.5 Bounding Graph Size In the common case, the merging behavior of the uni.cation \nalgo\u00adrithm we use prevents individual data structure graphs from blow\u00ading up, and in fact, keeps them \nvery compact. This occurs whenever a data structure is processed by a loop or recursion because, in ei\u00adther \ncase, a common variable must point to instances of objects in successive iterations or successive calls. \nUni.cation then forces these objects to be merged. In contrast, subset-based analyses can easily generate \nexponentially large graphs [34]. Nevertheless, the combination of .eld sensitivity and cloning makes \nit theoretically possible for our algorithm to build data struc\u00adture graphs that are exponential in the \nsize of the input program. Such cases can only occur if the program builds and processes a data structure \nusing non-loop, non-recursive code, and are thus ex\u00adtremely unlikely to occur in practice. 2 Nor should \nit. A different caller of E may cause the edge to be resolved to a different function, thus the BU graph \nfor E does not include information about a call edge which is not necessarily present in all calling \ncontexts. 1. { F } 2. { E } 3. { D }: mark unvisited 4. { B, D, C } 5. { A }  (a) Recursive Call \nGraph (indirect (b) Call Node Edges, After (c) SCC visitation order call E . C is dotted) inlining F \n&#38; E Figure 9. Handling recursion due to an indirect call in the Bottom-Up phase Using a technique \nlike k-limiting [17] to guard against such unlikely cases is unattractive because it could reduce precision \nfor reasonable data structures with paths more than k nodes long. Instead, we propose that implementations \neither simply impose a hard limit on graph size (e.g., 10,000 nodes, which is larger than real programs \nare likely to need), or use a smarter heuristic which monitors and limits the growth of graphs due to \ncloning. If this limit is exceeded, node merging can be used to reduce the size of the graph. Our results \nin Section 5 show that the maximum function graph size we have observed in practice across a wide range \nof programs is only 3196 nodes, and this maximum is only weakly correlated to program size (in fact, \nin our benchmarks, it is solely determined by one large call-graph SCC, because the graphs for functions \nin the SCC are merged). Maximum DS graph size is only 278 in the Linux kernel and only 401 in perlbmk \nwhich had the next-largest call-graph SCC after the two versions of gcc.  3.6 Complexity Analysis The \nlocal phase adds at most one new node, EV entry, and/or edge for each instruction in a procedure (before \nnode merging). Furthermore, node merging or collapsing only reduces the number of nodes and edges in \nthe graphs. We have implemented node merging using Tarjan s union-.nd algorithm [32], which ensures that \nthis phase requires O(na(n, n)) time and O(n) space for a program containing n instructions in all [31]. \na(n,n) is the inverse Ackerman s function. The BU and TD phases operate on DS graphs directly, so their \nperformance depends on the size of the graphs being cloned and the time to clone and merge each graph. \nWe denote these by K and l respectively, where l is O(Ka(K, K)) in the worst case. They also depend on \nthe average number of out-edges in the call graph per function, denoted c.If there are f functions, then \ne = fc is simply the total number of edges in the call graph. For the BU phase, each function must inline \nthe graphs for c callee functions, on average. Because each inlining operation requires l time, the time \nrequired is fcl = Ka(K)e. The call sites within an SCC do not introduce additional complexity, since \nevery potential callee is again inlined only once into its caller within or outside the SCC (in fact, \nthese are slightly faster because only a single graph is built, causing common nodes to be merged). Thus, \nthe time to compute the BU graph is T(Ka(K)e). The space required to represent the Bottom-Up graphs is \nT(fK).The TD phase is identical in complexity to the BU phase. Putting these together, the worst case \ntime and memory com\u00adplexity are T(na(n)+ Ka(K)e),and T(fK),  4. Major Engineering Choices Through our \nexperience scaling to increasingly larger programs over time, we have repeatedly found that several O(N2) \naspects of the algorithm were the main bottlenecks to scalability. In con\u00adtrast, we have never seen any \nsigni.cant combinatorial growth from cloning: uni.cation has been successful at preventing this problem. \nTo scale DSA to large programs, we have devised engineering so\u00adlutions to improve these N2 problems, \nmany of which should be applicable to other interprocedural heap or pointer analysis algo\u00adrithms. The \nspeedups achieved by these techniques are evaluated in Section 5.2. The Globals Graph: In the algorithm \nso far, global variables accessed anywhere in the program would propagate bottom-up to main, then top-down \nto all functions in the program, ballooning graph size by a factor that grows as O(N2). A key optimization \nwe add to DSA is to use a separate Globals Graph to hold information about global nodes and all nodes \nreachable from global nodes. We can then remove global variables from a function s graph if they are \nnot used in the current function (even though they may be used in callers or callees of that function). \nFor example, this eliminates the G nodes in graphs of all functions except addG (and main). Both Ruf \n[28] and Liang and Harrold [23] use a somewhat similar technique, but they do not motivate it primarily \nas an optimization and do not evaluate its impact on speed. We refer the reader to [18] for the detailed \nsteps we use. Shrinking EV with Global Equivalence Classes: Even with the above re.nement, programs that \nuse large global arrays of pointers to globals can be problematic (e.g. an array of pointers to strings). \nThe EV entries for the target globals are replicated in every function that accesses any one of those \nglobals. Since DSA can never disambiguate these globals, we solved this by keeping only one representative \nglobal in each DS node (and removing the rest from each graph s EV as well). In programs with many globals, \nthis replaces O(N2) entries with O(N) in EV . Ef.cient Graph Inlining: The version of function clone-GraphInto \nshown in Figure 4 sometimes proved very slow in prac\u00adtice because it allocates and copies many nodes, \nonly to discard them soon after creation. To solve this issue, we now inline graphs using a recursive \ntraversal (of both graphs) from the common point\u00aders, only visiting nodes that will actually be re.ected \ninto the target graph. This is possible because uni.cation ensures a 1-1 mapping of paths in the two \ngraphs, though there may be a many-to-one mapping of nodes from source to target. Ruf used a similar \ntech\u00adnique, although the bene.t wasn t evaluated [28].  5. Experimental Results We implemented the DSA \nalgorithm in the LLVM Compiler Infras\u00adtructure [19]. The analysis is performed at link-time, using stubs \nfor C library functions while treating unknown external functions conservatively. We have successfully \nused DSA for optimizations and/or memory safety for a wide range of programs and for the Linux kernel \n[18, 20, 21, 9, 8], and LLVM is used by several com\u00admercial organizations. We present results evaluating \nDSA on several benchmark suites: the ptrdist 1.1 benchmarks, the SPEC 1995 and SPEC 2000 inte\u00adger and \n.oating point benchmarks (for those Fortran programs that we were able to convert successful to C using \nf2c), a few unbundled programs, and the Linux 2.4.22 kernel. povray31 includes sources for the zlib and \nlibpng libraries. The Linux kernel includes only a few drivers, but includes many other modules that \nwould normally be separately compiled and loaded, including many .le system and       Code Size \n Analysis Time (sec) Mem (KB)  #ofNodes max Coll- Benchmark LOC  MInsts |SCC| Local BU TD Total \nBU TD Total Max apsed ptrdistanagram 647 271 1 0.00 0.00 0.00 0.01 111 89 163 18 11 ptrdistks 684 546 \n1 0.00 0.00 0.00 0.01 97 68 207 24 0 ptrdistft 1301 433 1 0.00 0.00 0.00 0.01 112 86 150 14 0 ptrdistyacr2 \n3212 1621 1 0.01 0.01 0.01 0.02 222 212 369 17 0 ptrdistbc 6627 3729 1 0.01 0.02 0.02 0.05 591 408 \n738 29 16      130.li 7598 7894 24 0.03 0.09 0.04 0.16 1948 933 806 33 328 124.m88ksim 19233 7951 \n2 0.03 0.03 0.02 0.08 1334 774 1796 56 195 132.ijpeg 28178 12507 1 0.03 0.02 0.02 0.07 1453 935 1531 \n65 62 099.go 29246 20543 1 0.04 0.02 0.04 0.10 1299 906 2298 131 0 134.perl 26870 29940 19 0.08 0.12 \n0.06 0.26 2038 1201 1463 232 136 147.vortex 67211 37632 23 0.09 0.14 0.07 0.30 2785 1678 3529 355 242 \n126.gcc 205085 129083 255 0.44 1.87 0.37 2.68 10982 6586 12226 3046 1109 145.fpppp 2784 4447 2 0.01 \n 0.01 0.01 0.03 472 261 623 48 43 104.hydro2d 4292 5773 2 0.02 0.02 0.01 0.05 672 384 688 48 88 110.applu \n3868 5854 2 0.01 0.02 0.00 0.03 518 289 583 57 19 103.su2cor 2332 6450 2 0.02 0.02 0.01 0.05 759 437 \n1080 160 49 146.wave5 7764 11333 2 0.03 0.01 0.01 0.05 908 521 1171 70 164 181.mcf 2412 991 1 0.00 \n0.00 0.01 0.01 76 53 103 49 0 256.bzip2 4647 1315 1 0.01 0.00 0.00 0.01 131 80 205 76 3 164.gzip 8616 \n1785 1 0.00 0.01 0.00 0.01 217 127 290 60 1 175.vpr 17728 8972 1 0.03 0.02 0.01 0.06 969 614 2106 366 \n118 197.parser 11391 10086 3 0.04 0.03 0.03 0.1 1301 758 1291 109 121 186.crafty 20650 14035 2 0.05 \n 0.03 0.04 0.12 1344 817 2890 701 45 300.twolf 20459 19686 1 0.05 0.01 0.03 0.09 1179 807 2022 411 37 \n255.vortex 67220 37601 23 0.07 0.10 0.09 0.26 2765 1669 3515 392 241 254.gap 71363 47389 9 0.17 0.41 \n0.19 0.77 8040 3837 5889 370 728 252.eon 35819 51897 6 0.24 0.18 0.14 0.56 7818 4450 6936 411 511 253.perlbmk \n85055 98386 250 0.31 2.11 0.24 2.65 8785 3517 2038 401 510 176.gcc 222208 139790 337 0.44 2.38 0.42 \n3.24 11628 7101 12736 3196 1000 168.wupwise 2184 5087 2 0.01 0.01 0.01 0.03 523 302 608 64 33 173.applu \n3980 5966 2 0.02 0.01 0.01 0.04 523 291 593 68 19 188.ammp 13483 10551 1 0.00 0.00 0.01 0.01 865 546 \n897 281 69 177.mesa 58724 43352 1 0.10 0.06 0.07 0.23 4115 2476 3038 98 857 fpgrowth 634 544 1 0.01 \n 0.00 0.01 0.02 58 37 108 49 0 boxed-sim 11641 12287 1 0.02 0.03 0.01 0.06 693 442 480 65 61 NAMD 5312 \n19002 1 0.05 0.02 0.02 0.09 1042 761 1539 224 276 povray31 108273 62734 56 0.18 0.23 0.13 0.54 5582 \n3389 5278 318 732 linux 355384 305073 28 1.35 1.00 0.76 3.10 14498 31232 47348 278 9666 Table 1. Program \ninformation, analysis time, memory consumption, and graph statistics. networking modules. It is complete \nenough for us to use it as a standard con.guration on a modern workstation. Table 1 describes relevant \nproperties of the benchmarks, as well as the experimental results. LOC is the raw number of lines of \nC code for each benchmark, MInsts is the number of memory instructions3 for each program in the LLVM \ninternal representation, and SCC is the size of the largest SCC in the call-graph for the program. We \nomitted eight SPEC programs of less than 2000 lines (101, 102, 107, 129, 171, 172, 179, 183) for lack \nof space, but their results are available in [18].  5.1 Analysis Time &#38; Memory Consumption We evaluated \nthe time and space usage of our analysis on a Linux workstation with an AMD Athlon MP 2100+ processor, \nwhich runs at 1733MHz. We compiled LLVM with GCC 3.4.2 at the -O3 level of optimization. Table 1 shows \nthe running times and memory usage, and graph node statistics for DSA (the Local phase uses little memory \nand is not shown). The six largest programs in our study, vmlinux, 176.gcc, 126.gcc, povray31, 253.perlbmk \nand 254.gap are both fairly large and contain non-trivial SCCs in the call graph. Nevertheless, it takes \nonly between 0.54 and 3.24 seconds to perform the complete algorithm on these programs. To put this into \nperspective, we com\u00ad 3 Memory instructions are load, store, malloc, alloca, call, and structure or array \nindexing instructions. Analysis time and memory usage correlate better with this number than with LOC, \nas shown in [18]. piled the 176.gcc, 253.perlbmk, and povray31 benchmarks with our system GCC compiler \nat the -O3 level of optimization. GCC takes 94.7s, 47.4s, and 38.5s to compile and link these programs, \neven though it does not perform any cross-.le optimizations.DSA s times are only 3.4%, 5.6%,and 1.4% \nof the total GCC compile times for these cases, which we consider low enough to be practi\u00adcal for production \noptimizing compilers. The table shows that memory consumption of DSA is also quite small. The peak memory \n(which is almost exactly equal to BU+TD) consumed for the largest code is less than 46MB,which seems \nvery reasonable for a modern compiler. These numbers are noteworthy because the algorithm is performing \na context-sensitive whole-program analysis with cloning, and memory use can often be the bottleneck in \nscaling such analyses to large programs. The # of Nodes columns show information about the DS graph nodes. \nWe exclude EV entries since those are simple map entries, not actual nodes. Total is the aggregate number \nof nodes in the TD graphs for all functions and Max is the maximum size of any particular function s \ngraph (the term K in the notation of Section 3.6). Most importantly, the table shows that Total as well \nas K both grow quite slowly with total program size, indicating that context-sensitive analysis and heap \ncloning do not cause any blowup in the points-to graphs; this would not have been the case with a subset-based \nanalysis [34]. Collapsed is the total number of TD graph nodes collapsed, which happens due to incompatible \ntypes on merged nodes. In most of the largest programs, roughly about 7-15% of the nodes are collapsed, \nand the most common rea\u00adson appears to be merging of Global nodes, which in some cases causes other nodes \nto be merged. In some cases, however, unrecog\u00adnized custom memory allocators produce signi.cantly higher \nfrac\u00adtions of Collapsed nodes, e.g., 25% in 253.perlbmk and 20% in linux. We have also examined the scaling \nbehavior of the analysis (these graphs are omitted here for lack of space but are available in [18]). \nAcross programs spanning .ve orders of magnitude of pro\u00adgram size, the Local and TD passes take O(n) \ntime, where n is the number of memory operations in the program (column MInsts in the table). The BU \nphase is slightly worse, but mainly for pro\u00adgrams with large numbers of globals such as 254.gap, 253.perlbmk \nand 176.gcc (in other words, number of memory operations does not fully capture the analysis complexity). \nWe believe this behav\u00adior can be further improved with more sophisticated handling of global equivalence \nclasses. 5.2 Importance of Optimizations No equivalence classes No globals graph Unoptimized graph merging \nDirect Call No SCC collapsing 10.2x 21.8x 9.7x 10.7x No optimizations 8x Note that the Linux kernel is \nsubstantially different from other large programs. It has a very large number of globals, which ex\u00adplains \nthe over 10x improvement with global equivalence classes. The globals graph gives a smaller speedup of \nabout 2.3x, perhaps because of shallow call depths (but we have not veri.ed that). Linux also has signi.cantly \nsmaller graphs compared with other programs of similar size. This helps explain the speed of the analysis \non the kernel, and also the unusually small improvement from ef.cient inlining. Perhaps most importantly, \nthese results show that the bene.t of the optimizations is correlated with program size. To show this \ntrend, the Table 2 below lists the bene.ts for the 12 largest pro\u00adgrams by LOC, averaged across groups \nof four. The bene.ts grow strongly with program size, re.ecting the O(N2) nature of the bot\u00adtlenecks \neliminated. Both the use of many globals and having to inline large graphs (due to large SCCs and more \nindirect calls) are more common in complex, larger programs. Avg. LOC Total opt. speedup Largest 4 programs \n280k 10.8x Second largest 4 72k 4.37x Third largest 4 52k 2.74x Table 2. Speedup trend for 12 largest \nprograms Slowdown v.s. Fully Optimized 7x  5.3 Analysis of DSA Precision 6x DSA is powerful in some \nrespects (context sensitivity with heap 5x cloning and .eld-sensitivity) but weak in others (uni.cation \nand 4x .ow insensitivity). We compared the precision of DSA with several other standard pointer analysis \nalgorithms, for several clients, in\u00adcluding alias analysis, interprocedural mod/ref analysis, and a few \n3x 2x data.ow optimizations [18]. We present results for an alias analysis 1x client here and brie.y \nsummarize the mod/ref results. Figure 12 compares the alias analysis precision of DSA against 0x vmlinux254.gap176.gccpovray252.eon253.perlbmksiod255.vortex147.vortex134.perlmean \nFigure 11. Relative Performance Impact of Optimizations To investigate the effectiveness of the major \noptimizations, in\u00ad cluding the engineering ones described in Section 4 and the collaps\u00ad ing of SCCs described \nin Section 3.3.2, we measured the analysis times with each optimization turned off individually and also \nwith four other pointer analysis algorithms: local: a simple/fast lo\u00adcal analysis that traverses SSA \nedges intraprocedurally and in\u00adcludes base-offset analysis; steens-. and steens-fs: implementations of \nSteensgaard s [31] uni.cation-based, context-insensitive algo\u00adrithm, in .eld-insensitive and .eld-sensitive \nversions; and anders: Andersen s [2] context-insensitive, .eld-insensitive algorithm. Ev\u00adery other algorithm \nfalls back (if it answers may alias ) to local, ensuring that any query resolvable by the local algorithm \nwill be resolved by all analyses. This means that comparing against the re\u00ad sults for local alone isolates \nthe actual bene.t each of the more aggressive analyses would provide in a realistic compiler (such all \nthese optimizations off. This shows the incremental speedup each optimization provides in the presence \nof all the others,as well as the overall speedup. In Figure 11, we show results for the 10 longest-running \nbench\u00ad marks in terms of unoptimized analysis time, and the average across these 10 benchmarks. The data \nfor each program are normalized to the runtime of the fully optimized analysis, i.e, a number larger \nthan  1.0 is a slowdown. Overall, the speedups range from 2x to almost 16x in most cases. The two most \nimportant optimizations, on average, are equivalence classes of globals, and inlining only nodes reachable \nin the result graph. The former reduces the size of the nodes and the size of the lookup tables mapping \nIR variables to nodes. The latter chaining of alias analyses has been shown to be important in prac\u00ad \n.ow-insensitive algorithms (all the others) cannot. pointer variables (which include intermediate pointers \nobtained by indexing into a struct or array) within each function in a program and counts the number \nof queries that return may alias (i.e., can\u00ad not be proven to be no alias or must alias ). A lower percentage \ntherefore corresponds to a more precise analysis. Because this eval\u00ad uates alias pairs within a function, \nit approximates the use of an reduces the number of nodes allocated during inlining as well as nodes \nwalked during unreachable node elimination. Each of these optimizations individually produces speedups \nof about 1.5x-10x, and on average just over 3x. abstract intraprocedural optimization client. Figure \n12 focuses on Brie.y, the major .ndings are: Using a separate globals graph is second best for several \npro\u00ad grams; it reduces the size of each graph, and makes graph inlining The alias disambiguation precision \nof DSA is comparable faster. It gives speedups ranging from 1.0x to 3.5x, and on average to Andersens \nalgorithm in many cases (256.bzip2, 164.gzip, over 2x. The two optimizations related to globals reduce \nmemory 183.equake, 176.gcc, etc). As expected, there are cases where usage considerably, e.g., by 67% \nand 55% respectively for 176.gcc. uni.cation reduces precision (197.parser, 255.vortex), but the 181.mcf256.bzip2164.gzip \n 175.vpr197.parser186.crafty300.twolf255.vortex254.gap252.eon253.perlbmk176.gccInt_Mean179.art183.equake171.swim172.mgrid168.wupwise173.applu188.ammp177.mesaFP_Mean \nFigure 12. Percentage of alias queries that return May Alias (lower is more precise) difference is quite \nsmall in both cases4. There are many more cases (181.mcf, 175.vpr, 186.crafty, 254.gap, 300.twolf, 188.ammp, \n177.mesa, etc) where the combination of context sensitivity and uni.cation are signi.cantly more precise \nthan non-context-sensitive subset-based analysis. This is partly be\u00adcause (as other researchers [5, 11] \nhave shown) bidirectional argument binding is the leading cause of precision loss in a uni.cation-based \nanalysis. This problem can be solved by using either context sensitivity or a subset-based analysis. \nConversely, the context sensitivity can help precision signi.cantly in some cases that are not handled \nwell by subset-based analyses. Using a cloning-based context-sensitive analysis can yield far more accurate \npoints-to results than using a static naming scheme for heap and stack objects. The effect is most pro\u00adnounced \nin programs that use a large amount of heap allocated data and have few static allocation sites. For \nexample, 175.vpr, 300.twolf, and 252.eon have simple wrapper functions around malloc that would prevent \nordinary context-insensitive algo\u00adrithms from disambiguating many pairs of heap references. Fur\u00adther \nexperiments are needed to determine how deep the context\u00adsensitivity must go to achieve these results \n[26].  Comparing steens-. to steens-fs, we see that .eld sensitivity substantially improves the precision \nof uni.cation-based analy\u00adsis only in rare cases (188.ammp), but generally has only a very small effect. \nThe most important reason is that the local analy\u00adsis captures many common cases of .eld sensitivity \nwith sim\u00adple base/offset disambiguation, e.g., two references to different .elds off the same pointer. \nField sensitivity may be more im\u00adportant when combined with context-sensitivity, because there is greater \nlikelihood of disambiguating pointers to heap objects, but we have not evaluated that hypothesis.  In \nour study of precision for interprocedural mod/ref analysis, the results show that DSA (like any context-sensitive \nalgorithm) produces signi.cantly better mod/ref information than Andersen s algorithm. For example, in \n24 of the 40 programs in that study, DSA returns NoModRef 40% more often than Andersen s [18]. Finally, \nwe have also evaluated how effective DSA is in iden\u00adtifying linked data structure instances and their \nproperties. We did this by manually inspecting the DS graphs and correlating them 4 Note that this compares \na .eld-sensitive algorithm (DSA) with a .eld-insensitive one (Andersen s) and so we cannot draw any de.nite \nconclusions comparing context\u00adsensitive with subset-based analysis. Our goal here is simply to show that \nDSA is reasonably precise compared with state-of-the-art interprocedural analyses. with source code information \nfor a few of our benchmarks [24]. The results showed that the algorithm is usually successful in identify\u00ading \ndifferent kinds of linked data structures as well as their type and lifetime information, and is sometimes \nsuccessful at distinguishing instances of such structures. The most common reason for failure in the \nlatter is due to lack of .ow-sensitivity.  6. Related Work There is a vast literature on pointer analysis \n(e.g., see the survey by Hind [17]), but the majority of that work focuses on context\u00adinsensitive analyses. \nWe focus here on context-sensitive techniques. Several algorithms are context-sensitive in the analysis \nbut not the heap naming: they name heap objects by allocation site and not by call path. Liang and Harrold \ns FICS algorithm processes programs up to 25K lines of code in a few seconds each [22]. Fahndrich et \nal [11] analyze programs as large as 200K lines of code (the Spec95 version of gcc) in about 3 minutes. \nThe Whaley-Lam algorithm takes up to 19 minutes for one program and over 10 minutes for four out of twenty \nprograms [34]. It also potentially considers all possible acyclic call paths (e.g., they report 1024 \npaths in one program). Both of these raise concerns for production compilers. Conversely, the GOLF algorithm \n[6], which is both fast and scalable, uses only one level of context-sensitivity. The key question is \nwhether full cloning of heap objects, which we term full context sensitivity, can achieve similar speed \nand scalability. The most similar algorithm to ours is called MoPPA [23]. It is also .ow-insensitive \nand context-sensitive, uses uni.cation, and its structure is similar to our algorithm, including Local, \nBottom-Up, and Top-Down phases, and uses a separate Globals Graph. MoPPA seems to require much higher \nmemory than our algorithm: it runs out of memory analyzing povray3 with .eld-sensitivity on a ma\u00adchine \nwith 640M of memory. It has several other practical limita\u00adtions: it can only use .eld-sensitivity for \ncompletely type-safe pro\u00adgrams, requires a complete program, and requires a precomputed call-graph. We \navoid these limitations via .ne-grain completeness tracking and the use of call nodes for incompletely \nresolved calls. Ruf s synchronization removal algorithm for Java [28] also shares several important properties \nwith ours and with MoPPA, including combining context-sensitivity with uni.cation, a non\u00aditerative analysis \nwith local, bottom-up and top-down phases, and node .ags to mark global nodes. Unlike our algorithm, \nhis work requires a call graph to be speci.ed, it is limited to type-safe pro\u00adgrams, and does not appear \nto handle incomplete programs. A few papers describe context-sensitive algorithms with full heap cloning \nwithout using uni.cation [3, 25]. Although the lat\u00adter [25] has been shown to scale to quite large programs, \nwhich is an impressive result for a non-uni.cation-based context-sensitive analysis, it does not provide \nconvincing evidence that the technique could be used in production compilers. The work of Nystrom et \nal. is about 2 orders of magnitude slower than ours in absolute compile time, e.g., 192 s. for 176.gcc \ncompared with 3.24 seconds for ours, measured on similar systems. 7. Conclusion The experimental results \nin this paper provide strong evidence that a context-sensitive points-to analysis with full heap cloning \n(by acyclic call paths) can be made ef.cient and practical for use in pro\u00adduction compilers. Our algorithm \nrequires only a few seconds for programs of 100-350K lines of code and scales well across .ve or\u00adders \nof magnitude of program size. Some of the key technical ideas emerging from this work are the careful \ndesign choices to con\u00adstruct the call graph incrementally during the analysis, to eliminate O(N2) behaviors, \nand to track .ne-grained completeness informa\u00adtion as a uni.ed solution for several dif.cult algorithmic \nproblems. Despite important simpli.cations for scalability (uni.cation and .ow-insensitivity), the analysis \nis powerful enough to provide bet\u00adter alias analysis precision than Andersen s algorithm in several programs. \nDSA can distinguish instances of logical data structures in programs and provide useful compile-time \ninformation about such data structures, without an expensive shape analysis. In fact, DSA, in combination \nwith Automatic Pool Allocation [20], enables novel analyses and transformations that can operate on entire \nrecur\u00adsive data structures instead of individual loads and stores or data elements [18]. References \n[1] LLVM Link Time Optimization: Design and Implementation. http://llvm.org/docs/LinkTimeOptimization.html. \n [2] L.O.Andersen. Program Analysis and Specialization for the C Programming Language. PhD thesis, DIKU, \nUniversity of Copenhagen, May 1994. [3] B.-C. Cheng and W. mei Hwu. Modular interprocedural pointer \nanalysis using access paths: Design, implementation, and evaluation. In PLDI, Vancouver, British Columbia, \nCanada, June 2000. [4] S. Chong and R. Rugina. Static analysis of accessed regions in recursive data \nstructures. In SAS, 2003. [5] M. Das. Uni.cation-based pointer analysis with directional assign\u00adments. \nIn PLDI, pages 35 46, 2000. [6] M. Das, B. Liblit, M. F\u00a8ahndrich, and J. Rehof. Estimating the impact \nof scalable pointer analysis on optimization. In SAS, 2001. [7] A. Deutsch. Interprocedural may-alias \nanalysis for pointers: Beyond k-limiting. In PLDI, pages 230 241, June 1994. [8] D. Dhurjati, S. Kowshik, \nand V. Adve. SAFECode: Enforcing alias analysis for weakly typed languages. In PLDI, June 2006. [9] D. \nDhurjati, S. Kowshik, V. Adve, and C. Lattner. Memory safety without garbage collection for embedded \napplications. ACM Trans. on Embedded Computing Systems, Feb. 2005. [10] M. Emami, R. Ghiya, and L. J. \nHendren. Context-sensitive interprocedural points-to analysis in the presence of function pointers. In \nPLDI, pages 242 256, Orlando, FL, June 1994. [11] M. F\u00a8ahndrich, J. Rehof, and M. Das. Scalable context-sensitive \n.ow analysis using instantiation constraints. In PLDI, 2000. [12] J. S. Foster, M. F\u00a8ahndrich, and A. \nAiken. Polymorphic versus monomorphic .ow-insensitive points-to analysis for c. In Proc. Int l Symp. \non Static Analysis (SAS), London, UK, 2000. [13] R. Ghiya and L. J. Hendren. Connection analysis: A practical \ninterprocedural heap analysis for C. International Journal of Parallel Programming, 24(6):547 578, 1996. \n[14] R. Ghiya and L. J. Hendren. Is it a tree, a DAG, or a cyclic graph? A shape analysis for heap-directed \npointers in C. In POPL, 1996. [15] R. Ghiya, D. Lavery, and D. Sehr. On the importance of points-to analysis \nand other memory disambiguation methods for C programs. In PLDI, 2001. [16] B. Hackett and R. Rugina. \nRegion-based shape analysis with tracked locations. In POPL, pages 310 323, New York, NY, USA, 2005. \n[17] M. Hind. Pointer analysis: Haven t we solved this problem yet? In PASTE, 2001. [18] C. Lattner. \nMacroscopic Data Structure Analysis and Optimization. PhD thesis, Comp. Sci. Dept., Univ. of Illinois, \nUrbana, IL, May 2005. [19] C. Lattner and V. Adve. LLVM: A Compilation Framework for Lifelong Program \nAnalysis and Transformation. In Int l Symp. on Code Generation and Optimization, Mar 2004. [20] C. Lattner \nand V. Adve. Automatic pool allocation: Improving performance by controlling data structure layout in \nthe heap. In PLDI, Chicago, IL, Jun 2005. [21] C. Lattner and V. Adve. Transparent Pointer Compression \nfor Linked Data Structures. In MSP, Chicago, IL, Jun 2005. [22] D. Liang and M. J. Harrold. Ef.cient \npoints-to analysis for whole\u00adprogram analysis. In ESEC, 1999. [23] D. Liang and M. J. Harrold. Ef.cient \ncomputation of parameterized pointer information for interprocedural analysis. In SAS 2001,July 2001. \n[24] P. Meredith, B. Pankaj, S. Sahoo, C. Lattner, and V. Adve. How successful is data structure analysis \nin isolating and analyzing linked data structures? Tech. Report UIUCDCS-R-2005-2658, Computer Science \nDept., Univ. of Illinois at Urbana-Champaign, Nov 2005. [25] E. M. Nystrom, H.-S. Kim, and W. mei W. \nHwu. Bottom-up and top-down context-sensitive summary-based pointer analysis. In SAS 2004, 2004. [26] \nE. M. Nystrom, H.-S. Kim, and W. mei W. Hwu. Importance of heap specialization in pointer analysis. In \nProc. ACM SIGPLAN-SIGSOFT Workshop on Program Analysis for Software Tools and Engineering (PASTE), pages \n43 48, New York, NY, USA, 2004. [27] R. O Callahan and D. Jackson. Lackwit: a program understanding tool \nbased on type inference. In ICSE 97: Proceedings of the 19th international conference on Software engineering, \npages 338 348, New York, NY, USA, 1997. ACM Press. [28] E. Ruf. Effective synchronization removal for \njava. In PLDI,pages 208 218, 2000. [29] M. Sagiv, T. Reps, and R. Wilhelm. Solving shape-analysis problems \nin languages with destructive updating. TOPLAS, 20(1), Jan. 1998. [30] B. Steensgaard. Points-to analysis \nby type inference of programs with structures and unions. In Compiler Construction, pages 136 150, London, \nUK, 1996. [31] B. Steensgaard. Points-to analysis in almost linear time. In POPL, 1996. [32] R. E. Tarjan. \nEf.ciency of a good but not linear set union algorithm. J. ACM, 22(2):215 225, 1975. [33] F. Vivien and \nM. Rinard. Incrementalized pointer and escape analysis. In PLDI, pages 35 46, 2001. [34] J. Whaley and \nM. S. Lam. Cloning-based context-sensitive pointer alias analysis using binary decision diagrams. In \nPLDI, 2004. [35] R. P. Wilson and M. S. Lam. Effective context sensitive pointer analysis for C programs. \nIn PLDI, pages 1 12, June 1995.  \n\t\t\t", "proc_id": "1250734", "abstract": "<p>Context-sensitive pointer analysis algorithms with full \"heapcloning\" are powerful but are widely considered to be too expensive to include in production compilers. This paper shows, for the first time, that a context-sensitive, field-sensitive algorithm with fullheap cloning (by acyclic call paths) can indeed be both scalable and extremely fast in practice. Overall, the algorithm is able to analyze programs in the range of 100K-200K lines of C code in 1-3 seconds,takes less than 5% of the time it takes for GCC to compile the code (which includes no whole-program analysis), and scales well across five orders of magnitude of code size. It is also able to analyze the Linux kernel (about 355K linesof code) in 3.1 seconds. The paper describes the major algorithmic and engineering design choices that are required to achieve these results, including (a) using flow-insensitive and unification-basedanalysis, which are essential to avoid exponential behavior in practice;(b) sacrificing context-sensitivity within strongly connected components of the call graph; and (c) carefully eliminating several kinds of O(<i>N</i><sup>2</sup>) behaviors (largely without affecting precision). The techniques used for (b) and (c) eliminated several major bottlenecks to scalability, and both are generalizable to other context-sensitive algorithms. We show that the engineering choices collectively reduce analysis time by factors of up to 10x-15xin our larger programs, and have found that the savings grow strongly with program size. Finally, we briefly summarize results demonstrating the precision of the analysis.</p>", "authors": [{"name": "Chris Lattner", "author_profile_id": "81100275287", "affiliation": "Apple Inc., Cupertino, CA", "person_id": "P449180", "email_address": "", "orcid_id": ""}, {"name": "Andrew Lenharth", "author_profile_id": "81331497544", "affiliation": "University of Illinois at Urbana-Champaign, Urbana, IL", "person_id": "P871665", "email_address": "", "orcid_id": ""}, {"name": "Vikram Adve", "author_profile_id": "81100524180", "affiliation": "University of Illinois at Urbana-Champaign, Urbana, IL", "person_id": "P291197", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1250734.1250766", "year": "2007", "article_id": "1250766", "conference": "PLDI", "title": "Making context-sensitive points-to analysis with heap cloning practical for the real world", "url": "http://dl.acm.org/citation.cfm?id=1250766"}