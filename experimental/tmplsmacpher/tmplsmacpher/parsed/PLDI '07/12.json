{"article_publication_date": "06-10-2007", "fulltext": "\n Automatic Inference of Optimizer Flow Functions from Semantic Meanings Erika Rice Scherpelz Sorin Lerner \nCraig Chambers Google UC San Diego University of Washington erikars@google.com lerner@cs.ucsd.edu chambers@cs.washington.edu \nAbstract Previous work presented a language called Rhodium for writing program analyses and transformations, \nin the form of declarative .ow functions that propagate instances of user-de.ned data.ow fact schemas. \nEach data.ow fact schema speci.es a semantic meaning, which allows the Rhodium system to automatically \nver\u00adify the correctness of the user s .ow functions. In this work, we have reversed the roles of the \n.ow functions and semantic mean\u00adings: rather than checking the correctness of the user-written .ow functions \nusing the facts semantic meanings, we automatically in\u00adfer correct .ow functions solely from the meanings \nof the data.ow fact schemas. We have implemented our algorithm for inferring .ow functions from fact \nschemas in the context of the Whirlwind compiler, and have used this implementation to infer .ow func\u00adtions \nfor a variety of fact schemas. The automatically generated .ow functions cover most of the situations \ncovered by an earlier suite of handwritten rules. Categories and Subject Descriptors D.3.4 [Programming \nLan\u00adguages]: Processors compilers, optimization General Terms Languages, Algorithms, Reliability 1. \nIntroduction Compilers are an important part of the computing infrastructure relied upon by software \ndevelopers. Of course, developers require their compilers to be correct, i.e., to translate a source \nprogram into a target program that has the same externally visible behavior; an incorrect compiler can \nfatally compromise any guarantees made by the source program. In addition, developers need their compilers \nto produce ef.cient target programs. As the level of the source pro\u00adgram moves farther above the level \nof the target program, increas\u00ading demands are placed on the compiler s optimizer to achieve this ef.ciency. \nUnfortunately, it is hard to develop an optimizer that is both correct and that produces ef.cient target \ncode. When devel\u00adoping an optimization, it is dif.cult to reason about the possible effects of all the \ndifferent kinds of statements in the compiler s in\u00adtermediate representation, and it is hard to develop \ntest suites that adequately exercise all the combinations of statements and probe the corner cases. Moreover, \nan optimizer is comprised of a col- Permission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior \nspeci.c permission and/or a fee. PLDI 07 June 11 13, 2007, San Diego, California, USA. Copyright c . \n2007 ACM 978-1-59593-633-2/07/0006. . . $5.00 lection of optimizations, which can interact in subtle \nways, further exploding the test space. As a result, it often takes years of testing and use for an optimizing \ncompiler to become mature and reliable, and even so it can still contain numerous, rarely-occurring bugs. \nAside from having an impact on software reliability in general, the dif.culty of writing correct optimizing \ncompilers also hinders the development of new languages and new architectures, and it dis\u00ad courages end \nprogrammers from extending compilers with domain\u00ad speci.c checkers or optimizers. Our broad research \nagenda is to provide better support for writing correct, ef.cient optimizers. Previously we presented \na language called Rhodium for writing compiler optimizations that could be checked for correctness automatically \n[17, 16]. Rhodium users (i.e., compiler developers) declare what kinds of information they want to compute \nusing data.ow fact schemas, and then they write propagation rules (a.k.a. .ow functions) for generating \nand propagating instances of these fact schemas across intermediate\u00ad language statements, as well as \ntransformation rules for using in.owing facts to optimize statements. By providing specialized support \nfor the task of writing optimizations, Rhodium makes op\u00ad timizations easier to write and maintain, which \nhelps in reducing errors. Moreover, the restricted domain makes Rhodium amenable to rigorous static checking \nthat would otherwise be infeasible. In particular, the user provides a simple semantic meaning for each \ndata.ow fact schema, which the Rhodium system uses to automat\u00ad ically prove that the user-de.ned propagation \nand transformation rules are correct. Although Rhodium makes it easier to write data.ow analyses and \noptimizations, the user still has to invest a signi.cant amount of effort writing propagation rules for \neach kind of fact and for each kind of statement. In this paper, we present a technique for auto\u00ad matically \ninferring correct forward propagation rules given only a set of declared data.ow fact schemas and their \nassociated seman\u00ad tic meanings. In the case of constant propagation, for example, the user would only \ndeclare the fact schema hasConstValue(X :Var,C :Const) with meaning s(X)=s(C)  westhatadata.owfactoft \nhichspeci.heformhasConstValue(X,C), where X is a variable and C is a constant, can be correctly prop\u00adagated \nto a control .ow graph edge if, for every possible run-time program state s that can exist when control \nreaches that edge, the result of evaluating X equals the result of evaluating C.Our system would then \ncombine the user-de.ned semantic meaning with the system-provided semantics of each intermediate language \nstatement form to automatically infer correct propagation rules for this fact schema. As a result, the \nuser need not be burdened with writing propagation rules, making it even easier to produce correct data.ow \nanalyses. In effect, we have recast the semantic mean\u00adings associated with facts from a cost to a bene.t: \nin the original Rhodium system, semantic meanings were extra redundant infor\u00admation provided solely to \nverify existing rules (and provide formal documentation), while our new algorithm constructively uses \nthe meanings to derive rules automatically. To automatically generate forward propagation rules for a \nfact schema, our algorithm works backward from its semantic meaning. For an instance of a fact schema \nto be correctly introduced on the outgoing edge of a CFG node, the fact s semantic meaning must hold \nfor all possible program states that could arise when control reaches that edge. Our algorithm uses weakest \npreconditions to compute the condition that must hold before the node in order for the fact s semantic \nmeaning to hold after the node. Finally, our algorithm applies a series of simpli.cations and strengthenings \nto re-express this weakest precondition in terms of some combination of the semantic meanings of user-de.ned \nfact schemas. The contributions of this paper are as follows: We show how the problem that we are solving \nin the con\u00adtext of Rhodium is an instance of the parametric predicate ab\u00adstraction (PPA) problem, a generalization \nof predicate abstrac\u00adtion [9, 6, 2, 14] introduced by Cousot [5]. The PPA problem has been solved previously, \nbut only by hand, and only for a par\u00adticular set of parametric predicates. We present an algorithm for \ninferring correct forward propagation rules from a set of fact schema declarations, in the process solving \nthe PPA problem automatically and for any set of parametric predicates. (Hand\u00adwritten propagation rules \ncan still be provided to augment the inferred set, ensuring that any limitations of the inference algo\u00adrithm \ndo not block its use.) Section 3 presents an overview of our approach, and section 4 presents our algorithm \nin detail.  We have implemented our algorithm in the context of the Whirl\u00adwind compiler, and we have \nrun it to generate rules for a vari\u00adety of fact schemas. (In Whirlwind, Rhodium-based optimiza\u00adtions \ninteroperate seamlessly with non-Rhodium optimizations, allowing incremental migration of non-Rhodium \noptimizations to Rhodium form.) Section 5 describes these fact schemas and the generated rules.  We \nhave developed a method to automatically compare the the\u00adoretical coverage of two sets of Rhodium rules, \nindependent of any particular program that the rules may run on. We have used this method to compare \nthe coverage of the automatically gen\u00aderated propagation rules to an earlier set of handwritten rules. \nFor the facts we considered, our automatically generated rules covered 77% of the cases of the handwritten \nrules, and the au\u00adtomatically generated rules included cases that were not covered by the handwritten \nrules. Section 5.1 presents a more detailed description of these results.  We have also compared the \npractical coverage of the automat\u00adically generated rules versus the handwritten rules, in terms of the \noptimizing transformations triggered when the rules are run on a set of benchmark programs. Our results \nshow that the in\u00adferred rules were able to trigger over 95% of the transformations triggered by the handwritten \nrules, and some transformations not triggered by the handwritten rules.  By reducing the burden of writing \ncorrect data.ow analyses, we hope that our work will allow compiler writers to focus on the more creative \nparts of their task, such as determining what novel transformations could improve performance, and what \ndata.ow facts are needed to justify these transformations. Furthermore, by reducing the barrier to entry, \nour system makes it feasible to open up compilers to user extensions without compromising correctness. \nOftentimes, it is the end programmer who has the application\u00addomain knowledge and the coding-pattern \nknowledge necessary to implement useful analyses and optimizations. The Rhodium rule Stmts Exprs Ops \ns e op ::= ::= ::= x := e |*x := x |decl x |x := new |decl x[x] |x := new[x] |if x goto l else l |skip \nc |x |*x | op x. . . x |&#38;x |&#38;(x[x]) various operators with arity =1 Vars Consts x c ::= ::= x \n|y |z |.. . constants Labels l ::= statement labels Figure 1. Grammarofthe IL 1. de.ne edge fact schema \nhasConstValue(X:Var, C:Const) 2. with meaning s(X)= s(C) 3. decl X:Var, C:Const 4. if currStmt=[X \n:= C] 5. then hasConstValue(X, C)@out 6. if hasConstValue(X, C)@in .currStmt=[Z := Y ] .X =.Z 7. then \nhasConstValue(X, C)@out 8. if hasConstValue(X, C)@in .currStmt=[*Z := Y ]. 9. mustNotPointTo(Z, X)@in \n 10. then hasConstValue(X, C)@out  Figure 2. Simple constant propagation analysis in Rhodium inferencer \ncould allow end programmers with little knowledge of program analysis to implement such analyses and \noptimizations without the worry of breaking the compiler.  2. Background on Rhodium This section presents \nbackground material on how data.ow anal\u00adyses are written in Rhodium, and how they are checked for cor\u00adrectness \nautomatically. Rhodium analyses run over a C-like inter\u00admediate language (IL) with functions, recursion, \nunstructured lo\u00adcal control .ow, pointers to stack-and heap-allocated memory, and stack-and heap-allocated \narrays. The IL program is represented as a control .ow graph (CFG) with each node representing a simple \nregister-transfer-level statement, shown in Figure 1. (We currently do not attempt to infer propagation \nrules for function call state\u00adments, so they are omitted from the .gure.) Data.ow information is encoded \nin Rhodium by means of data.ow facts, which are user-de.ned function symbols applied to asg.,hasConstVal \netofterms,e.ue(x,5)andexprIsAvailable(x,a+b). A Rhodium analysis uses propagation rules, which are a \nstylized way of writing .ow functions, to specify how data.ow facts are introduced after and propagated \nacross CFG nodes. These user\u00adde.ned .ow functions collectively de.ne a data.ow analysis, whose solution \nis the greatest .xed point of the equations induced by the .ow functions. The example in Figure 2 shows \na partial implementation of constant propagation in Rhodium. We encode the data.ow infor\u00admation for this \nanalysis using the hasConstValue(X, C) edge fact schema declared on line 1. A fact schema is a parameterized \ndata.ow fact (a pattern, in essence) that can be instantiated to cre\u00adate actual data.ow facts. With the \ndeclaration on line 1, each edge in the CFG will be annotated with a set containing facts of the form \nhasConstValue(X, C),where X ranges over variables in the IL program being analyzed, and C ranges over \nconstants. The conven\u00adtion used throughout this paper is that Rhodium meta-variables C and K range over \nconstants while W , X, Y ,and Z range over IL variables. Propagation rules in Rhodium indicate how edge \nfacts are introduced after and propagated across CFG nodes. For exam\u00adple, the rule on lines 4-5 of Figure \n2 speci.es that a variable is known to have a particular constant value immediately after ev\u00adery statement \nthat assigns the constant to the variable. The rule on lines 6-7 of Figure 2 de.nes a condition for preserving \na hasConstValue fact across an assignment statement: if the fact hasConstValue(X, C) appears on the incoming \nCFG edge of an assignment and X is not the variable assigned to, then the data.ow fact hasConstValue(X, \nC) should appear on the outgoing edge of the assignment. Rules can refer to more than one kind of fact \nas well, allowing the results of multiple analyses to cooperate. For example, the rule on lines 8-10 \nof Figure 2 leverages points\u00adto analysis (de.ned elsewhere) to preserve constant propagation information \nacross pointer store statements. Each propagation rule is interpreted within the context of a CFG node, \nreferred to as currStmt. References to edge facts are fol\u00adlowed by @ signs, with the name after the @ \nsign indicating the edge on which the fact appears. The in edge name refers to the in\u00adcoming edge of \nthe current node, and the out edge name refers to the outgoing edge. For nodes with more than one incoming \nor outgoing edge, such as conditional branches, edge names can be subscripted. For example, hasConstValue(X, \nC)@out[0] and hasConstValue(X, C)@out[1] can be used to refer to constant propagation facts along a conditional \nbranch s true and false out\u00adgoing edges, respectively. The part of a rule before the then keyword is \ncalled the an\u00adtecedent and the part after is called the consequent. Intuitively, the antecedent is a \n.rst order logic formula over data.ow facts (de\u00adtails of the antecedent syntax can be found elsewhere \n[15]). The semantics of a propagation rule on a CFG node is as follows: for each substitution of the \nrule s free variables that make the an\u00adtecedent valid at some node in the CFG, the fact in the consequent \nis generated on the named edge. For the rule described above, the hasConstValue(X, C) fact will be generated \non the outgoing edge of a node for each substitution of X and C with variables and con\u00adstants that makes \nthe antecedent valid. To check Rhodium data.ow analyses for correctness, program\u00admers must specify a \nsemantic meaning for each fact schema, in the form of a .rst order logic predicate over a program state \ns.For ex\u00adample, the meaning of hasConstValue(X, C), shown on line 2, is that the value of X in s, denoted \nby s(X), is equal to the value of the constant C, denoted by s(C). This meaning states that if hasConstValue(X, \nC) appears on an edge in the CFG, then for any s that may occur at run-time when control reaches that \nedge, s(X)= s(C) holds. Rhodium data.ow analyses are checked for correctness auto\u00admatically by discharging \na local correctness obligation for each propagation rule. For each rule an automated theorem prover is \nasked to show that if the meaning of the antecedent holds before a node for an otherwise-arbitrary program \nstate s, then the mean\u00ading of the consequent holds after executing the node in s.We have shown by hand \nthat if all the propagation rules in a Rhodium pro\u00adgram pass this condition, then the induced data.ow \nanalysis is cor\u00adrect. The full details of the Rhodium language, including syntax for de.ning data.ow \nfacts and meanings, can be found elsewhere [15].  3. Overview The Rhodium system makes it easier to \nwrite data.ow analyses by providing a convenient way to express .ow functions, and by automatically checking \ntheir correctness. However, there is still a signi.cant amount of effort that the Rhodium user has to \ninvest to write the .ow functions. There are a many statement forms to be considered, and many of the \nresulting propagation rules can be quite similar. For example, the rule on lines 6-7 of Figure 2 is a \ntypical preservation rule, which states the conditions under which hasConstValue(X, C) is preserved through \na certain statement form; a similar rule should be written for many other assignment statement forms. \nEven though many of these rules are simple and straightforward, subtle corner cases can exist, particularly \nin the face of aliasing and mutation. Moreover, information computed by one analysis can be used to help \ncompute information for another analysis, as illustrated by the rule on lines 8-10; it is dif.cult (and \nnon-modular) for Rhodium users to think through all the possible bene.cial interactions among a set of \ndata.ow analyses. Finally, the programmer has to invest time and effort debugging any rules that fail \nthe correctness checks. To alleviate these burdens, we wish to automatically infer the most complete \nset of correct forward propagation rules we can, given only a set of data.ow fact schema declarations. \nFor the example in Figure 2, our goal would be to infer propagation rules such as the ones on lines 4-10 \n(and many more) from the data.ow fact schema de.nition on lines 1-2 (and the de.nitions of other fact \nschemas such as mustNotPointTo). This problem of inferring Rhodium propagation rules is related to predicate \nabstraction [9, 6, 2, 14] and its generalization para\u00admetric predicate abstraction [5]. The next two \nsubsections describe predicate abstraction and previous algorithms for performing it, and explain how \nthat work is insuf.cient for solving our problem. The last subsection presents an overview of our approach. \n3.1 Predicate Abstraction The goal of predicate abstraction is to propagate a given set of predicates \nthrough a program. Formally, predicate abstraction is de.ned in terms of a .nite set B = {B1 ...Bk} of \npredicate names. Each predicate name Bi has an interpretation .Bi. = .i, which is the predicate for which \nBi stands. A cube over B is a conjunction of possibly negated symbols from B, which can be represented \nas a set of possibly-negated names drawn from B.The domain of predicate abstraction is the set of all \ncubes, and one cube is computed at each program point. Given these de.nitions, the problem of predicate \nabstraction (PA) can be stated as follows: PA PROBLEM STATEMENT. Given a set of predicates B with associated \ninterpretations, a program statement s, and a cube over B .owing into the statement, determine the cube \nover B that .ows out of the statement. For example, consider the set of predicate names B = {B1,B2}, \nwhere .B1. . (a = b) and .B2. . (a = b + 1). Given the cube B1 .\u00acB2 and the statement a := b + 1, then \npredicate abstraction would compute that the cube \u00acB1 .B2 should be propagated after the statement. Our \nproblem of inferring Rhodium propagation rules from fact schemas is different from predicate abstraction \nin a simple yet fun\u00addamental way. Although our algorithm starts with a set of named predicates as in \npredicate abstraction (and these named predicates are called fact schemas in the context of Rhodium), \nour named predicates are parametric. For example, our algorithm doesn t start with concrete predicates, \nsuch as a = 5 or a = b, but rather with parametric predicates, such as X = C or X = Y (we tempo\u00adrally \nomit s from our meanings here to be consistent with the s-less notation used in predicate abstraction). \nThese parametric predicates represent an in.nite set of instantiated concrete pred\u00adicates. In addition \nto predicates being parametric, the statements that our algorithm considers are also parametric. For \nexample, our algorithm doesn t infer .ow functions for concrete statements such as a := a + b, but rather \nfor parametric statements (or statement forms)such as X := Y OP Z. This re.ects the fact that our goal \nis to infer .ow functions prior to running the analysis on any par\u00adticular program. In this light, the \nproblem that we are solving can be seen as a generalization of predicate abstraction that applies to \nparametric predicates. Such a generalization was proposed by Cousot [5], and he called the generalized \nproblem parametric predicate abstraction (PPA). Formally, PPA is de.ned in terms of a .nite set B = {B1 \n...Bk} of parametric predicate names, where we denote by args(Bi ) the sequence of parameter names. Each \npredicate name Bi has a interpretation .Bi. = .i, where any free variables in .i must be in args(Bi). \nApplied to the Rhodium context, a predicate name Bi would correspond to a Rhodium fact schema name, args(Bi) \nto the parameters of the fact schema, and its interpretation to the meaning of the fact schema. The PPA \nproblem can then be stated as follows: PPA PROBLEM STATEMENT. Given a set of parametric predicates B \nwith associated interpretations, and given a parametric statement (corresponding to a statement form \nin the intermediate language), determine a representation of the .ow function for that statement form. \n 3.2 Algorithms for Predicate Abstraction There are a variety of approaches to performing traditional \npred\u00adicate abstraction automatically [9, 6, 2, 11, 14], but they are all based on the observation that, \nbecause there are a .nite number of predicates, one can test each predicate separately to see if it (or \nits negation) holds after the statement. This test is usually done by ask\u00ading a theorem prover to show \nan implication. For example, to test if a predicate Bi holds after a statement s given that a cube c \nholds before the statement, one simply asks an automatic theorem prover to show that the cube c implies \nwp(s, Bi), the weakest precondi\u00adtion of Bi with respect to s. The cube that is propagated includes all \nthe possibly-negated predicates for which the theorem prover can show this implication. The computation \nof which cube to propagate can be performed while predicate abstraction is running on a given program, \nor it can be performed ahead of time, once the program is known, by enumerating all possible incoming \ncubes to all the state\u00adments in the program (which again is possible because the set of predicates is \n.nite). The approach to predicate abstraction described above is es\u00adsentially an enumerate-and-test approach. \nIn our context, such an approach would amount to enumerating possible antecedents and checking whether \nor not each antecedent implies the con\u00adsequent we want to propagate. For example, consider two fact schemas \nvarEqualsPlus(X, Y, Z) with meaning s(X)= s(Y )+ s(Z) and varNotEqualsPlus(X, Y, Z) with meaning s(X) \n. = s(Y )+ s(Z), and consider the problem of propagating the fact varEqualsPlus(X, Y, Z) on the out edge \nof the paramet\u00adric statement A1 := A2 + A3. The most natural enumerate\u00adand-test approach would be to \nenumerate possible predicates . involving varEqualsPlus and varNotEqualsPlus, and then use the Rhodium \ncorrectness checker to see whether or not the rule similarly for varNotEqualsPlus, so there are a total \nof 432 instan\u00adtiations of the fact schemas. If we limit ourselves to conjunctions of size three, we would \nhave 432 choose 3 = 13,343,760 possible con\u00adjunctions to check. With the optimistic assumption that correctness \nchecking of a candidate propagation rule takes about 1 second, this amounts to about 5 months of continuous \nchecking. Furthermore, this simple computation is only an under-approximation, because it includes just \ntwo fact schemas; it does not include all the possible equality and inequality constraints between meta-variables \n(such as the one present on line 6 of Figure 2), and it does not include the possibility of having meta-variables \nin the antecedent of a rule that are not present in the consequent. Although there are techniques to \nreduce the search space for the traditional problem of predicate abstraction [2], the broader problem \nwith the enumerate-and-test approach is simply that it is not goal-directed: it blindly enumerates predicates, \nwithout any consideration for the end goal, and then it does a forward check to see if the end goal is \nmet. Solutions to the parametric predicate abstraction problem would likely be more appropriate to our \ntask of inferring Rhodium propa\u00adgation rules, but previous solutions to the PPA problem have been constructed \nby hand, and only for particular sets of parametric pred\u00adicates. A key contribution of our work is to \ndevelop an algorithm that solves the PPA problem automatically for any given set of para\u00admetric predicates. \n 3.3 Our Approach The solution we propose in this paper takes a goal-directed ap\u00adproach, which is much \nbetter suited to the PPA problem than an enumerate-and-test approach. We start with the predicate we \nwant to establish, compute the weakest precondition of the predicate with respect to the current statement, \nand .nally perform a back\u00adward search through an inference system to .nd a valid antecedent that implies \nthe weakest precondition. By performing a targeted search, our goal-directed approach prunes the search \nspace very ef\u00adfectively, allowing us to generate rules for a total of 10 fact schemas in under 10 minutes \non a modern desktop machine. To give an overview of our technique, we describe the steps that our algorithm \nwould take on a simple example. Consider the fact schema hasConstValue on lines 1-2 of Figure 2. In this \ncase, our problem of inferring propagation rules reduces to .nding formulas .thatmakethepropagationruleif.thenhasConstValue(X, \nC)@out correct. Intuitively, this rule is correct if, whenever . holds be\u00adfore currStmt, the meaning \nof hasConstValue(X, C) holds after currStmt, i.e., . must be a precondition with respect to currStmt \nthat ensures s(X)= s(C) as a postcondition. Since we wish to .nd the most general such ., our algorithm \n.rst computes the weakest precondition f with respect to currStmt that establishes s(X)= s(C). As we \nshall see, the weakest precondition may if currStmt=[A1 := A2+A3].. then varEqualsPlus(X, Y, Z)@out \nrefer to run-time information not available at analysis-time, and is correct. so f itself may not be \ndirectly usable in a Rhodium propagation Unfortunately, because the antecedent can have more free vari-rule. \nConsequently, our algorithm searches through the space of ables than appear in the consequent, the number \nof possible . formulas that imply f to .nd formulas . that refer only to analysis\u00adpredicates is in.nite. \nEven if one restricts the predicates to only time information (i.e., data.ow fact instances and syntactic \ntests of mention meta-variables bound by the statement form and conse-statement forms). In logical terms, \nthe process of .nding formulas quent fact to propagate, the number of possible predicates grows . that \nimply f is called strengthening. very fast. Consider the example above, where we are consider-To compute \nthe weakest precondition of s(X)= s(C) with ing propagation rules for the statement A1 := A2 + A3 and \nrespect to currStmt, we do a case analysis on the possible forms outgoing fact varEqualsPlus(X, Y, Z)@out, \nwhose antecedent of currStmt. Consider for example the case where currStmt is can mention instances of \nthe fact schemas varEqualsPlus and of the form Y := K,where Y and K range respectively over varNotEqualsPlus \non the in edge. There are a total of 6 meta-variables and constants of the IL program. The traditional \nweakest variables (X, Y , Z, A1, A2,and A3) bound in the statement and precondition rule for an arbitrary \nassignment is as follows: the outgoing fact. Since the fact schema varEqualsPlus(X, Y, Z) has three parameters, \nit can be instantiated in 63 = 216 ways, and wp(Z := E, P )= P [Z ..E] where P [Z . 1 . E] denotes the \nformula P with Z replaced by E. In our case, we would therefore have: f = wp(Y := K, s(X)= s(C)) =(s(X)= \ns(C))[Y . . K] In the above equation, the Rhodium meta-variables X and Y range over IL variables. The \nresult of the substitution operation [Y . . K] thus depends on whether or not X and Y are instantiated \nto the same IL variable. If they are, then (s(X)= s(C))[Y .. K] expands to s(K)= s(C); otherwise, it \nremains s(X)= s(C). Since K and C are constants, s(K)= s(C) in turn simpli.es to K . C (in the rest of \nthis paper, we use the symbol . for syntactic equality of abstract syntax trees (including variable names \nand constants) and = for semantic equality of run-time values). The weakest precondition f is then as \nfollows: f =(X . Y . K . C) . (X . . Y . s(X)= s(C)) The antecedent of a rule cannot contain references \nto s since s is only known at run-time, not analysis-time. As a result, we cannot use the above weakest \nprecondition directly as the antecedent .. However, the antecedent of a rule can refer to data.ow facts, \nand the meanings of these data.ow facts can refer to s. Our task then is to encapsulate all the s s in \nf using data.ow facts. In the constant propagation example, the user would provide the data.ow fact schema \nhasConstValue(X, C) with meaning s(X)= s(C). By searching for syntactic matches of the form s(X)= s(C) \nin f and replacing them with hasConstValue(X, C)@in,we get: . =(X . Y .K . C).(X . . Y .hasConstValue(X, \nC)@in) This antecedent does not refer to s and can therefore be used to generate a correct propagation \nrule for statements of the form Y := K: if currStmt.[Y := K] . . then hasConstValue(X, C)@out If we wish, \nwe can split the disjunction in . into two separate rules: if currStmt.[Y := K] . X . Y . K . C then \nhasConstValue(X, C)@out if currStmt=[Y := K] . X . . Y . hasConstValue(X, C)@in then hasConstValue(X, \nC)@out The rules above are equivalent to the handwritten rules from lines 4-5 and 6-7 of Figure 2. In \nthis simple illustrative example, the mapping from the weak\u00adest precondition f to a valid antecedent \n. was immediate. In more complicated cases, the mapping will not be immediate. Our algo\u00adrithm will perform \nlogical rewrites in an attempt to .nd a way to eliminate all s s. Because we are searching for formulas \nthat im\u00adply the weakest precondition f, permissible rewrites include sim\u00adpli.cations (.nding a f. that \nis equivalent to f) and strengthenings (.nding a less general f. that implies f). Intuitively, strengthenings \nsacri.ce precision to make the condition f statically computable in terms of available facts. Each logical \nrewrite, be it a simpli.cation or a strengthening, can be seen as a single backward step in an inference \nsystem. Our algorithm therefore performs a backward search through an inference system starting at the \nweakest precondition f. All of the formulas considered in this backward search will imply f, and our \ngoal is to .nd such a formula that does not contain references to s. Once we .nd such a f, the sequence \nof inferences we performed during the search, if reversed to be in the forward direction, will constitute \na proof of correctness for the rule we just generated. Indeed, this forward sequence of inference steps \nis a derivation of 1 This rule ignores the potential aliasing effects of pointers and meta\u00advariables. \nWe return to this issue in Section 4.1. the condition that our Rhodium checker would send to the theorem \nprover on the newly generated rule. In this way, the rules we generate are guaranteed to be correct. \nThe search that we perform through an inference system is sim\u00adilar in many ways to searches that are \ndone in automated or semi\u00adautomated theorem provers. Our search is goal-directed, in that we start with \na goal, and search backwards through the space of proof\u00adtrees to .nd formulas that imply the goal. Many \ntheorem provers use such a goal-directed search, for example PVS [19], NuPRL [4], Twelf [21, 28], the \nBoyer Moore theorem prover [12, 13], Is\u00adabelle [20], and HOL [8]. However, the key difference between \nour work and automated theorem proving lies in the problem that we are solving. A theorem prover is given \na set of axioms, and asked to prove a theorem. If it uses a backward search, the theorem prover starts \nwith the theorem to be proven and applies inference rules backward to reach the axioms. Our problem is \ndifferent because we are not given axioms that we must search towards. The goal of our backward search \nis not to reach a set of axioms, but rather to reach a formula in a restricted form, i.e., one that no \nlonger con\u00adtains occurrences of s. Although the goals are different, there are nonetheless strong similarities \nin the techniques. In particular, users of semi-automated theorem provers often use tactics (or variations \nthereof) to guide the theorem prover in its backward search of the large proof-tree space. One can view \nour work as a set of special\u00adized tactics for the purposes of .nding a statically computable for\u00admula \nfrom a formula that mentions run-time values.  4. Algorithm Our algorithm for generating rules is shown \nin Figure 3. To sim\u00adplify the presentation, we assume that statements have only one outgoing CFG edge \ncalled out our implementation, however, also handles statements with multiple outgoing edges, such as \ncondi\u00adtional branches. The function GenerateRules (lines 1-9) takes a set of fact schema declarations \nand returns a set of propagation rules. For each fact schema F (V.) declared by the user, for each statement \nform S (covering the different kinds of IL statements de.ned in Figure 1), the algorithm .nds the rules \nthat propagate fact F (V.) on the edge out of statement S. In the body of this loop, we .rst compute \nthe weakest precondition of the meaning of fact F (V.) with respect to the statement form, using the \nwp function (line 4). We then perform a backward search in our in\u00adference system starting from the weakest \nprecondition (line 5). The backward search is a loop that ends when we have removed all s s from f. The \n.rst step in the loop is to express f in a simpli\u00ad.ed form using the Simplify function (line 10). If \nall references to s have been removed (line 11), then we have found a valid an\u00adtecedent and the search \nis over (line 12). If not, we strengthen the formula using Strengthen in order to remove some of the \nrefer\u00adences to s (line 13), and then we continue the search with the re\u00adsulting stronger formula (line \n14). The algorithm in Figure 3 depends on the three functions wp, Simplify,and Strengthen. These three \nfunctions are described in more detail in the following three subsections. Throughout our explanations, \nwe will use the examples in Fig\u00adures 4 and 5, which show the inference steps used by our al\u00adgorithm for \nhasConstValue(X, C) on statements of the forms Y := K and *Y := Z, respectively. In these examples, the \nfact schemas available for matching are hasConstValue(X, C) with meaning s(X)= s(C), mustNotPointTo(X, \nW ) with mean\u00ading s(X) . = s(&#38;W ),and mustPointTo(X, W ) with meaning s(X)= s(&#38;W ). The examples \nshould be read bottom-up: the bottommost formula is the meaning of hasConstValue(X, C), and the topmost \nformula is the .nal predicate . that we use to gen\u00aderate a rule. The .nal top-down sequence represents \na valid logical function GenerateRules(decls: set[FactSchemaDecl]): set[Rule] 1. let results := \u00d8  \n2. for each [de.ne edge fact schema F (V.) with meaning M] .decls do 3. for each statement form S do \n 4. let f := wp(S, M)  5. let . := RemoveRuntimeState(f, decls) 6. if . =.false then  7. let rule \n:= if currStmt.S .. then F (V.)@out 8. results := results .{rule}  9. return results function RemoveRuntimeState(f: \nFormula, decls: set[FactSchemaDecl]): Formula 10. let fsimp := Simplify(f) 11. if fsimp contains no \ns then 12. return fsimp  13. let fstren := Strengthen(fsimp, decls) 14. return RemoveRuntimeState(fstren \n, decls)  Figure 3. Algorithm for generating rules from fact declarations (Y . X .K . C) .(Y . . X \n.hasConstValue(X, C)@in) (5)   logSimp, fact match (.s. Y . X ..s. K . C ) .(.s. Y .. X ..s. s(X)= \ns(C)) (4)  logSimp .s. Y . X .K . C ..s. Y ..X .s(X)= s(C) (3)  case .s. (Y . X .K . C) .(Y .. X .s(X)= \ns(C)) (2) . .&#38;, .&#38;, .c .s. (s(&#38;Y )= s(&#38;X) .s(K)= s(C)) .(s(&#38;Y )=.s(&#38;X) .s(X)= \ns(C)) (1) wp(Y := K).s .Sout . (s(X)= s(C)) Figure 4. Inference steps for hasConstValue(X, C) for statements \nof the form Y := K (mustPointTo(Y, X)@in .hasConstValue(Z, C)@in) . (mustNotPointTo(Y, X)@in .hasConstValue(X, \nC)@in) . (hasConstValue(Z, C)@in .hasConstValue(X, C)@in) (5)  fact match (.s. s(Y )= s(&#38;X) ..s. \ns(Z)= s(C)) . (.s. s(Y ) .= s(&#38;X) ..s. s(X)= s(C)) . (.s. s(Z)= s(C) ..s. s(X)= s(C)) (4)  logSimp \n.s. s(Y )= s(&#38;X) .s(Z)= s(C) . .s. s(Y ) .= s(&#38;X) .s(X)= s(C) . .s. s(Z)= s(C) .s(X)= s(C) \n(3)  .resolution .s. (s(Y )= s(&#38;X) .s(Z)= s(C)) .(s(Y )=.s(&#38;X) .s(X)= s(C)) (1) wp(*Y := Z).s \n.Sout . (s(X)= s(C)) Figure 5. Inference steps for hasConstValue(X, C) for statements of the form *Y \n:= Z deduction. Each backward inference step represents one or more steps taken by wp, Simplify, and/or \nStrengthen. The labels to the right of an inference step identify exactly what steps were taken. These \nlabels refer to rewrite rules, except for wp, which refers to the application of the wp function. The \ncollection of all rewrite rules in our system can be found in Figures 7 and 8 4.1 Weakest Precondition \nThe function wp(S, P ) computes the weakest liberal precondition of a predicate P with respect to a statement \nS. The weakest liberal precondition is the weakest condition Q such that if Q holds before the statement \nS,and S terminates, then P will hold after S.2 2A liberal precondition guarantees the postcondition only \nunder the as\u00adsumption that the statement terminates without error, as opposed to a strict precondition, \nwhich must also ensure that the statement terminates without It will be important in our algorithm to \nmake the quanti.ca\u00adtion over s explicit in all formulas. For example, the meaning for hasConstValue(X, \nC)@Edge becomes .s . SEdge.s(X)= s(C),where SEdge is the set of all possible states the program can be \nin if and when control reaches the edge Edge. (We will often omit . Sin clauses.) The traditional method \nfor calculating wp(S, P ) directly ma\u00adnipulates the expressions in P based on the effect of S [10]. However, \ndirect manipulation becomes increasingly complicated in the presence of aliasing, e.g., via pointers \nor via Rhodium meta\u00advariables that might or might not be instantiated to the same pro\u00adgram variables. \nFor example, consider computing the weakest pre\u00ad error. The liberal version is appropriate for our task \nbecause the meaning of any data.ow facts computed on a CFG edge must hold for all possible program states \nif and when control reaches that edge; analysis results need not ensure that control actually reaches \nany particular edge. step(decl X, (., .)) = .(.[X .newloc], .[newloc . ..]) step(X := C, (., .)) = . \n(., .[.[X] .C]) step(X := &#38;Y, (., .)) = . (., .[.[X] ..[Y ]]) step(X := Y, (., .)) = . (., .[.[X] \n..[.[Y ]]]) step(X := *Y, (., .)) =(., .[.[X] ...[.[.[Y ]]]]) step(*X := Y, (., .)) =(., .[.[.[X]] ...[.[Y \n]]]) The newloc symbol stands for a fresh location. .is a value that gives a run-time error if read. \nFigure 6. Some cases of the step function condition of the formula .s . Sout .s(X)= s(C) with respect \nto a pointer store statement *Y := Z, as in Figure 5. When run on a particular program, the X meta-variable \nmight be instantiated to the same program variable as the Y and/or Z meta-variables, or to a variable \npointed to by the program variable that Y is instantiated to. We must compute a correct precondition \nin all these cases. To solve these problems, we use an approach based on the work by Cartwright and Oppen \n[3]. The key idea is that wp(S, .so . Sout .P ) is equal to .si . Sin .P [so . . step(S, si)] where the \nstep function represents the operational semantics of our IL in terms of standard select and update map \noperators applied to the environment . and store . components of the incoming program state si. Some \nof the cases for the step functionare shownin Figure 6; our technical report [27] contains full details. \nFinally, selections from updated maps M are simpli.ed using the following rule: (M[X .= Y then V else \nM[Y ] . V ])[Y ]= if X This simpli.cation naturally introduces the necessary aliasing\u00adcondition case \nanalysis. For example, consider computing wp(*Y := Z, .so . Sout .so(X)= so(C)) Replacing s with explicit \n.o and .o and expanding s(\u00b7) short\u00adhands yields wp(*Y := Z, .(.o,.o) . Sout ..o[.o[X]] = C) Applying \nthe de.nition of wp gives .(.i,.i) . Sin . (.o[.o[X]] = C)[(.o,.o) . . step(*Y := Z, (.i,.i))] Applying \nthe de.nition of step for *Y := Z gives .(.i,.i) . Sin . (.o[.o[X]] = C)[(.o,.o) .. . (.i,.i[.i[.i[Y \n]] . .i[.i[Z]]])]] Applying the substitution of output for input state components gives .(.i,.i) . Sin \n.(.i[.i[.i[Y ]] .. .i[.i[Z]]])[.i[X]] = C Applying the select/update simpli.cation gives .(.i,.i) . Sin \n. (if .i[.i[Y ]] = .i[X] then .i[.i[Z]] else .i[.i[X]]) = C Reexpressing using s short-hands gives .s \n. Sin . (if s(Y )= s(&#38;X) then s(Z) else s(X)) = s(C) which is equivalent to the expected weakest \nprecondition .s . Sin . (s(Y )= s(&#38;X) . s(Z)= s(C)) . (s(Y ) . = s(&#38;X) . s(X)= s(C)) Context \nRules F t[T1] . F t[T2] if T1 . T2 F [F1] . F [F2] if F1 . F2  Logical Simpli.cations [trueelim] P .true \n. P [\u00ac push] \u00ac(P .Q) . \u00acP .\u00acQ ...many more... IL Simpli.cations Term Rewrite Rules [&#38;* E ] s(&#38;(*X)) \n. s(X) [*&#38;E ] s(*(&#38;X)) . s(X) Formula Rewrite Rules [.&#38;] s(&#38;X)= s(&#38;Y ) . X . Y [.c] \ns(K)= s(C) . K . C [Fc&#38;] s(C)= s(&#38;X) . false [F&#38; op ] s(&#38;X)= s( op T1 ... Tn) . false \n[T=] s(T )= s(T ) . true [T.] T . T . true [F.] T . T . . false (if T and T . are incompatible forms) \nFigure 7. Some simpli.cation rules 4.2 Simpli.cation The Simplify function transforms a given formula \ninto an equivalent formula that is in a simpler form by applying simpli\u00ad.cation rules until no more are \napplicable. This process involves two kinds of simpli.cations: logical simpli.cations, which sim\u00adplify \nlogical connectives; and IL simpli.cations, which simplify IL terms and expressions based on the semantics \nof our IL. IL simpli.cations include such simpli.cations as rewriting *(&#38;Y ) to Y . We express the \nsimpli.cation process as a rewrite system, where the rewrite rule F1 . F2 says that the term or formula \nF1 is rewrit\u00adten to F2. Figure 7 shows the most important of the rules of this rewrite system (the rest \ncan be found in our technical report [27]). We use F t[\u00b7] to represent a context F t with a term hole \nand F [\u00b7] to represent a context F with a formula hole. The context rules allow terms and subformulas \nto be rewritten inside of a formula. Logical simpli.cations are used to put the formula in a simpli.ed \nlogical form. IL simpli.cations are used to simplify the formula based on the semantics of our IL; each \nof the formula rewrite rules also has a disequality version which is not shown explicitly. 4.3 Strengthening \nIf Simplify cannot remove all occurrences of s from f,the for\u00admula must be strengthened using the Strengthen \nfunction. Unlike the Simplify function, the Strengthen function makes a formula less precise. Our algorithm \nuses many strengthening rewrites; Fig\u00adure 8 shows the most important of these. F1 .S F2 denotes that \nF1 can be rewritten to F2 in a positive position, meaning that, from a logical point of view, F2 . F1.We \nuse F +[\u00b7] to represent a con\u00adtext F + with a formula hole that appears in a positive position in F +. \nThe context rule at the top of Figure 8 allows subformulas to be rewritten inside a formula. (Allowing \nstrengthenings in negative holes would lead to a weaker overall formula, which is incorrect for our application. \nIt would be safe to allow weakenings in negative holes.) The Strengthen function collects all rewrites \nthat apply to f and applies each rewrite r separately to f to produce fr. Strengthen then returns the \ndisjunction of all the resulting fr  Context Rule F +[F1] .S F +[F2] if F1 .S F2 Strengthening Rules \n [fact match] for each [de.ne edge fact schema F (V.) with meaning M] .decls : .s .S.M .S F (V.)@in [syntactic] \ns(T1)= s(T2) .S T1 . T2  [case] .x. F1 .F2 .S .x. F1 ..x. F2 ` \u00b4` \u00b4` \u00b4 [.resolution] .x. (F1 .F2) \n.(\u00acF1 .F3) .S .x. F1 .F2 ..x. \u00acF1 .F3 ..x. F2 .F3 `V \u00b4 F t[s( op T1 ...Tn)] .S s(Ci)= s(Ti).F t[eval( \nop C1 ...Cn)] [opexpand ] i=1...n [quant swap] .x..y.P (x, y) .S .y..x.P (x, y) [mathematical] Strengthening \nrewrites based on mathematical properties such as s(X) =s(Y ) .S s(X) <s(Y ) Figure 8. Strengthening \nrules formulas. Although in theory this causes a worst-case exponential run-time, in practice only a \nsmall number of strengthenings apply to a given f, and in our experience we have not seen the exponential \nworst case. The strengthening rewrites can be divided into three categories: rewrites for introducing \nfacts, rewrites for approximating run-time knowledge at analysis-time, and rewrites for encoding mathemat\u00adical \nreasoning. The following three subsections describe each one of these in more detail. Fact Matching. \nThe .rst strengthening, labeled [fact match], replaces a subformula of f with an instance of a fact schema. \nIn Figure 8, decls is the set of declared fact schemas that is passed in as a parameter to Strengthen. \nFor each fact schema, our strengthen\u00ading algorithm considers rewriting instances of the schema s mean\u00ading \nto instances of the fact. Because we have made quanti.ers explicit, the meaning of a fact is universally \nquanti.ed over s, so that, for example, the meaning of hasConstValue(X, C)@in is .s .Sin .[s(X)= s(C)].As \na result, in step (5) of Figure 4, the Strengthen function replaces the subformula .s.[s(X)= s(C)] with \nhasConstValue(X, C)@in, as indicated by the [.s.M .S F (V.)@in] label to the right of the inference step. \nMapping meanings to facts is a strengthening rather than an equivalence because the presence of a fact \non an edge implies that its meaning holds at that edge, but not necessarily the reverse. The meaning \nmight hold at an edge, but the fact might not have been computed on that edge, perhaps because the rest \nof the analysis was not precise enough to compute the fact whenever its meaning was true. Consequently, \nthe absence of a Rhodium fact provides no information, and it would be unsound in general to map meanings \nto facts in negative positions. Approximating Run-time Knowledge. The second kind of strengthening embodies \nthe idea of approximating run-time knowl\u00adedge with analysis-time knowledge. For example, the run-time \npredicate s(X)= s(Y ) is always true when the Rhodium meta\u00advariables X and Y are instantiated to the \nsame IL variable. Thus, the analysis-time information X . Y approximates the knowledge that X and Y produce \nthe same value at run time. The [syntactic] rule captures the general case for arbitrary terms. Another \nexample of approximating run-time information is en\u00adcoded in the [case] rule, which approximates run-time \nchoice with analysis-time choice. Intuitively, .s. F1 .F2 allows each run\u00adtime s to make an independent \nchoice of whether F1 or F2 is true, while .s. F1 ..s. F2 requires the analysis to choose whether F1 \nor F2 is true for every run-time s.The [.resolution] rule is a re.nement of the [case] rule that retains \nmore precision when the Fi have a special form. The use of these case analysis rewrites has allowed our \nalgorithm to infer useful rules that we had not thought of previously when writing the .ow functions \nby hand. Mathematical Reasoning. The third kind of strengthening en\u00adcodes mathematical knowledge. One \nexample of such a strengthen\u00ading, shown in Figure 8, is s(X) =s(Y ) .S s(X) <s(Y ).The mathematical strengthenings \nused in our algorithm encode simple information about arithmetic. These mathematical strengthenings can \neasily be extended without changing the rest of the algorithm. 4.4 Termination Theorem 1. GenerateRules \nterminates on .nite input. Proof. A sketch of the proof is included here; the full proof can be found \nin our technical report [27]. The key question is whether the recursive RemoveRuntimeState function terminates. \nTermination of this function is proven using strong induction over the height of the formula. For each \nof the rewrites potentially applied during simpli.cation and strengthening, a case analysis over the \npossible forms of the formula being rewritten shows that only a bounded number of rewrites can follow, \nexcluding rewrites on strict subfor\u00admulas of the original formula, which inductively are assumed to terminate. \n  5. Evaluation To evaluate the effectiveness of our algorithm for automatically inferring propagation \nrules from fact schemas, we .rst de.ned a set of forward fact schemas and then wrote a collection of \nfor\u00adward propagation and transformation rules by hand. These rules were intended to be as complete as \npossible, and represent a kind of gold standard against which other versions of the rules could be compared. \nThese facts and rules compute information suf.cient for a variety of traditional intraprocedural analyses \nand optimiza\u00adtions, including constant propagation, copy propagation, must-and may-point-to analyses \nfor pointers to variables and to array ele\u00adments, pointer-target-contents analysis suitable for scalar \nreplace\u00adment, available expressions, an analysis tracking which variables point to stack-allocated local \nvariables as opposed to heap-allocated data, dynamic type analyses tracking which variables hold ints, \nar\u00adrays, pointers, and pointers to array elements, strength reduction for induction variables, and symbolic \nrange analysis. In total, this gold standard is comprised of 10 fact schemas, 116 propagation rules, \nand 15 transformation rules. We then ran our tool on the same 10 fact schemas to generate propagation \nrules automatically. Our tool generated 6,185 propaga\u00adtion rules, taking just under 10 minutes on a modern \nworkstation. Category Number Percent Covered 472 77.0 Introduces new variable or constant 77 12.6 Weakness \nof purely syntactic reasoning 56 9.1 Lost information 4 0.7 Missing strengthening rewrites 2 0.3 Missing \nmathematical reasoning 2 0.3 Total 613 100.0 Table 1. Coverage of handwritten rules by inferred rules \nOur tool generates a separate rule for each statement form. In con\u00adtrast, some handwritten rules apply \nto several statement forms. If such handwritten rules are duplicated for each different statement form \nto which they apply, then the number of per-statement-form handwritten rules is 613, which is more directly \ncomparable to the 6,185 automatically generated per-statement-form rules. We wish to know how many of \nthe facts computed by the handwritten rules would also be computed by the automatically generated rules, \nand whether there are any facts computed by the automatically generated rules that would have been missed \nby the handwritten rules. We compare the two sets of rules in two ways: In section 5.1, we perform a \ntheoretical comparison to see if there could exist situations in which one set of propagation rules computes \na fact while the other set does not.  In section 5.2, we assess how much optimization is triggered by \nthe two competing sets of propagation rules when optimizing a collection of benchmark programs.  5.1 \nComparison in Principle 5.1.1 Methodology To evaluate the theoretical quality of the rules inferred by \nour algorithm, we have developed a general method for comparing two sets of Rhodium rules without regard \nto any particular program that the rules may run on. If we let Sg be the set of automatically generated \nrules, and Sh of be the set of handwritten rules, then we want to know which of the rules in Sh are coveredbyrules \nin Sg, and vice versa. A rule r propagating an instance of a fact schema f is covered by a set of rules \nS if, whenever r .res (i.e., whenever its antecedent holds) and propagates a particular instance of f, \nsome rule from S would also .re and propagate the same instance of f.A rule r = if . then f(T.)@out is \ncovered by the rules in S = {s1,...,sm}iff ..|s1|.....|sm|.f(T.)@out where |si|is the translation of \na rule si into closed logical form: |if .i then fi(T.i)@out|= .X1,...,Xn ..i .fi(T.i)@out and X1,...,Xn \nare the free variables of si. This condition, along with axioms describing the semantics of our IL, is \nsent to the Simplify theorem prover [7]. If the theorem prover returns valid, then we know that Scovers \nr, i.e., that whenever rwould compute a fact instance, Swould too. If the theorem prover cannot show \nthe above condition, then we gain no information (since the theorem prover is incomplete). If the theorem \nprover can show the negation of this condition, then we know that the S does not cover r. By testing \ncoverage of each rule r . Sh against S = Sg,we can determine which of the handwritten rules are covered \nby the automatically generated ones. Conversely, by testing coverage of each rule r . Sg against S = \nSh, we can .nd automatically gen\u00aderated rules that are not covered by the handwritten ones, i.e., that \nare novel. In the following two subsections, we present comparison results for both of these directions. \n 5.1.2 Comparing Handwritten Rules to Inferred Rules We .rst applied our comparison algorithm to determine \nwhat rules from our handwritten gold standard are covered by the automat\u00adically generated rules. The \nresults of this comparison are shown in table 1. The .rst line of the table says that our inference method \nis able to infer rules that cover 472 (77%) of the 613 expanded hand\u00adwritten rules. The rest of the table \nreports the number of handwrit\u00adten rules that were not covered by any set of inferred rules, broken down \ninto categories. Overcoming these limitations is an important area for future work. Introduces new variable \nor constant: Our inference algo\u00adrithm only infers rules containing meta-variables de.ned in the statement \nform or the propagated fact. Some handwritten rules, particularly those performing a kind of transitive \nclosure operation over facts, mention additional intermediate meta\u00advariables.  Weakness of purely syntactic \nreasoning: A rewrite in our system is triggered only if its left-hand-side exactly matches some subformula \nof the current f. This syntactic matching algorithm leads to cases where super.cial changes in a formula \nor meaning affects whether or not the algorithm can detect a match. Just as one example, there are cases \nwhere the resolution strengthening rewrite [. resolution] is not applied because the current formula \ndoes not exactly match the form of the left-hand-side of [. resolution], even though it is logically \nequivalent to the left-hand-side.  Lost information: There is some information about the exe\u00adcution \nof IL statements that we have not yet added to our infer\u00adence algorithm. For example, our algorithm is \nnot aware of the fact that after a branch statement, the guard is true on out[0] and false on out[1]. \nAs a result, handwritten rules that exploit this information are not covered. Another kind of unused \nin\u00adformation is dynamic typing. For example, after the statement decl X[I], X is guaranteed to be an \narray. Handwritten rules that make use of this information are not covered.  Missing strengthening rewrites: \nOur inference algorithm does not currently detect whether or not it has been in a partic\u00adular state before, \nand to ensure termination, the set of strength\u00adening rewrites cannot include rewrites that may be potentially \ncyclic. For example, we can only include one of the two follow\u00ading sets of strengthening rules:  s(T1)= \ns(T2) .S s(T1 =T2)= s(true) .s(T1 =T2)= s(true) or s(T1 =T2)= s(true) .S s(T1)= s(T2) s(T1 =T2)= s(true) \n.S s(T1)= s(T2) We could remove this limitation by adding a kind of closure operation that performs a \ncollection of related strengthenings all at once, and never again. Missing mathematical reasoning: Some \nof the handwritten rules exploit mathematical reasoning, for example distributivity and commutativity, \nthat is currently not encoded in our rules for simpli.cation and strengthening. One approach to solving \nthis problem would be to add rules that encode these mathematical concepts. Alternatively, one could \ninvestigate ways of integrat\u00ading decision procedures that perform mathematical reasoning into the inference \nalgorithm. 5.1.3 Comparing Inferred Rules to Handwritten Rules We also used our comparison method to \ndetermine how many automatically generated rules were covered by the handwritten rules. Table 2 summarizes \nthe results. In particular, of the 6,185 Category Number Covered 2, 488 Potentially novel 3, 697 Total \n6, 185 Percent 40.2 59.8 100.0 Table 2. Coverage of inferred rules by handwritten rules test suite 7, \n432 Java stdlib 55, 993 soturon 396 counter 599 towers 909 mandelbrot 1, 708 cassowary 14, 747 raytrace \n16, 836 esspresso 81, 355 Total 32 41 0 0 0 0 38 0 43 154 578 847 16 24 57 59 352 320 1, 517 3, 770 57 \n0 0 0 0 0 0 0 2 59 Table 3. Number of triggered transformations automatically generated rules, our comparison \nmethod .nds that 3,697 rules were not shown to be covered, and so may be novel. There are too many potentially \nnovel rules to systematically analyze manually. Instead, we will give an example of an inferred rule \nthat is novel: if currStmt.[*Z := Y ] . varEqualsExpr (X, W )@in . varEqualsExpr (Y, W )@in . mustNotPointTo(Z, \nW )@in then varEqualsExpr (X, W )@out This rule says that if X and Y are both equal to W before *Z := \nY , and the assignment does not change W ,then X will be equal to W after the assignment. As .rst, this \nrule may seem surpris\u00ading, because it propagates varEqualsExpr (X, W ) unchanged through a pointer store \n*Z := Y without knowing whether or not Z points to X. This rule is nonetheless correct: if Z points to \nX, then the assignment is storing Y in X, in which case varEqualsExpr (Y, W )@in guarantees varEqualsExpr \n(X, W )@out; on the other hand, if Z does not point to X, then the assignment does not modify X, in which \ncase varEqualsExpr (X, W )@in guarantees varEqualsExpr (X, W )@out. Although this is a syn\u00adtactically \nsimple rule, the effects of pointers and mutation make it dif.cult to reason about. This example, along \nwith the large num\u00adber of potentially novel rules, points out that humans are not good at .nding all \nthe corner cases for data.ow analyses; our inference algorithm helps to cover such cases.  5.2 Comparison \nin Practice The theoretical comparison from section 5.1 showed that the rules inferred by our algorithm \ncover a reasonably large proportion of the handwritten rules. However, the theoretical comparison does \nnot indicate how useful the rules are in practice. We would like to know whether the rules inferred by \nour algorithm trigger as many program transformations as the handwritten rules. Using the Rhodium execution \nengine in Whirlwind, we optimized several benchmark programs, once with the handwritten rules and once \nwith the inferred rules. Table 3 lists the benchmarks and gives the number of lines of Whirlwind IL in \neach. test suite is a handwritten regression test suite, while the others are translated mechanically \nfrom Java .class .les. Java stdlib is the translation of 135 Java classes from the core of the Java standard \nlibrary, while the other seven are the translation of small-to medium-sized Java programs, excluding \nthe standard library. We counted the number of times any of the 15 transformation rules in our gold standard \nwas triggered by the handwritten rules and the inferred rules. Table 3 shows the results of our compari\u00adson. \nThe column labeled Handwritten only shows the number of transformations triggered by the handwritten \nrules, but not by the inferred ones; the column labeled Inferred only shows the num\u00adber of transformations \ntriggered by the inferred rules, but not by the handwritten ones; and the column labeled Both shows the \nnum\u00adber of transformations triggered by both sets of rules. Summing across all benchmarks, the inferred \nrules triggered over 95% of the optimizing transformations triggered by the hand\u00adwritten rules, and some \nmissed by the handwritten rules.  6. Related Work As pointed out in section 3, our work is a solution \nto a general\u00adized version of the predicate abstraction problem called parametric predicate abstraction. \nOne of the limitations of predicate abstrac\u00adtion that this generalization addresses is the .nite-domain \nrestric\u00adtion in predicate abstraction. The recent work of Reps, Sagiv, and Yorsh [25, 29] also ad\u00addresses \nthe .nite-domain limitation of the predicate-abstraction ap\u00adproach: they have derived the best .ow function \nfor a more general class of domains, namely .nite-height domains. For these domains, Reps et al. present \nan algorithm that computes the best possible abstract information .owing out of a statement given the \nabstract information .owing into it. Their algorithm has the nice theoreti\u00adcal property of providing \nthe best possible transformer, a property which we do not guarantee. However, the .ow function of Reps \net al. is not specialized with respect to the domain: they describe one single .ow function that works \nfor all .nite-height domains. As a result, each invocation of the .ow function uses an iterative approximation \ntechnique that makes successive calls to a decision procedure (a theorem prover). In contrast, our approach \ngenerates .ow functions that are speci.c to the domain speci.ed by the user, and as a result our generated \n.ow functions can be expressed as simple rules that only perform syntactic checks. Another way to view \nthe difference between our work and that of Reps et al. is that we try to pre-compute as much as possible \nof the .ow functions when we generate them, leaving little work for when the .ow func\u00adtions are executed, \nwhereas Reps et al. do all the work when the .ow functions are executed. Finally, the approach that we \ntake is different in nature from the Reps et al. approach. Our algorithm is goal-directed, in that we \nstart with the fact that we want to propa\u00adgate after the statement, and then work our way backwards to \nthe condition that must hold before the statement. In contrast, the Reps et al. approach works in the \nforward direction. Another system that infers .ow functions automatically is the TVLA system [18]. TVLA \ncan automatically generate the abstract semantics of a program from its concrete semantics. Here again, \nthe main difference compared to our work is that the original TVLA system runs an interpretation algorithm \nin the forward direction on every call to the .ow function. Our system, on the other hand, pre\u00adcomputes \nmuch of the .ow function when generating it, resulting in simple .ow functions that can be evaluated \nusing syntactic checks. Reps et al. have extended the original TVLA system with an approach for automatically \ncomputing transfer functions for TVLA instrumentation predicates [24]. In this extended TVLA system, \nthe transfer functions are computed ahead of time, before the analysis even runs once, as in our work. \nHowever, the Reps et al. approach is different from ours in a variety of ways. Their technique is based \non a .nite-differencing characterization of the relationship between the predicates holding before and \nafter a given statement. In contrast, our approach uses a standard weakest precondition computation to \ncapture this relationship, but we introduce a variety of strengthenings in order to re-express the weakest \nprecondition in terms of a given set of fact schemas. Also, the Reps et al. approach focuses on the three-valued \nlogic domain and pointer analysis, and it currently cannot directly handle constants and function symbols \nsuch as plus and minus. Our work is also related to the HOIST system for automati\u00adcally deriving static \nanalyzers for embedded systems [23, 22]. The HOIST work derives abstract operations by creating a table \nof the complete input-output behavior of concrete operations, and then abstracting this table to the \nabstract domain. Our work differs from HOIST in that we can handle concrete domains of in.nite size, \nwhereas the HOIST approach inherently requires the concrete do\u00admain to be .nite. As described in section \n3, the search that we perform through an inference system is closely related to the ideas of tactics \nand tacticals from automated theorem provers. Another related proof\u00adsearch technique is focusing [1], \nwhich is a way of alternating the application of so-called invertible rules (rules where the premise \nis equivalent to the conclusion) and non-invertible rules (rules where the premise implies but is not \nequivalent to the conclusion). In par\u00adticular, invertible rules are applied eagerly until none apply, \nand then non-invertible rules are repeatedly applied to a focused sub\u00adformula until invertible rules \nagain become applicable. As with fo\u00adcusing, we exhaustively apply invertible rules (during our simpli\u00ad.cation \nphase), and then we apply non-invertible rules (during our strengthening phase) to uncover more opportunities \nfor applying invertible rules. A preliminary design for our inference algorithm was presented informally \nat the COCV workshop [26]. 7. Conclusion We have presented an algorithm for automatically inferring \ndata.ow analysis .ow functions from data.ow fact schemas that are as\u00adcribed semantic meanings. By reducing \nthe burden on the analysis\u00adwriter, while at the same time guaranteeing that .ow functions are correct, \nwe hope that our work will not only make it easier to write correct program analysis tools, but will \nalso make it feasible to open up program analysis tools to safe user extension. In the future, we would \nlike to pursue less-syntactic (and there\u00adfore less-brittle) methods for representing and manipulating \nformu\u00adlas, such as using the E-graph data structure from Simplify [7] to better represent equalities. \nWe also would like to investigate in\u00adferring not only propagation rules but also the fact schemas them\u00adselves \nfrom just a set of desirable transformations.  References [1] J.M. Andreoli. Logic programming with \nfocusing proofs in linear logic. Journal of Logic and Computation, 1992. [2] T. Ball, R. Majumdar, T. \nMillstein, and S. K. Rajamani. Automatic predicate abstraction of C programs. In Programming Language \nDesign and Implementation (PLDI), June 2001. [3] R. Cartwright and D. Oppen. The logic of aliasing. Technical \nReport STAN-CS-79-740, Stanford University, September 1979. [4] R.L. Constable, S. F. Allen, H. M. Bromley, \nW. R. Cleaveland, J. F. Cremer, R. W. Harper, D. J. Howe, T. B. Knoblock, N. P. Mendler, P. Panangaden, \nJ. T. Sasaki, and S. F. Smith. Implementing Mathematics with the Nuprl Proof Development System. Prentice-Hall, \n1986. [5] P. Cousot. Veri.cation by abstract interpretation. In Veri.cation Theory &#38; Practice, volume \n2772 of LNCS. Springer-Verlag, 2003. [6] S. Das, D. L. Dill, and S. Park. Experience with predicate abstraction. \nIn Computer Aided Veri.cation (CAV), June 1999. [7] D. Detlefs, G. Nelson, and J. B. Saxe. Simplify: \nA theorem prover for program checking. Journal of the ACM, 52(3), May 2005. [8] M.J.C. Gordon. HOL: A \nproof generating system for higher-order logic. In VLSI Speci.cation Veri.cation and Synthesis.Kluwer \nAcademic Publishers, 1988. [9] S. Graf and H. Saidi. Construction of abstract state graphs of in.nite \nsystems with PVS. In Computer Aided Veri.cation (CAV), June 1997. [10] D. Gries. The Science of Programming. \nSpringer-Verlag, 1981. [11] T. A. Henzinger, R. Jhala, R. Majumdar, and G. Sutre. Lazy abstraction. In \nPrinciples of Programming Languages (POPL), January 2002. [12] M. Kauffmann and R.S. Boyer. The Boyer-Moore \ntheorem prover and its interactive enhancement. Computers and Mathematics with Applications, 29(2), 1995. \n[13] M. Kaufmann, P. Manolios, and J. S. Moore. Computer-Aided Reasoning: An Approach. Kluwer Academic \nPublishers, 2000. [14] S. Lahiri, T. Ball, and B. Cook. Predicate abstraction via symbolic decision procedures. \nIn Computer Aided Veri.cation (CAV),July 2005. [15] S. Lerner. Automatically Proving the Correctness \nof Program Analyses and Transformations. PhD thesis, University of Washington, March 2006. [16] S. Lerner, \nT. Millstein, and C. Chambers. Automatically proving the correctness of compiler optimizations. In Programming \nLanguage Design and Implementation (PLDI), June 2003. [17] S. Lerner, T. Millstein, E. Rice, and C. Chambers. \nAutomated soundness proofs for data.ow analyses and transformations via local rules. In Principles of \nProgramming Languages (POPL), January 2005. [18] T. Lev-Ami and M. Sagiv. TVLA: A system for implementing \nstatic analyses. In Static Analysis Symposium (SAS), June 2000. [19] S. Owre, J.M. Rushby, and N. Shankar. \nPVS: A prototype veri.cation system. In Automated Deduction (CADE), volume 607 of LNAI. Springer-Verlag, \n1992. [20] L. C. Paulson. Isabelle: A generic theorem prover, volume 828 of LNCS. Springer-Verlag, 1994. \n[21] F. Pfenning and C. Schurmann. Sytsem description: Twelf a meta\u00adlogical framework for deductive \nsystems. In Automated Deduction (CADE), volume 1632 of LNAI. Springer-Verlag, July 1999. [22] J. Regehr \nand U. Duongsaa. Deriving abstract transfer functions for analyzing embedded software. In Languages, \nCompilers, and Tools for Embedded Systems (LCTES), June 2006. [23] J. Regehr and A. Reid. HOIST: A system \nfor automatically deriving static analyzers for embedded systems. In Architectural Support for Programming \nLanguages and Operating Systems (ASPLOS), October 2004. [24] T. Reps, M. Sagiv, and A. Loginov. Finite \ndifferencing of logical formulas for static analysis. In European Symposium on Programming (ESOP), April \n2003. [25] T. Reps, M. Sagiv, and G. Yorsh. Symbolic implementation of the best transformer. In Veri.cation, \nModel Checking, and Abstract Interpretation (VMCAI), January 2004. [26] E. Rice, S. Lerner, and C. Chambers. \nAutomatically inferring sound data.ow functions from data.ow fact schemas. In Workshop on Compiler Optimization \nMeets Compiler Veri.cation (COCV),April 2005. [27] E. Rice, S. Lerner, and C. Chambers. Automatic inference \nof data.ow analyses. Technical Report UW-CSE-2006-06-04, University of Washington, June 2006. [28] C. \nSchurmann and F. Pfenning. Automated theorem proving in a simple meta-logic for LF. In Automated Deduction \n(CADE), volume 1421 of LNCS. Springer-Verlag, July 1998. [29] G. Yorsh, T. Reps, and M. Sagiv. Symbolically \ncomputing most\u00adprecise abstract operations for shape analysis. In Tools and Algorithms for the Construction \nand Analysis of Systems (TACAS), March 2004. \n\t\t\t", "proc_id": "1250734", "abstract": "<p>Previous work presented a language called Rhodium for writing program analyses and transformations, in the form of declarative flow functions that propagate instances of user-defined dataflow fact schemas. Each dataflow fact schema specifies a semantic meaning, which allows the Rhodium system to automatically verify the correctness of the user's flow functions. In this work, we have reversed the roles of the flow functions and semantic meanings: rather than <i>checking</i> the correctness of the user-written flow functions using the facts' semantic meanings, we automatically <i>infer</i> correct flow functions solely from the meanings of the dataflow fact schemas. We have implemented our algorithm for inferring flow functions from fact schemas in the context of the Whirlwind compiler, and have used this implementation to infer flow functions for a variety of fact schemas. The automatically generated flow functions cover most of the situations covered by an earlier suite of handwritten rules.</p>", "authors": [{"name": "Erika Rice Scherpelz", "author_profile_id": "81331503369", "affiliation": "Google, Kirkland, WA", "person_id": "P871670", "email_address": "", "orcid_id": ""}, {"name": "Sorin Lerner", "author_profile_id": "81100399150", "affiliation": "University of California, San Diego, San Diego, CA", "person_id": "PP36037589", "email_address": "", "orcid_id": ""}, {"name": "Craig Chambers", "author_profile_id": "81100528252", "affiliation": "University of Washington, Seattle, WA", "person_id": "PP39047059", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1250734.1250750", "year": "2007", "article_id": "1250750", "conference": "PLDI", "title": "Automatic inference of optimizer flow functions from semantic meanings", "url": "http://dl.acm.org/citation.cfm?id=1250750"}