{"article_publication_date": "06-10-2007", "fulltext": "\n Online Optimizations Driven by Hardware Performance Monitoring Florian T. Schneider Mathias Payer Thomas \nR. Gross Department of Computer Science Department of Computer Science Department of Computer Science \nETH Z\u00a8urich ETH Z\u00a8urich ETH Z\u00a8urich Zurich, Switzerland Zurich, Switzerland Zurich, Switzerland Abstract \nHardware performance monitors provide detailed direct feedback about application behavior and are an \nadditional source of infor\u00admation that a compiler may use for optimization. A JIT compiler is in a good \nposition to make use of such information because it is running on the same platform as the user applications. \nAs hard\u00adware platforms become more and more complex, it becomes more and more dif.cult to model their \nbehavior. Pro.le information that captures general program properties (like execution frequency of methods \nor basic blocks) may be useful, but does not capture suf.\u00adcient information about the execution platform. \nMachine-level per\u00adformance data obtained from a hardware performance monitor can not only direct the \ncompiler to those parts of the program that de\u00adserve its attention but also determine if an optimization \nstep actu\u00adally improved the performance of the application. This paper presents an infrastructure based \non a dynamic com\u00adpiler+runtime environment for Java that incorporates machine-level information as an \nadditional kind of feedback for the compiler and runtime environment. The low-overhead monitoring system \npro\u00advides .ne-grained performance data that can be tracked back to individual Java bytecode instructions. \nAs an example, the paper presents results for object co-allocation in a generational garbage collector \nthat optimizes spatial locality of objects on-line using measurements about cache misses. In the best \ncase, the execution time is reduced by 14% and L1 cache misses by 28%. Categories and Subject Descriptors \nD.3.4 [Processors]: Compil\u00aders, Optimization, Runtime Environments, Memory Management General Terms Measurement, \nPerformance Keywords Java, Just-in-time Compilation, Dynamic Optimiza\u00adtion, Hardware Performance Monitors \nThis research was supported, in part, by the NCCR Mobile Information and Communication Systems , a research \nprogram of the Swiss National Science Foundation, and by a gift from the Microprocessor Research Lab \n(MRL) of Intel Corporation. Permission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies are not made or distributed for \npro.t or commercial advantage and that copies bear this notice and the full citation on the .rst page. \nTo copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci.c \npermission and/or a fee. PLDI 07 June 11 13, 2007, San Diego, California, USA. Copyright c &#38;#169; \n2007 ACM 978-1-59593-633-2/07/0006. . . $5.00. 1. Introduction Object-oriented programming languages \nlike Java or C# allow changes to an executing program at runtime, e.g., through the use of a dynamic \nclass loader. At the same time, modern processor architectures are dif.cult compiler targets if the compiler \naims to optimize a program for speed of execution; features like prefetch\u00ading and prediction are (sometimes) \ndif.cult to model in a compiler. So a code generator is faced with two dif.culties: the dynamic nature \nof the target program complicates analysis of program prop\u00aderties (e.g., it is dif.cult to determine \npointer aliasing or to analyze the memory referencing patterns) and important performance as\u00adpects (e.g., \nnumber and location of cache misses) are only evident at runtime. Fortunately, programs written in such \nan object-oriented lan\u00adguage are usually executed in a virtual machine that includes a JIT (dynamic) \ncompiler. The dynamic compiler has the opportunity to immediately make use of information obtained at \nruntime. We dis\u00adtinguish between two kinds of information about an application that can be obtained at \nruntime: information that is independent of the execution platform like the execution frequency of methods, \nbasic blocks or instruc\u00adtions; often the term pro.les is used for this kind.  machine-level information, \ni.e. performance data about the lower levels of the execution platform (OS, hardware). Exam\u00adples for \nthis type of information are cache misses, TLB misses, or branch prediction failures.  Pro.les are a \nuseful input to the code generator (not only in a JIT compiler but also in an ahead-of-time compiler). \nHowever, many previous optimizations (static and dynamic) focus only on the platform-independent information \nand did not include direct feed\u00adback from the hardware level [23, 20]. Yet most modern CPUs (like the \nPentium 4, Itanium, PowerPC) have a performance measure\u00adment unit to obtain performance-related information \nand therefore could provide input to a dynamic code generator that optimizes a program for a speci.c \nhardware platform. To be useful for an optimizing JIT compiler and associated runtime system1 the collected \nperformance information must be accurate enough and cheap to obtain at run-time. There are a couple of \nrequirements for a module that makes information from the hardware performance monitors available in \nsuch an execution environment: The interface between the VM and the performance monitoring hardware should \nhide machine-speci.c details where possible. 1 We consider the JIT compiler, the virtual machine (VM), \nand the runtime system as one unit since all components must cooperate to perform most interesting optimizations. \nIt should be .exible to allow obtaining different execution met\u00adrics. The overhead to collect the data \nshould be as low as possible, and the system should not perturb executed applications too much.  The \ninformation must be accurate enough to be useful for online optimization. Often the granularity of a \nmethod or even a basic block is too coarse to infer which operation is responsible for some event (e.g. \ncache misses).  The platform should work for off-the-shelf VMs, with only small or no changes to the \ncore VM code. Otherwise the effort to port the infrastructure to another VM or to a new release would \nbe prohibitively large.  In this paper we show how a Java system can bene.t from using machine-level \nperformance data, but the approach and results are not tied to the Java programming language. Of course, \nany com\u00adpiler that uses platform-speci.c information may also use pro.le information, e.g., to decide \nwhere and when to exploit the results obtained from the performance measurement unit, but this aspect \nis not discussed further in this paper. We describe and evaluate a module to feed data from the hardware \nperformance measurement unit of a modern processor into the Java system. Our infrastructure is built \non top of the Jikes RVM, a freely available open-source research VM implemented in Java. In our system \nwe exploit spe\u00adcial features of the P4 processor that allow to correlate measured events to single instructions \nand to the source program (in our case Java bytecode). The overhead of the runtime hardware sampling \nis reasonably low (<1% avg). As an example application of our infrastructure we present a garbage collector \nthat is guided by online hardware feedback and report the results for a selection of standard Java benchmarks. \nThe garbage collector improves data locality of Java programs automat\u00adically by co-allocating heap objects \nusing information about data cache misses. The principal idea is to identify those objects and references \nthat produce the largest number of cache misses. The garbage collector uses these hints to adapt its \nbehavior for better data locality. Our systems is, however, not aimed just at data lo\u00adcality optimizations \nin the GC. Instead machine-level performance data should be thought of an additional feedback for the \nwhole run\u00adtime environment. We chose this optimization to demonstrate that the overhead of the approach \nis low in practice to allow a code gen\u00aderator/runtime system to deal with memory performance one of \nthe dif.cult areas for a compiler for object-oriented programs. The next section discusses prior work \nin the area. Section 3 shortly presents the hardware and software platform we used. We present an adaptive \nruntime monitoring system in more detail in Section 4. Section 5 describes an example application of \nour system on data locality optimization during GC. Finally, we discuss the overhead and the impact on \nperformance in Section 6. The key contribution of this paper is that it is actually possible to collect \ndetailed data from the hardware during runtime that can be directly applied for optimization by the runtime \nenvironment. 2. Related work There are two areas of prior work that we discuss in this paper: data gathering \ntechniques using pro.les or hardware performance monitors (HPMs) and data locality optimizations. For \ndata gather\u00ading techniques we focus on approaches for dynamic compilation and optimization. There exists \na fair bit of prior work about pro\u00ad.ling and pro.le-guided optimization in ahead-of-time compilers (see, \ne.g., [21, 12]) that is however not central to the topic of this paper. Hardware performance counters \nare frequently used for off-line performance analysis and characterization of workloads. Hauswirth et \nal. [16] study the interaction between the VM and the lower lev\u00adels of the execution platform (OS, libraries, \nhardware). They mea\u00adsure how these layers in.uence each other by introducing software performance counters \nwhich capture performance metrics of the software subsystems and correlate them to the information gath\u00adered \nby the hardware performance counters. To correlate data from the hardware with Java methods Georges et \nal [15] instrument method entries and exits with reads of the hardware performance counters. Their approach \nreduces the num\u00adber of instrumentations signi.cantly by .rst identifying execution phases and then only \ninstrumenting the start and the end of these phases. This way the high overhead of instrumenting every \nmethod can be avoided. Several high performance JVMs use adaptive optimization based on run-time pro.ling \n[4, 14, 27]. The Jikes RVM [7] uses timer-based sampling of the call stack to .nd frequently executed \nmethods. The frequency pro.les are used to determine where to spend the most effort for optimization: \nThe more often a method is invoked, the more expensive optimizations are applied to it. A static cost/bene.t \nmodel for the different optimizations is used to evalu\u00adate whether a method should be recompiled. It \nalso has the ability to use continuous pro.ling feedback to improve performance of long-running applications \n[8]. Several studies show that data locality optimizations can im\u00adprove the performance of programs with \nirregular memory access patterns. Field reordering [19, 22] is a technique that targets ob\u00adjects that \ndo not .t into one cache line. It places .elds with high temporal af.nity together to improve cache utilization. \nClass split\u00adting [13] achieves a similar effect by splitting data structures into two: a hot (frequently \naccessed) and a cold part. The hot parts are allocated together to avoid infrequently used data to use \nup cache memory. Usually these techniques rely on pro.ling information to approximate a good data layout \nbecause it is generally hard to stat\u00adically optimize data locality in object-oriented programs. Adl-Tabatabai \net al. [3] present a dynamic optimization to elim\u00adinate long-latency cache misses. They insert prefetch \ninstructions after dynamically monitoring cache misses using hardware perfor\u00admance monitors of the Itanium \nprocessor. The approach exploits the fact that objects that are linked through a reference often have \na constant delta between their starting addresses. Software prefetch\u00ading must be used consciously because \nfetching the wrong data into the cache may have a negative performance impact. By using hard\u00adware performance \nmonitors to guide the prefetching they achieve a speedup of 14% for the SPEC JBB2000 benchmark. In our \nappli\u00adcation we do not target prefetching, but instead we reorder object instances to reduce the number \nof cache misses. Online object reordering [17] is a different dynamic locality optimization for Java. \nIt reorders objects at garbage collection time using a copying GC. The heuristic for reordering is determined \nby pro.ling the .eld access operation with a light-weight pro.ling mechanism. Objects with hot .elds \n(frequently accessed) are placed adjacent to their referent objects to increase spatial locality by visiting \nthose references .rst during the copying process in the GC. and reduce the number of cache misses. The \napproach requires a copying garbage collector (which is present in many modern VMs). Our work takes a \nsimilar approach, but we do not rely on execution frequencies as a metric for locality. Instead we use \ndirect feedback from the memory hierarchy about cache misses to guide compiler and GC decisions. Similar \nideas have been used to improve code locality. Dynamic code management [18] is a code reordering algorithm \nto improve code locality and reduced ITLB stalls. The system builds up a call graph at runtime and uses \na light-weight reordering heuristic to determine the optimized code layout which results in a speedup \nof upto 6%. Shuf et al [24] also use the memory management system to im\u00adprove data locality and present \nan object allocation scheme that at\u00adtempts to place frequently instantiated types that are connected \nvia references close together in memory. They also show that a locality\u00adbased heap traversal algorithm \ncan improve GC performance. Lau et al [20] show how to use direct measures of performance (cycle counts) \nto guide inlining decisions in a dynamic compiler. The JIT generates two version of each method: one \nwith aggressive inlining and one with the default (more conservative) heuristic. By executing each of \nthe two versions randomly during the measure\u00adment phase the compiler collects timing information about \neach version. After .ltering out outliers it can use those timings to de\u00adcide which version of the method \nto use in future. Our approach also uses real machine metrics as feedback, but gathers more .ne\u00adgrained \ninformation about the program s interaction with the exe\u00adcution platform (like cache or TLB misses). \n 3. Background 3.1 P4 hardware performance monitors The P4 offers a large variety of performance events \nfor counting [2, 26]. Two modes of operation are supported: Normal counting: The performance counters \nare con.gured to count events detected by the CPU s event detectors. A tool can read those counter values \nafter program execution and reports the total number of events. This mode can be used to obtain numbers \nlike cache miss rate, total execution cycles, etc.) More .ne-grained information (e.g., on a method level) \ncan be ob\u00adtained by instrumenting the program for reading counter values. An application of this mode \nwould be to evaluate the precise ef\u00adfect of program transformations.  Sampling-based counting: Whenever \na certain number of events has occurred, the CPU samples its register contents. This way it is possible \nto locate the source of an event. The P4 supports precise event-based sampling (PEBS), i.e., it reports \nthe exact instruction where the sampled event happened and the register contents at that point although \nthe design is heavily pipelined. Previous CPUs could only measure an approximate location for sampled \nevents because of a super-scalar design and out\u00adof-order execution.  To reduce the overhead of sampling, \nthe CPU has a special microcode routine that saves the CPU state to a buffer supplied by the OS whenever \nthe interval counter triggers the sampling of an event. Setting the interval counter to n means that \nevery nth event will be sampled. 3.2 Jikes RVM Our implementation is done with the IBM Jikes RVM (version \n2.4.2) [5, 4], a high performance Java virtual machine written mostly in Java. It includes an adaptive \noptimization system (AOS) [7]: First, every method is compiled with a simple and quick base\u00adline compiler. \nTo estimate the execution frequency for methods it samples the call stack in regular intervals and records \nwhich methods are on top of the stack. Methods that are executed fre\u00adquently enough are recompiled and \noptimized further. The VM uses a static cost model to decide which optimization level to apply for a \nmethod.  4. Infrastructure 4.1 Event sampling We use the precise event-based sampling (PEBS) feature \nof the P4 processor [2] to measure cache misses. This mechanism has two advantages that make it useful \nfor monitoring applications during runtime: First, the CPU collects event samples on its own using a \nmicrocode routine and stores them into a buffer supplied by the OS kernel module. An interrupt is generated \nonly when this buffer is .lled to a speci.ed mark. The second advantage is that PEBS reports the exact \ninstruction (program counter plus all register contents) for the sampled events. This allows the compiler \nto recover higher-level information about the collected events, e.g., method, bytecode instruction, or \n.eld variable accessed. The P4 has a number of events that can be selected for PEBS (e.g. L1, L2 cache \nmisses and DTLB misses), but it allows only one event to be measured at a time. The system consists of \nthree parts, see [23] for an overview and general description: 1. Perfmon loadable kernel module [1]2: \nThis kernel module is part of the Perfmon infrastructure and is developed at HP. It offers the functions \nto access the performance counter hardware for a variety of hardware platforms. The kernel module hides \nthe platform-speci.c details from the JVM. It also provides the interrupt handler that is called by the \nsampling hardware when the CPU buffer for the samples is full. 2. Native shared library (C): Since we \ncannot call device drivers directly from Java or from the Jikes RVM we developed a native library to \nprovide an interface to the kernel functions and access it via the Java Native Interface (JNI). The library \nallows to read samples from the kernel module. The challenge here is to make the data exchange between \nJava and the kernel as ef.cient as possible. We provide a pre-allocated array to the native code. The \nlibrary function then copies all collected samples into this array directly without any JNI calls. We \nonly need to make sure that the GC does not interfere during this transfer. This can be done by disabling \nthe GC for the short period of time while the samples are copied from the native library. Since no allocation \nhappens in the native code that is responsible for copying, we can make sure that the GC is not triggered \nwhile samples are copied from the kernel space. 3. Collector thread (Java): We use a separate Java thread \nthat polls the kernel device driver via the JNI interface whether there are any new samples. The polling \ninterval is adaptively set between 10ms and 1000ms depending on the size of the sample buffer and the \nsampling rate. This makes sure that no samples will be dropped due to a full sample buffer.  The copying \nof samples into user-space is necessary to allow the use of a different hardware platform with very few \nchanges to the user-space library. The library is not limited to Java and can also be used from other \nruntime environments and languages. Basically the system can also be integrated with other Java VMs that \nsupport JNI. 4.2 Mapping HPM data to Java bytecode One sample on the P4 platform has a size of 40 bytes. \nIt contains the program counter (EIP) where the sampled event occurred and the values of all registers \nat that time. At the moment we do not monitor the data register contents and only analyze the EIP register. \nTo actually use the raw data for optimization, we need to obtain higher-level information about each \nsample: 2 available for download at http://www.hpl.hp.com/research/linux/perfmon/ First the collector \nthread extracts the samples that are of impor\u00adtance for the VM. Addresses outside the VM address space \n(e.g., from kernel space or native libraries) are dropped immediately since we are only interested in \nevents that occur in the machine code generated by the JIT compiler. The next step is to .nd the Java \nmethod where the event happened. For this lookup we keep a sorted table of all methods with their start \nand end address. Whenever a method is compiled the .rst time or recompiled by the optimiz\u00ading compiler \nwe update its entry accordingly. For simplicity, code for compiled methods is allocated in the immortal \nobject space of the VM which is not garbage-collected. This way the copying GC does not move compiled \ncode which would require an update of the lookup table after every GC run (up to 18K method objects in \nour benchmarks). The resulting space overhead due to stale method ob\u00adjects is however reasonably small \nbecause in our setup only a small fraction of methods are re-compiled and replaced by the optimizing \ncompiler. Finally the system determines the exact bytecode instruction for each sample. For this purpose \nwe need to extend the instruction mapping information that the compiler keeps for each method: Ba\u00adsically, \nwe need to store a machine code mapping like a source\u00adlevel debugger (from machine code addresses to \nJava bytecode in this case). This is already performed for methods that are compiled with the baseline \ncompiler. For opt-compiled methods however the compiler only stores this information for the GC points. \nWe ex\u00adtended the optimizing compiler so that it generates the bytecode index mapping for each machine \ncode instruction, not only for GC points. From that point on we are able to count events for each IR \n(intermediate representation) instruction. Those event counts are updated by the sample collector thread \nperiodically. This step is required to keep the IR data structures in memory after compila\u00adtion. We found \nthat the additional space overhead does not affect application performance signi.cantly.  5. Approach \nIn this section we show an example optimization for data locality that applies the gathered performance \ndata in a modi.ed genera\u00adtional garbage collector [30]. 5.1 HPM-guided co-allocation in the GC Our system \nuses a generational mark-and-sweep garbage collector. It does bump-pointer allocation for young objects \nand copies ma\u00adtured objects into a mark-and-sweep collected heap. Tenured ob\u00adjects are managed using \na free-list allocator that allocates objects into 40 different size classes up to 4 KBytes (=VM default \nsetting) to minimize heap fragmentation. Larger objects are handled in a separate portion of the heap. \nThis collector offers better space ef.ciency than a pure copying GC (no copy reserve needed). On the \nother hand a copying GC is known to generally enhance data locality [9]. The goal of our co\u00adallocation \nis to combine those two advantages, i.e. having a space\u00adef.cient GC that provides good data locality \nautomatically by using feedback from the hardware: The online optimization consists of three parts: 1. \nFiltering of instructions of interest at method compilation time 2. Monitoring cache misses for individual \nclasses and references 3. Nursery tracing algorithm that support co-allocation  The .rst part is performed \nfor each method compiled by the opt\u00adcompiler. As a consequence the monitoring system does not con\u00adsider \ninstructions in non-optimized methods. However, this is not a major limitation since those methods are \nrarely executed (other\u00adwise they would be selected for re-compilation by the JIT). Part two is done concurrently \nto the execution of the application. The class A { void foo() { Ax; A y; ... = p.y.i; int i; }} I1: aload_2 \n// Local var p I2: getfield y; // Load field y I3: getfield i; // Load field i Figure 1. Example bytecode \nfor expression p.y.i. sample collector thread periodically invokes the monitoring mod\u00adule that performs \nthe bookkeeping and translates the raw data. The third part is implemented in the garbage collector where \nthe cache miss data about .eld references are used to guide co-allocation. 5.2 Finding source instructions \nFor each method that is compiled with the opt-compiler (as selected by the AOS) the sample collector \nthread performs an additional pass to .lter out instructions that must be monitored for cache misses \nin the HPM module: we are interested in reads/writes to objects that are referenced from another heap \nobject. Initially, the compiler creates a mapping of instruction pairs: For each heap access instruction \nS it checks if the target address is loaded from a .eld variable f (also located on the heap). If yes, \nit saves a tuple (S, f). The motivation is that co-allocating the parent object with the child object \nincreases the chance that both objects lie in the same cache line. This way the child object is implicitly \nprefetched when accessing the parent object. The opt-compiler computes this mapping by walking the use-def \nedges upwards from heap access instructions (.eld/array access, virtual calls and object\u00adheader access). \nFigure 1 shows an example access path expression with its Java bytecode. Our analysis would create a \nmapping with instruction and .eld y (I3, A::y). For illustration we show the bytecode here \u00adinternally \nwe actually use the actual high-level IR instructions that correspond to the bytecode. If we encounter \na miss on I3 (load of .eld i), we increase the event count for associated reference .eld (A::y). We keep \na per-reference event count which tells the runtime system how many misses occurred when dereferencing \nthe corresponding access path expressions. 5.3 Online monitoring Samples from the HPM unit are buffered \nand processed in batches inside the VM: a sample is attributed to a reference .eld f if the source instruction \nS is among the instructions of interest (i.e. a mapping (S, f) exists). Currently we set the system up \nto monitor events in the application classes only and exclude events occurring inside VM code. This is \nnot a limitation of the monitoring system itself, but just because the optimization deals with objects \nallocated in the user code. The rate of events for each reference .eld is measured through\u00adout the execution \nand this allows detecting phase changes in the execution or checking whether an optimization decision \nby the JIT or the GC had a positive or a negative impact. On many platforms, the effect of a data locality \noptimization is dif.cult to predict in general. A system that includes feedback based on a performance \nreporting unit allows an assessment of the effectiveness of an op\u00adtimization step. If the transformation \nimproved performance, the system can proceed normally. If the transformation reduced per\u00adformance, either \na different optimization step can be performed or it is possible to revert to the old code. This system \nis a step into an performance-aware runtime environment that can judge which optimizations actually bring \nbene.ts and which do not. 5.4 Nursery tracing with co-allocation When the GC hits an object that contains \na reference .elds during performing a nursery space collection it checks if it is possible to co-allocate \nthe most frequently missed child object: we have to check if both objects together do not exceed the \nsize limit for the free-list allocator. Object larger than this limit are allocated in a separate large \nobject space. The VM keeps a list the reference .elds for each class type sorted by number of associated \ncache misses. When deciding to co-allocate two objects the GC just requests enough space to .t both objects. \nThey will be assigned to the appropriate size class by the free-list allocator that manages the mature \nspace. Without co-allocation the objects may -depending on their size -end up in different size classes. \nThis would reduce spatial locality in the mature space. Note that this approach may increase internal \nfragmentation because there is only a limited number of size classes (40 in our allocator) that do not \ncover each size exactly. The actual results depend on how co-allocated objects .t into their assigned \nsize classes. We chose the GenMS collector because we want to combine space-ef.ciency and good locality. \nNone of the existing collectors provides this combination. Of course an optimized static copying strategy \ncould achieve a similar bene.t in many scenarios [25], but adapting to an individual application s memory \naccess pattern proved to be important [17], and it has been shown that data locality optimizations often \nhelp in some cases and hurt in others. Detecting those cases at run-time is a strong argument for using \nperformance counters for guidance.  6. Evaluation 6.1 Experimental platform We carried out our experiments \non a 3 GHz Pentium 4 with 1M L2 cache and 1 GB of main memory. The L1 cache for data is 16K. One cache \nline contains 128 bytes. The P4 has an out-of-order execution engine and can issue several instructions \nin parallel. It also includes hardware-based prefetching of data streams. The system runs a Linux 2.6.16 \nkernel with the Perfmon2 patches and the corresponding libpfm library (version 3.2). The baseline to \nwhich we compare our measurements is Jikes RVM 2.4.2 with the FastAdaptiveGenMS build con.guration which \nis one of the most ef.cient con.gurations (default pro\u00adduction build). It includes the adaptive optimizing \nJIT compiler [7] and a generational garbage collector with an Appel-style vari\u00adable size nursery [6]. \nThe mature space is managed by a mark-and\u00adsweep collector. This collector is included with MMTk [10] \n-the garbage collection framework that comes together with the Jikes RVM. We use a pseudo-adaptive con.guration \nfor the Jikes JIT com\u00adpiler. Each program runs with a pre-generated compilation plan. This ensures that \nthe compiler optimizes exactly the same meth\u00adods and the variations due to the adaptive optimization \nsystem are minimized. The different benchmarks are listed in Table 1. All timing results are averages \nover 3 program executions using the largest input size for the SPECjvm98 programs and the default input \nsize for the DaCapo programs3. We also report the standard deviations for the execution times but found \nthose to be very small in practice. 3 The programs chart, eclipse and xalan were excluded because they \nare not compatible with version 2.4.2 of Jikes RVM. db mtrt compress Programs from the SPEC JVM98 jess \nbenchmarks [29] with the largest javac workload (s=100) repeated 3 times. mpegaudio jack antlr Programs \nfrom the DaCapo benchmark bloat suite (version 10-2006 MR-2) [11]. fop hsqldb jython luindex lusearch \npmd pseudojbb This is a version of SPEC JBB2000 [28] with a .xed number of transactions (n=100000, max \n6 warehouses). Table 1. Benchmark programs. The chosen sampling intervals are randomized by changing \nthe lower order bits randomly (8 bits in our con.guration). This should prevent us from measuring biased \nresults by sampling at the same locations over and over. 6.2 Time and space overhead of runtime monitoring \nIn this section we show how expensive the runtime monitoring in\u00adfrastructure is in terms of execution \ntime and space overhead. Both must be reasonable to make optimization using runtime monitoring possible. \nThe systems needs to allocate additional memory for gathering detailed source-level performance data. \nFirst, there are buffers for temporary storage of the samples collected. The user-space library keeps \nan 80K byte buffer and the VM data collection thread stores the raw data in an int[] array of the same \nsize. In the VM we need additional tables to resolve raw samples to Java methods and bytecode. The space \noverhead of the additional meta-data in the VM is shown in Table 2. The second column (machine code) \nshows the size of the machine code generated by the compiler in KBytes. Column 4 (MC maps) shows the \nsize of the machine code maps that are needed to resolve raw samples. For comparison, we show the size \nof the GC maps alone in Column 3. The last row shows the total size and the map sizes of the Jikes boot \nimage. The boot image maps are pre-generated at compile\u00adtime and do not contribute to execution time. \nWe can see that the machine code maps are 4 to 5 times as large as the GC maps, but the total sizes of \nthe maps for an application are tiny compared to the maps that are contained in the boot image. We consider \nonly library and application classes and leave out VM internal classes at the moment because we do not \nconsider them for optimization. Including these would just make the boot image larger but would not in.uence \napplication performance. Cur\u00adrently, the whole boot image is about 9MB bigger than the origi\u00adnal (increase \nof 20% from 45M to 54M). The maps for application classes take up to 5x the space needed for the GC maps. \nHowever, in absolute numbers the size of the maps generated is moderate (up to 1870K bytes for jython). \nOverall the amount is small compared to the maximal heap size There is potential for improving the space \nef.ciency of the machine code mapping to reduce the size of the boot image. We reused the existing implementation \nfor GC maps and it would be possible to custom-tailor the data structure for our needs. But the program \nmachine code GC maps only MC maps compress 12 6 28 jess 20 12 43 db 7 4 20 javac 55 30 140 mpegaudio \n71 31 168 mtrt 46 26 120 jack 40 22 111 pseudojbb 316 164 948 antlr 38 26 90 bloat 77 46 247 fop 8 4 \n16 hsqldb 117 67 290 jython 685 422 1870 luindex 119 58 316 lusearch 93 46 239 pmd 64 43 174 boot image \n14975 10380 8260 Table 2. Space overhead: Size of machine code maps in KB. runtime overhead of using \nthe existing data structures is low enough to use it for our purpose. Figure 2 shows the execution time \ncompared to the original VM con.guration without runtime event sampling using different sam\u00adpling intervals \nfrom 25K to 100K. In this experiment we con.gured the system to monitor L1 cache misses. We measured \nthe execu\u00adtion time for 3 different sampling intervals to evaluate the relation between sampling rate \nand execution time overhead. The auto con.guration has a variable sampling interval because it adaptively \nchanges the sampling interval at runtime. The reported numbers for execution time are averages over 3 \nexecutions of each program, and they include all overhead from mapping raw sample data. For most programs \nthe time overhead is proportional to the sam\u00adpling rate (e.g. db and pseudojbb). A smaller sampling interval \nmeans higher sampling frequency and thus more data to be pro\u00adcessed by the monitoring module. For others \n(e.g., mpegaudio)the constant portion of the overhead dominates. The absolute number of samples is not \nvery high in these cases. The worst case is an in\u00adcrease of almost 3% for mtrt, compress and hsqldb with \nthe small\u00adest interval(25K). For the auto interval setting and an interval of 100K the average overhead \nis below 1% a value that is low com\u00adpared to software-only pro.ling techniques. Our experiments with \ndata locality optimizations indicate that for our set of programs the largest interval is often suf.cient \nto obtain enough coverage. 6.3 Effect of object co-allocation Now we study the effect of the GC optimization \nthat we performed using the runtime performance data. We compare the baseline with our co-allocating \nGC in different con.gurations and use L1 cache misses to guide the optimization. For the execution time \nwe used a fully autonomous mode that adapts the sampling interval to obtain a certain number of samples \nper second. With a sampling approach the choice of an appropriate sampling interval is critical. The \nin\u00adterval should be .ne enough to give a statistically representative picture of the program behavior. \nBut, since we are performing the sampling during program execution the overhead should also be reasonably \nlow. The runtime overhead is proportional to the num\u00adber of samples collected by the VM.4 4 In automatic \nmode the only monitoring parameter is samples/sec -in practice we found that a default of 200 samples/sec \nprovides reasonable accuracy and low overhead for all benchmarks programs. Figure 3 shows the number \nof co-allocated object for different sampling intervals using a logarithmic scale. There are 2 programs \n(compress and mpegaudio) where no objects are co-allocated. They allocate mostly large objects which \nare placed in the sepa\u00adrate large-object space by the allocator or only allocate few objects. Therefore, \nthey have no candidate objects for co-allocation. The programs with a large number of co-allocated objects \n(db, pseudo\u00adjbb, hsqldb, luindex and pmd) are less sensitive to the choice of the sampling interval: \nThe largest interval is enough to cover most ob\u00adjects. In the remaining programs the number of co-allocated \nobjects is several orders of magnitude lower, and co-allocation is more sen\u00adsitive to the choice of the \nsampling interval. On the other hand, the impact on L1 cache performance is also less signi.cant because \nthe absolute number of objects is much smaller.  Performance impact Figure 4 shows the number of L1 \ncache misses with co-allocation in the GC turned on relative to the baseline using a large heap size. \nIn Figure 5 we summarize the impact on application performance using a range of heap sizes (1-4x minimum \nheap size). For large heaps, there is a noticeable reduction in L1 cache misses using HPM-guided co-allocation \nfor several programs (jess, db, pseudojbb, bloat and pmd). mpegaudio shows varying num\u00adbers (from -6% \nto +5%) that are not due to co-allocation (no candi\u00addate objects), but rather show in.uences from the \nevent monitoring and processing. There is little or no effect on the other programs. From all benchmarks \ndb gets the most bene.t: 28% fewer L1 cache misses. This bene.t translates into an execution time reduction \nof up to 13.9%. Figure 6 analyzes the performance of db in more detail. Now we compare a generational \ncopying (GenCopy) collector versus the generational mark-and-sweep (GenMS) with object co-allocation. \nThe GenCopy collector generally improves spatial locality in the mature space over a non-moving collector \n-on the other hand it has a larger GC cost at small heap sizes [9]. This result is con.rmed in our experiment. \nThe maximum speedup versus GenCopy is only 10% vs. 13.9% compared to the baseline GenMS collector without \nco-allocation. We also see that GenMS + co-allocation outperforms GenCopy throughout all heap sizes (from \n7% for large heaps to 10% for a small heap). This indicates that the GenMS collector with object co-allocation \ncombines good locality with space-ef.ciency. The reduction on L1 misses for jbb, one of the most memory\u00adintensive \nprograms from this suite, is only between 2 and 6%. The resulting speedup is up to 2% for large heaps. \nHere we observe that there are many frequently missed objects (2.4 million objects were co-allocated) \nand that the majority of those objects are relatively large (long[] arrays with a size of >128 bytes). \nAs a consequence, optimizing for reduced cache misses at the cache-line level does not yield a signi.cant \nbene.t for this program. (Using TLB misses as driver for the optimization decisions does not improve \nthe results.) For the majority of the JVM98 benchmarks the number of co\u00adallocated objects is rather small \n(in the order of thousands). There are not many mature objects that cause frequent cache misses: These \nprograms have relatively small working sets and/or many young objects that do not bene.t from better \nspatial locality in the mature space. Overall, three programs (db, pseudojbb, bloat) show a speedup, \nand 7 programs are slightly slowed down by using dynamic co-allocation. The worst case for large heaps \nis javac with -2.1% which is similar to the sampling overhead reported in Figure 2. Note that monitoring \nis turned on throughout the whole execution even when no candidate objects are found. The overhead could \nbe reduced by turning off monitoring for most of the time in such a scenario. For small heaps sizes the \npicture looks different. db is the only program that still shows a speedup at minimum heap size (9.3%). \n coalloc occurs exactly when the co-allocation is switched on. The stepwise-constant shape of the measurement \nis caused by our batch-processing of samples in the monitoring module. Figure 6. GenCopy vs GenMS with \nco-allocation Figure 7(b) shows the L1 cache miss rate over time. It is locally quite volatile (in part \nalso due to our monitoring infrastructure), but Normalized execution time: _209_db larger the allocated \nchunks the more internal fragmentation exists due to the limited number of size classes in the free-list \nallocator. 1.05 When running at the minimum heap size this space overhead factor gets more dominant and \nalmost all programs are either slowed 1 down or have a smaller speedup (e.g., db) with co-allocation. \n0.95 6.4 Runtime feedback 0.9 To actually guide optimization automatically a VM needs accu\u00ad 0.85 rate \nfeedback. Figure 7 depicts two types of data that we col\u00adlect for programs, here shown for the db benchmark: \nFigure 7(a) 0.8 shows the cumulative total count of L1 cache misses when deref\u00ad 1x 1.5x 2x 3x 4x erencing \nthe .eld String::value. The sharp bend for dyn\u00ad compresscompressjessjessdbdbjavacjavacmpegaudiompegaudiomtrtmtrtjackjack \n pseudojbbpseudojbb antlrantlr bloatbloat fop fophsqldb hsqldb jythonjythonluindexluindexlusearchlusearchpmd \npmd Figure 3. Number of co-allocated objects at different sampling intervals (heap size = 4x min heap \nsize). Generally, co-allocation yields better results at larger heaps. The  Figure 2. Execution time \noverhead compared to the baseline con.guration with different sampling intervals (heap size = 4x minimum \nheap size). Number of coallocated objects Overhead of sampling Reduction of L1 cache misses  compressjessdbjavacmpegaudiomtrtjackpseudojbbantlrbloatfophsqldbjythonluindexlusearchpmd \n25K 50K 100K Figure 4. L1 miss reduction with co-allocated objects (heap size = 4x minimum heap size). \nNormalized execution time 1x   1.5x 2x 3x 4x Figure 5. Execution time relative to the baseline \nfor different heap sizes (sampling interval is auto-selected, heap size from 1-4x min heap size). we \ncan see the drop in the miss rate at the same time as in Figure 7(a) when co-allocation becomes active \nafter the warm-up phase. The bold lines show the actual measured values. In addition we plot the moving \naverage over the last 3 periods for both versions as thin lines. This metric follows the general trend \nwithout heavy local .uctuations. The precise association of the miss events with object types and references \nallows the VM to assess the effect of individual optimization decisions: in this case the internal char[] \nwas co-allocated with the String object which resulted in a total reduction of misses on those objects \nby around 60%. For long-running application the VM also needs to detect when an optimization has a negative \neffect on overall performance. To il\u00adlustrate such a situation we show the cache misses over time when \nthe GC happens to perform a poorly performing optimization in a controlled setting. Figure 8 shows the \ncache misses over time for String objects in db starting out with a good allocation order. We then instructed \nthe GC manually to place one cache line of empty space (128 bytes) between the String and the char[] \nobjects \u00adeffectively undoing the originally well performing setting. Moni\u00adtoring the cache miss rate \nfor individual classes allows the system to discover that this transformation does not improve performance, \nand after several measurement periods it triggers a switch back to the original con.guration. Currently, \na simple heuristic is used to determine when to switch, and we are still investigating suitable settings. \nAlso, mature objects that are already co-allocated remain in place -only newly promoted objects will \nfollow the new copy\u00ading policy. Figure 8 shows the effect on the miss rate after switching back to the \noriginal allocation policy; the miss rate returns to its old value. We did not see such a situation where \nundoing co-allocation was necessary during our experiments with co-allocation this may be more important \nfor other optimizations.  7. Conclusions We presented a system that uses the results of a modern hardware\u00adbased \nperformance monitoring unit. As an example we discussed a L1 misses _209_db L1 misses _209_db time \nin cycles time in cycles (a) Total number of cache misses: The sharp bend for dyn-coalloc indi-(b) Miss \nrate over time: after the co-allocation starts the miss rate goes down cates the time when co-allocation \nkicks in Figure 7. Effect of co-allocation: Cache misses sampled for String objects db db: events for \nString objects 450 400 350 300 250 200 150 100 50 0 time in cycles Figure 8. Cache misses sampled for \nString objects db with an hardware architecture is very hard, and it is often a challenge to pre\u00addict \nthe effect of an optimization prior to performing the transfor\u00admation. Feedback from the lower layers \nof the execution platform can be valuable information to guide such optimizations.  8. Acknowledgments \nWe thank the referees for their helpful comments.  References [1] Perfmon project. http://www.hpl.hp.com/research/linux/perfmon/. \n[2] IA-32 Intel Architecture Software Developer s Manual, Volume 3: System Programming Guide. 2005. [3] \nA.-R. Adl-Tabatabai, R. L. Hudson, M. J. Serrano, and S. Subra\u00admoney. Prefetch injection based on hardware \nmonitoring and object metadata. In Proc. of Conf. on Programming Language Design and Implementation (PLDI \n2004), pages 267 276, New York, NY, USA, 2004. ACM Press. poorly performing locality optimization data \nlocality optimization to show how such performance informa\u00adtion can be used in a dynamic runtime environment. \nThe overhead imposed by monitoring is reasonably low (<1% avg), and with ap\u00adpropriate compiler assistance \nit is possible to map performance\u00adrelated events to source-level constructs. Our example optimization \nshows that a garbage collector with knowlegde about frequently missed objects and references can improve \ndata locality and can detect at run-time if a data-locality optimization has a positive or a negative \nimpact on performance. With the co-allocation technique for matured objects that is discussed here, L1 \ncache misses are re\u00adduced by up to 28%. The resulting application speedup is up to 14%, though this optimization \nis effective only for some programs. A more re.ned model of the micro-architecture in the compiler may \nbe able to better exploit the performance data. The infrastructure is .exible to allow compiler and GC \nimple\u00admenters to include such information into their system as an addi\u00adtional source of runtime feedback. \nIn our system the VM can ac\u00adtually observe the effect of data locality optimization. This is es\u00adpecially \nimportant since modeling the behavior of complex modern [4] B. Alpern, C. R. Attanasio, J. J. Barton, \nA. Cocchi, S. F. Hummel, D. Lieber, T. Ngo, M. F. Mergen, J. C. Shepherd, and S. Smith. Implementing \nJalapeno in Java. In Proc. of the ACM Conf. on Object-Oriented Programming, Systems, Languages, and Applications \n(OOPLSA 1999), pages 314 324, 1999. [5] B. Alpern, D. Attanasio, J. Barton, M. Burke, P. Cheng, J.-D. \nChoi, A. Cocchi, S. Fink, D. Grove, M. Hind, S. F. Hummel, D. Lieber, V. Litvinov, T. Ngo, M. Mergen, \nV. Sarkar, M. Serrano, J. Shepherd, S. Smith, V. C. Sreedhar, H. Srinivasan, and J. Whaley. The Jalapeno \nvirtual machine. IBM Systems Journal, Java Performance Issue, 39(1), 2000. [6] A. W. Appel. Simple generational \ngarbage collection and fast allocation. Softw. Pract. Exper., 19(2):171 183, 1989. [7] M. Arnold, S. \nFink, D. Grove, M. Hind, and P. F. Sweeney. Adaptive optimization in the Jalapeno JVM. In Proc. of the \nConf. on Object-Oriented Programming, Systems, Languages, and Applications (OOPSLA 2000), pages 47 65, \nNew York, 2000. ACM Press. [8] M. Arnold, M. Hind, and B. G. Ryder. Online feedback-directed optimization \nof Java. In Proc. of the Conf. on Object-Oriented Pro\u00adgramming, Systems, Languages, and Applications \n(OOPSLA 2002), pages 111 129, New York, USA, 2002. ACM Press. [9] S. M. Blackburn, P. Cheng, and K. S. \nMcKinley. Myths and realities: the performance impact of garbage collection. In SIGMETRICS 2004/PERFORMANCE \n2004: Proc. of the Joint Intl. Conf. on Measurement and Modeling of Computer Systems, pages 25 36, New \nYork, NY, USA, 2004. ACM Press. [10] S. M. Blackburn, P. Cheng, and K. S. McKinley. Oil and water? high \nperformance garbage collection in Java with mmtk. In Proc. of the Intl. Conf. on Software Engineering \n(ICSE 04), pages 137 146. IEEE Computer Society, 2004. [11] S. M. Blackburn, R. Garner, C. Hoffman, A. \nM. Khan, K. S. McKinley, R. Bentzur, A. Diwan, D. Feinberg, D. Frampton, S. Z. Guyer, M. Hirzel, A. Hosking, \nM. Jump, H. Lee, J. E. B. Moss, A. Phansalkar, D. Stefanovi\u00b4c, T. VanDrunen, D. von Dincklage, and B. \nWiedermann. The DaCapo benchmarks: Java benchmarking development and analysis. In Proc. of the Conf. \non Object-Oriented Programing, Systems, Languages, and Applications (OOPSLA 2006), New York, Oct. 2006. \nACM Press. [12] P. P. Chang, S. A. Mahlke, and W. W. Hwu. Using pro.le information to assist classic \ncode optimizations. Software Practice and Experience, 21(12):1301 1321, Dec 1991. [13] T. M. Chilimbi, \nB. Davidson, and J. R. Larus. Cache-conscious structure de.nition. In Proc. of the ACM SIGPLAN 99 Conf. \non Programming Language Design and Implementation (PLDI 1999), pages 13 24, New York, NY, USA, 1999. \nACM Press. [14] M. Cierniak, G.-Y. Lueh, and J. M. Stichnoth. Practicing judo: Java under dynamic optimizations. \nIn Proc. of the ACM Conf. on Programming Language Design and Implementation (PLDI 2000), pages 13 26, \nNew York, NY, USA, 2000. ACM Press. [15] A. Georges, D. Buytaert, L. Eeckhout, and K. D. Bosschere. Method-level \nphase behavior in Java workloads. In Proc. of the ACM SIGPLAN Conf. on Object-Oriented Programming, Systems, \nLanguages, and Applications (OOPSLA 2004), pages 270 287, New York, NY, USA, 2004. ACM Press. [16] M. \nHauswirth, P. F. Sweeney, A. Diwan, and M. Hind. Vertical pro.ling: understanding the behavior of object-priented \napplications. In Proc. of Conf. on Object-Oriented Programming, Systems, Languages, and Applications \n(OOPSLA 2004), pages 251 269, New York, NY, USA, 2004. ACM Press. [17] X. Huang, S. M. Blackburn, K. \nS. McKinley, J. E. B. Moss, Z. Wang, and P. Cheng. The garbage collection advantage: improving program \nlocality. In Proc. of the ACM Conf. on Object-Oriented Programming, Systems, Languages, and Applications \n(OOPSLA 2004), pages 69 80, New York, NY, USA, 2004. ACM Press. [18] X. Huang, B. T. Lewis, and K. S. \nMcKinley. Dynamic code management: Improving whole program code locality in managed runtimes. In VEE \n06: Proc. of the Intl. Conf. on Virtual Execution Environments, pages 133 143, New York, USA, 2006. ACM \nPress. [19] T. Kistler and M. Franz. Automated data-member layout of heap objects to improve memory-hierarchy \nperformance. ACM Trans. Program. Lang. Syst., 22(3):490 505, 2000. [20] J. Lau, M. Arnold, M. Hind, and \nB. Calder. Online performance auditing: Using hot optimizations without getting burned. In Proc. Conf. \non Programming Language Design and Implementation (PLDI 2006), pages 239 251, New York, USA, 2006. ACM \nPress. [21] K. Pettis and R. Hansen. Pro.le guided code positioning. In Proc. ACM SIGPLAN 90 Conf. on \nProg. Language Design and Implementation, pages 16 27, White Plains, N.Y., June 1990. ACM. [22] S. Rubin, \nR. Bodik, and T. Chilimbi. An ef.cient Pro.le-Analysis framework for data-layout optimizations. In Proc. \nof the Symp. on Principles Of Programming Languages (POPL 2002), pages 140 153, New York, NY, USA, 2002. \nACM Press. [23] F. Schneider and T. Gross. Using platform-speci.c performance counters for dynamic compilation. \nIn Proc. of the Intl. Workshop on Compilers for Parallel Computing (LCPC 2005), Oct. 2005. [24] Y. Shuf, \nM. Gupta, H. Franke, A. Appel, and J. P. Singh. Creating and preserving locality of Java applications \nat allocation and garbage collection times. In Proc. of the Conf. on Object-Oriented Programming, Systems, \nLanguages, and Applications (OOPSLA 2002), pages 13 25, New York, 2002. ACM Press. [25] D. Siegwart and \nM. Hirzel. Improving locality with parallel hierarchical copying GC. In Proceedings of the 2006 Intl. \nSymposium on Memory Management (ISMM 2006), pages 52 63, New York, USA, 2006. ACM Press. [26] B. Sprunt. \nPentium 4 performance monitoring features. In IEEE Micro, pages 72 82, July August 2002. [27] T. Suganuma, \nT. Yasue, M. Kawahito, H. Komatsu, and T. Nakatani. A dynamic optimization framework for a Java just-in-time \ncompiler. In Proc. of the ACM Conf. on Object Oriented Programming, Systems, Languages, and Applications \n(OOPLSA 2001), pages 180 195, New York, NY, USA, 2001. ACM Press. [28] The Standard Performance Evaluation \nCorporation. SPEC JBB2000 Benchmark. http://www.spec.org/jbb2000/. [29] The Standard Performance Evaluation \nCorporation. SPEC JVM98 Benchmarks. http://www.spec.org/osg/jvm98, 1996. [30] D. Ungar. Generation scavenging: \nA non-disruptive high performance storage reclamation algorithm. In Proc. of the Software Engineering \nSymposium on Practical Software Development Environments (SDE 1), pages 157 167, New York, USA, 1984. \nACM Press. \n\t\t\t", "proc_id": "1250734", "abstract": "<p>Hardware performance monitors provide detailed direct feedback about application behavior and are an additional source of infor-mation that a compiler may use for optimization. A JIT compiler is in a good position to make use of such information because it is running on the same platform as the user applications. As hardware platforms become more and more complex, it becomes more and more difficult to model their behavior. Profile information that captures general program properties (like execution frequency of methods or basic blocks) may be useful, but does not capture sufficient information about the execution platform. Machine-level performance data obtained from a hardware performance monitor can not only direct the compiler to those parts of the program that deserve its attention but also determine if an optimization step actually improved the performance of the application.</p> <p>This paper presents an infrastructure based on a dynamic compiler+runtime environment for Java that incorporates machine-level information as an additional kind of feedback for the compiler and runtime environment. The low-overhead monitoring system provides fine-grained performance data that can be tracked back to individual Java bytecode instructions. As an example, the paper presents results for object co-allocation in a generational garbage collector that optimizes spatial locality of objects on-line using measurements about cache misses. In the best case, the execution time is reduced by 14% and L1 cache misses by 28%.</p>", "authors": [{"name": "Florian T. Schneider", "author_profile_id": "81331503470", "affiliation": "ETH Z&#252;rich, Zurich, Switzerland", "person_id": "P871671", "email_address": "", "orcid_id": ""}, {"name": "Mathias Payer", "author_profile_id": "81331501507", "affiliation": "ETH Z&#252;rich, Zurich, Switzerland", "person_id": "P871683", "email_address": "", "orcid_id": ""}, {"name": "Thomas R. Gross", "author_profile_id": "81332502168", "affiliation": "ETH Z&#252;rich, Zurich, Switzerland", "person_id": "PP39070892", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1250734.1250777", "year": "2007", "article_id": "1250777", "conference": "PLDI", "title": "Online optimizations driven by hardware performance monitoring", "url": "http://dl.acm.org/citation.cfm?id=1250777"}