{"article_publication_date": "06-10-2007", "fulltext": "\n Effective Automatic Parallelization of Stencil Computations Sriram Krishnamoorthy1 Muthu Baskaran1 Uday \nBondhugula1 J. Ramanujam2 Atanas Rountev1 P. Sadayappan1 1Dept. of Computer Science and Engineering 2Dept. \nof Electrical &#38; Computer Engg. and The Ohio State University Center for Computation &#38; Technology \n2015 Neil Ave. Columbus, OH, USA Louisiana State University {krishnsr,baskaran,bondhugu,rountev,saday}@cse.ohio-state.edu \njxr@ece.lsu.edu Abstract Performance optimization of stencil computations has been widely studied in \nthe literature, since they occur in many computationally intensive scienti.c and engineering appli\u00adcations. \nCompiler frameworks have also been developed that can transform sequential stencil codes for optimization \nof data locality and parallelism. However, loop skewing is typ\u00adically required in order to tile stencil \ncodes along the time dimension, resulting in load imbalance in pipelined parallel execution of the tiles. \nIn this paper, we develop an approach for automatic parallelization of stencil codes, that explicitly \naddresses the issue of load-balanced execution of tiles. Ex\u00adperimental results are provided that demonstrate \nthe effec\u00adtiveness of the approach. Categories and Subject Descriptors D.3.4 [Programming Languages]: \nProcessors Compilers, Optimization General Terms Algorithms, Performance Keywords Stencil computations, \nTiling, Automatic paral\u00adlelization, Load balance 1. Introduction Stencil computations represent a practically \nimportant class of computations that arise in many scienti.c/engineering codes. Computational domains \nthat involve stencils in\u00adclude those that use explicit time-integration methods for numerical solution \nof partial differential equations (e.g., climate/weather/ocean modeling [23], computational elec\u00adtromagnetics \ncodes using the Finite Difference Time Do\u00admain method [27]), and multimedia/image-processing ap\u00adplications \nthat perform smoothing and other neighbor pixel based computations [13]. There has been some prior work \nfrom the computer science community that has addressed Permission to make digital or hard copies of all \nor part of this work for personal or classroom use is granted without fee provided that copies are not \nmade or distributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. PLDI 07 Jun 11 13, 2007, San Diego, California, USA. Copyright \nc &#38;#169; 2007 ACM 978-1-59593-633-2/07/0006. . . $5.00 performance optimization of stencil computations \n(e.g., [24, 19, 18, 10]). Since stencil computations are character\u00adized by a regular computational structure, \nthey are amenable to automatic compile-time analysis and transformation for exploitation of parallelism \nand data locality optimization. However, as elaborated later through an example, existing compiler frameworks \nhave limitations in generating ef.cient code optimized for parallelism and data locality. Loop tiling \nis the key transformation to enable paral\u00adlelization and data-locality optimization of stencil codes. \nMuch research has been published on tiling of iteration spaces [17, 29, 28, 26, 8, 25, 21, 22, 14, 7, \n15, 9, 16, 3]. With few exceptions (e.g. work of Griebl [11, 12]), re\u00adsearch on performance optimization \nwith tiling has gener\u00adally focused on one or the other of the two complemen\u00adtary aspects: (a) data locality \noptimization [2, 3, 28, 26, 8]; or (b) tile size/shape optimization for parallel execution [25, 21, 6, \n14, 7, 15, 9, 16]. Tiling for data locality optimiza\u00adtion involves maximization of data reuse, i.e., \ntiling along directions of the data dependence vectors. But such tiling may result in inter-tile dependences \nthat inhibit concurrent execution of tiles on different processors. To the best of our knowledge, no \nprior work has addressed in an integrated fashion, the issues of tiling for data locality optimization \nand load balancing for parallel execution. We .rst use the simple example of a one-dimensional Jacobi \ncode to illustrate the problem and introduce two approaches we propose to avoid the problem: overlapped \ntiles and split tiles. As an example of a stencil computation, let us consider the one-dimensional Jacobi \ncode shown in Figure 1. Optimizing this stencil com\u00adputation for reduction of cache misses requires loop \nfusion and tiling; in order to fuse the two inner loops, loop skew\u00ading is needed. Frameworks have been \npreviously proposed for data locality optimizations of imperfectly nested loops. Using an approach proposed \nby Ahmed et al. [3, 4], the loop nest can be transformed into the one shown in Figure 2 by .rst embedding \nthe iterations in the imperfectly-nested loops into a perfectly-nested iteration space. Loop trans\u00adformations \nand tiling can then applied in the transformed perfectly-nested iteration space. The transformed iteration \nspace can subsequently translated into ef.cient code by re\u00adducing/eliminating the control overhead [20]. \nIn this pa\u00adper, we focus on load-balanced parallel execution of tiled iteration spaces that have already \nbeen embedded into a perfectly-nested iteration space using a technique such as that developed in [4]. \nFigure 3 shows a single-statement form of the 1-D Jacobi code obtained by adding an additional dimension \nto array A. The .ow dependences in this code are the same as that of the previously shown version, but \nthere are no anti-dependences. Hence a single statement is suf.cient in the loop body in\u00adstead of a sequence \nof two statements, for update and copy, respectively, as seen in Figures 1 and 2. Although such a memory-inef.cient \ncode would not be used in practice, it is more convenient to use a single-statement iteration space in \nexplaining the main ideas in this paper. However, the de\u00adveloped approach is not restricted to such single-statement \nloops, but is applicable to general multi-statement stencil codes such as the one in Figure 1. The generalization \nof the approach for the more memory-ef.cient multi-statement versions of code is explained in the Appendix. \nThe experi\u00admental results presented later also use the memory-ef.cient multi-statement versions. The \nperfect loop nest of Figure 3 has constant depen\u00addences (1, 0), (1, 1), and (1, -1). Tiling for data \nreuse op\u00adtimization (e.g. using the approach presented in [2]) results in tiles of shape as shown in \nFigure 4. The horizontal axis corresponds to the spatial dimension, with time along the vertical dimension. \nUsing a suf.ciently large tile size along the time dimension facilitates signi.cant data reuse within \ncaches/registers. However, there are inter-tile dependences in the horizontal direction, inhibiting concurrent \nexecution of tiles by different processors. However, if the vertical tile size is reduced to one (i.e., \ntiling is eliminated along the time dimension), all tiles along the spatial dimension (ad\u00adjoining the \nx-axis) can be executed concurrently. Thus there is a trade-off between achieving good data reuse and \nload balancing of parallel execution. Instead of the standard tiling described above, consider the tiling \nshown in Figure 5. Starting with the tiles formed by the same hyperplanes, an additional triangular region \nis added to the left of the tile, overlapping with the points at the right end of the neighboring tile. \nWith this tiling, the iteration points processed by the tiles are no longer disjoint. Some of the iterations \nare executed redundantly by two neighboring tiles. This results in an increase in the computation cost. \nBut doing so eliminates the dependence between tiles along the horizontal direction. All processors can \nstart executing in parallel, eliminating the initial processor idling that results with the pipelined \nparallel execution of tiles in Figure 4. While standard tiling can enhance data locality in this context, \noverlapped tiling can both improve data locality and for t=0toT-1 for i = 1 to N-1 B[i] = (A[i-1]+A[i]+A[i+1])/3; \n(S1) for i = 1 to N-1 A[i] = B[i]; (S2) Figure 1. Example: 1-D Jacobi code for t = 0 to T-1 for i = \n1 to N if(i>=1 and i<=N-1) B[i] = (A[i-1]+A[i]+A[i+1])/3; (S1) if(i>=2 and i<=N) A[i-1] = B[i-1]; (S2) \nFigure 2. Fused 1-D Jacobi code for t = 0 to T-1 for i = 1 to N-1 A[t,i] = (A[t-1,i-1] + A[t-1,i] + \nA[t-1,i+1])/3; Figure 3. Single-statement form of 1-D Jacobi code  Figure 4. Standard tiling for one-dimensional \nJacobi. s1 and s2 denote the inter-tile dependences. eliminate the overhead of pipelined parallelism, \nat the cost of slightly increased computation time. However, the increased computational cost is independent \nof tile size. Therefore the fractional computation overhead is inversely proportional to the tile size \nin the direction of overlapped tiling, and can be made insigni.cant if a suf.ciently large tile size \nis chosen along the time dimension. An alternate approach, shown in Figure 6, splits the inte\u00adrior of \neach tile into two sub-tiles, where the points in only one of the two sub-tiles (shaded) are dependent \non points in the neighbor tile, while the points in the other sub-tile are not dependent on any neighboring \ntile s points, and therefore ex\u00adecutable concurrently. With this approach, each standard tile is split \ninto two sub-tiles, and load-balanced concurrent ex\u00adecution is possible as a sequence of two steps: .rst \nall non\u00addependent sub-tiles are concurrently executed and communi\u00adcate with the neighbor tiles, and then \nthe dependent sub-tiles are all concurrently executed. The paper is organized as follows. Section 2 de.nes \nthe problem addressed in this paper. In Section 3, we character\u00adize the conditions under which tiled \niteration spaces can ben\u00ade.t from overlapped/split tiling. In Section 4, we show how to transform a given \ntiled iteration space in order for over\u00adlapped/split tiling to be applicable. Section 5 discusses code \ngeneration and Section 6 analyzes the cost bene.ts of over\u00adlapped tiling. Section 7 provides experimental \nresults that demonstrate the bene.ts of overlapped/split tiling. In Sec\u00adtion 8, we discuss related work \nand conclude in Section 9 with a summary. Figure 5. Overlapped tiling for 1-D Jacobi. Figure 6. Split \ntiling for 1-D Jacobi.  2. Background and Problem Statement This section introduces some standard background \non the polyhedral model of computation, and de.nes the problem statement. Consider a perfectly-nested \nloop nest with n lev\u00adels of nesting. The iteration space polyhedron de.nes an n\u00addimensional set of points, \ncharacterized by a set of bounding hyperplanes and modeled as B.I = b where I is the itera\u00adtion vector. \nThe rows bi of B de.ne the normals to the corre\u00adsponding bounding hyperplanes. For example, the iteration \nspace for the one-dimensional Jacobi example is . 1 0 . . 0 . ) . . -1 0 0 1 . . . t i = . . -T +1 1 \n. . 0 -1 -N +1 The dependences in the computation can be represented by amatrix D where each column \nde.nes a dependence vector. The dependences in the 1-D Jacobi example are ) 111 D =d1 d2 d3= -101 Assume \nthat we are given a set of tiling hyperplanes that tile the iteration space. These hyperplanes are encoded \nby amatrix H, where each row represents the normal vector of a tiling hyperplane. For example, the tiling \nhyperplanes corresponding to Figure 4 are encoded as ) ) h1 10 H =h2 =11 A tiling de.ned by a set of \ntiling hyperplanes is legal if each tile can be executed atomically and there exists a valid total ordering \nof the tiles. Intuitively, a tiling is legal if no two tiles in.uence each other. It can be shown [17] \nthat this validity condition is given by H.D = 0  Figure 7. Iteration spaces with (1, 0) and (0, 1) \ndependen\u00adcies: (a) concurrent start is not possible (b) concurrent start is possible from the gray boundary. \nA schedule has a concurrent start if all processors can start execution in parallel, without a pipeline \nstart-up overhead. Such a schedule is referred to as a concurrent start schedule. Problem Statement. \nIn this paper, we are interested in the following problem. Consider a given (non-tiled) iteration space \nin which a concurrent start schedule is possible. How\u00adever, for a given tiling of this space de.ned by \na set of tiling hyperplanes, it is possible that the tile dependencies in the corresponding tiled iteration \nspace inhibit concurrent start. We consider the following question: How can concurrent start be achieved \nin the tiled iteration space? Our .rst goal is to characterize analytically the situations in which tiling \ninhibits concurrent start. Next, we de.ne two approaches, overlapped tiling and split tiling, that enable \nconcurrent start in the tiled space and recover the load-balancing properties lost due to tiling. 3. \nInhibition of Concurrent Start If the original non-tiled iteration space does not have a con\u00adcurrent \nstart schedule, tiling cannot enable such a schedule. However, if concurrent start is possible in the \nabsence of tiling, the introduction of tiling can potentially inhibit this concurrent start. This section \ncharacterizes the conditions under which a non-tiled space supports a concurrent start schedule, and \nthen derives a concurrent start inhibition con\u00addition for the tiled space. For simplicity of presentation, \nthe discussion assumes an iteration space with a single state\u00adment, but we have de.ned a general version \nof the tech\u00adnique for multi-statement iteration spaces (outlined in the appendix). 3.1 Concurrent Start \nin the Non-Tiled Space First, we describe the condition for the existence of concur\u00adrent start in the \noriginal non-tiled iteration space. Consider, for example, dependence vectors (1, 0) and (0, 1). Two \nitera\u00adtion spaces with these dependences are shown in Figure 7. In Figure 7(a), the parallel computation \nhas to begin from the origin (0, 0) and suffers from pipeline start-up overhead. On the other hand, the \niteration space in Figure 7(b) can be tra\u00adversed by all processors in parallel starting from the bound\u00adary \nshown in gray. In general, the presence of concurrent start in an itera\u00adtion space depends on the boundaries \nthat de.ne the itera\u00adtion space polyhedron. An iteration space supports concur\u00adrent start if there exists \na bounding hyperplane that does not contain a dependence, i.e. carries all dependences. A hyper\u00adplane \ncontains a dependence if both the source and destina\u00adtion iteration points of the dependence are contained \nin the hyperplane. Since the rows bi of B de.ne the normal vectors of the bounding hyperplanes, this \nproperty is represented by the condition .bi . B : .dj . D : bi.dj > 0 Note that this condition is independent \nof the tiling hyper\u00adplanes. We will refer to this property as the point-wise con\u00adcurrent start condition. \nWhen this condition does not hold, no tiled iteration space can have concurrent start. For the 1-D Jacobi \nexample, the condition holds because the normal vec\u00adtor b1 =(1 0)for one of the bounding hyperplanes \nsatis.es b1.dj > 0for all dependence vectors dj.  3.2 Inhibition of Concurrent Start in the Tiled Space \nNext, we consider the condition for the inhibition of the concurrent start condition in the tiled iteration \nspace. Given the tiling hyperplanes and their normal vectors hi . H, we de.ne the shift vector si for \nthe hyperplane with hi as normal to be a vector connecting two instances of the same hyperplane, while \ntraveling parallel to all other hyperplanes. Clearly, the following holds for the set S of shift vectors: \n.si . S : .j= i : hj .si =0 For the 1-D Jacobi example, we will use shift vectors ) 01 S = s1 s2 = 1 \n-1 as illustrated in Figure 4. The execution of two adjacent tiles should be ordered if there is a dependence \nvector dj such that for some iteration points i1 and i2 related by dj, point i1 is in one of the tiles \nand point i2 is in the other one. Note that this is possible only if there is a dependence that passes \nthrough the hyperplane that separates the two tiles in other words, if the following condition holds \n.dk . D : hi.dk =0 When this condition is satis.ed for a given hyperplane with hi . H, the shift direction \nsi along that dimension car\u00adries the inter-tile dependence. For the 1-D Jacobi example, both s1 and s2 \ncarry inter-tile dependencies (for example, h1.d1 > 0and h2.d1 > 0). The inter-tile dependences can introduce \ndependence di\u00adrections that do not exist in the original iteration space. The concurrent start condition \nis inhibited in the tiled iteration space, if for some boundary bi, the concurrent start condi\u00adtion is \nsatis.ed by the dependences in the original iteration space, but not by the inter-tile dependences in \nthe tiled iter\u00adation space. A tiling inhibits concurrent start if .bi . B, hj . H, dk . D :bi.D > 0. \nbi.sj =0. hj .dk =0 When the above condition is true, there exists an inter\u00adtile dependence within a \nhyperplane parallel to the boundary bi, precluding concurrent execution of all the tiles in the boundary. \nThus, concurrent start is inhibited even though the original iteration space supports it. This situation \noccurs for the 1-D Jacobi example due to bounding plane normal b1 =(1 0), tiling hyperplane normal h1 \n=(1 0), and any dependence dk for k =1... 3.  4. Overlapped Tiling The basic idea behind overlapped \ntiling is to eliminate cer\u00adtain inter-tile dependencies by duplicating points in the original iteration \nspace. As a result, the same iteration point can be a member of two neighboring tiles (i.e., the tiles \ncan overlap). This section outlines a constructive procedure to determine overlapping tiles that eliminate \nthe inter-tile de\u00adpendences, which removes the inhibition on concurrent start. The key step is the construction \nof a companion hyperplane that eliminates the dependence along a desired direction. The new tile will \nnot have any incoming dependence along the direction in which the dependence was eliminated. In standard \ntiling, a hyperplane with a normal vector hi de.nes two faces of the tile. We will denote these faces \nas hi(l) (the back face) and hi(l +1) (the front face). The front face is shared with the subsequent \ntile along the shift direction de.ned by shift vector si. The back face hi(l)has no incoming dependences \nif hi.D = 0. On the other hand, the front face hi(l +1), by the tiling validity condition, does not have \nany incoming dependences. All dependences between the hyperplanes can be eliminated if the back face \nof the tile is replaced by an overlapped hyperplane with a normal vector h.such that i .dj . D :h.i.dj \n= 0 Note that the hyperplanes span the iteration space and any vector in the iteration space; hence, \nthe companion hyper\u00adplane can be de.ned as a linear combination of the existing hyperplanes. Scaling \na given hyperplane vector hi does not eliminate any additional dependences. In addition, we are interested \nin the companion hyperplane that forms the back face of the tile. Thus, it is constructed by going backwards \non the other hyperplanes, represented by a negative linear combination of the hyperplanes, and is given \nby: hi.D = 0. h.=hi -kj .hj . hi..D = 0. kj > 0 i j# =i Such a companion hyperplane eliminates dependences \nalong a shift vector. This procedure is repeated for every hyperplane/shift vector that inhibits concurrent \nstart. 4.1 Cost analysis for overlapped tiling Consider n-D Jacobi with an n +1 dimensional iteration \nspace, and an n dimensional data space with a length of N along each dimension. Let B be the space tile \nsize along each of the n space dimensions. Let p be the number of processors v organized in an n-dimensional \ngrid. B = N/ n p.Let t be the time tile size. Exact comm volume local Figure 8. Overlapped tiling for \n2-D Jacobi: top view The schedule for overlapped tiling requires the processors to cycle to maintain \nload balance. We illustrate the deter\u00admination of communication frequency using a simpler vari\u00adation. \nStarting from orthogonal tiling, both planes can be swiveled partially to form trapezoid-like tiles for \n1-D Jacobi, and a square pyramid for 2-D Jacobi, top view for which is shown in Figure 8. This overlapped \ntiling scheme has the same communication volume as the original one, but double the number of startups. \nHowever, code generation is simpler for this case due to the absence of the need to cycle. The number \nof startup s do not matter when the communication volume is higher; this is particularly true for higher \ndimen\u00adsional Jacobi (greater than 1) for which the space tile size comes into the volume. Consider the \noverlapped tiling scheme that is obtained from orthogonal tiling. The point-wise difference between the \ncoordinates of a given processor and any of its neighbors in the processor space is an n-vector, and \neach of its n com\u00adponents being 1, 0, or -1. Discounting the all zeros case, we have 3n -1neighbors. \nHence, the number of communication startups per tile (without forwarding) is given by: S1 =3n - 1 (1) \nFor example, for 3-D Jacobi, we have 8 corners, 12 edges, and 6 faces, i.e., a total of 26 (=33-1) neighbors \nto send and receive data to/from to compute the overlapped tile. With communication forwarding, the number \nof commu\u00adnication startups per tile can be reduced to 2n (one for each of the faces). S1 =2n (2) Similarly \nthe number of startups for the original schedule without and with forwarding are: S2 =2n - 1 (3) S2 =n \n(4) The exact communication volume assuming orthogonal tiling is given by: n ) n V =2iB(n-i)f(i, t) (5) \nn - i i=1 2ntBn-1 when t . B where t-1 i(n-1) f(k, t)= ... in (6) in-k+1=1 in=1 The communication volume \nfor the original schedule re\u00ad duces to: V = n i=1 n n - i)B(n-i)f(i, 2t) (7) 2ntBn-1 when t . B The \ncommunication schedule and the data being com\u00admunication can be quite complex for higher dimensions. \nAdding a small number of points to the communication volume greatly simpli.es code generation. In Figure \n8, the points in each of the four corners are those that can be added. The total communication volume \nthen becomes: V =(B +2t)n - Bn 2 = nC1Bn-1(2t)+nC2Bn-2(2t) n + ... +(2t)(8) 2ntBn-1 if t . B =T(tBn-1) \n(9) For n =2: 2 V =4tB +4t(10)  4.2 Split Tiling Overlapped tiling eliminates inter-tile dependences \nby re\u00addundantly computing portions of a tile. While eliminating dependences, this approach increases \nthe overall amount of computation. In this section we leverage the idea of depen\u00addence inhibition to \ndevelop an alternative approach, referred to as split tiling, in order to enable concurrent start without \nthe computation overhead. In split tiling, rather than redun\u00addantly computing a portion of the predecessor \ntile along a dimension, the processor executing the predecessor tile .rst computes that portion and sends \nthe results to its successor along that dimension. We show that for stencil computations, a tile sub-region \ncan be identi.ed such that this sub-region can be executed in parallel in all tiles. This enables concurrent \nstart. We outline an algorithm that divides a tile into sub-regions and schedules the computation and \ncommunication to achieve concurrent start and load-balanced execution in which all processors execute \nthe same amount of work in all the steps in the schedule. 4.2.1 Tile Regions A tile in a stencil computation \nis bounded by the hyperplane instances: .I, B.I = b, hj . H :hj.I = loj ,hj.I = hij where two parallel \ninstances of each hyperplane are de.ned, one bounding the tile below along that dimension and an\u00adother \nbounding the tile from above. Along a dimension j, dependence inhibition identi.es a partner hyperplane \nsuch that the region enclosed by the partner hyperplane (hj) in the positive direction (hj.I =loi can \nbe computed independently of the rest of the tile. This region was redundantly computed in the overlapped \ntiling approach. DEFINITION 1. The independent region along a dimension j is denoted by \u00acj. The rest \nof the tile along that region will be denoted by j. In the subsequent discussion, it should be clear \nfrom the con\u00adtext whether j refers to the dimension or to the complement of the independent region along \nthat dimension. The region \u00acj is de.ned by making the partner hyper\u00adplane to be bounded from below along \nthat dimension: .I,B.I =b,hk .H,k =j :hk.I =lok,hk.I =hik .I,B.I =b :hj .I =lok,hj .I =hij Note that \nthe hyperplanes along all the other dimensions remain unchanged. A tile can be divided into these two \nregions along each of the dimensions. The various intersections of these regions k divides the tile into \n2tile components for k such dimen\u00adsions. We only consider dimensions along which there is po\u00adtential \nfor dependence inhibition, which would eliminate the time dimension. For example, a tile in two-dimensional \nJa\u00adcobi with x and y as the dimensions can be divided into the components \u00acx n\u00acy, \u00acx ny, x n\u00acy, and x \nny. From the de.nition of independent region, a tile compo\u00adnent \u00aci n... is not dependent on its predecessor \nalong di\u00admension i. Thus, the tile component that is the intersection of the independent tile region \nalong all the processors can be computed in parallel, without any communication that is, all processors \ncan start executing this in parallel, resulting in concurrent start. Consider the tile component i n..., \nwhere all other tile regions are independent. This tile component does not carry any dependence along \nany dimension other than i.The re\u00adgion in the predecessor tile that it depends on is derived as the tile-component \nwith the same hyperplanes along all other dimensions as the tile component, with the hyperplanes along \ndimension i replaced by the lower-bounding hyper\u00adplane for this tile becoming the upper-bounding hyperplane, \nand the partner hyperplane for dependent inhibition becom\u00ading the lower-bounding hyperplane. This is \nthe tile compo\u00adnent \u00aci n.... Thus, the tile component i n... can be com\u00adputed once the boundary along \ni computed by \u00acin... in the predecessor tile. In general, for each dimension i along which a tile com\u00adponent \nis dependent, the inter-tile boundary is computed by the tile component in the predecessor tile obtained \nby re\u00adplacing i by \u00aci For example, the tile component x ny in two-dimensional Jacobi, can be computed \nafter the shared boundary with \u00acxny is received from the predecessor along x, and the one with x n\u00acy \nis received from the predecessor along y. 1.If (n==1), say a dimension x. Compute \u00acx, send and receive \nthe result along the x dimension, compute x and return. 2. Execute algorithm for (n-1)-dimensional stencil \ncomputation for all dimensions except one, say z. Thus all values computed will be for those independent \nalong z (all tile sections have \u00acz as the z dimension component). 3. Send all computed values along \nthe z dimension. 4. Execute algorithm for n-dimensional stencil computation for all dimensions except \nz. But this time, all values computed will be dependent for dependent regions along z.  Figure 9. Computation/communication \nscheduling algo\u00adrithm for split-tiling Figure 9 presents a scheduling algorithm with 2n-1com\u00admunication \nsteps for an n-dimensional stencil computation. In this recursive formulation, the number of communication \nsteps is given by : L(n)=2*L(n -1)+1 with L(1)=1; that is, L(n)=2n -1. Note that this approach does \nnot incur any addition computation cost. In addition, only inter-tile boundaries in the spatial dimensions \nare com\u00admunicated, thus incurring the same communication volume cost as standard tiling.  5. Code Generation \nIn this section, we discuss the generation of the code for the iteration space with the overlapped and \nsplit tiles. We describe the derivation of the parameters necessary to utilize the code generation framework \ndescribed by Ancourt and Irigoin [5]. Each tile in the tiled iteration space is identi.ed by a tile origin. \nThe execution of the tiled iteration space is de.ned as the traversal of the tiles in terms of their \norigins, together with the execution of the iterations mapped to each tile as it is traversed. The origin \nof the tiled iteration space de.ned to be the origin of the original iteration space. Given the origin, \nall the tile origins can be enumerated as linear combinations of the shift vectors. The tile size is \nde.ned as the distances between the tile origins along the shift vector, and is embedded in the speci.cation \nof the shift vector itself. The matrix of shift vectors speci.es the traversal order of the tile origins. \nThe shift vectors are ordered to enable an outer loop along the direction bi so that there is parallelism\u00adinner \nsynchronization-outer. Given the tile origin x0, de.ned equivalently in terms of the shift vectors or \nas iteration points in the original iteration space, each of the hyperplanes bounding the tiles can be \nidenti.ed by a point in it. For hyperplanes hi along which no overlap is identi.ed as necessary, the \niteration points x in the iteration space that form this tile satisfy the following conditions: hi.x \n= hi.x0 . hi.x < hi.(x0 + si) Note that x0 is a vertex on all the non-overlapped hyper\u00adplanes that form \nthe back face of the tile. x0 + si is a point on the front face of the tile for all hyperplanes hi. Since \nover\u00adlapping does not change the front face, this is also true for hyperplanes that utilize overlap. \nWhen an overlapped hyperplane is identi.ed along a di\u00admension, we replace the back face of the original \nhyperplane hi by an overlapped hyperplane hi. Since hi is constructed from hi by only shifting it along \nthe other hyperplanes, the point x0 +j=#i si is a valid point on it irrespective of the choice of hi. \nThus the boundary conditions for the tile for these hyperplanes is given by: hi.x = hi.(x0 + si) . hi.x \n< hi.(x0 + si) j# =i Given the tile origins and their traversals, and the shape of the overlapped tile, \nthe code generation procedure of An\u00adcourt and Irigoin [5] can be used to generate code. The gen\u00aderated \ncode would have n outer tile space loops, each corre\u00adsponding to a tiling hyperplane, and inner loops \nenumerating all iterations belonging to a tile. Let us assume that k of the n hyperplanes have been identi.ed \nfor overlapped tiling. Over\u00adlapped tiling enables concurrent startup along a hyperplane by eliminating \nany inter-tile dependence along that hyper\u00adplane. Hence, the tile space loops corresponding to the re\u00admaining \nn - k hyperplanes carry all inter-tile dependences, and can be run sequentially as the outer loops, and \nthe k tile space loops corresponding to overlapped tiling hyperplanes can all be run in parallel by mapping \nto a k-dimensional or lower processor space. The traversal of tile origins for split tiling is the same \nas that for standard tiling. The intra-tile code is generated for the various tile components by scanning \nthe polytopes de\u00adrived by specifying the appropriate hyperplane instances that bound the tile component, \nas de.ned earlier. The appropri\u00adate hyperplane boundaries between sub-tiles de.ne the data to be communicated \nbetween processors for the communi\u00adcation phases, as discussed earlier.  6. Experimental Evaluation \nBoth the proposed tiling schemes overlapped tiling and split tiling enable load-balanced tiled execution \nof sten\u00adcil codes that inherently satisfy the concurrent-start crite\u00adrion. The degree of exploited concurrency \nis the same with both schemes; they differ in the computation/communication overheads relative to standard \ntiling. With overlapped tiling, there is a small amount of computational overhead and also a small increase \nin the total communication volume. Split tiling requires no additional redundant computations and re\u00adquires \nexactly the same total communication volume as stan\u00addard tiling, but requires additional messages, i.e., \nincurs a higher message-startup-cost overhead. Below, we report experimental results comparing over\u00adlapped/split \ntiling with standard (pipelined) tiling for the 1-D Jacobi code. The experiments were conducted on a \ncluster consisting of 32 compute nodes each of which is a 2.8 GHz dual-processor Opteron 254 (single \ncore) with 4GB of RAM and 1MB L2 cache, running Linux kernel 2.6.9. We used one processor per node in \nour experiments. The code was compiled using the Intel C Compiler with -O3 optimization .ag. The iteration \nspace of 1-D Jacobi has a space dimension and a time dimension. Two versions of pipelined schedule were \nimplemented: (i) one in which the processor space was mapped along the time dimension and time along \nthe space, and (ii) the other one in which the processors were distributed in a block-cyclic fashion \nto execute tiles along time dimension. First we conducted experiments to determine the opti\u00admal time \ntile size and space tile size for the two pipelined schedules. The experiments were conducted for 1000 \ntime steps on 32 processors for a total problem size of 64000 ele\u00adments. The execution times are shown \nin Figures 10 and 11. The number of communication startups decreases with an increase in the spatial \ntile size. This typically results in a de\u00adcrease in the execution time with an increase in the space \ntile size. But for larger space tile sizes, the pipeline startup costs increase thus dominating and increasing \nthe execution time. Increase in the time tile size reduces the number of time tiles and hence the number \nof synchronizations. But larger time tile sizes as in the case of larger space tile sizes increase the \npipeline startup costs. Hence an increase in the time tile size decreases the execution time until the \npipeline startup costs begin to dominate. The execution times for both the pipelined schedules, as inferred \nfrom the experiments, are minimum for a time tile size of 16 and space tile size of 1000. Hence a time \ntile size of 16 and space tile size of 1000 were used for subsequent evaluation of the schemes. For overlapped \nand split tiling, the space tile size is .xed to be N/nproc, where N is the space dimension size and \nnproc is the number of processors used for parallel execu\u00adtion. The time tile size is chosen to be 16 \nto match the choice for the pipelined schedules. Given these choices of space and time tile sizes, the \nperformance of the four schemes for various problem sizes is shown in Figure 12. The split and overlapped \ntiling schemes result in a linear increase in execution time with problem size, unlike the pipelined \ntiling solutions. The improvement in execution time achieved by split and overlapped tiling schemes with \nincrease in problem size is due to the better exploitation of data locality. In addition, unlike the \npipelined schedules, the communication cost is independent of the problem size. The improved scalability \nof the overlapped and split tiling schemes, due to an absence of the pipeline startup cost, is shown \nin Figure 13. The problem size was .xed at 20000 elements per processor. The number of proces\u00adsors was \nvaried to measure the weak scaling capability of the various schemes. A straight line parallel to the \nx-axis corresponds to linear scaling. The split tiling solution per\u00ad Execution time (ms) 1200 1000 800 \n600 400 200 0   Figure 10. Optimal space and time tile size for pipelined Figure 12. Execution Time \nas a function of problem sizeschedule 1 Execution time (ms) 500 400 300 200 100  0 Figure 11. Optimal \nspace and time tile size for pipelined schedule 2 forms best, followed by the overlapped tiling solution. \nThe pipelined schedules suffer from performance degradation with increase in the number of processors. \n 6.1 Multi-statement stencils We now consider multi-statement stencil codes that are rep\u00adresentative \nof multimedia applications. The code is a se\u00adquence of loop nests with a producer-consumer relationship \nbetween adjacent ones as shown in Figure 14. The parallel implementation exploits do-all parallelism \nin each loop nest with synchronization after each of the loop nests. The .nite number of statements limits \nsolutions exploiting pipelined parallelism to .ve processors. Figures 14 and 15 show the performance \nmeasured with overlapped and split tiling for this code. As can be seen from Figure 15, split and over\u00adlapped \ntiling perform better than the straightforward paral\u00adlel implementation. The speedup with overlapped \nand split tiling is super-linear due to exploitation of data locality and enabling of concurrent start. \nFigure 13. Execution Time for 1-D Jacobi code for i=2 to N-2 a1[i] = 0.33*(in[i-1] + in[i] + in[i+1]); \nfor i=3 to N-3 a2[i] = 0.33*(a1[i-1] + a1[i] + a1[i+1]); for i=4 to N-4 a3[i] = 0.33*(a2[i-1] + a2[i] \n+ a2[i+1]); for i=5 to N-5 a4[i] = 0.33*(a3[i-1] + a3[i] + a3[i+1]); for i=6 to N-6 a5[i] = 0.33*(a4[i-1] \n+ a4[i] + a4[i+1]);  7. Related Work Several recent works have presented manual optimizations and experimental \nstudies on stencil computations [19, 18, 10]. Iteration space tiling [17, 29] is a method of aggregating \na number of loop iterations into tiles where the tiles execute atomically; communication (or synchronization) \nwith other processors takes place before or after the tile but not during Execution time (us) Execution \ntime (us) 140 120 100 80 60 40 20 0 Problem size Figure 14. Multi-statement stencil: 32 processors 300 \n250 200 150 100 50 0 Figure 15. Multi-statement stencil: problem size = 64K the execution of the iterations \nof a tile. Several works have used tiling for exploiting data locality [2, 3, 28, 26, 8]. Others have \naddressed the selection of tile shape and size to minimize overall execution time [25, 21, 6, 22, 14, \n7]. The size of tiles has an impact on the amount of parallelism and communication: smaller tiles increase \nparallelism by reducing pipelined startup cost, while larger tiles reduce frequency of communication \namong processors. This has been studied by a number of researchers [6, 22, 14, 15, 9, 16]. Griebl [11, \n12] presents an integrated framework for optimizing data locality and parallelism in the use of tiling; \nhowever, pipelining issues are not considered. Sawdey and O Keefe [24] describe TOPAZ the tool that explores \nthe replicated computation of boundary values in the context of SPMD execution of stencil codes, in which \nthe user marks regions of code to be replicated; the tool then analyzes and generates the correct code. \nThis approach helps with reducing communication costs and improving load balance. Adve et al. [1] describe \ncomputation parti\u00adtioning strategies used in the dHPF compiler that exploit replicated computation using \nthe LOCALIZE directive that is available in dHPF. Both these approaches rely on user\u00adspeci.cation of \nreplicated computation, unlike our approach to automatic parallelization.  8. Conclusions Iteration \nspace tiling has received considerable attention mo\u00adtivated by optimizing for data locality as well as \nby exploit\u00ading parallelism for nested loops. The choice of the shape of iteration space tiles may result \nin inter-tile dependences that inhibit concurrent execution of tiles on different pro\u00adcessors, leading \nto a pipelined start overhead. This paper has addressed the issue of enhancing concurrency with tiled \nexecution of loop computations with constant dependences. Two approaches, namely overlapped tiling and \nsplit tiling were presented, that enabled the removal of inter-tile de\u00adpendences, thereby enabling additional \nconcurrency. Exper\u00adimental results demonstrated the effectiveness of the pro\u00adposed schemes.  Acknowledgments \nThis work is supported in part by the National Science Foundation through awards 0121676, 0121706, 0403342, \n0508245, 0509442, and 0509467. We thank David Callahan for suggesting split tiling. Appendix: Treatment \nof multi-statement iteration spaces The characterization of the feasibility of enhanced concur\u00adrency \nthrough overlapped/split tiling is directly applicable for single-statement iteration spaces, such as \nthe simpli.ed (but space-wise inef.cient) version of the Jacobi code of Fig\u00adure 2. But the ef.cient version \nof the Jacobi code contains two distinct statements. In this Appendix, we discuss how overlapped/split \ntiling can be used with multi-statement iter\u00adation spaces. Consider the Jacobi code of Figure 2. The \ntwo statements S1 and S2 are nested within the t and i loops. If we treat the entire body of the of the \nnested loop (i.e., S1 and S2) as the basis for de.ning dependences, the data dependences are (0,1), (0,2), \n(1,0), (1,-1), and (1,-2). Considering as be\u00adfore, the bounding hyperplane b corresponding to t=0, i.e., \nwith normal vector (1,0), we .nd that the dependence vec\u00adtors (0,1) and (0,2) have a zero dot-product \nwith b. In other words, the point-wise concurrent start condition is not satis\u00ad.ed. The problem here \nis due to the coarse granularity used in de.ning dependences based on the entire loop body. In\u00adstead, \nit is possible to take a .ner-grained view, separating out dependences due to instances of S1 and S2. \nThere is a .ow dependence (0,1) from S1 to S2, i.e., a .ow dependence from S1(t,i) to S2 (t,i+1)) and \nanti-dependences (0,0), (0,1) and (0,2) from S1 to S2. From S2 to S1, we have .ow depen\u00addences (1,0), \n(1,-1), and (1,-2), as well as anti-dependence (1,-1). It is clear by examining the dependences between \nin\u00adstances of S1 and S2 (rather than the aggregate computation from S1 and S2 at each iteration space \npoint) that all in\u00adstances of S1 for a particular value of t are all concurrently executable: there are \nno direct dependences between them, and all incoming dependences are from instances of S2 at time-step \nt-1. The instances of S2 at a given time step are also concurrently executable, because the incoming \ndepen\u00addences are all from instances of S1 at the same time-step. Given a multi-statement iteration space \nfor a stencil com\u00adputation with statements S1, S2, ...Sk, we .rst form strongly connected components \namong the statements. For each strongly connected component, all self-transitive depen\u00addences are computed \nstarting from some statement, forming all possible chains of dependences that end in an instance of the \nsame statement. These self-transitive dependences are then used for checking for concurrent-start, instead \nof the single-statement dependences assumed in the treatment of Section 4. This technique allows a hyperplane-based \nap\u00adproach, typically employed to restructure perfectly-nested loops, to be applied in this context. Considering \nthe Jacobi example of Figure 2, the self\u00adtransitive dependences may be computed from S1 through S2 back \nto S1, or vice versa (S2 through S1 to S2). The dependences from S1 to S2 are (0,0), (0,1), and (0,2), \nwhile the S2-S1 dependences are (1,0), (1,-1), and (1,-2). Forming all possible transitive dependences \nfrom S1 to S1, we get (1,\u00ad2), (1,-1), (1,0), (1,1), and (1,2). With these self-transitive dependences, \nit may be seen that their dot product with the t=0 boundary hyperplane (with normal (1,0)) is always \npositive, i.e., point-wise concurrent start is feasible for this iteration space.  References [1] V. \nAdve, G. Jin, J. Mellor-Crummey, and Q. Yi. High performance fortran compilation techniques for parallelizing \nscienti.c codes. In Proceedings of Supercomputing 98, pages 1 23, 1998. [2] N. Ahmed, N. Mateev, and \nK. Pingali. Synthesizing transformations for locality enhancement of imperfectly nested loops. In Proceedings \nof ACM ICS 2000, pages 141 152, 2000. [3] N. Ahmed, N. Mateev, and K. Pingali. Tiling imperfectly\u00adnested \nloop nests. In Proceedings of SC 00, page 31, 2000. [4] N. Ahmed, N. Mateev, and K. Pingali. Synthesizing \ntransfor\u00admations for locality enhancement of imperfectly-nested loop nests. International Journal of \nParallel Programming, 29(5), Oct. 2001. [5] C. Ancourt and F. Irigoin. Scanning polyhedra with do loops. \nIn Proceedings of PPOPP 91, pages 39 50, 1991. [6] R. Andonov, S. Balev, S. Rajopadhye, and N. Yanev. \nOptimal semi-oblique tiling. IEEE Trans. Par. &#38; Dist. Sys., 14(9):944 960, 2003. [7] P. Boulet, A. \nDarte, T. Risset, and Y. Robert. (Pen)-ultimate tiling? Integration, the VLSI Journal, 17(1):33 51, 1994. \n[8] S. Coleman and K. S. McKinley. Tile size selection using cache organization and data layout. In Proceedings \nof PLDI 95, pages 279 290, 1995. [9] F. Desprez, J. Dongarra, F. Rastello, and Y. Robert. De\u00adtermining \nthe idle time of a tiling: new results. Journal of Information Science and Engineering, 14:167 190, 1998. \n[10] M. Frigo and V. Strumpen. The memory behavior of cache oblivious stencil computations. J. of Supercomputing, \n2006. [11] M. Griebl. On tiling space-time mapped loop nests. In Proceedings of SPAA 01, pages 322 323, \n2001. [12] M. Griebl. Automatic Parallelization of Loop Programs for Distributed Memory Architectures. \nUniversity of Passau, 2004. Habilitation Thesis. [13] R. Haralick and L. Shapiro. Computer and Robot \nVision. Addison Wesley, 1992. [14] E. Hodzic and W. Shang. On time optimal supernode shape. IEEE Trans. \nPar. &#38; Dist. Sys., 13(12):1220 1233, 2002. [15] K. Hogstedt, L. Carter, and J. Ferrante. Determining \nthe idle time of a tiling. In Proceedings of POPL 97, pages 160 173, 1997. [16] K. Hogstedt, L. Carter, \nand J. Ferrante. Selecting tile shape for minimal execution time. In Proceedings of SPAA 99, pages 201 \n211, 1999. [17] F. Irigoin and R. Triolet. Supernode partitioning. In Proceedings of POPL 88, pages 319 \n329, 1988. [18] S. Kamil, K. Datta, S. Williams, L. Oliker, J. Shalf, and K. Yelick. Implicit and explicit \noptimizations for stencil computations. In Proceedings of MSPC 06, pages 51 60, 2006. [19] S. Kamil, \nP. Husbands, L. Oliker, J. Shalf, and K. Yelick. Im\u00adpact of modern memory subsystems on cache optimizations \nfor stencil computations. In Proceedings of MSP 05, pages 36 43, 2005. [20] W. Kelly, W. Pugh, and E. \nRosser. Code generation for multiple mappings. In Proceedings of FRONTIERS 95,page 332, 1995. [21] J. \nRamanujam and P. Sadayappan. Tiling multidimensional iteration spaces for nonshared memory machines. \nIn Proceedings of Supercomputing 91, pages 111 120, 1991. [22] L. Renganarayana and S. Rajopadhye. A \ngeometric program\u00adming framework for optimal multi-level tiling. In Proceed\u00adings of SC 04, page 18, 2004. \n[23] A. Sawdey, M. O Keefe, and R. Bleck. The design, imple\u00admentation, and performance of a parallel \nocean circulation model. In Proceedings of 6th ECMWF Workshop on the Use of Parallel Processors in Meteorology: \nComing of Age, pages 523 550, 1995. [24] A. Sawdey and M. T. O Keefe. Program analysis of overlap area \nusage in self-similar parallel programs. In Proceedings of LCPC 97, pages 79 93, 1998. [25] R. Schreiber \nand J. Dongarra. Automatic blocking of nested loops. Technical report, University of Tennessee, Knoxville, \nTN, Aug. 1990. [26] Y. Song and Z. Li. New tiling techniques to improve cache temporal locality. In Proceedings \nof PLDI 99, pages 215 228, 1999. [27] A. Ta.ove and S. C. Hagness. Computational Electrody\u00adnamics: The \nFinite-Difference Time-Domain Method, Third Edition. Artech House Publishers, 2005. [28] M. E. Wolf and \nM. S. Lam. A data locality optimizing algorithm. In Proceedings of PLDI 91, pages 30 44, 1991. [29] M. \nWolfe. More iteration space tiling. In Proceedings of Supercomputing 89, pages 655 664, 1989. \n\t\t\t", "proc_id": "1250734", "abstract": "<p>Performance optimization of stencil computations has been widely studied in the literature, since they occur in many computationally intensive scientific and engineering applications. Compiler frameworks have also been developed that can transform sequential stencil codes for optimization of data locality and parallelism. However, loop skewing is typically required in order to tile stencil codes along the time dimension, resulting in load imbalance in pipelined parallel execution of the tiles. In this paper, we develop an approach for automatic parallelization of stencil codes, that explicitly addresses the issue of load-balanced execution of tiles. Experimental results are provided that demonstrate the effectiveness of the approach.</p>", "authors": [{"name": "Sriram Krishnamoorthy", "author_profile_id": "81309487278", "affiliation": "The Ohio State University, Columbus, OH", "person_id": "PP39067269", "email_address": "", "orcid_id": ""}, {"name": "Muthu Baskaran", "author_profile_id": "81351594894", "affiliation": "The Ohio State University, Columbus, OH", "person_id": "P871686", "email_address": "", "orcid_id": ""}, {"name": "Uday Bondhugula", "author_profile_id": "81326487775", "affiliation": "The Ohio State University, Columbus, OH", "person_id": "P816227", "email_address": "", "orcid_id": ""}, {"name": "J. Ramanujam", "author_profile_id": "81100351630", "affiliation": "Lousiana State University, Baton Rouge, LA", "person_id": "PP39060362", "email_address": "", "orcid_id": ""}, {"name": "Atanas Rountev", "author_profile_id": "81100162864", "affiliation": "The Ohio State University, Columbus, OH", "person_id": "PP14067180", "email_address": "", "orcid_id": ""}, {"name": "P Sadayappan", "author_profile_id": "81100364545", "affiliation": "The Ohio State University, Columbus, OH", "person_id": "P871688", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1250734.1250761", "year": "2007", "article_id": "1250761", "conference": "PLDI", "title": "Effective automatic parallelization of stencil computations", "url": "http://dl.acm.org/citation.cfm?id=1250761"}