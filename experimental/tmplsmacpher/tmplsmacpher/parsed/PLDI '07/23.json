{"article_publication_date": "06-10-2007", "fulltext": "\n Shape Analysis with Inductive Recursion Synthesis Bolei Guo Neil Vachharajani David I. August Department \nof Computer Science Princeton University {bguo,nvachhar,august}@princeton.edu Abstract Separation logic \nwith recursively de.ned predicates allows for con\u00adcise yet precise description of the shapes of data \nstructures. How\u00adever, most uses of separation logic for program analysis rely on pre-de.ned recursive \npredicates, limiting the class of programs an\u00adalyzable to those that manipulate only a priori data structures. \nThis paper describes a general algorithm based on inductive program synthesis that automatically infers \nrecursive shape invariants, yield\u00ading a shape analysis based on separation logic that can be applied \nto any program. A key strength of separation logic is that it facilitates, via ex\u00adplicit expression of \nstructural separation, local reasoning about heap where the effects of altering one part of a data structure \nare analyzed in isolation from the rest. The interaction between local reasoning and the global invariants \ngiven by recursive predicates is a dif.cult area, especially in the presence of complex internal shar\u00ading \nin the data structures. Existing approaches, using logic rules speci.cally designed for the list predicate \nto unfold and fold linked\u00adlists, again require a priori knowledge about the shapes of the data structures \nand do not easily generalize to more complex data struc\u00adtures. We introduce a notion of truncation points \nin a recursive predicate, which gives rise to generic algorithms for unfolding and folding arbitrary \ndata structures. Categories and Subject Descriptors F.3.1 [Logics and Meanings of Programs]: Specifying \nand Verifying and Reasoning about Pro\u00adgrams; F.3.2 [Logics and Meanings of Programs]: Semantics of Programming \nLanguages Program analysis General Terms Languages, Theory Keywords Shape analysis, separation logic, \nloop invariant infer\u00adence, inductive recursion synthesis, arti.cial intelligence 1. Introduction Shape \nanalysis aims at an accurate description of the program heap layout, which can enable aggressive optimizations, \nprogram veri.cation, and program understanding tools. With the prevalent use of dynamic memory allocation, \na heap abstraction must have some way of describing in.nite number of concrete heaps with a .nite representation. \nAmong such techniques are summary node [1] which groups elements of potentially unbounded data structures \ninto a .nite number of abstract heap nodes, and k-limiting [2] which only distinguishes elements of a \nlinked data structure up to depth k. In both cases, the approximation of memory states leads to Permission \nto make digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior speci.c permission and/or a fee. PLDI 07 June 11 \n13, 2007, San Diego, California, USA. Copyright c &#38;#169; 2007 ACM 978-1-59593-633-2/07/0006. . . \n$5.00. loss of information about the shapes of recursive data structures. Inductively de.ned predicates \nsuch as those used in separation logic [3] allow for concise yet precise description of recursive data \nstructures. For example, an acyclic linked-list is captured by . list(x)=(x = null . emp) . (x . a * \nlist(a)). Although expressive, a problem with these predicates is that it is dif.cult to infer them from \nthe program. As a result, current uses of separation logic usually have a handful of pre-de.ned predicates \nhardwired into the logic and are limited to program veri.cation where the logic engine is supplied with \nuser speci.cations that a predicate holds at certain program point. In the case of linked-lists, logic \nrules can be designed to recognize certain patterns in the logic formulae and rewrite them to synthesize \nthe list predicate. Two analyses of list-processing programs are proposed [4, 5], both containing a rule \nthat says if x points to y and y points to z then there is a list segment between x and z. It is dif.cult \nto generalize this to arbitrary data structures. Lee et al. propose a grammar-based shape analysis [6] \nthat automatically discovers grammars which can be translated to recursive predicates. However, their \ngrammars can have only one explicit parameter, limiting the class of data structures describable. We \npropose a shape analysis that performs inductive recur\u00adsion synthesis to automatically infer arbitrary \nrecursive predicates, effectively reverse-engineering the data types in the program. This technique leverages \nan existing method in arti.cial intelli\u00adgence called inductive program synthesis, originally developed \nfor constructing recursive logic programs from sample input/output pairs [7, 8]. It allows the analysis \nto extract a loop invariant from a constant number of symbolically executed loop iterations. Sound\u00adness \nis guaranteed by verifying that the invariant derives itself over the loop body. If so, then it allows \nthe analysis to converge over the loop and proceed, but unlike many widening operations used to reach \n.xed points, there is no approximation involved and hence no loss of precision. Otherwise, the analysis \nwill halt and report failure. This technique can infer any data type with a tree-like backbone and some \nother pointer .elds that point in the back\u00adbone, possibly producing dags and cycles. This gives our analysis \nthe same descriptive power as the Pointer Assertion Logic [9]. However, in their framework shape invariants \nare already given by non-traditional data type declarations and the logic engine relies on user speci.cations \nincluding procedure pre-and post-conditions, and loop invariants. our analysis starts with zero knowledge \nand infers everything, the data types, the procedure summaries, and the loop invariants from scratch. \nInductive recursion synthesis is also used to converge over recursive procedures. Another dif.culty in \nusing recursive predicates lies in the fact that while they express global properties that hold over \nentire data structures, most programs perform many local alterations (in\u00adsertions, deletions, rotations, \netc.) to the data structures and re\u00adestablish global properties afterwards. Ideally, the analysis should \nbe able to zoom in on a small part of a data structure, reason about it ignoring the rest, and then zoom \nback out. The spatial conjunc\u00adtion operator of separation logic is designed to facilitate this kind of \nlocal reasoning via explicit expression of structure separation and aliasing. However, for data structures \nwith complex internal sharing, isolating a sub-structure separate from the rest is dif.cult. In [4, 5], \nlogic rules tailored to the list predicate can unfold a list to expose a list element and fold it back. \nBut, again, this does not easily generalize to other data structures. To enable smooth transi\u00adtioning \nbetween local reasoning and global invariants, we introduce the notion of truncation points in a recursive \npredicate, which helps the analysis to cut corners out of a data structure. Generic algorithms based \non truncation points are then designed to unfold and fold arbitrary recursive data structures. The shape \nanalysis presented in this paper is interprocedural. Like the analysis by Gotsman et al. [10], at each \nprocedure entry, it extracts the region of heap accessed by the procedure, called local heap, from the \nrest of the heap and, upon return, re-incorporates this updated local heap using the Frame rule of separation \nlogic. Cutpoints [11], the nodes that separate the local heap from the frame, are preserved so that upon \nreturn the callee s effects can be properly propagated to the caller. In the presence of recursive procedures, \nthe number of cutpoints can be in.nite. [10] bounds the number of cutpoints at the cost of potential \nprecision loss. In our case, inductive recursion synthesis allows cutpoints to be described inductively \nin the entry/exit invariants of recursive procedures, hence there is no need to bound them. Our goal \nis to handle real-life C programs like those in the SPEC benchmarks, whose data structures are complex \nand cannot be eas\u00adily taken apart into independent pieces. We will use as a running ex\u00adample the benchmark \n181.mcf from SPEC2000, which builds and manipulates a left-child right-sibling tree with two kinds of \nback\u00adward links a parent link and a left-sibling link. As shown in Fig\u00adure 1, there is a great degree \nof internal sharing which makes both inferring its shape and reasoning about its shape challenging. Ad\u00additionally, \nmany applications perform their own memory manage\u00adment using arrays. To model this correctly, our analysis \ntracks alias\u00ading that arises from pointer arithmetic. Finally, the algorithm per\u00adforms a pre-pass including \na fast pointer analysis and program slic\u00ading to preserve only code that may affect the result of shape \nanaly\u00adsis. This effectively reduces the overhead of being .ow-sensitive on realistic programs, which \nis important because .ow-sensitivity al\u00adlows strong updates, key to shape analysis. This also reduces \nnoises that may confuse the inductive recursion synthesis algorithm. Figure 1. A specimen of the tree \nused in 181.mcf In summary, the contributions of this work are: An algorithm for inferring recursive \nshape invariants based on inductive program synthesis.  A general algorithm for unrolling and rolling \nback arbitrary recursive data structures, even those with internal sharing.  Techniques to handle real-life \napplications.  Section 2 de.nes the abstract semantics. Section 3 describes inductive recursion synthesis, \nand Section 4 presents the algorithm for unrolling and rolling recursive predicates. Section 5 describes \nthe implementation of our analysis. Preliminary test results are reported in Section 6. Section 7 discusses \nrelated work and .nally, Section 8 concludes.  2. Abstract States and Abstract Operational Semantics \nLabels l . Label Globals g . Global Registers r . Reg Exprs e ::= null |g |r Insts s ::= r = e |r = malloc() \n|free(r) |r = f(xx) | [r1]= r2 |r1 =[r2] |goto l |if c goto l Branch Conds c ::= r1 = r2 |r1= r2 Vars \na . Var Recursion vars A . Rec Heap names h ::= g |a |h.n Symbolic vals v ::= null |h |h + n Pure assertions \nP ::= v1 = v2 |v1= v2 Heap assertions H ::= h1.n .h2 |A(h1, ..., hn[; h 1, ..., h ]) Register values \n. ::= 0|.,r = v Heap formulae S ::= emp |H |S *S Pure formulae F ::= P |F .F States S ::= . |S |F m . \nPredicate defs T ::= 0|T,A = P .S Proc summaries G ::= 0|G, (f, .entry |Sentry|Fentry , .exit|Sexit|Fexit) \n Table 1. Target Language and Abstract States The target language of our analysis is an assembly like \nlow\u00adlevel intermediate language used by our optimizing C compiler. The syntax of the language is shown \nin Table 1. Globals are names of heap locations allocated for global variables. Our analysis also handles \ninstructions such as r1 = r2 * n and r1 = r2 + n,which perform pointer arithmetic. For simplicity of \npresentation, they are not included in the discussion here. Detailed treatment of arrays and pointer \narithmetic is based on the low-level pointer analysis by Guo et al. [12]. The symbolic values computed \nin this analysis contain offset information. Indistinguishable array elements are collapsed into one \nelement. The rest of this section describes the abstract representation of states and an abstract operational \nsemantics of the instructions tailored to the unstructured control .ow of machine-level code. 2.1 Abstract \nStates An abstract state . | S | F consists of a mapping . from registers to their symbolic values, a \nseparation logic formula S that is the conjunction of a .nite number of atomic heap assertions, and a \npure formula F that records true branch conditions along the execution paths with which the state is \nassociated and also records aliasing between pointer arithmetic and heap names. Two global environments \nare maintained: T for recording the de.nitions of recursive predicates and G for tabulating procedure \nsummaries. Table 1 gives the de.nition of the state. Unlike the symbolic heaps de.ned in [13], which \nuse pro\u00adgram variables (the high-level counterpart of registers) to name heap locations and record alias \nrelationships between program vari\u00adables, our analysis takes the points-to approach, assigning unique \nnames to heap locations and recording the target of each register explicitly. Aside from the bene.t that \nthere is no need for re\u00adarrangement rules which set the pre-state in a suitable form by going through \nthe alias pairs, this facilitates the inductive recursion synthesis algorithm. As will be explained in \nSection 3, the access\u00adpath-like heap names encode important patterns of spatial rela\u00adtionships between \nheap locations that can be recognized and then generalized via inductive reasoning. The heap names can \nbe simply thought of as logic variables with long names. Recursive predicates are parameterized so that \nthey are expres\u00adsive enough to describe data structures with internal sharing via backward links. The \n.rst parameter represents the top of the data structure and the rest represent the targets of backward \nlinks. For example, the left-child right-sibling tree with parent and left-sibling links from 181.mcf \ncan be written as: . mcf tree(x1,x2,x3)=(x1 =null . emp). (x1.parent . x2 * x1.child . a * mcf tree(a, \nx1, null)* x1.sib prev . x3 * x2.sib . \u00df * mcf tree(\u00df, x2,x1)). An instance of such a tree where the \nroot h has null parent and sib prev links is described by instantiating the predicate: mcf tree(h, null, \nnull). We also introduce a new type of recursive predicates called truncated recursive predicates, A(h1, \n..., hn;h'1, ..., h')where n m is the arity of A. This is designed for handling modi.cations to a data \nstructure when the analysis needs to isolate relevant parts of the data structure so as to reason about \nthe modi.cations in a localized fashion. The second set of parameters {h'1, ..., h'} is of m variable \nlength and is what we call the set of truncation points in the data structure rooted at h1. This predicate \nis syntactic sugar for (*i=1..m.\u00dfi,1, ..., \u00dfi,n-1.A(h'i,\u00dfi,1, ..., \u00dfi,n-1))-*A(h1, ..., hn) (both the \niterated spatial conjunction operator and the magic wand operator -* are de.ned in [3]). It identi.es \na heap that, when combined with m heaps rooted at h'1, ..., h'on each of m which A holds, yields a heap \nrooted at h1 on which A holds. In other words, this is the data structure reachable from h1, with all \nsub-graphs rooted at h'1, ..., h'cut out from it. Because the pred\u00ad m icates are precise [14] in that \neach unambiguously identi.es a piece of heap, when A(h1, ..., hn)holds on the combined piece of heap, \nit must go through all the nodes in it. Hence it is impossible to have a situation where the truncation \npoints do not truncate the data structure at all, ensuring the correctness of our de.nition. The de.nition \nalso speci.es that the sub-graphs are mutually disjoint, i.e. no truncation point can be in the sub-graph \nof another trun\u00adcation point. This invariant is crucial for unrolling predicates as it constrains the \nnumber of possible outcomes (details are in Sec\u00adtion 4). In Figure 1, suppose that at some program point, \nthere is a pointer to an interior node h' of the tree, then the heap is described as mcf tree(h, null, \nnull;h')* mcf tree(h',\u00df1,\u00df2). By the de.ni\u00adtion of mcf tree, we know that the dangling points \u00df1 and \n\u00df2 of the heap mcf tree(h',\u00df1,\u00df2)are backward links and therefore reside in the other half of the heap. \nA linked-list fragment between x and y can be described by list(x;y), which looks similar to the list \nsegment predicate . list(x, y)=(x = y . emp). (x . a * list(a, y))de.ned in [15]. However, this predicate \nis de.ned by specifying a path by which y is reached from x and is therefore hard to generalize to more \ncomplex data structures, whereas we avoid this complica\u00adtion entirely by working not from the top of \nthe data structure, but from the bottom, and hiding the reaching path information with the magic wand \n. Not only is our approach completely general and capable of handling messy backward links, it is also \nmore .exible by allowing a variable number of truncation points. This is impor\u00adtant because unlike lists, \nother data structures may have more than one end. The ability to model this comes in handy, for example, \nwhen cutting and grafting sub-trees. Let [].,F be a function that evaluates each expression e to a heap \nname or null. [nulll.,F = null [gl.,F = g 8 > h if .(r)= h + n and F records the alias h + n = h < a \nif .(r)= h + n and F recordsno aliasof h + n, [rl.,F = > a is a fresh variable : .(r) otherwise The partial \norder . over the set of abstract states is de.ned as follows: .1 | S1 | F1 . .2 | S2 | F2 if there exists \na mapping f between the heap names in the two states such that (i) for each r . Domain(.1),if [r].1,F1 \n= null,then [r].2,F2 = null; otherwise f([r].1,F1 )=[r].2,F2 ,and (ii) for each atomic H in S1, f (H)is \nin S2, f replaces each h appearing in H with f(h), and (iii) for each atomic P in F1, f (P )is in F2, \nf replacess each h in P with f(h). Obviously there could be in.nitely increasingly chains of abstract \nstates. Termination of the analysis is achieved via inductive recursion synthesis.  2.2 Abstract Operational \nSemantics We give the abstract operational semantics for the target language in the style of Lc, a compositional \nlogic for control .ow [16], with some modi.cations. Most noticeably, our logic rules are written for \nforward analysis while those in [16] are for backward analysis. The judgment we use is: .,F f .'. F is \na set of program fragments l(s)l', with label l identifying the entry of instruction s and l' identifying \nthe exit. .and .' are sets of labeled states: .= {l1 : S1, ..., ln : Sn}, .' = {l': S1', ..., l': S'}. \n1 mmLabels l1, ..., ln are where the control .ow may enter F and labels l1', ..., l'are where it may \nleave F . The judgment is read as: if for m i =1..n, the state at entry li is Si, and the execution \nof F does not get stuck, then for j =1..m, the state at exit lj'is Sj'. Table 2 lists the operational \nrules that transform entry states of a program fragment to exit states. It includes one rule for each \nprimi\u00adtive instruction, composition rules COMBINE, DISCHARGE and WEAKEN for combining individual instructions, \nand the rule UNFOLD for unrolling a recursive predicate to reveal a points-to fact. Note, the rules shown \nin the table perform strong updates to the abstract state. In the case of aliasing due to array elements \ncollapsed into a single heap element, the analysis would have to use an alternate set of rules which \nperform weak updates. In the rule MALLOC, a.? .? simply registers a as an allocated heap node whose content \nis unknown. MUTATE invokes an important sub-routine rearrange names, shown in Figure 2, to encode access-path \ninfo in heap names. The recursion synthesis algorithm relies on this to identify the basic structure \nof a recursion. rearrange names assumes that the current heap satis.es h1.n . h2 and that v is to be \nwritten to h1.n.The appropriate name for v is determined based on its form: If it is a simple variable, \nthen we assign h1.n as its new name. If the old content stored in .eld n of h1 has already claimed this \nname, then the old content is renamed to a fresh variable.  If it is a heap name plus an offset, then \nit points to the middle of a structure, most likely an array element. As in the .rst case, h1.n is assigned \nas its new name. Additionally, the analysis records in F that the pointer arithmetic aliases with h1.n \nso that if later the location is visited via pointer arithmetic instead of access path, the analysis \nwill recognize it as well.  Otherwise, v points to a heap location that has already been linked to a \nparent, and no special action is necessary.  The intuition behind this is: While a heap location may \nbe reach\u00adable via multiple access paths (one data structure may contain cross pointers to another data \nstructure; or, within a single data structure, a node may be internally shared in the presence of dags \nand cycles), the algorithm chooses the access path that reveals the acyclic back\u00adbone of the recursive \ndata structure to which the location belongs. Our heuristic is to inherit the access path of .rst location \nit is linked to, taking advantage of the fact that such a link is usually created when adding a new expansion \nto a recursive data structure. The rule PROC CALL is the same as the one given in [10]. It exploits the \nFrame rule by breaking the heap at a call site into the local heap accessed by the callee and a frame. \ns is a map\u00adping between the formal parameters and the actuals, and between the return value and destination \nregister of the call instruction. PROC CALL is understood as follows: If there exists in Ga rearrange \nnames(h1,n,h2,v) if v = a then if h2 = h1.n then replace h2 everywhere with a fresh variable replace \nv everywhere with h1.n return h1.n else if v = h + n then if h2 = h1.n then replace h2 everywhere with \na fresh variable record alias (h + n, h1.n)return h1.n return v Figure 2. Algorithm of rearrange names \nrecorded summary of the callee, (f, .entry | Sentry | Fentry, .exit |Sexit | Fexit ), and the current \nheap S can be separated into disjoint pieces s(Sentry) and R, then the heap after the call instruction \nis a conjunction of s(Sexit) and R;and r is assigned the return value translated by s (ret is special \nregister for holding return values). Since we are not concerned with bounding the number of cutpoints, \nthey are simply treated as dangling points from the frame. The rule responsible for termination of the \nanalysis is normal\u00adize. There are two kinds of normalization operations. From a sub\u00adheap, synthesis infers \na recursive description that is guaranteed to be more general. fold reduces the size of the state by \nfolding sur\u00adrounding heap nodes into a recursive predicate.   3. Inferring Recursive Predicates This \nsection describes the algorithm for automatically inferring re\u00adcursive predicates. It enables the analysis \nto arrive at loop invariants without introducing unnecessary approximation. Loop invariant in\u00adference \nproceeds in the following steps: 1. Symbolically execute the loop body up to a .xed number of times (2 \nsuf.ces in the experimentation). 2. If the analysis does not converge over the loop at this point, then \ninvoke inductive recursion synthesis, which returns a hypothe\u00adsized loop invariant. 3. Verify the soundness \nof the loop invariant by assuming that it holds on loop entry and checking that for each control .ow \npath in the loop, if S ' is the heap at the end of the path, then foldT(S ' )=Sinvariant. If the analysis \ndiverges, then halt and report failure. 4. Otherwise, the loop invariant is valid. By the algorithm \nof recursion synthesis, the states associated with the loop entry in the initial number of iterations \nmust be derivable from the invariant by unrolling it. Hence they are eliminated using the WEAKEN rule. \n  3.1 The Inductive Recursion Synthesis Algorithm Inductive program synthesis, the problem of automatic \nsynthesis of recursive programs from input/output samples, studied in AI re\u00adsearch, resembles loop invariant \ninference in the sense that the in\u00adput/output samples are provided by .nite executions of the loop and \nthe invariant can be seen as a highly abstracted encoding of the loop. The approach introduced by Summers \n[7] consists of two steps. First, the input/output samples are rewritten as .nite pro\u00adgram traces, then \na recurrence relation is identi.ed by inspecting the traces. In our case, the program trace is readily \navailable as the heap formula after execution of the loop, with crucial informa\u00adtion encoded in the logic \nvariable names by rearrange names in Section 2. The logic formula is translated into a term, the form \nof inputs on which the recurrence detection algorithm operates, using the domain knowledge about heap \nsemantics. Such global inspec\u00adtion of states is only conducted when converging over loops. The rest of \nthe analysis updates states locally.  3.1.1 Translating Heap Formulae into Terms The set of terms is \nde.ned in Figure 3. A term can be viewed as a tree where each symbol is a node. Terms t ::= x variables \n| c constants | f(t1, .., .tn) functions Figure 3. Set of Terms The idea is to map each heap location \nto a term that describes the data structure reachable from it, referred to as a heap term. We start by \nassigning a function symbol to each logic operator, * n for spatial conjunctions, . for points-to assertions \nwith n being the .eld, and the predicate name for predicate instantiations. As the heap locations are \ninterconnected with each other, naturally some terms will be sub-trees of other terms. While the heap \nmay contain dags and cycles, the term tree structure must remain acyclic (consistent with the fact that \nthe backbones of inductive de.nitions are acyclic). To achieve this, each heap location is also associated \nwith a name term. For all appearances of a heap location on the right hand side of a points-to assertion, \nonly one will result in the corresponding heap term being linked as a sub-tree of the left hand side. \nAll others are translated into name terms by the rewrite function [], cutting the points-to link in a \nsense. [null]= NULL [g]= g [a]= a [h.n]= n([h]). The translation process maintains a mapping . from \nheap locations to heap terms. [] is overloaded to translate heap formulae to terms. [A(h1, ..., hn[; \nh1, ..., h])] = .(h1)= m A([h1], ..., [hn][; [h1]..., [h ])) m [h.n1 . h1 * ... * h.nr . hr]= .(h)= n1 \nnr *(. ([h], get term(h, h1)), ..., . ([h], get term(h, hr))), j .(h2) if h2 = h1.n get term(h1,h2)= \n[h2] otherwise In a depth-.rst traversal of the abstract heap, every predicate in\u00adstantiation is translated \ninto the heap term of the .rst parameter; all points-to assertions with the same location on the left \nhand side are translated together into the heap term of that location. The choice between a heap term \nand a name term for the right hand side is guided by the access paths encoded in the names of the heap \nloca\u00adtions. The .nal result is a forest of top-level term trees and because of the heuristic adopted \nin rearrange names, each of these trees roughly corresponds to a different data structure in the program. \nFigure 4(a) contains a loop from 181.mcf that builds the left\u00adchild right-sibling tree with backward \nlinks. nodes is an array of tree nodes. All new tree nodes are subsequently requested from it. Figure \n4(b) shows the term tree after two iterations of the loop. Each * term represents a distinct node in \nthe data structure, whose name is given in the parenthesis next to it. For each .eld n in the n node, \nthe corresponding * term contains . term whose left sub\u00adterm is the name of the source location and the \nright sub-term is either the name of the target location or the * term representing the target location. \nIn the latter case, the expansion of the data structure is continued from below the * term. In the former \ncase, the data structure reachable from the target location will be expanded along some other access \npath reaching that target. The term in Figure 4(b) completely captures the effect of the loop on heap \nat this execution point and presents it in such a way that exposes the underlying recursive pattern to \nthe recurrence detection algorithm. ASSIGN {l :. |S |F}, {l (r = e) l }f{l :.[r = [el.,F] |S |F} , a \nfresh MALLOC {l :. |S |F}, {l (r = malloc()) l }f{l :.[r = a] |S *a.? .? |F} , H(h) ::= h.n .h |A(h, \n...) FREE {l :.,r = h |S *H(h) |F}, {l (free(r)) l }f{l :.,r = h |S |F} LOOKUP {l :.,r1 = h1 + n |S \n*h1.n .h2 |F}, {l (r2 =[r1]) l }f{l :.,r1 = h1 + n [r2 = h2] |S *h1.n .h2 |F} h2 = rearrange names(h1,n, \nh2,v) MUTATE {l :.,r1 = h1 + n, r2 = v |S *h1.n .h2 |F}, {l ([r1]= r2) l }f{l :.,r1 = h1 + n, r2 = h2 \n|S *h1.n .h2 |F} JUMP BRANCH {l : S}, {l (goto l1) l }f{l1 : S}{l : S}, {l (if c goto l1) l }f.lter(c)(l1 \n: S) ..lter(\u00acc)(l : S)} G f(f, .entry |Sentry |Fentry, .exit |Sexit |Fexit ) S |= s(Sentry ) *R PROC \nCALL {l :. |S |F}, {l (r = f(x )) l }f{l :.[r = s(.exit (ret))] |s(Sexit ) *R |F} unfoldT(l : S, h),F \nf. UNFOLD {l : S},F f.  .1,F1 f.1 .2,F2 f.2 . .{l : S},F f. .{l : S} .,F f.2 .2 ..1 COMBINE DISCHARGE \nWEAKEN .1 ..2,F1 .F2 f.1 ..2 .,F f. .{l : S} .,F f.1  .1 ..2 S S superset normalize .1 ..2 . .{l :. \n|S |F}.. .{l :. |S |F} recursion synthesis(S1)= A(h1, ..., hn[; a1, ..., am]) foldT(S1)= A(h1, ..., hn[; \na1, ..., am]) synthesis fold S *S1 S *A(h1, ..., hn[; a1, ..., am]) S *S1 S *A(h1, ..., hn[; a1, ..., \nam]) j {l :. |S |F .[r1l.,F = [r2l.,F} if F . [r1l.,F = [r2l.,F.lter(r1 = r2)(l :. |S |F) = 0 otherwise \nj {l :. |S |F .[r1l.,F = [r2l.,F} if F . [r1l.,F = [r2l.,F.lter(r1 = r2)(l :. |S |F) = 0 otherwise Table \n2. Abstract Operational Semantics nodes = malloc(MAX_NODES); * (h) g(x1,x2,x3)= *(  parent child sib \nsib_prev root = nodes; parentchild node = nodes+1; . (x1,x2), .(x1,g), (h.child) h h NULL h * NULL \nh NULL root->parent = null;  sibsib prev .(x1,g), . (x1,x3)) s1: root->child = node;  parent child \nsib sib_prev root->sib = null;  root->sib_prev = null; child h child NULL child (h.child.sib) child \nNULL * g(x1,x2,x3)= *( hhh h parent for (...) { . (x1,x2), node->parent = root;  parent child sib \nsib_prev child node->child = null; .(x1,g(child(x1),x1, NULL)), s2: node->sib = node+1; sib child \nsib NULL sib * (h.child.sib.sib) sib NULL sib  .(x1,g(sib(x1),x2,x1)), s3: node->sib_prev = node-1; \n child h child child child node++;  sib prev . (x1,x3)) } hh h h (a) Code (b) Term tree (c) Recurrence \nFigure 4. A loop in 181.mcf that builds its tree This cannot be achieved by ordinary separation logic \nformulae by unfolding the recursion body up to a .nite length. So a term can without the enhancement \nof access-path-based heap names or the be folded into a recursion by .nding a segmentation of the term \ndomain-speci.c translation into terms. corresponding to the unfolding points, together with parameter \nsubstitution rules. As pointed out by Summers [7], this can be 3.1.2 Recurrence Detection viewed as the \nconverse of using .xed points to give the semantics of a recursive function. The algorithm proceeds in \nthree steps: For convergence over loops, the recurrence detection algorithm is applied to each top-level \nterm (a loop may touch multiple data structures). The algorithm we build on is by Schmid [8]. The high-1. \nSearch for a valid segmentation of the input term. This can be level intuition is that if there is a \nrecurrence relation that explains quite complex as the recurrence relation can be of arbitrary a term, \nthen the term can be obtained from the recurrence relation form, not just simple linear recursions such \nas linked-lists. In Figure 4(b), bold lines cutting across tree edges segment the term such that the \ntarget node of each edge cut is an unfolding point. Three unfolding points are NULL nodes, which corre\u00adspond \nto the base case of the recursion (modi.cations are made to Schmid s algorithm to determine when NULL \nnodes are not unfolding points). The unfolding point h.child.sib.sib is where the symbolic execution \nof the loop and hence the expansion of the term tree stop. An unfolding point like this is a single * \nterm with no children. We refer to them as the un-expanded nodes. The basic algorithm for .nding a valid \nsegmentation is given in Figure 5. Formally, it searches for the set R of recursion points places in \nthe recurrence body where it invokes itself. The unfolding points in the term can be derived by repeated \nun\u00adrolling of the recurrence at its recursion points. In Figure 4(b), the recursion points coincide with \nthe top two unfolding points, closest to the root of the tree. The other four are results of un\u00adrolling \ntwice starting at the recursion point on the left. To ensure that the algorithm returns the minimal recurrence \nrelation that explains the input term, the search for the next recursion point proceeds from left to \nright and from top to bottom, backtracking when R does not induce a valid segmentation. Validity of seg\u00admentation \nis checked by .rst computing a skeleton tskel of the hypothetical recurrence body. tskel is the minimal \nterm tree that contains all paths in t leading to the recursion points, with the recursion points replaced \nby a special symbol 0. All other paths are replaced by fresh variables at the highest points. Rinduces \na valid segmentation if for each unfolding point uderived from R, the relation tskel = uholds. = is de.ned \ninductively as 0= t if t contains NULL or un-expanded nodes,  x = t if t does not contain NULL or un-expanded \nnodes,  f(t1, ..., tn)= f(t1, ..., t)if ti = ti for i =1..n.  n .nd valid segmentation(t) R ={} // \nThe set of recursion points x =leftmost child of t while x =null do if is potential recursion point(x, \nt) then R += x if is valid segmentation(R, t) then x =next pos right(x) continue else R -= x x =leftmost \nchild of x,ifany or next pos right(x) return the set of unfolding points induced by R  is potential \nrecursion point(x, t) return x is a NULL node . (x has the same symbol as t . x contains NULL or un-expanded \nnodes in its term tree) is valid segmentation(R, t) tskel = the minimal pattern of t reaching all nodes \nin R return . unfolding point u.tskel = u  next pos right(x) if x has no parent then return null if \nx has a right sibling y then return y return next pos right(parent of x) Figure 5. Algorithm to .nd valid \nsegmentations 2. Compute the body of the recurrence, which is the maximal over\u00adlapping portion of all \nsegments. This is done by anti-unifying (n) the segments: f(t1, ..., tn)n f (t1, ..., t)=.(f(t1, ..., \ntn),f (t1, ..., t)), m m f(t1, ..., tn)n f(t1, ..., t)=f(t1 n t1, ..., tn n t ). n n .is a one-to-one \nmapping between pairs of terms and variables which guarantees that identical sub-term pairs are replaced \nby the same variable throughout the whole term. 3. Find parameter substitutions. The sub-terms where \nthe seg\u00adments differ are instantiations of the parameters in the recur\u00adrence. Parameter substitutions \nare computed by identifying reg\u00adularities in these terms. In our case, the parameters are pre\u00adcisely \nthose terms translated from the names of heap locations. The access-paths, now encoded in the pre.x form, \nprovide the excellent opportunity for identifying interrelationships between the parameters. We de.ne \nthe notion of positions in a term tree t: (i) . is the position of the root (ii) if the node at position \nu, denoted as t|u, is a function, then its i-th child has position u.i. Within each segment s,let \u00dfs,xj \n,r denote the sub-term that is the instantiation of parameter xj at recursion point r. The sub\u00adstitution \nterm for parameter xj recursion point ris computed as sub(xj,r,.). sub(xj,r,u)= 8 > xk if is recurrent(xj,xk,r,u) \n> < f(sub(xj ,r,u.1), ..., if . segment s . \u00dfs,xj,r|u =f(...), > > : sub(xj,r, u.n)) with arity(f)=n, \nis recurrent(xj,xk,r,u)= . successive segments s, s . \u00dfs ,xj ,r|u =\u00dfs,xk,r|.. sub is de.ned by structural \ninduction on term trees. The leafs of the substitution term are parameter variables. They are de\u00adtermined \nthrough comparison of successive segment pairs in is recurrent to see if a general pattern emerges. For \nan internal position u in the substitution term, it must hold that the cor\u00adresponding parameter instantiations \nin all segments share the same function node at u. For the term in Figure 4(b), the recurrence body \nis shown in Figure 4(c) on the top, and the .nal recurrence relation with pa\u00adrameter substitutions is \nshown at the bottom, which translates to the predicate mcf tree(x1,x2,x3) de.ned in Section 2.  3.2 \nWhat Recursion Synthesis Can and Cannot Do The complete algorithm given in [8] handles the case where \nthe recursion does not start at the root of the term tree, which hap\u00adpens when a recursive data structure \nis conjoined with some extra data. It can handle mutual recursions and nested recursions, which allows \nthe analysis to support nested data structures, e.g. trees of linked-lists. It can also handle interdependencies \nbetween param\u00adeter instantiations and incomplete program traces. We believe the algorithm is powerful \nenough to decipher most recursive data types. However, our technique relies on the loop that constructs \nthe data structure to reveal the data structure s recursive backbone. It will fail, for example, if the \ncode reads a table that speci.es the data structure or copies a data structure by keeping a map between \npoint\u00aders in the original and those in the duplicate.  4. Local Reasoning under Global Invariants This \nsection discusses two functions used by the symbolic execu\u00adtion rules, unfoldT(l:S,h) and foldT(S). They \nconcern reasoning about local changes to recursive data structures described by global shape invariants, \nyielding a general algorithm for unfolding and folding recursive predicates. unfoldT takes a state S \nand a heap location h located either at the root of a recursive data structure or at the bottom sitting \nbetween the data structure and a truncation point, unrolls the predicate describing the data structure \nso that S contains explicit points-to assertions with h on the left hand side. It returns a set of states \nbecause case analysis is needed in the presence of truncation points. Peeling the data structure from \nthe top is conceptually easy, sim\u00adply replace the recursive predicate with its inductive de.nition, sub\u00adstituting \narguments passed to the predicate for parameters in the de.nition. Complication arises when the predicate \ncontains trun\u00adcation points. We do not know their exact positions relative to the root. They could be \nsitting right below the root, in which case they alias with the newly exposed targets of h, or they could \nbe farther way from h so that they become the truncation points in the sub data structures below h. Because \nspatial conjunction does not allow im\u00adplicit aliasing, we have to enumerate all possible scenarios of \nrela\u00adtive positioning between h and the truncation points, constrained by the invariant that the sub \ndata structures rooted at truncation points are mutually disjoint. Let n be the number of recursion points \n(Sec\u00adtion 3.1.2) in the de.nition of the recursive predicate and m be the number of truncation points \nin the predicate. The total number of possibilities is exponential in n\u00d7m.However, n is a small constant \n(1 for linked-lists, 2 for binary trees), m is also small because local updates only involve a few nodes \nand once done, the global invari\u00adant is restored and these truncation points are eliminated by foldT. \nConsider the heap: mcf tree(h, null, null; a) * mcf tree(a, \u00df1,\u00df2). Unfolding h yields four heaps: h.parent \n. null * h.child . a * mcf tree(a, h, null) * h.sib prev . null * h.sib . \u00df4 * mcf tree(\u00df4, null,h) \n h.parent . null * h.child . \u00df3 * mcf tree(\u00df3,h, null) * h.sib prev . null * h.sib . a * mcf tree(a, \nnull,h)  h.parent . null * h.child . \u00df3 * mcf tree(\u00df3,h, null; a) * h.sib prev . null * h.sib . \u00df4 \n* mcf tree(\u00df4, null,h)  h.parent . null * h.child . \u00df3 * mcf tree(\u00df3,h, null) * h.sib prev . null * \nh.sib . \u00df4 * mcf tree(\u00df4, null,h; a)  Unrolling a recursive predicate from the bottom up makes h a \nnew truncation point, causing some old truncation points to be removed to maintain mutual-disjointness \nof truncation points. Let T be the set of the original truncation points that point to h.Again, we do \nnot know the exact access path from h to a t . T , so case splitting is required as well. In this case \nthe link from t to h also limits the possible places where t may alias with a node under h, according \nto the de.nition of the recursive predicate. Consider again the heap mcf tree(h, null, null; a) * mcf \ntree(a, \u00df1,\u00df2).To unroll \u00df2, because the link a . \u00df2 is a sib prev link, by de.nition of mcf tree, a \nmust be the target of the sib link originating from \u00df2. Hence after unrolling \u00df2,we have mcf tree(h, \nnull, null; \u00df2) * \u00df2.parent . \u00df1 *\u00df2.child . \u00df3 *mcf tree(\u00df3,\u00df2, null)*\u00df2.sib . a * \u00df2.sib prev . \u00df4 \n* mcf tree(a, \u00df1,\u00df2). If we were to consider a as the target of the child link of \u00df2,then a would be \ndescribed as mcf tree(a, \u00df2, null), which is inconsistent with its description before the unrolling. \nSimilar inconsistency also arises if we do not consider a as any target of \u00df2. Our algorithm checks each \ncombination to rule out inconsistencies. In the case of unrolling \u00df1, there are two possibilities, either \na is the child of \u00df1 or it is a truncation point in the child sub-tree of \u00df1. Figure 6 contains the algorithm \nthat determines all possible spatial relationships between an unfolded node h, associated with a recursive \npredicate A, and a set of truncation points T . Each possibility is represented by a function p that \nmaps every t in T to either r or r,where r is one of the recursion points in the de.nition of A. r means \nthe t is located at the recursion point and r means that t is further below r. The algorithm assumes \nthat neighboring recursive elements in the data structure are one pointer traversal away. This assumption \ncan be removed by generalizing the algorithm. Details are omitted due to space constraint. The case analysis \nperformed in unfoldT closely mimics the way a programmer may reason informally about local updates If \nit is the case that x points y, then ... , but it does so exhaustively to ensure correctness. In comparison, \nfolding a heap formula is straightforward because we do not need to worry about accidentally creating \nimplicit aliasing, hence no case analysis is needed. It cleans up unused truncation points left behind \nby unfoldT in an attempt to incorporate cut-out pieces of the original data structure back into it, thereby \nrestoring the global invariant. foldT takes a            valid possibilities ={} for all \np do if .r, .t.p(t)=r . $t .t =t . p(t)=r or r then ok =true for all t . T do if . backward link t.n \n. h then let xj be the parameter in A s de.nition s.t. x1.n . xj let r be the recursion point s.t. p(t)=r \nor r if p(t)=r then if the recursive call at r substitutes x1 for xj then continue else if the recursive \ncall at r substitutes x1 for some xk . .r .the recursive call at r substitutes xk for xj then continue \nok =false break if ok then valid possibilities += p   Figure 6. Algorithm for case analysis in unfoldT \n heap, looks for locations not pointed to by any live register, and tries to merge it into a neighboring \ndata structure. Like unfoldT, it also works from two directions, starting either with locations sitting \ndirectly atop a recursive data structure and working its way upwards, or with truncation points and working \nits way downwards. It achieves similar effect of the rewrite rules for list in [4, 5], p . k * list(k, \nq) . list(p, q) and list(p, k) * k . q . list(p, q). However, we handle arbitrary predicates by crawling \nthe abstract heap such that each time, instead of a single heap cell, a whole chunk of heap .tting the \nde.nition of the recursive predicate (including backward links) are absorbed. To illustrate unfolding \nand folding, we will again turn to 181.mcf. Figure 7 contains a code fragment which cuts a sub-tree from \nunder its parent and connects it to a new parent. At entry l0, q and t are two truncation points in the \nmcf tree whose root is R. The parent link of t points to p. The code fragment removes the sub-tree rooted \nat t from under p, moving the right sibling of t, if any, towards the left to be the new child of p. \nt is added as the child of q, shifting the old child of q, if any, toward the right. The heap formulae \nassociated with each program label are listed in Ta\u00adble 3. For each label, parts of the heap formulae \nthat are different from the previous label are underlined. The unfold and fold actions taken at each \nstep are also listed. Formula S1,2 corresponds to the casewhere thebranch at l1 is not taken. We omit \nsubsequent for\u00admulae derived from it. They are similar to those listed here. The same is done for S3,2. \nThe registers that are live at the end of this code fragment are t and q. In the last step, we fold all \nother nodes back into the tree. The .nal heap S6,2,where t.sib points to null,is subsumed by S6,1 (based \non the de.nition of in Section 2.1). l0: if (t->sib) t->sib->sib_prev = t->sib_prev; l1: if (t->sib_prev) \nt->sib_prev->sib = t->sib; else p->child = t->sib; l2: t->parent = q;  t->sib = q->child; l3: if \n(t->sib)  t->sib->sib_prev = t; l4: q->child = t;  t->sib_prev = 0; l5: Figure 7. Local modi.cation \nto a tree in 181.mcf S0: mcf tree(r, null, null; q, t) * mcf tree(q, \u00df1,\u00df2)* l0 t.parent . p * t.child \n. a2 * mcf tree(a2,t, null)* t.sib prev . a1 * t.sib . a3 * mcf tree(a3,p,t)    S1,1: Unfold a3 S1,2: \nl1 mcf tree(r, null, null; q, t) * mcf tree(q, \u00df1,\u00df2)* t.parent . p * t.child . a2 * mcf tree(a2,t, null)* \nt.sib prev . a1 * t.sib . a3* mcf tree(r, null, null; q, t) * mcf tree(q, \u00df1,\u00df2)* t.parent . p * t.child \n. a2 * mcf tree(a2,t, null)* t.sib prev . a1 * t.sib . null a3.parent . p * a3.child . a4 * mcf tree(a4,a3, \nnull)* a3.sib prev . a1 * a3.sib . a5 * mcf tree(a5,p,a3) S2,1: Unfold a1 mcf tree(r, null, null; q, \na1) * mcf tree(q, \u00df1,\u00df2)* a1.parent . p * a1.child . a7 * mcf tree(a7,a1, null)*  a1.sib prev . a6 \n* a1.sib . a3* l2 t.parent . p * t.child . a2 * mcf tree(a2,t, null)* t.sib prev . a1 * t.sib . a3* \na3.parent . p * a3.child . a4 * mcf tree(a4,a3, null)* a3.sib prev . a1 * a3.sib . a5 * mcf tree(a5,p,a3) \n   S2,2: Unfold p mcf tree(r, null, null; q, p) * mcf tree(q, \u00df1,\u00df2)* p.parent . a8 * p.child . a3* \np.sib prev . a9 * p.sib . a10 * mcf tree(a10,a8,p)*    t.parent . p * t.child . a2 * mcf tree(a2,t, \nnull)* t.sib prev . null * t.sib . a3* a3.parent . p * a3.child . a4 * mcf tree(a4,a3, null)* a3.sib \nprev . null * a3.sib . a5 * mcf tree(a5,p,a3)    S3,1: Unfold q mcf tree(r, null, null; q, a1)* q.parent \n. \u00df1 * q.child . \u00df3 * mcf tree(\u00df3,q, null)* q.sib prev . \u00df2 * q.sib . \u00df4 * mcf tree(\u00df4,\u00df1,q)*  a1.parent \n. p * a1.child . a7 * mcf tree(a7,a1, null)* l3 a1.sib prev . a6 * a1.sib . a3* t.parent . q * t.child \n. a2 * mcf tree(a2,t, null)* t.sib prev . a1 * t.sib . \u00df3* a3.parent . p * a3.child . a4 * mcf tree(a4,a3, \nnull)* a3.sib prev . a1 * a3.sib . a5 * mcf tree(a5,p,a3)      S3,2: Unfold q mcf tree(r, null, \nnull; q, p)* q.parent . \u00df1 * q.child . \u00df3 * mcf tree(\u00df3,q, null)*  q.sib prev . \u00df2 * q.sib . \u00df4 * mcf \ntree(\u00df4,\u00df1,q)*  p.parent . a8 * p.child . a3* p.sib prev . a9 * p.sib . a10 * mcf tree(a10,a8,p)* t.parent \n. q * t.child . a2 * mcf tree(a2,t, null)* t.sib prev . null * t.sib . \u00df3* a3.parent . p * a3.child . \na4 * mcf tree(a4,a3, null)* a3.sib prev . null * a3.sib . a5 * mcf tree(a5,p,a3)     S4,1: Unfold \n\u00df3 mcf tree(r, null, null; q, a1)* q.parent . \u00df1 * q.child . \u00df3* q.sib prev . \u00df2 * q.sib . \u00df4 * mcf \ntree(\u00df4,\u00df1,q)* \u00df3.parent . q * \u00df3.child . \u00df5 * mcf tree(\u00df5,\u00df3, null)* \u00df3.sib prev . t * \u00df3.sib . \u00df6 \n* mcf tree(\u00df6,q,\u00df3)* l4 a1.parent . p * a1.child . a7 * mcf tree(a7,a1, null)* a1.sib prev . a6 * a1.sib \n. a3* t.parent . q * t.child . a2 * mcf tree(a2,t, null)* t.sib prev . a1 * t.sib . \u00df3* a3.parent . p \n* a3.child . a4 * mcf tree(a4,a3, null)* a3.sib prev . a1 * a3.sib . a5 * mcf tree(a5,p,a3)     \n  S4,2: mcf tree(r, null, null; q, a1)* q.parent . \u00df1 * q.child . null* q.sib prev . \u00df2 * q.sib . \u00df4 \n* mcf tree(\u00df4,\u00df1,q)* a1.parent . p * a1.child . a7 * mcf tree(a7,a1, null)* a1.sib prev . a6 * a1.sib \n. a3* t.parent . q * t.child . a2 * mcf tree(a2,t, null)* t.sib prev . a1 * t.sib . null* a3.parent . \np * a3.child . a4 * mcf tree(a4,a3, null)* a3.sib prev . a1 * a3.sib . a5 * mcf tree(a5,p, a3)    \n   S5,1: mcf tree(r, null, null; q, a1)* q.parent . \u00df1 * q.child . t* q.sib prev . \u00df2 * q.sib . \u00df4 \n* mcf tree(\u00df4,\u00df1,q)* \u00df3.parent . q * \u00df3.child . \u00df5 * mcf tree(\u00df5,\u00df3, null)* \u00df3.sib prev . t * \u00df3.sib \n. \u00df6 * mcf tree(\u00df6,q,\u00df3)* l5 a1.parent . p * a1.child . a7 * mcf tree(a7,a1, null)* a1.sib prev . a6 \n* a1.sib . a3* t.parent . q * t.child . a2 * mcf tree(a2,t, null)* t.sib prev . null * t.sib . \u00df3* a3.parent \n. p * a3.child . a4 * mcf tree(a4,a3, null)* a3.sib prev . a1 * a3.sib . a5 * mcf tree(a5,p,a3)   \n    S5,2: mcf tree(r, null, null; q, a1)* q.parent . \u00df1 * q.child . t* q.sib prev . \u00df2 * q.sib . \n\u00df4 * mcf tree(\u00df4,\u00df1,q)* a1.parent . p * a1.child . a7 * mcf tree(a7,a1, null)* a1.sib prev . a6 * a1.sib \n. a3* t.parent . q * t.child . a2 * mcf tree(a2,t, null)* t.sib prev . null * t.sib . null* a3.parent \n. p * a3.child . a4 * mcf tree(a4,a3, null)* a3.sib prev . a1 * a3.sib . a5 * mcf tree(a5,p, a3)   \n          Table 3. Intermediate states of a tree update in 181.mcf S6,1:Fold a1,a3,\u00df3 S6,2:Fold \na1,a3 mcf tree(r, null, null; q)* mcf tree(r, null, null; q)* q.parent . \u00df1 * q.child . t* q.parent . \n\u00df1 * q.child . t* fold q.sib prev . \u00df2 * q.sib . \u00df4 * mcf tree(\u00df4,\u00df1,q)* q.sib prev . \u00df2 * q.sib . \u00df4 \n* mcf tree(\u00df4,\u00df1,q)* t.parent . q * t.child . a2 * mcf tree(a2,t, null)* t.parent . q * t.child . a2 \n* mcf tree(a2,t, null)* t.sib prev . null * t.sib . \u00df3 * mcf tree(\u00df3,q, t) t.sib prev . null * t.sib \n. null 5. The Analysis This section puts various pieces of the analysis together. 5.1 Code Pruning Our \ninterprocedural shape analysis includes a pre-pass in which a simple pointer analysis is performed to \nidentify recursive data structures present in the program, and code that has no effect on the shape properties \nof these data structures is pruned away. Because this shape analysis targets low-level code with no type \ninformation, a pointer analysis similar to Steensgaard s analy\u00adsis [17] is used to roughly infer the \nhigh-level type of each pointer. This eliminates the need for shape analysis to track non-pointer data \n.elds. These .elds do not exhibit interesting recursive patterns and may confuse recursion synthesis. \nAn inferred pointer type rep\u00adresents a set of runtime locations, e.g. the next .eld in all nodes of a \nlinked-list. Our pointer analysis determines an inferred type for each load/store instruction, which \nover-approximates the set of locations the instruction accesses. Recursive types are identi.ed as those \nassociated with load instructions involved in traversing recur\u00adsive data structures. These loads share \nthe property that the destina\u00adtion register is used to compute the load address, a recurrence that is \neasily detected by computing strongly-connected components of the reaching-de.nition graph. Code pruning \nis achieved by the following program slicing algo\u00adrithm: It starts with an empty set of instructions \nand a set of tracked types initialized to the set of recursive types identi.ed above. For each store \nto a tracked type, all instructions (including branches and possibly crossing procedure boundaries) that \ncontribute to the computation of either the store address or the value to be stored are added. New pointer \ntypes that need to be tracked will be iden\u00adti.ed in the process, causing more instructions to be added. \nWhen the algorithm terminates, only instructions that may affect updates to recursive pointer .elds are \npreserved. This step is essential for managing large benchmarks.  5.2 Interprocedural Analysis As in \n[10] and [18], at each procedure entry, the analysis splits the state into a local heap and a frame, \nand sends the local heap, consisting of heap regions reachable from the actual parameters and the globals \nused in the procedure and all its callees, as the pre-state of the procedure (although any other splitting \nis sound). Cutpoints are preserved by telling the algorithm fold. not to fold them away. Our implementation \nadapts the tabulation algorithm in [18] which records procedure summaries for re-use under equivalent \ncalling contexts. It is modi.ed to perform inductive recursion synthesis, shown in Figure 8. In the interprocedural \ncontrol .ow graph, each call site is represented by a call node and a return node. Each procedure has \nan entry node and an exit node. The algorithm keeps a worklist of triples (n,Sentry,S), each saying that \nstate S holds right before node ngiven that Sentry holds on entry of the procedure containing n.If nis \na call node, the local heap of the callee is .rst extracted from S and the triple is registered as a \ncalling context associated with the callee and this local heap. If no summary of the callee exists for \nthis local heap, then it is pushed onto the worklist together with the callee s entry node. Upon popping \nthe callee s exit node off the worklist, the updated local heap is propagated to all calling contexts \nregistered with the callee to be re-combined into the frames. All other nodes are handled by transform(n \n. n ' ,S), which updates state S according to the semantics of n. For memory ef.ciency, we do not keep \naround intermediate states, state duplication only occurs at split points of control .ow and for recording \nprocedure summaries and loop invariants. 5.2.1 Handling Recursive Procedures Just like loops, recursive \nprocedures are handled by symbolic ex\u00adecution along sample execution paths, then followed by inductive \nrecursion synthesis. The control .ow graph of a recursive proce\u00addure contains two kinds of loop back \nedges, one for recursive calls and one for recursive returns. Recursion synthesis is applied for both \nto infer the entry and exit invariants respectively. The choice of sample execution paths does not affect \nsoundness as the inferred procedure entry/exit invariants are veri.ed to see that they can de\u00adrive themselves. \nWe choose those paths that are good representative of the runtime behavior of the recursive procedures \nand are hence likely to yield valid invariants. On entry of a strongly-connected\u00adcomponent (SCC) in the \ncall graph (representing mutually recur\u00adsive procedures), the analysis follows an execution path that \nenters each procedure in the SCC at least twice. At each branch instruc\u00adtion reaching recursive calls, \nthe analysis only propagates states to one branch target. The selection is performed by the subrou\u00adtine \ntransform in Figure 8, which returns NULL for the non-taken branch. If all procedures in the SCC have \nbeen visited at least twice, the target that does not lead to recursive calls is taken to ensure ter\u00admination; \notherwise, the other target is taken. If both targets lead to recursive calls, we favor the one leading \nto procedures that have not been visited twice yet. When checking the invariants of each procedure, all \nexecution paths are taken into account. worklist = {(entrymain,S0,S0)} while worklist is not empty do \nremove (n,Sentry,S) from worklist if nis a call node then Slocal = extract(S,callee) contexts((callee,Slocal)) \n+= (n,Sentry,S) if summary((callee,Slocal))= 0 then for all Sexit . summary((callee,Slocal)) do worklist \n+= (nreturn ,Sentry ,combine(Sexit ,S)) else worklist += (entrycallee,Slocal,Slocal) if callee is recursive \nthen latest entry statecallee = Slocal else if nis an exit node then if callee is recursive then latest \nexit statecallee = S if caller is not in the callgraph SCC of callee then for all procedure pin callee \ns SCC do recursion synthesis(latest entry statecallee) recursion synthesis(latest exit statecallee) \nfor all procedure pin callee s SCC do if !invariants valid(p) then halt summary((callee,Sentry)) += \nS for all (ncall,Se,Sc). contexts((callee,Sentry)) do worklist += (nreturn ,Se,combine(S,Sc))  else \nfor all control .ow edge n . n do S = transform(n . n ,S) if n . n is a back edge of loop lthen if analysis \nconverges or lhas not iterated twice then worklist += (n ,Sentry,S ) else recursion synthesis(S ) if \n!invariants valid(l) then halt else worklist += (n ,Sentry ,S ) Figure 8. The Interprocedural Algorithm \n   6. Experiment Results This analysis has been implemented in our C compiler. Prelimi\u00adnary experiment \nresults are reported in Table 4. Time was taken on a 3GHz P4 with 512KB cache and 2GB memory. In addition \nto 181.mcf which uses iterative algorithm to build and to traverse its data structure, we tested on four \nOlden benchmarks which use recursive procedures. The 2nd column lists the recursive data struc\u00adtures \nused in the benchmarks. Our analysis is able to infer and maintain precise shape predicates that describe \nthese data struc\u00adtures. For the most part, the shape analysis phase (last column) takes less time than \nthe pre-pass (4th and 5th columns), demon\u00adstrating the effectiveness of code pruning. Benchmark Data \nType #Insts Analysis Time (s) Pointer Slicing Shape   181.mcf mcf tree 2158 0.59 0.22 0.55 treeadd \nbinary tree 162 0.09 0.02 0.05 bisort binary tree 423 0.16 0.05 0.38 perimeter quaternary tree 624 0.20 \n0.06 0.10 w/ parent links  power lists 1054 0.37 0.07 0.06  Table 4. Experiment results  7. Related \nWork Recently there have been many interesting works in applying sep\u00adaration logic to program analysis, \nnot just veri.cation. Berdine et al. describe a form of symbolic execution that, for certain types of \npreconditions, generates post-conditions by updating the pre\u00adconditions in-place [13]. It does not by \nitself yield a suitable ab\u00adstract domain due to lack of guarantee for convergence. Two anal\u00adyses of list-processing \nprograms [4, 5] use rewrite rules tailored to the list predicate to reduce logic formulae and thereby \narrive at .xed points. Both analyses are intraprocedural. An interprocedural analysis is given in [10], \nlimited to pre-de.ned predicates as well. [19] studies pointer arithmetic in an abstract domain where \neach list node is a multiword. Most similar to our work, Lee et al. s grammar-based analy\u00adsis [6] can \nalso discover recursive predicates automatically. But since their grammar only allows one explicit argument, \nit cannot handle data structures with multiple backward links such as the mcf tree and it cannot handle \nmultiple pointers to the interior of a data structure. We support arbitrary number of parameters and \ncan handle dags in some cases while they cannot. In [20], inductive learning is also used to .nd instrumentation \npredicates. Their technique, based on successive re.nement given positive and negative examples, is different \nfrom recursion synthe\u00adsis. Although in principle their predicates can describe complex data structures, \nthe inference of such recursive predicates is not evaluated in their experimentation. 8. Conclusion \nand Future Work This paper presented an interprocedural shape analysis based on separation logic. With \ntwo powerful techniques, inductive recursion synthesis and generic recursion unrolling/rolling based \non trunca\u00adtion points, the analysis is able to take separation logic based pro\u00adgram analysis beyond simplistic \ndata structures. For future work, we would like more results on the conditions under which the re\u00adcursion \nsynthesis algorithm would fail. Though we do not expect it to fail often in practice, when it does, a \nmore elegant recovery scheme than halting is desirable. Potential solutions include a spe\u00adcial predicate \nthat says all pointers to a particular data structure may alias and prompting the user for further information. \n  References [1] D. R. Chase, M. Wegman, and F. K. Zadeck, Analysis of pointers and structures, in Proceedings \nof the ACM SIGPLAN 90 Conference on Programming Language Design and Implementation, pp. 296 310, June \n1990. [2] N. D. Jones and S. S. Muchnick, Flow analysis and optimization of Lisp-like structures, in \nProgram Flow Analysis: Theory and Applications (S. S. Muchnick and N. D. Jones, eds.), pp. 102 131, Englewood \nCliffs, NJ: Prentice-Hall, 1981. [3] J. Reynolds, Separation logic: A logic for shared mutable data structures, \nin Proceedings of the 7th Annual IEEE Symposium on Logic in Computer Science, July 2002. [4] D. Distefano, \nP. W. O Hearn, and H. Yang, A local shape analysis based on separation logic, in Lecture Notes in Computer \nScience, vol. 3920, pp. 287 302, Springer-Verlag, 2006. [5] S. Magill, A. Nanevski, E. Clarke, and P. \nLee, Inferring invariants in separation logic for imperative list-processing programs, in Workshop on \nSemantics, Program Analysis, and Computing Environments for Memory Management (SPACE), January 2006. \n[6] O. Lee, H. Yang, and K. Yi, Automatic veri.cation of pointer programs using grammar-based shape analysis, \nin Prceedings of the 2005 European Symposium on Programming (ESOP), 2005. [7] P. Summers, A methodology \nfor Lisp program construction from examples, Journal ACM, vol. 24(1), pp. 162 175, 1977. [8] U. Schmid, \nInductive synthesis of functional programs. Berlin, Germany: Springer-Verlag, 2003. [9] A.M.ller and \nSchwartzbach, The pointer assertion logic engine, in Proceedings of the ACM SIGPLAN 2001 Conference on \nProgramming Language Design and Implementation, pp. 221 231, 2001. [10] A. Gotsman, J. Berdine, and \nB. Cook, Interprocedural shape analysis with separated heap abstractions, in Proceedings of the 13th \nInternational Static Analysis Symposium (SAS), August 2006. [11] N. Rinetzky, J., Bauer, T. Reps, M. \nSagiv, and R. Wilhelm, A seman\u00adtics for procedure local heaps and its abstractions, in Proceedings of \nthe 32nd ACM Symposium on Principles of Programming Languages, pp. 296 309, January 2005. [12] B. Guo, \nM. J. Bridges, S. Triantafyllis, G. Ottoni, E. Raman, and D. I. August, Practical and accurate low-level \npointer analysis, in Proceedings of the 3rd International Symposium on Code Generation and Optimization, \nMarch 2005. [13] J. Berdine, C. Calcagno, and P. W. O Hearn, Symbolic execution with separation logic, \nin Lecture Notes in Computer Science, vol. 3780, pp. 52 68, Springer-Verlag, 2005. [14] P. W. O Hearn, \nH. Yang, and J. Reynolds, Separation and information hiding, in Proceedings of the 31st ACM symposium \non Principles of Programming Languages, pp. 268 280, January 2004. [15] J. Berdine, C. Calcagno, and \nP. W. O Hearn, A decidable fragment of separation logic, in Lecture Notes in Computer Science, vol. 3328, \npp. 97 109, Springer-Verlag, 2004. [16] G. Tan and A. W. Appel, A compositional logic for control .ow, \nin Lecture Notes in Computer Science, vol. 3855, pp. 80 94, Springer-Verlag, 2006. [17] B. Steensgaard, \nPoints-to analysis by type inference in programs with structures and unions, in Lecture Notes in Computer \nScience, 1060, pp. 136 150, Springer-Verlag, 1996. [18] N. Rinetzky, M. Sagiv, and E. Yahav, Interprocedural \nshape analysis for cutpoint-free programs, Tech. Rep. 26, Tel Aviv University, November 2004. [19] C. \nCalcagno, D. Distefano, P. W. O Hearn, and H. Yang, Beyong reachability: Shape abstraction in the presence \nof pointer arithmeetic, in Lecture Notes in Computer Science, vol. 4134, pp. 182 203, Springer-Verlag, \n2006. [20] A. Loginov, T. Reps, and M. Sagiv, Abstraction re.nement via induc\u00adtive learning, in Proceedings \nof the 17th International Conference on Computer Aided Veri.cation, pp. 519 533, 2005. \n\t\t\t", "proc_id": "1250734", "abstract": "<p>Separation logic with recursively defined predicates allows for concise yet precise description of the shapes of data structures. However, most uses of separation logic for program analysis rely on pre-defined recursive predicates, limiting the class of programs analyzable to those that manipulate only a priori data structures. This paper describes a general algorithm based on <i>inductive program synthesis</i> that automatically infers recursive shape invariants, yielding a shape analysis based on separation logic that can be applied to any program.</p> <p>A key strength of separation logic is that it facilitates, via explicit expression of structural separation, local reasoning about heap where the effects of altering one part of a data structure are analyzed in isolation from the rest. The interaction between local reasoning and the global invariants given by recursive predicates is a difficult area, especially in the presence of complex internal sharing in the data structures. Existing approaches, using logic rules specifically designed for the list predicate to unfold and fold linked-lists, again require a priori knowledge about the shapes of the data structures and do not easily generalize to more complex data structures. We introduce a notion of \"truncation points\" in a recursive predicate, which gives rise to generic algorithms for unfolding and folding arbitrary data structures.</p>", "authors": [{"name": "Bolei Guo", "author_profile_id": "81100085606", "affiliation": "Princeton University, Princeton, NJ", "person_id": "P714876", "email_address": "", "orcid_id": ""}, {"name": "Neil Vachharajani", "author_profile_id": "81100483107", "affiliation": "Princeton University, Princeton, NJ", "person_id": "P513572", "email_address": "", "orcid_id": ""}, {"name": "David I. August", "author_profile_id": "81100388492", "affiliation": "Princeton University, Princeton, NJ", "person_id": "P60452", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1250734.1250764", "year": "2007", "article_id": "1250764", "conference": "PLDI", "title": "Shape analysis with inductive recursion synthesis", "url": "http://dl.acm.org/citation.cfm?id=1250764"}