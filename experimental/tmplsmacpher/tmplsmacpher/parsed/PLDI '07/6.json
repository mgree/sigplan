{"article_publication_date": "06-10-2007", "fulltext": "\n Certi.ed Self-Modifying Code Hongxu Cai Zhong Shao Alexander Vaynberg Department of Computer Science \nand Department of Computer Science Department of Computer Science Technology, Tsinghua University Yale \nUniversity Yale University Beijing, 100084, China New Haven, CT 06520, USA New Haven, CT 06520, USA hxcai00@mails.tsinghua.edu.cn \nshao@cs.yale.edu alv@cs.yale.edu Abstract Self-modifying code (SMC), in this paper, broadly refers to \nany program that loads, generates, or mutates code at runtime. It is widely used in many of the world \ns critical software systems to sup\u00adport runtime code generation and optimization, dynamic loading and \nlinking, OS boot loader, just-in-time compilation, binary trans\u00adlation, or dynamic code encryption and \nobfuscation. Unfortunately, SMC is also extremely di.cult to reason about: existing formal veri.cation \ntechniques including Hoare logic and type system consistently assume that program code stored in memory \nis .xed and immutable; this severely limits their applicability and power. This paper presents a simple \nbut novel Hoare-logic-like frame\u00adwork that supports modular veri.cation of general von-Neumann machine \ncode with runtime code manipulation. By dropping the as\u00adsumption that code memory is .xed and immutable, \nwe are forced to apply local reasoning and separation logic at the very begin\u00adning, and treat program \ncode uniformly as regular data structure. We address the interaction between separation and code memory \nand show how to establish the frame rules for local reasoning even in the presence of SMC. Our framework \nis realistic, but designed to be highly generic, so that it can support assembly code under all modern \nCPUs (including both x86 and MIPS). Our system is ex\u00adpressive and fully mechanized. We prove its soundness \nin the Coq proof assistant and demonstrate its power by certifying a series of realistic examples and \napplications all of which can directly run on the SPIM simulator or any stock x86 hardware. Categories \nand Subject Descriptors D.2.4 [Software Engineer\u00ading]: Software/Program Veri.cation correctness proofs, \nformal methods; D.3.1 [Programming Languages]: Formal De.nitions and Theory; F.3.1 [Logics and Meanings \nof Programs]: Specify\u00ading and Verifying and Reasoning about Programs General Terms Languages, Veri.cation \nKeywords self-modifying code, runtime code manipulation, as\u00adsembly code veri.cation, modular veri.cation, \nHoare logic 1. Introduction Self-modifying code (SMC), in this paper, broadly refers to any program that \npurposely loads, generates, or mutates code at run- Permission to make digital or hard copies of all \nor part of this work for personal or classroom use is granted without fee provided that copies are not \nmade or distributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. PLDI 07 June 11 13, 2007, San Diego, California, USA. Copyright \n&#38;#169;c2007 ACM 978-1-59593-633-2/07/0006. . . $5.00 Examples System where Common Basic Constructs \nopcode modi.cation GCAP2 Sec 5 control .ow modi.cation GCAP2 TR[5] unbounded code rewriting GCAP2 Sec \n6.2 runtime code checking GCAP1 TR[5] runtime code generation GCAP1 TR[5] multilevel RCG GCAP1 Sec 4 \nself-mutating code block GCAP2 TR[5] mutual modi.cation GCAP2 TR[5] Typical Applications self-growing \ncode GCAP2 Sec 6.3 polymorphic code GCAP2 TR[5] code optimization GCAP2/1 TR[5] code compression GCAP1 \nTR[5] code obfuscation GCAP2 TR[5] code encryption GCAP1 Sec 6.4 OS boot loaders GCAP1 Sec 6.1 shellcode \nGCAP1 TR[5] Table 1. A summary of GCAP-supported applications time. It is widely used in many of the \nworld s critical software sys\u00adtems. For example, runtime code generation and compilation can improve \nthe performance of operating systems [20] and other ap\u00adplication programs [19, 13, 29]. Dynamic code \noptimization can improve the performance [4, 11] or minimize the code size [7]. Dy\u00adnamic code encryption \n[27] or obfuscation [15] can support code protection and tamper-resistant software [3]; they also make \nit hard for crackers to debug or decompile the protected binaries. Evolu\u00adtionary computing systems can \nuse dynamic techniques to support genetic programming [25]. SMC also arises in applications such as just-in-time \ncompiler, dynamic loading and linking, OS boot\u00adloaders, binary translation, and virtual machine monitor. \nUnfortunately, SMC is also extremely di.cult to reason about: existing formal veri.cation techniques \nincluding Hoare logic [8, 12] and type system [26, 22] consistently assume that program code stored in \nmemory is immutable; this signi.cantly limits their power and applicability. In this paper we present \na simple but powerful Hoare-logic\u00adstyle framework namely GCAP (i.e., CAP [31] on General von Neumann \nmachines) that supports modular veri.cation of gen\u00aderal machine code with runtime code manipulation. \nBy dropping the assumption that code memory is .xed and immutable, we are forced to apply local reasoning \nand separation logic [14, 28] at the very beginning, and treat program code uniformly as regular data \nstructure. Our framework is realistic, but designed to be highly generic, so that it can support assembly \ncode under all modern CPUs (including both x86 and MIPS). Our paper makes the fol\u00adlowing new contributions: \nOur GCAP system is the .rst formal framework that can suc\u00adcessfully certify any form of runtime code \nmanipulation (Machine) M ::= (Extension,Instr, Ec : Instr . ByteList, Next : Address . Instr . State \n. State, Npc : Address . Instr . State . Address) (State) S ::= (M,E) (Mem) M ::= {f \" b}* (Extension) \nE ::= ... (Address) f,pc ::= ... (nat nums) (Byte) b ::= ... (0..255) (ByteList) bs ::= b,bs | b (Instr) \n. ::= ... (World) W ::= (S,pc) Figure 1. De.nition of target machine GTM including all the common basic \nconstructs and important ap\u00adplications given in Table 1 (due to the space limit, we have to leave many \nexamples in the companion TR [5]).We are the .rst to successfully certify a realistic OS boot loader \nthat can di\u00adrectly boot on stock x86 hardware. All of our MIPS examples can be directly executed in the \nSPIM 7.3 simulator[17]. GCAP is the .rst successful extension of Hoare-style program logic that treats \nmachine instructions as regular mutable data structures. A general GCAP assertion can not only specify \nthe usual precondition for data memory but also can ensure that code segments are correctly loaded into \nmemory before execution. We develop the idea of parametric code blocks to specify and reason about all \npossible outputs of each self\u00admodifying program. These results are general and can be easily applied \nto other Hoare-style veri.cation systems.  GCAP supports both modular veri.cation [9] and frame rules \nfor local reasoning [28]. Program modules can be veri.ed separately and with minimized import interfaces. \nGCAP pin\u00adpoints the precise boundary between non-self-modifying code groups and those that do manipulate \ncode at runtime. Non-self\u00admodifying code groups can be certi.ed without any knowledge about each other \ns implementation, yet they can still be safely linked together with other self-modifying code groups. \n GCAP is highly generic in the sense that it is the .rst attempt to support di.erent machine architectures \nand instruction sets in the same framework without modifying any of its inference rules. This is done \nby making use of several auxiliary func\u00adtions that abstract away the machine-speci.c semantics and by \nconstructing generic (platform-independent) inference rules for certifying well-formed code sequences. \n In the rest of this paper, we .rst present our von-Neumann ma\u00adchine GTM in Section 2. We stage the \npresentation of GCAP: Sec\u00adtion 3 presents a Hoare-style program logic for GTM; Section 4 presents a simple \nextended GCAP1 system for certifying runtime code loading and generation; Section 5 presents GCAP2 which \nex\u00adtends GCAP1 with general support of SMC. In Section 6 and in the companion TR [5], we present a large \nset of certi.ed SMC applica\u00adtions to demonstrate the power and practicality of our framework. Our system \nis fully mechanized the Coq implementation (includ\u00ading the full soundness proof) is available on the \nweb [5]. 2. General Target Machine GTM Our general machine model, namely GTM, is an abstract frame\u00adwork \nfor von Neumann machines. GTM is general because it can be used to model modern computing architecture \nsuch as x86, MIPS, or PowerPC. Fig 1 shows the essential elements of GTM. An in-If Decode(S,pc,.) is \ntrue, then () (S,pc)Nextpc,.(S), Npcpc,.(S) -. Figure 2. GTM program execution stance Mof a GTM machine \nis modeled as a 5-tuple that deter\u00admines the machine s operational semantics. A machine state S should \nconsist of at least a memory com\u00adponent M, which is a partial map from the memory address to its stored \nByte value. Byte speci.es the machine byte which is the min\u00adimum unit of memory addressing. Note that \nbecause the memory component is a partial map, its domain can be any subset of nat\u00adural numbers. E represents \nother additional components of a state, which may include register .les and disks, etc. No explicit code \nheap is involved: all the code is encoded and stored in the memory and can be accessed just as regular \ndata. Instr speci.es the instruc\u00adtion set, with an encoding function Ec describing how instructions can \nbe stored in memory as byte sequences. We also introduce an auxiliary Decode predicate which is de.ned \nas follows: Decode((M,E),f,.) . Ec(.) = (M[f],...,M[f+|Ec(.)|-1]) In other words, Decode(S,f,.) states \nthat under the state S, certain consecutive bytes stored starting from memory address f are pre\u00adcisely \nthe encoding of instruction .. Program execution is modeled as a small-step transition relation between \ntwo Worlds (i.e., W -.W' ), where a world W is just a state plus a program counter pc that indicates \nthe next instruction to be executed. The de.nition of this transition relation is formalized in Fig 2. \nNext and Npc are two functions that de.ne the behavior of all available instructions. When instruction \n.located at address pc is executed at state S, Nextpc,.(S) is the resulting state and Npcpc,.(S) is the \nresulting program counter. Note that Next could be a partial function (since memory is partial) while \nNpc is always total. To make a step, a certain number of bytes starting from pc are fetched out and decoded \ninto an instruction, which is then executed following the Next and Npc functions. There will be no transition \nif Next is unde.ned on a given state. As expected, if there is no valid transition from a world, the \nexecution gets stuck. To make program execution deterministic, the following condi\u00adtion should be satis.ed: \n.S,f,.1,.2.Decode(S,f,.1) .Decode(S,f,.2) -..1 = .2 In other words, Ec should be pre.x-free: under no \ncircumstances should the encoding of one instruction be a pre.x of the encoding of another one. Instruction \nencodings on real machines follow regular patterns (e.g., the actual value for each operand is extracted \nfrom certain bits). These properties are critical when involving operand\u00admodifying instructions. Appel \net al [2, 21] gave a more speci.c decoding relation and an in-depth analysis. The de.nitions of the Next \nand Npc functions should also guar\u00adantee the following property: if ((M,E),pc),E' ),pc' ) and -.((M' \nM'' is a memory whose domain does not overlap with those of M and M' , then ((M .M'' ,E),pc),E' ),pc' \n). In other -.((M' .M'' words, adding extra memory does not a.ect the execution process of the original \nworld. MIPS specialization. The MIPS machine MMIPS is built as an instance of the GTM framework (Fig \n3). In MMIPS, the machine state consists of a (M,R) pair, where R is a register .le, de.ned as a map \nfrom each of the 31 registers to a stored value. $0 is not included in the register set since it always \nstores constant zero and is immutable according to MIPS convention. A machine Word is the composition \nof four Bytes. To achieve interaction between registers and memory, two operators (\u00b7h1 and (\u00b7h4 are \nde.ned (details omitted here) to do type conversion between Word and Value. (State) S ::= (M,R) (RegFile) \nR . Register .Value (Register) r ::= $1 | ... |$31 (Value) i,(w)1 ::= ... (int nums) (Word) w,(i)4 ::= \nb,b,b,b (Instr) . ::= nop |li rd,i |add rd,rs,rt |addi rt,rs,i |mul rd,rs,rt |lw rt,i(rs) |sw rt,i(rs) \n|la rd,f |j f |jr rs |beq rs,rt,i |jal f Figure 3. MMIPS data types Ec(.) . ..., if . = then Nextpc,.(M,R) \n= jal f (M,R{$31 pc+4}) nop (M,R) li rd,i / la rd,i (M,R{rd i}) add rd,rs,rt (M,R{rd R(rs)+R(rt)}) addi \nrt,rs,i (M,R{rt R(rs)+i}) mul rd,rs,rt (M,R{rd R(rs)\u00d7R(rt)}) lw rt,i(rs) (M,R{rt (M(f),...,M(f+3))1}) \nif f = R(rs)+i .dom(M) sw rt,i(rs) (M{f,...,f+3 (R(rt))4},R) if f = R(rs)+i .dom(M) Otherwise (M,R) \nand pc+|Ec(.)| Figure 4. MMIPS operational semantics (Word) w ::= b,b (State) S ::= (M,R,D) * 16 s (RegFile) \nR ::= {rw}* .{rw} (Disk) D ::= {l b}* (Word Regs) r16 ::= rAX |rBX |rCX |rDX |rSI |rDI |rBP |rSP 8 (Byte \nRegs) r::= rAH |rAL |rBH |rBL |rCH |rCL |rDH |rDL s (S egment Regs) r::= rDS |rES |rSS 16 8 (Instr) . \n::= movw w,r16 |movw r,rS |movb b,r|jmp b |jmpl w,w |int b |... Figure 5. Mx86 data types The set of \ninstructions Instr is minimal and it contains only the basic MIPS instructions, but extensions can be \neasily made. MMIPS supports direct jump, indirect jump, and jump-and-link (jal) instructions. It also \nprovides relative addressing for branch instructions (e.g. beq rs, rt, i), but for clarity we will continue \nusing code labels to represent the branching targets in our examples. The Ec function follows the o.cial \nMIPS documentation and is omitted. Interested readers can read our Coq implementation. Fig 4 gives the \nde.nitions of Next and Npc. It is easy to see that these functions indeed satisfy the requirements we \nmentioned earilier. x86 (16-bit) specialization. In Fig 5, we show our x86 machine, Mx86, as an instance \nof GTM. The speci.cation of Mx86 is a restriction of the real x86 architecture. However, it is adequate \nfor certi.cation of interesting examples such as OS boot loaders. if . = then Npcpc,.(M,R) = j f f jr \nrs R(rs) beq rs,rt,i . . . . . pc + i when R(rs) = R(rt), pc + 4 when R(rs) * R(rt) jal f f Otherwise \n Ec(.) . ..., if . = then Nextpc,.(M,R,D) = movw w,r16 (M,R{r16 w},D) movw r16 ,rS (M,R{rS R(r16)},D) \nmovb b,r8 (M,R{r8 b},D) jmp b (M,R,D) jmpl w1,w2 (M,R,D) int b BIOS Call b (M,R,D) ... ... and if . \n= then Npcpc,. (M,R) = jmp b pc + 2 + b jmpl w1,w2 w1 *16 + w2 Non-jmp instructions pc+|Ec(.)| ... ... \n Figure 6. Mx86 operational semantics Call 0x13 (disk operations) (id = 0x13) Command 0x02 (disk read) \n(R(rAH) = 0x02) Parameters Count = R(rAL) Cylinder = R(rCH) Sector = R(rCL) Head = R(rDH) Disk Id = R(rDL) \nBytes = Count *512 Src = (Sector -1) *512 Dest = R(rES ) *16 + R(rBX) Conditions Cylinder = 0 Head = \n0 Disk Id = 0x80 Sector < 63 E.ect M' = M{Dest + i D(Src + i)} (0 =i =Bytes) R' = R{rAH 0} D' = D Figure \n7. Subset of Mx86 BIOS operations .data # Data declaration section 100 new: addi $2, $2, 1 # the new \ninstr .text # Code section 200 main: beq $2, $4, modify # do modification 204 target: move $2, $4 # original \ninstr 208 j halt # exit 212 halt: j halt 216 modify: lw $9, new # load new instr 224 sw $9, target # \nstore to target 232 j target # return Figure 8. opcode.s: Opcode modi.cation In order to certify a boot \nloader, we augment the Mx86 state to include a disk. Everything else that is machine speci.c has no e.ect \non the GTM speci.cation. Operations on the 8-bit registers are done via their corresponding 16-bit registers \n(see TR [5] for more detail). To use the disk, Mx86 needs to call a .rmware such as BIOS, which we treat \nas a black box with proper formal speci.\u00adcations (Fig 7). We de.ne the BIOS call as a primitive operation \nin the semantics. In this paper, we only de.ne a BIOS command for disk read, as it is needed for our \nboot loader. Since we did not want to present a complex de.nition of the disk, we assume our disk has \nonly one cylinder, one side, and 63 sectors. A Taste of SMC We give a sample piece of self-modifying \ncode (i.e., opcode.s) in Fig 8. The example is written in MMIPS syntax. We use line numbers to indicate \nthe location of each instruction in memory. It seems that this program will copy the value of register \n$4 to register $2 and then call halt. But it could jump to the modify subroutine .rst which will overwrite \nthe target code with the new instruction addi $2,$2,1. So the actual result of this program can vary: \nif R($2) * R($4), the program copies the value of $4 to $2; otherwise, the program simply adds $2 by \n1. Even such a simple program cannot be handled by any existing veri.cation frameworks since none of \nthem allow code to be mu\u00adtated at anytime. General SMCs are even more challenging: they are di.cult to \nunderstand and reason about because the actual pro\u00adgram itself is changing during the execution it is \ndi.cult to .gure out the program s control and data .ow. 3. Hoare-Style Reasoning under GTM Hoare-style \nreasoning has always been done over programs with separate code memory. In this section we want to eliminate \nsuch re\u00adstriction. To reason about GTM programs, we formalize the syntax and the operational semantics \nof GTM inside a mechanized meta logic. For this paper we will use the calculus of inductive construc\u00adtions \n(CiC) [30] as our meta logic. Our implementation is done us\u00ading Coq [30] but all our results also apply \nto other proof assistants. We will use the following syntax to denote terms and predicates in the meta \nlogic: (Term) A,B ::= Set |Prop |Type |x |.x: A.B |AB |A .B |ind. def. |... (Prop) p,q ::= True |False \n|\u00acp |p .q |p .q |p .q |.x : A.p |.x : A.p |... The program safety predicate can be de.ned as follows: \n{ True if n = 0 Safenn(W) .W ' .W -.W ' .Safenn-1(W ')if n > 0 Safe(W) .n : Nat.Safenn(W) Safenn states \nthat the machine is safe to execute n steps from a world, while Safe describes that the world is safe \nto run forever. Invariant-based method [16] is a common technique for proving safety properties of programs. \nDe.nition 3.1 An invariant is a predicate, namely Inv, de.ned over machine worlds, such that the following \nholds: .W.Inv(W) -..W ' .(W -.W ' ) (Progress)  .W.Inv(W) .(W -.W ' ) -.Inv(W ' ) (Preservation)  \nThe existence of an invariant immediately implies program safety, as shown by the following theorem. \nTheorem 3.2 If Inv is an invariant then .W.Inv(W) .Safe(W). Traditional Hoare-style reasoning over assembly \nprograms (e.g., CAP [31]) is illustrated in Fig 9. Program code is assumed to be stored in a static code \nheap separated from the main mem\u00adory. A code heap can be divided into di.erent code blocks (i.e. consecutive \ninstruction sequences) which serve as basic certify\u00ading units. A precondition is assigned to every code \nblock, whereas no postcondition shows up since we often use CPS (continuation passing style) to reason \nabout low-level programs. Di.erent blocks can be independently veri.ed then linked together to form a \nglobal invariant and complete the veri.cation. Here we present a Hoare-logic-based system GCAP0 for GTM. \nDeveloping a Hoare logic for GTM is not trivial. Firstly, unify\u00ading di.erent types of instructions (especially \nbetween regular com\u00admand and control transfer instruction) without loss of usability re- Code Heap Code \nBlocks Control Flow     Figure 9. Hoare-style reasoning of assembly code (CodeBlock) B ::= f : \nI (InstrSeq) I ::= .;I |. (CodeHeap) C ::= {f . I}* (Assertion) a .State .Prop (ProgSpec) . .Address \n. Assertion Figure 10. Auxiliary constructs and speci.cations '' a .a .S.(a S .a ' S) a .a .S.(a S .a \n' S) ' \u00aca .S.\u00aca S a .a .S.a S .a ' S a .a ' .S.a S .a ' S a .a ' .S.a S .a ' S .x.a .S..x.(a S) .x.a \n.S..x.(a S) ' a *a .(M0,R)..M,M ' .M0=MlM ' .a(M,R) .a '(M ' ,R) where MIM ' M . M ' and dom(M) n dom(M \n')=\u00d8 Figure 11. Assertion operators quires an intrinsic understanding of the relation between instruc\u00adtions \nand program speci.cations. Secondly, code is easily guaran\u00adteed to be immutable in an abstract machine \nthat separates code heap as an individual component, which GTM is di.erent from. Surprisingly, the same \nimmutability can be enforced in the infer\u00adence rules using a simple separation conjunction borrowed from \nseparation logic [14, 28]. Speci.cation language. Our speci.cation language is de.ned in Fig 10. A code \nblock B is a syntactic unit that represents a sequence I of instructions, beginning at speci.c memory \naddress f. Note that in CAP, we usually insist that jump instructions can only appear at the end of a \ncode block. This is no longer required in our new system so the division of code blocks is much more \n.exible. The code heap C is a collection of code blocks that do not over\u00adlap, represented by a .nite \nmapping from addresses to instruction sequences. Thus a code block can also be understood as a singleton \ncode heap. To support Hoare-style reasoning, assertions are de.ned as predicates over GTM machine states \n(i.e., via shallow embed\u00adding ). A program speci.cation . is a partial function which maps a memory address \nto its corresponding assertion, with the intention to represent the precondition of each code block. \nThus, . only has entries at each location that indicates the beginning of a code block. Fig 11 de.nes \nan implication relation and a equivalence relation between two assertions (.) and also lifts all the \nstandard logical ' operators to the assertion level. Note the di.erence between a .a . . . .S.Decode(S,f,.) \nif I = . blk(f : I) . . .S.Decode(S,f,.) .(blk(f+|Ec(.)|: I ') S)if I = .;I ' blk(C) .f .dom(C).blk(f \n: C(f)) . . . .1(f) if f .dom(.1) \\dom(.2) . . . (.1 ..2)(f) . .2(f) if f .dom(.2) \\dom(.1) . . . . .1(f) \n..2( f )if f .dom(.1) ndom(.2) Figure 12. Prede.ned functions . HW (Well-formed World) . HC : . (a *(blk(C) \n.blk(pc : I)) S . H{a}pc : I (prog) . H(S,pc) . HC : . ' (Well-formed Code Heap) .2 HC2 : . ' dom(C1) \nndom(C2) = \u00d8 .1 HC1 : . ' 12 (link-c) .1 ..2 HC1 .C2 : . ' 1 .. ' 2 . H{a}f : I (cdhp) . H{f I}: {fa} \n. H{a}B (Well-formed Code Block) . H{a ' }(f+|Ec(.)|): I . .{f+|Ec(.)| a ' }H{a}f : . (seq) . H{a}f \n: .;I .S.a S ..(Npcf,. (S)) (Nextf,.(S)) (instr) . H{a}f : . Figure 13. Inference rules for GCAP0 and \na .a ': the former is an assertion, while the latter is a propo\u00adsition! We also de.ne standard separation \nlogic primitives [14, 28] as assertion operators. The separating conjunction (*) of two asser\u00adtions holds \nif they can be satis.ed on two separating memory ar\u00adeas (the register .le can be shared). Separating \nimplication, empty heap, or singleton heap can also be de.ned directly in our meta logic. Fig 12 de.nes \na few important macros: blk(B) holds if B is stored properly in the memory of the current state; blk(C) \nholds if all code blocks in the code heap C are properly stored. The union of two program speci.cations \nis just the disjunction of the two corresponding assertions at each address. Inference rules. Fig 13 \npresents the inference rules of GCAP0. We give three sets of judgments (from local to global): well-formed \ncode block, well-formed code heap, and well-formed world. Intuitively, a code block is well-formed (. \nf{a}B)i., starting from a state satisfying its precondition a, the code block is safe to execute until \nit jumps to a location in a state satisfying the spec\u00adi.cation .. The well-formedness of a single instruction \n(rule in\u00adstr) directly follows this understanding. Inductively, to validate the well-formedness of a \ncode block beginning with . under pre\u00adcondition a (rule seq), we should .nd an intermediate assertion \n' a serving simultaneously as the precondition of the tail code se\u00adquence, and the postcondition of .. \nIn the second premise of seq, ' since our syntax does not have a postcondition, a is directly fed into \nthe accompanied speci.cation. Note that for a well-formed code block, even though we have added an extra \nentry to the program speci.cation . when we validate each individual instruction, the . used for validating \neach code block and the tail code sequence remains the same. We can instantiate the instr and seq rules \non each instruction if necessary. For example, specializing instr over the direct jump (j f ') results \nin the following rule: a ..(f ') (j) . f{a}f : j f ' Specializing seq over the add instruction makes \n' } . f{af+4: I . .{f+4 a ' }f{a}f : add rd, rs, rt . f{a}f : add rd, rs, rt;I which via instr can be \nfurther reduced into . f{af+4: I ' } .(M, R). a (M, R) .a ' (M, R{rd R(rs)+R(rt)}) (add) . f{a}f : add \nrd, rs, rt;I Another interesting case is the conditional jump instructions, such as beq, which can be \ninstantiated from rule seq as . f{a ' }(f+4): I .(M, R). a (M, R) .((R(rs) = R(rt) . .(f+i)(M, R)) .(R(rs) \n* R(rt) .a ' (M, R))) (beq) . f{a}f : beq rs, rt, i; I The instantiated rules are straightforward to \nunderstand and convenient to use. Most importantly, they can be automatically generated directly from \nthe operational semantics for GTM. The well-formedness of a code heap (. fC : . ') states that given \n. ' specifying the preconditions of each code block of C,all the code in C can be safely executed with \nrespect to speci.cation .. Here the domain of C and . ' should always be the same. The cdhp rule casts \na code block into a corresponding well-formed singleton code heap, and the link-c rule merges two disjoint \nwell-formed code heaps into a larger one. A world is well-formed with respect to a global speci.cation \n. (the prog rule), if the entire code heap is well-formed with respect to .;  the code heap and the \ncurrent code block is properly stored;  A precondition a is satis.ed, separately from the code section; \n the instruction sequence is well-formed under a.  The prog rule also shows how we use separation conjunction \nto ensure that the whole code heap is indeed in the memory and always immutable; because assertion a \ncannot refer to the memory region occupied by C, and the memory domain never grow during the execution \nof a program, the whole reasoning process below the top level never involves the code heap region. This \nguarantees that no code-modi.cation can happen during the program execution. To verify the safety and \ncorrectness of a program, one needs to .rst establish the well-formedness of each code block. All the \ncode blocks are linked together progressively, resulting in a well-formed global code heap where the \ntwo accompanied speci.cations must match. Finally, the prog rule is used to prove the safety of the initial \nworld for the program. Soundness and frame rules. The soundness of GCAP0 guaran\u00adtees that any well-formed \nworld is safe to execute. Establishing a well-formed world is equivalent to an invariant-based proof \nof pro\u00adgram correctness: the accompanied speci.cation . corresponds to a global invariant that the current \nworld satis.es. Theorem 3.3 (Soundness of GCAP0) If . fW, then Safe(W). Detailed formal proofs can be \nfound in our TR [5]. The fol\u00adlowing lemma (a.k.a., the frame rule) captures the essence of local reasoning \nfor separation logic: . f{a}B Lemma 3.4 (frame-block) ' }where a ' is independent of every register modi.ed \nby B. (.f. .(f) *a ') f{a *a B . HW (Well-formed World) . H(C, .) C ' .C (a *(blk(C ') .blk(pc : I))) \nS . ' H{a}pc : I .f .dom(. '). (. '(f) *(blk(C ') .blk(pc : I)) ..(f)) (prog-g) . H(S, pc) . H(C, . ') \n (Well-formed Code Speci.cation) .1 H(C1, . ' ) .2 H(C2, . ' ) dom(C1) ndom(C2) = \u00d8 12 (link-g) .1 ..2 \nH(C1 .C2, . ' 1 .. ' ) 2 . HC : . ' (lift) . *blk(C) H(C, . ' *blk(C)) Figure 14. Inference rules for \nGCAP1 Note that the correctness of this rule relies on the condition we gave in Sec 2 (incorporating \nextra memory does not a.ect the program execution), as also pointed out by Reynolds [28]. With the frame-block \nrule, one can extend a locally certi.ed code block with an extra assertion, given the requirement that \nthis assertion holds separately in conjunction with the original assertion as well as the speci.cation. \nFrame rules at di.erent levels will be used as the main tool to divide code and data to solve the SMC \nissue later. All the derived rules and the soundness proof have been fully mechanized in Coq [5] and \nwill be used freely in our examples.  4. Certifying Runtime Code Generation GCAP1 is a simple extension \nof GCAP0 to support runtime code generation. In the top prog rule for GCAP0, the precondition a for the \ncurrent code block must not specify memory regions occupied by the code heap, and all the code must be \nstored in the memory and remain immutable during the whole execution process. In the case of runtime \ncode generation, this requirement has to be relaxed since the entire code may not be in the memory at \nthe very beginning some can be generated dynamically! Inference rules. GCAP1 borrows the same de.nition \nof well\u00adformed code heaps and well-formed code blocks as in GCAP0: they use the same set of inference \nrules (see Fig 13). To support runtime code generation, we change the top rule and insert an extra layer \nof judgments called well-formed code speci.cation (see Fig 14) between well-formed world and well-formed \ncode heap. If well-formed code heap is a static reasoning layer, well\u00adformed code speci.cation is more \nlike a dynamic one. Inside an assertion for a well-formed code heap, no information about program code \nis included, since it is implicitly guaranteed by the code immutability property. For a well-formed code \nspeci.cation, on the other hand, all information about the required program code should be provided in \nthe precondition for all code blocks. We use the lift rule to transform a well-formed code heap into \na well-formed code speci.cation by attaching the whole code information to the speci.cations on both \nsides. link-g rule has the same form as link-c, except that it works on the dynamic layer. The new top \nrule (prog-g) replaces a well-formed code heap with a well-formed code speci.cation. The initial condition \nis now weakened! Only the current (locally immutable) code heap with the current code block, rather than \nthe whole code heap, is required to be in the memory. Also, when proving the well-formedness of the current \ncode block, the current code heap information is stripped from the global program speci.cation. Local \nreasoning. On the dynamic reasoning layer, since code in\u00adformation is carried with assertions and passed \nbetween code mod\u00adules all the time, veri.cation of one module usually involves the The original code: \n. main: la $9, gen # get the target addr . . . . . li $8, 0xac880000 # load Ec(sw $8,0($4)) . . . . . \nsw $8, 0($9) # store to gen . . . . li $8, 0x00800008 # load Ec(jr $4) . . . B1 . sw $8, 4($9) # store \nto gen+4 . . . . la $4, ggen # $4 = ggen . . . . . la $9, main # $9 = main . . . . . li $8, 0x01200008 \n# load Ec(jr $9) to $8 . . j gen # jump to target gen: nop # to be generated nop # to be generated ggen: \nnop # to be generated The generated code: { gen: sw $8,0($4) B2 jr $4 B3 { ggen: jr $9 Figure 15. mrcg.s: \nMultilevel runtime code generation knowledge of code of another (as precondition). Sometimes, such knowledge \nis redundant and breaks local reasoning. Fortunately, a frame rule can be established on the code speci.cation \nlevel as well. We can .rst locally verify the module, then extend it with the frame rule so that it can \nbe linked with other modules later. . f(C,. ') Lemma 4.1 (frame-spec) (.f..(f) *a) f(C,.f.. '(f) *a) \nwhere a is independent of any register that is modi.ed by C. Proof: By induction over the derivation \nfor . f(C,. '). There are only two cases: if the .nal step is done via the link-g rule, the conclusion \nfollows immediately from the induction hypothesis; if the .nal step is via the lift rule, it must be \nderived from a well\u00adformed-code-heap derivation: ' .0 fC : . (1) 0 with .= .f..0(f) *blk(C) and . ' = \n.f.. ' (f) *blk(C); we .rst 0 apply the frame-cdhp rule to (1) obtain: ' (.f..0(f) *a) fC : .f..0(f) \n*a and then apply the lift rule to get the conclusion. In particular, by setting the above assertion \na to be the knowl\u00adedge about code not touched by the current module, the code can be excluded from the \nlocal veri.cation. As a more concrete example, suppose that we have two locally certi.ed code modules \nC1 and C2, where C2 is generated by C1 at runtime. We .rst apply frame-spec to extend C2 with assertion \nblk(C1), which reveals the fact that C1 does not change during the whole executing process of C2. After \nthis, the link-g rule is applied to link them together into a well-formed code speci.cation. We give \nmore examples about GCAP1 in Section 6. Soundness. The soundness of GCAP1 can be established in the same \nway as Theorem 3.3 (see TR [5] for more detail). Theorem 4.2 (Soundness of GCAP1) If . fW, then Safe(W). \nTo verify a program that involves run-time code generation, we .rst establish the well-formedness of \neach code module (which never modi.es its own code) using the rules for well-formed code heap as in GCAP0. \nWe then use the dynamic layer to combine these code modules together into a global code speci.cation. \nFinally we use the new prog-g rule to establish the initial state and prove the correctness of the entire \nprogram. Example: Multilevel Runtime Code Generation We use a small example mrcg.s in Fig 15 to demonstrate \nthe usability of GCAP1 on runtime code generation. Our mrcg.s is already fairly subtle it does multilevel \nRCG, which means that code generated at runtime may itself generate new code. Multilevel RCG has its \npractical us\u00adage [13]. In this example, the code block B1 can generate B2 (con\u00adtaining two instructions), \nwhich will again generate B3 (containing only a single instruction). The .rst step is to verify B1, B2 \nand B3 respectively and locally, as the following three judgments show: {gen a2 *blk(B2)}H{a1}B1 {ggen \na3 *blk(B3)}H{a2}B2 {main a1}H{a3}B3 where a1 = .S. True, a2 = .(M, R). R($9) = main .R($8) = Ec(jr \n$9) .R($4) = ggen, a3 = .(M, R). R($9) = main As we see, B1 has no requirement for its precondition, \nB2 simply requires that proper values are stored in the registers $4, $8, and $9, while B3 demands that \n$9 points to the label main. All the three judgments are straightforward to establish, by means of GCAP1 \ninference rules (the seq rule and the instr rule). For example, the pre-and selected intermediate conditions \nfor B1 are as follows: {.S. True} main: la $9, gen {.(M, R). R($9) = gen} li $8, 0xac880000 sw $8, 0($9) \n{(.(M, R). R($9) = gen) *blk(gen : sw $8, 0($4))} li $8, 0x00800008 sw $8, 4($9) {blk(B2)} la $4, ggen \nla $9, main li $8, 0x01200008 {a2 *blk(B2)} j gen The .rst .ve instructions generate the body of B2. \nThen, regis\u00adters are stored with proper values to match B2 s requirement. No\u00adtice the three li instructions: \nthe encoding for each generated in\u00adstruction are directly speci.ed as immediate value here. Notice that \nblk(B1) has to be satis.ed as a precondition of B3 since B3 points to B1. However, to achieve modularity \nwe do not require it in B3 s local precondition. Instead, we leave this condition to be added later via \nour frame rule. After the three code blocks are locally certi.ed, the cdhp rule and then the lift rule \nare respectively applied to each of them, as illustrated in Fig 16, resulting in three well-formed singleton \ncode heaps. Afterwards, B2 and B3 are linked together and we apply frame-spec rule to the resulting code \nheap, so that it can successfully be linked together with the other code heap, forming the coherent global \nwell-formed speci.cation (as Fig 16 indicates): .G = {main a1 *blk(B1), gen a2 *blk(B2) *blk(B1), ggen \na3 *blk(B3) *blk(B1)} which should satisfy .G f(C, .G ) (where C stands for the entire code heap). Now \nwe can .nish the last step applying the prog-g rule to the initial world, so that the safety of the whole \nprogram is assured.  5. Supporting General SMC Although GCAP1 is a nice extension to GCAP0, it can hardly \nbe used to certify general SMC. For example, it cannot verify the opcode modi.cation example given in \nFig 8 at the end of Sec 2. LINK:G FRAME:SPEC G G lk( ;*UbI ; Code Heap Code Blocks Control Flow  \n  Figure 17. Typical Control Flow in GCAP2 (ProgSpec) . .Address Assertion (CodeSpec) F .CodeBlock Assertion \nFigure 18. Assertion language for GCAP2 In fact, GCAP1 will not even allow the same memory region to \ncontain di.erent runtime instructions. General SMC does not distinguish between code heap and data heap, \ntherefore poses new challenges: .rst, at runtime, the instruc\u00adtions stored at the same memory location \nmay vary from time to time; second, the control .ow is much harder to understand and rep\u00adresent; third, \nit is unclear how to divide a self-modifying program into code blocks so that they can be reasoned about \nseparately. To tackle these obstacles, we have developed a new veri.cation system GCAP2 supporting general \nSMCs. Our system is still built upon our machine model GTM. The main idea of GCAP2 is illustrated in \nFig 17. Again, the po\u00adtential runtime code is decomposed into code blocks, representing the instruction \nsequences that may possibly be executed. Each code block is assigned with a precondition, so that it \ncan be certi.ed in\u00addividually. Unlike GCAP1, since instructions can be modi.ed, dif\u00adferent runtime code \nblocks may overlap in memory, even share the same entry location. Hence if a code block contains a jump \ninstruc\u00adtion to certain memory address (such as to f1 in Fig 17) at which several blocks start, it is \nusually not possible to tell statically which block it will exactly jump to at runtime. What our system \nrequires instead is that whenever the program counter reaches this address (e.g. f1 in Fig 17), there \nshould exist at least one code block there, whose precondition is matched. After all the code blocks \nare cer\u00ad . HW (Well-formed World) [F] HF a S [F] H{a}pc : I (prog) [F] H(S,pc) where [F] .f..I.F(f : \nI). . HF (Well-formed Code Speci.cation) .1 HF1 .2 HF2 dom(F1) ndom(F2) = \u00d8 (link) .1 ..2 HF1 .F2 .B \n.dom(F).. H{F B}B (cdhp) . HF . H{a}B (Well-formed Code Block) . H{a ' }f+|Ec(.)|: I . .{f+|Ec(.)| a \n' }H{a}f : . (seq) . H{a}f : .;I .S.a S .( Decode(S,f,.) ..(Npcf,.(S)) Nextf,. (S)) (instr) . H{a}f : \n. Figure 19. Inference rules for GCAP2 ti.ed, they can be linked together in a certain way to establish \nthe correctness of the program. To support self-modifying features, we relax the requirements of well-formed \ncode blocks. Speci.cally, a well-formed code block now describes an execution sequence of instructions \nstarting at cer\u00adtain memory address, rather than merely a static instruction se\u00adquence currently stored \nin memory. There is no di.erence between these two understandings under the non-self-modifying circum\u00adstance \nsince the static code always executes as it is, while a funda\u00admental di.erence could appear under the \nmore general SMC cases. The new understanding execution code block characterizes better the real control \n.ow of the program. Our TR discusses more about the importance of this generalization. Speci.cation language. \nThe speci.cation language is almost same as GCAP1, but GCAP2 introduces one new concept called code speci.cation \n(denoted as F in Fig 18), which generalizes the previous code and speci.cation pair to resolve the problem \nof having multiple code blocks starting at a single address. A code speci.cation is a partial function \nthat maps code blocks to their assertions. When certifying a program, the domain of the global F indicates \nall the code blocks that can show up at runtime, and the corresponding assertion of a code block describes \nits global precondition. The reader should note that though F is a partial function, it can have an in.nite \ndomain (indicating that there might be an in.nite number of possible runtime code blocks). Inference \nrules. GCAP2 has three sets of judgements (see Fig 19): well-formed world, well-formed code spec, and \nwell-formed code block. The key idea of GCAP2 is to eliminate the well-formed\u00adcode-heap layer in GCAP1 \nand push the dynamic reasoning layer down inside each code block, even into a single instruction. Inter\u00adestingly, \nthis makes the rule set of GCAP2 look much like GCAP0 rather than GCAP1. The inference rules for well-formed \ncode blocks has one tiny but essential di.erence from GCAP0/GCAP1. A well-formed instruc\u00adtion (instr) \nhas one more requirement that the instruction must ac\u00adtually be in the proper location of memory. Previously \nin GCAP1, this is guaranteed by the lift rule which adds the whole static code heap into the preconditions; \nfor GCAP2, it is only required that the current executing instruction be present in memory. Intuitively, \nthe well-formedness of a code block . f{a}f : I now states that if a machine state satis.es assertion \na, then I is the only possible code sequence to be executed starting from f, until we get to a program \npoint where the speci.cation . can be matched. The precondition for a non-self-modifying code block B \nmust now include B itself, i.e. blk(B). This extra requirement does not compromise modularity, since \nthe code is already present and can be easily moved into the precondition. For dynamic code, the initial \nstored code may di.er from the code actually being executed. Note that our generalization does not make \nthe veri.cation more di.cult: as long as the speci.cation and precondition are given, the well-formedness \nof a code block can be established in the same mechanized way as before. The judgment . fF (well-formed \ncode speci.cation) is fairly comparable with the corresponding judgment in GCAP1 if we notice that the \npair (C, .) is just a way to represent a more limited F. The rules here basically follow the same idea \nexcept that the cdhp rule allows universal quanti.cation over code blocks: if every block in a code speci.cation \ns domain is well-formed with respect to a program speci.cation, then the code speci.cation is well\u00adformed \nwith respect to the same program speci.cation. The interpretation operator [-] establishes the semantic \nre\u00adlation between program speci.cations and code speci.cations: it transforms a code speci.cation to \na program speci.cation by unit\u00ading the assertions (i.e. doing assertion disjunction) of all blocks starting \nat the same address together. In the judgment for well\u00adformed world (rule prog), we use [F] as the speci.cation \nto estab\u00adlish the well-formed code speci.cation and the current well-formed code block. We do not need \nto require the current code block to be stored in memory (as GCAP1 did) since such requirement will be \nspeci.ed in the assertion a already. Soundness and local reasoning. The soundness proof follows almost \nthe same techniques as in GCAP0. Theorem 5.1 (Soundness of GCAP2) If . fW, then Safe(W). Frame rules \nare still the key idea for supporting local reasoning. In fact, since we no longer have the static code \nlayer in GCAP2, the frame rules play a more important role in achieving modularity. For example, to link \ntwo code modules that do not modify each other, we .rst use the frame rule to feed the code information \nof the other module into each module s speci.cation and then apply link rule. Example: Opcode modi.cation. \nWe can now use GCAP2 to cer\u00adtify the opcode-modi.cation example given in Fig 8. There are four runtime \ncode blocks that need to be handled. Fig 20 shows the for\u00admal speci.cation for each code block, including \nboth the local ver\u00adsion and the global version. Note that B1 and B2 are overlapping in memory, so we \ncannot just use GCAP1 to certify this example. Locally, we need to make sure that each code block is \nindeed stored in memory before it can be executed. To execute B1 and B4, we also require that the memory \nlocation at the address new stores a proper instruction (which will be loaded later). On the other hand, \nsince B4 and B2 can be executed if and only if the branch occurs at main, they both have the precondition \nR($2) = R($4). After verifying all the code blocks based on their local speci.ca\u00adtions, we can apply \nthe frame rule to establish the extended speci.\u00adcations. As Fig 20 shows, the frame rule is applied to \nthe local judg\u00adments of B2 and B4, adding blk(B3) on their both sides to form the corresponding global \njudgments. And for B1, blk(B3) *blk(B4) is added; here the additional blk(B4) in the speci.cation entry \nfor halt will be weakened out by the link rule (the union of two pro\u00adgram speci.cations used in the link \nrule is de.ned in Fig 12). Finally, all these judgments are joined together via the link rule to establish \nthe well-formedness of the global code. This is similar to how we certify code using GCAP1 in the previous \nsection, except that the lift process is no longer required here. The global code speci.cation is exactly: \n. . main: beq $2, $4, modify . . B1 . move $2, $4 # B2 starts here . . j halt { target: addi $2, $2, \n1 B2 j halt B3 { halt: j halt . . modify: lw $9, new . . B4 . sw $9, target . . j target Let a1 blk(new \n: addi $2, $2, 1) *blk(B1) ' a blk(B3) *blk(B4) 1 a2 (.(M, R). R($2)=R($4)) *blk(B2) a3 (.(M, R). R($4) \n=R($2) =R($4) + 1) a4 (.(M, R). R($2)=R($4)) *blk(new : addi $2, $2, 1) *blk(B1) Then local judgments \nare as follows {modify a4, halt a3}H{a1}B1 {halt a3}H{a2}B2 {halt a3 *blk(B3)}H{a3 *blk(B3)}B3 {target \na2}H{a4 *blk(B4)}B4 After applying frame rules and linking, the global speci.cation becomes ' '' {modify \na4 *a1, halt a3 *a1}H{a1 *a1}B1 {halt a3 *blk(B3)}H{a2 *blk(B3)}B2 {halt a3 *blk(B3)}H{a3 *blk(B3)}B3 \n' {target a2 *blk(B3)}H{a4 *a1}B4 Figure 20. opcode.s: Code and speci.cation ' ' FG = {B1 a1 *a1, B2 \na2 *blk(B3), B3 a3 *blk(B3), B4 a4 *a1} which satis.es [FG ] fFG and ultimately the prog rule can be \nsuc\u00adcessfully applied to validate the correctness of opcode.s. Actually we have proved not only the type \nsafety of the program but also its partial correctness, for instance, whenever the program executes to \nthe line halt, the assertion a3 will always hold. Parametric code. In SMC, as mentioned earlier, the \nnumber of code blocks we need to certify might be in.nite. Thus, it is im\u00adpossible to enumerate and verify \nthem one by one. To resolve this issue, we introduce auxiliary variable(s) (i.e. parameters) into the \ncode body, developing parametric code blocks and, correspond\u00adingly, parametric code speci.cations. Traditional \nHoare logic only allows auxiliary variables to appear in the pre-or post-condition of code sequences. \nIn our new frame\u00adwork, by allowing parameters appearing in the code body and its assertion at the same \ntime, assertions, code body and speci.cations can interact with each other. This make our program logic \neven more expressive. One simplest case of parametric code block is as follows: f: li $2, k j halt with \nthe number k as a parameter. It simply represents a family of code blocks where k ranges over all possible \nnatural numbers. The code parameters can potentially be anything, e.g., instruc\u00adtions, code locations, \nor the operands of some instructions. Taking a whole code block or a code heap as parameter may allow \nus to express and prove more interesting applications. Certifying parametric code makes use of the universal \nquanti.er in the rule cdhp. In the example above we need to prove the judgment .k. (.k f{.S. True}f : \nli $2, k;j halt) {rDL = 0x80 .D(512) = Ec(jmp -2) *blk(B1)} . bootld: movw $0, %bx # can not use ES B1 \n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . movw movw movb movb movb \nmovb movb int ljmp %bx,%es $0x1000,%bx $1, %al $2, %ah $0, %ch $2, %cl $0, %dh $0x13 $0,$0x1000 # kernel \nsegment # kernel offset # num sectors # disk read command # starting cylinder # starting sector # starting \nhead # call BIOS # jump to kernel {blk(B2)}B2 { kernel: jmp $-2 # loop Figure 21. bootloader.s: Code \nand speci.cation  where .k(halt) = (.(M, R). R($2) = k), to guarantee that the para\u00admetric code block \nis well-formed with respect to the parametric speci.cation .. Parametric code blocks are not just used \nin verifying SMC; they can be used in other circumstances. For example, to prove position independent \ncode, i.e. code whose function does not depend on the absolute memory address where it is stored, we \ncan parameterize the base address of that code to do the certi.cation. Parametric code can also improve \nmodularity, for example, by abstracting out certain code modules as parameters. We will give more examples \nof parametric code blocks in Sec 6. Expressiveness. The following important theorem shows the ex\u00adpressiveness \nof our GCAP2 system: as long as there exists an in\u00advariant for the safety of a program, GCAP2 can be \nused to certify it with a program speci.cation which is equivalent to the invariant. Theorem 5.2 (Expressiveness \nof GCAP2) If Inv is an invariant of GTM, then there is a ., such that for any world (S, pc)wehave Inv(S, \npc) ..((. f(S, pc)) .(.(pc) S)). Together with the soundness theorem (Theorem 5.1), we have showed that \nthere is a correspondence relation between a global program speci.cation and a global invariant for any \nprogram. It should also come as no surprise that any program certi.ed under GCAP1 can always be translated \ninto GCAP2. In fact, the judgments of GCAP1 and GCAP2 have very close connections. See our TR [5] for \nmore discussions on both issues.  6. More Examples and Applications We show the certi.cation of a number \nof representative examples and applications using GCAP (see Table 1 in Sec 1). Due to the space limit, \nwe can only give a few of these in this section. More examples can be found in our TR [5]. 6.1 A Certi.ed \nOS Boot Loader An OS boot loader is a simple, yet prevalent application of runtime code loading. It is \nan artifact of a limited bootstrapping protocol, but one that continues to exist to this day. The limitation \non an x86 architecture is that the BIOS of the machine will only load the .rst 512 bytes of code into \nmain memory for execution. The boot loader is the code contained in those bytes that will load the rest \nof the OS from the disk into main memory, and begin executing the OS (Fig 22). Therefore certifying a \nboot loader is an important piece of a complete OS certi.cation. To show that we support a real boot \nloader, we have created one that runs on the Bochs simulator[18] and on a real x86 machine. The code \n(Fig 21) is very simple, it sets up the registers needed to .data # Data declaration section num: .byte \n8 .text # Code section main: lw $4, num lw $9, key li $8,1 li $2,1 loop: beq $8, $4, halt addi $8, $8, \n1 add $10, $9, $2 key: addi $2, $2, 0 sw $10, key j loop halt: j halt # set argument # $9 = Ec(add $2,$2,0) \n# counter # accumulator # check if done # inc counter # new instr to put # accumulate # store new instr \n# next round Figure 23. fib.s: Fibonacci number main: lw $4, num lw $9, key li $8,1 li $2,1  {$8=1,$2=1,$4=8} \nloop: beq $8, $4, halt loop: addi $8, $8, 1 add $10, $9, $2 addi $2, $2, 0 sw $10, key j loop 7 \n {$8=8,$2=21,$4=8} ... loop: beq $8, $4, halt addi $8, $8, 1 ...  add $10, $9, $2 addi $2, $2, 13 \nsw $10, key j loop {$8=2,$2=1,$4=8} beq $8, $4, halt addi $8, $8, 1 add $10, $9, $2 addi $2, $2, 1 sw \n$10, key j loop  halt:  Figure 24. fib.s: Control .ow make a BIOS call to read the hard disk into \nthe correct memory location, then makes the call to actually read the disk, then jumps to loaded memory. \nThe speci.cations of the boot loader are also simple. rDL = 0x80 makes sure that the number of the disk \nis given to the boot loader by the hardware. The value is passed unaltered to the int in\u00adstruction, and \nis needed for that BIOS call to read from the correct disk. D(512) = Ec(jmp -2) makes sure that the disk \nactually con\u00adtains a kernel with speci.c code. The code itself is not important, but the entry point \ninto this code needs to be veri.able under a triv\u00adial precondition, namely that the kernel is loaded. \nThe code itself can be changed. The boot loader proof will not change if the code changes, as it simply \nrelies on the proof that the kernel code is certi\u00ad.ed. The assertion blk(B1) just says that boot loader \nis in memory when executed. . . . . . . B1 . . . . . . . . . . . . . . . . B2,k . . . . . . . . . . B3 \n{ {blk(B1)} main: lw $4, num lw $9, key li $8,1 li $2,1 {(.(M, R). R($4)=M(num) . R($9)=Ec(addi $2, $2, \n0). R($8)=k+1 . R($2)=.b(k+1)) * blk(B2,k)}loop: beq $8, $4, halt addi $8, $8, 1 add $10, $9, $2 key: \naddi $2, $2, .b(k) sw $10, key j loop {(.(M, R). R($2)=.b(M(num))) * blk(B3)} halt: j halt Figure 25. \nfib.s: Code and speci.cation 6.2 Fibonacci Number and Parametric Code To demonstrate the usage of parametric \ncode, we construct an ex\u00adample fib.s to calculate the Fibonacci function .b(0) = 0, .b(1) = 1, .b(i+2) \n= .b(i) + .b(i+1), shown in Fig 23. More speci.cally, fib.s will calculate .b(M(num)) which is .b(8) \n= 21 and store it into register $2. It looks strange that this is possible since throughout the whole \nprogram, the only instructions that write $2 is the fourth instruction which assigns 1 to it and the \nline key which does nothing. The main trick, of course, comes from the code-modi.cation instruction on \nthe line next to key. In fact, the third operand of the addi instruction on the line key alters to the \nnext Fibonacci number (temporarily calculated and stored in register $10 before the instruction modi.cation) \nduring every loop. Fig 24 illustrates the complete execution process. Since the opcode of the line key \nwould have an unbounded number of runtime values, we need to seek help from paramet\u00adric code blocks. \nThe formal speci.cations for each code block is shown in Fig 25. We specify the program using three code \nblocks, where the second block the kernel loop of our program B2,k is a parametric one. The parameter \nk appears in the operand of the key instruction as an argument of the Fibonacci function. Consider the \nexecution of code block B2,k. Before it is exe\u00adcuted, $9 stores Ec(addi $2,$2,0), and $2 stores .b(k+1). \nThere\u00adfore, the execution of the third instruction addi $10,$9,$2 changes $10 into Ec(addi $2,$2,.b(k+1))1 \n, so at the end of the loop, $2 is now .b(k+1)+.b(k)=.b(k+2), and key has the instruction addi $2,$2,.b(k+1) \ninstead of addi $2,$2,.b(k), then the program continues to the next loop B2,k+1. The global code speci.cation \nwe .nally get is as follows : F {B1 a1, B2k a2k |k .Nat, B3 a3}(2) But note that this formulation is \njust for readability; it is not di\u00adrectly expressible in our meta logic. To express parameterized code \nblocks, we need to use existential quanti.ers. The example is in fact represented as F .B..S.(B=B1 .a1 \nS) .(.k.B=B2k .a2k S) .(B=B3 .a3 S). One can easily see the equivalence between this de.nition and (2). \n 6.3 Self Replication Combining self-reading code with runtime code generation, we can produce self-growing \nprogram, which keeps replicating itself forever. This kind of code appears commonly in Core War a game \nwhere di.erent people write assembly programs that attack the other programs. Our demo code selfgrow.s \nis shown in Fig 26. 1 To simplify the case, we assume the encoding of addi instruction has a linear relationship \nwith respect to its numerical operand in this example.  l a ... l w ... . ..... m ove..  {.S.True} \n{.S.True} main: la $8, pg decr: la $8, pg la $9, pgend la $9, pgend li $10,0xffffffff la $10,0xffffffff \n{a1 *blk(Bpg )}{a1 *a2} xor1: lw $11, 0($8) xor2: lw $11, 0($8) xor $11, $11, $10 xor $11, $11, $10 sw \n$11, 0($8) sw $11, 0($8) addi $8, $8, 4 addi $8, $8, 4  bne $8, $10, loop  $9 pg: li $2, l a ... l \nw ... . ..... m ove.. l w ... . ..... m ove.. l w ... . ..... m ove..   main: la $8, loop la $9, new \n blt $8, $9, xor1 blt $8, $9, xor1 move $10, $9 j decr j loop: lw $11, 0($8) pg sw $11, 0($9) addi \n$8, $8, 4 {.S.True} Bpg = {.(M,R).R($2) = 3} addi $9, $9, 4 1 halt: j halt $10, move  new: li $3,2 \n add $2, $2, $3 j halt a1 .(M,R).pg=R($8)<pgend .R($9)=pgend .R($10)=0xffffffff ( ) . {blk(B0)} a2 .(M,R). \nblk(Bpg ) {pg M(pg),...,pgend-1 M(pgend-1)} . main: la $8, loop . . B0 . la $9, new . . move $10, $9 \nFigure 28. encrypt.s: Code and speci.cation {(.(M,R)..0=i<6.R($8)=k+4i .R($9)=k+24+4i. R($10)=k+24 ..0=j<4i.M(k+ \nj)=M(k+24+ j)) *blk(Bk)} . . k: lw $11, 0($8) . . . . sw $11, 0($9) . . . . Code Heap Code Heap . addi \n$8, $8, 4 Bk . . . addi $9, $9, 4 . start: start: . . . . bne $8, $10, k . body: . move $10, $9 Figure \n27. selfgrow.s: Code and speci.cation decrypt n After initializing the registers, the code repeatedly \nduplicates itself and continue to execute the new copy . The block starting at loop is the code body \nthat keeps being duplicated. During the execution, this part is copied to the new lo\u00adcation. Then the \nprogram continues executing from new, until an\u00adother code block is duplicated. Note that here we rely \non the prop\u00aderty that instruction encodings for branches use relative addressing, thus every time our \ncode is duplicated, the target address of the bne instruction would change accordingly. The copying process \ngoes on and on, till the whole available The main block together with the xor1 block are the encryption \nroutine, which .ips all the bits stored between pg and pgend, thus results in a encrypted form. The decr \nblock and xor2 block, on the other hand, will decrypt the encrypted data and obtaining the original code. \nIn addition to the requirement that proper values are memory is consumed and, presumably, the program \nwould crash. However, under our assumption that the memory domain is in.nite, this code never kill itself \nand thus can be certi.ed. The speci.cation is shown in Fig 27. The code block B0 is certi.ed separately; \nits precondition merely requires that B0 itself stored in the registers $8 to $10, xor1 needs the precondition \nthat matches the memory. All the other code including the original loop body and every generated one \nare parameterized as a code block family and certi.ed altogether. In their preconditions, besides the \nrequirement that the code block matches the memory, there should exist an integer i ranged between 0 \nand 5 (both inclusive), such that the .rst i instructions have been copied properly, and the three registers \n$8, $9, and $10 are stored with proper values respectively.  6.4 Code Encryption Code encryption or \nmore accurately, runtime code decryption Bpg is properly stored, while xor2 on the contrary needs to \nmake sure that the .ip of Bpg is properly stored (as a2 describes). The encryption and the decryption \nroutines are independent and can be separately executed: one can do the encryption .rst and store the \nencrypted code together with the dynamic decryption program, so that at the next time the program is \nloaded, the code can be decrypted and executed dynamically, as shown in Fig 29. By making use of parametric \ncode, It is possible to certify this encrypt-decrypter even without knowledge of the content of Bpg . \nThat is, we abstract Bpg out as a parameter, and prove the general works similarly as runtime code generation, \nexcept that the code generator uses encrypted data located in the same memory region. A simple encryption \nand decryption example encrypt.s adapted from [27] with its speci.cation is shown in Fig 28. The code \nblock Bpg between the labels pg (inclusive) and pgend (exclusive) is the program that is going to be \nencrypted. In this example, Bpg simply calculates the sum of 1 and 2 and stores the result 3 into the \nregister $2, as the precondition of halt indicates. property of the main code : given any code block \nBpg , as long as it can be safely executed under certain precondition, the whole combined code is safe \nto execute under the same precondition; and if Bpg is properly stored before the encryption, it will \nstill be properly stored after the encryption-decryption cycle. More over, the combined code behaves \njust the same as Bpg (meaning that they are both well-formed with respect to the same precondition and \nspeci.cation).  7. Related Work and Conclusion Previous assembly code certi.cation systems (e.g., TAL \n[22], FPCC [1, 10], and CAP [31, 24]) all treat code separately from data memory, so that only immutable \ncode is supported. Appel et al [2, 21] described a model that treats machine instructions as data in \nvon Neumann style; they raised the veri.cation of runtime code generation as an open problem, but did \nnot provide a solu\u00adtion. TALT [6] also implemented the von Neumann machine model where code is stored \nin memory, but it still does not support SMC. TAL/T [13, 29] is a typed assembly language that provides \nsome limited capabilities for manipulating code at runtime. TAL/T code is compiled from Cyclone a type \nsafe C subset with extra support for template-based runtime code generation. However, since the code \ngeneration is implemented by speci.c macro instructions, it does not support any code modi.cation at \nruntime. Otherwise there was actually very little work done on the certi\u00ad.cation of self-modifying code \nin the past. Previous program ver\u00adi.cation systems including Hoare logic, type system, and proof\u00adcarrying \ncode [23] consistently maintain the assumption that pro\u00adgram code stored in memory is immutable. We have \ndeveloped a simple Hoare-style framework for mod\u00adularly verifying general von Neumann machine programs, \nwith strong support for self-modifying code. By statically specifying and reasoning about the possible \nruntime code sequences, we can now successfully verify arbitrary runtime code modi.cation and/or gen\u00aderation. \nTaking a uni.ed view of code and data has given us some surprising bene.ts: we can now apply separation \nlogic to support local reasoning on both program code and regular data structures.  Acknowledgment We \nthank Xinyu Feng, Zhaozhong Ni, Hai Fang, and anonymous referees for their suggestions and comments on \nan earlier version of this paper. Hongxu Cai s research is supported in part by the National Natural \nScience Foundation of China under Grant No. 60553001 and by the National Basic Research Program of China \nunder Grants No. 2007CB807900 and No. 2007CB807901. Zhong Shao and Alexander Vaynberg s research is based \non work sup\u00adported in part by gifts from Intel and Microsoft, and NSF grant CCR-0524545. Any opinions, \n.ndings, and conclusions contained in this document are those of the authors and do not re.ect the views \nof these agencies. References [1] A. W. Appel. Foundational proof-carrying code. In Proc. 16th IEEE \nSymp. on Logic in Computer Science, pages 247 258, June 2001. [2] A. W. Appel and A. P. Felty. A semantic \nmodel of types and machine instructions for proof-carrying code. In Proc. 27th ACM Symposium on Principles \nof Programming Languages, pages 243 253, Jan. 2000. [3] D. Aucsmith. Tamper resistant software: An implementation. \nIn Proceedings of the First International Workshop on Information Hiding, pages 317 333, London, UK, \n1996. Springer-Verlag. [4] V. Bala, E. Duesterwald, and S. Banerjia. Dynamo: a transparent dynamic optimization \nsystem. In Proc. 2000 ACM Conf. on Prog. Lang. Design and Implementation, pages 1 12, 2000. [5] H. Cai, \nZ. Shao, and A. Vaynberg. Certi.ed self-modifying code (extended version &#38; coq implementation). Technical \nReport YALEU/DCS/TR-1379, Yale Univ., Dept. of Computer Science, Mar. 2007. http://flint.cs.yale.edu/publications/smc.html. \n[6] K. Crary. Toward a foundational typed assembly language. In Proc. 30th ACM Symposium on Principles \nof Programming Languages, pages 198 212, Jan. 2003. [7] S. Debray and W. Evans. Pro.le-guided code compression. \nIn Proceedings of the 2002 ACM Conference on Programming Language Design and Implementation, pages 95 \n105, New York, NY, 2002. ACM Press. [8] R. W. Floyd. Assigning meaning to programs. Communications of \nthe ACM, Oct. 1967. [9] N. Glew and G. Morrisett. Type-safe linking and modular assembly language. In \nProc. 26th ACM Symposium on Principles of Program\u00adming Languages, pages 250 261, Jan. 1999. [10] N. A. \nHamid, Z. Shao, V. Trifonov, S. Monnier, and Z. Ni. A syntactic approach to foundational proof-carrying \ncode. In Proc. 17th Annual IEEE Symp. on Logic in Computer Science, pages 89 100, July 2002. [11] G. \nM. Henry. Flexible high-performance matrix multiply via a self\u00admodifying runtime code. Technical Report \nTR-2001-46, Department of Computer Sciences, The University of Texas at Austin, Dec. 2001. [12] C. A. \nR. Hoare. Proof of a program: FIND. Communications of the ACM, Jan. 1971. [13] L. Hornof and T. Jim. \nCertifying compilation and run-time code generation. Higher Order Symbol. Comput., 12(4):337 375, 1999. \n[14] S. S. Ishtiaq and P. W. O Hearn. BI as an assertion language for mutable data structures. In Proc. \n28th ACM Symposium on Principles of Programming Languages, pages 14 26, 2001. [15] Y. Kanzaki, A. Monden, \nM. Nakamura, and K. ichi Matsumoto. Exploiting self-modi.cation mechanism for program protection. In \nCOMPSAC 03, page 170, 2003. [16] L. Lamport. The temporal logic of actions. ACM Transactions on Programming \nLanguages and Systems, 16(3):872 923, May 1994. [17] J. Larus. SPIM: a MIPS32 simulator. v7.3, 2006. \n[18] K. Lawton. BOCHS: IA-32 emulator project. v2.3, 2006. [19] P. Lee and M. Leone. Optimizing ML with \nrun-time code generation. In Proc. 1996 ACM Conf. on Prog. Lang. Design and Implementation, pages 137 \n148. ACM Press, 1996. [20] H. Massalin. Synthesis: An E.cient Implementation of Fundamental Operating \nSystem Services. PhD thesis, Columbia University, 1992. [21] N. G. Michael and A. W. Appel. Machine instruction \nsyntax and semantics in higher order logic. In International Conference on Automated Deduction, pages \n7 24, 2000. [22] G. Morrisett, D. Walker, K. Crary, and N. Glew. From system F to typed assembly language. \nACM Transactions on Programming Languages and Systems, 21(3):527 568, 1999. [23] G. Necula. Proof-carrying \ncode. In Proc. 24th ACM Symposium on Principles of Programming Languages, pages 106 119, New York, Jan. \n1997. ACM Press. [24] Z. Ni and Z. Shao. Certi.ed assembly programming with embedded code pointers. In \nProc. 33rd ACM Symposium on Principles of Programming Languages, Jan. 2006. [25] P. Nordin and W. Banzhaf. \nEvolving turing-complete programs for a register machine with self-modifying code. In Proc. of the 6th \nInternational Conf. on Genetic Algorithms, pages 318 327, 1995. [26] B. C. Pierce. Advanced Topics in \nTypes and Programming Languages. The MIT Press, Cambridge, MA, 2005. [27] Ralph. Basics of SMC. http://web.archive.org/web/ \n20010425070215/awc.rejects.net/files/text/sm%c.txt, 2000. [28] J. Reynolds. Separation logic: a logic \nfor shared mutable data structures. In Proc. 17th IEEE Symp. on Logic in Computer Science, 2002. [29] \nF. M. Smith. Certi.ed Run-Time Code Generation. PhD thesis, Cornell University, Jan. 2002. [30] The Coq \nDevelopment Team, INRIA. The Coq proof assistant reference manual. The Coq release v8.0, 2004-2006. [31] \nD. Yu, N. A. Hamid, and Z. Shao. Building certi.ed libraries for PCC: Dynamic storage allocation. Science \nof Computer Programming, 50(1-3):101 127, Mar. 2004.  \n\t\t\t", "proc_id": "1250734", "abstract": "<p>Self-modifying code (SMC), in this paper, broadly refers to anyprogram that loads, generates, or mutates code at runtime. It is widely used in many of the world's critical software systems tosupport runtime code generation and optimization, dynamic loading and linking, OS boot loader, just-in-time compilation, binary translation,or dynamic code encryption and obfuscation. Unfortunately, SMC is alsoextremely difficult to reason about: existing formal verification techniques-including Hoare logic and type system-consistentlyassume that program code stored in memory is fixedand immutable; this severely limits their applicability and power.</p> <p>This paper presents a simple but novel Hoare-logic-like framework that supports modular verification of general von-Neumann machine code with runtime code manipulation. By dropping the assumption that code memory is fixed and immutable, we are forced to apply local reasoningand separation logic at the very beginning, and treat program code uniformly as regular data structure. We address the interaction between separation and code memory and show how to establish the frame rules for local reasoning even in the presence of SMC. Our frameworkis realistic, but designed to be highly generic, so that it can support assembly code under all modern CPUs (including both x86 andMIPS). Our system is expressive and fully mechanized. We prove itssoundness in the Coq proof assistant and demonstrate its power by certifying a series of realistic examples and applications-all of which can directly run on the SPIM simulator or any stock x86 hardware.</p>", "authors": [{"name": "Hongxu Cai", "author_profile_id": "81546510856", "affiliation": "Tsinghua University, Beijing, China", "person_id": "P871674", "email_address": "", "orcid_id": ""}, {"name": "Zhong Shao", "author_profile_id": "81351597965", "affiliation": "Yale University, New Haven, CT", "person_id": "PP14127817", "email_address": "", "orcid_id": ""}, {"name": "Alexander Vaynberg", "author_profile_id": "81314492420", "affiliation": "Yale University, New Haven, CT", "person_id": "P789535", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1250734.1250743", "year": "2007", "article_id": "1250743", "conference": "PLDI", "title": "Certified self-modifying code", "url": "http://dl.acm.org/citation.cfm?id=1250743"}