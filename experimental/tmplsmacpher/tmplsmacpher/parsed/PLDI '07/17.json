{"article_publication_date": "06-10-2007", "fulltext": "\n Combining Events And Threads For Scalable Network Services Implementation And Evaluation Of Monadic, \nApplication-level Concurrency Primitives Peng Li Steve Zdancewic University of Pennsylvania University \nof Pennsylvania lipeng@cis.upenn.edu stevez@cis.upenn.edu Abstract This paper proposes to combine two \nseemingly opposed program\u00adming models for building massively concurrent network services: the event-driven \nmodel and the multithreaded model. The result is a hybrid design that offers the best of both worlds \nthe ease of use and expressiveness of threads and the .exibility and performance of events. This paper \nshows how the hybrid model can be implemented en\u00adtirely at the application level using concurrency monads \nin Haskell, which provides type-safe abstractions for both events and threads. This approach simpli.es \nthe development of massively concurrent software in a way that scales to real-world network services. \nThe Haskell implementation supports exceptions, symmetrical mul\u00adtiprocessing, software transactional \nmemory, asynchronous I/O mechanisms and application-level network protocol stacks. Ex\u00adperimental results \ndemonstrate that this monad-based approach has good performance: the threads are extremely lightweight \n(scaling to ten million threads), and the I/O performance compares favor\u00adably to that of Linux NPTL. \nCategories and Subject Descriptors D.1.1 [Programming tech\u00adniques]: Applicative (Functional) Programming; \nD.1.3 [Program\u00adming techniques]: Concurrent Programming; D.2.11 [Software Engineering]: Software Architectures \nDomain-speci.c architec\u00adtures; D.3.3 [Programming Languages]: Language Constructs and Features Concurrent \nprogramming structures, Control structures, Frameworks; D.4.1 [Operating Systems]: Process Management \nConcurrency, Multiprocessing / multiprogramming / multitasking, Scheduling, Threads. General Terms Languages, \nDesign, Experimentation, Perfor\u00admance, Measurement. Keywords Event, Thread, Concurrency, Networking, \nProgram\u00adming, Scalability, Implementation, Monad, Haskell. 1. Introduction Modern network services present \nsoftware engineers with a num\u00adber of design challenges. Peer-to-peer systems, multiplayer games, Permission \nto make digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior speci.c permission and/or a fee. PLDI 07 June 11 \n13, 2007, San Diego, California, USA. Copyright c &#38;#169; 2007 ACM 978-1-59593-633-2/07/0006. . . \n$5.00 and Internet-scale data storage applications must accommodate tens of thousands of simultaneous, \nmostly-idle client connections. Such massively-concurrent programs are dif.cult to implement, es\u00adpecially \nwhen other requirements, such as high performance and strong security, must also be met. Events vs. threads: \nTwo implementation strategies for building such inherently concurrent systems have been successful. Both \nthe multithreaded and event-driven approaches have their proponents and detractors. The debate over which \nmodel is better has waged for many years, with little resolution. Ousterhout [19] has argued that threads \nare a bad idea (for most purposes), citing the dif.cul\u00adties of ensuring proper synchronization and debugging \nwith thread\u00adbased approaches. A counter argument, by von Behren, Condit, and Brewer [25], argues that \nevents are a bad idea (for high\u00adconcurrency servers), essentially because reasoning about control .ow \nin event-based systems is dif.cult and the apparent perfor\u00admance wins of the event-driven approach can \nbe completely re\u00adcouped by careful engineering [26]. The debate over threads and events never seems to \nend be\u00adcause a programmer often has to choose one model and give up the other. For example, if a Linux \nC programmer uses POSIX threads to write a web server, it is dif.cult to use asynchronous I/O. On the \nother hand, if the programmer uses epoll and AIO in Linux to write a web server, it will be inconvenient \nto represent control .ow for each client. The reason for this situation is that conventional thread abstraction \nmechanisms are too rigid: threads are imple\u00admented in the OS and runtime libraries and the user cannot \neasily customize these components and integrate them with the applica\u00adtion. Although threads can be made \nlightweight and ef.cient, an event-driven system still has the advantage on .exibility and cus\u00adtomizability: \nit can always be tailored and optimized to the applica\u00adtion s speci.c needs. The hybrid model: Ideally, \nthe programmer would design parts of the application using threads, where threads are the appropri\u00adate \nabstraction (for per-client code), and parts of the system using events, where they are more suitable \n(for asynchronous I/O inter\u00adfaces). To make this hybrid model feasible, not only should the sys\u00adtem provide \nthreads, but the thread scheduler interface must also provide a certain level of abstraction, in the \nform of event handlers, which hides the implementation details of threads and can be used by the programmer \nto construct a modular event-driven system. Many existing systems implement the hybrid model to various \ndegrees; most of them have a bias either toward threads or toward events. For example, Capriccio [26] \nis a user-level, cooperative thread library with a thread scheduler that looks very much like an event-driven \napplication. However, it provides no abstraction on the event-driven side: the scheduler uses a low-level, \nunsafe program\u00adming interface that is completely hidden from the programmer. On the other hand, many \nevent-driven systems uses continuation\u00adpassing style (CPS) programming to represent the control .ow for \neach client; the problem is that CPS programs are often dif.cult to write and understand. Although CPS \nis a scalable design, it is not as intuitive as conventional multithreaded programming styles (for most \nprogrammers). Application-level implementation: The hybrid model adopted here encourages both the multithreaded \ncomponents and the event\u00addriven components of the application be developed in a uniform programming environment. \nTo do so, it is most convenient to im\u00adplement the concurrency abstractions (both thread abstractions \nand event abstractions) entirely inside the application, using standard programming language idioms. \nWe call this idea application-level implementation. Programming concurrency abstractions entirely inside \nthe ap\u00adplication is a signi.cant challenge on legacy programming lan\u00adguage tools: for languages like \nC, implementing user-level threads and schedulers involves a lot of low-level, unsafe programming in\u00adterfaces. \nNevertheless, it is indeed feasible to implement the hybrid model directly at application-level using \nmodern functional pro\u00adgramming languages such as Haskell. In 1999, Koen Claessen [8] showed that (cooperative) \nthreads can be implemented using only a monad, without any change to the programming language itself. \nA hybrid framework for network services: This paper uses Claessen s technique to implement the hybrid \nmodel entirely inside the application and develop a framework for building massively concurrent network \nservices. Our implementation is based on Con\u00adcurrent Haskell [14], supported by the latest version of \nGHC [11]. Improving on Claessen s original, proof-of-concept design, our im\u00adplementation offers the following: \n True Parallelism: Application-level threads are mapped to mul\u00adtiple OS threads and take advantage of \nSMP systems.  Modularity and Flexibility: The scheduler is a customizable event-driven system that uses \nhigh-performance, asynchronous I/O mechanisms. We implemented support for Linux epoll and AIO; we even \nplugged a TCP stack to our system.  Exceptions: Multithreaded code can use exceptions to handle failure, \nwhich is common in network programming.  Thread synchronization: Non-blocking synchronization comes \nalmost for free software transactional memory (STM) in GHC can be transparently used in application-level \nthreads. We also implemented blocking synchronization mechanisms such as mutexes.  Evaluation: Our hybrid \nconcurrency framework in Haskell competes favorably against most existing thread-based or event\u00adbased \nsystems for building network services. It provides elegant in\u00adterfaces for both multithreaded and event-driven \nprogramming, and it is all type-safe! Experiments suggest that, although everything is written in Haskell, \na pure, lazy, functional programming language, the performance is acceptable in practice: I/O performance: \nOur implementation delivers performance comparable to Linux NPTL in disk and FIFO pipe performance tests, \neven when tens of thousands of threads are used.  Scalability: Besides bounds on memory size and other \nsystem resources, there is no limit on the number of concurrent clients that our implementation can handle. \nIn the I/O tests, our im\u00adplementation scaled to far more threads than Linux NPTL did. From the OS point \nof view, it is just as scalable as an event\u00addriven system.  Memory utilization: Our implementation has \nextremely ef.\u00adcient memory utilization. All the thread-local state is explicitly  controlled by the \nprogrammer. As we have tested, each monadic thread consumes as little as 48 bytes at run time, and our \nsys\u00adtem is capable of running 10,000,000 such threads on a real machine. Summary of contributions: The \nidea of the concurrency monad is not new at all we are building on work done by functional programming \nresearchers. Our contribution is to experiment with this elegant design in real-world systems programming \nand evalu\u00adate this technique, both qualitatively and quantitatively: 1. We scaled up the design of the \nconcurrency monad to a real\u00adworld implementation, providing elegant and .exible interfaces for building \nmassively concurrent network services using ef.\u00adcient asynchronous I/O. 2. We proved that the monad-based \ndesign has good performance: it delivers optimal I/O performance; it has ef.cient memory utilization \nand it scales as well as event-driven systems. 3. We demonstrated the feasibility of the hybrid programming \nmodel in high-performance network servers, providing future directions for both systems and programming \nlanguage re\u00adsearch.  Our experience also suggests that Haskell is a reasonable lan\u00adguage for building \nscalable systems software: it is expressive, suc\u00adcinct, ef.cient and type-safe; it interacts well with \nC libraries and APIs. 2. The hybrid programming model This section gives some background on the multithreaded \nand event-driven approaches for building massively concurrent network services, and motivates the design \nof the hybrid model. 2.1 A comparison of events vs. threads Programming: The primary advantage of the \nmultithreaded model is that the programmer can reason about the series of ac\u00adtions taken by a thread \nin the familiar way, just as for a sequential program. This approach leads to a natural programming style \nin which the control .ow for a single thread is made apparent by the program text, using ordinary language \nconstructs like conditional statements, loops, exceptions, and function calls. Event-driven programming, \nin contrast, is hard. Most general\u00adpurpose programming languages do not provide appropriate ab\u00adstractions \nfor programming with events. The control .ow graph of an event-driven program has to be decomposed into \nmultiple event handlers and represented as some form of state machine with ex\u00adplicit message passing \nor in continuation-passing style (CPS). Both representations are dif.cult to program with and reason \nabout, as indicated by the name of Python s popular, event-driven network\u00ading framework, Twisted [24]. \nPerformance: The multithreaded programming style does not come for free: In most operating systems, a \nthread uses a reserved segment of stack address space, and the virtual memory space ex\u00adhausts quickly \non 32-bit systems. Thread scheduling and context switching also have signi.cant overheads. However, such \nperfor\u00admance problems can be reduced by well engineered thread libraries and/or careful use of cooperative \nmultitasking a recent example in this vein is Capriccio [26], a user-level thread library speci.cally \nfor use in building highly scalable network services. The event-driven approach exposes the scheduling \nof inter\u00adleaved computations explicitly to the programmer, thereby permit\u00adting application-speci.c optimizations \nthat signi.cantly improve performance. The event handlers typically perform only small amounts of work \nand usually need only small amounts of local Figure 1. Threads vs. events storage. Compared to multithreaded \nsystems, event-driven systems can have the minimal per-thread memory overheads and context switching \ncosts. Furthermore, by grouping similar events together, they can be batch-processed to improve code \nand data locality [15]. Flexibility and customizability: Most thread systems provide an abstract yet \nrigid, synchronous programming interface and the implementation of the scheduler is mostly hidden from \nthe pro\u00adgramming interface. Hiding the scheduler makes it inconvenient when the program requires the \nuse of asynchronous I/O interfaces not supported by the thread library, especially those affecting the \nscheduling behavior. For example, if the I/O multiplexing of a user\u00adlevel thread library is implemented \nusing the portable select in\u00adterface, it is dif.cult to use an alternative high-performance inter\u00adface \nlike epoll without modifying the scheduler. Event-driven systems are more .exible and customizable be\u00adcause \nthe programmer has direct control of resource management and direct access to asynchronous OS interfaces. \nMany high\u00adperformance I/O interfaces (such as AIO, epoll and kernel event queues) provided by popular \nOSes are asynchronous or event\u00addriven, because this programming model corresponds more closely to the \nmodel of hardware interrupts. An event-driven system can directly take advantage of such asynchronous, \nnon-blocking inter\u00adfaces, while using thread pools to perform synchronous, blocking operations at the \nsame time. Another concern is that most user-level cooperative thread sys\u00adtems do not take advantages \nof multiple processors, and adding such support is often dif.cult [26]. Event-driven systems can easily \nutilize multiple processors by processing independent events con\u00adcurrently [29]. 2.2 The hybrid programming \nmodel In 1978, Lauer and Needham [16] argued that the multithreaded and event-driven models are dual \nto each other. They described an one-to-one mapping between the constructs of each paradigm and suggest \nthat the two approaches should be equivalent in the sense that either model can be made as ef.cient as \nthe other. The duality they presented looks like this: Threads Events thread continuation ~ event handler \nscheduler ~ event loop exported function ~ event blocking call ~ send event / await reply The Lauer-Needham \nduality suggests that despite their large conceptual differences and the way people think about program\u00adming \nin them, the multithreaded and event-driven models are really the same underneath. Most existing approaches \ntrade off threads for events or vice versa, choosing one model over the other. The current situation \nis shown in Figure 1: multithreaded systems are dif.cult to customize, and event-driven systems have \npoor pro\u00adgramming abstractions. We adopt a different route: rather than using the duality to justify \nchoosing threads over events or vice versa (since either Figure 2. The hybrid model and application-level \nimplementation choice can be made as ef.cient and scalable as the other), we see the duality as a strong \nindication that the programmer should be able to use both models of concurrency in the same system. The \nduality thus suggests that we should look for natural ways to support switching between the views as \nappropriate to the task at hand. Figure 2 presents the hybrid model that can be seen either as a multithreaded \nsystem with a programmable scheduler, or as an event-driven system with a thread abstraction for representing \ncon\u00adtrol .ow. The key is to implement both thread abstractions that rep\u00adresent control .ow, and event \nabstractions that provide scheduling primitives. These abstractions provide control inversion between \nthe two worlds: the threads play the active role and make blocking calls into the I/O system, but the \nunderlying event-driven system also plays the active role and schedules the execution of threads as if \ncalling event handlers. The hybrid model gives the best of two worlds: the expressiveness of threads \nand the customizability of events. 2.3 Application-level implementation Implementing the hybrid model \nis challenging. Conventional thread abstractions are provided outside the application by means of OS/VM/compiler/library \nsupport. The scheduler code may live in a different address space, be written in a different language, \nuse very low-level programming interfaces, be linked to different li\u00adbraries, and be compiled separately \nfrom the multithreaded code. Such differences in the programming environments makes it in\u00adconvenient \nto combine the threads and the scheduler in the same application. Thus, a challenge is how to develop \nboth components of the hybrid model (the boxes on the top and bottom of Figure 2) in the same programming \nenvironment, by which we mean: they are written in the same programming language;  they can share the \nsame software libraries;  they live in the same address space;  they can easily communicate using shared \ndata structures;  their source codes are compiled and optimized together to pro\u00adduce a single executable \n.le.  This problem can be solved if the thread/event abstractions (the center box of Figure 2) are also \ndeveloped in the same program\u00adming environment, entirely inside the application as program mod\u00adules. \nWe refer to this approach as application-level implementation. A good approximation of this idea is event-driven \nsystems with continuation-passing-style (CPS) event handlers [24]. In many lan\u00adguages, CPS can be implemented \nentirely inside the application, using standard programming idioms such as function closures or objects. \nFurthermore, the CPS approach also solves the problem of control inversion: CPS represents multithreaded \ncomputation, but such computation can be manipulated as values and called as event handlers. The only \ncaveat is that CPS is not as intuitive and easy to understand as C-like, imperative threads not every \nprogrammer can think in CPS! The next section shows that, with a little more support from programming \nlanguages, good abstractions can be provided to hide the details of CPS from the programmer, and the \nresult is an elegant implementation of the hybrid model.  3. Implementing the hybrid model in Haskell \nThe CPS programming technique provides a great foundation for application-level implementation of the \nhybrid model: CPS has expressive control .ow, and CPS can be readily represented in many programming \nlanguages. The challenge is to build the thread abstraction and hide the details of continuation passing. \nThe simplest idea is to perform a source-to-source CPS transla\u00adtion [3] in the compiler. This approach \nis not entirely application\u00adlevel because it requires nonstandard compiler extensions. Further\u00admore, \nbecause we want the multithreaded code to have the same programming environment as the scheduler code, \nthe compiler has to translate the same language to itself. Such translations are often verbose, inef.cient \nand not type-safe. Can we achieve the same goal without writing compiler ex\u00adtensions? One solution, developed \nin the functional programming community and supported in Haskell, is to use monads [18, 27]. The Haskell \nlibraries provide a Monad interface that allows generic programming with functional combinators. The \nsolution we adopt here is to design the thread control primitives (such as fork)as monadic combinators, \nand use them as a domain-speci.c language directly embedded in the program. Such primitives hide the \ninter\u00adnal plumbing of CPS in their implementation and gives an abstrac\u00adtion for multithreaded programming. \nIn principle, this monad-based approach can be used in any lan\u00adguage that supports the functional programming \nstyle. However, programming in the monadic style is often not easy, because it re\u00adquires frequent use \nof binding operators and anonymous functions, making the program look quite verbose. Haskell has two \nfeatures that signi.cantly simplify this programming style: Operator Overloading:Using type classes, \nthe standard monad operators can be made generic, because the overloading of such operators can be resolved \nby static typing.  Syntactic Sugar: Haskell has a special do-syntax for program\u00adming with monads. Using \nthis syntax, the programmer can write monadic code in a C-like imperative programming style, and the \ncompiler automatically translates this syntax to generic monadic operators and anonymous functions. \n In 1999, Koen Claessen showed that cooperative multithreading can be represented using a monad [8]. \nHis design extends to an elegant, application-level implementation technique for the hybrid model, where \nthe monad interface provides the thread abstraction and a lazy data structure provides the event abstraction. \nThis section revisits this design, and the next section shows how to use this technique to multiplex \nI/O in network server applications. 3.1 Traces and system calls In this paper, we use the phrase system \ncalls to refer to the following thread operations at run time: Thread control primitives, such as fork \nand yield.  I/O operations and other effectful IO computations in Haskell.  A central concept of Claessen \ns implementation is the trace,a structure describing the sequence of system calls made by a thread. A \ntrace may have branches because the corresponding thread can use fork to spawn new threads. For example, \nexecuting the (recur\u00adsive) server function shown on the left in Figure 4 generates the in.nite trace \nof system calls on the right. _ _ server = do { SYS_CALL_1 sys_call_1; | fork client; SYS_FORK server; \n/ \\ } SYS_CALL_2 SYS_CALL_1 / client = do { SYS_FORK sys_call_2; / \\ } SYS_CALL_2 SYS_CALL_1 ... ... \n_ _ Figure 4. Some threaded code (left) and its trace (right) A run-time representation of a trace can \nbe de.ned as a tree using algebraic data types in Haskell. The de.nition of the trace is essentially \na list of system calls, as shown in Figure 5. Each system call in the multithreaded programming interface \ncorresponds to exactly one type of tree node. For example, the SYS FORK node has two sub-traces, one \nfor the continuation of the parent thread and one for the continuation of the child. Note that Haskell \ns type system distinguishes code that may perform side effects as shown in the type of a SYS NBIO node, \nwhich contains an IO computation that returns a trace. _ -- A list of system calls used in the multithreaded \nprogramming style: sys_nbio c -- Perform a nonblocking IO function c sys_fork c -- Create a new thread \nrunning function c sys_yield -- Switch to another thread sys_ret -- Terminate the current thread sys_epoll_wait \nfd event -- Block and wait for an epoll event on a ... ... -- .le descriptor _ _ data Trace = -- Haskell \ndata type for traces SYS_NBIO (IO Trace) | SYS_FORK Trace Trace | SYS_YIELD Trace | SYS_RET | SYS_EPOLL_WAIT \nFD EPOLL_EVENT Trace | ... ... _ Figure 5. System calls and their corresponding traces Lazy evaluation \nof traces and thread control: We can think of a trace as the output of a thread execution: as the thread \nruns and makes system calls, the nodes in the trace are generated. What makes the trace interesting is \nthat computation is lazy in Haskell: a computation is not performed until its result is used. Using lazy \nevaluation, the consumer of a trace can control the execution of its producer, which is the thread: whenever \na node in the trace is ex\u00adamined (or, forced to be evaluated), the thread runs to the system call that \ngenerate the corresponding node, and the execution of that thread is suspended until the next node in \nthe trace is examined. In other words, the execution of threads can be controlled by travers\u00ading their \ntraces. Figure 3 shows how traces are used to control the thread exe\u00adcution. It shows a run-time snapshot \nof the system: the scheduler decides to resume the execution of a thread, which is blocked on a system \ncall sys epoll wait in the sock send function. The fol\u00adlowing happens in a sequence: 1. The scheduler \nforces the current node in the trace to be evalu\u00adated, by using the case expression to examine its value. \n Figure 3. Thread execution through lazy evaluation (the steps are described in the text) 2. Because \nof lazy evaluation, the current node of the trace is not known yet, so the continuation of the thread \nis called in order to compute the value of the node. 3. The thread continuation runs to the point where \nthe next system call sys nbio is performed. 4. The new node in the trace is generated, pointing to the \nnew continuation of the thread. 5. The value of the new node, SYS NBIO is available in the sched\u00aduler. \nThe scheduler then handles the system call by performing the I/O operation and runs the thread continuation, \nfrom Step 1 again.  The lazy trace gives the event abstraction we need. It is an abstract interface \nthat allows the scheduler to play the active role and the threads to play the passive role: the scheduler \ncan use traces to actively push the thread continuations to execute. Each node in the trace essentially \nrepresents part of the thread execution. The remaining problem is to .nd a mechanism that transforms \nmultithreaded code into traces.  3.2 The CPS monad A monad can provide the thread abstraction we need. \nSystem calls can be implemented as monad operations, and the do-syntax of Haskell offers an imperative \nprogramming style. The implemen\u00ad tation of this monad is quite tricky, but the details are hidden from \nthe programmer (in the box between the thread abstraction and the event abstraction in Figure 2). Haskell \ns monad typeclass, shown in Figure 6, requires a monad implemented using a parameterized abstract datatype \nm to support two key operations. The .rst operation, return, takes a value of type a and lifts it into \nthe monad. The second in.x operation, (>>=) (pronounced bind ), provides a sequential composition of \ncomputations in the monad. _ class Monad m where return :: a -> m a  (>>=) ::ma-> (a -> mb)->mb  \n_ Figure 6. Haskell s Monad interface (excerpt) The monad we need encapsulates the side effects of a \nmulti\u00adthreaded computation, that is, generating a trace. The tricky part is that if we simply represent \na computation with a data type that car\u00adries its result value and its trace, such a data type cannot \nbe used as a monad, because the monads require that computations be sequen\u00ad tially composable. Given \ntwo complete (possibly in.nite) traces, there is no meaningful way to compose them sequentially. The \nsolution is to represent computations in continuation\u00ad passing style (CPS), where the .nal result of \nthe computation is the trace. A computation of type a is thus represented as a func\u00ad tion of type (a->Trace)->Trace \nthat expects a continuation, itself a function of type (a->Trace), and produces a Trace.This representation \ncan be used to construct a monad M. The standard monadic operations (return and sequential composition) \nfor this monad are de.ned in Figure 7. _ newtype M a=M((a->Trace)->Trace) instance Monad M where returnx \n=M(\\c ->cx)  (M g)>>=f=M(\\c ->g(\\a -> let Mh= fain h c))  _ Figure 7. The CPS monad M Intuitively, \nreturn takes a value x of type a and, when given a continuation c, simply invokes the continuation c \non x.The im\u00ad plementation of (>>=) threads the continuations through the oper\u00ad ations to provide sequential \ncomposition: given the .nal contin\u00ad uation c, (M g) >>= f .rst calls g, giving it a continuation (the \nexpression beginning (\\a -> ...) that expects the result a com\u00ad puted by g. This result is then passed \nto f, the results of which are encapsulated in the closure h. Finally, since the .nal continuation of \nthe whole sequential composition is c, that continuation is passed to h. Altogether, we can think of \nthis as (roughly) doing g;f;c. Given a computation Ma in the above CPS monad, we can access its trace \nby adding a .nal continuation to it, that is, adding a leaf node SYS RET to the trace. The function build \ntrace in Figure 8 converts a monadic computation into a trace. _ build_trace :: M a -> Trace build_trace \n(M f) = f (\\c-> SYS_RET)  _ Figure 8. Converting monadic computation to a trace In Figure 9, each system \ncall is implemented as a monadic operation that creates a new node in the trace. The arguments of system \ncalls are .lled in to corresponding .elds in the trace node. Since the code is internally organized in \nCPS, we are able to .ll the trace pointers (.elds of type Trace ) with the continuation of the current \ncomputation (bound to the variable c in the code). For readers unfamiliar with monads in Haskell, it \nmay be dif.\u00ad cult to follow the above code. Fortunately, these bits of code encap\u00ad sulate all of the \ntwisted parts of the internal plumbing in the CPS. _ sys_nbio f = M(\\c->SYS_NBIO (do x<-f;return (c \nx))) sys_fork f = M(\\c->SYS_FORK (build_trace f) (c ()) sys_yield = M(\\c->SYS_YIELD (c ())) sys_ret = \nM(\\c->SYS_RET) sys_epoll_wait fd ev = M(\\c->SYS_EPOLL_WAIT fd ev (c ()))  _ Figure 9. Implementing some \nsystem calls The implementation of the monad can be put in a library, and the programmer only needs to \nunderstand its interface. To write multi\u00adthreaded code, the programmer simply uses the do-syntax and \nthe system calls; to access the trace of a thread in the event loop, one just applies the function build \ntrace to it to get the lazy trace.   4. Building network services With the concurrency primitives introduced \nin the previous sec\u00ad tion, we can combine events and threads to build scalable network services in an \nelegant way: the code for each client is written in a cheap , monad-based thread, while the entire application \nis an event-driven program that uses asynchronous I/O mechanisms. Our current implementation uses the \nGlasgow Haskell Compiler (GHC) [11]. It is worth noting that GHC already supports ef.cient, lightweight \nuser-level threads, but we do not use one GHC thread for each client directly, because the default GHC \nlibrary uses the portable (yet less scalable) select interface to multiplex I/O and it does not support \nnon-blocking disk I/O. Instead, we employ only a few GHC threads, each mapped to an OS thread in the \nGHC run\u00ad time system. In the rest of the paper, we use the name monadic thread for the application-level, \ncheap threads written in the do\u00ad syntax, and we do not distinguish GHC threads from OS threads. 4.1 \nProgramming with monadic threads In the do-syntax, the programmer can write code for each client session \nin the familiar, multithreaded programming style, just like in C or Java. All the I/O and effectful operations \nare performed through system calls. For example, the sys nbio system call takes a non-blocking Haskell \nIO computation as its argument. The sys epoll wait system call has a blocking semantics: it waits until \nthe supplied event happens on the .le descriptor. The multithreaded programming style makes it easy to \nhide the non-blocking I/O semantics and provide higher level abstrac\u00ad tions by encapsulating low-level \noperations in functions. For ex\u00ad ample, a blocking sock accept can be implemented using non\u00ad blocking \naccept as shown in Figure 10: it tries to accept a con\u00ad nection by calling the non-blocking accept function. \nIf it suc\u00ad ceeds, the accepted connection is returned, otherwise it waits for an EPOLL READ event on \nthe server socket, indicating that more connections can be accepted. _ sock_accept server_fd = do { \n new_fd <-sys_nbio (accept server_fd); if new_fd > 0 then return new_fd else do { sys_epoll_wait fd \nEPOLL_READ; sock_accept server_fd; } }  _ Figure 10. Wrapping non-blocking I/O calls to blocking \ncalls  4.2 Programming the scheduler Traces provide an abstract interface for writing thread schedulers: \na scheduler is just a tree traversal function. To make the technical _ worker_main ready_queue = do \n{ -- fetch a trace from the queue trace <-readChan ready_queue;  case trace of -- Nonblocking I/O \noperation: c has type IO Trace SYS_NBIO c -> do { -- Perform the I/O operation in c -- The result is \ncont, which has type Trace cont <-c; -- Add the continuation to the end of the ready queue writeChan \nready_queue cont; } -- Fork: write both continuations to the end of the ready queue SYS_FORK c1 c2 -> \ndo { writeChan ready_queue c1; writeChan ready_queue c2; } SYS_RET -> return (); -- thread terminated, \nforget it worker_main ready_queue; -- recursion }  _ Figure 11. A round-robin scheduler for three sys. \ncalls presentation simpler, suppose there are only three system calls: SYS NBIO, SYS FORK and SYS RET. \nFigure 11 shows code that implements a naive round-robin scheduler; it uses a task queue called ready \nqueue and an event loop called worker main.The scheduler does the following in each loop: (1) fetch a \ntrace from the queue, (2) force the corresponding monadic thread to execute (using case) until a system \ncall is made, (3) perform the requested system call, and (4) write the child nodes (the continuations) \nof the trace to the queue. In our actual scheduler implementation, more system calls are supported. Also, \na thread is executed for a large number of steps before switching to another thread to improve locality. \n _ sys_throw e -- raise an exception e sys_catch f g -- execute computation f using the exception handler \ng  data Trace = ... -- The corresponding nodes in the trace: | SYS_THROW Exception  | SYS_CATCH \nTrace (Exception->Trace) Trace  _ Figure 12. System calls for exceptions  4.3 Exceptions Exceptions \nare useful especially in network programming, where failures are common. Because the threaded code is \ninternally struc\u00ad tured in CPS, exceptions can be directly implemented as system calls (monad operations) \nshown in Figure 12. The code in Figure 13 illustrates how exceptions are used by a monadic thread. The \nscheduler code in Figure 11 needs to be extended to support these system calls. When it sees a SYS CATCH \nnode, it pushes the node onto a stack of exception handlers maintained for each thread. When it sees \na SYS RET or SYS THROW node, it pops one frame from the stack and continues with either the normal trace \nor exception handler, as appropriate.  4.4 Multiple event loops and SMP support Figure 14 shows the \nevent-driven system in our full implemen\u00adtation. It consists of several event loops, each running in \na sepa\u00adrate OS thread, repeatedly fetching a task from an input queue or waiting for an OS event and \nthen processing the task/event before putting the continuation of the task in the appropriate output \nqueue. The worker main event loops are simply thread schedulers: they  _ -- send a .le over a socket \nsend_file sock filename = do { fd <-file_open filename; buf <-alloc_aligned_memory buffer_size; sys_catch \n( copy_data fd sock buf 0 ) \\exception -> do { file_close fd; sys_throw exception; } -- so the caller \ncan catch it again file_close fd; }  -- copy data from a .le descriptor to a socket until EOF copy_data \nfd sock buf offset = do { num_read <-file_read fd offset buf; if num_read==0 then return () else do { \nsock_send sock buf num_read; copy_data fd sock buf (offset+num_read); } }  _ Figure 13. Multithreaded \ncode with exception handling execute the monadic threads and generate events that can be con\u00ad sumed by \nother event loops. To take advantage of SMP machines, the system runs multi\u00ad ple worker main event loops \nin parallel so that multiple monadic threads can make progress simultaneously. This setup is based on \nthe assumption that all the I/O operations submitted by SYS NBIO are nonblocking and thread-safe. Blocking \nor thread-unsafe I/O op\u00ad erations are handled in this framework either by using a separate queue and \nevent loop to serialize such operations, or by using syn\u00ad chronization primitives (described below) in \nthe monadic thread. This event-driven architecture is similar to that in SEDA [28], but our events are \n.ner-grained: instead of requiring the program\u00ad mer manually decompose a computation into stages and \nspecify what stages can be performed in parallel, this event-driven sched\u00ad uler automatically decomposes \na threaded computation into .ne\u00ad grained segments separated by system calls. Haskell s type system ensures \nthat each segment is a purely functional computation with\u00ad out I/O, so such segments can be safely executed \nin parallel. Most user-level thread libraries do not take advantage of multi\u00ad ple processors, primarily \nbecause synchronization is dif.cult due to shared state in their implementations. Our event abstraction \nmakes this task easier, because it uses a strongly typed interface in which pure computations and I/O \noperations are completely separated. Our current design can be further improved by implementing a sep\u00ad \narate task queue for each scheduler and using work stealing to bal\u00adance the loads.  4.5 Asynchronous \nI/O in Linux Our implementation supports two high-performance, event-driven I/O interfaces in Linux: \nepoll and AIO. Epoll provides readiness noti.cation of .le descriptors; AIO allows disk accesses to proceed \nin the background. A set of system calls for epoll and AIO are de.ned in Figure 15. _ -- Block and wait \nfor an epoll event on a .le descriptor sys_epoll_wait fd event  -- Submit AIO read requests, returning \nthe number of bytes read sys_aio_read fd offset buffer data Trace = ... -- The corresponding nodes in \nthe trace: | SYS_EPOLL_WAIT FD EPOLL_EVENT Trace | SYS_AIO_READ FD Integer Buffer (Int -> Trace)  _ \nFigure 15. System calls for epoll and (read-only) AIO _ worker_epoll sched = do { -- wait for some epoll \nevents results <-epoll_wait; -- for each thread object in the results, -- write it to the ready queue \nof the scheduler mapM (writeChan (ready_queue sched)) results; worker_epoll sched; } -- recursively \ncalls itself and loop  _ Figure 16. Dedicated event loop for epoll These system calls are interpreted \nin the scheduler worker main. For each sys epoll wait call, the scheduler uses a library func\u00ad tion to \nregister an event with the OS epoll device. The registered event contains a reference to c, the child \nnode that is the continua\u00ad tion of the application thread. When the registered event is triggered, such \nevents are harvested by a separate event loop worker epoll shown in Figure 16. It uses a library function \nto wait for events and retrieve the traces associated with these events. Then, it put the traces into \nthe ready queue so that their corresponding monadic threads can be resumed. Under the hood, the library \nfunctions such as epoll wait are just wrappers for their corresponding C library functions imple\u00ad mented \nthrough the Haskell Foreign Function Interface (FFI). AIO is similarly implemented using a separate event \nloop. In our system, the programmer can easily add other asynchronous I/O mechanisms in the same way. \n 4.6 Supporting blocking I/O For some I/O operations, the OS may provide only synchronous, blocking \ninterfaces. Examples are: opening a .le, getting the at\u00adtributes of a .le, resolving a network address, \netc. If such opera\u00adtions are submitted using the SYS NBIO system call, the scheduler event loops will \nbe blocked. The solution is to use a separate OS thread pool for such op\u00aderations, as shown in Figure \n14. Similar to sys nbio,we de.ne another system call sys blio with a trace node SYS BLIO (where BLIO \nmeans blocking IO). Whenever the scheduler sees SYS BLIO, it sends the trace to a queue dedicated for \nblocking I/O requests. On the other side of the queue, a pool of OS threads are used, each run\u00adning an \nevent loop that repeatedly fetches and processes the block\u00ading I/O requests. 4.7 Thread synchronization \nThe execution of a monadic thread is interleaved with purely func\u00adtional computations and effectful computations \n(submitted through system calls). On SMP machines, the GHC runtime system trans\u00adparently manages the \nsynchronization of purely functional compu\u00adtations (e.g. concurrent thunk evaluation) in multiple OS \nthreads. For the synchronization of effectful computations, our implemen\u00adtation offers several options. \nFor nonblocking synchronization, such as concurrent accesses to shared data structures, the software \ntransactional memory [12] (STM) provided by GHC can be used directly. Monadic threads can simply use \nsys nbio to submit STM computations as IO operations. As long as the STM computations are nonblocking \n(i.e. without retry operations), the scheduler event loops can run them smoothly without being blocked. \nFor blocking synchronization, like the producer-consumer model, that affects thread scheduling, we can \nde.ne our own synchroniza\u00adtion primitives as system calls and implement them as scheduler extensions. \nFor example, mutexes can be implemented using a sys\u00adtem call sys mutex. A mutex is represented as a memory \nreference that points to a pair (l,q) where l indicates whether the mutex is locked, and q is a linked \nlist of thread traces blocking on this mutex. Locking a locked mutex adds the trace to the waiting queue \ninside the mutex; unlocking a mutex with a non-empty waiting queue dispatches the next available trace \nto the scheduler s ready queue. Other synchronization primitives such as MVars in Concur\u00adrent Haskell \n[14] can also be similarly implemented. Finally, because our system supports the highly-scalable epoll \ninterface in Linux, monadic threads can also communicate ef.\u00adciently using pipes provided by the OS. \n 4.8 Application-level network stack Our implementation includes an optional, application-level TCP stack. \nThe end-to-end design philosophy of TCP suggests that the protocol can be implemented inside the application, \nbut it is often dif.cult due to the event-driven nature of TCP. In our hybrid programming model, the \nability to combine events and threads makes it practical to implement transport protocols like TCP at \nthe application-level in an elegant and type-safe way. We implemented a generic TCP stack in Haskell, \nin which the details of thread scheduling and packet I/O are all made abstract. The TCP implementation \nis systematically derived from a HOL speci.cation [6]. Although the development is a manual process, \nthe purely functional programming style makes the translation from the HOL speci.cation to Haskell straightforward. \n    We then glued the generic TCP code into our event-driven system in a modular fashion. In Figure \n14, there are two event loops for TCP processing: worker tcp input receives packets from a kernel event \nqueue and process them; worker tcp timer processes TCP timer events. The system call sys tcp implements \nthe user interface of TCP. A library (written in the monadic thread language) hides the sys tcp call \nand provides the same high-level programming interfaces as standard socket operations.  5. Experiments \nThe main goal of our tests was to determine whether the Haskell implementation of the hybrid concurrency \nmodel could achieve acceptable performance for massively-concurrent network appli\u00adcations like web servers, \npeer-to-peer overlays and multiplayer games. Such applications are typically bound by network or disk \nI/O and often have many idle connections. In addition, we wanted to investigate the memory overheads \nof using Haskell and the con\u00adcurrency monad. The concurrency primitives are implemented us\u00ading higher-order \nfunctions and lazy data structures that we were concerned would impose too many levels of indirection \nand lead to inef.cient memory usage. Such programs allocate memory very frequently and garbage collection \nplays an important role. We used a number of benchmarks designed to assess the per\u00adformance of our Haskell \nimplementation. For I/O benchmarks we tested against comparable C programs using the Native POSIX Thread \nLibrary (NPTL), which is an ef.cient implementation of Linux kernel threads. We chose NPTL because it \nis readily avail\u00adable in today s Linux distributions, and many user-level thread im\u00adplementations and \nevent-driven systems also use NPTL as a de\u00adfacto reference of performance. Software setup: The experiments \nused Linux (kernel version 2.6.15) and GHC 6.5, which supports SMP and software trans\u00adactional memory. \nThe C versions of our benchmarks con.gured NPTL so that the stack size of each thread is limited to 32KB. \nThis limitation allows NPTL to scale up to 16K threads in our tests. 5.1 Benchmarks Memory consumption:1 \nIn our application-level scheduler, each thread is represented using a trace and an exception stack. \nThe trace is an unevaluated thunk implemented as function clo\u00adsures, and the exception stack is a linked \nlist. All the thread-local state is encapsulated in these memory objects. To measure the minimal amount \nof memory needed to repre\u00adsent such a thread, we wrote a test program that launches ten mil\u00adlion threads \nthat just loop calling sys yield.Using the pro.l\u00ading information from the garbage collector, we found \nthat the live set of memory objects is as small as 480MB after major garbage collections each thread \ncosts only 48 bytes in this test. Of course, ten million threads are impractical in a real system. The \npoint of this benchmark is that the representation of a monadic thread is so lightweight it is never \na bottleneck of the system. The memory scalability of our system is like most event-driven systems; it \nis only limited by raw resources such as I/O buffers, .le descriptors, sockets and application-speci.c, \nper-client states. Disk performance:2 Our event-driven system uses the Linux asynchronous I/O library, \nso it bene.ts from the kernel disk head 1 The memory consumption test used a machine with dual Xeon processors \nand 2GB RAM. We set the GHC s suggested heap size for garbage collec\u00adtion to be 1GB. 2 The disk and FIFO \nI/O benchmarks were run on a single-processor Celeron 1.2GHz machine with a 32KB L1 cache, a 256KB L2 \ncache, 512MB RAM and a 7200RPM, 80GB EIDE disk with 8MB buffer. We set GHC s suggested heap size for \ngarbage collection to be 100MB.     10 100 1000 10000 100000 1 10 100   Number of working threads \nNumber of concurrent connections 100 1000 10000 100000 benchmarks, we are not trying to prove that \nour system has abso\u00ad lutely better performance. The goal is to demonstrate that the hy\u00adbrid programming \nmodel implemented in Haskell can deliver prac\u00adtical performance that is comparable to other systems: \na good pro\u00adgramming interface can be more important than a few percent of performance!  5.2 Case study: \nA simple web server To test our approach on a more realistic application, we imple\u00admented a simple web \nserver for static web pages using our thread library. We reused some HTTP parsing and manipulation mod\u00adules \nfrom the Haskell Web Server project [17], so the main server consists of only 370 lines of code using \nmonadic threads. To take advantage of Linux AIO, the web server implements its own caching. I/O errors \nare handled gracefully using exceptions. Not only is the multithreaded programming style natural and \nelegant, but the event-driven architecture also makes the scheduler clean. The scheduler, including the \nCPS monad, system call implemen\u00adtations, event loops and queues for AIO, epoll, mutexes, blocking I/O \nand exception handling (but not counting the wrapper inter\u00adfaces for C library functions), is only 220 \nlines of well-structured code. The scheduler is designed to be customized and tuned: the programmer can \neasily add more system I/O interfaces or imple\u00adment application-speci.c scheduling algorithms to improve \nper\u00adformance. The web server and the I/O scheduler are completely type-safe: debugging is made much easier \nbecause many low-level programming errors are rejected at compile-time. Figure 19 compares our simple \nweb server to Apache 2.0.55 for a disk-intensive load. We used the default Apache con.guration on Debian \nLinux except that we increased the limit for concurrent con\u00adnections. Using our system, we implemented \na multithreaded client load generator in which each client thread repeatedly requests a .le chosen at \nrandom from among 128K possible .les available on the server; each .le is 16KB in size. The server ran \non the same ma\u00adchine used for the IO benchmarks, and the client machine com\u00admunicated with the server \nusing a 100Mbps Ethernet connection. Our web server used a .xed cache size of 100MB. Before each trial \nrun we .ushed the Linux kernel disk cache entirely and pre\u00adloaded the directory cache into memory. The \n.gure plots the over\u00adall throughput as a function of the number of client connections. On both servers, \nCPU utilization .uctuates between 70% and 85% (which is mostly system time) when 1,024 concurrent connections \nare used. Our simple web server compares favorably to Apache on this disk-bound workload. For mostly-cached \nworkloads (not shown in the .gure), the performance of our web server is also similar to Apache. A future \nwork is to test our web server using more realistic workloads and scheduling algorithm just as the kernel \nthreads and other event\u00addriven systems do. We ran the benchmark used to assess Capric\u00adcio [26]: each \nthread randomly reads a 4KB block from a 1GB .le opened using O DIRECT without caching. Each test reads \na total of 512MB data and the overall throughput is measured, averaged over 5 runs. Figure 17 compares \nthe performance of our thread li\u00adbrary with NPTL. This test is disk-bound: the CPU utilization is 1% \nfor both programs when 16K threads are used. Our thread li\u00adbrary slightly outperforms NPTL when more \nthan 100 threads are used. The throughput of our thread library remains steady up to 64K threads it performs \njust like the ideal event-driven system. FIFO pipe performance mostly idle threads (IO): Our event-driven \nscheduler uses the Linux epoll interface for network I/O. To test its scalability, we wrote a multithreaded \nprogram to simulate network server applications where most connections are idle. The program uses 128 \npairs of active threads to send and re\u00adceive data over FIFO pipes. In each pair, one thread sends 32KB \ndata to the other thread, receives 32KB data from the other thread and repeats this conversation. The \nbuffer size of each FIFO pipe is 4KB. In addition to these 256 working threads, there are many idle threads \nin the program waiting for epoll events on idle FIFO pipes. Each run transfers a total amount of 64GB \ndata. The average throughput of 5 runs is used. Figure 18 shows the overall FIFO pipe throughput as the \nnumber of idle threads changes. This test is bound by CPU and memory performance. Both NPTL and our Haskell \nthreads demonstrated good scalability in this test, but the throughput of Haskell is 30% higher than \nNPTL. In all the I/O tests in Figures 17 and 18, garbage collection takes less than 0.2% of the total \nprogram execution time. implement more advanced scheduling algorithms, such as resource aware scheduling \nused in Capriccio [26]. The web server also works with our application-level TCP stack implementation. \nBy editing one line of code in the web server, the programmer can choose between the standard socket \nlibrary and the customized TCP library. Because the protocol stack is now part of the application, we \nare able to tailor and optimize the TCP stack to the server s speci.c requirements. For example, urgent \npointers and active connection setup are not needed. We can implement server-speci.c algorithms directly \nin the TCP stack to .ght against DDoS attacks. Furthermore, our TCP stack is a zero\u00adcopy implementation3; \nit uses IO vectors to represent data buffers indirectly. The combination of packet-driven I/O and disk \nAIO provides a framework for application-level implementation of high\u00adperformance servers.  6. Related \nwork and discussion 6.1 Language-based concurrency We are not the .rst to address concurrency problems \nby us\u00ading language-based techniques. There are languages speci.cally designed for concurrent programming, \nsuch as Concurrent ML (CML)[22] and Erlang [4], or for event-driven programming such as Esterel [5]. \nJava and C# also provide some support for threads and synchronization. Most of these approaches pick \neither the mul\u00adtithreaded or event model. Of the ones mentioned above, CML is closest to our work because \nit provides very lightweight threads and event primitives for constructing new synchronization mecha\u00adnisms, \nbut its thread scheduler is still hidden from the programmer. There are also domain-speci.c languages, \nsuch as Flux [7], in\u00adtended for building network services by composing existing C or Java libraries. \nRather than supporting lightweight threads directly in the lan\u00adguage, there is also a large body of work \non using language-level continuations to implement concurrency features [23, 9]. Our work uses a similar \napproach, except that we do not use language-level continuations the CPS monad and lazy data structures \nare suf.\u00adcient for our purpose. It is worth noting that this paper only focuses on the domain of massively-concurrent \nnetwork programming. Similar problems have also been studied in the domain of programming graphical user \ninterfaces some time ago, and prior work [10, 21] showed dif\u00adferent approaches to combine threads and \nevents by using explicit message passing with local event handlers. 6.2 Event-driven systems and user-level \nthreads The application-level thread library is motivated by two projects: SEDA [28] and Capriccio [26]. \nOur goal is to get the best parts from both projects: the event-driven architecture of SEDA and the multithreaded \nprogramming style of Capriccio. Capriccio uses compiler transformations to implement linked stack frames; \nour application-level threads uses .rst-class closures to achieve the same effect. Besides SEDA [28], \nthere are other high-performance, event\u00addriven web servers, such as Flash [20]. Larus and Parkes showed \nthat event-driven systems can bene.t from batching similar opera\u00adtions in different requests to improve \ndata and code locality [15]. However, for complex applications, the problem of representing control .ow \nwith events becomes challenging. There are libraries and tools designed to make event-driven programs \neasier by struc\u00adturing code in CPS, such as Python s Twisted package [24] and 3 Our current implementation \nuses iptables queues to read packets, so there is still unnecessary copying of incoming packets. However, \nthis is not a fundamental limit an asynchronous packet I/O interface would allow us to implement true \nzero-copying. C++ s Adaptive Communication Environment (ACE) [1]. Adya et al. [2] present a hybrid approach \nto automate stack management in C and C++ programming. Multiprocessor support for user-level threads \nis a challenging problem. Event-driven systems, in contrast, can more readily take advantage of multiple \nprocessors by processing independent events concurrently [29]. A key challenge is how to determine whether \ntwo pieces of code might interfere: our thread scheduler bene.ts from the strong type system of Haskell \nand the use of software transactional memory. 6.3 Type-safe construction of the software stack Our work \nis orthogonal, but in some sense similar to the Singular\u00adity [13] project, which constructs an operating \nsystem using type\u00adsafe languages. Singularity uses language-based abstractions to iso\u00adlate processes \nand eliminate overheads of hardware-enforced pro\u00adtection domains; we use language-based abstractions \nto isolate soft\u00adware components and eliminate overheads of OS abstractions. Sin\u00adgularity reconstructs \nthe traditional software stack from bottom up; our approach attempts to do so from top down. In our hybrid \nprogramming model, many software components that are traditionally implemented in the OS, such as threads, \nIO libraries and network stacks, can be modularly lifted into the appli\u00adcation and share the same programming \nenvironment. In fact, the event-driven system described in Figure 14 looks quite like a small operating \nsystem itself. This application-level approach has many advantages for building high-performance network \nservices: the in\u00adterfaces among these components are completely type-safe; each component can be customized \nto the application s needs; compil\u00ading them together also opens opportunities for optimizations across \ncomponents.  7. Conclusion Events and threads should be combined into a hybrid programming model in \ngeneral-purpose programming languages. With proper language support, application-level concurrency primitives \ncan be made extremely lightweight and easy to use. Our experiments demonstrate that this approach is \npractical and our programming experience suggests that this is a very appealing way of writing scalable, \nmassively concurrent systems software.  Acknowledgements We would like to thank all the members of \nthe PLClub at the University of Pennsylvania, Milo M.K. Martin, Yun Mao, Simon Peyton Jones and Simon \nMarlow for their help and feedbacks on this project. In addition, we would like to thank the PLDI reviewers \nfor their valuable comments and extensive proofreading of the original draft. This work is supported \nby NSF grant CCF-0541040. References [1] The ADAPTIVE Communication Environment: Object-Oriented Network \nProgramming Components for Developing Client/Server Applications. 11th and 12th Sun Users Group Conference, \nDecember 1993 and June 1994. [2] Atul Adya, Jon Howell, Marvin Theimer, William J. Bolosky, and John \nR. Douceur. Cooperative Task Management without Manual Stack Management. In Proceedings of the 2002 Usenix \nAnnual Technical Conference, 2002. [3] Andrew Appel. Compiling with Continuations. Cambridge University \nPress, 1992. [4] Joe Armstrong, Robert Virding, Claes Wikstr\u00a8om, and Mike Williams. Concurrent Programming \nin Erlang, Second Edition. Prentice-Hall, 1996. [5] Gerard Berry and Georges Gonthier. The Esterel Synchronous \nProgramming Language: Design, Semantics, Implementation. Science of Computer Programming, 19(2):87 152, \n1992. [6] Steve Bishop, Matthew Fairbairn, Michael Norrish, Peter Sewell, Michael Smith, and Keith Wansbrough. \nRigorous speci.cation and conformance testing techniques for network protocols, as applied to TCP, UDP, \nand sockets. In SIGCOMM 05: Proceedings of the 2005 conference on Applications, technologies, architectures, \nand protocols for computer communications, pages 265 276, New York, NY, USA, 2005. ACM Press. [7] Brendan \nBurns, Kevin Grimaldi, Alexander Kostadinov, Emery D. Berger, and Mark D. Corner. Flux: A language for \nprogramming high\u00adperformance servers. In 2006 USENIX Annual Technical Conference, pages 129 142, June \n2006. [8] Koen Claessen. A Poor Man s Concurrency Monad. Journal of Functional Programming, 9(3):313 \n323, 1999. [9] Kathleen Fisher and John Reppy. Compiler Support for Lightweight Concurrency. Technical \nmemorandum, Bell Labs, March 2002. [10] Emden R. Gansner and John H. Reppy. A Multi-threaded Higher\u00adorder \nUser Interface Toolkit. In User Interface Software, Bass and Dewan (Eds.), volume 1 of Software Trends, \npages 61 80. John Wiley &#38; Sons, 1993. [11] The Glasgow Haskell Compiler. http://www.haskell.org/ghc. \n[12] Tim Harris, Maurice Herlihy, Simon Marlow, and Simon Peyton-Jones. Composable Memory Transactions. \nIn Proceedings of the ACM Symposium on Principles and Practice of Parallel Programming, June 2005. [13] \nGalen C. Hunt, James R. Larus, Martin Abadi, Mark Aiken, Paul Barham, Manuel Fahndrich, Chris Hawblitzel, \nOrion Hodson, Steven Levi, Nick Murphy, Bjarne Steensgaard, David Tarditi, Ted Wobber, and Brian Zill. \nAn Overview of the Singularity Project. Technical Report MSR-TR-2005-135, Microsoft Research, Redmond, \nWA, USA, October 2005. [14] Simon Peyton Jones, Andrew Gordon, and Sigbjorn Finne. Con\u00adcurrent Haskell. \nIn POPL 96: The 23rd ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, pages 295 308, \nSt. Petersburg Beach, Florida, 1996. [15] James R. Larus and Michael Parkes. Using Cohort-Scheduling \nto Enhance Server Performance. In USENIX Annual Technical Conference, General Track, pages 103 114, 2002. \n[16] Hugh C. Lauer and Roger M. Needham. On the Duality of Operating Systems Structures. In Proceedings \nSecond International Symposium on Operating Systems. IRIA, October 1978. [17] Simon Marlow. Developing \na High-performance Web Server in Concurrent Haskell. Journal of Functional Programming, 12, 2002. [18] \nEugenio Moggi. Computational lambda-calculus and monads. In Proceedings of the Fourth Annual IEEE Symposium \non Logic in Computer Science, pages 14 23, Paci.c Grove, California, June 1989. IEEE. [19] John K. Outsterhout. \nWhy Threads Are A Bad Idea (for most purposes). In Presentation given at the 1996 Usenix Annual Technical \nConference, 1996. [20] Vivek S. Pai, Peter Druschel, and Willy Zwaenepoel. Flash: An ef.cient and portable \nWeb server. In Proceedings of the USENIX 1999 Annual Technical Conference, 1999. [21] Rob Pike. A Concurrent \nWindow System. Computing Systems, 2(2):133 153, Spring 1989. [22] John H. Reppy. CML: A Higher Concurrent \nLanguage. In PLDI 91: Proceedings of the ACM SIGPLAN 1991 Conference on Programming Language Design and \nImplementation, pages 293 305, New York, NY, USA, 1991. ACM Press. [23] Olin Shivers. Continuations and \nThreads: Expressing Machine Concurrency Directly in Advanced Languages. In Proceedings of the Second \nACM SIGPLAN Workshop on Continuations, 1997. [24] The Twisted Project. http://twistedmatrix.com/. [25] \nRob von Behren, Jeremy Condit, and Eric Brewer. Why Events Are A Bad Idea (for high-concurrency servers). \nIn Proceedings of the 10th Workshop on Hot Topics in Operating Systems (HotOS IX), May 2003. [26] Rob \nvon Behren, Jeremy Condit, Feng Zhou, George C. Necula, and Eric Brewer. Capriccio: Scalable threads \nfor internet services. In Proceedings of the Ninteenth Symposium on Operating System Principles (SOSP), \nOctober 2003. [27] Philip Wadler. Monads for Functional Programming. In Proceedings of the Marktoberdorf \nSummer School on Program Design Calculi, August 1992. [28] Matt Welsh, David E. Culler, and Eric A. Brewer. \nSEDA: An Architecture for Well-Conditioned, Scalable Internet Services. In Proceedings of the Symposium \non Operating System Principles (SOSP), 2001. [29] Nickolai Zeldovich, Alexander Yip, Frank Dabek, Robert \nMorris, David Mazi`eres, and Frans Kaashoek. Multiprocessor support for event\u00addriven programs. In Proceedings \nof the 2003 USENIX Annual Technical Conference (USENIX 03), San Antonio, Texas, June 2003.  \n\t\t\t", "proc_id": "1250734", "abstract": "<p>This paper proposes to combine two seemingly opposed programming models for building massively concurrent network services: the event-driven model and the multithreaded model. The result is a hybrid design that offers the best of both worlds--the ease of use and expressiveness of threads and the flexibility and performance of events.</p> <p>This paper shows how the hybrid model can be implemented entirely at the application level using <i>concurrency monads</i> in Haskell, which provides type-safe abstractions for both events and threads. This approach simplifies the development of massively concurrent software in a way that scales to real-world network services. The Haskell implementation supports exceptions, symmetrical multiprocessing, software transactional memory, asynchronous I/O mechanisms and application-level network protocol stacks. Experimental results demonstrate that this monad-based approach has good performance: the threads are extremely lightweight (scaling to ten million threads), and the I/O performance compares favorably to that of Linux NPTL. tens of thousands of simultaneous, mostly-idle client connections. Such massively-concurrent programs are difficult to implement, especially when other requirements, such as high performance and strong security, must also be met.</p>", "authors": [{"name": "Peng Li", "author_profile_id": "81100475426", "affiliation": "University of Pennsylvania, Philadelphia, PA", "person_id": "PP37028035", "email_address": "", "orcid_id": ""}, {"name": "Steve Zdancewic", "author_profile_id": "81384616728", "affiliation": "University of Pennsylvania, Philadelphia, PA", "person_id": "PP14144604", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1250734.1250756", "year": "2007", "article_id": "1250756", "conference": "PLDI", "title": "Combining events and threads for scalable network services implementation and evaluation of monadic, application-level concurrency primitives", "url": "http://dl.acm.org/citation.cfm?id=1250756"}