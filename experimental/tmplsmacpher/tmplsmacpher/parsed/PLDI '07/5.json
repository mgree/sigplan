{"article_publication_date": "06-10-2007", "fulltext": "\n ACerti.edType-Preserving Compilerfrom Lambda Calculusto Assembly Language * Adam Chlipala University \nof California, Berkeley adamc@cs.berkeley.edu Abstract We present a certi.ed compiler from the simply-typed \nlambda cal\u00adculus to assembly language. The compiler is certi.ed in the sense that it comes with a machine-checked \nproof of semantics preser\u00advation, performed with the Coq proof assistant. The compiler and the terms \nof its several intermediate languages are given depen\u00addent types that guarantee that only well-typed \nprograms are rep\u00adresentable. Thus, type preservation for each compiler pass fol\u00adlows without anysigni.cant \nproofs of the usual kind. Semantics preservation is proved based on denotational semantics assigned to \nthe intermediate languages.We demonstratehowworking witha type-preserving compiler enables type-directed \nproof search to dis\u00adcharge large parts of our proof obligations automatically. Categories and Subject \nDescriptors F.3.1[Logics and meanings of programs]: Mechanical veri.cation; D.2.4[Software Engineer\u00ading]: \nCorrectness proofs, formal methods, reliability; D.3.4[Pro\u00adgramming Languages]: Compilers General Terms \nLanguages,Veri.cation Keywords compiler veri.cation, interactive proof assistants, de\u00adpendent types, \ndenotational semantics 1. Introduction Compilers are some of the most complicated pieces of widely-used \nsoftware. Thisfact has unfortunate consequences, since almost all of the guarantees we would like our \nprograms to satisfy depend on the proper workings of our compilers. Therefore, proofs of compiler correctness \nare at the forefront of useful formal methods research, as results here stand to bene.t many users. Thus, \nit is not surprising that recently and historically there has been much interestinthisclassof problems.Thispaperisareportonourforay \ninto that area and the novel techniques that we developed in the process. One interesting compiler paradigm \nis type-directed compi\u00adlation, embodied in, for instance, the TIL Standard ML com\u00adpiler [TMC+96]. Where \ntraditional compilers use relatively type\u00ad * This researchwas supportedin partbya National Defense Science \nand Engineering Graduate Fellowship, as well as National Science Foundation grants CCF-0524784 and CCR-0326577. \nPermission to make digital or hard copies of all or part of this work for personal or classroom use is \ngranted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page.To copyotherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. PLDI \n07 June 11 13, 2007, San Diego, California, USA. Copyright c &#38;#169; 2007ACM 978-1-59593-633-2/07/0006... \n$5.00. impoverished intermediate languages, TIL employed typed inter\u00admediate languages such thatevery \nintermediate program hada typ\u00ading derivation witnessing its safety, up until the last few phases of compilation. \nThis type information can be used to drive important optimizations, including nearly tag-free garbage \ncollection, where the .nal binary comes with a table giving the type of each register orstackslotateach \nprogrampointwherethegarbage collectormay be called. As a result, there is no need to use anydynamic typing \nscheme for values in registers, such as tag bits or boxing. Most of the intricacies of TIL stemmed from \nruntime passing of type information to support polymorphism. In the work we present here, we instead \npick the modest starting point of simply\u00adtyped lambda calculus,butwiththe samegoal:wewanttocompile the \nprograms of this calculus into an idealized assembly language that uses nearly tag-freegarbage collection.We \nwill achieve this result by a series of six type-directed translations, with their typed target languages \nmaintaining coarser and coarser grained types as we proceed. Most importantly, we prove semantics preservation \nfor each translation and compose these results into a machine\u00adchecked correctness proof for the compiler. \nTo our knowledge, there exists no other computer formalization of such a proof for a type-preserving \ncompiler. At this point, the reader may be dubious about just howinvolved a study of simply-typed lambda \ncalculus can be.We hope that the exposition in the remainder of this paper will justify the interest \nof the domain. Perhaps the key dif.culty in our undertaking has been effective handling of variable binding. \nThe POPLmark Chal\u00adlenge [ABF+05] is a benchmark initiative for computer formaliza\u00adtion of programming \nlanguages whose results have highlighted just that issue. In the manydiscussions it has generated, there \nhas yet to emerge a clear winner among basic techniques for representing language constructs that bind \nvariables in local scopes. Each of the proposed techniques leads to some signi.cant amount of overhead \nnot present in traditional proofs. Moreover, the initial benchmark problem doesn t involve any component \nof relational reasoning, crucial for compiler correctness.Webegan thiswork as aninves\u00adtigation into POPLmark-style \nissues in the richer domain of rela\u00adtional reasoning. 1.1 Task Description The source language of our \ncompiler is thefamiliar simply-typed lambda calculus, whose syntax is: Types t ::= N | t . t Natural \nnumbers n Variables x, y, z Terms e ::= n | x | e e | .x : t. e Application, the third production for \ne, associates to the left, as usual. We will elaborate shortly on the language s semantics. Our target \nlanguage is an idealized assembly language, with in\u00ad.nitely many registers and memory cells, each of \nwhich holds un\u00adbounded natural numbers.Wemodel interaction witharuntime sys\u00adtem through special instructions. \nThe syntax of the target language is: Registers r Operands o ::= r | n | new (rr, rr)| read (r, n) Instructions \ni ::= r := o; i | jump r Programs p ::= (ri, i) As instruction operands, we have registers, constant \nnaturals, and allocation and reading of heap-allocated records. The new operand takes two arguments: \na list of root registers and a list of registers holding the .eld values for the object to allocate. \nThe semantics of the target language are such that new is allowed to performgarbage collections at will, \nand the root list is the usual parameter required for soundgarbage collection. Infact, we will let new \nrearrange memory however it pleases, as long as the result is indistinguishable from having simply allocated \na new record, from the point of view of the speci.ed roots. The read instruction determines which runtime \nsystem-speci.c procedure to use to read a given .eld of a record. A program is a list of basic blocks \nplus an initial basic block, where execution begins. A basic block is a sequence of register assignments \nfollowed by an indirect jump. The basic blocks are indexed in order by the natural numbers for the purposes \nof these jumps. We will additionally consider that a jump to the value 0 indicates that the program should \nhalt, and we distinguish one register that is said to hold the program s result at such points. We are \nnow ready to state informally the theorem whose proof is the goal of this work: THEOREM 1 (Informal statementof \ncompiler correctness). Given a term e of the simply-typed lambda calculus of type N, the compi\u00adlation \nof e is an assembly program that, when run, terminates with the sameresult as we obtainby running e. \nOur compiler itself is implemented entirely within the Coq proof assistant [BC04]. Coq s logic doubles \nas a functional pro\u00adgramming language. Through the program extraction process, the more exotic features \nof a development in this logic can be erased ina semantics-preservingway,leavingan OCaml programthat \ncan be compiled to ef.cient object code. In this way, we obtain a tradi\u00adtionalexecutableversionof our \ncompiler.We ignore issuesof pars\u00ading in this work, so our compiler must be composed with a parser. Assuming \na .ctitious machine that runs our idealized assembly language, the only remaining piece to be added is \na pretty-printer from our abstract syntax tree representation of assembly programs to whatever format \nthat machine requires. 1.2 Contributions We summarize the contributions of this work as follows: It \nincludes what is to our knowledge the .rst total correctness proof of an entire type-preserving compiler. \n It gives the proof using denotational semantics, in contrast to the typical use of operational semantics \nin related work.  The whole formal development is carried out in a completely rigorous way with the \nCoq proof assistant [BC04], yielding a proof that can be checked by a machine using a relatively small \ntrusted code base. Our approach is based on a general methodology for representing variable binding and \ndenotation functions in Coq.  We sketch a generic programming system that we have devel\u00adoped for automating \nconstruction of syntactic helper functions  over de Bruijn terms [dB72], as well as generic correctness \nproofs about these functions. In addition, we present the cata\u00adlogueofgeneric functionsthatwefoundsuf.cientforthiswork. \nFinally, we add to the list of pleasant consequences of mak\u00ading your compiler type-directed or type-preserving. \nIn particu\u00adlar, we show how dependently-typed formalizations of type\u00adpreserving compilers admit particularly \neffective automated proof methods, driven by type information.  1.3 Outline In the next section, we \npresent our compiler s intermediate lan\u00adguages and elaborate on the semantics of the source and target \nlan\u00adguages.Following that, we run through the basic elements of im\u00adplementinga compiler pass, noting \nchallenges that arisein the pro\u00adcess. Each of the next sections addresses one of these challenges and \nour solution to it. We discuss how to mechanize typed pro\u00adgramming languages and transformations over \nthem, how to use generic programming to simplify programming with dependently\u00adtyped abstract syntax trees, \nand how to apply automated theorem proving to broad classes of proof obligations arising in certi.ca\u00adtion \nof type-preserving compilers. After this broaddiscussion, we spend some time discussing interesting features \nof particular parts of our formalization. Finally, we provide some statistics on our im\u00adplementation, \ndiscuss related work, and conclude. 2. The Languages In this section, we present our source, intermediate, \nand target languages, along with their static and dynamic semantics. Our language progression is reminiscent \nof that from the paper From SystemF toTyped Assembly Language [MWCG99]. The main differences stem from \nthe fact that we are interested in meaning preservation, not just type safety. This distinction makes \nour task both harder and easier. Naturally,semantics preservation proofs are more dif.cult than type \npreservation proofs.However, thefact that we construct such a detailed understanding of program behavior \nallows us to retain less type information in later stages of the compiler. The mechanics of formalizing \nthese languages in Coq is the sub\u00adjectoflater sections.Wewillsticktoa pencilandpaper formaliza\u00adtion levelof \ndetailinthis section.Wetrytouse standard notations wherever possible. The reader can rest assured that \nanything that seems ambiguous in the presentation here is clari.ed inthe mech\u00adanization. 2.1 Source The \nsyntax of our source language (called Source hereafter) was already given in Section 1.1. The type system \nis the standard one for simply-typed lambda calculus, with judgments of the form G f e : t which mean \nthat, with respect to the type assignment to free variables provided by G, term e has type t . Following \nusual conventions, we require that, for any typing judgment that may be stated, let alone veri.ed to \nhold, all free and bound variables are distinct, and all free variables of the term appear in the context. \nIn the implementation, the .rst condition is discharged by using de Bruijn indices, and the second condition \nis coveredby the dependent typing rules we will assign to terms. Nextweneedtogive Sourceadynamic semantics.Forthisand \nall our other languages, we opted to use denotational semantics. The utility of this choice for compiler \ncorrectness proofs has been understood for a while, as reifying a program phrase s meaning as a mathematical \nobject makes it easy to transplant that meaning to new contexts in modeling code transformations. Our \nsemantics for Source is: [ t] : types . sets [ N] = N [ t1 . t2] = [ t1] . [ t2]  [[G] : contexts . \nsets [ \u00b7] = unit [[G,x : t] = [ G]]\u00d7[ t] [ e] : [G f e : t ] . [[G] . [ t] [ n] s = n [ x] s = s(x) \n[ e1 e2] s =[ e1] s [ e2] s [ .x : t. e] s = .x :[ t] . [ e]](s, x) Thetypeoftheterm denotation function(usedinexpressionsof \nthe form [ e] )indicates that it is a function whose domain istyping derivations for terms and whose \nrange depends on the particular G and t that appear in that derivation. As a result, we de.ne denotations \nonly for well-typed terms. Every syntactic class is being compiled into a single meta lan\u00adguage, Coq \ns Calculus of Inductive Constructions (CIC) [BC04]. However, we will not expect anyspecial knowledge \nof this formal system from the reader.Various other type theories could be sub\u00adstituted for CIC, and \nstandard set theory would do just as well for this informal presentation. We .rst give each Source type \na meaning by a recursive trans\u00adlation into sets. Note that, throughout this paper, we overload con\u00adstructs \nlike the function type constructor . that are found in both our object languages and the meta language \nCIC, in an effort to save the reader from a deluge of different arrows. The variety of arrow in question \nshould always be clear from context.With this convention in mind, the type translation for Source is \nentirely un\u00adsurprising. N denotes the mathematical set of natural numbers, as usual. We give a particular \ntype theoretical interpretation of typing contexts as tuples, and we then interpret a term that has type \nt in context G asafunctionfromthe denotationof G into the denotation of t . The translation hereisagain \nentirely standard.For the sakeof conciseness, we allow ourselves to be a little sloppywith notations \nlike s(x), which denotes the proper projection from the tuple s, corresponding to the position x occupies \nin G.We note that we use the meta language slambda binder to encode the object language s lambda binder \nin a natural way, in an example closely related to higher-order abstract syntax [PE88]. 2.2 Linear Our \n.rstcompilation stepisto convert Source programs intoaform that makes execution order explicit. This \nkind of translation is asso\u00adciated with continuation-passing style (CPS), and the composition of the \n.rst two translations accomplishes a transformation to CPS. The result of the .rst translation is the \nLinear language, and we now give its syntax. It inherits the type language of Source, though we interpret \nthe types differently. Operands o ::= n | x | .x : t. e Terms e ::= let x = o in e | throw (o)| xyz Linear \nterms are linearized in the sense that theyare sequences of binders of primitive operands to variables, \nfollowed by either a throw to the current continuation or a function call. The function call form shows \nthat functions taketwoarguments: .rst, the normal argument; and second, a function to call with the result. \nThe func\u00adtion and both its arguments must be variables, perhaps bound with earlier lets. For reasons \nof space, we omit the standard typing rules for Lin\u00adear and proceed to its denotational semantics. Recall, \nhowever, that the denotation functions are only de.ned over well-typed terms. [ t] : types . sets [ N] \n= N [ t1 . t2] =[ t1] . ([[t2] . N) . N [ o] : [G f o : t] . [[G] . [ t] [ n] s = n [ x] s = s(x) [ .x \n: t. e] s = .x :[ t] . [ e]](s, x) [ e] : [G f o : t] . [[G] . ([[t] . N) . N [ let x = o in e] s =[ \ne]](s,[ o] s) [ throw (o)] s = .k. k([[o] s) [ xyz] s = .k. s(x)(s(y)) (.v. k(s(z)(v))) We choose the \nnatural numbers as the result type of programs. Functions are interpreted as accepting continuations \nthat return naturalswhengivenavalueofthe range type.The let case relies on the implicitfact that the \nnewly-boundvariable x falls at the end of the proper typing context for the body e. The most interesting \ncase is the last one listed, that for function calls.We call the provided functionwithanew continuation \ncreatedby composingthe current continuation with the continuation supplied through the variable z. We \nuse Linear as our .rst intermediate language instead of going directly to standard CPS because the presence \nof distin\u00adguished throw terms makes it easier to optimize term representa\u00adtion by splicing terms together. \nThis separation is related to the issue of administrative redexes in standard two-phase CPS trans\u00adforms \n[Plo75]. 2.3 CPS The next transformation .nishes the job of translating Linear into genuine CPS form, \nand we call this next target language CPS. Types t ::= Nat | rt . N Operands o ::= n | x | .rx : rt. \ne Terms e ::= let x = o in e | x ry Compared to Linear, the main differences we see are that func\u00adtions \nmay now take multiple arguments and that we have collapsed throw and function call into a single construct. \nIn our intended use of CPS, functions will either correspond to source-level functions and take two arguments, \nor they will correspond to continuations and take single arguments. Our type language has changed to \nre\u00ad.ect that functions no longer return, but rather they lead to .nal natural number results. We omit \ndiscussion of the semantics of CPS, since the changes from Linear are quite incremental. 2.4 CC The \nnext thing the compiler does is closure convert CPS programs, hoisting all function de.nitions to the \ntop level and changing those functions to take records of their free variables as additional argu\u00adments.Wecallthe \nresult languageCC,andhereisits syntax. N Types t ::= N | rt . N | rt | rt \u00d7 rt . N Operands o ::= n | \nx |(x, ry)| pix Terms e ::= let x = o in e | x ry Programs p ::= let x =(.ry : rt. e) in p | e N We have \ntwo new type constructors: rt is the multiple\u00adargumentproducttypeof recordswhose.eldshavethetypesgiven \nby rt. rt \u00d7 rt . N is the type of code pointers, specifying .rst the type of the closure environment \nexpected and second the types of the regular arguments. Among the operands, we no longer have an anonymous \nfunc\u00adtion form. Instead, we have (x, ry), which indicates the creation of a closure with code pointer \nx and environment elements ry. These environment elements are packaged atomically into a record, and \nthe receiving function accesses the record s elements with projec\u00adtion operand form pix. This operand \ndenotes the ith element of record x. While we have added a number of features, there are no sur\u00adprises \nencountered in adapting the previous semantics, so we will proceed to the next language. 2.5 Alloc The \nnext step in compilation is to make allocation of products and closures explicit. Since giving denotational \nsemantics to higher\u00adorder imperative programs is tricky, we decided to perform code .attening as part \nof the same transformation. That is, we move to .rst-order programs with .xed sets of numbered code blocks, \nand every functioncall referstooneofthese blocksby number.Hereis the syntax of this language, called \nAlloc. Types t ::= N | ref | rt . N Operands o ::= n | x |(n)| new (rx)| pix Terms e ::= let x = o in \ne | x ry Programs p ::= let (.ry : rt. e) in p | e The .rst thing to note is that this phase is the .rst \nin which we lose type information: we have a single type ref for all heap references, forgetting what \nwe know about record .eld types.To compensate for this loss of information, we will give Alloc pro\u00adgramsa \nsemanticsthatallowsthemtofailwhentheymake incor\u00adrect guesses about the types of heap cells. Our set of \noperands nowincludes both natural number constants n and code pointer constants (n).We also have a generic \nrecord allocation construct in place of the old closure constructor, and we retain projections from records. \nThis language is the .rst to take a signi.cant departure from its predecessors so far as dynamic semantics \nis concerned. The key difference is that Alloc admits non-terminating programs. While variable scoping \nprevented cycles of function calls in CC and ear\u00adlier,welosethat restrictionwiththemovetoa.rst-orderform.This \nkind of transformation is inevitable at some point, since our target assembly language has the same property. \nDomain theory provides one answer to the questions that arise in modeling non-terminating programs denotationally,butitis \ndif\u00ad.cult to use the classical work on domain theory in the setting of constructivetype theory.Oneveryeffectivealternativecomesinthe \nform of the co-inductive types [Gim95] that Coq supports.Agen\u00aderal knowledge of this class of types is \nnot needed to understand what follows. We will con.ne our attention to one co-inductive type, a sort \nof possibly-in.nite streams. We de.ne this type with the in.nite closure of this grammar: Traces T ::= \nn |.| *, T In other words, a trace is either an in.nite sequence of stars or a .nite sequenceof stars \nfollowedbya natural number ora bottom value.The.rstofthese possibilitieswillbethe denotationofanon\u00adterminating \nprogram, the second will denoteaprogram returning an answer, and the third will denote a program that \ncrashes. Now we are ready to start modeling the semantics of Alloc. First, we need to .x a representation \nof the heap. We chose an abstract representation that identi.es heaps with lists of lists of tagged .elds, \nstanding for the set of .nite-size records currently allocated. Each .eld consists of a tag, telling \nwhether or not it is a pointer; and a data component. As we move to lower languages, these abstract heaps \nwill be mapped into more conventional .at, untyped heaps.We now de.ne some domains to represent heaps, \nalong with a function for determining the proper tag for a type: C = {Traced, Untraced}\u00d7 N M = list (list \nC) tagof(N)= Untraced tagof(ref)= Traced tagof(rt . N)= Untraced Next wegivea semantics to operands.We \nmake two different choices here than we did before. First, we allow execution of operands to fail when \na projection reads a value from the heap and .nds that it has the wrong tag. Second, the denotations \nof operands must be heap transformers, taking the heap as an extra argumentand returninganew oneto re.ectanychanges.We \nalso set the convention that program counter0denotes the distinguished top-level continuation of the \nprogram that, when called, halts the program with its .rst argument as the result. Any other program \ncounter n +1 denotes the nth function de.ned in the program. [ o] : [G f o : t ] . [[G] . M . (M\u00d7 N) \n. {.}[ n] sm =(m, n) [ x] sm =(m, s(x)) [ (n)] sm =(m, n + 1) [ new (rx)] sm =(m . [s(rx)], |m|) [ pix] \nsm = if ms(x),i =(tagof(t),v), then:(m, v) else:. We write. to indicate list concatenation, and the notation \ns(rx) to denote looking up in s the value of each variable in rx, forming a list of results where each \nis tagged appropriately based on the type of the source variable. The new case returns the length of \nthe heap because we represent heaprecord addresses with their zero-based positions in the heap list. \nTerms are more complicated. While one might think of terms as potentially non-terminating, we takea different \napproach here. The denotationofevery termisa terminating program that returns the next function call \nthat should be made, or signals an error with a . value. More precisely, a successful term execution \nreturns a tuple of a new heap, the program counter of the function to call, and a list of natural numbers \nthat are to be the actual arguments. [ e] : [G f e : t ] . [[G] . M . (M\u00d7 N\u00d7 list N) . {.} '' [ let x \n= o in e] sm = if[ o] sm =(m,v) then:[ e]](s, v)m else:. [ x ry] sm =(m, s(x),s(ry)) Finally, we come \nto the programs, where we put our trace domain to use. Since we have already converted to CPS, there \nis no needto consideranyaspectofa program s behaviorbutits result. Therefore, we interpret programs as \nfunctions from heaps to traces. We write:: for the binary operator that concatenatesa new element to \nthe head of a list, and we write rxpc for the lookup operation that extracts from the function de.nition \nlist rx the pcth element. [ p] : [G f let rx in e] . [[G] . M . Trace [ let rx in e] sm = if[ e] sm =(m \n' , 0,v :: rn) then:v else if [ e] sm =(m ' , pc +1, rn) then: if rxpc = .ry : rt. e ' and |yr| = |rn| \nthen: *,[ let rx in e ' ] rnm ' else:. else:. Though we have not provided details here, Coq s co-inductive \ntypes come with some quite stringent restrictions, designed to pre\u00advent unsound interactions with proofs. \nOur de.nition of program denotation is designed to satisfy those restrictions. The main idea here is \nthat functions de.ned as co-.xed points must be suf.\u00adciently productive ; for our trace example, a co-recursive \ncall may only be made after at least one token has already been added to the stream in the current call. \nThis restriction is the reason for in\u00adcluding the seemingly information-free stars in our de.nition of \ntraces. As we have succeeded in drafting a de.nition that satis.es this requirement,wearerewardedwitha \nfunctionthatisnotjustan arbitrary relational speci.cation,but rathera program that Coq is able to execute, \nresulting in a Haskell-style lazy list. 2.6 Flat We are now almost ready to move to assembly language. \nOur last stop before then isthe Flat language, which differs from assembly only in maintaining the abstract \nview of the heap as a list of tagged records.Wedoawaywithvariablesandmove insteadtoan in.nite bank of \nregisters. Function signatures are expressed as maps from natural numbers to types, and Flat programs \nare responsible for shiftingregisters aroundto compensateforthe removalofthebuilt\u00adin stack discipline \nthat variable binding provided. Types t ::= N | ref | . . N Typings .= N . t Registers r Operands o ::= \nn | r |(n)| new (r )| pir Terms e ::= r := o; e | jump r Programs p ::= let e in p | e In the style ofTyped \nAssembly Language, to each static point in a Flatprogram we associate a typing . expressing our expecta\u00adtions \nof register types whenever that point is reached dynamically. Itis crucial that wekeep this typing information,since \nwe will use itto creategarbage collector root tablesinthenextand .nal trans\u00adformation. Since there is \nno need to encode anyvariable binding for Flat, its denotational semanticsis entirely routine.Theonlyexceptionis \nthat we re-use the trace semantics from Alloc.  2.7 Asm Our idealized assembly language Asm was already \nintroduced in Section1.1.Wehavebynowalreadyprovidedthemain ingredients needed to give it a denotational \nsemantics. We re-use the trace\u00adbased approach from the last two languages. The difference we must account \nfor is the shift from abstract to concrete heaps, as well as thefact that Asmis parametricina runtime \nsystem. We should make it clear that we have not veri.ed anyruntime system or garbage collector, but \nonly stated conditions that they ought to satisfy and proved that those conditions imply the cor\u00adrectness \nof our compiler. Recent work on formal certi.cation of garbage collectors [MSLL07] gives us hope that \nthe task we have omitted is not insurmountable. In more detail, our formalization of Asm starts with \nthe de.ni\u00adtion of the domain Hof concrete heaps: H = N . N Aruntime system providesnew and read operations. \nThe read operation is simpler: read : H\u00d7 N\u00d7 N . N For a heap, a pointer to a heap-allocated record, and \na constant .eld offset within that record, read should return that .eld scurrent value. Runtime systems \nmay make different decisions on concrete layoutof records.For instance, there are severalwaysof including \ninformation on which .elds are pointers. Note that read is an arbitrary Coq function, not a sequence \nof assembly instructions, which should let us reason about many different runtime system design decisions \nwithin our framework. The new operation is more complicated, as it is designed to facilitategarbage collector \noperation. new : H\u00d7 list N\u00d7 list N . H\u00d7 list N\u00d7 N Its arguments are the current heap, the values to use \nto initialize the .elds of the record being allocated, and the values of all live registers holding pointer \nvalues. The idea is that, in the course of ful.lling the allocation request, the runtime system may rearrange \nmemory however it likes, so long as things afterward look the same as before to any type-safe program, \nfrom the perspective of the live registers. A return value of new gives the modi.ed heap, a fresh set \nof values for the pointer-holding registers (i.e., the garbage collectionroots,whichmayhavebeenmovedbyacopying \ncollector), and a pointer to the new record in the new heap. To state the logical conditions imposed \non runtime systems,we will need to make a de.nition based on the abstract heap model of earlier intermediate \nlanguages: DEFINITION 1 (Pointer isomorphism). Wesay that pointersp1,p2 . Nare isomorphic with respect \nto abstract heaps m1,m2 . Miff: 1. The p1th record of m1 and p2th record of m2 have the same number of \n.elds.  2. If the ith .eld of the p1threcordof m1 is tagged Untraced,then the ith .eld of the p2th recordof \nm2 is also tagged Untraced, and the two .elds have the same value.  3. If the ith .eld of the p1th recordof \nm1 is tagged Traced, then the ith .eld of the p2threcordof m2 is also tagged Traced, and the two .elds \ncontain isomorphic pointers.  The actual de.nitionis slightly moreinvolvedbutavoids this un\u00adquali.ed \nself-reference. Tofacilitatea parametric translation soundness proof,a candi\u00addate runtime system is required \nto provide a concretization func\u00adtion: . : M . H For an abstract heap m, .(m) is the concrete heap with \nwhich the runtime system s representation conventions associate it. We also overload . to stand for a \ndifferent function for concretizing pointers.Weabuse notationbyapplying . tovarious differentkinds of \nobjects, where it s clear how they ought to be converted from abstract to concrete in terms of the two \nprimary . translations; and, while some of these . functions really need to take the abstract heap as \nan additional argument, we omit it for brevity where the proper value is clear from context. The runtime \nsystem must come with proofs that its new and read operations satisfy the appropriate commutative diagrams \nwith these concretization functions. To give a speci.c example, we show the more complicated condition \nbetween the two operations, that for new. THEOREM 2 (Correctnessofanew implementation). For any ab\u00adstract \nheap m,taggedrecord.eld valuesrv (eachin C),andregister root set values r s, there exist new abstract \nheap m ', new root val\u00adues rs ', and new recordaddress a suchthat: r 1. new(.(m),.(rv),.(r s)) = (.(m \n' ),.(rsr'),.(a)) r and p ' are isomorphic with respect to m . [rv] and m ' . 2. For every pair of values \np and p ' in r s and rs ', respectively, p 3. |m| and a are isomorphic with respect to m . [rv] and \nm ' .  To understand the details of the last two conditions, recall that, in the abstract memory model, \nwe allocate new records at the end of the heap, which is itself a list of records. The length of the \nheap before an allocation gives the proper address for the next record to be allocated. 3. Implementation \nStrategy Overview Now that we have sketched the basic structure of our compiler, we move to introducing \nthe ideas behind its implementation in the Coq proof assistant. In this section, we single out the .rst \ncompiler phase and walk through its implementation and proof at a high level, noting the fundamental \nchallenges that we encounter along the way. We summarize our solution to each challenge and then discuss \neach in more detail in a later section. Recall that our .rst phase is analogous to the .rst phase of \na two-phase CPS transform. We want to translate the Source lan\u00adguage to the Linear language. Of course, \nbefore we can begin writing the translation, we need to represent our languages! Thus, our .rst challenge \nis to choose a representation of each language using Coq types. Our solution here is based on de Bruijn \nindices and dependently-typed abstract syntax trees. Our use of dependent typing will ensure that representable \nterms are free of dangling variable references by encoding a term s free variable set in its type. Moreover, \nwe will combine typing derivations and terms, essentiallyrepresenting each termasitstypingderivation.Inthisway,only \nwell-typed terms are representable. Now we can begin writing the translation from Source to Lin\u00adear.We \nhave some choices about how to represent translations in Coq, but, with our previous language representation \nchoice, it is quite natural to represent translations as dependently-typed Coq functions. Coq s type \nsystem will ensure that, when a translation is fed a well-typed input program, it produces a well-typed \noutput program.We can use Coq s program extraction facility to produce OCaml versions of our translations \nby erasing their dependently\u00adtyped parts. This strategy is very appealing, and it is the one we have \ncho\u00adsen, but it is not without its inconveniences. Since we represent terms as their typing derivations, \nstandard operations like bringing a new, unused variable into scope are not no-ops like theyare with \nsome other binding representations. These variable operations turn out to correspond to standard theorems \nabout typing contexts.For instance, bringing an unused variable into scope corresponds to a weakening \nlemma. These syntactic functions are tedious to write for each new language, especially when dealing \nwith strong de\u00adpendent types. Moreover, their implementations have little to do with details of particular \nlanguages. As a result, we have been able to create a generic programming system that produces them au\u00adtomatically \nfor arbitrary languages satisfying certain criteria. We not only produce the functions automatically,but \nwe also produce proofs that theycommute with arbitrary compositional denotation functions in the appropriate, \nfunction-speci.c senses. Now assume that we have our translation implemented. Its type ensuresthatit \npreserves well-typedness,butwealsowanttoprove that it preserves meaning. What proof technique should \nwe use? The technique of logical relations [Plo73] is the standard for this sortoftask.We characterizethe \nrelationships between programen\u00adtities and their compilations using relations de.ned recursively on type \nstructure. Usual logical relations techniques for denotational semantics translateveryeffectivelyintoour \nsetting,andweareable to take good advantage of the expressiveness of our meta language CIC in enabling \nsuccinct de.nitions. With these relations in hand, we reach the point where most pencil-and-paper proofswould \nsay that the rest followsby routine inductions of appropriate kinds. Unfortunately, since we want to \nconvince the Coq proof checker, we will need to provide consider\u00adably more detail. Thereis no magicbullet \nfor automating proofsof this sophistication,but we did identify some techniques thatworked surprisingly \nwell for simplifying the proof burden. In particular, since our compiler preserves type information, \nwe were able to au\u00adtomate signi.cant portions of our proofs using type-based heuris\u00adtics.Thekeyinsightwasthe \npossibilityofusing greedy quanti.er instantiation, since the dependent types we use are so particular \nthat most logical quanti.ers have domains compatible with just a single subterm of a proof sequent. 4. \nRepresentingTyped Languages We begin our example by representing the target language of its transformation. \nThe .rst step is easy; we give a standard algebraic datatype de.nition of the type language shared by \nSource and Linear. type : set Nat : type Arrow : type . type . type Things get more interesting when \nwe go to de.ne our term languages. We want to use dependent types to ensure that only terms with object \nlanguage typing derivations are representable, so we make our classes of operands and terms indexed typefamilies \nwhose indices tells us their object language types. Coq sexpressive dependent type system admits simple \nimplementations of this idea. Its inductive types are a generalization of the generalized algebraic datatypes \n[She04] that have become popular recently. We represent variables with de Bruijn indices. In other words, \na variable is represented as a natural number counting how many binders outward in lexical scoping order \none must search to .nd its binder. Since we are using dependent typing, variables are more than just \nnatural numbers.We represent our de Bruijn contexts as lists of types, and variables are in effect constructive \nproofs that a particular type is found in a particular context. A generic type familyvar encapsulates \nthis functionality. We de.ne operands and terms as two mutually-inductive Coq types. Below, the standard \nnotation . is used for a dependent function type. The notation .x : t1,t2 denotes a function from t1 \nto t2, where the variable x is bound in the range type t2, indicating that the function s result type \nmay depend on the value of its argument.Weelide type annotations from . types where they are clear from \ncontext. lprimop : list type . type . set LConst : .G, N . lprimop G Nat LVar : .G, .t, var G t . lprimop \nG t LLam : .G, .t1, .t2, lterm (t1 :: G) t2 . lprimop G(Arrow t1 t2) Inductive sty : Set := | SNat : \nsty | SArrow : sty -> sty -> sty. Inductive sterm : list sty -> sty | SVar : forall G t, Var Gt ->sterm \nGt | SLam : forall G dom ran, sterm (dom :: G) ran -> sterm G (SArrow dom ran) | SApp : forall G dom \nran, entirety in Figure 1. Though Coq syntax may seem strange at .rst to many readers, the structure \nof these de.nitions really mirrors their informal counterparts exactly. These snippets are only meant \ntogivea.avoroftheproject.We describein Section10howto -> Set := obtain the complete project source code, \nfor those whowantamore in-depth treatment. 5. RepresentingTransformations We are able to write the linearization \ntransformation quite naturally using ourde Bruijn terms.We start this section witha less formal presentation \nof it. u sterm G (SArrow dom ran) We .rst de.ne an auxiliary operation that, given two e1 e2 -> sterm \nG dom linear terms e1 and e2 and a variable u free in e2, returns a new -> sterm G ran linear term equivalent \nto running e1 and throwing its result to e2 by | SConst : forall G, nat -> sterm G SNat. binding that \nresult to u. u u Fixpoint styDenote (t : sty) : Set := (let y = o in e1) e2 = let y = o in (e1 e2) \nmatch t with u (throw (o)) e = let u = o in e u | SNat => nat u | SArrow t1 t2 => styDenote t1 -> styDenote \nt2 (xyz) e = let f =(.v. let g =(.u. e) in zvg) end. in xyf Fixpoint stermDenote (G : list sty) (t : \nsty) Now we cangive the linearization translation itself. (e : sterm G t) {struct e} Lnj = throw (n) \n: Subst styDenote G -> styDenote t := match e in (sterm G t) Lxj = throw (x) (Le2j v return (Subst styDenote \nG -> styDenote t) with Le1 e2j = Le1j (let f =(.x. throw (x)) in uvf))) throw (.x : t. Lej) | SVar_ \n_v =>funs => L.x : t. ej VarDenote = vs |SLam_ __ e =>funs=> fun x => stermDenote e (SCons x s) |SApp_ \n__ e1e2=> fun s=> (stermDenote e1 s) (stermDenote e2 s) |SConst _n =>fun_ =>n This translation can be \nconverted rather directly into Coq recur\u00adsive function de.nitions. The catch comes as a result of our \nstrong dependent types for program syntax. The Coq type checker is not able to verify the type-correctness \nof some of the clauses above. For a simple example, let us focus on part of the linearization end. case \nfor applications. There we produce terms of the form Le1j u v ...). The term e2 originally occurred \nin some context G. However, here Le2j, the compilation of e2, is used in a context formed by adding an \nadditional variable u to G.We know that this transplantation is harmless and ought to be allowed, but \nthe Coq type checker .ags this section as a type error. We need to perform an explicit coercion that \nadjusts the de Bruijn indices in Le2j. The operation we want corresponds to a (Le2j Figure 1. Coq source \ncode of Source syntax and semantics lterm : list type . type . set LLet : .G, .t1, .t2, lprimop G t1 \nweakening lemma for our typing judgment: If G f e : t , then . lterm (t1 :: G) t2 . lterm G t2 LThrow \n: .G, .t, lprimop G t . lterm G t G,x : t ' f e : t when x is not free in e. Translating this description \ninto our formalization with inductivetypes fromthe last LApp : .G, .t1, .t2, .t3, var G(Arrow t1 t2) \n. var G t1 . var G(Arrow t2 t3) . lterm G t3  In Coq, all constants are de.ned in the same syntactic \nclass, as opposed to using separate type and term languages. Thus, for in\u00adstance, lprimop and LConst \nare de.ned at the same level, though the type of the former tells us that it is a type. Asanexampleofthe \nencoding,the identity term .x : N. throw (x)is represented as: LLam [] Nat Nat (LThrow [Nat] Nat (LVar \n[Nat] Nat First)) where First is a constructor of var indicating the lexically inner\u00admost variable. To \ngive an idea of the make-up of our real implementation, we will also provide some concrete Coq code snippets \nthroughout this article. The syntax and static and dynamic semantics of our source language are simple \nenough that we can show them here in their informal u section, we want a function: weakenFront : .G, \n.t, lterm G t . .t ' , lterm (t ' :: G) t It is possible to write a custom weakening function for lin\u00adearized \nterms, but nothing about the implementation is speci.c to our language. There is a generic recipe for \nbuilding weaken\u00ading functions, based only on an understanding of where variable binders occur.Tokeepour \ntranslationsfreeofsuch details,wehave optedto createageneric programming system thatbuilds these syn\u00adtactic \nhelper functions for us. It is the subject of the next section. When we insert these coercions where \nneeded, we have a direct translation of our informal compiler de.nition into Coq code. As\u00adsuming for \nnow that the coercion functions have already been gen\u00aderated,wecangiveasanexampleofareal implementationtheCoq \ncode for the main CPS translation, in Figure 2.We lack the space to describe the code in detail,but we \npoint out that the Next and First constructors are usedtobuildde Bruijnvariablesin unary form. compose \nis the name of the function corresponding to our operator. Lterm is a module of helper functions gen\u00ad \nFixpoint cps (G : list sty) (t : sty) (e : sterm G t) {struct e} : lterm G t := match e in (sterm G t) \nreturn (lterm G t) with | SVar _ _ v => LThrow (LVar v) | SConst _ n => LThrow (LConst _ n) | SLam _ \n_ _ e => LThrow (LLam (cps e )) |SApp_ __ e1e2=> compose (cps e1) (compose (Lterm.weakenFront _ (cps \ne2)) (LBind (LLam (LThrow (LVar First))) (LApply (Next (Next First)) (Next First) First))) end. Figure \n2. Coq source code of the main CPS translation erated automatically, and it includes the coercion weakenFront \ndescribed earlier. Let us, then, turn to a discussion of the generic programming system that produces \nit. 6. Generic Syntactic Functions A variety of these syntactic helper functions come up again and again \nin formalization of programming languages.We have iden\u00adti.ed a few primitive functions that seemed to \nneed per-language implementations, along withanumberof derived functions that can be implemented parametrically \nin the primitives. Our generic pro\u00adgramming system writes the primitive functions for the user and then \nautomatically instantiates the parametric derived functions. We present here a catalogue of the functions \nthat we found to be importantin this presentwork. All operateover some typefamily term parameterizedby \ntypes from an arbitrary language. The .rst primitive is a generalization of the weakening function from \nthe last section.We modify its speci.cationto allow insertion intoanypositionofa context,notjustthebeginning. \n. denotes list concatenation, and it and single-element concatenation :: are right associative at the \nsame precedence level. weaken : .G1, .G2, .t, term (G1 . G2) t . .t ' , term (G1 . t ' :: G2) t Next \nis elementwise permutation. We swap the order of two adjacent context elements. permute : .G1, .G2, \n.t1, .t2, .t, term (G1 . t1 :: t2 :: G2) t . term (G1 . t2 :: t1 :: G2) t We also want to calculate the \nfree variables of a term. Here we mean those that actually appear, not just those that are in scope. \nThis calculation and related support functions are critical toef.cient closure conversion for anylanguage.We \nwrite P(G) to denote the type of subsets of the bindings in context G. freeVars : .G, .t, term G t .P(G) \nUsing the notion of free variables, we can de.ne strengthening, which removes unused variable bindings \nfrom a context. This op\u00aderation is also central to closure conversion. An argument of type G1 . G2 denotes \na constructive proof that every binding in G1 is also in G2. strengthen : .G, .t, .e : term G t, .G ' \n, freeVars G te . G ' . term G ' t We have found these primitive operations to be suf.cient for easing \nthe burden of manipulating our higher-level intermediate languages. The derived operations that our system \ninstantiates are: adding multiple bindings to the middle of a context and adding multiple bindings to \nthe end of a context, based on weaken;and moving a binding from the front to the middle or from the middle \nto the front of a context and swapping two adjacent multi-binding sections of a context, derived from \npermute. 6.1 Generic Correctness Proofs Writing these boilerplate syntactic functions is less than half \nof the challenge when it comes to proving semantics preservation. The se\u00admantic properties of these functions \nare crucial to enabling correct\u00adness proofs for the transformations that use them.Wealso automate the \ngeneration of the correctness proofs, based on an observation about the classes of semantic properties \nwe .nd ourselves needing toverifyfor them.Thekeyinsightisthatweonly requirethateach function commute \nwith any compositional denotation function in a function-speci.c way. Anexample should illustrate this \nidea best.Take the caseof the weakenFront function for our Source language. Fix an arbitrary denotation \nfunction [ \u00b7] for source terms.We claim that the correct\u00adness of a well-written compiler will only depend \non the following property of weakenFront, written with informal notation:For any G, t, and e with G f \ne : t, we have for anyt ', substitution s for the variables of G, and value v of type[ t ' ] , that: \n[ weakenFront G t et ' ]](s, v) = [ e] s We shouldn t need to know many speci.cs about[ \u00b7] to deduce \nthis theorem. Infact, it turns out that all we need is the standard property used to judge suitability \nof denotational semantics, com\u00adpositionality. In particular, we require that there exist functions fconst, \nfvar, fapp, and flam such that: [ n] s = fconst(n) [ x] s = fvar(s(x)) [ e1 e2] s = fapp([[e1] s,[ e2] \ns) [ .x : t. e] s = flam(.x :[ t] . [ e]](s, x)) Our generic programming system introspects into user-supplied \ndenotation function de.nitions and extracts the functions that wit\u00adness their compositionality. Using \nthese functions, it performs auto\u00admatic proofs of generic theorems like the one above about weaken-Front. \nEvery other generated syntactic function has a similar cus\u00adtomized theorem statement and automatic strategy \nfor proving it for compositional denotations. Though space limits prevent us from providing more detail \nhere, we mention that our implementation is interesting in that the code and proof generation themselves \nare implemented almost entirely inside of Coq s programming language, using dependent types to ensure \ntheir soundness. We rely on the technique of re.ective proofs [Bou97] to enable Coq programs to manipulate \nother Coq programs in a type-safe manner, modulo some small proof hints provided from outside the logic. \n7. Representing Logical Relations We now turn our attention to formulating the correctness proofs of \ncompiler phases, again using the linearization phase as our exam\u00adple.Weareabletogiveaverysimplelogical \nrelationsargumentfor this phase, since our meta languageCICissuf.cientlyexpressiveto encode naturally \nthe features of both source and target languages. The correctness theorem that wewant looks something \nlike the fol\u00adlowing, for some appropriate type-indexed relation .For disam\u00adbiguation purposes, we write \n[ \u00b7] S for Source language denotations and [ \u00b7] L for Linear denotations. Fixpoint val_lr (t : sty) : \nstyDenote t -> ltyDenote t -> Prop := match t return (styDenote t -> ltyDenote t -> Prop) with | SNat \n=> fun n1 n2 => n1 = n2 | SArrow t1 t2 => fun f1 f2 => forall x1 x2, val_lr t1 x1 x2 -> forall (k : styDenote \nt2 -> nat), exists thrown, f2 x2 k = k thrown /\\ val_lr t2 (f1 x1) thrown end. Figure 3. Coq source code \nfor the .rst-stage CPS transform s logical relation THEOREM 3 (Correctnessof linearization). For Source \nterm e such that G f e : t , if we have valuations sS and sL for [[G] S ' and [[G] L, respectively, such \nthat for every x : t . G, we have sS sLS LsL (x) t, (x), then [ e] sS t [ Lej] . This relation for values \nturns out to satisfy our requirements: n1 N n2 = n1 = n2 f1 t1.t2 f2 = .x1 :[ t1] S , .x2 :[ t1] L ,x1 \nt1 x2 ..v :[ t2] L , .k :[ t2] L . N, f2 x2 k = kv . f1 x1 t2 v We have a standard logical relation de.ned \nby recursion on the structure of types. e1 t e2 means that values e1 of type [ t] S and e2 of type[ t] \nL are equivalent in a suitable sense. Numbers are equivalent if and only if they are equal. Source function \nf1 and linearized function f2 are equivalent if and only if for every pair of arguments x1 and x2 related \nat the domain type, there exists some value v such that f2 called with x2 and a continuation k always \nthrows v to k, and the result of applying f1 to x1 is equivalent to v at the range type. The suitability \nof particular logical relations to use in compiler phase correctness speci.cations is hard to judge individually.We \nknow we have made proper choices when we are able to compose all of our correctness results to form the \noverall compiler correct\u00adness theorem. It is probably also worth pointing out here that our denotational \nsemantics are not fully abstract, in the sense that our target domains allow more behavior than our object \nlanguages ought to.For instance, the functions k quanti.ed over by the func\u00adtion case of the de.nition \nare drawn from the full Coq function space, which includes all manner of complicated functions rely\u00ading \non inductive and co-inductive types. The acceptability of this choice for our application is borne out \nby our success in using this logical relation to prove a .nal theorem whose statement does not depend \non such quanti.cations. Once we are reconciled with that variety of caveat, we .nd that Coq provides \nquite a congenial platform for de.ning logi\u00adcal relations for denotational semantics. The de.nition can \nbe transcribed quite literally, as witnessed by Figure 3. The set of all logical propositions in Coq \nis just another type Prop, and so we may write recursive functions that return values in it. Contrast \nour options here with those associated with proof assistants like Twelf [PS99], for which formalization \nof logical relations has his\u00adtorically been challenging. 8. ProofAutomation Nowthat we havethe statement \nof our theorem, we need to produce a formal proof of it. In general, our proofs will require signi.cant \nmanual effort. As anyone who has worked on computerized proofs can tell you, time-saving automation machinery \nis extremely wel\u00adcome. In proving the correctness theorems for our compiler, we were surprised to .nd \nthat a very simple automation technique is very effective on large classes of proof goals that appear. \nThe ef\u00adfectiveness of this technique has everything to do with the combi\u00adnation of typed intermediate \nlanguages and our use of dependent types to represent their programs. As an example, consider this proof \nobligation that occurs in the correctness proof for linearization. Know: .sS , .sL,sS G sL ..v :[ t1 \n. t2] L , .k : [ t1 . t2] L . N,[ Le1j] Lk = kv .[ e1] S sS t1.t2 v sL Know: .sS , .sL,sS G sL ..v \n:[ t1] L , .k :[ t1] L .  LL S N,[ Le2j] sk = kv .[ e2] sS t1 v Must prove: .sS , .sL,sS G sL ..v :[ \nt2] L , .k : LLsLS [ t2] . N,[ Le1 e2j] k = kv .[ e1 e2] sS t2 v It is safe to simplify the goal by moving \nall of the universal quanti.ers and implications that begin it into our proof context as new bound variables \nand assumptions; this rearrangement cannot alter the provability of the goal. Beyond that, Coq s standard \nau\u00adtomation support is stuck. However, it turns out that we can do much better for goals like this one, \nbased on greedy quanti.er in-stantiation.Traditional automated theorem provers spend most of their intelligence \nin determining how to use universally-quanti.ed facts and prove existentially-quanti.ed facts. When quanti.ers \nrange over in.nite domains, manysuchtheorem-proving problems are both undecidable and dif.cult in practice. \nHowever,examiningourgoalabove,we noticethatithasavery interesting property: every quanti.er has a rich \ntype depending on some object language type. Moreover, for any of these types, ex\u00adactly one subterm of \nproof state (bound variables, assumptions, and goal) that has that type ever appears at any point during \nthe proving process! This observation makes instantiation of quanti\u00ad.ers extremely easy: instantiate \nanyuniversal assumption or exis\u00adtential goal with the .rst properly-typed proof state subterm that appears. \nFor instance, in the example above, we had just moved the vari\u00adables sS and sL and the assumption sS \nG sL into our proof con\u00adtext. That means that we should instantiate the initial s quanti.ers of the two \nassumptions with these variables and use modus ponens to access the conclusions of their implications, \nby way of our new assumption. This operation leaves both assumptions at existential quanti.ers, so we \neliminate these quanti.ers by adding fresh vari\u00adables and new assumptions about those variables. The \ntypes of the k quanti.ers that we reach now don tmatch anysubterms in scope, so we stop here. Now it \ns time for a round of rewriting, using rules added by the human usertoahint database.Weuseallofthe boilerplate \nsyntac\u00adtic function soundness theoremsthat we generated automatically as left-to-right rewrite rules, \napplying them in the goal until no further changes are possible. Using also a rewrite theorem expressing \nthe soundness of the u composition operation, this process simpli.es the goal to a form with a subterm \n[ Le1j] L s L(.x :[ t1 . t2] L . ...). This lambda term has the right type to use as an instantiation \nfor the universally-quanti.ed k in the .rst assumption, so, by our greedy heuristic, we make that instantiation.We \nperform safe proposi\u00adtional simpli.cations on the newly-exposed part of that assumption and continue. \nIterating this heuristic, we discharge the proof obligation com\u00adpletely, with no human intervention. \nOur very naive algorithm has succeeded in guessing all of the complicated continua\u00adtions needed for quanti.er \ninstantiations, simply by searching for (* State the lemma characterizing the effect of * the CPS d term \ncomposition operator. *) Lemma compose_sound : forall (G : list sty) (t:sty)(e: ltermGt) (t : sty) (e \n: lterm (t :: G) t ) s (k : _ -> result), ltermDenote (compose e e ) s k = ltermDenote lin s (fun x \n=> ltermDenote lin (SCons x s) k). induction e; (* We prove it by induction on * the structure of * the \nterm e. *) equation_tac. (* A generic rewriting * procedure handles * the resulting cases. *) Qed. (* \n...omitted code to add compose_sound to our * rewriting hint base... *) (* State the theorem characterizing \nsoundness of * the CPS translation. *) Theorem cps_sound : forall G t (e : sterm G t), exp_lr e (cps \ne). unfold exp_lr; (* Expand the definition of the * logical relation on * expressions. *) induction \ne; (* Proceed by induction on the * structure of the term e. *) lr_tac. (* Use the generic greedy  \n* instantiation procedure to * discharge the subgoals. *) Qed. Figure 4. Snippets of the Coq proof script \nfor the CPS correctness theorem properly-typed subtermsoftheproof state.Infact,this same heuris\u00adtic discharges \nall of the cases of the linearization soundness proof, once we ve stocked the rewrite database with the \nappropriate syn\u00adtactic simpli.cations beforehand. To prove this theorem, all the user needs to do is \nspecify which induction principle to use and run the heuristic on the resulting subgoals. We have found \nthis approach to be very effective on high-level typed languages in general. The human prover s job is \nto determine the useful syntactic properties with which to augment those proved generically, prove these \nnewproperties, and add them to the rewrite hint database.Withvery little additionaleffort,the main theorems \ncan then be discharged automatically. Unfortunately, our lower\u00adlevel intermediate languages keep less \nprecise type information, so greedy instantiation would make incorrect choices too often to be practical. \nOur experience here provides a new justi.cation in support of type-preserving compilation. Figure 4 shows \nexample Coq code for proving the CPS cor\u00adrectness theorem, with a few smallsimpli.cations made for space \nreasons. 9. Further Discussion Some other aspects of our formalization outside of the running example \nareworth mentioning.Wesummarize themin this section. 9.1 Logical Relationsfor Closure Conversion Formalizations \nof closure conversion and its correctness, especially those using operational semantics, often involve \nexistential types and other relatively complicated notions. In our formalization, we are able to usea \nsurprisingly simple logical relation to characterize closure conversion. It relates denotations from \nthe CPS and CC languages, indicated with superscripts P and C, respectively.We lift various de.nitions \nto vectors in the usual way. n1 N n2 = n1 = n2 f1 .t.N f2 = .rx1 :[ rt] P , .x2 :[ rt] C ,x1 .t x2 . \nf1 x1 t2 f2 x2 This relation is almost identical to the most basic logical relation for simply-typed \nlambda calculus! The secret is that our meta language CIC has native support for closures. That is, Coq \ns function spaces already incorporate the appropriate reduction rules to capture free variables, so we \ndon t need to mention this process explicitly in our relation. In more detail, the relevant denotations \nof CC types are: [ rt . N] =[ t1] \u00d7 ... \u00d7[ tn] . N 12 1122 [ rt\u00d7 rt. N] = ([[t1] \u00d7 ... \u00d7[ tn]]) . ([[t1] \n\u00d7 ... \u00d7[ tm]]) . N Recall that the second variety of type shown is the type of code pointers, with the \nadditional .rst list of parameters denoting the expected environment type.ACPS language lambdaexpressionis \ncompiled into a packaging of a closure using a pointer to a fresh code block. That code block will havesome \ntype rt1 \u00d7rt2 . N. The denotationof this typeis some meta language type T1 . T2 . N. We perform an immediate \npartial application to an environment formed from the relevant free variables. This environment s type \nwill have denotation T1. Thus, the partial application s type has denotation T2 . N,making it compatible \nwith our logical relation. The effect of the closure packaging operation has been hidden using one of \nthe meta language s closures formed by the partial application. 9.2 Explicating Higher-Order Control \nFlow The translation from CC to Alloc moves from a higher-order, terminating language to a .rst-order \nlanguage that admits non\u00adtermination. As a consequence, the translation correctness proof must correspond \nto some explanation of whyCC s particular brand of higher-order control .ow leads to termination, and \nwhy the resulting .rst-order program returns an identical answer to the higher-order original. The basic \nproof technique has twomain pieces. First, the logical relation requires that anyvalue withacode type \nterminates withthe expected value when started in a heap and variable environment where the same condition \nholds for values that should have code type. Second,wetakeadvantageofthefactthat function de.nitions \noccur in dependencyorder in CC programs. Our variable binding restrictions enforcea lackofcycles via \ndependent types.We can prove by induction on the position of a code body in a program that anyexecution \nof it in a suitable starting state terminates with the correctvalue.Anycode pointerinvolvedintheexecution \neither comes earlier in the program, in which case we have its correctness by the inductivehypothesis; \nor the code pointer has been retrieved from a variable or heap slot, in which case its safety follows \nby an induction starting from our initialhypothesis and tracking the variable bindings and heap writes \nmadeby the program.  9.3 Garbage Collection Safety The soundness of the translation from Flat to Asm \ndepends on a delicate safety property of well-typed Flat programs. It is phrased in terms of the register \ntypings we have available at every program point, and it depends on the de.nition of pointer isomorphism \nwe gave in Section 2.7. THEOREM 4 (Heap rearrangement safety). For anyregister typing ., abstract heaps \nm1 and m2, and register .les R1 and R2, if: 1. For every register r with .(r)= ref, R1(r) is isomorphic \nto R2(r) with respect to m1 and m2. 2. For every register r with .(r)  = ref, R1(r)= R2(r). and p \nis a Flat program such that . f p, we have [ p] R1m1 = [ p] R2m2. This theorem says that it is safe to \nrearrange a heap if the relevant roots pointing into it are also rearranged equivalently. Well-typed \nFlat programs can t distinguish between the old and new situations. The denotations that we conclude \nare equal in the theorem are traces, so we have that valid Flat programs return identical results (if \nany) and make the same numbers of function calls in anyisomorphic initial states. The requirements on \nthe runtime system s new operation allow it to modify the heap arbitrarily on each call, so long as the \nnew heap and registers are isomorphic to the old heap and registers. The root set provided as an operand \nto new is used to determine the ap\u00adpropriate notion of isomorphism, so the heap rearrangement safety \ntheorem is critical to making the correctness proof go through.  9.4 PuttingIt AllTogether With all \nof the compiler phases proved, we can compose the proofs to forma correctness proof for theoverall compiler.Wegive \nthe formalversionofthe theoremdescribed informallyinthe Introduc\u00adtion.We use the notation T . n to denote \nthat trace T terminates with result n. THEOREM 5 (Compiler correctness). Let m . H be a heap ini\u00adtialized \nwith a closure for the top-level continuation, let p be a pointer to that closure, and let R be a register \n.le mapping the .rst register to p. Given a Source term e such that \u00b7f e : N, [ Lej] Rm . [ e]](). Even \nconsidering only the early phases of the compiler, it is dif.cult to give a correctness theorem in terms \nof equality for all source types. For instance, we can t compose parametrically the results for CPS transformation \nand closure conversion to yield a correctness theorem expressed with an equality between denota\u00adtions. \nThe problem lies in the lack of full abstraction for our se\u00admantics. Instead, we must use an alternate \nnotion of equality that only requires functions to agree at arguments that are denotations of terms. \nThis general notion also appears in the idea of pre-logical relations [HS99], a compositional alternative \nto logical relations. 10. Implementation The compiler implementation and documentation are available \non\u00adline at: http://ltamer.sourceforge.net/ We present lines-of-code counts for the different implementa\u00adtion \n.les, including proofs, in Figure 5. These .gures do not include 3520 lines of Coq code from a programming \nlanguage formalization support library that we have been developing in tandem with the compiler. Additionally, \nwe have 2716 lines of OCaml code supporting generic programming of syntactic support functions and their \ncorrectness proofs. Devel\u00adoping these re-usable pieces was the most time-consuming part of ouroveralleffort.We \nbelieve we improved our productivity an or\u00adder of magnitude by determining the right language representation \nFile LoC Source 31 ...to... 116 Linear 56 ...to... 115 CPS 87 ...to... 646 CC 185 ...to... 1321 Alloc \n217 ...to... 658 Flat 141 ...to... 868 Asm 111 File LoC Applicative dictionaries 119 Traces 96 GC safety \n741 Overall compiler 119 Figure 5. Project lines-of-code counts scheme and its library support code, \nand again by developing the generic programming system. With those pieces in place, imple\u00admenting the \ncompiler only took about one person-month of work, which we feel validates the general ef.cacyof our \ntechniques. Itisworth characterizinghowmuchof our implementation must be trusted to trust its outputs. \nOf course, the Coq system and the toolchain used to compile its programs (including operating system \nand hardware) must be trusted.Focusing on code speci.c to this project, we see that, if we want only \nto certify the behavior of particular output assembly programs, we must trust about 200 lines of code. \nThis .gure comes from taking a backwards slice from the statement of any individual program correctness \ntheorem. If we want to believe the correctness of the compiler itself, we must add additionally about \n100 lines to include the formalization of the Source language as well. 11. RelatedWork Moore produced \na veri.ed implementation of a compiler for the Piton language [Moo89] using the Boyer-Moore theorem prover. \nPiton did not have the higher-order features that make our present work interesting, and proofs in the \nBoyer-Moore tradition have fundamentally different trustworthiness characteristics than Coq proofs, being \ndependent on a large reasoning engine instead of a small proof-checkingkernel.However, the Pitonwork \ndealt with larger and more realistic source and target languages. The VLISP project [GRW95] produced \na Scheme system with a rigorous but non-mechanized proof of correctness. They also made heavy use of \ndenotational semantics, but they dealt with a dynamically-typed source language and so did not encounter \nmany of the interesting issues reported here related to type-preserving compilation. Semantics preservation \nproofs have been published before for individual phases of type-preserving compilers, including closure \nconversion [MMH95]. All of these proofs that we are aware of use operational semantics, forfeiting the \nadvantages of denotational semantics for mechanization that we have shown here. Recent work has implemented \ntagless interpreters [PTS02] us\u00ading generalized algebraic datatypes and other features related to dependent \ntyping.Tagless interpretershave muchin common with our approach to denotational semantics in Coq, but \nwe are not awareofanyproofsbeyond type safety carried outin such settings. The CompCert project [Ler06] \nhas used Coq to produce a cer\u00adti.ed compiler for a subset of C. Because of this source language choice, \nCompCert has not required reasoning about nested vari\u00adable scopes, .rst-class functions based on closures, \nor dynamic al\u00adlocation. On the other hand, theydeal with larger and more realis\u00adtic source and target \nlanguages. CompCert uses non-dependently\u00adtyped abstract syntax and operational semantics, in contrast \nto our use of dependent types and denotational semantics; and we focus more on proof automation. Manyadditional \npointers to work on compiler veri.cation can be found in the bibliographyby Dave [Dav03]. 12. Conclusion \nWehave outlineda non-trivial case studyin certi.cationof com\u00adpilers for higher-order programming languages. \nOur results lend credence to the suitability of our implementation strategy: the en\u00adcoding of language \nsyntax and static semantics using dependent types, along with the use of denotational semantics targeting \na rich but formalized meta language.Wehave describedhowgeneric pro\u00adgramming and proving can be used to \nease the development of type\u00adpreserving compilers and their proofs, and we have demonstrated how the \ncerti.cation of type-preserving compilers is congenial to automated proof. We hope to expand these techniques \nto larger and more realis\u00adtic source and target languages. Our denotational approach natu\u00adrally extends \nto features that can be encoded directly in CIC, in\u00adcluding (impredicative) universal types, (impredicative) \nexistential types, lists, and trees. Handling of effectful features like non\u00adtermination and mutability \nwithout .rst compiling to .rst-order form (as we ve done here in the later stages of the compiler) is \nan interesting open problem. We also plan to investigate further means of automating compiler correctness \nproofs andfactorizing useful aspects of them into re-usable libraries. There remains plenty of room to \nimprove the developer experi\u00adence in using our approach. Programming with dependent types has long been \nknown to be tricky. We implemented our proto\u00adtype by slogging through the messy details for the small \nnum\u00adber of features that we ve included. In scaling up to realistic lan\u00adguages, the costs of doing so \nmay proveprohibitive. Some recently\u00adproposed techniques for simplifying dependently-typed Coq pro\u00adgramming \n[Soz06] may turn out to be useful. Coercions between different dependent types appear frequently in the \napproach we follow. It turns out that effectivereasoning about coercions in type theory requires something \nmore than the compu\u00adtational equivalences that are used in systems like CIC. In Coq, this reasoning is \noften facilitated by adding an axiom describing the computational behavior of coercions. Ad-hoc axioms \nare a conve\u00adnientwayofextendingaproof assistant slogic,buttheirloosein\u00adtegration has drawbacks. Coq sbuilt-intype \nequivalence judgment, which is applied automatically during manystages of proof check\u00ading, will not takethe \naxioms into account. Instead, theymust be ap\u00adplied in explicit proofs. New languages like Epigram [MM04] \nde\u00adsign their type equivalence judgments tofacilitate reasoning about coercions. Future certi.ed compiler \nprojects might bene.t from be\u00ading developed in such environments, modulo the current immatu\u00adrity of their \ndevelopment tools compared to what Coq offers. Al\u00adternatively, it is probably worth experimenting with \nthe transplan\u00adtation of some of these new ideas into Coq. Acknowledgments Thanks to Manu Sridharan and \nthe anonymous referees for helpful comments on drafts of this paper. References [ABF+05] Brian E. Aydemir, \nAaron Bohannon, Matthew Fairbairn, J. NathanFoster, Benjamin C. Pierce, Peter Sewell, Dim\u00aditrios Vytiniotis, \nGeoffreyWashburn, StephanieWeirich, and Steve Zdancewic. Mechanized metatheory for the masses: ThePOPLMARKchallenge. \nIn Proc. TPHOLs,pages 50 65, 2005. [BC04] Yves Bertot and Pierre Cast\u00b4eran. Interactive Theorem Proving \nand Program Development. Coq Art: The Calculus of Inductive Constructions. Texts in Theoretical Computer \nScience. Springer Verlag, 2004. [Bou97] Samuel Boutin. Using re.ection to build ef.cient and certi.ed \ndecision procedures. In Proc. STACS, pages 515 529, 1997. [Dav03] Maulik A. Dave. Compiler veri.cation: \na bibliography. SIGSOFT Softw. Eng. Notes, 28(6):2 2, 2003. [dB72] Nicolas G. de Bruijn. Lambda-calculus \nnotation with nameless dummies: atool for automatic formal manipulation with application to the Church-Rosser \ntheorem. Indag. Math., 34(5):381 392, 1972. [Gim95] Eduardo Gim\u00b4enez. Codifying guarded de.nitions with \nrecursive schemes. In Proc. TYPES, pages 39 59. Springer-Verlag, 1995. [GRW95] Joshua D. Guttman, John \nD. Ramsdell, and Mitchell Wand. VLISP: A veri.ed implementation of Scheme. Lisp and Symbolic Computation, \n8(1/2):5 32, 1995. [HS99] Furio Honsell and Donald Sannella. Pre-logical relations. In Proc. CSL, pages \n546 561, 1999. [Ler06] Xavier Leroy. Formal certi.cation of a compiler back-end or: programming a compiler \nwith a proof assistant. In Proc. POPL, pages 42 54, 2006. [MM04] Conor McBride and James McKinna. The \nview from the left. J. Functional Programming, 14(1):69 111, 2004. [MMH95] Yasuhiko Minamide, Greg Morrisett, \nand Robert Harper. Typed closure conversion. Technical Report CMU-CS\u00adFOX-95-05, Carnegie Mellon University, \n1995. [Moo89] J. Strother Moore. A mechanically veri.ed language implementation. J. Automated Reasoning, \n5(4):461 492, 1989. [MSLL07] Andrew McCreight, Zhong Shao, Chunxiao Lin, and Long Li. Ageneral framework \nfor certifying garbage collectors and their mutators. In Proc. PLDI, 2007. [MWCG99] GregMorrisett, David \nWalker, Karl Crary, and Neal Glew. From System F to typed assembly language. ACM Trans. Program. Lang. \nSyst., 21(3):527 568, 1999. [PE88] F. Pfenning and C. Elliot. Higher-order abstract syntax. In Proc. \nPLDI, pages 199 208, 1988. [Plo73] G. D. Plotkin. Lambda-de.nability and logical relations. Memorandum \nSAI-RM-4, University of Edinburgh, 1973. [Plo75] Gordon D. Plotkin. Call-by-name, call-by-value, and \nthe lambda calculus. Theoretical Computer Science,1:125 159, 1975. [PS99] Frank Pfenning and Carsten \nSch\u00a8urmann. System descrip\u00adtion: Twelf -a meta-logical framework for deductive sys\u00adtems. In Proc. CADE, \npages 202 206, 1999. [PTS02] Emir Pasalic, Walid Taha, and Tim Sheard. Tagless staged interpreters for \ntyped languages. In Proc. ICFP, pages 218 229, 2002. [She04] Tim Sheard. Languages of the future. pages \n116 119, 2004. In Proc. OOPSLA, [Soz06] Matthieu Sozeau. Subset coercions in Coq. In Proc. TYPES, 2006. \n[TMC+96] D. Tarditi, G. Morrisett, P. Cheng, C. Stone, R. Harper, and P. Lee. TIL: a type-directed optimizing \ncompiler for ML. In Proc. PLDI, pages 181 192, 1996.    \n\t\t\t", "proc_id": "1250734", "abstract": "<p>We present a certified compiler from the simply-typed lambda calculus to assembly language. The compiler is certified in the sense that it comes with a machine-checked proof of semantics preservation, performed with the Coq proof assistant. The compiler and the terms of its several intermediate languages are given dependent types that guarantee that only well-typed programs are representable. Thus, type preservation for each compiler pass follows without any significant \"proofs\" of the usual kind. Semantics preservation is proved based on denotational semantics assigned to the intermediate languages. We demonstrate how working with a type-preserving compiler enables type-directed proof search to discharge large parts of our proof obligations automatically.</p>", "authors": [{"name": "Adam Chlipala", "author_profile_id": "81100341086", "affiliation": "University of California, Berkeley, Berkeley, CA", "person_id": "P707597", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1250734.1250742", "year": "2007", "article_id": "1250742", "conference": "PLDI", "title": "A certified type-preserving compiler from lambda calculus to assembly language", "url": "http://dl.acm.org/citation.cfm?id=1250742"}